MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1301?1309,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discriminating Gender on Twitter
John D. Burger and John Henderson and George Kim and Guido Zarrella
The MITRE Corporation
202 Burlington Road
Bedford, Massachusetts, USA 01730
{john,jhndrsn,gkim,jzarrella}@mitre.org
Abstract
Accurate prediction of demographic attributes from
social media and other informal online content is
valuable for marketing, personalization, and legal in-
vestigation. This paper describes the construction of
a large, multilingual dataset labeled with gender, and
investigates statistical models for determining the
gender of uncharacterized Twitter users. We explore
several different classifier types on this dataset. We
show the degree to which classifier accuracy varies
based on tweet volumes as well as when various
kinds of profile metadata are included in the models.
We also perform a large-scale human assessment us-
ing Amazon Mechanical Turk. Our methods signifi-
cantly out-perform both baseline models and almost
all humans on the same task.
1 Introduction
The rapid growth of social media in recent years, exem-
plified by Facebook and Twitter, has led to a massive
volume of user-generated informal text. This in turn has
sparked a great deal of research interest in aspects of so-
cial media, including automatically identifying latent de-
mographic features of online users. Many latent features
have been explored, but gender and age have generated
great interest (Schler et al, 2006; Burger and Henderson,
2006; Argamon et al, 2007; Mukherjee and Liu, 2010;
Rao et al, 2010). Accurate prediction of these features
would be useful for marketing and personalization con-
cerns, as well as for legal investigation.
In this work, we investigate the development of high-
performance classifiers for identifying the gender of
Twitter users. We cast gender identification as the ob-
vious binary classification problem, and explore the use
of a number of text-based features. In Section 2, we de-
scribe our Twitter corpus, and our methods for labeling
a large subset of this data for gender. In Section 3 we
discuss the features that are used in our classifiers. We
describe our Experiments in Section 4, including our ex-
ploration of several different classifier types. In Section 5
we present and analyze performance results, and discuss
some directions for acquiring additional data by simple
self-training techniques. Finally in Section 6 we summa-
rize our findings, and describe extensions to the work that
we are currently exploring.
2 Data
Twitter is a social networking and micro-blogging plat-
form whose users publish short messages or tweets. In
late 2010, it was estimated that Twitter had 175 million
registered users worldwide, producing 65 million tweets
per day (Miller, 2010). Twitter is an attractive venue
for research into social media because of its large vol-
ume, diverse and multilingual population, and the gener-
ous nature of its Terms of Service. This has led many re-
searchers to build corpora of Twitter data (Petrovic et al,
2010; Eisenstein et al, 2010). In April 2009, we began
sampling data from Twitter using their API at a rate of
approximately 400,000 tweets per day. This represented
approximately 2% of Twitter?s daily volume at the time,
but this fraction has steadily decreased to less than 1% by
2011. This decrease is because we sample roughly the
same number of tweets every day while Twitter?s overall
volume has increased markedly. Our corpus thus far con-
tains approximately 213 million tweets from 18.5 million
users, in many different languages.
In addition to the tweets that they produce, each Twitter
user has a profile with the following free-text fields:
? Screen name (e.g., jsmith92, kingofpittsburgh)
? Full name (e.g., John Smith, King of Pittsburgh)
? Location (e.g., Earth, Paris)
? URL (e.g., the user?s web site, Facebook page, etc.)
? Description (e.g., Retired accountant and grandfa-
ther)
All of these except screen name are completely op-
tional, and all may be changed at any time. Note that none
1301
Users Tweets
Training 146,925 3,280,532
Development 18,380 403,830
Test 18,424 418,072
Figure 1: Dataset Sizes
of the demographic attributes we might be interested in
are present, such as gender or age. Thus, the existing
profile elements are not directly useful when we wish to
apply supervised learning approaches to classify tweets
for these target attributes. Other researchers have solved
this problem by using labor-intensive methods. For ex-
ample, Rao et al (2010) use a focused search methodol-
ogy followed by manual annotation to produce a dataset
of 500 English users labeled with gender. It is infeasible
to build a large multilingual dataset in this way, however.
Previous research into gender variation in online dis-
course (Herring et al, 2004; Huffaker, 2004) has found
it convenient to examine blogs, in part because blog sites
often have rich profile pages, with explicit entries for gen-
der and other attributes of interest. Many Twitter users
use the URL field in their profile to link to another facet
of their online presence. A significant number of users
link to blogging websites, and many of these have well-
structured profile pages indicating our target attributes. In
many cases, these are not free text fields. Users on these
sites must select gender and other attributes from drop-
down menus in order to populate their profile informa-
tion. Accordingly, we automatically followed the Twitter
URL links to several of the most represented blog sites
in our dataset, and sampled the corresponding profiles.
By attributing this blogger profile information to the as-
sociated Twitter account, we created a corpus of approx-
imately 184,000 Twitter users labeled with gender.
We partitioned our dataset by user into three distinct
subsets, training, development, and test, with sizes as in-
dicated in Figure 1. That is, all the tweets from each user
are in a single one of the three subsets. This is the corpus
we use in the remainder of this paper.
This method of gleaning supervised labels for our
Twitter data is only useful if the blog profiles are in turn
accurate. We conducted a small-scale quality assurance
study of these labels. We randomly selected 1000 Twitter
users from our training set and manually examined the
description field for obvious indicators of gender, e.g.,
mother to 3 boys or just a dude. Only 150 descriptions
(15% of the sample) had such an explicit gender cue. 136
of these also had a blog profile with the gender selected,
and in all of these the gender cue from the user?s Twit-
ter description agreed with the corresponding blog pro-
file. This may only indicate that people who misrepresent
their gender are simply consistent across different aspects
of their online presence. However, the effort involved in
maintaining this deception in two different places sug-
gests that the blog labels on the Twitter data are largely
reliable.
Initial analysis using the blog-derived labels showed
that our corpus is composed of 55% females and 45%
males. This is consistent with the results of an earlier
study which used name/gender correlations to estimate
that Twitter is 55% female (Heil and Piskorski, 2009).
Figure 2 shows several statistics broken down by gender,
including the Twitter users who did not indicate their gen-
der on their blog profile. In our dataset females tweet at a
higher rate than males and in general users who provide
their gender on their blog profile produce more tweets
than users who do not. Additionally, of the 150 users
who provided a gender cue in their Twitter user descrip-
tion, 105 were female (70%). Thus, females appear more
likely to provide explicit indicators about their gender in
our corpus.
The average number of tweets per user is 22 and is
fairly consistent across our traing/dev/test splits. There
is wide variance, however, with some users represented
by only a single tweet, while the most prolific user in our
sample has nearly 4000 tweets.
It is worth noting that many Twitter users do not tweet
in English. Table 3 presents an estimated breakdown of
language use in our dataset. We ran automatic language
ID on the concatenated tweet texts of each user in the
training set. The strong preponderance of English in our
dataset departs somewhat from recent studies of Twitter
language use (Wauters, 2010). This is likely due in part to
sampling methodology differences between the two stud-
ies. The subset of Twitter users who also use a blog site
may be different from the Twitter population as a whole,
and may also be different from the users tweeting during
the three days of Wauters?s study. There are also possible
longitudinal differences: English was the dominant lan-
guage on Twitter when the online service began in 2006,
and this was still the case when we began sampling tweets
in 2009, but the proportion of English tweets had steadily
dropped to about 50% in late 2010. Note that we do not
use any explicit encoding of language information in any
of the experiments described below.
Our Twitter-blog dataset may not be entirely represen-
tative of the Twitter population at general, but this has
at least one advantage. As with any part of the Inter-
net, spam is endemic to Twitter. However by sampling
only Twitter users with blogs we have largely filtered out
spammers from our dataset. Informal inspection of a few
thousand tweets revealed a negligible number of commer-
cial tweets.
3 Features
Tweets are tagged with many sources of potentially dis-
criminative metadata, including timestamps, user color
1302
Users Tweets Mean tweets
Count Percentage Count Percentage per user
Female 100,654 42.3% 2,429,621 47.7% 24.1
Male 83,075 35.0 1,672,813 32.8 20.1
Not provided 53,817 22.7 993,671 19.5 18.5
Figure 2: Gender distribution in our blog-Twitter dataset
Language Users Percentage
English 98,004 66.7%
Portuguese 21,103 14.4
Spanish 8,784 6.0
Indonesian 6,490 4.4
Malay 1,401 1.0
German 1,220 0.8
Chinese 985 0.7
Japanese 962 0.7
French 878 0.6
Dutch 761 0.5
Swedish 686 0.5
Filipino 643 0.4
Italian 631 0.4
Other 4,377 3.0
Figure 3: Language ID statistics from training set
preferences, icons, and images. We have restricted our
experiments to a subset of the textual sources of features
as listed in Figure 4.
We use the content of the tweet text as well as three
fields from the Twitter user profile described in Section 2:
full name, screen name, and description. For each user in
our dataset, a field is in general a set of text strings. This
is obviously true for tweet texts but is also the case for
the profile-based fields since a Twitter user may change
any part of their profile at any time. Because our sam-
ple spans points in time where users have changed their
screen name, full name or description, we include all of
the different values for those fields as a set. In addition,
a user may leave their description and full name blank,
which corresponds to the empty set.
In general, our features are quite simple. Both word-
and character-level ngrams from each of the four fields
are included, with and without case-folding. Our fea-
ture functions do not count multiple occurrences of the
same ngram. Initial experiments with count-valued fea-
ture functions showed no appreciable difference in per-
formance. Each feature is a simple Boolean indicator
representing presence or absence of the word or character
ngram in the set of text strings associated with the partic-
ular field. The extracted set of such features represents
the item to the classifier.
For word ngrams, we perform a simple tokenization
Feature extraction
Char
ngrams
Word
ngrams
Distinct
features
Screen name 1?5 none 432,606
Full name 1?5 1 432,820
Description 1?5 1?2 1,299,556
Tweets 1?5 1?2 13,407,571
Total 15,572,522
Figure 4: Feature types and counts
that separates words at transitions between alphanumeric
characters and non-alphanumeric.1 We make no attempt
to tokenize unsegmented languages such as Chinese, nor
do we perform morphological analysis on language such
as Korean; we do no language-specific processing at all.
We expect the character-level ngrams to extract useful in-
formation in the case of such languages.
Figure 4 indicates the details and feature counts for the
fields from our training data. We ignore all features ex-
hibited by fewer than three users.
4 Experiments
We formulate gender labeling as the obvious binary clas-
sification problem. The sheer volume of data presents
a challenge for many of the available machine learning
toolkits, e.g. WEKA (Hall et al, 2009) orMALLET (Mc-
Callum, 2002). Our 4.1 million tweet training corpus
contains 15.6 million distinct features, with feature vec-
tors for some experiments requiring over 20 gigabytes
of storage. To speed experimentation and reduce the
memory footprint, we perform a one-time feature genera-
tion preprocessing step in which we convert each feature
pattern (such as ?caseful screen name character trigram:
Joh?) to an integer codeword. The learning algorithms
do not access the codebook at any time and instead deal
solely with vectors of integers. We compress the data fur-
ther by concatenating all of a user?s features into a single
vector that represents the union of every tweet produced
by that user. This condenses the dataset to about 180,000
vectors occupying 11 gigabytes of storage.
We performed initial feasibility experiments using a
wide variety of different classifier types, including Sup-
port Vector Machines, Naive Bayes, and Balanced Win-
1We use the standard regular expression pattern \b.
1303
now2 (Littlestone, 1988). These initial experiments were
based only on caseful word unigram features from tweet
texts, which represent less than 3% of the total feature
space but still include large numbers of irrelevant fea-
tures. Performance as measured on the development set
ranged from Naive Bayes at 67.0% accuracy to Balanced
Winnow2 at 74.0% accuracy. A LIBSVM (Chang and
Lin, 2001) implementation of SVM with a linear ker-
nel achieved 71.8% accuracy, but required over fifteen
hours of training time while Winnow needed less than
seven minutes. No classifier that we evaluated was able
to match Winnow?s combination of accuracy, speed, and
robustness to increasing amounts of irrelevant features.
We built our own implementation of the BalancedWin-
now2 algorithm which allowed us to iterate repeatedly
over the training data on disk rather than caching the en-
tire dataset in memory. This reduced our memory re-
quirements to the point that we were able to train on the
entire dataset using a single machine with 8 gigabytes of
RAM.
We performed a grid search to select learning parame-
ters by measuring their affect on Winnow?s performance
on the development set. We found that two sets of pa-
rameters were required: a low learning rate (0.03) was
effective when using only one type of input feature (such
as only screen name features, or only tweet text features),
and a higher learning rate (0.20) was required when mix-
ing multiple types of features in one classifier. In both
cases we used a relatively large margin (35%) and cooled
the learning rate by 50% after each iteration.
These learning parameters were used during all of the
experiments that follow. All gender prediction models
were trained using data from the training set and evalu-
ated on data from the development set. The test set was
held out entirely until we finalized our best performing
models.
4.1 Field combinations
We performed a number of experiments with the Winnow
algorithm described above. We trained it on the train-
ing set and evaluated on the development set for each of
the four user fields in isolation, as well as various com-
binations, in order to simulate different use cases for sys-
tems that perform gender prediction from social media
sources. In some cases we may have all of the metadata
fields available above, while in other cases we may only
have a sample of a user?s tweet content or perhaps just
one tweet. We simulated the latter condition by randomly
selecting a single tweet for each dev and test user; this
tweet was used for all evaluations of that user under the
single-tweet condition. Note, however, that for training
the single tweet classifier, we do not concatenate all of a
user?s tweets as described above. Instead, we pair each
user in the training set with each of their tweets in turn,
in order to take advantage of all the training data. This
amounted to over 3 million training instances for the sin-
gle tweet condition.
We paid special attention to three conditions: single
tweet, all fields, and all tweets. For these conditions, we
evaluated the learned models on the training data, the de-
velopment set, and the test set, to study over-training and
generalization. Note that for all experiments, the evalua-
tion includes some users who have left their full name or
description fields blank in their profile.
In all cases, we compare results to a maximum likeli-
hood baseline that simply labels all users female.
4.2 Human performance
We wished to compare our classifier?s efficacy to human
performance on the same task. A number of researchers
have recently experimented with the use of Amazon Me-
chanical Turk (AMT) to create and evaluate human lan-
guage data (Callison-Burch and Dredze, 2010). AMT
and other crowd-sourcing platforms allow simple tasks to
be posted online for large numbers of anonymous work-
ers to complete.
We used AMT to measure human performance on gen-
der determination for the all tweets condition. Each AMT
worker was presented with all of the tweet texts from
a single Twitter user in our development set and asked
whether the author was male or female. We redundantly
assigned five workers to each Twitter user, for a total of
91,900 responses from 794 different workers. We experi-
mented with a number of ways to combine the five human
labels for each item, including a simple majority vote and
a more sophisticated scheme using an expectation maxi-
mization algorithm.
4.3 Self-training
Our final experiments were focused on exploring the use
of unlabeled data, of which we have a great deal. We
performed some initial experiments on a self-training ap-
proach to labeling more data. We trained the all-fields
classifier on half of our training data, and applied it to the
other half. We trained a new classifier on this full train-
ing set, which now included label errors introduced by the
limitations of the first classifier. This provided a simula-
tion of a self-training setup using half the training data.
Any robust gains due to self-training should be revealed
by this setup.
5 Results
5.1 Field combinations
Figure 5 shows development set performance on various
combinations of the user fields, all of which outperform
the maximum likelihood baseline that classifies all users
as female. The single most informative field with respect
1304
Baseline (F) 54.9%
One tweet text 67.8
Description 71.2
All tweet texts 75.5
Screen name (e.g. jsmith92) 77.1
Full name (e.g. John Smith) 89.1
Tweet texts + screen name 81.4
Tweet texts + screen name + description 84.3
All four fields 92.0
Figure 5: Development set accuracy using various fields
Condition Train Dev Test
Baseline (F) 54.8% 54.9 54.3
One tweet text 77.8 67.8 66.5
Tweet texts 77.9 75.5 74.5
All fields 98.6 92.0 91.8
Figure 6: Accuracy on the training, development and test sets
to gender is the user?s full name, which provides an accu-
racy of 89.1%. Screen name is often a derivative of full
name, and it too is informative (77.1%), as is the user?s
self-assigned description (71.2).
Using only tweet texts performs better than using only
the user description (75.5% vs. 71.2). Tweet texts are
sufficient to decrease the error by nearly half over the
all-female prior. It appears that the tweet texts con-
vey more about a Twitter user?s gender than their own
self-descriptions. Even a single (randomly selected)
tweet text contains some gender-indicative information
(67.2%). These results are similar to previous work. Rao
et al (2010) report results of 68.7% accuracy on gender
from tweet texts alone using an ngram-only model, ris-
ing to 72.3 with hand-crafted ?sociolinguistic-based? fea-
tures. Test set differences aside, this is comparable with
the ?All tweet texts? line in Figure 5, where we achieve
an accuracy of 75.5%.
Performance of models built from various aggregates
of the four basic fields are shown in Figure 5 as well. The
combination of tweet texts and a screen name represents
a use case common to many different social media sites,
such as chat rooms and news article comment streams.
The performance of this combination (81.4%) is signif-
icantly higher than either of the individual components.
As we have observed, full name is the single most infor-
mative field. It out-performs the combination of the other
three fields, which perform at 84.3%. Finally, the classi-
fier that has access to features from all four fields is able
to achieve an accuracy of 92.0%.
The final test set accuracy is shown in Figure 6. This
test set was held out entirely during development and has
been evaluated only with the four final models reported
Rank MI Feature f P (Female|f)
1 0.0170 ! 0.601
2 0.0164 : 0.656
3 0.0163 lov 0.687
4 0.0162 love 0.680
5 0.0161 lov 0.676
6 0.0160 love 0.689
7 0.0160 ! 0.618
8 0.0149 :) 0.697
9 0.0148 y! 0.687
10 0.0145 my 0.637
11 0.0143 love 0.691
12 0.0143 haha 0.705
13 0.0141 my 0.634
14 0.0140 my 0.637
15 0.0140 :) 0.697
16 0.0139 my 0.634
17 0.0138 ! i 0.711
18 0.0138 hah 0.698
19 0.0137 hah 0.714
20 0.0135 so 0.661
21 0.0134 haha 0.714
22 0.0132 so 0.661
23 0.0128 i 0.618
24 0.0127 ooo 0.708
25 0.0126 ! i 0.743
26 0.0123 i lov 0.728
27 0.0120 ove 0.671
28 0.0117 ay! 0.718
29 0.0116 aha 0.678
30 0.0116 <3 0.856
31 0.0115 cute 0.826
32 0.0114 i lo 0.704
33 0.0114 :)$ 0.701
34 0.0110 :( 0.731
35 0.0109 :)$ 0.701
36 0.0109 !$ 0.614
37 0.0107 ahah 0.716
38 0.0106 <3 0.857
464 0.0051 ht | 0.506
465 0.0051 hank 0.641
466 0.0051 too 0.659
467 0.0051 yay! 0.818
468 0.0051 http | 0.506
469 0.0051 htt | 0.506
624 0.0047 Googl | 0.317
625 0.0047 ing! 0.718
626 0.0047 hair 0.749
627 0.0047 b 0.573
628 0.0047 y : 0.725
629 0.0046 Goog | 0.318
Figure 7: A selection of tweet text features, ranked by mutual
information. Character ngrams in Courier, words in bold.
Underscores are spaces, $ matches the end of the tweet text.
| marks ?male? features.1305
in this figure. The difference between the scores on the
train and development sets show how well the model can
fit the data. There are features in the user name and user
screen name fields that make the data trivially separable.
The tweet texts, however, present more ambiguity for the
learners. The difference between the development and
test set scores suggest that only minimal hill-climbing oc-
curred during our development.
We have performed experiments to better understand
how performance scales with training data size. Figure 8
shows how performance increases for both the all-fields
and tweet-texts-only classifiers as we train on more users,
with little indication of leveling off.
As discussed in Section 2, there is wide variance in
the number of tweets available from different users. In
Figure 9 we show how the tweet text classifier?s accu-
racy increases as the number of tweets from the user in-
creases. Each point is the average classifier accuracy for
the user cohort with exactly that many tweets in our dev
set. Performance increases given more tweets, although
the averages get noisy for the larger tweet sets, due to
successively smaller cohort sizes.
Some of the most informative features from tweet texts
are shown in Figure 7, ordered by mutual information
with gender. There are far more of these strong features
for the female category than the male: only five of the top
1000 features are associated more strongly with males,
i.e. they have lower P (Female|feature) than the prior,
P (Female) = 0.55.
Some of these features are content-based (hair, and
several fragments of love), while others are stylistic (ooo,
several emoticons). The presence of http as a strong
male feature might be taken to indicate that men include
links in their tweet texts far more often than women,
but a cursory examination seems to show instead that
women are simply more likely to include ?bare? links,
e.g., emnlp.org vs. http://emnlp.org.
5.2 Human performance
Figure 10 shows the results of the human performance
benchmarks using Amazon Mechanical Turk. The raw
per-response performance is 60.4%, only moderately bet-
ter than the all-female baseline. When averaged across
workers, however, this improves substantially, to 68.7.
This would seem to indicate that there were a few poor
workers who did many annotations, and in fact when we
limit the performance average to those workers who pro-
duced 100 or more responses, we do see a degradation to
62.2.
The problem of poor quality workers is endemic to
anonymous crowd sourcing platforms like Mechanical
Turk. A common way to combat this is to use redun-
dancy, with a simple majority vote to choose among mul-
tiple responses for each item. This allows us to treat the
Baseline 54.9
Average response 60.4
Average worker 68.7
Average worker (100 or more responses) 62.2
Worker ensemble, majority vote 65.7
Worker ensemble, EM-adjusted vote 67.3
Winnow all-tweet-texts classifier 75.5
Figure 10: Comparing with humans on the all tweet texts task
five workers who responded to each item as an ensem-
ble. As Figure 10 indicates, this provides some improve-
ment over the raw result (65.7% vs. 60.4). A different
approach, first proposed by Dawid and Skene (1979), is
to use an expectation maximization algorithm to estimate
the quality of each source of labels, as well as estimate the
posterior for each item. In this case, the first is an AMT
worker?s capability and the second is the distribution of
gender labels for each Twitter user.
The Dawid and Skene approach has previously been
applied to Mechanical Turk responses (Ipeirotis et al,
2010). We used their implementation on our AMT re-
sults but with only moderate improvement over the sim-
ple majority ensemble (67.3% vs. 65.7). All of the aggre-
gate human results are substantially below the all-tweet-
texts classifier score, suggesting that this is a difficult
task for people to perform. As Figure 11 indicates, most
workers perform below 80% accuracy, and less than 5%
of the prolific workers out-perform the automatic classi-
fier. These high-scoring workers may indeed be good at
the task, or they may have simply been assigned a less-
difficult subset of the data. Figure 12 illustrates this by
showing aligned worker performance and classifier per-
formance on the precise set of items that each worker
performed on. Here we see that, with few exceptions,
the automatic classifier performs as well or better than
the AMT workers on their subset.
5.3 Self-training
Finally, as described in Section 4.3, we performed some
initial experiments on a self-training approach to label-
ing more data. As described above the all-fields classi-
fier achieves an accuracy of 92% on the development set
when trained on the full training set. Training on half of
the training data results in a drop to 91.1%. The sec-
ond classifier trained on the full training set, but with
some label errors introduced by the first, had further de-
graded performance of 90.9%. Apparently the errorful la-
bels introduced by the simplistic self-training procedure
overwhelmed any new information that might have been
gained from the additional data. We are continuing to ex-
plore ways to use the large amounts of unsupervised data
in our corpus.
1306
Figure 8: Performance increases when training with more users
Figure 9: Performance increases with more tweets from target user
1307
Figure 11: Human accuracy in rank order (100 responses or more), with classifier performance (line)
Figure 12: Classifier vs. human accuracy on the same subsets (100 responses or more)
1308
6 Conclusion
In this paper, we have presented several configurations of
a language-independent classifier for predicting the gen-
der of Twitter users. The large dataset used for construc-
tion and evaluation of these classifiers was drawn from
Twitter users who also completed blog profile pages.
These classifiers were tested on the largest set of
gender-tagged tweets to date that we are aware of. The
best classifier performed at 92% accuracy, and the clas-
sifier relying only on tweet texts performed at 76% ac-
curacy. Human performance was assessed on this latter
condition, and only 5% of 130 humans performed 100 or
more classifications with higher accuracy than this ma-
chine.
In future work, we will explore how well such models
carry over to gender identification in other informal on-
line genres such as chat and forum comments. Further-
more, we have been able to assign demographic features
beside gender, including age and location, to our Twit-
ter dataset. We have begun to build classifiers for these
features as well.
Acknowledgements
The authors would like to thank the anonymous review-
ers. This work was funded under the MITRE Innovation
Program.
References
Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and
Jonathan Schler. 2007. Mining the blogosphere: Age,
gender, and the varieties of self-expression. First Monday,
12(9), September.
John D. Burger and John C. Henderson. 2006. An exploration
of observable features related to blogger age. In Computa-
tional Approaches to Analyzing Weblogs: Papers from the
2006 AAAI Spring Symposium. AAAI Press.
Chris Callison-Burch and Mark Dredze. 2010. Creating speech
and language data with Amazon?s Mechanical Turk. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechanical Turk,
CSLDAMT ?10. Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
A.P. Dawid and A.M. Skene. 1979. Maximum likelihood esti-
mation of observer error-rates using the EM algorithm. Jour-
nal of the Royal Statistical Society. Series C (Applied Statis-
tics), 28(1).
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith, and
Eric P. Xing. 2010. A latent variable model for geographic
lexical variation. In Conference on Empirical Methods on
Natural Language Processing.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations, 11(1).
Bill Heil and Mikolaj Jan Piskorski. 2009. New Twitter re-
search: Men follow men and nobody tweets. Harvard Busi-
ness Review, June 1.
Susan C. Herring, Inna Kouper, Lois Ann Scheidt, and Eli-
jah L. Wright. 2004. Women and children last: The discur-
sive construction of weblogs. In L. Gurak, S. Antonijevic,
L. Johnson, C. Ratliff, and J. Reyman, editors, Into the Bl-
ogosphere: Rhetoric, Community, and Culture of Weblogs.
http://blog.lib.umn.edu/blogosphere/.
David Huffaker. 2004. Gender similarities and differences in
online identity and language use among teenage bloggers.
Master?s thesis, Georgetown University. http://cct.
georgetown.edu/thesis/DavidHuffaker.pdf.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010.
Quality management on Amazon Mechanical Turk. In
Proceedings of the Second Human Computation Workshop
(KDD-HCOMP 2010).
Nick Littlestone. 1988. Learning quickly when irrelevant at-
tributes abound: A new linear-threshold algorithm. Machine
Learning, 2, April.
Andrew Kachites McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.cs.
umass.edu.
Claire Cain Miller. 2010. Why Twitter?s C.E.O.
demoted himself. New York Times, October 30.
http://www.nytimes.com/2010/10/31/
technology/31ev.html.
Arjun Mukherjee and Bing Liu. 2010. Improving gender clas-
sification of blog authors. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Process-
ing, Cambridge, MA, October. Association for Computa-
tional Linguistics.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. The
Edinburgh Twitter corpus. In Computational Linguistics in a
World of Social Media. AAAI Press. Workshop at NAACL.
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi
Gupta. 2010. Classifying latent user attributes in Twitter.
In 2nd International Workshop on Search and Mining User-
Generated Content. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James
Pennebaker. 2006. Effects of age and gender on blogging.
In Computational Approaches to Analyzing Weblogs: Papers
from the 2006 AAAI Spring Symposium. AAAI Press, March.
Robin Wauters. 2010. Only 50% of Twitter mes-
sages are in English, study says. TechCrunch, Febru-
ary 1. http://techcrunch.com/2010/02/24/
twitter-languages/.
1309
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 9?12,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Navigating Large Comment Threads With CoFi
Christine Doran, Guido Zarrella and John C. Henderson The MITRE Corporation Bedford, MA {cdoran,jzarrella,jhndrsn}@mitre.org  
Abstract 
Comment threads contain fascinating and use-ful insights into public reactions, but are chal-lenging to read and understand without computational assistance. We present a tool for exploring large, community-created com-ments threads in an efficient manner. 1 Introduction The comments made on blog posts and news arti-cles provide both immediate and ongoing public reaction to the content of the post or article. When a given site allows users to respond to each other (?threaded? responses), the comment sets become a genuine public conversation. However, this in-formation can be difficult to access. Comments are typically not indexed by search engines, the vol-ume is often enormous, and threads may continue to be added to over months or even years. This makes it hard to find particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the discussion at a high-level. Our goal in this work was to create a simple tool which would allow people to rapidly ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comment threads. 2 How CoFi works For a given set of comments, we create a distinct CoFi instance. Each instance is over a natural data set, e.g. all comments from a particular discussion group, comments attached to an individual news article, or tweets resulting from a topical search. Creating a CoFi instance has three steps: harvest-
ing the comments, clustering the comments, and responding to user interactions while they visualize and navigate (sorting and filtering) the dataset. 2.1 Harvesting the data Our comments are harvested from individual web sites. These need not be in English, or even in a single language. Typically, sites use proprietary javascript to present comments. Each web site has a unique interface and formatting to serve the comments to web browsers, and there is no general purpose tool to gather comments everywhere. The CoFi approach has been to factor this part of the problem into one harvesting engine per web site. Some sites provide an API that simplifies the prob-lem of harvesting comments that contain particular keywords. On other sites, there seems to be no re-liable alternative to developer ingenuity when it comes to altering the harvesting engines to ac-commodate data formats. Thus, we note that the harvesting activity is only semi-automated. 2.2 Clustering the data Once harvesting is complete, the rest of the process is automatic. Clusters are generated and labeled using a pipeline of machine learning tools. The open source package MALLET provides many of our document ingestion and clustering components (McCallum, 2002). Our processing components are language-independent and can be used with non-English or mixed language data sets. Specifically, we use a combination of Latent Dirichlet Allocation (LDA), K-Means clustering, and calculation of mutual information. LDA mod-els each document (aka comment) as a mixture of latent topics, which are in turn comprised of a probability distribution over words (Chen, 2011, gives a good overview). It?s an unsupervised algo-rithm that performs approximate inference. The topics it infers are the ones that best explain the statistical distributions of words observed in the 
9
data. It is highly parallelizable and so it scales well to very large data sets. In practice we ask LDA to search for 5k topics, where k is the number of clus-ters we will eventually display to the user. The second step is to perform K-Means cluster-ing on the documents, where the documents are represented as a mixture of LDA topics as de-scribed above, and the clustering chooses k clusters that minimize the differences between documents in the cluster while maximizing the difference be-tween documents that are not in the same clusters. This step is fast, in part because of the fact that we have already reduced the number of input features down to 5k (rather than having one feature for each word observed in the entire dataset.) Finally, we give the clusters titles by perform-ing a calculation of mutual information (MI) for each word or bigram in each cluster. Specifically, clustering terms (both words and bigrams) that oc-cur frequently in one cluster but rarely in other clusters will receive high scores. The terms with the highest MI scores are used as cluster labels. One significant advantage of this completely unsupervised approach is that CoFi is more robust to the language of comment data, e.g. grammatical and spelling inconsistency, informal language, which are a challenge for rule-based and super-vised NLP tools.  In addition to the machine-generated topic clus-ters, CoFi allows user-defined topics. These are search terms and topic labels hand-created by a domain expert. CoFi partitions the comments into machine-generated topics and also assigns each comment to any of the matching predefined topics. This approach is useful for domain experts, ena-bling them to quickly find things they already know they want while allowing them to also take advantage of unexpected topics which emerge from the system clustering. 2.3 Creating the visualizations CoFi uses the JQuery, Flot, and g.Raphael javascript libraries to provide a dynamic, respon-sive interface. When the user visits a CoFi URL, the data is downloaded into their browser which then computes the visualization elements locally, allowing fast response times and offline access to 
the data. The JQuery library is central to all of the javascript processing that CoFi performs, and ensures that all features of the interface are cross-compatible with major browser versions. The interface provides the ability to drill down further into any data, allowing the user to click on any aspect of the analysis to obtain more detail. Since the visualization is calculated locally, the software can create dynamically updated timelines that show the user how any subset of their data has changed over time. It is also important to prioritize all data present-ed to the user, allowing them to focus on the most useful documents first. CoFi applies an automatic summarization technique to perform relevance sorting. We evaluated several state-of-the-art au-tomatic document summarization techniques and settled on a Kullback-Leibler divergence inspired by techniques described in Kumar et al (2009). The ?relevance? sort relies on a measure of how representative each comment is relative to the en-tire collection of comments that the user is viewing at the time. This allows us to rapidly rank tens of thousands of comments in the order of their rele-vance to a summary. Several of the approaches we tested were chosen from among the leaders of NIST?s 2004 Document Understanding Conference (DUC) summarization evaluation. Many of them used slight variants of KL divergence for sentence scoring. We also implemented Lin & Bilmes? (2010) Budgeted Maximization of Submodular Functions system, which performed best according to the DUC evaluation. However, even after apply-ing a scaling optimization inspired by the ?buck-shot? technique of Cutting et al (1992) the processing speed was still too slow for dealing with datasets containing more than 10000 small documents. The KL divergence approach scales linearly in the number of comments while still of-fering cutting edge qualitative performance. This means that the calculation can be done on the fly in javascript in the browser when the user requests a relevance sort. This allows CoFi to tailor the re-sults to whatever sub-selection of data is currently being displayed. For CoFi?s typical use cases this computation can be completed in under 2 seconds. 
10
3 The CoFi Interface CoFi takes a set of comments and produces the interactive summary you see in Figure 1. CoFi works best when a user is operating with between 200 and 10,000 comments. With small numbers of comments, there may not be enough data for CoFi to find interesting topic clusters. With very large numbers of comments, a user?s web browser may struggle to display all comments while maintaining sufficient responsiveness.  The raw data is available for inspection in many ways. The summary screen in Figure 1 presents a list of automatically-discovered clusters on the left-hand side (typically 10-30, this is a parameter of the clustering algorithm), the posting volume time-line on the top, and some overall statistics and characteristic words and posters in the middle. The user can return to this view at any point using the Overview button. At the top of the page, CoFi pre-sents the total number of comments and partici-pants, and a summary of the level of threading, which is a good indicator of how interactive the data set is. Where community ratings appear on a site, we also present the highest and lowest rated comments (this is solely based on the community rating, and not on our relevance calculation). In the 
middle of the display are two hyperlinked word clouds containing the highest frequency words and users. Selecting one of the top words or users has the same effect as searching for that term in one of the Search boxes?both of these approaches will present the user with matching comments with the term highlighted, and color coding to indicate clus-ter membership. The links from most popular words and most active users bring up a multi-graph view as in Figure 3.  Each time a set of comments is selected, either via a cluster, full text search, or filtering on a par-ticular commenter, the set is presented to the user in a sorted order with the comments most repre-sentative of the set ordered above those that are less representative. In this way, the user can quick-ly get a handle on what the set is about without reading all of the items in detail. The comments can also be sorted into the original temporal order, which can be useful to see how a comment thread evolves over time, or to view an original comment and threaded replies in a nested ordering. Figure 2 shows a single cluster in CoFi. The full thread timeline now has a red overlay for the selected subset of comments.  
Figure 1: CoFi top level summary view 
11
 At the bottom of the cluster lists, there is a View All Comments option. Sorting the entire set by rel-evance gives a good snapshot of most and least useful comments in the thread. From any of the views, clicking on a user name will display all comments from that user, and clicking on the comment ID will present that sub-thread; top-level comments are numbered X, while replies are la-beled X.X.  The CoFi interface also allows the user to export individual comments, marking those comments as having been ?handled? and routed to a particular person. This makes it easier to incre-mentally process comments as they arrive. We have applied CoFi to 72 distinct data sets, including forum discussions, news article, blog and YouTube comments, Twitter and comments on regulatory changes submitted to government offic-es via Regulations.gov. These last documents are much longer than those CoFi was intended to han-dle, but CoFi was nonetheless able to support in-teresting analysis. In one instance, we identified a clear case of ?astroturfing? (fake grassroots movement) based on the CoFi clusters. Acknowledgements Over the course of this project, many people have supported our work. We?d particularly like to thank Mark Maybury, Robert Peller at USSOUTHCOM, and Marty Ryan, Robert Battle and Nathan Vuong at the MITRE Miami site.  This  
 technical data was produced for the U. S. Govern-ment under Contract No. W15P7T-11-C-F600, and is subject to the Rights in Technical Data-Noncommercial Items clause at DFARS 252.227-7013 (NOV 1995). ? 2012 The MITRE Corpora-tion. All Rights Reserved. Approved for Public Release: 12-1507. Distribution Unlimited. MITRE Document number MP120212. References Chen, Edwin (2011). Introduction to Latent Direchlet Allocation, http://blog.echen.me/2011/08/22/ introduction-to-latent-dirichlet-allocation/#comments Cutting, D., Karger, D., Pedersen, J., and Tukey, J. (1992). Scatter/Gather: a cluster-based approach to browsing large document collections. Proceedings of 15th Annual International ACM SIGIR conference, New York, NY, USA, 318-329 Kumar, C., P. Pingali, and V. Verma (2009). Estimating Risk Of Picking a Sentence for Document Summari-zation. Proceedings of CICLing 2009, LNCS 5449, 571-581. Lin, H. and Bilmes, J. (2010). Multi-document summa-rization via budgeted maximization of submodular functions. Proceedings of Human Language Tech-nologies 2010, Los Angeles, CA, USA, 912-920. McCallum, Andrew Kachites (2002). MALLET: A Ma-chine Learning for Language Toolkit. Mishne, Gilard and Natalie Glance (2006). Leave a re-ply: An Analysis of Weblog Comments. In Workshop on the Weblogging Ecosystem, 15th International World Wide Web Conference, May. 
Figure 3: The "small multiples" view of frequent contributors 
Figure 2: Single cluster view 
12
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 101?110,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Approved for Public Release; Distribution Unlimited. 13-1876
Discriminating Non-Native English with 350 Words
John Henderson, Guido Zarrella, Craig Pfeifer and John D. Burger
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730-1420, USA
{jhndrsn,jzarrella,cpfeifer,john}@mitre.org
Abstract
This paper describes MITRE?s participation in
the native language identification (NLI) task
at BEA-8. Our best effort performed at an ac-
curacy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the best per-
forming systems. We describe the variety
of machine learning approaches that we ex-
plored, including Winnow, language model-
ing, logistic regression and maximum-entropy
models. Our primary features were word and
character n-grams. We also describe several
ensemble methods that we employed for com-
bining these base systems.
1 Introduction
Investigations into the effect of authors? latent at-
tributes on language use have a long history in lin-
guistics (Labov, 1972; Biber and Finegan, 1993).
The rapid growth of social media has sparked in-
creased interest in automatically identifying author
attributes such as gender and age (Schler et al, 2006;
Burger and Henderson, 2006; Argamon et al, 2007;
Mukherjee and Liu, 2010; Rao et al, 2010). There
is also a long history of computational aids for lan-
guage pedagogy, both for first- and second-language
acquisition. In particular, automated native language
identification (NLI) is a useful aid to second lan-
guage learning. This is our first foray into NLI,
although we have recently described experiments
aimed at identifying the gender of unknown Twit-
ter authors (Burger et al, 2011). We performed well
using only character and word n-grams as evidence.
In the present work, we apply that same approach
to NLI, and combine it with several other baseline
classifiers.
In the remainder of this paper, we describe our
high-performing system for identifying the native
language of English writers. We explore a varied
set of learning algorithms and present two ensem-
ble methods used to produce a better system than
any of the individuals. In Section 2 we describe the
data and task in detail as well as the evaluation met-
ric. In Section 3 we discuss details of the particular
system configuration that scored best for us. We de-
scribe our experiments in Section 4, including our
exploration of several different classifier types and
parametrizations. In Section 5 we present and an-
alyze performance results, and inspect some of the
features that were useful in discrimination. Finally
in Section 6 we summarize our findings, and de-
scribe possible extensions to the work.
2 Task, data and evaluation
Native Language Identification was a shared task or-
ganized as part of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, 2013. The task was to identify an author?s
native language based on an English essay.
The data provided consisted of a set of 12,100
Test of English as a Foreign Language (TOEFL) ex-
aminations contributed by the Educational Testing
Service (Blanchard et al, to appear). These were
English essays written by native speakers of Arabic,
Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu, and Turkish. A set of 1000
essays for each language was identified as training
data, along with 100 per language for development,
101
and another 100 per language for a final test set. The
mean length of an essay is 348 words.
The primary evaluation metric for shared task
submissions was simple accuracy: the fraction of the
test essays for which the correct native language was
identified. A baseline accuracy would thus be about
9% (one out of eleven). Results were also reported
in terms of F-measure on a per-language basis. F-
measure is a harmonic mean of precision and recall:
F = 2PRP+R . For the evaluation, the precision de-
nominator was the number of items labeled with a
particular language by the system and the recall de-
nominator was the number of items marked with a
particular language in the reference set.
The training, development, and test sets all had
balanced distributions across the native languages,
so error rates and accuracy did not favor any partic-
ular language in any set.
3 System overview
The systems we used to generate results for the NLI
competition were all machine-learning-based, with
no handwritten rules or features. The final submitted
systems were ensembles built from the outputs and
confidence scores of independent eleven-way multi-
nomial classifiers.
3.1 Features
The features used to build these systems were
language-independent and were generated using the
same infrastructure designed for the experiments de-
scribed in Burger et al (2011).
We incorporated a variety of binary features into
our systems, each of which was hashed into a 64-bit
numeric representation using MurmurHash3 (Ap-
pleby, 2011). The bulk of our features were case-
sensitive word- and character-based n-grams, in
which a feature was turned ?on? if its sequence of
words or characters appeared at least once in the text
of an essay. We also added binary features describ-
ing surface characteristics of the text such as average
word length and word count. Features were sepa-
rated into tracks such that the word unigram ?i? and
the character unigram ?i? would each generate a dis-
tinct feature.
Part of speech tag n-grams were added to the
feature set after reviewing performance results in
Brooke and Hirst (2012). We used the Stan-
ford log-linear part of speech tagger described in
Toutanova et al (2003), with the english-left3words-
distsim.tagger pretrained model and the Penn Tree-
bank tagset. The tagger was run on each essay and
outputs were incorporated as sequence features with
n-grams up to length 5.
3.2 Classifiers
Carnie1 is a MITRE-developed linear classifier
that implements the Winnow2 algorithm of Carvalho
and Cohen (2006), generalized for multinomial clas-
sification. Carnie was developed to perform clas-
sification of short, noisy texts with many training
examples. It maintains one weight per feature per
output class, and performs multiplicative updates
that reinforce weights corresponding to the correct
class while penalizing weights associated with the
top-scoring incorrect class. The learner is mistake-
driven and performs an update of size  after an error
or when the ratio of weight masses of the correct and
top incorrect classes is below 1 + ?. It iterates over
the training data, cooling its updates after each itera-
tion. For the purposes of these experiments, an input
to Carnie was the text of a single TOEFL essay, and
the output was the highest scoring class and several
related scores.
SRI?s Language Modeling Toolkit (SRILM) is
a toolkit for sequence modeling that continues to
be relevant after more than a decade of develop-
ment (Stolcke, 2002). It can be used to both build
models of sequence likelihoods and to evaluate like-
lihoods of previously unseen sequences. Building
a multinomial text classifier with a language model
toolkit involves building one model for each target
class and choosing the label whose model gives the
highest probability.
Many smoothing methods are implemented by
SRILM, along with a variety of n-gram filter-
ing techniques. The out-of-the-box default con-
figuration produces trigram models with Good-
Turing smoothing. It worked well for this com-
petition. Using open vocabulary models (-unk),
turning off sentence boundary insertion (-no-sos
-no-eos) and treating each essay as one sentence
1It is named for entertainers who guess personal character-
istics of carnival goers.
102
worked best in our development environment.
LIBLINEAR is a popular open source library for
classification of large, sparse data. We experimented
with several of their standard Support Vector Ma-
chine and logistic regression configurations (Fan et
al., 2008). We selected multiclass `2-regularized
logistic regression with the dual-form solver and
default parameters. Inputs to the model were bi-
nary features generated from a single TOEFL essay.
Features for this model were generated by Carnie.
The model provided probability estimates for each
candidate output class (L1) for each essay, which
were then combined with the outputs of Carnie and
SRILM in an ensemble to produce a single predic-
tion.
3.3 Ensembles
The classifiers described above were selected for in-
clusion as components in a larger ensemble on the
basis of their performance and the observation that
errors committed by these systems were not highly
correlated. We used the entirety of our training data
for construction of each component system, leaving
scant data available for estimating parameters of en-
sembles. This scenario led us to choose naive Bayes
to combine the outputs of the original components.
Given h1, . . . , hk hypothesis labels from k differ-
ent systems, one approximates the conditional like-
lihood of the reference label P (R|H1 . . . Hk) using
the Bayes transform and the development set esti-
mates of P (Hi|R). One investigates all possible la-
bels to decode r? = argmaxr P (r)
?
i P (hi|r). The
class balance in every set we operated on made the
prior P (r) irrelevant for maximization and simpli-
fied many of the denominators along the way. This
is a typical formulation of naive Bayes.
Confidence All of our component systems pro-
duce scores as well as a predicted label. Carnie pro-
duces (non-probability) scores for all of the candi-
date labels, SRILM produces log-probabilities and
perplexities, and LIBLINEAR produces P (h|r), the
likelihood of each of the possible labels. We ex-
perimented with several transformations of those
scores to best use them to predict correctness of
their hypothesis. There were several graphical mod-
els we could use for folding these scores into the
Bayes ensemble, and we chose a simple, discretized
P (H,S|R). We evenly partitioned and relabeled our
system outputs according to their scores (S), and
used those partition labels in the Bayes ensemble.
Thus when a particular reference label was scored
in the ensemble during decoding, both its prediction
and score contributed to the label in the naive Bayes
table lookup.
3.4 Best configuration
We submitted five systems with a variety of con-
figurations. One of our systems was our individual
Carnie system on its own for calibration. The other
four were ensembles.
The best system we submitted was a Bayes en-
semble of the Carnie, SRILM, and LIBLINEAR
components each trained on the train+development
sets. Carnie was trained for twelve iterations with
 = 0.03, ? = 0.05, and a cooling rate of 0.1.
SRILM models were trained for open vocabulary
and the default trigram, Good-Turing setting. Lo-
gistic regression from LIBLINEAR was run with `2
regularization and using the dual form solver.
Parameters for the Bayes model were collected
from the development set when the components
were trained only on the training set. A grid search
was performed over likely candidates for ?, the
Dirichlet parameter, and ?, the number of score-
based partitions, resulting in ? = 0.03125 and ? =
2. The grid search was performed with the compo-
nent models trained only on the training set and us-
ing 10-fold cross validation on the development set.
4 Experiments
In all experiments described below, systems were
trained initially on the 9900 training examples alone,
with the 1100 item development set held back to al-
low for hyperparameter estimation. When prepar-
ing our final test set submissions, the development
set was folded into the training data, and all models
were re-trained on this new dataset containing 11000
items.
4.1 Baselines
How hard is the NLI task? Simple baselines of-
ten give us a quick glimpse into what matters in a
NLP task. In Figure 1, we give accuracy results
on ten different baselines we trained on the training
103
Baseline Accuracy(%)
random 9.1
char length 9.6
SRILM(letter unigram) 10.8
word length 12.0
proficiency 14.9
SRILM(letter bigram) 15.1
JS(vowels) 20.6
JS(consonants) 33.8
JS(vowels+consonants) 34.1
JS(bag-of-words) 52.5
Figure 1: Simple baseline development set scores.
set and evaluated on the development set. Predic-
tions based on simple character and word lengths
show only slight gains over random. Using the
high/medium/low proficiency score that accompa-
nied the data similarly gives a tiny amount of infor-
mation over baseline (14.9%). We ignored those rat-
ings elsewhere in our work, to focus on the core task
of prediction based on essay content.
We collected some simple distributions of vowel
and consonant clusters and used them for predic-
tion, scoring with Jensen-Shannon divergence. JS
divergence is a symmetrized form of KL divergence
to alleviate the mathematical problem involved with
missing observations. It has behaved well in the
context of language processing applications (Lee,
1999). The score progression from consonant clus-
ters, to vowel clusters, to words suggests that there
is NLI information scattered at various levels of sur-
face features.
4.2 Varied Carnie configurations
Carnie?s out-of-the-box configuration is one that has
been optimized for application to micro-blogs and
other ungrammatical short texts. While our hypoth-
esis was that this configuration would be well suited
to analysis of English TOEFL essays, we investi-
gated a number of possible techniques to help Carnie
adapt to the new domain.
We began by performing a grid search to select
model hyperparameters that enabled our standard
configuration to generalize well from the training
dataset to the development dataset. These values of
, ?, and cooling rate were then applied to various
new feature configurations.
The standard configuration included binary fea-
tures for word unigrams and bigrams, character n-
grams of sizes 1 to 5, and surface features. We
experimented here with word trigrams, character 6-
grams, and lowercased character n-grams of sizes 1
to 6. We also added skip bigrams, which were or-
dered word pairs in which 1 to 6 intervening words
were omitted. We incorporated part of speech tags in
a number of ways, including POS n-grams of lengths
1 to 5, POS k-skip bigrams with k ranging from 1 to
6, and POS n-grams in which closed-class POS tags
were replaced with the actual content word used.
We also measured the impact of using frequency-
weighted features.
Our standard approach with Carnie is to perform
multinomial classification using one model trained
on all the data simultaneously. We experimented
with other ways of framing the NLI problem, such
as building eleven binary classifiers, each of which
was trained on all of the data but with the sole task
of accepting or rejecting a single candidate L1. We
also partitioned the training data to build 55 binary
classifiers for all possible pairs of L1s. These bi-
nary classifiers were then combined via a voting
mechanism to select a single winner. This allowed
us to apply focused efforts to improve discrimina-
tion in language pairs which Carnie found challeng-
ing, such as Hindi-Telugu or Japanese-Korean. To
this end, we collected a substantial amount of ad-
ditional out-of-domain training data from the web-
sites lang8.com (70,000 entries) and gohackers.com
(40,000 entries). Although we did not use this
data in our final submission, we performed experi-
ments to measure the value of this new data in the
TOEFL11 domain with no adaptation, with feature
filtering to limit training features to items observed
in the test sets, and with ?frustratingly easy? do-
main adaptation, EasyAdapt, described in Daum?
and Marcu (2007).
4.3 Varied SRILM configurations
SRILM offers a number of parameters for ex-
perimentation. We hill-climbed on the train-
ing/development split to select a good configura-
tion. We experimented with n-gram lengths from
1-5 (bag of words through word 5-grams), using the
tokenization given by the NLI organizers. We tried
the lighter weight smoothing techniques offered by
104
System Confidence MRD
Carnie s(h1)/s(h2) 343
s(h1)/
?
i s(hi) 268
s(h1)? s(h2) 72
SRILM log p(h1)/ log p(h2) 315.7
log p(h1)? log p(h2) 315.3
ppl1(h1)/ppl1(h2) 315.12
ppl1(h1)? ppl1(h2) 260
ppl1 77
log p(h1) 40
MaxEnt
?
i p(hi) log p(hi) 385.7
(JCarafe) p(h1) 383.15
log p(h1) 383.15
p(h1)/p(h2) 373.75
log p(h1)/ log p(h2) 379.8
LIBLINEAR
?
i p(hi) log p(hi) 379.8
Figure 2: Confidence candidates measured in Mean Rank
Difference between correct and incorrect labels.
SRILM including Good-Turing, Witten-Bell, Ris-
tad?s natural discounting, both modified and original
Kneser-Ney. We built both closed vocabulary and
open vocabulary language models and with special
symbols added for sentence boundaries.
4.4 Component confidence experiments
Our components generate scores, but those scores
were not always scaled in the same way. Winnow
(in Carnie) is a margin-based, mistake-driven learner
generating scores which are interpretable only as
sums of weights. SRILM produces log p(dj |hi),
but renormalizing those (with priors) into estimates
of p(hi|dj) is unreliable because the different sub-
models are not connected with smoothing. Logistic
regression produces a distribution for p(hi|dj). We
aimed to express these notions of confidence in a
way that was common to all systems. We did this by
relabeling system hypotheses after sorting by confi-
dence, but not all metrics were equally good at this
sorting.
We performed an ad hoc assessment of several
candidate scoring functions. Our goal was to find
functions that best separated correct answers from
incorrect answers in a sorted ranking. We ran several
candidates on our development set and measured the
difference between the mean rank of correct answers
and the mean rank of incorrect answers. Figure 2
displays the results. In each case h1 was the best hy-
pothesis generated by the system and h2 is second
best. p(?) indicates probabilities, s(?) indicates non-
probability scores. We chose those functions with
the highest values.
4.5 Simple models for combination
In this work, we focused our ensembles only on the
output of our individual components, ignoring the
features from the original data that they attempt to
model. The base systems are all trained to minimize
errors, and did not appear to have any particular
preferential capabilities. Thus we rely on them en-
tirely for the primary processing and focus on their
outputs.
In our naive Bayes formulation, the random vari-
ables produced by the component systems (H) need
not take on values directly comparable with the ref-
erence labels to be predicted (R). We experimented
with folding in several one-shot systems that pro-
duced labels in {L, L?}, for particular native lan-
guage groups, but none of these proved to be good
complements for the components described above.
To cope with decode-time configurations of H
that hadn?t been seen during estimation, we used
a Dirichlet prior on R in this ensemble. A sin-
gle parameter, ?, was introduced. Thus our esti-
mates for P (hi|r) were based on smoothed counts:
c(hi,r)+?
c(r)+?|R| . The search for ? was performed using
cross-validation on the development set.
Assignment In many prediction settings, we know
that our evaluation data consists of examples drawn
from a particular allocation of candidate classes.
One can take advantage of this in a probabilistic
setting by doing a global search for the maximum
likelihood assignment of the test documents to the
L1 languages under the constraint that each L1 lan-
guage must have a particular occupancy by the doc-
uments ? in this case, an even split. More generally,
once we have p(hi|dj) for each candidate language
hi and document dj , we can find an assignmentA =
{(i, j) : ?i,j = 1} that maximizes the likelihood
P (H|D) =
?
(i,j)?A p(hi|dj) =
?
i,j p(hi|dj)
?i,j
under the constraints that
?
i ?i,j = |D|/|H| and?
j ?i,j = 1. The first constraint says that each lan-
guage should get an even allocation of documents
assigned to it and the second constraint says that
105
each document should be assigned to only one lan-
guage. This reduces to a maximum weight match-
ing on
?
i,j ?i,j log p(hi|dj). This problem is di-
rectly convertible into a max flow problem or a lin-
ear program. It can be solved with methods such
as the Hungarian algorithm, Ford-Fulkerson, or lin-
ear programming. In our case, we used LPSOLVE2
to find this global maximum. This looks at first
glance like an integer programming problem, but
one can relax the constraints into inequalities and
still be guaranteed that the solution will end up with
all ?i,j landing on either zero or one in the right
amounts. We applied this assignment combination
as a post-processing step to the probabilities gener-
ated in the naive Bayes ensemble and also to the raw
LIBLINEAR outputs. The hope in doing this is that
the optimizer will move the less likely assignments
around appropriately while preserving the assign-
ments where it has more confidence. We observed
mixed results on our development set and submitted
two systems using this ensemble technique.
4.6 Other components explored
LIBLINEAR provides an implementation of a linear
SVM as well as a logistic regression package. We
experimented with various combinations of `1- and
`2 -loss SVMs, with both `1 and `2-regularization,
but in the end opted to use the `2-regularized logistic
regression due to slightly superior performance and
the ease with which we could extract eleven values
of P (H) for inclusion in our ensemble.
Another component that was tested in develop-
ment of our ensemble systems was a maximum en-
tropy classifier. This particular effort used the imple-
mentation from JCarafe,3 which uses L-BFGS for
optimization.
We approached the NLI task as document classi-
fication, following a typical JCarafe recipe (Gibson
et al, 2007). The class of the document is the native
language of the author. Each document was treated
as a bag of words, and several classes of features
were extracted: token n-gram frequency, character
n-gram frequency, part of speech n-gram frequency.
The feature mix that produced the best score was
token bigrams and trigrams, character trigrams and
2http://lpsolve.sourceforge.net
3https://github.com/wellner/jcarafe
L1 Mean F Our Best F
GER 1 0.776 1 0.921
ITA 2 0.757 2 0.88
CHI 3 0.723 4 0.85
JPN 4 0.708 5 0.837
FRE 5 0.701 7 0.818
TEL 6 0.667 3 0.802
KOR 7 0.665 6 0.827
TUR 8 0.656 8 0.81
ARA 9 0.65 3 0.872
SPA 10 0.631 10 0.768
HIN 11 0.606 11 0.762
Figure 3: L1s by empirical prediction difficulty. Mean F
incorporates all submissions by all competition teams.
POS trigrams. A feature frequency threshold of 5
was used to curb the number of features.
5 Results
Our best performing ensemble was 82.6% accurate
when scored on the competition test set, and was
composed of Carnie, SRILM, and logistic regres-
sion, using naive Bayes to combine the subsystem
outputs and confidence scores into a single predic-
tion. The best performing subsystem during system
development scored 79.3% on the test set in isola-
tion, demonstrating once again the value of combin-
ing systems that make independent errors.
Certain L1s gave our systems more difficulty than
others. Our best submitted F-measure scores ranged
from 0.921 for German to 0.762 for Hindi. Fig-
ure 3 demonstrates that our systems? scores were
highly correlated with average scores from all sub-
missions by all teams (R2 = 0.84). From this we
infer that our performance differences between L1s
may be explained by inherent difficulties in certain
languages or by the selection of similar L1s as a part
of the competition task, rather than quirks of our ap-
proach. Our submissions do appear to have a partic-
ular advantage on Arabic and Korean, relative to the
field.
Figure 4 shows the overall performance of our
submissions and subsystems on the development
and test evaluation sets.
Our scores dropped 4 to 5% between development
and test evaluations, representing significant overfit-
106
Configuration dev % test %
Components
base Carnie 82.6
+ trigrams 83.1
+ POS tags 83.6 79.3
1v1 voted Carnie 79.4
SRILM 77.1
MaxEnt 77.7
Linear SVM 81.9
Logistic Regression 83.4
assignment(LR) 82.4
Ensembles
bayes(Carnie,SRILM,LR) 87.3 82.6
assign(Carnie,SRILM,LR) 86.5 82.0
assign(Carnie,SRILM,MaxEnt) 86.4 82.3
bayes(Carnie,SRILM) 86.9 81.7
Figure 4: Results.
ting to the development set. The development set
was used for model selection, ensemble parameteri-
zation, and eventually as additional training data for
final submissions. Later tests showed that this fi-
nal retraining actually reduced the Carnie score by
0.9%.
Figure 4 also shows the effect of various efforts to
improve our baseline Carnie system. Adding part-
of-speech n-grams and word trigrams as features
improved the score on the development set by 1%
in total. Meanwhile many of our experiments with
new types of features yielded no gains. Lowercased
character n-grams, skip bigrams and all non-vanilla
formulations of part-of-speech tags provided no im-
provement and were discarded.
It was observed that all of our systems showed
a strong preference for binary features over
frequency-weighted inputs. In the case of the
JCarafe classifier, switching to binary features
yielded a 10% accuracy gain. Although JCarafe
didn?t provide a gain over the ensemble of Carnie,
SRILM, and LIBLINEAR logistic regression, de-
velopment set results indicated that JCarafe served
capably as a replacement for LIBLINEAR in some
ensembles.
We also measured the impact of using out-of-
domain Japanese and Korean L1 data to train a pair-
wise JPN/KOR system. Only 78.5% of JPN and
KOR texts were correctly identified in our eleven-
Rank L1 Score Feature
14 GER 21.05 (for,example)
40 GER 15.95 (have,to)
55 HIN 14.80 (as,compared,to)
57 ITA 14.60 (I,think,that)
58 TEL 14.18 (and,also)
60 HIN 13.97 (as,compared)
79 TEL 12.82 (the,people)
96 TEL 12.14 (for,a)
101 ITA 11.83 (that,in)
116 ITA 10.94 (think,that)
119 GER 10.93 (has,to)
120 TEL 10.89 (with,the,statement)
Figure 5: Word n-gram features predicting particular L1.
way baseline system. We restricted train and evalu-
ation data to only those two L1s and found our base-
line technique was 86.5% accurate. When we added
our out-of-domain data with no domain adaptation
technique, that score dropped to 82.0%. Removing
features that didn?t appear in our test set only raised
the score to 82.5%. However, the EasyAdapt tech-
nique (Daum? and Marcu, 2007) showed promise.
By making an additional source-specific copy of
each feature, we were able to raise the score to
88.5%. While this result was of limited applicabil-
ity in our final submission, and was therefore not
submitted to the open data competition task, we be-
lieve that this technique may prove useful in en-
abling cross-domain NLI system transfer.
Figure 5 provides a small sample of word-level
features discovered by the Winnow classifier. The
table shows the rank of each n-gram relative to all
features, and the native language that the feature
predicts. The weight assigned by the Winnow2 al-
gorithm is not readily interpretable, although higher
weights indicate a stronger association.
Similarly, the top character n-grams can be seen in
Figure 7, along with manually selected examples of
each. These features can be seen to mainly fall into
several broad categories. There are mentions of the
authors? home countries as in Korean, Italian and
Turkey. There are also characteristic misspellings
and infelicities such as personnaly, perhaps incor-
rectly modeled from the French personnellement.
It is worth noting that the weights (and thus the
ranks) for the top character n-gram features are
107
System Accuracy (%) Errors
Carnie 80.4 2153
SRILM 74.5 2800
LIBLINEAR 80.8 2116
ensemble-assign 81.9 1990
ensemble-Bayes 82.2 1961
Figure 6: Training set cross-validation results.
higher than for the top word features, indicating that
Winnow found the former to be more informative.
Finally, the top part-of-speech n-gram features are
shown in Figure 8, again with manually selected
examples. These features have similar weights
to the character n-gram features and for the most
part seem to represent ungrammatical constructions
(e.g., the first feature indicates that a personal pro-
noun followed by an uninflected verb predicts Chi-
nese). However, there are some perfectly grammat-
ical items that are indicative of a particular native
language (e.g., as compared to for Hindi). One pos-
sible explanation might be a dominant L2 pedagogy
for that language.
5.1 Cross-validation results
The task organizers requested that the participants
run a ten-fold cross validation on a particular split of
the union of the training and development sets after
the evaluation was over. Results of our leading com-
ponent systems and ensemble systems are presented
in Table 6. These are comparable with the TOEFL-
11 column of Figure 3 in Tetreault et al (2012).
6 Conclusion
In this paper, we have presented MITRE?s partici-
pation in the native language identification task at
BEA-8. Our best system was a naive Bayes ensem-
ble combining component systems that used Win-
now, language modeling and logistic regression ap-
proaches, all using relatively simple character and
word n-gram features. This ensemble performed at
an accuracy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the winning systems
submitted by 29 teams. For individual native lan-
guages, our submission performed best among the
participants on Arabic, as ranked by F-measure.
In addition to the three base systems in our best
ensemble, we experimented with a maximum en-
tropy classifier and an assignment-based ensemble
method. We described a variety of experiments we
performed to determine the best configurations and
settings for the various systems. We also covered
experiments aimed at using out-of-domain data for
several native languages. In future work we will ex-
pand upon these, with the goal of applying domain
adaptation approaches.
One concern with NLI as framed in this evalua-
tion is the interaction between native language and
essay topic. The distribution of topics was very sim-
ilar in the various subcorpora, but in more natural
settings this is unlikely to be the case, and there is
a danger of overtraining on topic, to the detriment
of language identification performance. This is es-
pecially problematic for a highly lexical approach
such as ours. In future work, we intend to explore
the extent of this effect, using topic-based splits of
the corpus. Our initial experiments to remedy this
problem are likely to involve domain adaptation ap-
proaches, such as Daum? and Marcu (2007).
As described above, we have had success using
the Winnow-based system Carnie for other latent au-
thor attributes, such as gender. We would like to ex-
plore ensembles similar to those described here for
these attributes as well.
The techniques described in this paper success-
fully identified an author?s native language 82.6% of
the time using a sample of text averaging less than
350 words in length. Future work could study the
interaction of text length and NLI performance, in-
cluding texts shorter than 140 characters in length.
Acknowledgments
This work was funded under the MITRE Innovation
Program. Approved for Public Release; Distribution
Unlimited: 13-1876.
References
Austin Appleby. 2011. MurmurHash, mur-
mur3. https://sites.google.com/site/
murmurhash/.
Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
Age, gender, and the varieties of self-expression. First
Monday, 12(9), September.
108
Rank L1 Score Feature Snippet
1 KOR 57.34 orea first thing that Korean college students usually buy
2 GER 48.68 ,_tha the fact , that people have less moral values
3 SPA 23.65 omen consequences related with the enviroment and the atmosphere
4 ARA 23.23 _alot becouse you have alot of knowledge
6 TUR 22.84 s_abo their searchings about the products
11 ITA 21.56 Ital the Italian scholastic system
19 TEL 20.19 d_als the whole system and also the concept
20 TUR 19.96 urk in Turkey all young people go to the parties
21 CHI 19.51 Ta Take school teachers for example
23 GER 19.34 _-_ constantly - or as mentioned before even exponentially - breaking
27 JPN 17.62 s_,_I For those reasons , I think
32 FRE 16.90 ndeed Indeed , facts are just applications of ideas
36 JPN 16.57 apan been getting weaker these days in Japan .
37 FRE 16.57 onn I personnaly prefer
38 GER 16.04 ,_bec would be great , because so everyone
41 SPA 15.92 esa its not necesary to ask
47 HIN 15.23 in_i the main idea and concept
53 ITA 14.93 act_ due to the fact that too much
74 ITA 13.00 ,_in academic subjects and , in the mean time
81 TEL 12.74 h_ou cannot do with out a tour guide
Figure 7: Character n-gram features predicting particular L1.
Rank L1 Score Feature Snippet
35 CHI 16.58 (PRP,VB) What if he go and see
43 CHI 15.85 (NNS,POS) products ?s
45 SPA 15.41 (NNS,NNS) companies universities
59 TEL 14.05 (RB,IN,VBG) Usually in schooling
64 TEL 13.95 (DT,NNS,WDT) the topics which
65 TUR 13.71 (IN,DT,IN) after a while
66 TEL 13.69 (IN,VBG) in telling
69 TUR 13.42 (VBG,DT,NNS) learning the ways
70 HIN 13.39 (IN,VBN,TO) as compared to
80 HIN 12.81 (FW) [foreign word]
Figure 8: Part of Speech n-gram features predicting particular L1.
109
Douglas Biber and Edward Finegan, editors. 1993. Soci-
olinguistic Perspectives on Register. Oxford studies in
sociolinguistics. Oxford University Press.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. to appear. TOEFL11:
A Corpus of Non-Native English. Technical report,
Educational Testing Service.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Computational Approaches to Analyzing Weblogs:
Papers from the 2006 AAAI Spring Symposium. AAAI
Press.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on twitter.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics.
Vitor R. Carvalho and William W. Cohen. 2006. Single-
pass online learning: performance, voting schemes
and online feature selection. In Proceedings of
the 12th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?06,
pages 548?553, New York, NY, USA. ACM.
Hal Daum? and D Marcu. 2007. Frustratingly easy do-
main adaptation. In Proceedings of the Association for
Computational Linguistics, volume 45, page 256.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
John Gibson, Ben Wellner, and Susan Lubar. 2007.
Adaptive web-page content identification. In Proceed-
ings of the 9th Annual ACM International Workshop
on Web information and Data Management, WIDM
?07, pages 105?112, New York, NY, USA. ACM.
William Labov. 1972. Sociolinguistic Patterns. Conduct
& Communication Series. University of Pennsylvania
Press.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, ACL ?99,
pages 25?32, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In 2nd International Workshop on
Search and Mining User-Generated Content. ACM.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender on
blogging. In Computational Approaches to Analyzing
Weblogs: Papers from the 2006 AAAI Spring Sympo-
sium. AAAI Press, March.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
110
