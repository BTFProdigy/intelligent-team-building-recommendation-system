Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 581?589, Prague, June 2007. c?2007 Association for Computational Linguistics
Lexical Semantic Relatedness with Random Graph Walks
Thad Hughes and Daniel Ramage
Computer Science Department
Stanford University
Stanford, CA 94305
{thughes, dramage}@cs.stanford.edu
Abstract
Many systems for tasks such as question answer-
ing, multi-document summarization, and infor-
mation retrieval need robust numerical measures
of lexical relatedness. Standard thesaurus-based
measures of word pair similarity are based on
only a single path between those words in the
thesaurus graph. By contrast, we propose a new
model of lexical semantic relatedness that incor-
porates information from every explicit or im-
plicit path connecting the two words in the en-
tire graph. Our model uses a random walk over
nodes and edges derived from WordNet links
and corpus statistics. We treat the graph as a
Markov chain and compute a word-specific sta-
tionary distribution via a generalized PageRank
algorithm. Semantic relatedness of a word pair is
scored by a novel divergence measure, ZKL, that
outperforms existing measures on certain classes
of distributions. In our experiments, the resulting
relatedness measure is the WordNet-based mea-
sure most highly correlated with human similar-
ity judgments by rank ordering at ? = .90.
1 Introduction
Several kinds of Natural Language Processing systems
need measures of semantic relatedness for arbitrary word
pairs. For example, document summarization and ques-
tion answering systems often use similarity scores to
evaluate candidate sentence alignments, and information
retrieval systems use relatedness scores for query expan-
sion. Several popular algorithms calculate scores from
information contained in WordNet (Fellbaum, 1998), an
electronic dictionary where word senses are explicitly
connected by zero or more semantic relationships. The
central challenge of these algorithms is to compute rea-
sonable relatedness scores for arbitrary word pairs given
that few pairs are directly connected.
Most pairs in WordNet share no direct semantic
link, and for some the shortest connecting path can be
surprising?even pairs that seem intuitively related, such
?furnace? and ?stove? share a lowest common ancestor
in the hypernymy taxonomy (is-a links) all the way up
at ?artifact? (a man-made object). Several existing algo-
rithms compute relatedness only by traversing the hyper-
nymy taxonomy and find that ?furnace? and ?stove? are
relatively unrelated. However, WordNet provides other
types of semantic links in addition to hypernymy, such
as meronymy (part/whole relationships), antonymy, and
verb entailment, as well as implicit links defined by over-
lap in the text of definitional glosses. These links can
provide valuable relatedness information. If we assume
that relatedness is transitive across a wide variety of such
links, then it is natural to follow paths such as furnace?
crematory?gas oven?oven?kitchen appliance?stove and
find a higher degree of relatedness between ?furnace?
and ?stove.?
This paper presents the application of random walk
Markov chain theory to measuring lexical semantic re-
latedness. A graph of words and concepts is constructed
from WordNet. The random walk model posits the exis-
tence of a particle that roams this graph by stochastically
following local semantic relational links. The particle is
biased toward exploring the neighborhood around a target
word, and is allowed to roam until the proportion of time
it visits each node in the limit converges to a stationary
distribution. In this way we can compute distinct, word-
specific probability distributions over how often a particle
visits all other nodes in the graph when ?starting? from a
specific word. We compute the relatedness of two words
as the similarity of their stationary distributions.
The random walk brings with it two distinct advan-
tages. First, it enables the similarity measure to have
a principled means of combination of multiple types of
edges from WordNet. Second, by traversing all links, the
walk aggregates local similarity statistics across the en-
tire graph. The similarity scores produced by our method
are, to our knowledge, the WordNet-based scores most
highly correlated with human judgments.
581
2 Related work
Budanitsky and Hirst (2006) provide a survey of many
WordNet-based measures of lexical similarity based on
paths in the hypernym taxonomy. As an example, one
of the best performing is the measure proposed by Jiang
and Conrath (1997) (similar to the one proposed by (Lin,
1991)), which finds the shortest path in the taxonomic hi-
erarchy between two candidate words before computing
similarity as a function of the information content of the
two words and their lowest common subsumer in the hi-
erarchy. We note the distinction between word similarity
and word relatedness. Similarity is a special case of relat-
edness in that related words such as ?cat? and ?fur? share
some semantic relationships (such as meronymy), but do
not express the same likeness of form as would similar
words such as ?cat? and ?lion.? The Jiang-Conrath mea-
sure and most other measures that primarily make use of
of hypernymy (is-a links) in the WordNet graph are better
categorized as measures of similarity than of relatedness.
Other measures have been proposed that utilize the text
in WordNet?s definitional glosses, such as Extended Lesk
(Banerjee and Pedersen, 2003) and later the Gloss Vec-
tors (Patwardhan and Pedersen, 2006) method. These ap-
proaches are primarily based on comparing the ?bag of
words? of two synsets? gloss text concatenated with the
text of neighboring words? glosses in the taxonomy. As
a result, these gloss-based methods measure relatedness.
Our model captures some of this relatedness information
by including weighted links based on gloss text.
A variety of other measures of semantic relatedness
have been proposed, including distributional similarity
measures based on co-occurrence in a body of text?
see (Weeds and Weir, 2005) for a survey. Other mea-
sures make use of alternative structured information re-
sources than WordNet, such as Roget?s thesaurus (Jar-
masz and Szpakowicz, 2003). More recently, measures
incorporating information from Wikipedia (Gabrilovich
and Markovitch, 2007; Strube and Ponzetto, 2006) have
reported stronger results on some tasks than have been
achieved by existing measures based on shallower lexical
resources. The results of our algorithm are competitive
with some Wikipedia algorithms while using only Word-
Net 2.1 as the underlying lexical resource. The approach
presented here is generalizable to construction from any
underlying semantic resource.
PageRank is the most well-known example of a ran-
dom walk Markov chain?see (Berkhin, 2005) for a sur-
vey. It uses the local hyperlink structure of the web to
define a graph which it walks to aggregate popularity
information for different pages. Recent work has ap-
plied random walks to NLP tasks such as PP attachment
(Toutanova et al, 2004), word sense disambiguation (Mi-
halcea, 2005; Tarau et al, 2005), and query expansion
(Collins-Thompson and Callan, 2005). However, to our
knowledge, the literature in NLP has only considered us-
ing one stationary distribution per specially-constructed
graph as a probability estimator. In this paper, we in-
troduce a measure of semantic relatedness based on the
divergence of the distinct stationary distributions result-
ing from random walks centered at different positions in
the word graph. We believe we are the first to define such
a measure.
3 Random walks on WordNet
Our model is based on a random walk of a particle
through a simple directed graphG = (V,E)whose nodes
V and edges E are extracted from WordNet version 2.1.
Formally, we define the probability n(t)i of finding the
particle at node ni ? V at time t as the sum of all ways in
which the particle could have reached ni from any other
node at the previous time-step:
n(t)i =
?
nj?V
n(t?1)j P (ni | nj)
where P (ni | nj) is the conditional probability of mov-
ing to ni given that the particle is at nj . In partic-
ular, we construct the transition distribution such that
P (ni | nj) > 0 whenever WordNet specifies a local link
relationship of the form j ? i. Note that this random
walk is a Markov chain because the transition probabili-
ties at time t are independent of the particle?s past trajec-
tory.
The subsections that follow present the construction of
the graph for our random walk from WordNet and the
mathematics of computing the stationary distribution for
a given word.
3.1 Graph Construction
WordNet is itself a graph over synsets. A synset is best
thought of as a concept evoked by one sense of one or
more words. For instance, different senses of the word
?bank? take part in different synsets (e.g. a river bank
versus a financial institution), and a single synset can
be represented by multiple synonymous words, such as
?middle? and ?center.? WordNet explicitly marks seman-
tic relationships between synsets, but we are additionally
interested in representing relatedness between words. We
therefore extract the following types of nodes fromWord-
Net:
Synset Each WordNet synset has a corresponding node.
For example, one node corresponds to the synset re-
ferred to by ?dog#n#3,? the third sense of dog as
noun, whose meaning is ?an informal term for a
man.? There are 117,597 Synset nodes.
582
TokenPOS One node is allocated to every word cou-
pled with a part of speech, such as ?dog#n? mean-
ing dog as a noun. These nodes link to all the
synsets they participate in, so that ?dog#n? links
the Synset nodes for canine, hound, hot dog, etc.
Collocations?multi-word expressions such as ?hot
dog??that take part in a synsets are also represented
by these nodes. There are 156,588 TokenPOS nodes.
Token Every TokenPOS is connected to a Token node
corresponding to the word when no part of speech
information is present. For example, ?dog? links to
?dog#n? and ?dog#v? (meaning ?to chase?). There
are 148,646 Token nodes.
Synset nodes are connected with edges correspond-
ing to many of the relationship types in Word-
Net. We use these WordNet relationships to form
edges: hypernym/hyponym, instance/instance of, all
holonym/meronym links, antonym, entails/entailed by,
adjective satellite, causes/caused by, participle, pertains
to, derives/derived from, attribute/has attribute, and top-
ical (but not regional or usage) domain links. By con-
struction, each edge created from a WordNet relationship
is guaranteed to have a corresponding edge in the oppo-
site direction.
Edges that connect a TokenPOS to the Synsets using it
are weighted based on a Bayesian estimate drawn from
the SemCor frequency counts included in WordNet but
with a non-uniform Dirichlet prior. Our edge weights are
the SemCor frequency counts for each target Synset, with
pseudo-counts of .1 for all Synsets, 1 for first sense of
each word, and .1 for the first word in each Synset. Intu-
itively, this causes the particle to have a higher probabil-
ity of moving to more common senses of a TokenPOS; for
example, the edges from ?dog#n? to ?dog#n#1? (canine)
and ?dog#n#5? (hotdog) have un-normalized weights of
43.2 and 0.1, respectively. The edges connecting a To-
ken to the TokenPOS nodes in which it can occur are also
weighted by the sum of the weights of the outgoing To-
kenPOS?Synset links. Hence a walk starting at a com-
mon word like ?cat? is far more likely to follow a link to
?cat#n? than to rarities like ?cat#v? (to vomit). These
edges are uni-directional; no edges are created from a
Synset to a TokenPOS that can represent the Synset.
In order for our graph construction to incorporate
textual gloss-based information, we also create uni-
directional edges from Synset nodes to the TokenPOS
nodes for the words and collocations used in that synset?s
gloss definition. This requires part-of-speech tagging the
glosses, for which we use the Stanford maximum entropy
tagger (Toutanova et al, 2003). It is important to cor-
rectly weight these edges, because high-frequency stop-
words such as ?by? and ?he? do not convey much in-
formation and might serve only to smear the probability
mass across the whole graph. Gloss-based links to these
nodes should therefore be down-weighted or removed.
On the other hand, up-weighting extremely rare words
such as by tf-idf scoring might also be inappropriate
because such rare words would get extremely high scores,
which is an undesirable trait in similarity search. (Haveli-
wala et al, 2002) and others have shown that a ?non-
monotonic document frequency? (NMDF) weighting can
be more effective in such a setting. Because the frequency
of words in the glosses is distributed by a power-law, we
weight each word by its distance from the mean word
count in log space. Formally, the weight wi for a word
appearing ri times is
wi = exp
(
?
(log(ri)? ?)2
2?2
)
where ? and ? are the mean and standard deviation of
the logs of all word counts. This is a smooth approxima-
tion to the high and low frequency stop lists used effec-
tively by other measures such as (Patwardhan and Ped-
ersen, 2006). We believe that because non-monotonic
frequency scaling has no parameters and is data-driven,
it could stand to be more widely adopted among gloss-
based lexical similarity measures.
We also add bi-directional edges between Synsets
whose word senses overlap with a common TokenPOS.
These edges have raw weights given by the number of
TokenPOS nodes shared by the Synsets. The intuition be-
hind adding these edges is that WordNet often divides the
meanings of words into fine-grained senses with similar
meanings, so there is likely to be some semantic relation-
ship between Synsets sharing a common TokenPOS.
The final graph has 422,831 nodes and 5,133,281
edges. This graph is very sparse; fewer than 1 in 10,000
node pairs are directly connected. When only the un-
weighted WordNet relationship edges are considered,
the largest degree of any node is ?city#n#1? with 667
edges (mostly connecting to particular cities), followed
by ?law#n#2? with 602 edges (mostly connecting to a
large number of domain terms such as ?dissenting opin-
ion? and ?freedom of speech?), and each node is on aver-
age connected to 1.7 other nodes. When the gloss-based
edges are considered separately, the highest degree nodes
are those with the longest definitions; the maximum out-
degree is 56 and the average out-degree is 6.2. For the
edges linking TokenPOS nodes to the Synsets in which
they participate, TokenPOS nodes with many senses are
the most connected; ?break#v? with 59 outgoing edges
and ?make#v? with 49 outgoing edges have the highest
out-degrees, with the average out-degree being 1.3.
3.2 Computing the stationary distribution
Each of the K edge types presented above can be repre-
sented as separate transition matrix Ek ? RN?N where
583
N is the total number of nodes. For each matrix, col-
umn j contains contains a normalized outgoing proba-
bility distribution,1 so the weight in cell (i, j) contains
PK(ni | nj), the conditional probability of moving from
node nj to node ni in edge type K. For many of the edge
types, this is either 0 or 1, but for the weighted edges,
these are real valued. The full transition matrixM is then
the column normalized sum of all of the edge types:
M? =
?
k
Ek
M =
(?
?
?M?
?
?
?
?
)?1
? M?
M is a distillation of relevant relatedness information
about all nodes extracted from WordNet and is not tai-
lored for computing a stationary distribution for any spe-
cific word. In order to compute the stationary distribu-
tion vdog#n for a walk centered around the TokenPOS
?dog#n,? we first define an initial distribution v(0)dog#n that
places all the probability mass in the single vector entry
corresponding to ?dog#n.? Then at every step of the walk,
we will return to v(0) with probability ?. Intuitively, this
return probability captures the notion that nodes close to
?dog#n? should be given higher weight, and also guaran-
tees that the stationary distribution exists and is unique
(Bremaud, 1999). The stationary distribution v is com-
puted via an iterative update algorithm:
v(t) = ?v(0) + (1? ?)Mv(t?1)
Because the walk may return to the initial distribution
v(0) at any step with probability ?, we found that v(t)
converges to its unique stationary distribution v(?) in a
number of steps roughly proportional to ??1. We experi-
mented with a range of return probabilities and found that
our results were relatively insensitive to this parameter.
Our convergence criteria was
?
?v(t?1) ? v(t)
?
?
1
< 10?10,
which, for our graph with a return probability of ? = .1,
was met after about two dozen iterations. This computa-
tion takes under two seconds on a modern desktop ma-
chine.
Note that because M is sparse, each iteration of the
above computation is linear in the total number of non-
zero entries in P , i.e. linear in the total number of edges.
Introducing an edge type that is dense would dramatically
increase running time.
3.3 Model variants
For this paper, we consider three model variants that dif-
fer based on which subset of the edge types are included
1The frequency-count derived edges are normalized by the
largest column sum. This effectively preserves relative term fre-
quency information across the graph and causes some columns
to sum to less than one. We interpret this lost mass as a link to
?nowhere.?
in the transition matrix M .
MarkovLink This variant includes the explicit WordNet
relations such as hypernymy and the edges repre-
senting overlap between the TokenPOS nodes con-
tained in Synsets. A particle walking through this
graph reaches only Synset nodes and can step from
one Synset to another whenever WordNet specifies a
relationship between the Synsets or when the Synsets
share a common word. There is a single connected
component in this model variant. This model is
loosely analogous to a smoothed version of the path-
based WordNet measures surveyed in (Budanitsky
and Hirst, 2006) but differs in that it integrates mul-
tiple link types and aggregates relatedness informa-
tion across all paths in the graph.
MarkovGloss This variant includes only the weighted
uni-directional edges linking Synsets to the Token-
POS nodes contained in their gloss definitions, and
the edges from a TokenPOS node to the Synsets con-
taining it. The intuition behind this model variant is
that the particle can move as if it were recursively
looking up words in a dictionary, stepping from
Synsets to the Synsets used to define them. Because
WordNet?s gloss definitions are not sense-tagged,
the particle must make an intermediate step to a To-
kenPOS contained in the gloss definition and then
to a Synset representing a particular sense of that
TokenPOS. The availability of sense-tagged glosses
would eliminate the noise introduced by this inter-
mediate step. The particle can reach both Synsets
and TokenPOS nodes in this variant, but some parts
of the graph are not reachable from other parts. This
model incorporates much of the same information
as the gloss-based WordNet measures (Banerjee and
Pedersen, 2003; Patwardhan and Pedersen, 2006)
but differs in that it considers many more glosses
than just those in the immediate neighborhoods of
the candidate words.
MarkovJoined This variant is the natural combination
of the above two; we construct the graph containing
WordNet relation edges, Synset overlap edges, and
gloss-based Synset to TokenPOS edges.
Many of the characteristics of the model variants can
be understood in terms of how much probability mass
they assign to each node for a particular word-specific
stationary distribution. Table 1 shows the highest scoring
nodes in the word-specific stationary distributions cen-
tered around the Token node for ?wizard,? as computed by
the MarkovLink and MarkovGloss variants. In both vari-
ants, the ?wizard? Token?s only neighbors are the ?wiz-
ard#n? and ?wizard#a? TokenPOS nodes, and ?wizard#n?
584
MarkovLink MarkovGloss
Node Probability Node Probability
wizard 1.0E-1 wizard 1.3E-01
wizard#n 2.5E-3 wizard#n 2.9E-02
wizard#a 7.8E-5 wizard#a 9.1E-04
ace#n#3 4.2E-5 ace#n#3 1.1E-06
sorcerer#n#1 2.2E-6 sorcerer#n#1 5.8E-07
charming#a#2 2.2E-6 dazzlingly#r 2.4E-08
expert#n#1 1.1E-6 charming#a#2 1.6E-09
track star#n#1 1.1E-6 sorcery#n 2.6E-10
occultist#n#1 5.7E-7 magic#n 6.8E-12
Cagliostro#n#1 5.7E-7 magic#a 6.8E-12
star#v#2 5.5E-7 dazzlingly#r#1 4.3E-14
breeze_through#v#1 5.4E-7 dazzle#n 9.4E-16
magic#n#1 2.1E-8 beholder#n 9.4E-16
sorcery#n#1 2.1E-7 dazzle#v 9.4E-16
magician#n#1 1.9E-7 magic#n#1 5.1E-16
Table 1: Highest scoring nodes in the stationary distri-
butions for ?wizard#n? as generated by the MarkovLink
model and the MarkovGloss model with return probabil-
ity 0.1.
has a higher probability mass because of its higher Sem-
Cor usage counts. Likewise, the only possible steps per-
mitted in either variant from ?wizard#n? and ?wizard#a?
are to the Synsets that can be expressed with those nodes:
?ace#n#3,? ?sorcerer#n#1,? and ?charming#a#1.? Again,
the amount of mass given to these nodes depends on the
strength of these edge weights, which is determined by
the SemCor usage counts.
The highest probability nodes in the table are common
because both model variants share the same initial links.
However, the orders of the remaining nodes in the station-
ary distributions are different. In the MarkovLink variant,
the random walk can only proceed to other Synsets using
WordNet relationship edges; ?track star#n#1? and ?ex-
pert#n#1? are first reached by following hyponym and
hypernym edges from ?ace#n#1,? and ?occultist#n#1?
and ?Cagliostro#n#1? are first reached with hypernym
and instance edges from ?sorcerer#n#1.? The node
?breeze through#v#1? is reached through a path follow-
ing derivational links with ?ace#n? and ?ace#v.?
The MarkovGloss variant in table 1 shows how infor-
mation can be extracted solely from the textual glosses.
Once the random walk reaches the first Synset nodes, it
can step to the TokenPOS nodes in their glosses; for ex-
ample, ?ace#n#1? has the gloss ?someone who is daz-
zlingly skilled in any field.? Links to TokenPOS nodes
that are very common in glosses are down-weighted with
NMDF weighting, so ?someone#n? receives little mass
while ?dazzlingly#r? receives more. From there, the
random walk can step to another Synset such as ?daz-
MarkovLink model MarkovGloss model
Figure 1: Example stationary distributions plotted against
each other for similar (top) and dissimilar (bottom) word
pairs, using the MarkovLink (left) and MarkovGloss
(right) model variants.
zlingly#r#1,? and then on to other TokenPOS nodes used
in its definition: ?in a manner or to a degree that dazzles
the beholder.?
Figure 1 demonstrates how two word-specific station-
ary distributions are more highly correlated if the words
are related. In both model variants, random walks for
related words are more likely to visit the same parts of
the graph, and so assign higher probability to the same
nodes. Figure 1 also shows that the MarkovGloss variant
produces distributions with a much wider range of proba-
bilities than the MarkovLink, which might be a source of
difficulty in integrating the two model variants.
Figure 2 shows the correlation between the stationary
distributions produced by the two model variants for the
same word. The log-log scale makes it possible to see the
entire range of probabilities on the same axes, and shows
that distributions produced by these two model variants
share many of the same highest-probability words.
A noteworthy property of the constructed graphs is that
word relatedness can be computed directly by compar-
ing walks that start at Token nodes. By contrast, existing
WordNet-based measures require independent similarity
judgments for all word senses relevant to a target word
pair (of which the maximum relatedness value is usu-
ally taken). Our algorithm lends itself to comparisons
between walks centered at a Synset node, or a Token-
POS node, or a Token node, or any mixed distribution
thereof. And because the Synset nodes are strongly con-
nected, the model also admits direct comparison across
parts of speech.
585
Figure 2: Correlation of the stationary distributions for
?wizard#n,? produced by the MarkovLink variant (x-axis)
and the MarkovGloss variant (y-axis).
4 Similarity judgments
We have shown how to compute the word-specific sta-
tionary distribution from any starting distribution in the
graph. Now consider the task of deciding similarity be-
tween two words. Intuitively, if the random walk starting
at the first word?s node and the random walk starting at
the second word?s node tend to visit the same nodes, we
would like to consider them semantically related. For-
mally, we measure the divergence of their respective sta-
tionary distributions, p and q.
A wide literature exists on similarity measures between
probability distributions. One standard choice is to con-
sider p and q to be vectors and measure the cosine of
the angle between them, which is rank equivalent to Eu-
clidean distance.
simcos(p, q) =
?
i piqi
?p? ?q?
Because p and q are probability distributions, we would
also expect a strong contender from the information-
theoretic measures based on Kullback-Leibler diver-
gence, defined as:
DKL(p ? q) =
?
i
pi log
pi
qi
Unfortunately, KL divergence is undefined if any qi is
zero because those terms in the sum will have infinite
weight. Several modifications to avoid this issue have
been proposed in the literature. One is Jensen-Shannon
divergence (Lin, 1991), a symmetric measure based on
KL-divergence defined as the average of the KL diver-
gences of each distribution to their average distribution.
Jensen-Shannon is well defined for all distributions be-
cause the average of pi and qi is non-zero whenever either
number is.
These measures and others are surveyed in (Lee,
2001), who finds that Jensen-Shannon is outperformed
by the Skew divergence measure introduced by Lee in
(1999). The skew divergence2 accounts for zeros in q by
mixing in a small amount of p.
s?(p, q) = D(p ? ?q + (1? ?)p)
=
?
i pi log
pi
?qi+(1??)pi
Lee found that as ? ? 1, the performance of skew di-
vergence on natural language tasks improves. In partic-
ular, it outperforms most other models and even beats
pure KL divergence modified to avoid zeros with sophis-
ticated smoothing models. In exploring the performance
of divergence measures on our model?s stationary distri-
butions, we observed the same phenomenon. Note that
in the limit as ? ? 1, alpha skew is identically KL-
divergence.
4.1 Zero-KL Divergence
In this section we introduce a novel measure of distribu-
tional divergence based on a reinterpretation of the skew
divergence. Skew divergence avoids zeros in q by mixing
in some of p, but its performance on many natural lan-
guage tasks improves as it better approximates KL diver-
gence. We propose an alternative approximation to KL
divergence called Zero-KL divergence, or ZKL. When
qi is non-zero, we use exactly the term from KL diver-
gence. When qi = 0, we have a problem?in the limit as
? ? 1, the corresponding term approaches infinity. We
let ZKL use the Skew divergence value for these terms:
pi log
pi
?qi+(1??)pi
. Because qi = 0 this simplifies to
pi log
pi
(1??)pi
= pi log 11?? .
Lee showed skew divergence?s best performance was
for ? near to 1, so we formalize this intuition by choosing
? exponentially near to 1, i.e. we can choose our ? as
1?2?? for some ? ? R+. Zero terms in the sum can now
be written as pi log 12?? = pi log 2
? = pi ?. Note here an
analogy to the case with qj > 0 and where pj is exactly
one order of magnitude greater than qj , i.e. pj = 2 ? qj .
For such a term in the standard KL divergence, we would
get pj log
pj
qj
= pj log(2) = pj . Therefore, the ? term
in skew divergence implicitly defines a parameter stating
how many orders of magnitude smaller than pj to count
qj if qj = 0.
We define the Zero-KL divergence with respect to
2In Lee?s (1999) original presentation, skew divergence is
defined not as s?(p, q) but rather as s?(q, p). We reverse the ar-
gument order for consistency with the other measures discussed
here.
586
gamma:
ZKL?(p, q) =
?
i
pi
{
log piqi qi 6= 0
? qi = 0
Note that this is exactly KL-divergence when KL-
divergence is defined and, like skew divergence, approx-
imates KL divergence in the limit as ? ? ?.
A similar analysis of the skew divergence terms for
when 0 < qi  pi (and in particular with qi less than pi
by more than a factor of 2??) shows that such a term in
the skew divergence sum is again approximated by ? pi.
ZKL does not have this property. Because ZKL is a better
approximation to KL divergence and because they have
the same behavior in the limit, we expect ZKL?s perfor-
mance to dominate that of skew divergence in many dis-
tributions. However, if there is a wide range in the ex-
ponent of noisy terms, the maximum possible penalty to
such terms ascribed by skew divergence may be benefi-
cial.
Figure 3 shows the relative performance of ZKL versus
Jensen-Shannon, skew divergence, cosine similarity, and
the Jaccard score (a measure from information retrieval)
for correlations with human judgment on the MarkovLink
model. ZKL consistently outperforms the other measures
on distributions resulting from this model, but ZKL is not
optimal on distributions generated by our other models.
The next section explores this topic in more detail.
5 Evaluation
Traditionally, there have been two primary types of eval-
uation for measures of semantic relatedness: one is cor-
relation to human judgment, the other is the relative per-
formance gains of a task-driven system when it uses the
measure. The evaluation here focuses on correlation with
human judgments of relatedness. For consistency with
previous literature, we use rank correlation (Spearman?s
? coefficient) rather than linear correlation when compar-
ing sets of relatedness judgments because the rank corre-
lation captures information about the relative ordering of
the scores. However, it is worth noting that many applica-
tions that make use of lexical relatedness scores (e.g. as
features to a machine learning algorithm) would better be
served by scores on a linear scale with human judgments.
Rubenstein and Goodenough (1965) solicited human
judgments of semantic similarity for 65 pairs of com-
mon nouns on a scale of zero to four. Miller and Charles
(1991) repeated their experiment on a subset of 29 noun
pairs (out of 30 total) and found that although indi-
viduals varied among their judgments, in aggregate the
scores were highly correlated with those found by Ruben-
stein and Goodenough (at ? = .944 by our calculation).
Resnik (1999) replicated the Miller and Charles experi-
ment and reported that the average per-subject linear cor-
relation on the dataset was around r = 0.90, providing
a rough upper bound on any system?s linear correlation
performance with respect to the Miller and Charles data.
Figure 3 shows that the ZKL measure on the MarkovLink
model has linear correlation coefficient r = .903?at the
limit of human inter-annotator agreement.
Recently, a larger set of word relatedness judg-
ments was obtained by (Finkelstein et al, 2002) in the
WordSimilarity-353 (WS-353) collection. Despite the
collection?s name, the study instructed participants to
score word pairs for relatedness (on a scale of 0 to
10), which is in contrast to the similarity judgments re-
quested of the Miller and Charles (MC) and Rubenstein
and Goodenough (RG) participants. For this reason, the
WordSimilarity-353 data contains many pairs that are not
semantically similar but still receive high scores, such as
?computer-software? at 8.81. WS-353 contains pairs that
include non-nouns, such as ?eat-drink,? one proper noun
not appearing in WordNet (?Maradona-football?), and
some pairs potentially subject to political bias. Again,
the aggregate human judgments correlate well with ear-
lier data sets where they overlap?the 30 judgments that
WordSimilarity-353 shares with the Miller and Charles
data have ? = .939 and the 29 shared with Rubenstein
and Goodenough have ? = .904 (by our calculations).
We generated similarity scores for word pairs in all
three data sets using the three variants of our walk
model (MarkovLink, MarkovGloss, MarkovJoined) and
with multiple distributional distance measures. We used
the WordNet::Similarity package (Pedersen et al, 2004)
to compute baseline scores for several existing measures,
noting that one word pair was not processed in WS-353
because one of the words was missing from WordNet.
The results are summarized in Table 2. These num-
bers differ slightly from previously reported scores due to
variations in the exact experimental setup, WordNet ver-
sion, and the method of breaking ties when computing
? (here we break ties using the product-moment formu-
lation of Spearman?s rank correlation coefficient). It is
worth noting that in their experiments, (Patwardhan and
Pedersen, 2006) report that the Vector method has rank
correlation coefficients of .91 and .90 for MC and RG,
respectively, which are also top performing values.
In our experiments, the MarkovLink model with ZKL
distance measure was the best performing model over-
all. MarkovGloss and MarkovJoined were also strong
contenders but with the cosine measure instead of ZKL.
One reason for this distinction is that the stationary dis-
tributions resulting from the MarkovLink model are non-
zero for all but the initial word nodes (i.e. non-zero
for all Synset nodes). Consequently, ZKL?s re-estimate
for the zero terms adds little information. By contrast,
theMarkovGloss andMarkovJoined models include links
that traverse from Synset nodes to TokenPOS nodes, re-
587
Figure 3: Correlation with the Miller & Charles data sets by linear correlation (left) and rank correlation (right) for the
MarkovLink model. All data points were based on one set of stationary distributions over the graph; only the divergence
measure between those distributions is varied. Note that ZKL? dominates both graphs but skew divergence does well
for increasing ? (computed as 1? 2?). Gamma is swept over the range 0 to 1, then 1 through 20, then 20 through 40
at equal resolutions.
Model MC Rank RG Rank WS-353 Rank
MarkovLink (ZKL) .904 .817 .552
MarkovGloss (cosine) .841 .762 .467
MarkovJoined (cosine) .841 .838 .547
Gloss Vectors .888 .789 .445
Extended Lesk .869 .829 .511
Jiang-Conrath .653 .584 .195
Lin .625 .599 .216
Table 2: Spearman?s ? rank correlation coefficients with
human judgments using ? = 2.0 for ZKL. Note that fig-
ure 3 demonstrates ZKL?s insensitivity with regard to the
parameter setting for the MarkovLink model.
sulting in a final stationary distribution with more (and
more meaningful) zero/non-zero pairs. Hence the proper
setting of gamma (or alpha for skew divergence) is of
greater importance. ZKL?s performance improves with
tuning of gamma, but cosine similarity remained the more
robust measure for these distributions.
6 Conclusion
In this paper, we have introduced a new measure of
lexical relatedness based on the divergence of the sta-
tionary distributions computed from random walks over
graphs extracted WordNet. We have explored the struc-
tural properties of extracted semantic graphs and charac-
terized the distinctly different types of stationary distribu-
tions that result. We explored several distance measures
on these distributions, including ZKL, a novel variant of
KL-divergence. Our best relatedness measure is at the
limit of human inter-annotator agreement and is one of
the strongest measures of semantic relatedness that uses
only WordNet as its underlying lexical resource.
In future work, we hope to integrate other lexical re-
sources such as Wikipedia into the walk. Incorporat-
ing more types of links from more resources will un-
derline the importance of determining appropriate rela-
tive weights for all of the types of edges in the walk?s
matrix. Even for WordNet, we believe that certain link
types, such as antonyms, may be more or less appropriate
for certain tasks and should weighted accordingly. And
while our measure of lexical relatedness correlates well
with human judgments, we hope to show performance
gains in a real-word task from the use of our measure.
Acknowledgments
Thanks to Christopher D. Manning and Dan Jurafsky for
their helpful comments and suggestions. We are also
grateful to Siddharth Patwardhan and Ted Pedersen for
assistance in comparing against their system. Thanks to
Sushant Prakash, Rion Snow, and Varun Ganapathi for
their advice on pursuing some of the ideas in this pa-
per, and to our anonymous reviewers for their helpful cri-
tiques. Daniel Ramage was funded in part by an NDSEG
fellowship. This work was also supported in part by the
DTO AQUAINT Program, the DARPA GALE Program,
and the ONR (MURI award N000140510388).
588
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, Acapulco, pages 805?810.
P. Berkhin. 2005. A survey on pagerank computing. In-
ternet Mathematics, 2(1):73?120.
P. Bremaud. 1999. Markov chains: Gibbs fields, monte
carlo simulation, and queues. Springer-Verlag.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13?47.
K. Collins-Thompson and J. Callan. 2005. Query expan-
sion using random walk models. In CIKM ?05: Pro-
ceedings of the 14th ACM international conference on
Information and knowledge management, pages 704?
711, New York, NY, USA. ACM Press.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
T. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 2002.
Evaluating strategies for similarity search on the web.
In WWW2002.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings of
RANLP-03, pages 212?219.
J. J. Jiang and D. W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics (ROCLING X),
pages 19?33.
Lillian Lee. 1999. Measures of distributional similarity.
In 37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 25?32.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. In IEEE Transactions on Informa-
tion Theory, volume 37(1), pages 145?151.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 411?418, Morristown, NJ, USA.
Association for Computational Linguistics.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6:1?28.
S. Patwardhan and T. Pedersen. 2006. Using wordnet-
based context vectors to estimate the semantic related-
ness of concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Com-
putational Linguistics and Pyscholinguistics Together,
pages 1?8.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, (11):95?130.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence, pages 1419?1424.
Paul Tarau, Rada Mihalcea, and Elizabeth Figa. 2005.
Semantic document engineering with wordnet and
pagerank. In SAC ?05: Proceedings of the 2005
ACM symposium on Applied computing, pages 782?
786, New York, NY, USA. ACM Press.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models
for inducing word dependency distributions. In ICML
?04: Proceedings of the twenty-first international con-
ference on Machine learning, New York, NY, USA.
ACM Press.
Julie Weeds and David Weir. 2005. Co-occurrence re-
trieval: A flexible framework for lexical distributional
similarity. Comput. Linguist., 31(4):439?475.
589
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 248?256,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Labeled LDA: A supervised topic model for credit attribution in
multi-labeled corpora
Daniel Ramage, David Hall, Ramesh Nallapati and Christopher D. Manning
Computer Science Department
Stanford University
{dramage,dlwh,nmramesh,manning}@cs.stanford.edu
Abstract
A significant portion of the world?s text
is tagged by readers on social bookmark-
ing websites. Credit attribution is an in-
herent problem in these corpora because
most pages have multiple tags, but the tags
do not always apply with equal specificity
across the whole document. Solving the
credit attribution problem requires associ-
ating each word in a document with the
most appropriate tags and vice versa. This
paper introduces Labeled LDA, a topic
model that constrains Latent Dirichlet Al-
location by defining a one-to-one corre-
spondence between LDA?s latent topics
and user tags. This allows Labeled LDA to
directly learn word-tag correspondences.
We demonstrate Labeled LDA?s improved
expressiveness over traditional LDA with
visualizations of a corpus of tagged web
pages from del.icio.us. Labeled LDA out-
performs SVMs by more than 3 to 1 when
extracting tag-specific document snippets.
As a multi-label text classifier, our model
is competitive with a discriminative base-
line on a variety of datasets.
1 Introduction
From news sources such as Reuters to modern
community web portals like del.icio.us, a signif-
icant proportion of the world?s textual data is la-
beled with multiple human-provided tags. These
collections reflect the fact that documents are often
about more than one thing?for example, a news
story about a highway transportation bill might
naturally be filed under both transportation and
politics, with neither category acting as a clear
subset of the other. Similarly, a single web page
in del.icio.us might well be annotated with tags as
diverse as arts, physics, alaska, and beauty.
However, not all tags apply with equal speci-
ficity across the whole document, opening up new
opportunities for information retrieval and cor-
pus analysis on tagged corpora. For instance,
users who browse for documents with a particu-
lar tag might prefer to see summaries that focus
on the portion of the document most relevant to
the tag, a task we call tag-specific snippet extrac-
tion. And when a user browses to a particular
document, a tag-augmented user interface might
provide overview visualization cues highlighting
which portions of the document are more or less
relevant to the tag, helping the user quickly access
the information they seek.
One simple approach to these challenges can
be found in models that explicitly address the
credit attribution problem by associating individ-
ual words in a document with their most appropri-
ate labels. For instance, in our news story about
the transportation bill, if the model knew that the
word ?highway? went with transportation and that
the word ?politicians? went with politics, more
relevant passages could be extracted for either la-
bel. We seek an approach that can automatically
learn the posterior distribution of each word in a
document conditioned on the document?s label set.
One promising approach to the credit attribution
problem lies in the machinery of Latent Dirich-
let Allocation (LDA) (Blei et al, 2003), a recent
model that has gained popularity among theoreti-
cians and practitioners alike as a tool for automatic
corpus summarization and visualization. LDA is
a completely unsupervised algorithm that models
each document as a mixture of topics. The model
generates automatic summaries of topics in terms
of a discrete probability distribution over words
for each topic, and further infers per-document
discrete distributions over topics. Most impor-
tantly, LDA makes the explicit assumption that
each word is generated from one underlying topic.
Although LDA is expressive enough to model
248
multiple topics per document, it is not appropriate
for multi-labeled corpora because, as an unsuper-
vised model, it offers no obvious way of incorpo-
rating a supervised label set into its learning proce-
dure. In particular, LDA often learns some topics
that are hard to interpret, and the model provides
no tools for tuning the generated topics to suit an
end-use application, even when time and resources
exist to provide some document labels.
Several modifications of LDA to incorporate
supervision have been proposed in the literature.
Two such models, Supervised LDA (Blei and
McAuliffe, 2007) and DiscLDA (Lacoste-Julien
et al, 2008) are inappropriate for multiply labeled
corpora because they limit a document to being as-
sociated with only a single label. Supervised LDA
posits that a label is generated from each docu-
ment?s empirical topic mixture distribution. Dis-
cLDA associates a single categorical label variable
with each document and associates a topic mixture
with each label. A third model, MM-LDA (Ram-
age et al, 2009), is not constrained to one label
per document because it models each document as
a bag of words with a bag of labels, with topics for
each observation drawn from a shared topic dis-
tribution. But, like the other models, MM-LDA?s
learned topics do not correspond directly with the
label set. Consequently, these models fall short as
a solution to the credit attribution problem. Be-
cause labels have meaning to the people that as-
signed them, a simple solution to the credit attri-
bution problem is to assign a document?s words to
its labels rather than to a latent and possibly less
interpretable semantic space.
This paper presents Labeled LDA (L-LDA), a
generative model for multiply labeled corpora that
marries the multi-label supervision common to
modern text datasets with the word-assignment
ambiguity resolution of the LDA family of mod-
els. In contrast to standard LDA and its existing
supervised variants, our model associates each la-
bel with one topic in direct correspondence. In the
following section, L-LDA is shown to be a natu-
ral extension of both LDA (by incorporating su-
pervision) and Multinomial Naive Bayes (by in-
corporating a mixture model). We demonstrate
that L-LDA can go a long way toward solving the
credit attribution problem in multiply labeled doc-
uments with improved interpretability over LDA
(Section 4). We show that L-LDA?s credit attribu-
tion ability enables it to greatly outperform sup-
D
?
?
?
?
N
z
w
w
K
?
?
Figure 1: Graphical model of Labeled LDA: un-
like standard LDA, both the label set ? as well as
the topic prior ? influence the topic mixture ?.
port vector machines on a tag-driven snippet ex-
traction task on web pages from del.icio.us (Sec-
tion 6). And despite its generative semantics,
we show that Labeled LDA is competitive with
a strong baseline discriminative classifier on two
multi-label text classification tasks (Section 7).
2 Labeled LDA
Labeled LDA is a probabilistic graphical model
that describes a process for generating a labeled
document collection. Like Latent Dirichlet Allo-
cation, Labeled LDA models each document as a
mixture of underlying topics and generates each
word from one topic. Unlike LDA, L-LDA in-
corporates supervision by simply constraining the
topic model to use only those topics that corre-
spond to a document?s (observed) label set. The
model description that follows assumes the reader
is familiar with the basic LDA model (Blei et al,
2003).
Let each document d be represented by a tu-
ple consisting of a list of word indices w
(d)
=
(w
1
, . . . , w
N
d
) and a list of binary topic pres-
ence/absence indicators ?
(d)
= (l
1
, . . . , l
K
)
where eachw
i
? {1, . . . , V } and each l
k
? {0, 1}.
Here N
d
is the document length, V is the vocabu-
lary size and K the total number of unique labels
in the corpus.
We set the number of topics in Labeled LDA to
be the number of unique labels K in the corpus.
The generative process for the algorithm is found
in Table 1. Steps 1 and 2?drawing the multi-
nomial topic distributions over vocabulary ?
k
for
each topic k, from a Dirichlet prior ??remain
the same as for traditional LDA (see (Blei et al,
2003), page 4). The traditional LDA model then
draws a multinomial mixture distribution ?
(d)
over
allK topics, for each document d, from a Dirichlet
prior ?. However, we would like to restrict ?
(d)
to
be defined only over the topics that correspond to
249
1 For each topic k ? {1, . . . ,K}:
2 Generate ?
k
= (?
k,1
, . . . , ?
k,V
)
T
? Dir(?|?)
3 For each document d:
4 For each topic k ? {1, . . . ,K}
5 Generate ?
(d)
k
? {0, 1} ? Bernoulli(?|?
k
)
6 Generate ?
(d)
= L
(d)
??
7 Generate ?
(d)
= (?
l
1
, . . . , ?
l
M
d
)
T
? Dir(?|?
(d)
)
8 For each i in {1, . . . , N
d
}:
9 Generate z
i
? {?
(d)
1
, . . . , ?
(d)
M
d
} ? Mult(?|?
(d)
)
10 Generate w
i
? {1, . . . , V } ? Mult(?|?
z
i
)
Table 1: Generative process for Labeled LDA:
?
k
is a vector consisting of the parameters of the
multinomial distribution corresponding to the k
th
topic, ? are the parameters of the Dirichlet topic
prior and ? are the parameters of the word prior,
while ?
k
is the label prior for topic k. For the
meaning of the projection matrix L
(d)
, please re-
fer to Eq 1.
its labels ?
(d)
. Since the word-topic assignments
z
i
(see step 9 in Table 1) are drawn from this dis-
tribution, this restriction ensures that all the topic
assignments are limited to the document?s labels.
Towards this objective, we first generate the
document?s labels ?
(d)
using a Bernoulli coin toss
for each topic k, with a labeling prior probability
?
k
, as shown in step 5. Next, we define the vector
of document?s labels to be ?
(d)
= {k|?
(d)
k
= 1}.
This allows us to define a document-specific la-
bel projection matrix L
(d)
of size M
d
? K for
each document d, where M
d
= |?
(d)
|, as fol-
lows: For each row i ? {1, . . . ,M
d
} and column
j ? {1, . . . ,K} :
L
(d)
ij
=
{
1 if ?
(d)
i
= j
0 otherwise.
(1)
In other words, the i
th
row of L
(d)
has an entry of
1 in column j if and only if the i
th
document label
?
(d)
i
is equal to the topic j, and zero otherwise.
As the name indicates, we use the L
(d)
matrix to
project the parameter vector of the Dirichlet topic
prior ? = (?
1
, . . . , ?
K
)
T
to a lower dimensional
vector ?
(d)
as follows:
?
(d)
= L
(d)
?? = (?
?
(d)
1
, . . . , ?
?
(d)
M
d
)
T
(2)
Clearly, the dimensions of the projected vector
correspond to the topics represented by the labels
of the document. For example, suppose K = 4
and that a document d has labels given by ?
(d)
=
{0, 1, 1, 0}which implies ?
(d)
= {2, 3}, then L
(d)
would be:
(
0 1 0 0
0 0 1 0
)
.
Then, ?
(d)
is drawn from a Dirichlet distribution
with parameters ?
(d)
= L
(d)
? ? = (?
2
, ?
3
)
T
(i.e., with the Dirichlet restricted to the topics 2
and 3).
This fulfills our requirement that the docu-
ment?s topics are restricted to its own labels. The
projection step constitutes the deterministic step
6 in Table 1. The remaining part of the model
from steps 7 through 10 are the same as for reg-
ular LDA.
The dependency of ? on both ? and ? is in-
dicated by directed edges from ? and ? to ? in
the plate notation in Figure 1. This is the only ad-
ditional dependency we introduce in LDA?s repre-
sentation (please compare with Figure 1 in (Blei et
al., 2003)).
2.1 Learning and inference
In most applications discussed in this paper, we
will assume that the documents are multiply
tagged with human labels, both at learning and in-
ference time.
When the labels ?
(d)
of the document are ob-
served, the labeling prior ? is d-separated from
the rest of the model given ?
(d)
. Hence the model
is same as traditional LDA, except the constraint
that the topic prior ?
(d)
is now restricted to the
set of labeled topics ?
(d)
. Therefore, we can use
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) for training where the sampling probability
for a topic for position i in a document d in La-
beled LDA is given by:
P (z
i
= j|z
?i
) ?
n
w
i
?i,j
+ ?
w
i
n
(?)
?i,j
+ ?
T
1
?
n
(d)
?i,j
+ ?
j
n
(d)
?i,?
+ ?
T
1
(3)
where n
w
i
?i,j
is the count of word w
i
in topic j, that
does not include the current assignment z
i
, a miss-
ing subscript or superscript (e.g. n
(?)
?i,j
)) indicates
a summation over that dimension, and 1 is a vector
of 1?s of appropriate dimension.
Although the equation above looks exactly the
same as that of LDA, we have an important dis-
tinction in that, the target topic j is restricted to
belong to the set of labels, i.e., j ? ?
(d)
.
Once the topic multinomials ? are learned from
the training set, one can perform inference on any
new labeled test document using Gibbs sampling
250
restricted to its tags, to determine its per-word la-
bel assignments z. In addition, one can also com-
pute its posterior distribution ? over topics by ap-
propriately normalizing the topic assignments z.
It should now be apparent to the reader how
the new model addresses some of the problems in
multi-labeled corpora that we highlighted in Sec-
tion 1. For example, since there is a one-to-one
correspondence between the labels and topics, the
model can display automatic topical summaries
for each label k in terms of the topic-specific dis-
tribution ?
k
. Similarly, since the model assigns a
label z
i
to each word w
i
in the document d au-
tomatically, we can now extract portions of the
document relevant to each label k (it would be all
words w
i
? w
(d)
such that z
i
= k). In addition,
we can use the topic distribution ?
(d)
to rank the
user specified labels in the order of their relevance
to the document, thereby also eliminating spurious
ones if necessary.
Finally, we note that other less restrictive vari-
ants of the proposed L-LDA model are possible.
For example, one could consider a version that
allows topics that do not correspond to the label
set of a given document with a small probability,
or one that allows a common background topic in
all documents. We did implement these variants
in our preliminary experiments, but they did not
yield better performance than L-LDA in the tasks
we considered. Hence we do not report them in
this paper.
2.2 Relationship to Naive Bayes
The derivation of the algorithm so far has fo-
cused on its relationship to LDA. However, La-
beled LDA can also be seen as an extension of
the event model of a traditional Multinomial Naive
Bayes classifier (McCallum and Nigam, 1998) by
the introduction of a mixture model. In this sec-
tion, we develop the analogy as another way to
understand L-LDA from a supervised perspective.
Consider the case where no document in the
collection is assigned two or more labels. Now
for a particular document d with label l
d
, Labeled
LDA draws each word?s topic variable z
i
from a
multinomial constrained to the document?s label
set, i.e. z
i
= l
d
for each word position i in the doc-
ument. During learning, the Gibbs sampler will
assign each z
i
to l
d
while incrementing ?
l
d
(w
i
),
effectively counting the occurences of each word
type in documents labeled with l
d
. Thus in the
singly labeled document case, the probability of
each document under Labeled LDA is equal to
the probability of the document under the Multi-
nomial Naive Bayes event model trained on those
same document instances. Unlike the Multino-
mial Naive Bayes classifier, Labeled LDA does
not encode a decision boundary for unlabeled doc-
uments by comparing P (w
(d)
|l
d
) to P (w
(d)
|?l
d
),
although we discuss using Labeled LDA for multi-
label classification in Section 7.
Labeled LDA?s similarity to Naive Bayes ends
with the introduction of a second label to any doc-
ument. In a traditional one-versus-rest Multino-
mial Naive Bayes model, a separate classifier for
each label would be trained on all documents with
that label, so each word can contribute a count
of 1 to every observed label?s word distribution.
By contrast, Labeled LDA assumes that each doc-
ument is a mixture of underlying topics, so the
count mass of single word instance must instead be
distributed over the document?s observed labels.
3 Credit attribution within tagged
documents
Social bookmarking websites contain millions of
tags describing many of the web?s most popu-
lar and useful pages. However, not all tags are
uniformly appropriate at all places within a doc-
ument. In the sections that follow, we examine
mechanisms by which Labeled LDA?s credit as-
signment mechanism can be utilized to help sup-
port browsing and summarizing tagged document
collections.
To create a consistent dataset for experimenting
with our model, we selected 20 tags of medium
to high frequency from a collection of documents
dataset crawled from del.icio.us, a popular so-
cial bookmarking website (Heymann et al, 2008).
From that larger dataset, we selected uniformly at
random four thousand documents that contained
at least one of the 20 tags, and then filtered each
document?s tag set by removing tags not present
in our tag set. After filtering, the resulting cor-
pus averaged 781 non-stop words per document,
with each document having 4 distinct tags on aver-
age. In contrast to many existing text datasets, our
tagged corpus is highly multiply labeled: almost
90% of of the documents have more than one tag.
(For comparison, less than one third of the news
documents in the popular RCV1-v2 collection of
newswire are multiply labeled). We will refer to
251
this collection of data as the del.icio.us tag dataset.
4 Topic Visualization
A first question we ask of Labeled LDA is how its
topics compare with those learned by traditional
LDA on the same collection of documents. We ran
our implementations of Labeled LDA and LDA
on the del.icio.us corpus described above. Both
are based on the standard collapsed Gibbs sam-
pler, with the constraints for Labeled LDA imple-
mented as in Section 2.
web search site blog css content google list page posted great work comments read nice post great april blog march june wordpress
book image pdf review library posted read copyright books title
web
boo
ks
scie
nce
com
p
ute
r
reli
gion
java
cult
ure
works water map human life work science time world years sleep
windows file version linux comp-uter free system software mac
comment god jesus people gospel bible reply lord religion written
applications spring open web java pattern eclipse development ajax
people day link posted time com-ments back music jane permalink
news information service web on-line project site free search home
web images design content java css website articles page learning
jun quote pro views added check anonymous card core power ghz
life written jesus words made man called mark john person fact name
8
house light radio media photo-graphy news music travel cover
game review street public art health food city history science
13
19
4
3
2
12
Tag (Labeled LDA) (LDA) Topic ID
Figure 2: Comparison of some of the 20 topics
learned on del.icio.us by Labeled LDA (left) and
traditional LDA (right), with representative words
for each topic shown in the boxes. Labeled LDA?s
topics are named by their associated tag. Arrows
from right-to-left show the mapping of LDA topics
to the closest Labeled LDA topic by cosine simi-
larity. Tags not shown are: design, education, en-
glish, grammar, history, internet, language, phi-
losophy, politics, programming, reference, style,
writing.
Figure 2 shows the top words associated with
20 topics learned by Labeled LDA and 20 topics
learned by unsupervised LDA on the del.icio.us
document collection. Labeled LDA?s topics are
directly named with the tag that corresponds to
each topic, an improvement over standard prac-
tice of inferring the topic name by inspection (Mei
et al, 2007). The topics learned by the unsu-
pervised variant were matched to a Labeled LDA
topic highest cosine similarity.
The topics selected are representative: com-
pared to Labeled LDA, unmodified LDA allocates
many topics for describing the largest parts of the
The Elements of Style , William Strunk , Jr.
Asserting that one must first know the rules to break them, this 
classic reference book is a must-have for any student and 
conscientious writer.  Intended for use in which the practice of
composition is combined with the study of literature, it gives in
brief space the principal requirements of plain English style and
concentratesattention on the rules of usage and principles of
composition most commonly violated.
Figure 3: Example document with important
words annotated with four of the page?s tags as
learned by Labeled LDA. Red (single underline)
is style, green (dashed underline) grammar, blue
(double underline) reference, and black (jagged
underline) education.
corpus and under-represents tags that are less un-
common: of the 20 topics learned, LDA learned
multiple topics mapping to each of five tags (web,
culture, and computer, reference, and politics, all
of which were common in the dataset) and learned
no topics that aligned with six tags (books, english,
science, history, grammar, java, and philosophy,
which were rarer).
5 Tagged document visualization
In addition to providing automatic summaries of
the words best associated with each tag in the cor-
pus, Labeled LDA?s credit attribution mechanism
can be used to augment the view of a single doc-
ument with rich contextual information about the
document?s tags.
Figure 3 shows one web document from the col-
lection, a page describing a guide to writing En-
glish prose. The 10 most common tags for that
document are writing, reference, english, gram-
mar, style, language, books, book, strunk, and ed-
ucation, the first eight of which were included in
our set of 20 tags. In the figure, each word that has
high posterior probability from one tag has been
annotated with that tag. The red words come from
the style tag, green from the grammar tag, blue
from the reference tag, and black from the educa-
tion tag. In this case, the model does very well at
assigning individual words to the tags that, subjec-
tively, seem to strongly imply the presence of that
tag on this page. A more polished rendering could
add subtle visual cues about which parts of a page
are most appropriate for a particular set of tags.
252
books
L-LDA this classic reference book is a must-have for any
student and conscientious writer. Intended for
SVM the rules of usage and principles of composition
most commonly violated. Search: CONTENTS Bibli-
ographic
language
L-LDA the beginning of a sentence must refer to the gram-
matical subject 8. Divide words at
SVM combined with the study of literature, it gives in brief
space the principal requirements of
grammar
L-LDA requirements of plain English style and concen-
trates attention on the rules of usage and principles of
SVM them, this classic reference book is a must-have for
any student and conscientious writer.
Figure 4: Representative snippets extracted by
L-LDA and tag-specific SVMs for the web page
shown in Figure 3.
6 Snippet Extraction
Another natural application of Labeled LDA?s
credit assignment mechanism is as a means of se-
lecting snippets of a document that best describe
its contents from the perspective of a particular
tag. Consider again the document in Figure 3. In-
tuitively, if this document were shown to a user
interested in the tag grammar, the most appropri-
ate snippet of words might prefer to contain the
phrase ?rules of usage,? whereas a user interested
in the term style might prefer the title ?Elements
of Style.?
To quantitatively evaluate Labeled LDA?s per-
formance at this task, we constructed a set of 29
recently tagged documents from del.icio.us that
were labeled with two or more tags from the 20 tag
subset, resulting in a total of 149 (document,tag)
pairs. For each pair, we extracted a 15-word win-
dow with the highest tag-specific score from the
document. Two systems were used to score each
window: Labeled LDA and a collection of one-
vs-rest SVMs trained for each tag in the system.
L-LDA scored each window as the expected prob-
ability that the tag had generated each word. For
SVMs, each window was taken as its own doc-
ument and scored using the tag-specific SVM?s
un-thresholded scoring function, taking the win-
dow with the most positive score. While a com-
plete solution to the tag-specific snippet extraction
Model Best Snippet Unanimous
L-LDA 72 / 149 24 / 51
SVM 21 / 149 2 / 51
Table 2: Human judgments of tag-specific snippet
quality as extracted by L-LDA and SVM. The cen-
ter column is the number of document-tag pairs for
which a system?s snippet was judged superior. The
right column is the number of snippets for which
all three annotators were in complete agreement
(numerator) in the subset of document scored by
all three annotators (denominator).
problem might be more informed by better lin-
guistic features (such as phrase boundaries), this
experimental setup suffices to evaluate both kinds
of models for their ability to appropriately assign
words to underlying labels.
Figure 3 shows some example snippets output
by our system for this document. Note that while
SVMs did manage to select snippets that were
vaguely on topic, Labeled LDA?s outputs are gen-
erally of superior subjective quality. To quantify
this intuition, three human annotators rated each
pair of snippets. The outputs were randomly la-
beled as ?System A? or ?System B,? and the anno-
tators were asked to judge which system generated
a better tag-specific document subset. The judges
were also allowed to select neither system if there
was no clear winner. The results are summarized
in Table 2.
L-LDA was judged superior by a wide margin:
of the 149 judgments, L-LDA?s output was se-
lected as preferable in 72 cases, whereas SVM?s
was selected in only 21. The difference between
these scores was highly significant (p < .001) by
the sign test. To quantify the reliability of the judg-
ments, 51 of the 149 document-tag pairs were la-
beled by all three annotators. In this group, the
judgments were in substantial agreement,
1
with
Fleiss? Kappa at .63.
Further analysis of the triply-annotated sub-
set yields further evidence of L-LDA?s advantage
over SVM?s: 33 of the 51 were tag-page pairs
where L-LDA?s output was picked by at least one
annotator as a better snippet (although L-LDA
might not have been picked by the other annota-
tors). And of those, 24 were unanimous in that
1
Of the 15 judgments that were in contention, only two
conflicted on which system was superior (L-LDA versus
SVM); the remaining disagreements were about whether or
not one of the systems was a clear winner.
253
all three judges selected L-LDA?s output. By con-
trast, only 10 of the 51 were tag-page pairs where
SVMs? output was picked by at least one anno-
tator, and of those, only 2 were selected unani-
mously.
7 Multilabeled Text Classification
In the preceding section we demonstrated how La-
beled LDA?s credit attribution mechanism enabled
effective modeling within documents. In this sec-
tion, we consider whether L-LDA can be adapted
as an effective multi-label classifier for documents
as a whole. To answer that question, we applied
a modified variant of L-LDA to a multi-label doc-
ument classification problem: given a training set
consisting of documents with multiple labels, pre-
dict the set of labels appropriate for each docu-
ment in a test set.
Multi-label classification is a well researched
problem. Many modern approaches incorporate
label correlations (e.g., Kazawa et al (2004), Ji
et al (2008)). Others, like our algorithm are
based on mixture models (such as Ueda and Saito
(2003)). However, we are aware of no methods
that trade off label-specific word distributions with
document-specific label distributions in quite the
same way.
In Section 2, we discussed learning and infer-
ence when labels are observed. In the task of mul-
tilabel classification, labels are available at train-
ing time, so the learning part remains the same as
discussed before. However, inferring the best set
of labels for an unlabeled document at test time is
more complex: it involves assessing all label as-
signments and returning the assignment that has
the highest posterior probability. However, this
is not straight-forward, since there are 2
K
possi-
ble label assignments. To make matters worse, the
support of ?(?
(d)
) is different for different label
assignments. Although we are in the process of
developing an efficient sampling algorithm for this
inference, for the purposes of this paper we make
the simplifying assumption that the model reduces
to standard LDA at inference, where the document
is free to sample from any of the K topics. This
is a reasonable assumption because allowing the
model to explore the whole topic space for each
document is similar to exploring all possible label
assignments. The document?s most likely labels
can then be inferred by suitably thresholding its
posterior probability over topics.
As a baseline, we use a set of multiple one-vs-
rest SVM classifiers which is a popular and ex-
tremely competitive baseline used by most previ-
ous papers (see (Kazawa et al, 2004; Ueda and
Saito, 2003) for instance). We scored each model
based on Micro-F1 and Macro-F1 as our evalua-
tion measures (Lewis et al, 2004). While the for-
mer allows larger classes to dominate its results,
the latter assigns an equal weight to all classes,
providing us complementary information.
7.1 Yahoo
We ran experiments on a corpus from the Yahoo
directory, modeling our experimental conditions
on the ones described in (Ji et al, 2008).
2
We
considered documents drawn from 8 top level cat-
egories in the Yahoo directory, where each doc-
ument can be placed in any number of subcate-
gories. The results were mixed, with SVMs ahead
on one measure: Labeled LDA beat SVMs on five
out of eight datasets on MacroF1, but didn?t win
on any datasets on MicroF1. Results are presented
in Table 3.
Because only a processed form of the docu-
ments was released, the Yahoo dataset does not
lend itself well to error analysis. However, only
33% of the documents in each top-level category
were applied to more than one sub-category, so the
credit assignment machinery of L-LDA was un-
used for the majority of documents. We there-
fore ran an artificial second set of experiments
considering only those documents that had been
given more than one label in the training data. On
these documents, the results were again mixed, but
Labeled LDA comes out ahead. For MacroF1,
L-LDA beat SVMs on four datasets, SVMs beat
L-LDA on one dataset, and three were a statistical
tie.
3
On MicroF1, L-LDA did much better than on
the larger subset, outperforming on four datasets
with the other four a statistical tie.
It is worth noting that the Yahoo datasets are
skewed by construction to contain many docu-
ments with highly overlapping content: because
each collection is within the same super-class such
as ?Arts?, ?Business?, etc., each sub-categories?
2
We did not carefully tune per-class thresholds of each of
the one vs. rest classifiers in each model, but instead tuned
only one threshold for all classifiers in each model via cross-
validation on the Arts subsets. As such, our numbers were on
an average 3-4% less than those reported in (Ji et al, 2008),
but the methods were comparably tuned.
3
The difference between means of multiple runs were not
significantly different by two-tailed paired t-test.
254
Dataset %MacroF1 %MicroF1
L-LDA SVM L-LDA SVM
Arts 30.70(1.62) 23.23 (0.67) 39.81(1.85) 48.42 (0.45)
Business 30.81(0.75) 22.82 (1.60) 67.00(1.29) 72.15 (0.62)
Computers 27.55(1.98) 18.29 (1.53) 48.95(0.76) 61.97 (0.54)
Education 33.78(1.70) 36.03 (1.30) 41.19(1.48) 59.45 (0.56)
Entertainment 39.42(1.38) 43.22 (0.49) 47.71(0.61) 62.89 (0.50)
Health 45.36(2.00) 47.86 (1.72) 58.13(0.43) 72.21 (0.26)
Recreation 37.63(1.00) 33.77 (1.17) 43.71(0.31) 59.15 (0.71)
Society 27.32(1.24) 23.89 (0.74) 42.98(0.28) 52.29 (0.67)
Table 3: Averaged performance across ten runs of multi-label text classification for predicting subsets
of the named Yahoo directory categories. Numbers in parentheses are standard deviations across runs.
L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1.
vocabularies will naturally overlap a great deal.
L-LDA?s credit attribution mechanism is most ef-
fective at partitioning semantically distinct words
into their respective label vocabularies, so we ex-
pect that Labeled-LDA?s performance as a text
classifier would improve on collections with more
semantically diverse labels.
7.2 Tagged Web Pages
We also applied our method to text classification
on the del.icio.us dataset, where the documents are
naturally multiply labeled (more than 89%) and
where the tags are less inherently similar than in
the Yahoo subcategories. Therefore we expect La-
beled LDA to do better credit assignment on this
subset and consequently to show improved perfor-
mance as a classifier, and indeed this is the case.
We evaluated L-LDA and multiple one-vs-rest
SVMs on 4000 documents with the 20 tag sub-
set described in Section 3. L-LDA and multiple
one-vs-rest SVMs were trained on the first 80% of
documents and evaluated on the remaining 20%,
with results averaged across 10 random permuta-
tions of the dataset. The results are shown in Ta-
ble 4. We tuned the SVMs? shared cost parameter
C(= 10.0) and selected raw term frequency over
tf-idf weighting based on 4-fold cross-validation
on 3,000 documents drawn from an independent
permutation of the data. For L-LDA, we tuned the
shared parameters of threshold and proportional-
ity constants in word and topic priors. L-LDA and
SVM have very similar performance on MacroF1,
while L-LDA substantially outperforms on Mi-
croF1. In both cases, L-LDA?s improvement is
statistically significantly by a 2-tailed paired t-test
at 95% confidence.
Model %MacroF1 %MicroF1
L-LDA 39.85 (.989) 52.12 (.434)
SVM 39.00 (.423) 39.33 (.574)
Table 4: Mean performance across ten runs of
multi-label text classification for predicting 20
tags on del.icio.us data. L-LDA outperforms
SVMs significantly on both metrics by a 2-tailed,
paired t-test at 95% confidence.
8 Discussion
One of the main advantages of L-LDA on mul-
tiply labeled documents comes from the model?s
document-specific topic mixture ?. By explicitly
modeling the importance of each label in the doc-
ument, Labeled LDA can effective perform some
contextual word sense disambiguation, which sug-
gests why L-LDA can outperform SVMs on the
del.icio.us dataset.
As a concrete example, consider the excerpt
of text from the del.icio.us dataset in Figure 5.
The document itself has several tags, including
design and programming. Initially, many of the
likelihood probabilities p(w|label) for the (con-
tent) words in this excerpt are higher for the label
programming than design, including ?content?,
?client?, ?CMS? and even ?designed?, while de-
sign has higher likelihoods for just ?website? and
?happy?. However, after performing inference on
this document using L-LDA, the inferred docu-
ment probability for design (p(design)) is much
higher than it is for programming. In fact, the
higher probability for the tag more than makes up
the difference in the likelihood for all the words
except ?CMS? (Content Management System), so
255
The website is designed, CMS works, content has been added and the client is happy.
The website is designed, CMS works, content has been added and the client is happy.
Before Inference
After Inference
Figure 5: The effect of tag mixture proportions for credit assignment in a web document. Blue (single
underline) words are generated from the design tag; red (dashed underline) from the programming tag.
By themselves, most words used here have a higher probability in programming than in design. But
because the document as a whole is more about design than programming(incorporating words not shown
here), inferring the document?s topic-mixture ? enables L-LDA to correctly re-assign most words.
that L-LDA correctly infers that most of the words
in this passage have more to do with design than
programming.
9 Conclusion
This paper has introduced Labeled LDA, a novel
model of multi-labeled corpora that directly ad-
dresses the credit assignment problem. The new
model improves upon LDA for labeled corpora
by gracefully incorporating user supervision in the
form of a one-to-one mapping between topics and
labels. We demonstrate the model?s effectiveness
on tasks related to credit attribution within docu-
ments, including document visualizations and tag-
specific snippet extraction. An approximation to
Labeled LDA is also shown to be competitive with
a strong baseline (multiple one vs-rest SVMs) for
multi-label classification.
Because Labeled LDA is a graphical model
in the LDA family, it enables a range of natu-
ral extensions for future investigation. For exam-
ple, the current model does not capture correla-
tions between labels, but such correlations might
be introduced by composing Labeled LDA with
newer state of the art topic models like the Cor-
related Topic Model (Blei and Lafferty, 2006) or
the Pachinko Allocation Model (Li and McCal-
lum, 2006). And with improved inference for un-
supervised ?, Labeled LDA lends itself naturally
to modeling semi-supervised corpora where labels
are observed for only some documents.
Acknowledgments
This project was supported in part by the Presi-
dent of Stanford University through the IRiSS Ini-
tiatives Assessment project.
References
D. M. Blei and J. Lafferty. 2006. Correlated Topic
Models. NIPS, 18:147.
D. Blei and J McAuliffe. 2007. Supervised Topic
Models. In NIPS, volume 21.
D. M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. PNAS, 1:5228?35.
P. Heymann, G. Koutrika, and H. Garcia-Molina. 2008.
Can social bookmarking improve web search. In
WSDM.
S. Ji, L. Tang, S. Yu, and J. Ye. 2008. Extracting
shared subspace for multi-label classification. In
KDD, pages 381?389, New York, NY, USA. ACM.
H. Kazawa, H. Taira T. Izumitani, and E. Maeda. 2004.
Maximal margin labeling for multi-topic text catego-
rization. In NIPS.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. 2008. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS, volume 22.
D. D. Lewis, Y. Yang, T. G. Rose, G. Dietterich, F. Li,
and F. Li. 2004. RCV1: A new benchmark collec-
tion for text categorization research. JMLR, 5:361?
397.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In International conference on Machine
learning, pages 577?584.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
AAAI-98 workshop on learning for text categoriza-
tion, volume 7.
Q. Mei, X. Shen, and C Zhai. 2007. Automatic label-
ing of multinomial topic models. In KDD.
D. Ramage, P. Heymann, C. D. Manning, and
H. Garcia-Molina. 2009. Clustering the tagged web.
In WSDM.
N. Ueda and K. Saito. 2003. Parametric mixture mod-
els for multi-labeled text includes models that can be
seen to fit within a dimensionality reduction frame-
work. In NIPS.
256
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 23?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Random Walks for Text Semantic Similarity
Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{dramage,manning}@cs.stanford.edu
rafferty@eecs.berkeley.edu
Abstract
Many tasks in NLP stand to benefit from
robust measures of semantic similarity for
units above the level of individual words.
Rich semantic resources such as WordNet
provide local semantic information at the
lexical level. However, effectively com-
bining this information to compute scores
for phrases or sentences is an open prob-
lem. Our algorithm aggregates local re-
latedness information via a random walk
over a graph constructed from an underly-
ing lexical resource. The stationary dis-
tribution of the graph walk forms a ?se-
mantic signature? that can be compared
to another such distribution to get a relat-
edness score for texts. On a paraphrase
recognition task, the algorithm achieves an
18.5% relative reduction in error rate over
a vector-space baseline. We also show that
the graph walk similarity between texts
has complementary value as a feature for
recognizing textual entailment, improving
on a competitive baseline system.
1 Introduction
Many natural language processing applications
must directly or indirectly assess the semantic sim-
ilarity of text passages. Modern approaches to
information retrieval, summarization, and textual
entailment, among others, require robust numeric
relevance judgments when a pair of texts is pro-
vided as input. Although each task demands its
own scoring criteria, a simple lexical overlap mea-
sure such as cosine similarity of document vectors
can often serve as a surprisingly powerful base-
line. We argue that there is room to improve these
general-purpose similarity measures, particularly
for short text passages.
Most approaches fall under one of two cate-
gories. One set of approaches attempts to explic-
itly account for fine-grained structure of the two
passages, e.g. by aligning trees or constructing
logical forms for theorem proving. While these
approaches have the potential for high precision
on many examples, errors in alignment judgments
or formula construction are often insurmountable.
More broadly, it?s not always clear that there is a
correct alignment or logical form that is most ap-
propriate for a particular sentence pair. The other
approach tends to ignore structure, as canonically
represented by the vector space model, where any
lexical item in common between the two passages
contributes to their similarity score. While these
approaches often fail to capture distinctions im-
posed by, e.g. negation, they do correctly capture
a broad notion of similarity or aboutness.
This paper presents a novel variant of the vec-
tor space model of text similarity based on a ran-
dom walk algorithm. Instead of comparing two
bags-of-words directly, we compare the distribu-
tion each text induces when used as the seed of
a random walk over a graph derived from Word-
Net and corpus statistics. The walk posits the ex-
istence of a distributional particle that roams the
graph, biased toward the neighborhood surround-
ing an input bag of words. Eventually, the walk
reaches a stationary distribution over all nodes in
the graph, smoothing the peaked input distribution
over a much larger semantic space. Two such sta-
tionary distributions can be compared using con-
ventional measures of vector similarity, producing
a final relatedness score.
This paper makes the following contributions.
We present a novel random graph walk algorithm
23
Word Step 1 Step 2 Step 3 Conv.
eat 3 8 9 9
corrode 10 33 53 >100
pasta ? 2 3 5
dish ? 4 5 6
food ? ? 21 12
solid ? ? ? 26
Table 1: Ranks of sample words in the distribu-
tion for I ate a salad and spaghetti after a given
number of steps and at convergence. Words in the
vector are ordered by probability at time step t; the
word with the highest probability in the vector has
rank 1. ??? indicates that node had not yet been
reached.
for semantic similarity of texts, demonstrating its
efficiency as compared to a much slower but math-
ematically equivalent model based on summed
similarity judgments of individual words. We
show that walks effectively aggregate information
over multiple types of links and multiple input
words on an unsupervised paraphrase recognition
task. Furthermore, when used as a feature, the
walk?s semantic similarity score can improve the
performance of an existing, competitive textual
entailment system. Finally, we provide empiri-
cal results demonstrating that indeed, each step of
the random walk contributes to its ability to assess
paraphrase judgments.
2 A random walk example
To provide some intuition about the behavior of
the random walk on text passages, consider the
following example sentence: I ate a salad and
spaghetti.
No measure based solely on lexical identity
would detect overlap between this sentence and
another input consisting of only the word food.
But if each text is provided as input to the random
walk, local relatedness links from one word to an-
other allow the distributional particle to explore
nearby parts of the semantic space. The number of
non-zero elements in both vectors increases, even-
tually converging to a stationary distribution for
which both vectors have many shared non-zero en-
tries.
Table 1 ranks elements of the sentence vector
based on their relative weights. Observe that at the
beginning of the walk, corrode has a high rank due
to its association with the WordNet sense of eat
corresponding to eating away at something. How-
ever, because this concept is not closely linked
with other words in the sentence, its relative rank
drops as the distribution converges and other word
senses more related to food are pushed up. The
random walk allows the meanings of words to re-
inforce one another. If the sentence above had
ended with drank wine rather than spaghetti, the
final weight on the food node would be smaller
since fewer input words would be as closely linked
to food. This matches the intuition that the first
sentence has more to do with food than does the
second, although both walks should and do give
some weight to this node.
3 Related work
Semantic relatedness for individual words has
been thoroughly investigated in previous work.
Budanitsky and Hirst (2006) provide an overview
of many of the knowledge-based measures derived
from WordNet, although other data sources have
been used as well. Hughes and Ramage (2007) is
one such measure based on random graph walks.
Prior work has considered random walks on var-
ious text graphs, with applications to query expan-
sion (Collins-Thompson and Callan, 2005), email
address resolution (Minkov and Cohen, 2007), and
word-sense disambiguation (Agirre and Soroa,
2009), among others.
Measures of similarity have also been proposed
for sentence or paragraph length text passages.
Mihalcea et al (2006) present an algorithm for
the general problem of deciding the similarity of
meaning in two text passages, coining the name
?text semantic similarity? for the task. Corley
and Mihalcea (2005) apply this algorithm to para-
phrase recognition.
Previous work has shown that similarity mea-
sures can have some success as a measure of tex-
tual entailment. Glickman et al (2005) showed
that many entailment problems can be answered
using only a bag-of-words representation and web
co-occurrence statistics. Many systems integrate
lexical relatedness and overlap measures with
deeper semantic and syntactic features to create
improved results upon relatedness alone, as in
Montejo-R?aez et al (2007).
4 Random walks on lexical graphs
In this section, we describe the mechanics of
computing semantic relatedness for text passages
24
based on the random graph walk framework. The
algorithm underlying these computations is related
to topic-sensitive PageRank (Haveliwala, 2002);
see Berkhin (2005) for a survey of related algo-
rithms.
To compute semantic relatedness for a pair of
passages, we compare the stationary distributions
of two Markov chains, each with a state space de-
fined over all lexical items in an underlying corpus
or database. Formally, we define the probability of
finding the particle at a node n
i
at time t as:
n
(t)
i
=
?
n
j
?V
n
(t?1)
j
P (n
i
| n
j
)
where P (n
i
| n
j
) is the probability of transition-
ing from n
j
to n
i
at any time step. If those transi-
tions bias the particle to the neighborhood around
the words in a text, the particle?s distribution can
be used as a lexical signature.
To compute relatedness for a pair of texts, we
first define the graph nodes and transition proba-
bilities for the random walk Markov chain from
an underlying lexical resource. Next, we deter-
mine an initial distribution over that state space for
a particular input passage of text. Then, we sim-
ulate a random walk in the state space, biased to-
ward the initial distribution, resulting in a passage-
specific distribution over the graph. Finally, we
compare the resulting stationary distributions from
two such walks using a measure of distributional
similarity. The remainder of this section discusses
each stage in more detail.
4.1 Graph construction
We construct a graph G = (V,E) with vertices V
and edges E extracted from WordNet 3.0. Word-
Net (Fellbaum, 1998) is an annotated graph of
synsets, each representing one concept, that are
populated by one or more words. The set of ver-
tices extracted from the graph is all synsets present
in WordNet (e.g. foot#n#1 meaning the part of
the human leg below the ankle), all part-of-speech
tagged words participating in those synsets (e.g.
foot#n linking to foot#n#1 and foot#n#2 etc.), and
all untagged words (e.g. foot linking to foot#n and
foot#v). The set of edges connecting synset nodes
is all inter-synset edges contained in WordNet,
such as hyponymy, synonomy, antonymy, etc., ex-
cept for regional and usage links. All WordNet
relational edges are given uniform weight. Edges
also connect each part-of-speech tagged word to
all synsets it takes part in, and from each word to
all its part-of-speech. These edge weights are de-
rived from corpus counts as in Hughes and Ram-
age (2007). We also included a low-weight self-
loop for each node.
Our graph has 420,253 nodes connected by
1,064,464 edges. Because synset nodes do not link
outward to part-of-speech tagged nodes or word
nodes in this graph, only the 117,659 synset nodes
have non-zero probability in every random walk?
i.e. the stationary distribution will always be non-
zero for these 117,659 nodes, but will be non-zero
for only a subset of the remainder.
4.2 Initial distribution construction
The next step is to seed the random walk with an
initial distribution over lexical nodes specific to
the given sentence. To do so, we first tag the in-
put sentence with parts-of-speech and lemmatize
each word based on the finite state transducer of
Minnen et al (2001). We search over consecu-
tive words to match multi-word collocation nodes
found in the graph. If the word or its lemma is
part of a sequence that makes a complete colloca-
tion, that collocation is used. If not, the word or
its lemma with its part of speech tag is used if it
is present as a graph node. Finally, we fall back
to the surface word form or underlying lemma
form without part-of-speech information if neces-
sary. For example, the input sentence: The boy
went with his dog to the store, would result in mass
being assigned to underlying graph nodes boy#n,
go with, he, dog#n, store#n.
Term weights are set with tf.idf and then nor-
malized. Each term?s weight is proportional to the
number of occurrences in the sentence times the
log of the number of documents in some corpus
divided by the number of documents containing
that term. Our idf counts were derived from the
English Gigaword corpus 1994-1999.
4.3 Computing the stationary distribution
We use the power iteration method to compute the
stationary distribution for the Markov chain. Let
the distribution over the N states at time step t of
the random walk be denoted ~v
(t)
? R
N
, where
~v
(0)
is the initial distribution as defined above. We
denote the column-normalized state-transition ma-
trix as M ? R
N?N
. We compute the stationary
distribution of the Markov chain with probability
? of returning to the initial distribution at each
25
time step as the limit as t?? of:
~v
(t)
= ?~v
(0)
+ (1? ?)M~v
(t?1)
In practice, we test for convergence by examining
if
?
N
i=1
?v
(t)
i
? v
(t?1)
i
? < 10
?6
, which in our ex-
periments was usually after about 50 iterations.
Note that the resulting stationary distribution
can be factored as the weighted sum of the sta-
tionary distributions of each word represented in
the initial distribution. Because the initial distri-
bution ~v
(0)
is a normalized weighted sum, it can
be re-written as ~v
(0)
=
?
k
?
k
? ~w
(0)
k
for ~w
k
hav-
ing a point mass at some underlying node in the
graph and with ?
k
positive such that
?
k
?
k
= 1.
A simple proof by induction shows that the sta-
tionary distribution ~v
(?)
is itself the weighted sum
of the stationary distribution of each underlying
word, i.e. ~v
?
=
?
k
?
k
? ~w
(?)
k
.
In practice, the stationary distribution for a
passage of text can be computed from a single
specially-constructed Markov chain. The process
is equivalent to taking the weighted sum of every
word type in the passage computed independently.
Because the time needed to compute the station-
ary distribution is dominated by the sparsity pat-
tern of the walk?s transition matrix, the computa-
tion of the stationary distribution for the passage
takes a fraction of the time needed if the station-
ary distribution for each word were computed in-
dependently.
4.4 Comparing stationary distributions
In order to get a final relatedness score for a pair
of texts, we must compare the stationary distribu-
tion from the first walk with the distribution from
the second walk. There exist many measures for
computing a final similarity (or divergence) mea-
sure from a pair of distributions, including geo-
metric measures, information theoretic measures,
and probabilistic measures. See, for instance, the
overview of measures provided in Lee (2001).
In system development on training data, we
found that most measures were reasonably effec-
tive. For the rest of this paper, we report num-
bers using cosine similarity, a standard measure in
information retrieval; Jensen-Shannon divergence,
a commonly used symmetric measure based on
KL-divergence; and the dice measure extended to
weighted features (Curran, 2004). A summary of
these measures is shown in Table 2. Justification
Cosine
~x?~y
?~x?
2
?~y?
2
Jensen-Shannon
1
2
D(x?
x+y
2
) +
1
2
D(y?
x+y
2
)
Dice
2
P
i
min(x
i
,y
i
)
P
i
x
i
+
P
i
y
i
Table 2: Three measures of distributional similar-
ity between vectors ~x and ~y used to compare the
stationary distributions from passage-specific ran-
dom walks. D(p?q) is KL-divergence, defined as
?
i
p
i
log
p
i
q
i
.
for the choice of these three measures is discussed
in Section 6.
5 Evaluation
We evaluate the system on two tasks that might
benefit from semantic similarity judgments: para-
phrase recognition and recognizing textual entail-
ment. A complete solution to either task will cer-
tainly require tools more tuned to linguistic struc-
ture; the paraphrase detection evaluation argues
that the walk captures a useful notion of semantics
at the sentence level. The entailment system eval-
uation demonstrates that the walk score can im-
prove a larger system that does make use of more
fine-grained linguistic knowledge.
5.1 Paraphrase recognition
The Microsoft Research (MSR) paraphrase data
set (Dolan et al, 2004) is a collection of 5801
pairs of sentences automatically collected from
newswire over 18 months. Each pair was hand-
annotated by at least two judges with a binary
yes/no judgment as to whether one sentence was
a valid paraphrase of the other. Annotators were
asked to judge whether the meanings of each
sentence pair were reasonably equivalent. Inter-
annotator agreement was 83%. However, 67% of
the pairs were judged to be paraphrases, so the cor-
pus does not reflect the rarity of paraphrases in the
wild. The data set comes pre-split into 4076 train-
ing pairs and 1725 test pairs.
Because annotators were asked to judge if the
meanings of two sentences were equivalent, the
paraphrase corpus is a natural evaluation testbed
for measures of semantic similarity. Mihalcea et
al. (2006) defines a measure of text semantic sim-
ilarity and evaluates it in an unsupervised para-
phrase detector on this data set. We present their
26
algorithm here as a strong reference point for se-
mantic similarity between text passages, based on
similar underlying lexical resources.
The Mihalcea et al (2006) algorithm is a wrap-
per method that works with any underlying mea-
sure of lexical similarity. The similarity of a pair
of texts T
1
and T
2
, denoted as sim
m
(T
1
, T
2
), is
computed as:
sim
m
(T
1
, T
2
) =
1
2
f(T
1
, T
2
) +
1
2
f(T
2
, T
1
)
f(T
a
, T
b
) =
P
w?T
a
maxSim(w, T
b
) ? idf(w)
P
w?T
a
idf(w)
where the maxSim(w, T ) function is defined as
the maximum similarity of the word w within the
text T as determined by an underlying measure of
lexical semantic relatedness. Here, idf(w) is de-
fined as the number of documents in a background
corpus divided by the number of documents con-
taining the term. maxSim compares only within
the same WordNet part-of-speech labeling in or-
der to support evaluation with lexical relatedness
measures that cannot cross part-of-speech bound-
aries.
Mihalcea et al (2006) presents results for sev-
eral underlying measures of lexical semantic re-
latedness. These are subdivided into corpus-based
measures (using Latent Semantic Analysis (Lan-
dauer et al, 1998) and a pointwise-mutual infor-
mation measure) and knowledge-based resources
driven by WordNet. The latter include the methods
of Jiang and Conrath (1997), Lesk (1986), Resnik
(1999), and others.
In this unsupervised experimental setting, we
consider using only a thresholded similarity value
from our system and from the Mihalcea algorithm
to determine the paraphrase or non-paraphrase
judgment. For consistency with previous work, we
threshold at 0.5. Note that this threshold could be
tuned on the training data in a supervised setting.
Informally, we observed that on the training data a
threshold of near 0.5 was often a good choice for
this task.
Table 3 shows the results of our system and
a representative subset of those reported in (Mi-
halcea et al, 2006). All the reported measures
from both systems do a reasonable job of para-
phrase detection ? the majority of pairs in the cor-
pus are deemed paraphrases when the similarity
measure is thresholded at 0.5, and indeed this is
reasonable given the way in which the data were
System Acc. F
1
: c
1
F
1
: c
0
Macro F
1
Random Graph Walk
Walk (Cosine) 0.687 0.787 0.413 0.617
Walk (Dice) 0.708 0.801 0.453 0.645
Walk (JS) 0.688 0.805 0.225 0.609
Mihalcea et. al., Corpus-based
PMI-IR 0.699 0.810 0.301 0.625
LSA 0.684 0.805 0.170 0.560
Mihalcea et. al., WordNet-based
J&C 0.693 0.790 0.433 0.629
Lesk 0.693 0.789 0.439 0.629
Resnik 0.690 0.804 0.254 0.618
Baselines
Vector-based 0.654 0.753 0.420 0.591
Random 0.513 0.578 0.425 0.518
Majority (c
1
) 0.665 0.799 ? 0.399
Table 3: System performance on 1725 examples of
the MSR paraphrase detection test set. Accuracy
(micro-averaged F
1
), F
1
for c
1
?paraphrase? and
c
0
?non-paraphrase? classes, and macro-averaged
F
1
are reported.
collected. The first three rows are the perfor-
mance of the similarity judgments output by our
walk under three different distributional similar-
ity measures (cosine, dice, and Jensen-Shannon),
with the walk score using the dice measure outper-
forming all other systems on both accuracy and
macro-averaged F
1
. The output of the Mihalcea
system using a representative subset of underly-
ing lexical measures is reported in the second and
third segments. The fourth segment reports the re-
sults of baseline methods?the vector space simi-
larity measure is cosine similarity among vectors
using tf.idf weighting, and the random baseline
chooses uniformly at random, both as reported in
(Mihalcea et al, 2006). We add the additional
baseline of always guessing the majority class la-
bel because the data set is skewed toward ?para-
phrase.?
In an unbalanced data setting, it is important to
consider more than just accuracy and F
1
on the
majority class. We report accuracy, F
1
for each
class label, and the macro-averaged F
1
on all sys-
tems. F
1
: c
0
and Macro-F
1
are inferred for the sys-
tem variants reported in (Mihalcea et al, 2006).
Micro-averaged F
1
in this context is equivalent to
accuracy (Manning et al, 2008).
Mihalcea also reports a combined classifier
which thresholds on the simple average of the in-
dividual classifiers, resulting in the highest num-
bers reported in that work, with accuracy of 0.703,
?paraphrase? class F
1
: c
1
= 0.813, and inferred
Macro F
1
= 0.648. We believe that the scores
27
Data Set Cosine Dice Jensen-Shannon
RTE2 dev 55.00 51.75 55.50
RTE2 test 57.00 54.25 57.50
RTE3 dev 59.00 57.25 59.00
RTE3 test 55.75 55.75 56.75
Table 4: Accuracy of entailment detection when
thresholding the text similarity score output by the
random walk.
from the various walk measures might also im-
prove performance when in a combination clas-
sifier, but without access to the individual judg-
ments in that system we are unable to evaluate
the claim directly. However, we did create an up-
per bound reference by combining the walk scores
with easily computable simple surface statistics.
We trained a support vector classifier on the MSR
paraphrase training set with a feature space con-
sisting of the walk score under each distributional
similarity measure, the length of each text, the dif-
ference between those lengths, and the number of
unigram, bigram, trigram, and four-gram overlaps
between the two texts. The resulting classifier
achieved accuracy of 0.719 with F
1
: c
1
= 0.807
and F
1
: c
0
= 0.487 and Macro F
1
= 0.661. This
is a substantial improvement, roughly on the same
order of magnitude as from switching to the best
performing distributional similarity function.
Note that the running time of the Mihalcea et
al. algorithm for comparing texts T
1
and T
2
re-
quires |T
1
| ? |T
2
| individual similarity judgments.
By contrast, this work allows semantic profiles to
be constructed and evaluated for each text in a sin-
gle pass, independent of the number of terms in
the texts.
The performance of this unsupervised applica-
tion of walks to paraphrase recognition suggests
that the framework captures important intuitions
about similarity in text passages. In the next sec-
tion, we examine the performance of the measure
embedded in a larger system that seeks to make
fine-grained entailment judgments.
5.2 Textual entailment
The Recognizing Textual Entailment Challenge
(Dagan et al, 2005) is a task in which systems as-
sess whether a sentence is entailed by a short pas-
sage or sentence. Participants have used a variety
of strategies beyond lexical relatedness or overlap
for the task, but some have also used only rela-
tively simple similarity metrics. Many systems
Data Set Baseline Cosine Dice JS
RTE2 dev 66.00 66.75 65.75 66.25
RTE2 test 63.62 64.50 63.12 63.25
RTE3 dev 70.25 70.50 70.62 70.38
RTE3 test 65.44 65.82 65.44 65.44
Table 5: Accuracy when the random walk is
added as a feature of an existing RTE system
(left column) under various distance metrics (right
columns).
incorporate a number of these strategies, so we
experimented with using the random walk to im-
prove an existing RTE system. This addresses the
fact that using similarity alone to detect entailment
is impoverished: entailment is an asymmetric de-
cision while similarity is necessarily symmetric.
However, we also experiment with thresholding
random walk scores as a measure of entailment to
compare to other systems and provide a baseline
for whether the walk could be useful for entail-
ment detection.
We tested performance on the development and
test sets for the Second and Third PASCAL RTE
Challenges (Bar-Haim et al, 2006; Giampiccolo
et al, 2007). Each of these data sets contains 800
pairs of texts for which to determine entailment.
In some cases, no words from a passage appear
in WordNet, leading to an empty vector. In this
case, we use the Levenshtein string similarity mea-
sure between the two texts; this fallback is used in
fewer than five examples in any of our data sets
(Levenshtein, 1966).
Table 4 shows the results of using the simi-
larity measure alone to determine entailment; the
system?s ability to recognize entailment is above
chance on all data sets. Since the RTE data sets are
balanced, we used the median of the random walk
scores for each data set as the threshold rather than
using an absolute threshold. While the measure
does not outperform most RTE systems, it does
outperform some systems that used only lexical
overlap such as the Katrenko system from the sec-
ond challenge (Bar-Haim et al, 2006). These re-
sults show that the measure is somewhat sensitive
to the distance metric chosen, and that the best dis-
tance metric may vary by application.
To test the random walk?s value for improv-
ing an existing RTE system, we incorporated the
walk as a feature of the Stanford RTE system
(Chambers et al, 2007). This system computes
28
a weighted sum of a variety of features to make
an entailment decision. We added the random
walk score as one of these features and scaled it
to have a magnitude comparable to the other fea-
tures; other than scaling, there was no system-
specific engineering to add this feature.
As shown in Table 5, adding the random walk
feature improves the original RTE system. Thus,
the random walk score provides meaningful ev-
idence for detecting entailment that is not sub-
sumed by other information, even in a system with
several years of feature engineering and competi-
tive performance. In particular, this RTE system
contains features representing the alignment score
between two passages; this score is composed of a
combination of lexical relatedness scores between
words in each text. The ability of the random walk
to add value to the system even given this score,
which contains many common lexical relatedness
measures, suggests we are able to extract text sim-
ilarity information that is distinct from other mea-
sures. To put the gain we achieve in perspective,
an increase in the Stanford RTE system?s score of
the same magnitude would have moved the sys-
tem?s two challenge entries from 7th and 25th
to 6th and 17th, respectively, in the second RTE
Challenge. It is likely the gain from this feature
could be increased by closer integration with the
system and optimizing the initial distribution cre-
ation for this task.
By using the score as a feature, the system is
able to take advantage of properties of the score
distribution. While Table 4 shows performance
when a threshold is picked a priori, experiment-
ing with that threshold increases performance by
over two percent. By lowering the threshold (clas-
sifying more passages as entailments), we increase
recall of entailed pairs without losing as much pre-
cision in non-entailed pairs since many have very
low scores. As a feature, this aspect of the score
distribution can be incorporated by the system, but
it cannot be used in a simple thresholding design.
6 Discussion
The random walk framework smoothes an initial
distribution of words into a much larger lexical
space. In one sense, this is similar to the technique
of query expansion used in information retrieval.
A traditional query expansion model extends a bag
of words (usually a query) with additional related
words. In the case of pseudo-relevance feedback,
Figure 1: Impact of number of walk steps on cor-
relation with MSR paraphrase judgments. The
left column shows absolute correlation across ten
resampled runs (y-axis) versus number of steps
taken (x-axis). The right column plots the mean
ratio of performance at step t (x-axis) versus per-
formance at convergence.
29
these words come from the first documents re-
turned by the search engine, but other modes of se-
lecting additional words exist. In the random walk
framework, this expansion is analogous to taking
only a single step of the random walk. Indeed,
in the case of the translation model introduced in
(Berger and Lafferty, 1999), they are mathemati-
cally equivalent. However, we have argued that the
walk is an effective global aggregator of related-
ness information. We can formulate the question
as an empirical one?does simulating the walk un-
til convergence really improve our representation
of the text document?
To answer this question, we extracted a 200
items subset of the MSR training data and trun-
cated the walk at each time step up until our con-
vergence threshold was reached at around 50 it-
erations. We then evaluated the correlation of
the walk score with the correct label from the
MSR data for 10 random resamplings of 66 doc-
uments each. Figure 1 plots this result for dif-
ferent distributional similarity measures. We ob-
serve that as the number of steps increases, per-
formance under most of the distributional similar-
ity measures improves, with the exception of the
asymmetric skew-divergence measure introduced
in (Lee, 2001).
This plot also gives some insight into the qual-
itative nature of the stability of the various distri-
butional measures for the paraphrase task. For in-
stance, we observe that the Jensen-Shannon score
and dice score tend to be the most consistent be-
tween runs, but the dice score has a slightly higher
mean. This explains in part why the dice score was
the best performing measure for the task. In con-
trast, cosine similarity was observed to perform
poorly here, although it was found to be the best
measure when combined with our textual entail-
ment system. We believe this discrepancy is due
in part to the feature scaling issues described in
section 5.2.
7 Final remarks
Notions of similarity have many levels of gran-
ularity, from general metrics for lexical related-
ness to application-specific measures between text
passages. While lexical relatedness is well stud-
ied, it is not directly applicable to text passages
without some surrounding environment. Because
this work represents words and passages as in-
terchangeable mathematical objects (teleport vec-
tors), our approach holds promise as a general
framework for aggregating local relatedness infor-
mation between words into reliable measures be-
tween text passages.
The random walk framework can be used to
evaluate changes to lexical resources because it
covers the entire scope of a resource: the whole
graph is leveraged to construct the final distribu-
tion, so changes to any part of the graph are re-
flected in each walk. This means that the meaning-
fulness of changes in the graph can be evaluated
according to how they affect these text similarity
scores; this provides a more semantically relevant
evaluation of updates to a resource than, for ex-
ample, counting how many new words or links be-
tween words have been added. As shown in Jar-
masz and Szpakowicz (2003), an updated resource
may have many more links and concepts but still
have similar performance on applications as the
original. Evaluations of WordNet extensions, such
as those in Navigli and Velardi (2005) and Snow et
al. (2006), are easily conducted within the frame-
work of the random walk.
The presented framework for text semantic sim-
ilarity with random graph walks is more general
than the WordNet-based instantiation explored
here. Transition matrices from alternative linguis-
tic resources such as corpus co-occurrence statis-
tics or larger knowledge bases such as Wikipedia
may very well add value as a lexical resource un-
derlying the walk. One might also consider tailor-
ing the output of the walk with machine learning
techniques like those presented in (Minkov and
Cohen, 2007).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank
for word sense disambiguation. In EACL, Athens,
Greece.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
2nd PASCAL recognizing textual entailment chal-
lenge. In PASCAL Challenges Workshop on RTE.
A. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. SIGIR 1999, pages 222?
229.
P. Berkhin. 2005. A survey on pagerank computing.
Internet Mathematics, 2(1):73?120.
A. Budanitsky and G. Hirst. 2006. Evaluating
wordnet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1):13?47.
30
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon,
B. MacCartney, M. de Marneffe, D. Ramage, E. Yeh,
and C. D. Manning. 2007. Learning alignments and
leveraging natural logic. In ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
K. Collins-Thompson and J. Callan. 2005. Query ex-
pansion using random walk models. In CIKM ?05,
pages 704?711, New York, NY, USA. ACM Press.
C. Corley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In ACL Workshop on Em-
pirical Modeling of Semantic Equivalence and En-
tailment, pages 13?18, Ann Arbor, Michigan, June.
ACL.
J. R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
I. Dagan, O. Glickman, and B. Magnini. 2005.
The PASCAL recognizing textual entailment chal-
lenge. In Quinonero-Candela et al, editor, MLCW
2005, LNAI Volume 3944, pages 177?190. Springer-
Verlag.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Col-
ing 2004, pages 350?356, Geneva, Switzerland, Aug
23?Aug 27. COLING.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
2007. The 3rd PASCAL Recognizing Textual En-
tailment Challenge. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 1?9,
Prague, June.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In PASCAL
Challenges Workshop on RTE.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic
relatedness with random graph walks. In EMNLP-
CoNLL, pages 581?589.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In Proceedings of
RANLP-03, pages 212?219.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In ROCLING X, pages 19?33.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
L. Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. ACM SIGDOC: Pro-
ceedings of the 5th Annual International Conference
on Systems Documentation, 1986:24?26.
V. I. Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Ph.D. thesis, Soviet Physics Doklady.
C. Manning, P. Raghavan, and H. Schutze, 2008. In-
troduction to information retrieval, pages 258?263.
Cambridge University Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. AAAI 2006, 6.
E. Minkov and W. W. Cohen. 2007. Learning to rank
typed graph walks: Local and global approaches. In
WebKDD and SNA-KDD joint workshop 2007.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(03):207?223.
A. Montejo-R?aez, J.M. Perea, F. Mart??nez-Santiago,
M. A. Garc??a-Cumbreras, M. M. Valdivia, and
A. Ure?na L?opez. 2007. Combining lexical-syntactic
information with machine learning for recognizing
textual entailment. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 78?82,
Prague, June. ACL.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
P. Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. JAIR,
(11):95?130.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL, pages 801?808.
31
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 41?49,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
WikiWalk: Random walks on Wikipedia for Semantic Relatedness
Eric Yeh, Daniel Ramage,
Christopher D. Manning
Computer Science Department,
Stanford University
Stanford, CA, USA
{yeh1,dramage,manning}@cs.stanford.edu
Eneko Agirre, Aitor Soroa
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{e.agirre,a.soroa}@ehu.es
Abstract
Computing semantic relatedness of natural
language texts is a key component of tasks
such as information retrieval and sum-
marization, and often depends on knowl-
edge of a broad range of real-world con-
cepts and relationships. We address this
knowledge integration issue by comput-
ing semantic relatedness using person-
alized PageRank (random walks) on a
graph derived from Wikipedia. This pa-
per evaluates methods for building the
graph, including link selection strategies,
and two methods for representing input
texts as distributions over the graph nodes:
one based on a dictionary lookup, the
other based on Explicit Semantic Analy-
sis. We evaluate our techniques on stan-
dard word relatedness and text similarity
datasets, finding that they capture similar-
ity information complementary to existing
Wikipedia-based relatedness measures, re-
sulting in small improvements on a state-
of-the-art measure.
1 Introduction
Many problems in NLP call for numerical mea-
sures of semantic relatedness, including document
summarization, information retrieval, and textual
entailment. Often, measuring the relatedness of
words or text passages requires world knowledge
about entities and concepts that are beyond the
scope of any single word in the document. Con-
sider, for instance, the following pair:
1. Emancipation Proclamation
2. Gettysburg Address
To correctly assess that these examples are re-
lated requires knowledge of the United States Civil
War found neither in the examples themselves nor
in traditional lexical resources such as WordNet
(Fellbaum, 1998). Fortunately, a massive collabo-
ratively constructed knowledge resource is avail-
able that has specific articles dedicated to both.
Wikipedia is an online encyclopedia containing
around one million articles on a wide variety of
topics maintained by over one hundred thousand
volunteer editors with quality comparable to that
of traditional encyclopedias.
Recent work has shown that Wikipedia can be
used as the basis of successful measures of se-
mantic relatedness between words or text pas-
sages (Strube and Ponzetto, 2006; Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008). The
most successful measure, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007),
treats each article as its own dimension in a vec-
tor space. Texts are compared by first projecting
them into the space of Wikipedia articles and then
comparing the resulting vectors.
In addition to article text, Wikipedia stores a
great deal of information about the relationships
between the articles in the form of hyperlinks, info
boxes, and category pages. Despite a long his-
tory of research demonstrating the effectiveness
of incorporating link information into relatedness
measures based on the WordNet graph (Budanit-
sky and Hirst, 2006), previous work on Wikipedia
has made limited use of this relationship infor-
mation, using only category links (Bunescu and
Pasca, 2006) or just the actual links in a page
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008).
In this work, we combine previous approaches
by converting Wikipedia into a graph, mapping in-
put texts into the graph, and performing random
walks based on Personalized PageRank (Haveli-
wala, 2002) to obtain stationary distributions that
characterize each text. Semantic relatedness be-
tween two texts is computed by comparing their
distributions. In contrast to previous work, we
explore the use of all these link types when con-
41
structing the Wikipedia graph, the intuition being
these links, or some combination of them, con-
tain additional information that would allow a gain
over methods that use only just the article text. We
also discuss two methods for performing the initial
mapping of input texts to the graph, using tech-
niques from previous studies that utilized Word-
Net graphs and Wikipedia article text.
We find that performance is signficantly af-
fected by the strategy used to initialize the graph
walk, as well as the links selected when con-
structing the Wikipedia graph. Our best system
combines an ESA-initialized vector with random
walks, improving on state-of-the-art results over
the (Lee et al, 2005) dataset. An analysis of
the output demonstrates that, while the gains are
small, the random walk adds complementary re-
latedness information not present in the page text.
2 Preliminaries
A wide range of different methods, from corpus-
based distributional similarity methods, such as
Latent Semantic Analysis (Landauer et al, 1998),
to knowledge-based ones that employ structured
sources such as WordNet,
1
have been developed
to score semantic relatedness and similarity. We
now review two leading techniques which we use
as starting points for our approach: those that per-
form random walks over WordNet?s graph struc-
ture, and those that utilize Wikipedia as an under-
lying data source.
2.1 Random Graph Walks for Semantic
Relatedness
Some of the best performing WordNet-based al-
gorithms for computing semantic relatedness are
based on the popular Personalized PageRank al-
gorithm (Hughes and Ramage, 2007; Agirre and
Soroa, 2009). These approaches start by taking
WordNet as a graph of concepts G = (V,E) with
a set of vertices V derived from WordNet synsets
and a set of edges E representing relations be-
tween synsets. Both algorithms can be viewed
as random walk processes that postulate the ex-
istence of a particle that randomly traverses the
graph, but at any time may jump, or teleport, to
a new vertex with a given teleport probability. In
standard PageRank (Brin and Page, 1998), this tar-
get is chosen uniformly, whereas for Personalized
1
See (Budanitsky and Hirst, 2006) for a survey.
PageRank it is chosen from a nonuniform distribu-
tion of nodes, specified by a teleport vector.
The final weight of node i represents the propor-
tion of time the random particle spends visiting it
after a sufficiently long time, and corresponds to
that node?s structural importance in the graph. Be-
cause the resulting vector is the stationary distri-
bution of a Markov chain, it is unique for a par-
ticular walk formulation. As the teleport vector
is nonuniform, the stationary distribution will be
biased towards specific parts of the graph. In the
case of (Hughes and Ramage, 2007) and (Agirre
and Soroa, 2009), the teleport vector is used to re-
flect the input texts to be compared, by biasing the
stationary distribution towards the neighborhood
of each word?s mapping.
The computation of relatedness for a word pair
can be summarized in three steps: First, each input
word is mapped with to its respective synsets in
the graph, creating its teleport vector. In the case
words with multiple synsets (senses), the synsets
are weighted uniformly. Personalized PageRank is
then executed to compute the stationary distribu-
tion for each word, using their respective teleport
vectors. Finally, the stationary distributions for
each word pair are scored with a measure of vector
similarity, such as cosine similarity. The method
to compute relatedness for text pairs is analogous,
with the only difference being in the first step all
words are considered, and thus the stationary dis-
tribution is biased towards all synsets of the words
in the text.
2.2 Wikipedia as a Semantic Resource
Recent Wikipedia-based lexical semantic related-
ness approaches have been found to outperform
measures based on the WordNet graph. Two such
methods stand out: Wikipedia Link-based Mea-
sure (WLM) (Milne and Witten, 2008), and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007).
WLM uses the anchors found in the body of
Wikipedia articles, treating them as links to other
articles. Each article is represented by a list of
its incoming and outgoing links. For word relat-
edness, the set of articles are first identified by
matching the word to the text in the anchors, and
the score is derived using several weighting strate-
gies applied to the overlap score of the articles?
links. WLM does not make further use of the link
graph, nor does it attempt to differentiate the links.
42
In contrast to WLM, Explicit Semantic Analy-
sis (ESA) is a vector space comparison algorithm
that does not use the link structure, relying solely
on the Wikipedia article text. Unlike Latent Se-
mantic Analysis (LSA), the underlying concept
space is not computationally derived, but is instead
based on Wikipedia articles. For a candidate text,
each dimension in its ESA vector corresponds to
a Wikipedia article, with the score being the sim-
ilarity of the text with the article text, subject to
TF-IDF weighting. The relatedness of two texts
is computed as the cosine similarity of their ESA
vectors.
Although ESA reports the best results to date
on both the WordSim-353 dataset as well as the
Lee sentence similarity dataset, it does not utilize
the link structure, which motivated a combined ap-
proach as follows.
2.3 A Combined Approach
In this work, we base our random walk algorithms
after the ones described in (Hughes and Ramage,
2007) and (Agirre et al, 2009), but use Wikipedia-
based methods to construct the graph. As in previ-
ous studies, we obtain a relatedness score between
a pair of texts by performing random walks over
a graph to compute a stationary distribution for
each text. For our evaluations, the score is simply
the cosine similarity between the distributions. In
the following sections, we describe how we built
graphs from Wikipedia, and how input texts are
initially mapped into these structures.
3 Building a Wikipedia Graph
In order to obtain the graph structure of Wikipedia,
we simply treat the articles as vertices, and
the links between articles as the edges. There
are several sources of pre-processed Wikipedia
dumps which could be used to extract the arti-
cles and links between articles, including DBpe-
dia (Auer et al, 2008), which provides a rela-
tional database representation of Wikipedia, and
Wikipedia-Miner
2
, which produces similar infor-
mation from Wikipedia dumps directly. In this
work we used a combination of Wikipedia-Miner
and custom processing scripts. The dump used in
this work is from mid 2008.
As in (Milne and Witten, 2008), anchors in
Wikipedia articles are used to define links between
2
http://wikipedia-miner.sourceforge.net
articles. Because of different distributional proper-
ties, we explicitly distinguish three types of links,
in order to explore their impact on the graph walk.
Infobox links are anchors found in the infobox
section of Wikipedia articles. Article in-
foboxes, when present, often enumerate
defining attributes and characteristics for that
article?s topic.
Categorical links reference articles whose titles
belong in the Wiki namespace ?Category,?
as well as those with titles beginning with
?List of.? These pages are often just lists of
anchors to other articles, which may be use-
ful for capturing categorical information that
roughly contains a mixture of hyponymy and
meronymy relations between articles.
Content links are those that are not already clas-
sified as infobox nor categorical, and are in-
tended to represent the set of miscellaneous
anchors found solely in the article body.
These may include links already found in the
categorical and infobox categories.
Links can be further factored out according to
generality, a concept introduced in (Gabrilovich
and Markovitch, 2009). We say that one article
is more general than another when the number of
inlinks is larger. Although only a rough heuris-
tic, the intuition is that articles on general top-
ics will receive many links, whereas specific ar-
ticles will receive fewer. We will use +k notation
for links which point to more general articles, i.e.,
where the difference in generality between source
s and target t is #inlink(t)/#inlink(s) ? k.
We will use ?k for links to less general articles,
i.e., #inlink(s)/#inlink(t) ? k. Finally we
use =k when the generality is in the same order
of magnitude, i.e., when the link is neither +k
nor ?k. The original notion of generality from
(Gabrilovich and Markovitch, 2009) restricts con-
sideration to only more general articles by one or-
der of magnitude (+10), without reference to the
link types introduced above.
Given the size of the Wikipedia graph, we ex-
plored further methods inspired by (Gabrilovich
and Markovitch, 2009) to make the graph smaller.
We discarded articles with fewer than 2,000 non-
stop words and articles with fewer than 5 outgoing
and incoming links. We will refer to the complete
43
graph as full and to this reduced graph as reduced.
3
4 Initializing a Wikipedia Graph Walk
In order to apply Personalized PageRank to a
given passage of text or word, we need to con-
struct a custom teleport vector, representing the
initial distribution of mass over the article nodes.
In this section we introduce two such methods,
one based on constructing a direct mapping from
individual words to Wikipedia articles (which we
call dictionary-based initialization), and the other
based directly on the results of ESA. We will see
each technique in turn.
4.1 Dictionary based initialization
Given a target word, we would like to define
its teleport vector using the set of articles in
Wikipedia to which the word refers. This is analo-
gous to a dictionary, where an entry lists the set of
meanings pertaining to the entry.
We explored several methods for building such
a dictionary. The first method constructed the dic-
tionary using the article title directly, while also
including redirection pages and disambiguation
pages for additional ways to refer to the article. In
addition, we can use the anchor text to refer to arti-
cles, and we turned to Wikipedia-Miner to extract
this information. Anchors are indeed a rich source
of information, as they help to relate similar words
to Wikipedia articles. For instance, links to page
Monk are created by using textual anchors such as
lama, brothers, monastery, etc. As a result, the
dictionary entries for those words will have a link
to the Monk page. This information turned out to
be very valuable, so all experiments have been car-
ried out using anchors.
An additional difficulty was that any of these
methods yielded dictionaries where the entries
could refer to tens, even hundreds of articles. In
most of the cases we could see that relevant arti-
cles were followed by a long tail of loosely related
articles. We tried two methods to prune the dic-
tionary. The first, coarse, method was to eliminate
all articles whose title contains a space. The mo-
tivation was that our lexical semantic relatedness
datasets (cf. Section 5) do not contain multiword
entries (e.g., United States). In the second method,
we pruned articles from the dictionary which ac-
3
In order to keep category and infobox links, the 2,000
non-stop word filter was not applied to categories and lists of
pages.
Graphs
Graph # Vertices # Edges
Full 2,483,041 49,602,752
Reduced 1,002,411 30,939,288
Dictionaries
Dictionary # Entries Avg. Articles
all 6,660,315 1.31
1% 6,660,306 1.12
1% noent 1,058,471 1.04
Table 1: Graph and dictionary sizes. Avg. Articles
column details the average number of articles per
entry.
counted for less than 1% or 10% of the occur-
rences of that anchor word, as suggested by (Milne
and Witten, 2008).
In short, for this method of initialization, we ex-
plored the use of the following variants: all, all ar-
ticles are introduced in the dictionary; noent, arti-
cles with space characters are omitted; 1% (10%),
anchors that account for less than 1% (10%) of the
total number of anchors for that entry are omitted.
We did not use stemming. If a target word has no
matching Wikipedia article in the dictionary, then
it is ignored.
Table 1 shows the numbers for some graph and
dictionary versions. Although the average number
of articles per entry in the dictionary might seem
low, it is actually quite high for the words in the
datasets: for MC it?s 5.92, and for wordsim353 it?s
42.14. If we keep the articles accounting for 10%
of all occurrences, the numbers drops drastically
to 1.85 and 1.64 respectively.
As we will see in the results section, smaller
graphs and dictionaries are able to attain higher
results, but at the cost of losing information for
some words. That is, we observed that some fac-
tored, smaller graphs contained less noise, but that
meant that some articles and words are isolated in
the graph, and therefore we are not able to com-
pute relatedness for them. As a solution, we de-
vised an alternative way to initialize the random
walk. Instead of initializing it according to the ar-
ticles in the dictionary, we initialized it with the
vector weights returned by ESA, as explained in
the next section.
44
4.2 Initialization with ESA
In addition to the dictionary based approach, we
also explored the use of ESA to construct the tele-
port vector. In contrast to dictionary initialization,
ESA uses the text of the article body instead of an-
chor text or the article titles. Because ESA maps
query text to a weighted vector of Wikipedia arti-
cles, it can be naturally adapted as a teleport vector
for a random walk with a simple L
1
normaliza-
tion. We used Apache Lucene
4
to implement both
ESA?s repository of Wikipedia articles, and to re-
turn vectors for queries. Each article is indexed as
its own document, with page text preprocessed to
strip out Wiki markup.
Although we followed the steps outlined in
(Gabrilovich and Markovitch, 2007), we had to
add an extension to the algorithm: for a return
vector from ESA, we order the articles by score,
and retain only the scores for the top-n articles,
setting the scores of the remaining articles to 0.
Without this modification, our performance results
were will below the reported numbers, but with a
cutoff at 625 (determined by a basic grid search),
we obtained a correlation of 0.76 on the Lee sen-
tence similarity dataset, over the previously pub-
lished score of 0.72.
4.3 Teleport Probability
For this work, we used a value of 0.15 as the prob-
ability of returning to the teleport distribution at
any given step. The walk terminates when the vec-
tor converges with an L
1
error of 0.0001 (circa 30
iterations). Some preliminary experiments on a re-
lated Word Sense Disambiguation task indicated
that in this context, our algorithm is quite robust to
these values, and we did not optimize them. How-
ever, we will discuss using different return param-
eters in Section 6.1.
5 Experiments
In this section, we compare the two methods of
initialization as well as several types of edges. For
a set of pairs, system performance is evaluated by
how well the generated scores correlate with the
gold scores. Gold scores for each pair are the av-
erage of human judgments for that pair. In order to
compare against previous results obtained on the
datasets, we use the Spearman correlation coeffi-
cient on the Miller Charles (MC) and WordSim-
353 word-pair datasets, and the Pearson correla-
4
http://lucene.apache.org
Dictionary Graph MC
all full 0.369
1% full 0.610
1%, noent full 0.565 (0.824)
1% reduced 0.563
1% reduced +2 0.530
1% reduced +4 0.601
1% reduced +8 0.512
1% reduced +10 0.491 (0.522)
10% full 0.604 (0.750)
10% reduced 0.605 (0.751)
10% reduced +2 0.491 (0.540)
10% reduced +4 0.476 (0.519)
10% reduced +8 0.474 (0.506)
10% reduced +10 0.430 (0.484)
WordNet 0.90 / 0.89
WLM 0.70
ESA 0.72
Table 2: Spearman correlation on the MC dataset
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods. Between parenthesis, results
excluding pairs which had a word with an empty
dictionary entry.
tion coefficient on the (Lee et al, 2005) document-
pair dataset.
5.1 Dictionary-based Initialization
Given the smaller size of the MC dataset, we
explored the effect of the different variants to
build the graph and dictionary on this dataset.
Some selected results are shown in Table 2, along-
side those of related work, where we used Word-
Net for (Hughes and Ramage, 2007) and (Agirre
et al, 2009) (separated by ?/? in the results),
WLM for (Milne and Witten, 2008) and ESA for
(Gabrilovich and Markovitch, 2007).
We can observe that using the full graph and
dictionaries yields very low results. Reducing the
dictionary (removing articles with less than 1% or
10% of the total occurrences) produces higher re-
sults, but reducing the graph does not provide any
improvement. On a closer look, we realized that
pruning the dictionary to 10% or removing multi-
words (noent) caused some words to not get any
link to articles (e.g., magician). If we evaluate
only over pairs where both words get a Personal-
ized PageRank vector, the results raise up to 0.751
and 0.824, respectively, placing our method close
45
Dictionary Graph WordSim-353
1% full 0.449
1%, noent full 0.440 (0.634)
1% reduced 0.485
WordNet 0.55 / 0.66
WLM 0.69
ESA 0.75
WikiRelate 0.50
Table 3: Spearman correlation on the WordSim-
353 dataset with dictionary-based initialization.
Refer to Section 3 for explanation of dictionary
and graph building methods. Between parenthe-
sis, results excluding pairs which had a word with
an empty dictionary entry.
Dictionary Graph (Lee et al, 2005)
1%, noent Full 0.308
1% Reduced +4 0.269
ESA 0.72
Table 4: Pearson correlation on (Lee et al, 2005)
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods.
to the best results on the MC dataset. This came
at the cost of not being able to judge the related-
ness of 3 and 5 pairs, respectively. We think that
removing multiwords (noent) is probably too dras-
tic, but the positive effect is congruent with (Milne
and Witten, 2008), who suggested that the cover-
age of certain words in Wikipedia is not adequate.
The results in Table 3 show the Spearman cor-
relation for some selected runs over the WordSim-
353 dataset. Again we see that a restrictive dic-
tionary allows for better results on the pairs which
do get a dictionary entry, up to 0.63. WikiRelate
refers to the results in (Strube and Ponzetto, 2006).
We only tested a few combinations over (Lee et
al., 2005), with results given in Table 4. These are
well below state-of-the-art, and show that initial-
izing the random walk with all words in the doc-
ument does not characterize the documents well,
resulting in low correlation.
5.2 ESA-based initialization
While the results using a dictionary based ap-
proach were encouraging, they did not come close
to the state-of-the-art results achieved by ESA.
Here, we explore combining ESA and random
Method Text Sim
ESA@625 0.766
ESA@625+Walk All 0.556
ESA@625+Walk Categories 0.410
ESA@625+Walk Content 0.536
ESA@625+Walk Infobox 0.710
Table 5: Pearson correlation on the (Lee et al,
2005) dataset when walking on various types of
links. Note that walking tends to hurt performance
overall, with Infobox links by far the least harm-
ful.
walks, by using ESA to initialize the teleport vec-
tor. Following section 4.2, we used a top-n cutoff
of 625.
Table 5 displays the results of our ESA im-
plementation followed by a walk from that ESA
distribution. Walking on any link type actually
depresses performance below the baseline ESA
value, although the Infobox links seem the least
harmful.
However, as mentioned in Section 3, links be-
tween articles represent many different types of
relationships beyond the few well-defined links
present in lexical resources like WordNet. This
also extends to where the link is found, and the ar-
ticle it is pointing to. As such, not all links are cre-
ated equal, and we expect that some types of links
at different levels of generality will perform bet-
ter or worse than others. Table 6 presents a sam-
ple grid search across the category links choosing
more general, less general, or similar generality at
several factors of k, showing that there is a consis-
tent pattern across multiple link types. Note that
the best value indeed improves upon the score of
the ESA distribution, albeit modestly.
We performed a similar analysis across all link
types and found that the best link types were Cat-
egory links at +6 and Infobox links at =2. Intu-
itively, these link types make sense: for seman-
tic relatedness, it seem reasonable to expect more
general pages within the same category to help.
And for Infobox links, much rarer and much more
common pages can both introduce their own kind
of noise. While the improvement from each type
of edge walk is small, they are additive?the best
results on the sentence similarity dataset was from
walking across both link types. Our final Pearson
correlation coefficient of .772 is to our knowledge
the highest number reported in the literature, al-
46
Generality of Category links
+k -k =k
k = 2 0.760 0.685 0.462
k = 4 0.766 0.699 0.356
k = 6 0.771 0.729 0.334
k = 8 0.768 0.729 0.352
k = 10 0.768 0.720 0.352
Table 6: Pearson correlation on the (Lee et al,
2005) with random walks over only a subset of
the edges in the Category link information (scores
.410 when taking all edges). Note that factoring
the graph by link generality can be very helpful to
the walk.
Method Text Sim
ESA@625 0.766
ESA@625+Walk Cat@+6 0.770
ESA@625+Walk Cat@+6 Inf@=2 0.772
Bag of words (Lee et al, 2005) 0.5
LDA (Lee et al, 2005) 0.60
ESA* 0.72
Table 7: Pearson correlation on the (Lee et al,
2005) dataset for our best sytems compared to pre-
viously reported numbers. ESA* is the score for
raw ESA as reported number in (Gabrilovich and
Markovitch, 2007).
beit only a small improvement over our ESA@625
score.
Despite the results obtained for text similarity,
the best settings found for the Lee dataset did not
translate to consistent improvements over the ESA
baseline for Spearman rank correlation on the lex-
ical similarity datasets. While our scores on the
MC dataset of 30 word pairs did improve with the
walk in roughly the same way as in Lee, no such
improvements were found on the larger WordSim-
353 data. On WordSim-353, our implementa-
tion of ESA scored 0.709 (versus Gabrilovich?s
reported ESA score of 0.75), and our walk on
Cat@+6 showing no gain or loss. In contrast to
the text similarity dataset, Infobox links were no
longer helpful, bringing the correlation down to
.699. We believe this is because Infobox links
helped the most with entities, which are very rare
in the WordSim-353 data, but are more common
in the Lee dataset.
6 Discussion
Our results suggest that even with a simple
dictionary-based approach, the graph of Wikipedia
links can act as an effective resource for comput-
ing semantic relatedness. However, the dictio-
nary approach alone was unable to reach the re-
sults of state-of-the-art models using Wikipedia
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008) or using the same technique on
WordNet (Hughes and Ramage, 2007; Agirre
et al, 2009). Thus, it seems that the text of
Wikipedia provides a stronger signal than the link
structure. However, a pruned dictionary can im-
prove the results of the dictionary based initial-
ization, which indicates that some links are in-
formative for semantic relatedness while others
are not. The careful pruning, disambiguation and
weighting functions presented in (Milne and Wit-
ten, 2008) are directions for future work.
The use of WordNet as a graph provided ex-
cellent results (Hughes and Ramage, 2007), close
to those of ESA. In contrast with our dictionary-
based initialization on Wikipedia, no pruning of
dictionary or graph seem necessary to obtain high
results with WordNet. One straightforward expla-
nation is that Wikipedia is a noisy source of link
information. In fact, both ESA and (Milne and
Witten, 2008) use ad-hoc pruning strategies in or-
der to obtain good results.
6.1 ESA and Walk Comparison
By using ESA to generate the teleport distribu-
tion, we were able to introduce small gains us-
ing the random walk. Because these gains were
small, it is plausible that the walk introduces only
modest changes from the initial ESA teleport dis-
tributions. To evaluate this, we examined the dif-
ferences between the vector returned by ESA and
distribution over the equivalent nodes in the graph
after performing a random walk starting with that
ESA vector.
For this analysis, we took all of the text entries
used in this study, and generated two distributions
over the Wikipedia graph, one using ESA@625,
the other the result of performing a random walk
starting at ESA@625. We generated a list of the
concept nodes for both distributions, sorted in de-
creasing order by their associated scores. Start-
ing from the beginning of both lists, we then
counted the number of matched nodes until they
disagreed on ordering, giving a simple view of
47
Walk Type Avg Std Max
MC Cat@+6 12.1 7.73 35
Cat@+6 Inf@=2 5.39 5.81 20
WordSim Cat@+6 12.0 10.6 70
Cat@+6 Inf@=2 5.74 7.78 54
Lee Cat@+6 28.3 89.7 625
Cat@+6 Inf@=2 4.24 14.8 103
Table 8: Statistics for first concept match length,
by run and walk type.
how the walk perturbed the strongest factors in the
graphs. We performed this for both the best per-
forming walk models (ESA@625+Walk Cat@+6
and ESA@625+Walk Cat@+6 Inf@=2) against
ESA@625. Results are given in Table 8.
As expected, adding edges to the random walk
increases the amount of change from the graph,
as initialized by ESA. A cursory examination of
the distributions also revealed a number of outliers
with extremely high match lengths: these were
likely due to the fact that the selected edge types
were already extremely specialized. Thus for a
number of concept nodes, it is likely they did not
have any outbound edges at all.
Having established that the random walk does
indeed have an impact on the ESA vectors, the
next question is if changes via graph walk are
consistently helpful. To answer this, we com-
pared the performance of the walk on the (Lee et
al., 2005) dataset for probabilities at selected val-
ues, using the best link pruned Wikipedia graph
(ESA@625+Walk Cat@+6 Inf@=2), and using all
of the available edges in the graph for compari-
son. Here, a lower probability means the distribu-
tion spreads out further into the graph, compared
to higher values, where the distribution varies only
slightly from the ESA vector. Results are given in
Table 9. Performance for the pruned graph im-
proves as the return probability decreases, with
larger changes introduced by the graph walk re-
sulting in better scores, whereas using all available
links decreases performance. This reinforces the
notion that Wikipedia links are indeed noisy, but
that within a selected edge subset, making use of
all information via the random walk indeed results
in gains.
7 Conclusion
This paper has demonstrated that performing ran-
dom walks with Personalized PageRank over the
Prob Corr (Pruned) Corr (All)
0.01 0.772 0.246
0.10 0.773 0.500
0.15 0.772 0.556
0.30 0.771 0.682
0.45 0.769 0.737
0.60 0.767 0.758
0.90 0.766 0.766
0.99 0.766 0.766
Table 9: Return probability vs. correlation, on tex-
tual similarity data (Lee et al, 2005).
Wikipedia graph is a feasible and potentially fruit-
ful means of computing semantic relatedness for
words and texts. We have explored two methods of
initializing the teleport vector: a dictionary-based
method and a method based on ESA, the cur-
rent state-of-the-art technique. Our results show
the importance of pruning the dictionary, and for
Wikipedia link structure, the importance of both
categorizing by anchor type and comparative gen-
erality. We report small improvements over the
state-of-the-art on (Lee et al, 2005) using ESA as
a teleport vector and a limited set of links from
Wikipedia category pages and infoboxes.
In future work, we plan to explore new ways
to construct nodes, edges, and dictionary entries
when constructing the Wikipedia graph and dic-
tionary. We believe that finer grained methods of
graph construction promise to improve the value
of the Wikipedia link structure. We also plan to
further investigate the differences between Word-
Net and Wikipedia and how these may be com-
bined, from the perspective of graph and random
walk techniques. A public distribution of software
used for these experiments will also be made avail-
able.
5
Acknowledgements
The authors would like to thank Michael D. Lee
and Brandon Pincombe for access to their textual
similarity dataset, and the reviewers for their help-
ful comments. Eneko Agirre performed part of
the work while visiting Stanford, thanks to a grant
from the Science Ministry of Spain.
5
Please see http://nlp.stanford.edu/
software and http://ixa2.si.ehu.es/ukb
48
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings
of 14th Conference of the European Chapter of the
Association for Computational Linguistics, Athens,
Greece.
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasc?a, and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
Boulder, USA.
S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak,
and Z Ives. 2008. Dbpedia: A nucleus for a web
of open data. In Proceedings of 6th International
Semantic Web Conference, 2nd Asian Semantic Web
Conference (ISWC+ASWC 2007), pages 722?735.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
R. C. Bunescu and M. Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
In Proceedings of 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07).
E. Gabrilovich and S. Markovitch. 2009. Wikipedia-
based semantic interpretation. Journal of Artificial
Intelligence Research, 34:443?498.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings
of EMNLP-CoNLL, pages 581?589.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
M. D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document sim-
ilarity. In Proceedings of the 27th Annual Confer-
ence of the Cognitive Science Society, pages 1254?
1259, Mahwah, NJ. Erlbaum.
D. Milne and I.H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proceedings of the first AAAI
Workshop on Wikipedia and Artifical Intellegence
(WIKIAI?08), Chicago, I.L.
M. Strube and S.P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using Wikipedia. In
Proceedings of the 21st National Conference on Ar-
tificial Intelligence, pages 1419?1424.
49
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124?132,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Study of Academic Collaboration in Computational Linguistics with
Latent Mixtures of Authors
Nikhil Johri, Daniel Ramage
Department of Computer Science
Stanford University
Stanford, CA, USA
Daniel A. McFarland
School of Education
Stanford University
Stanford, CA, USA
{njohri2,dramage,dmcfarla,jurafsky}@stanford.edu
Daniel Jurafsky
Department of Linguistics
Stanford University
Stanford, CA, USA
Abstract
Academic collaboration has often been at
the forefront of scientific progress, whether
amongst prominent established researchers, or
between students and advisors. We suggest a
theory of the different types of academic col-
laboration, and use topic models to computa-
tionally identify these in Computational Lin-
guistics literature. A set of author-specific
topics are learnt over the ACL corpus, which
ranges from 1965 to 2009. The models are
trained on a per year basis, whereby only pa-
pers published up until a given year are used
to learn that year?s author topics. To determine
the collaborative properties of papers, we use,
as a metric, a function of the cosine similarity
score between a paper?s term vector and each
author?s topic signature in the year preceding
the paper?s publication. We apply this metric
to examine questions on the nature of collabo-
rations in Computational Linguistics research,
finding that significant variations exist in the
way people collaborate within different sub-
fields.
1 Introduction
Academic collaboration is on the rise as single au-
thored work becomes less common across the sci-
ences (Rawlings and McFarland, 2011; Jones et al,
2008; Newman, 2001). In part, this rise can be at-
tributed to the increasing specialization of individual
academics and the broadening in scope of the prob-
lems they tackle. But there are other advantages to
collaboration, as well: they can speed up produc-
tion, diffuse knowledge across authors, help train
new scientists, and are thought to encourage greater
innovation. Moreover, they can integrate scholarly
communities and foster knowledge transfer between
related fields. But all collaborations aren?t the same:
different collaborators contribute different material,
assume different roles, and experience the collabo-
ration in different ways. In this paper, we present
a new frame for thinking about the variation in col-
laboration types and develop a computational metric
to characterize the distinct contributions and roles of
each collaborator within the scholarly material they
produce.
The topic of understanding collaborations has at-
tracted much interest in the social sciences over the
years. Recently, it has gained traction in computer
science, too, in the form of social network analysis.
Much work focuses on studying networks formed
via citations (Radev et al, 2009; White and Mccain,
1998), as well as co-authorship links (Nascimento
et al, 2003; Liu et al, 2005). However, these works
focus largely on the graphical structure derived from
paper citations and author co-occurrences, and less
on the textual content of the papers themselves. In
this work, we examine the nature of academic col-
laboration using text as a primary component.
We propose a theoretical framework for determin-
ing the types of collaboration present in a docu-
ment, based on factors such as the number of es-
tablished authors, the presence of unestablished au-
thors and the similarity of the established authors?
past work to the document?s term vector. These col-
laboration types attempt to describe the nature of co-
authorships between students and advisors (e.g. ?ap-
prentice? versus ?new blood?) as well as those solely
between established authors in the field. We present
a decision diagram for classifying papers into these
types, as well as a description of the intuition behind
each collaboration class.
124
We explore our theory with a computational
method to categorize collaborative works into their
collaboration types using an approach based on topic
modeling, where we model every paper as a la-
tent mixture of its authors. For our system, we use
Labeled-LDA (LLDA (Ramage et al, 2009)) to train
models over the ACL corpus for every year of the
words best attributed to each author in all the papers
they write. We use the resulting author signatures
as a basis for several metrics that can classify each
document by its collaboration type.
We qualitatively analyze our results by examin-
ing the categorization of several high impact papers.
With consultation from prominent researchers and
textbook writers in the field, we demonstrate that our
system is able to differentiate between the various
types of collaborations in our suggested taxonomy,
based only on words used, at low but statistically
significant accuracy. We use this same similarity
score to analyze the ACL community by sub-field,
finding significant deviations.
2 Related Work
In recent years, popular topic models such as La-
tent Dirichlet Allocation (Blei et al, 2003) have
been increasingly used to study the history of sci-
ence by observing the changing trends in term based
topics (Hall et al, 2008), (Gerrish and Blei, 2010).
In the case of Hall et al, regular LDA topic mod-
els were trained over the ACL anthology on a per
year basis, and the changing trends in topics were
studied from year to year. Gerrish and Blei?s work
computed a measure of influence by using Dynamic
Topic Models (Blei and Lafferty, 2006) and study-
ing the change of statistics of the language used in a
corpus.
These models propose interesting ideas for utiliz-
ing topic modeling to understand aspects of scien-
tific history. However, our primary interest, in this
paper, is the study of academic collaboration be-
tween different authors; we therefore look to learn
models for authors instead of only documents. Pop-
ular topic models for authors include the Author-
Topic Model (Rosen-Zvi et al, 2004), a simple
extension of regular LDA that adds an additional
author variable over the topics. The Author-Topic
Model learns a distribution over words for each
topic, as in regular LDA, as well as a distribution
over topics for each author. Alternatively, Labeled
LDA (Ramage et al, 2009), another LDA variation,
offers us the ability to directly model authors as top-
ics by considering them to be the topic labels for the
documents they author.
In this work, we use Labeled LDA to directly
model probabilistic term ?signatures? for authors. As
in (Hall et al, 2008) and (Gerrish and Blei, 2010),
we learn a new topic model for each year in the cor-
pus, allowing us to account for changing author in-
terests over time.
3 Computational Methodology
The experiments and results discussed in this paper
are based on a variation of the LDA topic model run
over data from the ACL corpus.
3.1 Dataset
We use the ACL anthology from years 1965 to 2009,
training over 12,908 papers authored by over 11,355
unique authors. We train our per year topic mod-
els over the entire dataset; however, when evaluating
our results, we are only concerned with papers that
were authored by multiple individuals as the other
papers are not collaborations.
3.2 Latent Mixture of Authors
Every abstract in our dataset reflects the work, to
some greater or lesser degree, of all the authors of
that work. We model these degrees explicitly us-
ing a latent mixture of authors model, which takes
its inspiration from the learning machinery of LDA
(Blei et al, 2003) and its supervised variant La-
beled LDA (Ramage et al, 2009). These models
assume that documents are as a mixture of ?topics,?
which themselves are probability distributions over
the words in the vocabulary of the corpus. LDA
is completely unsupervised, assuming that a latent
topic layer exists and that each word is generated
from one underlying topic from this set of latent top-
ics. For our purposes, we use a variation of LDA in
which we assume each document to be a latent mix-
ture of its authors. Unlike LDA, where each docu-
ment draws a multinomial over all topics, the latent
mixture of authors model we use restricts a docu-
ment to only sample from topics corresponding to
125
its authors. Also, unlike models such as the Author-
Topic Model (Rosen-Zvi et al, 2004), where au-
thors are modeled as distributions over latent top-
ics, our model associates each author to exactly one
topic, modeling authors directly as distributions over
words.
Like other topic models, we will assume a genera-
tive process for our collection of D documents from
a vocabulary of size V . We assume that each docu-
ment d has Nd terms and Md authors from a set of
authors A. Each author is described by a multino-
mial distribution ?a over words V , which is initially
unobserved. We will recover for each document a
hidden multinomial ?(d) of length Md that describes
which mixture of authors? best describes the doc-
ument. This multinomial is in turn drawn from a
symmetric Dirichlet distribution with parameter ?
restrict to the set of authors ?(d) for that paper. Each
document?s words are generated by first picking an
author zi from ?(d) and then drawing a word from
the corresponding author?s word distribution. For-
mally, the generative process is as follows:
? For each author a, generate a distribution ?a over
the vocabulary from a Dirichlet prior ?
? For each document d, generate a multinomial mix-
ture distribution ?(d) ? Dir(?.1?(d))
? For each document d,
? For each i ? {1, ..., Nd}
? Generate zi ? {?
(d)
1 , ..., ?
(d)
Md
} ?
Mult(?(d))
? Generate wi ? {1, ..., V } ?Mult(?zi)
We use Gibbs sampling to perform inference in
this model. If we consider our authors as a label
space, this model is equivalent to that of Labeled
LDA (Ramage et al, 2009), which we use for in-
ference in our model, using the variational objec-
tive in the open source implementation1. After in-
ference, our model discovers the distribution over
terms that best describes that author?s work in the
presence of other authors. This distribution serves
as a ?signature? for an author and is dominated by
the terms that author uses frequently across collabo-
rations. It is worth noting that this model constrains
the learned ?topics? to authors, ensuring directly in-
terpretable results that do not require the interpreta-
1http://nlp.stanford.edu/software/tmt/
tion of a latent topic space, such as in (Rosen-Zvi et
al., 2004).
To imbue our model with a notion of time, we
train a separate LLDA model for each year in the
corpus, training on only those papers written before
and during the given year. Thus, we have separate
?signatures? for each author for each year, and each
signature only contains information for the specific
author?s work up to and including the given year.
Table 1 contains examples of such term signatures
computed for two authors in different years. The top
terms and their fractional counts are displayed.
4 Studying Collaborations
There are several ways one can envision to differen-
tiate between types of academic collaborations. We
focus on three factors when creating collaboration
labels, namely:
? Presence of unestablished authors
? Similarity to established authors
? Number of established authors
If an author whom we know little about is present
on a collaborative paper, we consider him or her to
be a new author. We threshold new authors by the
number of papers they have written up to the pub-
lication year of the paper we are observing. De-
pending on whether this number is below or above a
threshold value, we consider an author to be estab-
lished or unestablished in the given year.
Similarity scores are measured using the trained
LLDA models described in Section 3.2. For any
given paper, we measure the similarity of the pa-
per to one of its (established) authors by calculating
the cosine similarity of the author?s signature in the
year preceding the paper?s publication to the paper?s
term-vector.
Using the aforementioned three factors, we define
the following types of collaborations:
? Apprenticeship Papers are authored by one or
more established authors and one or more un-
established authors, such that the similarity of
the paper to more than half of the established
authors is high. In this case, we say that the
new author (or authors) was an apprentice of
126
Philipp Koehn, 2002 Philipp Koehn, 2009 Fernando Pereira, 1985 Fernando Pereira, 2009
Terms Counts Terms Counts Terms Counts Terms Counts
word 3.00 translation 69.78 grammar 14.99 type 40.00
lexicon 2.00 machine 34.67 phrase 10.00 phrase 30.89
noun 2.00 phrase 26.85 structure 7.00 free 23.14
similar 2.00 english 23.86 types 6.00 grammar 23.10
translation 1.29 statistical 19.51 formalisms 5.97 constraint 23.00
purely 0.90 systems 18.32 sharing 5.00 logical 22.41
accuracy 0.90 word 16.38 unification 4.97 rules 21.72
Table 1: Example term ?signatures? computed by running a Labeled LDA model over authors in the ACL corpus on a
per year basis: top terms for two authors in different years are shown alongside their fractional counts.
the established authors, continuing in their line
of work.
? New Blood Papers are authored by one estab-
lished author and one or more unestablished au-
thors, such that the similarity of the paper to the
established author is low. In this case, we say
that the new author (or authors) provided new
ideas or worked in an area that was dissimilar to
that which the established author was working
in.
? Synergistic Papers are authored only by es-
tablished authors such that it does not heavily
resemble any authors? previous work. In this
case, we consider the paper to be a product of
synergy of its authors.
? Catalyst Papers are similar to synergistic
ones, with the exception that unestablished au-
thors are also present on a Catalyst Paper. In
this case, we hypothesize that the unestablished
authors were the catalysts responsible for get-
ting the established authors to work on a topic
dissimilar to their previous work.
The decision diagram in Figure 1 presents an easy
way to determine the collaboration type assigned to
a paper.
5 Quantifying Collaborations
Following the decision diagram presented in Figure
1 and using similarity scores based on the values
returned by our latent author mixture models (Sec-
tion 3.2), we can deduce the collaboration type to
assign to any given paper. However, absolute cate-
gorization requires an additional thresholding of au-
thor similarity scores. To avoid the addition of an
arbitrary threshold, instead of directly categorizing
papers, we rank them based on the calculated sim-
ilarity scores on three different spectra. To facili-
tate ease of interpretation, the qualitative examples
we present are drawn from high PageRank papers as
calculated in (Radev et al, 2009).
5.1 The MaxSim Score
To measure the similarity of authors? previous work
to a paper, we look at the cosine similarity between
the term vector of the paper and each author?s term
signature. We are only interested in the highest co-
sine similarity score produced by an author, as our
categories do not differentiate between papers that
are similar to one author and papers that are sim-
ilar to multiple authors, as long as high similarity
to any single author is present. Thus, we choose
our measure, the MaxSim score, to be defined as:
max
a?est
cos(asig, paper)
We choose to observe the similarity scores only
for established authors as newer authors will not
have enough previous work to produce a stable term
signature, and we vary the experience threshold by
year to account for the fact that there has been a large
increase in the absolute number of papers published
in recent years.
Depending on the presence of new authors and
the number of established authors present, each pa-
per can be placed into one of the three spectra: the
Apprenticeship-New Blood spectrum, the Synergy
spectrum and the Apprenticeship-Catalyst spectrum.
Apprenticeship and Low Synergy papers are those
with high MaxSim scores, while low scores indicate
New Blood, Catalyst or High Synergy papers.
5.2 Examples
The following are examples of high impact papers
as they were categorized by our system:
127
Figure 1: Decision diagram for determining the collaboration type of a paper. A minimum of 1 established author is
assumed.
5.2.1 Example: Apprenticeship Paper
Improvements in Phrase-Based Statistical Ma-
chine Translation (2004)
by Richard Zens and Hermann Ney
This paper had a high MaxSim score, indicating high
similarity to established author Hermann Ney. This
categorizes the paper as an Apprenticeship Paper.
5.2.2 Example: New Blood Paper
Thumbs up? Sentiment Classification using
Machine Learning Techniques (2002)
by Lillian Lee, Bo Pang and Shivakumar
Vaithyanathan
This paper had a low MaxSim score, indicating
low similarity to established author Lillian Lee.
This categorizes the paper as a New Blood Pa-
per, with new authors Bo Pang and Shivakumar
Vaithyanathan. It is important to note here that new
authors do not necessarily mean young authors or
grad students; in this case, the third author on the
paper was experienced, but in a field outside of
ACL.
5.2.3 Example: High Synergy Paper
Catching the Drift: Probabilistic Content
Models, with Applications to Generation and
Summarization (2003)
by Regina Barzilay and Lillian Lee
This paper had low similarity to both established
authors on it, making it a highly synergistic paper.
Synergy here indicates that the work done on this
paper was mostly unlike work previously done by
either of the authors.
5.2.4 Example: Catalyst Paper
Answer Extraction (2000)
by Steven Abney, Michael Collins, Amit Singhal
This paper had a very low MaxSim score, as well
as the presence of an unestablished author, making
it a Catalyst Paper. The established authors (from
an ACL perspective) were Abney and Collins, while
Singhal was from outside the area and did not have
many ACL publications. The work done in this pa-
per focused on information extraction, and was un-
like that previously done by either of the ACL estab-
lished authors. Thus, we say that in this case, Sing-
hal played the role of the catalyst, getting the other
two authors to work on an area that was outside of
their usual range.
5.3 Evaluation
5.3.1 Expert Annotation
To quantitatively evaluate the performance of
our system, we prepared a subset of 120 papers
from among the highest scoring collaborative papers
based on the PageRank metric (Radev et al, 2009).
Only those papers were selected which had at least a
128
single established author. One expert in the field was
asked to annotate each of these papers as being ei-
ther similar or dissimilar to the established authors?
prior work given the year of publication, the title of
the publication and its abstract.
We found that the MaxSim scores of papers la-
beled as being similar to the established authors
were, on average, higher than those labeled as dis-
similar. The average MaxSim score of papers anno-
tated as low MaxSim collaboration types (High Syn-
ergy, New Blood or Catalyst papers) was 0.15488,
while that of papers labeled as high MaxSim types
(Apprentice or Low Synergy papers) had a mean
MaxSim score of 0.21312. The MaxSim scores of
the different sets were compared using a t-test, and
the difference was found to be statistically signifi-
cant with a two-tailed p-value of 0.0041.
Framing the task as a binary classification prob-
lem, however, did not produce very strong results.
The breakdown of the papers and success rates (as
determined by a tuned threshold) can be seen in Ta-
ble 3. The system had a relatively low success rate of
62.5% in its binary categorization of collaborations.
5.3.2 First Author Prediction
Studies have suggested that authorship order,
when not alphabetical, can often be quantified and
predicted by those who do the work (Sekercioglu,
2008). Through a survey of all authors on a sam-
ple of papers, Slone (1996) found that in almost all
major papers, ?the first two authors are said to ac-
count for the preponderance of work?. We attempt
to evaluate our similarity scores by checking if they
are predictive of first author.
Though similarity to previous work is only a small
contributor to determining author order, we find that
using the metric of cosine similarity between author
signatures and papers performs significantly better
at determining the first author of a paper than ran-
dom chance. Of course, this feature alone isn?t ex-
tremely predictive, given that it?s guaranteed to give
an incorrect solution in cases where the first author
of a paper has never been seen before. To solve the
problem of first author prediction, we would have
to combine this with other features. We chose two
other features - an alphabetical predictor, and a pre-
dictor based on the frequency of an author appearing
as first author. Although we don?t show the regres-
Predictor Feature Accuracy
Random Chance 37.35%
Author Signature Similarity 45.23%
Frequency Estimator 56.09%
Alphabetical Ordering 43.64%
Table 2: Accuracy of individual features at predicting the
first author of 8843 papers
sion, we do explore these two other features and find
that they are also predictive of author order.
Table 2 shows the performance of our prediction
feature alongside the others. The fact that it beats
random chance shows us that there is some infor-
mation about authorial efforts in the scores we have
computed.
6 Applications
A number of questions about the nature of collabo-
rations may be answered using our system. We de-
scribe approaches to some of these in this section.
6.1 The Hedgehog-Fox Problem
From the days of the ancient Greek poet
Archilochus, the Hedgehog-Fox analogy has
been frequently used (Berlin, 1953) to describe two
different types of people. Archilochus stated that
?The fox knows many things; the hedgehog one big
thing.? A person is thus considered a ?hedgehog?
if he has expertise in one specific area and focuses
all his time and resources on it. On the other hand,
a ?fox? is a one who has knowledge of several
different fields, and dabbles in all of them instead of
focusing heavily on one.
We show how, using our computed similarity
scores, one can discover the hedgehogs and foxes
of Computational Linguistics. We look at the top
100 published authors in our corpus, and for each
author, we compute the average similarity score the
author?s signature has to each of his or her papers.
Note that we start taking similarity scores into ac-
count only after an author has published 5 papers,
thereby allowing the author to stablize a signature
in the corpus and preventing the signature from be-
ing boosted by early papers (where author similarity
would be artificially high, since the author was new).
We present the authors with the highest average
similarity scores in Table 4. These authors can be
129
Collaboration Type True Positives False Positives Accuracy
New Blood, Catalyst or High Synergy Papers 43 23 65.15%
Apprentice or Low Synergy Papers 32 22 59.25%
Overall 75 45 62.50%
Table 3: Evaluation based on annotation by one expert
considered the hedgehogs, as they have highly sta-
ble signatures that their new papers resemble. On
the other hand, Table 5 shows the list of foxes, who
have less stable signatures, presumably because they
move about in different areas.
Author Avg. Sim. Score
Koehn, Philipp 0.43456
Pedersen, Ted 0.41146
Och, Franz Josef 0.39671
Ney, Hermann 0.37304
Sumita, Eiichiro 0.36706
Table 4: Hedgehogs - authors with the highest average
similarity scores
Author Avg. Sim. Score
Marcus, Mitchell P. 0.09996
Pustejovsky, James D. 0.10473
Pereira, Fernando C. N. 0.14338
Allen, James F. 0.14461
Hahn, Udo 0.15009
Table 5: Foxes - authors with the lowest average similar-
ity scores
6.2 Similarity to previous work by sub-fields
Based on the different types of collaborations dis-
cussed in, a potential question one might ask is
which sub-fields are more likely to produce appren-
tice papers, and which will produce new blood pa-
pers. To answer this question, we first need to deter-
mine which papers correspond to which sub-fields.
Once again, we use topic models to solve this prob-
lem. We first filter out a subset of the 1,200 highest
page-rank collaborative papers from the years 1980
to 2007. We use a set of topics built by running a
standard LDA topic model over the ACL corpus, in
which each topic is hand labeled by experts based on
the top terms associated with it. Given these topic-
term distributions, we can once again use the cosine
similarity metric to discover the highly associated
Topic Score
Statistical Machine Translation 0.2695
Prosody 0.2631
Speech Recognition 0.2511
Non-Statistical Machine Translation 0.2471
Word Sense Disambiguation 0.2380
Table 6: Topics with highest MaxSim scores (papers are
more similar to the established authors? previous work)
Topic Score
Question Answering 0.1335
Sentiment Analysis 0.1399
Dialog Systems 0.1417
Spelling Correction 0.1462
Summarization 0.1511
Table 7: Topics with lowest MaxSim scores (papers are
less similar to the established authors? previous work)
topics for each given paper from our smaller sub-
set, by choosing topics with cosine similarity above
a certain threshold ? (in this case 0.1).
Once we have created a paper set for each topic,
we can measure the ?novelty? for each paper by look-
ing at their MaxSim score. We can now find the av-
erage MaxSim score for each topic. This average
similarity score gives us a notion of how similar to
the established author (or authors) a paper in the sub
field usually is. Low scores indicate that new blood
and synergy style papers are more common, while
higher scores imply more non-synergistic or appren-
ticeship style papers. This could indicate that topics
with lower scores are more open ended, while those
with higher scores require more formality or train-
ing. The top five topics in each category are shown
in Tables 6 and 7. The scores of the papers from
the two tables were compared using a t-test, and the
difference in the scores of the two tables was found
to be very statistically significant with a two-tailed p
value << 0.01.
130
7 Discussion and Future Work
Once we have a robust way to score different kinds
of collaborations in ACL, we can begin to use these
scores as a quantitative tool to study phonemena in
the computational linguistics community. With our
current technique, we discovered a number of nega-
tive results; however, given that our accuracy in bi-
nary classification of categories is relatively low, we
cannot state for sure whether these are true negative
results or a limitation of our model.
7.1 Tentative Negative Results
Among the questions we looked into, we found the
following results:
? There was no signal indicating that authors
who started out as new blood authors were any
more or less likely to survive than authors who
started out as apprentices. Survival was mea-
sured both by the number of papers eventually
published by the author as well as the year of
the author?s final publication; however, calcu-
lations by neither measure correlated with the
MaxSim scores of the authors? early papers.
? Each author in the corpus was labeled for gen-
der. Gender didn?t appear to differentiate how
people collaborated. In particular, there was no
difference between men and women based on
how they started their careers. Women and men
are equally likely to begin as new blood authors
as they are to begin as apprentices.
? On a similar note, established male authors are
equally likely to partake in new blood or ap-
prentice collaborations as their female counter-
parts.
? No noticeable difference existed between aver-
age page rank scores of a certain categorization
of collaborative papers (e.g. high synergy pa-
pers vs. low synergy papers).
It is difficult to conclusively demonstrate negative
results, particularly given that our MaxSim scores
are by themselves not particularly strong discrimi-
nators in the binary classification tasks. We consider
these findings to be tentative and an opportunity to
explore in the future.
8 Conclusion
Not everything we need to know about academic
collaborations can be found in the co-authorship
graph. Indeed, as we have argued, not all types
of collaborations are equal, as embodied by differ-
ing levels of seniority and contribution from each
co-author. In this work, we have taken a first step
toward computationally modeling these differences
using a latent mixture of authors model and ap-
plied it to our own field, Computational Linguistics.
We used the model to examine how collaborative
works differ by authors and subfields in the ACL an-
thology. Our model quantifies the extent to which
some authors are more prone to being ?hedgehogs,?
whereby they heavily focus on certain specific ar-
eas, whilst others are more diverse with their fields
of study and may be analogized with ?foxes.?
We also saw that established authors in certain
subfields have more deviation from their previous
work than established authors in different subfields.
This could imply that the former fields, such as
?Sentiment Analysis? or ?Summarization,? are more
open to new blood and synergistic ideas, while other
latter fields, like ?Statistical Machine Translation?
or ?Speech Recognition? are more formal or re-
quire more training. Alternatively, ?Summarization?
or ?Sentiment Analysis? could just still be younger
fields whose language is still evolving and being in-
fluenced by other subareas.
This work takes a first step toward a new way of
thinking about the contributions of individual au-
thors based on their network of areas. There are
many design parameters that still exist in this space,
including alternative text models that take into ac-
count richer structure and, hopefully, perform bet-
ter at discriminating between the types of collabo-
rations we identified. We intend to use the ACL an-
thology as our test bed for continuing to work on tex-
tual models of collaboration types. Ultimately, we
hope to apply the lessons we learn on modeling this
familiar corpus to the challenge of answering large-
scale questions about the nature of collaboration as
embodied by large scale publication databases such
as ISI and Pubmed.
131
Acknowledgments
This research was supported by NSF grant NSF-
0835614 CDI-Type II: What drives the dynamic cre-
ation of science? We thank our anonymous review-
ers for their valuable feedback and the members of
the Stanford Mimir Project team for their insights
and engagement.
References
Isaiah Berlin. 1953. The hedgehog and the fox: An essay
on Tolstoy?s view of history. Simon & Schuster.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
113?120, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Sean M. Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In Pro-
ceedings of the 26th International Conference on Ma-
chine Learning.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 363?371, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. F. Jones, S. Wuchty, and B. Uzzi. 2008. Multi-
university research teams: Shifting impact, geography,
and stratification in science. Science, 322:1259?1262,
November.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. In-
formation Processing & Management, 41(6):1462 ?
1480. Special Issue on Infometrics.
Mario A. Nascimento, Jo?rg Sander, and Jeffrey Pound.
2003. Analysis of sigmod?s co-authorship graph. SIG-
MOD Rec., 32:8?10, September.
M. E. J. Newman. 2001. From the cover: The struc-
ture of scientific collaboration networks. Proceedings
of the National Academy of Science, 98:404?409, Jan-
uary.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256.
Craig M. Rawlings and Daniel A. McFarland. 2011. In-
fluence flows in the academy: Using affiliation net-
works to assess peer effects among researchers. Social
Science Research, 40(3):1001 ? 1017.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, UAI
?04, pages 487?494.
Cagan H. Sekercioglu. 2008. Quantifying coauthor con-
tributions. Science, 322(5900):371.
RM Slone. 1996. Coauthors? contributions to major
papers published in the ajr: frequency of undeserved
coauthorship. Am. J. Roentgenol., 167(3):571?579.
Howard D. White and Katherine W. Mccain. 1998. Visu-
alizing a discipline: An author co-citation analysis of
information science. Journal of the American Society
for Information Science, 49:1972?1995.
132
