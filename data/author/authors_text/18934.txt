Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1834?1843, Dublin, Ireland, August 23-29 2014.
Converting Phrase Structures to Dependency Structures in Sanskrit
Pawan Goyal
Department of CSE
Indian Institute of Technology
Kharagpur, India ? 721302
pawang@cse.iitkgp.ernet.in
Amba Kulkarni
Department of Sanskrit Studies
University of Hyderabad
Hyderabad, India ? 500046
apksh@uohyd.ernet.in
Abstract
Two annotations schemes for presenting the parsed structures are prevalent viz. the constituency
structure and the dependency structure. While the constituency trees mark the relations due to
positions, the dependency relations mark the semantic dependencies. Free word order languages
like Sanskrit pose more problems for constituency parses since the elements within a phrase are
dislocated. In this work, we show how the enriched constituency tree with the information of
displacement can help construct the unlabelled dependency tree automatically.
1 Introduction
Sanskrit has a rich tradition of linguistic analysis with intense discussions and argumentations on various
aspects of language analysis ranging from phonetics (?siks
.
?a), grammar (vy?akaran
.
a), logic (ny?aya), ritual
exegesis (karmam??m?am
.
s?a), and literary theory (alam
.
k?ara?s?astra) which is not only useful for analysing
Sanskrit but it also has much to offer computational linguistics in these areas. The series of symposia
in Sanskrit Computational Linguistics (Huet et al., 2009; Kulkarni and Huet, 2009), the consortium
project sponsored by the Technology Development for Indian Languages (TDIL) and the research of
individual scholars and the collaborations (Goyal et al., 2012) among them resulted into a) development
of several tools ranging from segmenters (Huet, 2009), morphological analysers (Kulkarni and Shukl,
2009), parsers (Goyal et al., 2009; Hellwig, 2009; Kumar, 2012; Kulkarni, 2013) to discourse annotators,
b) lexical resources ranging from dictionaries, WordNet (Kulkarni et al., 2010) to Knowledge-Nets (Nair,
2011), and c) annotated corpora [http://sanskrit.uohyd.ernet.in/scl].
P?an
.
inian grammar, the oldest dependency grammar, provides a formalism for annotation of the
sentences. While the Sanskrit consortium has annotated a few thousand sentences following the
dependency grammar, we also came across a very valuable source of annotation of Sanskrit sentences
following the constituency structure (Gillon, 1996). The constituency structure was enriched to suite
the requirements of Sanskrit. This aroused our curiosity to study the equivalence of the two annotation
schemes.
The importance of dependency structure has been well recognised by several computational linguists
(Culotta and Sorensen, 2004; Haghighi et al., 2005; Quirk et al., 2005) in the recent past. The dependency
format is preferred over the constituency not only from evaluation point of view (Lin, 1998) but also
because of its suitability (Marneffe et al., 2006) for a wide range of NLP tasks such as Machine
Translation (MT), information extraction, question answering etc.. This has upsurged several works
on converting a constituency structure into dependency. The parsers for English now produce the
dependency parse as well. Xia and Palmer discuss three different algorithms to convert dependency
structures to phrase structures for English (Xia and Palmer, 2001). Magerman gave a set of priority
lists, in the form of a head percolation table to find heads of constituents (Magerman, 1994). Yamada
and Matsumoto modified these head percolation rules further (Yamada and Matsumoto, 2003). Their
method was reimplemented by Nivre, who also defined certain heuristics to infer the arc labels in the
dependency tree produced (Nivre, 2006). Johansson and Nugues used a richer set of edge labels and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1834
introduced links to handle long-distance phenomena such as wh-movement, topicalization, expletives
and gapping (Johansson and Nugues, 2007). Their conversion procedure made use of this extended
structure in Penn Treebank. De et al. described a system for generating typed dependency parsed from
the phrase structure parses (De Marneffe et al., 2006). (Palmer et al., 2009; Xia et al., 2009; Bhatt et al.,
2009) discuss a multi-layered representation framework for Hindi and Urdu, where the information from
syntactic as well as dependency parse is presented together.
In this work, we explore the relationship between enriched constituency structures and dependency
structures for Sanskrit language, with main emphasis on the conversion from constituency to dependency
structures. This work aims not only at designing an algorithm to convert Treebanks from one type of
representation to the other, but also to judge the adequacy of the enriched constituency structure from
parsing point of view. This paper has been organized as follows. Section 2 discusses the history and
origin of this work. Section 3 describes the background of the constituency and dependency structures,
utilized for Sanskrit language. Section 4 discusses the algorithm we used for converting constituency
structure into dependency structure. Section 5 describes the results obtained by our approach, with some
examples. Section 6 concludes this paper with the directions for future work.
2 Origin of the work
The dataset we are using in this work has its origin in the remarkable treatise on Sanskrit Syntax by Apte
(Apte, 1885) which is the most authentic book on Sanskrit Syntax even after 125 years. The work was
initiated in 1986 by Brendan Gillon, then a senior fellow at the American Institute of Indian Studies,
at Deccan College, put all the prose exercise sentences from Apte?s Student Guide onto 5 ? 7 cards,
assigning a syntactic parse to each sentence, giving each sentence an English translation and annotating
each sentence for miscellaneous syntactic and semantic facts. On the basis of these sentences, Brendan
Gillon published the grammar underlying his syntactic parse of these sentences (Gillon, 1996).
In 1991, Brendan Gillon transferred the material from a paper format to an electronic format, making
revisions. An example sentence in this dataset is given below:
Example{3}
Source{1.1.3 (P) <U 4.5.3>} % Apte{7,3}
Parse
[S [INJ haa ] [ADV katham ]
[NP1s [NP6 (mahaaraaja<Dasharathasya) ] (dharma<daaraa.h) ]
[VP 0 [NP1 (priya<sakhii) [NP6 me ] [NP1 Kaushalyaa ] ] ] ]
Gloss{Oh, how is it that the legal wife of King Dasharatha is my dear
friend Kaushalyaa}
Comment{copula: covert: predicational: NP1s VP }
Each example is given a serial number, its source - the corresponding reference in Apte?s book. Then,
its constituency parse is provided in a tree structure. The Sanskrit text is transliterated into Roman
using the Velthuis notation
1
. Finally, the gloss (translation) of the prose is provided along with some
observations regarding syntax in the field ?comment?. The proper nouns are transliterated following the
English convention of capitalisation. The constituency structure is enriched reflecting the morphological
information such as the case marker. The underlying constituency structure of the compounds is also
shown clearly marking the head of the compound. The requirement that constituency tree be a binary is
also done away with, resulting into a more flat structure than the normal hierarchical phrase structure.
In 2004, G?erard Huet re-engineered the document in order to parse it mechanically, and he verified its
correct syntactic structure after typographical corrections. He devised an abstract syntax to formalize this
constituency structure. In the abstract syntax, the above constituency structure is represented as below:
list Tag_tree.syntax =
[S
1
Originally developed in 1991 by Frans Velthuis for use with his devnag Devanagari font, designed for the TeX typesetting
system.
1835
[INJ ("haa", 1); ADV [("katham", 2)];
NP
([Case 1; Role Subject],
[NP ([Case 6], [N (Compound (Stem <mahaaraaja>, Stem <Dasharathasya>),
3)]);
N (Compound (Stem <dharma>, Stem <daaraa.h>), 4)]);
VP0
[NP
([Case 1],
[N (Compound (Stem <priya>, Stem <sakhii>), 5);
NP ([Case 6], [N (Stem <me>, 6)]);
NP ([Case 1], [N (Stem <Kaushalyaa>, 7)])]);
NIL 8]]]
Each stem is given a unique index. The syntax, while preserving the original structure of the text, gives
additional structuring with the word numbers, explicit case markers and stems for the compounds. While
these constituent trees preserve much of the tagging related information, they still do not have the gender
and number information for the substantives, for instance. This information can enhance the constituency
representation further.
The same set of sentences were also parsed manually by Sheetal Pokar, a research scholar at the
University of Hyderabad, showing the dependency structure. Sheetal followed the annotation guidelines
developed by the Consortium of Institutes working on the Development of Sanskrit Computational
Tools
2
. This tagset has a little above 40 tags marking various relations. The dependency tree for the
example 3, discussed above, is shown in Figure 1. It is a directed tree with nodes corresponding to
the words in the sentence and edges corresponding to the relation between the head and the modifier.
Each node has a number indicating the word index. A generic relation sambandhah
.
(R) is used if the
relation does not fall under any of the given tags. As one may notice, both the constituency as well as
the dependency structures posit a NULL verb ?to be? asti. Among the Indian schools dealing with verbal
cognition, not all schools accept the insertion of missing copula. We follow the grammarian school who
accept this insertion.
(a) (b)
Figure 1: Example 3: (a). Constituency Parse and (b). Dependency Parse
While the two structures in Figure 1 mark different kind of information, we notice that the dominance
relation in the constituency structure under each phrase corresponds to the the modifier-modified relation
in the dependency tree. This was the main motivation to develop the converter to convert a phrase
structure into an unlabelled dependency structure.
2
http://sanskrit.uohyd.ernet.in/scl
1836
3 Dependency and Constituency Structures
Verbal understanding of any utterance requires the knowledge of how words in that utterance are related
to each other. There are two major representational frameworks for representing this knowledge as a
parse tree viz. constituency and dependency parse trees. Constituency trees show how the individual units
in a sentence are grouped together leading to semantically richer phrases in the constituency structures.
The dependency structure, on the other hand, shows how each word is related to other words in the
sentence either directly or indirectly.
The constituency structure derives from the subject-predicate division of Latin and Greek grammars
that is based on term logic. Basic clause structure is understood in terms of a binary division of the clause
into subject (noun phrase NP) and predicate (verb phrase VP). These ideas originated with Leonard
Bloomfield (Bloomfield, 1962) and were further developed by a number of American structuralist
linguists, including Harris (Harris, 1955) and Wells (Wells, 1947). Though these grammars were
initially conceived as applying only to phrases, it was shown that such rules could be used for analyzing
compounds as well as derivational morphology for Sanskrit. Gillon showed that the same extension
works for classical Sanskrit as well (Gillon, 1995).
The dependency analysis dates back to P?an
.
ini who uses the syntactico-semantic relations called k?araka
relations for the linguistic analysis of a sentence. In modern times, the seminal work of Tesni`ere
(Tesni`ere, 1959) became the basis for the work on dependency grammar. Meaning-Text Theory (Mel?cuk,
1988), Word grammar (Hudson, 1984), Functional Generative Description (Segall et al., 1986) are some
of the flavours of the dependency grammar. A dependency parse is generally modelled as a directed tree
with nodes representing the words and edges representing the possible relations between them. A typed
dependency parse also labels the relations. For every element (word or morph) in a sentence, there is just
one node in the syntactic structure.
Xia and Palmer (Xia and Palmer, 2001) discuss three different algorithms to convert dependency
structures to phrase structures for English. They also attempt to clarify the differences in representational
coverage of the two approaches. In the first stage, they identify the head of each constituent of the
sentence, which is further modified to retrieve the semantic head. In the second phase, they label each
of the dependency extracted with a grammatical relation using patterns defined over the phrase structure
tree. (Palmer et al., 2009; Xia et al., 2009; Bhatt et al., 2009) discuss a multi-layered representation
framework for Hindi and Urdu, where the information from syntactic as well as dependency parse is
presented together. They first construct a dependency parse and then convert it into the constituency
parse tree using conversion rules. A conversion rule is a (DS pattern, PS pattern) pair, where DS and PS
correspond to dependency and phrase structure respectively.
The constituency parse we are dealing with being enriched with linguistic information pertaining to the
morphology of the simple as well as compound words, it was much simpler to convert this structure into
a dependency structure. In the next section, we will discuss our algorithm for converting a constituency
structure to a dependency structure.
4 Conversion from Constituency to Dependency Structure in Sanskrit
The notion of ?head? is very important for both the constituency and dependency structures. In the
constituency structure, the head determines the main properties of the phrase. Head may have several
levels of projection. In the dependency structure, on the other hand, the head is linked to its dependents.
The core of the algorithm is to identify the head of each phrase in the constituency tree and establish its
relation with the head of its parent node. And also to establish the relation between the head with its
dependents. The head for each XP is the node X within that XP. Thus the head for an NP is the noun,
head for a VP is the verb, and so on. The head for the S is the head of a VP, in case it is a simple sentence,
and head of the VP of the main clause in case it is a complex sentence. In case of complex sentences,
we identified the main clause taking clues from the connectives. Each relation is named after the XP of
the modifier. Our algorithm for finding the head node is implemented on the abstract syntax discussed in
section 2 before. A rough outline of the algorithm is:
1)The head of VP is the ROOT node in the dependency tree.
1837
a) In the case of sentences with sub-ordinate clauses, identify the main clause taking clues from the
connectives.
b) Head of a clause is an auxiliary, if present, otherwise the main verb is the head.
c) In the case of sentences with quotative markers, the verb of the main clause is the head.
(The later rule is stronger than the previous. )
2) All the XPs within VP are dependent on the ROOT.
3) If S is the parent of VP, then all the XPs which are children of S are also dependent on this ROOT.
Finding the head for each node was not trivial though, as many of the parses involve dislocated phrases,
which were not fully marked. We had to enrich the constituency trees by incorporating the dislocation
information, which was provided in comments and was missing from the tree notations. We used ?!? and
?$? to indicate the dislocation. ?!? indicates the position from where a component is dislocated, while
?$? indicates the dislocated component. An example of a constituency parse, enriched with dislocation
information is given below.
Example{2}
Source{1.1.2 (P) <V 3.28; V 3.6.3>} % Apte{7,2}
Parse
[S [ADV sarvatra ] [NP6 audarikasya $1]
[VP 0 [NP1 abhyavahaaryam [PRT eva ] ] ]
[NP1s !1 vi.saya.h ] ]
Gloss{In every case, a glutton?s object is only food.}
Comment{copula: covert: predicational: VP NP1s
"eva" in predicate NP
left extraposition from NP1s of NP6 within MC, modulo adverbial ADV.}
This constituency tree involves one dislocated phrases. This information is marked with ?!1?for the
place from which it is dislocated and with ?$1? for the phrase that has been dislocated. This dislocation
information is used by our algorithm to find the right relata for the dislocated words. In case of more
than one dislocated phrases, they are numbered sequentially.
5 Results and Discussions
We implemented our algorithm on a dataset of 232 sentences and matched the output of our algorithm
with the Gold dataset, the dependency graph constructed manually for the sentences by Sheetal. Figure
2 shows the dependency structure for Example 3, produced by our conversion algorithm.
Figure 2: Dependency graph constructed by conversion from constituency structure
The conversion algorithm captures the relations between various constituents and produces a
dependency graph. Labels of the dependency graph correspond to the intermediate nodes in the phrase
structure. On comparing the graph in Figure 2 with the graph in Figure 1, we find that most of the
original connections were captured. However, there was a mismatch in the relation between words
kau?saly?a, priyasakh?? and me. This discrepancy is because of the non-agreement between the annotators
as to which one is the head. In case of sentences with apposition, in Sanskrit, the two annotators have
difference of opinion as to which among the two is the head. This resulted in the mismatch between the
two graphs. The relations in this graph are labelled by the dominating XP.
1838
P?an
.
ini?s grammar provides rules for assigning case markers given the syntactico-semantic relation
between the relata. Inverting these rules it should be possible to get the relation labels. These relation
labels, however, will not be obtained deterministically. The non-determinism will lead to multiple
labelled dependency structures. Hence we could not assign the labels from the tagset, and resorted
to the names of the phrases which the word belongs to.
Below we give an example where we found an exact match between the manual dependency graph
and the dependency graph, produced by our conversion algorithm, using the constituency parse of the
sentence.
Example{29}
[S [VP [NP7 tatra ] [CNJ ca ]
[NP5 [NP6 [AP6 (((nikhila<(dhara.nii<tala))<parya.tana)<khinnasya) ]
(nija<balasya) ]
(vizraama<heto.h) ]
[NP2 [AP2 katipayaan ] divasaan ]
ati.s.that ] ]
Gloss{And he remained there for a few days in order to rest his army
exhausted from roaming the entire surface of the earth.}
(a) (b)
Figure 3: Example 29: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
Out of 232 cases, we found 97 such cases with exact match. For the rest of the cases,
1) In 40 cases, number of words in dependency and phrase-converted graph are different. For example,
the words such as kad?acit ?probably?, yadyapi ?even if?, tath?api ?even then?, athav?a ?or? etc. were treated
in one structure as a single word while in the other as two words. These words at morphological level
consist of two morphemes which have independent existence. So it was natural to treat these as two
words, in the constituency trees. However, at the semantic level, these two words indicate a single
meaning which at times is non-compositional. The annotator of dependency graph has treated these as a
single word.
2) In 95 cases, one or more relations do not match. These were due to various reasons such as
1. differences in the treatment and identification of adjectives,
2. disagreement in the attachment, and
3. cases of ellipsis, null head, and cases where the treatment of conjunct ?ca? (and) differs.
1839
These differences are very much important from linguistic analysis point of view. However due to space
constraint we illustrate here only two cases where the treatment of the two annotators differ.
Example{72}
[S [VOC sakhi [VOC Vaasanti ] ]
[VP 0 [NP4 du.hkhaaya [PRT eva ] [NP6 su-h.rdaam ] ] ]
[NP1s [ADV idaaniim ] [NP6 Raamasya ] darshanam ] ]
Gloss{Oh my friend Vaasanti, seeing Raama now leads only to
the unhappiness of his friends.}
(a) (b)
Figure 4: Example 72: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
In this example, there are two places where the annotators disagree.
a) The attachment of the word id?an??m (now).
b) The decision of head in case of vocative with a modifier.
The null copula (VP 0) in the constituency structure is replaced by the Sanskrit verbal form asti
uniformly. The annotator of dependency graph has provided a more appropriate verb bhavis
.
yati.
However, we ignore this difference.
Let us look at another example, where the ambiguity in morphological analysis has led to the ambiguity
in the readings resulting in the disagreement in the parse.
Example{46}
[S [NP1s aarya.h ]
[VP daapayatu [NP2 [NP6 me ]
[NP4 (Vaisha.mpaayana<aanayanaaya) ]
(gamana<abhyanuj?naam) ]
[NP3 taatena ] ] ]
Gloss{May you, sir, make my father give me permission to go to bring
Vaisha.mpaayana.}
In this example, the pronominal form me is ambiguous between two readings. It can be either a genitive
or a dative of the first person pronoun asmad ?I?. The sub-ordinate clause is analysed by Gillon as ?the
permission for going to bring Vai?samp?ayan by me?. In Sanskrit the first person pronoun in such cases
takes genitive case marker. Sheetal on the other hand has analysed it as ?the permission to me for going to
bring Vai?samp?ayan?, where me is analysed with dative case. It is clear that ?to bring Vai?samp?ayan? is the
purpose for going. But since ?permission for going? is a compound in Sanskrit
3
, and the ?permission?
being the head of this compound, Sheetal avoided linking ?to bring Vai?samp?ayana? with ?permission
to go? as it results into an ?asamartha sam?asa (incompatible compound formation, where the external
modifier connects the modifier component of a compound and not the head). This has resulted into the
differences in annotation. Such compounds are not rare. Thus, in order to provide a correct parse, in
case of dependency structure, it is necessary to show the internal structure of the compounds as well,
3
In Sanskrit a compound is always written as a single word without any space in between.
1840
(a) (b)
Figure 5: Example 46: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
rather than treating a compound unanalysed. This will allow one to connect the elements to the part of a
compound other than the head. Similar treatment is necessary in the phrase structure annotation as well.
We end with example 2 from section 4, where the dislocation information in the constituency tree
helps in retrieving the correct dependency structure. In this example, even though the word audarikasya
has been displaced, the displacement information in the parse tree positions it at the correct place in the
dependency tree constructed from the constituency structure.
(a) (b)
Figure 6: Example 2: (a). Constituency structure and (b). Dependency graph constructed by conversion
from constituency structure
6 Conclusions and Future Work
This work focussed mainly on conversion from constituency to dependency structure. The sentences in
our dataset are chosen from (Apte, 1885), which is an authentic book for higher learning of Sanskrit,
covering a wide range of grammatical constructions. The tool was tested on a dataset of 232 sentences
and the initial results were encouraging. Specifically, most of the cases of mismatch were linguistic
issues and need further discussion. The phrase labels indicating the case labels is an important extension
of the constituency trees to accommodate morphologically rich languages. The enriched constituency
structure has an advantage of recording the word order, and at the same time marking the dislocation
information. In this work, we have shown that such enriched constituency tree can help construct the
unlabelled dependency tree automatically. Further, one may also try inferring the dependency relation
names, and use statistical parsing to resolve non-determinism favouring popular usages.
1841
Another interesting aspect would be to try the other way conversion, that is, from dependency to
phrase structure. The main challenge for this conversion is to find out the projection table corresponding
to each lexical item. This work will also be a first step towards an abstract syntax, which can inherit
the properties of both the constituency and dependency structures. If so, this would be an alternative
formalism for tagging the Sanskrit corpus.
Acknowledgements
The authors would like to acknowledge the discussions with G?erard Huet, INRIA Paris Rocquencourt,
towards enriching the abstract syntax. The work was also supported by Emilie Aussant and Sylvie
Archaimbault, laboratoire HTL, Universit?e Paris Diderot.
References
V?aman Shivar?am Apte. 1885. The Student?s Guide to Sanskrit Composition. A Treatise on Sanskrit Syntax for Use
of Schools and Colleges. Lokasamgraha Press, Poona, India.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
A multi-representational and multi-layered treebank for hindi/urdu. In Proceedings of the Third Linguistic
Annotation Workshop, pages 186?189. Association for Computational Linguistics.
Leonard Bloomfield. 1962. Language. New York: Holt.
A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL), pages 423?429, Barcelona, Spain.
Monali Das and Amba Kulkarni. 2013. Discourse level tagger for mah?abh?as
.
ya - a Sanskrit commentary on
p?an
.
ini?s grammar. In Proceedings of the 10th International Conference on NLP, Delhi, India.
Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed
dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449?454.
Brendan S. Gillon. 1995. Autonomy of word formation: evidence from Classical Sanskrit. Indian Linguistics, 56
(1-4), pages 15?52.
Brendan S Gillon. 1996. Word order in classical sanskrit. Indian Linguistics, 57(1-4):1?35.
Brendan S. Gillon. 2009. Tagging classical Sanskrit compounds. In Amba Kulkarni and G?erard Huet, editors,
Sanskrit Computational Linguistics 3, pages 98?105. Springer-Verlag LNAI 5406.
Pawan Goyal, Vipul Arora, and Laxmidhar Behera. 2009. Analysis of Sanskrit text: Parsing and semantic
relations. In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1
& 2, pages 200?218. Springer-Verlag LNAI 5402.
Pawan Goyal, G?erard Huet, Amba Kulkarni, Peter Scharf, and Ralph Bunker. 2012. A distributed platform for
sanskrit processing. In Proceedings of 24th COLING, Mumbai, India.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Robust textual inference via graph matching. In
Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 387?394, Vancouver, Canada.
Zellig S Harris. 1955. From phoneme to morpheme. Language, 31(2):190?222.
Oliver Hellwig. 2009. Extracting dependency trees from Sanskrit texts. In Amba Kulkarni and G?erard Huet,
editors, Sanskrit Computational Linguistics 3, pages 106?115. Springer-Verlag LNAI 5406.
R. Hudson. 1984. Word Grammar. Basil Blackwell, Oxford.
G?erard Huet, Amba Kulkarni, and Peter Scharf, editors. 2009. Sanskrit Computational Linguistics 1 & 2.
Springer-Verlag LNAI 5402.
G?erard Huet. 2009. Formal structure of Sanskrit text: Requirements analysis for a mechanical Sanskrit
processor. In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1
& 2. Springer-Verlag LNAI 5402.
1842
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In
Proceedings of the 16th Nordic Conference on Computational Linguistics (NODALIDA), pages 105?112.
Amba Kulkarni and Monali Das. 2012. Discourse analysis of sanskrit texts. In Proceedings of the workshop on
Advances in Discourse Analysis and its Computational Aspects, 24th COLING, Mumbai, India.
Amba Kulkarni and G?erard Huet, editors. 2009. Sanskrit Computational Linguistics 3. Springer-Verlag LNAI
5406.
Amba Kulkarni and K. V. Ramakrishnamacharyulu. 2013. Parsing Sanskrit texts: Some relation specific issues. In
Malhar Kulkarni, editor, Proceedings of the 5th International Sanskrit Computational Linguistics Symposium.
D. K. Printworld(P) Ltd.
Amba Kulkarni and Devanand Shukl. 2009. Sanskrit morphological analyser: Some issues. Indian Linguistics,
70(1-4):169?177.
Malhar Kulkarni, Chaitali Dangarikar, Irawati Kulkarni, Abhishek Nanda, and Pushpak Bhattacharyya. 2010.
Introducing sanskrit wordnet. In Christiane Felbaum Pushpak Bhattacharyya and Piek Vossen, editors,
Principles, Construction and Application of Multilingual Wordnets, Proceedings of the Global Wordnet
Conference, 2010. Narosa Publishing House, New Delhi.
Amba Kulkarni. 2013. A deterministic dependency parser with dynamic programming for Sanskrit. In
Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages
157?166, Prague, Czech Republic, August. Charles University in Prague, Matfyzpress, Prague, Czech Republic.
Anil Kumar. 2012. An automatic Sanskrit Compound Processing. Ph.D. thesis, University of Hyderabad,
Hyderabad.
Dekang Lin. 1998. Dependency-based evaluation of minipar. In Workshop on the evaluation of Parsing Systems,
Granada, Spain.
David Mitchell Magerman. 1994. Natural Language Parsing As Statistical Pattern Recognition. Ph.D. thesis,
Stanford, CA, USA. UMI Order No. GAX94-22102.
Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In Proceedings of LREC-06.
I. Mel?cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.
Sivaja Nair. 2011. The Knowledge Structure in Amarako?sa. Ph.D. thesis, University of Hyderabad, Hyderabad.
Joakim Nivre. 2006. Inductive dependency parsing. Springer.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure. In The 7th
International Conference on Natural Language Processing, pages 14?17.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt.
In 43rd Annual Meeting of the Association for, Computational Linguistics (ACL), pages 271?279, Ann Arbor,
USA.
Peter Scharf and Malcolm Hyman. 2009. Linguistic Issues in Encoding Sanskrit. Motilal Banarsidass, Delhi.
P. Segall, E. Hajiov, and J. Panevov. 1986. The Meaning of the Sentence in its Semantic and Pragmatic Aspects.
Springer, Heidelberg.
L Tesni`ere. 1959.
?
El?ements de syntaxe structurale. Klinksieck, Paris.
Rulon S Wells. 1947. Immediate constituents. Language, 23(2):81?117.
Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In Proceedings
of the first international conference on Human language technology research, pages 1?5. Association for
Computational Linguistics.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma. 2009. Towards a
multi-representational treebank. In The 7th International Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines.
Proceedings of IWPT, 3.
1843
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1020?1029,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
That?s sick dude!:
Automatic identification of word sense change across different timescales
Sunny Mitra
1
, Ritwik Mitra
1
, Martin Riedl
2
,
Chris Biemann
2
, Animesh Mukherjee
1
, Pawan Goyal
1
1
Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India ? 721302
2
FG Language Technology, Computer Science Department, TU Darmstadt, Germany
1
{sunnym,ritwikm,animeshm,pawang}@cse.iitkgp.ernet.in
2
{riedl,biem}@cs.tu-darmstadt.de
Abstract
In this paper, we propose an unsupervised
method to identify noun sense changes
based on rigorous analysis of time-varying
text data available in the form of millions
of digitized books. We construct distribu-
tional thesauri based networks from data
at different time points and cluster each
of them separately to obtain word-centric
sense clusters corresponding to the differ-
ent time points. Subsequently, we com-
pare these sense clusters of two different
time points to find if (i) there is birth of
a new sense or (ii) if an older sense has
got split into more than one sense or (iii)
if a newer sense has been formed from the
joining of older senses or (iv) if a partic-
ular sense has died. We conduct a thor-
ough evaluation of the proposed method-
ology both manually as well as through
comparison with WordNet. Manual eval-
uation indicates that the algorithm could
correctly identify 60.4% birth cases from
a set of 48 randomly picked samples and
57% split/join cases from a set of 21 ran-
domly picked samples. Remarkably, in
44% cases the birth of a novel sense is
attested by WordNet, while in 46% cases
and 43% cases split and join are respec-
tively confirmed by WordNet. Our ap-
proach can be applied for lexicography, as
well as for applications like word sense
disambiguation or semantic search.
1 Introduction
Two of the fundamental components of a natu-
ral language communication are word sense dis-
covery (Jones, 1986) and word sense disambigua-
tion (Ide and Veronis, 1998). While discovery
corresponds to acquisition of vocabulary, disam-
biguation forms the basis of understanding. These
two aspects are not only important from the per-
spective of developing computer applications for
natural languages but also form the key compo-
nents of language evolution and change.
Words take different senses in different contexts
while appearing with other words. Context plays
a vital role in disambiguation of word senses as
well as in the interpretation of the actual mean-
ing of words. For instance, the word ?bank? has
several distinct interpretations, including that of a
?financial institution? and the ?shore of a river.?
Automatic discovery and disambiguation of word
senses from a given text is an important and chal-
lenging problem which has been extensively stud-
ied in the literature (Jones, 1986; Ide and Vero-
nis, 1998; Sch?utze, 1998; Navigli, 2009). How-
ever, another equally important aspect that has not
been so far well investigated corresponds to one
or more changes that a word might undergo in its
sense. This particular aspect is getting increas-
ingly attainable as more and more time-varying
text data become available in the form of millions
of digitized books (Goldberg and Orwant, 2013)
gathered over the last centuries. As a motivat-
ing example one could consider the word ?sick?
? while according to the standard English dictio-
naries the word is normally used to refer to some
sort of illness, a new meaning of ?sick? refer-
ring to something that is ?crazy? or ?cool? is cur-
rently getting popular in the English vernacular.
This change is further interesting because while
traditionally ?sick? has been associated to some-
thing negative in general, the current meaning as-
sociates positivity with it. In fact, a rock band
by the name of ?Sick Puppies? has been founded
which probably is inspired by the newer sense of
the word sick. The title of this paper has been
motivated by the above observation. Note that
this phenomena of change in word senses has ex-
isted ever since the beginning of human commu-
nication (Bamman and Crane, 2011; Michel et
1020
al., 2011; Wijaya and Yeniterzi, 2011; Mihalcea
and Nastase, 2012); however, with the advent of
modern technology and the availability of huge
volumes of time-varying data it now has become
possible to automatically track such changes and,
thereby, help the lexicographers in word sense dis-
covery, and design engineers in enhancing vari-
ous NLP/IR applications (e.g., disambiguation, se-
mantic search etc.) that are naturally sensitive to
change in word senses.
The above motivation forms the basis of the
central objective set in this paper, which is to de-
vise a completely unsupervised approach to track
noun sense changes in large texts available over
multiple timescales. Toward this objective we
make the following contributions: (a) devise a
time-varying graph clustering based sense induc-
tion algorithm, (b) use the time-varying sense
clusters to develop a split-join based approach for
identifying new senses of a word, and (c) evalu-
ate the performance of the algorithms on various
datasets using different suitable approaches along
with a detailed error analysis. Remarkably, com-
parison with the English WordNet indicates that
in 44% cases, as identified by our algorithm, there
has been a birth of a completely novel sense, in
46% cases a new sense has split off from an older
sense and in 43% cases two or more older senses
have merged in to form a new sense.
The remainder of the paper is organized as fol-
lows. In the next section we present a short re-
view of the literature. In Section 3 we briefly
describe the datasets and outline the process of
co-occurrence graph construction. In Section 4
we present an approach based on graph cluster-
ing to identify the time-varying sense clusters and
in Section 5 we present the split-merge based ap-
proach for tracking word sense changes. Evalu-
ation methods are summarized in Section 6. Fi-
nally, conclusions and further research directions
are outlined in Section 7.
2 Related work
Word sense disambiguation as well as word sense
discovery have both remained key areas of re-
search right from the very early initiatives in nat-
ural language processing research. Ide and Vero-
nis (1998) present a very concise survey of the his-
tory of ideas used in word sense disambiguation;
for a recent survey of the state-of-the-art one can
refer to (Navigli, 2009). Some of the first attempts
to automatic word sense discovery were made by
Karen Sp?arck Jones (1986); later in lexicography,
it has been extensively used as a pre-processing
step for preparing mono- and multi-lingual dictio-
naries (Kilgarriff and Tugwell, 2001; Kilgarriff,
2004). However, as we have already pointed out
that none of these works consider the temporal as-
pect of the problem.
In contrast, the current study, is inspired by
works on language dynamics and opinion spread-
ing (Mukherjee et al, 2011; Maity et al, 2012;
Loreto et al, 2012) and automatic topic detection
and tracking (Allan et al, 1998). However, our
work differs significantly from those proposed in
the above studies. Opinion formation deals with
the self-organisation and emergence of shared vo-
cabularies whereas our work focuses on how the
different senses of these vocabulary words change
over time and thus become ?out-of-vocabulary?.
Topic detection involves detecting the occurrence
of a new event such as a plane crash, a murder, a
jury trial result, or a political scandal in a stream
of news stories from multiple sources and track-
ing is the process of monitoring a stream of news
stories to find those that track (or discuss) the
same event. This is done on shorter timescales
(hours, days), whereas our study focuses on larger
timescales (decades, centuries) and we are inter-
ested in common nouns, verbs and adjectives as
opposed to events that are characterized mostly by
named entities. Other similar works on dynamic
topic modelling can be found in (Blei and Laf-
ferty, 2006; Wang and McCallum, 2006). Google
books n-gram viewer
1
is a phrase-usage graphing
tool which charts the yearly count of selected letter
combinations, words, or phrases as found in over
5.2 million digitized books. It only reports fre-
quency of word usage over the years, but does not
give any correlation among them as e.g., in (Heyer
et al, 2009), and does not analyze their senses.
A few approaches suggested by (Bond et al,
2009; P?a?akk?o and Lind?en, 2012) attempt to aug-
ment WordNet synsets primarily using methods
of annotation. Another recent work by Cook et
al. (2013) attempts to induce word senses and then
identify novel senses by comparing two different
corpora: the ?focus corpora? (i.e., a recent version
of the corpora) and the ?reference corpora? (older
version of the corpora). However, this method
is limited as it only considers two time points to
1
https://books.google.com/ngrams
1021
identify sense changes as opposed to our approach
which is over a much larger timescale, thereby, ef-
fectively allowing us to track the points of change
and the underlying causes. One of the closest
work to what we present here has been put forward
by (Tahmasebi et al, 2011), where the authors an-
alyze a newspaper corpus containing articles be-
tween 1785 and 1985. The authors mainly report
the frequency patterns of certain words that they
found to be candidates for change; however a de-
tailed cause analysis as to why and how a particu-
lar word underwent a sense change has not been
demonstrated. Further, systematic evaluation of
the results obtained by the authors has not been
provided.
All the above points together motivated us to
undertake the current work where we introduce,
for the first time, a completely unsupervised and
automatic method to identify the change of a word
sense and the cause for the same. Further, we also
present an extensive evaluation of the proposed al-
gorithm in order to test its overall accuracy and
performance.
3 Datasets and graph construction
In this section, we outline a brief description
of the dataset used for our experiments and
the graph construction procedure. The primary
source of data have been the millions of digitized
books made available through the Google Book
project (Goldberg and Orwant, 2013). The Google
Book syntactic n-grams dataset provides depen-
dency fragment counts by the years. However, in-
stead of using the plain syntactic n-grams, we use
a far richer representation of the data in the form of
a distributional thesaurus (Lin, 1997; Rychl?y and
Kilgarriff, 2007). In specific, we prepare a distri-
butional thesaurus (DT) for each of the time peri-
ods separately and subsequently construct the re-
quired networks. We briefly outline the procedure
of thesauri construction here referring the reader
to (Riedl and Biemann, 2013) for further details.
In this approach, we first extract each word and a
set of its context features, which are formed by la-
beled and directed dependency parse edges as pro-
vided in the dataset. Following this, we compute
the frequencies of the word, the context and the
words along with their context. Next we calculate
the lexicographer?s mutual information LMI (Kil-
garriff, 2004) between a word and its features and
retain only the top 1000 ranked features for ev-
ery word. Finally, we construct the DT network as
follows: each word is a node in the network and
the edge weight between two nodes is defined as
the number of features that the two corresponding
words share in common.
4 Tracking sense changes
The basic idea of our algorithm for tracking sense
changes is as follows. If a word undergoes a
sense change, this can be detected by comparing
its senses obtained from two different time pe-
riods. Since we aim to detect this change au-
tomatically, we require distributional representa-
tions corresponding to word senses for different
time periods. We, therefore, utilize the basic hy-
pothesis of unsupervised sense induction to in-
duce the sense clusters over various time periods
and then compare these clusters to detect sense
change. The basic premises of the ?unsupervised
sense induction? are briefly described below.
4.1 Unsupervised sense induction
We use the co-occurrence based graph clustering
framework introduced in (Biemann, 2006). The
algorithm proceeds in three basic steps. Firstly,
a co-occurrence graph is created for every target
word found in DT. Next, the neighbourhood/ego
graph is clustered using the Chinese Whispers
(CW) algorithm (see (McAuley and Leskovec,
2012) for similar approaches). The algorithm, in
particular, produces a set of clusters for each target
word by decomposing its open neighborhood. We
hypothesize that each different cluster corresponds
to a particular sense of the target word. For a de-
tailed description, the reader is referred to (Bie-
mann, 2011).
If a word undergoes sense change, this can be
detected by comparing the sense clusters obtained
from two different time periods by the algorithm
outlined above. For this purpose, we use statis-
tics from the DT corresponding to two different
time intervals, say tv
i
and tv
j
. We then run the
sense induction algorithm over these two different
datasets. Now, for a given word w that appears
in both the datasets, we get two different set of
clusters, say C
i
and C
j
. Without loss of gener-
ality, let us assume that our algorithm detects m
sense clusters for the word w in tv
i
and n sense
clusters in tv
j
. Let C
i
= {s
i1
, s
i2
, . . . , s
im
} and
C
j
= {s
j1
, s
j2
, . . . , s
jn
}, where s
kz
denotes z
th
sense cluster for word w during time interval tv
k
.
1022
We next describe our algorithm for detecting sense
change from these sets of sense clusters.
4.2 Split, join, birth and death
We hypothesize that word w can undergo sense
change from one time interval (tv
i
) to another
(tv
j
) as per one of the following scenarios:
Split A sense cluster s
iz
in tv
i
splits into two (or
more) sense clusters, s
jp
1
and s
jp
2
in tv
j
Join Two sense clusters s
iz
1
and s
iz
2
in tv
i
join to
make a single cluster s
jp
in tv
j
Birth A new sense cluster s
jp
appears in tv
j
,
which was absent in tv
i
Death A sense cluster s
iz
in tv
i
dies out and does
not appear in tv
j
To detect split, join, birth or death, we build an
(m+1)? (n+1) matrix I to capture the intersec-
tion between sense clusters of two different time
periods. The first m rows and n columns corre-
spond to the sense clusters in tv
i
and tv
j
espec-
tively. We append an additional row and column to
capture the fraction of words, which did not show
up in any of the sense clusters in another time in-
terval. So, an element I
kl
of the matrix
? 1 ? k ? m, 1 ? l ? n: denotes the frac-
tion of words in a newer sense cluster s
jl
,
that were also present in an older sense clus-
ter s
ik
.
? k = m + 1, 1 ? l ? n: denotes the fraction
of words in the sense cluster s
jl
, that were not
present in any of the m clusters in tv
i
.
? 1 ? k ? m, l = n + 1: denotes the fraction
of words in the sense cluster s
ik
, that did not
show up in any of the n clusters in tv
j
.
Thus, the matrix I captures all the four possible
scenarios for sense change. Since we can not
expect a perfect split, birth etc., we used certain
threshold values to detect if a candidate word is
undergoing sense change via one of these four
cases. In Figure 1, as an example, we illustrate
the birth of a new sense for the word ?compiler?.
4.3 Multi-stage filtering
To make sure that the candidate words obtained
via our algorithm are meaningful, we applied
multi-stage filtering to prune the candidate word
list. The following criterion were used for the fil-
tering:
Stage 1 We utilize the fact that the CW algorithm
is non-deterministic in nature. We apply CW
three times over the source and target time inter-
vals. We obtain the candidate word lists using our
algorithm for the three runs, then take the inter-
section to output those words, which came up in
all the three runs.
Stage 2 From the above list, we retain only those
candidate words, which have a part-of-speech tag
?NN? or ?NNS?, as we focus on nouns for this
work.
Stage 3 We sort the candidate list obtained in
Stage 2 as per their occurrence in the first time
period. Then, we remove the top 20% and the
bottom 20% words from this list. Therefore, we
consider the torso of the frequency distribution
which is the most informative part for this type
of an analysis.
5 Experimental framework
For our experiments, we utilized DTs created for
8 different time periods: 1520-1908, 1909-1953,
1954-1972, 1973-1986, 1987-1995, 1996-2001,
2002-2005 and 2006-2008 (Riedl et al, 2014).
The time periods were set such that the amount
of data in each time period is roughly the same.
We will also use T
1
to T
8
to denote these time pe-
riods. The parameters for CW clustering were set
as follows. The size of the neighbourhood (N )
to be clustered was set to 200. The parameter n
regulating the edge density in this neighbourhood
was set to 200 as well. The parameter a was set to
lin, which corresponds to favouring smaller clus-
ters by hub downweighing
2
. The threshold values
used to detect the sense changes were as follows.
For birth, at least 80% words of the target cluster
should be novel. For split, each split cluster should
have at least 30% words of the source cluster and
the total intersection of all the split clusters should
be > 80%. The same parameters were used for the
join and death case with the interchange of source
and target clusters.
5.1 Signals of sense change
Making comparisons between all the pairs of time
periods gave us 28 candidate words lists. For
2
data available at http://sf.net/p/jobimtext/
wiki/LREC2014_Google_DT/
1023
Figure 1: Example of the birth of a new sense for the word ?compiler?
each of these comparison, we applied the multi-
stage filtering to obtain the pruned list of candidate
words. Table 1 provides some statistics about the
number of candidate words obtained correspond-
ing to the birth case. The rows correspond to the
source time-period and the columns correspond to
the target time periods. An element of the table
shows the number of candidate words obtained
by comparing the corresponding source and target
time periods.
Table 1: Number of candidate birth senses be-
tween all time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
8
T
1
2498 3319 3901 4220 4238 4092 3578
T
2
1451 2330 2789 2834 2789 2468
T
3
917 1460 1660 1827 1815
T
4
517 769 1099 1416
T
5
401 818 1243
T
6
682 1107
T
7
609
The table clearly shows a trend. For most of
the cases, the number of candidate birth senses
tends to increase as we go from left to right. Sim-
ilarly, this number decreases as we go down in
the table. This is quite intuitive since going from
left to right corresponds to increasing the gap be-
tween two time periods while going down cor-
responds to decreasing this gap. As the gap in-
creases (decreases), one would expect more (less)
new senses coming in. Even while moving diago-
nally, the candidate words tend to decrease as we
move downwards. This corresponds to the fact
that the number of years in the time periods de-
creases as we move downwards, and therefore, the
gap also decreases.
5.2 Stability analysis & sense change location
Formally, we consider a sense change from tv
i
to tv
j
stable if it was also detected while com-
paring tv
i
with the following time periods tv
k
s.
This number of subsequent time periods, where
the same sense change is detected, helps us to de-
termine the age of a new sense. Similarly, for a
candidate sense change from tv
i
to tv
j
, we say that
the location of the sense change is tv
j
if and only
if that sense change does not get detected by com-
paring tv
i
with any time interval tv
k
, intermediate
between tv
i
and tv
j
.
Table 1 gives a lot of candidate words for sense
change. However, not all the candidate words
were stable. Thus, it was important to prune these
results using stability analysis. Also, it is to be
noted that these results do not pin-point to the ex-
act time-period, when the sense change might have
taken place. For instance, among the 4238 candi-
date birth sense detected by comparing T
1
and T
6
,
many of these new senses might have come up in
between T
2
to T
5
as well. We prune these lists fur-
ther based on the stability of the sense, as well as
to locate the approximate time interval, in which
the sense change might have occurred.
Table 2 shows the number of stable (at least
twice) senses as well as the number of stable
sense changes located in that particular time pe-
riod. While this decreases recall, we found this to
be beneficial for the accuracy of the method.
Once we were able to locate the senses as well
as to find the age of the senses, we attempted to
1024
Table 2: Number of candidate birth senses ob-
tained for different time periods
T
2
T
3
T
4
T
5
T
6
T
7
T
1
2498 3319 3901 4220 4238 4092
stable 537 989 1368 1627 1540 1299
located 537 754 772 686 420 300
T
2
1451 2330 2789 2834 2789
stable 343 718 938 963 810
located 343 561 517 357 227
select some representative words and plotted them
on a timeline as per the birth period and their age
in Figure 2. The source time period here is 1909-
1953.
6 Evaluation framework
During evaluation, we considered the clusters ob-
tained using the 1909-1953 time-slice as our refer-
ence and attempted to track sense change by com-
paring these with the clusters obtained for 2002-
2005. The sense change detected was categorized
as to whether it was a new sense (birth), a single
sense got split into two or more senses (split) or
two or more senses got merged (join) or a particu-
lar sense died (death). We present a few instances
of the resulting clusters in the paper and refer the
reader to the supplementary material
3
for the rest
of the results.
6.1 Manual evaluation
The algorithm detected a lot of candidate words
for the cases of birth, split/join as well as death.
Since it was difficult to go through all the candi-
date sense changes for all the comparisons man-
ually, we decided to randomly select some can-
didate words, which were flagged by our algo-
rithm as undergoing sense change, while compar-
ing 1909-1953 and 2002-2005 DT. We selected 48
random samples of candidate words for birth cases
and 21 random samples for split/join cases. One
of the authors annotated each of the birth cases
identifying whether or not the algorithm signalled
a true sense change while another author did the
same task for the split/join cases. The accuracy as
per manual evaluation was found to be 60.4% for
the birth cases and 57% for the split/join cases.
Table 3 shows the evaluation results for a few
candidate words, flagged due to birth. Columns
3
http://cse.iitkgp.ac.in/resgrp/cnerg/
acl2014_wordsense/
correspond to the candidate words, words obtained
in the cluster of each candidate word (we will use
the term ?birth cluster? for these words, hence-
forth), which indicated a new sense, the results
of manual evaluation as well as the possible sense
this birth cluster denotes.
Table 4 shows the corresponding evaluation re-
sults for a few candidate words, flagged due to
split or join.
A further analysis of the words marked due
to birth in the random samples indicates that
there are 22 technology-related words, 2 slangs,
3 economics related words and 2 general words.
For the split-join case we found that there are
3 technology-related words while the rest of the
words are general. Therefore one of the key ob-
servations is that most of the technology related
words (where the neighborhood is completely
new) could be extracted from our birth results. In
contrast, for the split-join instances most of the re-
sults are from the general category since the neigh-
borhood did not change much here; it either got
split or merged from what it was earlier.
6.2 Automated evaluation with WordNet
In addition to manual evaluation, we also per-
formed automated evaluation for the candidate
words. We chose WordNet for automated evalua-
tion because not only does it have a wide coverage
of word senses but also it is being maintained and
updated regularly to incorporate new senses. We
did this evaluation for the candidate birth, join and
split sense clusters obtained by comparing 1909-
1953 time period with respect to 2002-2005. For
our evaluation, we developed an aligner to align
the word clusters obtained with WordNet senses.
The aligner constructs a WordNet dictionary for
the purpose of synset algnment. The CW clus-
ter is then aligned to WordNet synsets by compar-
ing the clusters with WordNet graph and the synset
with the maximum alignment score is returned as
the output. In summary, the aligner tool takes as
input the CW cluster and returns a WordNet synset
id that corresponds to the cluster words. The eval-
uation settings were as follows:
Birth: For a candidate word flagged as birth, we
first find out the set of all WordNet synset ids for
its CW clusters in the source time period (1909-
1953 in this case). Let S
init
denote the union of
these synset ids. We then find WordNet synset id
for its birth-cluster, say s
new
. Then, if s
new
/?
1025
Figure 2: Examples of birth senses placed on a timeline as per their location as well as age
Table 3: Manual evaluation for seven randomly chosen candidate birth clusters between time periods
1909-1953 and 2002-2005
Sl Candidate birth cluster Evaluation judgement,
No. Word comments
1 implant gel, fibre, coatings, cement, materials, metal, filler No, New set of words but
silicone, composite, titanium, polymer, coating similar sense already existed
2 passwords browsers, server, functionality, clients, workstation Yes, New sense related
printers, software, protocols, hosts, settings, utilities to ?a computer sense?
3 giants multinationals, conglomerates, manufacturers Yes, New sense as ?an
corporations, competitors, enterprises, companies organization with very great
businesses, brands, firms size or force?
4 donation transplantation, donation, fertilization, transfusions Yes, The new usage of donation
transplant, transplants, insemination, donors, donor ... associated with body organs etc.
5 novice negro, fellow, emigre, yankee, realist, quaker, teen No, this looks like a false
male, zen, lady, admiring, celebrity, thai, millionaire ... positive
6 partitions server, printers, workstation, platforms, arrays Yes, New usage related to
modules, computers, workstations, kernel ... the ?computing? domain
7 yankees athletics, cubs, tigers, sox, bears, braves, pirates Yes, related to the ?New
cardinals, dodgers, yankees, giants, cardinals ... York Yankees? team
S
init
, it implies that this is a new sense that was
not present in the source clusters and we call it a
?success? as per WordNet.
Join: For the join case, we find WordNet synset
ids s
1
and s
2
for the clusters obtained in the
source time period and s
new
for the join cluster
in the target time period. If s
1
6= s
2
and s
new
is
either s
1
or s
2
, we call it a ?success?.
Split: For the split case, we find WordNet synset
id s
old
for the source cluster and synset ids s
1
and s
2
for the target split clusters. If s
1
6= s
2
and either s
1
, or s
2
retains the id s
old
, we call it a
?success?.
Table 5 show the results of WordNet based eval-
uation. In case of birth we observe a success of
Table 5: Results of the automatic evaluation using
WordNet
Category No. of Candidate Words Success Cases
Birth 810 44%
Split 24 46%
Join 28 43%
44% while for split and join we observe a success
of 46% and 43% respectively. We then manually
verified some of the words that were deemed as
successes, as well as investigated WordNet sense
they were mapped to. Table 6 shows some of the
words for which the evaluation detected success
along with WordNet senses. Clearly, the cluster
words correspond to a newer sense for these words
1026
Table 4: Manual evaluation for five randomly chosen candidate split/join clusters between time periods
1909-1953 and 2002-2005
Sl Candidate Source and target clusters
No. Word
1 intonation S: whisper, glance, idioms, gesture, chant, sob, inflection, diction, sneer, rhythm, accents ...
(split) T
1
: nod, tone, grimace, finality, gestures, twang, shake, shrug, irony, scowl, twinkle ...
T
2
: accents, phrase, rhythm, style, phonology, diction, utterance, cadence, harmonies ...
Yes, T
1
corresponds to intonation in normal conversations while T
2
corresponds to the use of accents in
formal and research literature
2 diagonal S: coast, edge, shoreline, coastline, border, surface, crease, edges, slope, sides, seaboard ...
(split) T
1
: circumference, center, slant, vertex, grid, clavicle, margin, perimeter, row, boundary ..
T
2
: border, coast, seaboard, seashore, shoreline, waterfront, shore, shores, coastline, coasts
Yes, the split T
1
is based on mathematics where as T
2
is based on geography
3 mantra S
1
: sutra, stanza, chanting, chants, commandments, monologue, litany, verse, verses ...
(join) S
2
: praise, imprecation, benediction, praises, curse, salutation, benedictions, eulogy ...
T : blessings, spell, curses, spells, rosary, prayers, blessing, prayer, benediction ...
Yes, the two seemingly distinct senses of mantra - a contextual usage for chanting and prayer (S
1
)
and another usage in its effect - salutations, benedictions (S
2
) have now merged in T .
4 continuum S: circumference, ordinate, abscissa, coasts, axis, path, perimeter, arc, plane axis ...
(split) T
1
: roadsides, corridors, frontier, trajectories, coast, shore, trail, escarpment, highways ...
T
2
: arc, ellipse, meridians, equator, axis, axis, plane, abscissa, ordinate, axis, meridian ....
Yes, the split S
1
denotes the usage of ?continuum? with physical objects while the
the split S
2
corresponds to its usages in mathematics domain.
5 headmaster S
1
: master, overseer, councillor, chancellor, tutors, captain, general, principal ...
(join) S
2
: mentor, confessor, tutor, founder, rector, vicar, graduate, counselor, lawyer ...
T : chaplain, commander, surveyor, coordinator, consultant, lecturer, inspector ...
No, it seems a false positive
and the mapped WordNet synset matches the birth
cluster to a very high degree.
6.3 Evaluation with a slang list
Slangs are words and phrases that are regarded as
very informal, and are typically restricted to a par-
ticular context. New slang words come up every
now and then, and this plays an integral part in the
phenomena of sense change. We therefore decided
to perform an evaluation as to how many slang
words were being detected by our candidate birth
clusters. We used a list of slangs available from
the slangcity website
4
. We collected slangs for the
years 2002-2005 and found the intersection with
our candidate birth words. Note that the website
had a large number of multi-word expressions that
we did not consider in our study. Further, some
of the words appeared as either erroneous or very
transient (not existing more than a few months) en-
tires, which had to be removed from the list. All
these removal left us with a very little space for
comparison; however, despite this we found 25
slangs from the website that were present in our
birth results, e.g. ?bum?, ?sissy?, ?thug?, ?dude? etc.
4
http://slangcity.com/email_archive/
index_2003.htm
6.4 Evaluation of candidate death clusters
Much of our evaluation was focussed on the birth
sense clusters, mainly because these are more in-
teresting from a lexicographic perspective. Addi-
tionally, the main theme of this work was to de-
tect new senses for a given word. To detect a
true death of a sense, persistence analysis was re-
quired, that is, to verify if the sense was persist-
ing earlier and vanished after a certain time period.
While such an analysis goes beyond the scope of
this paper, we selected some interesting candidate
?death? senses. Table 7 shows some of these inter-
esting candidate words, their death cluster along
with the possible vanished meaning, identified by
the authors. While these words are still used in a
related sense, the original meaning does not exist
in the modern usage.
7 Conclusions
In this paper, we presented a completely unsu-
pervised method to detect word sense changes
by analyzing millions of digitized books archived
spanning several centuries. In particular, we con-
structed DT networks over eight different time
windows, clustered these networks and compared
these clusters to identify the emergence of novel
1027
Table 6: Example of randomly chosen candidate birth clusters mapped to WordNet
Sl Candidate birth cluster Synset Id,
No. Word WordNet sense
1 macro code, query, handler, program, procedure, subroutine 6582403, a set sequence of steps,
module, script part of larger computer program
2 caller browser, compiler, sender, routers, workstation, cpu 4175147, a computer that
host, modem, router, server provides client stations with access to files
3 searching coding, processing, learning, computing, scheduling 1144355, programming: setting an
planning, retrieval, routing, networking, navigation order and time for planned events
4 hooker bitch, whore, stripper, woman slut, prostitute 10485440, a woman who
girl, dancer ... engages in sexual intercourse for money
5 drones helicopters, fighters, rockets, flights, planes 4264914, a craft capable of
vehicles, bomber, missions, submarines ... traveling in outer space
6 amps inverters, capacitor, oscillators, switches, mixer 2955247, electrical device characterized
transformer, windings, capacitors, circuits ... by its capacity to store an electric charge
7 compilers interfaces, algorithms, programming, software 6566077, written programs pertaining
modules, libraries, routines, tools, utilities ... to the operation of a computer system
Table 7: Some representative examples for candidate death sense clusters
Sl Candidate death cluster Vanished meaning
No. Word
1 slop jeans, velveteen, tweed, woollen, rubber, sealskin, wear clothes and bedding supplied to
oilskin, sheepskin, velvet, calico, deerskin, goatskin, cloth ... sailors by the navy
2 blackmail subsidy, rent, presents, tributes, money, fine, bribes Origin: denoting protection money
dues, tolls, contributions, contribution, customs, duties ... levied by Scottish chiefs
3 repertory dictionary, study, compendium, bibliography, lore, directory Origin: denoting an index
catalogues, science, catalog, annals, digest, literature ... or catalog: from late Latin repertorium
4 phrasing contour, outline, construction, handling, grouping, arrangement in the sense ?style or manner of
structure, modelling, selection, form ... expression?: via late Latin Greek phrasis
senses. The performance of our method has been
evaluated manually as well as by comparison with
WordNet and a list of slang words. Through man-
ual evaluation we found that the algorithm could
correctly identify 60.4% birth cases from a set of
48 random samples and 57% split/join cases from
a set of 21 randomly picked samples. Quite strik-
ingly, we observe that (i) in 44% cases the birth of
a novel sense is attested by WordNet, (ii) in 46%
cases the split of an older sense is signalled on
comparison with WordNet and (iii) in 43% cases
the join of two senses is attested by WordNet.
These results might have strong lexicographic im-
plications ? even if one goes by very moderate es-
timates almost half of the words would be candi-
date entries in WordNet if they were not already
part of it. This method can be extremely useful
in the construction of lexico-semantic networks
for low-resource languages, as well as for keeping
lexico-semantic resources up to date in general.
Future research directions based on this work
are manifold. On one hand, our method can be
used by lexicographers in designing new dictio-
naries where candidate new senses can be semi-
automatically detected and included, thus greatly
reducing the otherwise required manual effort.
On the other hand, this method can be directly
used for various NLP/IR applications like seman-
tic search, automatic word sense discovery as well
as disambiguation. For semantic search, taking
into account the newer senses of the word can in-
crease the relevance of the query result. Similarly,
a disambiguation engine informed with the newer
senses of a word can increase the efficiency of
disambiguation, and recognize senses uncovered
by the inventory that would otherwise have to be
wrongly assigned to covered senses. In addition,
this method can be also extended to the ?NNP?
part-of-speech (i.e., named entities) to identify
changes in role of a person/place. Furthermore,
it would be interesting to apply this method to lan-
guages other than English and to try to align new
senses of cognates across languages.
Acknowledgements
AM would like to thank DAAD for supporting the
faculty exchange programme to TU Darmstadt.
PG would like to thank Google India Private Ltd.
for extending travel support to attend the confer-
ence. MR and CB have been supported by an IBM
SUR award and by LOEWE as part of the research
center Digital Humanities.
1028
References
J. Allan, R. Papka and V. Lavrenko. 1998. On-line
new event detection and tracking. In proceedings of
SIGIR, 37?45, Melbourne, Australia.
D. Bamman and G. Crane. 2011. Measuring Historical
Word Sense Variation. In proceedings of JCDL, 1?
10, New York, NY, USA.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, 73?80, New York, USA.
C. Biemann. 2011. Structure Discovery in Natural
Language. Springer Heidelberg Dordrecht London
New York. ISBN 978-3-642-25922-7.
D. Blei and J. Lafferty. 2006. Dynamic topic mod-
els. In proceedings of ICML, 113?120, Pittsburgh,
Pennsylvania.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayash and K. Kanzaki. 2009. Enhancing the
Japanese WordNet. In proceedings of workshop on
Asian Language Resources, 1?8, Suntec, Singapore.
P. Cook, J. H. Lau, M. Rundell, D. McCarthy, T. Bald-
win. 2013. A lexicographic appraisal of an auto-
matic approach for detecting new word senses. In
proceedings of eLex, 49-65, Tallinn, Estonia.
Y. Goldberg and J. Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In proceedings of the Joint
Conference on Lexical and Computational Seman-
tics (*SEM), 241?247, Atlanta, GA, USA.
G. Heyer, F. Holz and S. Teresniak. 2009. Change of
topics over time ? tracking topics by their change of
meaning. In proceedings of KDIR, Madeira, Portu-
gal.
N. Ide and J. Veronis. 1998. Introduction to the special
issue on word sense disambiguation: The state of the
art. Computational Linguistics, 24(1):1?40.
A. Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004. The sketch engine. In Proceedings of EU-
RALEX, 105?116, Lorient, France.
A. Kilgarriff and D. Tugwell. 2001. Word sketch: Ex-
traction and display of significant collocations for
lexicography. In proceedings of COLLOCATION:
Computational Extraction, Analysis and Exploita-
tion, 32?38, Toulouse, France.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In pro-
ceedings of ACL/EACL, 64?71, Madrid, Spain.
V. Loreto, A. Mukherjee and F. Tria. 2012. On the ori-
gin of the hierarchy of color names. PNAS, 109(18),
6819?6824.
S. K. Maity, T. M. Venkat and A. Mukherjee. 2012.
Opinion formation in time-varying social networks:
The case of the naming game. Phys. Rev. E, 86,
036110.
J. McAuley and J. Leskovec. 2012. Learning to dis-
cover social circles in ego networks. In proceedings
of NIPS, 548?556, Nevada, USA.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S. Pinker, M. A. Nowak and E. L. Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science, 331(6014):176?182.
R. Mihalcea and V. Nastase. 2012. Word epoch disam-
biguation: finding how words change over time. In
proceedings of ACL, 259?263, Jeju Island, Korea.
A. Mukherjee, F. Tria, A. Baronchelli, A. Puglisi and V.
Loreto. 2011. Aging in language dynamics. PLoS
ONE, 6(2): e16677.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
P. P?a?akk?o and K. Lind?en. 2012. Finding a location
for a new word in WordNet. In proceedings of the
Global WordNet Conference, Matsue, Japan.
M. Riedl and C. Biemann. 2013. Scaling to large
3
data: An efficient and effective method to compute
distributional thesauri. In proceedings of EMNLP,
884?890, Seattle, Washington, USA.
M. Riedl, R. Steuer and C. Biemann. 2014. Distributed
distributional similarities of Google books over the
centuries. In proceedings of LREC, Reykjavik, Ice-
land.
P. Rychl?y and A. Kilgarriff. 2007. An efficient al-
gorithm for building a distributional thesaurus (and
other sketch engine developments). In proceedings
of ACL, poster and demo sessions, 41?44, Prague,
Czech Republic.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
K. Sp?ark-Jones. 1986. Synonymy and Semantic Clas-
sification. Edinburgh University Press. ISBN 0-
85224-517-3.
N. Tahmasebi, T. Risse and S. Dietze. 2011. Towards
automatic language evolution tracking: a study on
word sense tracking. In proceedings of EvoDyn, vol.
784, Bonn, Germany.
X. Wang and A. McCallum. 2006. Topics over time:
a non-Markov continuous-time model of topical
trends. In proceedings of KDD, 424?433, Philadel-
phia, PA, USA.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In proceed-
ings of the workshop on Detecting and Exploiting
Cultural Diversity on the Social Web, 35?40, Glas-
gow, Scotland, UK.
1029
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25?33,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Novel Two-stage Framework for Extracting Opinionated Sentences
from News Articles
Pujari Rajkumar
1
, Swara Desai
2
, Niloy Ganguly
1
and Pawan Goyal
1
1
Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India ? 721302
2
Yahoo! India
1
rajkumarsaikorian@gmail.com, {niloy,pawang}@cse.iitkgp.ernet.in
2
swara@yahoo-inc.com
Abstract
This paper presents a novel two-stage
framework to extract opinionated sentences
from a given news article. In the first stage,
Na??ve Bayes classifier by utilizing the local
features assigns a score to each sentence
- the score signifies the probability of the
sentence to be opinionated. In the second
stage, we use this prior within the HITS
(Hyperlink-Induced Topic Search) schema to
exploit the global structure of the article and
relation between the sentences. In the HITS
schema, the opinionated sentences are treated
as Hubs and the facts around these opinions
are treated as the Authorities. The algorithm
is implemented and evaluated against a set of
manually marked data. We show that using
HITS significantly improves the precision
over the baseline Na??ve Bayes classifier.
We also argue that the proposed method
actually discovers the underlying structure of
the article, thus extracting various opinions,
grouped with supporting facts as well as other
supporting opinions from the article.
1 Introduction
With the advertising based revenues becoming the main
source of revenue, finding novel ways to increase
focussed user engagement has become an important
research topic. A typical problem faced by web
publishing houses like Yahoo!, is understanding the
nature of the comments posted by readers of 10
5
articles posted at any moment on its website. A lot
of users engage in discussions in the comments section
of the articles. Each user has a different perspective
and thus comments in that genre - this many a times,
results in a situation where the discussions in the
comment section wander far away from the articles
topic. In order to assist users to discuss relevant points
in the comments section, a possible methodology can
be to generate questions from the article?s content that
seek user?s opinions about various opinions conveyed
in the article (Rokhlenko and Szpektor, 2013). It
would also direct the users into thinking about a
spectrum of various points that the article covers
and encourage users to share their unique, personal,
daily-life experience in events relevant to the article.
This would thus provide a broader view point for
readers as well as perspective questions can be created
thus catering to users with rich user generated content,
this in turn can increase user engagement on the article
pages. Generating such questions manually for huge
volume of articles is very difficult. However, if one
could identify the main opinionated sentences within
the article, it will be much easier for an editor to
generate certain questions around these. Otherwise, the
sentences themselves may also serve as the points for
discussion by the users.
Hence, in this paper we discuss a two-stage
algorithm which picks opinionated sentences from
the articles. The algorithm assumes an underlying
structure for an article, that is, each opinionated
sentence is supported by a few factual statements that
justify the opinion. We use the HITS schema to
exploit this underlying structure and pick opinionated
sentences from the article.
The main contribtutions of this papers are as follows.
First, we present a novel two-stage framework for
extracting opinionated sentences from a news article.
Secondly, we propose a new evaluation metric that
takes into account the fact that since the amount
of polarity (and thus, the number of opinionated
sentences) within documents can vary a lot and thus,
we should stress on the ratio of opinionated sentences
in the top sentences, relative to the ratio of opinionated
sentences in the article. Finally, discussions on how the
proposed algorithm captures the underlying structure
of the opinions and surrounding facts in a news article
reveal that the algorithm does much more than just
extracting opinionated sentences.
This paper has been organised as follows. Section
2 discusses related work in this field. In section 3, we
discuss our two-stage model in further details. Section
4 discusses the experimental framework and the results.
Further discussions on the underlying assumption
behind using HITS along with error analysis are carried
out in Section 5. Conclusions and future work are
detailed in Section 6.
2 Related Work
Opinion mining has drawn a lot of attention in recent
years. Research works have focused on mining
25
opinions from various information sources such as
blogs (Conrad and Schilder, 2007; Harb et al., 2008),
product reviews (Hu and Liu, 2004; Qadir, 2009; Dave
et al., 2003), news articles (Kim and Hovy, 2006;
Hu and Liu, 2006) etc. Various aspects in opinion
mining have been explored over the years (Ku et
al., 2006). One important dimension is to identify
the opinion holders as well as opinion targets. (Lu,
2010) used dependency parser to identify the opinion
holders and targets in Chinese news text. (Choi
et al., 2005) use Conditional Random Fields to
identify the sources of opinions from the sentences.
(Kobayashi et al., 2005) propose a learning based
anaphora resolution technique to extract the opinion
tuple < Subject, Attribute, V alue >. Opinion
summarization has been another important aspect (Kim
et al., 2013).
A lot of research work has been done for opinion
mining from product reviews where most of the text
is opinion-rich. Opinion mining from news articles,
however, poses its own challenges because in contrast
with the product reviews, not all parts of news articles
present opinions (Balahur et al., 2013) and thus finding
opinionated sentences itself remains a major obstacle.
Our work mainly focus on classifying a sentence in a
news article as opinionated or factual. There have been
works on sentiment classification (Wiebe and Riloff,
2005) but the task of finding opinionated sentences is
different from finding sentiments, because sentiments
mainly convey the emotions and not the opinions.
There has been research on finding opinionated
sentences from various information sources. Some
of these works utilize a dictionary-based (Fei et al.,
2012) or regular pattern based (Brun, 2012) approach
to identify aspects in the sentences. (Kim and Hovy,
2006) utilize the presence of a single strong valence
wors as well as the total valence score of all words in
a sentence to identify opinion-bearing sentences. (Zhai
et al., 2011) work on finding ?evaluative? sentences in
online discussions. They exploit the inter-relationship
of aspects, evaluation words and emotion words to
reinforce each other.
Thus, while ours is not the first attempt at
opinion extraction from news articles, to the best
of our knowledge, none of the previous works has
exploited the global structure of a news article to
classify a sentence as opinionated/factual. Though
summarization algorithms (Erkan and Radev, 2004;
Goyal et al., 2013) utilize the similarity between
sentences in an article to find the important sentences,
our formulation is different in that we conceptualize
two different kinds of nodes in a document, as opposed
to the summarization algorithms, which treat all the
sentences equally.
In the next section, we describe the propsoed
two-stage algorithm in detail.
3 Our Approach
Figure 1 gives a flowchart of the proposed two-stage
method for extracting opinionated sentences from news
articles. First, each news article is pre-processed to
get the dependency parse as well as the TF-IDF vector
corresponding to each of the sentences present in the
article. Then, various features are extracted from
these sentences which are used as input to the Na??ve
Bayes classifier, as will be described in Section 3.1.
The Na??ve Bayes classifier, which corresponds to the
first-stage of our method, assigns a probability score
to each sentence as being an opinionated sentence.
In the second stage, the entire article is viewed as a
complete and directed graph with edges from every
sentence to all other sentences, each edge having a
weight suitably computed. Iterative HITS algorithm
is applied to the sentence graph, with opinionated
sentences conceptualized as hubs and factual sentences
conceptualized as authorities. The two stages of our
approach are detailed below.
3.1 Na??ve Bayes Classifier
The Na??ve Bayes classifier assigns the probability for
each sentence being opinionated. The classifier is
trained on 70 News articles from politics domain,
sentences of which were marked by a group
of annotators as being opinionated or factual.
Each sentence was marked by two annotators.
The inter-annotator agreement using Cohen?s kappa
coefficient was found to be 0.71.
The features utilized for the classifier are detailed
in Table 1. These features were adapted from those
reported in (Qadir, 2009; Yu and Hatzivassiloglou,
2003). A list of positive and negative polar words,
further expanded using wordnet synsets was taken
from (Kim and Hovy, 2005). Stanford dependency
parser (De Marneffe et al., 2006) was utilized to
compute the dependencies for each sentence within the
news article.
After the features are extracted from the sentences,
we used the Weka implementation of Na??ve Bayes to
train the classifier
1
.
Table 1: Features List for the Na??ve Bayes Classifier
1. Count of positive polar words
2. Count of negative polar words
3. Polarity of the root verb of the sentence
4. Presence of aComp, xComp and advMod
dependencies in the sentence
3.2 HITS
The Na??ve Bayes classifier as discussed in Section 3.1
utilizes only the local features within a sentence. Thus,
the probability that a sentence is opinionated remains
1
http://www.cs.waikato.ac.nz/ml/weka/
26
Figure 1: Flow Chart of Various Stages in Our Approach
independent of its context as well as the document
structure. The main motivation behind formulating
this problem in HITS schema is to utilize the hidden
link structures among sentences. HITS stands for
?Hyperlink-Induced Topic Search?; Originally, this
algorithm was developed to rank Web-pages, with a
particular insight that some of the webpages (Hubs)
served as catalog of information, that could lead users
directly to the other pages, which actually contained
the information (Authorities).
The intuition behind applying HITS for the task of
opinion extraction came from the following assumption
about underlying structure of an article. A news article
pertains to a specific theme and with that theme in
mind, the author presents certain opinions. These
opinions are justified with the facts present in the article
itself. We conceptualize the opinionated sentences
as Hubs and the associated facts for an opinionated
sentence as Authorities for this Hub.
To describe the formulation of HITS parameters,
let us give the notations. Let us denote a document
D using a set of sentences {S
1
, S
2
, . . . , S
i
, . . . , S
n
},
where n corresponds to the number of sentences in
the document D. We construct the sentence graph
where nodes in the graph correspond to the sentences
in the document. Let H
i
and A
i
denote the hub
and authority scores for sentence S
i
. In HITS, the
edges always flow from a Hub to an Authority. In
the original HITS algorithm, each edge is given the
same weight. However, it has been reported that using
weights in HITS update improves the performance
significantly (Li et al., 2002). In our formulation,
since each node has a non-zero probablility of acting
as a hub as well as an authority, we have outgoing as
well as incoming edges for every node. Therefore, the
weights are assigned, keeping in mind the proximity
between sentences as well as the probability (of being
opinionated/factual) assigned by the classifier. The
following criteria were used for deciding the weight
function.
? An edge in the HITS graph goes from a hub
(source node) to an authority (target node). So, the
edge weight from a source node to a target node
should be higher if the source node has a high hub
score.
? A fact corresponding to an opinionated sentence
should be discussing the same topic. So, the edge
weight should be higher if the sentences are more
similar.
? It is more probable that the facts around an
opinion appear closer to that opinionated sentence
in the article. So, the edge weight from a source to
target node decreases as the distance between the
two sentences increases.
Let W be the weight matrix such that W
ij
denotes
the weight for the edge from the sentence S
i
to the
sentence S
j
. Based on the criteria outlined above, we
formulate that the weight W
ij
should be such that
W
ij
? H
i
W
ij
? Sim
ij
W
ij
?
1
dist
ij
where we use cosine similarity between the sentence
vectors to compute Sim
ij
. dist
ij
is simply the number
27
of sentences separating the source and target node.
Various combinations of these factors were tried and
will be discussed in section 4. While factors like
sentence similarity and distance are symmetric, having
the weight function depend on the hub score makes it
asymmetric, consistent with the basic idea of HITS.
Thus, an edge from the sentence S
i
to S
j
is given
a high weight if S
i
has a high probability score of
being opinionated (i.e., acting as hub) as obtained the
classifier.
Now, for applying the HITS algorithm iteratively,
the Hubs and Authorities scores for each sentence
are initialized using the probability scores assigned
by the classifier. That is, if P
i
(Opinion) denotes
the probability that S
i
is an opinionated sentence as
per the Na??ve Bayes Classifier, H
i
(0) is initialized
to P
i
(Opinion) and A
i
(0) is initialized to 1 ?
P
i
(Opinion). The iterative HITS is then applied as
follows:
H
i
(k) = ?
j
W
ij
A
i
(k ? 1) (1)
A
i
(k) = ?
j
W
ji
H
i
(k ? 1) (2)
where H
i
(k) denote the hub score for the i
th
sentence during the k
th
iteration of HITS. The iteration
is stopped once the mean squared error between the
Hub and Authority values at two different iterations
is less than a threshold . After the HITS iteration is
over, five sentences having the highest Hub scores are
returned by the system.
4 Experimental Framework and Results
The experiment was conducted with 90 news articles in
politics domain from Yahoo! website. The sentences
in the articles were marked as opinionated or factual
by a group of annotators. In the training set, 1393
out of 3142 sentences were found to be opinianated.
In the test set, 347 out of 830 sentences were marked
as opinionated. Out of these 90 articles, 70 articles
were used for training the Na??ve Bayes classifier as
well as for tuning various parameters. The rest 20
articles were used for testing. The evaluation was
done in an Information Retrieval setting. That is, the
system returns the sentences in a decreasing order of
their score (or probability in the case of Na??ve Bayes)
as being opinionated. We then utilize the human
judgements (provided by the annotators) to compute
precision at various points. Let op(.) be a binary
function for a given rank such that op(r) = 1 if the
sentence returned as rank r is opinionated as per the
human judgements.
A P@k precision is calculated as follows:
P@k =
?
k
r=1
op(r)
k
(3)
While the precision at various points indicates how
reliable the results returned by the system are, it
does not take into account the fact that some of the
documents are opinion-rich and some are not. For
the opinion-rich documents, a high P@k value might
be similar to picking sentences randomly, whereas for
the documents with a very few opinions, even a lower
P@k value might be useful. We, therefore, devise
another evaluation metric M@k that indicates the ratio
of opinionated sentences at any point, normalized with
respect to the ratio of opinionated sentences in the
article.
Correspondingly, an M@k value is calculated as
M@k =
P@k
Ratio
op
(4)
where Ratio
op
denotes the fraction of opinionated
sentences in the whole article. Thus
Ratio
op
=
Number of opinionated sentences
Number of sentences
(5)
The parameters that we needed to fix for the HITS
algorithm were the weight function W
ij
and the
threshold  at which we stop the iteration. We varied
 from 0.0001 to 0.1 multiplying it by 10 in each step.
The results were not sensitive to the value of  and
we used  = 0.01. For fixing the weight function,
we tried out various combinations using the criteria
outlined in Section 3.2. Various weight functions and
the corresponding P@5 and M@5 scores are shown in
Table 2. Firstly, we varied k in Sim
ij
k
and found that
the square of the similarity function gives better results.
Then, keeping it constant, we varied l in H
i
l
and found
the best results for l = 3. Then, keeping both of these
constants, we varied ? in (? +
1
d
). We found the best
results for ? = 1.0. With this ?, we tried to vary l again
but it only reduced the final score. Therefore, we fixed
the weight function to be
W
ij
= H
i
3
(0)Sim
ij
2
(1 +
1
dist
ij
) (6)
Note that H
i
(0) in Equation 6 corresponds to the
probablity assigned by the classifier that the sentence
S
i
is opinionated.
We use the classifier results as the baseline for the
comparisons. The second-stage HITS algorithm is
then applied and we compare the performance with
respect to the classifier. Table 3 shows the comparison
results for various precision scores for the classifier
and the HITS algorithm. In practical situation, an
editor requires quick identification of 3-5 opinionated
sentences from the article, which she can then use to
formulate questions. We thus report P@k and M@k
values for k = 3 and k = 5.
From the results shown in Table 3, it is clear
that applying the second-stage HITS over the Na??ve
Bayes Classifier improves the performance by a large
degree, both in term of P@k and M@k. For
instance, the first-stage NB Classifier gives a P@5 of
0.52 and P@3 of 0.53. Using the classifier outputs
during the second-stage HITS algorithm improves the
28
Table 2: Average P@5 and M@5 scores: Performance
comparison between various functions for W
ij
Function P@5 M@5
Sim
ij
0.48 0.94
Sim
2
ij
0.57 1.16
Sim
3
ij
0.53 1.11
Sim
2
ij
H
i
0.6 1.22
Sim
2
ij
H
i
2
0.61 1.27
Sim
2
ij
H
i
3
0.61 1.27
Sim
2
ij
H
i
4
0.58 1.21
Sim
2
ij
H
i
3
1
d
0.56 1.20
Sim
2
ij
H
i
3
(0.2 +
1
d
) 0.60 1.25
Sim
2
ij
H
i
3
(0.4 +
1
d
) 0.61 1.27
Sim
2
ij
H
i
3
(0.6 +
1
d
) 0.62 1.31
Sim
2
ij
H
i
3
(0.8 +
1
d
) 0.62 1.31
Sim
2
ij
H
i
3
(1 +
1
d
) 0.63 1.33
Sim
2
ij
H
i
3
(1.2 +
1
d
) 0.61 1.28
Sim
2
ij
H
i
2
(1 +
1
d
) 0.6 1.23
Table 3: Average P@5, M@5, P@3 and M@3 scores:
Performance comparison between the NB classifier and
HITS
System P@5 M@5 P@3 M@3
NB Classifier 0.52 1.13 0.53 1.17
HITS 0.63 1.33 0.72 1.53
Imp. (%) +21.2 +17.7 +35.8 +30.8
preformance by 21.2% to 0.63 in the case of P@5. For
P@3, the improvements were much more significant
and a 35.8% improvement was obtained over the NB
classifier. M@5 and M@3 scores also improve by
17.7% and 30.8% respectively.
Strikingly, while the classifier gave nearly the same
scores for P@k and M@k for k = 3 and k = 5,
HITS gave much better results for k = 3 than k = 5.
Specially, the P@3 andM@3 scores obtained by HITS
were very encouraging, indicating that the proposed
approach helps in pushing the opinionated sentences to
the top. This clearly shows the advantage of using the
global structure of the document in contrast with the
features extracted from the sentence itself, ignoring the
context.
Figures 2 and 3 show the P@5, M@5, P@3 and
M@3 scores for individual documents as numbered
from 1 to 20 on the X-axis. The articles are
sorted as per the ratio of P@5 (and M@5) obtained
using the HITS and NB classifier. Y-axis shows the
corresponding scores. Two different lines are used to
represent the results as returned by the classifier and
the HITS algorithm. A dashed line denotes the scores
obtained by HITS while a continuous line denotes
the scores obtained by the NB classifier. A detailed
analysis of these figures can help us draw the following
conclusions:
? For 40% of the articles (numbered 13 to 20) HITS
improves over the baseline NB classifier. For
40% of the articles (numbered 5 to 12) the results
provided by HITS were the same as that of the
baseline. For 20% of the articles (numbered 1 to
4) HITS gives a performance lower than that of
the baseline. Thus, for 80% of the documents, the
second-stage performs at least as good as the first
stage. This indicates that the second-stage HITS
is quite robust.
? M@5 results are much more robust for the HITS,
with 75% of the documents having anM@5 score
> 1. AnM@k score> 1 indicates that the ratio of
opinionated sentences in top k sentences, picked
up by the algorithm, is higher than the overall ratio
in the article.
? For 45% of the articles, (numbered 6, 9 ? 11 and
15? 20), HITS was able to achieve a P@3 = 1.0.
Thus, for these 9 articles, the top 3 sentences
picked up by the algorithm were all marked as
opinionated.
The graphs also indicate a high correlation between
the results obtained by the NB classifier and HITS.
We used Pearson?s correlation to find the correlation
strength. For the P@5 values, the correlation was
found to be 0.6021 and for the M@5 values, the
correlation was obtained as 0.5954.
In the next section, we will first attempt to further
analyze the basic assumption behind using HITS,
by looking at some actual Hub-Authority structures,
captured by the algorithm. We will also take some
cases of failure and perform error analysis.
5 Discussion
First point that we wanted to verify was, whether
HITS is really capturing the underlying structure of
the document. That is, are the sentences identified as
authorities for a given hub really correspond to the facts
supporting the particular opinion, expressed by the hub
sentence.
Figure 4 gives two examples of the Hub-Authority
structure, as captured by the HITS algorithm, for two
different articles. For each of these examples, we show
the sentence identified as Hub in the center along with
the top four sentences, identified as Authorities for that
hub. We also give the annotations as to whether the
sentences were marked as ?opinionated? or ?factual? by
the annotators.
In both of these examples, the hubs were
actually marked as ?opinionated? by the annotators.
Additionally, we find that all the four sentences,
identified as authorities to the hub, are very relevant to
the opinion expressed by the hub. In the first example,
top 3 authority sentences are marked as ?factual? by the
annotator. Although the fourth sentence is marked as
?opinionated?, it can be seen that this sentence presents
a supporting opinion for the hub sentence.
While studying the second example, we found that
while the first authority does not present an important
fact, the fourth authority surely does. Both of these
29
(a) Comparison of P@5 values (b) Comparison of M@5 values
Figure 2: Comparison Results for 20 Test articles between the Classifier and HITS: P@5 and M@5
(a) Comparison of P@3 values
(b) Comparison of M@3 values
Figure 3: Comparison Results for 20 Test articles between the Classifier and HITS: P@3 and M@3
(a) Hub-Authority Structure: Example 1
(b) Hub-Authority Structure: Example 2
Figure 4: Example from two different test articles capturing the Hub-Authority Structure
were marked as ?factual? by the annotators. In this
particular example, although the second and third
authority sentences were annotated as ?opinionated?,
these can be seen as supporting the opinion expressed
by the hub sentence. This example also gives us
an interesting idea to improve diversification in the
final results. That is, once an opinionated sentence
is identified by the algorithm, the hub score of all
its suthorities can be reduced proportional to the edge
weight. This will reduce the chances of the supporting
opinions being reurned by the system, at a later stage
as a main opinion.
We then attempted to test our tool on a
recently published article, ?What?s Wrong with
a Meritocracy Rug??
2
. The tool could pick up a very
2
http://news.yahoo.com/
whats-wrong-meritocracy-rug-070000354.
html
30
important opinion in the article, ?Most people tend to
think that the most qualified person is someone who
looks just like them, only younger.?, which was ranked
2
nd
by the system. The supporting facts and opinions
for this sentence, as discovered by the algorithm
were also quite relevant. For instance, the top two
authorities corresponding to this sentence hub were:
1. And that appreciation, we learned painfully, can
easily be tinged with all kinds of gendered
elements without the person who is making the
decisions even realizing it.
2. And many of the traits we value, and how we
value them, also end up being laden with gender
overtones.
5.1 Error Analysis
We then tried to analyze certain cases of failures.
Firstly, we wanted to understand why HITS was not
performing as good as the classifier for 3 articles
(Figures 2 and 3). The analysis revealed that the
supporting sentences for the opinionated sentences,
extracted by the classifier, were not very similar on
the textual level. Thus a low cosine similarity score
resulted in having lower edge weights, thereby getting
a lower hub score after applying HITS. For one of the
articles, the sentence picked up by HITS was wrongly
annotated as a factual sentence.
Then, we looked at one case of failure due to the
error introduced by the classifier prior probablities.
For instance, the sentence, ?The civil war between
establishment and tea party Republicans intensified
this week when House Speaker John Boehner slammed
outside conservative groups for ridiculous pushback
against the bipartisan budget agreement which cleared
his chamber Thursday.? was classified as an
opinionanted sentence, whereas this is a factual
sentence. Looking closely, we found that the sentence
contains three polar words (marked in bold), as
well as an advMod dependency between the pair
(slammed,when). Thus the sentence got a high initial
prior by the classifier. As a result, the outgoing edges
from this node got a higher H
i
3
factor. Some of the
authorities identified for this sentence were:
? For Democrats, the tea party is the gift that keeps
on giving.
? Tea party sympathetic organizations, Boehner
later said, ?are pushing our members in places
where they don?t want to be?.
which had words, similar to the original sentence, thus
having a higher Sim
ij
factor as well. We found that
these sentences were also very close within the article.
Thus, a high hub prior along with a high outgoing
weight gave rise to this sentence having a high hub
score after the HITS iterations.
5.2 Online Interface
To facilitate easy usage and understanding of the
system by others, a web interface has been built for
the system
3
. The webpage caters for users to either
input a new article in form of text to get top opinionated
sentences or view the output analysis of the system over
manually marked test data consisting of 20 articles.
The words in green color are positive polar words,
red indicates negative polar words. Words marked in
violet are the root verbs of the sentences. The colored
graph shows top ranked opinionated sentences in
yellow box along with top supporting factual sentences
for that particluar opinionated sentence in purple boxes.
Snapshots from the online interface are provided in
Figures 5 and 6.
6 Conclusions and Future Work
In this paper, we presented a novel two-stage
framework for extracting the opinionated sentences
in the news articles. The problem of identifying
top opinionated sentences from news articles is very
challenging, especially because the opinions are not
as explicit in a news article as in a discussion forum.
It was also evident from the inter-annotator agreement
and the kappa coefficient was found to be 0.71.
The experiments conducted over 90 News
articles (70 for training and 20 for testing) clearly
indicate that the proposed two-stage method
almost always improves the performance of the
baseline classifier-based approach. Specifically, the
improvements are much higher for P@3 and M@3
scores (35.8% and 30.8% over the NB classifier). An
M@3 score of 1.5 and P@3 score of 0.72 indicates that
the proposed method was able to push the opinionated
sentences to the top. On an average, 2 out of top
3 sentences returned by the system were actually
opinionated. This is very much desired in a practical
scenario, where an editor requires quick identification
of 3-5 opinionated sentences, which she can then use
to formulate questions.
The examples discussed in Section 5 bring out
another important aspect of the proposed algorithm.
In addition to the main objective of extracting the
opinionated sentences within the article, the proposed
method actually discovers the underlying structure of
the article and would certainly be useful to present
various opinions, grouped with supporting facts as well
as supporting opinions in the article.
While the initial results are encouraging, there is
scope for improvement. We saw that the results
obtained via HITS were highly correlated with the
Na??ve Bayes classifier results, which were used in
assigning a weight to the document graph. One
direction for the future work would be to experiment
with other features to improve the precision of the
classifier. Additionally, in the current evaluation,
we are not evaluating the degree of diversity of the
opinions returned by the system. The Hub-Authority
3
available at http://cse.iitkgp.ac.in/
resgrp/cnerg/temp2/final.php
31
Figure 5: Screenshot from the Web Interface
Figure 6: Hub-Authority Structure as output on the Web Interface
structure of the second example gives us an interesting
idea to improve diversification and we would like to
implement that in future.
In the future, we would also like to apply this work
to track an event over time, based on the opinionated
sentences present in the articles. When an event occurs,
articles start out with more factual sentences. Over
time, opinions start surfacing on the event, and as the
event matures, opinions predominate the facts in the
articles. For example, a set of articles on a plane
crash would start out as factual, and would offer expert
opinions over time. This work can be used to plot the
maturity of the media coverage by keeping track of
facts v/s opinions on any event, and this can be used
by organizations to provide a timeline for the event.
We would also like to experiment with this model on
a different media like microblogs.
References
Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,
Vanni Zavarella, Erik Van Der Goot, Matina Halkia,
Bruno Pouliquen, and Jenya Belyaeva. 2013.
Sentiment analysis in the news. arXiv preprint
arXiv:1309.6202.
Caroline Brun. 2012. Learning opinionated patterns
for contextual opinion detection. In COLING
(Posters), pages 165?174.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction
32
patterns. In Proceedings of the conference on
Human Language Technology and Empirical
Methods in Natural Language Processing,
pages 355?362. Association for Computational
Linguistics.
Jack G Conrad and Frank Schilder. 2007. Opinion
mining in legal blogs. In Proceedings of the 11th
international conference on Artificial intelligence
and law, pages 231?236. ACM.
Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th international conference on
World Wide Web, pages 519?528. ACM.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
G?unes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457?479.
Geli Fei, Bing Liu, Meichun Hsu, Malu Castellanos,
and Riddhiman Ghosh. 2012. A dictionary-based
approach to identifying aspects im-plied by
adjectives for opinion mining. In Proceedings of
COLING 2012 (Posters).
Pawan Goyal, Laxmidhar Behera, and Thomas Martin
McGinnity. 2013. A context-based word indexing
model for document summarization. Knowledge
and Data Engineering, IEEE Transactions on,
25(8):1693?1705.
Ali Harb, Michel Planti?e, Gerard Dray, Mathieu Roche,
Franc?ois Trousset, and Pascal Poncelet. 2008.
Web opinion mining: How to extract opinions from
blogs? In Proceedings of the 5th international
conference on Soft computing as transdisciplinary
science and technology, pages 211?217. ACM.
Minqing Hu and Bing Liu. 2004. Mining opinion
features in customer reviews. In Proceedings
of Nineteeth National Conference on Artificial
Intellgience (AAAI).
Minqing Hu and Bing Liu. 2006. Opinion extraction
and summarization on the web. In AAAI, volume 7,
pages 1621?1624.
Soo-Min Kim and Eduard Hovy. 2005. Automatic
detection of opinion bearing words and sentences.
In Proceedings of IJCNLP, volume 5.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
the Workshop on Sentiment and Subjectivity in
Text, pages 1?8. Association for Computational
Linguistics.
Hyun Duk Kim, Malu Castellanos, Meichun
Hsu, ChengXiang Zhai, Umeshwar Dayal, and
Riddhiman Ghosh. 2013. Compact explanatory
opinion summarization. In Proceedings of the
22nd ACM international conference on Conference
on information & knowledge management, pages
1697?1702. ACM.
Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and
Yuji Matsumoto. 2005. Opinion extraction using
a learning-based anaphora resolution technique.
In The Second International Joint Conference
on Natural Language Processing (IJCNLP),
Companion Volume to the Proceeding of Conference
including Posters/Demos and Tutorial Abstracts.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and
tracking in news and blog corpora. In AAAI
Spring Symposium: Computational Approaches to
Analyzing Weblogs, volume 100107.
Longzhuang Li, Yi Shang, and Wei Zhang. 2002.
Improvement of hits-based algorithms on web
documents. In Proceedings of the 11th international
conference on World Wide Web, pages 527?535.
ACM.
Bin Lu. 2010. Identifying opinion holders and targets
with dependency parser in chinese news texts. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL.
Ashequl Qadir. 2009. Detecting opinion sentences
specific to product features in customer reviews
using typed dependency relations. In Proceedings
of the Workshop on Events in Emerging Text Types,
eETTs ?09, pages 38?43.
Oleg Rokhlenko and Idan Szpektor. 2013. Generating
synthetic comparable questions for news articles. In
ACL, pages 742?751.
Janyce Wiebe and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. In Computational Linguistics
and Intelligent Text Processing, pages 486?497.
Springer.
Hong Yu and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: Separating
facts from opinions and identifying the polarity
of opinion sentences. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?03, pages 129?136.
Zhongwu Zhai, Bing Liu, Lei Zhang, Hua Xu,
and Peifa Jia. 2011. Identifying evaluative
sentences in online discussions. In Proceedings
of the Twenty-Fifth AAAI Conference on Artificial
Intelligence.
33
