Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1335?1345, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Extracting Opinion Expressions with
semi-Markov Conditional Random Fields
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
Extracting opinion expressions from text is
usually formulated as a token-level sequence
labeling task tackled using Conditional Ran-
dom Fields (CRFs). CRFs, however, do not
readily model potentially useful segment-level
information like syntactic constituent struc-
ture. Thus, we propose a semi-CRF-based ap-
proach to the task that can perform sequence
labeling at the segment level. We extend the
original semi-CRF model (Sarawagi and Co-
hen, 2004) to allow the modeling of arbitrar-
ily long expressions while accounting for their
likely syntactic structure when modeling seg-
ment boundaries. We evaluate performance on
two opinion extraction tasks, and, in contrast
to previous sequence labeling approaches to
the task, explore the usefulness of segment-
level syntactic parse features. Experimental
results demonstrate that our approach outper-
forms state-of-the-art methods for both opin-
ion expression tasks.
1 Introduction
Accurate opinion expression identification is crucial
for tasks that benefit from fine-grained opinion anal-
ysis (Wiebe et al 2005): e.g., it is a first step
in characterizing the sentiment and intensity of the
opinion; it provides a textual anchor for identifying
the opinion holder and the target or topic of an opin-
ion; and these, in turn, form the basis of opinion-
oriented question answering and opinion summa-
rization systems. In this paper, we focus on opin-
ion expressions as defined in Wiebe et al(2005) ?
subjective expressions that denote emotions, senti-
ment, beliefs, opinions, judgments, or other private
states (Quirk et al 1985) in text. These include
direct subjective expressions (DSEs): explicit men-
tions of private states or speech events expressing
private states; and expressive subjective expressions
(ESEs): expressions that indicate sentiment, emo-
tion, etc. without explicitly conveying them. Follow-
ing are two example sentences labeled with DSEs
and ESEs.
(1) The International Committee of the
Red Cross, [as usual][ESE], [has refused to
make any statements][DSE].
(2) The Chief Minister [said][DSE] that [the
demon they have reared will eat up their
own vitals][ESE].
As a type of information extraction task, opinion
expression extraction has been successfully tackled
in the past via sequence tagging methods: Choi et
al. (2006) and Breck et al(2007), for example, ap-
ply conditional random fields (CRFs) (Lafferty et
al., 2001) using sophisticated token-level features.
In token-level sequence labeling, labels are assigned
to single tokens, and the label of each token depends
on the current token and the label of the previous to-
ken (we consider the usual first-order assumption).
Segment-based features ? features that describe a
set of related contiguous tokens, e.g., a phrase or
constituent ? might provide critical information for
identifying opinion expressions; they cannot, how-
ever, be readily and naturally represented in the CRF
model.
1335
Our goal in this work is to extract opinion ex-
pressions at the segment level with semi-Markov
conditional random fields (semi-CRFs). Semi-CRFs
(Sarawagi and Cohen, 2004) are more powerful than
CRFs in that they allow one to construct features
to capture characteristics of the subsequences of a
sentence. They are defined on semi-Markov chains
where labels are attached to segments instead of
tokens and label dependencies are modeled at the
segment-level. Previous work has shown that semi-
CRFs outperform CRFs on named entity recog-
nition (NER) tasks (Sarawagi and Cohen, 2004;
Okanohara et al 2006). However, to the best of
our knowledge, semi-CRF techniques have not been
investigated for opinion expression extraction.
The contribution of this paper is a semi-CRF-
based approach for opinion expression extraction
that leverages parsing information to provide better
modeling of opinion expressions. Specifically, pos-
sible segmentations are generated by taking into ac-
count likely syntactic structure during learning and
inference. As a result, arbitrarily long expressions
can be modeled and their boundaries can be influ-
enced by probable syntactic structure. We also ex-
plore the impact of syntactic features for extracting
opinion expressions.
We evaluate our model on two opinion extrac-
tion tasks: identifying direct subjective expres-
sions (DSEs) and expressive subjective expressions
(ESEs). Experimental results show that our ap-
proach outperforms the state-of-the-art approach for
the task by a large margin. We also identify useful
syntactic features for the task.
2 Related Work
Previous research to extract direct subjective ex-
pressions exists, but is mainly focused on single-
word expressions (Wiebe et al 2005; Wilson et
al., 2005; Munson et al 2005). More recent stud-
ies tackle opinion expression extraction at the ex-
pression level. Breck et al(2007) formulate the
problem as a token-level sequence labeling prob-
lem; their CRF-based approach was shown to sig-
nificantly outperform two subjectivity-clue-based
baselines. Others extend the token-level approach
to jointly identify opinion holders (Choi et al
2006), and to determine the polarity and inten-
sity of the opinion expressions (Choi and Cardie,
2010). Reranking the output of a simple sequence
labeler has been shown to further improve the ex-
traction of opinion expressions (Johansson and Mos-
chitti, 2010; Johansson and Moschitti, 2011); impor-
tantly, their reranking approach relied on features
that encoded syntactic structure. All of the above
approaches, however, are based on token-level se-
quence labeling, which ignores potentially useful
phrase-level information.
Semi-CRFs (Sarawagi and Cohen, 2004) are gen-
eral CRFs that relax the Markovian assumptions to
allow sequence labeling at the segment level. Pre-
vious work has shown that semi-CRFs are supe-
rior to CRFs for NER and Chinese word segmen-
tation (Sarawagi and Cohen, 2004; Okanohara et al
2006; Andrew, 2006). The task of opinion expres-
sion extraction is known to be harder than traditional
NER since subjective expressions exhibit substantial
lexical variation and their recognition requires more
attention to linguistic structure.
Parsing has been leveraged to improve perfor-
mance for numerous natural language tasks. In opin-
ion mining, numerous studies have shown that syn-
tactic parsing features are very helpful for opinion
analysis. A lot of work uses syntactic features to
identify opinion holders and opinion topics (Bethard
et al 2005; Kim and Hovy, 2006; Kobayashi et al
2007; Joshi and Carolyn, 2009; Wu et al 2009;
Choi et al 2005). Jakob et al(2010) recently
employed dependency path features for the extrac-
tion of opinion targets. Johansson and Moschitti
(2010; Johansson and Moschitti (2011) also success-
fully employed syntactic features that indicate de-
pendency relations between opinion expressions for
the task of opinion expression extraction. However,
as their approach is based on the output of a se-
quence labeler, these features cannot be encoded to
help the learning of the sequence labeler.
3 Approach
We formulate the extraction of opinion expres-
sions as a sequence labeling problem. Unlike
previous sequence-labeling approaches to the task
(e.g., Breck et al(2007)), however, we aim to model
segment-level, rather than token-level, information.
As a result, we explore the use of semi-CRFs, which
1336
can assign labels to segments instead of tokens;
hence, features can be defined at the segment level.
For example, features like JX is a verb phraseK can
be easily encoded in the model. In the following
subsections, we first introduce standard semi-CRFs
and then describe our semi-CRF-based approach for
opinion expression extraction.
3.1 Semi-CRFs
In semi-CRFs, each observed sentence x is repre-
sented as a sequence of consecutive segments s =
?s1, ..., sn?, where si is a triple si = (ti, ui, yi), ti
denotes the start position of segment si, ui denotes
the end position, and yi denotes the label of the seg-
ment. Segments are restricted to have positive length
less than or equal to a maximum length of L that has
been seen in the corpus (1 ? ui ? ti + 1 ? L).
Features in semi-CRFs are defined at the seg-
ment level rather than the word level. The fea-
ture function g(i, x, s) is a function of x, the cur-
rent segment si, and the label yi?1 of the previ-
ous segment si?1 (we consider the usual first-order
Markovian assumption). It can also be written as
g(x, ti, ui, yi, yi?1). The conditional probability of
a segmentation s given a sequence x is defined as
p(s|x) =
1
Z(x)
exp
{
?
i
?
k
?kgk(i, x, s)
}
(1)
where
Z(x) =
?
s??S
exp
{
?
i
?
k
?kgk(i, x, s
?)
}
and the set S contains all possible segmentations ob-
tained from segment candidates with length ranging
from 1 to the maximum length L.
The correct segmentation s of a sentence
is defined as a sequence of entity segments
(i.e., the entities to be extracted) and non-
entity segments. For example, the correct
segmentation of sentence (2) in Section 1 is
?(The,NONE),(Chief,NONE),(Minister,NONE),
(said,DSE),(that,NONE),(the demon they have
reared will eat up their own vitals,ESE),(.,NONE)?.
Here, non-entity segments are represented as
unit-length segments.
3.2 Semi-CRF-based Approach for Opinion
Expression Extraction
In this section, we present an extended version of
semi-CRFs in which we can make use of parsing in-
formation in learning entity boundaries and labels
for opinion expression extraction.
As discussed in Section 3.1, the maximum entity
length L is fixed during training to generate segment
candidates in the standard semi-CRFs. In opinion
expression extraction, L is unbounded since opin-
ion expressions may be clauses or whole sentences,
which can be arbitrarily long. Thus, fixing an upper
bound on segment length based on the observed en-
tities may lead to an incorrect removal of segments
during inference. Also note that possible segment
candidates are generated based on the length con-
straint, which means any span of the text consisting
of no more than L words would be considered as
a possible segment. This would lead to the consid-
eration of implausible segments, e.g., ?The Chief?
in sentence (2) is an incorrect segment within the
multi-word expression ?The Chief Minister?.
To address these problems, we propose tech-
niques to incorporate parsing information into the
modeling of segments in semi-CRFs. More specifi-
cally, we construct segment units from the parse tree
of each sentence1, and then build up possible seg-
ment candidates based on those units. In the parse
tree, each leaf phrase or leaf word is considered to be
a segment unit. Each segment unit performs as the
smallest unit in the model (words within a segment
unit will be automatically assigned the same label).
The segment units are highlighted in rectangles in
the parse tree example in Figure 1. As the segment
units are not separable, we avoid implausible seg-
ments, which truncate multi-word expressions. For
example, ?both ridiculous and?, would not be con-
sidered a possible segment in our model.
To generate segment candidates for the model,
we consider meaningful combinations of consecu-
tive segment units. Intuitively, a sentence is made
up of several parts, and each has its own grammati-
cal role or meaning. We define the boundary of these
parts based on the parse tree structure. Specifically,
1We use the Stanford Parser http://nlp.stanford.
edu/software/lex-parser.shtml to generate the
parse trees.
1337
NP VP .
VBD SBAR
S
root
that
.
IN S
NP VP
PRP
I found
DT NNS
the statements
VBP ADJP
are DT JJ JJCC
both riddiculous and odd
Go
G1 G4
G6
 ridiculous and odd
Figure 1: A parse tree example. There are seven segment units in the sentence. The shaded regions correspond to
segment groups, where Gi represents the segment group starting from segment unit Ui.
we consider each segment unit to belong to a mean-
ingful group defined by the span of its parent node.
Two consecutive segment units are considered to be-
long to the same group if the subtrees rooted in their
parent nodes have the same rightmost child. For ex-
ample, in Figure 1, segment units ?are? and ?both
ridiculous and odd? belong to the same group, while
?I? and ?found? belong to different groups.
Algorithm 1 Construction of segment candidates
Input: A training sentence x
Output: A set of segment candidates S
1: Obtain the segment units U = (U1, ..., Um) by
preorder traversal of the parse tree T , each Ui
corresponds to a node in T
2: for i = 1 to m do
3: j ? i? 1
4: while j < m? 1 and
commonGroup(Ui, ..., Uj+1) do
5: j ? j + 1
6: for k = i to j do
7: for t = 0 to j ? k do
8: s? segment(Uk, ..., Uk+t)
9: S ? S ? s
10: Return S
Following this idea, we generate possible seg-
ment candidates by Algorithm 1. Starting from
each segment unit Ui, we first find the rightmost
segment unit Uj that belongs to the same group
as Ui. Function commonGroup(Ui, ..., Uj) re-
turns True if Ui, ..., Uj are within the same group
(the parent nodes of Ui,...,Uj have the same right-
most child in their subtrees), otherwise it returns
False. Then we enumerate all possible combina-
tions of segment units Ui, ..., Uk where i ? k ?
j. segment(Ui, ..., Uj) denotes the segment ob-
tained by concatenating words in the consecutive
segment units Ui,...,Uj . This way, segment can-
didates are generated without constraints on length
and are meaningful for learning entity boundaries.
Based on the generated segment candidates, the
correct segmentation for each training sentence can
be obtained as follows. For opinion expressions
that do not match any segment candidate, we break
them down into smaller segments using a greedy
matching process. Starting from the start position
of the expression, we search for the longest candi-
date that is contained in the expression, add it to
the correct segmentation for the sentence, set the
start position to be the next position, and repeat the
process. Using this process, the correct segmen-
tation of sentence (2) would be s = ?(The Chief
Minister,NONE),(said,DSE),(that,NONE),(the de-
mon they have reared,ESE), (will eat up their own
vitals,ESE),(.)?. Note that here non-entities corre-
spond to segment units instead of single-word seg-
ments in the original semi-CRF model.2
After obtaining the set of possible segment candi-
dates and the correct segmentation s for each train-
ing sentence, the semi-CRF model can be trained.
The goal of learning is to find the optimal parameter
? by maximizing log-likelihood. We use the limited-
2There are cases where words within a segment unit have
different labels. This may be due to errors by the human anno-
tators or the errors in the parser. In such cases, we consider each
word within the segment unit as a segment.
1338
memory BFGS algorithm (Liu and Nocedal, 1989)
for optimization in our implementation, where the
gradient of the log-likelihood L (corresponding to
one instance x) is computed:
?L
??k
=
?
i
gk(x, ti, ui, yi, yi?1)
?
?
s??S
?
y,y?
?
j
gk(x, t
?
j , u
?
j , y, y
?)p(y, y?|x)
(2)
where S is all possible segmentations consisting of
the generated segment candidates, p(y, y?|x) is the
probability of having label y for the current segment
s?j (with boundary (t
?
j , u
?
j)) and label y
? for the pre-
vious segment s?j?1.
We use a forward-backward algorithm to com-
pute the marginal distribution p(y, y?|x) and the nor-
malization factor Z(x) efficiently. For inference we
seek the best segmentation s? = argmaxs p(s|x),
where p(s|x) is defined by Equation 1. We im-
plement efficient inference using an extension of
Viterbi algorithm to segments. In particular, define
V (j, y) as the largest unnormalized probability of
p(s1:j |x) with label y at the ending position j. Then
we have
V (j, y) = max
(i,j)?s:,j
max
y?
?(x, i, j, y, y?)V (i? 1, y?)
where
?(x, i, j, y, y?) = exp
{
?
k
?kgk(x, i, j, y, y
?)
}
and s:,j denotes the set of the generated segment
candidates ending at position j. The best segmen-
tation can be obtained from tracing the path of
maxy V (n, y).
3.3 Features
Here we described the features used in our model.
Very generally, we include CRF-style features that
are segment-level extensions of the token-level fea-
tures. We also include new segment-level features
that can be naturally represented in semi-CRFs but
not CRFs.
For CRF-style features, we consider the string
representation of the current word, its part-of-
speech, and a dictionary-derived feature, which is
based on a subjectivity lexicon provided by Wilson
et al(2005). The lexicon consists of a set of words
that can act as strong or weak cues to subjectivity.
If the current word appears as an entry in the lexi-
con, then a feature strong or weak will be fired if the
entry is of that strength. These features have been
successfully employed in previous work (Breck et
al., 2007). To employ them in our model, we sim-
ply extend the feature definition to the segment level.
For example, a token-level feature Jx is great K will
be extended to a segment-level feature Js contains
great K.
Previous work on semi-CRFs has explored fea-
tures such as the length of the segment, the position
of the segment in the current segmentation (at the be-
ginning or at the end), indicators for the start word
and end word within the segment, and indicators for
words before and after the segment. These features
have been shown useful for the task of NE recogni-
tion (Sarawagi and Cohen, 2004; Okanohara et al
2006). However, we only found the position of the
segment to be helpful for the extraction of opinion
expressions, probably due to the lack of patterns in
the length distribution and word choices of opinion
expressions.
Besides the above features, we design new
segment-level syntactic features to capture the syn-
tactic patterns of opinion expressions. Syntactic pat-
terns are often used to identify useful information in
information extraction tasks. In our task, we found
that the majority of opinion expressions involve verb
phrases.3 For example, ?was encouraged?, ?ex-
pressed goodwill?, ?cannot accept? are all within a
VP constituent. To capture such structural prefer-
ences, we define several syntax-based parse features
for VP-related constituents.4
Let VPROOT denote a VP constituent whose par-
ent node is not VP, and let VPLEAF denote a VP
constituent whose children nodes are non-VP. De-
note the head of VPLEAF as the predicate, and its
next segment unit as the argument. If a segment con-
sists of words in the VP nodes visited by the preorder
3The percentages of opinion expressions involving
VP/NP/PP are 64.13%/18.43%/5.92% for DSEs and
43.22%/24.99%/11.77% for ESEs in the data set we used.
4We also conducted experiments with NP and PP-related
features, and could not find any performance improvement for
the tasks.
1339
traversal from a VPROOT to a VPLEAF, then we re-
fer to it as a verb-cluster segment. If a segment con-
sists of a verb cluster and the argument in VPLEAF,
we consider it as a VP segment. The following fea-
tures are defined for verb-cluster segments and VP
segments.
VPcluster: Indicates whether or not the segment
matches the verb-cluster structure.
VPpred: A feature of the syntactic category and
the word of the head of VPLEAF. The head of
VPLEAF is the predicate of the verb phrase, which
may encode some intention of opinions in the verb
phrase. For example, if ?warned? is the head of
VPLEAF rather than ?informed?, the chance of the
segment being an opinion expression increases.
VParg: A feature of the syntactic category and
the head word of the argument in VPLEAF. For ex-
ample, the noun phrase ?a negative stand? is the ar-
gument of the predicate ?take? in the verb phrase
?take a negative stand?. The argument in the verb
phrase (could be a noun phrase, adjectival phrase or
prepositional phrase) may convey some relevant in-
formation for identifying opinion expressions.
VPsubj: Whether the verb clusters or the argu-
ment in the segment contains an entry from the sub-
jectivity lexicon. For example, the word ?negative?
is in the lexicon, so the segment ?take a negative
stand? has a feature ISVPSUBJ.
4 Experiments
For evaluation, we use the MPQA 1.2 corpus (Wiebe
et al 2005)5, a widely used data set for fine-grained
opinion analysis. It contains 535 news articles, a to-
tal of 11,114 sentences with subjectivity-related an-
notations at the phrase level. We focus on the task
of extracting two types of opinion expressions: di-
rect subjective expressions (DSEs) and expressive
subjective expressions (ESEs). Table 1 shows some
statistics of the corpus. As in prior research that
uses the corpus, we set aside the standard 135 docu-
ments as a development set and use 400 documents
as the evaluation set. All experiments employ 10-
fold cross validation on the evaluation set, and the
average over all runs is reported.
5Available at http://www.cs.pitt.edu/mpqa/.
DSEs ESEs
Sentences with opinions(%) 55.89 57.93
TotalNum 9746 11730
MaxLength 15 40
Length ? 1 (%) 43.38 71.65
Length ? 4 (%) 9.44 35.01
Table 1: Statistics of opinion expressions in the MPQA
Corpus.
4.1 Evaluation Metrics
We use precision, recall, and F-measure to evalu-
ate the quality of the model. Precision is defined
as |C?P ||P | and recall, as
|C?P |
|C| , where C and P are
the sets of correct and predicted expression spans,
respectively. F-measure is computed as 2PRP+R . Be-
cause the boundaries of opinion expressions are hard
to define even for human annotators (Wiebe et al
2005), previous research mainly focused on soft pre-
cision and recall measures for performance evalu-
ation. Breck et al(2007) introduced an overlap
measure, which considers a predicted expression to
be correct if it overlaps with a correct expression.
We refer to this metric as Binary Overlap. Johans-
son and Moschitti (2010) provides a stricter measure
that computes the proportion of overlapping spans:
if a correct expression s overlaps with a predicted
expression s?, the overlap contributes value |s?s
?|
|s?| to
|C ? P | instead of value 1. We refer to this metric
as Proportional Overlap. To compare with previous
work, we present our results according to both met-
rics.
4.2 Baseline Methods
As a baseline, we use the token-level CRF-based ap-
proach of Breck et al(2007) applied to the MPQA
dataset. We employ a very similar, but not iden-
tical set of features: indicators for specific words
at the current location and neighboring words in a
[?4,+4] window, part-of-speech features, and opin-
ion lexicon features for tokens that are contained in
the subjectivity lexicon (see Section 3.3). We do not
include WordNet, Levin?s verb categorization, and
FrameNet features.
We also include two variants of standard CRFs as
baselines: segment-CRF and syntactic-CRF. They
incorporate segmentation information into standard
CRFs without modifying the Markovian assump-
1340
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 82.83 49.38 61.87 78.56 43.57 56.05
segment-CRF 82.52 51.48 63.41 78.90 44.46 56.88
syntactic-CRF 82.48 49.09 61.55 78.41 43.39 55.95
semi-CRF 66.67 74.13 70.20 71.21 57.41 63.57
new-semi-CRF 67.72?? 74.33 70.87? 73.57??? 57.63 64.74??
semi-CRF(w/ syn) 64.86 74.10 69.17 70.68 56.61 62.87
new-semi-CRF(w/ syn) 70.12??? 74.74? 72.36??? 73.61??? 59.27??? 65.67???
Table 2: Results for extracting opinion expressions with Binary-Overlap metric. (w/ syn) indicates the inclusion of
syntactic parse features VPpre, VParg and VPsubj. Results of new-semi-CRF that are statistically significantly greater
than semi-CRF according to a two-tailed t-test are indicated with ?(p < 0.1), ??(p < 0.05), ???(p < 0.005). T-test
results are also shown for new-semi-CRF(w/ syn) versus semi-CRF(w/ syn).
DSE Extraction ESE Extraction
Method Precision Recall F-measure Precision Recall F-measure
CRF 77.91 46.45 58.20 67.72 37.55 48.31
segment-CRF 77.86 48.58 59.83 68.03 38.34 49.04
syntactic-CRF 77.73 46.27 58.01 67.80 37.60 48.37
semi-CRF 60.38 68.34 64.11 57.30 46.20 51.16
new-semi-CRF 62.50?? 68.59? 65.41? 61.69??? 47.44?? 53.63???
semi-CRF(w/ syn) 58.69 67.80 62.92 57.09 45.63 50.72
new-semi-CRF(w/ syn) 65.52??? 68.91??? 67.17??? 61.66??? 48.77??? 54.47???
Table 3: Results for extracting opinion expressions with Proportional-Overlap metric. Notation is the same as above.
tion. Segment-CRF treats segment units obtained
from the parser as word tokens. For example, in
Figure 1, the segment units the statement and both
ridiculous and odd will be treated as word tokens.
Syntactic-CRF encodes segment-level syntactic in-
formation in a standard token-level CRF as input
features. We consider the VP-related segment fea-
tures introduced in Section 3.3. VPPRE and VPARG
are added to the head word of the corresponding verb
phrase, and VPSUBJ and VPCLUSTER are added to
each token within the corresponding segment.
Another baseline method is the original semi-
CRF model (Sarawagi and Cohen, 2004). To the
best of our knowledge, our work is the first to ex-
plore the use of semi-CRFs on the extraction of
opinion expressions. They are considered to be more
powerful than CRFs since they allow information to
be represented at the expression level. The model
requires an input of the maximum entity length. We
set it to 15 for DSE and 40 for ESE. For segment fea-
tures, we used the same features as in our approach
(see Section 3.3).
4.3 Results
Table 2 and Table 3 show the results of DSE and
ESE extraction using two different metrics. The
standard token-based CRF baseline of Breck et al
(2007) is labeled CRF; the original semi-CRF base-
line is labeled semi-CRF; and our extended semi-
CRF approach is labeled new-semi-CRF. For semi-
CRF and new-semi-CRF, the results were obtained
using two different settings of features: the basic
feature set includes features described in Section 3.3
excluding the segment-level syntactic features. In
the second feature setting (labeled as w/ syn in the
tables), we further augment the basic features with
the syntactic parse features.
Using the basic features, we observe that
semi-CRF-based approaches significantly outper-
form CRF and its two variants segment-CRF and
syntactic-CRF in F-Measure on both DSE and ESE
extraction, and new-semi-CRF achieves the best re-
sults. By simply incorporating the segmentation
prior into the standard CRF, segment-CRF achieves
a slight improvement over standard CRF, but the
results are still worse than those of semi-CRF
and new-semi-CRF. However, adding segment-level
1341
DSE Extraction ESE Extraction
Feature set Precision Recall F-measure Precision Recall F-measure
Basic 67.72 74.33 70.87 73.57 57.63 64.74
Basic+VPpre 70.88 71.44 71.16 73.20 58.20 64.85
Basic+VParg 70.12 74.03 72.02 73.05 58.20 64.79
Basic+VPcluster 70.08 72.94 71.48 73.06 58.45 64.94
Basic+VPsubj 70.04 72.34 71.17 73.31 58.53 65.09
Basic+VPpre+VPsubj 70.91 72.54 71.72 73.61 58.29 65.07
Basic+VParg+VPsubj 70.45 73.53 71.96 74.45 57.80 65.07
Basic+VPpre+VParg+VPsubj 70.12 74.74 72.36 73.61 59.27 65.67
Basic+VPcluster+VPpre+VParg+VPsubj 70.91 72.54 71.72 72.84 58.45 64.86
Table 4: Effect of syntactic features on extracting opinion expressions with Binary-Overlap metric
syntactic features into standard CRF yields slightly
reduced performance. This is not surprising as en-
coding segment-level information into the token-
level CRF is not natural. These experiments in-
dicate that simply encoding segmentation informa-
tion into standard CRF cannot result in large per-
formance gains. The promising F-measure results
obtained by semi-CRF and new-semi-CRF confirm
that relaxing the Markovian assumption on segments
leads to better modeling of opinion expressions. We
can also see that new-semi-CRF consistently outper-
forms the original semi-CRF model. This further
confirms the benefit of taking into account syntactic
parsing information in modeling segments. In Ta-
ble 3, we observe the same general results trend as
in Table 2. The scores are generally lower since the
metric Proportional Overlap is stricter than Binary
Overlap.
We also study the impact of syntactic parse fea-
tures on the semi-Markov CRF models. Here we
consider the combination of VPPRE, VPARG and
VPSUBJ since they turned out to be the most help-
ful features for our tasks. Interestingly, we found
that after incorporating the syntactic parse features,
performance decreases on semi-CRF. This indicates
that syntactic information does not help if learning
and inference take place on segment candidates gen-
erated without accounting for parse information. In
contrast, our approach incorporates syntactic pars-
ing information in modeling segments and meaning-
ful segmentations. We can see in Tables 2 and 3
that adding syntactic features successfully boosts the
performance of our approach.
To further explore the effect of the syntactic fea-
tures, we include the results of our model with dif-
ferent configurations of syntactic features in Table 4
(here we focus on the Binary Overlap metric as
the results with Proportional Overlap demonstrate
a similar conclusion). We can see that using the ba-
sic features and the combination of VPPRE, VPARG
and VPSUBJ yields the best results for both DSE
and ESE extraction. For DSE extraction, combin-
ing these three features improves the precision no-
ticeably from 67.72% to 70.12% while the recall
slightly improves. This indicates that VP-related
structural information is very helpful for modeling
segments as DSEs. However, this trend is not so
clear for ESE extraction. This may be due to the fact
that DSEs often involve verb phrases while ESEs are
represented via a variety of syntactic structures.
Comparison with previous work. In Table 5, we
compare our results to the previous work on opinion
expression extraction (here we also focus on the Bi-
nary Overlap metric due to the similar trend demon-
strated by the Proportional Overlap metric). Breck
et al(2007) presents the state-of-the-art sequence
labeling approach on the tasks of DSE and ESE ex-
traction. Their best results are shown as Breck et
al. Baseline in the table. Johansson and Mos-
chitti (2010) use a reranking technique on the best
k outputs of a sequence labeler to further improve
their sequence labeling results on the task of ex-
tracting DSEs, ESEs and OSEs (Objective Speech
Events) (we don?t consider OSEs here). Results
using our re-implementation of their approach us-
ing SVM struct (Tsochantaridis et al 2004) on the
output of CRF are labeled CRF+Reranking Base-
line in the table. We use the same features and
1342
parameter settings as in their approach. Our ap-
proach+Reranking are results obtained by apply-
ing the reranking step on the output of our new-
semi-CRF approach.
We can see that our approach outperforms the
Breck et alBaseline on both DSE extraction and
ESE extraction in spite of the fact that we do not
use their WordNet, Levin?s verb categorization, and
FrameNet features. The CRF+Reranking Baseline
does provide a performance increase over the the
baseline CRF results, but overall it cannot beat the
other methods since the CRF baseline is very low.
As one might expect, reranking also succeeds in
boosting the performance of new-semi-CRF, achiev-
ing the best performance on F-measure for both DSE
and ESE extraction. Note that the interannotator
agreement results for these two tasks are 75% for
DSE and 72% for ESE using a similar metric to Bi-
nary Overlap. Our results are much closer to these
interannotator scores than previous systems espe-
cially for DSEs.
Task Method F-measure
DSE Extraction
Breck et alBaseline 70.65
CRF+Reranking Baseline 63.87
Our approach 72.36
Our approach+Reranking 73.12
ESE Extraction
Breck et alBaseline 63.43
CRF+Reranking Baseline 58.21
Our approach 65.67
Our approach+Reranking 67.01
Table 5: Comparison of our work with previous work on
opinion expression extraction using the Binary-Overlap
metric
4.4 Discussion
We note that our new-semi-CRF approach outper-
forms the original semi-CRF w.r.t. both precision
and recall, but compared to CRF, our approach
yields a clear improvement on recall but not on pre-
cision. An error analysis helps explain why. We
found that our semi-CRF approach predicted almost
the same number of DSEs as the gold standard la-
bels while CRF only predicted half of them (for ESE
extraction, the trend is similar). With more pre-
dicted entities, the precision is sacrificed but recall is
boosted substantially, and overall we see an increase
in F-measure.
Looking further into the errors, we found sev-
eral mistakes that could potentially be fixed to yield
better a precision score. Some errors were due to
the false prediction of speech events like ?said? or
?told? as DSEs in cases where they actually just in-
troduced statements of fact without expressing any
private state. Adding features to distinguish such
cases should help improve performance. Other er-
rors were due to inadequate modeling of the context
surrounding the expressions. For example, ?enjoy a
relative advantage? was falsely predicted as an ESE.
If incorporating information about the subject of this
verb phrase which is ?products?, this mistake could
be avoided since ?products? cannot hold or express
private state. We also noticed some errors caused
by inaccurate parsing and hope to study ways to ac-
count for these in our approach as future work.
By comparing the extraction results across differ-
ent methods, we see that full parsing provides many
benefits for modeling segment boundaries and im-
proving the prediction precision for opinion expres-
sion extraction. For example, given the sentence, ?...
who are living [a lot better][ESE] ...?, both CRF and
the original semi-CRF extract ?lot better? as an ESE,
while our approach correctly extracts ?a lot better?
as an ESE. And we also found many cases where
the original semi-CRF cannot extract the opinion ex-
pressions while our approach can. Another benefit
of utilizing parsing is to speed up learning and infer-
ence. Although in theory, the computational cost of
parsing is O(g ? n3) where g is the grammar size
and n is the sentence length while the cost of semi-
CRFs is O(K2 ? L? n) where K is the number of
labels and L is the maximum entity length, feature
extraction overhead and the potentially large num-
ber of learning iterations in parameter optimization
may lead to a long training time for semi-CRFs. In
our experiments on the MPQA data set, our machine
with Intel Core 2 Duo CPU and 4GB RAM took 2
hours to fully parse 11,114 sentences using the Stan-
ford Parser, and also 2 hours to train the standard
semi-CRF. With the parsing information, our semi-
CRF-based approach is able to finish training in 15
minutes. As full parsing would be expensive when
the average sentence length is very large, it would be
interesting to study how to utilize parsing with less
cost in our task.
1343
5 Conclusion
In this paper we propose a semi-CRF-based ap-
proach for extracting opinion expressions that takes
into account during learning and inference the struc-
tural information available from syntactic parsing.
Our approach allows opinion expressions to be iden-
tified at the segment level and their boundaries to
be influenced by their probable syntactic structure.
Experimental evaluations show that our model out-
performs the best existing approaches on two opin-
ion extraction tasks. In addition, we identify useful
syntactic parse features for these tasks that have not
been explored in previous work. Our error analysis
indicates that adding additional features that account
for subjectivity cues in the local context might fur-
ther improve the performance. In future work, we
hope to explore better ways of utilizing parsing in-
formation with less cost. Also, we will apply our
model to additional opinion analysis tasks such as
fine-grained opinion summarization and relation ex-
traction.
6 Acknowledgement
This work was supported in part by National Science
Foundation Grants IIS-1111176 and IIS-0968450,
and by a gift from Google. We thank Nikos Karam-
patziakis, Igor Labutov, Veselin Stoyanov, Ainur
Yessenalina and Jason Yosinski for their helpful
comments.
References
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP ?06.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
opinion propositions and opinion holders using syn-
tactic and lexical cues. In Shanahan, James G., Yan
Qu, and Janyce Wiebe, editors, Computing Attitude
and Affect in Text: Theory and Applications.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identi-
fying expressions of opinion in context. IJCAI?07.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of HLT ?05.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of EMNLP ?06.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of ACL 2010, Short Papers.
Richard Johansson and Alessandro Moschitti. 2010. Syn-
tactic and semantic structure for opinion expression
detection. In Proceedings of CoNLL ?10.
Niklas Jakob and Iryna Gurevych. Extracting opinion tar-
gets in a single- and cross-domain setting with condi-
tional random fields. In Proceedings of EMNLP? 10.
Mahesh Joshi and Penstein-Ros?e Carolyn. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of ACL/IJCNLP 2009, Short Papers
Track.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL ?03.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL-2007.
John D. Lafferty, Andrew McCallum, and Fernando C.
N. Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML ?01.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B 45(3): 503-528.
M Arthur Munson, Claire Cardie, and Rich Caruana. Op-
timizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP05, 2005.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. Improving the scalability
of semi-Markov conditional random fields for named
entity recognition. In Proceedings of ACL?06.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. A comprehensive grammar of the
English language. New York: Longman, 1985.
Richard Johansson and Alessandro Moschitti. Extract-
ing Opinion Expressions and Their Polarities - Explo-
ration of Pipelines and Joint Models. In Proceedings
of ACL ?11, Short Paper.
Ellen Riloff and Janyce M Wiebe. Learning extraction
patterns for subjective expressions. In Proceedings of
EMNLP 2003.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction. In Proceedings of NIPS 2004.
1344
Charles Sutton and Andrew McCallum. An Introduc-
tion to Conditional Random Fields. Foundations and
Trends in Machine Learning (FnT ML), 2010.
Janyce Wiebe, Theresa Wilson , and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT ?05.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. Opin-
ionFinder: A system for subjectivity analysis. EMNLP
2005. Demo abstract.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
Phrase dependency parsing for opinion mining. In Pro-
ceedings of EMNLP 2009.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. Support Vector Learn-
ing for Interdependent and Structured Output Spaces.
In Proceedings of ICML 2004.
1345
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568?1579,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Typed Tensor Decomposition of Knowledge Bases for Relation Extraction
Kai-Wei Chang
??
Wen-tau Yih
\
Bishan Yang
??
Christopher Meek
\
?
University of Illinois, Urbana, IL 61801, USA
?
Cornell University, Ithaca, NY 14850, USA
\
Microsoft Research, Redmond, WA 98052, USA
Abstract
While relation extraction has traditionally
been viewed as a task relying solely on
textual data, recent work has shown that
by taking as input existing facts in the form
of entity-relation triples from both knowl-
edge bases and textual data, the perfor-
mance of relation extraction can be im-
proved significantly. Following this new
paradigm, we propose a tensor decompo-
sition approach for knowledge base em-
bedding that is highly scalable, and is es-
pecially suitable for relation extraction.
By leveraging relational domain knowl-
edge about entity type information, our
learning algorithm is significantly faster
than previous approaches and is better
able to discover new relations missing
from the database. In addition, when ap-
plied to a relation extraction task, our ap-
proach alone is comparable to several ex-
isting systems, and improves the weighted
mean average precision of a state-of-the-
art method by 10 points when used as a
subcomponent.
1 Introduction
Identifying the relationship between entities from
free text, relation extraction is a key task for ac-
quiring new facts to increase the coverage of a
structured knowledge base. Given a pre-defined
database schema, traditional relation extraction
approaches focus on learning a classifier using tex-
tual data alone, such as patterns between the oc-
currences of two entities in documents, to deter-
mine whether the entities have a particular rela-
tion. Other than using the existing known facts
to label the text corpora in a distant supervision
setting (Bunescu and Mooney, 2007; Mintz et al.,
?
Work conducted while interning at Microsoft Research.
2009; Riedel et al., 2010; Ritter et al., 2013), an
existing knowledge base is typically not involved
in the process of relation extraction.
However, this paradigm has started to shift re-
cently, as researchers showed that by taking exist-
ing facts of a knowledge base as an integral part of
relation extraction, the model can leverage richer
information and thus yields better performance.
For instance, Riedel et al. (2013) borrowed the
idea of collective filtering and constructed a ma-
trix where each row is a pair of entities and each
column is a particular relation. For a true entity-
relation triple (e
1
, r, e
2
), either from the text cor-
pus or from the knowledge base, the correspond-
ing entry in the matrix is 1. A previously unknown
fact (i.e., triple) can be discovered through ma-
trix decomposition. This approach can be viewed
as creating vector representations of each relation
and candidate pair of entities. Because each entity
does not have its own representation, relationships
of any unpaired entities cannot be discovered. Al-
ternatively, Weston et al. (2013) created two types
of embedding ? one based on textual similarity and
the other based on knowledge base, where the lat-
ter maps each entity and relation to the same d-
dimensional vector space using a model proposed
by Bordes et al. (2013a). They also showed that
combining these two models results in a signif-
icant improvement over the model trained using
only textual data.
To make such an integrated strategy work, it is
important to capture all existing entities and rela-
tions, as well as the known facts, from both tex-
tual data and large databases. In this paper, we
propose a new knowledge base embedding model,
TRESCAL, that is highly efficient and scalable,
with relation extraction as our target application.
Our work is built on top of RESCAL (Nickel
et al., 2011), which is a tensor decomposition
method that has proven its scalability by factoring
YAGO (Biega et al., 2013) with 3 million entities
1568
and 41 million triples (Nickel et al., 2012). We
improve the tensor decomposition model with two
technical innovations. First, we exclude the triples
that do not satisfy the relational constraints (e.g.,
both arguments of the relation spouse-of need to
be person entities) from the loss, which is done
by selecting sub-matrices of each slice of the ten-
sor during training. Second, we introduce a math-
ematical technique that significantly reduces the
computational complexity in both time and space
when the loss function contains a regularization
term. As a consequence, our method is more than
four times faster than RESCAL, and is also more
accurate in discovering unseen triples.
Our contributions are twofold. First, compared
to other knowledge base embedding methods de-
veloped more recently, it is much more efficient
to train our model. As will be seen in Sec. 5,
when applied to a large knowledge base created
using NELL (Carlson et al., 2010) that has 1.8M
entity-relation triples, our method finishes training
in 4 to 5 hours, while an alternative method (Bor-
des et al., 2013a) needs almost 3 days. Moreover,
the prediction accuracy of our model is competi-
tive to others, if not higher. Second, to validate its
value to relation extraction, we apply TRESCAL to
extracting relations from a free text corpus along
with a knowledge base, using the data provided
in (Riedel et al., 2013). We show that TRESCAL
is complementary to existing systems and signif-
icantly improves their performance when using it
as a subcomponent. For instance, this strategy im-
proves the weighted mean average precision of the
best approach in (Riedel et al., 2013) by 10 points
(47% to 57%).
The remainder of this paper is organized as fol-
lows. We survey most related work in Sec. 2 and
provide the technical background of our approach
in Sec. 3. Our approach is detailed in Sec. 4, fol-
lowed by the experimental validation in Sec. 5. Fi-
nally, Sec. 6 concludes the paper.
2 Related Work
Our approach of creating knowledge base em-
bedding is based on tensor decomposition, which
is a well-developed mathematical tool for data
analysis. Existing tensor decomposition models
can be categorized into two main families: the
CP and Tucker decompositions. The CP (CAN-
DECOMP/PARAFAC) decomposition (Kruskal,
1977; Kiers, 2000) approximates a tensor by a sum
of rank-one tensors, while the Tucker decompo-
sition (Tucker, 1966), also known as high-order
SVD (De Lathauwer et al., 2000), factorizes a ten-
sor into a core tensor multiplied by a matrix along
each dimension. A highly scalable distributional
algorithm using the Map-Reduce architecture has
been proposed recently for computing CP (Kang et
al., 2012), but not for the Tucker decomposition,
probably due to its inherently more complicated
model form.
Matrix and tensor decomposition methods have
been applied to modeling multi-relational data.
For instance, Speer et al. (2008) aimed to cre-
ate vectors of latent components for representing
concepts in a common sense knowledge base us-
ing SVD. Franz et al. (2009) proposed TripleRank
to model the subject-predicate-object
RDF triples in a tensor, and then applied the CP
decomposition to identify hidden triples. Fol-
lowing the same tensor encoding, Nickel et al.
(2011) proposed RESCAL, a restricted form of
Tucker decomposition for discovering previously
unknown triples in a knowledge base, and later
demonstrated its scalability by applying it to
YAGO, which was encoded in a 3M ? 3M ? 38
tensor with 41M triples (Nickel et al., 2012).
Methods that revise the objective function
based on additional domain information have been
proposed, such as MrWTD, a multi-relational
weighted tensor decomposition method (London
et al., 2013), coupled matrix and tensor fac-
torization (Papalexakis et al., 2014), and col-
lective matrix factorization (Singh and Gordon,
2008). Alternatively, instead of optimizing for the
least-squares reconduction loss, a non-parametric
Bayesian approach for 3-way tensor decomposi-
tion for modeling relational data has also been pro-
posed (Sutskever et al., 2009). Despite the exis-
tence of a wide variety of tensor decomposition
models, most methods do not scale well and have
only been tested on datasets that are much smaller
than the size of real-world knowledge bases.
Multi-relational data can be modeled by neural-
network methods as well. For instance, Bordes et
al. (2013b) proposed the Semantic Matching En-
ergy model (SME), which aims to have the same
d-dimensional vector representations for both en-
tities and relations. Given the vectors of entities
e
1
, e
2
and relation r. They first learn the latent
representations of (e
1
, r) and (e
2
, r). The score
of (e
1
, r, e
2
) is determined by the inner product
1569
of the vectors of (e
1
, r) and (e
2
, r). Later, they
proposed a more scalable method called translat-
ing embeddings (TransE) (Bordes et al., 2013a).
While both entities and relations are still repre-
sented by vectors, the score of (e
1
, r, e
2
) becomes
the negative dissimilarity measure of the corre-
sponding vectors ??e
i
+ r
k
? e
j
?, motivated by
the work in (Mikolov et al., 2013b; Mikolov et al.,
2013a). Alternatively, Socher et al. (2013) pro-
posed a Neural Tensor Network (NTN) that repre-
sents entities in d-dimensional vectors created sep-
arately by averaging pre-trained word vectors, and
then learns a d?d?m tensor describing the inter-
actions between these latent components in each
of the m relations. All these methods optimize
for loss functions that are more directly related to
the true objective ? the prediction accuracy of cor-
rect entity-relation triples, compared to the mean-
squared reconstruction error in our method. Nev-
ertheless, they typically require much longer train-
ing time.
3 Background
In this section, we first describe how entity-
relation triples are encoded in a tensor. We then
introduce the recently proposed tensor decompo-
sition method, RESCAL (Nickel et al., 2011) and
explain how it adopts an alternating least-squares
method, ASALSAN (Bader et al., 2007), to com-
pute the factorization.
3.1 Encoding Binary Relations in a Tensor
Suppose we are given a knowledge base with
n entities and m relation types, and the facts
in the knowledge base are denoted as a set of
entity-relation triples T = {(e
i
, r
k
, e
j
)}, where
i, j ? {1, 2, ? ? ?n} and k ? {1, 2, ? ? ?m}. A
triple (e
i
, r
k
, e
j
) simply means that the i-th en-
tity and the j-th entity have the k-th relation.
Following (Franz et al., 2009), these triples can
naturally be encoded in a 3-way tensor X ?
{0, 1}
n?n?m
, such that X
i,j,k
= 1 if and only if
the triple (e
i
, r
k
, e
j
) ? T
1
. The tensor can be
viewed as consisting of m slices, where each slice
is an n?n square matrix, denoting the interactions
of the entities of a particular relation type. In the
remainder of this paper, we will use X
k
to refer to
the k-th slice of the tensor X . Fig. 1 illustrates this
representation.
1
This representation can easily be extended for a proba-
bilistic knowledge base by allowing nonnegative real values.
e1  en
e 1  
 e n
?? k
Figure 1: A tensor encoding of m binary relation
types and n entities. A sliceX
k
denotes the entities
having the k-th relation.
3.2 RESCAL
In order to identify latent components in a ten-
sor for collective learning, Nickel et al. (2011)
proposed RESCAL, which is a tensor decomposi-
tion approach specifically designed for the multi-
relational data described in Sec. 3.1. Given a ten-
sor X
n?n?m
, RESCAL aims to have a rank-r ap-
proximation, where each slice X
k
is factorized as
X
k
? AR
k
A
T
. (1)
A is an n ? r matrix, where the i-th row denotes
the r latent components of the i-th entity. R
k
is an
asymmetric r ? r matrix that describes the inter-
actions of the latent components according to the
k-th relation. Notice that while R
k
differs in each
slice, A remains the same.
A and R
k
are derived by minimizing the loss
function below.
min
A,R
k
f(A,R
k
) + ? ? g(A,R
k
), (2)
where f(A,R
k
) =
1
2
(
?
k
?X
k
?AR
k
A
T
?
2
F
)
is the mean-squared reconstruction error and
g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
is the regu-
larization term.
RESCAL is a special form of Tucker decom-
position (Tucker, 1966) operating on a 3-way ten-
sor. Its model form (Eq. (1)) can also be regarded
as a relaxed form of DEDICOM (Bader et al.,
2007), which derives the low-rank approximation
as: X
k
? AD
k
RD
k
A
T
. To compare RESCAL
to other tensor decomposition methods, interested
readers can refer to (Kolda and Bader, 2009).
1570
The optimization problem in Eq. (2) can be
solved using the efficient alternating least-squares
(ALS) method. This approach alternatively fixes
R
k
to solve for A and then fixes A to solve
R
k
. The whole procedure stops until
f(A,R
k
)
?X?
2
F
con-
verges to some small threshold  or the maximum
number of iterations has been reached.
By finding the solutions where the gradients are
0, we can derive the update rules of A and R
k
as
below.
A?
[
?
k
X
k
AR
T
k
+X
T
k
AR
k
][
?
k
B
k
+C
k
+?I
]
?1
,
where B
k
= R
k
A
T
AR
T
k
and C
k
= R
T
k
A
T
AR
k
.
vec(R
k
)?
(
Z
T
Z + ?I
)
?1
Z
T
vec(X
k
), (3)
where vec(R
k
) is the vectorization of R
k
, Z =
A?A and the operator ? is the Kronecker prod-
uct.
Complexity Analysis Following the analysis in
(Nickel et al., 2012), we assume that each X
k
is a
sparse matrix, and let p be the number of non-zero
entries
2
. The complexity of computing X
k
AR
T
k
and X
T
k
AR
k
is O(pr + nr
2
). Evaluating B
k
and
C
k
requires O(nr
2
) and the matrix inversion re-
quires O(r
3
). Therefore, the complexity of updat-
ing A isO(pr+nr
2
) assuming n r. The updat-
ing rule of R
k
involves inverting an r
2
? r
2
ma-
trix. Therefore, directly computing the inversion
requires time complexity O(r
6
) and space com-
plexity O(r
4
). Although Nickel et al. (2012) con-
sidered using QR decomposition to simplify the
updates, it is still time consuming with the time
complexity O(r
6
+ pr
2
). Therefore, the total time
complexity isO(r
6
+pr
2
) and the step of updating
R
k
is the bottleneck in the optimization process.
We will describe how to reduce the time complex-
ity of this step to O(nr
2
+ pr) in Section 4.2.
4 Approach
We describe how we leverage the relational do-
main knowledge in this section. By removing the
incompatible entity-relation triples from the loss
2
Notice that we use a slightly different definition of p
from the one in (Nickel et al., 2012). The time complexity
of multiplying an n ? n sparse matrix X
k
with p non-zero
entries by an n? r dense matrix is O(pr) assuming n r.
function, training can be done much more effi-
ciently and results in a model with higher pre-
diction accuracy. In addition, we also introduce
a mathematical technique to reduce the compu-
tational complexity of the tensor decomposition
methods when taking into account the regulariza-
tion term.
4.1 Applying Relational Domain Knowledge
In the domain of knowledge bases, the notion of
entity types is the side information that commonly
exists and dictates whether some entities can be
legitimate arguments of a given predicate. For
instance, suppose the relation of interest is born-
in, which denotes the birth location of a person.
When asked whether an incompatible pair of en-
tities, such as two person entities like Abraham
Lincoln and John Henry, having this rela-
tion, we can immediately reject the possibility. Al-
though the type information and the constraints
are readily available, it is overlooked in the pre-
vious work on matrix and tensor decomposition
models for knowledge bases (Riedel et al., 2013;
Nickel et al., 2012). Ignoring the type information
has two implications. Incompatible entity-relation
triples still participate in the loss function of the
optimization problem, which incurs unnecessary
computation. Moreover, by choosing values for
these incompatible entries we introduce errors in
training the model that can reduce the quality of
the model.
Based on this observation, we propose Typed-
RESCAL, or TRESCAL, which leverages the en-
tity type information to improve both the effi-
ciency of model training and the quality of the
model in term of prediction accuracy. We em-
ploy a direct and simple approach by excluding
the triples of the incompatible entity types from
the loss in Eq. (2). For each relation, let L
k
and
R
k
be the set of entities with a compatible type to
the k-th relation. That is, (e
i
, r
k
, e
j
) is a feasible
triple if and only if e
i
? L
k
and e
j
? R
k
. For no-
tational convenience, we use A
k
l
,A
k
r
to denote
the sub-matrices of A that consists of rows asso-
ciated with L
k
and R
k
, respectively. Analogously,
let X
k
lr
be the sub-matrix of X
k
that consists of
only the entity pairs compatible to the k-th rela-
tion. The rows and columns of X
k
lr
map to the en-
tities in A
k
l
and A
k
r
, respectively. In other words,
entries of X
k
but not in X
k
lr
do not satisfy the type
constraint and are ignored from the computation.
1571
~ ~ ? ? 
?k A 
A TRk
A kl A krT?klr
e ??Lk
e ??Rk
Figure 2: The construction of TRESCAL. Suppose
the k-th relation is born-in. L
k
is then a set of
person entities and R
k
is a set of location entities.
Only the sub-matrix corresponds to the compati-
ble entity pairs (i.e., X
k
lr
) and the sub-matrices of
the associated entities (i.e., A
k
l
and A
T
k
r
) will be
included in the loss.
Fig. 2 illustrates this construction.
TRESCAL solves the following optimization
problem:
min
A,R
k
f
?
(A,R
k
) + ? ? g(A,R
k
), (4)
where f
?
(A,R
k
) =
1
2
?
k
?X
k
lr
?A
k
l
R
k
A
T
k
r
?
2
F
and g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
.
Similarly, A and R
k
can be solved using the
alternating least-squares method. The update rule
of A is
A?
[
?
k
(
X
k
lr
A
k
r
R
T
k
+ X
T
k
lr
A
k
l
R
k
)
]
?
[
?
k
B
k
r
+ C
k
l
+ ?I
]
?1
,
where B
k
r
= R
k
A
T
k
r
A
k
r
R
T
k
and C
k
l
=
R
T
k
A
T
k
l
A
k
l
R
k
.
The update ofR
k
becomes:
vec(R
k
)?
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
?
vec(A
k
l
T
X
k
lr
A
k
r
),
(5)
Complexity Analysis Let n? be the average
number of entities with a compatible type to a
relation. Follow a similar derivation in Sec. 3.2,
the time complexity of updating A isO(pr+ n?r
2
)
and the time complexity of updating R
k
remains
to be O(r
6
+ pr
2
).
4.2 Handling Regularization Efficiently
Examining the update rules of both RESCAL
and TRESCAL, we can see that the most time-
consuming part is the matrix inversions. For
RESCAL, this is the term (Z
T
Z+?I)
?1
in Eq. (3),
where Z = A?A. Nickel et al. (2011) made the
observation that if ? = 0, the matrix inversion can
be calculated by
(Z
T
Z)
?1
= (A
T
A)
?1
A? (A
T
A)
?1
A.
Then, it only involves an inversion of an r? r ma-
trix, namely A
T
A. However, if ? > 0, directly
calculating Eq. (3) requires to invert an r
2
? r
2
matrix and thus becomes a bottleneck in solving
Eq. (2).
To reduce the computational complexity of
the update rules of R
k
, we compute the inver-
sion
(
Z
T
Z + ?I
)
?1
by applying singular value
decomposition (SVD) to A, such that A =
U?V
T
, where U and V are orthogonal matrices
and ? is a diagonal matrix. Then by using proper-
ties of the Kronecker product we have:
(
Z
T
Z + ?I
)
?1
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
.
The last equality holds because V ? V is
also an orthogonal matrix. We leave the de-
tailed derivations in Appendix A. Notice that
(
?I + ?
2
??
2
)
?1
is a diagonal matrix. There-
fore, the inversion calculation is trivial.
This technique can be applied to TRESCAL
as well. By applying SVD to both A
k
l
and A
k
r
, we have A
k
l
= U
k
l
?
k
l
V
T
k
l
and
A
k
r
= U
k
r
?
k
r
V
T
k
r
, respectively. The computa-
tion of
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
of Eq. (5)
thus becomes:
(V
k
l
?V
k
r
)
(
?I + ?
2
k
l
??
2
k
r
)
?1
(V
k
l
?V
k
r
)
T
.
The procedure of updating R is depicted in Al-
gorithm 1.
Complexity Analysis For RESCAL, V and ?
can be computed by finding eigenvectors of A
T
A.
Therefore, computing SVD of A costs O(nr
2
+
r
3
) = O(nr
2
). Computing Step 4 in Algorithm 1
takes O(nr
2
+ pr). Step 5 and Step 6 require
1572
Algorithm 1 UpdatingR in TRESCAL
Require: X , A, and entity sets R
k
,L
k
,?k
Ensure: R
k
,?k.
1: for k = 1 . . .m do
2: [U
k
l
,?
2
k
l
,V
k
l
]? SVD(A
T
k
l
A
k
l
).
3: [U
k
r
,?
2
k
r
,V
k
r
]? SVD(A
T
k
r
A
k
r
).
4: M
1
? V
T
k
l
A
T
k
l
X
k
lr
A
k
r
V
k
r
.
5: M
2
? diag(?
2
k
l
) diag(?
2
k
r
)
T
+ ?1.
(1 is a matrix of all ones. Function diag
converts the diagonal entries of a matrix to
a vector. )
6: R
k
? V
k
l
(M
1
./M
2
)V
T
k
r
.
(The operator ?./? is element-wise divi-
sion.)
7: end for
O(r
2
) and O(r
3
), respectively. The overall time
complexity of updatingR
k
becomesO(nr
2
+pr).
Using a similar derivation, the time complex-
ity of updating R
k
in TRESCAL is O(n?r
2
+ pr).
Therefore, the total complexity of each iteration is
O(n?r
2
+ pr).
5 Experiments
We conduct two sets of experiments. The first
evaluates the proposed TRESCAL algorithm on
inferring unknown facts using existing relation?
entity triples, while the second demonstrates its
application to relation extraction when a text cor-
pus is available.
5.1 Knowledge Base Completion
We evaluate our approach on a knowledge base
generated by the CMU Never Ending Language
Learning (NELL) project (Carlson et al., 2010).
NELL collects human knowledge from the web
and has generated millions of entity-relation
triples. We use the data generated from version
165 for training
3
, and collect the new triples gen-
erated between NELL versions 166 and 533 as the
development set and those generated between ver-
sion 534 and 745 as the test set
4
. The data statistics
of the training set are summarized in Table 1. The
numbers of triples in the development and test sets
are 19,665 and 117,889, respectively. Notice that
this dataset is substantially larger than the datasets
used in recent work. For example, the Freebase
data used in (Socher et al., 2013) and (Bordes et
3
http://www.cs.cmu.edu/
?
nlao/
4
http://bit.ly/trescal
NELL
# entities 753k
# relation types 229
# entity types 300
# entity-relation triples 1.8M
Table 1: Data statistics of the training set from
NELL in our experiments.
al., 2013a) have 316k and 483k
5
triples, respec-
tively, compared to 1.8M in this dataset.
In the NELL dataset, the entity type informa-
tion is encoded in a specific relation, called Gen-
eralization. Each entity in the knowledge base is
assigned to at least one category presented by the
Generalization relationship. Based on this infor-
mation, the compatible entity type constraint of
each relation can be easily identified. Specifically,
we examined the entities and relations that occur
in the triples of the training data, and counted all
the types appearing in these instances of a given
relation legitimate.
We implement RESCAL and TRESCAL in
MATLAB with the Matlab tensor Toolbox (Bader
et al., 2012). With the efficient implementation
described in Section 4.2, all experiments can be
conducted on a commodity PC with 16 GB mem-
ory. We set the maximal number of iterations of
both RESCAL and TRESCAL to be 10, which we
found empirically to be enough to generate a sta-
ble model. Note that Eq. (4) is non-convex, and the
optimization process does not guarantee to con-
verge to a global minimum. Therefore, initial-
izing the model properly might be important for
the performance. Following the implementation of
RESCAL, we initialize A by performing singular
value decomposition over
?
X =
?
k
(X
k
+ X
T
k
),
such that
?
X = U?V
T
and set A = U. Then,
we apply the update rule ofR
k
to initialize {R
k
}.
RESCAL and TRESCAL have two types of param-
eters: (1) the rank r of the decomposed tensor and
(2) the regularization parameter ?. We tune the
rank parameter on development set in a range of
{100, 200, 300, 400} and the regularization pa-
rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.
For comparison, we also use the code released
by Bordes et al. (2013a), which is implemented
using Python and the Theano library (Bergstra
et al., 2010), to train a TransE model using the
5
In (Bordes et al., 2013a), there is a much larger dataset,
FB1M, that has 17.5M triples used for evaluation. However,
this dataset has not been released.
1573
Entity Retrieval Relation Retrieval
TransE RESCAL TRESCAL TransE RESCAL TRESCAL
w/o type checking 51.41%
?
51.59% 54.79% 75.88% 73.15%
?
76.12%
w/ type checking 67.56% 62.91%
?
69.26% 70.71%
?
73.08%
?
75.70%
Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.
? and ? indicate the comparison to TRESCAL in the same setting is statistically significant using a paired-
t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type
constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.
same NELL dataset. We reserved randomly 1%
of the training triples for the code to evaluate the
model performance in each iteration. As sug-
gested in their paper, we experiment with sev-
eral hyper-parameters, including learning rate of
{0.01, 0.001}, the latent dimension of {50, 100}
and the similarity measure of {L1, L2}. In addi-
tion, we also adjust the number of batches of {50,
100, 1000}. Of all the configurations, we keep the
models picked by the method, as well as the fi-
nal model after 500 training iterations. The final
model is chosen by the performance on our devel-
opment set.
5.1.1 Training Time Reduction
We first present experimental results demonstrat-
ing that TRESCAL indeed reduces the time re-
quired to factorize a knowledge database, com-
pared to RESCAL. The experiment is conducted
on NELL with r = 300 and ? = 0.1. When
? 6= 0, the original RESCAL algorithm described
in (Nickel et al., 2011; Nickel et al., 2012) cannot
handle a large r, because updating matrices {R
k
}
requires O(r
4
) memory. Later in this section, we
will show that in some situation a large rank r is
necessary for achieving good testing performance.
Comparing TRESCAL with RESCAL, each it-
eration of TRESCAL takes 1,608 seconds, while
that of RESCAL takes 7,415 seconds. In other
words, by inducing the entity type information
and constraints, TRESCAL enjoys around 4.6 times
speed-up, compared to an improved regularized
version of RESCAL. When updating A and {R
k
}
TRESCAL only requires operating on sub-matrices
of A, {R
k
} and {X
k
}, which reduces the compu-
tation substantially. In average, TRESCAL filters
96% of entity triples that have incompatible types.
In contrast, it takes TransE at least 2 days and 19
hours to finish training the model (the default 500
iterations)
6
, while TRESCAL finishes the training
6
It took almost 4 days to train the best TransE model that
in roughly 4 to 5 hours
7
.
5.1.2 Test Performance Improvement
We consider two different types of tasks to evalu-
ate the prediction accuracy of different models ?
entity retrieval and relation retrieval.
Entity Retrieval In the first task, we collect a
set of entity-relation pairs {(e
i
, r
k
)} and aim at
predicting e
j
such that the tuple (e
i
, r
k
, e
j
) is a
recorded triple in the NELL knowledge base. For
each pair (e
i
, r
k
), we collect triples {(e
i
, r
k
, e
?
j
)}
from the NELL test corpus as positive samples
and randomly pick 100 entries e
?
j
to form negative
samples {e
i
, r
k
, e
?
j
}. Given A and R
k
from the
factorization generated by RESCAL or TRESCAL,
the score assigned to a triple {e
i
, r
k
, e
?
j
} is com-
puted by a
T
i
R
k
a
j
where a
i
and a
j
are the i-th
and j-th rows of A. In TransE, the score is de-
termined by the negative dissimilarity measures of
the learned embeddings: ?d(e
i
, r
k
, e
?
j
) = ??e
i
+
r
k
? e
?
j
?
2
2
.
We evaluate the performance using mean aver-
age precision (MAP), which is a robust and sta-
ble metric (Manning et al., 2008). As can be
observed in Table 2 (left), TRESCAL achieves
54.79%, which outperforms 51.59% of RESCAL
and 51.41% of TransE. Adding constraints during
test time by assigning the lowest score to the en-
tity triples with incompatible types improves re-
sults of all models ? TRESCAL still performs the
best (69.26%), compared to TransE (67.56%) and
RESCAL (62.91%).
Relation Retrieval In the second task, given a
relation type r
k
, we are looking for the entity pairs
(e
i
, e
j
) that have this specific relationship. To gen-
erate test data, for each relation type, we collect
is included in Table 2.
7
We also tested the released code from (Socher et al.,
2013) for training a neural tensor network model. However,
we are not able to finish the experiments as each iteration of
this method takes almost 5 hours.
1574
gold entity pairs from the NELL knowledge base
as positive samples and randomly pick a set of en-
tity pairs as negative samples such that the number
of positive samples are the same as negative ones.
Results presented in Table 2 (right) show that
TRESCAL achieves 76.12%, while RESCAL and
TransE are 73.15% and 75.88%, respectively.
Therefore, incorporating the type information in
training seems to help in this task as well. Enforc-
ing the type constraints during test time does not
help as in entity retrieval. By removing incom-
patible entity pairs, the performance of TRESCAL,
RESCAL and TransE drop slightly to 75.70%,
73.08% and 70.71% respectively. One possible
explanation is that the task of relation retrieval is
easier than entity retrieval. The incorrect type in-
formation of some entities ends up filtering out a
small number of entity pairs that were retrieved
correctly by the model.
Notice that TRESCAL achieves different levels
of performance on various relations. For example,
it performs well on predicting AthletePlaysSport
(81%) and CoachesInLeague (88%), but achieves
suboptimal performance on predicting Works-
For (49%) and BuildingLocatedInCity (35%).
We hypothesize that it is easier to gener-
alize entity-relation triples when the relation
has several related relations. For examples,
AthletePlaysForTeam and TeamPlaysSport may
help discover entity-relation triples of Ath-
letePlaysSport.
5.1.3 Sensitivity to Parameters
We also study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?,
where the detailed results can be found in Ap-
pendix B. In short, we found that increasing the
rank r generally leads to better models. Also,
while the model is not very sensitive to the value
of the regularization parameter ?, tuning ? is still
necessary for achieving the best performance.
5.2 Relation Extraction
Next, we apply TRESCAL to the task of extract-
ing relations between entities, jointly from a text
corpus and a structured knowledge base. We use
a corpus from (Riedel et al., 2013) that is cre-
ated by aligning the entities in NYTimes and Free-
base. The corpus consists of a training set and a
test set. In the training set, a list of entity pairs
are provided, along with surface patterns extracted
from NYTimes and known relations obtained from
Freebase. In the test set, only the surface patterns
are given. By jointly factoring a matrix consist-
ing of the surface patterns and relations, Riedel et
al. (2013) show that their model is able to capture
the mapping between the surface patterns and the
structured relations and hence is able to extract the
entity relations from free text. In the following, we
show that TRESCAL can be applied to this task.
We focus on the 19 relations listed in Table 1
of (Riedel et al., 2013) and only consider the
surface patterns that co-occur with these 19 re-
lations. We prune the surface patterns that oc-
cur less than 5 times and remove the entities that
are not involved in any relation and surface pat-
tern. Based on the training and test sets, we
build a 80,698?80,698?1,652 tensor, where each
slice captures a particular structured relation or a
surface pattern between two entities. There are
72 fine types extracted from Freebase assigned
to 53,836 entities that are recorded in Freebase.
In addition, special types, PER, LOC, ORG and
MISC, are assigned to the remaining 26,862 enti-
ties based on the predicted NER tags provided by
the corpus. A type is considered incompatible to a
relation or a surface pattern if in the training data,
none of the argument entities of the relation be-
longs to the type. We use r = 400 and ? = 0.1 in
TRESCAL to factorize the tensor.
We compare the proposed TRESCAL model to
RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011),
MI09 (Mintz et al., 2009) and SU12 (Surdeanu et
al., 2012)
8
. We follow the protocol used in (Riedel
et al., 2013) to evaluate the results. Given a re-
lation as query, the top 1,000 entity pairs output
by each system are collected and the top 100 ones
are judged manually. Besides comparing individ-
ual models, we also report the results of combined
models. To combine the scores from two models,
we simply normalize the scores of entity-relation
tuples to zero mean and unit variance and take the
average. The results are summarized in Table 3.
As can been seen in the table, using TRESCAL
alone is not very effective and its performance is
only compatible to MI09 and YA11, and is sig-
nificantly inferior to RI13. This is understandable
because the problem setting favors RI13 as only
entity pairs that have occurred in the text or the
database will be considered in RI13, both during
model training and testing. In contrast, TRESCAL
8
The corpus and the system outputs are from http://
www.riedelcastro.org/uschema
1575
Relation # MI09 YA11 SU12 RI13 TR TR+SU12 TR+RI13
person/company 171 0.41 0.40 0.43 0.49 0.43 0.53 0.64
location/containedby 90 0.39 0.43 0.44 0.56 0.23 0.46 0.58
parent/child 47 0.05 0.10 0.25 0.31 0.19 0.24 0.35
person/place of birth 43 0.32 0.31 0.34 0.37 0.50 0.61 0.66
person/nationality 38 0.10 0.30 0.09 0.16 0.13 0.16 0.22
author/works written 28 0.52 0.53 0.54 0.71 0.00 0.39 0.62
person/place of death 26 0.58 0.58 0.63 0.63 0.54 0.72 0.89
neighborhood/neighborhood of 13 0.00 0.00 0.08 0.67 0.08 0.13 0.73
person/parents 8 0.21 0.24 0.51 0.34 0.01 0.16 0.38
company/founders 7 0.14 0.14 0.30 0.39 0.06 0.17 0.44
film/directed by 4 0.06 0.15 0.25 0.30 0.03 0.13 0.35
sports team/league 4 0.00 0.43 0.18 0.63 0.50 0.29 0.63
team/arena stadium 3 0.00 0.06 0.06 0.08 0.00 0.04 0.09
team owner/teams owned 2 0.00 0.50 0.70 0.75 0.00 0.00 0.75
roadcast/area served 2 1.00 0.50 1.00 1.00 0.50 0.83 1.00
structure/architect 2 0.00 0.00 1.00 1.00 0.00 0.02 1.00
composer/compositions 2 0.00 0.00 0.00 0.12 0.00 0.00 0.12
person/religion 1 0.00 1.00 1.00 1.00 0.00 1.00 1.00
film/produced by 1 1.00 1.00 1.00 0.33 0.00 1.00 0.25
Weighted MAP 0.33 0.36 0.39 0.47 0.30 0.44 0.57
Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.
Bold faced are winners per relation, italics indicate ties based on a sign test.
predicts all the possible combinations between en-
tities and relations, which makes the model less fit
to the task. However, when combining TRESCAL
with a pure text-based method, such as SU12,
we can clearly see TRESCAL is complementary
to SU12 (0.39 to 0.44 in weighted MAP score),
which makes the results competitive to RI13.
Interestingly, although both TRESCAL and RI13
leverage information from the knowledge base, we
find that by combining them, the performance is
improved quite substantially (0.47 to 0.57). We
suspect that the reason is that in our construc-
tion, each entity has its own vector representa-
tion, which is lacked in RI13. As a result, the
new triples that TRESCAL finds are very different
from those found by RI13. Nevertheless, com-
bining more methods do not always yield an im-
provement. For example, combining TR, RI13 and
SU12 together (not included in Table 3) achieves
almost the same performance as TR+RI13.
6 Conclusions
In this paper we developed TRESCAL, a tensor
decomposition method that leverages relational
domain knowledge. We use relational domain
knowledge to capture which triples are potentially
valid and found that, by excluding the triples that
are incompatible when performing tensor decom-
position, we can significantly reduce the train-
ing time and improve the prediction performance
as compared with RESCAL and TransE. More-
over, we demonstrated its effectiveness in the ap-
plication of relation extraction. Evaluated on the
dataset provided in (Riedel et al., 2013), the per-
formance of TRESCAL alone is comparable to sev-
eral existing systems that leverage the idea of dis-
tant supervision. When combined with the state-
of-the-art systems, we found that the results can
be further improved. For instance, the weighted
mean average precision of the previous best ap-
proach in (Riedel et al., 2013) has been increased
by 10 points (47% to 57%).
There are a number of interesting potential ex-
tensions of our work. First, while the experiments
in this paper are on traditional knowledge bases
and textual data, the idea of leveraging relational
domain knowledge is likely to be of value to other
linguistic databases as well. For instance, part-of-
speech tags can be viewed as the ?types? of words.
Incorporating such information in other tensor de-
composition methods (e.g., (Chang et al., 2013))
may help lexical semantic representations. Sec-
ond, relational domain knowledge goes beyond
entity types and their compatibility with specific
relations. For instance, the entity-relation triple
(e
1
, child-of, e
2
) can be valid only if e
1
.type =
person ? e
2
.type = person ? e
1
.age < e
2
.age.
It would be interesting to explore the possibility
of developing efficient methods to leverage other
types of relational domain knowledge. Finally, we
would like to create more sophisticated models of
knowledge base embedding, targeting complex in-
1576
ference tasks to better support semantic parsing
and question answering.
Acknowledgments
We thank Sebastian Riedel for providing the data
for experiments. We are also grateful to the anony-
mous reviewers for their valuable comments.
References
Brett W Bader, Richard A Harshman, and Tamara G
Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In ICDM, pages 33?42. IEEE.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan-
uary.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Joanna Biega, Erdal Kuzey, and Fabian M Suchanek.
2013. Inside YOGO2s: a transparent information
extraction architecture. In WWW, pages 325?328.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko. 2013a. Translating Embeddings
for Modeling Multi-relational Data. In Advances in
Neural Information Processing Systems 26.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2013b. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1?27.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 576?583, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In The Semantic Web-
ISWC 2009, pages 213?228. Springer.
U Kang, Evangelos Papalexakis, Abhay Harpale, and
Christos Faloutsos. 2012. Gigatensor: scaling ten-
sor analysis up by 100 times-algorithms and discov-
eries. In KDD, pages 316?324. ACM.
Henk AL Kiers. 2000. Towards a standardized nota-
tion and terminology in multiway analysis. Journal
of chemometrics, 14(3):105?122.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Joseph B Kruskal. 1977. Three-way arrays: rank and
uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Lin-
ear algebra and its applications, 18(2):95?138.
Alan J Laub, 2005. Matrix analysis for scientists and
engineers, chapter 13, pages 139?150. SIAM.
Ben London, Theodoros Rekatsinas, Bert Huang, and
Lise Getoor. 2013. Multi-relational learning using
weighted tensor decomposition with modular loss.
Technical report, University of Maryland College
Park. http://arxiv.org/abs/1303.1733.
C. Manning, P. Raghavan, and H. Schutze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, pages
809?816.
1577
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: scalable ma-
chine learning for linked data. In WWW, pages 271?
280.
Evangelos E Papalexakis, Tom M Mitchell, Nicholas D
Sidiropoulos, Christos Faloutsos, Partha Pratim
Talukdar, and Brian Murphy. 2014. Turbo-smt:
Accelerating coupled sparse matrix-tensor factoriza-
tions by 200x. In SDM.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML/PKDD
2010. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL, pages 74?84.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367?378, October.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 650?658. ACM.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.
Robert Speer, Catherine Havasi, and Henry Lieberman.
2008. Analogyspace: Reducing the dimensionality
of common sense knowledge. In AAAI, pages 548?
553.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing Bayesian clustered tensor factorization. In NIPS,
pages 1821?1828.
Ledyard R Tucker. 1966. Some mathematical notes
on three-mode factor analysis. Psychometrika,
31(3):279?311.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Appendix A Detailed Derivation
We first introduce some lemmas that will be useful
for our derivation. Lemmas 2, 3 and 4 are the basic
properties of the Kronecker product. Their proofs
can be found at (Laub, 2005).
Lemma 1. Let V be an orthogonal matrix and
? a diagonal matrix. Then (I + V?V
T
)
?1
=
V(I + ?)
?1
V
T
.
Proof.
(I + V?V
T
)
?1
= (VIV
T
+ V?V
T
)
?1
= V(I + ?)
?1
V
T
Lemma 2. (A?B)(C?D) = AC?BD.
Lemma 3. (A?B)
T
= A
T
?B
T
.
Lemma 4. If A and B are orthogonal matrices,
then A?B will also be an orthogonal matrix.
Let Z = A ? A and apply singular value
decomposition to A = U?V
T
. The term
(
Z
T
Z + ?I
)
?1
can be rewritten as:
(
Z
T
Z + ?I
)
?1
=
(
?I + (A
T
?A
T
)(A?A)
)
?1
(6)
=
(
?I + A
T
A?A
T
A
)
?1
(7)
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
(8)
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
(9)
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
(10)
Eq. (6) is from replacing Z with A ? A and
Lemma 3. Eq. (7) is from Lemma 2. Eq. (8) is
from the properties of SVD, where U and V are
orthonormal matrices. Eq. (9) is from Lemma 2
and Lemma 3. Finally, Eq. (10) comes from
Lemma 1.
1578
Figure 3: Prediction performance of TRESCAL
and RESCAL with different rank (r).
Figure 4: Prediction performance of TRESCAL
with different regularization parameter (?).
Appendix B Hyper-parameter Sensitivity
We study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?.
We use the task of relation retrieval and present
the model performance on the development set.
Fig. 3 shows the performance of TRESCAL and
RESCAL with different rank (r) values while fix-
ing ? = 0.01. Results show that both TRESCAL
and RESCAL achieve better performance when r
is reasonably large. TRESCAL obtains a bet-
ter model with smaller r than RESCAL, because
TRESCAL only needs to fit the triples of the com-
patible entity types. Therefore, it allows to use
smaller number of latent variables to fit the train-
ing data.
Fixing r = 400, Fig. 4 shows the performance
of TRESCAL at different values of the regulariza-
tion parameter ?, including no regularization at
all (? = 0). While the results suggest that the
method is not very sensitive to ?, tuning ? is still
necessary for achieving the best performance.
1579
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640?1649,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Inference for Fine-grained Opinion Extraction
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
This paper addresses the task of fine-
grained opinion extraction ? the identi-
fication of opinion-related entities: the
opinion expressions, the opinion hold-
ers, and the targets of the opinions, and
the relations between opinion expressions
and their targets and holders. Most ex-
isting approaches tackle the extraction
of opinion entities and opinion relations
in a pipelined manner, where the inter-
dependencies among different extraction
stages are not captured. We propose a joint
inference model that leverages knowledge
from predictors that optimize subtasks
of opinion extraction, and seeks a glob-
ally optimal solution. Experimental re-
sults demonstrate that our joint inference
approach significantly outperforms tradi-
tional pipeline methods and baselines that
tackle subtasks in isolation for the problem
of opinion extraction.
1 Introduction
Fine-grained opinion analysis is concerned with
identifying opinions in text at the expression level;
this includes identifying the subjective (i.e., opin-
ion) expression itself, the opinion holder and the
target of the opinion (Wiebe et al, 2005). The
task has received increasing attention as many nat-
ural language processing applications would ben-
efit from the ability to identify text spans that cor-
respond to these key components of opinions. In
question-answering systems, for example, users
may submit questions in the form ?What does en-
tity A think about target B??; opinion-oriented
summarization systems also need to recognize
opinions and their targets and holders.
In this paper, we address the task of identifying
opinion-related entities and opinion relations. We
consider three types of opinion entities: opinion
expressions or direct subjective expressions as de-
fined in Wiebe et al (2005) ? expressions that ex-
plicitly indicate emotions, sentiment, opinions or
other private states (Quirk et al, 1985) or speech
events expressing private states; opinion targets
? expressions that indicate what the opinion is
about; and opinion holders ? mentions of whom
or what the opinion is from. Consider the follow-
ing examples in which opinion expressions (O) are
underlined and targets (T) and holders (H) of the
opinion are bracketed.
S1: [The workers][H1,2] were irked[O1]
by [the government report][T1] and
were worried[O2] as they went about
their daily chores.
S2: From the very start it could be
predicted[O1] that on the subject of
economic globalization, [the developed
states][T1,2] were going to come across
fierce opposition[O2].
The numeric subscripts denote linking relations,
one of IS-ABOUT or IS-FROM. In S1, for in-
stance, opinion expression ?were irked? (O1) IS-
ABOUT ?the government report? (T1). Note that
the IS-ABOUT relation can contain an empty tar-
get (e.g. ?were worried? in S1); similarly for IS-
FROM w.r.t. the opinion holder (e.g. ?predicted? in
S2). We also allow an opinion entity to be involved
in multiple relations (e.g. ?the developed states? in
S2).
Not surprisingly, fine-grained opinion extrac-
tion is a challenging task due to the complexity
and variety of the language used to express opin-
ions and their components (Pang and Lee, 2008).
Nevertheless, much progress has been made in ex-
tracting opinion information from text. Sequence
labeling models have been successfully employed
to identify opinion expressions (e.g. (Breck et al,
1640
2007; Yang and Cardie, 2012)) and relation ex-
traction techniques have been proposed to extract
opinion holders and targets based on their link-
ing relations to the opinion expressions (e.g. Kim
and Hovy (2006), Kobayashi et al (2007)). How-
ever, most existing work treats the extraction of
different opinion entities and opinion relations in a
pipelined manner: the interaction between differ-
ent extraction tasks is not modeled jointly and er-
ror propagation is not considered. One exception
is Choi et al (2006), which proposed an ILP ap-
proach to jointly identify opinion holders, opinion
expressions and their IS-FROM linking relations,
and demonstrated the effectiveness of joint infer-
ence. Their ILP formulation, however, does not
handle implicit linking relations, i.e. opinion ex-
pressions with no explicit opinion holder; nor does
it consider IS-ABOUT relations.
In this paper, we present a model that jointly
identifies opinion-related entities, including opin-
ion expressions, opinion targets and opinion hold-
ers as well as the associated opinion linking rela-
tions, IS-ABOUT and IS-FROM. For each type of
opinion relation, we allow implicit (i.e. empty) ar-
guments for cases when the opinion holder or tar-
get is not explicitly expressed in text. We model
entity identification as a sequence tagging prob-
lem and relation extraction as binary classifica-
tion. A joint inference framework is proposed to
jointly optimize the predictors for different sub-
problems with constraints that enforce global con-
sistency. We hypothesize that the ambiguity in
the extraction results will be reduced and thus,
performance increased. For example, uncertainty
w.r.t. the spans of opinion entities can adversely
affect the prediction of opinion relations; and evi-
dence of opinion relations might provide clues to
guide the accurate extraction of opinion entities.
We evaluate our approach using a standard cor-
pus for fine-grained opinion analysis (the MPQA
corpus (Wiebe et al, 2005)) and demonstrate that
our model outperforms by a significant margin tra-
ditional baselines that do not employ joint infer-
ence for extracting opinion entities and different
types of opinion relations.
2 Related Work
Significant research effort has been invested into
fine-grained opinion extraction for open-domain
text such as news articles (Wiebe et al, 2005; Wil-
son et al, 2009). Many techniques were proposed
to identify the text spans for opinion expressions
(e.g. (Breck et al, 2007; Johansson and Moschitti,
2010b; Yang and Cardie, 2012)), opinion hold-
ers (e.g. (Choi et al, 2005)) and topics of opin-
ions (Stoyanov and Cardie, 2008). Some consider
extracting opinion targets/holders along with their
relation to the opinion expressions. Kim and Hovy
(2006) identifies opinion holders and targets by us-
ing their semantic roles related to opinion words.
Ruppenhofer et al (2008) argued that semantic
role labeling is not sufficient for identifying opin-
ion holders and targets. Johansson and Moschitti
(2010a) extract opinion expressions and holders
by applying reranking on top of sequence label-
ing methods. Kobayashi et al (2007) considered
extracting ?aspect-evaluation? relations (relations
between opinion expressions and targets) by iden-
tifying opinion expressions first and then search-
ing for the most likely target for each opinion ex-
pression via a binary relation classifier. All these
methods extract opinion arguments and opinion
relations in separate stages instead of extracting
them jointly.
Most similar to our method is Choi et al (2006),
which jointly extracts opinion expressions, hold-
ers and their IS-FROM relations using an ILP ap-
proach. In contrast, our approach (1) also consid-
ers the IS-ABOUT relation which is arguably more
complex due to the larger variety in the syntac-
tic structure exhibited by opinion expressions and
their targets, (2) handles implicit opinion relations
(opinion expressions without any associated argu-
ment), and (3) uses a simpler ILP formulation.
There has also been substantial interest in opin-
ion extraction from product reviews (Liu, 2012).
Most existing approaches focus on the extrac-
tion of opinion targets and their associated opin-
ion expressions and usually employ a pipeline
architecture: generate candidates of opinion ex-
pressions and opinion targets first, and then use
rule-based or machine-learning-based approaches
to identify potential relations between opinions
and targets (Hu and Liu, 2004; Wu et al, 2009;
Liu et al, 2012). In addition to pipeline ap-
proaches, bootstrapping-based approaches were
proposed (Qiu et al, 2009; Qiu et al, 2011; Zhang
et al, 2010) to identify opinion expressions and
targets iteratively; however, they suffer from the
problem of error propagation.
There is much work demonstrating the bene-
fit of performing global inference. Roth and Yih
1641
(2004) proposed a global inference approach in the
formulation of a linear program (LP) and applied
it to the task of extracting named entities and re-
lations simultaneously. Their problem is similar
to ours ? the difference is that Roth and Yih Roth
and Yih (2004) assume that named entity spans are
known a priori and only their labels need to be as-
signed. Joint inference has also been applied to
semantic role labeling (Punyakanok et al, 2008;
Srikumar and Roth, 2011; Das et al, 2012), where
the goal is to jointly identify semantic arguments
for given lexical predicates. The problem is con-
ceptually similar to identifying opinion arguments
for opinion expressions, however, we do not as-
sume prior knowledge of opinion expressions (un-
like in SRL, where predicates are given).
3 Model
As proposed in Section 1, we consider the task of
jointly identifying opinion entities and opinion re-
lations. Specifically, given a sentence, our goal is
to identify spans of opinion expressions, opinion
arguments (targets and holders) and their associ-
ated linking relations. Training data consists of
text with manually annotated opinion expression
and argument spans, each with a list of relation
ids specifying the linking relation between opin-
ion expressions and their arguments.
In this section, we will describe how we model
opinion entity identification and opinion relation
extraction, and how we combine them in a joint
inference model.
3.1 Opinion Entity Identification
We formulate the task of opinion entity identifica-
tion as a sequence labeling problem and employ
conditional random fields (CRFs) (Lafferty et al,
2001) to learn the probability of a sequence as-
signment y for a given sentence x. Through in-
ference we can find the best sequence assignment
for sentence x and recover the opinion entities ac-
cording to the standard ?IOB? encoding scheme.
We consider four entity labels: D,T,H,N , where
D denotes opinion expressions, T denotes opinion
targets, H denotes opinion holders and N denotes
?NONE? entities.
We define potential function fiz that gives the
probability of assigning a span i with entity label
z, and the probability is estimated based on the
learned parameters from CRFs. Formally, given
a within-sentence span i = (a, b), where a is the
starting position and b is the end position, and la-
bel z ? {D,T,H}, we have
fiz = p(ya = Bz,ya+1 = Iz, ...,
yb = Iz,yb+1 6= Iz|x)
fiN = p(ya = O, ...,yb = O|x)
These probabilities can be efficiently computed
using the forward-backward algorithm.
3.2 Opinion Relation Extraction
We consider extracting the IS-ABOUT and IS-
FROM opinion relations. In the following we will
not distinguish these two relations, since they can
both be characterized as relations between opinion
expressions and opinion arguments, and the meth-
ods for relation extraction are the same.
We treat the relation extraction problem as a
combination of two binary classification prob-
lems: opinion-arg classification, which decides
whether a pair consisting of an opinion candidate o
and an argument candidate a forms a relation; and
opinion-implicit-arg classification, which decides
whether an opinion candidate o is linked to an im-
plicit argument, i.e. no argument is mentioned. We
define a potential function r to capture the strength
of association between an opinion candidate o and
an argument candidate a,
roa = p(y = 1|x)? p(y = 0|x)
where p(y = 1|x) and p(y = 0|x) are the logistic
regression estimates of the positive and negative
relations. Similarly, we define potential ro? to de-
note the confidence of predicting opinion span o
associated with an implicit argument.
3.2.1 Opinion-Arg Relations
For opinion-arg classification, we construct can-
didates of opinion expressions and opinion argu-
ments and consider each pair of an opinion can-
didate and an argument candidate as a potential
opinion relation. Conceptually, all possible sub-
sequences in the sentence are candidates. To filter
out candidates that are less reasonable, we con-
sider the opinion expressions and arguments ob-
tained from the n-best predictions by CRFs1. We
also employ syntactic patterns from dependency
1We randomly split the training data into 10 parts and ob-
tained the 50-best CRF predictions on each part for the gen-
eration of candidates. We also experimented with candidates
generated from more CRF predictions, but did not find any
performance improvement for the task.
1642
trees to generate candidates. Specifically, we se-
lected the most common patterns of the shortest
dependency paths2 between an opinion candidate
o and an argument candidate a in our dataset, and
include all pairs of candidates that satisfy at least
one dependency pattern. For the IS-ABOUT rela-
tion, the top three patterns are (1) o ?dobj a, (2)
o ?ccomp x ?nsubj a (x is a word in the path that is
not covered by either o nor a), (3) o ?ccomp a; for
the IS-FROM relation, the top three patterns are (1)
o ?nsubj a, (2) o ?poss a, (3) o ?ccomp x ?nsubj a.
Note that generating candidates this way will
give us a large number of negative examples. Sim-
ilar to the preprocessing approach in (Choi et al,
2006), we filter pairs of opinion and argument can-
didates that do not overlap with any gold standard
relation in our training data.
Many features we use are common features
in the SRL tasks (Punyakanok et al, 2008)
due to the similarity of opinion relations to the
predicate-argument relations in SRL (Ruppen-
hofer et al, 2008; Choi et al, 2006). In general,
the features aim to capture (a) local properties of
the candidate opinion expressions and arguments
and (b) syntactic and semantic attributes of their
relation.
Words and POS tags: the words contained in the
candidate and their POS tags.
Lexicon: For each word in the candidate, we
include its WordNet hypernyms and its strength
of subjectivity in the Subjectivity Lexicon3
(e.g. weaksubj, strongsubj).
Phrase type: the syntactic category of the deepest
constituent that covers the candidate in the parse
tree, e.g. NP, VP.
Semantic frames: For each verb in the opinion
candidate, we include its frame types according to
FrameNet4.
Distance: the relative distance (number of words)
between the opinion and argument candidates.
Dependency Path: the shortest path in the
dependency tree between the opinion candidate
and the target candidate, e.g. ccomp?nsubj?. We
also include word types and POS types in the
paths, e.g. opinion?ccompsuffering?nsubjpatient,
2We use the Stanford Parser to generate parse trees and
dependency graphs.
3http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
4https://framenet.icsi.berkeley.edu/
fndrupal/
NN?ccompVBG?nsubjNN. The dependency path
has been shown to be very useful in extracting
opinion expressions and opinion holders (Johans-
son and Moschitti, 2010a).
3.2.2 Opinion-Implicit-Arg Relations
When the opinion-arg relation classifier predicts
that there is no suitable argument for the opinion
expression candidate, it does not capture the possi-
bility that an opinion candidate may associate with
an implicit argument. To incorporate knowledge
of implicit relations, we build an opinion-implicit-
arg classifier to identify an opinion candidate with
an implicit argument based on its own properties
and context information.
For training, we consider all gold-standard
opinion expressions as training examples ?
including those with implicit arguments ? as
positive examples and those associated with
explicit arguments as negative examples. For
features, we use words, POS tags, phrase types,
lexicon and semantic frames (see Section 3.2.1
for details) to capture the properties of the opinion
expression, and also features that capture the
context of the opinion expression:
Neighboring constituents: The words and gram-
matical roles of neighboring constituents of the
opinion expression in the parse tree ? the left and
right sibling of the deepest constituent containing
the opinion expression in the parse tree.
Parent Constituent: The grammatical role of
the parent constituent of the deepest constituent
containing the opinion expression.
Dependency Argument: The word types and
POS types of the arguments of the dependency
patterns in which the opinion expression is
involved. We consider the same dependency
patterns that are used to generate candidates for
opinion-arg classification.
3.3 Joint Inference
The inference goal is to find the optimal prediction
for both opinion entity identification and opinion
relation extraction. For a given sentence, we de-
note O as a set of opinion candidates, Ak as a set
of argument candidates, where k denotes the type
of opinion relation ? IS-ABOUT or IS-FROM ?
and S as a set of within-sentence spans that cover
all of the opinion candidates and argument can-
1643
didates. We introduce binary variable xiz , where
xiz = 1 means span i is associated with label z.
We also introduce binary variable uij for every
pair of opinion candidate i and argument candidate
j, where uij = 1 means i forms an opinion rela-
tion with j, and binary variable vik for every opin-
ion candidate i in relation type k, where vik = 1
means i associates with an implicit argument in
relation k. Given the binary variables xiy, uij , vik,
it is easy to recover the entity and relation assign-
ment by checking which spans are labeled as opin-
ion entities, and which opinion span and argument
span form an opinion relation.
The objective function is defined as a linear
combination of the potentials from different pre-
dictors with a parameter ? to balance the contribu-
tion of two components: opinion entity identifica-
tion and opinion relation extraction.
argmax
x,u,v
?
?
i?S
?
z
fizxiz
+ (1? ?)
?
k
?
i?O
?
??
j?Ak
rijuij + ri?vik
?
?
(1)
It is subject to the following linear constraints:
Constraint 1: Uniqueness. For each span i, we
must assign one and only one label z, where z ?
{H,D, T,N}.
?
z
xiz = 1
Constraint 2: Non-overlapping. If two spans i and
j overlap, then at most one of the spans can be
assigned to a non-NONE entity label: H,D, T .
?
z 6=N
xiz +
?
z 6=N
xjz ? 1
Constraint 3: Consistency between the opinion-
arg and opinion-implicit-arg classifiers. For an
opinion candidate i, if it is predicted to have an
implicit argument in relation k, vik = 1, then no
argument candidate should form a relation with i.
If vik = 0, then there exists some argument can-
didate j ? Ak such that uij = 1. We introduce
two auxiliary binary variables aik and bik to limit
the maximum number of relations associated with
each opinion candidate to be less than or equal to
three5. When vik = 1, aik and bik have to be 0.
?
j?Ak
uij = 1? vik + aik + bik
aik ? 1? vik, bik ? 1? vik
Constraint 4: Consistency between opinion-arg
classifier and opinion entity extractor. Suppose
an argument candidate j in relation k is assigned
an argument label by the entity extractor, that is
xjz = 1 (z = T for IS-ABOUT relation and z = H
for IS-FROM relation), then there exists some opin-
ion candidates that associate with j. Similar to
constraint 3, we introduce auxiliary binary vari-
ables cj and dj to enforce that an argument j links
to at most three opinion expressions. If xjz = 0,
then no relations should be extracted for j.
?
i?O
uij = xjz + cjk + djk
cjk ? xjz, djk ? xjz
Constraint 5: Consistency between the opinion-
implicit-arg classifier and opinion entity extractor.
When an opinion candidate i is predicted to asso-
ciate with an implicit argument in relation k, that
is vik = 1, then we allow xiD to be either 1 or
0 depending on the confidence of labeling i as an
opinion expression. When vik = 0, there exisits
some opinion argument associated with the opin-
ion candidate, and we enforce xiD = 1, which
means the entity extractor agrees to label i as an
opinion expression.
vik + xiD ? 1
Note that in our ILP formulation, the label
assignment for a candidate span involves one
multiple-choice decision among different opinion
entity labels and the ?NONE? entity label. The
scores of different label assignments are compara-
ble for the same span since they come from one
entity extraction model. This makes our ILP for-
mulation advantageous over the ILP formulation
proposed in Choi et al (2006), which needs m bi-
nary decisions for a candidate span, wherem is the
number of types of opinion entities, and the score
for each possible label assignment is obtained by
5It is possible to add more auxiliary variables to allow
more than three arguments to link to an opinion expression,
but this rarely happens in our experiments. For the IS-FROM
relation, we set aik = 0, bik = 0 since an opinion expression
usually has only one holder.
1644
the sum of raw scores from m independent extrac-
tion models. This design choice also allows us
to easily deal with multiple types of opinion ar-
guments and opinion relations.
4 Experiments
For evaluation, we used version 2.0 of the MPQA
corpus (Wiebe et al, 2005; Wilson, 2008), a
widely used data set for fine-grained opinion anal-
ysis.6 We considered the subset of 482 docu-
ments7 that contain attitude and target annotations.
There are a total of 9,471 sentences with opinion-
related labels at the phrase level. We set aside 132
documents as a development set and use 350 doc-
uments as the evaluation set. All experiments em-
ploy 10-fold cross validation on the evaluation set;
the average over the 10 runs is reported.
Our gold standard opinion expressions, opinion
targets and opinion holders correspond to the di-
rect subjective annotations, target annotations and
agent annotations, respectively. The IS-FROM re-
lation is obtained from the agent attribute of each
opinion expression. The IS-ABOUT relation is ob-
tained from the attitude annotations: each opinion
expression is annotated with attitude frames and
each attitude frame is associated with a list of tar-
gets. The relations may overlap: for example, in
the following sentence, the target of relation 1 con-
tains relation 2.
[John]H1 is happyO1 because [[he]H2
lovesO2 [being at Enderly Park]T2]T1 .
We discard relations that contain sub-relations be-
cause we believe that identifying the sub-relations
usually is sufficient to recover the discarded rela-
tions. (Prediction of overlapping relations is con-
sidered as future work.) In the example above, we
will identify (loves, being at Enderly Park) as an
IS-ABOUT relation and happy as an opinion ex-
pression associated with an implicit target. Table 1
shows some statistics of the corpus.
We adopted the evaluation metrics for entity and
relation extraction from Choi et al (2006), which
include precision, recall, and F1-measure accord-
ing to overlap and exact matching metrics.8 We
6Available at http://www.cs.pitt.edu/mpqa/.
7349 news articles from the original MPQA corpus, 84
Wall Street Journal articles (Xbank), and 48 articles from the
American National Corpus.
8Overlap matching considers two spans to match if they
overlap, while exact matching requires two spans to be ex-
actly the same.
Opinion Target Holder
TotalNum 5849 4676 4244
Opinion-arg Relations Implicit Relations
IS-ABOUT 4823 1302
IS-FROM 4662 1187
Table 1: Data Statistics of the MPQA Corpus.
will focus our discussion on results obtained us-
ing overlap matching, since the exact boundaries
of opinion entities are hard to define even for hu-
man annotators (Wiebe et al, 2005).
We trained CRFs for opinion entity identifica-
tion using the following features: indicators for
words, POS tags, and lexicon features (the sub-
jectivity strength of the word in the Subjectivity
Lexicon). All features are computed for the cur-
rent token and tokens in a [?1,+1] window. We
used L2-regularization; the regularization param-
eter was tuned using the development set. We
trained the classifiers for relation extraction using
L1-regularized logistic regression with default pa-
rameters using the LIBLINEAR (Fan et al, 2008)
package. For joint inference, we used GLPK9 to
provide the optimal ILP solution. The parameter
? was tuned using the development set.
4.1 Baseline Methods
We compare our approach to several pipeline base-
lines. Each extracts opinion entities first using
the same CRF employed in our approach, and
then predicts opinion relations on the opinion en-
tity candidates obtained from the CRF prediction.
Three relation extraction techniques were used in
the baselines:
? Adj: Inspired by the adjacency rule used
in Hu and Liu (2004), it links each argu-
ment candidate to its nearest opinion candi-
date. Arguments that do not link to any opin-
ion candidate are discarded. This is also used
as a strong baseline in Choi et al (2006).
? Syn: Links pairs of opinion and argument
candidates that present prominent syntactic
patterns. (We consider the syntactic patterns
listed in Section 3.2.1.) Previous work also
demonstrates the effectiveness of syntactic
information in opinion extraction (Johansson
and Moschitti, 2012).
9http://www.gnu.org/software/glpk/
1645
Opinion Expression Opinion Target Opinion Holder
Method P R F1 P R F1 P R F1
CRF 82.21 66.15 73.31 73.22 48.58 58.41 72.32 49.09 58.48
CRF+Adj 82.21 66.15 73.31 80.87 42.31 55.56 75.24 48.48 58.97
CRF+Syn 82.21 66.15 73.31 81.87 30.36 44.29 78.97 40.20 53.28
CRF+RE 83.02 48.99 61.62 85.07 22.01 34.97 78.13 40.40 53.26
Joint-Model 71.16 77.85 74.35? 75.18 57.12 64.92?? 67.01 66.46 66.73??
CRF 66.60 52.57 58.76 44.44 29.60 35.54 65.18 44.24 52.71
CRF+Adj 66.60 52.57 58.76 49.10 25.81 33.83 68.03 43.84 53.32
CRF+Syn 66.60 52.57 58.76 50.26 18.41 26.94 74.60 37.98 50.33
CRF+RE 69.27 40.09 50.79 60.45 15.37 24.51 75 38.79 51.13
Joint-Model 57.39 62.40 59.79? 49.15 38.33 43.07?? 62.73 62.22 62.47??
Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
the bottom table uses exact). Two-tailed t-test results are shown on F1 measure for our method compared to the other baselines
(statistical significance is indicated with ?(p < 0.05), ??(p < 0.005)).
IS-ABOUT IS-FROM
Method P R F1 P R F1
CRF+Adj 73.65 37.34 49.55 70.22 41.58 52.23
CRF+Syn 76.21 28.28 41.25 77.48 36.63 49.74
CRF+RE 78.26 20.33 32.28 74.81 37.55 50.00
CRF+Adj-merged-10-best 25.05 61.18 35.55 30.28 62.82 40.87
CRF+Syn-merged-10-best 41.60 45.66 43.53 48.08 54.03 50.88
CRF+RE-merged-10-best 51.60 33.09 40.32 47.73 54.40 50.84
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63??
Table 3: Performance on opinion relation extraction using the overlap metric.
? RE: Predicts opinion relations by employ-
ing the opinion-arg classifier and opinion-
implicit-arg classifier. First, the opinion-arg
classifier identifies pairs of opinion and argu-
ment candidates that form valid opinion rela-
tions, and then the opinion-implicit-arg clas-
sifier is used on the remaining opinion candi-
dates to further identify opinion expressions
without explicit arguments.
We report results using opinion entity candi-
dates from the best CRF output and from the
merged 10-best CRF output.10 The motivation of
merging the 10-best output is to increase recall for
the pipeline methods.
5 Results
Table 2 shows the results of opinion entity identi-
fication using both overlap and exact metrics. We
compare our approach with the pipeline baselines
and CRF (the first step of the pipeline). We can
see that our joint inference approach significantly
outperforms all the baselines in F1 measure on ex-
tracting all types of opinion entities. In general,
10It is similar to the merged 10-best baseline in Choi et
al. (2006). If an entity Ei extracted by the ith-best sequence
overlaps with an entity Ej extracted by the jth-best sequence,
where i ? j, then we discard Ej . If Ei and Ej do not over-
lap, then we consider both entities.
by adding the relation extraction step, the pipeline
baselines are able to improve precision over the
CRF but fail at recall. CRF+Syn and CRF+Adj
provide the same performance as CRF, since the
relation extraction step only affects the results of
opinion arguments. By incorporating syntactic
information, CRF+Syn provides better precision
than CRF+Adj on extracting arguments at the ex-
pense of recall. This indicates that using simple
syntactic rules would mistakenly filter many cor-
rect relations. By using binary classifiers to pre-
dict relations, CRF+RE produces high precision
on opinion and target extraction but also results in
very low recall. Using the exact metric, we ob-
serve the same general trend in the results as the
overlap metric. The scores are lower since the
metric is much stricter.
Table 3 shows the results of opinion relation ex-
traction using the overlap metric. We compare our
approach with pipelined baselines in two settings:
one employs relation extraction on 1-best output
of CRF (top half of table) and the other employs
the merged 10-best output of CRF (bottom half of
table). We can see that in general, using merged
10-best CRF outputs boosts the recall while sac-
rificing precision. This is expected since merging
the 10-best CRF outputs favors candidates that are
1646
IS-ABOUT Relation Extraction IS-FROM Relation Extraction
Method P R F1 P R F1
ILP-W/O-ENTITY 49.10 40.48 44.38 44.77 58.24 50.63
ILP-W-SINGLE-RE 63.88 49.35 55.68 53.64 65.02 58.78
ILP-W/O-IMPLICIT-RE 62.00 44.73 51.97 73.23 51.28 60.32
Joint-Model 64.38 51.20 57.04?? 64.97 58.61 61.63?
Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
believed to be more accurate by the CRF predictor.
If CRF makes mistakes, the mistakes will propa-
gate to the relation extraction step. The poor per-
formance on precision further confirms the error
propagation problem in the pipeline approaches.
In contrast, our joint-inference method success-
fully boosts the recall while maintaining reason-
able precision. This demonstrates that joint infer-
ence can effectively leverage the advantage of in-
dividual predictors and limit error propagation.
To demonstrate the effectiveness of different
potentials in our joint inference model, we con-
sider three variants of our ILP formulation that
omit some potentials in the joint inference: one
is ILP-W/O-ENTITY, which extracts opinion rela-
tions without integrating information from opin-
ion entity identification; one is ILP-W-SINGLE-RE,
which focuses on extracting a single opinion re-
lation and ignores the information from the other
relation; the third one is ILP-W/O-IMPLICIT-RE,
which omits the potential for opinion-implicit-arg
relation and assumes every opinion expression is
linked to an explicit argument. The objective func-
tion of ILP-W/O-ENTITY can be represented as
argmax
u
?
k
?
i?O
?
j?Ak
rijuij (2)
which is subject to constraints on uij to enforce
relations to not overlap and limit the maximum
number of relations that can be extracted for each
opinion expression and each argument. For ILP-
W-SINGLE-RE, we simply remove the variables as-
sociated with one opinion relation in the objective
function (1) and constraints. The formulation of
ILP-W/O-IMPLICIT-RE removes the variables as-
sociated with potential ri in the objective function
and corresponding constraints. It can be viewed
as an extension to the ILP approach in Choi et al
(2006) that includes opinion targets and uses sim-
pler ILP formulation with only one parameter and
fewer binary variables and constraints to represent
entity label assignments 11.
11We compared the proposed ILP formulation with the ILP
Table 4 shows the results of these methods on
opinion relation extraction. We can see that with-
out the knowledge of the entity extractor, ILP-
W/O-ENTITY provides poor performance on both
relation extraction tasks. This confirms the effec-
tiveness of leveraging knowledge from entity ex-
tractor and relation extractor. The improvement
yielded by our approach over ILP-W-SINGLE-RE
demonstrates the benefit of jointly optimizing dif-
ferent types of opinion relations. Our approach
also outperforms ILP-W/O-IMPLICIT-RE, which
does not take into account implicit relations. The
results demonstrate that incorporating knowledge
of implicit opinion relations is important.
6 Discussion
We note that the joint inference model yields a
clear improvement on recall but not on precision
compared to the CRF-based baselines. Analyz-
ing the errors, we found that the joint model ex-
tracts comparable number of opinion entities com-
pared to the gold standard, while the CRF-based
baselines extract significantly fewer opinion enti-
ties (around 60% of the number of entities in the
gold standard). With more extracted opinion enti-
ties, the precision is sacriced but recall is boosted
substantially, and overall we see an increase in
F-measure. We also found that a good portion
of errors were made because the generated candi-
dates failed to cover the correct solutions. Recall
that the joint model finds the global optimal solu-
tion over a set of opinion entity and relation can-
didates, which are obtained from the n-best CRF
predictions and constituents in the parse tree that
satisfy certain syntactic patterns. It is possible
that the generated candidates do not contain the
gold standard answers. For example, our model
failed to identify the IS-ABOUT relation (offers,
general aid) from the following sentence Powell
had contacted ... and received offersO1 of [gen-
formulation in Choi et al (2006) on extracting opinion hold-
ers, opinion expressions and IS-FROM relations, and showed
that the proposed ILP formulation performs better on all three
extraction tasks.
1647
eral aid]T1 ... because both the CRF predictor and
syntactic heuristics fail to capture (offers, general
aid) as a potential relation candidate. By applying
simple heuristics such as treating all verbs or verb
phrases as opinion candidates would not help be-
cause it would introduce a large number of nega-
tive candidates and lower the accuracy of relation
extraction (only 52% of the opinion expressions
are verbs or verb phrases and 64% of the opinion
targets are noun or noun phrases in the corpus we
used). Therefore a more effective candidate gen-
eration method is needed to allow more candidates
while limiting the number of negative candidates.
We also observed incorrect parsing to be a cause of
error. We hope to study ways to account for such
errors in our approach as future work.
For computational time, our ILP formulation
can be solved very efficiently using advanced ILP
solvers. In our experiment, using GLPK?s branch-
and-cut solver took 0.2 seconds to produce opti-
mal ILP solutions for 1000 sentences on a machine
with Intel Core 2 Duo CPU and 4GB RAM.
7 Conclusion
In this paper we propose a joint inference ap-
proach for extracting opinion-related entities and
opinion relations. We decompose the task into
different subproblems, and jointly optimize them
using constraints that aim to encourage their con-
sistency and reduce prediction uncertainty. We
show that our approach can effectively integrate
knowledge from different predictors and achieve
significant improvements in overall performance
for opinion extraction. For future work, we plan to
extend our model to handle more complex opinion
relations, e.g. nesting or cross-sentential relations.
This can be potentially addressed by incorporat-
ing more powerful predictors and more complex
linguistic constraints.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant 12475008 and NSF grant BCS-
0904822. We thank Igor Labutov for helpful dis-
cussion and suggestions, Ainur Yessenalina for
early discussion of the work, as well as the reviews
for helpful comments.
References
E. Breck, Y. Choi, and C. Cardie. 2007. Identifying
expressions of opinion in context. In Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 2683?2688. Morgan Kaufmann
Publishers Inc.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 355?362.
Association for Computational Linguistics.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 431?439. Association for Computational
Linguistics.
D. Das, A.F.T. Martins, and N.A. Smith. 2012. An
exact dual decomposition algorithm for shallow se-
mantic parsing with constraints. Proceedings of*
SEM.[ii, 10, 50].
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
M. Hu and B. Liu. 2004. Mining opinion features
in customer reviews. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
755?760. Menlo Park, CA; Cambridge, MA; Lon-
don; AAAI Press; MIT Press; 1999.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 519?527. As-
sociation for Computational Linguistics.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76. Association for Com-
putational Linguistics.
Richard Johansson and Alessandro Moschitti. 2012.
Relational features in fine-grained opinion analysis.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?
8. Association for Computational Linguistics.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007.
Extracting aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
1648
Language Learning (EMNLP-CoNLL), pages 1065?
1074.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
K. Liu, L. Xu, and J. Zhao. 2012. Opinion target
extraction using word-based translation model. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st
international jont conference on Artifical intelli-
gence, pages 1199?1204. Morgan Kaufmann Pub-
lishers Inc.
G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics, 37(1):9?
27.
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik, and
D. Crystal. 1985. A comprehensive grammar of
the English language, volume 397. Cambridge Univ
Press.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. Defense Technical Information Center.
J. Ruppenhofer, S. Somasundaran, and J. Wiebe. 2008.
Finding the sources and targets of subjective expres-
sions. In Proceedings of LREC.
Vivek Srikumar and Dan Roth. 2011. A joint model
for extended semantic role labeling. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 129?139. Association
for Computational Linguistics.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 817?824. Asso-
ciation for Computational Linguistics.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2):165?
210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399?433.
Theresa Wilson. 2008. Fine-Grained Subjectivity
Analysis. Ph.D. thesis, Ph. D. thesis, University of
Pittsburgh. Intelligent Systems Program.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 3-Volume
3, pages 1533?1541. Association for Computational
Linguistics.
B. Yang and C. Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random
fields. In Proceedings of the conference on Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470. Asso-
ciation for Computational Linguistics.
1649
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 325?335,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Context-aware Learning for Sentence-level Sentiment Analysis
with Posterior Regularization
Bishan Yang
Department of Computer Science
Cornell University
bishan@cs.cornell.edu
Claire Cardie
Department of Computer Science
Cornell University
cardie@cs.cornell.edu
Abstract
This paper proposes a novel context-aware
method for analyzing sentiment at the
level of individual sentences. Most ex-
isting machine learning approaches suf-
fer from limitations in the modeling of
complex linguistic structures across sen-
tences and often fail to capture non-
local contextual cues that are important
for sentiment interpretation. In contrast,
our approach allows structured modeling
of sentiment while taking into account
both local and global contextual infor-
mation. Specifically, we encode intu-
itive lexical and discourse knowledge as
expressive constraints and integrate them
into the learning of conditional random
field models via posterior regularization.
The context-aware constraints provide ad-
ditional power to the CRF model and can
guide semi-supervised learning when la-
beled data is limited. Experiments on
standard product review datasets show that
our method outperforms the state-of-the-
art methods in both the supervised and
semi-supervised settings.
1 Introduction
The ability to extract sentiment from text is cru-
cial for many opinion-mining applications such as
opinion summarization, opinion question answer-
ing and opinion retrieval. Accordingly, extract-
ing sentiment at the fine-grained level (e.g. at the
sentence- or phrase-level) has received increasing
attention recently due to its challenging nature and
its importance in supporting these opinion analysis
tasks (Pang and Lee, 2008).
In this paper, we focus on the task of sentence-
level sentiment classification in online reviews.
Typical approaches to the task employ supervised
machine learning algorithms with rich features
and take into account the interactions between
words to handle compositional effects such as po-
larity reversal (e.g. (Nakagawa et al, 2010;
Socher et al, 2013)). Still, their methods can en-
counter difficulty when the sentence on its own
does not contain strong enough sentiment signals
(due to the lack of statistical evidence or the re-
quirement for background knowledge). Consider
the following review for example,
1. Hearing the music in real stereo is a true reve-
lation. 2. You can feel that the music is no longer
constrained by the mono recording. 3. In fact, it
is more like the players are performing on a stage
in front of you ...
Existing feature-based classifiers may be effective
in identifying the positive sentiment of the first
sentence due to the use of the word revelation,
but they could be less effective in the last two sen-
tences due to the lack of explicit sentiment signals.
However, if we examine these sentences within the
discourse context, we can see that: the second sen-
tence expresses sentiment towards the same aspect
? the music ? as the first sentence; the third sen-
tence expands the second sentence with the dis-
course connective In fact. These discourse-level
relations help indicate that sentence 2 and 3 are
likely to have positive sentiment as well.
The importance of discourse for sentiment anal-
ysis has become increasingly recognized. Most
existing work considers discourse relations be-
tween adjacent sentences or clauses and incor-
porates them as constraints (Kanayama and Na-
sukawa, 2006; Zhou et al, 2011) or features in
classifiers Trivedi and Eisenstein (2013; Lazari-
dou et al (2013). Very little work has explored
long-distance discourse relations for sentiment
analysis. Somasundaran et al (2008) defines
coreference relations on opinion targets and ap-
plies them to constrain the polarity of sentences.
325
However, the discourse relations were obtained
from fine-grained annotations and implemented as
hard constraints on polarity.
Obtaining sentiment labels at the fine-grained
level is costly. Semi-supervised techniques have
been proposed for sentence-level sentiment classi-
fication (T?ackstr?om and McDonald, 2011a; Qu et
al., 2012). However, they rely on a large amount
of document-level sentiment labels that may not
be naturally available in many domains.
In this paper, we propose a sentence-level senti-
ment classification method that can (1) incorporate
rich discourse information at both local and global
levels; (2) encode discourse knowledge as soft
constraints during learning; (3) make use of un-
labeled data to enhance learning. Specifically, we
use the Conditional Random Field (CRF) model
as the learner for sentence-level sentiment classi-
fication, and incorporate rich discourse and lexi-
cal knowledge as soft constraints into the learn-
ing of CRF parameters via Posterior Regulariza-
tion (PR) (Ganchev et al, 2010). As a framework
for structured learning with constraints, PR has
been successfully applied to many structural NLP
tasks (Ganchev et al, 2009; Ganchev et al, 2010;
Ganchev and Das, 2013). Our work is the first to
explore PR for sentiment analysis. Unlike most
previous work, we explore a rich set of structural
constraints that cannot be naturally encoded in the
feature-label form, and show that such constraints
can improve the performance of the CRF model.
We evaluate our approach on the sentence-
level sentiment classification task using two stan-
dard product review datasets. Experimental re-
sults show that our model outperforms state-of-
the-art methods in both the supervised and semi-
supervised settings. We also show that dis-
course knowledge is highly useful for improving
sentence-level sentiment classification.
2 Related Work
There has been a large amount of work on sen-
timent analysis at various levels of granular-
ity (Pang and Lee, 2008). In this paper, we focus
on the study of sentence-level sentiment classifi-
cation. Existing machine learning approaches for
the task can be classified based on the use of two
ideas. The first idea is to exploit sentiment sig-
nals at the sentence level by learning the relevance
of sentiment and words while taking into account
the context in which they occur: Nakagawa et
al. (2010) uses tree-CRF to model word interac-
tions based on dependency tree structures; Choi
and Cardie (2008) applies compositional inference
rules to handle polarity reversal; Socher et al
(2011) and Socher et al (2013) compute composi-
tional vector representations for words and phrases
and use them as features in a classifier.
The second idea is to exploit sentiment signals
at the inter-sentential level. Polanyi and Zaenen
(2006) argue that discourse structure is important
in polarity classification. Various attempts have
been made to incorporate discourse relations into
sentiment analysis: Pang and Lee (2004) explored
the consistency of subjectivity between neighbor-
ing sentences; Mao and Lebanon (2007),McDon-
ald et al (2007), and T?ackstr?om and McDonald
(2011a) developed structured learning models to
capture sentiment dependencies between adjacent
sentences; Kanayama and Nasukawa (2006) and
Zhou et al (2011) use discourse relations to con-
strain two text segments to have either the same
polarity or opposite polarities; Trivedi and Eisen-
stein (2013) and Lazaridou et al (2013) encode
the discourse connectors as model features in su-
pervised classifiers. Very little work has explored
long-distance discourse relations. Somasundaran
et al (2008) define opinion target relations and ap-
ply them to constrain the polarity of text segments
annotated with target relations. Recently, Zhang
et al (2013) explored the use of explanatory dis-
course relations as soft constraints in a Markov
Logic Network framework for extracting subjec-
tive text segments.
Leveraging both ideas, our approach exploits
sentiment signals from both intra-sentential and
inter-sentential context. It has the advantages of
utilizing rich discourse knowledge at different lev-
els of context and encoding it as soft constraints
during learning.
Our approach is also semi-supervised. Com-
pared to the existing work on semi-supervised
learning for sentence-level sentiment classification
(T?ackstr?om and McDonald, 2011a; T?ackstr?om and
McDonald, 2011b; Qu et al, 2012), our work
does not rely on a large amount of coarse-grained
(document-level) labeled data, instead, distant
supervision mainly comes from linguistically-
motivated constraints.
Our work also relates to the study of posterior
regularization (PR) (Ganchev et al, 2010). PR has
been successfully applied to many structured NLP
326
tasks such as dependency parsing, information ex-
traction and cross-lingual learning tasks (Ganchev
et al, 2009; Bellare et al, 2009; Ganchev et al,
2010; Ganchev and Das, 2013). Most previous
work using PR mainly experiments with feature-
label constraints. In contrast, we explore a rich
set of linguistically-motivated constraints which
cannot be naturally formulated in the feature-label
form. We also show that constraints derived from
the discourse context can be highly useful for dis-
ambiguating sentence-level sentiment.
3 Approach
In this section, we present the details of our pro-
posed approach. We formulate the sentence-level
sentiment classification task as a sequence label-
ing problem. The inputs to the model are sentence-
segmented documents annotated with sentence-
level sentiment labels (positive, negative or neu-
tral) along with a set of unlabeled documents.
During prediction, the model outputs sentiment la-
bels for a sequence of sentences in the test docu-
ment. We utilize conditional random fields and use
Posterior Regularization (PR) to learn their param-
eters with a rich set of context-aware constraints.
In what follows, we first briefly describe the
framework of Posterior Regularization. Then we
introduce the context-aware constraints derived
based on intuitive discourse and lexical knowl-
edge. Finally we describe how to perform learning
and inference with these constraints.
3.1 Posterior Regularization
PR is a framework for structured learning with
constraints (Ganchev et al, 2010). In this work,
we apply PR in the context of CRFs for sentence-
level sentiment classification.
Denote x as a sequence of sentences within a
document and y as a vector of sentiment labels
associated with x. The CRF model the following
conditional probabilities:
p
?
(y|x) =
exp(? ? f(x,y))
Z
?
(x)
where f(x,y) are the model features, ? are the
model parameters, and Z
?
(x) =
?
y
exp(? ?
f(x,y)) is a normalization constant. The objec-
tive function for a standard CRF is to maximize
the log-likelihood over a collection of labeled doc-
uments plus a regularization term:
max
?
L(?) = max
?
?
(x,y)
log p
?
(y|x)?
||?||
2
2
2?
2
PR makes the assumption that the labeled data
we have is not enough for learning good model
parameters, but we have a set of constraints on the
posterior distribution of the labels. We can define
the set of desirable posterior distrbutions as
Q = {q(Y) : E
q
[?(X,Y)] = b} (1)
where ? is a constraint function, b is a vector of
desired values of the expectations of the constraint
functions under the distribution q
1
. Note that the
distribution q is defined over a collection of un-
labeled documents where the constraint functions
apply, and we assume independence between doc-
uments.
The PR objective can be written as the origi-
nal model objective penalized with a regulariza-
tion term, which minimizes the KL-divergence be-
tween the desired model posteriors and the learned
model posteriors with an L2 penalty
2
for the con-
straint violations.
max
?
L(?)?min
q?Q
{KL(q(Y)||p
?
(Y|X))
+ ?||E
q
[?(X,Y)]? b||
2
2
}
(2)
The objective can be optimized by an EM-like
scheme that iteratively solves the minimization
problem and the maximization problem. Solving
the minimization problem is equivalent to solving
its dual since the objective is convex. The dual
problem is
argmax
?
? ? b? logZ
?
(X)?
1
4?
||?||
2
2
(3)
We optimize the objective function 2 using
stochastic projected gradient, and compute the
learning rate using AdaGrad (Duchi et al, 2010).
3.2 Context-aware Posterior Constraints
We develop a rich set of context-aware poste-
rior constraints for sentence-level sentiment anal-
ysis by exploiting lexical and discourse knowl-
edge. Specifically, we construct the lexical con-
straints by extracting sentiment-bearing patterns
1
In general, inequality constraints can also be used. We
focus on the equality constraints since we found them to ex-
press the sentiment-relevant constraints well.
2
Other convex functions can be used for the penalty. We
use L2 norm because it works well in practice. ? is a regular-
ization constant
327
within sentences and construct the discourse-level
constraints by extracting discourse relations that
indicate sentiment coherence or sentiment changes
both within and across sentences. Each constraint
can be formulated as equality between the expec-
tation of a constraint function value and a desired
value set by prior knowledge. The equality is not
strictly enforced (due to the regularization in the
PR objective 2). Therefore all the constraints are
applied as soft constraints. Table 1 provides in-
tuitive description and examples for all the con-
straints used in our model.
Lexical Patterns The existence of a polarity-
carrying word alone may not correctly indicate the
polarity of the sentence, as the polarity can be re-
versed by other polarity-reversing words. We ex-
tract lexical patterns that consist of polar words
and negators
3
, and apply the heuristics based on
compositional semantics (Choi and Cardie, 2008)
to assign a sentiment value to each pattern.
We encode the extracted lexical patterns along
with their sentiment values as feature-label con-
straints. The constraint function can be written as
?
w
(x, y) =
?
i
f
w
(x
i
, y
i
)
where f
w
(x
i
, y
i
) is a feature function which has
value 1 when sentence x
i
contains the lexical pat-
tern w and its sentiment label y
i
equals to the ex-
pected sentiment value and has value 0 otherwise.
The constraint expectation value is set to be the
prior probability of associating w with its senti-
ment value. Note that sentences with neutral senti-
ment can also contain such lexical patterns. There-
fore we allow the lexical patterns to be assigned a
neutral sentiment with a prior probability r
0
(we
compute this value as the empirical probability of
neutral sentiment in the training documents). Us-
ing the polarity indicated by lexical patterns to
constrain the sentiment of sentences is quite ag-
gressive. Therefore we only consider lexical pat-
terns that are strongly discriminative (many opin-
ion words in the lexicon only indicate sentiment
with weak strength). The selected lexical patterns
include a handful of seed patterns (such as ?pros?
and ?cons?) and the lexical patterns that have high
precision (larger then 0.9) of predicting sentiment
in the training data.
3
The polar words are identified using the MPQA lexicon
and the negators are identified using a handful of seed words
extended by the General Inquirer dictionary and WordNet as
described in (Choi and Cardie, 2008).
Discourse Connectives. Lexical patterns can
be limited in capturing contextual information
since they only look at interactions between words
within an expression. To capture context at the
clause or sentence level, we consider discourse
connectives, which are cue phrases or words that
indicate discourse relations between adjacent sen-
tences or clauses. To identify discourse connec-
tives, we apply a discourse tagger trained on the
Penn Discourse Treebank (Prasad et al, 2008)
4
to our data. Discourse connectives are tagged with
four senses: Expansion, Contingency, Compari-
son, Temporal.
Discourse connectives can operate at both intra-
sentential and inter-sentential level. For example,
the word ?although? is often used to connect two
polar clauses within a sentence, while the word
?however? is often used to at the beginning of
the sentence to connect two polar sentences. It
is important to distinguish these two types of dis-
course connectives. We consider a discourse con-
nective to be intra-sentential if it has the Com-
parison sense and connects two polar clauses with
opposite polarities (determined by the lexical pat-
terns). We construct a feature-label constraint for
each intra-sentential discourse connective and set
its expected sentiment value to be neutral.
Unlike the intra-sentential discourse connec-
tives, the inter-sentential discourse connectives
can indicate sentiment transitions between sen-
tences. Intuitively, discourse connectives with
the senses of Expansion (e.g. also, for example,
furthermore) and Contingency (e.g. as a result,
hence, because) are likely to indicate sentiment
coherence; discourse connectives with the sense
of Comparison (e.g. but, however, nevertheless)
are likely to indicate sentiment changes. This in-
tuition is reasonable but it assumes the two sen-
tences connected by the discourse connective are
both polar sentences. In general, discourse con-
nectives can also be used to connect non-polar
(neutral) sentences. Thus it is hard to directly
constrain the posterior expectation for each type
of sentiment transitions using inter-sentential dis-
course connectives.
Instead, we impose constraints on the model
posteriors by reducing constraint violations. We
4
http://www.cis.upenn.edu/
?
epitler/
discourse.html
328
Types Description and Examples Inter-sentential
Lexical patterns
The sentence containing a polar lexical pattern w tends to have the polarity
indicated by w. Example lexical patterns are annoying, hate, amazing, not dis-
appointed, no concerns, favorite, recommend.
Discourse Connectives
(clause)
The sentence containing a discourse connective cwhich connects its two clauses
that have opposite polarities indicated by the lexical patterns tends to have neu-
tral sentiment. Example connectives are while, although, though, but.
Discourse Connectives
(sentence)
Two adjacent sentences which are connected by a discourse connective c tends
to have the same polarity if c indicates a Expansion or Contingency relation,
e.g. also, for example, in fact, because ; opposite polarities if c indicates a
Comparison relation, e.g. otherwise, nevertheless, however.
X
Coreference
The sentences which contain coreferential entities appeared as targets of opinion
expressions tend to have the same polarity.
X
Listing patterns
A series of sentences connected via a listing tend to have the same polarity.
X
Global labels
The sentence-level polarity tends to be consistent with the document-level po-
larity.
X
Table 1: Summarization of Posterior Constraints for Sentence-level Sentiment Classification
define the following constraint function:
?
c,s
(x, y) =
?
i
f
c,s
(x
i
, y
i
, y
i?1
)
where c denotes a discourse connective, s indi-
cates its sense, and f
c,s
is a penalty function that
takes value 1.0 when y
i
and y
i?1
form a contradic-
tory sentiment transition, that is, y
i
6=
polar
y
i?1
if
s ? {Expansion,Contingency}, or y
i
=
polar
y
i?1
if s = Comparison. The desired value for the con-
straint expectation is set to 0 so that the model is
encouraged to have less constraint violations.
Opinion Coreference Sentences in a discourse
can be linked by many types of coherence rela-
tions (Jurafsky et al, 2000). Coreference is one
of the commonly used relations in written text.
In this work, we explore coreference in the con-
text of sentence-level sentiment analysis. We con-
sider a set of polar sentences to be linked by the
opinion coreference relation if they contain core-
ferring opinion-related entities. For example, the
following sentences express opinions towards ?the
speaker phone?, ?The speaker phone? and ?it? re-
spectively. As these opinion targets are corefer-
ential (referring to the same entity ?the speaker
phone?), they are linked by the opinion corefer-
ence relation
5
.
My favorite features are the speaker
phone and the radio. The speaker
phone is very functional. I use it in
the car, very audible even with freeway
noise.
5
In general, the opinion-related entities include both the
opinion targets and the opinion holders. In this work, we
only consider the targets since we experiment with single-
author product reviews. The opinion holders can be included
in a similar way as the opinion targets.
Our coreference relations indicated by opinion
targets overlap with the same target relation intro-
duced in (Somasundaran et al, 2009). The dif-
ferences are: (1) we encode the coreference re-
lations as soft constraints during learning instead
of applying them as hard constraints during infer-
ence time; (2) our constraints can apply to both
polar and non-polar sentences; (3) our identifica-
tion of coreference relations is automatic without
any fine-grained annotations for opinion targets.
To extract coreferential opinion targets, we ap-
ply Stanford?s coreference system (Lee et al,
2013) to extract coreferential mentions in the doc-
ument, and then apply a set of syntactic rules to
identify opinion targets from the extracted men-
tions. The syntactic rules correspond to the
shortest dependency paths between an opinion
word and an extracted mention. We consider
the 10 most frequent dependency paths in the
training data. Example dependency paths include
nsubj(opinion, mention), nobj(opinion, mention),
and amod(mention, opinion).
For sentences connected by the opinion coref-
erence relation, we expect their sentiment to be
consistent. To encode this intuition, we define the
following constraint function:
?
coref
(x, y) =
?
i,ant(i)=j,j?0
f
coref
(x
i
, x
j
, y
i
, y
j
)
where ant(i) denotes the index of the sentence
which contains an antecedent target of the target
mentioned in sentence i (the antecedent relations
over pairs of opinion targets can be constructed
using the coreference resolver), and f
coref
is a
penalty function which takes value 1.0 when the
expected sentiment coherency is violated, that is,
y
i
6=
polar
y
j
. Similar to the inter-sentential dis-
329
course connectives, modeling opinion coreference
via constraint violations allows the model to han-
dle neutral sentiment. The expected value of the
constraint functions is set to 0.
Listing Patterns Another type of coherence re-
lations we observe in online reviews is listing,
where a reviewer expresses his/her opinions by
listing a series of statements followed by a se-
quence of numbers. For example, ?1. It?s smaller
than the ipod mini .... 2. It has a removable battery
....?. We expect sentences connected by a listing
to have consistent sentiment. We implement this
constraint in the same form as the coreference con-
straint (the antecedent assignments are constructed
from the numberings).
Global Sentiment Previous studies have
demonstrated the value of document-level sen-
timent in guiding the semi-supervised learning
of sentence-level sentiment (T?ackstr?om and
McDonald, 2011b; Qu et al, 2012). In this work,
we also take into account this information and
encode it as posterior constraints. Note that these
constraints are not necessary for our model and
can be applied when the document-level sentiment
labels are naturally available.
Based on an analysis of the Amazon review
data, we observe that sentence-level sentiment
usually doesn?t conflict with the document-level
sentiment in terms of polarity. For example, the
proportion of negative sentences in the positive
documents is very small compared to the propor-
tion of positive sentences. To encode this intuition,
we define the following constraint function:
?
g
(x, y) =
n
?
i
?(y
i
6=
polar
g)/n
where g ? {positive, negative} denotes the sen-
timent value of a polar document, n is the total
number of sentences in x, and ? is an indicator
function. We hope the expectation of the con-
straint function takes a small value. In our experi-
ments, we set the expected value to be the empiri-
cal estimate of the probability of ?conflicting? sen-
timent in polar documents using the training data.
3.3 Training and Inference
During training, we need to compute the constraint
expectations and the feature expectations under
the auxiliary distribution q at each gradient step.
We can derive q by solving the dual problem in 3:
q(y|x) =
exp(? ? f(x,y) + ? ? ?(x,y))
Z
?,?
(X)
(4)
where Z
?,?
(X) is a normalization constant. Most
of our constraints can be factorized in the same
way as factorizing the model features in the first-
order CRF model, and we can compute the expec-
tations under q very efficiently using the forward-
backward algorithm. However, some of our dis-
course constraints (opinion coreference and list-
ing) can break the tractable structure of the model.
For constraints with higher-order structures, we
use Gibbs Sampling (Geman and Geman, 1984) to
approximate the expectations. Given a sequence
x, we sample a label y
i
at each position i by com-
puting the unnormalized conditional probabilities
p(y
i
= l|y
?i
) ? exp(? ? f(x,y
i
= l,y
?i
) + ? ?
?(x,y
i
= l,y
?i
)) and renormalizing them. Since
the possible label assignments only differ at posi-
tion i, we can make the computation efficient by
maintaining the structure of the coreference clus-
ters and precomputing the constraint function for
different types of violations.
During inference, we find the best label assign-
ment by computing argmax
y
q(y|x). For doc-
uments where the higher-order constraints apply,
we use the same Gibbs sampler as described above
to infer the most likely label assignment, other-
wise, we use the Viterbi algorithm.
4 Experiments
We experimented with two product review
datasets for sentence-level sentiment classifica-
tion: the Customer Review (CR) data (Hu and Liu,
2004)
6
which contains 638 reviews of 14 prod-
ucts such as cameras and cell phones, and the
Multi-domain Amazon (MD) data from the test set
of T?ackstr?om and McDonald (2011a) which con-
tains 294 reivews from 5 different domains. As in
Qu et al (2012), we chose the books, electronics
and music domains for evaluation. Each domain
also comes with 33,000 extra reviews with only
document-level sentiment labels.
We evaluated our method in two settings: su-
pervised and semi-supervised. In the supervised
setting, we treated the test data as unlabeled data
and performed transductive learning. In the semi-
supervised setting, our unlabeled data consists of
6
Available at http://www.cs.uic.edu/
?
liub/
FBS/sentiment-analysis.html.
330
both the available unlabeled data and the test data.
For each domain in the MD dataset, we made
use of no more than 100 unlabeled documents in
which our posterior constraints apply. We adopted
the evaluation schemes used in previous work: 10-
fold cross validation for the CR dataset and 3-fold
cross validation for the MD dataset. We also report
both two-way classification (positive vs. negative)
and three-way classification results (positive, neg-
ative or neutral). We use accuracy as the per-
formance measure. In our tables, boldface num-
bers are statistically significant by paired t-test for
p < 0.05 against the best baseline developed in
this paper
7
.
We trained our model using a CRF incorpo-
rated with the proposed posterior constraints. For
the CRF features, we include the tokens, the part-
of-speech tags, the prior polarities of lexical pat-
terns indicated by the opinion lexicon and the
negator lexicon, the number of positive and neg-
ative tokens and the output of the vote-flip algo-
rithm (Choi and Cardie, 2009). In addition, we in-
clude the discourse connectives as local or transi-
tion features and the document-level sentiment la-
bels as features (only available in the MD dataset).
We set the CRF regularization parameter ? = 1
and set the posterior regularization parameter ?
and ? (a trade-off parameter we introduce to bal-
ance the supervised objective and the posterior
regularizer in 2) by using grid search
8
. For
approximation inference with higher-order con-
straints, we perform 2000 Gibbs sampling itera-
tions where the first 1000 iterations are burn-in it-
erations. To make the results more stable, we con-
struct three Markov chains that run in parallel, and
select the sample with the largest objective value.
All posterior constraints were developed using
the training data on each training fold. For the MD
dataset, we also used the dvd domain as additional
labeled data for developing the constraints.
Baselines. We compared our method to a num-
ber of baselines: (1) CRF: CRF with the same set
of model features as in our method. (2) CRF-
INF: CRF augmented with inference constraints.
We can incorporate the proposed constraints (con-
straints derived from lexical patterns and discourse
connectives) as hard constraints into CRF during
7
Significance test was not conducted over the previous
methods as we do not have their results for each fold.
8
We conducted 10-fold cross-validation on each training
fold with the parameter space: ? : [0.01, 0.05, 0.1, 0.5, 1.0]
and ? : [0.1, 0.5, 1.0, 5.0, 10.0].
Methods CR MD
CRF 81.1 67.0
CRF-inf
lex
80.9 66.4
CRF-inf
disc
81.1 67.2
PR
lex
81.8 69.7
PR 82.7 70.6
Previous work
TreeCRF (Nakagawa et al, 2010) 81.4 -
Dropout LR (Wang and Manning, 2013) 82.1 -
Table 2: Accuracy results (%) for supervised sen-
timent classification (two-way)
Books Electronics Music Avg
VoteFlip 44.6 45.0 47.8 45.8
DocOracle 53.6 50.5 63.0 55.7
CRF 57.4 57.5 61.8 58.9
CRF-inf
lex
56.7 56.4 60.4 57.8
CRF-inf
disc
57.2 57.6 62.1 59.0
PR
lex
60.3 59.9 63.2 61.1
PR 61.6 61.0 64.4 62.3
Previous work
HCRF 55.9 61.0 58.7 58.5
MEM 59.7 59.6 63.8 61.0
Table 3: Accuracy results (%) for semi-supervised
sentiment classification (three-way) on the MD
dataset
inference by manually setting ? in equation 4 to a
large value,
9
. When ? is large enough, it is equiva-
lent to adding hard constraints to the viterbi infer-
ence. To better understand the different effects of
lexical and discourse constraints, we report results
for applying only the lexical constraints (CRF-
INF
lex
) as well as results for applying only the
discourse constraints (CRF-INF
disc
). (3) PR
lex
:
a variant of our PR model which only applies the
lexical constraints. For the three-way classifica-
tion task on the MD dataset, we also implemented
the following baselines: (4) VOTEFLIP: a rule-
based algorithm that leverages the positive, nega-
tive and neutral cues along with the effect of nega-
tion to determine the sentence sentiment (Choi
and Cardie, 2009). (5) DOCORACLE: assigns
each sentence the label of its corresponding doc-
ument.
4.1 Results
We first report results on a binary (positive or neg-
ative) sentence-level sentiment classification task.
For this task, we used the supervised setting and
performed transductive learning for our model.
Table 2 shows the accuracy results. We can see
9
We set ? to 1000 for the lexical constraints and -1000 to
the discourse connective constraints in the experiments
331
Books Electronics Music
pos/neg/neu pos/neg/neu pos/neg/neu
VoteFlip 43/42/47 45/46/44 50/46/46
DocOracle 54/60/49 57/54/42 72/65/52
CRF 47/51/64 60/61/52 67/60/58
CRF-inf
lex
46/52/63 59/61/50 65/59/57
CRF-inf
disc
47/51/64 60/61/52 67/61/59
PR
lex
50/56/66 64/63/53 67/64/59
PR 52/56/68 64/66/53 69/65/60
Table 4: F1 scores for each sentiment cate-
gory (positive, negative and neutral) for semi-
supervised sentiment classification on the MD
dataset
that PR significantly outperforms all other base-
lines in both the CR dataset and the MD dataset
(average accuracy across domains is reported).
The poor performance of CRF-INF
lex
indicates
that directly applying lexical constraints as hard
constraints during inference could only hurt the
performance. CRF-INF
disc
slightly outperforms
CRF but the improvement is not significant. In
contrast, both PR
lex
and PR significantly outper-
form CRF, which implies that incorporating lex-
ical and discourse constraints as posterior con-
straints is much more effective. The superior per-
formance of PR over PR
lex
further suggests that
the proper use of discourse information can signif-
icantly improve accuracy for sentence-level senti-
ment classification.
We also analyzed the model?s performance on a
three-way sentiment classification task. By intro-
ducing the ?neutral? category, the sentiment clas-
sification problem becomes harder. Table 4 shows
the results in terms of accuracy for each domain
in the MD dataset. We can see that both PR and
PR
lex
significantly outperform all other baselines
in all domains. The rule-based baseline VOTE-
FLIP gave the weakest performance because it has
no prediction power on sentences with no opinion
words. DOCORACLE performs much better than
VOTEFLIP and performs especially well on the
Music domain. This indicates that the document-
level sentiment is a very strong indicator of the
sentence-level sentiment label. For the CRF base-
line and its invariants, we observe a similar per-
formance trend as in the two-way classification
task: there is nearly no performance improve-
ment from applying the lexical and discourse-
connective-based constraints during CRF infer-
ence. In contrast, both PR
lex
and PR provide
substantial improvements over CRF. This con-
firms that encoding lexical and discourse knowl-
edge as posterior constraints allows the feature-
based model to gain additional learning power
for sentence-level sentiment prediction. In par-
ticular, incorporating discourse constraints leads
to consistent improvements to our model. This
demonstrates that our modeling of discourse in-
formation is effective and that taking into account
the discourse context is important for improving
sentence-level sentiment analysis. We also com-
pare our results to the previously published results
on the same dataset. HCRF (T?ackstr?om and Mc-
Donald, 2011a) and MEM (Qu et al, 2012) are
two state-of-the-art semi-supervised methods for
sentence-level sentiment classification. We can
see that our best model PR gives the best results
in most categories.
Table 4 shows the results in terms of F1 scores
for each sentiment category (positive, negative and
neutral). We can see that the PR models are able to
provide improvements over all the sentiment cate-
gories compared to all the baselines in general. We
observe that the DOCORACLE baseline provides
very strong F1 scores on the positive and nega-
tive categories especially in the Books and Mu-
sic domains, but very poor F1 on the neutral cate-
gory. This is because it over-predicts the polar sen-
tences in the polar documents, and predicts no po-
lar sentences in the neutral documents. In contrast,
our PR models provide more balanced F1 scores
among all the sentiment categories. Compared to
the CRF baseline and its variants, we found that
the PR models can greatly improve the precision
of predicting positive and negative sentences, re-
sulting in a significant improvement on the pos-
itive/negative F1 scores. However, the improve-
ment on the neutral category is modest. A plausi-
ble explanation is that most of our constraints fo-
cus on discriminating polar sentences. They can
help reduce the errors of misclassifying polar sen-
tences, but the model needs more constraints in
order to distinguish neutral sentences from polar
sentences. We plan to address this issue in future
work.
4.2 Discussion
We analyze the errors to better understand the mer-
its and limitations of the PR model. We found
that the PR model is able to correct many CRF
errors caused by the lack of labeled data. The first
row in Table 5 shows an example of such errors.
332
Example Sentences CRF PR
Example 1: ?neg? If I could, I would like to return it or exchange
for something better.?/neg?
?neu? ? X
Example 2: ?neg? Things I wasn?t a fan of ? the ending was to
cutesy for my taste.?/neg? ?neg? Also, all of the side characters
(particularly the mom, vee, and the teacher) were incredibly flat
and stereotypical to me.?/neg?
?neu? ?pos? ? X
Example 3: ?neg? I also have excessive noise when I talk and
have phone in my pocket while walking.?/neg? ?neu? But other
models are no better.?/neu?
?neg? ?pos? ? ?neg? ?pos? ?
Table 5: Example sentences where PR succeeds and fails to correct the mistakes of CRF
The lexical features return and exchange may
be good indicators of negative sentiment for the
sentence. However, with limited labeled data, the
CRF learner can only associate very weak senti-
ment signals to these features. In contrast, the PR
model is able to associate stronger sentiment sig-
nals to these features by leveraging unlabeled data
for indirect supervision. A simple lexicon-based
constraint during inference time may also correct
this case. However, hard-constraint baselines can
hardly improve the performance in general be-
cause the contributions of different constraints are
not learned and their combination may not lead to
better predictions. This is also demonstrated by
the limited performance of CRF-INF in our exper-
iments.
We also found that the discourse constraints
play an important role in improving the sentiment
prediction. The lexical constraints alone are of-
ten not sufficient since their coverage is limited by
the sentiment lexicon and they can only constrain
sentiment locally. On the contrary, discourse con-
straints are not dependent on sentiment lexicons,
and more importantly, they can provide sentiment
preferences on multiple sentences at the same
time. When combining discourse constraints with
features from different sentences, the PR model
becomes more powerful in disambiguating senti-
ment. The second example in Table 5 shows that
the PR model learned with discourse constraints
correctly predicts the sentiment of two sentences
where no lexical constraints apply.
However, discourse constraints are not always
helpful. One reason is that they do not constrain
the neutral sentiment. As a result they could not
help disambiguate neutral sentiment from polar
sentiment, such as the third example in Table 5.
This is also a problem for most of our lexical con-
straints. In general, it is hard to learn reliable indi-
cators for the neutral sentiment. In the MD dataset,
a neutral label may be given because the sentence
contains mixed sentiment or no sentiment or it is
off-topic. We plan to explore more refined con-
straints that can deal with the neutral sentiment in
future work. Another limitation of the discourse
constraints is that they could be affected by the er-
rors of the discourse parser and the coreference re-
solver. A potential way to address this issue is to
learn discourse constraints jointly with sentiment.
We plan to study this in future research.
5 Conclusion
In this paper, we propose a context-aware ap-
proach for learning sentence-level sentiment. Our
approach incorporates intuitive lexical and dis-
course knowledge as expressive constraints while
training a conditional random field model via pos-
terior regularization. We explore a rich set of
context-aware constraints at both intra- and inter-
sentential levels, and demonstrate their effective-
ness in the analysis of sentence-level sentiment.
While we focus on the sentence-level task, our ap-
proach can be easily extended to handle sentiment
analysis at finer levels of granularity. Our exper-
iments show that our model achieves better accu-
racy than existing supervised and semi-supervised
models for the sentence-level sentiment classifica-
tion task.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and NSF grant
BCS-0904822. We thank Igor Labutov for help-
ful discussion and suggestions; Oscar T?ackstr?om
and Lizhen Qu for providing their Amazon review
datasets; and the anonymous reviewers for helpful
comments and suggestions.
References
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning
333
with expectation constraints. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence, pages 43?50. AUAI Press.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801. Association
for Computational Linguistics.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 590?598. Association for Compu-
tational Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence models
with posterior regularization.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
ACL-IJCNLP, pages 369?377.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001?2049.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, (6):721?741.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Dan Jurafsky, James H Martin, Andrew Kehler, Keith
Vander Linden, and Nigel Ward. 2000. Speech
and language processing: An introduction to natu-
ral language processing, computational linguistics,
and speech recognition, volume 2. MIT Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 355?363. Association
for Computational Linguistics.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In To Appear in Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. Advances in
neural information processing systems, 19:961.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured mod-
els for fine-to-coarse sentiment analysis. In An-
nual Meeting-Association For Computational Lin-
guistics, volume 45, page 432.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786?794.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
Livia Polanyi and Annie Zaenen. 2006. Contextual
valence shifters. In Computing attitude and affect in
text: Theory and applications, pages 1?10. Springer.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Lizhen Qu, Rainer Gemulla, and Gerhard Weikum.
2012. A weakly supervised model for sentence-level
semantic orientation analysis with multiple experts.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 149?159. Association for Computational Lin-
guistics.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
334
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 801?808. Association for Computational Lin-
guistics.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 170?179. Association for Com-
putational Linguistics.
Oscar T?ackstr?om and Ryan McDonald. 2011a. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In Advances in Infor-
mation Retrieval, pages 368?374. Springer.
Oscar T?ackstr?om and Ryan McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis.
Rakshit Trivedi and Jacob Eisenstein. 2013. Discourse
connectors for latent subjectivity in sentiment analy-
sis. In Proceedings of NAACL-HLT, pages 808?813.
Sida Wang and Christopher Manning. 2013. Fast
dropout training. In Proceedings of the 30th Inter-
national Conference on Machine Learning (ICML-
13), pages 118?126.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic.
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,
and Kam-Fai Wong. 2011. Unsupervised discovery
of discourse relations for eliminating intra-sentence
polarity ambiguities. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 162?171. Association for Com-
putational Linguistics.
335
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
