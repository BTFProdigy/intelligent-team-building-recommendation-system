BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 38?45,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The BioScope corpus: annotation for negation, uncertainty and their 
scope in biomedical texts 
 
 
Gy?rgy Szarvas1, Veronika Vincze1, Rich?rd Farkas2 and J?nos Csirik2 
1Department of Informatics 2Research Group on Artificial Intelligence 
University of Szeged Hungarian Academy of Science 
H-6720, Szeged, ?rp?d t?r 2. H-6720, Szeged, Aradi v?rtan?k tere 1. 
{szarvas, vinczev, rfarkas, csirik}@inf.u-szeged.hu 
 
Abstract 
This article reports on a corpus annotation 
project that has produced a freely available re-
source for research on handling negation and 
uncertainty in biomedical texts (we call this 
corpus the BioScope corpus). The corpus con-
sists of three parts, namely medical free texts, 
biological full papers and biological scientific 
abstracts. The dataset contains annotations at 
the token level for negative and speculative 
keywords and at the sentence level for their 
linguistic scope. The annotation process was 
carried out by two independent linguist anno-
tators and a chief annotator ? also responsible 
for setting up the annotation guidelines ? who 
resolved cases where the annotators disagreed. 
We will report our statistics on corpus size, 
ambiguity levels and the consistency of anno-
tations. 
1 Introduction 
Detecting uncertain and negative assertions is es-
sential in most Text Mining tasks where in general, 
the aim is to derive factual knowledge from textual 
data. This is especially so for many tasks in the 
biomedical (medical and biological) domain, 
where these language forms are used extensively in 
textual documents and are intended to express im-
pressions, hypothesised explanations of experi-
mental results or negative findings. Take, for 
example, the clinical coding of medical reports, 
where the coding of a negative or uncertain disease 
diagnosis may result in an over-coding financial 
penalty. Another example from the biological do-
main is interaction extraction, where the aim is to 
mine text evidence for biological entities with cer-
tain relations between them. Here, while an uncer-
tain relation or the non-existence of a relation 
might be of some interest for an end-user as well, 
such information must not be confused with real 
textual evidence (reliable information). A general 
conclusion is that for text mining, extracted infor-
mation that is within the scope of some negative / 
speculative (hedge or soft negation) keyword 
should either be discarded or presented separately 
from factual information.  
Even though many successful text processing 
systems (Friedman et al, 1994, Chapman et al 
2001, Elkin et al 2005) handle the above-
mentioned phenomena, most of them exploit hand-
crafted rule-based negation/uncertainty detection 
modules. To the best of our knowledge, there are 
no publicly available standard corpora of reason-
able size that are usable for evaluating the auto-
matic detection and scope resolution of these 
language phenomena. The availability of such a 
resource would undoubtedly facilitate the devel-
opment of corpus-based statistical systems for ne-
gation/hedge detection and resolution.  
Our study seeks to fill this gap by presenting the 
BioScope corpus, which consists of medical and 
biological texts annotated for negation, speculation 
and their linguistic scope. This was done to permit 
a comparison between and to facilitate the devel-
opment of systems for negation/hedge detection 
and scope resolution. The corpus described in this 
paper has been made publicly available for re-
search purposes and it is freely downloadable1. 
                                                          
1 www.inf.u-szeged.hu/rgai/bioscope  
38
1.1 Related work 
Chapman et al (2001) created a simple regular 
expression algorithm called NegEx that can detect 
phrases indicating negation and identify medical 
terms falling within the negative scope. With this 
process, a large part of negatives can be identified 
in discharge summaries. 
Mutalik et al (2001) earlier developed 
Negfinder in order to recognise negated patterns in 
medical texts. Their lexer uses regular expressions 
to identify words indicating negation and then it 
passes them as special tokens to the parser, which 
makes use of the single-token look-ahead strategy. 
Thus, without appealing to the syntactic structure 
of the sentence, Negfinder can reliably identify 
negated concepts in medical narrative when they 
are located near the negation markers. 
Huang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified on 
the basis of syntactic categories and they are 
located in parse trees. Their hybrid approach is 
able to identify negated concepts in radiology 
reports even when they are located at some 
distance from the negative term. 
The Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor in order to encode 
clinical documents in a structured form (Friedman 
et al, 1994). Negated concepts and certainty 
modifiers are also encoded within the system, thus 
it enables them to make a distinction between 
negated/uncertain concepts and factual information 
which is crucial in information retrieval. 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words in order 
to identify negated statements and their scope. 
Although a fair amount of literature on 
uncertainty (or hedging) in scientific texts has been 
produced since the 1990s (e.g. Hyland, 1994), 
speculative language from a Natural Language 
Processing perspective has only been studied in the 
past few years. Previous studies (Light et al, 2004) 
showed that the detection of hedging can be solved 
effectively by looking for specific keywords which 
imply speculative content. 
Another possibility is to treat the problem as a 
classification task and train a statistical  model to 
discriminate speculative and non-speculative 
assertions. This approach requires the availability 
of labeled instances to train the models on. 
Medlock and Briscoe (2007) proposed a weakly 
supervised setting for hedge classification in 
scientific texts where the aim is to minimise human 
supervision needed to obtain an adequate amount 
of training data. Their system focuses on locating 
hedge cues in text and thus they do not determine 
the scopes (in other words in a text they define the 
scope to be a whole sentence). 
1.2 Related resources 
Even though the problems of negation (mainly in 
the medical domain) and hedging (mainly in the 
scientific domain) have received much interest in 
the past few years, open access annotated resources 
for training, testing and comparison are rare and 
relatively small in size. Our corpus is the first one 
with an annotation of negative/speculative 
keywords and their scope. The authors are only 
aware of the following related corpora: 
 
? The Hedge classification corpus (Medlock 
and Briscoe, 2007), which has been 
annotated for hedge cues (at the sentence 
level) and consists of five full biological 
research papers (1537 sentences). No scope 
annotation is given in the original corpus. 
We included this publicly available corpus 
in ours, enriching the data with annotation 
for negation cues and linguistic scope for 
both hedging and negation. 
? The Genia Event corpus (Kim et al, 2008), 
which annotates biological events with 
negation and three levels of uncertainty 
(1000 abstracts). 
? The BioInfer corpus (Pyysalo et al, 2007), 
where biological relations are annotated for 
negation (1100 sentences in size).  
In the two latter corpora biological terms 
(relations and events) have been annotated for both 
negation and hedging, but linguistic cues (i.e. 
which keyword modifies the semantics of the 
statement) have not been annotated. We annotated 
keywords and their linguistic scope, which is very 
useful for machine learning or rule-based negation 
and hedge detection systems. 
39
2 Annotation guidelines 
This section describes the basic principles on the 
annotation of speculative and negative scopes in 
biomedical texts. Some basic definitions and tech-
nical details are given in Section 2.1, then the gen-
eral guidelines are discussed in Section 2.2 and the 
most typical keywords and their scopes are illus-
trated with examples in Section 2.3. Some special 
cases and exceptions are listed in Section 2.4, then 
the annotation process of the corpus is described 
and discussed in Section 2.5. The complete annota-
tion guidelines document is available from the cor-
pus homepage. 
2.1 Basic issues 
In a text, just sentences with some instance of 
speculative or negative language are considered for 
annotation. The annotation is based on linguistic 
principles, i.e. parts of sentences which do not con-
tain any biomedical term are also annotated if they 
assert the non-existence/uncertainty of something.  
As for speculative annotation, if a sentence is a 
statement, that is, it does not include any specula-
tive element that suggests uncertainty, it is disre-
garded. Questions inherently suggest uncertainty ? 
which is why they are asked ?, but they will be 
neglected and not annotated unless they contain 
speculative language. 
Sentences containing any kind of negation are 
examined for negative annotation. Negation is un-
derstood as the implication of the non-existence of 
something. However, the presence of a word with 
negative content does not imply that the sentence 
should be annotated as negative, since there are 
sentences that include grammatically negative 
words but have a speculative meaning or are actu-
ally regular assertions (see the examples below). 
In the corpus, instances of speculative and nega-
tive language ? that is, keywords and their scope ? 
are annotated. Speculative elements are marked by 
angled brackets: <or>, <suggests> etc., while 
negative keywords are marked by square brackets: 
[no], [without] etc. The scope of both negative and 
speculative keywords is denoted by parentheses. 
Also, the speculative or negative cue is always in-
cluded within its scope: 
This result (<suggests> that the valency of Bi in 
the material is smaller than + 3). 
Stable appearance the right kidney ([without] hy-
dronephrosis). 
In the following, the general guidelines for specu-
lative and negative annotation are presented. 
2.2 General guidelines 
During the annotation process, we followed a min-
max strategy for the marking of keywords and their 
scope. When marking the keywords, a minimalist 
strategy was followed: the minimal unit that ex-
pressed hedging or negation was marked as a key-
word. However, there are some cases when hedge 
or negation can be expressed via a phrase rather 
than a single word. Complex keywords are phrases 
that express uncertainty or negation together, but 
they cannot do this on their own (the meaning or 
the semantics of its subcomponents are signifi-
cantly different from the semantics of the whole 
phrase). An instance of a complex keyword can be 
seen in the following sentence: 
Mild bladder wall thickening (<raises the question 
of> cystitis). 
On the other hand, a sequence of words cannot be 
marked as a complex keyword if it is only one of 
those words that express speculative or negative 
content (even without the other word). Thus prepo-
sitions, determiners, adverbs and so on are not an-
notated as parts of the complex keyword if the 
keyword can have a speculative or negative con-
tent on its own: 
The picture most (<likely> reflects airways dis-
ease). 
Complex keywords are not to be confused with the 
sequence of two or more keywords because they 
can express hedge or negation on their own, that is, 
without the other keyword as well. In this case, 
each keyword is annotated separately, as is shown 
in the following example: 
Slightly increased perihilar lung markings (<may> 
(<indicate> early reactive airways disease)). 
2.3 Scope marking 
When marking the scopes of negative and specula-
tive keywords, we extended the scope to the big-
gest syntactic unit possible (in contrast to other 
corpora like the one described in (Mutalik et al, 
2001)). Thus, annotated scopes always have the 
40
maximal length ? as opposed to the strategy for 
annotating keywords, where we marked the mini-
mal unit possible. Our decision was supported by 
two facts. First, since scopes must contain their 
keywords, it seemed better to include every ele-
ment in between the keyword and the target word 
in order to avoid ?empty? scopes, that is, scopes 
without a keyword. In the next example, however 
is not affected by the hedge cue but it should be 
included within the scope, otherwise the keyword 
and its target phrase would be separated: 
(Atelectasis in the right mid zone is, however, 
<possible>). 
Second, the status of modifiers is occasionally 
vague: it is sometimes not clear whether the modi-
fier of the target word belongs to its scope as well. 
The following sentence can describe two different 
situations: 
There is [no] primary impairment of glucocorti-
coid metabolism in the asthmatics. 
First, the glucocorticoid metabolism is impaired in 
the asthmatics but not primarily, that is, the scope 
of no extends to primary. Second, the scope of no 
extends to impairment (and its modifiers and com-
plements as well), thus there is no impairment of 
the glucocorticoid metabolism at all. Another ex-
ample is shown here: 
Mild viral <or> reactive airways disease is de-
tected. 
The syntactic structure of the above sentence is 
ambiguous. First, the airways disease is surely 
mild, but it is not known whether it is viral or reac-
tive; or second, the airways disease is either mild 
and viral or reactive and not mild. Most of the sen-
tences with similar problems cannot be disambigu-
ated on the basis of contextual information, hence 
the proper treatment of such sentences remains 
problematic. However, we chose to mark the wid-
est scope available: in other words, we preferred to 
include every possible element within the scope 
rather than exclude elements that should probably 
be included. 
 The scope of a keyword can be determined on 
the basis of syntax. The scope of verbs, auxiliaries, 
adjectives and adverbs usually extends to the right 
of the keyword. In the case of verbal elements, i.e. 
verbs and auxiliaries, it ends at the end of the 
clause (if the verbal element is within a relative 
clause or a coordinated clause) or the sentence, 
hence all complements and adjuncts are included, 
in accordance with the principle of maximal scope 
size. Take the following examples: 
The presence of urothelial thickening and mild 
dilatation of the left ureter (<suggest> that the 
patient may have continued vesicoureteral reflux). 
These findings that (<may> be from an acute 
pneumonia) include minimal bronchiectasis as 
well. 
These findings (<might> be chronic) and (<may> 
represent reactive airways disease). 
The scope of attributive adjectives generally ex-
tends to the following noun phrase, whereas the 
scope of predicative adjectives includes the whole 
sentence. For example, in the following two state-
ments: 
This is a 3 month old patient who had (<possible> 
pyelonephritis) with elevated fever. 
(The demonstration of hormone receptor proteins 
in cells from malignant effusions is <possible>). 
Sentential adverbs have a scope over the entire 
sentence, while the scope of other adverbs usually 
ends at the end of the clause or sentence. For in-
stance, 
(The chimaeric oncoprotein <probably> affects 
cell survival rather than cell growth). 
Right upper lobe volume loss and (<probably> 
pneumonia). 
The scope of conjunctions extends to all members 
of the coordination. That is, it usually extends to 
the both left and right: 
Symptoms may include (fever, cough <or> itches). 
Complex keywords such as either ? or have one 
scope: 
Mild perihilar bronchial wall thickening may rep-
resent (<either> viral infection <or> reactive 
airways disease). 
Prepositions have a scope over the following 
(noun) phrase: 
Mildly hyperinflated lungs ([without] focal opac-
ity). 
41
When the subject of the sentence contains the 
negative determiners no or neither, its scope ex-
tends to the entire sentence: 
Surprisingly, however, ([neither] of these proteins 
bound in vitro to EBS1 or EBS2). 
The main exception that changes the original scope 
of the keyword is the passive voice. The subject of 
the passive sentence was originally the object of 
the verb, that is, it should be within its scope. This 
is why the subject must also be marked within the 
scope of the verb or auxiliary. For instance, 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Another example of scope change is the case of 
raising verbs (seem, appear, be expected, be likely 
etc.). These can have two different syntactic pat-
terns, as the following examples suggest:  
It seems that the treatment is successful. 
The treatment seems to be successful. 
In the first case, the scope of seems starts right 
with the verb. If this was the case in the second 
pattern, the treatment would not be included in the 
scope, but it should be like that shown in the first 
pattern. Hence in the second sentence, the scope 
must be extended to the subject as well: 
It (<seems> that the treatment is successful). 
(The treatment <seems> to be successful). 
Sometimes a negative keyword is present in the 
text apparently without a scope: negative obviously 
expresses negation, but the negated fact ? what 
medical problem the radiograph is negative for ? is 
not part of the sentence. In such cases, the keyword 
is marked and the scope contains just the keyword: 
([Negative]) chest radiograph. 
In the case of elliptic sentences, the same strategy 
is followed: the keyword is marked and its scope 
includes only the keyword since the verbal phrase, 
that is, the scope of not, is not repeated in the sen-
tence. 
This decrease was seen in patients who responded 
to the therapy as well as in those who did ([not]). 
Generally, punctuation marks or conjunctions 
function as scope boundary markers in the corpus, 
in contrast to the corpus described in (Mutalik et 
al., 2001) where certain lexical items are treated as 
negation-termination tokens. Since in our corpus 
the scope of negation or speculation is mostly ex-
tended to the entire clause in the case of verbal 
elements, it is clear that markers of a sentence or 
clause boundary determine the end of their scope. 
2.4 Special cases 
It seems unequivocal that whenever there is a 
speculative or negative cue in the sentence, the 
sentence expresses hedge or negation. However, 
we have come across several cases where the pres-
ence of a speculative/negative keyword does not 
imply a hedge/negation. That is, some of the cues 
do not denote speculation or negation in all their 
occurrences, in other words, they are ambiguous. 
For instance, the following sentence is a state-
ment and it is the degree of probability that is pre-
cisely determined, but it is not an instance of 
hedging although it contains the cue probable: 
The planar amide groups in which is still digging 
nylon splay around 30 less probable event. 
As for negative cues, sentences including a nega-
tive keyword are not necessarily to be annotated 
for negation. They can, however, have a specula-
tive content as well. The following sentence con-
tains cannot, which is a negative keyword on its 
own, but not in this case: 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Some other sentences containing a negative key-
word are not to be annotated either for speculation 
or for negation. In the following example, the 
negative keyword is accompanied by an adverb 
and their meaning is neither speculative nor nega-
tive. The sequence of the negative keyword and the 
adverb can be easily substituted by another adverb 
or adjective having the same (or a similar) mean-
ing, which is by no means negative ? as shown in 
the example. In this way, the sentence below can 
be viewed as a positive assertion (not a statement 
of the non-existence of something). 
Thus, signaling in NK3.3 cells is not always 
(=sometimes) identical with that in primary NK 
cells. 
As can be seen from the above examples, hedging 
or negation is determined not just by the presence 
42
of an apparent cue: it is rather an issue of the key-
word, the context and the syntactic structure of the 
sentence taken together. 
2.5 Annotation process 
Our BioScope corpus was annotated by two inde-
pendent linguists following the guidelines written 
by our linguist expert before the annotation of the 
corpus was initiated. These guidelines were devel-
oped throughout the annotation process as annota-
tors were often confronted with problematic issues. 
The annotators were not allowed to communicate 
with each other as far as the annotation process 
was concerned, but they could turn to the expert 
when needed and regular meetings were also held 
between the annotators and the linguist expert in 
order to discuss recurring and/or frequent problem-
atic issues. When the two annotations for one sub-
corpus were finalised, differences between the two 
were resolved by the linguist expert, yielding the 
gold standard labeling of the subcorpus. 
3 Corpus details 
In this section we will discuss in detail the overall 
characteristics of the corpus we developed, includ-
ing a brief description of the texts that constitute 
the BioScope corpus and some general statistics 
concerning the size of each part, distribution of 
negation/hedge cues, ambiguity levels and finally 
we will present statistics on the final results of the 
annotation work. 
3.1 Corpus texts 
The corpus consists of texts taken from 4 different 
sources and 3 different types in order to ensure that 
it captures the heterogenity of language use in the 
biomedical domain. We decided to add clinical 
free-texts (radiology reports), biological full papers 
and biological paper abstracts (texts from Genia). 
Table 1 summarises the chief characteristics of 
the three subcorpora. The 3rd and 5th rows of the 
table show the ratio of sentences which contain 
negated or uncertain statements. The 4rd and 6th 
rows show the number of negation and hedge cue 
occurrences in the given corpus.  
A major part of the corpus consists of clinical 
free-texts. We chose to add medical texts to the 
corpus in order to facilitate research on nega-
tion/hedge detection in the clinical domain. The 
radiology report corpus that was used for the clini-
cal coding challenge (Pestian et al, 2007) organ-
ised by the Computational Medicine Center in 
Cincinatti, Ohio in 2007 was annotated for nega-
tions and uncertainty along with the scopes of each 
phenomenon. This part contains 1954 documents, 
each having a clinical history and an impression 
part, the latter being denser in negated and specula-
tive parts. 
Another part of the corpus consists of full sci-
entific articles. 5 articles from FlyBase (the same 
data were used by Medlock and Briscoe (2007) for 
evaluating sentence-level hedge classifiers) and 4 
articles from the open access BMC Bioinformatics 
website were downloaded and annotated for nega-
tions, uncertainty and their scopes. Full papers are 
particularly useful for evaluating negation/hedge 
classifiers as different parts of an article display 
different properties in the use of speculative or ne-
gated phrases. Take, for instance, the Conclusions 
section of scientific papers that tends to contain 
significantly more uncertain or negative findings 
than the description of Experimental settings and 
methods. 
Scientific abstracts are the main targets for 
various Text Mining applications like protein-
protein interaction mining due to their public ac-
cessibility (e.g. through PubMed). We therefore 
decided to include quite a lot of texts from the ab-
stracts of scientific papers. This is why we in-
cluded the abstracts of the Genia corpus (Collier et 
al., 1999). This decision was straightforward for 
two reasons. First, the Genia corpus contains syn-
tax tree annotation, which allows a comparison 
between scope annotation and syntactic structure. 
Being syntactic in nature, scopes should align with 
the bracket structure of syntax trees, while scope 
resolution algorithms that exploit treebank data can 
be used as a theoretical upper bound for the 
evaluation of parsers for resolving negative/hedge 
scopes. The other reason was that scope annotation 
can mutually benefit from the rich annotations of 
the Genia corpus, such as term annotation (evalua-
tion) and event annotation (comparison with the 
biologist uncertainty labeling of events). 
The corpus consists of more than 20.000 anno-
tated sentences altogether. We consider this size to 
be sufficiently large to serve as a standard evalua-
tion corpus for negation/hedge detection in the 
biomedical domain. 
 
43
 Clinical Full Paper Abstract 
#Documents 1954 9 1273 
#Sentences 6383 2624 11872 
Negation  
sentences 6.6% 13.76% 13.45% 
#Negation cues 871 404 1757 
Hedge sentences 13.4% 22.29% 17.69% 
#Hedge cues 1137 783 2691 
Table 1: Statistics of the three subcorpora. 
3.2 Agreement analysis 
We measured the consistency level of the annota-
tion using inter-annotator agreement analysis. The 
inter-annotator agreement rate is defined as the 
F?=1 measure of one annotation, treating the second 
one as the gold standard. We calculated agreement 
rates for all three subcorpora between the two in-
dependent annotators and between the two annota-
tors and the gold standard labeling. The gold 
standard labeling was prepared by the creator of 
the annotation guide, who resolved all cases where 
the two annotators disagreed on a keyword or its 
scope annotation. 
We measured the agreement rate of annotating 
negative and hedge keywords, and the agreement 
rate of annotating the linguistic scope for each 
phenomenon. We distinguished left-scope, right-
scope and full scope agreement that required both 
left and right scope boundaries to match exactly to 
be considered as coinciding annotations. A detailed 
analysis of the consistency levels for the three sub-
corpora and the ambiguity levels for each negative 
and hedge keyword (that is, the ratio of a keyword 
being annotated as a negative/speculative cue and 
the number of all the occurrences of the same 
keyword in the corpus) can be found at the corpus 
homepage. 
 
3.3 BioScope corpus availability 
The corpus is available free of charge for research 
purposes and can be obtained for a modest price 
for business use. For more details, see the Bio-
Scope homepage: 
www.inf.u-szeged.hu/rgai/bioscope. 
4 Conclusions 
In this paper we reported on the construction of a 
corpus annotated for negations, speculations and 
their linguistic scopes. The corpus is accessible for 
academic purposes and is free of charge. Apart 
from the intended goal of serving as a common 
resource for the training, testing and comparison of 
biomedical Natural Language Processing systems, 
the corpus is also a good resource for the linguistic 
analysis of scientific and clinical texts. 
The most obvious conclusions here are that the 
usual language of clinical documents makes it 
much easier to detect negation and uncertainty 
cues than in scientific texts because of the very 
high ratio of the actual cue words (i.e. low ambigu-
ity level), which explains the high accuracy scores 
reported in the literature. In scientific texts ? which 
are nowadays becoming a popular target for Text 
Mining (for literature-based knowledge discovery) 
? the detection and scope resolution of negation 
and uncertainty is, on the other hand, a problem of 
great complexity, with the percentage of non-
hedge occurrences being as high as 90% for some 
hedge cue candidates in biological paper abstracts. 
Take for example the keyword or which is labeled 
as a speculative keyword in only 11.32% of the 
cases in scientific abstracts, while it was labeled as 
speculative in 97.86% of the cases in clinical texts. 
Identifying the scope is also more difficult in sci-
entific texts where the average sentence length is 
much longer than in clinical data, and the style of 
the texts is also more literary in the former case. 
In our study we found that hedge detection is a 
more difficult problem than identifying negations 
because the number of possible cue words is higher 
and the ratio of real cues is significantly lower in 
the case of speculation (higher keyword/non-
keyword ambiguity). The annotator-agreement ta-
ble also confirms this opinion: the detection of 
hedging is more complicated than negation even 
for humans. 
Our corpus statistics also prove the importance 
of negation and hedge detection. The ratio of ne-
gated and hedge sentences in the corpus varies in 
the subcorpora, but we can say that over 20% of 
the sentences contains a modifier that radically 
influences the semantic content of the sentence. 
One of the chief construction principles of the 
BioScope corpus was to facilitate the train-
ing/development of automatic negation and hedge 
detection systems. Such systems have to solve two 
sub-problems: they have to identify real cue words 
(note that the probability of any word being a key-
word can be different for various domains) and 
44
then they have to determine the linguistic scope of 
actual keywords. 
These automatic hedge and negation detection 
methods can be utilised in a variety of ways in a 
(biomedical) Text Mining system. They can be 
used as a preprocessing tool, i.e. each word in a 
detected scope can be removed from the docu-
ments if we seek to extract true assertions. This can 
significantly reduce the level of noise for process-
ing in such cases where only a document-level la-
beling is provided (like that for the ICD-9 coding 
dataset) and just clear textual evidence for certain 
things should be extracted. On the other hand, 
similar systems can classify previously extracted 
statements according to their certainty or uncer-
tainty, which is generally an important issue in the 
automatic processing of scientific texts. 
Acknowledgments 
This work was supported in part by the NKTH 
grant of the Jedlik ?nyos R&D Programme 2007 
(project codename TUDORKA7) of the Hungarian 
government. The authors wish to thank the anony-
mous reviewers for their useful suggestions and 
comments. The authors also wish to thank the crea-
tors of the ICD-9 coding dataset and the Genia 
corpus for making the texts that were used here 
publicly available. The authors thank Jin-Dong 
Kim as well for the useful comments and sugges-
tions on the annotation guide and Orsolya Vincze 
and Mih?ly Mink? (the two annotators) for their 
work. 
References  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34(5):301?310. 
N. Collier, H. S. Park, N. Ogata, Y. Tateishi, C. Nobata, 
T. Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsu-
jii. 1999. The GENIA project: corpus-based knowl-
edge acquisition and information extraction from 
genome research papers. Proceedings of EACL-99. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom and 
Dietlind L. Wahner-Roedler. 2005. A controlled trial 
of automated classification of negation from clinical 
notes. BMC Medical Informatics and Decision Mak-
ing 5:13 doi:10.1186/1472-6947-5-13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural-language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161?174. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clini-
cal Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304?311. 
Ken Hyland. 1994. Hedging in academic writing and 
EAP textbooks. English for Specific Purposes, 
13(3):239?256. 
Jin-Dong Kim, Tomoko Ohta, and Jun'ichi Tsujii. 2008. 
Corpus annotation for mining biomedical events 
from literature. BMC Bioinformatics 2008, 9:10. 
Marc Light, Xin Ting Qui, and Padmini Srinivasan. 
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of 
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users. 
Boston, Massachusetts, Association for Computa-
tional Linguistics, 17?24. 
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics. Pra-
gue, Association for Computational Linguistics, 992?
999. 
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of General-purpose 
Negation Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study Using the 
UMLS. Journal of the American Medical Informatics 
Association, 8(6):598?609. 
John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ 
Hovermale, Neil Johnson, K. Bretonnel Cohen, and 
Wlodzislaw Duch. 2007. A shared task involving 
multi-label classification of clinical free text. In Bio-
logical, translational, and clinical language process-
ing. Prague, Association for Computational 
Linguistics, 97?104. 
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio 
Salakoski. 2007. BioInfer: a corpus for information 
extraction in the biomedical domain. BMC Bioinfor-
matics 2007, 8:50 doi:10.1186/1471-2105-8-50. 
45
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1110?1118,
Beijing, August 2010
Hungarian Corpus of Light Verb Constructions
Veronika Vincze
University of Szeged
Department of Informatics
vinczev@inf.u-szeged.hu
Ja?nos Csirik
Hungarian Academy of Sciences
Research Group on Artificial Intelligence
csirik@inf.u-szeged.hu
Abstract
The precise identification of light verb
constructions is crucial for the successful
functioning of several NLP applications.
In order to facilitate the development of
an algorithm that is capable of recogniz-
ing them, a manually annotated corpus of
light verb constructions has been built for
Hungarian. Basic annotation guidelines
and statistical data on the corpus are also
presented in the paper. It is also shown
how applications in the fields of machine
translation and information extraction can
make use of such a corpus and an algo-
rithm.
1 Introduction
In this paper, we report a corpus containing light
verb constructions in Hungarian. These expres-
sions are neither productive nor idiomatic and
their meaning is not totally compositional (the
noun is usually taken in one of its literal senses but
the verb usually loses its original sense to some
extent), as it can be seen in the examples from dif-
ferent languages shown below. Since their mean-
ing is the same, only literal translations are pro-
vided:
? English: to give a lecture, to come into
bloom, the problem lies (in)
? German: halten eine Vorlesung to hold a pre-
sentation, in Blu?te stehen in bloom to stand,
das Problem liegt (in) the problem lies (in)
? French: faire une pre?sentation to make a pre-
sentation, e?tre en fleur to be in bloom, le
proble`me re?side (dans) the problem resides
(in)
? Hungarian: elo?ada?st tart presentation-
ACC holds, vira?gba borul bloom-ILL falls,
proble?ma rejlik (vmiben) problem hides (in
sg)
Several terms like complex verb structures, sup-
port verb constructions or light verb constructions
have been used1 for these constructions in the lit-
erature (Langer, 2004). In this paper, the term
light verb constructions will be employed.
The structure of the paper is as follows. First,
the importance of the special NLP treatment of
light verb constructions is emphasized in section
2. The precise identification of such constructions
is crucial for the successful functioning of NLP
applications, thus, it is argued that an algorithm
is needed to automatically recognize them (sec-
tion 4). In order to facilitate the development of
such an algorithm, a corpus of light verb construc-
tions has been built for Hungarian, which is pre-
sented together with statistical data in section 5.
Finally, it is shown how NLP applications in the
fields of machine translation and information ex-
traction can profit from the implementation of an
algorithm capable of identifying light verb con-
structions (section 6).
2 Light verb constructions in NLP
In natural language processing, one of the most
challenging tasks is the proper treatment of col-
1There might be slight theoretical differences in the usage
of these terms ? e.g. semantically empty support verbs are
called light verbs in e.g. Meyers et al (2004a), that is, the
term support verb is a hypernym of light verb. However,
these differences are not analyzed in detail in this paper.
1110
locations, which term comprises light verb con-
structions as well. Every multiword expression
is considered to be a collocation if its members
often co-occur and its form is fixed to some ex-
tent (Siepmann, 2005; Siepmann, 2006; Sag et al,
2001; Oravecz et al, 2004; Va?radi, 2006). Col-
locations are frequent in language use and they
usually exhibit unique behaviour, thus, they often
pose a problem to NLP systems.
Light verb constructions deserve special atten-
tion in NLP applications for several reasons. First,
their meaning is not totally compositional, that is,
it cannot be computed on the basis of the mean-
ings of the parts of the collocation and the way
they are related to each other. Thus, the result of
translating the parts of the collocation can hardly
be considered as the proper translation of the orig-
inal expression. Second, light verb constructions
(e.g. make a mistake) often share their syntac-
tic pattern with other constructions such as lit-
eral verb + noun combinations (e.g. make a cake)
or idioms (e.g. make a meal), thus, their identi-
fication cannot be based on solely syntactic pat-
terns. Third, since the syntactic and the seman-
tic head of the construction are not the same ?
the syntactic head being the verb and the seman-
tic head being the noun ?, they require special
treatment when parsing. It can be argued that
they form a complex verb similarly to phrasal or
prepositional verbs (as reflected in the term com-
plex verb structures). Thus, it is advisable to indi-
cate their special syntacto-semantic relationship:
in dependency grammars, the new role QUASI-
ARGUMENT might be proposed for this purpose.
3 Related work
Light verb constructions ? as a subtype of multi-
word expressions ? have been paid special atten-
tion in NLP literature. Sag et al (2001) classify
them as a subtype of lexicalized phrases and flex-
ible expressions. They are usually distinguished
from productive or literal verb + noun construc-
tions on the one hand and idiomatic verb + noun
expressions on the other hand: e.g. Fazly and
Stevenson (2007) use statistical measures in order
to classify subtypes of verb + noun combinations
and Diab and Bhutada (2009) developed a chunk-
ing method for classifying multiword expressions.
Identifying multiword expressions in general
and light verb constructions in particular is not
unequivocal since constructions with similar syn-
tactic structure (e.g. verb + noun combinations)
can belong to different subclasses on the produc-
tivity scale (i.e. productive combinations, light
verb constructions and idioms). That is why well-
designed and tagged corpora of multiword ex-
pressions are invaluable resources for training and
testing algorithms that are able to identify multi-
word expressions. For instance, Gre?goire (2007)
describes the design and implementation of a lexi-
con of Dutch multiword expressions. Focusing on
multiword verbs, Kaalep and Muischnek (2006;
2008) present an Estonian database and a corpus
and Krenn (2008) describes a database of German
PP-verb combinations. The Prague Dependency
Treebank also contains annotation for light verb
constructions (Cinkova? and Kola?r?ova?, 2005) and
NomBank (Meyers et al, 2004b) provides the ar-
gument structure of common nouns, paying atten-
tion to those occurring in support verb construc-
tions as well. On the other hand, Zarrie? and Kuhn
(2009) make use of translational correspondences
when identifying multiword expressions (among
them, light verb constructions). A further exam-
ple of corpus-based identification of light verb
constructions in English is described in Tan et al
(2006).
Light verb constructions are considered to be
semi-productive, that is, certain verbs tend to co-
occur with nouns belonging to a given semantic
class. A statistical method is applied to measure
the acceptability of possible light verb construc-
tions in Stevenson et al (2004), which correlates
reasonably well with human judgments.
4 Identifying light verb constructions
A database of light verb constructions and an an-
notated corpus might be of great help in the au-
tomatic recognition of light verb constructions.
They can serve as a training database when imple-
menting an algorithm for identifying those con-
structions.
The recognition of light verb constructions can-
not be solely based on syntactic patterns for other
(productive or idiomatic) combinations may ex-
hibit the same verb + noun scheme (see section
1111
2). However, in agglutinative languages such as
Hungarian, nouns can have several grammatical
cases, some of which typically occur in a light
verb construction when paired with a certain verb.
For instance, the verb hoz ?bring? is a transitive
verb, that is, it usually occurs with a noun in the
accusative case. On the other hand, when it is pre-
ceded or followed by a noun in the sublative or
illative case (the typical position of the noun in
Hungarian light verb constructions being right be-
fore or after the verb2), it is most likely a light verb
construction. To illustrate this, we offer some ex-
amples:
vizet hoz
water-ACC bring
?to bring some water?
zavarba hoz
trouble-ILL bring
?to embarrass?
The first one is a productive combination (with
the noun being in the accusative form) while the
second one is a light verb construction. Note that
the light verb construction also has got an argu-
ment in the accusative case (syntactically speak-
ing, a direct object complement) as in:
Ez a megjegyze?s mindenkit zavarba ho-
zott.
this the remark everyone-ACC trouble-
ILL bring-PAST-3SG
?This remark embarrassed everybody.?
Thus, the presence of an argument in the ac-
cusative does not imply that the noun + verb com-
bination is a light verb construction. On the other
hand, the presence of a noun in the illative or
sublative case immediately preceding or follow-
ing the verb strongly suggests that a light verb in-
stance of hoz is under investigation.
Most light verb constructions have a verbal
counterpart derived from the same stem as the
noun, which entails that it is mostly deverbal
2In a neutral sentence, the noun is right before the verb,
in a sentence containing focus, it is right after the verb.
nouns that occur in light verb constructions (as
in make/take a decision compared to decide or
do?nte?st hoz vs. do?nt in Hungarian). The identifi-
cation of such nouns is possible with the help of a
morphosyntactic parser that is able to treat deriva-
tion as well (e.g. hunmorph for Hungarian (Tro?n
et al, 2005)), and the combination of a possible
light verb and a deverbal noun typically results in
a light verb construction.
Thus, an algorithm that makes use of mor-
phosyntactic and derivational information and
previously given lists can be constructed to iden-
tify light verb constructions in texts. It is impor-
tant that the identification of light verb construc-
tions precedes syntactic parsing, for the noun and
the verb in the construction form one complex
predicate, which has its effects on parsing: other
arguments belong not solely to the verb but to the
complex predicate.
To the best of our knowledge, there are no cor-
pora of light verb constructions available for Hun-
garian. That is why we decided to build such a
corpus. The corpus is described in detail in sec-
tion 5. On the basis of the corpus developed, we
plan to design an algorithm to automatically iden-
tify light verb constructions in Hungarian.
5 The corpus
In order to facilitate the extraction and the NLP
treatment of Hungarian light verb constructions,
we decided to build a corpus in which light verb
constructions are annotated. The Szeged Tree-
bank (Csendes et al, 2005) ? a database in which
words are morphosyntactically tagged and sen-
tences are syntactically parsed ? constitutes the
basis for the annotation. We first selected the
subcorpora containing business news, newspaper
texts and legal texts for annotation since light verb
constructions are considered to frequently occur
in these domains (see B. Kova?cs (1999)). How-
ever, we plan to extend the annotation to other
subcorpora as well (e.g. literary texts) in a later
phase. Statistical data on the annotated subcor-
pora can be seen in Table 1.
5.1 Types of light verb constructions
As Hungarian is an agglutinative language, light
verb constructions may occur in various forms.
1112
sentences words
business news 9574 186030
newspapers 10210 182172
legal texts 9278 220069
total 29062 582871
Table 1: Number of sentences and words in the
annotated subcorpora
For instance, the verbal component may be in-
flected for tense, mood, person, number, etc.
However, these inflectional differences can be eas-
ily resolved by a lemmatizer. On the other hand,
besides the prototypical noun + verb combination,
light verb constructions may be present in differ-
ent syntactic structures, that is, in participles and
infinitives and they can also undergo nominaliza-
tion. These types are all annotated in the corpus
texts since they also occur relatively frequently
(see statistical data in 5.3). All annotated types
are illustrated below.
? Noun + verb combination <verb>
bejelente?st tesz
announcement-ACC makes
?to make an announcement?
? Participles <part>
? Present participle
e?letbe le?po? (inte?zkede?s)
life-ILL stepping (instruction)
?(an instruction) taking effect?
? Past participle
cso?dbe ment (ce?g)
bankrupt-ILL gone (firm)
?(a firm) that went bankrupt?
? Future participle
fontolo?ra veendo? (aja?nlat)
consideration-SUB to be taken (offer)
?(an offer) that is to be taken into con-
sideration?
? Infinitive
forgalomba hozni
circulation-ILL bring-INF
?to put into circulation?
? Nominalization <nom>
be?rbe ve?tel
rent-ILL taking
?hiring?
Split light verb constructions, where the noun
and the verb are not adjacent, are also annotated
and tagged. In this way, their identification be-
comes possible and the database can be used for
training an algorithm that automatically recog-
nizes (split) light verb constructions.
5.2 Annotation principles
Corpus texts contain single annotation, i.e. one
annotator worked on each text. Light verb con-
structions can be found in between XML tags
<FX></FX>. In order to decide whether a noun
+ verb combination is a light verb construction or
not, annotators were suggested to make use of a
test battery developed for identifying Hungarian
light verb constructions (Vincze, 2008).
The annotation process was carried out manu-
ally on the syntactically annotated version of the
Szeged Treebank, thus, phrase boundaries were
also taken into consideration when marking light
verb constructions. Since the outmost boundary
of the nominal component was considered to be
part of the light verb construction, in several cases
adjectives and other modifiers of the nominal head
are also included in the construction, e.g.:
<FX>nyilva?nos aja?nlatot tesz</FX>
public offer-ACC make
?to make a public offer?
In the case of participles, NP arguments may
be also included (although in English, the same
argument is expressed by a PP):
<FX>Ny??regyha?za?n tartott
u?le?se?n</FX>
Ny??regyha?za-SUP hold-PPT session-
3SGPOSS-SUP
?at its session held in Ny??regyha?za?
Constructions with a nominal component in the
accusative case can be nominalized in two ways
in Hungarian, as in:
1113
szerzo?de?st ko?t
contract-ACC bind
?to make a contract?
<FX>szerzo?de?sko?te?s</FX>
contract+bind-GERUND
?making a contract?
<FX>ada?sve?teli szerzo?de?sek
megko?te?se</FX>
sale contract-PL PREVERB-bind-
GERUND-3SGPOSS
?making of sales contracts?
Both types are annotated in the corpus.
Besides the prototypical occurrences of light
verb constructions (i.e. a bare common noun +
verb3), other instances were also annotated in the
corpus. For instance, the noun might be accompa-
nied by an article or a modifier (recall that phrase
boundaries were considered during annotation) or
? for word order requirements ? the noun follows
the verb as in:
O? hozta a jo? do?nte?st.
he bring-PAST-3SG-OBJ the good
decision-ACC
?It was him who made the good deci-
sion.?
For the above reasons, a single light verb con-
struction manifests in several different forms in
the corpus. However, each occurrence is manu-
ally paired with its prototypical (i.e. bare noun +
verb) form in a separate list, which is available at
the corpus website.
5.3 Statistics on corpus data
The database contains 3826 occurrences of 658
light verb constructions altogether in 29062 sen-
tences. Thus, a specific light verb construction
3As opposed to other languages where prototypical light
verb constructions consist of a verb + a noun in accusative or
a verb + a prepositional phrase (see e.g. Krenn (2008)), in
Hungarian, postpositional phrases rarely occur within a light
verb construction. However, annotators were told to annotate
such cases as well.
occurs 5.8 times in the corpus on average. How-
ever, the participle form ira?nyado? occurs in 607
instances (e.g. in ira?nyado? kamat ?prime rate?)
due to the topic of the business news subcorpus,
which may distort the percentage rates. For this
reason, statistical data in Table 2 are shown the
occurrences of ira?nyado? excluded.
verb part nom split total
business 565 270 90 40 965
news 58.6% 28% 9.3% 4.1% 25.2%
news- 458 192 55 67 772
papers 59.3% 24.9% 7.1% 8.7% 20.2%
legal 640 504 709 236 2089
texts 30.7% 24.1% 33.9% 11.3% 54.6%
total 1663 966 854 236 3826
43.5% 25.2% 22.3% 9% 100%
Table 2: Subtypes of light verb constructions in
the corpus
It is revealed that although it is verbal occur-
rences that are most frequent, the percentage rate
of participles is also relatively high. The number
of nominalized or split constructions is consider-
ably lower (except for the law subcorpus, where
their number is quite high), however, those to-
gether with participles are responsible for about
55% of the data, which indicates the importance
of their being annotated as well.
As for the general frequency of light verb con-
structions in texts, we compared the number of
verb + argument relations found in the Szeged De-
pendency Treebank (Vincze et al, 2010) where
the argument was a common noun to that of light
verb constructions. It has turned out that about
13% of verb + argument relations consist of light
verb constructions. This again emphasizes that
they should be paid attention to, especially in the
legal domain (where this rate is as high as 36.8%).
Statistical data are shown in Table 3.
V + argument LVC
business news 9524 624 (6.6%)
newspapers 3637 539 (14.8%)
legal texts 2143 889 (36.8%)
total 15574 2052 (13.2%)
Table 3: Verb + argument relations and light verb
constructions
The corpus is publicly available for re-
1114
search and/or educational purposes at
www.inf.u-szeged.hu/rgai/nlp.
6 The usability of the corpus
As emphasized earlier, the proper treatment of
light verb constructions is of primary importance
in NLP applications. In order to achieve this,
their identification is essential. The corpus cre-
ated can function as the training database for the
implementation of an algorithm capable of recog-
nizing light verb constructions, which we plan to
develop in the near future. In the following, the
ways machine translation and information extrac-
tion can profit from such a corpus and algorithm
are shortly presented.
6.1 Light verb constructions and machine
translation
When translating collocations, translation pro-
grams face two main problems. On the one hand,
parts of the collocation do not always occur next
to each other in the sentence (split collocations).
In this case, the computer must first recognize that
the parts of the collocation form one unit (Oravecz
et al, 2004), for which the multiword context of
the given word must be considered. On the other
hand, the lack (or lower degree) of compositional-
ity blocks the possibility of word-by-word trans-
lation (Siepmann, 2005; Siepmann, 2006). How-
ever, a (more or less) compositional account of
light verb constructions is required for successful
translation (Dura and Gawron?ska, 2005).
To overcome these problems, a reliable method
is needed to assure that the nominal and verbal
parts of the construction be matched. This re-
quires an algorithm that can identify light verb
constructions. In our corpus, split light verb con-
structions are also annotated, thus, it is possible to
train the algorithm to recognize them as well: the
problem of split collocations can be eliminated in
this way.
A comprehensive list of light verb construc-
tions can enhance the quality of machine transla-
tion ? if such lists are available for both the source
and the target language. Annotated corpora (es-
pecially and most desirably, parallel corpora) and
explanatory-combinatorial dictionaries4 are possi-
4Explanatory combinatorial dictionaries are essential for
ble sources of such lists. Since in foreign language
equivalents of light verb constructions, the nomi-
nal components are usually literal translations of
each other (Vincze, 2009), by collating the cor-
responding noun entries in these lists the foreign
language variant of the given light verb construc-
tion can easily be found. On the other hand, in or-
der to improve the building of such lists, we plan
to annotate light verb constructions in a subcorpus
of SzegedParalell, a Hungarian-English manually
aligned parallel corpus (To?th et al, 2008).
6.2 Light verb constructions and
information extraction
Information extraction (IE) seeks to process large
amounts of unstructured text, in other words, to
collect relevant items of information and to clas-
sify them. Even though humans usually overper-
form computers in complex information process-
ing tasks, computers also have some obvious ad-
vantages due to their capacity of processing and
their precision in performing well-defined tasks.
For several IE applications (e.g. relationship
extraction) it is essential to identify phrases in
a clause and to determine their grammatical role
(subject, object, verb) as well. This can be carried
out by a syntactic parser and is a relatively sim-
ple task. However, the identification of the syn-
tactic status of the nominal component is more
complex in the case of light verb constructions
for it is a quasi-argument of the verb not to be
confused with other arguments (Alonso Ramos,
1998). Thus, the parser should recognize the spe-
cial status of the quasi-argument and treat it in a
specific way as in the following sentences, one of
which contains a light verb construction while the
other one a verbal counterpart of the construction:
Pete made a decision on his future.
Pete decided on his future.
relation descriptions (up to the present, only fractions of the
dictionary have been completed for Russian (Mel?c?uk and
Z?olkovskij, 1984) and for French (see Mel?c?uk et al (1984
1999)), besides, trial entries have been written in Polish, En-
glish and German that contain the relations of a certain lexi-
cal unit to other lexemes given by means of lexical functions
(see e.g. Mel?c?uk et al (1995)). These dictionaries indicate
light verb constructions within the entry of the nominal com-
ponent.
1115
In the sentence with the verbal counterpart, the
event of deciding involves two arguments: he and
his future. In the sentence with the light verb con-
struction, the same arguments can be found, how-
ever, it is unresolved whether they are the argu-
ments of the verb (made) or the nominal compo-
nent (decision). If a precise syntactic analysis is
needed, it is crucial to know which argument be-
longs to which governor. Nevertheless, it is still
debated if syntactic arguments should be divided
between the nominal component and the verb (see
Meyers et al (2004a) on argument sharing) and if
yes, how (Alonso Ramos, 2007).
For the purpose of information extraction, such
a detailed analysis is unnecessary and in general
terms, the nominal component can be seen as part
of the verb, that is, they form a complex verb sim-
ilarly to phrasal or prepositional verbs and this
complex verb is considered to be the governor
of arguments. Thus, the following data can be
yielded by the IE algorithm: there is an event
of decision-making, Pete is its subject and it is
about his future (and not an event of making
with the arguments decision, Pete and his fu-
ture). Again, the precise identification of light
verb constructions can highly improve the perfor-
mance of parsers in recognizing relations between
the complex verb and its arguments.
7 Conclusion
In this paper, we have presented the development
of a corpus of Hungarian light verb constructions.
Basic annotation guidelines and statistical data
have also been included. The annotated corpus
can serve as a training database for implementing
an algorithm that aims at identifying light verb
constructions. Several NLP applications in the
fields of e.g. machine translation and information
extraction may profit from the successful integra-
tion of such an algorithm into the system, which
we plan to develop in the near future.
Acknowledgements
This work was supported in part by the National
Office for Research and Technology of the Hun-
garian government within the framework of the
project MASZEKER.
The authors wish to thank Gyo?rgy Szarvas for
his help in developing the annotation tool and
Richa?rd Farkas for his valuable comments on an
earlier draft of this paper.
References
Alonso Ramos, Margarita. 1998. Etude se?mantico-
syntaxique des constructions a` verbe support. Ph.D.
thesis, Universite? de Montre?al, Montreal, Canada.
Alonso Ramos, Margarita. 2007. Towards the Syn-
thesis of Support Verb Constructions. In Wanner,
Leo, editor, Selected Lexical and Grammatical Is-
sues in the Meaning-Text Theory. In Honour of Igor
Mel?c?uk, pages 97?138, Amsterdam / Philadelphia.
Benjamins.
B. Kova?cs, Ma?ria. 1999. A funkcio?ige?s szerkezetek
a jogi szaknyelvben [Light verb constructions in the
legal terminology]. Magyar Nyelvo?r, 123(4):388?
394.
Cinkova?, Silvie and Veronika Kola?r?ova?. 2005. Nouns
as Components of Support Verb Constructions in the
Prague Dependency Treebank. In S?imkova?, Ma?ria,
editor, Insight into Slovak and Czech Corpus Lin-
guistics, pages 113?139. Veda Bratislava, Slovakia.
Csendes, Do?ra, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged TreeBank.
In Matousek, Va?clav, Pavel Mautner, and Toma?s
Pavelka, editors, Proceedings of the 8th Interna-
tional Conference on Text, Speech and Dialogue,
TSD 2005, Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg, September.
Springer.
Diab, Mona and Pravin Bhutada. 2009. Verb Noun
Construction MWE Token Classification. In Pro-
ceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambigua-
tion and Applications, pages 17?22, Singapore, Au-
gust. Association for Computational Linguistics.
Dura, Elz?bieta and Barbara Gawron?ska. 2005. To-
wards Automatic Translation of Support Verbs Con-
structions: the Case of Polish robic/zrobic and
Swedish go?ra. In Proceedings of the 2nd Language
& Technology Conference, pages 450?454, Poznan?,
Poland, April. Wydawnictwo Poznan?skie Sp. z o.o.
Fazly, Afsaneh and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
1116
Gre?goire, Nicole. 2007. Design and Implemen-
tation of a Lexicon of Dutch Multiword Expres-
sions. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 17?
24, Prague, Czech Republic, June. Association for
Computational Linguistics.
Kaalep, Heiki-Jaan and Kadri Muischnek. 2006.
Multi-Word Verbs in a Flective Language: The Case
of Estonian. In Proceedings of the EACL Workshop
on Multi-Word Expressions in a Multilingual Con-
texts, pages 57?64, Trento, Italy, April. Association
for Computational Linguistics.
Kaalep, Heiki-Jaan and Kadri Muischnek. 2008.
Multi-Word Verbs of Estonian: a Database and a
Corpus. In Proceedings of the LREC Workshop
Towards a Shared Task for Multiword Expressions
(MWE 2008), pages 23?26, Marrakech, Morocco,
June.
Krenn, Brigitte. 2008. Description of Evaluation Re-
source ? German PP-verb data. In Proceedings
of the LREC Workshop Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 7?10,
Marrakech, Morocco, June.
Langer, Stefan. 2004. A Linguistic Test Battery for
Support Verb Constructions. Lingvisticae Investi-
gationes, 27(2):171?184.
Mel?c?uk, Igor and Aleksander Z?olkovskij. 1984.
Explanatory Combinatorial Dictionary of Modern
Russian. Wiener Slawistischer Almanach, Vienna,
Austria.
Mel?c?uk, Igor, Andre? Clas, and Alain Polgue`re. 1995.
Introduction a` lexicologie explicative et combina-
toire. Duculot, Louvain-la-Neuve, France.
Mel?c?uk, Igor, et al 1984?1999. Dictionnaire ex-
plicatif et combinatoire du franc?ais contemporain:
Recherches lexico-se?mantiques I?IV. Presses de
l?Universite? de Montre?al, Montreal, Canada.
Meyers, Adam, Ruth Reeves, and Catherine Macleod.
2004a. NP-External Arguments: A Study of Argu-
ment Sharing in English. In Tanaka, Takaaki, Aline
Villavicencio, Francis Bond, and Anna Korhonen,
editors, Second ACL Workshop on Multiword Ex-
pressions: Integrating Processing, pages 96?103,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004b. The NomBank
Project: An Interim Report. In Meyers, Adam,
editor, HLT-NAACL 2004 Workshop: Frontiers in
Corpus Annotation, pages 24?31, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Oravecz, Csaba, Ka?roly Varasdi, and Viktor Nagy.
2004. To?bbszavas kifejeze?sek sza?m??to?ge?pes
kezele?se [The treatment of multiword expressions
in computational linguistics]. In Alexin, Zolta?n
and Do?ra Csendes, editors, MSzNy 2004 ? II. Ma-
gyar Sza?m??to?ge?pes Nyelve?szeti Konferencia, pages
141?154, Szeged, Hungary, December. University
of Szeged.
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002, pages 1?15, Mexico City,
Mexico.
Siepmann, Dirk. 2005. Collocation, colligation and
encoding dictionaries. Part I: Lexicological Aspects.
International Journal of Lexicography, 18(4):409?
444.
Siepmann, Dirk. 2006. Collocation, colligation
and encoding dictionaries. Part II: Lexicographical
Aspects. International Journal of Lexicography,
19(1):1?39.
Stevenson, Suzanne, Afsaneh Fazly, and Ryan North.
2004. Statistical Measures of the Semi-Productivity
of Light Verb Constructions. In Tanaka, Takaaki,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions: Integrating Processing, pages 1?
8, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Tan, Yee Fan, Min-Yen Kan, and Hang Cui. 2006.
Extending corpus-based identification of light verb
constructions using a supervised learning frame-
work. In Proceedings of the EACL Workshop on
Multi-Word Expressions in a Multilingual Contexts,
pages 49?56, Trento, Italy, April. Association for
Computational Linguistics.
To?th, Krisztina, Richa?rd Farkas, and Andra?s Kocsor.
2008. Hybrid algorithm for sentence alignment of
Hungarian-English parallel corpora. Acta Cyber-
netica, 18(3):463?478.
Tro?n, Viktor, Gyo?rgy Gyepesi, Pe?ter Hala?csy, Andra?s
Kornai, La?szlo? Ne?meth, and Da?niel Varga. 2005.
hunmorph: Open Source Word Analysis. In Pro-
ceedings of the ACL Workshop on Software, pages
77?85, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Va?radi, Tama?s. 2006. Multiword Units in an MT
Lexicon. In Proceedings of the EACL Workshop on
Multi-Word Expressions in a Multilingual Contexts,
pages 73?78, Trento, Italy, April. Association for
Computational Linguistics.
1117
Vincze, Veronika, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010.
Hungarian Dependency Treebank. In Calzolari,
Nicoletta, Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odjik, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
May. European Language Resources Association
(ELRA).
Vincze, Veronika. 2008. A puszta ko?zne?v + ige kom-
plexumok sta?tusa?ro?l [On the status of bare common
noun + verb constructions]. In Sinkovics, Bala?zs,
editor, LingDok 7. Nyelve?sz-doktoranduszok dolgo-
zatai, pages 265?283, Szeged, Hungary. University
of Szeged.
Vincze, Veronika. 2009. Fo?ne?v + ige szerkezetek a
szo?ta?rban [Noun + verb constructions in the dictio-
nary]. In Va?radi, Tama?s, editor, III. Alkalmazott
Nyelve?szeti Doktorandusz Konferencia, pages 180?
188, Budapest. MTA Nyelvtudoma?nyi Inte?zet.
Zarrie?, Sina and Jonas Kuhn. 2009. Exploit-
ing Translational Correspondences for Pattern-
Independent MWE Identification. In Proceedings
of the Workshop on Multiword Expressions: Identi-
fication, Interpretation, Disambiguation and Appli-
cations, pages 23?30, Singapore, August. Associa-
tion for Computational Linguistics.
1118
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1392?1401, Dublin, Ireland, August 23-29 2014.
An Empirical Evaluation of Automatic Conversion from Constituency to
Dependency in Hungarian
Katalin Ilona Simk
?
o
1
, Veronika Vincze
1,2
, Zsolt Sz
?
ant
?
o
1
, Rich
?
ard Farkas
1
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
kata.simko@gmail.com
{vinczev,szantozs,rfarkas}@inf.u-szeged.hu
Abstract
In this paper, we investigate the differences between Hungarian sentence parses based on auto-
matically converted and manually annotated dependency trees. We also train constituency parsers
on the manually annotated constituency treebank and then convert their output to dependency
trees. We argue for the importance of training on gold standard corpora, and we also demon-
strate that although the results obtained by training on the constituency treebank and converting
the output to dependency format and those obtained by training on the automatically converted
dependency treebank are similar in terms of accuracy scores, the typical errors made by different
systems differ from each other.
1 Introduction
Nowadays, two popular approaches to data-driven syntactic parsing are based on constituency grammar
on the one hand and dependency grammar on the other hand. There exist constituency-based treebanks
for many languages and dependency treebanks for most of these languages are converted automatically
from constituent trees with the help of conversion rules, which is the case for e.g. the languages used in
the SPMRL-2013 Shared Task (Seddah et al., 2013) with the exception of Basque, where constituency
trees are converted from manually annotated dependency trees (Aduriz et al., 2003), and Hungarian,
where both treebanks are manually annotated (Csendes et al., 2005; Vincze et al., 2010). However, the
quality of automatic dependency conversion is hardly investigated.
Hungarian is one of those rare examples where there exist manual annotations for both constituency
and dependency syntax on the same bunch of texts, the Szeged (Dependency) Treebank (Csendes et al.,
2005; Vincze et al., 2010), which makes it possible to evaluate the quality of a rule-based automatic con-
version from constituency to dependency trees, to compare the two sets of manual annotations and also
the output of constituency and dependency parsers trained on converted and gold standard dependency
trees.
We investigate the effect of automatic conversions related to the two parsing paradigms as well. It is
well known that for English, the automatic conversion of a constituency parser?s output to dependency
format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output
trained on automatically converted trees
1
(cf. Petrov et al. (2010)). One of the possible explanations for
this is that English is a configurational language, hence constituency parsers have advantages over depen-
dency parsers here. We check whether this hypothesis holds for Hungarian too, which is the prototype
of free word order languages.
In this paper, we compare three pairs of dependency analyses in order to evaluate the usefulness
of converted trees. First, we examine the errors of the conversion itself by comparing the converted
dependency trees with the manually annotated gold standard ones. Second, we argue for the importance
of training parsers on gold standard trees by looking at the typical differences between the outputs of
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
However, it has been pointed out that errors in the conversion script may significantly influence the results of parsing, see
e.g. Petrov and McDonald (2012) and Pitler (2012)
1392
dependency parsers trained on converted (silver standard) trees, parsers trained on gold standard trees and
the manual annotation itself. Third, we demonstrate that similar to English, training on a constituency
treebank and converting the results to dependency format can achieve similar results in terms of ULA to
the dependency parser trained on the automatically converted treebank, but the typical errors they make
differ in both cases.
2 Parsing Hungarian on the Szeged Treebank
Hungarian is a morphologically rich language, where word order encodes information structure, which
makes its syntactic analysis very different from English?s as the arguments in a sentence cannot be
determined by their position but by their suffixes, cf.
?
E. Kiss (2002). Words? grammatical functions
are signified by case suffixes and verbs are marked for the number and person of their subject and the
definiteness of their object, thus these arguments may be often omitted from the sentence: L?atlak (see-
1SG2OBJ) ?I see you?. Due to word order reasons, words that form one syntactic phrase may not be
adjacent (long-distance dependencies), which is true for the possessive construction as well: the posses-
sor and the possessed may be situated in two distant positions: A fi?unak elvette a kalapj?at (the boy-DAT
take-PAST-3SGOBJ the hat-POSS3SG-ACC) ?He took the boy?s hat?. Verbless clauses are also com-
mon in Hungarian, as the copula in third person singular present tense indicative form is phonologically
empty, while it is present in all other moods and tenses: A kalap piros (the hat red) ?The hat is red?, but
A kalap piros volt (the hat red was) ?The hat was red?.
The Szeged Treebank (Csendes et al., 2005) is a manually annotated constituency treebank for Hun-
garian consisting of 82,000 sentences. Besides the phrase structure, grammatical roles of the verbs?
arguments and morphological information are also annotated. It incorporates texts from six different
domains: short business news, newspaper, law, literature, compositions and informatics, however, in this
paper, we just focus on the short business news domain.
The Szeged Dependency Treebank (Vincze et al., 2010) contains manual dependency syntax annota-
tions for the same texts. Certain linguistic phenomena ? such as discontinuous structures ? are annotated
in this treebank, but not in the constituency treebank. In the dependency treebank, the possessor is linked
to the possession while this connection is not annotated in the constituency treebank. The two types of
trees can be seen in Figure 1.
CP
PUNC
.
NP-ACC
N
kalapj?at
T
a
V
V0
V
elvette
NP-GEN
N
fi?unak
T
A
A fi?unak elvette a kalapj?at .
ROOT
DET
GEN
DET
OBJ
PUNCT
Figure 1: Discontinuous structure A fi?unak elvette a kalapj?at (the boy-DAT take-past3SGOBJ the hat-
POSS3SG-ACC) ?He took the boy?s hat? in constituency and dependency analysis.
Another difference between the two treebanks is the way they represent different types of complex
sentences, as can be seen in Figure 2. In the dependency treebank subordinations and coordinations are
1393
handled very similarly. The head of one of the clauses (the subordinated clause or the second clause in
the case of coordination) is linked to the head of the other clause (the matrix clause of the subordination
or the first clause of the coordination), only the type of relation between the two heads differs in the
two structures, in the dependency tree in Figure 2, the heads of the three clauses (?atj?ott ?came over?,
meg??g?erte ?promised? and elj?on ?come?) are linked to one another through their conjunctions with either
an ATT relation in the case of subordination or COORD for coordination. In the constituency treebank
these sentences are represented very differently: in the case of subordination, the subordinated clause is
within the matrix clause: CP
3
is within CP
2
in the constituency tree in Figure 2. Coordinated clauses
appear at the same level in the structure, in the same figure CP
1
and CP
2
are coordinated clauses.
CP
PUNC
.
CP
2
CP
3
elj?on velem
C0
hogy
PUNC
,
V
meg??g?erte
C0
?es
CP
1
?
Atj?ott hozz?am
?
Atj?ott hozz?am ?es meg??g?erte , hogy elj?on velem .
ROOT
OBL
CONJ
COORD
PUNCT
CONJ
ATT
OBL
PUNCT
Figure 2: Constituency and dependency analysis of coordination and subordination in the sentence
?
Atj?ott
hozz?am ?es meg??g?erte, hogy elj?on velem (through.come-PAST-3SG to.me and promise-PAST-3SG-OBJ
that away.come-3SG with.me) ?He came over and promised that he will come with me?.
The parallels of these two manually annotated treebanks make them suitable for testing our hypotheses
about automatic dependency conversion. The differences between them originate from the characteristics
of constituent and dependency syntax.
3 Converting Constituency Trees to Dependency Trees
In this section, we present our methods to convert constituency trees to dependency trees and we also
discuss the most typical sources of errors during conversion.
3.1 Conversion rules
In order to convert constituency trees to dependency trees, we used a rule based system. Sentences with
virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their
treatment in dependency trees is also problematic (Farkas et al., 2012; Seeker et al., 2012). As a result,
we worked with 7,372 sentences and 162,960 tokens.
First, we determined the head of each clause (CP) and the relations between CPs in complex sentences.
In most cases the head of the CP is a finite verb, if the CP contains no finite verb, the head is the either an
infinitive verb or a participle, if none of these are present in the CP, the head can be a nominal expression.
The relations between the CP heads make up the base of the dependency structure using ROOT relation
for the sentence?s main verb, COORD for coordination and ATT for subordination, as well as CONJ in
the case of conjunctions between the CPs.
1394
The arguments of verbs, infinitives and participles in the CP were linked to their governor and marked
for their grammatical role in the Szeged Treebank. We used this information to construct the appropriate
dependency relations between governors and their arguments. The main grammatical roles such as sub-
ject, object, dative have their own label in dependency syntax, while minor ones are assigned the oblique
(OBL) relation. The argument?s modifiers were then linked to the head or other modifiers based on the
phrase structure with relations according to their morphological code.
Long distance dependencies, like the connection between a genitive case possessor and the possessed
are not annotated in the constituency treebank. In these cases we used morphological information to link
these elements together in the dependency tree. Figure 3 shows an example of converting a constituency
tree to a dependency tree.
CP
PUNC
.
V
V0
V
volt
NEG
R
nem
NP
N
?uzletk?ot?es
NP
N
h?uspiacon
T
A
A h?uspiacon ?uzletk?ot?es nem volt .
ROOT
DET
OBL
SUBJ
NEG
PUNCT
Figure 3: Conversion of the sentence A h?uspiacon ?uzletk?ot?es nem volt (the meat.market-SUP transaction
not was) ?There were no transactions at the meat market.? from constituency to dependency trees.
3.2 Error Analysis
We automatically converted the constituency treebank into dependency trees following the prin-
ciples described above and detailed at our website (http://www.inf.u-szeged.hu/rgai/
SzegedTreebank). For evaluation, we applied the metrics labeled attachment score (LAS) and un-
labeled attachment score (ULA), without punctuation marks. The accuracy of the conversion was 96.51
(ULA) and 93.85 (LAS). The errors made during conversion were categorized manually in 200 sentences
selected randomly from the short business news subcorpus of the Szeged Dependency Treebank, and the
most typical ones are listed in Table 1, Column convError.
As it is shown, the most common source of error was when more than one modifier was within a
phrase as the example in Figure 4 shows. In each figure, the gold standard parse can be seen on the left
hand side while the erroneous one can be seen on the right hand side.
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
ATT
eur?opai , olcs?o utakat k??n?al?o l?egit?arsas?ag
ATT
PUNCT
ATT
OBJ
COORD
Figure 4: Multiple modifier error in eur?opai, olcs?o utakat k??n?al?o l?egit?arsas?ag (European cheap trips-
ACC offering airline) ?European airline offering cheap trips?.
1395
Error type convError goldTrain silverTrain BerkeleyConv convDep
# % # % # % # % # %
Coordination 26 13.00 39 13.22 59 14.82 55 16.37 64 19.57
Multiple modifiers 26 13.00 30 10.17 49 12.31 52 15.48 47 14.37
Determiner 7 3.50 28 9.49 25 6.28 31 9.23 31 9.48
Conj./adverb attached 33 16.50 23 7.80 45 11.31 39 11.61 42 12.84
Arg. of verbal element 10 5.00 27 9.15 34 8.54 59 17.56 44 13.46
Sub- vs. coordination 7 3.50 9 3.05 12 3.02 ? ? ? ?
Possessor 9 4.50 14 4.75 16 4.02 28 8.33 22 6.73
Wrong root 14 7.00 17 5.76 23 5.78 35 10.42 27 8.26
Consecutive nouns 4 2.00 11 3.73 14 3.52 13 3.87 15 4.59
Multiword NE 8 4.00 25 8.47 33 8.29 8 2.38 19 5.81
Wrong MOD label 25 12.50 26 8.81 34 8.54 ? ? ? ?
Wrong other label 17 8.50 33 11.19 30 7.54 ? ? ? ?
Other errors 14 7.00 13 4.41 24 6.03 16 4.76 16 4.89
Total 200 100 295 100 398 100 336 100 327 100
Table 1: Error Types. convError: errors made during converting constituency trees to dependency trees.
goldTrain: errors in the output got by training the Bohnet parser on the gold standard data. silverTrain:
errors in the output got by training the Bohnet parser on the silver standard data. BerkeleyConv: errors in
the output got by training the Berkeley parser on the gold standard constituency data and converting the
output into dependency format. convDep: errors in the output got by training the Bohnet parser without
dependency labels on the silver standard data.
Coordination errors occurred when multiple members of a coordination were wrongly connected. On
the other hand, the attachment of conjunctions and some adverbs was also problematic, for example in
Figure 5 the conjunction is ?also? is connected to the verb in the gold standard and to the noun in the
converted version.
a miniszt?erium is besz?all
DET
SUBJ
CONJ
a miniszt?erium is besz?all
DET
SUBJ
CONJ
Figure 5: Conjunction attachment error in a miniszt?erium is besz?all (the ministry also steps.in) ?the
ministry also steps in?.
Also, the constituency treebank did not mark all the grammatical relations (e.g. numerals and deter-
miners were simply parts of an NP but had no distinct labeling, like [NP az ?ot [ADJP fekete] kutya]
(the five black dog) ?the five black dogs?), but it was necessary to assign them a dependency label and
a parent node during conversion. However, in some cases it was not straightforward which modifier
modifies which parent node: for instance, in [NP nem [ADJP megfelel?o] m?odszerek] (not appropriate
methods) ?inappropriate methods?, the negation word nem is erroneously attached to the noun instead of
the adjective in the converted phrase. Determiner errors were those where the determiner was attached
to the wrong noun in a NP with a noun modifier. In CPs with multiple verbal elements (both a finite verb
and an infinitive or a participle in the CP) the arguments were sometimes linked to the wrong verb, as in
Figure 6.
1396
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
a saj?at pecseny?ej?ukkel voltak elfoglalva
DET
ATT
OBL
MODE
Figure 6: Verbal argument error in a saj?at pecseny?ej?ukkel voltak elfoglalva (the own roast-3PLPOSS-INS
were busy) ?they were busy with their own thing?.
Possessors are sometimes wrongly identified during conversion as long distance dependencies are not
marked in the constituency treebank (see Figure 7).
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
SUBJ
OBL
a gy?art?o sz?ar??t?o?uzem?eben hasznos??t
DET
ATT
OBL
Figure 7: Possessor attachment error in a gy?art?o sz?ar??t?o?uzem?eben hasznos??t (the manufacturer
drying.plant-3SGPOSS-INE utilizes) ?the manufacturer utilizes it in its drying plant?.
In CPs with more verbal element, sometimes the wrong word is selected as the root, as in Figure 8.
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
ATT
ATT
SUBJ
ATT
OBJ
PREVERB
a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be
ROOT
DET
OBL
COORD
ATT
SUBJ
ATT
OBJ
PREVERB
Figure 8: Root error in a tenderre jelentkezett m?asik aj?anlattev?o ?erv?enytelen p?aly?azatot ny?ujtott be (the
tender-SUB applied other bidder invalid application-ACC submit-PAST-3SG) ?the other bidder applying
to the tender submitted an invalid application?.
In some cases, consecutive (but separate) noun phrases were taken as one unit as if one noun modified
the other, for example in Figure 9.
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
a tervezettn?el t?obb munkahelyet sz?untet meg
DET
OBL
ATT
OBJ
PREVERB
Figure 9: Consecutive noun error in a tervezettn?el t?obb munkahelyet sz?untet meg (the planned-ADE more
workplace-ACC terminates) ?it terminates more workplaces than planned?.
Multiword NEs also caused some problems in the conversion, as in Figure 10.
1397
Besz?all??t?oi Befektet?o Rt.
NE
NE
Besz?all??t?oi Befektet?o Rt.
ATT
NE
Figure 10: Multiword NE error in Besz?all??t?oi Befektet?o Rt. (a name of a company) .
In other cases, divergences between the gold standard and the converted trees are due to some erro-
neous annotations either in the constituency treebank or in the dependency treebank. A typical example
of this is the wrong MOD (modifier) label. In the treebank, locative and temporal modifiers were classi-
fied according to the tridirectionality typical of Hungarian adverbs and case suffixes: where, from where
and to where (or when, from what time and till what time) the action is taken place. Thus, there are
six dependency relations dedicated to these aspects and all the other adverbials are grouped under the
relation MOD. However, this distinction is rather semantic in nature and was sometimes erroneously
annotated in the constituency treebank, which was later corrected in the dependency one and thus now
resulted in conversion errors, as shown in Figure 11.
ny?ar v?ege fel?e kezdik
ATT
ATT
MODE
ny?ar v?ege fel?e kezdik
ATT
ATT
TO
Figure 11: MOD label error in ny?ar v?ege fel?e kezdik (summer end-3SGPOSS around begin) ?they begin
around the end of the summer?.
There were also some atypical errors that occurred too rarely to categorize them in a different class,
like cases when an article or determiner got erroneously attached to a verb and so on, so they were
lumped into the category of ?other errors? in Table 1.
4 Training on Gold Standard and Silver Standard Trees
We also experimented with training the Bohnet dependency parser (Bohnet, 2010) on the manually an-
notated (gold standard) and the converted (silver standard) treebank. The Bohnet parser (Bohnet, 2010)
is a state-of-the-art
2
graph-based parser, which employs online training with a perceptron. The parser
contains a feature function for the first order factor, one for the sibling factor, and one for the grandchil-
dren.
From the corpus, 5,892 sentences (130,211 tokens) were used in the training dataset and the remaining
1,480 sentences (32,749 tokens) in the test dataset. For evaluation, we again applied the metrics LAS
and ULA. Results are shown in Table 2, Rows goldTrain and silverTrain.
As the numbers show, better results can be achieved when the gold standard data are used as training
database than when the parser is trained on the silver standard data, the differences being 1.6% (ULA)
and 3.16% (LAS). Besides evaluation scores, we also compared the outputs of the two scenarios: we
used the same set of randomly selected sentences as when investigating conversion errors and carried out
a manual error analysis against the gold standard data in each case: see Table 1, Columns goldTrain and
silverTrain.
There are some common error types that seem to cause problems for both ways of parsing. For
instance, coordination and multiple modifiers are among the most frequent sources of errors in both
cases as for the error rates are concerned. However, with regard to the absolute numbers, we can see
that both error types are reduced when the gold standard dataset is used for training. On the other hand,
finding the parent node of a conjunction or an adverb seems to improve significantly when the parser is
trained on gold standard data. This is probably due to the fact that they are not marked in the constituency
treebank and thus training data for these grammatical phenomena are very noisy in the silver standard
treebank. All in all, we argue that there are some grammatical phenomena ? e.g. the attachment of
2
For a comparative evaluation with other dependency parsers on the same treebank see Farkas et al. (2012). According to
their results, the Bohnet parser achieved the best scores on the treebank hence we also used this parser in our experiments.
1398
Setting LAS ULA
Conversion 93.85 96.51
goldTrain 93.48 95.17
silverTrain 90.32 93.57
BerkeleyConv ? 92.78
convDep ? 93.23
Table 2: Results of the experiments. Conversion: converting constituency trees to dependency trees.
goldTrain: training the Bohnet parser on the gold standard data. silverTrain: training the Bohnet parser
on the silver standard data. BerkeleyConv: training the Berkeley parser on the gold standard constituency
data and converting the output into dependency format. convDep: training the Bohnet parser without
dependency labels on the silver standard data.
conjunctions or adverbs ? that require manual checking even if automatic conversion from constituency
to dependency is applied.
5 Pre- or Post Conversion?
It is well known that for English, converting a constituency parser?s output to dependency format (post
conversion) can achieve competitive ULA scores to a dependency parser?s output trained on automati-
cally converted trees (pre conversion) (Petrov et al., 2010; Farkas and Bohnet, 2012). One of the pos-
sible reasons for this may be that English is a configurational language, hence constituency parsers are
expected to perform better here. In this paper, we investigate whether this is true for Hungarian, which
is the prototype of morphologically rich languages with free word order.
We employed the product-of-grammars procedure (Petrov, 2010) of the Berkeleyparser (Petrov et al.,
2006), where grammars are trained on the same dataset but with different initialization setups, which
leads to different grammars. We trained 8 grammars and used tree-level inference. The output of the
parser was then automatically converted to dependency format, based on the rules described in Section
3 (BerkeleyConv). Second, we used the silver standard dependency treebank for training the Bohnet
parser (convDep). Since our constituency parser did not produce grammatical functions for the nodes,
we trained the Bohnet parser on unlabeled dependency trees in order to ensure a fair comparison here
(that is the difference between the columns BerkeleyConv and convDep in Table 1).
As the numbers show, competitive results can be obtained with both methods, yielding an ULA score
of 92.78 and 93.23, respectively. This means that the same holds for Hungarian as for English and the
surprisingly good results of post conversion are not related to the configurational level of the language.
Manually analysing the errors on the same set of sentences as before, there are again some error cate-
gories that occur frequently in both cases such as coordination, the attachment of conjunctions, modifiers
and determiners. On the other hand, training on constituency trees seems to have some specific sources
of errors. First, the possessor in possessive constructions is less frequently attached to its possessed,
which may be due to the fact that the genitive possessor is not linked to the possessed in the constituency
treebank and thus the parser is not able to learn this relationship. Second, arguments of verbal elements
(i.e. verbs, participles and infinitives) are also somewhat more difficult to find when there are at least two
verbal elements within the clause, which is especially true for adverbial participles and infinitives. In
Figure 6, the differences between the two trees are shown. The noun pecseny?ej?ukkel (roast-3PLPOSS-
INS) ?with their thing? is linked to the adverbial participle in the correct analysis, but it connects to the
main verb in the other. Third, identifying the root node of the sentence may also be problematic for this
setting. As Farkas and Bohnet (2012) reported that preconversion can achieve better results for finding
the root node in English, this seems to be a language-specific issue and it represents an interesting differ-
ence between English and Hungarian. Nevertheless, training on constituency trees has a beneficial effect
on finding multiword named entities. Hence, it can be concluded that although the evaluation scores are
similar, the errors the two systems make differ from each other.
1399
6 Discussion and Conclusions
Here, we compared dependency analyses of Hungarian obtained in different ways. It was revealed that
although the accuracy scores are similar to each other, each system makes different types of errors. On
the other hand, there are some specific linguistic phenomena that seem to be difficult for dependency
parsing generally as they were among the most frequent sources of errors in each case (e.g. coordination,
multiple modifiers and the attachment of conjunctions and adverbs).
Converting constituency trees into dependency trees enabled us to experiment with a silver standard
dependency corpus as well. Our results empirically showed that better results can be achieved on the
gold standard corpus, hence manual annotation of dependency trees is desirable. However, when there
is no access to manually annotated dependency data, converting the output of a constituency parser into
dependency format or training the dependency parser on converted data may also be viable: similar to
English, both solutions result in competitive scores but the errors the systems make differ from each
other.
In the future, we would like to investigate how the advantages of constituency and dependency repre-
sentations may be further exploited in parsing Hungarian and we also plan to carry out some uptraining
experiments with both types of parsers.
Acknowledgements
This work was supported in part by the European Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013).
References
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola, Aitziber Atutxa, A. Diaz de Ilarraza, Aitzpea Garmendia,
and Maite Oronoz. 2003. Construction of a Basque dependency treebank. In Proceedings of the 2nd Workshop
on Treebanks and Linguistic Theories (TLT), pages 201?204, V?axj?o, Sweden.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd
International Conference on Computational Linguistics (Coling 2010), pages 89?97.
D?ora Csendes, J?anos Csirik, Tibor Gyim?othy, and Andr?as Kocsor. 2005. The Szeged TreeBank. In V?aclav
Matousek, Pavel Mautner, and Tom?as Pavelka, editors, Proceedings of the 8th International Conference on
Text, Speech and Dialogue, TSD 2005, Lecture Notes in Computer Science, pages 123?132, Berlin / Heidelberg,
September. Springer.
Katalin
?
E. Kiss. 2002. The Syntax of Hungarian. Cambridge University Press, Cambridge.
Rich?ard Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Rich?ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency Parsing of Hungarian: Baseline Re-
sults and Challenges. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 55?65, Avignon, France, April. Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Linguistics, pages 433?440.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 705?713, Cambridge, MA, October. Association for Computational Linguistics.
Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
1400
Emily Pitler. 2012. Conjunction representation and ease of domain adaptation. Notes of the First Workshop on
Syntactic Analysis of Non-Canonical Language (SANCL).
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie Candito, Jinho D. Choi, Rich?ard Farkas, Jennifer Foster,
Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Yuval Marton, Joakim Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Woli?nski, and Alina Wr?oblewska. 2013. Overview of the SPMRL 2013
shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the
Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182, Seattle, Washing-
ton, USA, October. Association for Computational Linguistics.
Wolfgang Seeker, Rich?ard Farkas, Bernd Bohnet, Helmut Schmid, and Jonas Kuhn. 2012. Data-driven depen-
dency parsing with empty heads. In Proceedings of COLING 2012: Posters, pages 1081?1090, Mumbai, India,
December. The COLING 2012 Organizing Committee.
Veronika Vincze, D?ora Szauter, Attila Alm?asi, Gy?orgy M?ora, Zolt?an Alexin, and J?anos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings of LREC 2010, Valletta, Malta, May. ELRA.
1401
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1844?1853, Dublin, Ireland, August 23-29 2014.
Uncertainty Detection in Hungarian Texts
Veronika Vincze
1,2
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
Abstract
Uncertainty detection is essential for many NLP applications. For instance, in information re-
trieval, it is of primary importance to distinguish among factual, negated and uncertain informa-
tion. Current research on uncertainty detection has mostly focused on the English language, in
contrast, here we present the first machine learning algorithm that aims at identifying linguistic
markers of uncertainty in Hungarian texts from two domains: Wikipedia and news media. The
system is based on sequence labeling and makes use of a rich feature set including orthographic,
lexical, morphological, syntactic and semantic features as well. Having access to annotated data
from two domains, we also focus on the domain specificities of uncertainty detection by compar-
ing results obtained in indomain and cross-domain settings. Our results show that the domain of
the text has significant influence on uncertainty detection.
1 Introduction
Uncertainty detection has become one of the most intensively studied problems of natural language pro-
cessing (NLP) in these days (Morante and Sporleder, 2012). For several NLP applications, it is essential
to distinguish between factual and nonfactual, i.e. negated or uncertain information: for instance, in
medical information retrieval, it must be known whether the patient definitely suffers, probably suffers
or does not suffer from an illness. This type of information can only be revealed from the texts of the
documents if reliable uncertainty detectors are available, which are able to identify linguistic markers
of uncertainty, i.e. cues within the text. To the best of our knowledge, uncertainty detectors have been
mostly developed for the English language (Morante and Sporleder, 2012; Farkas et al., 2010). Here,
we present our machine learning based uncertainty detector developed for Hungarian, a morphologically
rich language, and report our results on a manually annotated uncertainty corpus, which contains texts
from two domains: first, Hungarian Wikipedia texts and second, pieces of news from a Hungarian news
portal.
The main contributions of this paper are the following:
? it presents the first uncertainty corpus for Hungarian;
? it reports the first results on uncertainty detection in Hungarian texts;
? it introduces new features in the machine learning setting like semantic and pragmatic features;
? we show that there are domain specificities in the distribution of uncertainty cues in Hungarian texts;
? we show that domain specificities have a considerable effect on the efficiency of machine learning.
The structure of the paper is the following. First, related work on uncertainty detection is presented.
Then our corpus is described in detail, which is followed by the elaboration of machine learning methods
and results on uncertainty detection. The paper concludes with a discussion of results and possible ways
for future work are also outlined.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1844
2 Related Work
In these days, identifying uncertainty cues is one of the popular topics in NLP. This is supported by the
CoNLL-2010 Shared Task, which aimed at detecting uncertainty cues in biological papers and Wikipedia
articles written in English (Farkas et al., 2010). Moreover, a special issue of the journal Computational
Linguistics (Vol. 38, No. 2) was recently dedicated to detecting modality and negation in natural lan-
guage texts (Morante and Sporleder, 2012). As indicated above, most earlier research on uncertainty
detection focused on the English language. As for the domains of the texts, newspapers (Saur?? and
Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al.,
2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al.,
2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments.
Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al.,
2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see
e.g. Medlock and Briscoe (2007), Morante et al. (2009),
?
Ozg?ur and Radev (2009), Szarvas et al. (2012)
and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest
tendencies, our system here is also based on supervised machine learning techniques, which employs a
rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual
features.
Supervised machine learning methods require annotated corpora. There have been several corpora
annotated for uncertainty in different domains such as biology (Medlock and Briscoe, 2007; Kim et al.,
2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner
et al., 2009), news media (Saur?? and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010),
encyclopedia (Farkas et al., 2010), reviews (Konstantinova et al., 2012; Cruz D??az, 2013) and social
media (Wei et al., 2013). For our experiments, however, we make use of the first Hungarian uncertainty
corpus created for the purpose of this study.
3 Experiments
In this section, we present our methodology to detect uncertainty cues in Hungarian. We first describe the
uncertainty categories applied and report some statistics on the corpus. Then we describe our machine
learning approach based on a rich feature set.
3.1 The hUnCertainty Corpus
For the purpose of this study, we manually annotated texts from two domains. First, we randomly
selected 1,081 paragraphs from the Hungarian Wikipedia dump. This selection contains 9,722 sentences
and 180,000 tokens. Second, we downloaded 300 pieces of criminal news from a Hungarian news portal
(http://www.hvg.hu), which altogether consist of 5,481 sentences and 94,000 tokens. In total, the
hUnCertainty corpus consists of 15,203 sentences and 274,000 tokens.
During annotation, we followed the categorization of uncertainty phenomena as described in Szarvas
et al. (2012) and Vincze (2013) with some slight modifications, due to the morphologically rich nature
of Hungarian (for instance, modal auxiliaries like may correspond to a derivational suffix in Hungarian,
which required that in the case of j?ohet ?may come? the whole word was annotated as uncertain, not
just the suffix -het). Here we just briefly summarize uncertainty categories that were annotated ? for a
detailed discussion, please refer to Szarvas et al. (2012) and Vincze (2013).
Linguistic uncertainty is traditionally connected to modality and the semantics of the sentence. For
instance, the sentence It may be raining does not contain enough information to determine whether it
is really raining (semantic uncertainty). There are several phenomena that are categorized as semantic
uncertainty. A proposition is epistemically uncertain if its truth value cannot be determined on the
basis of world knowledge. Conditionals and investigations also belong to this group ? the latter is
especially frequent in research papers, where authors usually formulate the research question with the
help of linguistic devices expressing this type of uncertainty. Non-epistemic types of modality may also
be listed here such as doxastic uncertainty, which is related to beliefs.
1845
However, there are other uncertainty phenomena that only become uncertain within the context of
communication. For instance, the sentence Many researchers think that COLING will be the best con-
ference of the year does not reveal how many (and which) researchers think that, hence the source of
the proposition about COLING remains uncertain. This is a type of discourse-level uncertainty, more
specifically, it is called weasel (Ganter and Strube, 2009). On the other hand, hedges make the meaning
of words fuzzy: they blur the exact meaning of some quality/quantity. Finally, peacock cues express
unprovable (or unproven) evaluations, qualifications, understatements and exaggerations.
Some examples of uncertainty cues are offered here (in English, for the sake of simplicity):
EPISTEMIC: It may be raining.
DYNAMIC: I have to go.
DOXASTIC: He believes that the Earth is flat.
INVESTIGATION: We examined the role of NF-kappa B in protein activation.
CONDITION: If it rains, we?ll stay in.
WEASEL: Some note that the number of deaths during confrontations with police is relatively
proportional for a city the size of Cincinnati.
HEDGE: Magdalene Asylums were a generally accepted social institution until well into the
second half of the 20th century.
PEACOCK: The main source of their inspiration was native Georgia, with its rich and complex
history and culture, its breathtaking landscapes and its courageous and hardworking people.
Table 1 reports some statistics on the frequency of uncertainty cues in Hungarian and it is also vi-
sualized in Figure 1. It is revealed that the domain of the texts has a strong effect on the distribution
of uncertainty cues: the distribution of semantic uncertainty cues and discourse-level uncertainty cues
is balanced in the news subcorpus but in the Wikipedia corpus, about 85% of the cues belong to the
discourse-level uncertainty type.
Regarding different classes of uncertainty, we should mention that while weasels constitute the most
frequent cue category in Wikipedia texts, they occur less frequently in the news corpus. On the other
hand, doxastic cues are frequent in the news corpus but in Wikipedia texts, their number is considerably
smaller.
Uncertainty cue Wikipedia News Total
# % # % # %
Weasel 2150 35.95 258 10.93 2408 28.87
Hedge 2100 35.12 800 33.88 2900 34.77
Peacock 788 13.18 94 3.98 882 10.57
Discourse-level total 5038 84.25 1152 48.79 6190 74.21
Epistemic 441 7.37 358 15.16 799 9.58
Doxastic 316 5.28 710 30.07 1026 13.30
Conditional 154 2.58 128 5.42 282 3.38
Investigation 31 0.52 13 0.55 44 0.53
Semantic total 942 15.75 1209 51.21 2151 25.79
Total 5980 100 2361 100 8341 100
Table 1: Uncertainty cues.
3.2 Machine Learning Methods
In order to automatically identify uncertainty cues, we developed a machine learning method to be dis-
cussed below. In our experiments, we used the above-described corpus and morphologically and syntac-
tically parsed it with the help of the toolkit magyarlanc (Zsibrita et al., 2013).
1846
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
News Wikipedia hUnCertainty
investigation
condition
doxastic
epistemic
peacock
hedge
weasel
Figure 1: Distribution of cues across domains.
On the basis of results reported in earlier literature, sequence labeling proved to be one of the most
successful methods on English uncertainty detection (see e.g. Szarvas et al. (2012)), hence we also ap-
plied a method based on conditional random fields (CRF) (Lafferty et al., 2001) in our experiments. We
used the MALLET implementation (McCallum, 2002) of CRF with the following rich feature set:
? Orthographic features: we investigated whether the word contains punctuation marks, digits, up-
percase or lowercase letters, the length of the word, consonant bi- and trigrams...
? Lexical features: we automatically collected uncertainty cues from the English corpora annotated
on the basis of similar linguistic principles and manually translated these lists into Hungarian. Lists
were used as binary features: if the lemma of the given word occurred in one of the lists, the feature
was assigned the value true, else it was false.
? Morphological features: for each word, its part of speech and lemma were noted. As mentioned
before, modality and mood are morphologically expressed in Hungarian (e.g. in csin?alhatn?ank do-
MOD-COND-1PL ?we could do?, the suffix -hat refers to modality and the suffix -n?a refers to
conditional) hence for each verb, it was investigated whether it had a modal suffix, whether it was
in the conditional mood and whether its form was first person plural or third person plural as these
two latter verbal forms are typical instances of expressing generic phrases or generalizations in
Hungarian, which are related to weasels. For each noun, its number (i.e. singular/plural) was marked
as feature. For each pronoun, we checked whether it was an indefinite one since indefinite pronouns
like valaki ?someone? or valamilyen ?some? are often used as weasel cues. For each adjective, we
marked whether it was comparative or superlative as they can occur as peacock cues.
? Syntactic features: for each word, its dependency label was marked. For each noun, it was checked
whether it had a determiner as determinerless nouns may be used as weasels in Hungarian. For each
verb, it was checked whether it had a subject
1
.
? Semantic/pragmatic features: we manually compiled a list of speech act verbs in Hungarian and
checked whether the given verb was one of them. Besides, we translated lists of English words with
1
Hungarian is a pro-drop language, hence the subject is not obligatorily present in the clause. Moreover, applying a third
person plural verb without a subject is a common way to express generalization in Hungarian, which is one typical strategy of
weasels.
1847
positive and negative content developed for sentiment analysis (Liu, 2012) and checked whether the
lemma of the given word occurred in these lists.
As contextual features for each word, we applied as features the POS tags and dependency labels of
words within a window of size two. Although earlier research on English uncertainty detection mostly
made use of orthographical, morphological and syntactic information (see e.g. Szarvas et al. (2012)),
here we included some new feature types in our feature set, namely, pragmatic and semantic features.
Based on this feature set, we carried out our experiments. Since only 3% of the tokens in the corpus
function as uncertainty cues, it seemed necessary to filter the training database: half of the cueless
sentences were randomly selected and deleted from the training dataset. Moreover, as there were only
44 investigation cues in the data, we omitted this class from training and evaluation as well, due to
sparseness problems.
First, we applied ten-fold cross validation on the corpus. Since we had two domains of texts at hand, it
enabled us to experiment with the two domains separately as well: ten-fold cross validation was carried
out for both domains individually and we also made use of cross-domain settings, where one of the
domains was used as the training database but the evaluation was performed on the other domain. For
evaluation, we used the metrics precision, recall and F-score. The results of our experiments will be
presented in Section 4.
3.3 Baseline Methods
As a baseline, we applied a simple dictionary lookup method. Lists mentioned among the lexical features
were utilized here: whenever the lemma of the given word matched one of the words in the list, we tagged
it as an uncertainty cue of the type determined by the given list.
4 Results
Table 2 shows the results of the baseline and machine learning experiments on the hUnCertainty corpus,
obtained by ten-fold cross validation.
Dictionary lookup Machine learning Difference
Type Precision Recall F-score Precision Recall F-score Precision Recall F-score
Weasel 18.12 35.92 24.09 52.48 30.73 38.76 +34.37 -5.19 +14.68
Hedge 55.10 32.42 40.82 61.26 48.94 54.41 +6.17 +16.52 +13.59
Peacock 21.66 30.77 25.42 32.61 11.88 17.41 +10.95 -18.89 -8.01
Epistemic 42.46 30.02 35.18 63.18 34.07 44.27 +20.72 +4.04 +9.09
Doxastic 29.30 46.16 35.85 52.42 46.26 49.15 +23.12 +0.10 +13.30
Condition 31.73 62.90 42.18 51.41 25.80 34.35 +19.68 -37.10 -7.83
Micro P/R/F 29.09 35.74 32.07 55.95 37.46 44.87 +26.86 +1.72 +12.80
Table 2: Results on the hUnCertainty corpus.
The results of the machine learning approach have outperformed those achieved by the baseline dic-
tionary lookup method, except for two classes. This is primarily due to better precision, which has grown
for each uncertainty category in the case of sequence labeling. However, recall values are more diverse:
for hedges and epistemic cues, it has grown, for doxastic cues it has not changed significantly, but for
peacocks and conditional cues we can see a serious decrease. The low recall values might be the reason
why the F-score obtained by the dictionary lookup method is higher than the one obtained by machine
learning in the case of peacocks and conditionals.
We also experimented separately on the two domains. Table 3 shows those on the news subcorpus,
whereas Table 4 shows the results achieved on the Wikipedia subcorpus.
In both domains, we can observe that machine learning methods outperform the baseline dictionary
lookup method, except for the peacock and conditional cue classes. However, there are domain differ-
ences in the results. First, weasels seem to be much hard to detect in the news subcorpus than in the
1848
Dictionary lookup Machine learning Difference
Type Precision Recall F-score Precision Recall F-score Precision Recall F-score
Weasel 3.24 17.83 5.48 37.50 15.12 21.55 +34.26 -2.71 +16.06
Hedge 53.61 39.05 45.18 61.55 49.69 54.99 +7.94 +10.64 +9.80
Peacock 13.82 31.91 19.29 47.06 8.51 14.41 +33.23 -23.40 -4.88
Epistemic 31.90 20.67 25.08 56.63 39.39 46.46 +24.73 +18.72 +21.37
Doxastic 33.50 37.61 35.43 57.05 51.83 54.32 +23.55 +14.23 +18.88
Condition 35.27 57.03 43.58 54.39 24.22 33.51 +19.12 -32.81 -10.07
Micro P/R/F 23.21 34.17 27.65 57.31 41.93 48.43 +34.10 +7.76 +20.78
Table 3: Results on the news subcorpus.
Dictionary lookup Machine learning Difference
Type Precision Recall F-score Precision Recall F-score Precision Recall F-score
Weasel 26.03 38.50 31.06 59.26 34.74 43.80 +33.23 -3.76 +12.74
Hedge 55.86 29.92 38.97 64.59 50.02 56.38 +8.73 +20.10 +17.41
Peacock 23.29 30.63 26.46 37.85 13.8 20.22 +14.56 -16.83 -6.24
Epistemic 49.57 37.34 42.59 63.95 36.03 46.09 +14.38 -1.31 +3.50
Doxastic 25.24 65.20 36.40 54.31 33.54 41.47 +29.07 -31.66 +5.07
Condition 29.66 67.74 41.26 47.12 31.61 37.84 +17.46 -36.13 -3.42
Micro P/R/F 32.28 36.40 34.21 59.70 37.5 46.06 +27.42 +1.10 +11.85
Table 4: Results on the Wikipedia subcorpus.
Wikipedia subcorpus (21.55 vs. 43.8 in terms of F-score). Second, peacocks are also harder to detect in
the news subcorpus (F-scores of 14.41 vs. 20.22). Third, there is a considerable gap between the recall
scores in the case of doxastic cues: in the Wikipedia subcorpus, the dictionary lookup method outper-
forms CRF (the difference is 36.13 percentage points) but in the news subcorpus, CRF achieves higher
recall with 14.23 percentage points.
To further explore domain differences, we carried out some cross validation experiments. First, we
trained our CRF model on the Wikipedia domain and then evaluated it on the news domain. Later, the
model was trained on the news domain and evaluated on the Wikipedia domain. Tables 5 and 6 present
the results, respectively, contrasted to the results achieved in the indomain settings. It is also striking
that although the gain in micro F-score is almost the same in the two settings, the biggest difference
can be observed for semantic uncertainty classes in the case of the Wikipedia? news setting, while the
difference is much bigger for discourse-level uncertainty types in the news?Wikipedia setting.
Cross validation Indomain ten fold Difference
Type Precision Recall F-score Precision Recall F-score Precision Recall F-score
Weasel 17.53 19.77 18.58 37.50 15.12 21.55 +19.97 -4.65 +2.97
Hedge 57.40 39.30 46.66 61.55 49.69 54.99 +4.15 +10.39 +8.33
Peacock 22.81 13.83 17.22 47.06 8.51 14.41 +24.25 -5.32 -2.80
Epistemic 50.00 16.76 25.10 56.63 39.39 46.46 +6.63 +22.63 +21.35
Doxastic 46.63 10.70 17.41 57.05 51.83 54.32 +10.43 +41.13 +36.91
Condition 62.96 26.56 37.36 54.39 24.22 33.51 -8.58 -2.34 -3.85
Micro P/R/F 44.48 23.35 30.62 57.31 41.93 48.43 +12.83 +18.58 +17.81
Table 5: Cross-domain results: Wikipedia? news.
As some uncertainty detectors aim at identifying uncertain sentences only, that is, they handle the task
at the sentence level and do not pay attention to the detection of individual cues (Medlock and Briscoe,
2007), we also applied a more relaxed evaluation metric. If at least one of the tokens within the sentence
was labeled as an uncertainty cue ? regardless of its type ?, the sentence was considered as uncertain.
1849
Cross validation Indomain ten fold Difference
Type Precision Recall F-score Precision Recall F-score Precision Recall F-score
Weasel 71.26 6.87 12.53 59.26 34.74 43.8 -12.00 +27.87 +31.27
Hedge 63.48 26.33 37.22 64.59 50.02 56.38 +1.11 +23.69 +19.16
Peacock 43.14 5.57 9.87 37.85 13.80 20.22 -5.29 +8.23 +10.35
Epistemic 78.65 30.57 44.03 63.95 36.03 46.09 -14.70 +5.46 +2.06
Doxastic 39.55 33.23 36.12 54.31 33.54 41.47 +14.76 +0.31 +5.35
Condition 47.31 28.39 35.48 47.12 31.61 37.84 -0.19 +3.22 +2.36
Micro P/R/F 59.98 18.00 27.68 59.7 37.5 46.06 -0.28 +19.50 +18.38
Table 6: Cross-domain results: news?Wikipedia.
Results on the identification of uncertain sentences are summarized in Table 7, in terms of precision,
recall and F-score. It is revealed that here there are no sharp differences in performance as far as the
indomain settings are concerned since the system can achieve an F-score of about 70 in both domains
and on the whole corpus as well. However, in the cross-domain settings lower precision values and
F-scores can be observed, while recall values basically remain the same with regard to the indomain
settings.
Evaluation setting Precision Recall F-score
hUnCertainty 10 fold 62.20 78.06 69.23
News 10 fold 67.38 78.01 72.30
Wikipedia 10 fold 60.32 80.05 68.80
Wikipedia? news 45.88 74.21 56.70
News?Wikipedia 35.73 84.61 50.24
Table 7: Machine learning results at the sentence level.
5 Discussion
Our results prove that a sequence labeling approach can be efficiently used for the automatic identification
of uncertainty cues in Hungarian texts. With our baseline dictionary lookup method, the best results
were achieved on the epistemic, conditional and hedge cues while the sequence labeling approach was
the most successful on the hedge, epistemic and doxastic cues. All of this indicates that hedge and
epistemic cues are the easiest to detect. On the other hand, uncertainty types where there was a small
difference between the results achieved by the two approaches (for instance, semantic uncertainty cues in
the Wikipedia subcorpus) are mostly expressed by lexical means and these cues are less ambiguous. In
this setting, the detection of discourse-level uncertainty categories, however, profits more from machine
learning, which is most probably due to the fact that here context (discourse) plays a more important rule
hence a sequence labeling algorithm is more appropriate for the task, which takes into account contextual
information as well.
In the case of peacocks and conditional cues the sequence labeling approach obtained worse results
than dictionary lookup: in each case, precision got higher but recall seriously decreased. This suggests
that these classes highly rely on lexical features and our machine learning system needs further improve-
ment, with special regard to specific (lexical) features defined for these uncertainty categories.
As for domain differences, we found that the distribution of uncertainty cues differs in the two sub-
corpora, weasels being more frequent in Wikipedia whereas doxastic cues are more probable to occur
in the news subcorpus. Domain differences concerning weasels and doxastic cues are highlighted in the
cross domain experiments as well. When the training dataset contains fewer cues of the given uncertainty
type, the performance falls back on the target domain: when trained on the news subcorpus, an F-score
of 12.53 can be obtained for weasels in the Wikipedia subcorpus, which is 31.27 points less than the
indomain results. Similarly, an F-score of 17.41 can be obtained for doxastic cues in the news domain
1850
when Wikipedia is used as the training set but the indomain setting yields an F-score of 54.32.
All of the above facts may be related to the characteristics of the texts. Weasels are sourceless propo-
sitions and in the news media, it is indispensable to know who the source of the news is, thus, pieces
are usually reported with their source provided and so, propositions with no explicit source (i.e. weasels)
occur rarely in the news subcorpus. On the other hand, doxastic cues are related to beliefs and the news
subcorpus consists of criminal news (mostly related to murders). When describing the possible reasons
behind each criminal act, phrases that refer to beliefs and mental states are often used and thus this type
of uncertainty is likely to be present in such pieces of news but not in Wikipedia articles.
In the cross domain experiments, indomain results outperform those obtained by the cross domain
models. The difference in performance is significant (t-test, p = 0.042 for the news subcorpus and p
= 0.0103 for the Wikipedia subcorpus). That is, the choice of the training dataset significantly affects
the results, which indicates that there really are domain differences in uncertainty detection. There are
only two exceptions that do not correspond to these tendencies: the peacock and conditional cues in the
Wikipedia ? news setting. The reason why a model trained on a different domain can perform better
might lie in the size of the subcorpora. The Wikipedia domain contains much more peacock cues than
the news domain and although the domains are different, training on a dataset with more cue instances
seems to be beneficial for the results.
If we evaluate the models? performance at the sentence level rather than at the cue level, it can be
observed that better results can be achieved, especially with regard to recall values. One reason for
that may be that a single uncertain sentence may include more than one cues and should one of them
be missed, it does not seriously harm performance (in case at least one cue per sentence is correctly
detected).
If our results are compared to those achieved on semantic uncertainty cues found in English Wikipedia
articles (Szarvas et al., 2012), it can be seen that the task seems to be somewhat easier in English than
in Hungarian: that paper reports F-scores from 0.6 to 0.8. One possible reason for this is that there are
typological differences between English and Hungarian and so, uncertainty marking is rather lexically
determined in English but in Hungarian, morphology also plays an essential role. For instance, the
modal suffixes -hat/-het correspond to the auxiliaries may and might and while in English they function
as separate lexical items, in Hungarian they are always attached to the verbal stem and never occur on
their own. This is reflected in the number of different cues as well: in the English dataset, there are 166
different semantic cues while in Hungarian, there are 319 (and note that the Hungarian corpus is about
half of the size of the English one). As such, applying the word form or the lemma as features may
result in relatively high F-scores in English, where the word form itself denotes uncertainty, but these
features are less effective in Hungarian without any morphological features included. Another language-
specific feature is that Hungarian is a pro-drop language, so in some cases, the pronominal subject may be
omitted from the sentence. Subjectless sentences are a typical strategy in Hungarian to express sourceless
statements (weasels), but the subject can be deleted due to syntactic ellipsis as well, thus distinguishing
between subjectless sentences that denote uncertainty and those that do not is a special task in Hungarian
uncertainty detection.
The outputs of the machine learning system were further investigated, in order to find the most typi-
cal errors our system made. It was revealed that the most problematic issue was the disambiguation of
ambiguous cues. For instance, the words sz?amos ?several? or sok ?many? may function as hedges or
weasels, or nagy ?big? may be a hedge or a peacock, depending on the context. Such cues were often
misclassified by the system. Another common source of errors was that some cues have non-cue mean-
ings as well, like the verb tart, which can be a doxastic cue with the meaning ?think? but when it means
?keep?, it is not uncertain at all. The identification of epistemic cues that include negation words was
also not straightforward: multiword cues such as nem z?arhat?o ki ?it cannot be excluded? or nem tudni ?it
is not known? were not marked as cues by the system.
1851
6 Conclusions
In this paper, we presented the first results on Hungarian uncertainty detection. For this, we made use
of a manually annotated corpus, which contains texts from two domains: Wikipedia articles and pieces
of news from a news portal. We contrasted the cue distribution in the two domains and we also experi-
mented with uncertainty detection. For this purpose, we applied a supervised machine learning approach,
which was based on sequence labeling and exploited a rich feature set. We reported the first results on
uncertainty detection for Hungarian, which also prove that the performance on uncertainty detection is
influenced by the domain of the texts. We hope that this study will enhance research on uncertainty
detection for languages other than English.
In the future, we would like to improve our methods, especially in order to achieve better recall at the
cue level. Furthermore, we would like to investigate domain specificities in more detail and we would
also like to carry out some domain adaptation experiments as well.
Acknowledgments
This research was supported by the European Union and the State of Hungary, co-financed by the Eu-
ropean Social Fund in the framework of T
?
AMOP-4.2.4.A/2-11/1-2012-0001 ?National Excellence Pro-
gram?.
References
Wendy W. Chapman, David Chu, and John N. Dowling. 2007. Context: An algorithm for identifying contextual
features from clinical text. In Proceedings of the ACL Workshop on BioNLP 2007, pages 81?88.
Noa P. Cruz D??az. 2013. Detecting negated and uncertain information in biomedical and review texts. In Proceed-
ings of the Student Research Workshop associated with RANLP 2013, pages 45?50, Hissar, Bulgaria, September.
RANLP 2013 Organising Committee.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the
Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1?
12, Uppsala, Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia
Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 173?176, Suntec, Singapore, August. Association for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008. Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(Suppl 10).
Natalia Konstantinova, Sheila C.M. de Sousa, Noa P. Cruz, Manuel J. Mana, Maite Taboada, and Ruslan Mitkov.
2012. A review corpus annotated for negation, speculation and their scope. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey. European Language Resources Association (ELRA).
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of ICML-01, 18th Int. Conf. on Machine Learning,
pages 282?289. Morgan Kaufmann.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations,
and statements in between. In Proc. of the HLT-NAACL 2004 Workshop: Biolink 2004, Linking Biological
Literature, Ontologies and Databases, pages 17?24.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers.
Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Ben Medlock and Ted Briscoe. 2007. Weakly Supervised Learning for Hedge Classification in Scientific Litera-
ture. In Proceedings of the ACL, pages 992?999, Prague, Czech Republic, June.
1852
Roser Morante and Caroline Sporleder. 2012. Modality and negation: An introduction to the special issue.
Computational Linguistics, 38:223?260, June.
Roser Morante, Vincent van Asch, and Antal van den Bosch. 2009. Joint memory-based learning of syntactic and
semantic dependencies in multiple languages. In Proceedings of CoNLL, pages 25?30.
Raheel Nawaz, Paul Thompson, and Sophia Ananiadou. 2010. Evaluating a meta-knowledge annotation scheme
for bio-events. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,
pages 69?77, Uppsala, Sweden, July. University of Antwerp.
Arzucan
?
Ozg?ur and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398?1407,
Singapore, August. Association for Computational Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando. 2005. Certainty identification in texts: Categorization
model and manual tagging results. In J.G. Shanahan, J. Qu, and J. Wiebe, editors, Computing attitude and affect
in text: Theory and applications (the information retrieval series), New York. Springer Verlag.
Victoria L. Rubin. 2010. Epistemic modality: From uncertainty to certainty in the context of information seeking
as interactions with texts. Information Processing & Management, 46(5):533?540.
Roser Saur?? and James Pustejovsky. 2009. FactBank: a corpus annotated with event factuality. Language Re-
sources and Evaluation, 43:227?268.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings
of the NIPS Workshop on Cost-Sensitive Learning, pages 1?10.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and W. John Wilbur. 2008. Multi-dimensional classification of
biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics,
24(18):2086?2093.
Gy?orgy Szarvas, Veronika Vincze, Rich?ard Farkas, Gy?orgy M?ora, and Iryna Gurevych. 2012. Cross-genre and
cross-domain detection of semantic uncertainty. Computational Linguistics, 38:335?367, June.
?
Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda. 2009. Machine Learning and Rule-based Approaches to
Assertion Classification. Journal of the American Medical Informatics Association, 16(1):109?115, January.
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas, Gy?orgy M?ora, and J?anos Csirik. 2008. The BioScope Corpus:
Biomedical Texts Annotated for Uncertainty, Negation and their Scopes. BMC Bioinformatics, 9(Suppl 11):S9.
Veronika Vincze. 2013. Weasels, Hedges and Peacocks: Discourse-level Uncertainty in Wikipedia Articles.
In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 383?391,
Nagoya, Japan, October. Asian Federation of Natural Language Processing.
Zhongyu Wei, Junwen Chen, Wei Gao, Binyang Li, Lanjun Zhou, Yulan He, and Kam-Fai Wong. 2013. An em-
pirical study on uncertainty identification in social media context. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pages 58?62, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh, Pittsburgh.
J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas. 2013. magyarlanc: A Toolkit for Morphological and Depen-
dency Parsing of Hungarian. In Proceedings of RANLP-2013, pages 763?771, Hissar, Bulgaria.
1853
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?65,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing of Hungarian: Baseline Results and Challenges
Richa?rd Farkas1, Veronika Vincze2, Helmut Schmid1
1Institute for Natural Language Processing, University of Stuttgart
{farkas,schmid}@ims.uni-stuttgart.de
2Research Group on Artificial Intelligence, Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
Abstract
Hungarian is a stereotype of morpholog-
ically rich and non-configurational lan-
guages. Here, we introduce results on de-
pendency parsing of Hungarian that em-
ploy a 80K, multi-domain, fully manu-
ally annotated corpus, the Szeged Depen-
dency Treebank. We show that the results
achieved by state-of-the-art data-driven
parsers on Hungarian and English (which is
at the other end of the configurational-non-
configurational spectrum) are quite simi-
lar to each other in terms of attachment
scores. We reveal the reasons for this and
present a systematic and comparative lin-
guistically motivated error analysis on both
languages. This analysis highlights that ad-
dressing the language-specific phenomena
is required for a further remarkable error re-
duction.
1 Introduction
From the viewpoint of syntactic parsing, the lan-
guages of the world are usually categorized ac-
cording to their level of configurationality. At one
end, there is English, a strongly configurational
language while Hungarian is at the other end of
the spectrum. It has very few fixed structures
at the sentence level. Leaving aside the issue of
the internal structure of NPs, most sentence-level
syntactic information in Hungarian is conveyed
by morphology, not by configuration (E?. Kiss,
2002).
A large part of the methodology for syntactic
parsing has been developed for English. How-
ever, parsing non-configurational and less config-
urational languages requires different techniques.
In this study, we present results on Hungarian de-
pendency parsing and we investigate this general
issue in the case of English and Hungarian.
We employed three state-of-the-art data-driven
parsers (Nivre et al 2004; McDonald et al 2005;
Bohnet, 2010), which achieved (un)labeled at-
tachment scores on Hungarian not so different
from the corresponding English scores (and even
higher on certain domains/subcorpora). Our in-
vestigations show that the feature representation
used by the data-driven parsers is so rich that they
can ? without any modification ? effectively learn
a reasonable model for non-configurational lan-
guages as well.
We also conducted a systematic and compar-
ative error analysis of the system?s outputs for
Hungarian and English. This analysis highlights
the challenges of parsing Hungarian and sug-
gests that the further improvement of parsers re-
quires special handling of language-specific phe-
nomena. We believe that some of our findings
can be relevant for intermediate languages on the
configurational-non-configurational spectrum.
2 Chief Characteristics of the
Hungarian Morphosyntax
Hungarian is an agglutinative language, which
means that a word can have hundreds of word
forms due to inflectional or derivational affixa-
tion. A lot of grammatical information is encoded
in morphology and Hungarian is a stereotype of
morphologically rich languages. The Hungarian
word order is free in the sense that the positions
of the subject, the object and the verb are not fixed
within the sentence, but word order is related to
information structure, e.g. new (or emphatic) in-
formation (the focus) always precedes the verb
55
and old information (the topic) precedes the focus
position. Thus, the position relative to the verb
has no predictive force as regards the syntactic
function of the given argument: while in English,
the noun phrase before the verb is most typically
the subject, in Hungarian, it is the focus of the
sentence, which itself can be the subject, object
or any other argument (E?. Kiss, 2002).
The grammatical function of words is deter-
mined by case suffixes as in gyerek ?child? ? gye-
reknek (child-DAT) ?for (a/the) child?. Hungarian
nouns can have about 20 cases1 which mark the
relationship between the head and its arguments
and adjuncts. Although there are postpositions
in Hungarian, case suffixes can also express re-
lations that are expressed by prepositions in En-
glish.
Verbs are inflected for person and number and
the definiteness of the object. Since conjugational
information is sufficient to deduce the pronominal
subject or object, they are typically omitted from
the sentence: Va?rlak (wait-1SG2OBJ) ?I am wait-
ing for you?. This pro-drop feature of Hungar-
ian leads to the fact that there are several clauses
without an overt subject or object.
Another peculiarity of Hungarian is that the
third person singular present tense indicative form
of the copula is phonologically empty, i.e. there
are apparently verbless sentences in Hungarian:
A ha?z nagy (the house big) ?The house is big?.
However, in other tenses or moods, the copula
is present as in A ha?z nagy lesz (the house big
will.be) ?The house will be big?.
There are two possessive constructions in
Hungarian. First, the possessive relation is only
marked on the possessed noun (in contrast, it is
marked only on the possessor in English): a fiu?
kutya?ja (the boy dog-POSS) ?the boy?s dog?. Sec-
ond, both the possessor and the possessed bear a
possessive marker: a fiu?nak a kutya?ja (the boy-
DAT the dog-POSS) ?the boy?s dog?. In the latter
case, the possessor and the possessed may not be
adjacent within the sentence as in A fiu?nak la?tta a
kutya?ja?t (the boy-DAT see-PAST3SGOBJ the dog-
POSS-ACC) ?He saw the boy?s dog?, which results
in a non-projective syntactic tree. Note that in
the first case, the form of the possessor coincides
1Hungarian grammars and morphological coding sys-
tems do not agree on the exact number of cases, some rare
suffixes are treated as derivational suffixes in one grammar
and as case suffixes in others; see e.g. Farkas et al(2010).
with that of a nominative noun while in the second
case, it coincides with a dative noun.
According to these facts, a Hungarian parser
must rely much more on morphological analysis
than e.g. an English one since in Hungarian it
is morphemes that mostly encode morphosyntac-
tic information. One of the consequences of this
is that Hungarian sentences are shorter in terms
of word numbers than English ones. Based on
the word counts of the Hungarian?English paral-
lel corpus Hunglish (Varga et al 2005), an En-
glish sentence contains 20.5% more words than its
Hungarian equivalent. These extra words in En-
glish are most frequently prepositions, pronomi-
nal subjects or objects, whose parent and depen-
dency label are relatively easy to identify (com-
pared to other word classes). This train of thought
indicates that the cross-lingual comparison of fi-
nal parser scores should be conducted very care-
fully.
3 Related work
We decided to focus on dependency parsing in
this study as it is a superior framework for non-
configurational languages. It has gained inter-
est in natural language processing recently be-
cause the representation itself does not require
the words inside of constituents to be consecu-
tive and it naturally represent discontinuous con-
structions, which are frequent in languages where
grammatical relations are often signaled by mor-
phology instead of word order (McDonald and
Nivre, 2011). The two main efficient approaches
for dependency parsing are the graph-based and
the transition-based parsers. The graph-based
models look for the highest scoring directed span-
ning tree in the complete graph whose nodes are
the words of the sentence in question. They solve
the machine learning problem of finding the opti-
mal scoring function of subgraphs (Eisner, 1996;
McDonald et al 2005). The transition-based ap-
proaches parse a sentence in a single left-to-right
pass over the words. The next transition in these
systems is predicted by a classifier that is based
on history-related features (Kudo and Matsumoto,
2002; Nivre et al 2004).
Although the available treebanks for Hungar-
ian are relatively big (82K sentences) and fully
manually annotated, the studies on parsing Hun-
garian are rather limited. The Szeged (Con-
stituency) Treebank (Csendes et al 2005) con-
56
sists of six domains ? namely, short business
news, newspaper, law, literature, compositions
and informatics ? and it is manually annotated
for the possible alternatives of words? morpho-
logical analyses, the disambiguated analysis and
constituency trees. We are aware of only two
articles on phrase-structure parsers which were
trained and evaluated on this corpus (Barta et al
2005; Iva?n et al 2007) and there are a few studies
on hand-crafted parsers reporting results on small
own corpora (Babarczy et al 2005; Pro?sze?ky et
al., 2004).
The Szeged Dependency Treebank (Vincze et
al., 2010) was constructed by first automatically
converting the phrase-structure trees into depen-
dency trees, then each of them was manually
investigated and corrected. We note that the
dependency treebank contains more information
than the constituency one as linguistic phenom-
ena (like discontinuous structures) were not anno-
tated in the former corpus, but were added to the
dependency treebank. To the best of our knowl-
edge no parser results have been published on this
corpus. Both corpora are available at www.inf.
u-szeged.hu/rgai/SzegedTreebank.
The multilingual track of the CoNLL-2007
Shared Task (Nivre et al 2007) addressed also
the task of dependency parsing of Hungarian. The
Hungarian corpus used for the shared task con-
sists of automatically converted dependency trees
from the Szeged Constituency Treebank. Several
issues of the automatic conversion tool were re-
considered before the manual annotation of the
Szeged Dependency Treebank was launched and
the annotation guidelines contained instructions
related to linguistic phenomena which could not
be converted from the constituency representa-
tion ? for a detailed discussion, see Vincze et al
(2010). Hence the annotation schemata of the
CoNLL-2007 Hungarian corpus and the Szeged
Dependency Treebank are rather different and the
final scores reported for the former are not di-
rectly comparable with our reported scores here
(see Section 5).
4 The Szeged Dependency Treebank
We utilize the Szeged Dependency Treebank
(Vincze et al 2010) as the basis of our experi-
ments for Hungarian dependency parsing. It con-
tains 82,000 sentences, 1.2 million words and
250,000 punctuation marks from six domains.
The annotation employs 16 coarse grained POS
tags, 95 morphological feature values and 29 de-
pendency labels. 19.6% of the sentences in the
corpus contain non-projective edges and 1.8% of
the edges are non-projective2, which is almost 5
times more frequent than in English and is the
same as the Czech non-projectivity level (Buch-
holz and Marsi, 2006). Here we discuss two an-
notation principles along with our modifications
in the dataset for this study which strongly influ-
ence the parsers? accuracies.
Named Entities (NEs) were treated as one to-
ken in the Szeged Dependency Treebank. Assum-
ing a perfect phrase recogniser on the whitespace
tokenised input for them is quite unrealistic. Thus
we decided to split them into tokens for this study.
The new tokens automatically got a proper noun
with default morphological features morphologi-
cal analysis except for the last token ? the head of
the phrase ?, which inherited the morphological
analysis of the original multiword unit (which can
contain various grammatical information). This
resulted in an N N N N POS sequence for Kova?cs
e?s ta?rsa kft. ?Smith and Co. Ltd.? which would
be annotated as N C N N in the Penn Treebank.
Moreover, we did not annotate any internal struc-
ture of Named Entities. We consider the last word
of multiword named entities as the head because
of morphological reasons (the last word of multi-
word units gets inflected in Hungarian) and all the
previous elements are attached to the succeeding
word, i.e. the penultimate word is attached to the
last word, the antepenultimate word to the penulti-
mate one etc. The reasons for these considerations
are that we believe that there are no downstream
applications which can exploit the information of
the internal structures of Named Entities and we
imagine a pipeline where a Named Entity Recog-
niser precedes the parsing step.
Empty copula: In the verbless clauses (pred-
icative nouns or adjectives) the Szeged Depen-
dency Treebank introduces virtual nodes (16,000
items in the corpus). This solution means that
a similar tree structure is ascribed to the same
sentence in the present third person singular and
all the other tenses / persons. A further argu-
ment for the use of a virtual node is that the vir-
tual node is always present at the syntactic level
2Using the transitive closure definition of Nivre and Nils-
son (2005).
57
corpus Malt MST Mate
ULA LAS ULA LAS ULA LAS
Hungarian
dev 88.3 (89.9) 85.7 (87.9) 86.9 (88.5) 80.9 (82.9) 89.7 (91.1) 86.8 (89.0)
test 88.7 (90.2) 86.1 (88.2) 87.5 (89.0) 81.6 (83.5) 90.1 (91.5) 87.2 (89.4)
English
dev 87.8 (89.1) 84.5 (86.1) 89.4 (91.2) 86.1 (87.7) 91.6 (92.7) 88.5 (90.0)
test 88.8 (89.9) 86.2 (87.6) 90.7 (91.8) 87.7 (89.2) 92.6 (93.4) 90.3 (91.5)
Table 1: Results achieved by the three parsers on the (full) Hungarian (Szeged Dependency Treebank) and
English (CoNLL-2009) datasets. The scores in brackets are achieved with gold-standard POS tagging.
since it is overt in all the other forms, tenses and
moods of the verb. Still, the state-of-the-art de-
pendency parsers cannot handle virtual nodes. For
this study, we followed the solution of the Prague
Dependency Treebank (Hajic? et al 2000) and vir-
tual nodes were removed from the gold standard
annotation and all of their dependents were at-
tached to the head of the original virtual node and
they were given a dedicated edge label (Exd).
Dataset splits: We formed training, develop-
ment and test sets from the corpus where each
set consists of texts from each of the domains.
We paid attention to the issue that a document
should not be separated into different datasets be-
cause it could result in a situation where a part of
the test document was seen in the training dataset
(which is unrealistic because of unknown words,
style and frequently used grammatical structures).
As the fiction subcorpus consists of three books
and the law subcorpus consists of two rules, we
took half of one of the documents for the test
and development sets and used the other part(s)
for training there. This principle was followed at
our cross-fold-validation experiments as well ex-
cept for the law subcorpus. We applied 3 folds for
cross-validation for the fiction subcorpus, other-
wise we used 10 folds (splitting at documentary
boundaries would yield a training fold consisting
of just 3000 sentences).3
5 Experiments
We carried out experiments using three state-of-
the-art parsers on the Szeged Dependency Tree-
bank (Vincze et al 2010) and on the English
datasets of the CoNLL-2009 Shared Task (Hajic?
et al 2009).
3Both the training/development/test and the cross-
validation splits are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Tools: We employed a finite state automata-
based morphological analyser constructed from
the morphdb.hu lexical resource (Tro?n et al
2006) and we used the MSD-style morphological
code system of the Szeged TreeBank (Alexin et
al., 2003). The output of the morphological anal-
yser is a set of possible lemma?morphological
analysis pairs. This set of possible morphologi-
cal analyses for a word form is then used as pos-
sible alternatives ? instead of open and closed tag
sets ? in a standard sequential POS tagger. Here,
we applied the Conditional Random Fields-based
Stanford POS tagger (Toutanova et al 2003) and
carried out 5-fold-cross POS training/tagging in-
side the subcorpora.4 For the English experiments
we used the predicted POS tags provided for the
CoNLL-2009 shared task (Hajic? et al 2009).
As the dependency parser we employed three
state-of-the-art data-driven parsers, a transition-
based parser (Malt) and two graph-based parsers
(MST and Mate parsers). The Malt parser (Nivre
et al 2004) is a transition-based system, which
uses an arc-eager system along with support vec-
tor machines to learn the scoring function for tran-
sitions and which uses greedy, deterministic one-
best search at parsing time. As one of the graph-
based parsers, we employed the MST parser (Mc-
Donald et al 2005) with a second-order feature
decoder. It uses an approximate exhaustive search
for unlabeled parsing, then a separate arc label
classifier is applied to label each arc. The Mate
parser (Bohnet, 2010) is an efficient second or-
der dependency parser that models the interaction
between siblings as well as grandchildren (Car-
reras, 2007). Its decoder works on labeled edges,
i.e. it uses a single-step approach for obtaining
labeled dependency trees. Mate uses a rich and
4The JAVA implementation of the morphological anal-
yser and the slightly modified POS tagger along with trained
models are available at http://www.inf.u-szeged.
hu/rgai/magyarlanc.
58
corpus #sent. length CPOS DPOS ULA all ULA LAS all LAS
newspaper 9189 21.6 97.2 96.5 88.0 (90.0) +0.8 84.7 (87.5) +1.0
short business 8616 23.6 98.0 97.7 93.8 (94.8) +0.3 91.9 (93.4) +0.4
fiction 9279 12.6 96.9 95.8 87.7 (89.4) -0.5 83.7 (86.2) -0.3
law 8347 27.3 98.3 98.1 90.6 (90.7) +0.2 88.9 (89.0) +0.2
computer 8653 21.9 96.4 95.8 91.3 (92.8) -1.2 88.9 (91.2) -1.6
composition 22248 13.7 96.7 95.6 92.7 (93.9) +0.3 88.9 (91.0) +0.3
Table 2: Domain results achieved by the Mate parser in cross-validation settings. The scores in brackets are
achieved with gold-standard POS tagging. The ?all? columns contain the added value of extending the training
sets with each of the five out-domain subcorpora.
well-engineered feature set and it is enhanced by
a Hash Kernel, which leads to higher accuracy.
Evaluation metrics: We apply the Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (ULA), taking into account punctuation as
well for evaluating dependency parsers and the
accuracy on the main POS tags (CPOS) and a
fine-grained morphological accuracy (DPOS) for
evaluating the POS tagger. In the latter, the analy-
sis is regarded as correct if the main POS tag and
each of the morphological features of the token in
question are correct.
Results: Table 1 shows the results got by the
parsers on the whole Hungarian corpora and on
the English datasets. The most important point
is that scores are not different from the English
scores (although they are not directly compara-
ble). To understand the reasons for this, we man-
ually investigated the set of firing features with
the highest weights in the Mate parser. Although
the assessment of individual feature contributions
to a particular decoder decision is not straightfor-
ward, we observed that features encoding config-
urational information (i.e. the direction or length
of an edge, the words or POS tag sequences/sets
between the governor and the dependent) were
frequently among the highest weighted features
in English but were extremely rare in Hungarian.
For instance, one of the top weighted features for
a subject dependency in English was the ?there is
no word between the head and the dependent? fea-
ture while this never occurred among the top fea-
tures in Hungarian.
As a control experiment, we trained the Mate
parser only having access to the gold-standard
POS tag sequences of the sentences, i.e. we
switched off the lexicalization and detailed mor-
phological information. The goal of this experi-
ment was to gain an insight into the performance
of the parsers which can only access configura-
tional information. These parsers achieved worse
results than the full parsers by 6.8 ULA, 20.3 LAS
and 2.9 ULA, 6.4 LAS on the development sets
of Hungarian and English, respectively. As ex-
pected, Hungarian suffers much more when the
parser has to learn from configurational informa-
tion only, especially when grammatical functions
have to be predicted (LAS). Despite this, the re-
sults of Table 1 show that the parsers can practi-
cally eliminate this gap by learning from morpho-
logical features (and lexicalization). This means
that the data-driven parsers employing a very rich
feature set can learn a model which effectively
captures the dependency structures using feature
weights which are radically different from the
ones used for English.
Another cause of the relatively high scores is
that the CPOS accuracy scores on Hungarian
and English are almost equal: 97.2 and 97.3, re-
spectively. This also explains the small differ-
ence between the results got by gold-standard and
predicted POS tags. Moreover, the parser can
also exploit the morphological features as input
in Hungarian.
The Mate parser outperformed the other two
parsers on each of the four datasets. Comparing
the two graph-based parsers Mate and MST, the
gap between them was twice as big in LAS than in
ULA in Hungarian, which demonstrates that the
one-step approach looking for the maximum
labeled spanning tree is more suitable for Hun-
garian than the two-step arc labeling approach of
MST. This probably holds for other morpholog-
ically rich languages too as the decoder can ex-
ploit information from the labels of decoded arcs.
Based on these results, we decided to use only
Mate for our further experiments.
59
Table 2 provides an insight into the effect of
domain differences on POS tagging and pars-
ing scores. There is a noticeable difference be-
tween the ?newspaper? and the ?short business
news? corpora. Although these domains seem to
be close to each other at the first glance (both are
news), they have different characteristics. On the
one hand, short business news is a very narrow
domain consisting of 2-3 sentence long financial
short reports. It frequently uses the same gram-
matical structures (like ?Stock indexes rose X per-
cent at the Y Stock on Wednesday?) and the lexi-
con is also limited. On the other hand, the news-
paper subcorpus consists of full journal articles
covering various domains and it has a fancy jour-
nalist style.
The effect of extending the training dataset with
out-of-domain parses is not convincing. In spite
of the ten times bigger training datasets, there
are two subcorpora where they just harmed the
parser, and the improvement on other subcorpora
is less than 1 percent. This demonstrates well the
domain-dependence of parsing.
The parser and the POS tagger react to do-
main difficulties in a similar way, according to
the first four rows of Table 2. This observation
holds for the scores of the parsers working with
gold-standard POS tags, which suggests that do-
main difficulties harm POS tagging and parsing as
well. Regarding the two last subcorpora, the com-
positions consist of very short and usually simple
sentences and the training corpora are twice as big
compared with other subcorpora. Both factors are
probably the reasons for the good parsing perfor-
mance. In the computer corpus, there are many
English terms which are manually tagged with an
?unknown? tag. They could not be accurately pre-
dicted by the POS tagger but the parser could pre-
dict their syntactic role.
Table 2 also tells us that the difference between
CPOS and DPOS is usually less than 1 percent.
This experimentally supports that the ambigu-
ity among alternative morphological analyses
is mostly present at the POS-level and the mor-
phological features are efficiently identified by
our morphological analyser. The most frequent
morphological features which cannot be disam-
biguated at the word level are related to suffixes
with multiple functions or the word itself cannot
be unambiguously segmented into morphemes.
Although the number of such ambiguous cases is
low, they form important features for the parser,
thus we will focus on the more accurate handling
of these cases in future work.
Comparison to CoNLL-2007 results: The
best performing participant of the CoNLL-2007
Shared Task (Nivre et al 2007) achieved an ULA
of 83.6 and LAS of 80.3 (Hall et al 2007) on
the Hungarian corpus. The difference between the
top performing English and Hungarian systems
were 8.14 ULA and 9.3 LAS. The results reported
in 2007 were significantly lower and the gap be-
tween English and Hungarian is higher than our
current values. To locate the sources of difference
we carried out other experiments with Mate on
the CoNLL-2007 dataset using the gold-standard
POS tags (the shared task used gold-standard POS
tags for evaluation).
First we trained and evaluated Mate on the
original CoNLL-2007 datasets, where it achieved
ULA 84.3 and LAS 80.0. Then we used the sen-
tences of the CoNLL-2007 datasets but with the
new, manual annotation. Here, Mate achieved
ULA 88.6 and LAS 85.5, which means that the
modified annotation schema and the less erro-
neous/noisy annotation caused an improvement of
ULA 4.3 and LAS 5.5. The annotation schema
changed a lot: coordination had to be corrected
manually since it is treated differently after con-
version, moreover, the internal structure of ad-
jectival/participial phrases was not marked in the
original constituency treebank, so it was also
added manually (Vincze et al 2010). The im-
provement in the labeled attachment score is prob-
ably due to the reduction of the label set (from 49
to 29 labels), which step was justified by the fact
that some morphosyntactic information was dou-
bly coded in the case of nouns (e.g. ha?zzal (house-
INS) ?with the/a house?) in the original CoNLL-
2007 dataset ? first, by their morphological case
(Cas=ins) and second, by their dependency label
(INS).
Lastly, as the CoNLL-2007 sentences came
from the newspaper subcorpus, we can compare
these scores with the ULA 90.0 and LAS 87.5
of Table 2. The ULA 1.5 and LAS 2.0 differ-
ences are the result of the bigger training corpus
(9189 sentences on average compared to 6390 in
the CoNLL-2007 dataset).
60
Hungarian English
label attachment label attachment
virtual nodes 31.5% 39.5% multiword NEs 15.2% 17.6%
conjunctions and negation ? 11.2% PP-attachment ? 15.9%
noun attachment ? 9.6% non-canonical word order 6.4% 6.5%
more than 1 premodifier ? 5.1% misplaced clause ? 9.7%
coordination 13.5% 16.5% coordination 8.5% 12.5%
mislabeled adverb 16.3% ? mislabeled adverb 40.1% ?
annotation errors 10.7% 6.8% annotation errors 9.7% 8.5%
other 28.0% 11.3% other 20.1% 29.3%
TOTAL 100% 100% TOTAL 100% 100%
Table 3: The most frequent corpus-specific and general attachment and labeling error categories (based on a
manual investigation of 200?200 erroneous sentences).
6 A Systematic Error Analysis
In order to discover specialties and challenges of
Hungarian dependency parsing, we conducted an
error analysis of parsed texts from the newspaper
domain both in English and Hungarian. 200 ran-
domly selected erroneous sentences from the out-
put of Mate were investigated in both languages
and we categorized the errors on the basis of the
linguistic phenomenon responsible for the errors
? for instance, when an error occurred because of
the incorrect identification of a multiword Named
Entity containing a conjunction, we treated it as
a Named Entity error instead of a conjunction er-
ror ?, i.e. our goal was to reveal the real linguistic
sources of errors rather than deducing from auto-
matically countable attachment/labeling statistics.
We used the parses based on gold-standard
POS tagging for this analysis as our goal was to
identify the challenges of parsing independently
of the challenges of POS tagging. The error cate-
gories are summarized in Table 3 along with their
relative contribution to attachment and labeling
errors. This table contains the categories with
over 5% relative frequency.5
The 200 sentences contained 429/319 and
353/330 attachment/labeling errors in Hungarian
and English, respectively. In Hungarian, attach-
ment errors outnumber label errors to a great ex-
tent whereas in English, their distribution is basi-
cally the same. This might be attributed to the
higher level of non-projectivity (see Section 4)
and to the more fine-grained label set of the En-
glish dataset (36 against 29 labels in English and
5The full tables are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Hungarian, respectively).
Virtual nodes: In Hungarian, the most common
source of parsing errors was virtual nodes. As
there are quite a lot of verbless clauses in Hungar-
ian (see Section 2 on sentences without copula), it
might be difficult to figure out the proper depen-
dency relations within the sentence, since the verb
plays the central role in the sentence, cf. Tesnie`re
(1959). Our parser was not efficient in identify-
ing the structure of such sentences, probably due
to the lack of information for data-driven parsers
(each edge is labeled as Exd while they have sim-
ilar features to ordinary edges). We also note that
the output of the current system with Exd labels
does not contain too much information for down-
stream applications of parsing. The appropriate
handling of virtual nodes is an important direction
for future work.
Noun attachment: In Hungarian, the nomi-
nal arguments of infinitives and participles were
frequently erroneously attached to the main
verb. Take the following sentence: A Horn-
kabinet ideje?n jo?l beva?lt mo?dszerhez pro?ba?lnak
meg visszate?rni (the Horn-government time-
3SGPOSS-SUP well tried method-ALL try-3PL
PREVERB return-INF) ?They are trying to return
to the well-tried method of the Horn government?.
In this sentence, a Horn-kabinet ideje?n ?during
the Horn government? is a modifier of the past
participle beva?lt ?well-tried?, however, it is at-
tached to the main verb pro?ba?lnak ?they are try-
ing? by the parser. Moreover, mo?dszerhez ?to the
method? is an argument of the infinitive visszate?r-
ni ?to return?, but the parser links it to the main
61
verb. In free word order languages, the order of
the arguments of the infinitive and the main verb
may get mixed, which is called scrambling (Ross,
1986). This is not a common source of error in
English as arguments cannot scramble.
Article attachment: In Hungarian, if there is
an article before a prenominal modifier, it can be-
long to the head noun and to the modifier as well.
In a szoba ajtaja (the room door-3SGPOSS) ?the
door of the room? the article belongs to the modi-
fier but when the prenominal modifier cannot have
an article (e.g. a februa?rban indulo? projekt (the
February-INE starting project) ?the project start-
ing in February?), it is attached to the head noun
(i.e. to projekt ?project?). It was not always clear
for the parser which parent to select for the arti-
cle. In contrast, these cases are not problematic
in English since the modifier typically follows the
head and thus each article precedes its head noun.
Conjunctions or negation words ? most typ-
ically the words is ?too?, csak ?only/just? and
nem/sem ?not? ? were much more frequently at-
tached to the wrong node in Hungarian than in
English. In Hungarian, they are ambiguous be-
tween being adverbs and conjunctions and it is
mostly their conjunctive uses which are problem-
atic from the viewpoint of parsing. On the other
hand, these words have an important role in mark-
ing the information structure of the sentence: they
are usually attached to the element in focus posi-
tion, and if there is no focus, they are attached
to the verb. However, sentences with or with-
out focus can have similar word order but their
stress pattern is different. Dependency parsers
obviously cannot recognize stress patterns, hence
conjunctions and negation words are sometimes
erroneously attached to the verb in Hungarian.
English sentences with non-canonical word
order (e.g. questions) were often incorrectly
parsed, e.g. the noun following the main verb is
the object in sentences like Replied a salesman:
?Exactly.?, where it is the subject that follows the
verb for stylistic reasons. However, in Hungarian,
morphological information is of help in such sen-
tences, as it is not the position relative to the verb
but the case suffix that determines the grammati-
cal role of the noun.
In English, high or low PP-attachment was
responsible for many parsing ambiguities: most
typically, the prepositional complement which
follows the head was attached to the verb instead
of the noun or vice versa. In contrast, Hungarian
is a head-after-dependent language, which means
that dependents most often occur before the head.
Furthermore, there are no prepositions in Hungar-
ian, and grammatical relations encoded by prepo-
sitions in English are conveyed by suffixes or
postpositions. Thus, if there is a modifier before
the nominal head, it requires the presence of a
participle as in: Felvette a kirakatban levo? ruha?t
(take.on-PAST3SGOBJ the shop.window-INE be-
ing dress-ACC) ?She put on the dress in the shop
window?. The English sentence is ambiguous (ei-
ther the event happens in the shop window or the
dress was originally in the shop window) while
the Hungarian has only the latter meaning.6
General dependency parsing difficulties:
There were certain structures that led to typical
label and/or attachment errors in both languages.
The most frequent one among them is coordi-
nation. However, it should be mentioned that
syntactic ambiguities are often problematic even
for humans to disambiguate without contextual
or background semantic knowledge.
In the case of label errors, the relation between
the given node and its parent was labeled incor-
rectly. In both English and Hungarian, one of the
most common errors of this type was mislabeled
adverbs and adverbial phrases, e.g. locative ad-
verbs were labeled as ADV/MODE. However, the
frequency rate of this error type is much higher
in English than in Hungarian, which may be re-
lated to the fact that in the English corpus, there
is a much more balanced distribution of adverbial
labels than in the Hungarian one (where the cat-
egories MODE and TLOCY are responsible for
90% of the occurrences). Assigning the most fre-
quent label of the training dataset to each adverb
yields an accuracy of 82% in English and 93% in
Hungarian, which suggests that there is a higher
level of ambiguity for English adverbial phrases.
For instance, the preposition by may introduce an
adverbial modifier of manner (MNR) in by cre-
ating a bill and the agent in a passive sentence
(LGS). Thus, labeling adverbs seems to be a more
6However, there exists a head-before-dependent version
of the sentence (Felvette a ruha?t a kirakatban), whose pre-
ferred reading is ?She was in the shop window while dressing
up?, that is, the modifier belongs to the verb.
62
difficult task in English.7
Clauses were also often mislabeled in both lan-
guages, most typically when there was no overt
conjunction between clauses. Another source of
error was when more than one modifier occurred
before a noun (5.1% and 4.2% of attachment er-
rors in Hungarian and in English): in these cases,
the first modifier could belong to the noun (a
brown Japanese car) or to the second modifier (a
brown haired girl).
Multiword Named Entities: As we mentioned
in Section 4, members of multiword Named Enti-
ties had a proper noun POS-tag and an NE label
in our dataset. Hence when parsing is based on
gold standard POS-tags, their recognition is al-
most perfect while it is a frequent source or er-
rors in the CoNLL-2009 corpus. We investigated
the parse of our 200 sentences with predicted POS
tags at NEs and found that this introduces several
errors (about 5% of both attachment and labeling
errors) in Hungarian. On the other hand, the re-
sults are only slightly worse in English, i.e. iden-
tifying the inner structure of NEs does not depend
on whether the parser builds on gold standard or
predicted POS-tags since function words like con-
junctions or prepositions ? which mark grammat-
ical relations ? are tagged in the same way in both
cases. The relative frequency of this error type is
much higher in English even when the Hungar-
ian parser does not have access to the gold proper
noun POS tags. The reason for this is simple: in
the Penn Treebank the correct internal structure of
the NEs has to be identified beyond the ?phrase
boundaries? while in Hungarian their members
just form a chain.
Annotation errors: We note that our analysis
took into account only sentences which contained
at least one parsing error and we crawled only
the dependencies where the gold standard anno-
tation and the output of the parser did not match.
Hence, the frequency of annotation errors is prob-
ably higher than we found (about 1% of the en-
tire set of dependencies) during our investigation
as there could be annotation errors in the ?error-
free? sentences and also in the investigated sen-
tences where the parser agrees with that error.
7We would nevertheless like to point out that adverbial
labels have a highly semantic nature, i.e. it could be argued
that it is not the syntactic parser that should identify them but
a semantic processor.
7 Conclusions
We showed that state-of-the-art dependency
parsers achieve similar results ? in terms of at-
tachment scores ? on Hungarian and English. Al-
though the results with this comparison should be
taken with a pinch of salt ? as sentence lengths
(and information encoded in single words) differ,
domain differences and annotation schema diver-
gences are uncatchable ? we conclude that parsing
Hungarian is just as hard a task as parsing English.
We argued that this is due to the relatively good
POS tagging accuracy (which is a consequence
of the low ambiguity of alternative morphological
analyses of a sentence and the good coverage of
the morphological analyser) and the fact that data-
driven dependency parsers employ a rich feature
representation which enables them to learn differ-
ent kinds of feature weight profiles.
We also discussed the domain differences
among the subcorpora of the Szeged Dependency
Treebank and their effect on parsing results. Our
results support that there can be higher differences
in parsing scores among domains in one language
than among corpora from a similar domain but
different languages (which again marks pitfalls of
inter-language comparison of parsing scores).
Our systematic error analysis showed that han-
dling the virtual nodes (mostly empty copula) is
a frequent source of errors. We identified several
phenomena which are not typically listed as Hun-
garian syntax-specific features but are challeng-
ing for the current data-driven parsers, however,
they are not problematic in English (like the at-
tachment of conjunctions and negation words and
the attachment problem of nouns and articles).
We concluded ? based on our quantitative analy-
sis ? that a further notable error reduction is only
achievable if distinctive attention is paid to these
language-specific phenomena.
We intend to investigate the problem of vir-
tual nodes in dependency parsing in more depth
and to implement new feature templates for the
Hungarian-specific challenges as future work.
Acknowledgments
This work was supported in part by the Deutsche
Forschungsgemeinschaft grant SFB 732 and the
NIH grant (project codename MASZEKER) of
the Hungarian government.
63
References
Zolta?n Alexin, Ja?nos Csirik, Tibor Gyimo?thy, Ka?roly
Bibok, Csaba Hatvani, Ga?bor Pro?sze?ky, and La?szlo?
Tihanyi. 2003. Annotated Hungarian National Cor-
pus. In Proceedings of the EACL, pages 53?56.
Anna Babarczy, Ba?lint Ga?bor, Ga?bor Hamp, and
Andra?s Rung. 2005. Hunpars: a rule-based sen-
tence parser for Hungarian. In Proceedings of the
6th International Symposium on Computational In-
telligence.
Csongor Barta, Do?ra Csendes, Ja?nos Csirik, Andra?s
Ho?cza, Andra?s Kocsor, and Korne?l Kova?cs. 2005.
Learning syntactic tree patterns from a balanced
Hungarian natural language database, the Szeged
Treebank. In Proceedings of 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering, pages 225 ? 231.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged Treebank. In
TSD, pages 123?131.
Katalin E?. Kiss. 2002. The Syntax of Hungarian.
Cambridge University Press, Cambridge.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, COLING ?96, pages 340?
345.
Richa?rd Farkas, Da?niel Szeredi, Da?niel Varga, and
Veronika Vincze. 2010. MSD-KR harmoniza?cio? a
Szeged Treebank 2.5-ben [Harmonizing MSD and
KR codes in the Szeged Treebank 2.5]. In VII. Ma-
gyar Sza?m??to?ge?pes Nyelve?szeti Konferencia, pages
349?353.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Anne Abeille?, editor, Treebanks: Building and
Using Parsed Corpora, pages 103?127. Amster-
dam:Kluwer.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, pages 1?18.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single Malt or Blended?
A Study in Multilingual Parser Optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 933?939.
Szila?rd Iva?n, Ro?bert Orma?ndi, and Andra?s Kocsor.
2007. Magyar mondatok SVM alapu? szintaxis
elemze?se [SVM-based syntactic parsing of Hun-
garian sentences]. In V. Magyar Sza?m??to?ge?pes
Nyelve?szeti Konferencia, pages 281?283.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning - Volume 20, COLING-02, pages
1?7.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37:197?230.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 99?
106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learning
(CoNLL-2004), pages 49?56.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Ga?bor Pro?sze?ky, La?szlo? Tihanyi, and Ga?bor L. Ugray.
2004. Moose: A Robust High-Performance Parser
and Generator. In Proceedings of the 9th Workshop
of the European Association for Machine Transla-
tion.
John R. Ross. 1986. Infinite syntax! ABLEX, Nor-
wood, NJ.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe struc-
turale. Klincksieck, Paris.
64
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, pages 173?180.
Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s
Rung, Eszter Simon, and Pe?ter Vajda. 2006. Mor-
phdb.hu: Hungarian lexical database and morpho-
logical grammar. In Proceedings of 5th Inter-
national Conference on Language Resources and
Evaluation (LREC ?06).
Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, and Viktor Tro?n. 2005. Par-
allel corpora for medium density languages. In Pro-
ceedings of the RANLP, pages 590?596.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian Dependency Treebank. In Proceedings of the
Seventh Conference on International Language Re-
sources and Evaluation (LREC?10).
65
Cross-Genre and Cross-Domain Detection of
Semantic Uncertainty
Gyo?rgy Szarvas?
Technische Universita?t Darmstadt
Veronika Vincze??
Hungarian Academy of Sciences
Richa?rd Farkas?
Universita?t Stuttgart
Gyo?rgy Mo?ra?
University of Szeged
Iryna Gurevych?
Technische Universita?t Darmstadt
Uncertainty is an important linguistic phenomenon that is relevant in various Natural
Language Processing applications, in diverse genres from medical to community generated,
newswire or scientific discourse, and domains from science to humanities. The semantic un-
certainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical
cues) and the key steps of uncertainty detection in an application include the steps of locating
the (genre- and domain-specific) lexical cues, disambiguating them, and linking them with the
units of interest for the particular application (e.g., identified events in information extraction).
In this study, we focus on the genre and domain differences of the context-dependent semantic
uncertainty cue recognition task.
We introduce a unified subcategorization of semantic uncertainty as different domain appli-
cations can apply different uncertainty categories. Based on this categorization, we normalized
the annotation of three corpora and present results with a state-of-the-art uncertainty cue
recognition model for four fine-grained categories of semantic uncertainty.
? Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt - FB 20
Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: {szarvas,gurevych}@tk.informatik
.tu-darmstadt.de.
?? Hungarian Academy of Sciences, Research Group on Artificial Intelligence, Tisza Lajos krt. 103,
6720 Szeged, Hungary. E-mail: vinczev@inf.u-szeged.hu.
? Universita?t Stuttgart, Institut fu?r Maschinelle Sprachverarbeitung. Azenbergstrasse 12, D-70174 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? University of Szeged, Department of Informatics, A?rpa?d te?r 2, 6720 Szeged, Hungary.
E-mail: gymora@inf.u-szeged.hu.
Submission received: 6 April 2011; revised submission received: 1 October 2011; accepted for publication:
30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
Our results reveal the domain and genre dependence of the problem; nevertheless, we
also show that even a distant source domain data set can contribute to the recognition
and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to
cover a new domain. Thus, the unified subcategorization and domain adaptation for training
the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty
recognition.
1. Introduction
In computational linguistics, especially in information extraction and retrieval, it is
of the utmost importance to distinguish between uncertain statements and factual
information. In most cases, what the user needs is factual information, hence uncertain
propositions should be treated in a special way: Depending on the exact task, the
system should either ignore such texts or separate them from factual information. In
machine translation, it is also necessary to identify linguistic cues of uncertainty because
the source and the target language may differ in their toolkit to express uncertainty
(one language uses an auxiliary, the other uses just a morpheme). To cite another
example, in clinical document classification, medical reports can be grouped according
to whether the patient definitely suffers, probably suffers, or does not suffer from an
illness.
There are several linguistic phenomena that are referred to as uncertainty in the
literature. We consider propositions to which no truth value can be attributed, given
the speaker?s mental state, as instances of semantic uncertainty. In contrast, uncertainty
may also arise at the discourse level, when the speaker intentionally omits some infor-
mation from the statement, making it vague, ambiguous, or misleading. Determining
whether a given proposition is uncertain or not may involve using a finite dictionary of
linguistic devices (i.e., cues). Lexical cues (such as modal verbs or adverbs) are respon-
sible for semantic uncertainty whereas discourse-level uncertainty may be expressed by
lexical cues and syntactic cues (such as passive constructions) as well. We focus on four
types of semantic uncertainty in this study and henceforth the term cuewill be taken to
mean lexical cue.
The key steps of recognizing semantically uncertain propositions in a natural
language processing (NLP) application include the steps of locating lexical cues for
uncertainty, disambiguating them (as not all occurrences of the cues indicate uncer-
tainty), and finally linking them with the textual representation of the propositions
in question. The linking of a cue to the textual representation of the proposition can
be performed on the basis of syntactic rules that depend on the word class of the
lexical cue, but they are independent of the actual application domain or text type
where the cue is observed. The set of cues used and the frequency of their certain
and uncertain usages are domain and genre dependent, however, and this has to be
addressed if we seek to craft automatic uncertainty detectors. Here we interpret genre
as the basic style and formal characteristics of the writing that is independent of its topic
(e.g., scientific papers, newswire texts, or business letters), and domain as a particular
field of knowledge and is related to the topic of the text (e.g., medicine, archeology, or
politics).
Uncertainty cue candidates do not display uncertainty in all of their occurrences.
For instance, the mathematical sense of probable is dominant in mathematical texts
whereas its ratio can be relatively low in papers in the humanities. The frequency of
the two distinct meanings of the verb evaluate (which can be a synonym of judge [an
336
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertain meaning] and calculate) is also different in the bioinformatics and cell biology
domains. Compare:
(1) To evaluateCUE the PML/RARalpha role in myelopoiesis, transgenic mice
expressing PML/RARalpha were engineered.
(2) Our method was evaluated on the Lindahl benchmark for fold recognition.
In this article we focus on the domain-dependent aspects of uncertainty detection
and we examine the recognition of uncertainty cues in context. We do not address the
problem of linking cues to propositions in detail (see, e.g., Chapman, Chu, and Dowling
[2007] and Kilicoglu and Bergler [2009] for the information extraction case).
For the empirical investigation of the domain dependent aspects, data sets are
required from various domains. To date, several corpora annotated for uncertainty have
been constructed for different genres and domains (BioScope, FactBank, WikiWeasel,
and MPQA, to name but a few). These corpora cover different aspects of uncertainty,
however, being grounded on different linguistic models, which makes it hard to exploit
cross-domain knowledge in applications. These differences in part stem from the varied
application needs across application domains. Different types of uncertainty and classes
of linguistic expressions are relevant for different domains. Although hypotheses and
investigations form a crucial part of the relevant cases in scientific applications, they
are less prominent in newswire texts, where beliefs and rumors play a major role. This
finding motivates a more fine-grained treatment of uncertainty. In order to bridge the
existing gaps between application goals, these typical cases need to be differentiated. A
fine-grained categorization enables the individual treatment of each subclass, which
is less dependent on domain differences than using one coarse-grained uncertainty
class. Moreover, this approach enables each particular application to identify and select
from a pool of models only those aspects of uncertainty that are relevant in the specific
domain.
As one of the main contributions of this study, we propose a uniform subcategoriza-
tion of semantic uncertainty in which all the previous corpus annotation works can be
placed, and which reveals the fundamental differences between the currently existing
resources. In addition, we manually harmonized the annotations of three corpora and
performed the fine-grained labeling according to the suggested subcategorization so as
to be able to perform cross-domain experiments.
An important factor in training robust cross-domain models is to focus on shallow
features that can be reliably obtained for many different domains and text types, and
to craft models that exploit the shared knowledge from different sources as much
as possible, making the adaptation to new domains efficient. The study of learning
efficient models across different domains is the subject of transfer learning and domain
adaptation research (cf. Daume? III and Marcu 2006; Pan and Yang 2010). The domain
adaptation setting assumes a target domain (for which an accurate model should be
learned with a limited amount of labeled training data), a source domain (with charac-
teristics different from the target and for which a substantial amount of labeled data is
available), and an arbitrary supervised learning model that exploits both the target and
source domain data in order to learn an improved target domain model.
The success of domain adaptation mainly depends on two factors: (i) the similarity
of the target and source domains (the two domains should be sufficiently similar to
allow knowledge transfer); and (ii) the application of an efficient domain adaptation
337
Computational Linguistics Volume 38, Number 2
method (which permits the learning algorithm to exploit the commonalities of the
domains while preserving the special characteristics of the target domain).
As our second main contribution, we study the impact of domain differences on
uncertainty detection, how this impact depends on the distance between target and
source domains concerning their domains and genres, and how these differences can
be reduced to produce accurate target domain models with limited annotation effort.
Because previously existing resources exhibited fundamental differences that made
domain adaptation difficult,1 to our knowledge this is the first study to analyze domain
differences and adaptability in the context of uncertainty detection in depth, and also
the first study to report consistently positive results in cross-training.
The main contributions of the current paper can be summarized as follows:
 We provide a uniform subcategorization of semantic uncertainty (with
definitions, examples, and test batteries for annotation) and classify all
major previous studies on uncertainty corpus annotation into the proposed
categorization system, in order to reveal and analyze the differences.
 We provide a harmonized, fine-grained reannotation of three corpora,
according to the suggested subcategorization, to allow an in-depth
analysis of the domain dependent aspects of uncertainty detection.
 We compare the two state-of-the-art approaches to uncertainty cue
detection (i.e., the one based on token classification and the one on
sequence labeling models), using a shared feature set, in the context of the
CoNLL-2010 shared task, to understand their strengths and weaknesses.2
 We train an accurate semantic uncertainty detector that distinguishes four
fine-grained categories of semantic uncertainty (epistemic, doxastic,
investigation, and condition types) and thus is better for future
applications in various domains than previous models. Our experiments
reveal that, similar to the best model of the CoNLL-2010 shared task for
biological texts but in a fine-grained context, shallow features provide
good results in recognizing semantic uncertainty. We also show that this
representation is less suited to detecting discourse-level uncertainty
(which was part of the CoNLL task for Wikipedia texts).
 We examine in detail the differences between domains and genres as
regards the language used to express semantic uncertainty, and learn how
the domain or genre distance affects uncertainty recognition in texts with
unseen characteristics.
 We apply domain adaptation techniques to fully exploit out-of-domain
data and minimize annotation costs to adapt to a new domain, and we
report successful results for various text domains and genres.
The rest of the paper is structured as follows. In Section 2, our classification of
uncertainty phenomena is presented in detail and it is compared with the concept of
1 Only 3 out of the more than 20 participants of the related CoNLL-2010 shared task (Farkas et al 2010)
managed to exploit out-of-domain data to improve their results, and only by a negligible margin.
2 The most successful CoNLL systems were based on these approaches, but different feature
representations make direct comparisons difficult.
338
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertainty used in existing corpora. A framework for detecting semantic uncertainty
is then presented in Section 3. Relatedwork on cue detection is summarized in Section 4,
which is followed by a description of our cue recognition system and a presentation of
our experimental set-up using various source and target genre and domain pairs for
cross-domain learning and domain adaptation in Section 5. Our results are elaborated
on in Section 6 with a focus on the effect of domain similarities and on the annotation
effort needed to cover a new domain. We then conclude with a summary of our results
and make some suggestions for future research.
2. The Phenomenon Uncertainty
In order to be able to introduce and discuss our data sets, experiments, and findings,
we have to clarify our understanding of the term uncertainty. Uncertainty?in its most
general sense?can be interpreted as lack of information: The receiver of the information
(i.e., the hearer or the reader) cannot be certain about some pieces of information. In this
respect, uncertainty differs from both factuality and negation; as regards the former,
the hearer/reader is sure that the information is true and as for the latter, he is sure
that the information is not true. From the viewpoint of computer science, uncertainty
emerges due to partial observability, nondeterminism, or both (Russell and Norvig
2010). Linguistic theories usually associate the notion of modality with uncertainty:
Epistemic modality encodes how much certainty or evidence a speaker has for the
proposition expressed by his utterance (Palmer 1986) or it refers to a possible state of
the world in which the given proposition holds (Kiefer 2005). The common point in
these approaches is that in the case of uncertainty, the truth value/reliability of the
proposition cannot be decided because some other piece of information is missing.
Thus, uncertain propositions are those in our understanding whose truth value or
reliability cannot be determined due to lack of information.
In the following, we focus on semantic uncertainty and we suggest a tentative
classification of several types of semantic uncertainty. Our classification is grounded on
the knowledge of existing corpora and uncertainty recognition tools and our chief goal
here is to provide a computational linguistics-oriented classification. With this in mind,
our subclasses are intended to be well-defined and easily identifiable by automatic
tools. Moreover, this classification allows different applications to choose the subset of
phenomena to be recognized in accordance with their main task (i.e., we tried to avoid
an overly coarse or fine-grained categorization).
2.1 Classification of Uncertainty Types
Several corpora annotated for uncertainty have been published in different domains
such as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven,
and Friedland 2008; Shatkay et al 2008; Vincze et al 2008; Nawaz, Thompson, and
Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin,
Liddy, and Kando 2005; Wilson 2008; Saur?? and Pustejovsky 2009; Rubin 2010), and
encyclopedia (Farkas et al 2010). As can be seen from publicly available annotation
guidelines, there are many overlaps but differences as well in the understanding of
uncertainty, which is sometimes connected to domain- and genre-specific features of
the texts. Here we introduce a domain- and genre-independent classification of several
types of semantic uncertainty, which was inspired by both theoretical and computa-
tional linguistic considerations.
339
Computational Linguistics Volume 38, Number 2
Figure 1
Types of uncertainty. FB = FactBank; Genia = Genia Event; Rubin = the data set described
in Rubin, Liddy and Noriko (2005); META = the data set described in Nawaz, Thompson
and Ananiadou (2010); Medlock = the data set described in Medlock and Briscoe (2007);
Shatkay = the data set described in Shatkay et al (2008); Settles = the data set described in
Settles et al (2008).
2.1.1 A Tentative Classification. Based on corpus data and annotation principles, the
expression uncertainty can be used as an umbrella term for covering phenomena at the
semantic and discourse levels.3 Our classification of semantic uncertainty is assumed
to be language-independent, but our examples presented here come from the English
language, to keep matters simple.
Semantically uncertain propositions can be defined in terms of truth conditional
semantics. They cannot be assigned a truth value (i.e., it cannot be stated for sure
whether they are true or false) given the speaker?s current mental state.
Semantic level uncertainty can be subcategorized into epistemic and hypothetical
(see Figure 1). The main difference between epistemic and hypothetical uncertainty is
that whereas instances of hypothetical uncertainty can be true, false or uncertain, epis-
temically uncertain propositions are definitely uncertain?in terms of possible worlds,
hypothetical propositions allow that the proposition can be false in the actual world,
but in the case of epistemic uncertainty the factuality of the proposition is not known.
In the case of epistemic uncertainty, it is known that the proposition is neither true
nor false: It describes a possible world where the proposition holds but this possible
world does not coincide with the speaker?s actual world. In other words, it is certain
that the proposition is uncertain. Epistemic uncertainty is related to epistemic modality:
a sentence is epistemically uncertain if on the basis of our world knowledge we cannot
decide at the moment whether it is true or false (hence the name) (Kiefer 2005). The
source of an epistemically uncertain proposition cannot claim the uncertain proposition
and be sure about its opposite at the same time.
(3) EPISTEMIC: Itmay be raining.
3 The entire typology of semantic uncertainty phenomena and a test battery for their classification are
described in a supplementary file. Together with the corpora and the experimental software, they are
available at http://www.inf.u-szeged.hu/rgai/uncertainty.
340
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
As for hypothetical uncertainty, the truth value of the propositions cannot be
determined either and nothing can be said about the probability of their happening.
Propositions under investigation are an example of such statements: Until further anal-
ysis, the truth value of the proposition under question cannot be stated. Conditionals
can also be classified as instances of hypotheses. It is also common in these two types of
uncertain propositions that the speaker can utter them while it is certain (for others or
even for him) that its opposite holds hence they can be called instances of paradoxical
uncertainty.
Hypothetical uncertainty is connectedwith non-epistemic types ofmodality aswell.
Doxastic modality expresses the speaker?s beliefs?which may be known to be true or
false by others in the current state of the world. Necessity (duties, obligation, orders)
is the main objective of deontic modality; dispositional modality is determined by the
dispositions (i.e., physical abilities) of the person involved; and circumstantial modality
is defined by external circumstances. Buletic modality is related to wishes, intentions,
plans, and desires. An umbrella term for deontic, dispositional, circumstantial, and
buletic modality is dynamic modality (Kiefer 2005).
HYPOTHETICAL:
(4) DYNAMIC: I have to go.
(5) DOXASTIC: He believes that the Earth is flat.
(6) INVESTIGATION: We examined the role of NF-kappa B in protein activation.
(7) CONDITION: If it rains, we?ll stay in.
Conditions and instances of dynamic modality are related to the future: In the
future, they may happen but at the moment it is not clear whether they will take place
or not / whether they are true, false, or uncertain.
2.1.2 Comparison with other Classifications. The feasibility of the classification proposed in
this study can be justified by mapping the annotation schemes used in other existing
corpora to our subcategorizations of uncertainty. This systematic comparison also high-
lights the major differences between existing works and partly explains why examples
for successful cross-domain application of existing resources and models are hard to
find in the literature.
Most of the annotations found in biomedical corpora (Medlock and Briscoe 2007;
Settles, Craven, and Friedland 2008; Shatkay et al 2008; Thompson et al 2008; Nawaz,
Thompson, and Ananiadou 2010) fall into the epistemic uncertainty class. BioScope
(Vincze et al 2008) annotations mostly belong to the epistemic uncertainty category,
with the exception of clausal hypotheses (i.e., hypotheses that are expressed by a clause
headed by if or whether), which are instances of the investigation class. The probable
class of Genia Event (Kim, Ohta, and Tsujii 2008) is of the epistemically uncertain type
and the doubtful class belongs to the investigation class. Rubin, Liddy, and Kando (2005)
consider uncertainty as a phenomenon belonging to epistemicmodality: The high, mod-
erate, and low levels of certainty coincide with our epistemic uncertainty category. The
speculation annotations of the MPQA corpus also belong to the epistemic uncertainty
class, with four levels (Wilson 2008). The probable and possible classes found in FactBank
(Saur?? and Pustejovsky 2009) are of the epistemically uncertain type, events with a
generic source belong to discourse-level uncertainty, whereas underspecified events are
341
Computational Linguistics Volume 38, Number 2
classified as hypothetical uncertainty in our system as, by definition, their truth value
cannot be determined. WikiWeasel (Farkas et al 2010) contains annotation for epistemic
uncertainty, but discourse-level uncertainty is also annotated in the corpus (see Figure 1
for an overview). The categories used for themachine reading task described inMorante
and Daelemans (2011) also overlap with our fine-grained classes: Uncertain events in
their system fall into our epistemic uncertainty class. Their modal events expressing
purpose, need, obligation, or desire are instances of dynamic modality, whereas their
conditions are understood in a similar way to our condition class. The modality types
listed in Baker et al (2010) can be classified as types of dynamic modality, except for
their belief category. Instances of the latter category are either certain (It is certain that he
met the president) or epistemic or doxastic modality in our system.
2.2 Types of Semantic Uncertainty Cues
We assume that the nature of the lexical unit determines the type of uncertainty it
represents, that is, semantic uncertainty is highly lexical in nature. The part of speech of
the uncertainty cue candidates serves as the basis for categorization, similar to the ones
found in Hyland (1994, 1996, 1998) and Rizomilioti (2006). In English, modality is often
associated with modal auxiliaries (Palmer 1979), but, as Table 1 shows, there are many
other parts of speech that can express uncertainty. It should be added that there are
cues where it depends on the context, rather than the given lexical item, what subclass
of uncertainty the cue refers to, for example, may can denote epistemic modality (It may
rain. . . ) or dynamic modality (Now you may open the door). These categories are listed in
Table 1.
3. A Framework for Detecting Semantic Uncertainty
In our model, uncertainty detection is a standalone task that is largely independent of
the underlying application. In this section, we briefly discuss how uncertainty detection
Table 1
Uncertainty cues.
Adjectives / adverbs
probable, likely, possible, unsure, possibly, perhaps, etc. epistemic
Auxiliaries
may, might, can, would, should, could, etc. semantic
Verbs
speculative: suggest, question, seem, appear, favor, etc. epistemic
psych: think, believe, etc. doxastic
analytic: investigate, analyze, examine, etc. investigation
prospective: plan, want, order, allow, etc. dynamic
Conjunctions
if, whether, etc. investigation
Nouns
nouns derived speculation, proposal, consideration, etc. same as the verb
from uncertain verb:
other rumor, idea, etc. doxastic
uncertain nouns:
342
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
can be incorporated into an information extraction task, which is probably the most
relevant application area (see Kim et al [2009] for more details). In the information
extraction context, the key steps of recognizing uncertain propositions are locating the
cues, disambiguating them (as not all occurrences of the cues indicate uncertainty; recall
the example of evaluate), and finally linking them with the textual representation of
the propositions in question. We note here that marking the textual representations of
important propositions (often referred to as events in information extraction) is actually
the main goal of an information extraction system, hence we will not focus on their
identification and just assume that they are already marked in texts.
The following is an example that demonstrates the process of uncertainty detection:
(8) In this study we hypothesizedCUE that the phosphorylation of TRAF2
inhibitsEVENT binding to the CD40 cytoplasmic domain.
Here the EVENT mark-up is produced by the information extraction system, and
uncertainty detection consists of i) the recognition of the cue word hypothesized, and
determining whether it denotes uncertainty in this specific case (producing the CUE
mark-up) and ii) determining whether the cue hypothesizedmodifies the event triggered
by inhibits or not (positive example in this case).
3.1 Uncertainty Cue Detection and Disambiguation
The cue detection and disambiguation problem can be essentially regarded as a token
labeling problem. Here the task is to assign a label to each of the tokens of a sentence
in question according to whether it is the starting token of an uncertainty cue (B-
CUE TYPE), an inside token of a cue (I-CUE TYPE), or it is not part of any cue (O). Most
previous studies assume a binary classification task, namely, each token is either part of
an uncertainty cue, or it is not a cue. For fine-grained uncertainty detection, a different
label has to be used for each uncertainty type to be distinguished. This way, the label
sequence of a sentence naturally identifies all uncertainty cues (with their types) in the
sentence, and disambiguation is solved jointly with recognition.
Because the uncertainty cue vocabulary and the distribution of certain and uncer-
tain senses of cues vary in different domains and genres, uncertainty cue detection and
disambiguation are the main focus of the current study.
3.2 Linking Uncertainty Cues to Propositions
The task of linking the detected uncertainty cues to propositions can be formulated
as a binary classification task over uncertainty cue and event marker pairs. The relation
holds and is considered true if the cuemodifies the truth value (confidence) of the event;
it does not hold and is considered false if the cue does not have any impact on the
interpretation of the event. That is, the pair (hypothesized, inhibits) in Example (8) is an
instance of positive relation.
The linking of uncertainty cues and event markers can be established by using de-
pendency grammar rules (i.e., the problem is mainly syntax driven). As the grammatical
properties of the language are similar in various domains and genres, this task is largely
domain-independent, as opposed to the recognition and disambiguation task. Because
of this, we sketch the most important matching patterns, but do not address the linking
task in great detail here.
343
Computational Linguistics Volume 38, Number 2
The following are the characteristic rules that can be used to link uncertainty cues
to event markers. For practical implementations of heuristic cue/event matching, see
Chapman, Chu, and Dowling (2007) and Kilicoglu and Bergler (2009).
 If the event clue has an uncertain verb, noun, preposition, or auxiliary as a
(not necessarily direct) parent in the dependency graph of the sentence,
the event is regarded as uncertain.
 If the event clue has an uncertain adverb or adjective as its child, it is
treated as uncertain.
4. Related Work on Uncertainty Cue Detection
Herewe review the publishedworks related to uncertainty cue detection. Earlier studies
focused either on in-domain cue recognition for a single domain or on cue lexicon
extraction from large corpora. The latter approach is applicable tomultiple domains, but
does not address the disambiguation of uncertain and other meanings of the extracted
cue words. We are also aware of several studies that discussed the differences of cue
distributions in various domains, without developing a cue detector. To the best of
our knowledge, our study is the first to address the genre- and domain-adaptability of
uncertainty cue recognition systems and thus uncertainty detection in a general context.
We should add that there are plenty of studies on end-application oriented uncer-
tainty detection, that is, how to utilize the recognized cues (see, for instance, Kilicoglu
and Bergler [2008], Uzuner, Zhang, and Sibanda [2009] and Saur?? [2008] for information
extraction or Farkas and Szarvas [2008] for document labeling applications), and a
recent pilot task sought to exploit negation and hedge cue detectors in machine reading
(Morante and Daelemans 2011). As the focus of our paper is cue recognition, however,
we omit their detailed description here.
4.1 In-Domain Cue Detection
In-domain uncertainty detectors have been developed since the mid 1990s. Most of
these systems use hand-crafted lexicons for cue recognition and they treat each oc-
currence of the lexicon items as a cue?that is, they do not address the problem of
disambiguating cues (Friedman et al 1994; Light, Qiu, and Srinivasan 2004; Farkas and
Szarvas 2008; Saur?? 2008; Conway, Doan, and Collier 2009; Van Landeghem et al 2009).
ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to define cues
and ?pseudo-triggers?. A pseudo-trigger is a superstring of a cue and it is basically
used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be
regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and
Baldwin (2009) introduced a system which also used non-consecutive tokens as cues
(like not+as+yet).
Utilizing manually labeled corpora, machine learning?based uncertainty cue de-
tectors have also been developed (to the best of our knowledge each of them uses an
in-domain training data set). They use token classification (Morante and Daelemans
2009; Clausen 2010; Fernandes, Crestana, and Milidiu? 2010; Sa?nchez, Li, and Vogel
2010) or sequence labeling approaches (Li et al 2010; Rei and Briscoe 2010; Tang et al
2010; Zhang et al 2010). In both cases the tokens are labeled according to whether
they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of
344
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
tokens) thus it naturally deals with the context of a particular word. On the other hand,
context information for a token is built into the feature space of the token classification
approaches. O?zgu?r and Radev (2009) and Velldal (2010) match cues from a lexicon then
apply a binary classifier based on features describing the context of the cue candidate.
Each of these approaches uses a rich feature representation for tokens, which usu-
ally includes surface-level, part-of-speech, and chunk-level features. A few systems
have also used dependency relation types originating at the cue (Rei and Briscoe 2010;
Sa?nchez, Li, and Vogel 2010; Velldal, ?vrelid, and Oepen 2010; Zhang et al 2010); the
CoNLL-2010 Shared Task final ranking suggests that it has only a limited impact on the
performance of an entire system (Farkas et al 2010), however. O?zgu?r and Radev (2009)
further extended the feature set with the other cues that occur in the same sentence as
the cue, and positional features such as the section header of the article in which the cue
occurs (the latter is only defined for scientific publications). Velldal (2010) argues that
the dimensionality of the uncertainty cue detection feature space is too high and reports
improvements by using the sparse random indexing technique.
Ganter and Strube (2009) proposed a rather different approach for (weasel) cue
detection?exploiting weasel tags4 in Wikipedia articles given by editors. They used
syntax-based patterns to recognize the internal structure of the cues, which has proved
useful as discourse-level uncertainty cues are usually long and have a complex internal
structure (as opposed to semantic uncertainty cues).
As can be seen, uncertainty cue detectors have mostly been developed in the bio-
logical and medical domains. All of these studies, however, focus on only one domain,
namely, in-domain cue detection is carried out, which assumes the availability of a
training data set of sufficient size. The only exceptionwe are aware of is the CoNLL-2010
Shared Task (Farkas et al 2010), where participants had the chance to use Wikipedia
data on biomedical domain and vice versa. Probably due to the differences in the
annotated uncertainty types and the stylistic and topical characteristics of the texts,
very few participants performed cross-domain experiments and reported only limited
success (see Section 5.3.2 for more on this).
Overall, the findings of these studies indicate that disambiguating cue candidates is
an important aspect of uncertainty detection and that the domain specificity of disam-
biguation models and domain adaptation in general are largely unexplored problems
in uncertainty detection.
4.2 Weakly Supervised Extraction of Cue Lexicon
Similar to our approach, several studies have addressed the problem of developing
an uncertainty detector for a new domain using as little annotation effort as possible.
The aim of these studies is to identify uncertain sentences; this is carried out by semi-
automatic construction of cue lexicons. The weakly supervised approaches start with
very small seed sets of annotated certain and uncertain sentences, and use bootstrap-
ping to induce a suitable training corpus in an automatic way. Such approaches collect
potentially certain and uncertain sentences from a large unlabeled pool based on their
similarity to the instances in the seed sets (Medlock and Briscoe 2007), or based on the
known errors of an information extraction system that is itself sensitive to uncertain
texts (Szarvas 2008). Further instances are then collected (in an iterative fashion) on
the basis of their similarity to the current training instances. Based on the observation
4 See http://en.wikipedia.org/wiki/Wikipedia:Embrace_weasel_words.
345
Computational Linguistics Volume 38, Number 2
that uncertain sentences tend to contain more than one uncertainty cue, these models
successfully extend the seed sets with automatically labeled sentences, and can produce
an uncertainty classifier with a sentence-level F-score of 60?80% for the uncertain class,
given that the texts of the seed examples, the unlabeled pool, and the actual evaluation
data share very similar properties.
Szarvas (2008) showed that these models essentially learn the uncertainty lexicon
(set of cues) of the given domain, but are otherwise unable to disambiguate the potential
cue words?that is, to distinguish between the uncertain and certain uses of the previ-
ously seen cues. This deficiency of the derived models is inherent to the bootstrapping
process, which considers all occurrences of the cue candidates as good candidates for
positive examples (as opposed to unlabeled sentences without any previously seen cue
words).
Kilicoglu and Bergler (2008) proposed a semi-automatic method to expand a seed
cue lexicon. Their linguistically motivated approach is also based on the weakly super-
vised induction of a corpus of uncertain sentences. It exploits the syntactic patterns of
uncertain sentences to identify new cue candidates.
The previous studies on weakly supervised approaches to uncertainty detection
did not tackle the problem of disambiguating the certain and uncertain uses of cue
candidates, which is a major drawback from a practical point of view.
4.3 Cue Distribution Analyses
Besides automatic uncertainty recognition, several studies investigated the distribution
of hedge cues in scientific papers from different domains (Hyland 1998; Falahati 2006;
Rizomilioti 2006). The effect of different domains on the frequency of uncertain expres-
sions was examined in Rizomilioti (2006). Based on a previously defined dictionary of
hedge cues, she analyzed the linguistic tools expressing epistemic modality in research
papers from three domains, namely, archeology, literary criticism, and biology. Her
results indicated that archaeological papers tend to contain the most uncertainty cues
(which she calls downtoners) and the fewest uncertainty cues can be found in literary
criticism papers. Different academic disciplines were contrasted in Hyland (1998) from
the viewpoint of hedging: Papers belonging to the humanities contain significantly
more hedging devices than papers in sciences. It is interesting to note, however, that
in both studies, biological papers are situated in the middle as far as the percentage rate
of uncertainty cues is concerned. Falahati (2006) examined hedges in research articles in
medicine, chemistry, and psychology and concluded that it is psychology articles that
contain the most hedges.
Overall, these studies demonstrate that there are substantial differences in the way
different technical/scientific domains and different genres express uncertainty in gen-
eral, and in the use of semantic uncertainty in particular. Differences are found not just
in the use of different vocabulary for expressing uncertainty, but also in the frequency
of certain and uncertain usage of particular uncertainty cues. These findings underpin
the practical importance of domain portability and domain adaptation of uncertainty
detectors.
5. Uncertainty Cue Recognition
In this section, we present our uncertainty cue detector and the results of the cross-genre
and -domain experiments carried out by us. Before describing ourmodel and discussing
the results of the experiments, a short overview of the texts used as training and test
346
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
data sets will be given along with an empirical analysis of the sense distributions of the
most frequent cues.
5.1 Data Sets
In our investigations, we selected three corpora (i.e., BioScope, WikiWeasel, and Fact-
Bank) from different domains (biomedical, encyclopedia, and newswire, respectively).
Genres also vary in the corpora (in the scientific genre, there are papers and abstracts
whereas the other corpora contain pieces of news and encyclopedia articles). We pre-
ferred corpora on which earlier experiments had been carried out because this allowed
us to compare our results with those of previous studies. This selectionmakes it possible
to investigate domain and genre differences because each domain has its characteristic
language use (which might result in differences in cue distribution) and different genres
also require different writing strategies (e.g., in abstracts, implications of experimental
results are often emphasized, which usually involves the use of uncertain language).
The BioScope corpus (Vincze et al 2008) contains clinical texts as well as biological
texts from full papers and scientific abstracts; the texts were manually annotated for
hedge cues and their scopes. In our experiments, 15 other papers annotated for the
CoNLL-2010 Shared Task (Farkas et al 2010) were also added to the set of BioScope
papers. The WikiWeasel corpus (Farkas et al 2010) was also used in the CoNLL-2010
Shared Task and it was manually annotated for weasel cues and semantic uncertainty
in randomly selected paragraphs taken from Wikipedia articles. The FactBank corpus
contains texts from the newswire domain (Saur?? and Pustejovsky 2009). Events are
annotated in the data set and they are evaluated on the basis of their factuality from
the viewpoint of their sources.
Table 2 provides statistical data on the three corpora. Because in our experimental
set-up, texts belonging to different genres also play an important role, data on abstracts
and papers are included separately.
5.1.1 Genres and Domains. Texts found in the three corpora to be investigated can be
categorized into three genres, which can be further divided to subgenres at a finer level
of distinction. Figure 2 depicts this classification.
The majority of BioScope texts (papers and abstracts) belong to the scientific dis-
course genre. FactBank texts can be divided into broadcast and written news, and
Wikipedia texts belong to the encyclopedia genre.
As for the domain of the texts, there are three broad domains, namely, biology, news,
and encyclopedia. Once again, these domains can be further divided into narrower
Table 2
Data on the corpora. sent. = sentence; epist. = epistemic cue; dox. = doxastic cue;
inv. = investigation cue; cond. = condition cue.
Data Set #sent. #epist. #dox. #inv. #cond. Total
BioScope papers 7676 1373 220 295 187 2075
BioScope abstracts 11797 2478 200 784 24 3486
BioScope total 19473 3851 420 1079 211 5561
WikiWeasel 20756 1171 909 94 491 3265
FactBank 3123 305 201 36 178 720
Total 43352 5927 1530 1209 880 9546
347
Computational Linguistics Volume 38, Number 2
Figure 2
Genres of texts.
Figure 3
Domains of texts.
topics at a fine-grained level, which is shown in Figure 3. All abstracts and five papers
in BioScope are related to the MeSH terms human, blood cell, and transcription factor (hbc
in Figure 3). Nine BMC Bioinformatics papers come from the bioinformatics domain
(bmc in Figure 3), and ten papers describe some experimental results on the Drosophila
species (fly). FactBank news can be classified as stock news, political news, and
criminal news. Encyclopedia articles cover a broad range of topics, hence no detailed
classification is given here.
5.1.2 The Normalization of the Corpora. In order to uniformly evaluate our methods in
each domain and genre (and each corpus), the evaluation data sets were normalized.
This meant that cues had to be annotated in each data set and differentiated for types
of semantic uncertainty. This resulted in the reannotation of BioScope, WikiWeasel, and
FactBank.5 In BioScope, the originally annotated cues were separated into epistemic
cues and subtypes of hypothetical cues and instances of hypothetical uncertainty not
yet marked were also annotated. In FactBank, epistemic and hypothetical cues were
annotated: Uncertain events were matched with their uncertainty cues and instances
of hypothetical uncertainty that were originally not annotated were also marked in
the corpus. In the case of WikiWeasel, these two types of cues were separated from
discourse-level cues.
One class of hypothetical uncertainty (i.e., dynamic modality) was not annotated
in any of the corpora. Although dynamic modality seems to play a role in the news
domain, it is less important and less represented in the other two domains we investi-
gated here. The other subclasses are more of general interest for the applications. For
example, one of our training corpora comes from the scientific domain, where it is more
important to distinguish facts from hypotheses and propositions under investigation
5 The corpora are available at http://www.inf.u-szeged.hu/rgai/uncertainty.
348
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
(which can be later confirmed or rejected, compare the meta-knowledge annotation
scheme developed for biological events [Nawaz, Thompson, andAnaniadou 2010]), and
from propositions that depend on each other (conditions).
5.1.3 Uncertainty Cues in the Corpora. An analysis of the cue distributions reveals some
interesting trends that can be exploited in uncertainty detection across domains and
genres. The most frequent cue stems in the (sub)corpora used in our study can be seen
in Table 3 and they are responsible for about 74% of epistemic cue occurrences, 55% of
doxastic cue occurrences, 70% of investigation cue occurrences, and 91% of condition
cue occurrences.
As can be seen, one of the most frequent epistemic cues in each corpus is may. If,
possible, might, and suggest also occur frequently in our data set.
The distribution of the uncertainty cues was also analyzed from the perspective of
uncertainty classes in each corpus, which is presented in Figure 4. Inmost of the corpora,
epistemic cues are the most frequent (except for FactBank) and they vary the most:
Out of the 300 cue stems occurring in the corpora, 206 are epistemic cues. Comparing
the domains, it can readily be seen that in biological texts, doxastic uncertainty is not
frequent, which is especially true for abstracts, whereas in FactBank and WikiWeasel
they cover about 27% of the data. The most frequent doxastic keywords exhibit some
domain-specific differences, however: In BioScope, the most frequent ones include puta-
tive and hypothesis, which rarely occur in FactBank and WikiWeasel. Nevertheless, cues
belonging to the investigation class can be found almost exclusively in scientific texts
(89% of them are in BioScope), which can be expected because the aim of scientific pub-
lications is to examine whether a hypothesized phenomenon occurs. Among the most
Table 3
The most frequent cues in the corpora. epist. = epistemic cue; dox. = doxastic cue; inv. =
investigation cue; cond. = condition cue.
Global Abstracts Full papers BioScope FactBank WikiWeasel
Epist. may 1508 suggest 616 may 228 suggest 810 may 43 may 721
suggest 928 may 516 suggest 194 may 744 could 29 probable 112
indicate 421 indicate 301 indicate 103 indicate 404 possible 26 suggest 108
possible 304 appear 143 possible 84 appear 213 likely 24 possible 93
appear 260 or 119 might 83 or 197 might 23 likely 80
might 256 possible 101 or 78 possible 185 appear 15 might 78
likely 221 might 72 can 73 might 155 seem 11 seem 67
or 198 potential 72 appear 70 can 117 potential 10 could 55
could 196 likely 60 likely 57 likely 117 probable 10 perhaps 51
probable 157 could 56 could 56 could 112 suggest 10 appear 32
Dox. consider 276 putative 43 putative 37 putative 80 expect 75 consider 250
believe 222 think 43 hypothesis 33 hypothesis 77 believe 25 believe 173
expect 136 hypothesis 43 assume 24 think 66 think 24 allege 81
think 131 believe 14 think 24 assume 32 allege 8 think 61
putative 83 consider 10 expect 22 predict 26 accuse 7 regard 58
Invest. whether 247 investigate 177 whether 73 investigate 221 whether 26 whether 52
investigate 222 examine 160 investigate 44 examine 183 if 3 if 20
examine 183 whether 96 test 25 whether 169 remain to be seen 2 whether or not 7
study 102 study 88 examine 23 study 101 question 1 assess 3
determine 90 determine 67 determine 20 determine 87 determine 1 evaluate 3
Cond. if 418 if 14 if 85 if 99 if 65 if 254
would 238 would 6 would 46 would 52 would 50 would 136
will 80 until 2 will 20 will 20 will 21 will 39
until 40 could 1 should 11 should 11 until 16 until 15
could 30 unless 1 could 9 could 10 could 9 unless 14
349
Computational Linguistics Volume 38, Number 2
Figure 4
Cue type distributions in the corpora.
frequent cues, investigate, examine, and study belong to this group. These data reveal
that the frequency of doxastic and investigation cues is strongly domain-dependent,
and this explains the fact that the investigation vocabulary is very limited in Factbank
and WikiWeasel. Only about 10 cue stems belong to this uncertainty class in these cor-
pora. The set of condition cue stems, however, is very small in each corpus; altogether
18 condition cue stems can be found in the data, although if and would are responsible
for almost 75% of condition cue occurrences. It should also be mentioned that the
percentage of condition cues is higher in FactBank than in the other corpora.
Another interesting trend was observed when word forms were considered instead
of stemmed forms: Certain verbs in third person singular (e.g., expects or believes) occur
mostly in FactBank and WikiWeasel. The reason for this may be that when speaking
about someone else?s opinion in scientific discourse, the source of the opinion is usually
provided in the form of references or citations?usually at the end of the sentence?and
due to this, the verb is often used in the passive form, as in Example (9).
(9) It is currently believed that both RAG1 and RAG2 proteins were originally
encoded by the same transposon recruited in a common ancestor of jawed
vertebrates [3,12,13,16].
In contrast, impersonal constructions are hardly used in news media, where the ob-
jective is to inform listeners about the source of the news presented as well in order
to enable them to judge the reliability of a piece of news. Here, a clause including the
source and a communication verb is usually attached to the proposition.
A genre-related difference between scientific abstracts and full papers is that con-
dition cues can rarely be found in abstracts, although they occur more frequently in
papers (with the non-cue usage still being much more frequent). Another difference is
the percentage of cues of the investigation type, which may be related to the structure
350
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
of abstracts. Biological abstracts usually present the problem they examine and describe
methods they use. This entails the application of predicates belonging to the investiga-
tion class of uncertainty. It can be argued, however, that scientific papers also have these
characteristics but abstracts are much shorter than papers (generally, they contain about
10?12 sentences). Hence, investigation cues are responsible for a greater percentage of
cues.
There are some lexical differences among the corpora that are related to domain or
genre specificity. For instance, due to their semantics, the words charge, accuse, allege,
fear, worry, and rumor are highly unlikely to occur in scientific publications, but they
occur relatively often in news texts and in Wikipedia articles. As for lexical divergences
between abstracts and papers, many of them are related to verbs of investigation and
their different usage. In the corpora, verbs of investigations were marked only if it was
not clear whether the event/phenomenon would take place or not. If it has already
happened (The police are investigating the crime) or the existence of the thing under
investigation can be stated with certainty, independently of the investigation (The top
ten organisms were examined), then they are not instances of hypotheses, so they were
not annotated. As the data sets make clear, there were some candidates of investigation
verbs that occurred in the investigation sense mostly in abstracts but in another sense in
papers, especially in the bmc data set (e.g. assess or examine). Evaluate also had a special
mathematical sense in bmc papers, which did not occur in abstracts.
It can also be seen that some of the very frequent cues in papers do not occur (or
only relatively rarely) in abstracts. This is especially true for the bmc data set, where can,
if, would, could, and will are among the 15 most frequent cues and represent 23.21% of
cue occurrences, but only 3.85% in abstracts. It is also apparent that the rate of epistemic
cues is lower in bmc papers than in abstracts or other types of papers.
Genre-dependent characteristics can be analyzed if BioScope abstracts and hbc
papers are compared because their fine-grained domain is the same. Thus, it may be
assumed that differences between their cues are related to the genre. The sets of cues
used are similar, but the sense distributions may differ for certain ambiguous cues. For
instance, indicate mostly appears in the ?suggest? sense in abstracts, whereas in papers
it is used in the ?signal? sense. Another difference is that the percentage rate of doxastic
cues is almost twice as high in papers as in abstracts (10.6% and 5.7%, respectively).
Besides these differences, the two data sets are quite similar.
Domain-related differences can be analyzed when the three subdomains of biolog-
ical papers are contrasted. As stressed earlier, bmc papers contain fewer instances of
epistemic uncertainty, but condition cues occur more frequently in them. Nevertheless,
fly and hbc papers are rather similar in these respects but hbc papers contain more
investigation cues than the other two subcorpora. As regards lexical issues, the non-cue
usage of possible in comparative constructions is more frequent in the bmc data set than
in the other papers and many occurrences of if in bmc are related to definitions, which
were not annotated as uncertain. On the basis of this information, the fly and the hbc
domains seem to be more similar to each other than to the BMC data set from a linguistic
point of view.
From the perspective of genre and domain adaptation, the following points should
be highlighted concerning the distribution of uncertainty cues across corpora. Doxastic
uncertainty is of primary importance in the news and encyclopedia domains whereas
the investigation class is characteristic of the biological domain. Within the latter, there
is a genre-related difference as well: It is the epistemic and investigation classes that
are mainly present in abstracts whereas in papers cues belonging to other uncertainty
classes can also be found. Thus, when applying techniques developed for biological
351
Computational Linguistics Volume 38, Number 2
texts or abstracts to news texts, for example, doxastic uncertainty cues deserve special
attention as it might well be the case that there are insufficient training examples for this
class of uncertainty cues. The adaptation of an uncertainty cue detector constructed for
encyclopedia texts requires the special treatment of investigation cues, however, if, for
instance, scientific discourse is the target genre since they are underrepresented in the
source genre.
5.2 Evaluation Metrics
As evaluation metrics, we used cue-level and sentence-level F?=1 scores for the uncer-
tain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task)
and denote them by Fcue and Fsent, respectively. We report cue-level F?=1 scores on the
individual subcategories of uncertainty and the unlabeled (binary) F?=1 scores as well.
A sentence is treated as uncertain (in the gold standard and prediction) iff it contains at
least one cue. Note that the cue-level metric is quite strict as it is based on recognized
phrases?that is, only cues with perfect boundary matches are true positives. For the
sentence-level evaluation we simply labeled those sentences as uncertain that contained
at least one recognized cue.
5.3 Cross-Domain Cue Recognition Model
In order to minimize the development cost of a labeled corpus and an uncertainty
detector for a new genre/domain, we need to induce an accurate model from a minimal
amount of labeled data, or take advantage of existing corpora for different genres
and/or domains and use a domain adaptation approach. Experiments investigating the
value and sufficiency of existing corpora (which are usually out-of-domain) and simple
domain adaptation methods were carried out. For this purpose, we implemented a cue
recognition model, which is described in this section.
To train our models, we applied surface level (e.g., capitalization) and shallow
syntactic features (part-of-speech tags and chunks) and avoided the use of lexicon-based
features listing potential cue words, in order to reduce the domain dependence of the
learned models. Now we will introduce our model, which is competitive with the state-
of-the-art systems and focus on its domain adaptability. We will also describe the im-
plementation details of the learning model and the features employed. We should add
that the optimization of a cue detector was not the main focus of our study, however.
5.3.1 Feature Set.We extracted two types of features for each token to describe the token
itself, together with its local context in a window of limited size (1, 2, or no window,
depending on the feature).
The first group consists of features describing the surface form of the tokens. Here
we provide the list of the surface features with the corresponding window sizes:
 Stems of the tokens by the Porter stemmer in a window of size 2 (current
token and two tokens to the left and right).
 Surface pattern of the tokens in a window of size one (current token
and 1 token to the left and right). These patterns are similar to the word
shape feature described in Sun et al (2007). This feature can describe the
capitalization and other orthographic features as well. Patterns represent
352
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
character sequences of the same type with one single character for a
given word. There are six different pattern types denoting capitalized and
lowercased character sequences with the characters ?A? and ?a?, number
sequences with ?0?, Greek letter sequences with ?G? and ?g?, Roman
numerals with ?R? and ?r?, and non-alphanumerical characters with ?!?.
 Prefixes and suffixes of word forms from three to five characters long.
The second group of features describes the syntactic properties of the token and its
local context. The list of the syntactic features with the corresponding window sizes is
the following:
 Part-of-speech (POS) tags of the tokens by the C&C POS-tagger in a
window of size 2.
 Syntactic chunk of the tokens, as given by the C&C chunker,6 and the
chunk code of the tokens in a window of size 2.
 Concatenated stem, POS, and chunk labels similar to the features used
by Tang et al (2010). These feature strings were a combination of the stem
and the chunk code of the current token, the stem of the current token
combined with the POS-codes of the token left and right, and the chunk
code of the current token with the stems of the neighboring tokens.
5.3.2 CoNLL-2010 Experiments. The CoNLL-2010 shared task Learning to detect hedges and
their scope in natural language text focused on uncertainty detection. Two subtasks were
defined at the shared task: The first task sought to recognize sentences that contain some
uncertain language in two different domains and the second task sought to recognize
lexical cues together with their linguistic scope in biological texts (i.e., the text span in
terms of constituency grammar that covers the part of the sentence that is modified
by the cue). The lexical cue recognition subproblem of the second task7 is identical
to the problem setting used in this study, with the only major difference being the
types of uncertainty addressed: In the CoNLL-2010 task biological texts contained only
epistemic, doxastic, and investigation types of uncertainty. Apart from these differences,
the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertainty
detection model with other state-of-the-art approaches for uncertainty detection and to
compare different classification approaches. Here we present our detailed experiments
using the CoNLL data sets, analyze the performance of our models, and select the most
suitable models for further experiments.
CoNLL systems. The uncertainty detection systems that were submitted to the CoNLL
shared task can be classified into three major types. The first set of systems treats the
problem as a sentence classification task, that is, one to decide whether a sentence
contains any uncertain element or not. These models operate at the sentence level and
are unsuitable for cue detection. The second group handles the problem as a token
6 POS-tagging and chunking were performed on all corpora using the C&C Tools (Curran, Clark, and
Bos 2007).
7 As an intermediate level, participants of the first task could submit the lexical cues found in sentences
for evaluation, without their scope, which gave some insight into the nature of cue detection on the
Wikipedia corpus (where scope annotation does not exist) as well.
353
Computational Linguistics Volume 38, Number 2
Table 4
Results on the original CoNLL-2010 data sets. The first three rows correspond to our baseline,
token-based, and sequence labeling models. The BEST/SEQ row shows the results of the best
sequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rows
show the best token-based models, and the BEST/SENT rows show the best sentence-level
classifiers (these models did not produce cue-level results).
BIOLOGICAL WIKIPEDIA
Fcue Fsent Fcue Fsent
BASELINE 74.5 81.4 19.5 58.6
TOKEN/MAXENT 79.7 85.8 22.3 58.1
SEQUENCE/CRF 81.4 87.0 32.7 47.0
BEST/SEQ (Tang et al 2010) 81.3 86.4 36.5 55.0
BEST/TOK BIO (Velldal, ?vrelid, and Oepen 2010) 78.7 85.2 ? ?
BEST/TOKWIKI (Morante, Van Asch, and Daelemans 2010) 76.7 81.7 11.3 57.3
BEST/SENT BIO (Ta?ckstro?m et al 2010) ? 85.2 ? 55.4
BEST/SENTWIKI (Georgescul 2010) ? 78.5 ? 60.2
classification task, and classifies each token independently as uncertain (or not).
Contextual information is only included in the form of feature functions. The third
group of systems handled the task as a sequential token labeling problem, that is, de-
termined the most likely label sequence of a sentence in one step, taking the informa-
tion about neighboring labels into account. Sequence labeling and token classification
approaches performed best for biological texts and sentence-level models and token
classification approaches gave the best results for Wikipedia texts (see Table 6 in Farkas
et al [2010]). Here we compare a state-of-the-art token classification and sequence
labeling approach using a shared feature representation to decide which model to use
in further experiments.
Classifier models. We used a first-order linear chain conditional random fields (CRF)
model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a
token classifier, implemented in the Mallet (McCallum 2002) package for training the
uncertainty cue detectors. This choicewasmotivated by the fact that thesewere themost
popular classification approaches among the CoNLL-2010 participants, and that CRF
models are known to provide high accuracy for the detection of phrases with accurate
boundaries (e.g., in named entity recognition). We trained the CRF and Maxent models
with their default settings in Mallet for 200 iterations or until convergence (CRF), and
also until convergence (Maxent) in each experimental set-up.
As a baseline model, we applied a simple dictionary-based approach which clas-
sifies every uni- and bigram as uncertain that is tagged as uncertain in over 50% of
the cases in the training data. Hence, it is a similar system to that presented by Tjong
Kim Sang (2010), without tuning the decision threshold for predicting uncertainty.
CoNLL results. An overview of the results achieved on the CoNLL-2010 data sets can
be found in Table 4. A comparison of our models with the CoNLL systems reveals that
our uncertainty detection model is very competitive when applied on the biological
data set. Our CRF model trained on the official training data set of the shared task
achieved a cue-level F-score of 81.4 and sentence-level F-score of 87.0 on the biological
354
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
evaluation data set. These results would have come first in the shared task, with a
marginal difference compared to the top performing participant. In contrast, our model
is less competitive on the Wikipedia data set: The Maxent model achieved a cue-level
F-score of 22.3 and sentence-level F-score of 58.1 on the Wikipedia evaluation data
set, whereas our CRF model was not competitive with the best participating systems.
The observation that sequence-labeling models perform worse than token-based
approaches on Wikipedia, especially for sentence-level evaluation measures, coincides
with the findings of the shared task: The discourse-level uncertainty cues in the
Wikipedia data set are rather long and heterogeneous and sequence labeling models
often revert to not annotating any token in a sentence when the phrase boundaries are
hard to detect. Still, sequence labeling models have an advantage in terms of cue-level
accuracy. This is not surprising because CRF is a state-of-the-art model for chunking /
sequence labeling tasks.
We conclude from Table 4 that our model is competitive with the state-of-the-art
systems for detecting semantic uncertainty (which is closer to the biological subtask),
but it is less suited to recognizing discourse-level uncertainty. In the subsequent exper-
iments we used our CRF model, which performed best in detecting uncertainty cues in
natural language sentences.
5.3.3 Domain Adaptation Model. In supervised machine learning, the task is to learn how
to make predictions on previously unseen, new examples based on a statistical model
learned from a collection of labeled training examples (i.e., a set of examples coupled
with the desired output for them). The classification setting assumes a set of labels L, a
set of features X, and a probability distribution p(X) describing the examples in terms of
their features. Then the training examples are assumed to be given in the form of {xi, li}
pairs and the goal of classification is to estimate the label distribution p(L|X), which can
be used later on to predict the labels for unseen examples.
Domain adaptation focuses on the problem where the same (or a closely related)
learning task has to be solved in multiple domains which have different characteristics
in terms of their features: The set of features X may be different or the probability dis-
tributions p(X) describing the inputs may be different. When the target tasks are treated
as different (but related), the label distribution p(L|X) is dependent on the domain. That
is, given a domain d, the problem can be formalized as modeling p(L|X)d based on Xd,
p(X)d and a set of examples: {xi,d, li}.8 In the context of domain adaptation, there is a
target domain t and a source domain s, with labeled data available for both, and the
goal is to induce a more accurate target domain model p(L|X)t from {xi,t, li} ? {xi,s, li}
than the one learned from {xi,t, li} only. In practical scenarios, the goal is to exploit the
source data to acquire an accurate model from just limited target data which are alone
insufficient to train an accurate in-domain model, and thus to port the model to a new
domain with moderate annotation costs. The problem is difficult because it is nontrivial
for a learning method to account for the different data (and label) distributions between
target and source, which causes a remarkable drop in model accuracy when it is applied
to classifying examples taken from the target domain.
In our experimental context, both topic- and genre-related differences of texts pose
an adaptation problem as these factors have an impact on both the vocabulary (p(X))
and the sense distributions of the cues (p(L|X)) found in different texts. There is some
8 The literature also describes the case when the set of labels depends on the domain, but we omit this case
to simplify our notation and discussion. For details, see Pan and Yang (2010).
355
Computational Linguistics Volume 38, Number 2
confusion in the literature regarding the terminology describing the various domain
mismatches in the learning problem. For example, Daume? III (2007) describes a domain
adaptation method where he assumes that the label distribution is unchanged (we note
here that this assumption is not exploited in the method, and that the label distribution
changes in our problem), whereas Pan and Yang (2010) uses the term inductive transfer
learning to refer to our scenario (in their paper, domain adaptation refers to a different
setting).9 In this study we always use the term domain adaptation to refer to our problem
setting, that is, where both p(X) and p(L|X) are assumed to change.
In our experiments, we used various data sets taken from multiple genres and
domains (see Section 5.1.1 for an overview) and applied a simple but effective do-
main adaptation model (Daume? III 2007) for training our classifiers. In this model,
domain adaptation is carried out by defining each feature over the target and source
data sets twice?just once for target domain instances, and once for both the tar-
get and source domain instances. Formally, having a target domain t and a source
domain s and n features {f1, f2, . . . fn}, for each fi we have a target-only version fi,t
and a shared version fi,t+s. Each target domain example is described by 2n features:
{ f1,t, f2,t, . . . fn,t, f1,t+s, f2,t+s, . . . fn,t+s} and source domain examples are described by only
the n shared features: { f1,t+s, f2,t+s, . . . fn,t+s}. Using the union of the source and target
training data sets {xi,t, li} ? {xi,s, li} and this feature representation, any standard super-
vised machine learning technique can be used and it becomes possible for the algorithm
to learn target-dependent and shared patterns at the same time and handle the changes
in the underlying distributions. This easy domain adaptation technique has been found
to work well in many NLP-oriented tasks. We used the CRF models introduced herein
and in this way, we were able to exploit feature?label correspondences across domains
(for features that behave consistently across domains) and also to learn patterns specific
to the target domain.
5.4 Cross-Domain and Genre Experiments
We defined several settings (target and source pairs) with varied domain and genre
distances and target data set sizes. These experiments allowed us to study the po-
tential of transferring knowledge across existing corpora for the accurate detection of
uncertain language in a wide variety of text types. In our experiments, we used all the
combinations of genres and domains that we found plausible. News texts (and their
subdomains) were not used as source data because FactBank is significantly smaller
than the other corpora (WikiWeasel or scientific texts). As the source data set is typically
larger than the target data set in practical scenarios, news texts can only be used as target
data. Abstracts were only used as source data because information extraction typically
addresses full texts whereas abstracts just provide annotated data for development pur-
poses. Besides these restrictions, we experimented with all possible target and source
pairs.
We used four different machine-learning settings for each target?source pair in our
investigations. In the purely cross-domain (CROSS) setting, the model was trained on
the source domain and evaluated on the target (i.e., no labeled target domain data
sets were used for training). In the purely in-domain setting (TARGET), we performed
9 More on this can be found in Pan and Yang (2010) and at http://nlpers.blogspot.com/2007/11/
domain-adaptation-vs-transfer-learning.html.
356
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Table 5
Experimental results on different target and source domain pairs. The third column contains the
ratio of the target train and source data sets? sizes in terms of sentences. DIST shows the distance
of the source and target domain/genre (?-? same; ?+? fine-grade difference; ?++? coarse-grade
difference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientific
abstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly =
scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ 68.0 74.2 82.4 87.4 82.6 87.6 82.6 87.6
news sci paper+ abs 6.2 ++/++ 64.4 70.5 68.7 77.1 72.7 79.5 73.8 81.0
news enc 6.6 ++/++ 68.2 74.8 68.7 77.1 73.7 81.2 73.1 80.0
sci paper enc 2.7 ++/++ 67.8 75.1 78.8 84.4 80.0 85.9 79.8 85.4
sci paper bmc sci abs hbc 4.3 +/+ 58.2 70.5 64.0 74.5 68.1 76.7 69.3 77.8
sci paper fly sci abs hbc 3.4 +/+ 70.5 79.1 80.0 85.1 83.3 88.2 82.9 87.8
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 82.9 74.2 80.2 84.2 88.6 83.0 88.9
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 77.6 64.0 74.5 70.0 78.2 69.4 78.1
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 83.5 80.0 85.1 82.6 87.0 82.9 87.0
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 85.9 74.2 80.2 80.7 86.9 80.7 85.9
AVERAGE: 70.4 77.4 73.5 80.6 77.8 84.0 77.8 84.0
10-fold cross-validation on the target data (i.e., no source domain data were used). In
the two domain adaptation settings, we again performed 10-fold cross-validation on
the target data but exploited the source data set (as described in Section 5.3). Here, we
either used each sentence of the source data set (DA/ALL) or only those sentences that
contained a cue observed in the target train data set (DA/CUE).
Table 5 lists the results obtained on various target and source domains in various
machine learning settings and Table 6 contains the absolute differences between a
particular result and the in-domain (TARGET) results.
Fine-grained semantic uncertainty classification results are summarized in Tables 7
and 8. Table 7 contrasts the coarse-grained Fcue with the unlabeled/binary Fcue of fine-
grained experiments, therefore it quantifies the difference in accuracy due to the more
difficult classification setting and the increased sparseness of the task. Table 8 shows
the per class Fcue scores, namely, how accurately our model recognizes the individual
uncertainty types.
Table 6
The absolute difference between the F-scores of Table 5 relative to the baseline TARGET setting.
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ ?14.4 ?13.2 82.4 87.4 0.2 0.2 0.2 0.2
news sci paper+ abs 6.2 ++/++ ?4.3 ?6.6 68.7 77.1 4.0 2.4 5.1 3.9
news enc 6.6 ++/++ ?0.5 ?2.3 68.7 77.1 5.0 4.1 4.4 2.9
sci paper enc 2.7 ++/++ ?11.0 ?9.3 78.8 84.4 1.2 1.5 1.0 1.0
sci paper bmc sci abs hbc 4.3 +/+ ?5.8 ?4.0 64.0 74.5 4.1 2.2 5.3 3.3
sci paper fly sci abs hbc 3.4 +/+ ?9.5 ?6.0 80.0 85.1 3.3 3.1 2.9 2.7
sci paper hbc sci abs hbc 8.2 ?/+ 2.3 2.7 74.2 80.2 10.0 8.4 8.8 8.7
sci paper bmc sci paper fly+ hbc 1.8 +/? 5.8 3.1 64.0 74.5 6.0 3.7 5.4 3.6
sci paper fly sci paper bmc+ hbc 1.2 +/? ?1.6 ?1.6 80.0 85.1 2.6 1.9 2.9 1.9
sci paper hbc sci paper bmc+ fly 4.4 +/? 7.5 5.7 74.2 80.2 6.5 6.7 6.5 5.7
AVERAGE: ?3.1 ?3.2 73.5 80.6 4.3 3.4 4.3 3.4
357
Computational Linguistics Volume 38, Number 2
Table 7
Comparison of cue-level binary (Fbin) and unlabeled F-scores (Funl). Binary F-score corresponds
to coarse-grained classification (uncertain vs. certain), and unlabeled F-score is the fine-grained
classification converted to binary (disregarding the fine-grained category labels).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fbin Funl Fbin Funl Fbin Funl Fbin Funl
enc sci paper+ abs 0.9 ++/++ 68.0 67.4 82.4 82.4 82.6 81.9 82.6 81.7
news sci paper+ abs 6.2 ++/++ 64.4 59.9 68.7 66.4 72.7 71.5 73.8 71.8
news enc 6.6 ++/++ 68.2 67.0 68.7 66.4 73.7 73.6 73.1 73.4
sci paper enc 2.7 ++/++ 67.8 67.2 78.8 78.3 80.0 80.2 79.8 79.5
sci paper bmc sci abs hbc 4.3 +/+ 58.2 66.3 64.0 61.9 68.1 68.5 69.3 67.9
sci paper fly sci abs hbc 3.4 +/+ 70.5 78.7 80.0 79.2 83.3 83.4 82.9 83.2
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 83.6 74.2 69.3 84.2 83.1 83.0 83.4
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 69.7 64.0 61.9 70.0 69.5 69.4 65.9
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 77.7 80.0 79.2 82.6 82.1 82.9 82.5
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 81.9 74.2 69.3 80.7 81.3 80.7 81.2
AVERAGE: 70.4 71.9 73.5 71.4 77.8 77.5 77.8 77.0
Table 8
The per class cue-level F-scores in fine-grained classification. Fcrs, Ftgt, and Fda correspond to the
CROSS, TARGET, and DA/CUE settings, respectively (same as previous). The DA/ALL setting
is not shown for space reasons and due to its similarity to the DA/CUE results.
EPISTEMIC INVESTIGATION DOXASTIC CONDITION
TARGET SOURCE Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda
enc sci paper+ abs 75.9 83.4 82.8 67.3 67.5 70.4 48.8 89.2 88.1 54.4 62.6 61.2
news sci paper+ abs 70.9 65.4 75.2 79.5 75.9 83.1 39.1 68.9 71.3 47.2 57.1 57.5
news enc 65.4 65.4 74.5 74.6 75.9 87.5 76.3 68.9 78.0 50.6 57.1 56.7
sci paper enc 72.9 81.2 81.9 36.5 72.9 72.4 63.6 74.9 79.8 57.0 58.9 59.7
sci paper bmc sci abs hbc 71.5 68.3 72.6 56.1 37.7 58.1 68.1 61.9 69.4 45.5 45.0 49.5
sci paper fly sci abs hbc 82.9 82.1 85.3 69.0 68.6 76.6 75.1 71.7 75.4 28.6 63.4 64.1
sci paper hbc sci abs hbc 87.5 77.7 86.4 76.5 53.5 77.5 80.6 39.0 76.7 26.1 10.0 33.3
sci paper bmc sci paper fly+ hbc 74.4 68.3 69.2 55.9 37.7 57.4 63.7 61.9 64.7 57.3 45.0 50.7
sci paper fly sci paper bmc+ hbc 80.3 82.1 84.3 66.7 68.6 75.8 77.7 71.7 77.3 53.5 63.4 68.0
sci paper hbc sci paper bmc+ fly 85.2 77.7 86.0 74.0 53.5 70.3 75.9 39.0 70.2 58.1 10.0 41.4
AVERAGE: 76.7 75.2 79.8 65.6 61.2 72.9 66.9 64.7 75.1 47.8 47.3 54.2
The size of the target training data sets proved to be an important factor in these
investigations. Hence, we performed experiments with different target data set sizes.
We utilized the DA/ALL model (which is more robust for extremely small target data
sizes [e.g., 100-400 sentences]) and performed the same 10-fold cross validation on the
target data set as in Tables 5-8. For each fold of the cross-validation here, however, we
just used n sentences (x axis of the figures) from the target training data set and a fixed
set of 4,000 source sentences to alleviate the effect of varying data set sizes. Figure 5
depicts the learning curves for two target/source data set pairs.
6. Discussion
As Table 5 shows, incorporating labeled data from different genres and/or domains
consistently improves the performance. The successful applicability of domain adap-
tation tells us that the problem of detecting uncertainty has similar characteristics
across genres and domains. The uncertainty cue lexicons of different domains and
358
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Figure 5
Learning curves: Results achieved with different target train sizes. The left and right figures
show two selected source/target pairs. The upper figures depict coarse-grained classification
results (Fcue); DA, CROSS, and TARGET with the same settings as in Table 5. The lower figures
show the per class Fcue of the DA/ALL model in the fine-grained classification.
genres indeed share a core vocabulary and despite the differences in sense distributions,
labeled data from a different source improves uncertainty classification in a new genre
and domain if the different data sets are annotated consistently. This justifies our aim
to create a consistent representation of uncertainty that can be applied to multiple
domains.
6.1 Domain Adaptation Results
The size of the target and source data sets largely influences to what extent external
data can improve results. The only case where domain adaptation had only a negligible
effect (an F-score gain less than 1%) is where the target data set is itself very large.
This is expected as the more target data one has, the less crucial it is to incorporate
additional data with some undesirable characteristics (difference in style, domain,
certain/uncertain sense distribution, etc.).
The performance scores for the CROSS setting clearly indicate the domain/genre
distance of the data sets: Themore distant the domain and genre of the source and target
data sets are, the more the CROSS performance (where no labeled target data is used)
degrades, compared with the TARGET model. In general, when the distance between
both the domain and the genre of texts is substantial (++/++ and +/+ rows in Tables 5
359
Computational Linguistics Volume 38, Number 2
and 6), this accounts for a 6?10% decrease in both the sentence and cue-level F-scores.
An exception is the case of encyclopedic source and news target domains. Here the
performance is very close to the target domain performance. This indicates that these
settings are not so different from each other as it might seem at the first glance. The
encyclopedic and news genres share quite a lot of commonalities (compare cue distribu-
tions in Figure 4, for instance). We verified this observation by using a knowledge-poor
quantitative estimator of similarity between domains (Van Asch and Daelemans 2010):
Using cosine as the similarity measure, the newswire and encyclopedia texts are found
to be the second most similar domain pair in our experiments, with a score comparable
to those obtained for the pairs of scientific article types bmc, hbc, and fly.
When there is a domain or genre match between source and target (?/+ and +/?
rows in Tables 5 and 6), however, and the distance regarding the other is just moderate,
the cross-training performance is close to or even better than the target-only results.
That is, the larger amount of source training data balances the differences between the
domains. These results indicate that the learned uncertainty classifiers can be directly
applied to slightly different data sets. This suitability is due to the learned disambigua-
tion models, which generalize well in similar settings. This is contrary to the findings
of earlier studies, which built the uncertainty detectors using seed examples and boot-
strapping. These models were not designed to learn any disambiguation models for
the cue words found, and their performance degraded even for slightly different data
(Szarvas 2008).
Comparing the two domain adaptation procedures DA/CUE and DA/ALL, adap-
tation via transferring only source sentences that contain a target domain cue is, on
average, comparable to transferring all the data from the source domain. In other words,
when we have a small but sufficient amount of target data available, it is enough to
account for source data corresponding to the uncertainty cues we saw in the limited
target data set. This observation has several consequences, namely:
 The source-only cues, or to be more precise, their disambiguation models,
are not helpful for the target domains as they cannot be adapted. This is
due to the differences in the source and target disambiguation models.
 Similarly, domain adaptation improves the disambiguation models for the
observed target cues, rather than introducing new vocabulary into the
target domain. This mechanism coincides with our initial goal of using
domain adaptation to learn better semantic models. This effect is the
opposite of how bootstrapping-based weakly supervised approaches
improve the performance in an underresourced domain. This observation
suggests a promising future direction of combining the two approaches to
maximize the gains while minimizing the annotation costs.
 In a general context, we can effectively extend the data for a given domain
if we have robust knowledge of the potential uncertainty vocabulary for
that domain. Given the wide variety of the domains and genres of our data
sets, it is reasonable to suppose that they represent uncertain language in
general quite well, and the joint vocabularies provide a good starting point
for a targeted data development for further domains.
As regards the fine-grained classification results, Table 7 demonstrates that the
fine-grained distinction results in only a small, or no, loss in performance. The coarse-
grained model is slightly more accurate than the fine-grained model (counting correctly
360
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
recognized but misclassified cues as true positives) in most settings. The most signif-
icant difference is observed for the target-only settings, where no out-of-domain data
are used for the training and thus the data sets are accordingly smaller. A noticeable
exception is when scientific abstracts are used for cross training: In those settings the
coarse-grained model performs poorly, due to its lower recall, which we attribute to
overfitting the special characteristics of abstracts. The fact that in fine-grained classi-
fication the CROSS results consistently outperform the TARGET models (see Table 8)
even for distant domain pairs, also underlines that the increased sparseness caused by
the differentiation of the various subtypes of uncertainty is an important factor only for
smaller data sets. The improvement by domain adaptation is clearly more prominent
in fine-grained than in coarse-grained classification, however: The individual cue types
benefit by 5?10% points in terms of the F-score from out-of-domain data and domain
adaptation. Moreover, as Table 8 shows, for the domain pairs and fine-grained classes
where a nice amount of positive examples are at hand, the per class Fcue scores are
also around 80% and above. This means that it is possible to accurately identify the
individual subtypes of semantic uncertainty, and thus it also proves the feasibility of
the subcategorization and annotation scheme proposed in this study (Section 2). Other
important observations here are that domain adaptation is even more significant in the
more difficult fine-grained classification setting, and that the condition class represents
a challenge for our model. The performance for the condition class is lower than that
for the other classes, which can only in part be attributed to the fact that this is the
least represented subtype in our data sets: as opposed to other cue types, condition cues
are typically used in many different contexts and they may belong to other uncertainty
classes as well.
6.2 The Required Amount of Annotation
Based on our experiments, we may conclude that a manually annotated training data
set consisting of 3,000?5,000 sentences is sufficient for training an accurate cue detector
for a new genre/domain. The results of our learning curve experiments (Figure 5)
illustrate the situations where only a limited amount of annotated data (fewer than 3,000
sentences) is available for the target domain. The feasibility of decreasing annotation
efforts and the real added value of domain adaptation are more prominent in this range.
It is easy to see that the TARGET results approach to DA results with more target data.
Figure 5 shows that the size of the target training data set where the supervised
TARGET setting outperforms the CROSS model (trained on 4,000 source sentences) is
around 1,000 sentences. Aswementioned earlier, even distant domain data can improve
the cue recognition model in the absence of a sufficient target data set. Figure 5 justifies
this observation, as the CROSS and DA settings outperform the TARGET setting on
each source?target data set pair. It can also be observed that the doxastic type is more
domain-dependent than the others and its results consistently improve by increasing
the size of the target domain annotation (which coincides with the cue frequency
investigations of Section 5.1.3). In the news target domain, however, the investigation
and epistemic classes benefit a lot from a small amount of annotated target data but their
performance scores increase just slightly after that. This indicates that most of the im-
portant domain-dependent (probably lexical) knowledge could be gathered from 100?
400 sentences. In the biological experiments, we may conclude that the investigation
class is already covered by the source domain (intuitively, the investigation cues are well
represented in the abstracts) and its results are not improved significantly by usingmore
361
Computational Linguistics Volume 38, Number 2
target data. The condition class is underrepresented in both the source and target data
sets and hence no reliable observations can bemade regarding this subclass (see Table 2).
Overall, if we would like to have an uncertainty cue detector for a new
genre/domain: (i) We can achieve performance around 60?70% by using cross training
depending on the difference between the domains (i.e., without any annotation effort);
(ii) By annotating around 3,000 sentences, we can have a performance of 70?80%,
depending on the level of difficulty of the texts; (iii) We can get the same 70?80% results
with annotating just 1,000 sentences and using domain adaptation.
6.3 Interesting Examples and Error Analysis
As might be expected, most of the erroneous cue predictions were due to vocabulary
differences, for example, fear or accuse occurred only in news texts, which is why they
were not recognized by models trained on biological or encyclopedia texts. Another
example is the case of or, which is a frequent cue in biological texts. Still, it is rarely
used as a cue in other domains but without domain adaptation, the model trained on
biological texts marks quite a few occurrences of or as cues in the news or encyclope-
dia domains. Many of these anomalies were eliminated by the application of domain
adaptation techniques, however.
Many errors were related to multi-class cues. These cues are especially hard to
disambiguate because not only can they refer to several classes of uncertainty, but
they typically have non-cue usage as well. For instance, the case of would is rather
complicated because it can fulfill several functions:
(10) EPISTEMIC USAGE (?IT IS HIGHLY PROBABLE?): Further biochemical studies
on the mechanism of action of purified kinesin-5 from multiple systems
would obviously be fruitful. (Corpus: fly)
(11) CONDITIONAL: ?If religion was a thing that money could buy,/The rich
would live and the poor would die.? (Corpus: WikiWeasel)
(12) FUTURE IN THE PAST: This Aarup can trace its history back to 1500, but it
would be 1860?s before it would become a town. (Corpus: WikiWeasel)
(13) REPEATED ACTION IN THE PAST (?USED TO?): ?Becker? was the next T.V.
Series for Paramount that Farrell would co-star in. (Corpus: WikiWeasel)
(14) DYNAMIC MODALITY: Individuals would first have a small lesion at the
site of the insect bite, which would eventually leave a small scar. (Corpus:
WikiWeasel)
(15) PRAGMATIC USAGE: Although some would dispute the fact, the joke
related to a peculiar smell that follows his person. (Corpus: WikiWeasel)
The epistemic uses of would are annotated as epistemic cues whereas its occurrences in
conditionals are marked as hypothetical cues. The habitual past meaning is not related
to uncertainty, hence it is not annotated. The future in the past meaning (i.e., past tense
of will), however, denotes an event of which it is known that happened later, so it is
certain. The dynamically modal would is similar to the future will (which is an instance
of dynamic modality as well), but it is not annotated in the corpora. The pragmatic
362
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
use of would does not refer to semantic uncertainty (the semantic value of the sentence
would be exactly the same without it or if it is replaced with may, might, will, etc., that
is, some will/may/might/? dispute the fact mean the same). It is rather a stylistic issue
to further express uncertainty at the discourse level (i.e., there are some unidentified
people who dispute the fact, hence the opinion cannot be associated with any definite
source).
The last two uses of would are not typically described in grammars of English
and seem to be characteristic primarily of the news and encyclopedia domains. Thus
it is advisable to explore such cases and treat them with special consideration when
adapting an algorithm trained and tested in a specific domain to another domain.
Another interesting example is may in its non-cue usage. Being (one of) the most
frequent cues in each subcorpus, its non-cue usage is rather limited but can be found
occasionally in FactBank and WikiWeasel. The following instance of may in FactBank
was correctly marked as non-cue by the cue detector when trained on Wikipedia texts.
On the other hand, it was marked as a cue when trained on biological texts since in this
case, there were insufficient training examples of may not being a cue:
(16) ?Wellmay we say ?God save the Queen,? for nothing will save the
republic,? outraged monarchist delegate David Mitchell said. (Corpus:
FactBank)
A final example to be discussed is concern. This word also has several uses:
(17) NOUN MEANING ?COMPANY?: The insurance concern said all conversion
rights on the stock will terminate on Nov. 30. (Corpus: FactBank)
(18) NOUN MEANING ?WORRY?: Concern about declines in other markets,
especially New York, caused selling pressure. (Corpus: FactBank)
(19) PREPOSITION: The company also said it continues to explore all options
concerning the possible sale of National Aluminum?s 54.5% stake in an
aluminum smelter in Hawesville, Ky. (Corpus: FactBank)
(20) VERB: Many of the predictions in these two data sets concern protein pairs
and proteins that are not present in other data sets. (Corpus: bmc)
Among these examples, only the second one should be annotated as uncertain. POS-
tagging seems to provide enough information for excluding the verbal and preposi-
tional uses of the word but in the case of nominal usage, additional information is also
required to enable the system to decide whether it is an uncertainty cue or not (in this
case, the noun in the ?company? sense cannot have an argument while in the ?worry?
sense, it can have [about declines]). Again, the frequency of the two senses depends
heavily on the domain of the texts, which should also be considered when adapting
the cue detector to a different domain. We should mention that the role of POS-tagging
is essential in cue detection because many ambiguities can be resolved on the basis of
POS-tags. Hence, POS-tagging errors can lead to a serious decline in performance.
We think that an analysis of similar examples can further support domain adapta-
tion and cue detection across genres and domains.
363
Computational Linguistics Volume 38, Number 2
7. Conclusions and Future Work
In this article, we introduced an uncertainty cue detection model that can perform well
across different domains and genres. Even though several types of uncertainty exist,
available corpora and resources focus only on some of the possible types and thereby
only cover particular aspects of the phenomenon. This means that uncertainty models
found in the literature are heterogeneous, and the results of experiments on different
corpora are hardly comparable. These facts motivated us to offer a unified model of
semantic uncertainty enhanced by linguistic and computer science considerations. In
accordance with this classification, we reannotated three corpora from several domains
and genres using our uniform annotation guidelines.
Our results suggest that simple cross training can be employed and it achieves a
reasonable performance (60?70% cue-level F-score) when no annotated data is at hand
for a new domain. When some annotated data is available (here somemeans fewer than
3,000 annotated sentences for the target domain), domain adaptation techniques are
the best choice: (i) they lead to a significant improvement compared to simple cross
training, and (ii) they can provide a reasonable performance with significantly less
annotation. In our experiments, the annotation of 3,000 sentences and training only on
these is roughly equivalent to the annotation of 1,000 sentences using external data and
domain adaptation. If the size of the training data set is sufficiently large (larger than
5,000 sentences) the effect of incorporating additional data?having some undesirable
characteristics?is not crucial.
Comparing different domain adaptation techniques, we found that similar results
could be attained when the source domain was filtered for sentences that contained
cues in the target domain. This tells us that models learn to better disambiguate the
cues seen in the target domain instead of finding new, unseen cues. In this sense, this
approach can be regarded as a complementarymethod toweakly supervised techniques
for lexicon extraction. A promising way to further minimize annotation costs while
maximizing performance would be the integration of the two approaches, which we
plan to investigate in the near future.
In our study, we did not pay attention to dynamic modality (due to the lack of an-
notated resources), but the detection of such phenomena is also desirable. For instance,
dynamically modal events cannot be treated as certain?that is, the event of buying
cannot be assigned the same truth value in They agreed to buy the company and They
bought the company. Whereas the second sentence expresses a fact, the first one informs
us about the intention of buying the company, which will be certainly carried out in a
world where moral or business laws are observed but at the moment it cannot be stated
whether the transaction takes place (i.e., that it is certain). Hence, in the future, we also
intend to integrate the identification of dynamically modal cues into our uncertainty
cue detector.
Acknowledgments
This work was supported in part by
the NIH grants (project codenames
MASZEKER and BELAMI) of the
Hungarian government, by the German
Ministry of Education and Research
under grant SiDiM (grant no. 01IS10054G),
and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship
Program (grant no. I/82806). Richa?rd
Farkas was funded by the Deutsche
Forschungsgemeinschaft grant SFB 732.
References
Baker, Kathy, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
364
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Sergei Nirenburg, Christine Piatko,
Owen Rambow, and Gramm Richardson.
2010. Modality Annotation Guidelines.
Technical Report 4, Human Language
Technology Center of Excellence,
Baltimore, MD.
Chapman, Wendy W., David Chu, and
John N. Dowling. 2007. ConText: An
algorithm for identifying contextual
features from clinical text. In Proceedings
of the ACL Workshop on BioNLP 2007,
pages 81?88, Prague, Czech Republic.
Clausen, David. 2010. HedgeHunter:
A system for hedge detection and
uncertainty classification. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 120?125, Uppsala.
Conway, Mike, Son Doan, and Nigel Collier.
2009. Using hedges to enhance a disease
outbreak report text mining system. In
Proceedings of the BioNLP 2009 Workshop,
pages 142?143, Boulder, CO.
Curran, James, Stephen Clark, and Johan
Bos. 2007. Linguistically motivated
large-scale NLP with C&C and Boxer. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 33?36, Prague.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 256?263,
Prague.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Falahati, Reza. 2006. The use of hedging
across different disciplines and rhetorical
sections of research articles. In Proceedings
of the 22nd NorthWest Linguistics Conference
(NWLC22), pages 99?112, Burnaby.
Farkas, Richa?rd and Gyo?rgy Szarvas. 2008.
Automatic construction of rule-based
ICD-9-CM coding systems. BMC
Bioinformatics, 9:1?9.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 1?12, Uppsala.
Fernandes, Eraldo R., Carlos E. M. Crestana,
and Ruy L. Milidiu?. 2010. Hedge detection
using the RelHunter approach. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 64?69,
Uppsala.
Friedman, Carol, Philip O. Alderson,
John H. M. Austin, James J. Cimino, and
Stephen B. Johnson. 1994. A General
natural-language text processor for clinical
radiology. Journal of the American Medical
Informatics Association, 1(2):161?174.
Ganter, Viola and Michael Strube. 2009.
Finding hedges by chasing weasels: Hedge
detection using Wikipedia tags and
shallow linguistic features. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 173?176, Suntec.
Georgescul, Maria. 2010. A hedgehop over a
max-margin framework using hedge cues.
In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 26?31,
Uppsala.
Hyland, Ken. 1994. Hedging in academic
writing and EAP textbooks. English for
Specific Purposes, 13(3):239?256.
Hyland, Ken. 1996. Writing without
conviction? Hedging in scientific
research articles. Applied Linguistics,
17(4):433?454.
Hyland, Ken. 1998. Boosters, hedging and
the negotiation of academic knowledge.
Text, 18(3):349?382.
Kiefer, Ferenc. 2005. Leheto?se?g e?s
szu?kse?gszeru?se?g [Possibility and
necessity]. Tinta Kiado?, Budapest.
Kilicoglu, Halil and Sabine Bergler.
2008. Recognizing speculative
language in biomedical research articles:
A linguistically motivated perspective.
In Proceedings of the Workshop on Current
Trends in Biomedical Natural Language
Processing, pages 46?53, Columbus, OH.
Kilicoglu, Halil and Sabine Bergler. 2009.
Syntactic dependency based heuristics for
biological event extraction. In Proceedings
of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 119?127,
Boulder, CO.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 1?9, Boulder, OH.
Kim, Jin-Dong, Tomoko Ohta, and Jun?ichi
Tsujii. 2008. Corpus annotation for mining
biomedical events from literature. BMC
Bioinformatics, 9(Suppl 10).
365
Computational Linguistics Volume 38, Number 2
Li, Xinxin, Jianping Shen, Xiang Gao, and
Xuan Wang. 2010. Exploiting rich features
for detecting hedges and their scope. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 78?83,
Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, Massachusetts, USA.
MacKinlay, Andrew, David Martinez, and
Timothy Baldwin. 2009. Biomedical event
annotation with CRFs and precision
grammars. In Proceedings of the Workshop
on Current Trends in Biomedical Natural
Language Processing: Shared Task,
BioNLP ?09, pages 77?85, Uppsala.
McCallum, Andrew Kachites. 2002.
MALLET: A Machine Learning for
Language Toolkit. Available at
http://mallet.cs.umass.edu.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature. In
Proceedings of the ACL, pages 992?999,
Prague.
Morante, Roser and Walter Daelemans.
2009. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans. 2011.
Annotating modality and negation for a
machine reading evaluation. In Proceedings
of CLEF 2011, Amsterdam, Netherlands.
Morante, Roser, Vincent Van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scopes of hedge
cues. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 40?47, Uppsala, Sweden.
Nawaz, Raheel, Paul Thompson, and
Sophia Ananiadou. 2010. Evaluating a
meta-knowledge annotation scheme for
bio-events. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 69?77, Uppsala.
O?zgu?r, Arzucan and Dragomir R. Radev.
2009. Detecting speculations and their
scopes in scientific text. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 1398?1407, Singapore.
Palmer, Frank Robert. 1979.Modality and
the English Modals. Longman, London.
Palmer, Frank Robert. 1986.Mood and
Modality. Cambridge University Press,
Cambridge.
Pan, Sinno Jialin and Qiang Yang. 2010.
A survey on transfer learning. IEEE
Transactions on Knowledge and Data
Engineering, 22(10):1345?1359.
Rei, Marek and Ted Briscoe. 2010. Combining
manual rules and supervised learning
for hedge cue and scope detection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 56?63,
Uppsala.
Rizomilioti, Vassiliki. 2006. Exploring
epistemic modality in academic discourse
using corpora. In Elisabet Arno? Macia,
Antonia Soler Cervera, and Carmen Rueda
Ramos, editors, Information Technology in
Languages for Specific Purposes, volume 7
of Educational Linguistics. Springer US,
New York, pages 53?71.
Rubin, Victoria L. 2010. Epistemic modality:
From uncertainty to certainty in the
context of information seeking as
interactions with texts. Information
Processing & Management, 46(5):533?540.
Rubin, Victoria L., Elizabeth D. Liddy,
and Noriko Kando. 2005. Certainty
identification in texts: Categorization
model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce
Wiebe, editors, Computing Attitude and
Affect in Text: Theory and Applications (the
Information Retrieval Series), Springer
Verlag, New York, pages 61?76.
Russell, Stuart J. and Peter Norvig. 2010.
Artificial Intelligence?AModern Approach
(3rd international edition). Upper Saddle
River, NJ: Pearson Education.
Sa?nchez, Liliana Mamani, Baoli Li, and
Carl Vogel. 2010. Exploiting CCG
structures with tree kernels for speculation
detection. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 126?131, Uppsala.
Saur??, Roser. 2008. A Factuality Profiler
for Eventualities in Text. Ph.D. thesis,
Brandeis University, Waltham, MA.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43:227?268.
Settles, Burr, Mark Craven, and Lewis
Friedland. 2008. Active learning with
real annotation costs. In Proceedings of
366
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
the NIPS Workshop on Cost-Sensitive
Learning, pages 1?10, Vancouver, Canada.
Shatkay, Hagit, Fengxia Pan, Andrey
Rzhetsky, and W. John Wilbur. 2008.
Multi-dimensional classification of
biomedical text: Toward automated,
practical provision of high-utility
text to diverse users. Bioinformatics,
24(18):2086?2093.
Sun, Chengjie, Lei Lin, Xiaolong Wang,
and Yi Guan. 2007. Using maximum
entropy model to extract protein-protein
interaction information from biomedical
literature. In De-Shuang Huang, Donald C.
Wunsch, Daniel S. Levine, and Kang-Hyun
Jo, editors, Advanced Intelligent Computing
Theories and Applications. With Aspects
of Theoretical and Methodological Issues.
Springer Verlag, Heidelberg,
pages 730?737.
Szarvas, Gyo?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords.
In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, OH.
Ta?ckstro?m, Oscar, Sumithra Velupillai,
Martin Hassel, Gunnar Eriksson,
Hercules Dalianis, and Jussi Karlgren.
2010. Uncertainty detection as
approximate max-margin sequence
labelling. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010):
Shared Task, pages 84?91, Uppsala.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 13?17,
Uppsala.
Thompson, Paul, Giulia Venturi, John
McNaught, Simonetta Montemagni, and
Sophia Ananiadou. 2008. Categorising
modality in biomedical texts. In Proceedings
of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text
Mining, pages 27?34, Marrakech, Morocco.
Tjong Kim Sang, Erik. 2010. A baseline
approach for detecting sentences
containing uncertainty. In Proceedings
of the Fourteenth Conference on
Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 148?150, Uppsala.
Uzuner, O?zlem, Xiaoran Zhang, and
Tawanda Sibanda. 2009. Machine
learning and rule-based approaches to
assertion classification. Journal of the
American Medical Informatics Association,
16(1):109?115.
Van Asch, Vincent and Walter Daelemans.
2010. Using domain similarity for
performance estimation. In Proceedings of
the 2010 Workshop on Domain Adaptation for
Natural Language Processing, pages 31?36,
Uppsala.
Van Landeghem, Sofie, Yvan Saeys,
Bernard De Baets, and Yves Van de Peer.
2009. Analyzing text in search of
bio-molecular events: A high-precision
machine learning framework. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 128?136, Boulder, CO.
Velldal, Erik. 2010. Detecting uncertainty
in biomedical literature: A simple
disambiguation approach using sparse
random indexing. In Proceedings of SMBM
2010, pages 75?83, Cambridge.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the Fourteenth Conference
on Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 48?55, Uppsala.
Vincze, Veronika, Gyo?rgy Szarvas, Richa?rd
Farkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008. The BioScope Corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics,
9(Suppl 11):S9.
Wilson, Theresa Ann. 2008. Fine-grained
Subjectivity and Sentiment Analysis:
Recognizing the Intensity, Polarity, and
Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh, PA.
Zhang, Shaodian, Hai Zhao, Guodong Zhou,
and Bao-Liang Lu. 2010. Hedge detection
and scope finding by sequence labeling
with normalized feature selection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 92?99,
Uppsala.
367
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 255?261,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying English and Hungarian Light Verb Constructions:
A Contrastive Approach
Veronika Vincze1,2, Istva?n Nagy T.2 and Richa?rd Farkas2
1Hungarian Academy of Sciences, Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
2Department of Informatics, University of Szeged
{nistvan,rfarkas}@inf.u-szeged.hu
Abstract
Here, we introduce a machine learning-
based approach that allows us to identify
light verb constructions (LVCs) in Hun-
garian and English free texts. We also
present the results of our experiments on
the SzegedParalellFX English?Hungarian
parallel corpus where LVCs were manu-
ally annotated in both languages. With
our approach, we were able to contrast
the performance of our method and define
language-specific features for these typo-
logically different languages. Our pre-
sented method proved to be sufficiently ro-
bust as it achieved approximately the same
scores on the two typologically different
languages.
1 Introduction
In natural language processing (NLP), a signifi-
cant part of research is carried out on the English
language. However, the investigation of languages
that are typologically different from English is
also essential since it can lead to innovations that
might be usefully integrated into systems devel-
oped for English. Comparative approaches may
also highlight some important differences among
languages and the usefulness of techniques that are
applied.
In this paper, we focus on the task of identify-
ing light verb constructions (LVCs) in English and
Hungarian free texts. Thus, the same task will be
carried out for English and a morphologically rich
language. We compare whether the same set of
features can be used for both languages, we in-
vestigate the benefits of integrating language spe-
cific features into the systems and we explore how
the systems could be further improved. For this
purpose, we make use of the English?Hungarian
parallel corpus SzegedParalellFX (Vincze, 2012),
where LVCs have been manually annotated.
2 Light Verb Constructions
Light verb constructions (e.g. to give advice) are
a subtype of multiword expressions (Sag et al,
2002). They consist of a nominal and a verbal
component where the verb functions as the syn-
tactic head, but the semantic head is the noun. The
verbal component (also called a light verb) usu-
ally loses its original sense to some extent. Al-
though it is the noun that conveys most of the
meaning of the construction, the verb itself can-
not be viewed as semantically bleached (Apres-
jan, 2004; Alonso Ramos, 2004; Sanroma?n Vi-
las, 2009) since it also adds important aspects to
the meaning of the construction (for instance, the
beginning of an action, such as set on fire, see
Mel?c?uk (2004)). The meaning of LVCs can be
only partially computed on the basis of the mean-
ings of their parts and the way they are related to
each other, hence it is important to treat them in a
special way in many NLP applications.
LVCs are usually distinguished from productive
or literal verb + noun constructions on the one
hand and idiomatic verb + noun expressions on
the other (Fazly and Stevenson, 2007). Variativ-
ity and omitting the verb play the most significant
role in distinguishing LVCs from productive con-
structions and idioms (Vincze, 2011). Variativity
reflects the fact that LVCs can be often substituted
by a verb derived from the same root as the nomi-
nal component within the construction: productive
constructions and idioms can be rarely substituted
by a single verb (like make a decision ? decide).
Omitting the verb exploits the fact that it is the
nominal component that mostly bears the seman-
tic content of the LVC, hence the event denoted
by the construction can be determined even with-
out the verb in most cases. Furthermore, the very
same noun + verb combination may function as an
LVC in certain contexts while it is just a productive
construction in other ones, compare He gave her a
255
ring made of gold (non-LVC) and He gave her a
ring because he wanted to hear her voice (LVC),
hence it is important to identify them in context.
In theoretical linguistics, Kearns (2002) distin-
guishes between two subtypes of light verb con-
structions. True light verb constructions such as
to give a wipe or to have a laugh and vague ac-
tion verbs such as to make an agreement or to
do the ironing differ in some syntactic and se-
mantic features and can be separated by various
tests, e.g. passivization, WH-movement, pronom-
inalization etc. This distinction also manifests in
natural language processing as several authors pay
attention to the identification of just true light verb
constructions, e.g. Tu and Roth (2011). However,
here we do not make such a distinction and aim to
identify all types of light verb constructions both
in English and in Hungarian, in accordance with
the annotation principles of SZPFX.
The canonical form of a Hungarian light verb
construction is a bare noun + third person singular
verb. However, they may occur in non-canonical
versions as well: the verb may precede the noun,
or the noun and the verb may be not adjacent due
to the free word order. Moreover, as Hungarian
is a morphologically rich language, the verb may
occur in different surface forms inflected for tense,
mood, person and number. These features will be
paid attention to when implementing our system
for detecting Hungarian LVCs.
3 Related Work
Recently, LVCs have received special interest in
the NLP research community. They have been au-
tomatically identified in several languages such as
English (Cook et al, 2007; Bannard, 2007; Vincze
et al, 2011a; Tu and Roth, 2011), Dutch (Van de
Cruys and Moiro?n, 2007), Basque (Gurrutxaga
and Alegria, 2011) and German (Evert and Ker-
mes, 2003).
Parallel corpora are of high importance in the
automatic identification of multiword expressions:
it is usually one-to-many correspondence that is
exploited when designing methods for detecting
multiword expressions. Caseli et al (2010) de-
veloped an alignment-based method for extracting
multiword expressions from Portuguese?English
parallel corpora. Samardz?ic? and Merlo (2010) an-
alyzed English and German light verb construc-
tions in parallel corpora: they pay special attention
to their manual and automatic alignment. Zarrie?
and Kuhn (2009) argued that multiword expres-
sions can be reliably detected in parallel corpora
by using dependency-parsed, word-aligned sen-
tences. Sinha (2009) detected Hindi complex
predicates (i.e. a combination of a light verb and
a noun, a verb or an adjective) in a Hindi?English
parallel corpus by identifying a mismatch of the
Hindi light verb meaning in the aligned English
sentence. Many-to-one correspondences were also
exploited by Attia et al (2010) when identifying
Arabic multiword expressions relying on asym-
metries between paralell entry titles of Wikipedia.
Tsvetkov and Wintner (2010) identified Hebrew
multiword expressions by searching for misalign-
ments in an English?Hebrew parallel corpus.
To the best of our knowledge, parallel corpora
have not been used for testing the efficiency of an
MWE-detecting method for two languages at the
same time. Here, we investigate the performance
of our base LVC-detector on English and Hungar-
ian and pay special attention to the added value of
language-specific features.
4 Experiments
In our investigations we made use of the Szeged-
ParalellFX English-Hungarian parallel corpus,
which consists of 14,000 sentences and contains
about 1370 LVCs for each language. In addition,
we are aware of two other corpora ? the Szeged
Treebank (Vincze and Csirik, 2010) and Wiki50
(Vincze et al, 2011b) ?, which were manually an-
notated for LVCs on the basis of similar principles
as SZPFX, so we exploited these corpora when
defining our features.
To automatically identify LVCs in running
texts, a machine learning based approach was ap-
plied. This method first parsed each sentence and
extracted potential LVCs. Afterwards, a binary
classification method was utilized, which can au-
tomatically classify potential LVCs as an LVC or
not. This binary classifier was based on a rich fea-
ture set described below.
The candidate extraction method investi-
gated the dependency relation among the verbs
and nouns. Verb-object, verb-subject, verb-
prepositional object, verb-other argument (in the
case of Hungarian) and noun-modifier pairs were
collected from the texts. The dependency labels
were provided by the Bohnet parser (Bohnet,
2010) for English and by magyarlanc 2.0
(Zsibrita et al, 2013) for Hungarian.
256
The features used by the binary classifier can be
categorised as follows:
Morphological features: As the nominal com-
ponent of LVCs is typically derived from a verbal
stem (make a decision) or coincides with a verb
(have a walk), the VerbalStem binary feature fo-
cuses on the stem of the noun; if it had a verbal
nature, the candidates were marked as true. The
POS-pattern feature investigates the POS-tag se-
quence of the potential LVC. If it matched one pat-
tern typical of LVCs (e.g. verb + noun) the
candidate was marked as true; otherwise as false.
The English auxiliary verbs, do and have often
occur as light verbs, hence we defined a feature for
the two verbs to denote whether or not they were
auxiliary verbs in a given sentence.The POS code
of the next word of LVC candidate was also ap-
plied as a feature. As Hungarian is a morpholog-
ically rich language, we were able to define vari-
ous morphology-based features like the case of the
noun or its number etc. Nouns which were histor-
ically derived from verbs but were not treated as
derivation by the Hungarian morphological parser
were also added as a feature.
Semantic features: This feature also exploited
the fact that the nominal component is usually de-
rived from verbs. Consequently, the activity
or event semantic senses were looked for among
the upper level hyperonyms of the head of the
noun phrase in English WordNet 3.11 and in the
Hungarian WordNet (Miha?ltz et al, 2008).
Orthographic features: The suffix feature is
also based on the fact that many nominal compo-
nents in LVCs are derived from verbs. This feature
checks whether the lemma of the noun ended in
a given character bi- or trigram. The number of
words of the candidate LVC was also noted and
applied as a feature.
Statistical features: Potential English LVCs
and their occurrences were collected from 10,000
English Wikipedia pages by the candidate extrac-
tion method. The number of occurrences was used
as a feature when the candidate was one of the syn-
tactic phrases collected.
Lexical features: We exploit the fact that the
most common verbs are typically light verbs.
Therefore, fifteen typical light verbs were selected
from the list of the most frequent verbs taken from
the Wiki50 (Vincze et al, 2011b) in the case of En-
glish and from the Szeged Treebank (Vincze and
1http://wordnet.princeton.edu
Csirik, 2010) in the case of Hungarian. Then, we
investigated whether the lemmatised verbal com-
ponent of the candidate was one of these fifteen
verbs. The lemma of the noun was also applied
as a lexical feature. The nouns found in LVCs
were collected from the above-mentioned corpora.
Afterwards, we constructed lists of lemmatised
LVCs got from the other corpora.
Syntactic features: As the candidate extraction
methods basically depended on the dependency
relation between the noun and the verb, they could
also be utilised in identifying LVCs. Though the
dobj, prep, rcmod, partmod or nsubjpass
dependency labels were used in candidate extrac-
tion in the case of English, these syntactic relations
were defined as features, while the att, obj,
obl, subj dependency relations were used in the
case of Hungarian. When the noun had a deter-
miner in the candidate LVC, it was also encoded
as another syntactic feature.
Our feature set includes language-independent
and language-specific features as well. Language-
independent features seek to acquire general fea-
tures of LVCs while language-specific features can
be applied due to the different grammatical char-
acteristics of the two languages or due to the avail-
ability of different resources. Table 1 shows which
features were applied for which language.
We experimented with several learning algo-
rithms and decision trees have been proven per-
forming best. This is probably due to the fact that
our feature set consists of compact ? i.e. high-
level ? features. We trained the J48 classifier of the
WEKA package (Hall et al, 2009). This machine
learning approach implements the decision trees
algorithm C4.5 (Quinlan, 1993). The J48 classi-
fier was trained with the above-mentioned features
and we evaluated it in a 10-fold cross validation.
The potential LVCs which are extracted by the
candidate extraction method but not marked as
positive in the gold standard were classed as neg-
ative. As just the positive LVCs were annotated
on the SZPFX corpus, the F?=1 score interpreted
on the positive class was employed as an evalu-
ation metric. The candidate extraction methods
could not detect all LVCs in the corpus data, so
some positive elements in the corpora were not
covered. Hence, we regarded the omitted LVCs
as false negatives in our evaluation.
257
Features Base English Hungarian
Orthographical ? ? ?
VerbalStem ? ? ?
POS pattern ? ? ?
LVC list ? ? ?
Light verb list ? ? ?
Semantic features ? ? ?
Syntactic features ? ? ?
Auxiliary verb ? ? ?
Determiner ? ? ?
Noun list ? ? ?
POS After ? ? ?
LVC freq. stat. ? ? ?
Agglutinative morph. ? ? ?
Historical derivation ? ? ?
Table 1: The basic feature set and language-
specific features.
English Hungarian
ML 63.29/56.91/59.93 66.1/50.04/56.96
DM 73.71/29.22/41.67 63.24/34.46/44.59
Table 2: Results obtained in terms of precision, re-
call and F-score. ML: machine learning approach
DM: dictionary matching method.
5 Results
As a baseline, a context free dictionary matching
method was applied. For this, the gold-standard
LVC lemmas were gathered from Wiki50 and the
Szeged Treebank. Texts were lemmatized and if
an item on the list was found in the text, it was
treated as an LVC.
Table 2 lists the results got on the two differ-
ent parts of SZPFX using the machine learning-
based approach and the baseline dictionary match-
ing. The dictionary matching approach yielded the
highest precision on the English part of SZPFX,
namely 73.71%. However, the machine learning-
based approach proved to be the most successful
as it achieved an F-score that was 18.26 higher
than that with dictionary matching. Hence, this
method turned out to be more effective regard-
ing recall. At the same time, the machine learn-
ing and dictionary matching methods got roughly
the same precision score on the Hungarian part of
SZPFX, but again the machine learning-based ap-
proach achieved the best F-score. While in the
case of English the dictionary matching method
got a higher precision score, the machine learning
approach proved to be more effective.
An ablation analysis was carried out to exam-
ine the effectiveness of each individual feature of
the machine learning-based candidate classifica-
Feature English Hungarian
All 59.93 56.96
Lexical -19.11 -14.05
Morphological -1.68 -1.75
Orthographic -0.43 -3.31
Syntactic -1.84 -1.28
Semantic -2.17 -0.34
Statistical -2.23 ?
Language-specific -1.83 -1.05
Table 3: The usefulness of individual features in
terms of F-score using the SZPFX corpus.
tion. For each feature type, a J48 classifier was
trained with all of the features except that one. We
also investigated how language-specific features
improved the performance compared to the base
feature set. We then compared the performance to
that got with all the features. Table 3 shows the
contribution of each individual feature type on the
SZPFX corpus. For each of the two languages,
each type of feature contributed to the overall per-
formance. Lexical features were very effective in
both languages.
6 Discussion
According to the results, our base system is ro-
bust enough to achieve approximately the same
results on two typologically different languages.
Language-specific features further contribute to
the performance as shown by the ablation anal-
ysis. It should be also mentioned that some of
the base features (e.g. POS-patterns, which we
thought would be useful for English due to the
fixed word order) were originally inspired by one
of the languages and later expanded to the other
one (i.e. they were included in the base feature set)
since it was also effective in the case of the other
language. Thus, a multilingual approach may be
also beneficial in the case of monolingual applica-
tions as well.
The most obvious difference between the per-
formances on the two languages is the recall scores
(the difference being 6.87 percentage points be-
tween the two languages). This may be related to
the fact that the distribution of light verbs is quite
different in the two languages. While the top 15
verbs covers more than 80% of the English LVCs,
in Hungarian, this number is only 63% (and in or-
der to reach the same coverage, 38 verbs should be
included). Another difference is that there are 102
258
different verbs in English, which follow the Zipf
distribution, on the other hand, there are 157 Hun-
garian verbs with a more balanced distributional
pattern. Thus, fewer verbs cover a greater part of
LVCs in English than in Hungarian and this also
explains why lexical features contribute more to
the overall performance in English. This fact also
indicates that if verb lists are further extended, still
better recall scores may be achieved for both lan-
guages.
As for the effectiveness of morphological and
syntactic features, morphological features perform
better on a language with a rich morphologi-
cal representation (Hungarian). However, syntax
plays a more important role in LVC detection in
English: the added value of syntax is higher for
the English corpora than for the Hungarian one,
where syntactic features are also encoded in suf-
fixes, i.e. morphological information.
We carried out an error analysis in order to see
how our system could be further improved and
the errors reduced. We concluded that there were
some general and language-specific errors as well.
Among the general errors, we found that LVCs
with a rare light verb were difficult to recognize
(e.g. to utter a lie). In other cases, an originally
deverbal noun was used in a lexicalised sense to-
gether with a typical light verb ((e.g. buildings
are given (something)) and these candidates were
falsely classed as LVCs. Also, some errors in
POS-tagging or dependency parsing also led to
some erroneous predictions.
As for language-specific errors, English verb-
particle combinations (VPCs) followed by a noun
were often labeled as LVCs such as make up
his mind or give in his notice. In Hungar-
ian, verb + proper noun constructions (Hamletet
ja?tssza?k (Hamlet-ACC play-3PL.DEF) ?they are
playing Hamlet?) were sometimes regarded as
LVCs since the morphological analysis does not
make a distinction between proper and common
nouns. These language-specific errors may be
eliminated by integrating a VPC detector and a
named entity recognition system into the English
and Hungarian systems, respectively.
Although there has been a considerable amount
of literature on English LVC identification (see
Section 3), our results are not directly comparable
to them. This may be explained by the fact that dif-
ferent authors aimed to identify a different scope
of linguistic phenomena and thus interpreted the
concept of ?light verb construction? slightly dif-
ferently. For instance, Tu and Roth (2011) and Tan
et al (2006) focused only on true light verb con-
structions while only object?verb pairs are consid-
ered in other studies (Stevenson et al, 2004; Tan et
al., 2006; Fazly and Stevenson, 2007; Cook et al,
2007; Bannard, 2007; Tu and Roth, 2011). Several
other studies report results only on light verb con-
structions formed with certain light verbs (Steven-
son et al, 2004; Tan et al, 2006; Tu and Roth,
2011). In contrast, we aimed to identify all kinds
of LVCs, i.e. we did not apply any restrictions on
the nature of LVCs to be detected. In other words,
our task was somewhat more difficult than those
found in earlier literature. Although our results are
somewhat lower on English LVC detection than
those attained by previous studies, we think that
despite the difficulty of the task, our method could
offer promising results for identifying all types of
LVCs both in English and in Hungarian.
7 Conclusions
In this paper, we introduced our machine learning-
based approach for identifying LVCs in Hungar-
ian and English free texts. The method proved
to be sufficiently robust as it achieved approxi-
mately the same scores on two typologically dif-
ferent languages. The language-specific features
further contributed to the performance in both lan-
guages. In addition, some language-independent
features were inspired by one of the languages, so
a multilingual approach proved to be fruitful in the
case of monolingual LVC detection as well.
In the future, we would like to improve our sys-
tem by conducting a detailed analysis of the effect
of each feature on the results. Later, we also plan
to adapt the tool to other types of multiword ex-
pressions and conduct further experiments on lan-
guages other than English and Hungarian, the re-
sults of which may further lead to a more robust,
general LVC system. Moreover, we can improve
the method applied in each language by imple-
menting other language-specific features as well.
Acknowledgments
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
259
References
Margarita Alonso Ramos. 2004. Las construcciones
con verbo de apoyo. Visor Libros, Madrid.
Jurij D. Apresjan. 2004. O semantic?eskoj nepustote
i motivirovannosti glagol?nyx leksic?eskix funkcij.
Voprosy jazykoznanija, (4):3?18.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic
Extraction of Arabic Multiword Expressions. In
Proceedings of the 2010 Workshop on Multiword
Expressions: from Theory to Applications, pages
19?27, Beijing, China, August. Coling 2010 Orga-
nizing Committee.
Colin Bannard. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop
on a Broader Perspective on Multiword Expressions,
MWE ?07, pages 1?8, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the Work-
shop on a Broader Perspective on Multiword Ex-
pressions, MWE ?07, pages 41?48, Morristown, NJ,
USA. Association for Computational Linguistics.
Stefan Evert and Hannah Kermes. 2003. Experiments
on candidate data for collocation extraction. In Pro-
ceedings of EACL 2003, pages 83?86.
Afsaneh Fazly and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Antton Gurrutxaga and In?aki Alegria. 2011. Auto-
matic Extraction of NV Expressions in Basque: Ba-
sic Issues on Cooccurrence Techniques. In Proceed-
ings of the Workshop on Multiword Expressions:
from Parsing and Generation to the Real World,
pages 2?7, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Kate Kearns. 2002. Light verbs in English.
Manuscript.
Igor Mel?c?uk. 2004. Verbes supports sans peine.
Lingvisticae Investigationes, 27(2):203?217.
Ma?rton Miha?ltz, Csaba Hatvani, Judit Kuti, Gyo?rgy
Szarvas, Ja?nos Csirik, Ga?bor Pro?sze?ky, and Tama?s
Va?radi. 2008. Methods and Results of the Hun-
garian WordNet Project. In Attila Tana?cs, Do?ra
Csendes, Veronika Vincze, Christiane Fellbaum, and
Piek Vossen, editors, Proceedings of the Fourth
Global WordNet Conference (GWC 2008), pages
311?320, Szeged. University of Szeged.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002, pages 1?15, Mexico City,
Mexico.
Tanja Samardz?ic? and Paola Merlo. 2010. Cross-lingual
variation of light verb constructions: Using parallel
corpora and automatic alignment for linguistic re-
search. In Proceedings of the 2010 Workshop on
NLP and Linguistics: Finding the Common Ground,
pages 52?60, Uppsala, Sweden, July. Association
for Computational Linguistics.
Begon?a Sanroma?n Vilas. 2009. Towards a seman-
tically oriented selection of the values of Oper1.
The case of golpe ?blow? in Spanish. In David
Beck, Kim Gerdes, Jasmina Milic?evic?, and Alain
Polgue`re, editors, Proceedings of the Fourth In-
ternational Conference on Meaning-Text Theory ?
MTT?09, pages 327?337, Montreal, Canada. Univer-
site? de Montre?al.
R. Mahesh K. Sinha. 2009. Mining Complex Predi-
cates In Hindi Using A Parallel Hindi-English Cor-
pus. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 40?46, Singa-
pore, August. Association for Computational Lin-
guistics.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical Measures of the Semi-Productivity
of Light Verb Constructions. In Takaaki Tanaka,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions: Integrating Processing, pages 1?
8, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Yee Fan Tan, Min-Yen Kan, and Hang Cui. 2006.
Extending corpus-based identification of light verb
constructions using a supervised learning frame-
work. In Proceedings of the EACL Workshop on
260
Multi-Word Expressions in a Multilingual Contexts,
pages 49?56, Trento, Italy, April. Association for
Computational Linguistics.
Yulia Tsvetkov and Shuly Wintner. 2010. Extrac-
tion of multi-word expressions from small parallel
corpora. In Coling 2010: Posters, pages 1256?
1264, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Yuancheng Tu and Dan Roth. 2011. Learning English
Light Verb Constructions: Contextual or Statistical.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Parsing and Generation to the Real
World, pages 31?39, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on a Broader
Perspective on Multiword Expressions, MWE ?07,
pages 25?32, Morristown, NJ, USA. Association for
Computational Linguistics.
Veronika Vincze and Ja?nos Csirik. 2010. Hungar-
ian corpus of light verb constructions. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 1110?
1118, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Veronika Vincze, Istva?n Nagy T., and Ga?bor Berend.
2011a. Detecting Noun Compounds and Light Verb
Constructions: a Contrastive Study. In Proceedings
of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages
116?121, Portland, Oregon, USA, June. ACL.
Veronika Vincze, Istva?n Nagy T., and Ga?bor Berend.
2011b. Multiword expressions and named entities in
the Wiki50 corpus. In Proceedings of RANLP 2011,
Hissar, Bulgaria.
Veronika Vincze. 2011. Semi-Compositional Noun
+ Verb Constructions: Theoretical Questions and
Computational Linguistic Analyses. Ph.D. thesis,
University of Szeged, Szeged, Hungary.
Veronika Vincze. 2012. Light Verb Constructions
in the SzegedParalellFX English?Hungarian Paral-
lel Corpus. In Nicoletta Calzolari, Khalid Choukri,
Thierry Declerck, Mehmet Ug?ur Dog?an, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Trans-
lational Correspondences for Pattern-Independent
MWE Identification. In Proceedings of the Work-
shop on Multiword Expressions: Identification,
Interpretation, Disambiguation and Applications,
pages 23?30, Singapore, August. Association for
Computational Linguistics.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. magyarlanc 2.0: szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s [magyarlanc 2.0:
Syntactic parsing and accelerated POS-tagging].
In Attila Tana?cs and Veronika Vincze, editors,
MSzNy 2013 ? IX. Magyar Sza?m??to?ge?pes Nyelve?szeti
Konferencia, pages 368?374, Szeged. Szegedi Tu-
doma?nyegyetem.
261
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text
Richa?rd Farkas1,2, Veronika Vincze1, Gyo?rgy Mo?ra1, Ja?nos Csirik1,2, Gyo?rgy Szarvas3
1 University of Szeged, Department of Informatics
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
3 Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing Lab
{rfarkas,vinczev,gymora,csirik}@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de
Abstract
The CoNLL-2010 Shared Task was dedi-
cated to the detection of uncertainty cues
and their linguistic scope in natural lan-
guage texts. The motivation behind this
task was that distinguishing factual and
uncertain information in texts is of essen-
tial importance in information extraction.
This paper provides a general overview
of the shared task, including the annota-
tion protocols of the training and evalua-
tion datasets, the exact task definitions, the
evaluation metrics employed and the over-
all results. The paper concludes with an
analysis of the prominent approaches and
an overview of the systems submitted to
the shared task.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
provides a competitive shared task for the Com-
putational Linguistics community. After a five-
year period of multi-language semantic role label-
ing and syntactic dependency parsing tasks, a new
task was introduced in 2010, namely the detection
of uncertainty and its linguistic scope in natural
language sentences.
In natural language processing (NLP) ? and in
particular, in information extraction (IE) ? many
applications seek to extract factual information
from text. In order to distinguish facts from unre-
liable or uncertain information, linguistic devices
such as hedges (indicating that authors do not
or cannot back up their opinions/statements with
facts) have to be identified. Applications should
handle detected speculative parts in a different
manner. A typical example is protein-protein in-
teraction extraction from biological texts, where
the aim is to mine text evidence for biological enti-
ties that are in a particular relation with each other.
Here, while an uncertain relation might be of some
interest for an end-user as well, such information
must not be confused with factual textual evidence
(reliable information).
Uncertainty detection has two levels. Auto-
matic hedge detectors might attempt to identify
sentences which contain uncertain information
and handle whole sentences in a different man-
ner or they might attempt to recognize in-sentence
spans which are speculative. In-sentence uncer-
tainty detection is a more complicated task com-
pared to the sentence-level one, but it has bene-
fits for NLP applications as there may be spans
containing useful factual information in a sentence
that otherwise contains uncertain parts. For ex-
ample, in the following sentence the subordinated
clause starting with although contains factual in-
formation while uncertain information is included
in the main clause and the embedded question.
Although IL-1 has been reported to con-
tribute to Th17 differentiation in mouse
and man, it remains to be determined
{whether therapeutic targeting of IL-1
will substantially affect IL-17 in RA}.
Both tasks were addressed in the CoNLL-2010
Shared Task, in order to provide uniform manu-
ally annotated benchmark datasets for both and to
compare their difficulties and state-of-the-art so-
lutions for them. The uncertainty detection prob-
lem consists of two stages. First, keywords/cues
indicating uncertainty should be recognized then
either a sentence-level decision is made or the lin-
guistic scope of the cue words has to be identified.
The latter task falls within the scope of semantic
analysis of sentences exploiting syntactic patterns,
as hedge spans can usually be determined on the
basis of syntactic patterns dependent on the key-
word.
1
2 Related Work
The term hedging was originally introduced by
Lakoff (1972). However, hedge detection has re-
ceived considerable interest just recently in the
NLP community. Light et al (2004) used a hand-
crafted list of hedge cues to identify specula-
tive sentences in MEDLINE abstracts and several
biomedical NLP applications incorporate rules for
identifying the certainty of extracted information
(Friedman et al, 1994; Chapman et al, 2007; Ara-
maki et al, 2009; Conway et al, 2009).
The most recent approaches to uncertainty de-
tection exploit machine learning models that uti-
lize manually labeled corpora. Medlock and
Briscoe (2007) used single words as input features
in order to classify sentences from biological ar-
ticles (FlyBase) as speculative or non-speculative
based on semi-automatically collected training ex-
amples. Szarvas (2008) extended the methodology
of Medlock and Briscoe (2007) to use n-gram fea-
tures and a semi-supervised selection of the key-
word features. Kilicoglu and Bergler (2008) pro-
posed a linguistically motivated approach based
on syntactic information to semi-automatically re-
fine a list of hedge cues. Ganter and Strube (2009)
proposed an approach for the automatic detec-
tion of sentences containing uncertainty based on
Wikipedia weasel tags and syntactic patterns.
The BioScope corpus (Vincze et al, 2008) is
manually annotated with negation and specula-
tion cues and their linguistic scope. It consists
of clinical free-texts, biological texts from full pa-
pers and scientific abstracts. Using BioScope for
training and evaluation, Morante and Daelemans
(2009) developed a scope detector following a su-
pervised sequence labeling approach while O?zgu?r
and Radev (2009) developed a rule-based system
that exploits syntactic patterns.
Several related works have also been published
within the framework of The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), where
a separate subtask was dedicated to predicting
whether the recognized biological events are un-
der negation or speculation, based on the GENIA
event corpus annotations (Kilicoglu and Bergler,
2009; Van Landeghem et al, 2009).
3 Uncertainty Annotation Guidelines
The shared task addressed the detection of uncer-
tainty in two domains. As uncertainty detection
is extremely important for biomedical information
extraction and most existing approaches have tar-
geted such applications, participants were asked
to develop systems for hedge detection in bio-
logical scientific articles. Uncertainty detection
is also important, e.g. in encyclopedias, where
the goal is to collect reliable world knowledge
about real-world concepts and topics. For exam-
ple, Wikipedia explicitly declares that statements
reflecting author opinions or those not backed up
by facts (e.g. references) should be avoided (see
3.2 for details). Thus, the community-edited en-
cyclopedia, Wikipedia became one of the subjects
of the shared task as well.
3.1 Hedges in Biological Scientific Articles
In the biomedical domain, sentences were manu-
ally annotated for both hedge cues and their lin-
guistic scope. Hedging is typically expressed by
using specific linguistic devices (which we refer to
as cues in this article) that modify the meaning or
reflect the author?s attitude towards the content of
the text. Typical hedge cues fall into the following
categories:
? auxiliaries: may, might, can, would, should,
could, etc.
? verbs of hedging or verbs with speculative
content: suggest, question, presume, suspect,
indicate, suppose, seem, appear, favor, etc.
? adjectives or adverbs: probable, likely, possi-
ble, unsure, etc.
? conjunctions: or, and/or, either . . . or, etc.
However, there are some cases where a hedge is
expressed via a phrase rather than a single word.
Complex keywords are phrases that express un-
certainty together, but not on their own (either the
semantic interpretation or the hedging strength of
its subcomponents are significantly different from
those of the whole phrase). An instance of a com-
plex keyword can be seen in the following sen-
tence:
Mild bladder wall thickening {raises
the question of cystitis}.
The expression raises the question of may be sub-
stituted by suggests and neither the verb raises nor
the noun question convey speculative meaning on
their own. However, the whole phrase is specula-
tive therefore it is marked as a hedge cue.
2
During the annotation process, a min-max strat-
egy for the marking of keywords (min) and their
scope (max) was followed. On the one hand, when
marking the keywords, the minimal unit that ex-
presses hedging and determines the actual strength
of hedging was marked as a keyword. On the other
hand, when marking the scopes of speculative key-
words, the scope was extended to the largest syn-
tactic unit possible. That is, all constituents that
fell within the uncertain interpretation were in-
cluded in the scope. Our motivation here was that
in this way, if we simply disregard the marked text
span, the rest of the sentence can usually be used
for extracting factual information (if there is any).
For instance, in the example above, we can be sure
that the symptom mild bladder wall thickening is
exhibited by the patient but a diagnosis of cystitis
would be questionable.
The scope of a speculative element can be de-
termined on the basis of syntax. The scopes of
the BioScope corpus are regarded as consecutive
text spans and their annotation was based on con-
stituency grammar. The scope of verbs, auxil-
iaries, adjectives and adverbs usually starts right
with the keyword. In the case of verbal elements,
i.e. verbs and auxiliaries, it ends at the end of the
clause or sentence, thus all complements and ad-
juncts are included. The scope of attributive ad-
jectives generally extends to the following noun
phrase, whereas the scope of predicative adjec-
tives includes the whole sentence. Sentential ad-
verbs have a scope over the entire sentence, while
the scope of other adverbs usually ends at the end
of the clause or sentence. Conjunctions generally
have a scope over the syntactic unit whose mem-
bers they coordinate. Some linguistic phenomena
(e.g. passive voice or raising) can change scope
boundaries in the sentence, thus they were given
special attention during the annotation phase.
3.2 Wikipedia Weasels
The chief editors of Wikipedia have drawn the at-
tention of the public to uncertainty issues they call
weasel1. A word is considered to be a weasel
word if it creates an impression that something im-
portant has been said, but what is really commu-
nicated is vague, misleading, evasive or ambigu-
ous. Weasel words do not give a neutral account
of facts, rather, they offer an opinion without any
1http://en.wikipedia.org/wiki/Weasel_
word
backup or source. The following sentence does
not specify the source of information, it is just the
vague term some people that refers to the holder of
this opinion:
Some people claim that this results in a
better taste than that of other diet colas
(most of which are sweetened with as-
partame alone).
Statements with weasel words usually evoke ques-
tions such as Who says that?, Whose opinion is
this? and How many people think so?.
Typical instances of weasels can be grouped in
the following way (we offer some examples as
well):
? Adjectives and adverbs
? elements referring to uncertainty: prob-
able, likely, possible, unsure, often, pos-
sibly, allegedly, apparently, perhaps,
etc.
? elements denoting generalization:
widely, traditionally, generally, broadly-
accepted, widespread, etc.
? qualifiers and superlatives: global, su-
perior, excellent, immensely, legendary,
best, (one of the) largest, most promi-
nent, etc.
? elements expressing obviousness:
clearly, obviously, arguably, etc.
? Auxiliaries
? may, might, would, should, etc.
? Verbs
? verbs with speculative content and their
passive forms: suggest, question, pre-
sume, suspect, indicate, suppose, seem,
appear, favor, etc.
? passive forms with dummy subjects: It
is claimed that . . . It has been men-
tioned . . . It is known . . .
? there is / there are constructions: There
is evidence/concern/indication that. . .
? Numerically vague expressions / quantifiers
? certain, numerous, many, most, some,
much, everyone, few, various, one group
of, etc. Experts say . . . Some people
think . . .More than 60% percent . . .
3
? Nouns
? speculation, proposal, consideration,
etc. Rumour has it that . . . Common
sense insists that . . .
However, the use of the above words or grammat-
ical devices does not necessarily entail their being
a weasel cue since their use may be justifiable in
their contexts.
As the main application goal of weasel detec-
tion is to highlight articles which should be im-
proved (by reformulating or adding factual is-
sues), we decided to annotate only weasel cues
in Wikipedia articles, but we did not mark their
scopes.
During the manual annotation process, the fol-
lowing cue marking principles were employed.
Complex verb phrases were annotated as weasel
cues since in some cases, both the passive con-
struction and the verb itself are responsible for the
weasel. In passive forms with dummy subjects and
there is / there are constructions, the weasel cue
included the grammatical subject (i.e. it and there)
as well. As for numerically vague expressions, the
noun phrase containing a quantifier was marked
as a weasel cue. If there was no quantifier (in the
case of a bare plural), the noun was annotated as
a weasel cue. Comparatives and superlatives were
annotated together with their article. Anaphoric
pronouns referring to a weasel word were also an-
notated as weasel cues.
4 Task Definitions
Two uncertainty detection tasks (sentence clas-
sification and in-sentence hedge scope detec-
tion) in two domains (biological publications and
Wikipedia articles) with three types of submis-
sions (closed, cross and open) were given to the
participants of the CoNLL-2010 Shared Task.
4.1 Detection of Uncertain Sentences
The aim of Task1 was to develop automatic proce-
dures for identifying sentences in texts which con-
tain unreliable or uncertain information. In par-
ticular, this task is a binary classification problem,
i.e. factual and uncertain sentences have to be dis-
tinguished.
As training and evaluation data
? Task1B: biological abstracts and full articles
(evaluation data contained only full articles)
from the BioScope corpus and
? Task1W: paragraphs from Wikipedia possi-
bly containing weasel information
were provided. The annotation of weasel/hedge
cues was carried out on the phrase level, and sen-
tences containing at least one cue were considered
as uncertain, while sentences with no cues were
considered as factual. The participating systems
had to submit a binary classification (certain vs.
uncertain) of the test sentences while marking cues
in the submissions was voluntary (but participants
were encouraged to do this).
4.2 In-sentence Hedge Scope Resolution
For Task2, in-sentence scope resolvers had to be
developed. The training and evaluation data con-
sisted of biological scientific texts, in which in-
stances of speculative spans ? that is, keywords
and their linguistic scope ? were annotated manu-
ally. Submissions to Task2 were expected to auto-
matically annotate the cue phrases and the left and
right boundaries of their scopes (exactly one scope
must be assigned to a cue phrase).
4.3 Evaluation Metrics
The evaluation for Task1 was carried out at the
sentence level, i.e. the cue annotations in the sen-
tence were not taken into account. The F?=1 mea-
sure (the harmonic mean of precision and recall)
of the uncertain class was employed as the chief
evaluation metric.
The Task2 systems were expected to mark cue-
and corresponding scope begin/end tags linked to-
gether by using some unique IDs. A scope-level
F?=1 measure was used as the chief evaluation
metric where true positives were scopes which ex-
actly matched the gold standard cue phrases and
gold standard scope boundaries assigned to the cue
word. That is, correct scope boundaries with in-
correct cue annotation and correct cue words with
bad scope boundaries were both treated as errors.
This scope-level metric is very strict. For in-
stance, the requirement of the precise match of the
cue phrase is questionable as ? from an application
point of view ? the goal is to find uncertain text
spans and the evidence for this is not so impor-
tant. However, the annotation of cues in datasets
is essential for training scope detectors since lo-
cating the cues usually precedes the identification
of their scope. Hence we decided to incorporate
cue matches into the evaluation metric.
4
Another questionable issue is the strict bound-
ary matching requirement. For example, includ-
ing or excluding punctuations, citations or some
bracketed expressions, like (see Figure 1) from
a scope is not crucial for an otherwise accurate
scope detector. On the other hand, the list of
such ignorable phenomena is arguable, especially
across domains. Thus, we considered the strict
boundary matching to be a straightforward and un-
ambiguous evaluation criterion. Minor issues like
those mentioned above could be handled by sim-
ple post-processing rules. In conclusion we think
that the uncertainty detection community may find
more flexible evaluation criteria in the future but
the strict scope-level metric is definitely a good
starting point for evaluation.
4.4 Closed and Open Challenges
Participants were invited to submit results in dif-
ferent configurations, where systems were allowed
to exploit different kinds of annotated resources.
The three possible submission categories were:
? Closed, where only the labeled and unla-
beled data provided for the shared task were
allowed, separately for each domain (i.e.
biomedical train data for biomedical test set
and Wikipedia train data for Wikipedia test
set). No further manually crafted resources
of uncertainty information (i.e. lists, anno-
tated data, etc.) could be used in any domain.
On the other hand, tools exploiting the man-
ual annotation of linguistic phenomena not
related to uncertainty (such as POS taggers
and parsers trained on labeled corpora) were
allowed.
? Cross-domain was the same as the closed one
but all data provided for the shared task were
allowed for both domains (i.e. Wikipedia
train data for the biomedical test set, the
biomedical train data for Wikipedia test set
or a union of Wikipedia and biomedical train
data for both test sets).
? Open, where any data and/or any additional
manually created information and resource
(which may be related to uncertainty) were
allowed for both domains.
The motivation behind the cross-domain and the
open challenges was that in this way, we could
assess whether adding extra (i.e. not domain-
specific) information to the systems can contribute
to the overall performance.
5 Datasets
Training and evaluation corpora were annotated
manually for hedge/weasel cues and their scope
by two independent linguist annotators. Any dif-
ferences between the two annotations were later
resolved by the chief annotator, who was also re-
sponsible for creating the annotation guidelines
and training the two annotators. The datasets
are freely available2 for further benchmark experi-
ments at http://www.inf.u-szeged.hu/
rgai/conll2010st.
Since uncertainty cues play an important role
in detecting sentences containing uncertainty, they
are tagged in the Task1 datasets as well to enhance
training and evaluation of systems.
5.1 Biological Publications
The biological training dataset consisted of the bi-
ological part of the BioScope corpus (Vincze et al,
2008), hence it included abstracts from the GE-
NIA corpus, 5 full articles from the functional ge-
nomics literature (related to the fruit fly) and 4 ar-
ticles from the open access BMC Bioinformatics
website. The automatic segmentation of the doc-
uments was corrected manually and the sentences
(14541 in number) were annotated manually for
hedge cues and their scopes.
The evaluation dataset was based on 15 biomed-
ical articles downloaded from the publicly avail-
able PubMedCentral database, including 5 ran-
dom articles taken from the BMC Bioinformat-
ics journal in October 2009, 5 random articles to
which the drosophila MeSH term was assigned
and 5 random articles having the MeSH terms
human, blood cells and transcription factor (the
same terms which were used to create the Genia
corpus). These latter ten articles were also pub-
lished in 2009. The aim of this article selection
procedure was to have a theme that was close to
the training corpus. The evaluation set contained
5003 sentences, out of which 790 were uncertain.
These texts were manually annotated for hedge
cues and their scope. To annotate the training and
the evaluation datasets, the same annotation prin-
ciples were applied.
2under the Creative Commons Attribute Share Alike li-
cense
5
For both Task1 and Task2, the same dataset was
provided, the difference being that for Task1, only
hedge cues and sentence-level uncertainty were
given, however, for Task2, hedge cues and their
scope were marked in the text.
5.2 Wikipedia Datasets
2186 paragraphs collected from Wikipedia
archives were also offered as Task1 training
data (11111 sentences containing 2484 uncertain
ones). The evaluation dataset contained 2346
Wikipedia paragraphs with 9634 sentences, out of
which 2234 were uncertain.
For the selection of the Wikipedia paragraphs
used to construct the training and evaluation
datasets, we exploited the weasel tags added by
the editors of the encyclopedia (marking unsup-
ported opinions or expressions of a non-neutral
point of view). Each paragraph containing weasel
tags (5874 different ones) was extracted from the
history dump of EnglishWikipedia. First, 438 ran-
domly selected paragraphs were manually anno-
tated from this pool then the most frequent cue
phrases were collected. Later on, two other sets
of Wikipedia paragraphs were gathered on the ba-
sis of whether they contained such cue phrases or
not. The aim of this sampling procedure was to
provide large enough training and evaluation sam-
ples containing weasel words and also occurrences
of typical weasel words in non-weasel contexts.
Each sentence was annotated manually for
weasel cues. Sentences were treated as uncer-
tain if they contained at least one weasel cue, i.e.
the scope of weasel words was the entire sentence
(which is supposed to be rewritten by Wikipedia
editors).
5.3 Unlabeled Data
Unannotated but pre-processed full biological arti-
cles (150 articles from the publicly available Pub-
MedCentral database) and 1 million paragraphs
from Wikipedia were offered to the participants as
well. These datasets did not contain any manual
annotation for uncertainty, but their usage permit-
ted data sampling from a large pool of in-domain
texts without time-wasting pre-processing tasks
(cleaning and sentence splitting).
5.4 Data Format
Both training and evaluation data were released
in a custom XML format. For each task, a sep-
arate XML file was made available containing the
whole document set for the given task. Evaluation
datasets were available in the same format as train-
ing data without any sentence-level certainty, cue
or scope annotations.
The XML format enabled us to provide more
detailed information about the documents such as
segment boundaries and types (e.g. section titles,
figure captions) and it is the straightforward for-
mat to represent nested scopes. Nested scopes
have overlapping text spans which may contain
cues for multiple scopes (there were 1058 occur-
rences in the training and evaluation datasets to-
gether). The XML format utilizes id-references
to determine the scope of a given cue. Nested
constructions are rather complicated to represent
in the standard IOB format, moreover, we did not
want to enforce a uniform tokenization.
To support the processing of the data files,
reader and writer software modules were devel-
oped and offered to the participants for the uCom-
pare (Kano et al, 2009) framework. uCompare
provides a universal interface (UIMA) and several
text mining and natural language processing tools
(tokenizers, POS taggers, syntactic parsers, etc.)
for general and biological domains. In this way
participants could configure and execute a flexible
chain of analyzing tools even with a graphical UI.
6 Submissions and Results
Participants uploaded their results through the
shared task website, and the official evaluation was
performed centrally. After the evaluation period,
the results were published for the participants on
the Web. A total of 23 teams participated in the
shared task. 22, 16 and 13 teams submitted output
for Task1B, Task1W and Task2, respectively.
6.1 Results
Tables 1, 2 and 3 contain the results of the submit-
ted systems for Task1 and Task2. The last name
of the first author of the system description pa-
per (published in these proceedings) is used here
as a system name3. The last column contains the
type of submission. The system of Kilicoglu and
Bergler (2010) is the only open submission. They
adapted their system introduced in Kilicoglu and
Bergler (2008) to the datasets of the shared task.
Regarding cross submissions, Zhao et al (2010)
and Ji et al (2010) managed to achieve a no-
ticeable improvement by exploiting cross-domain
3O?zgu?r did not publish a description of her system.
6
Name P / R / F type
Georgescul 72.0 / 51.7 / 60.2 C
Ji 62.7 / 55.3 / 58.7 X
Chen 68.0 / 49.7 / 57.4 C
Morante 80.6 / 44.5 / 57.3 C
Zhang 76.6 / 44.4 / 56.2 C
Zheng 76.3 / 43.6 / 55.5 C
Ta?ckstro?m 78.3 / 42.8 / 55.4 C
Mamani Sa?nchez 68.3 / 46.2 / 55.1 C
Tang 82.3 / 41.4 / 55.0 C
Kilicoglu 67.9 / 46.0 / 54.9 O
Tjong Kim Sang 74.0 / 43.0 / 54.4 C
Clausen 75.1 / 42.0 / 53.9 C
O?zgu?r 59.4 / 47.9 / 53.1 C
Zhou 85.3 / 36.5 / 51.1 C
Li 88.4 / 31.9 / 46.9 C
Prabhakaran 88.0 / 28.4 / 43.0 C
Ji 94.2 / 6.6 / 12.3 C
Table 1: Task1 Wikipedia results (type ?
{Closed(C), Cross(X), Open(O)}).
data. Zhao et al (2010) extended the biological
cue word dictionary of their system ? using it as
a feature for classification ? by the frequent cues
of the Wikipedia dataset, while Ji et al (2010)
used the union of the two datasets for training
(they have reported an improvement from 47.0 to
58.7 on the Wikipedia evaluation set after a post-
challenge bugfix).
Name P / R / F type
Morante 59.6 / 55.2 / 57.3 C
Rei 56.7 / 54.6 / 55.6 C
Velldal 56.7 / 54.0 / 55.3 C
Kilicoglu 62.5 / 49.5 / 55.2 O
Li 57.4 / 47.9 / 52.2 C
Zhou 45.6 / 43.9 / 44.7 O
Zhou 45.3 / 43.6 / 44.4 C
Zhang 46.0 / 42.9 / 44.4 C
Fernandes 46.0 / 38.0 / 41.6 C
Vlachos 41.2 / 35.9 / 38.4 C
Zhao 34.8 / 41.0 / 37.7 C
Tang 34.5 / 31.8 / 33.1 C
Ji 21.9 / 17.2 / 19.3 C
Ta?ckstro?m 2.3 / 2.0 / 2.1 C
Table 2: Task2 results (type ? {Closed(C),
Open(O)}).
Each Task2 and Task1W system achieved a
Name P / R / F type
Tang 85.0 / 87.7 / 86.4 C
Zhou 86.5 / 85.1 / 85.8 C
Li 90.4 / 81.0 / 85.4 C
Velldal 85.5 / 84.9 / 85.2 C
Vlachos 85.5 / 84.9 / 85.2 C
Ta?ckstro?m 87.1 / 83.4 / 85.2 C
Shimizu 88.1 / 82.3 / 85.1 C
Zhao 83.4 / 84.8 / 84.1 X
O?zgu?r 77.8 / 91.3 / 84.0 C
Rei 83.8 / 84.2 / 84.0 C
Zhang 82.6 / 84.7 / 83.6 C
Kilicoglu 92.1 / 74.9 / 82.6 O
Morante 80.5 / 83.3 / 81.9 X
Morante 81.1 / 82.3 / 81.7 C
Zheng 73.3 / 90.8 / 81.1 C
Tjong Kim Sang 74.3 / 87.1 / 80.2 C
Clausen 79.3 / 80.6 / 80.0 C
Szidarovszky 70.3 / 91.0 / 79.3 C
Georgescul 69.1 / 91.0 / 78.5 C
Zhao 71.0 / 86.6 / 78.0 C
Ji 79.4 / 76.3 / 77.9 C
Chen 74.9 / 79.1 / 76.9 C
Fernandes 70.1 / 71.1 / 70.6 C
Prabhakaran 67.5 / 19.5 / 30.3 X
Table 3: Task1 biological results (type ?
{Closed(C), Cross(X), Open(O)}).
higher precision than recall. There may be two
reasons for this. The systems may have applied
only reliable patterns, or patterns occurring in the
evaluation set may be imperfectly covered by the
training datasets. The most intense participation
was on Task1B. Here, participants applied vari-
ous precision/recall trade-off strategies. For in-
stance, Tang et al (2010) achieved a balanced pre-
cision/recall configuration, while Li et al (2010)
achieved third place thanks to their superior preci-
sion.
Tables 4 and 5 show the cue-level performances,
i.e. the F-measure of cue phrase matching where
true positives were strict matches. Note that it was
optional to submit cue annotations for Task1 (if
participants submitted systems for both Task2 and
Task1B with cue tagging, only the better score of
the two was considered).
It is interesting to see that Morante et al (2010)
who obtained the best results on Task2 achieved
a medium-ranked F-measure on the cue-level (e.g.
their result on the cue-level is lower by 4% com-
7
pared to Zhou et al (2010), while on the scope-
level the difference is 13% in the reverse direc-
tion), which indicates that the real strength of the
system of Morante et al (2010) is the accurate de-
tection of scope boundaries.
Name P / R / F
Tang 63.0 / 25.7 / 36.5
Li 76.1 / 21.6 / 33.7
O?zgu?r 28.9 / 14.7 / 19.5
Morante 24.6 / 7.3 / 11.3
Table 4: Wikipedia cue-level results.
Name P / R / F type
Tang 81.7 / 81.0 / 81.3 C
Zhou 83.1 / 78.8 / 80.9 C
Li 87.4 / 73.4 / 79.8 C
Rei 81.4 / 77.4 / 79.3 C
Velldal 81.2 / 76.3 / 78.7 C
Zhang 82.1 / 75.3 / 78.5 C
Ji 78.7 / 76.2 / 77.4 C
Morante 78.8 / 74.7 / 76.7 C
Kilicoglu 86.5 / 67.7 / 76.0 O
Vlachos 82.0 / 70.6 / 75.9 C
Zhao 76.7 / 73.9 / 75.3 X
Fernandes 79.2 / 64.7 / 71.2 C
Zhao 63.7 / 74.1 / 68.5 C
Ta?ckstro?m 66.9 / 58.6 / 62.5 C
O?zgu?r 49.1 / 57.8 / 53.1 C
Table 5: Biological cue-level results (type ?
{Closed(C), Cross(X), Open(O)}).
6.2 Approaches
The approaches to Task1 fall into two major cat-
egories. There were six systems which handled
the task as a classical sentence classification prob-
lem and employed essentially a bag-of-words fea-
ture representation (they are marked as BoW in
Table 6). The remaining teams focused on the
cue phrases and sought to classify every token if
it was a part of a cue phrase, then a sentence was
predicted as uncertain if it contained at least one
recognized cue phrase. Five systems followed a
pure token classification approach (TC) for cue de-
tection while others used sequential labeling tech-
niques (usually Conditional Random Fields) to
identify cue phrases in sentences (SL).
The feature set employed in Task1 systems typ-
ically consisted of the wordform, its lemma or
stem, POS and chunk codes and about the half of
the participants constructed features from the de-
pendency and/or constituent parse tree of the sen-
tences as well (see Table 6 for details).
It is interesting to see that the top ranked sys-
tems of Task1B followed a sequence labeling ap-
proach, while the best systems on Task1W applied
a bag-of-words sentence classification. This may
be due to the fact that biological sentences have
relatively simple patterns. Thus the context of the
cue words (token classification-based approaches
used features derived from a window of the token
in question, thus, they exploited the relationship
among the tokens and their contexts) can be uti-
lized while Wikipedia weasels have a diverse na-
ture. Another observation is that the top systems
in both Task1B and Task1W are the ones which
did not derive features from syntactic parsing.
Each Task2 system was built upon a Task1 sys-
tem, i.e. they attempted to recognize the scopes
for the predicted cue phrases (however, Zhang et
al. (2010) have argued that the objective functions
of Task1 and Task2 cue detection problems are
different because of sentences containing multiple
hedge spans).
Most systems regarded multiple cues in a sen-
tence to be independent from each other and
formed different classification instances from
them. There were three systems which incor-
porated information about other hedge cues (e.g.
their distance) of the sentence into the feature
space and Zhang et al (2010) constructed a cas-
cade system which utilized directly the predicted
scopes (it processes cue phrases from left to right)
during predicting other scopes in the same sen-
tence.
The identification of the scope for a certain cue
was typically carried out by classifying each to-
ken in the sentence. Task2 systems differ in the
number of class labels used as target and in the
machine learning approaches applied. Most sys-
tems ? following Morante and Daelemans (2009)
? used three class labels (F)IRST, (L)AST and
NONE. Two participants used four classes by
adding (I)NSIDE, while three systems followed
a binary classification approach (SCOPE versus
NONSCOPE). The systems typically included a
post-processing procedure to force scopes to be
continuous and to include the cue phrase in ques-
tion. The machine learning methods applied can
be again categorized into sequence labeling (SL)
8
NA
ME
ap
pro
ach
ma
ch
ine
fea
tur
e
fea
tur
es
em
plo
ye
d
lea
rne
r
sel
ect
ion
dic
t
ort
ho
lem
ma
/st
em
PO
S
ch
un
k
de
p
do
cp
art
oth
er
Cl
au
sen
Bo
W
Ma
xE
nt
+
+
he
dg
ec
ue
dis
tan
ce
Ch
en
Bo
W
Ma
xE
nt
sta
tis
tic
al
+
+
+
+
sen
ten
cel
en
gth
Fe
rna
nd
es
SL
ET
L
+
+
+
Ge
org
esc
ul
Bo
W
SV
M+
pa
ram
tun
ing
+
Ji
TC
Mo
dA
vg
Pe
rce
ptr
on
+
Ki
lic
og
lu
TC
ma
nu
al
+
+
+
ex
ter
na
ld
ict
Li
SL
CR
F+
po
stp
roc
gre
ed
yf
wd
+
+
+
Ma
ma
ni
Sa?
nc
he
z
Bo
W
SV
MT
ree
Ke
rne
l
+
+
+
+
+
Mo
ran
te
(w
iki
)
TC
SV
M+
po
stp
roc
sta
tis
tic
al
+
+
+
+
+
Mo
ran
te
(bi
o)
SL
KN
N
sta
tis
tic
al
+
+
+
+
+
+
Pra
bh
ak
ara
n
SL
CR
F
gre
ed
yf
wd
+
+
+
+
Le
vin
Cl
ass
Re
i
SL
CR
F
+
+
+
+
Sh
im
izu
SL
Ba
ye
sP
oin
tM
ach
ine
s
GA
+
+
+
+
NE
s,u
nla
be
led
da
ta
Sz
ida
rov
szk
y
SL
CR
F
ex
ha
ust
ive
+
+
+
Ta?
ck
str
o?m
Bo
W
SV
M
gre
ed
yf
wd
+
+
+
+
+
sen
ten
cel
en
gth
Ta
ng
SL
CR
F,S
VM
HM
M
sta
tis
tic
al
+
+
+
+
+
Tjo
ng
Ki
m
Sa
ng
TC
Na
ive
Ba
ye
s
Ve
lld
al
TC
Ma
xE
nt
ma
nu
al
+
+
+
+
Vl
ach
os
TC
Ba
ye
sia
nL
og
Re
g
ma
nu
al
+
+
+
+
Zh
an
g
SL
CR
F+
fea
tur
ec
om
bin
ati
on
gre
ed
yf
wd
+
+
+
+
+
NE
s
Zh
ao
SL
CR
F
sta
tis
tic
al
+
+
+
+
Zh
en
g
SL
CR
F,M
ax
En
t
ma
nu
al
+
+
+
+
Co
nst
itu
en
tP
ars
ing
Zh
ou
SL
CR
F
sta
tis
tic
al
+
+
+
+
Wo
rdN
et
Ta
ble
6:
Sy
ste
m
arc
hit
ect
ure
so
ve
rvi
ew
for
Ta
sk1
.A
pp
roa
ch
es:
seq
ue
nc
el
ab
eli
ng
(SL
),t
ok
en
cla
ssi
fic
ati
on
(T
C)
,b
ag
-of
-w
ord
sm
od
el
(B
oW
);M
ach
ine
lea
rne
rs:
En
tro
py
Gu
ide
dT
ran
sfo
rm
ati
on
Le
arn
ing
(E
TL
),A
ve
rag
ed
Pe
rce
ptr
on
(A
P),
k-n
ear
est
ne
igh
bo
ur
(K
NN
);F
eat
ure
sel
ect
ion
:g
ath
eri
ng
ph
ras
es
fro
m
the
tra
ini
ng
co
rpu
su
sin
gs
tat
ist
ica
lth
res
ho
lds
(st
ati
sti
cal
);F
eat
ure
s:
ort
ho
gra
ph
ica
lin
for
ma
tio
na
bo
ut
the
tok
en
(or
tho
),l
em
ma
or
ste
m
of
the
tok
en
(st
em
),P
art
-of
-Sp
eec
h
co
de
s(
PO
S),
syn
tac
tic
ch
un
ki
nfo
rm
ati
on
(ch
un
k),
de
pe
nd
en
cy
pa
rsi
ng
(de
p),
po
sit
ion
ins
ide
the
do
cu
me
nt
or
sec
tio
ni
nfo
rm
ati
on
(do
cp
os)
an
dt
ok
en
cla
ssi
fic
ati
on
(T
C)
ap
pro
ach
es
(se
eT
ab
le
7).
Th
ef
eat
ure
set
su
sed
he
re
are
the
sam
ea
sf
or
Ta
sk1
,e
xte
nd
ed
by
sev
era
lf
eat
ure
sd
esc
rib
ing
the
rel
ati
on
shi
pb
etw
een
the
cu
ep
hra
se
an
dt
he
tok
en
in
qu
est
ion
mo
stl
yb
yd
esc
rib
ing
the
de
pe
nd
en
cy
pa
th
be
tw
een
the
m.
9
NAME approach scope ML postproc tree dep multihedge
Fernandes TC FL ETL
Ji TC I AP +
Kilicoglu HC manual + + +
Li SL FL CRF, SVMHMM + + +
Morante TC FL KNN + +
Rei SL FIL manual+CRF + +
Ta?ckstro?m TC FI SVM +
Tang SL FL CRF + + +
Velldal HC manual +
Vlachos TC I Bayesian MaxEnt + +
Zhang SL FIL CRF + +
Zhao SL FL CRF +
Zhou SL FL CRF + +
Table 7: System architectures overview for Task2. Approaches: sequence labeling (SL), token clas-
sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning
(ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predict-
ing first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system
applied a mechanism for handling multiple hedges inside a sentence
and token classification (TC) approaches (see Ta-
ble 7). The feature sets used here are the same
as for Task1, extended by several features describ-
ing the relationship between the cue phrase and the
token in question mostly by describing the depen-
dency path between them.
7 Conclusions
The CoNLL-2010 Shared Task introduced the
novel task of uncertainty detection. The challenge
consisted of a sentence identification task on un-
certainty (Task1) and an in-sentence hedge scope
detection task (Task2). In the latter task the goal
of automatic systems was to recognize speculative
text spans inside sentences.
The relatively high number of participants in-
dicates that the problem is rather interesting for
the Natural Language Processing community. We
think that this is due to the practical importance
of the task for (principally biomedical) applica-
tions and because it addresses several open re-
search questions. Although several approaches
were introduced by the participants of the shared
task and we believe that the ideas described in
this proceedings can serve as an excellent starting
point for the development of an uncertainty de-
tector, there is a lot of room for improving such
systems. The manually annotated datasets and
software tools developed for the shared task may
act as benchmarks for these future experiments
(they are freely available at http://www.inf.
u-szeged.hu/rgai/conll2010st).
Acknowledgements
The authors would like to thank Joakim Nivre
and Llu??s Ma?rquez for their useful suggestions,
comments and help during the organisation of the
shared task.
This work was supported in part by the
National Office for Research and Technol-
ogy (NKTH, http://www.nkth.gov.hu/)
of the Hungarian government within the frame-
work of the projects TEXTREND, BELAMI and
MASZEKER.
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185?192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Wendy W. Chapman, David Chu, and John N. Dowl-
ing. 2007. ConText: An Algorithm for Identifying
Contextual Features from Clinical Text. In Proceed-
ings of the ACL Workshop on BioNLP 2007, pages
81?88.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
10
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142?143, Boulder, Colorado,
June. Association for Computational Linguistics.
Carol Friedman, Philip O. Alderson, John H. M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A General Natural-language Text Processor
for Clinical Radiology. Journal of the American
Medical Informatics Association, 1(2):161?174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. De-
tecting Hedge Cues and their Scopes with Average
Perceptron. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 139?146,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Yoshinobu Kano, William A. Baumgartner, Luke
McCrohon, Sophia Ananiadou, Kevin B. Cohen,
Lawrence Hunter, and Jun?ichi Tsujii. 2009. U-
Compare: Share and Compare Text Mining Tools
with UIMA. Bioinformatics, 25(15):1997?1998,
August.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46?53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syn-
tactic Dependency Based Heuristics for Biological
Event Extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 119?127, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and Their
Scopes. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 103?110, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
George Lakoff. 1972. Linguistics and natural logic.
In The Semantics of Natural Language, pages 545?
665, Dordrecht. Reidel.
Xinxin Li, Jianping Shen, Xiang Gao, and Xuan
Wang. 2010. Exploiting Rich Features for Detect-
ing Hedges and Their Scope. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning (CoNLL-2010): Shared Task,
pages 36?41, Uppsala, Sweden, July. Association
for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Proceed-
ings of the HLT-NAACL 2004 Workshop: Biolink
2004, Linking Biological Literature, Ontologies and
Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Roser Morante andWalter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based Resolution of In-
sentence Scopes of Hedge Cues. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 48?55, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. De-
tecting Speculations and their Scopes in Scientific
Text. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1398?1407, Singapore, August. Associ-
ation for Computational Linguistics.
Gyo?rgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for De-
tecting Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 25?29, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2009. Analyzing Text in
Search of Bio-molecular Events: A High-precision
Machine Learning Framework. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
11
Shared Task, pages 128?136, Boulder, Colorado,
June. Association for Computational Linguistics.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Shaodian Zhang, Hai Zhao, Guodong Zhou, and Bao-
liang Lu. 2010. Hedge Detection and Scope Find-
ing by Sequence Labeling with Procedural Feature
Selection. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 70?77, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong
Cheng. 2010. Learning to Detect Hedges and their
Scope Using CRF. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 64?
69, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li,
and Yuansheng Yang. 2010. Exploiting Multi-
Features to Detect Hedges and Their Scope in
Biomedical Texts. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 56?
63, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
12
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 28?31,
Uppsala, July 2010.
Speculation and negation annotation in natural language texts: what the
case of BioScope might (not) reveal
Veronika Vincze
University of Szeged
Szeged, Hungary
vinczev@inf.u-szeged.hu
1 Introduction
In information extraction, it is of key impor-
tance to distinguish between facts and uncertain
or negated information. In other words, IE appli-
cations have to treat sentences / clauses containing
uncertain or negated information differently from
factual information that is why the development of
hedge and negation detection systems has received
much interest ? e.g. the objective of the CoNLL-
2010 Shared Task was also to develop hedge de-
tection systems (Farkas et al, 2010). For the train-
ing and evaluation of such systems, corpora anno-
tated for negation and speculation are necessary.
There are several linguistic phenomena that can
be grouped under the term uncertainty. Besides
hedge and speculation, doubtful events are also
considered as a subtype of uncertainty (Kim et al,
2008) and Ganter and Strube (2009) argue that the
notion of weasel words are similar to hedges. A
word is considered to be a weasel word if it creates
an impression that something important has been
said, but what is really communicated is vague,
misleading, evasive or ambiguous, thus, it is also
related to uncertainty. All these phenomena might
be of interest for IE applications, which yields that
the creation of corpora with uncertainty annotation
is indispensable.
2 Related work
There exist some corpora that contain annota-
tion for speculation and/or negation. The GE-
NIA Event corpus (Kim et al, 2008) annotates
biological events with negation and two types of
uncertainty. In the BioInfer corpus (Pyysalo et
al., 2007) biological relations are annotated for
negation. The system developed by Medlock and
Briscoe (2007) made use of a corpus consisting
of six papers from genomics literature in which
sentences were annotated for speculation. Set-
tles et al (2008) constructed a corpus where sen-
tences are classified as either speculative or defi-
nite, however, no keywords are marked in the cor-
pus and Shatkay et al (2008) describe a database
where sentences are annotated for certainty among
other features. As a corpus specifically annotated
for weasel words, WikiWeasel should be men-
tioned, which was constructed for the CoNLL-
2010 Shared Task (Farkas et al, 2010) and con-
tains Wikipedia paragraphs annotated for weasel
words.
3 The BioScope corpus
The BioScope corpus (Vincze et al, 2008) is ? to
our best knowledge ? the largest corpus available
that is annotated for both negation and hedge key-
words and the only one that contains annotation
for linguistic scopes. It includes three types of
texts from the biomedical domain ? namely, radio-
logical reports, biological full papers and abstracts
from the GENIA corpus. (15 new full biomedi-
cal papers were annotated for hedge cues and their
scopes, which served as the evaluation database
of the CoNLL-2010 Shared Task (Farkas et al,
2010), and this dataset will be added to BioScope
in the near future.) The annotation was carried out
by two students of linguistics supervised by a lin-
guist. Problematic cases were continuously dis-
cussed among the annotators and dissimilar anno-
tations were later resolved by the linguist.
3.1 Annotation principles
In BioScope, speculation is understood as the pos-
sible existence of a thing is claimed ? neither its
existence nor its non-existence is known for sure.
Only one level of uncertainty is marked (as op-
posed to the GENIA corpus (Kim et al, 2008) or
Shatkay et al (2008)) and no weasels are anno-
tated. Negation is seen as the implication of non-
existence of something.
The annotation was based on four basic princi-
ples:
28
? Each keyword has a scope.
? The scope must include its keyword.
? Min-max strategy:
? The minimal unit expressing
hedge/negation is marked as keyword.
? The scope is extended to the maximal
syntactic unit.
? No intersecting scopes are allowed.
These principles were determined at the very
beginning of the annotation process and they were
strictly followed throughout the corpus building.
3.2 Problematic cases
However, in some cases, some language phenom-
ena seemed to contradict the above principles.
These issues required a thorough consideration of
the possible solutions in accordance with the basic
principles in order to keep the annotation of the
corpus as consistent as possible. The most notable
examples include the following:
? Negative keywords without scope:
[Negative] chest radiograph.
In this case, the scope contains only the key-
word.
? Elliptic sentences
Moreover, ANG II stimulated NF-
kappaB activation in human mono-
cytes, but [not] in lymphocytes
from the same preparation.
With the present encoding scheme of scopes,
there is no way to signal that the negation
should be extended to the verb and the object
as well.
? Nested scopes
One scope includes another one:
These observations (suggest that
TNF and PMA do (not lead to NF-
kappa B activation through induc-
tion of changes in the cell redox
status)).
The semantic interpretation of such nested
scopes should be understood as ?it is possi-
ble that there is no such an event that. . . ?.
? Elements in between keyword and target
word
Although however is not affected by the
hedge cue in the following example, it is in-
cluded in the scope since consecutive text
spans are annotated as scopes:
(Atelectasis in the right mid zone
is, however, <possible>).
? Complex keywords
Sometimes a hedge / negation is expressed
via a phrase rather than a single word: these
are marked as complex keywords.
? Inclusion of modifiers and adjuncts
It is often hard to decide whether a modifier
or adjunct belongs to the scope or not. In or-
der not to lose potentially important informa-
tion, the widest scope possible is marked in
each case.
? Intersecting scopes
When two keywords occur within one sen-
tence, their scopes might intersect, yielding
one apparently empty scope (i.e. scope with-
out keyword) and a scope with two keywords:
(Repression did ([not] <seem> to
involve another factor whose activ-
ity is affected by the NSAIDs)).
In such cases, one of the scopes (usually the
negative one) was extended:
((Repression did [not] <seem> to
involve another factor whose activ-
ity is affected by the NSAIDs)).
On the other hand, there were some cases where
the difficulty of annotation could be traced back to
lexical issues. Some of the keyword candidates
have several senses (e.g. if ) or can be used in dif-
ferent grammatical structures (e.g. indicate vs. in-
dicate that) and not all of them are to be marked
as a keyword in the corpus. Thus, senses / usages
to be annotated and those not to be annotated had
to be determined precisely.
Finally, sometimes an apparently negative key-
word formed part of a complex hedge keyword
(e.g. cannot be excluded), which refers to the
fact that speculation can be expressed also by a
negated word, thus, the presence of a negative
word does not automatically entail that the sen-
tence is negated.
29
4 Outlook: Comparison with other
corpora
Besides BioScope, the GENIA Event corpus (Kim
et al, 2008) also contains annotation for negation
and speculation. In order to see what the main dif-
ferences are between the corpora, the annotation
principles were contrasted:
? in GENIA Event, no modifier keywords are
marked, however, in BioScope, they are;
? the scope of speculation and negation is ex-
plicitly marked in BioScope and it can be
extended to various constituents within the
clause / sentence though in GENIA Event, it
is the event itself that is within the scope;
? two subtypes of uncertainty are distinguished
in GENIA Event: doubtful and probable,
however, in BioScope there is one umbrella
term for them (speculation).
An essential difference in annotation principles
between the two corpora is that GENIA Event fol-
lows the principles of event-centered annotation
while BioScope annotation does not put special
emphasis on events. Event-centered annotation
means that annotators are required to identify as
many events as possible within the sentence then
label each separately for negation / speculation.
The multiplicity of events in GENIA and the
maximum scope principle exploited in BioScope
(see 3.1) taken together often yields that a GENIA
event falls within the scope of a BioScope key-
word, however, it should not be seen as a specu-
lated or negated event on its own. Here we provide
an illustrative example:
In summary, our data suggest that
changes in the composition of tran-
scription factor AP-1 is a key molecu-
lar mechanism for increasing IL-2 tran-
scription and may underlie the phe-
nomenon of costimulation by EC.
According to the BioScope analysis of the sen-
tence, the scope of suggest extends to the end of
the sentence. It entails that in GENIA it is only
the events is a key molecular mechanism and un-
derlie the phenomenon that are marked as proba-
ble, nevertheless, the events changes, increasing,
transcription and costimulation are also included
in the BioScope speculative scope. Thus, within
this sentence, there are six GENIA events out of
which two are labeled as probable, however, in
BioScope, all six are within a speculative scope.
In some cases, there is a difference in between
what is seen as speculative / negated in the cor-
pora. For instance, negated ?investigation? verbs
in Present Perfect are seen as doubtful events in
GENIA and as negative events in BioScope:
However, a role for NF-kappaB in hu-
man CD34(+) bone marrow cells has not
been described.
According to GENIA annotation principles, the
role has not been described, therefore it is doubt-
ful what the role exactly is. However, in BioScope,
the interpretation of the sentence is that there has
not been such an event that the role for NF-kappaB
in human CD34(+) bone marrow cells has been de-
scribed. Thus, it is marked as negative.
Another difference between the annotation
schemes of BioScope and GENIA is that instances
of weaseling are annotated as probable events in
GENIA, however, in BioScope they are not. An
example for a weasel sentence is shown below:
Receptors for leukocyte chemoattrac-
tants, including chemokines, are tradi-
tionally considered to be responsible for
the activation of special leukocyte func-
tions such as chemotaxis, degranulation,
and the release of superoxide anions.
5 Conclusions
Some interesting conclusions can be drawn from
the difficulties encountered during annotation pro-
cess of the BioScope corpus. As for method-
ology, it is unquestionable that precisely defined
rules (on scope marking, keyword marking and on
the interpretation of speculation / negation) are es-
sential for consistent annotation, thus, pre-defined
guidelines can help annotation work a lot. How-
ever, difficulties or ambiguities not seen previ-
ously may emerge (and they really do) only dur-
ing the process of annotation. In this way, a con-
tinuous reformulation and extension of annotation
rules is required based on the corpus data. On the
other hand, problematic issues sometimes might
be solved in several different ways. When decid-
ing on their final treatment, an ideal balance be-
tween gain and loss should be reached, in other
words, the min-max strategy as a basic annotation
30
principle can also be applied here (minimize the
loss and maximize the gain that the solution can
provide).
Acknowledgments
This work was supported in part by the National
Office for Research and Technology (NKTH,
http://www.nkth.gov.hu/) of the Hun-
garian government within the framework of the
project MASZEKER.
References
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(Suppl 10).
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj?orne, Jorma Boberg, Jouni J?arvinen, and Tapio
Salakoski. 2007. BioInfer: a corpus for information
extraction in the biomedical domain. BMC Bioin-
formatics, 8(50).
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS Workshop on Cost-Sensitive
Learning, pages 1?10.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and
W. John Wilbur. 2008. Multi-dimensional classifi-
cation of biomedical text: Toward automated, prac-
tical provision of high-utility text to diverse users.
Bioinformatics, 24(18):2086?2093.
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,
Gy?orgy M?ora, and J?anos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
31
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 116?121,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Detecting noun compounds and light verb constructions: a contrastive study
Veronika Vincze1, Istva?n Nagy T.2 and Ga?bor Berend2
1Hungarian Academy of Sciences, Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
2Department of Informatics, University of Szeged
{nistvan,berendg}@inf.u-szeged.hu
Abstract
In this paper, we describe our methods to
detect noun compounds and light verb con-
structions in running texts. For noun com-
pounds, dictionary-based methods and POS-
tagging seem to contribute most to the per-
formance of the system whereas for light
verb constructions, the combination of POS-
tagging, syntactic information and restrictions
on the nominal and verbal component yield
the best result. However, focusing on deverbal
nouns proves to be beneficial for both types
of MWEs. The effect of syntax is negligible
on noun compound detection whereas it is un-
ambiguously helpful for identifying light verb
constructions.
1 Introduction
Multiword expressions are lexical items that can be
decomposed into single words and display idiosyn-
cratic features (Sag et al, 2002; Calzolari et al,
2002; Kim, 2008). They are frequent in language
use and they usually exhibit unique and idiosyn-
cratic behavior, thus, they often pose a problem to
NLP systems. A compound is a lexical unit that
consists of two or more elements that exist on their
own. Light verb constructions are verb and noun
combinations in which the verb has lost its meaning
to some degree and the noun is used in one of its
original senses (e.g. have a walk or give advice).
In this work, we aim at identifying nominal com-
pounds and light verb constructions by using rule-
based methods. Noun compounds belong to the
most frequent MWE-classes (in the Wikipedia cor-
pus we developed for evaluation (see 3.2), about
75% of the annotated multiword expressions were
noun compounds) and they are productive, i.e. new
nominal compounds are being formed in language
use all the time, which yields that they cannot be
listed exhaustively in a dictionary (as opposed to
e.g. prepositional compounds). Their inner syntactic
structure varies: they can contain nouns, adjectives
and prepositions as well.
Light verb constructions are semi-productive, that
is, new light verb constructions might enter the lan-
guage following some patterns (e.g. give a Skype
call on the basis of give a call). On the other hand,
they are less frequent in language use (only 9.5% of
multiword expressions were light verb constructions
in the Wikipedia database) and they are syntactically
flexible, that is, they can manifest in various forms:
the verb can be inflected, the noun can occur in its
plural form and the noun can be modified. The nom-
inal and the verbal component may not even be ad-
jacent in e.g. passive sentences.
Our goal being to compare how different ap-
proaches perform in the case of the different types
of multiword expressions, we have chosen these two
types of MWEs that are dissimilar in several aspects.
2 Related work
There are several applications developed for identi-
fying MWEs, which can be classified according to
the methods they make use of (Piao et al, 2003).
First, statistical models rely on word frequencies,
co-occurrence data and contextual information in
deciding whether a bigram or trigram (or even an
n-gram) of words can be labeled as a multiword ex-
pression or not. Such systems are used for several
116
languages and several types of multiword expres-
sions, see e.g. Bouma (2010). The advantage of
statistical systems is that they can be easily adapted
to other languages and other types of multiword ex-
pressions, however, they are not able to identify rare
multiword expressions (as Piao et al (2003) empha-
size, 68% of multiword expressions occur at most
twice in their corpus).
Some hybrid systems make use of both statisti-
cal and linguistic information as well, that is, rules
based on syntactic or semantic regularities are also
incorporated into the system (Evert and Kermes,
2003; Bannard, 2007; Cook et al, 2007; Al-Haj and
Wintner, 2010). This results in better coverage of
multiword expressions. On the other hand, these
methods are highly language-dependent because of
the amount of linguistic rules encoded, thus, it re-
quires much effort to adapt them to different lan-
guages or even to different types of multiword ex-
pressions. However, the combination of different
methods may improve the performance of MWE-
extracting systems (Pecina, 2010).
Several features are used in identifying multi-
word expressions, which are applicable to differ-
ent types of multiword expressions to various de-
grees. Co-occurrence statistics and POS-tags seem
to be useful for all types of multiword expressions,
for instance the tool mwetoolkit (Ramisch et al,
2010a) makes use of such features, which is illus-
trated through the example of identifying English
compound nouns (Ramisch et al, 2010b).
Caseli et al (2010) developed an alignment-based
method for extracting multiword expressions from
parallel corpora. This method is also applied to
the pediatrics domain (Caseli et al, 2009). Zarrie?
and Kuhn (2009) argue that multiword expressions
can be reliably detected in parallel corpora by using
dependency-parsed, word-aligned sentences. Sinha
(2009) detects Hindi complex predicates (i.e. a com-
bination of a light verb and a noun, a verb or an ad-
jective) in a Hindi?English parallel corpus by iden-
tifying a mismatch of the Hindi light verb meaning
in the aligned English sentence. Van de Cruys and
Moiro?n (2007) describe a semantic-based method
for identifying verb-preposition-noun combinations
in Dutch, which relies on selectional preferences for
both the noun and the verb. Cook et al (2007) dif-
ferentiate between literal and idiomatic usages of
verb and noun constructions in English. They make
use of syntactic fixedness of idioms when develop-
ing their unsupervised method. Bannard (2007) also
seeks to identify verb and noun constructions in En-
glish on the basis of syntactic fixedness. Samardz?ic?
and Merlo (2010) analyze English and German light
verb constructions in parallel corpora. They found
that linguistic features (i.e. the degree of composi-
tionality) and the frequency of the construction both
have an effect on aligning the constructions.
3 Experiments
In order to identify multiword expressions, simple
methods are worth examining, which can serve as a
basis for implementing more complex systems and
can be used as features in machine learning set-
tings. Our aim being to compare the effect of dif-
ferent methods on the identification of noun com-
pounds and light verb constructions, we considered
it important to develop methods for both MWE types
that make use of their characteristics and to adapt
those methods to the other type of MWE ? in this
way, the efficacy and the MWE-(in)dependence of
the methods can be empirically evaluated, which can
later have impact on developing statistical MWE-
detectors.
Earlier studies on the detection of light verb con-
structions generally take syntactic information as a
starting point (Cook et al, 2007; Bannard, 2007;
Tan et al, 2006), that is, their goal is to classify verb
+ object constructions selected on the basis of syn-
tactic pattern as literal or idiomatic. However, we
do not aim at classifying LVC candidates filtered by
syntactic patterns but at identifying them in running
text without assuming that syntactic information is
necessarily available. In our investigations, we will
pay distinctive attention to the added value of syn-
tactic features on the system?s performance.
3.1 Methods for MWE identification
For identifying noun compounds, we made use of a
list constructed from the English Wikipedia. Lower-
case n-grams which occurred as links were collected
from Wikipedia articles and the list was automati-
cally filtered in order to delete non-English terms,
named entities and non-nominal compounds etc. In
the case of the method ?Match?, a noun compound
117
candidate was marked if it occurred in the list. The
second method we applied for noun compounds in-
volved the merge of two possible noun compounds:
if A B and B C both occurred in the list, A B C was
also accepted as a noun compound (?Merge?). Since
the methodology of dictionary building was not ap-
plicable for collecting light verb constructions (i.e.
they do not function as links in Wikipedia), we could
not apply these two methods to them.
In the case of ?POS-rules?, a noun compound
candidate was marked if it occurred in the list and
its POS-tag sequence matched one of the previ-
ously defined patterns (e.g. JJ (NN|NNS)). For
light verb constructions, the POS-rule method meant
that each n-gram for which the pre-defined patterns
(e.g. VB.? (NN|NNS)) could be applied was ac-
cepted as light verb constructions. For POS-tagging,
we used the Stanford POS Tagger (Toutanova and
Manning, 2000). Since the methods to follow rely
on morphological information (i.e. it is required
to know which element is a noun), matching the
POS-rules is a prerequisite to apply those methods
to identify MWEs.
The ?Suffix? method exploited the fact that many
nominal components in light verb constructions are
derived from verbs. Thus, in this case only construc-
tions that contained nouns ending in certain deriva-
tional suffixes were allowed and for nominal com-
pounds the last noun had to have this ending.
The ?Most frequent? (MF) method relied on the
fact that the most common verbs function typically
as light verbs (e.g. do, make, take, have etc.) Thus,
the 15 most frequent verbs typical of light verb con-
structions were collected and constructions where
the stem of the verbal component was among those
of the most frequent ones were accepted. As for
noun compounds, the 15 most frequent nouns in En-
glish were similarly collected1 and the lemma of the
last member of the possible compound had to be
among them.
The ?Stem? method pays attention to the stem of
the noun. In the case of light verb constructions, the
nominal component is typically one that is derived
from a verbal stem (make a decision) or coincides
with a verb (have a walk). In this case, we accepted
1as listed at http://en.wikipedia.org/wiki/
Most\_common\_words\_in\_English
only candidates that had the nominal component /
the last noun whose stem was of verbal nature, i.e.
coincided with a stem of a verb.
Syntactic information can also be exploited in
identifying MWEs. Typically, the syntactic relation
between the verb and the nominal component in a
light verb construction is dobj or prep ? using
Stanford parser (Klein and Manning, 2003)). The re-
lation between the members of a typical noun com-
pound is nn or amod in attributive constructions.
The ?Syntax? method accepts candidates among
whose members these syntactic relations hold.
We also combined the above methods to identify
noun compounds and light verb constructions in our
databases (the union of candidates yielded by the
methods is denoted by ? while the intersection is
denoted by ? in the respective tables).
3.2 Results
For the evaluation of our models, we developed a
corpus of 50 Wikipedia articles, in which several
types of multiword expressions (including nomi-
nal compounds and light verb constructions) and
Named Entities were marked. The database contains
2929 occurrences of nominal compounds and 368
occurrences of light verb constructions and can be
downloaded under the Creative Commons licence at
http://www.inf.u-szeged.hu/rgai/mwe.
Table 1 shows the results of our experiments.
Methods were evaluated on the token level, i.e. each
occurrence of a light verb construction had to be
identified in text. It can be seen that the best result
for noun compound identification can be obtained
if the three dictionary-based methods are combined.
We also evaluated the method of POS-rules without
taking into account dictionary matches (POS-rules
w/o dic), which result serves as the baseline for com-
paring the effect of LVC-specific methods on noun
compound detection.
As can be seen, by adding any of the LVC-specific
features, the performance of the system declines, i.e.
none of them can beat the baseline. While the fea-
ture ?Stem? (and its combinations) improve preci-
sion, recall severely falls back: especially ?Most fre-
quent noun? (MFN) has an extremely poor effect on
it. This was expected since the lexical constraint
on the last part of the compound heavily restricts
the scope of the noun compounds available. On the
118
other hand, the 15 most frequent nouns in English
are not derived from verbs hence they do not end in
any of the pre-defined suffixes, thus, the intersection
of the features ?MFN? and ?Suffix? does not yield
any noun compound (the intersection of all the three
methods also behaves similarly). It must be men-
tioned, however, that the union of all features yields
the best recall as expected and the best F-measure
can be achieved by the union of ?Suffix? and ?Stem?.
The effect of adding syntactic rules to the system
is not unequivocal. In many cases, the improvement
is marginal (it does not exceed 1% except for the
POS-rules w/o dic method) or the performance even
degrades. The latter is most obvious in the case of
the combination of dictionary-based rules, which is
mainly caused by the decline in recall, however, pre-
cision improves. The overall decline in F-score may
thus be related to possible parsing errors.
In the case of light verb constructions, the recall
of the baseline (POS-rules) is high, however, its pre-
cision is low (i.e. not all of the candidates defined
by the POS patterns are light verb constructions).
The ?Most frequent verb? (MFV) feature proves to
be the most useful: the verbal component of the light
verb construction is lexically much more restricted
than the noun, which is exploited by this feature.
The other two features put some constraints on the
nominal component, which is typically of verbal na-
ture in light verb constructions: ?Suffix? simply re-
quires the noun to end in a given n-gram (without ex-
ploiting further grammatical information) whereas
?Stem? allows nouns derived from verbs. When
combining a verbal and a nominal feature, union re-
sults in high recall (the combinations typical verb +
non-deverbal noun or atypical verb + deverbal noun
are also found) while intersection yields high preci-
sion (typical verb + deverbal noun combinations are
found only).
We also evaluated the performance of the ?Syn-
tax? method without directly exploiting POS-rules.
Results are shown in Table 2. It is revealed that
the feature dobj is much more effective in identify-
ing light verb constructions than the feature prep,
on the other hand, dobj itself outperforms POS-
rules. If we combine the dobj feature with the
best LVC-specific feature (namely, MFV), we can
achieve an F-measure of 26.46%. The feature dobj
can achieve a recall of 59.51%, which suggests
Method P R F
Dobj 10.39 59.51 17.69
Prep 0.46 7.34 0.86
Dobj ? Prep 2.09 38.36 3.97
Dobj ? MFV 31.46 22.83 26.46
Prep ? MFV 3.24 5.12 4.06
Dobj ? Prep ? MFV 8.78 19.02 12.02
Table 2: Results of syntactic methods for light verb con-
structions in terms of precision (P), recall (R) and F-
measure (F). Dobj: verb + object pairs, Prep: verb +
prepositional complement pairs, MFV: the verb is among
the 15 most frequent light verbs.
that about 40% of the nominal components in our
database are not objects of the light verb. Thus, ap-
proaches that focus on only verb-object pairs (Cook
et al, 2007; Bannard, 2007; Tan et al, 2006) fail to
identify a considerable part of light verb construc-
tions found in texts.
The added value of syntax was also investigated
for LVC detection as well. As the results show, syn-
tax clearly helps in identifying LVCs ? its overall
effect is to add up to 4% to the F-score. The best
result, again, is yielded by the MFV method, which
is about 30% above the baseline.
4 Discussion
When contrasting results achieved for light verb
constructions and noun compounds, it is revealed
that the dictionary-based method applying POS-
rules yields the best result for noun compounds and
the MFV feature combined with syntactic informa-
tion is the most useful for LVC identification. If
no dictionary matches were taken into consideration,
the combination of the features ?Suffix? and ?Stem?
achieved the best result, however, ?Stem? alone can
also perform similarly. Since ?Stem? identifies de-
verbal nouns, that is, nouns having an argument
structure, it is not surprising that this feature is valu-
able in noun compound detection because the first
part in the compound is most probably an argument
of the deverbal noun (as in noun compound detection
the object of detection is noun compound, in other
words, we detect noun compounds). Thus, it will be
worth examining how the integration of the ?Stem?
feature can improve dictionary-based models.
Making use of only POS-rules does not seem to
119
Method
Noun compounds NC + syntax LVC LVC + syntax
P R F P R F P R F P R F
Match 37.7 54.73 44.65 49.64 48.31 48.97 - - - - - -
Merge 40.06 57.63 47.26 51.69 47.86 49.70 - - - - - -
POS-rules 55.56 49.98 52.62 59.18 46.39 52.02 - - - - - -
Combined 59.46 52.48 55.75 62.07 45.81 52.72 - - - - - -
POS-rules w/o dic 28.33 66.23 39.69 29.97 64.18 40.87 9.35 72.55 12.86 7.02 76.63 16.56
Suffix 27.02 8.91 13.4 28.58 8.84 13.5 9.62 16.3 12.1 11.52 15.22 13.11
MF 12.26 1.33 2.4 12.41 1.29 2.34 33.83 55.16 41.94 40.21 51.9 45.31
Stem 29.87 37.62 33.3 31.69 36.63 33.99 8.56 50.54 14.64 11.07 47.55 17.96
Suffix?MF 0 0 0 - - - 44.05 10.05 16.37 11.42 54.35 18.88
Suffix?MF 23.36 10.24 14.24 24.50 10.13 14.34 19.82 61.41 29.97 23.99 57.88 33.92
Suffix?Stem 28.4 6.49 10.56 30.03 6.42 10.58 10.35 11.14 11.1 12.28 11.14 11.68
Suffix?Stem 29.35 40.05 33.87 31.12 39.06 34.64 8.87 57.61 15.37 11.46 54.35 18.93
MF?Stem 9.16 0.41 0.78 9.6 0.41 0.79 39.53 36.96 38.2 46.55 34.78 39.81
MF?Stem 29.13 38.55 33.18 31.85 36.04 33.81 10.42 68.75 18.09 13.36 64.67 22.15
Suffix?MF?Stem 0 0 0 - - - 47.37 7.34 12.7 50.0 6.79 11.96
Suffix?MF?Stem 28.68 40.97 33.74 30.33 39.95 34.48 10.16 72.28 17.82 13.04 68.2 21.89
Table 1: Experimental results in terms of precision (P), recall (R) and F-measure (F). Match: dictionary match, Merge:
merge of two overlapping noun compounds, POS-rules: matching of POS-patterns, Combined: the union of Match,
Merge and POS-rules, POS-rules w/o dic: matching POS-patterns without dictionary lookup, Suffix: the (head) noun
ends in a given suffix, MF: the head noun/verb is among the 15 most frequent ones, Stem: the (head) noun is deverbal.
be satisfactory for LVC detection. However, the
most useful feature for identifying LVCs, namely,
MFV/MFN proves to perform poorly for noun com-
pounds, which can be explained by the fact that the
verbal component of LVCs usually comes from a
well-defined set of frequent verbs, thus, it is lexically
more restricted than the parts of noun compounds.
The feature ?Stem? helps improve recall and this fea-
ture can be further enhanced since in some cases,
the Porter stemmer did not render the same stem to
derivational pairs such as assumption ? assume. For
instance, derivational information encoded in word-
net relations might contribute to performance.
Concerning syntactic information, it has clearly
positive effects on LVC identification, however, this
influence is ambiguous in the case of noun com-
pounds. Since light verb constructions form a syn-
tactic phrase and noun compounds behave syntac-
tically as one unit (having an internal syntactic hi-
erarchy though), this result suggests that for noun
compound detection, POS-tagging provides enough
information while for light verb constructions, syn-
tactic information is expected to improve the system.
5 Conclusions
In this paper, we aimed at identifying noun com-
pounds and light verb constructions in running texts
with rule-based methods and compared the effect
of several features on detecting those two types
of multiword expressions. For noun compounds,
dictionary-based methods and POS-tagging seem
to contribute most to the performance of the sys-
tem whereas for light verb constructions, the com-
bination of POS-tagging, syntactic information and
restrictions on the nominal and verbal component
yield the best result. Although the effect of syntax
is negligible on noun compound detection, it is un-
ambiguously helpful for identifying light verb con-
structions. Our methods can be improved by extend-
ing the set and scope of features and refining POS-
and syntactic rules and they can be also adapted to
other languages by creating language-specific POS-
rules, lists of suffixes and light verb candidates.
For higher-level of applications, it is necessary to
know which tokens form one (syntactic or semantic)
unit, thus, we believe that our results in detecting
noun compounds and light verb constructions can be
fruitfully applied in e.g. information extraction or
machine translation as well.
Acknowledgments
This work was supported in part by the National In-
novation Office of the Hungarian government within
the framework of the project MASZEKER.
120
References
Hassan Al-Haj and Shuly Wintner. 2010. Identifying
multi-word expressions by leveraging morphological
and syntactic idiosyncrasy. In Proceedings of Coling
2010, Beijing, China, August.
Colin Bannard. 2007. A measure of syntactic flexi-
bility for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, MWE
?07, pages 1?8, Morristown, NJ, USA. ACL.
Gerlof Bouma. 2010. Collocation extraction beyond the
independence assumption. In Proceedings of the ACL
2010 Conference Short Papers, pages 109?114, Upp-
sala, Sweden, July. ACL.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod,
and Antonio Zampolli. 2002. Towards best practice
for multiword expressions in computational lexicons.
In Proceedings of LREC-2002, pages 1934?1940, Las
Palmas.
Helena de Medeiros Caseli, Aline Villavicencio, Andre?
Machado, and Maria Jose? Finatto. 2009. Statistically-
driven alignment-based multiword expression identi-
fication for technical domains. In Proceedings of
the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 1?8, Singapore, August. ACL.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1-2):59?77.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, pages
41?48, Morristown, NJ, USA. ACL.
Stefan Evert and Hannah Kermes. 2003. Experiments on
candidate data for collocation extraction. In Proceed-
ings of EACL 2003, pages 83?86.
Su Nam Kim. 2008. Statistical Modeling of Multiword
Expressions. Ph.D. thesis, University of Melbourne,
Melbourne.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?03, pages 423?430, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):137?158.
Scott S. L. Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting multi-
word expressions with a semantic tagger. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment, pages 49?
56, Morristown, NJ, USA. ACL.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010a. Multiword Expressions in the wild?
The mwetoolkit comes in handy. In Coling 2010:
Demonstrations, Beijing, China, August.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Coling 2010: Posters, Beijing, China, August.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceedings
of CICLing-2002, pages 1?15, Mexico City, Mexico.
Tanja Samardz?ic? and Paola Merlo. 2010. Cross-lingual
variation of light verb constructions: Using paral-
lel corpora and automatic alignment for linguistic re-
search. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
52?60, Uppsala, Sweden, July. ACL.
R. Mahesh K. Sinha. 2009. Mining Complex Predicates
In Hindi Using A Parallel Hindi-English Corpus. In
Proceedings of the Workshop on Multiword Expres-
sions: Identification, Interpretation, Disambiguation
and Applications, pages 40?46, Singapore, August.
ACL.
Yee Fan Tan, Min-Yen Kan, and Hang Cui. 2006. Ex-
tending corpus-based identification of light verb con-
structions using a supervised learning framework. In
Proceedings of the EACL Workshop on Multi-Word
Expressions in a Multilingual Contexts, pages 49?56,
Trento, Italy, April. ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings of
EMNLP 2000, pages 63?70, Stroudsburg, PA, USA.
ACL.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction. In
Proceedings of the Workshop on a Broader Perspective
on Multiword Expressions, MWE ?07, pages 25?32,
Morristown, NJ, USA. ACL.
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Transla-
tional Correspondences for Pattern-Independent MWE
Identification. In Proceedings of the Workshop on
Multiword Expressions: Identification, Interpretation,
Disambiguation and Applications, pages 23?30, Sin-
gapore, August. ACL.
121
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 99?103,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
How to Evaluate Opinionated Keyphrase Extraction?
Ga?bor Berend
University of Szeged
Department of Informatics
A?rpa?d te?r 2., Szeged, Hungary
berendg@inf.u-szeged.hu
Veronika Vincze
Hungarian Academy of Sciences
Research Group on Artificial Intelligence
Tisza Lajos krt. 103., Szeged, Hungary
vinczev@inf.u-szeged.hu
Abstract
Evaluation often denotes a key issue in
semantics- or subjectivity-related tasks. Here
we discuss the difficulties of evaluating opin-
ionated keyphrase extraction. We present our
method to reduce the subjectivity of the task
and to alleviate the evaluation process and
we also compare the results of human and
machine-based evaluation.
1 Introduction
Evaluation is a key issue in natural language pro-
cessing (NLP) tasks. Although for more basic tasks
such as tokenization or morphological parsing, the
level of ambiguity and subjectivity is essentially
lower than for higher-level tasks such as question
answering or machine translation, it is still an open
question to find a satisfactory solution for the (auto-
matic) evaluation of certain tasks. Here we present
the difficulties of finding an appropriate way of eval-
uating a highly semantics- and subjectivity-related
task, namely opinionated keyphrase extraction.
There has been a growing interest in the NLP
treatment of subjectivity and sentiment analysis ?
see e.g. Balahur et al (2011) ? on the one hand and
on keyphrase extraction (Kim et al, 2010) on the
other hand. The tasks themselves are demanding for
automatic systems due to the variety of the linguis-
tic ways people can express the same linguistic con-
tent. Here we focus on the evaluation of subjective
information mining through the example of assign-
ing opinionated keyphrases to product reviews and
compare the results of human- and machine-based
evaluation on finding opinionated keyphrases.
2 Related Work
As the task we aim at involves extracting keyphrases
that are responsible for the author?s opinion toward
the product, aspects of both keyphrase extraction
and opinion mining determine our methodology and
evaluation procedure. There are several sentiment
analysis approaches that make use of manually an-
notated review datasets (Zhuang et al, 2006; Li
et al, 2010; Jang and Shin, 2010) and Wei and
Gulla (2010) constructed a sentiment ontology tree
in which attributes of the product and sentiments
were paired.
For evaluating scientific keyphrase extraction,
several methods have traditionally been applied. In
the case of exact match, the gold standard key-
words must be in perfect overlap with the ex-
tracted keywords (Witten et al, 1999; Frank et al,
1999) ? also followed in the SemEval-2010 task
on keyphrase extraction (Kim et al, 2010), while
in other cases, approximate matches or semanti-
cally similar keyphrases are also accepted (Zesch
and Gurevych, 2009; Medelyan et al, 2009). In this
work we applied the former approach for the evalu-
ation of opinion phrases and made a thorough com-
parison with the human judgement.
Here, we use the framework introduced in Berend
(2011) and conducted further experiments based on
it to point out the characteristics of the evaluation
of opinionated keyphrase extraction. Here we pin-
point the severe differences in performance mea-
sures when the output is evaluated by humans com-
pared to strict exact match principles and also exam-
ine the benefit of hand-annotated corpus as opposed
99
to an automatically crawled one. In addition, the
extent to which original author keyphrases resemble
those of independent readers? is also investigated in
this paper.
3 Methodology
In our experiments, we used the methodology de-
scribed in Berend (2011) to extract opinionated
keyphrase candidates from the reviews. The sys-
tem treats it as a supervised classification task us-
ing Maximum Entropy classifier, in which certain
n-grams of the product reviews are treated as classi-
fication instances and the task is to classify them as
proper or improper ones. It incorporates a rich fea-
ture set, relying on the usage of SentiWordNet (Esuli
et al, 2010) and further orthological, morphological
and syntactic features. Next, we present the diffi-
culties of opinionated keyphrase extraction and offer
our solutions to the emerging problems.
3.1 Author keyphrases
In order to find relevant keyphrases in the texts,
first the reviews have to be segmented into ana-
lyzable parts. We made use of the dataset de-
scribed in Berend (2011), which contains 2000 prod-
uct reviews each from two quite different domains,
i.e. mobile phone and video film reviews from the re-
view portal epinions.com. In the free-text parts
of the reviews, the author describes his subjective
feelings and views towards the product, and in the
sections Pros and cons and Bottomline he summa-
rizes the advantages and disadvantages of the prod-
uct, usually by providing some keyphrases or short
sentences. However, these pros and cons are noisy
since some authors entered full sentences while oth-
ers just wrote phrases or keywords. Furthermore,
the segmentation also differs from review to review
or even within the same review (comma, semicolon,
ampersand etc.). There are also non-informative
comments such as none among cons. For the above
reasons, the identification of the appropriate gold
standard phrases is not unequivocal.
We had to refine the pros and cons of the re-
views so that we could have access to a less noisy
database. Refinement included segmenting pros
and cons into keyphrase-like units and also bring-
ing complex phrases into their semantically equiva-
Auth. Ann1 Ann2 Ann3
Auth. ? 0.415 0.324 0.396
Ann1 0.601 ? 0.679 0.708
Ann2 0.454 0.702 ? 0.713
Ann3 0.525 0.690 0.688 ?
Table 1: Inter-annotator agreement among the author?s
and annotators? sets of opinion phrases. Elements above
and under the main diagonal refer to the agreement rates
in Dice coefficient for pro and con phrases, respectively.
lent, yet much simpler forms, e.g. instead of ?even I
found the phones menus to be confusing?, we would
like to have ?confusing phones menus?. Refinement
was carried out both automatically by using hand-
crafted transformation rules (based on POS patterns
and parse trees) and manual inspection. The an-
notation guidelines for the human refinement and
various statistics on the dataset can be accessed at
http://rgai.inf.u-szeged.hu/proCon.
3.2 Annotator keyphrases
The second problem with regard to opinionated
keyphrase extraction is the subjectivity of the task.
Different people may have different opinions on the
very same product, which is often reflected in their
reviews. On the other hand, people can gather dif-
ferent information from the very same review due
to differences in interpretation, which again compli-
cates the way of proper evaluation.
In order to evaluate the difficulty of identifying
opinion-related keyphrases, we decided to apply the
following methodology. We selected 25 reviews re-
lated to the mobile phone Nokia 6610, which were
also collected from the website epinions.com.
The task for three linguists was to write positive
and negative aspects of the product in the form of
keyphrases, similar to the original pros and cons. In
order not to be influenced by the keyphrases given
by the author of the review, the annotators were only
given the free-text part of the review, i.e. the origi-
nal Pros and cons and Bottomline sections were re-
moved. In this way, three different pro and con an-
notations were produced for each review, besides,
those of the original author were also at hand. The
inter-annotator agreement rate is in Table 1.
Concerning the subjectivity of the task, pro and
con phrases provided by the three annotators and
100
Eval Ref Top-5 Top-10 Top-15
3Ann? man 32.14 44.66 53.92
3Ann? auto 27.68 38.17 45.78
Merged? man 28.52 41.09 52.18
Merged? auto 27.39 37.67 46.34
3Ann? man 34.89 43.31 44.92
3Ann? auto 29.96 34.34 35.54
Merged? man 24.75 26.12 22.22
Merged? auto 21.39 20.94 21.89
Author man 27.14 33.5 35.24
Author auto 20.61 22.34 25.03
Table 2: F-scores of the human evaluation of the automat-
ically extracted opinion phrases. Columns Eval and Ref
show the way gold standard phrases were obtained and if
they were refined manually or automatically.
the original author showed a great degree of variety
although they had access to the very same review.
Sometimes it happened that one annotator did not
give any pro or con phrases for a review whereas the
others listed a bunch of them, which reflects that the
very same feature can be judged as still tolerable,
neutral or absolutely negative for different people.
Thus, as even human annotations may differ from
each other to a great extent, it is not unequivocal to
decide which human annotation should be regarded
as the gold standard upon evaluation.
3.3 Evaluation methodology
Since the comparison of annotations highlighted
the subjectivity of the task, we voted for smooth-
ing the divergences of annotations. We wanted to
take into account all the available annotations which
were manually prepared and regarded as acceptable.
Thus, an annotator formed the union and the inter-
section of the pro and con features given by each an-
notator either including or excluding those defined
by the original author. With this, we aimed at elim-
inating subjectivity since in the case of union, every
keyphrase mentioned by at least one annotator was
taken into consideration while in the case of inter-
section, it is possible to detect keyphrases that seem
to be the most salient for the annotators as regards
the given document. Thus, four sets of pros and cons
were finally yielded for each review depending on
whether the unions or intersections were determined
purely on the phrases of the annotators excluding the
original phrases of the author or including them. The
following example illustrates the way new sets were
created based on the input sets (in italics):
Pro1 : radio, organizer, phone book
Pro2 : radio, organizer, loudspeaker
Pro3 : radio, organizer, calendar
Union: radio, organizer, calendar, loud-
speaker, phone book
Intersection: radio, organizer
Proauthor : clear, fun
Merged Union: radio, organizer, calen-
dar, loudspeaker, phone book, clear, fun
Merged Intersection: ?
The reason behind this methodology was that it
made it possible to evaluate our automatic meth-
ods in two different ways. Comparing the automatic
keyphrases to the union of human annotations means
that a bigger number of keyphrases is to be identi-
fied, however, with a bigger number of gold standard
keywords it is more probable that the automatic key-
words occur among them. At the same time having a
larger set of gold standard tags might affect the recall
negatively since there are more keyphrases to return.
On the other hand, in the case of intersection it can
be measured whether the most important features
(i.e. those that every annotator felt relevant) can be
extracted from the text. Note that our strategy is sim-
ilar to the one applied in the case of BLEU/ROUGE
score (Papineni et al, 2002; Lin, 2004) with respect
to the fact that multiple good solutions are taken
into account whereas the application of union and
intersection is determined by the nature of the task:
different annotators may attach several outputs (in
other words, different numbers of keyphrases) to the
same document in the case of keyphrase extraction,
which is not realistic in the case of machine trans-
lation or summarization (only one output is offered
for each sentence / text).
3.4 Results
In our experiments, we used the opinion phrase ex-
traction system based on the paper of Berend (2011).
Results vary whether the manually or the automat-
ically refined set of the original sets of pros and
cons were regarded as positive training examples
and also whether the evaluation was carried out
101
Mobiles Movies
A/A 9.95 9.55 8.61 7.58 7.1 6.24
A/M 13.51 12.73 11.2 9.95 9.05 7.72
M/A 10.15 9.7 8.69 7.52 6.92 5.97
M/M 15.27 14.11 12.17 12.22 10.63 8.67
Table 3: F-scores achieved with different keyphrase re-
finement strategies. A and M as the first (second) charac-
ter indicate the fact that the training (testing) was based
on the automatically and manually defined sets of gold
standard expressions, respectively.
against purely the original set of author-assigned
keyphrases or the intersection/union of the man-
ual annotations including and excluding the author-
assigned keyphrases on the 25 mobile phone re-
views. Results of the various combinations in the
experiments for the top 5, 10 and 15 keyphrases
are reported in Table 2 containing both cases when
human and automatic refinement of the gold stan-
dard opinion phrases were carried out. Automatic
keyphrases were manually compared to the above
mentioned sets of keyphrases, i.e. human annotators
judged them as acceptable or not. Human evaluation
had the advantage over automated ones, that they
could accept the extracted term ?MP3? when there
was only its mistyped version ?MP+? in the set of
gold standard phrases (as found in the dataset).
Table 3 presents the results of our experiments on
keyphrase refinement on the mobiles and movies do-
mains. In these settings strict matches were required
instead of human evaluation. Results differ with re-
spect to the fact whether the automatically or manu-
ally refined sets of the original author phrases were
utilized for training and during the strict evaluation.
Having conducted these experiments, we could ex-
amine the possibility of a fully automatic system that
needs no manually inspected training data, but it can
create it automatically as well.
4 Discussion and conclusions
Both human and automatic evaluation reveal that
the results yielded when the system was trained on
manually refined keyphrases are better. The usage
of manually refined keyphrases as the training set
leads to better results (the difference being 5.9 F-
score on average), which argues for human annota-
tion as opposed to automatic normalization of the
gold standard opinion phrases. Note, however, that
even though results obtained with the automatic re-
finement of training instances tend to stay below the
results that are obtained with the manual refinement
of gold standard phrases, they are still comparable,
which implies that with more sophisticated rules,
training data could be automatically generated.
If the inter-annotator agreement rates are com-
pared, it can be seen that the agreement rates be-
tween the annotators are considerably higher than
those between a linguist and the author of the prod-
uct review. This may be due to the fact that the
linguists were to conform to the annotation guide-
lines whereas the keyphrases given by the authors
of the reviews were not limited in any way. Still,
it can be observed that among the author-annotator
agreement rates, the con phrases could reach higher
agreement than the pro phrases. This can be due to
psychological reasons: people usually expect things
to be good hence they do not list all the features that
are good (since they should be good by nature), in
contrast, they list negative features because this is
what deviates from the normal expectations.
In this paper, we discussed the difficulties of eval-
uating opinionated keyphrase extraction and also
conducted experiments to investigate the extent of
overlap between the keyphrases determined by the
original author of a review and those assigned by
independent readers. To reduce the subjectivity of
the task and to alleviate the evaluation process, we
presented our method that employs several indepen-
dent annotators and we also compared the results of
human and machine-based evaluation. Our results
reveal that for now, human evaluation leads to bet-
ter results, however, we believe that the proper treat-
ment of polar expressions and ambiguous adjectives
might improve automatic evaluation among others.
Besides describing the difficulties of the auto-
matic evaluation of opinionated keyphrase extrac-
tion, the impact of training on automatically crawled
gold standard opinionated phrases was investigated.
Although not surprisingly they lag behind the ones
obtained based on manually refined training data,
the automatic creation of gold standard keyphrases
can be a much cheaper, yet feasible option to manu-
ally refined opinion phrases. In the future, we plan to
reduce the gap between manual and automatic eval-
uation of opinionated keyphrase extraction.
102
Acknowledgments
This work was supported in part by the NIH grant
(project codename MASZEKER) of the Hungarian
government.
References
Alexandra Balahur, Ester Boldrini, Andres Montoyo, and
Patricio Martinez-Barco, editors. 2011. Proceedings
of the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2.011).
ACL, Portland, Oregon, June.
Ga?bor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 1162?1170, Chiang Mai, Thailand,
November. Asian Federation of Natural Language Pro-
cessing.
Andrea Esuli, Stefano Baccianella, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ing of 16th International Joint Conference on Artifi-
cial Intelligence, pages 668?673. Morgan Kaufmann
Publishers.
Hayeon Jang and Hyopil Shin. 2010. Language-specific
sentiment analysis in morphologically rich languages.
In Coling 2010: Posters, pages 498?506, Beijing,
China, August. Coling 2010 Organizing Committee.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-
othy Baldwin. 2010. Semeval-2010 task 5: Auto-
matic keyphrase extraction from scientific articles. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ?10, pages 21?26, Mor-
ristown, NJ, USA. ACL.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 653?
661, Beijing, China, August. Coling 2010 Organizing
Committee.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. ACL.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327, Singapore, Au-
gust. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, Pennsylvania, USA, July. ACL.
Wei Wei and Jon Atle Gulla. 2010. Sentiment learn-
ing on product reviews via sentiment ontology tree. In
Proceedings of the 48th Annual Meeting of the ACL,
pages 404?413, Uppsala, Sweden, July. ACL.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In ACM DL,
pages 254?255.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate Matching for Evaluating Keyphrase Extraction.
In Proceedings of the 7th International Conference
on Recent Advances in Natural Language Processing,
pages 484?489, September.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, CIKM ?06, pages 43?50,
New York, NY, USA. ACM.
103
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62?67,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
LFG-based Features for Noun Number and Article Grammatical Errors
Ga?bor Berend1, Veronika Vincze2, Sina Zarriess3, Richa?rd Farkas1
1University of Szeged
Department of Informatics
{berendg,rfarkas}@inf.u-szeged.hu
2Research Group on Artificial Intelligence
Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
3University of Stuttgart
Institute for Natural Language Processing
zarriesa@ims.uni-stuttgart.de
Abstract
We introduce here a participating system
of the CoNLL-2013 Shared Task ?Gram-
matical Error Correction?. We focused on
the noun number and article error cate-
gories and constructed a supervised learn-
ing system for solving these tasks. We car-
ried out feature engineering and we found
that (among others) the f-structure of an
LFG parser can provide very informative
features for the machine learning system.
1 Introduction
The CoNLL-2013 Shared Task aimed at identify-
ing and correcting grammatical errors in the NU-
CLE learner corpus of English (Dahlmeier et al,
2013). This task has become popular in the natural
language processing (NLP) community in the last
few years (Dale and Kilgariff, 2010), which mani-
fested in the organization of shared tasks. In 2011,
the task Helping Our Own (HOO 2011) was held
(Dale and Kilgariff, 2011), which targeted the pro-
motion of NLP tools and techniques in improving
the textual quality of papers written by non-native
speakers of English within the field of NLP. The
next year, HOO 2012 (Dale et al, 2012) specifi-
cally focused on the correction of determiner and
preposition errors in a collection of essays writ-
ten by candidates sitting for the Cambridge ESOL
First Certificate in English (FCE) examination. In
2013, the CoNLL-2013 Shared Task has continued
this direction of research.
The CoNLL-2013 Shared Task is based on the
NUCLE corpus, which consists of about 1,400
student essays from undergraduate university stu-
dents at The National University of Singapore
(Dahlmeier et al, 2013). The corpus contains over
one million words and it is completely annotated
with grammatical errors and corrections. Among
the 28 error categories, this year?s shared task fo-
cused on the automatic detection and correction of
five specific error categories.
In this paper, we introduce our contribution of
the CoNLL-2013 Shared Task. We propose a su-
pervised learning-based approach. The main con-
tribution of this work is the exploration of several
feature templates for grammatical error categories.
We focused on the two ?nominal? error categories:
1.1 Article and Determiner Errors
This error type involved all kinds of errors
which were related to determiners and articles
(ArtOrDet). It required multiple correction
strategies. On the one hand, superfluous articles
or determiners should be deleted from the text.
On the other hand, missing articles or determin-
ers should be inserted and at the same time it was
sometimes also necessary to replace a certain type
of article or determiner to an other type. Here is
an example:
For nations like Iran and North Ko-
rea, the development of nuclear power
is mainly determined by the political
forces. ? For nations like Iran and
North Korea, the development of nu-
clear power is mainly determined by po-
litical forces.
62
1.2 Wrong Number of the Noun
The wrong number of nouns (Nn) meant that either
a singular noun should occur in the plural form or
a plural noun should occur in the singular form.
A special case of such errors was that sometimes
uncountable nouns were used in the plural, which
is ungrammatical. The correction involved here
the change of the number. Below we provide an
example:
All these measures are implemented to
meet the safety expectation of the op-
eration of nuclear power plant. ? All
these measures are implemented to meet
the safety expectation of the operation
of nuclear power plants.
2 System Description
Our approach for grammatical error detection was
to construct supervised classifiers for each candi-
date of grammatical error locations. In general,
our candidate extraction and features are based
on the output of the preprocessing step provided
by the organizers which contained both the POS-
tag sequences and the constituency phrase struc-
ture outputs for every sentence in the training and
test sets determined by Stanford libraries. We em-
ployed the Maximum Entropy based supervised
classification model using the MALLET API (Mc-
Callum, 2002), which was responsible for suggest-
ing the various corrections.
The most closely related approach to ours is
probably the work of De Felice and Pulman
(2008). We also employ a Maximum Entropy clas-
sifier and a syntax-motivated feature set. However,
we investigate deeper linguistic features (based on
the f-structure of an LFG parser).
In the following subsections we introduce our
correction candidate recognition procedure and
the features used for training and prediction of
the machine learning classifier. We employed the
same feature set for each classification task.
2.1 Candidate Locations
We used the following heuristics for the recogni-
tion of the possible locations of grammatical er-
rors. We also describe the task of various classi-
fiers at these candidate locations.
Article and Determiner Error category We
handled the beginning of each noun phrase
(NP) as a possible location for errors related
to articles or determiners. The NP was
checked if it started with any definite or
indefinite article. If it did, we asked our
three-class classifier whether to leave it
unmodified, change its type (i.e. an indefinite
to a definite one or vice versa) or simply
delete it. However, when there was no article
at all at the beginning of a noun phrase,
the decision made by a different three-class
classifier was whether to leave that position
empty or to put a definite or indefinite article
in that place.
Wrong Number of the Noun Error category
Every token tagged as a noun (either in plural
or singular) was taken into consideration at
this subtask. We constructed two ? i.e. one
for the word forms originally written in plu-
ral and singular ? binary classifiers whether
the number (i.e. plural or singular) of the
noun should be changed or left unchanged.
2.2 LFG parse-based features
We looked for the minimal governing NP for each
candidate location. We reparsed this NP with-
out context by a Lexical Functional Grammar
(LFG) parser and we acquired features from its
f-structure. In the following paragraph, LFG is
introduced briefly while Table 1 summarizes the
features extracted from the LFG parse.
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)-structure.
C-structure is represented by context free
phrase-structure trees, and captures surface gram-
matical configurations. F-structures approximate
basic predicate-argument and adjunct structures.
The experiments reported in this paper use the
English LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The gram-
mar is implemented in XLE, a grammar develop-
ment environment, which includes a very efficient
LFG parser. Within the spectrum of approaches to
natural language parsing, XLE can be considered
a hybrid system combining a hand-crafted gram-
mar with a number of automatic ambiguity man-
agement techniques:
(i) c-structure pruning where, based on informa-
tion from statistically obtained parses, some trees
are ruled out before f-structure unification (Cahill
et al, 2007)
63
COORD NP/PP is coordinated +/-
COORD-LEVEL syntactic category of coordi-
nated phrase
DEG-DIM dimension for comparitive NPs,
(?equative?/?pos?/?neg?)
DEGREE semantic type of adjec-
tival modifier (?posi-
tive?/?comparative?/?superlative?)
DET-TYPE type of determiner
(?def?/?indef?/?demon?)
LOCATION-TYPE marks locative NPs
NAME-TYPE ?first name?/?last name?
NSYN syntactic noun type (?com-
mon?/?proper?/?pronoun?)
PRON-TYPE syntactic pronoun type (e.g.
?pers?, ?refl?, ?poss?)
PROPER-TYPE type of proper noun (e.g. ?com-
pany?, ?location?, ?name?)
Table 1: Short characterization of the LFG fea-
tures incorporated in our models designed to cor-
rect noun phrase-related grammatical errors
(ii) an Optimality Theory-style constraint mecha-
nism for filtering and ranking competing analyses
(Frank et al, 2001),
and (iii) a stochastic disambiguation component
which is based on a log-linear probability model
(Riezler et al, 2002) and works on the packed rep-
resentations.
Although we use a deep, hand-crafted LFG
grammar for processing the data, our approach is
substantially different from other grammar-based
approaches to CALL. For instance, Fortmann and
Forst (2004) supplement a German LFG devel-
oped for newspaper text with so-called malrules
that accept marked or ungrammatical input of
some predefined types. In our work, we apply an
LFG parser developed for standard texts to get a
rich feature representation that can be exploited
by a classifier. While malrules would certainly be
useful for finding other error types, such as agree-
ment errors, the NP- and PP-errors are often ana-
lyzed as grammatical by the parser (e.g. ?the po-
litical forces? vs. ?political forces?). Thus, the
grammaticality of a phrase predicted by the gram-
mar is not necessarily a good indicator for correc-
tion in our case.
2.3 Phrase-based contextual features
Besides the LFG features describing the internal
structure of the minimal NP that dominates a can-
didate location, we defined features describing its
context as well. Phrase-based contextual features
searched for those minimal prepositional and noun
phrases that governed a token at a certain can-
Final results Corrected output
P 0.0552 0.1260
R 0.0316 0.0292
F 0.0402 0.0474
Table 2: Overall results aggregated over the five
error types
didate location of the sentence where a decision
was about to be taken. Then features encoding the
types of the phrases that preceded and succeeded
a given minimal governing noun or prepositional
phrase were extracted.
The length of those minimal governing noun
and prepositional phrases as well as those of the
preceding and succeeding ones were taken into
account as numeric features. The motivation be-
hind using the span size of the minimal governing
and neighboring noun and prepositional phrases
is that it was assumed that grammatical errors in
the sentence result in unusual constituency subtree
patterns that could manifest in minimal governing
phrases having too long spans for instance. The
relative position of the candidate position inside
the smallest dominating noun and prepositional
phrases was also incorporated as a feature since
this information might carry some information for
noun errors.
2.4 Token-based contextual features
A third group of features described the context of
the candidate location at the token level. Here, two
sets of binary features were introduced to mark the
fact if the token was present in the four token-sized
window to its left or right. Dedicated nominal fea-
tures were introduced to store the word form of
the token immediately preceding a decision point
within a sentence and the POS-tags at the preced-
ing and actual token positions.
Two lists were manually created which con-
sisted of entirely uncountable nouns (e.g. blood)
and nouns that are uncountable most of the times
(e.g. aid or dessert). When generating fea-
tures for those classifiers that can modify the plu-
rality of a noun, we marked the fact in a binary
feature if they were present in any of these lists.
Another binary feature indicated if the actual noun
to be classified could be found at an earlier point
of the document.
64
Only erroneous All sentences
P 0.1260 0.1061
R 0.0292 0.0085
F 0.0474 0.0158
Table 3: Overall results aggregated over the five
error types
Only erroneous All sentences
P 0.2500 0.0167
R 0.0006 0.0006
F 0.0012 0.0012
Table 4: Overall results aggregated over the five
error types, not using the LFG parser based fea-
tures
3 Results
It is important to note that our officially submit-
ted architecture included an unintended step which
meant that whenever our system predicted that at
a certain point an indefinite article should be in-
serted or (re-)written, the indefinite article an was
put at that place erroneously when the succeeding
token started with a consonant (e.g. outputting an
serious instead of a serious).
Since the output that contained this kind of error
served as the basis of the official ranking we in-
clude in Table 2 the results achieved with the out-
put affected by this unintended behavior, however,
in the following we present our results in such a
manner where this kind of error is eliminated from
the output of our system.
Upon training our systems we followed two
strategies. For the first approach we used all the
sentences regardless if they had any error in them
at all. However, in an alternative approach we uti-
lized only those sentences from the training corpus
that had at least one error in them from the five er-
ror categories to be dealt with in the shared task.
The different results achieved on the test set ac-
cording to the two approaches are detailed in Ta-
ble 3. Turning off the LFG features ended up in
the results detailed in Table 4.
Since our framework in its present state only
aims at the correction of errors explicitly re-
lated to noun phrases, no error categories besides
ArtOrDet and Nn (for more details see Sections
1.1 and 1.2, respectively) could be possibly cor-
rected by our system. Note that these two error
categories covered 66.1% of the corrections on the
test set, so with our approach this was the highest
possibly achievable score in recall.
In order to get a clearer picture on the effective-
ness of our proposed methodology on the two error
types that we aimed at, we present results focusing
on those two error classes.
Nn ArtOrDet
P 0.4783 (44/92) 0.0151 (4/263)
R 0.1111 (44/396) 0.0058 (4/690)
F 0.1803 0.0084
Table 5: The scores achieved and the number of
true positive, suggestions, real errors for the Noun
Number (Nn) and Article and Determiner Errors
(ArtOrDet) categories.
4 Error Analysis
In order to analyze the performance of our system
in more detail, we carried out an error analysis.
As our system was optimized for errors related to
nouns (i.e. Nn and ArtOrDet errors), we focus
on these error categories in our discussion and ne-
glect verbal and prepositional errors.
Some errors in our system?s output were due
to pronouns, which are conventionally tagged as
nouns (e.g. something), but were incorrectly put
in the plural, resulting in the erroneous correc-
tion somethings. These errors would have been
avoided by including a list of pronouns which
could not be used in the plural (even if they are
tagged as nouns).
Another common source of errors was that
countable and uncountable uses of nouns which
can have both features in different senses or
metonymic usage (e.g. coffee as a substance is un-
countable but coffee meaning ?a cup of coffee? is
countable) were hard to separate. Performance on
this class of nouns could be ameliorated by apply-
ing word sense disambiguation/discrimination or
a metonymy detector would also prove useful for
e.g. mass nouns.
A great number of nominal errors involved
cases where a singular noun occurred in the text
without any article or determiner. In English, this
is only grammatical in the case of uncountable
nouns which occur in generic sentences, for in-
stance:
Radio-frequency identification is a
technology which uses a wireless non-
contact system to scan and transfer the
data [...]
65
The above sentence offers a definition of radio-
frequency identification, hence it is a generic state-
ment and should be left as it is. In other cases,
two possible strategies are available for correc-
tion. First, the noun gets an article or a determiner.
The actual choice among the articles or determin-
ers depends on the context: if the noun has been
mentioned previously and thus is already known
(definite) in the context, it usually gets a definite
article (or a possessive determiner). If it is men-
tioned for the first time, it gets an indefinite arti-
cle (unless it is a unique thing such as the sun).
The difficulty of the problem lies in the fact that
in order to adequately assign an article or deter-
miner to the noun, it is not sufficient to rely only
on the sentence. Thus, is also necessary to go be-
yond the sentence and move on the level of text
or discourse, which requires natural language pro-
cessing techniques that we currently lack but are
highly needed. With the application of such tech-
niques, we would have probably achieved better
results but this remains now for future work.
Second, the noun could be put in the plural.
This strategy is usually applied when either there
are more than one of the thing mentioned or it is a
generic sentence (i.e. things are discussed in gen-
eral and no specific instances of things are spo-
ken of). In this case, the detection of generic sen-
tences/events would be helpful, which again re-
quires deep semantic processing of the discourse
and is also a possible direction for future work.
To conclude, the successful identification of
noun number and article errors would require a
much deeper semantic (and even pragmatic) anal-
ysis and representation of the texts in question.
5 Discussion and further work
Comparing the columns of Table 3 we can con-
clude that restricting the training sentences to only
those which had some kind of grammatical error
in them had a useful effect on the overall effec-
tiveness of our system.
In a similar way, it can be stated based on the
results in Table 4 that composing features from the
output of an LFG parser is essentially beneficial
for the determination of Nn-type errors. Table 5
reveals, however, that those features which work
relatively well on the correction of Nn type errors
are less useful on ArtOrDet-type errors without
any modification.
As our only target at this point was to suggest
error corrections related to noun phrases, our ob-
vious future plans include the extension of our sys-
tem to deal with error categories of different types.
Simultaneously, we are planning to utilize large
scale corpus statistics, such as the Google N-gram
Corpus to build a more effective system.
Acknowledgements
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The Parallel Grammar Project. In Proceedings of
COLING-2002 Workshop on Grammar Engineering
and Evaluation, Taipei, Taiwan.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning. In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013), Atlanta, Georgia, USA. Asso-
ciation for Computational Linguistics.
Robert Dale and Adam Kilgariff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 261?265, Dublin, Ireland.
Robert Dale and Adam Kilgariff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, Nancy, France.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Com-
66
putational Linguistics (Coling 2008), pages 169?
176.
Christian Fortmann and Martin Forst. 2004. An LFG
Grammar Checker for CALL. In Proceedings of
ICALL 2004.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars.
In Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367?397. CSLI
Publications.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques. In Proceedings of ACL
2002.
67
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 17?25,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
VPCTagger: Detecting Verb-Particle Constructions
With Syntax-Based Methods
Istv
?
an Nagy T.
1
and Veronika Vincze
1,2
1
Department of Informatics, University of Szeged
?
Arp?ad t?er 2., 6720 Szeged, Hungary
nistvan@inf.u-szeged.hu
2
Hungarian Academy of Sciences, Research Group on Artificial Intelligence
Tisza Lajos krt. 103., 6720 Szeged, Hungary
vinczev@inf.u-szeged.hu
Abstract
Verb-particle combinations (VPCs) con-
sist of a verbal and a preposition/particle
component, which often have some addi-
tional meaning compared to the meaning
of their parts. If a data-driven morpholog-
ical parser or a syntactic parser is trained
on a dataset annotated with extra informa-
tion for VPCs, they will be able to iden-
tify VPCs in raw texts. In this paper,
we examine how syntactic parsers perform
on this task and we introduce VPCTag-
ger, a machine learning-based tool that is
able to identify English VPCs in context.
Our method consists of two steps: it first
selects VPC candidates on the basis of
syntactic information and then selects gen-
uine VPCs among them by exploiting new
features like semantic and contextual ones.
Based on our results, we see that VPC-
Tagger outperforms state-of-the-art meth-
ods in the VPC detection task.
1 Introduction
Verb-particle constructions (VPCs) are a subclass
of multiword expressions (MWEs) that contain
more than one meaningful tokens but the whole
unit exhibits syntactic, semantic or pragmatic
idiosyncracies (Sag et al., 2002). VPCs consist
of a verb and a preposition/particle (like hand in
or go out) and they are very characteristic of the
English language. The particle modifies the mean-
ing of the verb: it may add aspectual informa-
tion, may refer to motion or location or may totally
change the meaning of the expression. Thus, the
meaning of VPCs can be compositional, i.e. it
can be computed on the basis of the meaning of
the verb and the particle (go out) or it can be
idiomatic; i.e. a combination of the given verb and
particle results in a(n unexpected) new meaning
(do in ?kill?). Moreover, as their syntactic sur-
face structure is very similar to verb ? preposi-
tional phrase combinations, it is not straightfor-
ward to determine whether a given verb + prepo-
sition/particle combination functions as a VPC or
not and contextual information plays a very impor-
tant role here. For instance, compare the follow-
ing examples: The hitman did in the president and
What he did in the garden was unbelievable. Both
sentences contain the sequence did in, but it is
only in the first sentence where it functions as a
VPC and in the second case, it is a simple verb-
prepositional phrase combination. For these rea-
sons, VPCs are of great interest for natural lan-
guage processing applications like machine trans-
lation or information extraction, where it is neces-
sary to grab the meaning of the text.
The special relation of the verb and particle
within a VPC is often distinctively marked at sev-
eral annotation layers in treebanks. For instance,
in the Penn Treebank, the particle is assigned a
specific part of speech tag (RP) and it also has
a specific syntactic label (PRT) (Marcus et al.,
1993), see also Figure 1. This entails that if a data-
driven morphological parser or a syntactic parser
is trained on a dataset annotated with extra infor-
mation for VPCs, it will be able to assign these
kind of tags as well. In other words, the morpho-
logical/syntactic parser itself will be able to iden-
tify VPCs in texts.
In this paper, we seek to identify VPCs on the
basis of syntactic information. We first examine
how syntactic parsers perform on Wiki50 (Vincze
et al., 2011), a dataset manually annotated for
different types of MWEs, including VPCs. We
then present our syntax-based tool called VPC-
Tagger to identify VPCs, which consists of two
steps: first, we select VPC candidates (i.e. verb-
preposition/particle pairs) from the text and then
we apply a machine learning-based technique to
classify them as genuine VPCs or not. This
17
The hitman did in the president .
root
det nsubj prt
dobj
det
punct
Figure 1: A dependency parse of the sentence
?The hitman did in the president?.
method is based on a rich feature set with new
features like semantic or contextual features. We
compare the performance of the parsers with that
of our approach and we discuss the reasons for any
possible differences.
2 Related Work
Recently, some studies have attempted to iden-
tify VPCs. For instance, Baldwin and Villavicen-
cio (2002) detected verb-particle constructions in
raw texts with the help of information based on
POS-tagging and chunking, and they also made
use of frequency and lexical information in their
classifier. Kim and Baldwin (2006) built their
system on semantic information when deciding
whether verb-preposition pairs were verb-particle
constructions or not. Nagy T. and Vincze (2011)
implemented a rule-based system based on mor-
phological features to detect VPCs in raw texts.
The (non-)compositionality of verb-particle
combinations has also raised interest among
researchers. McCarthy et al. (2003) implemented
a method to determine the compositionality of
VPCs and Baldwin (2005) presented a dataset in
which non-compositional VPCs could be found.
Villavicencio (2003) proposed some methods to
extend the coverage of available VPC resources.
Tu and Roth (2012) distinguished genuine
VPCs and verb-preposition combinations in con-
text. They built a crowdsourced corpus of VPC
candidates in context, where each candidate was
manually classified as a VPC or not. How-
ever, during corpus building, they applied lexi-
cal restrictions and concentrated only on VPCs
formed with six verbs. Their SVM-based algo-
rithm used syntactic and lexical features to clas-
sify VPCs candidates and they concluded that their
system achieved good results on idiomatic VPCs,
but the classification of more compositional VPCs
is more challenging.
Since in this paper we focus on syntax-based
VPC identification more precisely, we also iden-
tify VPCs with syntactic parsers, it seems nec-
essary to mention studies that experimented with
parsers for identifying different types of MWEs.
For instance, constituency parsing models were
employed in identifying contiguous MWEs in
French and Arabic (Green et al., 2013). Their
method relied on a syntactic treebank, an MWE
list and a morphological analyzer. Vincze et al.
(2013) employed a dependency parser for identi-
fying light verb constructions in Hungarian texts
as a ?side effect? of parsing sentences and report
state-of-the-art results for this task.
Here, we make use of parsers trained on the
Penn Treebank (which contains annotation for
VPCs) and we evaluate their performance on the
Wiki50 corpus, which was manually annotated for
VPCs. Thus, we first examine how well these
parsers identify VPCs (i.e. assigning VPC-specific
syntactic labels) and then we present how VPC-
Tagger can carry out this task. First, we select
VPC candidates from raw text and then, we clas-
sify them as genuine VPCs or not.
3 Verb-particle Constructions in English
As mentioned earlier, verb-particle constructions
consist of a verb and a particle. Similar construc-
tions are present in several languages, although
there might be different grammatical or ortho-
graphic norms for such verbs in those languages.
For instance, in German and in Hungarian, the par-
ticle usually precedes the verb and they are spelt as
one word, e.g. aufmachen (up.make) ?to open? in
German or kinyitni (out.open) ?to open? in Hun-
garian. On the other hand, languages like Swedish,
Norwegian, Icelandic and Italian follow the same
pattern as English; namely, the verb precedes the
particle and they are spelt as two words (Masini,
2005). These two typological classes require dif-
ferent approaches if we would like identify VPCs.
For the first group, morphology-based solutions
can be implemented that can identify the inter-
nal structure of compound words. For the second
group, syntax-based methods can also be success-
ful, which take into account the syntactic relation
between the verb and the particle.
Many of the VPCs are formed with a motion
verb and a particle denoting directions (like go
out, come in etc.) and their meaning reflects this:
they denote a motion or location. The meaning
of VPCs belonging to this group is usually trans-
18
parent and thus they can be easily learnt by sec-
ond language learners. In other cases, the particle
adds some aspectual information to the meaning
of the verb: eat up means ?to consume totally?
or burn out means ?to reach a state where some-
one becomes exhausted?. These VPCs still have a
compositional meaning, but the particle has a non-
directional function here, but rather an aspectual
one (cf. Jackendoff (2002)). Yet other VPCs have
completely idiomatic meanings like do up ?repair?
or do in ?kill?. In the latter cases, the meaning
of the construction cannot be computed from the
meaning of the parts, hence they are problematic
for both language learners and NLP applications.
Tu and Roth (2012) distinguish between two
sets of VPCs in their database: the more com-
positional and the more idiomatic ones. Dif-
ferentiating between compositional and idiomatic
VPCs has an apt linguistic background as well (see
above) and it may be exploited in some NLP appli-
cations like machine translation (parts of compo-
sitional VPCs may be directly translated while
idiomatic VPCs should be treated as one unit).
However, when grouping their data, Tu and Roth
just consider frequency data and treat one VPC
as one lexical entry. This approach is some-
what problematic as many VPCs in their dataset
are highly ambiguous and thus may have more
meanings (like get at, which can mean ?criticise?,
?mean?, ?get access?, ?threaten?) and some of
them may be compositional, while others are not.
Hence, clustering all these meanings and classify-
ing them as either compositional or idiomatic may
be misleading. Instead, VPC and non-VPC uses
of one specific verb-particle combination could be
truly distinguished on the basis of frequency data,
or, on the other hand, a word sense disambigua-
tion approach may give an account of the compo-
sitional or idiomatic uses of the specific unit.
In our experiments, we use the Wiki50 corpus,
in which VPCs are annotated in raw text, but no
semantic classes are further distinguished. Hence,
our goal here is not the automatic semantic classi-
fication of VPCs because we believe that first the
identification of VPCs in context should be solved
and then in a further step, genuine VPCs might be
classified as compositional or idiomatic, given a
manually annotated dataset from which this kind
of information may be learnt. This issue will be
addressed in a future study.
Figure 2: System Architecture
4 VPC Detection
Our goal is to identify each individual VPC in run-
ning texts; i.e. to take individual inputs like How
did they get on yesterday? and mark each VPC in
the sentence. Our tool called VPCTagger is based
on a two-step approach. First, we syntactically
parse each sentence, and extract potential VPCs
with a syntax-based candidate extraction method.
Afterwards, a binary classification can be used
to automatically classify potential VPCs as VPCs
or not. For the automatic classification of candi-
date VPCs, we implemented a machine learning
approach, which is based on a rich feature set with
new features like semantic and contextual features.
Figure 2 outlines the process used to identify each
individual VPC in a running text.
4.1 Corpora
To evaluate of our methods, we made use of two
corpora. Statistical data on the corpora can be seen
in Table 1. First, we used Wiki50 (Vincze et al.,
2011), in which several types of multiword expres-
sions (including VPCs) and Named Entities were
marked. This corpus consists of 50 Wikipedia
pages, and contains 466 occurrences of VPCs.
Corpus Sentences Tokens VPCs #
Wiki50 4,350 114,570 466 342
Tu&Roth 1,348 38,132 878 23
Table 1: Statistical data on the corpora.
In order to compare the performance of our sys-
tem with others, we also used the dataset of Tu
and Roth (2012), which contains 1,348 sentences
taken from different parts of the British National
Corpus. However, they only focused on VPCs in
this dataset, where 65% of the sentences contain
19
a phrasal verb and 35% contain a simplex verb-
preposition combination. As Table 1 indicates,
the Tu&Roth dataset only focused on 23 different
VPCs, but 342 unique VPCs were annotated in the
Wiki50 corpus.
4.2 Candidate Extraction
In this section, we concentrate on the first step of
our approach, namely how VPC candidates can be
selected from texts. As we mentioned in Section
1, our hypothesis is that the automatic detection of
VPCs can be basically carried out by dependency
parsers. Thus, we examined the performance of
two parsers on VPC-specific syntactic labels.
As we had a full-coverage VPC annotated cor-
pus where each individual occurrence of a VPC
was manually marked, we were able to exam-
ine the characteristics of VPCs in a running text
and evaluate the effectiveness of the parsers on
this task. Therefore, here we examine depen-
dency relations among the manually annotated
gold standard VPCs, provided by the Stanford
parser (Klein and Manning, 2003) and the Bohnet
parser (Bohnet, 2010) for the Wiki50 corpus. In
order to compare the efficiency of the parsers, both
were applied using the same dependency represen-
tation. We found that only 52.57% and 58.16% of
the annotated VPCs in Wiki50 had a verb-particle
syntactic relation when we used the Stanford and
Bohnet parsers, respectively. As Table 2 shows,
there are several other syntactic constructions in
which VPCs may occur.
Edge type Stanford Bohnet
# % # %
prt 235 52.57 260 58.16
prep 23 5.15 107 23.94
advmod 56 12.52 64 14.32
sum 314 70.24 431 96.42
other 8 1.79 1 0.22
none 125 27.97 15 3.36
sum 447 100.00 447 100.00
Table 2: Edge types in the Wiki50 corpus. prt: par-
ticle. prep: preposition. advmod: adverbial mod-
ifier. other: other dependency labels. none: no
direct syntactic connection between the verb and
particle.
Therefore, we extended our candidate extrac-
tion method, where besides the verb-particle
dependency relation, the preposition and adver-
bial modifier syntactic relations were also investi-
gated among verbs and particles. With this modifi-
cation, 70.24% and 96.42% of VPCs in the Wiki50
corpus could be identified. In this phase, we found
that the Bohnet parser was more successful on
the Wiki50 corpus, i.e. it could cover more VPCs,
hence we applied the Bohnet parser in our further
experiments.
Some researchers filtered LVC candidates by
selecting only certain verbs that may be part
of the construction. One example is Tu and
Roth (2012), where the authors examined a verb-
particle combination only if the verbal compo-
nents were formed with one of the previously
given six verbs (i.e. make, take, have, give, do,
get).
Since Wiki50 was annotated for all VPC occur-
rences, we were able to check what percentage of
VPCs could be covered if we applied this selec-
tion. As Table 3 shows, the six verbs used by Tu
and Roth (2012) are responsible for only 50 VPCs
on the Wiki50 corpus, so it covers only 11.16% of
all gold standard VPCs.
Table 4 lists the most frequent VPCs and the
verbal components on the Wiki50 corpus. As
can be seen, the top 10 VPCs are responsible
for only 17.41% of the VPC occurrences, while
the top 10 verbal components are responsible for
41.07% of the VPC occurrences in the Wiki50 cor-
pus. Furthermore, 127 different verbal compo-
nent occurred in Wiki50, but the verbs have and
do ? which are used by Tu and Roth (2012) ?
do not appear in the corpus as verbal component
of VPCs. All this indicates that applying lexical
restrictions and focusing on a reduced set of verbs
will lead to the exclusion of a considerable number
of VPCs occurring in free texts and so, real-world
tasks would hardly profit from them.
verb #
take 27
get 10
give 5
make 3
have 0
do 0
sum 50
Table 3: The frequency of verbs on the Wiki50
corpus used by Tu and Roth (2012).
20
VPC # verb #
call for 11 set 28
point out 9 take 27
carry out 9 turn 26
set out 8 go 21
grow up 8 call 21
set up 7 come 15
catch up 7 carry 13
turn on 7 look 13
take up 6 break 10
pass on 6 move 10
sum 78 sum 184
Table 4: The most frequent VPCs and verbal com-
ponents on the Wiki50 corpus.
4.3 Machine Learning Based Candidate
Classication
In order to perform an automatic classification
of the candidate VPCs, a machine learning-based
approach was implemented, which will be elabo-
rated upon below. This method is based on a rich
feature set with the following categories: ortho-
graphic, lexical, syntactic, and semantic. More-
over, as VPCs are highly ambiguous in raw texts,
contextual features are also required.
? Orthographic features: Here, we examined
whether the candidate consists of two or
more tokens. Moreover, if the particle com-
ponent started with ?a?, which prefix, in
many cases, etymologically denotes a move-
ment (like across and away), it was also noted
and applied as a feature.
? Lexical features: We exploited the fact that
the most common verbs occur most fre-
quently in VPCs, so we selected fifteen verbs
from the most frequent English verbs
1
. Here,
we examined whether the lemmatised verbal
component of the candidate was one of these
fifteen verbs. We also examined whether
the particle component of the potential VPC
occurred among the common English parti-
cles. Here, we apply a manually built par-
ticle list based on linguistic considerations.
Moreover, we also checked whether a poten-
tial VPC is contained in the list of typical
English VPCs collected by Baldwin (2008).
1
http://en.wikipedia.org/wiki/Most common words in English
? Syntactic features: the dependency label
between the verb and the particle can also be
exploited in identifying LVCs. As we typ-
ically found when dependency parsing the
corpus, the syntactic relation between the
verb and the particle in a VPC is prt, prep
or advmod ? applying the Stanford parser
dependency representation, hence these syn-
tactic relations were defined as features. If
the candidate?s object was a personal pro-
noun, it was also encoded as another syntac-
tic feature.
? Semantic features: These features were based
on the fact that the meaning of VPCs may
typically reflect a motion or location like go
on or take away. First, we examine that the
verbal component is a motion verb like go
or turn, or the particle indicates a direction
like out or away.
Moreover, the semantic type of the prepo-
sitional object, object and subject in the
sentence can also help to decide whether
the candidate is a VPC or not. Conse-
quently, the person, activity, animal,
artifact and concept semantic senses
were looked for among the upper level hyper-
onyms of the nominal head of the preposi-
tional object, object and subject in Princeton
WordNet 3.1
2
.
When several different machine learning algo-
rithms were experimented on this feature set, the
preliminary results showed that decision trees per-
formed the best on this task. This is probably due
to the fact that our feature set consists of a few
compact (i.e. high-level) features. The J48 clas-
sifier of the WEKA package (Hall et al., 2009)
was trained with its default settings on the above-
mentioned feature set, which implements the C4.5
(Quinlan, 1993) decision tree algorithm. More-
over, Support Vector Machines (SVM) (Cortes and
Vapnik, 1995) results are also reported to compare
the performance of our methods with that of Tu
and Roth (2012).
As the investigated corpora were not sufficiently
large for splitting them into training and test sets
of appropriate size, we evaluated our models in a
cross validation manner on the Wiki50 corpus and
the Tu&Roth dataset.
2
http://wordnetweb.princeton.edu/perl/webwn
21
As Tu and Roth (2012) presented only the accu-
racy scores on the Tu & Roth dataset, we also
employed an accuracy score as an evaluation met-
ric on this dataset, where positive and negative
examples were also marked. But, in the case
of Wiki50 corpus, where only the positive VPCs
were manually annotated, the F
?=1
score was
employed and interpreted on the positive class
as an evaluation metric. Moreover, all potential
VPCs were treated as negative that were extracted
by the candidate extraction method but were not
marked as positive in the gold standard. Thus, in
the resulting dataset negative examples are over-
represented.
As Table 2 shows, the candidate extraction
method did not cover all manually annotated
VPCs in the Wiki50 corpus. Hence, we treated the
omitted LVCs as false negatives in our evaluation.
As a baseline, we applied a context-free dictio-
nary lookup method. In this case, we applied the
same VPC list that was described among the lex-
ical features. Then we marked candidates of the
syntax-based method as VPC if the candidate VPC
was found in the list. We also compared our results
with the rule-based results available for Wiki50
(Nagy T. and Vincze, 2011) and also with the 5-
fold cross validation results of Tu and Roth (2012).
5 Results
Table 5 lists the results obtained using the base-
line dictionary lookup, rule-based method, depen-
dency parsers and machine learning approaches
on the Wiki50 corpus. It is revealed that the
dictionary lookup method performed worst and
achieved an F-score of 35.43. Moreover, this
method only achieved a precision score of 49.77%.
However, the rule-based method achieved the
highest precision score with 91.26%, but the
dependency parsers also got high precision scores
of about 90% on Wiki50. It is also clear that the
machine learning-based approach, the VPCTag-
ger, is the most successful method on Wiki50: it
achieved an F-score 10 points higher than those
for the rule-based method and dependency parsers
and more than 45 points higher than that for the
dictionary lookup.
In order to compare the performance of our sys-
tem with others, we evaluated it on the Tu&Roth
dataset (Tu and Roth, 2012). Table 6 compares the
results achieved by the dictionary lookup and the
rule-based method on the Tu&Roth dataset. More-
Method Prec. Rec. F-score
Dictionary Lookup 49.77 27.5 35.43
Rule-based 91.26 58.52 71.31
Stanford Parser 91.09 52.57 66.67
Bohnet Parser 89.04 58.16 70.36
ML J48 85.7 76.79 81.0
ML SVM 89.07 65.62 75.57
Table 5: Results obtained in terms of precision,
recall and F-score.
over, it also lists the results of Tu and Roth (2012)
and the VPCTagger evaluated in the 5-fold cross
validation manner, as Tu and Roth (2012) applied
this evaluation schema. As in the Tu&Roth dataset
positive and negative examples were also marked,
we were able to use accuracy as evaluation met-
ric besides the F
?=1
scores. It is revealed that
the dictionary lookup and the rule-based method
achieved an F-score of about 50, but our method
seems the most successful on this dataset, as it can
yield an accuracy 3.32% higher than that for the
Tu&Roth system.
Method Accuracy F-score
Dictionary Lookup 51.13 52.24
Rule Based 56.92 43.84
VPCTagger 81.92 85.69
Tu&Roth 78.6% ?
Table 6: 5-fold cross validation results on the
Tu&Roth dataset in terms of accuracy and F-score.
6 Discussion
The applied machine learning-based method
extensively outperformed our dictionary lookup
and rule-based baseline methods, which under-
lines the fact that our approach can be suitably
applied to VPC detection in raw texts. It is
well demonstrated that VPCs are very ambigu-
ous in raw text, as the dictionary lookup method
only achieved a precision score of 49.77% on the
Wiki50 corpus. This demonstrates that the auto-
matic detection of VPCs is a challenging task and
contextual features are essential. In the case of the
dictionary lookup, to achieve a higher recall score
was mainly limited by the size of the dictionary
used.
As Table 5 shows, VPCTagger achieved an F-
score 10% higher than those for the dependency
22
parsers, which may refer to the fact that our
machine learning-based approach performed well
on this task. This method proved to be the most
balanced as it got roughly the same recall, preci-
sion and F-score results on the Wiki50 corpus. In
addition, the dependency parsers achieve high pre-
cision with lower recall scores.
Moreover, the results obtained with our
machine learning approach on the Tu&Roth
dataset outperformed those reported in Tu and
Roth (2012). This may be attributed to the inclu-
sion of a rich feature set with new features like
semantic and contextual features that were used in
our system.
As Table 6 indicates, the dictionary lookup
and rule-based methods were less effective when
applied on the Tu&Roth dataset. Since the corpus
was created by collecting sentences that contained
phrasal verbs with specific verbs, this dataset con-
tains a lot of negative and ambiguous examples
besides annotated VPCs, hence the distribution of
VPCs in the Tu&Roth dataset is not comparable
to those in Wiki50, where each occurrence of a
VPCs were manually annotated in a running text.
Moreover, in this dataset, only one positive or neg-
ative example was annotated in each sentence, and
they examined just the verb-particle pairs formed
with the six verbs as a potential VPC. However,
the corpus probably contains other VPCs which
were not annotated. For example, in the sentence
The agency takes on any kind of job ? you just
name the subject and give us some indication of
the kind of thing you want to know, and then we
go out and get it for you., the only phrase takes on
was listed as a positive example in the Tu&Roth
dataset. But two examples, (go out ? positive and
get it for ? negative) were not marked. This is
problematic if we would like to evaluate our can-
didate extractor on this dataset as it would identify
all these phrases, even if it is restricted to verb-
particle pairs containing one of the six verbs men-
tioned above, thus yielding false positives already
in the candidate extraction phase.
In addition, this dataset contains 878 positive
VPC occurrences, but only 23 different VPCs.
Consequently, some positive examples were over-
represented. But the Wiki50 corpus may con-
tain some rare examples and it probably reflects
a more realistic distribution as it contains 342
unique VPCs.
A striking difference between the Tu & Roth
database and Wiki50 is that while Tu and Roth
(2012) included the verbs do and have in their
data, they do not occur at all among the VPCs
collected from Wiki50. Moreover, these verbs are
just responsible for 25 positive VPCs examples in
the Tu & Roth dataset. Although these verbs are
very frequent in language use, they do not seem
to occur among the most frequent verbal compo-
nents concerning VPCs. A possible reason for this
might be that VPCs usually contain a verb refer-
ring to movement in its original sense and neither
have nor do belong to motion verbs.
An ablation analysis was carried out to examine
the effectiveness of each individual feature types
of the machine learning based candidate classifi-
cation. Besides the feature classification described
in Section 4.3, we also examined the effectiveness
of the contextual features. In this case, the feature
which examined whether the candidates object
was a personal pronoun or not and the semantic
type of the prepositional object, object and subject
were treated as contextual features. Table 7 shows
the usefulness of each individual feature type on
the Wiki50 corpus. For each feature type, a J48
classifier was trained with all of the features except
that one. Then we compared the performance to
that got with all the features. As the ablation anal-
ysis shows, each type of feature contributed to the
overall performance. We found that the lexical
and orthographic features were the most powerful,
the semantic, syntactic features were also useful;
while contextual features were less effective, but
were still exploited by the model.
Features Prec. Rec. F-score Diff.
All 85.7 76.79 81.0 ?
Semantic 86.55 66.52 75.22 -5.78
Orthographic 83.26 65.85 73.54 -7.46
Syntax 84.31 71.88 77.6 -3.4
Lexical 89.68 60.71 72.41 -8.59
Contextual 86.68 74.55 80.16 -0.84
Table 7: The usefulness of individual features in
terms of precision, recall and F-score using the
Wiki50 corpus.
The most important features in our system are
lexical ones, namely, the lists of the most frequent
English verbs and particles. It is probably due to
the fact that the set of verbs used in VPCs is rather
limited, furthermore, particles form a closed word
class that is, they can be fully listed, hence the par-
23
ticle component of a VPC will necessarily come
from a well-defined set of words.
Besides the ablation analysis, we also investi-
gated the decision tree model produced by our
experiments. The model profited most from the
syntactic and lexical features, i.e. the dependency
label provided by the parsers between the verb and
the particle also played an important role in the
classification process.
We carried out a manual error analysis in order
to find the most typical errors our system made.
Most errors could be traced back to POS-tagging
or parsing errors, where the particle was classi-
fied as a preposition. VPCs that include an adverb
(as labeled by the POS tagger and the parser)
were also somewhat more difficult to identify, like
come across or go back. Preposition stranding (in
e.g. relative clauses) also resulted in false positives
like in planets he had an adventure on.
Other types of multiword expressions were also
responsible for errors. For instance, the system
classified come out as a VPC within the idiom
come out of the closet but the gold standard anno-
tation in Wiki50 just labeled the phrase as an idiom
and no internal structure for it was marked. A sim-
ilar error could be found for light verb construc-
tions, for example, run for office was marked as
a VPC in the data, but run for was classified as
a VPC, yielding a false positive case. Multiword
prepositions like up to also led to problems: in
he taught up to 1986, taught up was erroneously
labeled as VPC. Finally, in some cases, annotation
errors in the gold standard data were the source of
mislabeled candidates.
7 Conclusions
In this paper, we focused on the automatic detec-
tion of verb-particle combinations in raw texts.
Our hypothesis was that parsers trained on texts
annotated with extra information for VPCs can
identify VPCs in texts. We introduced our
machine learning-based tool called VPCTagger,
which allowed us to automatically detect VPCs
in context. We solved the problem in a two-step
approach. In the first step, we extracted poten-
tial VPCs from a running text with a syntax-
based candidate extraction method and we applied
a machine learning-based approach that made use
of a rich feature set to classify extracted syntactic
phrases in the second step. In order to achieve a
greater efficiency, we defined several new features
like semantic and contextual, but according to our
ablation analysis we found that each type of fea-
tures contributed to the overall performance.
Moreover, we also examined how syntactic
parsers performed in the VPC detection task on
the Wiki50 corpus. Furthermore, we compared
our methods with others when we evaluated our
approach on the Tu&Roth dataset. Our method
yielded better results than those got using the
dependency parsers on the Wiki50 corpus and the
method reported in (Tu and Roth, 2012) on the
Tu&Roth dataset.
Here, we also showed how dependency parsers
performed on identifying VPCs, and our results
indicate that although the dependency label pro-
vided by the parsers is an essential feature in
determining whether a specific VPC candidate is
a genuine VPC or not, the results can be further
improved by extending the system with additional
features like lexical and semantic features. Thus,
one possible application of the VPCTagger may be
to help dependency parsers: based on the output
of VPCTagger, syntactic labels provided by the
parsers can be overwritten. With backtracking, the
accuracy of syntactic parsers may increase, which
can be useful for a number of higher-level NLP
applications that exploit syntactic information.
In the future, we would like to improve our
system by defining more complex contextual fea-
tures. We also plan to examine how the VPCTag-
ger improve the performance of higher level NLP
applications like machine translation systems, and
we would also like to investigate the systematic
differences among the performances of the parsers
and VPCTagger, in order to improve the accuracy
of parsing. In addition, we would like to com-
pare different automatic detection methods of mul-
tiword expressions, as different types of MWEs
are manually annotated in the Wiki50 corpus.
Acknowledgments
Istv?an Nagy T. was partially funded by the
National Excellence Program T
?
AMOP-4.2.4.A/
2-11/1-2012-0001 of the State of Hungary, co-
financed by the European Social Fund. Veronika
Vincze was funded in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-
11/1/KONV-2012-0013).
24
References
Timothy Baldwin and Aline Villavicencio. 2002.
Extracting the unextractable: A case study on verb-
particles. In Proceedings of the 6th Conference on
Natural Language Learning - Volume 20, COLING-
02, pages 1?7, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Timothy Baldwin. 2005. Deep lexical acquisition of
verb-particle constructions. Computer Speech and
Language, 19(4):398?414, October.
Timothy Baldwin. 2008. A resource for evaluating
the deep lexical acquisition of English verb-particle
constructions. In Proceedings of the LREC Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of Coling 2010, pages 89?97.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks, volume 20. Kluwer Academic
Publishers.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Ray Jackendoff. 2002. English particle constructions,
the lexicon, and the autonomy of syntax. In Nicole
Deh, Ray Jackendoff, Andrew McIntyre, and Silke
Urban, editors, Verb-Particle Explorations, pages
67?94, Berlin / New York. Mouton de Gruyter.
Su Nam Kim and Timothy Baldwin. 2006. Automatic
identification of English verb particle constructions
using linguistic features. In Proceedings of the Third
ACL-SIGSEM Workshop on Prepositions, pages 65?
72.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Annual Meeting of the
ACL, volume 41, pages 423?430.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?331.
Francesca Masini. 2005. Multi-word expressions
between syntax and the lexicon: The case of Italian
verb-particle constructions. SKY Journal of Linguis-
tics, 18:145?173.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositional-
ity in phrasal verbs. In Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment - Volume 18, MWE
?03, pages 73?80, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Istv?an Nagy T. and Veronika Vincze. 2011. Identify-
ing Verbal Collocations in Wikipedia Articles. In
Proceedings of the 14th International Conference
on Text, Speech and Dialogue, TSD?11, pages 179?
186, Berlin, Heidelberg. Springer-Verlag.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San
Mateo, CA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In
Proceedings of CICLing 2002, pages 1?15, Mexico
City, Mexico.
Yuancheng Tu and Dan Roth. 2012. Sorting out the
Most Confusing English Phrasal Verbs. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings
of the Main Conference and the Shared Task, and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation, SemEval ?12,
pages 65?69, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aline Villavicencio. 2003. Verb-particle constructions
and lexical resources. In Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment - Volume 18, MWE
?03, pages 57?64, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Veronika Vincze, Istv?an Nagy T., and G?abor Berend.
2011. Multiword Expressions and Named Entities
in the Wiki50 Corpus. In Proceedings of RANLP
2011, pages 289?295, Hissar, Bulgaria, September.
RANLP 2011 Organising Committee.
Veronika Vincze, J?anos Zsibrita, and Istv?an Nagy T.
2013. Dependency Parsing for Identifying Hungar-
ian Light Verb Constructions. In Proceedings of
the Sixth International Joint Conference on Natu-
ral Language Processing, pages 207?215, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
25
LAW VIII - The 8th Linguistic Annotation Workshop, pages 64?69,
Dublin, Ireland, August 23-24 2014.
Annotating Uncertainty in Hungarian Webtext
Veronika Vincze
1,2
, Katalin Ilona Simk
?
o
1
, Viktor Varga
1
1
University of Szeged
Department of Informatics
2
MTA-SZTE Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
{kata.simko,viktor.varga.1991}@gmail.com
Abstract
Uncertainty detection has been a popular topic in natural language processing, which manifested
in the creation of several corpora for English. Here we show how the annotation guidelines origi-
nally developed for English standard texts can be adapted to Hungarian webtext. We annotated a
small corpus of Facebook posts for uncertainty phenomena and we illustrate the main character-
istics of such texts, with special regard to uncertainty annotation. Our results may be exploited
in adapting the guidelines to other languages or domains and later on, in the construction of
automatic uncertainty detectors.
1 Background
Detecting uncertainty in natural language texts has received a considerable amount of attention in the
last decade (Farkas et al., 2010; Morante and Sporleder, 2012). Several manually annotated corpora have
been created, which serve as training and test databases of state-of-the-art uncertainty detectors based
on supervised machine learning techniques. Most of these corpora are constructed for English, however,
their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008;
Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner
et al., 2009), pieces of news (Saur?? and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin,
2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze,
2013), reviews (Konstantinova et al., 2012; Cruz D??az, 2013) and tweets (Wei et al., 2013) have been
annotated for uncertainty, just to mention a few examples.
The diversity of the resources also manifests in the fact that the annotation principles behind the cor-
pora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three
corpora (BioScope, FactBank and WikiWeasel) and they offered a unified classification of semantic
uncertainty phenomena, on the basis of which these corpora were reannotated, using uniform guide-
lines. Some other uncertainty-related linguistic phenomena are described as discourse-level uncertainty
in Vincze (2013). As a first objective of our paper, we will carry out a pilot study and investigate how
these unified guidelines can be adapted to texts written in a language that is typologically different from
English, namely, Hungarian.
As a second goal, we will also focus on annotating texts in a new domain: social media texts ?
apart from Wei et al. (2013) ? have not been extensively investigated from the uncertainty detection
perspective. As the use and communication through the internet is becoming more and more important
in people?s lives, the huge amount of data available from this domain is a valuable source of information
for computation linguistics. However, processing texts from the web ? especially social media texts from
blogs, status updates, chat logs and comments ? revealed that they are very challenging for applications
trained on standard texts. Most studies in this area focus on English, for instance, sentiment analysis
from tweets has been the focus of recent challenges (Wilson et al., 2013) and Facebook posts have
been analysed from the perspective of computational psychology (Celli et al., 2013). A syntactically
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
64
annotated treebank of webtext has been also created for English (Bies et al., 2012). However, methods
developed for processing English webtext require serious alterations to be applicable to other languages,
for example Hungarian, which is very different from English syntactically and morphologically. Thus,
in our pilot study we will annotate Hungarian webtext for uncertainty and examine the possible effects
of the domain and the language on uncertainty detection.
In the following, we will present the uncertainty categories that were annotated in Hungarian webtext
and we will illustrate the difficulties of both annotating Hungarian webtext and annotating uncertainty
phenomena in them.
2 Uncertainty Categories
Here we just briefly summarize uncertainty categories that we applied in the annotation, based on Szarvas
et al. (2012) and Vincze (2013).
Linguistic uncertainty is related to modality and the semantics of the sentence. For instance, the
sentence It may be raining does not contain enough information to determine whether it is really raining
(semantic uncertainty). There are several phenomena that are categorized as semantic uncertainty. A
proposition is epistemically uncertain if its truth value cannot be determined on the basis of world
knowledge. Conditionals and investigations also belong to this group ? the latter is characteristic of
research papers, where research questions usually express this type of uncertainty. Non-epistemic types
of modality are also be listed here such as doxastic uncertainty, which is related to beliefs.
However, there are other linguistic phenomena that only become uncertain within the context of com-
munication. For instance, the sentence Many people think that Dublin is the best city in the world does
not reveal who exactly think that, hence the source of the proposition about Dublin remains uncertain.
This is a type of discourse-level uncertainty, more specifically, it is called weasel (Ganter and Strube,
2009). On the other hand, hedges make the meaning of words fuzzy: they blur the exact meaning of some
quality/quantity. Finally, peacock cues express unprovable evaluations, qualifications, understatements
and exaggerations.
The above categories proved to be applicable to Hungarian texts as well. However, the morpholog-
ically rich nature of Hungarian required some slight changes in the annotation process. For instance,
modal auxiliaries like may correspond to a derivational suffix in Hungarian, which required that in the
case of j?ohet ?may come? the whole word was annotated as uncertain, not just the suffix -het.
3 Annotating Hungarian Webtext
Annotating uncertainty in webtexts comes with the usual difficulties of working with this domain. We
annotated Hungarian posts and comments from Facebook, which made the uncertainty annotation more
challenging than on standard texts. Texts were randomly selected from the public posts available at the
Facebook-sites of some well-known brands (like mobile companies, electronic devices, nutrition expert
companies etc.) and from the comments that users made on these posts. For our pilot annotation, we
used 1373 sentences and 18,327 tokens (as provided by magyarlanc, a linguistic preprocessing toolkit
developed for standard Hungarian texts (Zsibrita et al., 2013)).
One fundamental property of social media texts is their similarity to oral communication despite their
written form. The communication is online and multimodal; its speed causing a number of possibilities
for error. The quick typing makes typos, abbreviations and lack of capitalization, punctuation and accen-
tuated letters more common in these texts. Accentuated and unaccentuated vowels represent different
sounds in Hungarian that can change the meaning of words (kerek ?round?, ker?ek ?wheel? and k?erek ?I
want?). Other types of linguistic creativity are also common, such as the use of emoticons and English
words and abbreviations in Hungarian texts. However, these attributes do not characterize social media
texts homogeneously. For instance, blog posts are closer to standard texts since they are usually written
by a PR expert from the side of the brand, who presumably spends more time with elaborating on the
text of the posts than an average user. On the other hand, comments and chat texts are closer to oral
communication because users here want to react as quickly as possible, making them harder to analyze.
65
Our corpus of Facebook posts and comments exhibited a number of these properties. It contained
a lot of typos, abbreviations and letters that should have been accentuated. These sometimes caused
interpretation problems even for the human annotators; especially as these posts and comments were
annotated without broader context. Lack of capitalization and punctuation was more common in the
comment section of the corpus than in the posts. Emoticons were also frequent in both parts of the
corpus.
Example 1: Typos in our corpus.
ugya ilynem van csak fekete el?ol ?es sz?urke h?at ?ul ? original
ugyanilyenem van csak fekete el
?
ol ?es sz?urke h
?
atul ? standardized
(same.kind-POSS1SG have but black front and grey back)
?I have the same kind but its front is black and its back is grey.?
Example 2: Abbreviation in our corpus.
Am?ugy meg sztem Nektek nem kellene a Saj?at oldalatokon magyar?azkodni! ? original
Am?ugy meg szerintem Nektek nem kellene a saj?at oldalatokon magyar?azkodni! ? standard-
ized
(by.the.way PART according.to-POSS1SG you-DAT not should the own site-POSS3PL-SUP
explain.yourselves-INF)
?By the way I think you should not be explaining yourselves on your own site.?
Example 3: Lack of accentuation in our corpus.
es Marai Sandornak is ma van a szuletesnapja. ? original
?es M?arai S?andornak is ma van a sz?ulet?esnapja. ? standardized
(and M?arai S?andor-GEN also today has the birthday-POSS3SG)
?And today is also M?arai S?andor?s birthday?
4 Uncertainty in Hungarian Webtext
Apart from the above mentioned usual problems when dealing with webtext, other difficulties emerged
during their uncertainty annotation. Uncertainty is often related to opinions, but writers of these texts do
not usually express these as opinions, but as factual elements. Linguistic uncertainty is not annotated in
these cases, as these sentences do not hold uncertain meanings semantically, even if certain facts in them
are clearly not true or at least the writers obviously lack evidence to back them up.
Example 4: Information without evidence in our corpus.
?
Uj megfigyel?es, hogy az elektronok ?ugy viselkednek, mint az antioxid?ansok.
(new observation that the electrons that.way behave as the antioxidants)
?It is a new observation that electrons behave as antioxidants.?
The uncertainty annotation of this text differed greatly from our corpus of Hungarian Wikipedia arti-
cles and news (Vincze, 2014), which domains are much closer to standard language use. Table 1 shows
the distribution of the different types of uncertainty cues in these domains. Comparing this new subcorpus
with the other two shows certain domain specific characteristics. Unlike Facebook posts and comments,
the other two domains should not contain subjective opinions according to the objective nature of news
media and encyclopedias. This is consistent with the difference in the proportion of peacock cues in each
subcorpus: Facebook posts abound in them but their number is low in the other types of texts.
The relatively small number of hedges and epistemic uncertainty may be attributed to the previously
mentioned observation that the writers of these posts and comments often make confident statements,
even if these are not actual facts.
66
The resemblance of Facebook posts and comments to oral communication also means that elements
that could also signify uncertainty can have different uses in this context. Certain phrases may indi-
cate politeness or other pragmatic functions that in a different domain would mean and be annotated as
linguistic uncertainty.
Example 5: The use of uncertain elements for politeness reasons in our corpus.
sajnos ?ugy t?unik a fut?araink valami?ert val?oban nem ?erkeztek meg hozz?atok szombaton
(unfortunately that.way seems the carriers-POSS1PL something-CAU really not arrive-PAST-
3PL you-ALL Saturday-SUP)
?Unfortunately it seems like our carriers did not get to you on Saturday for some reason.?
The phrase ?ugy t?unik ?it seems? can express uncertainty in some contexts, but in the above example,
it is used as a marker of politeness, in order to apologize for and mitigate the inconvenience they caused
to their customers by not delivering some package in time.
Uncertainty cue Wikipedia News Webtext
# % # % # %
Weasel 1801 32.02 258 10.93 50 9.72
Hedge 2098 37.3 799 33.86 147 28.59
Peacock 787 14 94 3.98 192 37.35
Discourse-level total 4686 83.3 1151 48.77 389 75.6
Epistemic 439 7.8 358 15.16 21 4.08
Doxastic 315 5.6 710 30.08 44 8.56
Conditional 154 2.74 128 5.42 59 11.47
Investigation 31 0.55 13 0.55 1 0.19
Semantic total 939 16.69 1209 51.22 125 24.3
Total 5625 100 2360 100 514 100
Table 1: Uncertainty cues.
5 Conclusions
In this paper, we focused on annotating Hungarian Facebook posts and comments for uncertainty phe-
nomena. We adapted guidelines proposed for uncertainty annotation of standard English texts to Hun-
garian, and we also showed that this domain exhibit certain characteristics which are not present in other
domains that are more similar to standard language use. First, users usually express their opinions as
facts, thus relatively less markers of hedges or epistemic uncertainty occur in the corpus. Second, uncer-
tainty cue candidates can fulfill politeness functions, and apparently they do not signal uncertainty in
these contexts. Third, the characteristics of webtext may cause difficulties in annotation since in some
cases, the meaning of the text is vague due to typos or other errors.
Our pilot study of annotating Hungarian webtext for uncertainty leads us to conclude that the annota-
tion guidelines are mostly applicable to Hungarian as well and webtexts also exhibit the same uncertainty
categories as more standard texts, although the distribution of uncertainty categories differ among differ-
ent types of text. Besides, politeness factors should get more attention in this domain. Our results may be
employed in adapting annotation guidelines of uncertainty to other languages or domains as well. Later
on, we would like to extend our corpus and we would like to implement machine learning methods to
automatically detect uncertainty in Hungarian webtext, for which these findings will be most probably
fruitfully exploited.
Acknowledgements
This research was funded in part by the European Union and the European Social Fund through the
project FuturICT.hu (grant no.: T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013). Veronika Vincze was par-
67
tially funded by the National Excellence Program T
?
AMOP-4.2.4.A/2-11/1-2012-0001 of the State of
Hungary, co-financed by the European Social Fund.
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. Linguistic Data Consortium,
Philadelphia.
Fabio Celli, Fabio Pianesi, David Stilwell, and Michal Kosinski. 2013. Extracting evaluative conditions from
online reviews: Toward enhancing opinion mining. In Workshop on Computational Personality Recognition,
Boston, July.
Noa P. Cruz D??az. 2013. Detecting negated and uncertain information in biomedical and review texts. In Proceed-
ings of the Student Research Workshop associated with RANLP 2013, pages 45?50, Hissar, Bulgaria, September.
RANLP 2013 Organising Committee.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the
Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1?
12, Uppsala, Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia
Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 173?176, Suntec, Singapore, August. Association for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008. Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(Suppl 10).
Natalia Konstantinova, Sheila C.M. de Sousa, Noa P. Cruz, Manuel J. Mana, Maite Taboada, and Ruslan Mitkov.
2012. A review corpus annotated for negation, speculation and their scope. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and
Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Ben Medlock and Ted Briscoe. 2007. Weakly Supervised Learning for Hedge Classification in Scientific Litera-
ture. In Proceedings of the ACL, pages 992?999, Prague, Czech Republic, June.
Roser Morante and Caroline Sporleder. 2012. Modality and negation: An introduction to the special issue.
Computational Linguistics, 38:223?260, June.
Raheel Nawaz, Paul Thompson, and Sophia Ananiadou. 2010. Evaluating a meta-knowledge annotation scheme
for bio-events. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,
pages 69?77, Uppsala, Sweden, July. University of Antwerp.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando. 2005. Certainty identification in texts: Categorization
model and manual tagging results. In J.G. Shanahan, J. Qu, and J. Wiebe, editors, Computing attitude and affect
in text: Theory and applications (the information retrieval series), New York. Springer Verlag.
Victoria L. Rubin. 2010. Epistemic modality: From uncertainty to certainty in the context of information seeking
as interactions with texts. Information Processing & Management, 46(5):533?540.
Roser Saur?? and James Pustejovsky. 2009. FactBank: a corpus annotated with event factuality. Language
Resources and Evaluation, 43:227?268.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings
of the NIPS Workshop on Cost-Sensitive Learning, pages 1?10.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and W. John Wilbur. 2008. Multi-dimensional classification of
biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics,
24(18):2086?2093.
Gy?orgy Szarvas, Veronika Vincze, Rich?ard Farkas, Gy?orgy M?ora, and Iryna Gurevych. 2012. Cross-genre and
cross-domain detection of semantic uncertainty. Computational Linguistics, 38:335?367, June.
?
Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda. 2009. Machine Learning and Rule-based Approaches to
Assertion Classification. Journal of the American Medical Informatics Association, 16(1):109?115, January.
68
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas, Gy?orgy M?ora, and J?anos Csirik. 2008. The BioScope Corpus:
Biomedical Texts Annotated for Uncertainty, Negation and their Scopes. BMC Bioinformatics, 9(Suppl 11):S9.
Veronika Vincze. 2013. Weasels, Hedges and Peacocks: Discourse-level Uncertainty in Wikipedia Articles.
In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 383?391,
Nagoya, Japan, October. Asian Federation of Natural Language Processing.
Veronika Vincze. 2014. Uncertainty detection in Hungarian texts. In Proceedings of Coling 2014.
Zhongyu Wei, Junwen Chen, Wei Gao, Binyang Li, Lanjun Zhou, Yulan He, and Kam-Fai Wong. 2013. An empir-
ical study on uncertainty identification in social media context. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), pages 58?62, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ?13, June.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh, Pittsburgh.
J?anos Zsibrita, Veronika Vincze, and Rich?ard Farkas. 2013. magyarlanc: A Toolkit for Morphological and Depen-
dency Parsing of Hungarian. In Proceedings of RANLP-2013, pages 763?771, Hissar, Bulgaria.
69
