Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 29?37,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Opinum: statistical sentiment analysis for opinion classification
Boyan Bonev, Gema Ram??rez-Sa?nchez, Sergio Ortiz Rojas
Prompsit Language Engineering
Avenida Universidad, s/n. Edificio Quorum III.
03202 Elche, Alicante (Spain)
{boyan,gramirez,sortiz}@prompsit.com
Abstract
The classification of opinion texts in positive
and negative can be tackled by evaluating sep-
arate key words but this is a very limited ap-
proach. We propose an approach based on the
order of the words without using any syntac-
tic and semantic information. It consists of
building one probabilistic model for the posi-
tive and another one for the negative opinions.
Then the test opinions are compared to both
models and a decision and confidence mea-
sure are calculated. In order to reduce the
complexity of the training corpus we first lem-
matize the texts and we replace most named-
entities with wildcards. We present an accu-
racy above 81% for Spanish opinions in the
financial products domain.
1 Introduction
Most of the texts written by humans reflect some
kind of sentiment. The interpretation of these sen-
timents depend on the linguistic skills and emo-
tional intelligence of both the author and the reader,
but above all, this interpretation is subjective to the
reader. They don?t really exist in a string of charac-
ters, for they are subjective states of mind. Therefore
sentiment analysis is a prediction of how most read-
ers would react to a given text.
There are texts which intend to be objective and
texts which are intentionally subjective. The latter is
the case of opinion texts, in which the authors inten-
tionally use an appropriate language to express their
positive or negative sentiments about something. In
this paper we work on the classification of opinions
in two classes: those expressing positive sentiment
(the author is in favour of something) and those ex-
pressing negative sentiment, and we will refer to
them as positive opinions and negative opinions.
Sentiment analysis is possible thanks to the opin-
ions available online. There are vast amounts of text
in fora, user reviews, comments in blogs and social
networks. It is valuable for marketing and sociolog-
ical studies to analyse these freely available data on
some definite subject or entity. Some of the texts
available do include opinion information like stars,
or recommend-or-not, but most of them do not. A
good corpus for building sentiment analysis systems
would be a set of opinions separated by domains. It
should include some information about the cultural
origin of authors and their job, and each opinion
should be sentiment-evaluated not only by its own
author, but by many other readers as well. It would
also be good to have a marking of the subjective and
objective parts of the text. Unfortunately this kind
of corpora are not available at the moment.
In the present work we place our attention at the
supervised classification of opinions in positive and
negative. Our system, which we call Opinum1, is
trained from a corpus labeled with a value indicat-
ing whether an opinion is positive or negative. The
corpus was crawled from the web and it consists of a
160MB collection of Spanish opinions about finan-
cial products. Opinum?s approach is general enough
and it is not limited to this corpus nor to the financial
domain.
There are state-of-the-art works on sentiment
1An Opinum installation can be tested from a web interface
at http://aplica.prompsit.com/en/opinum
29
analysis which care about differentiating between
the objective and the subjective part of a text. For
instance, in the review of a film there is an objec-
tive part and then the opinion (Raaijmakers et al,
2008). In our case we work directly with opinion
texts and we do not make such difference. We have
noticed that in customer reviews, even when stating
objective facts, some positive or negative sentiment
is usually expressed.
Many works in the literature of sentiment anal-
ysis take lexicon-based approaches (Taboada et al,
2011). For instance (Hu and Liu, 2004; Blair-
Goldensohn et al, 2008) use WordNet to extend
the relation of positive and negative words to other
related lexical units. However the combination of
which words appear together may also be impor-
tant and there are comparisons of different Ma-
chine learning approaches (Pang et al, 2002) in
the literature, like Support Vector Machines, k-
Nearest Neighbours, Naive-Bayes, and other classi-
fiers based on global features. In (McDonald et al,
2007) structured models are used to infer the senti-
ment from different levels of granularity. They score
cliques of text based on a high-dimensional feature
vector.
In the Opinum approach we score each sentence
based on its n-gram probabilites. For a complete
opinion we sum the scores of all its sentences. Thus,
if an opinion has several positive sentences and it fi-
nally concludes with a negative sentence which set-
tles the whole opinion as negative, Opinum would
probably fail. The n-gram sequences are good at
capturing phrasemes (multiwords), the motivation
for which is stated in Section 2. Basically, there
are phrasemes which bear sentiment. They may
be different depending on the domain and it is rec-
ommendable to build the models with opinions be-
longing to the target domain, for instance, financial
products, computers, airlines, etc. A study of do-
main adaptation for sentiment analysis is presented
in (Blitzer et al, 2007). In Opinum different clas-
sifiers would be built for different domains. Build-
ing the models does not require the aid of experts,
only a labeled set of opinions is necessary. Another
contribution of Opinum is that it applies some sim-
plifications on the original text of the opinions for
improving the performance of the models.
In the remainder of the paper we first state the mo-
tivation of our approach in Section 2, then in Sec-
tion 3 we describe in detail the Opinum approach.
In Section 4 we present our experiments with Span-
ish financial opinions and we state some conclusions
and future work in Section 5.
2 Hypothesis
When humans read an opinion, even if they do
not understand it completely because of the techni-
cal details or domain-specific terminology, in most
cases they can notice whether it is positive or nega-
tive. The reason for this is that the author of the opin-
ion, consciously or not, uses nuances and structures
which show a positive or negative feeling. Usually,
when a user writes an opinion about a product, the
intention is to communicate that subjective feeling,
apart from describing the experience with the prod-
uct and giving some technical details.
The hypothesis underlying the traditional
keyword or lexicon-based approaches (Blair-
Goldensohn et al, 2008; Hu and Liu, 2004) consist
in looking for some specific positive or negative
words. For instance, ?great? should be positive and
?disgusting? should be negative. Of course there
are some exceptions like ?not great?, and some
approaches detect negation to invert the meaning of
the word. More elaborate cases are constructions
like ?an offer you can?t refuse? or ?the best way to
lose your money?.
There are domains in which the authors of the
opinions might not use these explicit keywords. In
the financial domain we can notice that many of the
opinions which express the author?s insecurity are
actually negative, even though the words are mostly
neutral. For example, ?I am not sure if I would get
a loan from this bank? has a negative meaning. An-
other difficulty is that the same words could be posi-
tive or negative depending on other words of the sen-
tence: ?A loan with high interests? is negative while
?A savings account with high interests? is positive.
In general more complex products have more com-
plex and subtle opinions. The opinion about a cud-
dly toy would contain many keywords and would be
much more explicit than the opinion about the con-
ditions of a loan. Even so, the human readers can
get the positive or negative feeling at a glance.
The hypothesis of our approach is that it is pos-
30
sible to classify opinions in negative and positive
based on canonical (lemmatized) word sequences.
Given a set of positive opinions Op and a set of
negative opinions On, the probability distributions
of their n-gram word sequences are different and
can be compared to the n-grams of a new opin-
ion in order to classify it. In terms of statistical
language models, given the language models M p
and M n obtained from Op and On, the probability
ppo = P (o|Op) that a new opinion would be gener-
ated by the positive model is smaller or greater than
the probability pno = P (o|O
N ) that a new opinion
would be generated by the negative model.
We build the models based on sequences of
canonical words in order to simplify the text, as ex-
plained in the following section. We also replace
some named entities like names of banks, organiza-
tions and people by wildcards so that the models do
not depend on specific entities.
3 The Opinum approach
The proposed approach is based on n-gram language
models. Therefore building a consistent model is the
key for its success. In the field of machine transla-
tion a corpus with size of 500MB is usually enough
for building a 5-gram language model, depending on
the morphological complexity of the language.
In the field of sentiment analysis it is very diffi-
cult to find a big corpus of context-specific opinions.
Opinions labeled with stars or a positive/negative la-
bel can be automatically downloaded from differ-
ent customers? opinion websites. The sizes of the
corpora collected that way range between 1MB and
20MB for both positive and negative opinions.
Such a small amount of text would be suitable for
bigrams and would capture the difference between
?not good? and ?really good?, but this is not enough
for longer sequences like ?offer you can?t refuse?.
In order to build consistent 5-gram language mod-
els we need to simplify the language complexity by
removing all the morphology and replacing the sur-
face forms by their canonical forms. Therefore we
make no difference between ?offer you can?t refuse?
and ?offers you couldn?t refuse?.
We also replace named entities by wildcards: per-
son entity, organization entity and company entity.
Although these replacements also simplify the lan-
guage models to some extent, their actual purpose
is to avoid some negative constructions to be as-
sociated to concrete entities. For instance, we do
not care that ?do not trust John Doe Bank? is neg-
ative, instead we prefer to know that ?do not trust
company entity? is negative regardless of the entity.
This generality allows us to better evaluate opinions
about new entities. Also, in the cases when all the
opinions about some entity E1 are good and all the
opinions about some other entity E2 are bad, entity
replacement prevents the models from acquiring this
kind of bias.
Following we detail the lemmatization process,
the named entities detection and how we build and
evaluate the positive and negative language models.
3.1 Lemmatization
Working with the words in their canonical form is
for the sake of generality and simplification of the
language model. Removing the morphological in-
formation does not change the semantics of most
phrasemes (or multiwords).
There are some lexical forms for which we keep
the surface form or we add some morphological in-
formation to the token. These exceptions are the
subject pronouns, the object pronouns and the pos-
sessive forms. The reason for this is that for some
phrasemes the personal information is the key for
deciding the positive or negative sense. For instance,
let us suppose that some opinions contain the se-
quences
ot = ?They made money from me?,
oi = ?I made money from them?.
Their lemmatization, referred to as L0(?), would be2
L0(ot) = L0(oi) = ?SubjectPronoun make money
from ObjectPronoun?,
Therefore we would have equally probable
P (ot|Mp) = P (oi|Mp) and P (ot|Mn) =
P (oi|Mn), which does not express the actual
sentiment of the phrasemes. In order to capture this
2The notation we use here is for the sake of readability and
it slightly differs from the one we use in Opinum.
31
kind of differences we prefer to have
L1(ot) = ?SubjectPronoun 3p make money
from ObjectPronoun 1p?,
L1(oi) = ?SubjectPronoun 1p make money
from ObjectPronoun 3p?.
The probabilities still depend on how many times do
these lexical sequences appear in opinions labeled as
positive or negative, but with L1(?) we would have
that
P (ot|M
p) < P (oi|M
p),
P (ot|M
n) > P (oi|M
n),
that is, oi fits better the positive model than ot does,
and vice versa for the negative model.
In our implementation lemmatization is per-
formed with Apertium, which is an open-source
rule-based machine translation engine. Thanks to
its modularized architecture (described in (Tyers et
al., 2010)) we use its morphological analyser and
its part-of-speech disambiguation module in order
to take one lexical form as the most probable one,
in case there are several possibilities for a given sur-
face. Apertium currently has morphological anal-
ysers for 30 languages (most of them European),
which allows us to adapt Opinum to other languages
without much effort.
3.2 Named entities replacement
The corpora with labeled opinions are usually lim-
ited to a number of enterprises and organizations.
For a generalization purpose we make the texts in-
dependent of concrete entities. We do make a differ-
ence between names of places, people and organiza-
tions/companies. We also detect dates, phone num-
bers, e-mails and URL/IP. We substitute them all by
different wildcards. All the rest of the numbers are
substituted by a ?Num? wildcard. For instance, the
following subsequence would have aL2(oe) lemma-
tization + named entity substitution:
oe = ?Joe bought 300 shares
of Acme Corp. in 2012?
L2(oe) = ?Person buy Num share
of Company in Date?
The named entity recognition task is integrated
within the lemmatization process. We collected a
list of names of people, places, companies and orga-
nizations to complete the morphological dictionary
of Apertium. The morphological analysis module is
still very fast, as the dictionary is first compiled and
transformed to the minimal deterministic finite au-
tomaton. For the dates, phone numbers, e-mails, IP
and URL we use regular expressions which are also
supported by the same Apertium module.
Regarding the list of named entities, for a given
language (Spanish in our experiments) we download
its Wikipedia database which is a freely available re-
source. We heuristically search it for organizations,
companies, places and people. Based on the number
of references a given entity has in Wikipedia?s arti-
cles, we keep the first 1.500.000 most relevant en-
tities, which cover the entities with 4 references or
more (the popular entities are referenced from tens
to thousands of times).
Finally, unknown surface forms are replaced by
the ?Unknown? lemma (the known lemmas are low-
ercase). These would usually correspond to strange
names of products, erroneous words and finally to
words which are not covered by the monolingual
dictionary of Apertium. Therefore our approach is
suitable for opinions written in a rather correct lan-
guage. If unknown surfaces were not replaced, the
frequently misspelled words would not be excluded,
which is useful in some domains. This is at the cost
of increasing the complexity of the model, as all mis-
spelled words would be included. Alternatively, the
frequently misspelled words could be added to the
dictionary.
3.3 Language models
The language models we build are based on n-gram
word sequences. They model the likelihood of a
wordwi given the sequence of n?1 previous words,
P (wi|wi?(n?1), . . . , wi?1). This kind of models as-
sume independence between the word wi and the
words not belonging to the n-gram, wj , j < i ? n.
This is a drawback for unbounded dependencies
but we are not interested in capturing the complete
grammatical relationships. We intend to capture
the probabilities of smaller constructions which may
hold positive/negative sentiment. Another assump-
tion we make is independence between different sen-
32
tences.
In Opinum the words are lemmas (or wildcards
replacing entities), and the number of words among
which we assume dependence is n = 5. A max-
imum n of 5 or 6 is common in machine transla-
tion where huge amounts of text are used for build-
ing a language model (Kohen et al, 2007). In our
case we have at our disposal a small amount of data
but the language is drastically simplified by remov-
ing the morphology and entities, as previously ex-
plained. We have experimentally found that n > 5
does not improve the classification performance of
lemmatized opinions and could incur over-fitting.
In our setup we use the IRSTLM open-source li-
brary for building the language model. It performs
an n-gram count for all n-grams from n = 1 to
n = 5 in our case. To deal with data sparseness
a redistribution of the zero-frequency probabilities
is performed for those sets of words which have not
been observed in the training set L(O). Relative fre-
quencies are discounted to assign positive probabil-
ities to every possible n-gram. Finally a smoothing
method is applied. Details about the process can be
found in (Federico et al, 2007). For Opinum we run
IRSTLM twice during the training phase: once tak-
ing as input the opinions labeled as positive and once
taking the negatives:
M p ? Irstlm (L (Op))
M n ? Irstlm (L (On))
These two models are further used for querying new
opinions on them and deciding whether it is positive
or negative, as detailed in the next subsection.
3.4 Evaluation and confidence
In the Opinum system we query the M p ,M n mod-
els with the KenLM (Heafield, 2011) open-source
library because it answers the queries very quickly
and has a short loading time, which is suitable for
a web application. It also has an efficient mem-
ory management which is positive for simultaneous
queries to the server.
The queries are performed at sentence level. Each
sentence s ? ot is assigned a score which is the log
probability of the sentence being generated by the
language model. The decision is taken by compar-
ing its scores for the positive and for the negative
models. For a given opinion ot, the log-probability
sums can be taken:
dot =
?
s?ot
logP (s|M p)?
?
s?ot
logP (s|M n) ?
?
0
If this difference is close to zero, |dot |/wot < ?0,
it can be considered that the classification is neutral.
The number of words wot is used as a normalization
factor. If it is large, |dot |/wot > ?1, it can be con-
sidered that the opinion has a very positive or very
negative sentiment. Therefore Opinum classifies the
opinions with qualifiers: very/somewhat/little posi-
tive/negative depending on the magnitude |dot |/wot
and sign(dot), respectively.
The previous assessment is also accompanied by a
confidence measure given by the level of agreement
among the different sentences of an opinion. If all its
sentences have the same positivity/negativity, mea-
sured by sign(dsj ), sj ? o, with large magnitudes
then the confidence is the highest. In the opposite
case in which there is the same number of positive
and negative sentences with similar magnitudes the
confidence is the lowest. The intermediate cases are
those with sentences agreeing in sign but some of
them with very low magnitude, and those with most
sentences of the same sign and some with different
sign. We use Shannon?s entropy measure H(?) to
quantify the amount of disagreement. For its esti-
mation we divide the range of possible values of d
in B ranges, referred to as bins:
Hot =
B?
b=1
p(db) log
1
p(db)
.
The number of bins should be low (less than 10),
otherwise it is difficult to get a low entropy mea-
sure because of the sparse values of db. We set two
thresholds ?0 and ?1 such that the confidence is said
to be high/normal/low if Hot < ?0, ?0 < Hot < ?1
or Hot > ?1, respectively
The thresholds ?, ? and the number of bins B
are experimentally set. The reason for this is that
they are used to tune subjective qualifiers (very/little,
high/low confidence) and will usually depend on the
training set and on the requirements of the applica-
tion. Note that the classification in positive or neg-
ative sentiment is not affected by these parameters.
33
From a human point of view it is also a subjective
assessment but in our setup it is looked at as a fea-
ture implicitly given by the labeled opinions of the
training set.
4 Experiments and results
In our experimental setup we have a set of positive
and negative opinions in Spanish, collected from a
web site for user reviews and opinions. The opin-
ions are constrained to the financial field including
banks, savings accounts, loans, mortgages, invest-
ments, credit cards, and all other related topics. The
authors of the opinions are not professionals, they
are mainly customers. There is no structure required
for their opinions, and they are free to tell their ex-
perience, their opinion or their feeling about the en-
tity or the product. The users meant to communicate
their review to other humans and they don?t bear in
mind any natural language processing tools. The au-
thors decide whether their own opinion is positive or
negative and this field is mandatory.
The users provide a number of stars as well: from
one to five, but we have not used this information. It
is interesting to note that there are 66 opinions with
only one star which are marked as positive. There
are also 67 opinions with five stars which are marked
as negative. This is partially due to human errors,
a human can notice when reading them. However
we have not filtered these noisy data, as removing
human errors could be regarded as biasing the data
set with our own subjective criteria.
Regarding the size of the corpus, it consists of
9320 opinions about 180 different Spanish banks
and financial products. From these opinions 5877
are positive and 3443 are negative. There is a total of
709741 words and the mean length of the opinions
is 282 words for the positive and 300 words for the
negative ones. In the experiments we present in this
work, we randomly divide the data set in 75% for
training and 25% for testing. We check that the dis-
tribution of positive and negative remains the same
among test and train.
After the L2(?) lemmatization and entity substitu-
tion, the number of different words in the data set is
13067 in contrast with the 78470 different words in
the original texts. In other words, the lexical com-
plexity is reduced by 83%. Different substitutions
play a different role in this simplification. The ?Un-
known? wildcard represents a 7,13% of the origi-
nal text. Entities were detected and replaced 33858
times (7807 locations, 5409 people, 19049 com-
panies, 502 e-mails addresses and phone numbers,
2055 URLs, 1136 dates) which is a 4,77% of the
text. There are also 46780 number substitutions, a
7% of the text. The rest of complexity reduction is
due to the removal of the morphology as explained
in Subsection 3.1.
In our experiments, the training of Opinum con-
sisted of lemmatizing and susbstituting entities of
the 6990 opinions belonging the training set and
building the language models. The positive model
is built from 4403 positive opinions and the neg-
ative model is built from 2587 negative opinions.
Balancing the amount of positive and negative sam-
ples does not improve the performance. Instead, it
obliges us to remove an important amount of pos-
itive opinions and the classification results are de-
creased by approximately 2%. This is why we use
all the opinions available in the training set. Both
language models are n-grams with n ? [1, 5]. Hav-
ing a 37% less samples for the negative opinions
is not a problem thank to the smoothing techniques
applied by IRSTLM. Nonetheless if the amount of
training texts is too low we would recommend tak-
ing a lower n. A simple way to set n is to take
the lowest value of n for which classification perfor-
mance is improved. An unnecessarily high n could
overfit the models.
The tests are performed with 2330 opinions (not
involved in building the models). For measuring the
accuracy we do not use the qualifiers information
but only the decision about the positive or negative
class. In Figure 1 we show the scores of the opin-
ions for the positive and negative models. The score
is the sum of scores of the sentences, thus it can be
seen that longer opinions (bigger markers) have big-
ger scores. Independence of the size is not necessary
for classifying in positive and negative. In the diag-
onal it can be seen that positive samples are close
to the negative ones, this is to be expected: both
positive and negative language models are built for
the same language. However the small difference
in their scores yields an 81,98% success rate in the
classification. An improvement of this rate would be
difficult to achieve taking into account that there is
34
Test Original Spanish text Meaning in English Result
Similar
words,
different
meaning
?Al tener la web, no pierdes
el tiempo por tele?fono.?
As you have the website you
don?t waste time on the phone.
Positive
?En el telfono os hacen perder
el tiempo y no tienen web.?
They waste your time on the phone
and they don?t have a website.
Negative
?De todas formas me
solucionaron el problema.?
Anyway, they solved my problem. Positive
?No hay forma de que
me solucionen el problema.?
There is no way to make them
solve my problem.
Negative
A negative
opinion
of several
sentences
?Con XXXXXX me fue muy bien.? I was fine with XXXXXX. Positive
?Hasta que surgieron los problemas.? Until the problems began. Negative
?Por hacerme cliente me regalaban
100 euros.?
They gave me 100 euros for
becoming a client.
Positive
?Pero una vez que eres cliente
no te aportan nada bueno.?
But once you are a client, they
they do not offer anything good.
Negative
?Estoy pensando cambiar de banco.?
I am considering switching to
another bank.
Negative
The complete
opinion
?Con XXXXXX me fue muy
[. . .] cambiar de banco.?
I was fine with XXXXXX
[. . .] switching to another bank.
Negative
Table 1: Some tests on Opinum for financial opinions in Spanish.
noise in the training set and that there are opinions
without a clear positive or negative feeling. A larger
corpus would also contribute to a better result. Even
though we have placed many efforts in simplifying
the text, this does not help in the cases in which a
construction of words is never found in the corpus.
A construction could even be present in the corpus
but in the wrong class. For instance, in our corpus
?no estoy satisfecho? (meaning ?I am not satisfied?)
appears 3 times among the positive opinions and 0
times among the negative ones. This weakness of
the corpus is due to sentences referring to a money
back guarantee: ?si no esta satisfecho le devolvemos
el dinero? which are used in a positive context.
Usually in long opinions a single sentence does
not change the positiveness score. For some exam-
ples see Table 4. In long opinions every sentence is
prone to show the sentiment except for the cases of
irony or opinions with an objective part. The per-
formance of Opinum depending on the size of the
opinions of the test set is shown in Figure 2. In Fig-
ure 3 the ROC curve of the classifier shows its sta-
bility against changing the true-positive versus false-
negative rates. A comparison with other methods
would be a valuable source of evaluation. It is not
feasible at this moment because of the lack of free
customers opinions databases and opionion classi-
fiers as well. The success rate we obtain can be com-
pared to the 69% baseline given by a classifier based
on the frequencies of single words.
?2500 ?2000 ?1500 ?1000 ?500 0?2500
?2000
?1500
?1000
?500
0
Similarity to positive LM
Sim
ilar
ity 
to 
ne
ga
tive
 LM
Similarity to the Language Models and text sizes (Test set)
Figure 1: Relation between similarity to the models (x
and y axis) and the relative size of the opinions (size of
the points).
The query time of Opinum on a standard com-
puter ranges from 1, 63 s for the shortest opinions to
1, 67 s for those with more than 1000 words. In our
setup, most of the time is spent in loading the mor-
phological dictionary, few milliseconds are spent in
the morphological analysis of the opinion and the
named entity substitution, and less than a millisec-
ond is spent in querying each model. In a batch
35
0 500 1000 1500
0
20
40
60
80
100
120
140
160
Opinion size (characters)
ev
en
ts
Distribution of test?text sizes
 
 
Successes
Errors
Figure 2: Number of successful and erroneous classifi-
cations (vertical axis) depending on the size of the test
opinions (horizontal axis).
mode, the morphological analysis could be done
for all the opinions together and thousands of them
could be evaluated in seconds. In Opinum?s web in-
terface we only provide the single opinion queries
and we output the decision, the qualifiers informa-
tion and the confidence measure.
5 Conclusions and future work
Opinum is a sentiment analysis system designed for
classifying customer opinions in positive and neg-
ative. Its approach based on morphological sim-
plification, entity substitution and n-gram language
models, makes it easily adaptable to other classifica-
tion targets different from positive/negative. In this
work we present experiments for Spanish in the fi-
nancial domain but Opinum could easily be trained
for a different language or domain. To this end an
Apertium morphological analyser would be neces-
sary (30 languages are currently available) as well
as a labeled data set of opinions. Setting n for the n-
gram models depends on the size of the corpus but
it would usually range from 4 to 6, 5 in our case.
There are other parameters which have to be exper-
imentally tuned and they are not related to the pos-
itive or negative classification but to the subjective
qualifier very/somewhat/little and to the confidence
measure.
The classification performance of Opinum in
our financial-domain experiments is 81,98% which
would be difficult to improve because of the noise in
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
ROC on test set
False positive rate
Tru
e p
os
itiv
e r
ate
Figure 3: Receiver Operating Characteristic (ROC) curve
of the Opinum classifier for financial opinions.
the data and the subjectivity of the labeling in posi-
tive and negative. The next steps would be to study
the possibility to classify in more than two classes
by using several language models. The use of an
external neutral corpus should also be considered in
the future.
It is necessary to perform a deeper analysis of the
impact of lexical simplification on the accuracy of
the language models. It is also very important to
establish the limitations of this approach for differ-
ent domains. Is it equally successful for a wider do-
main? For instance, trying to build the models from
a mixed set of opinions of the financial domain and
the IT domain. Would it work for a general domain?
Regarding applications, Opinum could be trained
for a given domain without expert knowledge. Its
queries are very fast which makes it feasible for free
on-line services. An interesting application would
be to exploit the named entity recognition and as-
sociate positive/negative scores to the entities based
on their surrounding text. If several domains were
available, then the same entities would have differ-
ent scores depending on the domain, which would
be a valuable analysis.
References
Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and
E. Herbst. 2007. Moses: open source toolkit for sta-
36
tistical machine translation. Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pp. 177?180. Prague, Czech
Republic, 2007.
Sasha Blair-Goldensohn, Tyler Neylon, Kerry Hannan,
George A. Reis, Ryan Mcdonald and Jeff Reynar.
2008. Building a sentiment summarizer for local ser-
vice reviews. In NLP in the Information Explosion
Era, NLPIX2008, Beiging, China, April 22nd, 2008.
Hu, Minqing and Liu, Bing. 2004. Mining and sum-
marizing customer reviews. Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, Seattle, WA, USA,
2004.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells and Jeff Reynar. 2007. Structured Models for
Fine-to-Coarse Sentiment Analysis. Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, 2007.
John Blitzer, Mark Dredze and Fernando Pereira. 2007.
Biographies, bollywood, boomboxes and blenders:
Domain adaptation for sentiment classification. ACL,
2007.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. EMNLP, 2002.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, Vol. 37, Nr. 2, pp. 267?307, June 2011.
Stephan Raaijmakers, Khiet P. Truong and Theresa Wil-
son. 2008. Multimodal Subjectivity Analysis of Mul-
tiparty Conversation. EMNLP, 2008.
Tyers, F. M., Snchez-Martnez, F., Ortiz-Rojas, S. and
Forcada, M. L. 2010. Free/open-source resources
in the Apertium platform for machine translation re-
search and development. The Prague Bulletin of
Mathematical Linguistics, No. 93, pp. 67?76, 2010.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of N-gram Language Models for Statistical
Machine Translation. ACL 2007 Workshop on SMT,
Prague, Czech Republic, 2007.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. ACL 6th Workshop on SMT,
Edinburgh, Scotland, UK, July 30?31, 2011.
37
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171?177,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Abu-MaTran at WMT 2014 Translation Task:
Two-step Data Selection and RBMT-Style Synthetic Rules
Raphael Rubino
?
, Antonio Toral
?
, Victor M. S
?
anchez-Cartagena
??
,
Jorge Ferr
?
andez-Tordera
?
, Sergio Ortiz-Rojas
?
, Gema Ram??rez-S
?
anchez
?
,
Felipe S
?
anchez-Mart??nez
?
, Andy Way
?
?
Prompsit Language Engineering, S.L., Elche, Spain
{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com
?
NCLT, School of Computing, Dublin City University, Ireland
{atoral,away}@computing.dcu.ie
?
Dep. Llenguatges i Sistemes Inform`atics, Universitat d?Alacant, Spain
fsanchez@dlsi.ua.es
Abstract
This paper presents the machine trans-
lation systems submitted by the Abu-
MaTran project to the WMT 2014 trans-
lation task. The language pair concerned
is English?French with a focus on French
as the target language. The French to En-
glish translation direction is also consid-
ered, based on the word alignment com-
puted in the other direction. Large lan-
guage and translation models are built us-
ing all the datasets provided by the shared
task organisers, as well as the monolin-
gual data from LDC. To build the trans-
lation models, we apply a two-step data
selection method based on bilingual cross-
entropy difference and vocabulary satura-
tion, considering each parallel corpus in-
dividually. Synthetic translation rules are
extracted from the development sets and
used to train another translation model.
We then interpolate the translation mod-
els, minimising the perplexity on the de-
velopment sets, to obtain our final SMT
system. Our submission for the English to
French translation task was ranked second
amongst nine teams and a total of twenty
submissions.
1 Introduction
This paper presents the systems submitted by the
Abu-MaTran project (runs named DCU-Prompsit-
UA) to the WMT 2014 translation task for the
English?French language pair. Phrase-based sta-
tistical machine translation (SMT) systems were
submitted, considering the two translation direc-
tions, with the focus on the English to French di-
rection. Language models (LMs) and translation
models (TMs) are trained using all the data pro-
vided by the shared task organisers, as well as
the Gigaword monolingual corpora distributed by
LDC.
To train the LMs, monolingual corpora and the
target side of the parallel corpora are first used
individually to train models. Then the individ-
ual models are interpolated according to perplex-
ity minimisation on the development sets.
To train the TMs, first a baseline is built us-
ing the News Commentary parallel corpus. Sec-
ond, each remaining parallel corpus is processed
individually using bilingual cross-entropy differ-
ence (Axelrod et al., 2011) in order to sepa-
rate pseudo in-domain and out-of-domain sen-
tence pairs, and filtering the pseudo out-of-
domain instances with the vocabulary saturation
approach (Lewis and Eetemadi, 2013). Third,
synthetic translation rules are automatically ex-
tracted from the development set and used to train
another translation model following a novel ap-
proach (S?anchez-Cartagena et al., 2014). Finally,
we interpolate the four translation models (base-
line, in-domain, filtered out-of-domain and rules)
by minimising the perplexity obtained on the de-
velopment sets and investigate the best tuning and
decoding parameters.
The reminder of this paper is organised as fol-
lows: the datasets and tools used in our experi-
ments are described in Section 2. Then, details
about the LMs and TMs are given in Section 3 and
Section 4 respectively. Finally, we evaluate the
performance of the final SMT system according to
different tuning and decoding parameters in Sec-
tion 5 before presenting conclusions in Section 6.
171
2 Datasets and Tools
We use all the monolingual and parallel datasets
in English and French provided by the shared task
organisers, as well as the LDC Gigaword for the
same languages
1
. For each language, a true-case
model is trained using all the data, using the train-
truecaser.perl script included in the MOSES tool-
kit (Koehn et al., 2007).
Punctuation marks of all the monolingual and
parallel corpora are then normalised using the
script normalize-punctuation.perl provided by the
organisers, before being tokenised and true-cased
using the scripts distributed with the MOSES tool-
kit. The same pre-processing steps are applied to
the development and test sets. As development
sets, we used all the test sets from previous years
of WMT, from 2008 to 2013 (newstest2008-2013).
Finally, the training parallel corpora are cleaned
using the script clean-corpus-n.perl, keeping the
sentences longer than 1 word, shorter than 80
words, and with a length ratio between sentence
pairs lower than 4.
2
The statistics about the cor-
pora used in our experiments after pre-processing
are presented in Table 1.
For training LMs we use KENLM (Heafield et
al., 2013) and the SRILM tool-kit (Stolcke et al.,
2011). For training TMs, we use MOSES (Koehn
et al., 2007) version 2.1 with MGIZA++ (Och and
Ney, 2003; Gao and Vogel, 2008). These tools are
used with default parameters for our experiments
except when explicitly said.
The decoder used to generate translations is
MOSES using features weights optimised with
MERT (Och, 2003). As our approach relies on
training individual TMs, one for each parallel cor-
pus, our final TM is obtained by linearly interpo-
lating the individual ones. The interpolation of
TMs is performed using the script tmcombine.py,
minimising the cross-entropy between the TM
and the concatenated development sets from 2008
to 2012 (noted newstest2008-2012), as described
in Sennrich (2012). Finally, we make use of the
findings from WMT 2013 brought by the win-
ning team (Durrani et al., 2013) and decide to use
the Operation Sequence Model (OSM), based on
minimal translation units and Markov chains over
sequences of operations, implemented in MOSES
1
LDC2011T07 English Gigaword Fifth Edition,
LDC2011T10 French Gigaword Third Edition
2
This ratio was empirically chosen based on words fertil-
ity between English and French.
Corpus Sentences (k) Words (M)
Monolingual Data ? English
Europarl v7 2,218.2 59.9
News Commentary v8 304.2 7.4
News Shuffled 2007 3,782.5 90.2
News Shuffled 2008 12,954.5 308.1
News Shuffled 2009 14,680.0 347.0
News Shuffled 2010 6,797.2 157.8
News Shuffled 2011 15,437.7 358.1
News Shuffled 2012 14,869.7 345.5
News Shuffled 2013 21,688.4 495.2
LDC afp 7,184.9 869.5
LDC apw 8,829.4 1,426.7
LDC cna 618.4 45.7
LDC ltw 986.9 321.1
LDC nyt 5,327.7 1,723.9
LDC wpb 108.8 20.8
LDC xin 5,121.9 423.7
Monolingual Data ? French
Europarl v7 2,190.6 63.5
News Commentary v8 227.0 6.5
News Shuffled 2007 119.0 2.7
News Shuffled 2008 4,718.8 110.3
News Shuffled 2009 4,366.7 105.3
News Shuffled 2010 1,846.5 44.8
News Shuffled 2011 6,030.1 146.1
News Shuffled 2012 4,114.4 100.8
News Shuffled 2013 9,256.3 220.2
LDC afp 6,793.5 784.5
LDC apw 2,525.1 271.3
Parallel Data
10
9
Corpus
21,327.1
549.0 (EN)
642.5 (FR)
Common Crawl 3,168.5
76.0 (EN)
82.7 (FR)
Europarl v7 1,965.5
52.5 (EN)
56.7 (FR)
News Commentary v9 181.3
4.5 (EN)
5.3 (FR)
UN 12,354.7
313.4 (EN)
356.5 (FR)
Table 1: Data statistics after pre-processing of the
monolingual and parallel corpora used in our ex-
periments.
and introduced by Durrani et al. (2011).
3 Language Models
The LMs are trained in the same way for both
languages. First, each monolingual and parallel
corpus is considered individually (except the par-
allel version of Europarl and News Commentary)
and used to train a 5-gram LM with the modified
Kneser-Ney smoothing method. We then interpo-
late the individual LMs using the script compute-
best-mix available with the SRILM tool-kit (Stol-
cke et al., 2011), based on their perplexity scores
on the concatenation of the development sets from
2008 to 2012 (the 2013 version is held-out for the
tuning of the TMs).
172
The final LM for French contains all the word
sequences from 1 to 5-grams contained in the
training corpora without any pruning. However,
with the computing resources at our disposal, the
English LMs could not be interpolated without
pruning non-frequent n-grams. Thus, n-grams
with n ? [3; 5] with a frequency lower than 2 were
removed. Details about the final LMs are given in
Table 2.
1-gram 2-gram 3-gram 4-gram 5-gram
English 13.4 198.6 381.2 776.3 1,068.7
French 6.0 75.5 353.2 850.8 1,354.0
Table 2: Statistics, in millions of n-grams, of the
interpolated LMs.
4 Translation Models
In this Section, we describe the TMs trained for
the shared task. First, we present the two-step data
selection process which aims to (i) separate in and
out-of-domain parallel sentences and (ii) reduce
the total amount of out-of-domain data. Second,
a novel approach for the automatic extraction of
translation rules and their use to enrich the phrase
table is detailed.
4.1 Parallel Data Filtering and Vocabulary
Saturation
Amongst the parallel corpora provided by the
shared task organisers, only News Commentary
can be considered as in-domain regarding the de-
velopment and test sets. We use this training
corpus to build our baseline SMT system. The
other parallel corpora are individually filtered us-
ing bilingual cross-entropy difference (Moore and
Lewis, 2010; Axelrod et al., 2011). This data
filtering method relies on four LMs, two in the
source and two in the target language, which
aim to model particular features of in and out-of-
domain sentences.
We build the in-domain LMs using the source
and target sides of the News Commentary paral-
lel corpus. Out-of-domain LMs are trained on a
vocabulary-constrained subset of each remaining
parallel corpus individually using the SRILM tool-
kit, which leads to eight models (four in the source
language and four in the target language).
3
3
The subsets contain the same number of sentences and
the same vocabulary as News Commentary.
Then, for each out-of-domain parallel corpus,
we compute the bilingual cross-entropy difference
of each sentence pair as:
[H
in
(S
src
)?H
out
(S
src
)] + [H
in
(S
trg
)?H
out
(S
trg
)] (1)
where S
src
and S
trg
are the source and the tar-
get sides of a sentence pair, H
in
and H
out
are
the cross-entropies of the in and out-of-domain
LMs given a sentence pair. The sentence pairs are
then ranked and the lowest-scoring ones are taken
to train the pseudo in-domain TMs. However,
the cross-entropy difference threshold required to
split a corpus in two parts (pseudo in and out-of-
domain) is usually set empirically by testing sev-
eral subset sizes of the top-ranked sentence pairs.
This method is costly in our setup as it would lead
to training and evaluating multiple SMT systems
for each of the pseudo in-domain parallel corpora.
In order to save time and computing power,
we consider only pseudo in-domain sentence pairs
those with a bilingual cross-entropy difference be-
low 0, i.e. those deemed more similar to the
in-domain LMs than to the out-of-domain LMs
(H
in
< H
out
). A sample of the distribution of
scores for the out-of-domain corpora is shown in
Figure 1. The resulting pseudo in-domain corpora
are used to train individual TMs, as detailed in Ta-
ble 3.
-4
-2
 0
 2
 4
 6
 8
 10
0 2k 4k 6k 8k 10k
Bilin
gual
 Cro
ss-E
ntro
py D
iffer
ence
Sentence Pairs
Common CrawlEuroparl10^9UN
Figure 1: Sample of ranked sentence-pairs (10k)
of each of the out-of-domain parallel corpora with
bilingual cross-entropy difference
The results obtained using the pseudo in-
domain data show BLEU (Papineni et al., 2002)
scores superior or equal to the baseline score.
Only the Europarl subset is slightly lower than
the baseline, while the subset taken from the 10
9
corpus reaches the highest BLEU compared to the
other systems (30.29). This is mainly due to the
173
size of this subset which is ten times larger than
the one taken from Europarl. The last row of Ta-
ble 3 shows the BLEU score obtained after interpo-
lating the four pseudo in-domain translation mod-
els. This system outperforms the best pseudo in-
domain one by 0.5 absolute points.
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 208.3 27.73
Europarl 142.0 27.63
10
9
Corpus 1,442.4 30.29
UN 642.4 28.91
Interpolation - 30.78
Table 3: Number of sentence pairs and BLEU
scores reported by MERT on English?French new-
stest2013 for the pseudo in-domain corpora ob-
tained by filtering the out-of-domain corpora with
bilingual cross-entropy difference. The interpola-
tion of pseudo in-domain models is evaluated in
the last row.
After evaluating the pseudo in-domain parallel
data, the remaining sentence pairs for each cor-
pora are considered out-of-domain according to
our filtering approach. However, they may still
contain useful information, thus we make use of
these corpora by building individual TMs for each
corpus (in a similar way we built the pseudo in-
domain models). The total amount of remaining
data (more than 33 million sentence pairs) makes
the training process costly in terms of time and
computing power. In order to reduce these costs,
sentence pairs with a bilingual cross-entropy dif-
ference higher than 10 were filtered out, as we no-
ticed that most of the sentences above this thresh-
old contain noise (non-alphanumeric characters,
foreign languages, etc.).
We also limit the size of the remaining data by
applying the vocabulary saturation method (Lewis
and Eetemadi, 2013). For the out-of-domain sub-
set of each corpus, we traverse the sentence pairs
in the order they are ranked by perplexity differ-
ence and filter out those sentence pairs for which
we have seen already each 1-gram at least 10
times. Each out-of-domain subset from each par-
allel corpus is then used to train a TM before inter-
polating them to create the pseudo out-of-domain
TM. The results reported by MERT obtained on
the newstest2013 development set are detailed in
Table 4.
Mainly due to the sizes of the pseudo out-of-
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 1,598.7 29.84
Europarl 461.9 28.87
10
9
Corpus 5,153.0 30.50
UN 1,707.3 29.03
Interpolation - 31.37
Table 4: Number of sentence pairs and BLEU
scores reported by MERT on English?French
newstest2013 for the pseudo out-of-domain cor-
pora obtained by filtering the out-of-domain cor-
pora with bilingual cross-entropy difference, keep-
ing sentence pairs below an entropy score of 10
and applying vocabulary saturation. The interpo-
lation of pseudo out-of-domain models is evalu-
ated in the last row.
domain subsets, the reported BLEU scores are
higher than the baseline for the four individual
SMT systems and the interpolated one. This latter
system outperforms the baseline by 3.61 absolute
points. Compared to the results obtained with the
pseudo in-domain data, we observe a slight im-
provement of the BLEU scores using the pseudo
out-of-domain data. However, despite the com-
paratively larger sizes of the latter datasets, the
BLEU scores reached are not that higher. For in-
stance with the 10
9
corpus, the pseudo in and out-
of-domain subsets contain 1.4 and 5.1 million sen-
tence pairs respectively, and the two systems reach
30.3 and 30.5 BLEU. These scores indicate that
the pseudo in-domain SMT systems are more ef-
ficient on the English?French newstest2013 devel-
opment set.
4.2 Extraction of Translation Rules
A synthetic phrase-table based on shallow-transfer
MT rules and dictionaries is built as follows. First,
a set of shallow-transfer rules is inferred from the
concatenation of the newstest2008-2012 develop-
ment corpora exactly in the same way as in the
UA-Prompsit submission to this translation shared
task (S?anchez-Cartagena et al., 2014). In sum-
mary, rules are obtained from a set of bilingual
phrases extracted from the parallel corpus after
its morphological analysis and part-of-speech dis-
ambiguation with the tools in the Apertium rule-
based MT platform (Forcada et al., 2011).
The extraction algorithm commonly used in
phrase-based SMT is followed with some added
heuristics which ensure that the bilingual phrases
174
extracted are compatible with the bilingual dic-
tionary. Then, many different rules are generated
from each bilingual phrase; each of them encodes
a different degree of generalisation over the partic-
ular example it has been extracted from. Finally,
the minimum set of rules which correctly repro-
duces all the bilingual phrases is found based on
integer linear programming search (Garfinkel and
Nemhauser, 1972).
Once the rules have been inferred, the phrase
table is built from them and the original rule-
based MT dictionaries, following the method
by S?anchez-Cartagena et al. (2011), which was
one of winning systems
4
(together with two on-
line SMT systems) in the pairwise manual evalu-
ation of the WMT11 English?Spanish translation
task (Callison-Burch et al., 2011). This phrase-
table is then interpolated with the baseline TM and
the results are presented in Table 5. A slight im-
provement over the baseline is observed, which
motivates the use of synthetic rules in our final MT
system. This small improvement may be related
to the small coverage of the Apertium dictionar-
ies: the English?French bilingual dictionary has a
low number of entries compared to more mature
language pairs in Apertium which have around 20
times more bilingual entries.
System BLEU
dev
Baseline 27.76
Baseline+Rules 28.06
Table 5: BLEU scores reported by MERT on
English?French newstest2013 for the baseline
SMT system standalone and with automatically
extracted translation rules.
5 Tuning and Decoding
We present in this Section a short selection of our
experiments, amongst 15+ different configura-
tions, conducted on the interpolation of TMs, tun-
ing and decoding parameters. We first interpolate
the four TMs: the baseline, the pseudo in and out-
of-domain, and the translation rules, minimising
the perplexity obtained on the concatenated de-
velopment sets from 2008 to 2012 (newstest2008-
2012). We investigate the use of OSM trained on
pseudo in-domain data only or using all the paral-
lel data available. Finally, we make variations of
4
No other system was found statistically significantly bet-
ter using the sign test at p ? 0.1.
the number of n-bests used by MERT.
Results obtained on the development set new-
stest2013 are reported in Table 6. These scores
show that adding OSM to the interpolated trans-
lation models slightly degrades BLEU. However,
by increasing the number of n-bests considered by
MERT to 200-bests, the SMT system with OSM
outperforms the systems evaluated previously in
our experiments. Adding the synthetic translation
rules degrades BLEU (as indicated by the last row
in the Table), thus we decide to submit two sys-
tems to the shared task: one without and one with
synthetic rules. By submitting a system without
synthetic rules, we also ensure that our SMT sys-
tem is constrained according to the shared task
guidelines.
System BLEU
dev
Baseline 27.76
+ pseudo in + pseudo out 31.93
+ OSM 31.90
+ MERT 200-best 32.21
+ Rules 32.10
Table 6: BLEU scores reported by MERT on
English?French newstest2013 development set.
As MERT is not suitable when a large number
of features are used (our system uses 19 fetures),
we switch to the Margin Infused Relaxed Algo-
rithm (MIRA) for our submitted systems (Watan-
abe et al., 2007). The development set used is
newstest2012, as we aim to select the best decod-
ing parameters according to the scores obtained
when decoding the newstest2013 corpus, after de-
truecasing and de-tokenising using the scripts dis-
tributed with MOSES. This setup allowed us to
compare our results with the participants of the
translation shared task last year. We pick the de-
coding parameters leading to the best results in
terms of BLEU and decode the official test set of
WMT14 newstest2014. The results are reported in
Table 7. Results on newstest2013 show that the de-
coding parameters investigation leads to an over-
all improvement of 0.1 BLEU absolute. The re-
sults on newstest2014 show that adding synthetic
rules did not help improving BLEU and degraded
slightly TER (Snover et al., 2006) scores.
In addition to our English?French submission,
we submitted a French?English translation. Our
French?English MT system is built on the align-
ments obtained from the English?French direc-
tion. The training processes between the two sys-
175
System BLEU13A TER
newstest2013
Best tuning 31.02 60.77
cube-pruning (pop-limit 10000) 31.04 60.71
increased table-limit (100) 31.06 60.77
monotonic reordering 31.07 60.69
Best decoding 31.14 60.66
newstest2014
Best decoding 34.90 54.70
Best decoding + Rules 34.90 54.80
Table 7: Case sensitive results obtained with
our final English?French SMT system on new-
stest2013 when experimenting with different de-
coding parameters. The best parameters are kept
to translate the WMT14 test set (newstest2014)
and official results are reported in the last two
rows.
tems are identical, except for the synthetic rules
which are not extracted for the French?English
direction. Tuning and decoding parameters for
this latter translation direction are the best ones
obtained in our previous experiments on this
shared task. The case-sensitive scores obtained
for French?English on newstest2014 are 35.0
BLEU13A and 53.1 TER, which ranks us at the
fifth position for this translation direction.
6 Conclusion
We have presented the MT systems developed by
the Abu-MaTran project for the WMT14 trans-
lation shared task. We focused on the French?
English language pair and particularly on the
English?French direction. We have used a two-
step data selection process based on bilingual
cross-entropy difference and vocabulary satura-
tion, as well as a novel approach for the extraction
of synthetic translation rules and their use to en-
rich the phrase table. For the LMs and the TMs,
we rely on training individual models per corpus
before interpolating them by minimising perplex-
ity according to the development set. Finally, we
made use of the findings of WMT13 by including
an OSM model.
Our English?French translation system was
ranked second amongst nine teams and a total of
twenty submissions, while our French?English
submission was ranked fifth. As future work,
we plan to investigate the effect of adding to the
phrase table synthetic translation rules based on
larger dictionaries. We also would like to study the
link between OSM and the different decoding pa-
rameters implemented in MOSES, as we observed
inconsistent results in our experiments.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation Via Pseudo In-domain
Data Selection. In Proceedings of EMNLP, pages
355?362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceed-
ings of WMT, pages 22?64.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL/HLT,
pages 1045?1054.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of WMT, pages 112?119.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio P?erez-Ortiz, Felipe S?anchez-Mart??nez, Gema
Ram??rez-S?anchez, and Francis M Tyers. 2011.
Apertium: A Free/Open-source Platform for Rule-
based Machine Translation. Machine Translation,
25(2):127?144.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Robert S Garfinkel and George L Nemhauser. 1972.
Integer Programming, volume 4. Wiley New York.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL, pages 690?696.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, Interactive Poster and Demonstra-
tion Sessions, pages 177?180.
176
William D. Lewis and Sauleh Eetemadi. 2013. Dra-
matically Reducing Training Data Size Through Vo-
cabulary Saturation. In Proceedings of WMT, pages
281?291.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of ACL, pages 220?224.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, volume 1, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, pages 311?318.
V??ctor M. S?anchez-Cartagena, Felipe S?anchez-
Mart??nez, and Juan Antonio P?erez-Ortiz. 2011. In-
tegrating Shallow-transfer Rules into Phrase-based
Statistical Machine Translation. In Proceedings of
MT Summit XIII, pages 562?569.
V??ctor M. S?anchez-Cartagena, Juan Antonio P?erez-
Ortiz, and Felipe S?anchez-Mart??nez. 2014. The
UA-Prompsit Hybrid Machine Translation System
for the 2014 Workshop on Statistical Machine
Translation. In Proceedings of WMT.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statisti-
cal Machine Translation. In Proceedings of EACL,
pages 539?549.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of ASRU.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-margin Train-
ing for Statistical Machine Translation. In Proceed-
ings of EMNLP.
177
