Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 861?869,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semantic Tagging of Web Search Queries   Mehdi Manshadi Xiao Li University of Rochester Microsoft Research Rochester, NY Redmond, WA mehdih@cs.rochester.edu xiaol@microsoft.com       Abstract 
We present a novel approach to parse web search queries for the purpose of automatic tagging of the queries. We will define a set of probabilistic context-free rules, which generates bags (i.e. multi-sets) of words. Us-ing this new type of rule in combination with the traditional probabilistic phrase structure rules, we define a hybrid grammar, which treats each search query as a bag of chunks (i.e. phrases). A hybrid probabilistic parser is used to parse the queries. In order to take contextual information into account, a discriminative model is used on top of the parser to re-rank the n-best parse trees gen-erated by the parser. Experiments show that our approach outperforms a basic model, which is based on Conditional Random Fields. 1 Introduction     Understanding users? intent from web search queries is an important step in designing an intel-ligent search engine.  While it remains a chal-lenge to have a scientific definition of ''intent'', many efforts have been devoted to automatically mapping queries into different domains i.e. topi-cal classes such as product, job and travel (Broder et al 2007; Li et al 2008). This work goes beyond query-level classification. We as-sume that the queries are already classified into the correct domain and investigate the problem of semantic tagging at the word level, which is to assign a label from a set of pre-defined semantic labels (specific to the domain) to every word in the query. For example, a search query in the product domain can be tagged as:  cheap       garmin   streetpilot   c340       gps     |               |              |             |           | SortOrder  Brand      Model      Model    Type 
 Many specialized search engines build their in-dexes directly from relational databases, which contain highly structured information. Given a query tagged with the semantic labels, a search engine is able to compare the values of semantic labels in the query (e.g., Brand = ?garmin?) with its counterpart values in documents, thereby pro-viding users with more relevant search results.     Despite this importance, there has been rela-tively little published work on semantic tagging of web search queries. Allan and Raghavan (2002) and Barr et al (2008) study the linguistic structure of queries by performing part-of-speech tagging.  Pasca et al (2007) use queries as a source of knowledge for extracting prominent attributes for semantic concepts.  On the other hand, there has been much work on extracting structured information from larger text segments, such as addresses (Kushmerick 2001), bibliographic citations (McCallum et al 1999), and classified advertisements (Grenager  et al 2005),  among many others. The most widely used approaches to these problems have been sequential models including hidden Markov models (HMMs), maximum entropy Markov mod-els (MEMMs) (Mccallum 2000), and conditional random fields (CRFs) (Lafferty et al 2001) These sequential models, however, are not op-timal for processing web search queries for the following reasons.. The first problem is that the global constraints and long distance dependencies on state variables are difficult to capture using sequential models. Because of this limitation, Viola and Narasimhand (2007) use a discrimina-tive context-free (phrase structure) grammar for extracting information from semi-structured data and report higher performances over CRFs.      Secondly, sequential models treat the input text as an ordered sequence of words. A web search query, however, is often formulated by a user as a bag of keywords. For example, if a user is look-
861
ing for cheap garmin gps, it is possible that the query comes in any ordering of these three words. We are looking for a model that, once it observes this query, assumes that the other per-mutations of the words in this query are also likely. This model should also be able to handle cases where some local orderings have to be fixed as in the query buses from New York City to Boston, where the words in the phrases from New York city and to Boston have to come in the exact order.  The third limitation is that the sequential mod-els treat queries as unstructured (linear) se-quences of words. The study by Barr et al (2008) on Yahoo! query logs suggests that web search queries, to some degree, carry an underlying lin-guistic structure. As an example, consider a query about finding a local business near some location such as:  seattle wa drugstore 24/7 98109  This query has two constituents: the Business that the user is looking for (24/7 drugstore) and the Neighborhood (seattle wa 98109). The model should not only be able to recognize the two con-stituents but it also needs to understand the struc-ture of each constituent. Note that the arbitrary ordering of the words in the query is a big chal-lenge to understanding the structure of the query. The problem is not only that the two constituents can come in either order, but also that a sub-constituent such as 98109 can also be far from the other words belonging to the same constitu-ent. We are looking for a model that is able to generate a hierarchical structure for this query as shown in figure (1).  The last problem that we discuss here is that the two powerful sequential models i.e. MEMM and CRF are discriminative models; hence they are highly dependent on the training data. Prepar-ing labeled data, however, is very expensive. Therefore in cases where there is no or a small amount of labeled data available, these models do a poor job.   In this paper, we define a hybrid, generative grammar model (section 3) that generates bags of phrases (also called chunks in this paper). The chunks are generated by a set of phrase structure (PS) rules. At a higher level, a bag of chunks is generated from individual chunks by a second type of rule, which we call context-free multiset generating rules. We define a probabilistic ver-sion of this grammar in which every rule has a probability associated with it. Our grammar model eliminates the local dependency assump-tion made by sequential models and the ordering 
constraints imposed by phrase structure gram-mars (PSG). This model better reflects the under-lying linguistic structure of web search queries. The model?s power, however, comes at the cost of increased time complexity, which is exponen-tial in the length of the query. This, is less of an issue for parsing web search queries, as they are usually very short (2.8 words/query in average (Xue et al, 2004)).   Yet another drawback of our approach is due to the context-free nature of the proposed gram-mar model. Contextual information often plays a big role in resolving tagging ambiguities and is one of the key benefits of discriminative models such as CRFs. But such information is not straightforward to incorporate in our grammar model. To overcome this limitation, we further present a discriminative re-ranking module on top of the parser to re-rank the n-best parse trees gen-erated by the parser using contextual features. As seen later, in the case where there is not a large amount of labeled data available, the parser part is the dominant part of the module and performs reasonably well. In cases where there is a large amount of labeled data available, the discrimina-tive re-ranking incorporates into the system and enhances the performance. We evaluate this model on the task of tagging search queries in the product domain. As seen later, preliminary ex-periments show that this hybrid genera-tive/discriminative model performs significantly better than a CRF-based module in both absence and presence of the labeled data. The structure of the paper is as follows. Sec-tion 2 introduces a linguistic grammar formalism that motivates our grammar model. In section 3, we define our grammar model. In section 4 we address the design and implementation of a parser for this kind of grammar. Section 5 gives an example of such a grammar designed for the purpose of automatic tagging of queries. Section 6 discusses motivations for and benefits of run-ning a discriminative re-ranker on top of the parser. In section 7, we explain the evaluations 
 Figure 1. A simple grammar for product domain  
862
and discuss the results. Section 8 summarizes this work and discusses future work. 2 ID/LP Grammar Context-free phrase structure grammars are widely used for parsing natural language. The adequate power of this type of grammar plus the efficient parsing algorithms available  for it has made it very popular.  PSGs treat a sentence as an ordered sequence of words. There are however natural languages that are free word order. For example, a three-word sentence consisting of a subject, an object and a verb in Russian, can occur in all six possible orderings. PSGs  are not a well-suited model for this type of language, since six different PS-rules must be defined in order to cover such a simple structure. To address this issue, Gazdar (1985) introduced the concept of ID/LP rules within the framework of Generalized Phrase Structure Grammar (GPSG). In this framework, Immediate Dominance or ID rules are of the form: (1) A? B, C This rule specifies that a non-terminal A can be rewritten as B and C, but it does not specify the order. Therefore A can be rewritten as both BC and CB. In other words the rule in (1) is equivalent to two PS-rules: (2) A ? BC A ? CB Similarly one ID rule will suffice to cover the simple subject-object-verb structure in Russian: (3) S ? Sub, Obj, Vrb However even in free-word-order languages, there are some ordering restrictions on some of the constituents. For example in Russian an adjective always comes before the noun that it modifies. To cover these ordering restrictions, Gazdar defined Linear Precedence (LP) rules. (4) gives an example of a linear precedence rule: (4) ADJ < N This specifies that ADJ always comes before N when both occur on the right-hand side of a single rule.     Although very intuitive, ID/LP rules are not widely used in the area of natural language processing. The main reason is the time-complexity issue of ID/LP grammar. It has been shown that parsing ID/LP rules is an NP-complete problem (Barton 1985). Since the length of a natural language sentence can easily reach 30-40 (and sometimes even up to 100) words, ID/LP grammar is not a practical model for natural language syntax. In our case, however, 
the time-complexity is not a bottleneck as web search queries are usually very short (2.8 words per query in average). Moreover, the nature of ID rules can be deceptive as it might appear that ID rules allow any reordering of the words in a valid sentence to occur as another vaild sentence of the language. But in general this is not the case. For example consider a grammar with only two ID rules given in (5) and consider S as the start symbol: (5) S ?  B, c B ?  d, e It can be easily verified that dec is a sentence of the language but dce is not. In fact, although the permutation of subconstituents of a constituent is allowed, a subconstituent can not be pulled out from its mother consitutent and freely move within the other constituents. This kind of movement however is a common behaviour in web search queries as shown in figure (1). It means that even ID rules are not powerful enough to model the free-word-order nature of web search queries.  This leads us to define to a new type of grammar model. 3 Our Grammar Model 3.1  The basic model We propose a set of rules in the form: (6) S ?  {B, c} B ?  {D, E} D ?  {d} E ?  {e} which can be used to generate multisets of words. For the notation convenience and consistancy, throughout this paper, we show terminals and non-terminals by lowercase and uppercase letters, respectively and sets and multisets by bold font uppercase letters. Using the rules in (6) a sentence of the language (which is a multiset in this model) can be derived as follows: (7) S ? {B, c} ? {D, E, c} ? {D, e, c}? {d, e, c} Once the set is generated, it can be realized as any of the six permutation of d, e, and c. Therefore a single sequence of derivations can lead to six different strings of words. As another example consider the grammar in (8). (8) Query ?  {Business, Location} Business ?  {Attribute, Business} Location ? {City, State} Business ?  {drugstore} | {Resturant} Attribute? {Chinese} | {24/7} City? {Seattle} | {Portland} State? {WA} | {OR} 
863
where Query is the start symbol and by A ? B|C we mean two differnet rules A ? B and A ? C. Figures (2) and (3) show the tree structures for the queries Restaurant Rochester Chinese MN, and Rochester MN Chinese Restaurant, respectively. As seen in these figures, no matter what the order of the words in the query is, the grammar always groups the words Resturant and Chinese together as the Business and the words Rochester and MN together as the Location. It is important to notice that the above grammars are context-free as every non-terminal A, which occurs on the left-hand side of a rule r, can be replaced with the set of terminals and non-terminals on the right-hand side of r, no matter what the context in which A occurs is.  More formally we define a Context-Free multiSet generating Grammar (CFSG) as a 4-tuple G=(N, T, S, R) where ? N is a set of non-terminals;  ? T is a set of terminals; ? S ? N is a special non-terminal called start symbol,  ? R is a set of rules {Ai? Xj} where Ai is a non-terminal and Xj is a set of terminals and non-terminals. Given two multisets Y and Z over the set N ?  T, we say Y dervies Z (shown as Y ? Z) iff there exists A, W, and X such that: Y = W + {A}1 Z = W + X A? X ? R Here ?* is defined as the reflexive transitive closure of ?. Finally we define the language of multisets generated by the grammar G (shown as L(G)) as L = { X | X is a multiset over N?T and S ?*X} The sequence of ? used to derive X from S is called a derivation of X. Given the above                                                 1 If X and Y are two multisets, X+Y simply means append-ing X to Y. For example {a, b, a} + {b, c, d} = {a, b, a, b, c, d}. 
definitions, parsing a multiset X means to find all (if any) the derivations of  X from S. 2 3.2 Probabilisic CFSG Very often a sentence in the language has more than one derivation, that is the sentence is syntactically ambiguous. One natural way of resolving the ambiguity is using a probabilistic grammar. Analogous to PCFG (Manning and Sch?tze 1999), we define the probabilistic version of a CFSG, in which every rule Ai?Xj has a probability P(Ai?Xj) and for every non-terminal Ai, we have: (9) ?j P(Ai? Xj) = 1 Consider a sentence w1w2?wn, a parse tree T of this sentence, and an interior node v in T labeled with Av and assume that v1, v2, ?vk are the children of the node v in T. We define: (10) ?(v) = P(Av? {Av1? Avk})?(v1) ? ?(vk) with the initial conditions ?(wi)=1. If u is the root of the tree T we have: (11) P(w1w2?wn , T) = ?(u) The parse tree that the probabilistic model assigns to the sentence is defined as: (12) Tmax = argmaxT (P(w1w2?wn , T))  where T ranges over all possible parse trees of the sentence. 4 Parsing Algorithm 4.1 Deterministic parser The parsing algorithm for the CFSG is straight-forward. We used a modified version of the Bot-tom-Up Chart Parser for the phrase structure grammars (Allen 1995, see 3.4). Given the grammar G=(N,T,S,R) and the query q=w1w2?wn, the algorithm in figure (4) is used to parse q. The algorithm is based on the concept of an active arc. An active arc is defined as a 3?                                                2 Every sentence of a language corresponds to a vector of |T| integers where the kth element represents how many times the kth terminal occurs in the multi-set. In fact, the languages defined by grammars are not interesting but the derivations are.   
 Figure 2. A CFSG parse tree  
 Figure 3. A CFSG parse tree  
864
tuple (r, U, I) where r is a rule A ? X in R, U is a subset of X, and I is a subset of {1, 2 ?n} (where n is the number of words in the query). This ac-tive arc tries to find a match to the right-hand side of r (i.e. X) and suggests to replace it with the non-terminal A. U contains the part of the right-hand side that has not been matched yet. There-fore when an arc is newly created U=X. Equiva-lently, X\U3 is the part of the right hand side that has so far been matched with a subset of words in the query, where I stores the positions of these words in q.  An active arc is completed when U=?. Every completed active arc can be reduced to a tuple (A, I), which we call a constituent. A constituent (A, I) shows that the non-terminal A matches the words in the query that are positioned at the numbers in I. Every constituent that is built by the parser is stored in a data structure called chart and remains there throughout the whole process. Agenda is another data structure that temporarily stores the constituents. At initialization step, the constituents (w1, {1}), ? (wn, {n}) are added to both chart and agenda. At each iteration, we pull out a constituent from the agenda and try to find a match to this constituent from the remaining list of terminals and non-terminals on the right-hand side of an active arc. More precisely, given a constituent c=(A, I) and an active arc ? = (r:B?X, U, J), we check if A ? U and I ? J = ?; if so, ? is extendable by c, therefore we extend ? by removing A from U and appending I to J. Note that the extension process keeps a copy of every active arc before it extends it. In practice every active arc and every constituent keep a set of pointers to its children constituents (stored in chart). This information is necessary for the ter-mination step in order to print the parse trees. The algorithm succeeds if there is a constituent in the chart that corresponds to the start symbol and covers all the words in the query, i.e. there is a constituent of the form (S, {1,2,?.n}) in the chart. 4.2 Probabilistic Parser The algorithm given in figure (4) works for a de-terministic grammar. As mentioned before, we use a probabilistic version of the grammar. Therefore the algorithm is modified for the prob-abilistic case. The probabilistic parser keeps a probability p for every active arc and every con-stituent: ? = (r, U, J, p? )                                                 3 A\B is defined as {x | x ? A & x ? B} 
c =(A, I, pc ) When extending ? using c, we have: (13) p? ? p? pc When creating c from the completed active arc ? : (14) pc ? p? p(r) Although search queries are usually short, the running time is still an issue when the length of the query exceeds 7 or 8. Therefore a couple of techniques have been used to make the na?ve al-gorithm more efficient. For example we have used pruning techniques to filter out structures with very low probability. Also, a dynamic pro-gramming version of the algorithm has been used, where for every subset I of the word posi-tions and every non-terminal A only the highest-ranking constituent c=(A, I, p) is kept and the rest are ignored. Note that although more efficient, the dynamic programming version is still expo-nential in the length of the query. 5 A grammar for semantic tagging  As mentioned before, in our system queries are already classified into different domains like movies, books, products, etc. using an automatic query classifier. For every domain we have a schema, which is a set of pre-defined tags. For example figure (5) shows an example of a schema for the product domain. The task defined for this system is to automatically tag the words in the query with the tags defined in the schema: cheap       garmin   streetpilot   c340       gps |                |               |               |            | SortOrder  Brand      Model      Model    Type  
Initialization: For each word wi in q add (wi, {i}) to Chart and to Agenda  For all r: A?X in R, create an active arc (r, X, {}) and add it to the list of active arcs.  Iteration Repeat Pull a constituent c = (A, I) from Agenda For every active arc ? =(r:B?X, U, I)   Extend ? using c if extendable   If U=? add (B, I) to Chart and to Agenda Until Agenda is empty   Termination For every item c=(S, {1..n}) in Chart, return the tree rooted at c. Figure 4. An algorithm for parsing deterministic CFSG 
865
We mentioned that one of the motivations of parsing search queries is to have a deeper under-standing of the structure of the query. The evaluation of such a deep model, however, is not an easy task. There is no Treebank available for web search queries. Furthermore, the definition of the tree structure for a query is quite arbitrary. Therefore even when human resources are avail-able, building such a Treebank is not a trivial task. For these reasons, we evaluate our grammar model on the task of automatic tagging of queries for which we have labeled data available. The other advantage of this evaluation is that there exists a CRF-based module in our system used for the task of automatic tagging. The perform-ance of this module can be considered as the baseline for our evaluation.  We have manually designed a grammar for the purpose of automatic tagging. The resources available for training and testing were a set of search queries from the product domain. There-fore a set of CFSG rules were written for the product domain. We defined very simple and intuitive rules (shown in figure 6) that could eas-ily be generalized to the other domains  Note that Type, Brand, Model, ? could be either pre-terminals generating word tokens, or non-terminals forming the left-hand side of the phrase structure rules. For the product domain, Type and Attribute are generated by a phrase structure grammar. Model and Attribute may also be generated by a set of manually designed regu-
lar expressions. The rest of the tags are simply pre-terminals generating word tokens. Note that we have a lexicon, e.g.., a Brand lexicon, for all the tags except Type and Attribute. The model, however, extends the lexicon by including words discovered from labeled data (if available). The gray color for a non-terminal on the right-hand side (RHS) of some rule means that the non-terminal is optional (see Query rule in figure (6)). We used the optional non-terminals to make the task of defining the grammar easier. For example if we consider a rule with n optional non-terminals on its RHS, without optional non-terminals we have to define 2n different rules to have an equivalent grammar. The parser can treat the optional non-terminals in different ways such as pre-compiling the rules to the equivalent set of rules with no optional non-terminal, or directly handling optional non-terminals during the pars-ing. The first approach results in exponentially many rules in the system, which causes sparsity issues when learning the probability of the rules. Therefore in our system the parser handles op-tional non-terminals directly. In fact, every non-terminal has its own probability for not occurring on the RHS of a rule, therefore the model learns n+1 probabilities for a rule with n optional non-terminals on its RHS: one for the rule itself and one for every non-terminal on its RHS. It means that instead of learning 2n probabilities for 2n dif-ferent rules, the model only learns n+1 probabili-ties. That solves the sparsity problem, but causes another issue which we call short length prefer-ence. This occurs because we have assumed that the probability of a non-terminal being optional is independent of other optional non-terminals. Since for almost all non-terminals on the RHS of the query rule, the probability that the non-terminal does not exist in an instance of a query is higher than 0.5, a null query is the most likely query that the model generates! We solve this problem by conditioning the probabilities on the length of queries. This brings a trade-off between the two other alternatives: ignoring sparsity prob-lem to prevent making many independence as-sumptions and making a lot of independence assumptions to address the sparsity issue.  Unlike sequential models, the grammar model is able to capture critical global con-straints. For example, it is very unlikely for a query to have more than one Type, Brand, etc. This is an important property of the product que-ries that can help to resolve the ambiguity in many cases. In practice, the probability that the model learns for a rule like:  
Query ? {Brand*, Product*, Model*, ?} Brand* ? {Brand} Brand* ? {Brand*, Brand} Type* ? {Type} Type* ? {Type*, Type} Model* ? {Model} Model* ? {Model*, Model} ? Figure 6. A simple grammar for product domain  
Type: Camera, Shoe, Cell phone, ?  Brand: Canon, Nike, At&t, ? Model: dc1700, powershot, ipod nano Attribute: 1GB, 7mpixel, 3X, ? BuyingIntenet: Sale, deal, ? ResearchIntent:  Review, compare, ? SortOrder: Best, Cheap, ? Merchant:  Walmart, Target, ?  Figure 5. Example of schema for product domain  
866
Type* ? {Type*, Type} compared to the rule: Type* ? Type is very small; the model penalizes the occurrence of more than one Type in a query. Figure (7a) shows an example of a parse tree generated for the query ?Canon vs Sony Camera? in which B, Q, and T are abbreviations for Brand, Query, and Type, and U is a special tag for the words that does not fall into any other tag categories and have been left unlabeled in our corpus such as a, the, for, etc. Therefore the parser assigns the tag sequence B U B T to this query. It is true that the word ?vs? plays a critical role in this query, rep-resenting that the user?s intention is to compare the two brands; but as mentioned above in our labeled data such words has left unlabeled. The general model, however, is able to easily capture these sorts of phenomena. A more careful look at the grammar shows that there is another parse tree for this query as shown in figure (7b). These two trees basically represent the same structure and generate the same sequence of tags. The number of trees gen-erated for the same structure increases exponen-tially with the number of equal tags in the tree. To prevent this over-generation we used rules analogous to GPSG?s LP rules such as: B* < B which allows only a unique way of generating a bag of the Brand tags.  Using this LP rule, the only valid tree for the above query is the one in figure (7a). 6 Discriminative re-ranking By using a context-free grammar, we are missing a great source of clues that can help to resolve ambiguity. Discriminative models, on the other hand, allow us to define numerous features, which can cooperate to resolve the ambiguities. Similar studies in parsing natural language sen-
tences (Collins and Koo 2005) have shown that if, instead of taking the most likely tree structure generated by a parser, the n-best parse trees are passed through a discriminative re-ranking mod-ule, the accuracy of the model will increase sig-nificantly. We use the same idea to improve the performance of our model. We run a Support Vector Machine (SVM) based re-ranking module on top of the parser. Several contextual features (such as bigrams) are defined to help in disam-biguation. This combination provides a frame-work that benefits from the advantages of both generative and discriminative models. In particu-lar, when there is no or a very small amount of labeled data, a parser could still work by using unsupervised learning approaches to learn the rules, or by simply using a set of hand-built rules (as we did above for the task of semantic tag-ging). When there is enough labeled data, then a discriminative model can be trained on the la-beled data to learn contextual information and to further enhance the tagging performance.  7 Evaluation Our resources are a set of 21000 manually la-beled queries, a manually designed grammar, a lexicon for every tag (except Type and Attribute), and a set of regular expressions defined for Mod-els and Attributes. Note that with a grammar similar to the one in figure (6), generating a parse tree from a labeled query is straightforward. Then the parser is trained on the trees to learn the pa-rameters of the model (probabilities in this case). We randomly extracted 3000, out of 21000, queries as the test set and used the remaining 18000 for training. We created training sets with different sizes to evaluate the impact of training data size on tagging performance.  Three modules were used in the evaluation: the CRF-based model4, the parser, and the parser plus the SVM-based re-ranking. Figure (8) shows the learning curve of the word-level F-score for all the three modules. As seen in this plot, when there is a small amount of training data, the parser performs better than the CRF module and parser+SVM module performs better than the other two. With a large amount of training data, the CRF and parser almost have the same per-formance. Once again the parser+SVM module                                                 4 The CRF module also uses the lexical resources and regu-lar expressions. In fact, it applies a deterministic context free grammar to the query to find all the possible groupings of words into chunks and uses this information as a set of fea-tures in the system. 
 Figure 7. Two equivalent CFSG parse trees   
867
outperforms the other two. These results show that, as expected, the CRF-based model is more dependent on the training data than the parser. Parser+SVM always performs at least as well as the parser-only module even with a very small set of training data. This is because the rank given to every parse tree by the parser is used as a feature in the SVM module. When there is a very small amount of training data, this feature is dominant and the output of the re-reranking module is basically the same as the parser?s highest-rank output. Table (1) shows the per-formance of all three modules when the whole training set was used to train the system. The first three columns in the table show the word-level precision, recall, and F-score; and the last column represents the query level accuracy (a query is considered correct if all the words in the query have been labeled correctly). There are two rows for the parser+SVM in the table: one for n=2 (i.e. re-ranking the 2-Best trees) and one for n=10. It is interesting to see that even with the re-ranking of only the first two trees generated by the parser, the difference between the accuracy of the parser+SVM module and the parser-only module is quite significant. Re-ranking with a larger number of trees (n>10) did not increase performance significantly. 8 Summary We introduced a novel approach for deep parsing of web search queries. Our approach uses a grammar for generating multisets called a con-text-free multiset generating grammar (CFSG). We used a probabilistic version of this grammar. A parser was designed for parsing this type of grammar. Also a discriminative re-ranking mod-ule based on a support vector machine was used 
to take contextual information into account. We have used this system for automatic tagging of web search queries and have compared it with a CRF-based model designed for the same task.  The parser performs much better when there is a small amount of training data, but an adequate lexicon for every tag. This is a big advantage of the parser model, because in practice providing labeled data is very expensive but very often the lexicons can be easily extracted from the struc-tured data on the web (for example extracting movie titles from imdb or book titles from Ama-zon).  Our hybrid model (parser plus discriminative re-ranking), on the other hand, outperforms the other two modules regardless of the size of the training data.  The main drawback with our approach is to completely ignore the ordering. Note that al-though strict ordering constraints such as those imposed by PSG is not appropriate for modeling query structure, it might be helpful to take order-ing information into account when resolving am-biguity. We leave this for future work. Another interesting and practically useful problem that we have left for future work is to design an unsuper-vised learning algorithm for CFSG similar to its phrase structure counterpart: inside-outside algo-rithm (Baker 1979). Having such a capability, we are able to automatically learn the underlying structure of queries by processing the huge amount of available unlabeled queries. Acknowledgement We need to thank Ye-Yi Wang for his helpful advices. We also thank William de Beaumont for his great comments on the paper. 
References  
Allan, J. and Raghavan, H. (2002) Using Part-of-speech Patterns to Reduce Query Ambiguity, Pro-ceedings of SIGIR 2002, pp. 307-314. Allen, J. F. (1995) Natural Language Understanding, Benjamin Cummings. Baker, J. K. (1979) Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, MIT, Cambridge, MA. Barton, E. (1985) On the complexity of ID/LP rules, Computational Linguistics, Volume 11, Pages 205-218. 
 Figure 8. The learning curve for the three modules  
Train?No?=?18000?Test?No?=?3000? P? R? F? Q?CRF? 0.815? 0.812? 0.813? 0.509?Parser? 0.808? 0.814? 0.811? 0.494?Parser+SVM?(n?=?2)? 0.823? 0.827? 0.825? 0.531?Parser+SVM?(n?=?10)? 0.832? 0.835? 0.833? 0.555?Table 1. The results of evaluating the three modules 
868
Barr, C., Jones, R., Regelson, M., (2008) The Linguis-tic Structure of English Web-Search Queries, In Proceedings of EMNLP-08: conference on Empiri-cal Methods in Natural Language Processing. Broder, A., Fontoura, M., Gabrilovich, E., Joshi, A., Josifovski, V., and Zhang, T. (2007) Robust classi-fication of rare queries using web knowledge. In Proceedings of SIGIR?07 Collins, M., Koo, T., (2005) Discriminative Reranking for Natural Language Parsing, Computational Lin-guistics, v.31 p.25-70. Gazdar, G., Klein, E., Sag, I., Pullum, G., (1985) Gen-eralized Phrase Structure Grammar, Harvard Uni-versity Press. Grenager, T., Klein, D., and Manning, C. (2005) Un-supervised learning of field segmentation models for information extraction, In Proceedings of ACL-05. Kushmerick, N., Johnston, E., and McGuinness, S. (2001). Information extraction by text classifica-tion, In Proceedings of the IJCAI-01 Workshopon Adaptive Text Extraction and Mining. Li, X., Wang, Y., and Acero, A. (2008) Learning query intent from regularized click graphs. In Pro-ceedings of SIGIR?08 Manning, C., Sch?tze, H. (1999) Foundations of Sta-tistical Natural Language Processing, The MIT Press, Cambridge, MA. McCallum, A., Freitag, D., Pereira, F. (2000) Maxi-mum entropy markov models for information ex-traction and segmentation, Proceedings of the Seventeenth International Conference on Machine Learning, Pages: 591 - 598 McCallum, A., Nigam, K., Rennie, J., and Seymore, K. (1999) A machine learning approach to building domain-specific search engines, In IJCAI-1999. Pasca, M., Van Durme, B., and Garera, N.  (2007) The Role of Documents vs. Queries in Extracting Class Attributes from Text, ACM Sixteenth Conference on Information and Knowledge Management (CIKM 2007). Lisboa, Portugal. Viola, P., Narasimhan, M., Learning to extract infor-mation from semi-structured text using a discrimi-native context free grammar SIGIR 2005: 330-337. Xue, GR, HJ Zeng, Z Chen, Y Yu, WY Ma, WS Xi, WG Fan, (2004), Optimizing web search using web click-through data, Proceedings of the thirteenth ACM international conference.   
869
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Deep Linguistic Processing for Spoken Dialogue Systems
James Allen
Department of Computer Science
University of Rochester
james@cs.rochester.edu
Myroslava Dzikovska
ICCS-HCRC
University of Edinburgh
mdzikovs@inf.ed.ac.uk
Mehdi Manshadi
Department of Computer Science
University of Rochester
mehdih@cs.rochester.edu
Mary Swift
Department of Computer Science
University of Rochester
swift@cs.rochester.edu
Abstract
We describe a framework for deep linguis-
tic processing for natural language under-
standing in task-oriented spoken dialogue
systems. The goal is to create domain-
general processing techniques that can be
shared across all domains and dialogue
tasks, combined with domain-specific op-
timization based on an ontology mapping
from the generic LF to the application  on-
tology. This framework has been tested in
six domains that involve tasks such as in-
teractive planning, coordination operations,
tutoring, and learning.
1 Introduction
Deep linguistic processing is essential for spoken
dialogue systems designed to collaborate with us-
ers to perform collaborative tasks. We describe the
TRIPS natural language understanding system,
which is designed for this purpose. As we develop
the system, we are constantly balancing two com-
peting needs: (1) deep semantic accuracy: the need
to produce the semantically and pragmatically deep
interpretations for a specific application; and (2)
portability: the need to reuse our grammar, lexicon
and discourse interpretation processes across do-
mains.
We work to accomplish portability by using a
multi-level representation. The central components
are all based on domain general representations,
including a linguistically based detailed semantic
representation (the Logical Form, or LF), illocu-
tionary acts, and a collaborative problem-solving
model. Each application then involves using a do-
main-specific ontology and reasoning components.
The generic LF is linked to the domain-specific
representations by a set of ontology mapping rules
that must be defined for each domain. Once the
ontology mapping is defined, we then can auto-
matically specialize the generic grammar to use the
stronger semantic restrictions that arise from the
specific domain. In this paper we mainly focus on
the generic components for deep processing. The
work on ontology mapping and rapid grammar ad-
aptation is described elsewhere (Dzikovska et al
2003; forthcoming).
2 Parsing for deep linguistic processing
The parser uses a broad coverage, domain-
independent lexicon and grammar to produce the
LF. The LF is a flat, unscoped representation that
includes surface speech act analysis, dependency
information, word senses (semantic types) with
semantic roles derived from the domain-
independent language ontology, tense, aspect, mo-
dality, and implicit pronouns. The LF supports
fragment and ellipsis interpretation, discussed in
Section 5.2
2.1 Semantic Lexicon
The content of our semantic representation comes
from a domain-independent ontology linked to a
domain-independent lexicon.  Our syntax relies on
a frame-based design in the LF ontology, a com-
mon representation in semantic lexicons (Baker et
al., 1998, Kipper et al, 2000). The LF type hierar-
chy is influenced by argument structure, but pro-
vides a more detailed level of semantic analysis
than found in most broad coverage parsers as it
distinguishes senses even if the senses take the
same argument structure, and may collapse lexical
entries with different argument structures to the
same sense. As a very simple example, the generic
lexicon includes the senses for the verb take shown
49
in Figure 1. Our generic senses have been inspired
by FrameNet (Baker et al, 1998).
In addition, types are augmented with semantic
features derived from EuroWordNet (Vossen et al,
1997) and extended. These are used to provide se-
lectional restrictions, similar to VerbNet (Kipper et
al., 2000). The constraints are intentionally weak,
excluding utterances unsuitable in most contexts
(the idea slept) but not attempting to eliminate
borderline combinations.
The generic selectional restrictions are effective
in improving overall parsing accuracy, while re-
maining valid across multiple domains. An
evaluation with an earlier version of the grammar
showed that if generic selectional restrictions were
removed, full sentence semantic accuracy de-
creased from 77.8% to 62.6% in an emergency
rescue domain, and from 67.9 to 52.5% in a medi-
cal domain (using the same versions of grammar
and lexicon) (Dzikovska, 2004).
The current version of our generic lexicon con-
tains approximately 6400 entries (excluding mor-
phological variants), and the current language on-
tology has 950 concepts. The lexicon can be sup-
plemented by searching large-scale lexical re-
sources such as WordNet (Fellbaum, 1998) and
Comlex (Grisham et al, 1994). If an unknown
word is encountered, an underspecified entry is
generated on the fly. The entry incorporates as
much information from the resource as possible,
such as part of speech and syntactic frame. It is
assigned an underspecified semantic classification
based on correspondences between our language
ontology and WordNet synsets.
2.2 Grammar
The grammar is context-free, augmented with fea-
ture structures and feature unification, motivated
from X-bar theory, drawing on principles from
GPSG (e.g., head and foot features) and HPSG. A
detailed description of an early non-lexicalized
version of the formalism is in (Allen, 1995). Like
HPSG, our grammar is strongly lexicalized, with
the lexical features defining arguments and com-
plement structures for head words. Unlike HPSG,
however, the features are not typed and rather than
multiple inheritance, the parser supports a set of
orthogonal single inheritance hierarchies to capture
different syntactic and semantic properties. Struc-
tural variants such as passives, dative shifts, ger-
unds, and so on are captured in the context-free
rule base. The grammar has broad coverage of
spoken English, supporting a wide range of con-
versational constructs. It also directly encodes
conventional conversational acts, including stan-
dard surface speech acts such as inform, request
and question, as well as acknowledgments, accep-
tances, rejections, apologies, greetings, corrections,
and other speech acts common in conversation.
To support having both a broad domain-general
grammar and the ability to produce deep domain-
specific semantic representations, the semantic
knowledge is captured in three distinct layers (Fig-
ure 2), which are compiled together before parsing
to create efficient domain-specific interpretation.
The first level is primarily encoded in the gram-
mar, and defines an interpretation of the utterance
in terms of generic grammatical relations. The sec-
ond is encoded in the lexicon and defines an inter-
pretation in terms of a generic language-based on-
tology and generic roles. The third is encoded by a
set of ontology-mapping rules that are defined for
each domain, and defines an interpretation in terms
of the target application ontology. While these lev-
els are defined separately, the parser can produce
all three levels simultaneously, and exploit do-
main-specific semantic restrictions to simultane-
ously improve semantic accuracy and parsing effi-
ciency. In this paper we focus on the middle level,
the generic LF.
CONSUME Take an aspirin
MOVE Take it to the store
ACQUIRE Take a picture
SELECT I?ll take that one
COMPATIBLE
WITH
The projector takes 100 volts
TAKE-TIME It took three hours
 Figure 1: Some generic senses of take in lexicon
50
The rules in the grammar are weighted, and
weights are combined, similar to how probabilities
are computed in a PCFG. The weights, however,
are not strictly probabilities (e.g., it is possible to
have weights greater than 1); rather, they encode
structural preferences. The parser operates in a
best-first manner and as long as weights never ex-
ceed 1.0, is guaranteed to find the highest weighted
parse first. If weights are allowed to exceed 1.0,
then the parser becomes more ?depth-first? and it
is possible to ?garden-path? and find globally sub-
optimal solutions first, although eventually all in-
terpretations can still be found.
The grammar used in all our applications uses
these hand-tuned rule weights, which have proven
to work relatively well across domains. We do not
use a statistical parser based on a trained corpus
because in most dialogue-system projects, suffi-
cient amounts of training data are not available and
would be too time consuming to collect. In the one
domain in which we have a reasonable amount of
training data (about 9300 utterances), we experi-
mented with a PCFG using trained probabilities
with the Collins algorithm, but were not able to
improve on the hand-tuned preferences in overall
performance (Elsner et al, 2005).
Figure 3 summarizes some of the most impor-
tant preferences encoded in our rule weights. Be-
cause we are dealing with speech, which is often
ungrammatical and fragmented, the grammar in-
cludes ?robust? rules (e.g., allowing dropped de-
terminers) that would not be found in a grammar of
written English.
3 The Logical Form Language
The logical form language captures a domain-
independent semantic representation of the utter-
ance. As shown later in this paper, it can be seen as
a variant of MRS (Copestake et al, 2006) but is
expressed in a frame-like notation rather than
predicate calculus. In addition, it has a relatively
simple method of computing possible quantifier
scoping, drawing from the approaches by (Hobbs
& Shieber, 1987) and (Alshawi, 1990).
A logical form is set of terms that can be viewed
as a rooted graph with each term being a node
identified by a unique ID (the variable). There are
three types of terms. The first corresponds to gen-
eralized quantifiers, and is on the form (<quant>
<id> <type> <modifiers>*). As a simple example,
the NP Every dog would be captured by the term
(Every d1 DOG). The second type of term is the
propositional term, which is represented in a neo-
Davidsonian representation (e.g., Parsons, 1990)
using reified events and properties. It has the form
(F <id> <type> <arguments>*). The propositional
terms produced from Every dog hates a cat would
be (F h1 HATE :Experiencer d1 :Theme c1).  The
third type of term is the speech act, which has the
same form as propositional terms except for the
initial indicator SA identifying it as a performed
speech act. The speech act for Every dog hates a
cat would be (SA sa1 INFORM :content h1). Put-
ting this all together, we get the following (con-
densed) LF representation from the parser for
Every large dog hates a cat (shown in graphical
Figure 2: The Levels of Representation computed by the Parser
Prefer
? Interpretations without gaps to those with gaps
? Subcategorized interpretations over adjuncts
? Right attachment of PPs and adverbials
? Fully specified constituents over those with
dropped or ?implicit? arguments
? Adjectival modification over noun-noun modifi-
cation
? Standard rules over ?robust? rules
Figure 3: Some Key Preferences used in Parsing
51
form in Figure 4).
(SA x1 TELL :content x2)
(F x2 HATE :experience x3 :theme x5)
(Every x3 DOG :mods  (x4))
(F x4 LARGE :of x3)
(A x5 CAT)
4 Comparison of LF and MRS
Minimal Recursion Semantics (MRS) (Copestake
et al 2006) is a semantic formalism which has
been widely adopted in the last several years. This
has motivated some research on how this formal-
ism compares to some traditional semantic for-
malisms. For example, Fuchss et al (2004) for-
mally show that the translation from MRS to
Dominance Constraints is feasible. We have also
found that MRS is very similar to LF in its de-
scriptive power. In fact, we can convert every LF
to an equivalent MRS structure with a simple algo-
rithm.
First, consider the sentence Every dog hates a
cat. Figure 5 shows the LF and MRS representa-
tions for this sentence.
Figure 5: The LF (left) and MRS (right) representations
for the sentence ?Every dog hates a cat.?
The first step toward converting LF to MRS is to
express LF terms as n-ary relationships. For exam-
ple we express the LF term (F v1 Hate
:Experiencer x :Theme y) as Hate(x, y). For quanti-
fier terms, we break the LF term into two relations:
one for the quantifier itself and one for the restric-
tion. For example (Every x Dog) is converted to
Every(x) and Dog(x).
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head?s. To cover this, for each LF term T
which has a (:MODS v
k
) slot,  and the LF term T1
which defines the variable v
k
, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
There is a small change in the conversion proce-
dure when the sentence contains some modifiers.
Consider the modifier large in the sentence Every
large dog hates a cat. In the LF, we bring the
modifier in the term which defines the semantic
head, using a :MODS slot. In the MRS, however,
modifiers are separate EPs labeled with same han-
dle as the head?s. To cover this, for each LF term T
which has a (:MODS v
k
) slot,  and the LF term T1
which defines the variable v
k
, we assign the same
handle to both T and T1. For example for the terms
(F x Dog :MODS v2) and (F v2 Large :OF x), we
assign the same handle to both Dog(x) and
Large(x). Similar approach applies when the modi-
fier itself is a scopal term, such as in the sentence
Every cat in a room sleeps. Figure 7 shows LF and
MRS representations for this sentence. Figure 8,
summarizes all these steps as an algorithm which
takes a LF representation as the input and gener-
ates its equivalent MRS.
The next step is to bring handles into the repre-
Figure 4: The LF in graphical form
Figure 6: The steps of converting the LF for
?Every cat hates a cat? to its MRS representation
52
sentation. First, we assign a different handle to
each term. Then, for each quantifier term such as
Every(x), we add two handles as the arguments of
the relation: one for the restriction and one for the
body as in h2: Every(x, h6, h7). Finally, we add the
handle constraints to the MRS. We have two types
of handle constraint. The first type comes from the
restriction of each quantifier. We add a qeq rela-
tionship between the restriction handle argument of
the quantifier term and the handle of the actual re-
striction term. The second type of constraint is the
qeq relationship which defines the top handle of
the MRS. The speech act term in every LF refers to
a formula term as content (:content slot), which is
actually the heart of the LF. We build a qeq rela-
tionship between h0 (the top handle) and the han-
dle of this formula term. Figure 6 shows the effect
of applying these steps to the above example.
Figure 7: The LF and MRS representations for the sen-
tence ?Every cat in a room sleeps.?
Another interesting issue about these two formal-
isms is that the effect of applying the simple scop-
ing algorithms referred in section 3 to generate all
possible interpretations of a LF is the same as ap-
plying MRS axioms and handle constraints to gen-
erate all scope-resolved MRSs. For instance, the
example in (Copestake et al 2006), Every nephew
of some famous politician saw a pony has the same
5 interpretations using either approach.
As the last point here, we need to mention that
the algorithm in Figure 8 does not consider fixed-
scopal terms such as scopal adverbials or negation.
However, we believe that the framework itself is
able to support these types of scopal term and with
a small modification, the scoping algorithm will
work well in assigning different possible interpre-
tations. We leave the full discussion about these
details as well as the detailed proof of the other
claims we made here to another paper.
5 Generic Discourse Interpretation
With a generic semantic representation, we can
then define generic discourse processing capabili-
ties that can be used in any application. All of
these methods have a corresponding capability at
the domain-specific level for an application, but we
will not discuss this further here. We also do not
discuss the support for language generation which
uses the same discourse context.
There are three core discourse interpretation ca-
pabilities that the system provides: reference reso-
lution, ellipsis processing, and speech act interpre-
tation. All our different dialog systems use the
same discourse processing, whether the task in-
volves collaborative problem solving, learning
from instruction or automated tutoring.
5.1 Reference Resolution
Our domain-independent representation supports
reference resolution in two ways. First, the quanti-
fiers and dependency structure extracted from the
sentence allow for implementing reference resolu-
tion algorithms based on extracted syntactic fea-
tures. The system uses different strategies for re-
Figure 8: The LF-MRS conversion algorithm
53
solving each type of referring expression along the
lines described in (Byron, 2002).
Second, domain-independent semantic informa-
tion helps greatly in resolving pronouns and defi-
nite descriptions. The general capability provided
for resolving referring expressions is to search
through the discourse history for the most recent
entity that matches the semantic requirements,
where recency within an utterance may be reor-
dered to reflect focusing heuristics (Tetreault,
2001). For definite descriptions, the semantic in-
formation required is explicit in the lexicon. For
pronouns, the parser can often compute semantic
features from verb argument restrictions.  For in-
stance, the pronoun it carries little semantic infor-
mation by itself, but in the utterance Eat it we
know we are looking for an edible object. This
simple technique performs well in practice.
Because of the knowledge in the lexicon for role
nouns such as author, we can also handle simple
bridging reference. Consider the discourse frag-
ment That book came from the library. The author
?. The semantic representation of the author in-
cludes its implicit argument, e.g., (The x1
AUTHOR :of b1). Furthermore, the term b1 has
the semantic feature INFO-CONTENT, which in-
cludes objects that ?contain? information such as
books, articles, songs, etc.., which allows the pro-
noun to correctly resolve via bridging to the book
in the previous utterance.
5.2 Ellipsis
The parser produces a representation of fragmen-
tary utterances similar to (Schlangen and Las-
carides, 2003). The main difference is that instead
of using a single underspecified unknown_rel
predicate to resolve in discourse context, we use a
speech act term as the underspecified relation, dif-
ferentiating between a number of common rela-
tions such as acknowledgments, politeness expres-
sions, noun phrases and underspecified predicates
(PP, ADJP and VP fragments). The representations
of the underspecified predicates also include an
IMPRO in place of the unspecified argument.
We currently handle only a few key cases of el-
lipsis. The first is question/answer pairs. By re-
taining the logical form of the question in the dis-
course history, it is relatively easy to reconstruct
the full content of short answers (e.g., in Who ate
the pizza? John? the answer maps to the represen-
tation that John ate the pizza).  In addition, we
handle common follow-up questions  (e.g., Did
John buy a book? How about a magazine?) by per-
forming a semantic closeness matching of the
fragment into the previous utterance and substitut-
ing the most similar terms. The resulting term can
then be used to update the context. This process is
similar to the resolution process in (Schlangen and
Lascarides, 2003), though the syntactic parallelism
constraint is not checked. It could also be easily
extended to cover other fragment types, as the
grammar provides all the necessary information.
5.3 Speech Act Interpretation
The presence of domain-independent semantic
classes allows us to encode a large set of these
common conversational pattern independently of
the application task and domain. These include
rules to handle short answers to questions, ac-
knowledgements and common politeness expres-
sions, as well as common inferences such as inter-
preting I need to do X as please do X.
Given our focus on problem solving domains,
we are generally interested in identifying more
than just the illocutionary force of an utterance.
For instance, in a domain for planning how to
evacuate people off an island, the  utterance Can
we remove the people by helicopter? is not only
ambiguous between being a true Y-N question or a
suggestion of a course of action, but at the problem
solving level it might intended to (1) introduce a
new goal, (2)  elaborate or extend the solution to
the current problem, or (3) suggest a modification
to an existing solution (e.g., moving them by
truck). One can only choose between these read-
ings using domain specific reasoning about the
current task. The point here is that the interpreta-
tion rules are still generic across all domains and
expressed using the generic LF, yet the interpreta-
tions produced are evaluated using domain-specific
reasoning. This interleaving of generic interpreta-
tion and domain-specific reasoning is enabled by
our ontology mappings.
Similarly, in tutoring domains students often
phrase their answers as check questions. In an an-
swer to the question Which components are in a
closed path, the student may say Is the bulb in 3 in
a closed path? The domain-independent represen-
tation is used to identify the surface form of this
utterance as a yes-no question. The dialogue man-
ager then formulates two hypotheses: that this is a
hedged answer, or a real question. If a domain-
54
specific tutoring component confirms the former
hypothesis, the dialogue manager will proceed
with verifying answer correctness and carrying on
remediation as necessary. Otherwise (such as for Is
the bulb in 5 connected to a battery in the same
context), the utterance is a question that can be
answered by querying the domain reasoner.
5.4 A Note on Generic Capabilities
A key point is that these generic discourse inter-
pretation capabilities are enabled because of the
detailed generic semantic interpretation produced
by the parser. If the parser produced a more shal-
low representation, then the discourse interpreta-
tion techniques would be significantly degraded.
On the other hand, if we developed a new repre-
sentation for each domain, then we would have to
rebuild all the discourse processing for the domain.
6 Evaluation
Our evaluation is aimed at assessing two main
features of the grammar and lexicon: portability
and accuracy. We use two main evaluation criteria:
full sentence accuracy, that takes into account both
syntactic and semantic accuracy of the system, and
sense tagging accuracy, to demonstrate that the
word senses included in the system can be distin-
guished with a combination of syntactic and do-
main-independent semantic information.
As a measure of the breadth of grammatical
coverage of our system, we have evaluated our
coverage on the CSLI LKB (Linguistic Knowledge
Building) test suite (Copestake, 1999). The test
suite contains approximately 1350 sentences, of
which about 400 are ungrammatical. We use a full-
sentence accuracy measure to evaluate our cover-
age, since this is the most meaningful measure in
terms of what we require as parser output in our
applications. For a sentence representation to be
counted as correct by this measure, both the syn-
tactic structure and the semantic representation
must be correct, which includes the correct as-
signment of word senses, dependency relations
among terms, and speech act type. Our current
coverage for the diverse grammatical phenomena
in the corpus is 64% full-sentence accuracy.
We also report the number of spanning parses
found, because in our system there are cases in
which the syntactic parse is correct, but an incor-
rect word sense may have been assigned, since we
disambiguate senses using not only syntactic
structure but also semantic features as selectional
restrictions on arguments. For example, in The
manager interviewed Browne after working, the
parser assigns working the sense LF::FUNCTION,
used with non-agentive subjects, instead of the cor-
rect sense for agentive subjects, LF::WORKING.
For the grammatical utterances in the test suite, our
parser found spanning parses for 80%.
While the ungrammatical sentences in the set are
an important tool for constraining grammar output,
our grammar is designed to find a reasonable inter-
pretation for natural speech, which often is less
than perfect. For example, we have low preference
grammar rules that allow dropped subjects, miss-
ing determiners, and wrong subject verb agree-
ment. In addition, utterances are often fragmentary,
so even those without spanning parses may be con-
sidered correct. Our grammar allows all major con-
stituents (NP, VP, ADJP, ADVP) as valid utter-
ances. As a result, our system produces spanning
parses for 46% of the ?ungrammatical? utterances.
We have not yet done a detailed error analysis.
As a measure of system portability to new do-
mains, we have evaluated our system coverage on
the ATIS (Airline Travel Information System)
speech corpus, which we have never used before.
For this evaluation, the proper names (cities, air-
ports, airline companies) in the ATIS corpus were
added to our lexicon, but no other development
work was performed. We parsed 116 randomly
selected test sentences and hand-checked the re-
sults using our full-sentence accuracy measure.
Our baseline coverage of these utterances is 53%
full-sentence semantic accuracy. Of the 55 utter-
ances that were not completely correct, we found
spanning parses for 36% (20). Reasons that span-
ning parses were marked as wrong include incor-
rect word senses (e.g., for stop in I would like it to
have a stop in Phoenix) or PP-attachment. Reasons
that no spanning parse was found include missing
senses for existing words (e.g., serve as in Does
that flight serve dinner).
7 Discussion
We presented a deep parser and semantic inter-
preter for use in dialogue systems. An important
question to ask is how it compares to other existing
formalisms. At present there is no easy way to
make such comparison. One possible criterion is
grammatical coverage. Looking at the grammar
coverage/accuracy on the TSNLP suite that was
55
used to evaluate the LINGO ERG grammar, our
grammar demonstrates 80% coverage (number of
spanning parses). The reported figure for LINGO
ERG coverage of CSLI is 77% (Oepen, 1999), but
this number has undoubtedly improved in the  9-
year development period. For example, the current
reported coverage figures on spoken dialogue cor-
pora are  close to 90% (Oepen et al, 2002).
However, the grammar coverage alone is not a
satisfactory measure for a deep NLP system for use
in practical applications, because the logical forms
and therefore the capabilities of deep NLP systems
differ significantly. A major distinguishing feature
of our system is that the logical form it outputs
uses semantically motivated word senses. LINGO
ERG, in contrast, contains only syntactically moti-
vated word senses. For example, the words end and
finish are not related in any obvious way. This re-
flects a difference in underlying philosophy.
LINGO ERG aims for linguistic precision, and as
can be seen from our experiments, requiring the
parser to select correct domain-independent word
senses lowers accuracy.
Our system, however, is built with the goal of
easy portability within the context of dialogue
systems. The availability of word senses simplifies
the design of domain-independent interpretation
components, such as reference resolution and
speech act interpretation components that use do-
main-independent syntactic and semantic informa-
tion to encode conventional interpretation rules.
If the LINGO ERG grammar were to be put in a
dialogue system that requires domain interpretation
and reasoning, an additional lexical interpretation
module would have to be developed to perform
word sense disambiguation as well as interpreta-
tion, something that has not yet been done.
Acknowledgments
We thank 3 reviewers for helpful comments. This
work was supported by NSF IIS-0328811, DARPA
NBCHD30010 via subcontract to SRI #03-000223
and ONR N00014051004-3 and ?8.
References
H. Alshawi. 1990. Resolving Quasi Logical Forms.
Computational Linguistics 16(3):133-144.
W. Baker, C. Fillmore and J. B. Lowe. 1998. The Ber-
keley FrameNet Project. COLING-ACL'98, Montr?al.
D. Byron. 2002. Resolving Pronominal Reference to
Abstract Entities. ACL-02, Philadelphia.
A. Copestake. 1999. The (New) LKB System. CSLI.
A. Copestake, D. Flickinger, C. Pollard and I. Sag.
2006. Minimal Recursion Semantics: An Introduc-
tion. Research on Language and Computation,
3(4):281-332.
M. Dzikovska. 2004. A Practical Semantic Representa-
tion for Natural Language Parsing. Ph.D. Thesis,
University of Rochester.
M. Dzikovska, J. Allen and M. Swift. Forthcoming.
Linking Semantic and Knowledge Representations in
a Multi-domain Dialogue System. Journal of Logic
and Computation.
M. Dzikovska, J. Allen and M. Swift. 2003. Integrating
Linguistic and Domain Knowledge for Spoken Dia-
logue Systems in Multiple Domains. Workshop on
Knowledge and Reasoning in Practical Dialogue
Systems, IJCAI-2003, Acapulco.
M. Elsner, M. Swift, J. Allen and D. Gildea. 2005. On-
line Statistics for a Unification-based Dialogue
Parser. IWPT05, Vancouver.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Fuchss, A. Koller, J. Niehren, S. Thater. 2004.
Minimal Recursion Semantics as Dominance Con-
straints. ACL-04, Barcelona.
R. Grisham, C. Macleod and A. Meyers. 1994. Comlex
Syntax: Building a Computational Lexicon. COLING
94, Kyoto.
J. Hobbs and S. Shieber. 1987. An Algorithm for Gen-
erating Quantifier Scopings. Computational Linguis-
tics 13(1-2):47-63.
K. Kipper, H. T. Dang and M. Palmer. 2000.  Class-
based Construction of a Verb Lexicon. AAAI-2000.
S. Oepen, D. Flickinger, K. Toutanova and C. Manning.
2002. Lingo Redwoods: A Rich and Dynamic Tree-
bank for HPSG. First Workshop on Treebanks and
Linguistic Theories (TLT2002).
S. Oepen (1999). [incr tsdb()] User Manual.
www.delph-in.net/itsdb/publications/manual.ps.gz.
T. Parsons. 1990. Events in the Semantics of English. A
Study in Subatomic Semantics. MIT Press.
D. Schlangen and A. Lascarides 2003. The Interpreta-
tion of Non-Sentential Utterances in Dialogue. SIG-
DIAL-03, Sapporo.
J. Tetreault. 2001. A Corpus-Based Evaluation of Cen-
tering and Pronoun Resolution. Computational Lin-
guistics. 27(4):507-520.
Vossen, P. (1997) EuroWordNet: A Multilingual Data-
base for Information Retrieval. In Proc. of the Delos
workshop on Cross-language Information Retrieval.
56
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 141?146,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Corpus of Scope-disambiguated English Text 
  Mehdi Manshadi,  James Allen,  Mary Swift Department of Computer Science, University of Rochester Rochester, NY, 14627, USA {mehdih,james,swift}@cs.rochester.edu 
      Abstract 
Previous work on quantifier scope annotation focuses on scoping sentences with only two quantified noun phrases (NPs), where the quan-tifiers are restricted to a predefined list. It also ignores negation, modal/logical operators, and other sentential adverbials. We present a com-prehensive scope annotation scheme. We anno-tate the scope interaction between all scopal terms in the sentence from quantifiers to scopal adverbials, without putting any restriction on the number of scopal terms in a sentence. In ad-dition, all NPs, explicitly quantified or not, with no restriction on the type of quantification, are investigated for possible scope interactions. 1 Introduction Since the early days of natural language under-standing (NLU), quantifier scope disambiguation has been an extremely hard task. Therefore, early NLU systems either devised some mechanism for leaving the semantic representation underspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to sentences based on heuristics (VanLehn 1978, Moran 1988, Alshawi 1992).  There has been a lot of work since then on devel-oping frameworks for scope-underspecified seman-tic representations (Alshawi and Crouch 1992, Bos 1996, Copestake et al, 2001, Egg et al, 2001). The motivation of most recent formalisms is to develop a constraint-based framework where you can in-crementally add constraints to filter out unwanted scopings. However, almost all of these formalisms are based on hard constraints, which have to be 
satisfied in every reading of the sentence. It seems that the story is different in practice. Most of the constraints one can hope for (imposed by dis-course, pragmatics, word knowledge, etc.) are soft constraints, that is they define a preference over the possible readings of a sentence. As a result, statistical methods seem to be well suited for scope disambiguation. Surprisingly enough, after two decades of ex-tensive work on statistical techniques in natural language processing, there has not been much work on scope disambiguation (see section 6 for a review). In addition, as discussed later, this work is very restricted. It considers sentences with only two quantifiers, where the quantifiers are picked from a predefined list. For example, it ignores de-finites, bare singulars/plurals, and proper nouns, as well as negations and other scopal operators. A major reason for the lack of work on statisti-cal scope disambiguation is the lack of a comprehensive scope-disambiguated corpus. In fact, there is not even a standard test set for evaluation purposes. The reason behind this latter fact is simple. Scope disambiguation is very hard even for humans. In fact, our own early effort to annotate part of the Penn Treebank with full scope information soon proved to be too ambitious.  Instead, we have picked a domain that covers many challenging phenomena in scope disam-biguation, while keeping the scope disambiguation fairly intuitive. This helps us to build the first moderately sized corpus of natural language text with full scope information. By fully scoping a sentence, we mean to label the scope interaction between every two scopal elements in that sen-
141
tence. We scope all scope-bearing NPs (quantified or not), negations, logical/modal operators, and other sentential adverbials. We also annotate plu-rals with their distributive vs. collective readings. In addition, we label sentences with coreference relations because they affect the scope interaction between NPs. 2 Domain The domain is the description of tasks about edit-ing plain text files; in other words, a natural lan-guage interface for text editors such as Linux SED, AWK, or EMACS programs. Figure (1) gives some sentences from the corpus. This domain has several properties that make it a great choice for a first effort to build a comprehensive scope-disambiguated corpus. First, it carries a lot of scope interactions. As shown in the examples, the domain carries many quantified NPs. Also, scopal operators such as ne-gation, and logical operators occur pretty often in the domain. Second, scope disambiguation is criti-cal for deep understanding in this domain. Third, scoping is fairly intuitive, because a conscious knowledge of scoping is required in order to be able to accomplish the explained task. This is ex-actly the key property of this domain that makes building a comprehensive scope-disambiguated corpus feasible. 3 Corpus 3.1 The core corpus The core part of the corpus has been gathered from three different resources, each making up roughly one third of the core corpus. ? One liners: These are help documents found on the web for Linux command-line text editors such as SED and AWK, giving a description of a task plus one line of code performing the task. ?  Online tutorials: Many other online tutorials on 
using command-line editors and regular expres-sions exist. Sentences were manually extracted from examples and exercises in these tutorials. ? Computer science graduate students: These are the sentences provided by CS graduate students describing some of the routine text editing tasks they often do. The sentences have been provided by both native and non-native English speakers.  3.2 Expanding corpus with crowd sourcing  The core corpus was used to get more sentences using crowd sourcing. We provided input/output (I/O) examples for each task in the core corpus, and asked the workers on Mechanical Turk to pro-vide the description of the task based on the I/O example(s). Figure (2) shows an example of two I/O pairs given to the workers in order to get the description of a single task. The reason for using two I/O pairs (instead of only one) is that there is almost always a trivial description for a single I/O pair. Even with two I/O pairs, we sometimes get the description of a different task, which happens to work for the both pairs. For example the original description for the task given in figure (2) is: 1. Sort all the lines by their second field. The following descriptions are provided by three workers based on the given input/output texts:  2. Sort the lines alphabetically by the values in the 2nd column. 3. Sort the lines by the first group of letters. 4. Alphabetize each line using the first letter of each word in the second column. (3) gives the description of a different task, but it works for the given I/O pairs. This is not a problem for us, but actually a case that we would prefer to happen, because this way, we not only get a variety of sentences defining the same task, but also obtain descriptions of new tasks. We can add these new tasks to the core corpus, label them with new I/O 
1. Find an occurrence of the word "TBA" in every line and remove it from the line. 2. Print a list of the lines that do not start with a digit or end with a letter. 3. Replace every string "anti" possibly followed by a hyphen with "not". Figure 1. Some examples from the core corpus 
INPUT OUTPUT 1000  NY  April 3000  HU  August 4000  OR  May 4000  AL  June 
4000  AL  June 3000  HU  August 1000  NY  April 4000  OR  May c  josh   21 a  adams  23 d  sam   26 b  john   25 
a  adams  23 b  john   25 c  josh   21 d  sam   26  Figure 2. Two I/O pairs given for a single task 
142
pairs and hence expand the corpus in a bootstrap-ping fashion. The data acquired from Mechanical Turk is of-ten quite noisy, therefore all sentences are re-viewed manually and tagged with different categories (e.g. paraphrase of the original descrip-tion, wrong but coherent description, etc.).  3.3 Pre-processing the corpus The corpus is tokenized and parsed using the Stan-ford PCFG parser (Klein and Manning 2003). We guide the parser by giving suggestions on part-of-speech (POS) tags based on the gold standard POS tags provided for some classes of words such as verbs. Shallow NP chunks and negations are auto-matically extracted from the parse trees and in-dexed. The resulting NP-chunked sentences are then reviewed manually, first to fix the chunking errors, hence providing gold standard chunks, and second, to add chunks for other scopal operators such as sentential adverbials since the above auto-mated approach will not extract those. Figure (3) shows the examples in figure (1) after chunking. As shown in these examples, NP chunks are in-dexed by numbers, negation by the letter ?N? fol-lowed by a number and all other scopal operators by the letter ?O? followed by a number.  4 Scope annotation The chunked sentences are given to the annotators for scope annotation. Given a pair of chunks i and j, three kinds of relation could hold between them.  ? Outscoping constraints: represented as (i>j), which means chunk i outscopes (i.e. has a wider scope over) chunk j.   ? Coreference relations: represented as (i=j). This could be between a pronoun and its antecedent or between two nouns.1 ? No scope interaction: If a pair is left unscoped, it means that either there is no scope interaction between the chunks, or switching the order of the chunks results in a logically equivalent formula.  The overall scoping is represented as a list of semicolon-separated constraints. The annotators                                                            1  Bridging anaphora relations are simply represented as out-scoping relations, because often there is not a clear distinction between the two. However for theoretical purposes, an out-scoping constraint (i>j), where i is not accessible to j, is being understood as a bridging anaphora relation. 
are allowed to cascade constraints to form a more concise representation (see Figure 3).  4.1 Logical equivalence vs. intuitive scoping  Our early experiments showed that a main source of inter-annotator disagreement are pairs of chunks for which, both orderings are logically equivalent (e.g. two existentials or two universals), but an an-notator may label them with outscoping constraints based on his/her intuition. It turns out that the an-notators? intuitions are not consistent in these cases. Even a single annotator does not remain consistent throughout the data in such cases. Al-though it does not make any difference in logic, this shows up as inter-annotator disagreement. In order to prevent this, annotators were asked to rec-ognize these cases and leave them unscoped. 4.2 Plurals Plurals, in general, introduce a major source of complexity both in formal and computational se-mantics (Link 1997). From a scope?disambiguation point of view, the main issue with plurals come from the fact that they carry two pos-sible kinds of readings: collective vs. distributive. We treat plurals as a set of individuals and assume that the index of a plural NP refers to the set (col-lective reading). However, we also assume that every plural potentially carries an implicit univer-sal quantifier ranging over all elements in the set. We represent this implicit universal with id (?d? for distributive) where i is the index of the plural NP. It is important to notice that while most theoretical papers talk about the collectivity vs. distributivity distinction at the sentence level, for us the right treatment is to make this distinction at the con-straint level. That is, a plural may have a collective reading in one constraint but a distributive reading in another, as shown in example 2 in figure (3). 
1. Find [1/ an instance] of [2/ the word "TBA"] in [3/ every line] and remove [4/ it] from [5/ the line].  (3>1 ; 3=5 ; 1=4) // concise form: (5=3>1=4) 2. Print [1/ a list] of [2/ the lines] that do [N1/ not] start with [3/ a digit] [O1/ or] end with [4/ a letter]. (2>1 ; 2d>N1>3,4 ; N1>O1) // (i>j,k) ? (i>j; i>k) 3. Replace [1/ every string "anti"] [O1/ possibly] fol-lowed by [2/ a hyphen] with [3/ "not"]. (1>O1>2 ; 1>3) Figure 3. Chunked sentences labeled with scopings 
143
4.3 Other challenges of scope annotation  In spite of choosing a specific domain with fairly intuitive quantifier scoping, the scope annotation has been a very challenging job. There are several major sources of difficulty in scope annotation. First, there has not been much work on corpus-based study of quantifier scoping. Most work on quantifier scoping focuses on scoping phenomena, which may be interesting from theoretical perspec-tive, but do not occur very often in practice. There-fore many challenging practical phenomena remain unexplored. During annotation of the corpus, we encountered a lot of these phenomena, which we have tried to generalize and find a reasonable treatment for. Second, other sources of ambiguity are likely to show up as scope disagreement. Fi-nally, very often the disagreement in scoping does not result from the different interpretations of the sentence, but the different representations of the same interpretation. In writing the annotation scheme, extreme care has been taken to prevent these spurious disagreements. Technical details of the annotation scheme are beyond the scope of this paper. We leave those for a longer paper. 5 Statistics The current corpus contains around 500 sentences in the core level and 2000 sentences acquired from crowd sourcing. The number of scopal terms per sentence is 3.9, out of which 95% are NPs and the rest are scopal operators. Table (1) shows the per-centage of different types of NP in the corpus. The core corpus has already been annotated, out of which a hundred sentences have been anno-tated by three annotators in order to measure the inter-annotator agreement (IAA). Two of the anno-tators are native English speakers and the third is a non-native speaker who is fluent in English. All three have some background in linguistics. 5.1 Inter-annotator agreement  Although coreference relations were labeled in the corpus, we do not incorporate them in calculating IAA. This is because, annotating coreference rela-tions is much easier than scope disambiguation, so incorporating them favors toward higher IAAs, which may be deceiving. Furthermore previous work only considers scope relations and hence we do the same in order to have a fair comparison. 
We represent each scoping using a directed graph over the chunk indices. For every outscoping rela-tion i>j, node i is connected to node j by the di-rected edge (i,j). For example, figure (4a) represents the scoping in (5). 5. Delete [1/ the first character] of [2/ every word] and [3/ the first word] of  [4/ every line] in [5/ the file]. (5>2>1 ; 5>4>3) Note that the directed graph must be a DAG (di-rected acyclic graph), otherwise the scoping is not valid. In order to be able to measure the similarity of two DAGs corresponding to two different scop-ings of a single sentence, we borrow the notion of transitive closure from graph theory. The transitive closure (TC) of a directed graph G=(V,E) is the graph G+=(V,E+), where E+ is defined as follows: 6. E+={(i,j) | i,j ?V and i reaches j using a non-null directed path in G} Given the TC graph of a scoping, every pair (i,j), where i precedes j in the sentence, has one of the following three labels: ? WS (i outscopes j): (i,j) ? E+  ? NS (j outscopes i):  (j,i) ? E+  ? NI (no interaction): (i,j) ? E+ ? (j,i)  ? E+   A pair is considered a match between two scop-ings, if it has the same label in both. We define the metrics at two levels, constraint level and sentence level. At constraint level, every pair of chunks in every sentence is considered one instance. At sen-tence level, every sentence is treated as an in-
Type of NP chunk Percentage NPs with explicit quantifiers  (including indefinite A) 35% Definites 27% Bare singulars/plurals 25% Pronouns  7% Proper names (files, variables, etc.) 6% Table 1. Corpus statistics  
                 (a)   (b) Figure 4. DAG of scoping in (5) and its TC  
144
stance. A sentence counts as a match if and only if every pair of chunks in the sentence has the same label in both scopings. Unlike previous work (sec-tion 6) where there is a strong skew in label distri-bution, in our corpus the labels are almost evenly distributed, each consisting around 33% of the in-stances. We use Cohen?s kappa score for multiple annotators (Davies & Fleiss 1982) to measure IAA. Table (2) reports the kappa score.  The IAA defined above serves well for theo-retical purposes, but an easier metric could be de-fined which works fine for most practical purposes. For example, if the target language is first order logic with generalized quantifiers, the relative scope of the chunks labeled NI does not affect the interpretation.2 Therefore, we define a new version of observed agreement in which we consider a pair a match if it is labeled NI in one scoping or as-signed the same label in both scopings. Table (2) reports the IAA based on the latter similarity measure, called ?-EZ. 6 Related work To the best of our knowledge, there have been three major efforts on building a scope-disambiguated corpus for statistical scope disam-biguation, among which Higgins and Sadock (2003) is the most comprehensive. Their corpus consists of 890 sentences from the Wall Street journal section of the Penn Treebank. They pick sentences containing exactly two quantifiers from a predefined list. This list does not include definites, indefinites, or bare singulars/plurals. Every sen-tence is labeled with one of the three labels corresponding to the first quantifier having wide-scope, the second quantifier having wide scope, or no scope interaction between the two. They achieve an IAA of 52% on this task. The majority of sentences in their corpus (more than 60%) have been labeled with no scope interaction.   Galen and McCartney (2004) is another effort to provide scope-disambiguated data. They pick a set of sentences from LSAT and GRE logic games, which again contain only two quantifiers from a limited list of quantifiers. Their corpus consists of 305 sentences. In around 70% of these sentences,                                                            2 Note that any pair left unscoped is labeled NI. Most of these pairs are those whose both orderings are logically equivalent (section 4.1). Besides, we assume all the scopings are valid that is there is at least one interpretation satisfying them. 
the first quantifier has wide scope. A major prob-lem with this data is that the sentences are artifi-cially constructed for the LSAT and GRE tests.  In a recent work Srinivasan and Yates (2009) study the usage of pragmatic knowledge in finding the intended scoping of a sentence. Their labeled data set consists of 46 sentences, extracted from Web1Tgram (from Google, Inc) and hence is open-domain. The corpus consists of short sentences with two specific quantifiers: Every and A. All sen-tences share the same syntactic structure, an active voice English sentence of the form (S (NP (V (NP | PP)))). In fact, they try to isolate the effect of pragmatic knowledge on scope disambiguation.  7 Summary and future work We have constructed a comprehensive scope?disambiguated corpus of English text within the domain of editing plain text files. The domain car-ries many scope interactions. Our work does not put any restriction on the type or the number of scope-bearing elements in the sentence. We achieve the IAA of 75% on this task. Previous work focuses on annotating the relative scope of two NPs per sentence, while ignoring the complex scope-bearing NPs such as definites and indefi-nites, and achieves the IAA of 52%.   The current corpus contains 2500 sentences, out of which 500 sentences have already been an-notated. Our goal is to expand the corpus up to twice in size. 20% of the corpus will be annotated and the rest will be left for the purpose of semi-supervised learning. Since world knowledge plays a major role in scope disambiguation, we believe that leveraging unlabeled domain specific data in order to extract lexical information is a promising approach for scope disambiguation. We hope that availability of this corpus motivates more research on statistical scope disambiguation.  Acknowledgments This work was supported in part by grants from the National Science Foundation (IIS-1012205) and The Office of Naval Research (N000141110417).  
 Constraint-level Sentence-level ? 75.0% 66% ?-EZ 92.3% 89% Table 2. Inter-annotator agreement   
145
References  Alshawi, H.  (ed.)  (1992) The core language Engine. Cambridge, MA, MIT Press.  Alshawi, H.  and Crouch, R. (1992) Monotonic semantic interpretation. In Proc. 30th ACL, pages 32?39.  Bos, J. (1996) Predicate logic unplugged. In Proc. 10th Amsterdam Colloquium, pages 133?143. Copestake, A., Lascarides, A. and Flickinger, D. (2001) An Algebra for Semantic Construction in Constraint-Based Grammars. ACL-01. Toulouse, France. Davies, M. and Fleiss, J. (1982) Measuring Agreement for Multinomial Data. Biometrics, 38:1047?1051, Egg M., Koller A., and Niehren J. (2001) The constraint language for lambda structures. Journal of Logic, Language, and Information, 10:457?485. Galen, A. and MacCartney, B. (2004). Statistical resolu-tion of scope ambiguity in Natural language. http://nlp.stanford.edu/nlkr/scoper.pdf. Higgins, D. and Sadock, J. (2003). A machine learning ap-proach to modeling scope preferences. Computa-tional Linguistics, 29(1).  Hobbs, J. and Shieber, S. M. (1987) An Algorithm for Generating Quantifier Scopings. Computational Lin-guistics 13, pp. 47?63. Klein, D. and Manning, C. D. (2003). Accurate Unlexi-calized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430. Link, G. (1998) Ten Years of Research on Plurals - Where Do We Stand? Plurality and quantification By Fritz Hamm, Erhard W. Hinrichs, 1998  Kluwer Academic Publishers. Moran, D. B. (1988). Quantifier scoping in the SRI core language engine. In Proceedings of the 26th Annual Meeting of the Association for Computational Lin-guistics. Srinivasan, P., and Yates, A. (2009). Quantifier scope disambiguation using extracted pragmatic knowl-edge: Preliminary results. In Proceedings of the Con-ference on Empirical Methods in Natural Language Processing. VanLehn, K. (1988) Determining the scope of English quantifiers, TR AI-TR-483, AI Lab, MIT. Woods, W. A.  (1978) Semantics and quantification in natural language question answering, Advances in. Computers, vol. 17, pp 1-87. 
146
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 64?72,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Plurality, Negation, and Quantification:
Towards Comprehensive Quantifier Scope Disambiguation
Mehdi Manshadi, Daniel Gildea, and James Allen
University of Rochester
734 Computer Studies Building
Rochester, NY 14627
mehdih,gildea,james@cs.rochester.edu
Abstract
Recent work on statistical quantifier scope
disambiguation (QSD) has improved upon
earlier work by scoping an arbitrary num-
ber and type of noun phrases. No corpus-
based method, however, has yet addressed
QSD when incorporating the implicit uni-
versal of plurals and/or operators such as
negation. In this paper we report early,
though promising, results for automatic
QSD when handling both phenomena. We
also present a general model for learning
to build partial orders from a set of pair-
wise preferences. We give an n log n algo-
rithm for finding a guaranteed approxima-
tion of the optimal solution, which works
very well in practice. Finally, we signifi-
cantly improve the performance of the pre-
vious model using a rich set of automati-
cally generated features.
1 Introduction
The sentence there is one faculty member in ev-
ery graduate committee is ambiguous with respect
to quantifier scoping, since there are at least two
possible readings: If one has wide scope, there is
a unique faculty member on every committee. If
every has wide scope, there can be different fac-
ulty members on each committee. Over the past
decade there has been some work on statistical
quantifier scope disambiguation (QSD) (Higgins
and Sadock, 2003; Galen and MacCartney, 2004;
Manshadi and Allen, 2011a). However, the extent
of the work has been quite limited for several rea-
sons. First, in the past two decades, the main focus
of the NLP community has been on shallow text
processing. As a deep processing task, QSD is not
essential for many NLP applications that do not re-
quire deep understanding. Second, there has been
a lack of comprehensive scope-disambiguated cor-
pora, resulting in the lack of work on extensive
statistical QSD. Third, QSD has often been con-
sidered only in the context of explicit quantifica-
tion such as each and every versus some and a/an.
These co-occurrences do not happen very often in
real-life data. For example, Higgins and Sadock
(2003) find fewer than 1000 sentences with two or
more explicit quantifiers in the Wall Street journal
section of Penn Treebank. Furthermore, for more
than 60% of those sentences, the order of the quan-
tifiers does not matter, either as a result of the logi-
cal equivalence (as in two existentials), or because
they do not have any scope interaction.
Having said that, with deep language processing
receiving more attention in recent years, QSD is
becoming a real-life issue.1 At the same time, new
scope-disambiguated corpora have become avail-
able (Manshadi et al, 2011b). In this paper, we
aim at tackling the third issue mentioned above.
We push statistical QSD beyond explicit quantifi-
cation, and address an interesting, yet practically
important, problem in QSD: plurality and quan-
tification. In spite of an extensive literature in
theoretical semantics (Hamm and Hinrichs, 2010;
Landmann, 2000), this topic has not been well in-
vestigated in computational linguistics. To illus-
trate the phenomenon, consider (1):
1. Three words start with a capital letter.
A deep understanding of this sentence, requires
deciding whether each word in the set, referred
to by Three words, starts with a potentially dis-
tinct capital letter (as in Apple, Orange, Banana)
or there is a unique capital letter which each word
starts with (as in Apple, Adam, Athens). By treat-
ing the NP Three words as a single atomic entity,
earlier work on automatic QSD has overlooked
this problem. In general, every plural NP poten-
tially introduces an implicit universal, ranging
1For example, Liang et al (2011) in their state-of-the-art
statistical semantic parser within the domain of natural lan-
guage queries to databases, explicitly devise quantifier scop-
ing in the semantic model.
64
over the collection of entities introduced by the
plural.2 Scoping this implicit universal is just as
important. While explicit universals may not oc-
cur very often in natural language, the usage of
plurals is very common. Plurals form 18% of the
NPs in our corpus and 20% of the nouns in Penn
Treebank. Explicit universals, on the other hand,
form less than 1% of the determiners in Penn Tree-
bank. Quantifiers are also affected by negation.
Previous work (e.g., Morante and Blanco, 2012)
has investigated automatically detecting the scope
and focus of negation. However, the scope of
negation with respect to quantifiers is a different
phenomenon. Consider the following sentence.
2. The word does not start with a capital letter.
Transforming this sentence into a meaning repre-
sentation language, for almost any practical pur-
poses, requires deciding whether the NP a capital
letter lies in the scope of the negation or outside
of it. The former describes the preferred reading
where The word starts with a lowercase letter as
in apple, orange, banana, but the latter gives the
unlikely reading, according to which there exists a
particular capital letter, say A, that The word starts
with, as in apple, Orange, Banana. By not in-
volving negation in quantifier scoping, a semantic
parser may produce an unintended interpretation.
Previous work on statistical QSD has been quite
restricted. Higgins and Sadock (2003), which
we refer to as HS03, developed the first statisti-
cal QSD system for English. Their system dis-
ambiguates the scope of exactly two explicitly
quantified NPs in a sentence, ignoring indefinite
a/an, definites and bare NPs. Manshadi and Allen
(2011a), hence MA11, go beyond those limita-
tions and scope an arbitrary number of NPs in a
sentence with no restriction on the type of quantifi-
cation. However, although their corpus annotates
the scope of negations and the implicit universal of
plurals, their QSD system does not handle those.
As a step towards comprehensive automatic
QSD, in this paper we present our work on auto-
matic scoping of the implicit universal of plurals
and negations. For data, we use a new revision
of MA11?s corpus, first introduced in Manshadi et
al. (2011b). The new revision, called QuanText,
carries a more detailed, fine-grained scope annota-
tion (Manshadi et al, 2012). The performance of
2Although plurals carry different types of quantification
(Herbelot and Copestake, 2010), almost always there exists
an implicit universal. The importance of scoping this univer-
sal, however, may vary based on the type of quantification.
our model defines a baseline for future efforts on
(comprehensive) QSD over QuanText. In addition
to addressing plurality and negation, this work im-
proves upon MA11?s in two directions.
? We theoretically justify MA11?s ternary-
classification approach, formulating it as a
general framework for learning to build par-
tial orders. An n log n algorithm is then given
to find a guaranteed approximation within a
fixed ratio of the optimal solution from a set
of pairwise preferences (Sect. 3.1).
? We replace MA11?s hand-annotated features
with a set of automatically generated linguis-
tic features. Our rich set of features signifi-
cantly improves the performance of the QSD
model, even though we give up the gold-
standard dependency features (Sect. 3.3).
2 Task definition
In QuanText, scope-bearing elements (or, as we
call them, scopal terms) of each sentence have
been identified using labeled chunks, as in (3).
3. Replace [1/ every line] in [2/ the file] ending
in [3/ punctuation] with [4/ a blank line] .
NP chunks follow the definition of baseNP
(Ramshaw and Marcus, 1995) and hence are flat.
Outscoping relations are used to specify the rel-
ative scope of scopal terms. The relation i > j
means that chunk i outscopes (or has wide scope
over) chunk j. Equivalently, chunk j is said to
have narrow scope with respect to i. Each sen-
tence is annotated with its most preferred scoping
(according to the annotators? judgement), repre-
sented as a partial order:
4. SI : (2 > 1 > 4; 1 > 3)
If neither i > j nor j > i is entailed from the
scoping, i and j are incomparable. This happens
if both orders are equivalent (as in two existentials)
or when the two chunks have no scope interaction.
Since a partial order can be represented by a Di-
rected Acyclic Graph (DAG), we use DAGs to
represent scopings. For example, G1 in Figure 1
represents the scoping in (4).
2.1 Evaluation metrics
Given the gold standard DAG Gg = (V,Eg) and
the predicted DAG Gp = (V,Ep), a similarity
measure may be defined based on the ratio of the
number of pairs (of nodes) labeled correctly to the
65
21
3 4
(a) G1
2
1
3 4
(b) G+1
2
1 4
3
(c) G2
2 1 3 4
(d) G3
Figure 1: Scoping as DAG
total number of pairs. In order to take the transi-
tivity of outscoping relations into account, we use
the transitive closure (TC) of DAGs. Let G+ =
(V,E+) represent the TC of a DAG G = (V,E).3
G1 and G+1 in Figure 1 illustrate this concept. We
now define the similiarty metric S+ as follows:
?+ =
|E+p ? E+g | ? |E?+p ? E?+g |
|V |(|V | ? 1)/2 (1)
in which G? = (V, E?) is the complement of the
underlying undirected version of G.
HS03 and others have used such a similarity
measure for evaluation purposes. A disadvantage
of this metric is that it gives the same weight to
outscoping and incomparability relations. In prac-
tice, if two scopal terms with equivalent ordering
(and hence, no outscoping relation) are incorrectly
labeled with an outscoping, the logical form still
remains valid. But if an outscoping relation is mis-
labeled, it will change the interpretation of the sen-
tence. Therefore, in MA11, we suggest defining a
precision/recall based on the number of outscop-
ing relations recovered correctly: 4
P+ =
|E+p ? E+g |
|E+p |
, R+ =
|E+p ? E+g |
|E+g |
(2)
3 (u, v) ? G+ ?? ((u, v)?G ?
?w1 . . . wn?V, (u,w1) . . . (wn, v) ? E )
4MA11 argues that TC-based metrics tend to produce
higher numbers. For example if G3 in Figure 1 is a gold-
standard DAG andG1 is a candidate DAG, TC-based metrics
count 2>3 as another match, even though it is entailed from
2 > 1 and 1 > 3. They give an alternative metric based on
transitive reduction (TR), obtained by removing all the re-
dundant edges of a DAG. TR-based metrics, however, have
their own disadvantage. For example, if G2 is another candi-
date forG3, TR-based metrics produce the same numbers for
both G1 and G2, even though G1 is clearly closer to G3 than
G2. Therefore, in this paper we stick to TC-based metrics.
3 Our framework
3.1 Learning to do QSD
Since we defined QSD as a partial ordering, auto-
matic QSD would become the problem of learn-
ing to build partial orders. The machine learning
community has studied the problem of learning to-
tal orders (ranking) in depth (Cohen et al, 1999;
Furnkranz and Hullermeier, 2003; Hullermeier et
al., 2008). Many ranking systems create partial
orders as output when the confidence level for the
relative order of two objects is below some thresh-
old. However, the target being a partial order is
a fundamentally different problem. While the lack
of order between two elements is interpreted as the
lack of confidence in the former, it should be inter-
preted as incomparability in the latter. Learning
to build partial orders has not attracted much atten-
tion in the learning community, although as seen
shortly, the techniques developed for ranking can
be adopted for learning to build partial orders.
As mentioned before, a partial order P can be
represented by a DAG G, with a preceding b in P
if and only if a reaches b in G by a directed path.
Although there could be many DAGs representing
a partial order P , only one of those is a transitive
DAG.5 Therefore, in order to have a one-to-one re-
lationship between QSDs and DAGs, we only con-
sider the class of transitive DAGs, or TDAG. Ev-
ery non-transitive DAG will be converted into its
transitive counterpart by taking its transitive clo-
sure (as shown in Figure 1).
Consider V , a set of nodes and a TDAG G =
(V,E). It would help to think of disconnected
nodes u, v of G, as connected with a null edge .
We define the labeling function ?G : V ? V ??
{+,?, } assigning one of the three labels to each
pair of nodes in G:
?G(u, v) =
?
?
?
+ (u, v) ? G
? (v, u) ? G
 otherwise
(3)
Given the true TDAG G? = (V, E?), and a candidate
TDAG G, we define the Loss function to be the
total number of incorrect edges:
L(G, G?) =
?
u?v?V
I(?G(u, v) 6= ?G?(u, v)) (4)
in which ? is an arbitrary total order over the
nodes in V 6, and I(?) is the indicator function. We
5G is transitive iff (u, v), (v, w) ? G =? (u,w) ? G.
6E.g., the left-to-right order of the corresponding chunks
in the sentence.
66
adopt a minimum Bayes risk (MBR) approach,
with the goal of finding the graph with the lowest
expected loss against the (unknown) target graph:
G? = argmin
G?TDAG
EG?
[
L(G, G?)
]
(5)
Substituting in the definition of the loss function
and exchanging the order of the expectation and
summation, we get:
G? = argmin
G?TDAG
?
u?v?V
EG?
[
I(?G(u, v) 6= ?G?(u, v)
]
= argmin
G?TDAG
?
u?v?V
P (?G(u, v) 6= ?G?(u, v)) (6)
This means that in order to solve Eq. (5), we need
only the probabilities of each of the three labels for
each of the C(n, 2) = n(n? 1)/2 pairs of nodes7
in the graph, rather than a probability for each
of the superexponentially many possible graphs.
We train a classifier to estimate these probabili-
ties directly for a given pair. Therefore, we have
reduced the problem of predicting a partial order
to pairwise comparison, analogous to ranking by
pairwise comparison or RPC (Hullermeier et al,
2008; Furnkranz and Hullermeier, 2003), a popu-
lar technique in learning total orders. The differ-
ence though is that in RPC, the comparison is a
(soft) binary classification, while for partial orders
we have the case of incomparability (the label ),
hence a (soft) ternary classification.
A soft ternary classifier generates three proba-
bilities, pu,v(+), pu,v(?), and pu,v() for each pair
(u, v),8 corresponding to the three labels. Hence,
equation Eq. (6) can be rearranged as follows:
G? = argmax
G?TDAG
?
u?v?V
pu,v(?G(u, v)) (7)
Let ?p be a graph like the one in Figure 2, contain-
ing exactly three edges between every two nodes,
weighted by the probabilities from the n(n? 1)/2
classifiers. We call ?p the preference graph. In-
tuitively speaking, the solution to Eq. (7) is the
transitive directed acyclic subgraph of ?p that has
the maximum sum of weights. Unfortunately find-
ing this subgraph is an NP-hard problem.9
7Throughout this subsection, unless otherwise specified,
by a pair of nodes we mean a pair (u, v) with u?v.
8pv,u for u?v is defined in the obvious way: pv,u(+) =
pu,v(?), pv,u(?) = pu,v(+), and pv,u() = pu,v().
9 The proof is beyond the scope of this paper, but the idea
is similar to that of Cohen et al (1999), on finding total or-
ders. Although they don?t use an RPC technique, Cohen et
3
2
0.5
10.10.8
0.2
0.3
0.1
0.3
0.1
0.6
Figure 2: A preference graph over three nodes.
1. Let ?p be the preference graph and
set G to ?.
2. ?u ? V , let pi(u) =?v pu,v(+)?
?
v pu,v(?).
3. Let u? = argmaxu pi(u),
S? =
?
v?G pv,u?(?) & S =
?
v?G pv,u?().
4. Remove u? and all its incident edges
from ?p.
5. Add u? to G; also if S? > S, for
every v ? G? u?, add (v, u?) to G.
6. If ?p is empty, output G, otherwise
repeat steps 2-5.
Figure 3: An approximation algorithm for Eq. (7)
Since it is very unlikely to find an efficient al-
gorithm to solve Eq. (7), instead, we propose the
algorithm in Figure 3 which finds an approximate
solution. The idea of the algorithm is simple. By
finding u? with the highest pi(u) in step 3, we form
a topological order for the nodes in G in a greedy
way (see Footnote 9). We then add u? to G. A
directed edge is added either from every node in
G?u? to u? or from no node, depending on which
case makes the sum of the weights in G higher.
Theorem 1 The algorithm in Figure 3 is a 1/3-
OPT approximation algorithm for Eq. (7).
Proof idea. First of all, note that G is a TDAG,
because edges are only added to the most recently
created node in step 5. Let OPT be the optimum
value of the right hand side of Eq. (7). The sum of
all the weights in ?p is an upper bound for OPT :
?
u?v?V
?
??{+,?,}
pu,v(?) ? OPT
Step 5 of the algorithm guarantees that the labels
?G(u, v) satisfy:
?
u?v?V
pu,v(?G(u, v)) ?
?
u?v?V
pu,v(?) (8)
al. (1999) encounter a similar optimization problem. They
propose an approximation algorithm which finds the solution
(a total order) in a greedy way. Here we use the same greedy
technique to find a total order, but take it only as the topolog-
ical order of the solution (Figure 3).
67
for any ? ? {+,?, }. Hence:
?
u?v?V
pu,v(?G(u, v))=
1
3
(
3
?
u?v?V
pu,v(?G(u, v))
)
? 13
?
u?v?V
?
??{+,?,}
pu,v(?)
? 13OPT
In practice, we improve the algorithm in Figure 3,
while maintaining the approximation guarantee, as
follows. When adding a node u? to graph G, we
do not make a binary decision as to whether con-
nect every node in G to u? or none, but we use
some heuristics to choose a subset of nodes (pos-
sibly empty) in G that if connected to u? results
in a TDAG whose sum of weights is at least as
big as the binary none-vs-all case. As described in
Sec. 4, the algorithm works very well in our QSD
system, finding the optimum solution in virtually
all cases we examined.
3.2 Dealing with plurality and negation
Consider the following sentence with the plural
NP chunk the lines.
5. Merge [1p/ the lines], ending in [2/ a punctu-
ation], with [3/ the next non-blank line].
6. SI : (1c > 1d > 2; 1d > 3) 10
In QuanText, plural chunks are indexed with a
number followed by the lowercase letter ?p?. As
seen in (6), the scoping looks different from before
in that the terms 1d and 1c are not the label of any
chunk. These two terms refer to the two quantified
terms introduced by the plural chunk 1p: 1c (for
collection) represents the set (or in better words
collection) of entities, defined by the plural, and 1d
(for distribution) refers to the implicit universal,
introduced by the plural. In other words, for a plu-
ral chunk ip, id represents the universally quanti-
fied entity over the collection ic. The outscoping
relation 1d > 2 in (6) states that every line in the
collection, denoted by 1c, starts with its own punc-
tuation character. Similarly, 1d > 3 indicates that
every line has its own next non-blank line. Fig-
ure 4(a) shows a DAG for the scoping in (6).
In (7) we have a sentence containing a negation.
In QuanText, negation chunks are labeled with an
uppercase ?N? followed by a number.
10This scoping corresponds to the logical formula:
Dx1c, Collection(x1c) ? ?x1d, In(x1d, x1c)?
(Line(x1d)?(?x2, Punctuation(x2)?EndIn(x1d, x2))?
(Dx3,?blank(x3) ? next(x1d, x3) ?merge(x1d, x3)))
It is straightforward to write a formula for, say, 1c > 2 > 1d.
(a)
1c 1d
2
3
(b)
2 1
3
N1 4
Figure 4: DAGs for scopings in (6) and (8)
7. Extract [1/ every word] in [2/ file ?1.txt?],
which starts with [3/ a capital letter], but
does [N1/ not] end with [4/ a capital letter].
8. SI : (2 > 1 > 3; 1 > N1 > 4)
As seen here, a negation simply introduces a
chunk, which participates in outscoping relations
like an NP chunk. Figure 4(b) represents the scop-
ing in (8) as a DAG.
From these examples, as long as we create two
nodes in the DAG corresponding to each plu-
ral chunk, and one node corresponding to each
negation, there is no need to modify the under-
lying model (defined in the previous section).
However, when u (or v) is a negation (Ni) or
an implicit universal (id) node, the probabilities
p?u,v (? ? {+,?, }) may come from a different
source, e.g. a different classification model or the
same model with a different set of features, as de-
scribed in the following section.
3.3 Feature selection
Previous work has shown that the lexical item
of quantifiers and syntactic clues (often extracted
from phrase structure trees) are good at predicting
quantifier scoping. Srinivasan and Yates (2009)
use the semantics of the head noun in a quantified
NP to predict the scoping. MA11 also find the lex-
ical item of the head noun to be a good predictor.
In this paper, we introduce a new set of syntac-
tic features which we found very informative: the
?type? dependency features of de Marneffe et al
(2006). Adopting this new set of features, we out-
perform MA11?s system by a large margin. An-
other point to mention here is that the features that
are predictive of the relative scope of quantifiers
are not necessarily as helpful when determining
the scope of negation and vice versa. Therefore we
do not use exactly the same set of features when
68
one of the scopal terms in the pair11 is a negation,
although most of the features are quite similar.
3.3.1 NP chunks
We first describe the set of features we have
adopted when both scopal terms in a pair are NP-
chunks. We have organized the features into dif-
ferent categories listed below.
Individual NP-chunk features
Following features are extracted for both NP
chunks in a pair.
? The part-of-speech (POS) tag of the head of chunk
? The lexical item of the head noun
? The lexical item of the determiner/quantifier
? The lexical item of the pre-determiner
? Does the chunk contain a constant (e.g. ?do?, ?x?)?
? Is the NP-chunk a plural?
Implicit universal of a plural
Remember that every plural chunk i introduces
two nodes in the DAG, ic and id. Both nodes
are introduced by the same chunk i, therefore they
use the same set of features. The only exception
is a single additional binary feature for plural NP
chunks, which determines whether the given node
refers to the implicit universal of the plural (i.e. id)
or to the collection itself (i.e. ic).
? Does this node refer to an implicit universal?
Syntactic features ? phrase structure tree
As mentioned above, we have used two sets
of syntactic features. The first is motivated by
HS03?s work and is based on the constituency (i.e.
phrase structure) tree T of the sentence. Since
our model is based on pairwise comparison, the
following features are defined for each pair of
chunks. In the following, by chunk we mean the
deepest phrase-level node in T dominating all the
words in the chunk. If the constituency tree is cor-
rect, this node is usually an NP node. Also, P
refers to the undirected path in T connecting the
two chunks.
? Syntactic category of the deepest common ancestor
? Does 1st/2nd chunk C-command 2nd/1st one?
? Length of the path P
? Syntactic categories of nodes on P
? Is there a conjoined node on P ?
? List of punctuation marks dominated by nodes on P
Syntactic features ? dependency tree
Although regular ?untyped? dependency relations
do not seem to help our QSD system in the pres-
ence of phrase-structure trees, we found the col-
11Since our model is based on pairwise comparison, every
sample is in fact a pair of nodes (u, v) of the DAG.
lapsed typed dependencies (de Marneffe and Man-
ning, 2008) very helpful, even when used on top of
the phrase-structure features. Below is the list of
features we extract from the collapsed typed de-
pendency tree Td of each sentence. In the follow-
ing, by noun we mean the node in Td which corre-
sponds to the head of the chunk. The choice of the
word noun, however, may be sloppy, as the head
of an NP chunk may not be a noun.
? Does 1st/2nd noun dominate 2nd/1st noun?
? Does 1st/2nd noun immediately dominate 2nd/1st?
? Type of incoming dependency relation of each noun
? Syntactic category of the deepest common ancestor
? Lexical item of the deepest common ancestor
? Length of the undirected path between the two
3.3.2 Negations
There are no sentences in our corpus with more
than one negation. Therefore, for every pair of
nodes with one negation, the other node must re-
fer to an NP chunk. We use the following word-
level, phrase-structure, and dependency features
for these pairs.
? Lexical item of the determiner for the NP chunk
? Does the NP chunk contain a constant?
? Is the NP chunk a plural?
? If so, does this node refer to its implicit universal?
? Does the negation C-command the NP chunk in T ?
? Does the NP chunk C-command the negation in T ?
? What is the POS of the parent p of negation in Td?
? Does p dominate the noun in Td?
? Does the noun dominate p in Td?
? Does p immediately dominate the noun in Td?
? If so, what is the type of the dependency?
? Does the noun immediately dominate p in Td?
? If so, what is the type of the dependency?
? Length of the undirected path between the two in Td
4 Experiments
QuanText contains 500 sentences with a total of
1750 chunks, that is 3.5 chunks/sentence on av-
erage. Of those, 1700 chunks are NP chunks.
The rest are scopal operators, mainly negation. Of
all the NP chunks, 320 (more than 18%) are plu-
ral, each introducing an implicit universal, that is,
an additional node in the DAG. Since we feed
each pair of elements to the classifiers indepen-
dently, each (unordered) pair introduces one sam-
ple. Therefore, a sentence with n scopal elements
creates C(n, 2) = n(n ? 1)/2 samples for classi-
fication. When all the elements are taken into ac-
count,12 the total number of samples in the corpus
will be:
12Here by all elements we mean explicit chunks and the
implicit universals. QuanText labels some other (implicit) el-
ements, which we have not been handled in this work. In
particular, some nouns introduce two entities: a type and a
69
?i
C(ni, 2) ? 4500 (9)
Where ni is the number of scopal terms introduced
by sentence i. Out of the 4500 samples, around
1800 involve at least one implicit universal (i.e.,
id), but only 120 samples contain a negation. We
evaluate the performance of the system for implicit
universals and negation both separately and in the
context of full scope disambiguation. We split the
corpus at random into three sets of 50, 100, and
350 sentences, as development, test, and train sets
respectively.13
To extract part-of-speech tags, phrase structure
trees, and typed dependencies, we use the Stan-
ford parser (Klein and Manning, 2003; de Marn-
effe et al, 2006) on both train and test sets. Since
we are using SVM, we have passed the confidence
levels through a softmax function to convert them
into probabilities P ?u,v before applying the algo-
rithm of Section 3. We take MA11?s system as the
baseline. However, in order to have a fair com-
parison, we have used the output of the Stanford
parser to automatically generate the same features
that MA11 have hand-annotated.14 In order to run
the baseline system on implicit universals, we take
the feature vector of a plural NP and add a fea-
ture to indicate that this feature vector represents
the implicit universal of the corresponding chunk.
Similarly, for negation we add a feature to show
that the chunk represents a negation. As shown in
Section 3.3.2, we have used a more compact set
of features for negations. Once again, in order to
have a fair comparison, we apply a similar modifi-
cation to the baseline system. We also use the ex-
act same classifier as used in MA11.15 Figure 5(a)
compares the performance of our model, which we
refer to as RPC-SVM-13, with the baseline sys-
tem, but only on explicit NP chunks.16 The goal
for running this experiment has been to compare
the performance of our model to the baseline sys-
token, as described by Manshadi et al (2012). In this work,
we have only considered the token entity introduced by those
nouns and have ignored the type entity.
13Since the percentage of sentences with negation is small,
we made sure that those sentences are distributed uniformly
between three sets.
14MA11?s features are similar to part-of-speech tags and
untyped dependency relations.
15SVMMulticlass from SVM-light (Joachims, 1999).
16In all experiments, we ignore NP conjunctions. Previous
work treats a conjunction of NPs as separate NPs. However,
similar to plurals, NP conjunctions (disjunctions) introduce
an extra scopal element: a universal (existential). We are
working on an annotation scheme for NP conjunctions, so
we have left this for after the annotations become available.
NP-Chunks only (no id or negation) ?+ P+ R+ F+ AR ABaseline (MA11) 0.762 0.638 0.484 0.550 0.59 0.47Our model (RPC-SVM-13) 0.827 0.743 0.677 0.709 0.68 0.55
(a) Scoping explicit NP chunksOverall system (including negation and implicit universals) ?+ P+ R+ F+ AR ABaseline (MA11) 0.787 0.688 0.469 0.557 0.59 0.47Our model (RPC-SVM-13) 0.863 0.784 0.720 0.751 0.69 0.55
(b) Scoping all elements (including id and Ni)
Figure 5: Performance on QuanText data
tem on the task that it was actually defined to per-
form (that is scoping only explicit NP chunks).
As seen in this table, by incorporating a richer
set of features and a better learning algorithm, our
model outperforms the baseline by almost 15%.
The measure A in these figures shows sentence-
based accuracy. A sentence counts as correct iff
every pair of scopal elements has been labeled
correctly. Therefore A is a tough measure. Fur-
thermore, it is sensitive to the length of the sen-
tence. Following MA11, we have computed an-
other sentence-based accuracy measure, AR. In
computing AR, a sentence counts as correct iff all
the outscoping relations have been recovered cor-
rectly ? in other words, iff R = 100%, regardless
of the value of P. AR may be more practically
meaningful, because if in the correct scoping of
the sentence there is no outscoping between two
elements, inserting one does not affect the inter-
pretation of the sentence. In other words, precision
is less important for QSD in practice.
Figure 5(b) gives the performance of the over-
all model when all the elements including the im-
plicit universals and the negations are taken into
account. That the F-score of our model for the
second experiment is 0.042 higher than F-score for
the first indicates that scoping implicit universals
and/or negations must be easier than scoping ex-
plicit NP chunks. In order to find how much one or
both of the two elements contribute to this gain, we
have run two more experiments, scoping only the
pairs with at least one implicit universal and pairs
with one negation, respectively. Figure 6 reports
the results. As seen, the contribution in boosting
the overall performance comes from the implicit
universals while negations, in fact, lower the per-
formance. The performance for pairs with implicit
universal is higher because universals, in general,
70
Implicit universals only (pairs with at least one id) P+ R+ F+Baseline (MA11) 0.776 0.458 0.576Our model (RPC-SVM-13) 0.836 0.734 0.782
(a) Pairs with at least one implicit universalNegation only (pairs with one negation) P+ R+ F+Baseline (MA11) 0.502 0.571 0.534Our model (RPC-SVM-13) 0.733 0.55 0.629
(b) Pairs with at least one negation
Figure 6: Implicit universals and negations
are easier to scope, even for the human annota-
tors.17 There are several reasons for poor perfor-
mance with negations as well. First, the number
of negations in the corpus is small, therefore the
data is very sparse. Second, the RPC model does
not work well for negations. Scoping a negation
relative to an NP chunk, with which it has a long
distance dependency, often depends on the scope
of the elements in between. Third, scoping nega-
tion usually requires a deep semantic analysis.
In order to see how well our approximation al-
gorithm is working, similar to the approach of
Chambers and Jurafsky (2008), we tried an ILP
solver18 for DAGs with at most 8 nodes to find the
optimum solution, but we found the difference in-
significant. In fact, the approximation algorithm
finds the optimum solution in all but one case.19
5 Related work
Since automatic QSD is in general challenging,
traditionally quantifier scoping is left underspec-
ified in deep linguistic processing systems (Al-
shawi and Crouch, 1992; Bos, 1996; Copestake et
al., 2001). Some efforts have been made to move
underspecification frameworks towards weighted
constraint-based graphs in order to produce the
most preferred reading (Koller et al, 2008), but
the source of these types of constraint are often
discourse, pragmatics, world knowledge, etc., and
hence, they are hard to obtain automatically. In or-
17Trivially, we have taken the relation outscoping ic > id
for granted and not counted it towards higher performance.
18lpsolve: http://sourceforge.net/projects/lpsolve
19To find the gain that can be obtained with gold-standard
parses, we used MA11?s system with their hand-annotated
and the equivalent automatically generated features. The
former boost the performance by 0.04. Incidentally, HS03
lose almost 0.04 when switching to automatically generated
parses.
der to evade scope disambiguation, yet be able to
perform entailment, Koller and Thater (2010) pro-
pose an algorithm to calculate the weakest read-
ings20 from a scope-underspecified representation.
Early efforts on automatic QSD (Moran, 1988;
Hurum, 1988) were based on heuristics, manually
formed into rules with manually assigned weights
for resolving conflicts. To the best of our knowl-
edge, there have been four major efforts on statisti-
cal QSD for English: Higgins and Sadock (2003),
Galen and MacCartney (2004), Srinivasan and
Yates (2009), and Manshadi and Allen (2011a).
The first three only scope two scopal terms in a
sentence, where the scopal term is an NP with an
explicit quantification. MA11 is the first to scope
any number of NPs in a sentence with no restric-
tion on the type of quantification. Besides ignor-
ing negation and implicit universals, their system
has some other limitations too. First, the learning
model is not theoretically justified. Second, hand-
annotated features (e.g. dependency relations) are
used on both the train and the test data.
6 Summary and future work
We develop the first statistical QSD model ad-
dressing the interaction of quantifiers with nega-
tion and the implicit universal of plurals, defining
a baseline for this task on QuanText data (Man-
shadi et al, 2012). In addition, our work improves
upon Manshadi and Allen (2011a)?s work by (ap-
proximately) optimizing a well justified criterion,
by using automatically generated features instead
of hand-annotated dependencies, and by boosting
the performance by a large margin with the help of
a rich feature vector.
This work can be improved in many directions,
among which are scoping more elements such as
other scopal operators and implicit entities, de-
ploying more complex learning models, and de-
veloping models which require less supervision.
Acknowledgement
We need to thank William de Beaumont and
Jonathan Gordon for their comments on the pa-
per and Omid Bakhshandeh for his assistance.
This work was supported in part by NSF grant
1012205, and ONR grant N000141110417.
20Those which can be entailed from other readings but do
not entail any other reading
71
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic
semantic interpretation. In Proceedings of Associa-
tion for Computational Linguistics, pages 32?39.
Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the 10th Amsterdam Colloquium, pages
133?143.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 698?706, Stroudsburg, PA.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243?270.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of As-
sociation for Computational Linguistics ?01, pages
140?147.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
Proceedings of International Conference on Lan-
guage Resources and Evaluation ?12.
Johannes Furnkranz and Eyke Hullermeier. 2003.
Pairwise preference learning and ranking. In Pro-
ceedings of the 14th European Conference on Ma-
chine Learning, volume 2837, pages 145?156.
Andrew Galen and Bill MacCartney. 2004. Statistical
resolution of scope ambiguity in natural language.
http://nlp.stanford.edu/nlkr/scoper.pdf.
Fritz Hamm and Edward W. Hinrichs. 2010. Plurality
and Quantification. Studies in Linguistics and Phi-
losophy. Springer.
Aurelie Herbelot and Ann Copestake. 2010. Anno-
tating underquantification. In Proceedings of the
Fourth Linguistic Annotation Workshop, LAW IV
?10, pages 73?81.
Derrick Higgins and Jerrold M. Sadock. 2003. A ma-
chine learning approach to modeling scope prefer-
ences. Computational Linguistics, 29(1):73?96.
Eyke Hullermeier, Johannes Furnkranz, Weiwei
Cheng, and Klaus Brinker. 2008. Label ranking
by learning pairwise preferences. Artificial Intelli-
gence, 172(1617):1897 ? 1916.
Sven Hurum. 1988. Handling scope ambiguities in
English. In Proceedings of the second conference
on Applied natural language processing, ANLC ?88,
pages 58?65.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in kernel methods,
pages 169?184. MIT Press, Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430.
Alexander Koller and Stefan Thater. 2010. Comput-
ing weakest readings. In Proceedings of the 48th
Annual Meeting on Association for Computational
Linguistics, Uppsala, Sweden.
Alexander Koller, Michaela Regneri, and Stefan
Thater. 2008. Regular tree grammars as a formal-
ism for scope underspecification. In Proceedings of
Annual Meeting on Association for Computational
Linguistics and Human Language Technologies ?08.
Fred Landmann. 2000. Events and plurality. Kluwer
Academic Publishers, Dordrecht.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of Association for Computa-
tional Linguistics (ACL).
Mehdi Manshadi and James Allen. 2011a. Unre-
stricted quantifier scope disambiguation. In Pro-
ceedings of Association for Computational Linguis-
tics ?11, Workshop on Graph-based Methods for
NLP (TextGraph-6).
Mehdi Manshadi, James Allen, and Mary Swift.
2011b. A corpus of scope-disambiguated English
text. In Proceedings of Association for Computa-
tional Linguistics and Human Language Technolo-
gies ?11: short papers, pages 141?146.
Mehdi Manshadi, James Allen, and Mary Swift. 2012.
An annotation scheme for quantifier scope disam-
biguation. In Proceedings of International Confer-
ence on Language Resources and Evaluation ?12.
Douglas Moran. 1988. Quantifier scoping in the SRI
core language engine. In Proceedings of the 26th
Annual Meeting on Association for Computational
Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text
Chunking Using Transformation-Based Learning.
In Proceedings of the Third Workshop on Very Large
Corpora, pages 82?94, Somerset, New Jersey.
Prakash Srinivasan and Alexander Yates. 2009. Quan-
tifier scope disambiguation using extracted prag-
matic knowledge: preliminary results. In Proceed-
ings of EMNLP ?09.
72
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 142?150,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Expanding the Range of Tractable Scope-Underspecified Semantic
Representations
Mehdi Manshadi and James Allen
Department of Computer Science
University of Rochester
Rochester, NY 14627
{mehdih,james}@cs.rochester.edu
Abstract
Over the past decade, several underspec-
ification frameworks have been proposed
that efficiently solve a big subset of scope-
underspecified semantic representations
within the realm of the most popular
constraint-based formalisms. However, there
exists a family of coherent natural language
sentences whose underspecified representa-
tion does not belong to this subset. It has
remained an open question whether there ex-
ists a tractable superset of these frameworks,
covering this family. In this paper, we show
that the answer to this question is yes. We
define a superset of the previous frameworks,
which is solvable by similar algorithms with
the same time and space complexity.
1 Introduction
Scope ambiguity is a major source of ambiguity in
semantic representation. For example, the sentence
1. Every politician has a website.
has at least two possible interpretations, one in
which each politician may have a different website
(i.e., Every has wide scope) and one in which there
is a unique website for all the politicians (i.e., Every
has narrow scope). Since finding the most preferred
reading automatically is very hard, the most widely
adopted solution is to use an Underspecified Rep-
resentation (UR), that is to encode the ambiguity in
the semantic representation and leave scoping un-
derspecified.
In an early effort, Woods (1986) developed an un-
scoped logical form where the above sentence is rep-
resented (roughly) as the formula:
2. Has(?Every x Politician?, ?A y Website?)
To obtain a fully scoped formula, the quantifiers
are pulled out one by one and wrapped around the
formula. If we pull out Every first, we produce the
fully-scoped formula:
3. A(y,Website(y),
Every(x, Politician(x), Has(x, y))
If we had pulled out A first, we would have had
the other reading, with Every having wide scope.
Hobbs and Shieber (1987) extend this formalism
to support operators (such as not) and present an
enumeration algorithm that is more efficient than the
naive wrapping approach.
Since the introduction of Quasi Logical Form (Al-
shawi and Crouch, 1992), there has been a lot of
work on designing constraint-based underspecifica-
tion formalisms where the readings of a UR are not
defined in a constructive fashion as shown above, but
rather by a set of constraints. A fully-scoped struc-
ture is a reading iff it satisfies all the constraints. The
advantage of these frameworks is that as the pro-
cessing goes deeper, new (say pragmatically-driven)
constraints can be added to the representation in or-
der to filter out unwanted readings. Hole Seman-
tics (Bos, 1996; Bos, 2002), Constraint Language
for Lambda Structures (CLLS) (Egg et al, 2001),
and Minimal Recursion Semantics (MRS) (Copes-
take et al, 2001) are among these frameworks.
In an effort to bridge the gap between the above
formalisms, a graph theoretic model of scope under-
specification was defined by Bodirsky et al (2004),
called Weakly Normal Dominance Graphs. This
142
Figure 1: UG for Every child of a politician runs.
framework and its ancestor, Dominance Con-
straints (Althaus et al, 2003), are broad frameworks
for solving constrained tree structures in general.
When it comes to scope underspecification, some of
the terminology becomes counter-intuitive. There-
fore, here we first define (scope) Underspecifica-
tion Graphs (UG), a notational variant of weakly
normal dominance graphs, solely defined to model
scope underspecification.1 Figure 1 shows a UG for
the following sentence.
4. Every child of a politician runs.
The big circles and the dot nodes are usually re-
ferred to as the hole nodes (or simply holes) and the
label nodes (or simply labels) respectively. The left
and the right holes of each quantifier are placehold-
ers for the restriction and the body of the quanti-
fier. A fully scoped structure is built by plugging
labels into holes, as shown in Figure 2(a). The dot-
ted edges represent the constraints. For example,
the constraint from the restriction hole of Every(x)
to the node Politician(x) states that this label node
must be within the scope of the restriction of Ev-
ery(x) in every reading of the sentence. The con-
straint edge from Every(x) to Run(x) forces the bind-
ing constraint for variable x; that is variable x in
Run(x) must be within the scope of its quantifier.
Figure 2(b) represents the other possible reading of
the sentence. Now consider the sentence:
5. Every politician, whom I know a child of, prob-
ably runs.
with its UG shown in Figure 3. This sentence con-
tains a scopal adverbial (a.k.a. fixed-scopal; cf.
Copestake et al (2005)), the word Probably. Since
in general, quantifiers can move inside or outside a
1The main difference is in the concept of solution in the two
frameworks. See Section 4.3 for details.
Figure 2: Solutions of the UG in Figure 1.
scopal operator, the scope of Probably is left under-
specified, and hence represented by a hole. It is easy
to verify that the corresponding UG has five possible
readings, two of which are shown in Figure 4.
There are at least two major algorithmic problems
that need to be solved for any given UG U : the sat-
isfiability problem; that is whether there exists any
reading satisfying all the constraints in U , and the
enumeration problem; that is enumerating all the
possible readings of a satisfiable U . Unfortunately,
both problems are NP-complete for UG in its gen-
eral form (Althaus et al, 2003). This proves that
Hole Semantics and Minimal Recursion Semantics
are also intractable in their general form (Thater,
2007). In the last decade, there has been a se-
ries of interesting work on finding a tractable subset
of those frameworks, broad enough to cover most
structures occurring in practice. Those efforts re-
sulted in two closely related tractable frameworks:
(dominance) net and weak (dominance) net. Intu-
itively, the net condition requires the following prop-
erty. Given a UG U , for every label node in U with
n holes, if the node together with all its holes is re-
moved from U , the remaining part is composed of at
most n (weakly) connected components. A differ-
ence between net and weak net is that in nets, label-
Figure 3: UG for the sentence in (5).
143
Figure 4: Two of the solutions to the UG in Figure 3.
to-label constraints (e.g. the constraint between Ev-
ery(x) and Run(x) in Figure 1) are not allowed.
Using a sample grammar for CLLS, Koller et al
(2003) conjecture that the syntax/semantics inter-
face of CLLS only generates underspecified repre-
sentations that follow the definition of net and hence
can be solved in polynomial time. They also prove
that the same efficient algorithms can be used to
solve the underspecification structures of Hole Se-
mantics which satisfy the net condition.
Unlike Hole Semantics and CLLS, MRS implic-
itly carries label-to-label constraints; hence the con-
cept of net could not be applied to MRS. In order
to address this, Niehren and Thater (2003) define
the notion of weak net and conjecture that it cov-
ers all semantically complete MRS structures occur-
ring in practice. Fuchss et al (2004) supported the
claim by investigating MRS structures in the Red-
woods corpus (Oepen et al, 2002). Later coherent
sentences were found in other corpora or suggested
by other researchers (see Section 6.2.2 in Thater
(2007)), whose UR violates the net condition, inval-
idating the conjecture. However, violating the net
condition occurs in a similar way in those examples,
suggesting a family of non-net structures, character-
ized in Section 4.2. Since then, it has been an open
question whether there exists a tractable superset of
weak nets, covering this family of non-net UGs.
In the rest of this paper, we answer this ques-
tion. We modify the definition of weak net to de-
fine a superset of it, which we call super net. Super
net covers the above mentioned family of non-net
structures, yet is solvable by (almost) the same al-
gorithms as those solving weak nets with the same
time and space complexity.
The structure of the paper is as follows. We de-
fine our framework in Section 2 and present the
polynomial-time algorithms for its satisfiability and
enumeration problems in Section 3. In Section 4,
we compare our framework with nets and weak nets.
Section 5 discusses the related work, and Section 6
summarizes this work and discusses future work.
2 Super net
We first give a formal definition of underspecifica-
tion graph (UG). We then define super net as a sub-
set of UG. In the following definitions, we openly
borrow the terminology from Hole Semantics, Dom-
inance Constraints, and MRS, in order to avoid in-
venting new terms to name old concepts.
Definition 1 (Fragments). Consider L a set of la-
bels, H a set of holes, and S a set of directed solid
edges from labels to holes, such thatF = (LunionmultiH,S)
is a forest of ordered trees of depth at most 1, whose
root and only the root is a label node. Each of these
trees is called a fragment.
Following this definition, the number of trees in
F (including single-node trees) equals the number
of labels. For example, if we remove all the dotted
edges in Figure 1, we obtain a forest of 5 fragments.
Definition 2 (Underspecification Graph). Let F =
(L unionmulti H,S) be a forest of fragments and C be a set
of directed dotted edges from LunionmultiH to L, called the
set of constraints.2 U = (L unionmulti H,S unionmulti C) is called
an underspecification graph or UG.
Figures 1 and 3 each represent a UG.
Definition 3 (Plugging). (Bos, 1996)
Given a UG U = (L unionmultiH,S unionmulti C), a plugging P is
a total one-to-one function from H to L.
In Figure 1, if lA, lE , lP , lC , and lR represent
the nodes labeled by A(y), Every(x), Politician(y),
Child(x,y), and Run(x) respectively and hrA (h
b
A) and
hrE (h
b
E) represent the restriction (body) hole of A
and Every respectively, then P in (6) is a plugging.
6. P = {(hrA, lP ), (h
b
A, lC), (h
r
E , lA), (h
b
E , lR)}
We use TU,P to refer to the graph, formed from U
by removing all the constraints and plugging P (h)
into h for every hole h. For example if U is the UG
in Figure 1 and P is the plugging in (6), then TU,P
is the graph shown in Figure 2(a).
2We assume that there is no constraint edge between two
nodes of the same fragment.
144
Definition 4 (Permissibility/Solution). TU,P satis-
fies the constraint (u,v) in U , iff u dominates3 v in
TU,P .4 A plugging P is permissible, iff TU,P is a for-
est satisfying all the constraints in U . TU,P is called
a solution of U iff P is a permissible plugging. In
informal contexts, solutions are sometimes referred
to as readings.
It is easy to see that the plugging in (6) is a per-
missible plugging for the UG in Figure 1, and hence
Figure 2(a) is a solution of this UG. Similarly, Fig-
ures 4(a,b) represent two solutions of the UG in Fig-
ure 3. The solutions in Figures 2 and 4 are all tree
structures. This is because UGs in Figures 1 and 3
are weakly connected.5 Lemma 2 proves that this
holds in general, that is:
Proposition 1. Every solution of a weakly con-
nected UG is a tree.
Throughout the rest of this paper, unless other-
wise specified, UGs are assumed to be weakly con-
nected, hence solutions are tree structures.6
Lemma 2. (Bodirsky et al, 2004)
Given a UG U and a solution T of U , if the nodes u
and v in U are connected using an undirected path
p, there exists a node w on p such that w dominates
both u and v in T.
This Lemma is proved using induction on the
length of p. As mentioned before, satisfiability and
enumeration are two fundamental problems to be
solved for a UG. A straightforward approach is de-
picted in Figure 5. We pick a label l; remove it from
U ; recursively solve each of the resulting weakly
connected components (WCCs; cf. footnote 2) and
3u dominates v in the directed graph G, iff u reaches v in G
by a directed path.
4Here, we are referring to the nodes in TU,P by calling the
nodes u and v in U . This is a sound strategy, as every node in
U is mapped into a unique node in TU,P . The inverse is not true
though, as every node (except the root) in TU,P corresponds to
one hole and one label in U . Addressing TU,P ?s nodes in this
way is convenient, so we practice that throughout the paper.
5Given a directed graph G and the nodes u and v in G, u is
said to be weakly connected to v (and vice versa), iff u and v are
connected in the underlying undirected graph of G. A weakly
connected graph is a graph in which every two nodes are weakly
connected. Since weak connectedness is an equivalence rela-
tion, it partitions a directed graph into equivalent classes each
of which is called a weakly connected component or WCC.
6Since fragments are ordered trees, solutions are ordered
trees as well.
Figure 5: Recursively solving UGs.
plug the root of the returned trees into the corre-
sponding holes of l. A problem to be addressed
though is whether there exists any solution rooted
at l. This leads us to the following definition.
Definition 5 (Freeness). (Bodirsky et al, 2004)
A label l in U is called a free node, iff there exists
some solution of U rooted at l. The fragment rooted
at l is called a free fragment.
The following proposition states the necessary
conditions for a label (or fragment) to be free.7
Proposition 3. Let l in U be the root of a fragment
F with m holes. l is a free node of U , only if
P3a. l has no incoming (constraint) edge;
P3b. Every distinct hole of F lies in a distinct WCC
in U?l;
P3c. U?F consists of at least m WCCs.
Proof. The first condition is trivial. To see why
the second condition must hold, let T be a solution
rooted at l, and assume to the contrary that h1 and
h2 lie in the same WCC in U? l. From Lemma 2,
all the nodes in this WCC must be in the scope of
both h1 and h2. But this is not possible, because T
is a tree. The third condition is proved similarly. As-
sume to the contrary that U?F has m ? 1 WCCs.
From Lemma 2, all the nodes in a WCC must be in
the scope of a single hole of F . But there are m
holes and only m ? 1 WCCs. It means that one of
the holes in T is left unplugged. Contradiction!
The motivation behind defining super nets is to
find a subset of UG for which these conditions are
also sufficient. The following concept from Althaus
et al (2003) plays an important role.
7Necessary conditions of freeness in a UG are not exactly
the same as the ones in a weakly normal dominance graph, as
depicted in Bodirsky et al (2004), because the definition of so-
lution is different for the two frameworks (c.f. Section 4.3).
145
Figure 6: UG for Illustration of hypernormal path.
Definition 6 (Hypernormal Connectedness). Given
a UG U , a hypernormal path is an undirected path8
with no two consecutive constraint edges emanating
from the same node. Node u is hypernormally con-
nected to node v iff there is at least one hypernormal
path between the two. U is called hypernormally
connected iff every pair of nodes in U are hypernor-
mally connected.
For example, in Figure 2, p2 is a hypernormal
path, but p1 is not. In spite of that, the whole graph
is hypernormally connected.9 The following simple
notion will also come handy.
Definition 7 (Openness). (Thater, 2007)
A node u of a fragment F is called an open node iff
it has no outgoing constraint edge.
For example, l in Figure 5(a) is an open label
node. In Figure 2(b), h2 is an open hole. We are
finally ready to define super net.
Definition 8 (Super net). A UG U is called a super
net if for every fragment F rooted at l:
D8a. F has at most one open node.
D8b. If l1 and l2 are two dominance children of a
hole h of F, then l1 and l2 are hypernormally
connected in U?h.
D8c. ? Case 1: F has no open hole.
Every dominance child10 of l is hypernor-
mally connected to some hole of F in U?l.
? Case 2: F has an open hole.
All dominance children of l, not hypernor-
mally connected to a hole of F in U?l, are
hypernormally connected together.
8Throughout this paper, by path we always mean a simple
path, that is no node may be visited more than once on a path.
9Note that even though p1 is not a hypernormal path, there
is another hypernormal path connecting the same two nodes
10v is a dominance child of u in a UG U , if (u, v) is a con-
straint edge in U .
Figure 7: Illustration of super net conditions.
Definition 9 (Types of fragment). Following Defini-
tion 8, super net alows for three possible types of
fragment:
D9a. Open-root: Only the root is open (Figure 5a)
D9b. Open-hole: Only a hole is open (Figure 2b)
D9c. Closed: F There is no open node. (Figure 2a)
Definition 8 guarantees the following property.
Lemma 4. For a super net U and a fragment F of
U with m holes, which satisfies the conditions in
Proposition 3, U?F consists of exactly m WCCs,
each of which is a super net.
Proof sketch. The detailed proof of this Lemma is
long. Therefore, we sketch the proof here and leave
the details for a longer paper. First, we show that
U?F consists of exactly m WCCs. Following con-
ditions (D8b) and (D8c), no matter what structure F
has, U?F consists of at most m WCCs. On the other
hand, based on condition (P3c), U?F has at least m
WCCs. Therefore, U?F has exactly m WCCs. To
prove that each WCC in U?F is a super net, all we
need to prove is that if two nodes u and v, which do
not belong to F , are hypernormally connected in U ,
they are also hypernormally connected in U?F . This
is proved by showing that there is no hypernormal
path between u and v in U that visits some node of
F . Suppose that F is an open-hole fragment rooted
at l, as in Figure 2(b) (the two other cases are proved
similarly) and assume to the contrary that there is a
hypernormal path p between u and v that visits some
node of F . One of the following three cases holds.
i. p visits exactly one node of F .
ii. p visits (at least) two holes of F .
iii. p visits l and exactly one hole of F .
All the three cases results in a contradiction: (i)
proves that p is not hypernormal; (ii) proves that F
146
Figure 8: Proof of Proposition 5
is not a free fragment because it violates condition
(P3b); and (iii) proves that U is not a super net be-
cause F violates condition (D8c).
Proposition 5. If U is a satisfiable super net, the
necessary freeness conditions in Proposition 3 are
also sufficient.
Proof sketch. Let F rooted at l be a fragment satis-
fying the three conditions in Proposition 3. Among
all the solutions of U , we pick a solution T in which
the depth d of l is minimal. Using proof by con-
tradiction, we show that d = 0, which proves l is
the root of T . If d > 0, there is some node u that
outscopes l (Figure 8(a)). Lemma 2 and 4 guarantee
that at least one of the trees in Figures 8(b,c) is a so-
lution of U . So U has a solution in which, the depth
of l is smaller than d. Contradiction!
3 SAT and ENUM algorithms
Following Lemma 4 and Proposition 5, Table 1 gives
the algorithms for the satisfiability (SAT), and the
enumeration (ENUM) of super nets.
Theorem 6. ENUM and SAT are correct.
Proof sketch. Using Lemma 4 and induction on the
depth of the recursion, it is easy to see that if ENUM
or SAT returns a tree T , T is a solution of U . This
proves that ENUM and SAT are sound. An induc-
tive proof is used to prove the completeness as well.
Consider a solution T of depth n of U (Figure 5). It
can be shown that T1 and T2 must be the solutions
to U1 and U2. Therefore based on the induction as-
sumption they are generated by Solve(G), hence T
is also generated by Solve(G).
Let U = (L unionmulti H,S unionmulti C). The running time of
the algorithms depends on the depth of the recursion
which is equal to the number of fragments/labels,
|L|. At each depth it takes O(|U |) to find the
set of free fragments (Bodirsky et al, 2004) and
also to compute U ?F for some free fragment F .
Solve(U)
1. If U contains a single (label) node, return U .
2. Pick a free fragment F with m holes
rooted at l, otherwise fail.
// For SAT: pick arbitrarily.
// For ENUM: pick non-deterministically.
3. Let U1, U2, ? ? ? , Um be WCCs of U?F .
4. Let Ti = Solve(Ui) for i = 1 ? ? ?m.
5. Let hi be the hole of F connected to
Ui in U?l, for i = 1 ? ? ?m.
(If for some k, Uk is not connected to any hole
of F in U?l, let hk be the open hole of F .)
6. Build T by plugging the root of Ti into hi,
for i = 1 ? ? ?m.
7. Return T .
Table 1: ENUM and SAT algorithms
(|U | =def |V | + |E|, where |V | =def |L| + |H|,
and |E| =def |S| + |C|). Therefore SAT (and each
branch of ENUM) run(s) in O(|L|.|U |) step. There-
fore the worst-case time complexity of SAT and each
branch of ENUM is quadratic in the size of U .
4 Super net versus weak net
Although net is a subset of weak net, to better un-
derstand the three frameworks, we first define net.
4.1 Net
Net was first defined by Koller et al (2003), in or-
der to find a subset of Hole Semantics that can be
solved in polynomial-time. Nets do not contain any
label-to-label constraints. In fact, out of the three
possible structures that super net alows for a frag-
ment F (Definition 9), net only allows for the first
one, that is open-root.
Definition 10 (Net). (Thater, 2007)
Let U be a UG with no label-to-label constraints. U
is called a net iff for every fragment F :
D10a. F has no open hole.
D10b. If l1, l2 are two dominance children of a hole
h of F , then l1 and l2 are hypernormally
connected in U?h.
The root of F is open, therefore (D8a) subsumes
(D10a). Condition (D10b) is exactly the same as
(D8b). Therefore, super net is a superset of net.
Strictness of the superset relationship is trivial.
147
4.2 Weak net
Weak net was first introduced by Niehren and Thater
(2003), in order to find a tractable subset of MRS. In
order to model MRS, weak net alows for label-to-
label constraints, but to stay a tractable framework it
forces the following restrictions.
Definition 11 (Weak net). (Thater, 2007)
A UG U is a weak net iff for every fragment F :
D11a. F has exactly one open node.
D11b. If l1, l2 are two dominance children of a
node u of F , then l1 and l2 are hypernor-
mally connected in U?u.
Weak nets suffer from two limitations with re-
spect to super nets.
First, out of the three possible types of frag-
ment allowed by super net (Definition 9), weak net
only allows for the first two; open-root and open-
hole. In practice this becomes an issue only if
new constraints are to be added to a UG after syn-
tax/semantic interface. Since weak net requires one
node of every fragment to be open, a constraint can-
not be added if it violates this condition.11
Second, open-hole fragments in weak nets are
more restricted than open-hole fragments in super
nets. This is the Achilles? heel of weak nets (D11b).
To see why, consider the UG in Figure 3 for the sen-
tence Every politician, whom I know a child of, runs
which we presented in Section 1. If F is the frag-
ment for the quantifier Every and l is the root of F ,
the two dominance children of l are not (hypernor-
mally) connected in U ? l. Therefore, U is not a
weak net. All the non-net examples we have found
so far behave similarly. That is, there is a quanti-
fier with more than one outgoing dominance edge.
Once you remove the quantifier node, the dominance
children are no longer weakly (and hence hypernor-
mally) connected, violating condition (D11b). In
super net, however, we define case 2 of condition
(D8c) such that it does not force dominance chil-
dren of l to be (hypernormally) connected, allowing
for non-net structures such as the one in Figure 3.12
11As discussed in Section 5, by defining the notion of down-
ward connectedness, Koller and Thater (2007) address this issue
of weak nets, at the expense of cubic time complexity.
12For simplicity, throughout this paper we have used the term
non-net to refer to non-(weak net) UGs.
Proposition 7. Weak net is a strict subset of super
net.
Proof. Consider an arbitrary weak net U , and let F
be an arbitrary fragment of U rooted at l.
(i). F has exactly one open node, so it satisfies con-
dition (D8a).
(ii). For every two holes of F , condition (D11b)
guarantees that condition (D8b) holds.
(iii). ? Case 1) F has no open hole:
Based on condition (D11a) the root of F is
open, hence it has no dominance children.
(D8c) trivially holds in this case.
? Case 2) F has an open hole:
Based on condition (D11b) every two dom-
inance children of l are hypernormally con-
nected, so (D8c) holds in this case too.
Therefore, every fragment F satisfies all the con-
ditions in Definition 8, hence U is a super net. This
and the fact that Figure 3 is a super net but not a
weak net complete the proof.
4.3 Underspecification graph vs. weakly
normal dominance graph
Dominance graphs and their ancestor, dominance
constraints, are designed for solving constrained tree
structures in general. Therefore, some of the ter-
minology of dominance graph may seem counter-
intuitive when dealing with scope underspecifica-
tion. For example the notion of solution in that for-
malism is broader than what is known as solution
in scope underspecification formalisms. As defined
there (but translated into our terminology), a solu-
tion may contain unplugged holes, or holes plugged
with more than one label. This broad notion of so-
lution is computationally less expensive such that an
algorithm very similar to the one in Table 1 can be
used to solve every weakly normal dominance graph
(Bodirsky et al, 2004). Solution, as defined in this
paper (Definition 4), corresponds to the notion of
simple leaf-labeled solved forms (a.k.a. configu-
ration) in dominance graphs. Although solutions
of a weakly normal dominance graph can be found
in polynomial time, finding configurations is NP-
complete. Solvability of underspecification graphs
is equivalent to configurability of weakly normal
dominance graphs, and hence NP-complete.
148
5 Related work
We already compared our model with nets and weak
nets. Koller and Thater (2007) present another ex-
tension of weak nets, downward connected nets.
They show that if a dominance graph has a subgraph
which is a weak net, it can be solved in polynomial
time. This addresses the first limitation of weak nets,
discussed in Section 4.2, but it does not solve the
second one, because the graph in Figure 3 neither is
a weak net, nor has a weak-net subgraph.
Downward connected dominance graph, in its
general form, goes beyond weakly normal domi-
nance graph (and hence UG), incorporating label-to-
hole constraints. It remains for future work to inves-
tigate whether allowing for label-to-hole constraints
adds any value to the framework within the context
of scope underspecified semantics, or whether it is
possible to model the same effect using hole-to-label
and label-to-label constraints. In any case, the same
extension can be applied to super nets as well, defin-
ing downward connected super nets, a strict super
set of downward connected nets, solvable using sim-
ilar algorithms with the same time/space complexity.
Another tractable framework presented in the past
is our own framework, Canonical Form Under-
specified Representation (CF-UR) (Manshadi et al,
2009), motivated by Minimal Recursion Semantics.
CF-UR is defined to characterize the set of all MRS
structures generated by the MRS semantic composi-
tion process (Manshadi et al, 2008). CF-UR in its
general form is not tractable. Therefore, we define
a notion of coherence called heart-connectedness
and show that all heart-connected CF-UR struc-
tures can be solved efficiently. We also show that
heart-connected CF-UR covers the family of non-net
structures, so CF-UR is in fact the first framework to
address the non-net structures. In spite of that, CF-
UR is quite restricted and does not allow for adding
new constraints after semantic composition.
In recent work, Koller et al (2008) suggest us-
ing Regular Tree Grammars for scope underspeci-
fication, a probabilistic version of which could be
used to find the best reading. The framework goes
beyond the formalisms discussed in this paper and
is expressively complete in Ebert (2005)?s sense of
completeness, i.e. it is able to describe any subset
of the readings of a UR. However, this power comes
at the cost of exponential complexity. In practice,
RTG is built on top of weak nets, benefiting from the
compactness of this framework to remain tractable.
Being a super set of weak net, super net provides a
more powerful core for RTG.
Koller and Thater (2010) address the problem of
finding the weakest readings of a UR, which are
those entailed by some reading(s), but not entailing
any other reading of the UR. By only considering
the weakest readings, the space of solutions will be
dramatically reduced. Note that entailment using the
weakest readings is sound but not complete.
6 Summary and Future work
Weakly normal dominance graph brings many cur-
rent constraint-based formalisms under a uniform
framework, but its configurability is intractable in its
general form. In this paper, we present a tractable
subset of this framework. We prove that this sub-
set, called super net, is a strict superset of weak net,
a previously known tractable subset of the frame-
work, and that it covers a family of coherent natural
language sentences whose underspecified represen-
tation are known not to belong to weak nets.
As mentioned in Section 5, another extension of
weak nets, downward connected nets, has been pro-
posed by Koller and Thater (2007), which addresses
some of the limitations of weak nets, yet is unable
to solve the known family of non-net structures. A
thorough comparison between super nets and down-
ward connected nets remains for future work.
Another interesting property of super nets to be
explored is how they compare to heart-connected
graphs. Heart-connectedness has been introduced
as a mathematical criterion for verifying the coher-
ence of an underspecified representation within the
framework of underspecification graph (Manshadi et
al., 2009). Our early investigation shows that super
nets may contain all heart-connected UGs. If this
conjecture is true, super net would be broad enough
to cover every coherent natural language sentence
(under this notion of coherence). We leave a detailed
investigation of this conjecture for the future.
Acknowledgments
This work was support in part by NSF grant
1012205, and ONR grant N000141110417.
149
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic
semantic interpretation. In Proceedings of ACL ?92,
pages 32?39.
Ernst Althaus, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. J. Algorithms, 48(1):194?219, August.
Manuel Bodirsky, Denys Duchier, Joachim Niehren, and
Sebastian Miele. 2004. An efficient algorithm for
weakly normal dominance constraints. In In ACM-
SIAM Symposium on Discrete Algorithms. The ACM
Press.
J. Bos. 1996. Predicate logic unplugged. In In Proceed-
ings of the 10th Amsterdam Colloquium, pages 133?
143.
J. Bos. 2002. Underspecification and Resolution in Dis-
course Semantics. Saarbru?cken dissertations in com-
putational linguistics and language technology. DFKI.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of ACL
?01, pages 140?147.
Christian Ebert. 2005. Formal investigations of un-
derspecified representations. Technical report, King?s
College, London, UK.
M. Egg, A. Koller, and J. Niehren. 2001. The constraint
language for lambda structures. J. of Logic, Lang. and
Inf., 10(4):457?485, September.
Ruth Fuchss, Alexander Koller, Joachim Niehren, and
Stefan Thater. 2004. Minimal recursion semantics
as dominance constraints: Translation, evaluation, and
analysis. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04),
Main Volume, pages 247?254, Barcelona, Spain, July.
Jerry R. Hobbs and Stuart M. Shieber. 1987. An al-
gorithm for generating quantifier scopings. Comput.
Linguist., 13(1-2):47?63, January.
Alexander Koller and Stefan Thater. 2007. Solving unre-
stricted dominance graphs. In Proceedings of the 12th
Conference on Formal Grammar, Dublin.
Alexander Koller and Stefan Thater. 2010. Computing
weakest readings. In Proceedings of the 48th ACL,
Uppsala.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecification
formalisms: Hole semantics as dominance constraints.
In Proceedings of the 11th EACL, Budapest.
Alexander Koller, Michaela Regneri, and Stefan Thater.
2008. Regular tree grammars as a formalism for scope
underspecification. In Proceedings of ACL-08: HLT,
Columbus, Ohio.
Mehdi H. Manshadi, James F. Allen, and Mary Swift.
2008. Toward a universal underspecified semantic rep-
resentation. In Proceedings of the 13th Conference on
Formal Grammar, Hamburg, Germany, August.
Mehdi H. Manshadi, James F. Allen, and Mary Swift.
2009. An efficient enumeration algorithm for canon-
ical form underspecified semantic representations. In
Proceedings of the 14th Conference on Formal Gram-
mar, Bordeaux, France, July.
Joachim Niehren and Stefan Thater. 2003. Bridging the
gap between underspecification formalisms: minimal
recursion semantics as dominance constraints. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 367?374, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. Oepen, K. Toutanova, S. Shieber, C. Manning,
D. Flickinger, and T. Brants. 2002. The lingo red-
woods treebank motivation and preliminary applica-
tions. In Proceedings of COLING ?02, pages 1?5.
S. Thater. 2007. Minimal Recursion Semantics as Dom-
inance Constraints: Graph-theoretic Foundation and
Application to Grammar Engineering. Saarbru?cken
dissertations in computational linguistics and language
technology. Universita?t des Saarlandes.
W A Woods. 1986. Semantics and quantification in
natural language question answering. In Barbara J.
Grosz, Karen Sparck-Jones, and Bonnie Lynn Web-
ber, editors, Readings in natural language processing,
pages 205?248. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
150
Proceedings of the TextGraphs-6 Workshop, pages 51?59,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Unrestricted Quantifier Scope Disambiguation 
  Mehdi Manshadi  and  James Allen Department of Computer Science, University of Rochester Rochester, NY, 14627, USA {mehdih,james}@cs.rochester.edu 
      Abstract We present the first work on applying sta-tistical techniques to unrestricted Quanti-fier Scope Disambiguation (QSD), where there is no restriction on the type or the number of quantifiers in the sentence. We formulate unrestricted QSD as learning to build a Directed Acyclic Graph (DAG) and define evaluation metrics based on the properties of DAGs. Previous work on sta-tistical scope disambiguation is very lim-ited, only considering sentences with two explicitly quantified noun phrases (NPs). In addition, they only handle a restricted list of quantifiers. In our system, all NPs, ex-plicitly quantified or not (e.g. definites, bare singulars/plurals, etc.), are considered for possible scope interactions. We present early results on applying a simple model to a small corpus. The preliminary results are encouraging, and we hope will motivate further research in this area. 
1 Introduction There are at least two interpretations for the fol-lowing sentence: (1) Every line ends with a digit. In one reading, there is a unique digit (say 2) at the end of all lines. This is the case where the quanti-fier A outscopes (aka having wide-scope over) the quantifier Every. The other case is the one in which 
Every has wide-scope (or alternatively A has nar-row-scope), and represents the reading in which different lines could possibly end with distinct dig-its. This phenomenon is known as quantifier scope ambiguity.  Shortly after the first efforts to build natural lan-guage understanding systems, Quantifier Scope Disambiguation (QSD) was realized to be very difficult. Woods (1978) was one of the first to sug-gest a way to get around this problem. He pre-sented a framework for scope-underspecified semantic representation. He suggests representing the Logical Form (LF) of the above sentence as: (2) <Every x Line> <A y Digit> Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified. Since then scope underspecifica-tion has been the most popular way to deal with quantifier scope ambiguity in deep language understanding systems (e.g. Boxer (Bos 2004), TRAINS (Allen et al 2007), BLUE (Clark and Harrison 2008), and DELPH-IN1). Scope under-specification works in practice, only because many NLP applications (e.g. machine translation) could be achieved without quantifier scope disambigua-tion. QSD on the other hand, is critical for many other NLP tasks such as question answering sys-tems, dialogue systems and computing entailment. Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn                                                            1 http://www.delph-in.net/ 
51
1978, Moran 1988, Alshawi 1992). For example, it is widely known that in English, the quantifier each tends to have the widest scope. Also, the sub-ject of a sentence often outscopes the direct ob-ject. 2  In cases where these heuristics conflict, (manually) weighted preference rules are adopted to resolve the conflict (Hurum 1988, , Pafel 1997). In the last decade there has been some effort to apply statistical and machine learning (ML) tech-niques to QSD. All the previous efforts, however, suffer from the following two limitations (see sec-tion 2 for details): ? They only allow scoping two NPs per sentence. ? The NPs must be explicitly quantified (e.g. they ignore definites or bare singulars/plurals), and the quantifiers are restricted to a predefined list. In this paper, we present the first work on applying statistical techniques to unrestricted QSD, where we put no restriction on the type or the number of NPs to be scoped in a sentence. In fact, every two NPs, explicitly quantified or not (including defi-nites, indefinites, bare singulars/plurals, pronouns, etc.), are examined for possible scope interactions. Scoping only two quantifiers per sentence, the pre-vious work defines QSD as a single classification task (e.g. 0 where the first quantifier has wide-scope, and 1 otherwise). As a result standard met-rics for classification tasks are used for evaluation purposes. We formalize the unrestricted form of QSD as learning to build a DAG over the set of NP chunks in the sentence. We define accuracy, preci-sion and recall metrics based on the properties of DAGs for evaluation purposes. We report the application of our model to a small corpus. As seen later, the early results are promising and shall motivate further research on applying ML techniques to unrestricted QSD. In fact, they set a baseline for future work in this area.  The structure of this paper is as follows. Section (2) reviews the related work. In (3) we briefly de-scribe our corpus. We formalize the problem of quantifier scope disambiguation for multiple quan-tifiers in section (4) and define some evaluation metrics in (5). (6) presents our model including the kinds of features we have used. We present our experiments in (7) and give a discussion of the re-sults in (8). (9) summarizes the current work and gives some directions for the future work.                                                            2 Allen (1995) discusses some of these heuristics and gives an algorithm to incorporate those for scoping while parsing.  
2 Related work Earlier we mentioned that a standard approach to deal with quantifier scope ambiguity is scope un-derspecification. More recent underspecification formalisms such as Hole Semantics (Bos 1996), Minimal Recursion Semantics (Copestake et al 2001), and Dominance Constraints (Egg et al 2001), present constraint-based frameworks. Every constraint forces one term to be in the scope of another, hence filters out some of the possible readings. For example, one may add a constraint to an underspecified representation (UR) to force is-land constraints. Constraints can be added incre-mentally to the UR as the sentence processing goes deeper (e.g. at the discourse and/or pragmatic level). The main drawback with these formalisms is that they only allow for hard constraints; that is every scope-resolved representation must satisfy all the constraints in order to be a valid interpreta-tion of the sentence.  In practice, however, most constraints that can be drawn from discourse or pragmatic knowledge have a soft nature; that is, they describe a scope preference that is allowed to be violated, though at a cost.  Motivated by the above problem, Koller et al (2008) define an underspecified scope representa-tion based on regular tree grammars, which allows for both hard constraints and weighted soft con-straints. They present a PCFG-style algorithm that computes the reading, which satisfies all the hard constraints and has the maximum product of the weights. However, they assume that the weights are already given. Their algorithm, for example, can be used in traditional QSD approaches with weighted heuristics to systematically compute the best reading. The main question though is how to automatically learn those weights. One solution is using corpus-based methods to learn soft con-straints and the cost associated with their violation, in terms of features and their weights. To the best of our knowledge, there have been three major efforts on statistical scope disambigua-tion for English. Higgins and Sadock (2003), hence HS03, is the first work among these systems. They define a list of quantifiers that they consider for scope disambiguation. This list does not include definites, indefinites, and many other challenging scope phenomena. They extract all sentences from the Wall Street journal section of the Penn Tree-bank, containing exactly two quantifiers from this 
52
list. This forms a corpus of 890 sentences, each labeled with the relative scope of the two quantifi-ers, with the possibility of no scope interaction. The no scope interaction case happens to be the majority class in their corpus and includes more than 61% of the sentences, defining a baseline for their QSD system. They achieve the inter-annotator agreement of only 52% on this task. They treat QSD as a classification task with three possible classes (wide scope, narrow scope, and no scope interaction). Three forms of feature are incorporated into the classifier: part-of-speech (POS) tags, lexical features, and syntactic proper-ties. Several classification models including na?ve Bayes classifier, maximum entropy classifier, and single-layer perceptron are tested, among which the single-layer perceptron performs the best, with the accuracy of 77%. Galen and MacCartney (2004), hence GM04, build a corpus of 305 sentences from LSAT and GRE logic games, each containing exactly two quantifiers from an even more restricted list of quantifiers. They use an additional label for the case where the two scopings are equivalent (as in the case of two existentials). In around 70% of the sentences in their corpus, the first quantifier has wide scope, defining a majority class baseline of 70% for their QSD system.3 Three classifiers are tried: na?ve Bayes, logistics regression, and support vector machine (SVM), among which SVM per-forms the best and achieves the accuracy of 94%. In a recent work, Srinivasan and Yates (2009) study the usage of pragmatic knowledge in finding the preferred scoping of natural language sen-tences. The sentences are all extracted from 5-grams in Web1Tgram (from Google, Inc) and share the same syntactic structure: an active voice English sentence of the form (S (NP (V (NP | PP)))). For the task of finding the most preferred reading, they annotate 46 sentences, each contain-ing two quantifiers: Every and A, where the first quantifier is always A. Each sentence is annotated with one of the two labels (Every has wide scope or not). They use a totally different approach for finding the preferred reading. The n-grams in Web1Tgram are used to extract relations such as Live(Person, City), and to estimate the expected cardinality of the two classes, which form the ar-guments of the relation, that is Person and City.                                                            3 They do not report any inter-annotator agreement. 
They decide on the preferred scoping by compar-ing the size of the two classes, achieving the accu-racy of 74% on their test set. The main advantage of this work is that it is open domain. 3 Our corpus  The fact that HS03, in spite of ignoring challeng-ing scope phenomena and scoping only two quanti-fiers per sentence, achieve the IAA of 52% shows how hard scope disambiguation could be for hu-mans. It becomes enormously more challenging when there is no restriction on the type or the number of quantifiers in the sentence, especially when NPs without explicit quantifiers such as de-finites, indefinites, and bare singulars/plurals are taken into account. As a matter of fact, our own early effort to annotate part of the Penn Treebank with full scope information soon proved to be too ambitious. Instead, we picked a domain that covers most challenging phenomena in scope disambigua-tion, while keeping the scope disambiguation fairly intuitive. This made building the first corpus of English text with full quantifier scope information feasible. Our domain of choice is the description of tasks about editing plain text files, in other words, a natural language interface for text editors such as SED, AWK, or EMACS. Figure (1) gives some sentences from the corpus. The reason behind scoping in this domain being fairly intuitive is that given any of these sentences, a conscious knowl-edge of scoping is critical in order to be able to accomplish the explained task. Our corpus consists of 500 sentences manually extracted from the web. The sentences have been labeled with gold standard NP chunks, where each NP chunk has been indexed with a number 1 through n (n is the number of chunks in the sen-tence). The annotators are asked to use outscoping relations represented by ?>? to specify the relative scope of every pair 1?i,j?n, with an option to leave 
1. Print [1/ every line] of [2/ the file] that starts with [3/ a digit] followed by [4/ punctuation]. QSD: {2>1, 2>3, 1>3, 2>4, 1>4} 2. Delete [1/ the first character] of [2/ every word] and [3/ the first word] of  [4/ every line] in [5/ the file]. QSD: {5>4, 5>3, 4>3, 5>2, 5>1, 2>1} Figure 1. Two NP-chunked sentences with QSDs 
53
the pair unscoped. For example a relation (2>3) states that the second NP in the sentence outscopes (aka dominates) the third NP. Since outscoping relation is transitive, for the convenience of the annotation, the outscoping relations are allowed to be cascaded forming dominance chains. For exam-ple, the scoping for the sentence 2 in figure (1) can alternatively be represented as shown in (3). (3) (5>2>1 ; 5>4>3) As a result, every pair <i,j> (1?i<j?n) is implicitly labeled with one of the three labels: i. Wide scope: either explicitly given by the annotator as i>j or implied using the transi-tive property of outscoping4 ii. Narrow scope: either explicitly given by the annotator as j>i or implied using the transi-tive property of outscoping iii. No interaction: where neither wide scope nor narrow scope could be inferred from the given scoping.5 We achieved the IAA of 75% (based on Cohen?s kappa score) on this corpus, significantly better than the 52% IAA of HS03, especially considering the fact that we put no restriction on the type of the quantification. Our sentence-level IAA is around 66%. The details of the corpus, and the annotation scheme are beyond the scope of this paper and can be found in Manshadi et al (2011). 4 Formalization  Outscoping is an anti-symmetric transitive relation, so it defines an order over the chunks. Since we do not force every two chunks to be involved in an outscoping relation, QSD defines a partial order over the NP chunks. Formally, Definition 1: Given a sentence S with NP chunks 1..n, a relation P over {1..n} is called a QSD for S, if and only if P is a partial order. Definition 2: Given a sentence S with NP chunks 1..n, and the QSD P, we say (chunk) i outscopes (chunk) j if and only if  (i>j)  ? P.                                                            4 That is if i outscopes j and j outscopes k then i outscopes k. 5 The no interaction class includes two cases: no scope interac-tion and logical equivalence which means we follow the three-label scheme of HS03 as opposed to the four-label scheme of GM04. This is because when there is a logical equivalence, except for trivial cases, there are no clear criteria based on which one can decide whether there is a scope interaction or not. Furthermore, distinguishing these two cases does not make much difference in practice. 
Definition 3: Given a sentence S with NP chunks 1..n, and the QSD P, chunk i is said to be disjoint with chunk j if and only if   (i>j) ? P ? (j>i) ? P. 4.1 QSD and directed acyclic graphs Partial orders can be represented using Directed Acyclic Graphs (DAGs) in which dominance (aka reachability) determines the order. More precisely, every DAG G over n nodes v1..vn defines a partial order PG over the set {v1..vn} in which, vi precedes vj in PG if and only if vi dominates6 vj in G.  Definition 4: Given a sentence S with NP chunks 1..n, every DAG G over n nodes (labeled 1?n) defines a QSD PG for S, such that (i>j)  ? PG ? i dominates j in G For example figure (2a,b) represent the DAGs cor-responding to the QSD of sentence 2 in figure (1) and the QSD in (3) respectively. Following defini-tion 3 and 4, the no interaction relation defined in section (3) translates to corresponding nodes in the DAG being disjoint7. Therefore the three types of scope interaction defined in i, ii, and iii (section 3), translate to the following relations in a DAG. (4) Wide Scope (WS): i dominates j Narrow Scope (NS): j dominates i No Interaction (NI): i and j are disjoint. 5 Evaluation metrics  Intuitively the similarity of two QSDs, given for a sentence S, can be defined as the ratio of the chunk pairs that have the same label in both QSDs to the total number of pairs. For example, consider the                                                            6 Given a DAG G=(V, E), node u is said to immediately dominate node v if and only if (u,v)  ? E. ?dominates? is the reflexive transitive closure of ?immediately dominates?. 7 The nodes u and v of the DAG G are said to be disjoint if neither u dominates v nor v dominates u. 
                             (a)             (b) Figure 2. Scopings represented as DAGs  
54
two DAGs in figure (2). Although looking differ-ent, both DAGs define the same partial order (i.e. QSD). This is because the partial order represented by a DAG G corresponds to the transitive closure (TC) of G. 5.1 Transitive closure The transitive closure of G, shown as G+, is de-fined as follows: (5) G+= {(i,j) | i dominates j in G} For example, figure (2a) is the transitive closure of the DAG in figure (2b). Given this, the similarity metric mentioned above can be formally defined as the number of (unordered) pairs of node that match between G1+ and G2+ divided by the total number of (unordered) pairs. Definition 5: Similarity measure or ?. Given sentence S with n NP chunks and two scop-ings represented by DAGs G1 and G2, we define: M(G1, G2)= { {i,j} |    ((i,j) ? G1+ ? (i,j) ? G2+) ? ((j,i) ? G1+ ? (j,i) ? G2+) ?   ((i,j),(j,i) ? G1+ ? (i,j),(j,i) ? G2+) } ?(G1, G2) = 2|M(G1, G2)|/ [n(n-1)] Where |.| represents the cardinality of a set. ? is a value between 0 and 1 (inclusive) where 1 means that the QSDs are equivalent and 0 means that they do not agree on the label of any pair. ? is useful for measuring the similarity of two scope annotations when calculating IAA. It can also be used as an accuracy metric for evaluating an automatic scope disambiguation system where the similarity of a predicted QSD is calculated respect to a gold stan-dard QSD. In fact, if n =2, ? is equivalent to the metric that HS03 use to evaluate their system.  The similarity metric defined above has some disadvantages. For example, HS03 report that more than 61% of the scope relations in their corpus are of type no interaction. Using this metric, a model that leaves everything unscoped has more than 61% percent accuracy on their corpus! In fact, the output of a QSD system on pairs with no interac-tion is not practically important. 8 What is more 
                                                           8 In practice the target language is often first order logic or a variant of that. When a pair is labeled NI in gold standard data, if there exist valid interpretations (satisfying hard constraints) in which either of the two quantifiers can be in the scope of 
important is to recover the pairs with scope interac-tion correctly. The standard way to address this is to define precision/recall metrics. Definition 6: Precision and Recall (TC version) Given the gold standard DAG Gg and the predicted DAG Gp, we define the precision (P) and the recall (R) as follows: TP = | { (i,j) |  (i,j) ? Gp+ ? (i,j) ? Gg+} | N = | { (i,j) |  (i,j) ? Gp+} | M = | { (i,j) | (i,j) ? Gg+} | P = TP / N R = TP / M 5.2 Transitive reduction The TC-based metrics implicitly count some matching pairs more than once. For example, if in both QSDs we have 1>2 and 2>3, then 1>3 is im-plied, so counting it as another match is redundant and favors toward higher accuracies. Naturally, there are so many redundancies in TC. To address this issue, we define another set of metrics based on the concept of transitive reduction (TR). Given a directed graph G, the transitive reduction of G, represented as G -, is intuitively a graph with the same reachability (i.e. dominance) relation but with no redundant edges. More formally, the tran-sitive reduction of G is a graph G - such that  ? (G -)+ = G+  ? ? G?,    (G?)+ = G+  ?   |G -| ? |G? | For example, figure (2b) represents the transitive reduction of the DAG in figure (2a). Fortunately if a directed graph is acyclic, its transitive reduction is unique (Aho et al, 1972). Therefore, defining TR-based precision/recall metrics is valid. Definition 7: Precision and Recall (TR version) Simply replace every ?+? in definition 6 with a ?-?. 6 The model We extend HS03?s approach for scoping two NPs per sentence to the general case of n NPs. Every pair of chunks <i,j> (where  1?i<j?n) is treated as an independent sample to be classified as one of the three classes defined in (3), that is WS, NS, or NI. Therefore a sentence with n NP chunks con-sists of C(n, 2)=n(n-1)/2 samples. The average                                                                                              the other, then the ordering of this pair does not matter; that is switching the order of such pairs result in equivalent formulas. 
55
number of NPs per sentence in the corpus is 3.7, so the corpus provides 1850 samples. Since the scop-ing of each pair is predicted independent of the other pairs in the sentence, it may result in an ill-formed scoping, i.e. a scoping with cycles. As ex-plained later, this case did not happen in our cor-pus. A MultiClass SVM (Crammer et al 2001), referred to as SVM-MC in the rest of the paper, is used as the classifier. We provide more supervision by annotating data with the following labels.  I. Determiner features For every NP chunk, we tag pre-determiner (/PD), determiner (/D), possessive determiner (/POS), and number (/CD) (if they exist) as part of the deter-miner (see figure 3). Given the pair <i,j>, for ei-ther of the chunks i and j, and every tag mentioned above, we use a binary feature, which shows whether this tag exists in that chunk or not. For tags that do exist (except /CD) the lexical word is also used as a feature. II. Semantic head features  We tag the semantic head of the NP and use its lexical word as feature. Also the plurality of the NP (/S tag for plurals) is used as a binary feature.  III. 3. Dependency features The above two sets of feature are about the indi-vidual properties of the chunks. But this last cate-gory represents how each NP contributes to the semantics of the whole sentence. We borrow from Manshadi et al (2009) the concept of Dependency Graph (DG), which encodes this information in a compact way. DG represents the argument struc-ture of the predicates that form the logical form of a sentence. The DG of a sentence with n NP chunks contains n+1 nodes labeled 0..n. Node i (i>0) represents the predicate or the conjunction of the predicates that describes the NP chunk i, and node 0 represents the main predicate (or conjunc-tion of predicates) of the sentence. An edge from i 
to j shows that chunk i is an argument of a predi-cate represented by node j.  For example, in sentence (1) of figure (3), chunk 1 is clearly the argument of the verb Print (the main predicate of the sentence), therefore there is an edge from 1 to 0 in the DG of this sen-tence as shown in figure (4a). Also, chunks 2..4 are part of the description of chunk 1, so they are the arguments of the predicate(s) describing chunk 1. This means that there must be edges from nodes 2..4 to node 1 in the DG. Similarly for sentence 2 in figure (3), chunk 5 is part of the description (hence an argument of the predicates) of chunks 2 and 4; chunks 2 and 4 are part of the description of 1 and 3 respectively; and 1 and 3 are both argu-ments of the verb Delete, the main predicate of the sentence, resulting in the DG given in figure (4b). The following features are extracted from the DG for every sample <i,j>(1?i<j?n): - Does i (or j) immediately dominate 0? - Does i (or j) immediately dominate j (or i)? - Does i (or j) dominate j (or i)? - Are i,j siblings ? - Do i,j share the same child? Note that DG has a close relationship with the de-pendency tree of a sentence; for example, it shows the dependency relation(s) between a noun or verb and their modifier(s). Therefore it actually encodes some syntactic properties of a sentence. 7 Experiments 100 sentences from the corpus were picked at ran-dom as the development set, in order to study the relevant features and their contribution to QSD. The rest of the corpus (400 sentences) was then used to do a 5 fold cross validation. We used SVMMulticlass from SVM-light toolkit (Joachims 1999) as the classifier.  
                (a)                (b) Figure 4. Dependency Graphs for figure (3) sentences  
1. Print [1/ every/D line/H] of [2/ the/D file/H] that starts with [3/ two/CD digits/H/S] followed by [4/ punctuation/H]. 2. Delete [1/ the/D first character/H] of [2/ every/D word/H] and [3/ the/D first word/H] of  [4/ every/D line/H] in [5/ the/D file/H]. Figure 3. Labeling determiners and head nouns 
56
Before giving the results, we define a baseline. HS03 use the most frequent label as the baseline and the similarity metric given in definition (5) to evaluate the performance. Since more than 61% of the labels in their corpus is NI, the baseline system (that leaves every sentence unscoped) has the accu-racy above 61%. In our corpus, the majority class is WS containing around 35% of the samples. NS and NI each contain 34% and 31% of the samples respectively. This means that there is a slight ten-dency for having scope preference in chronological order. Therefore, the linear order of the chunks (i.e. from left to right) defines a reasonable baseline.  The results of our experiments are shown in table 1. The table lists the parameters P, R, and F-score9 for our SVM-MC model vs. the baseline system. For each system, two sets of metrics have been reported: TC-based and TR-based.  Table 2 lists the sentence-level accuracy of the system. We computed two metrics for sentence-level accuracy: Acc and Acc-EZ. In calculating Acc, a sentence is considered correct if all the labels (including NI) exactly match the gold standard la-bels. However, this is an unnecessarily tough met-ric. As mentioned before (footnote 8), in practice the output of the system for the samples labeled NI is not important; all we care is that all outscoping (i.e. WS/NS) relations are recovered correctly. In other words, in practice, the system?s recall is the most important parameter. Regarding this fact, we define Acc-EZ as the percentage of sentences with 100% recall (ignoring the value of precision). In order to compare our system with that of HS03, we applied our model unmodified to their corpus using the same set-up, a 10-fold cross vali-dation. However, since their corpus is not anno-tated with DG, we translated our dependency features to the properties of the Penn Treebank?s phrase structure trees. Table (3) lists the accuracy                                                            9 F-score is defined as F=2PR/(P+R). 
of their best model, their baseline, and our SVM-MC model. As seen in this table, their model out-performs ours. This, however, is not surprising. First, although we trained our model on their cor-pus, the feature engineering of our model was done based on our own development set. Second, since our corpus is not annotated with phrase structure trees, our model does not use any of their features that can only be extracted from phrase structure trees. It remains for future work to incorporate the features extracted from phrase structure trees (which is not already encoded in DG) and evaluate the performance of the model on either corpus. 8 Discussion As seen in tables 1 and 2, for a first effort at full quantifier scope disambiguation, the results are promising. The constraint-based F-score of 78% is already higher than the inter-annotator agreement, which is 75% (measured using the TC-based simi-larity metric; see definition 5). Furthermore, our system outperforms the baseline, by more than 40% (judging by the constraint-based F-score). This is significant, comparing to the work of HS03, which outperforms the baseline by 16%.  We mentioned before that in our corpus in aver-age there are around 4 NPs per sentence resulting in 6 samples per sentence. Therefore the chance of predicting all the labels correctly is very slim. However, the baseline (i.e. the left to right order) does a good job and predicts the correct QSD for 27% of the sentences. At the sentence level, our model does not reach the IAA, but the performance (62%) is not much lower than the IAA (66%). A question may arise that since the model treats 
 ? Baseline  61.1% HS04 77.0% Our Model 73.3% Table 3. Comparison with HS04 system on their dataset 
 P R F Baseline (TC) 31.8% 49.7% 38.8% Baseline (TR) 27.4% 33.9% 30.3% SVM-MC (TC) 73.0% 84.7% 78.4% SVM-MC (TR) 70.6% 76.2% 73.2% 
Table 1. Constraint-level results 
 Acc Acc-EZ Baseline 27.0% 43.8% SVM-MC 62.3% 78.0% Table 2. Sentence level accuracy 
57
the pairs of NP independently, what guarantees that the scopings are valid; that is the predicted directed graphs are in fact DAGs. For example, for a sentence with 3 NP chunks, the classifier may predict that 1>2, 2>3, and 3>1, which results in a loop! As a matter of fact, there is nothing in the model that guarantees the validness of the pre-dicted scopings. In spite of that, surprisingly all generated graphs in our tests were in fact DAGs! In order to explain this fact, we run two experi-ments. In the first experiment, corresponding to every sentence S in the corpus with n chunks, we generated a random directed graph over n nodes. Only 10% of the graphs had cycles. It means that more than 90% of randomly generated directed graphs with n nodes (where the distribution of n is its distribution in our corpus) are acyclic. In the second experiment, for every sentence with n chunks, we created the samples <i,j> by randomly selecting values for all the features. We then tested the classifier in our original set-up, a 5-fold cross validation. In this case, only 4% of the sentences were assigned inconsistent labeling. This means that chances of having a loop in the scoping are small even when the classifier is trained on sam-ples with randomly valued features, therefore it is not surprising that a classifier trained on the actual data learns some useful structures which make the chance of assigning inconsistent labels very slim.  In general, if the classifier predicts such incon-sistent scopings, the PCFG-style algorithm of Koller et al (2008) comes handy in order to find a valid scoping with the highest weight. 9 Summary and future work We presented the first work on unrestricted statis-tical scope disambiguation in which all NP chunks in a sentence are considered for possible scope in-teractions. We defined the task of full scope dis-ambiguation as assigning a directed acyclic graph over n nodes to a sentence with n NP chunks. We then defined some metrics for evaluation purposes based on the two well-known concepts for DAGs: transitive closure and transitive reduction.  We use a simple model for automatic QSD. Our model treats QSD as a ternary classification task on every pair of NP chunks. A multiclass SVM together with some POS, lexical and dependency features is used to do the classification. We apply this model to a corpus of English text in the do-
main of editing plain text files, which has been annotated with full scope information. The pre-liminary results reach the F-score of 73% (based on transitive reduction metrics) at the constraint level and the accuracy of 62% at the sentence level. The system outperforms the baseline by a high margin (43% at the constraint level and 35% at the sentence level).  Our ternary SVM-based classification model is a preliminary model, used for justification of our theoretical framework. Many improvements are possible, for example, directly predicting the whole DAG as a structured output. Also, the features that we use are rather basic. There are other linguisti-cally motivated features that can be incorporated, e.g. some properties of the phrase structure trees, not already encoded in dependency graphs. Another problem with the current system is that the extra supervision has been provided by manu-ally labeling the data (e.g. with dependency graphs). This could be done automatically by ap-plying off the shelf parsers or POS taggers, possi-bly by adapting them to our domain.  Although we consider all NPs for scope resolu-tion, scopal operators such as negation, mo-dal/logical operators have been ignored in this work. We also do not distinguish distributive vs. collective reading of plurals in the current sys-tem.10 Incorporating scopal operators and handling distributivity vs. collectivity would be the next step in expanding this work. Finally, since hand annotation of scope infor-mation is very challenging, applying semi-supervised or even unsupervised techniques to QSD is very demanding. In fact, leveraging unla-beled data to do QSD seems quite promising. This is because domain dependent knowledge plays a critical role in scope disambiguation and this knowledge can be learned from unlabeled data us-ing unsupervised methods. Acknowledgement  We would like to thank Derrick Higgins for pro-viding us with the HS03?s corpus. This work was supported in part by grants from the National Sci-ence Foundation (IIS-1012205) and The Office of Naval Research (N000141110417).                                                             10 The corpus has already been annotated with all this informa-tion, but our QSD model is not designed for such a compre-hensive scope disambiguation. 
58
References  Aho, A., Garey, M., Ullman, J. (1972). The Transitive Reduction of a Directed Graph. SIAM Journal on Computing 1 (2): 131?137. Allen, J. (1995) Natural Langue Understanding, Ben-jamin-Cummings Publishing Co., Inc.  Allen, J., Dzikovska, M., Manshadi, M., Swift, M. (2007) Deep linguistic processing for spoken dia-logue systems. Proceedings of the ACL-07 Workshop on Deep Linguistic Processing, pp. 49-56. Alshawi, H.  (ed.)  (1992) The core language Engine. Cambridge, MA, MIT Press.  Bos, J., S. Clark, M. Steedman, J. R. Curran, and J. Hockenmaier (2004). Wide-coverage semantic repre-sentations from a CCG parser. In Proceedings of COLING 2004, Geneva, Switzerland, pp. 1240? 1246. Bos, J. (1996) Predicate logic unplugged. In Proc. 10th Amsterdam Colloquium, pages 133?143. Clark P., Harrison, P. (2008) Boeing's NLP system and the challenges of semantic representation, Semantics in Text Processing. STEP 2008. Copestake, A., Lascarides, A. and Flickinger, D. (2001) An Algebra for Semantic Construction in Constraint-Based Grammars. ACL-01. Toulouse, France. Crammer, K., Y. Singer, N. Cristianini ,  J. Shawe-taylor,  B. Williamson (2001). On the Algorithmic Implementation of Multi-class SVMs, Journal of Ma-chine Learning Research. Egg M., Koller A., and Niehren J. (2001) The constraint language for lambda structures. Journal of Logic, Language, and Information, 10:457?485. Galen, A. and MacCartney, B. (2004). Statistical resolu-tion of scope ambiguity in Natural language. http://nlp.stanford.edu/nlkr/scoper.pdf. Higgins, D. and Sadock, J. (2003). A machine learning approach to modeling scope preferences. Computa-tional Linguistics, 29(1).  Hurum, S. O. (1988) Handling scope ambiguities in English. In Proceeding of the second conference on Applied Natural Language Processing (ANLC '88). Koller, A., Michaela, R., Thater, S. (2008) Regular Tree Grammars as a Formalism for Scope Underspecifi-cation. ACL-08, Columbus, USA. Joachims, T. (1999) Making Large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning, B. Sch?lkopf and C. Burges and A. Smola (ed.), MIT Press.  
Manshadi, M., Allen J., and Swift, M. (2009) An Effi-cient Enumeration Algorithm for Canonical Form Underspecified Semantic Representations. Proceed-ings of the 14th Conference on Formal Grammar (FG 2009), Bordeaux, France July 25-26. Moran, D. B. (1988). Quantifier scoping in the SRI core language engine. In Proceedings of the 26th Annual Meeting of the Association for Computational Lin-guistics. Pafel, J. (1997). Skopus und logische Struktur. Studien zum Quantorenskopus im Deutschen. PHD thesis, University of T?bingen. Srinivasan, P., and Yates, A. (2009). Quantifier scope disambiguation using extracted pragmatic knowl-edge: Preliminary results. In Proceedings of the Con-ference on Empirical Methods in Natural Language Processing (EMNLP). VanLehn, K. (1988) Determining the scope of English quantifiers, TR AI-TR-483, AI Lab, MIT. Woods, W. A.  (1978) Semantics and quantification in natural language question answering, Advances in. Computers, vol. 17, pp 1-87.  
59
