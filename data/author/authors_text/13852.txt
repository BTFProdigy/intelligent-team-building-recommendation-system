Automatic Detection of Nonreferential It in Spoken Multi-Party Dialog
Christoph Mu?ller
EML Research gGmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
christoph.mueller@eml-research.de
Abstract
We present an implemented machine
learning system for the automatic detec-
tion of nonreferential it in spoken dialog.
The system builds on shallow features ex-
tracted from dialog transcripts. Our exper-
iments indicate a level of performance that
makes the system usable as a preprocess-
ing filter for a coreference resolution sys-
tem. We also report results of an annota-
tion study dealing with the classification of
it by naive subjects.
1 Introduction
This paper describes an implemented system for
the detection of nonreferential it in spoken multi-
party dialog. The system has been developed on
the basis of meeting transcriptions from the ICSI
Meeting Corpus (Janin et al, 2003), and it is in-
tended as a preprocessing component for a coref-
erence resolution system in the DIANA-Summ di-
alog summarization project. Consider the follow-
ing utterance:
MN059: Yeah. Yeah. Yeah. I?m sure I could learn a lot
about um, yeah, just how to - how to come up with
these structures, cuz it?s - it?s very easy to whip up
something quickly, but it maybe then makes sense to -
to me, but not to anybody else, and - and if we want to
share and integrate things, they must - well, they must
be well designed really. (Bed017)
In this example, only one of the three instances of
it is a referential pronoun: The first it appears in
the reparandum part of a speech repair (Heeman
& Allen, 1999). It is replaced by a subsequent al-
teration and is thus not part of the final utterance.
The second it is the subject of an extraposition
construction and serves as the placeholder for the
postposed infinitive phrase to whip up something
quickly. Only the third it is a referential pronoun
which anaphorically refers to something.
The task of the system described in the follow-
ing is to identify and filter out nonreferential in-
stances of it, like the first and second one in the
example. By preventing these instances from trig-
gering the search for an antecedent, the precision
of a coreference resolution system is improved.
Up to the present, coreference resolution has
mostly been done on written text. In this domain,
the detection of nonreferential it has by now be-
come a standard preprocessing step (e.g. Ng &
Cardie (2002)). In the few works that exist on
coreference resolution in spoken language, on the
other hand, the problem could be ignored, because
almost none of these aimed at developing a sys-
tem that could handle unrestricted input. Eck-
ert & Strube (2000) focus on an unimplemented
algorithm for determining the type of antecedent
(mostly NP vs. non-NP), given an anaphorical
pronoun or demonstrative. The system of Byron
(2002) is implemented, but deals mainly with how
referents for already identified discourse-deictic
anaphors can be created. Finally, Strube & Mu?ller
(2003) describe an implemented system for re-
solving 3rd person pronouns in spoken dialog, but
they also exclude nonreferential it from consider-
ation. In contrast, the present work is part of a
project to develop a coreference resolution system
that, in its final implementation, can handle unre-
stricted multi-party dialog. In such a system, no
a priori knowledge is available about whether an
instance of it is referential or not.
The remainder of this paper is structured as fol-
lows: Section 2 describes the current state of the
art for the detection of nonreferential it in writ-
ten text. Section 3 describes our corpus of tran-
scribed spoken dialog. It also reports on the anno-
tation that we performed in order to collect train-
ing and test data for our machine learning experi-
ments. The annotation also offered interesting in-
sights into how reliably humans can identify non-
referential it in spoken language, a question that,
49
to our knowledge, has not been adressed before.
Section 4 describes the setup and results of our
machine learning experiments, Section 5 contains
conclusion and future work.
2 Detecting Nonreferential It In Text
Nonreferential it is a rather frequent phenomenon
in written text, though it still only constitutes a mi-
nority of all instances of it. Evans (2001) reports
that his corpus of approx. 370.000 words from the
SUSANNE corpus and the BNC contains 3.171
examples of it, approx. 29% of which are nonref-
erential. Dimitrov et al (2002) work on the ACE
corpus and give the following figures: the news-
paper part of the corpus (ca. 61.000 words) con-
tains 381 instances of it, with 20.7% being nonref-
erential, and the news wire part (ca. 66.000 words)
contains 425 instances of it, 16.5% of which are
nonreferential. Boyd et al (2005) use a 350.000
word corpus from a variety of genres. They count
2.337 instances of it, 646 of which (28%) are non-
referential. Finally, Clemente et al (2004) report
that in the GENIA corpus of medical abstracts the
percentage of nonreferential it is as high as 44%
of all instances of it. This may be due to the fact
that abstracts tend to contain more stereotypical
formulations.
It is worth noting here that in all of the above
studies the referential-nonreferential decision im-
plicitly seems to have been made by the author(s).
To our knowledge, no study provides figures re-
garding the reliability of this classification.
Paice & Husk (1987) is the first corpus-based
study on the detection of nonreferential it in writ-
ten text. From examples drawn from a part of
the LOB corpus (technical section), Paice & Husk
(1987) create rather complex pattern-based rules
(like SUBJECT VERB it STATUS to TASK),
and apply them to an unseen part of the corpus.
They report a final success rate of 92.2% on the
test corpus. Nowadays, most current coreference
resolution systems for written text include some
means for the detection of nonreferential it. How-
ever, evaluation figures for this task are not always
given. As the detection of nonreferential it is sup-
posed to be a filtering condition (as opposed to
a selection condition), high precision is normally
considered to be more important than high recall.
A false negative, i.e. a nonreferential it that is not
detected, can still be filtered out later when reso-
lution fails, while a false positive, i.e. a referen-
tial it that is wrongly removed, is simply lost and
will necessarily harm overall recall. Another point
worth mentioning is that mere classification accu-
racy (percent correct) is not an appropriate eval-
uation measure for the detection of nonreferential
it. Accuracy will always be biased in favor of pre-
dicting the majority class referential which, as the
above figures show, can amount to over 80%.
The majority of works on detecting nonreferen-
tial it in written text uses some variant of the partly
syntactic and partly lexical tests described by Lap-
pin & Leass (1994), the first work about computa-
tional pronoun resolution to address the potential
benefit of detecting nonreferential it. Lappin &
Leass (1994) mainly supply a short list of modal
adjectives and cognitive verbs, as well as seven
syntactic patterns like It is Cogv-ed that S. Like
many works that treat the detection of nonrefer-
ential it only as one of several steps of the coref-
erence resolution process, Lappin & Leass (1994)
do not give any figures about the performance of
this filtering method.
Dimitrov et al (2002) modify and extend the
approach of Lappin & Leass (1994) in several re-
spects. They extend the list of modal adjectives
to 86 (original: 15), and that of cognitive verbs to
22 (original: seven). They also increase the cov-
erage of the syntactic patterns, mainly by allowing
for optional adverbs at certain positions. Dimitrov
et al (2002) report performance figures for each
of their syntactic patterns individually. The first
thing to note is that 41.3% of the instances of non-
referential it in their corpus do not comply with
any of the patterns they use, so even if each pat-
tern worked perfectly, the maximum recall to be
reached with this method would be 58.7%. The ac-
tual recall is 37.7%. Dimitrov et al (2002) do not
give any precision figures. One interesting detail
is that the pattern involving the passive cognitive
verb construction accounts for only three instances
in the entire corpus, of which only one is found.
Evans (2001) employs memory-based machine
learning. He represents instances of it as vectors of
35 features. These features encode, among other
things, information about the parts of speech and
lemmata of words in the context of it (obtained au-
tomatically). Other features encode the presence
or absence of, resp. the distance to, certain ele-
ment sequences indicative of pleonastic it, such as
complementizers or present participles. Some fea-
tures explicitly reference structural properties of
50
the text, like position of the it in its sentence, and
position of the sentence in its paragraph. Sentence
boundaries are also used to limit the search space
for certain distance features. Evans (2001) reports
a precision of 73.38% and a recall of 69.25%.
Clemente et al (2004) work on the GENIA cor-
pus of medical abstracts. They assume perfect pre-
processing by using the manually assigned POS
tags from the corpus. The features are very similar
to those used by Evans (2001). Using an SVMma-
chine learning approach, Clemente et al (2004)
obtain an accuracy of 95.5% (majority base line:
approx. 56%). They do not report any precision or
recall figures. Clemente et al (2004) also perform
an analysis of the relative importance of features in
various settings. It turns out that features pertain-
ing to the distance or number of complementizers
following the it are consistently among the most
important.
Finally, Boyd et al (2005) also use a machine
learning approach. They use 25 features, most of
which represent syntactic patterns like it VERB
ADJ that. These features are numeric, having as
their value the distance from a given instance of
it to the end of the match, if any. Pattern match-
ing is limited to sentences, sentence breaks being
identified by punctuation. Other features encode
the (simplified) POS tags that surround a given in-
stance of it. Like in the system of Clemente et al
(2004), all POS tag information is obtained from
the corpus, so no (error-prone) automatic tagging
is performed. Boyd et al (2005) obtain a precision
of 82% and a recall of 71% using a memory-based
machine learning approach, and a similar preci-
sion but much lower recall (42%) using a decision
tree classifier.
In summary, the best approaches for detecting
nonreferential it in written text already work rea-
sonably well, yielding an F-measure of over 70%
(Evans, 2001; Boyd et al, 2005). This can at least
partly be explained by the fact that many instances
are drawn from texts coming from rather stereo-
typical domains, like e.g. news wire text or scien-
tific abstracts. Also, some make the rather unreal-
istic assumption of perfect POS information, and
even those who do not make this assumption take
advantage of the fact that automatic POS tagging
is generally very good for these types of text. This
is especially true in the case of complementizers
(like that) which have been shown to be highly in-
dicative of extraposition constructions. Structural
properties of the context of it, including sentence
boundaries and position within sentence or para-
graph, are also used frequently, either as numeri-
cal features in their own right, or as means to limit
the search space for pattern matching.
3 Nonreferential It in Spoken Dialog
Spontaneous speech differs considerably from
written text in at least two respects that are rele-
vant for the task described in this paper: it is less
structured and more noisy than written text, and it
contains significantly more instances of it, includ-
ing some types of nonreferential it not found in
written text.
3.1 The ICSI Meeting Corpus
The ICSI Meeting Corpus (Janin et al, 2003) is
a collection of 75 manually transcribed group dis-
cussions of about one hour each, involving 3 to 13
speakers. It features a semiautomatically gener-
ated segmentation in which the corpus developers
tried to track the flow of the dialog by inserting
segment starts approximately whenever a person
started talking. Each of the resulting segments is
associated with a single speaker and contains start
and end time information. The transcription con-
tains manually added punctuation, and it also ex-
plicitly records disfluencies and speech repairs by
marking both interruption points and word frag-
ments (Heeman & Allen, 1999). Consider the fol-
lowing example:
ME010: Yeah. Yeah. No, no. There was a whole co- There
was a little contract signed. It was - Yeah. (Bed017)
Note, however, that the extent of the reparandum
(i.e. the words that are replaced by following
words) is not part of the transcription.
3.2 Annotation of It
We performed an annotation with two external an-
notators. We chose annotators outside the project
in order to exclude the possibility that our own pre-
conceived ideas influence the classification. The
purpose of the annotation was twofold: Primar-
ily, we wanted to collect training and test data for
our machine learning experiments. At the same
time, however, we wanted to investigate how re-
liably this kind of annotation could be done. The
annotators were asked to label instances of it in
five ICSI Meeting Corpus dialogs1 as belonging
1Bed017, Bmr001, Bns003, Bro004, and Bro005
51
to one of the classes normal, vague, discarded,
extrapos it, prop-it, or other.2 The idea behind
using this five-fold classification (as opposed to a
binary one) was that we wanted to be able to in-
vestigate the inter-annotator reliability for each of
the sub-types individually (cf. below). The first
two classes are sub-types of referential it: Normal
applies to the normal, anaphoric use of it. Vague
it (Eckert & Strube, 2000) is a form of it which
is frequent in spoken language, but rare in written
text. It covers instances of it which are indeed ref-
erential, but whose referent is not an identifiable
linguistic string in the context of the pronoun. A
frequent (but not the only) type of vague it is the
one referring to the current discourse topic, like in
the following example:
ME011: [...] [M]y vision of it is you know each of us
will have our little P D A in front of us Pause and so
the acoustics - uh you might want to try to match the
acoustics. (Bmr001)
Note that we treat vague it as referential here even
though, in the context of a coreference resolution
preprocessing filter, it would make sense to treat
it as nonreferential since it does not have an an-
tecedent that it can be linked to. However, we fol-
low Evans (2001) in assuming that the information
that is required to classify an instance of it as a
mention of the discourse topic is far beyond the lo-
cal information that can reasonably be represented
for an instance of it.
The classes discarded, extrapos it and prop-
it are sub-types of nonreferential it. The first two
types have already been shown in the example in
Section 1. The class prop-it3 was included to
cover cases like the following:
FE004: So it seems like a lot of - some of the issues are the
same. [...] (Bed017)
The annotators received instructions including de-
scriptions and examples for all categories, and a
decision tree diagram. The diagram told them e.g.
to use wh-question formation as a test to distin-
guish extrapos it and prop-it on the one hand
from normal and vague on the other. The crite-
rion for distinguishing between the latter two phe-
nomena was to use normal if an antecedent could
be identified, and vague otherwise. For normal
2The actual tag set was larger, including categories like
idiom which, however, the annotators turned out to use ex-
tremely rarely only. These values are therefore conflated in
the category other in the following.
3Quirk et al (1991)
pronouns, the annotators were also asked to indi-
cate the antecedent. The annotators were also told
to tag as extrapos it only those cases in which
an extraposed element (to-infinitive, ing-form or
that-clause with or without complementizer) was
available, and to use prop-it otherwise. The an-
notators individually performed the annotation of
the five dialogs. The results of this initial anno-
tation were analysed and problems and ambigui-
ties in the annotation scheme were identified and
corrected. The annotators then individually per-
formed the actual annotation again. The results
reported in the following are from this second an-
notation.
We then examined the inter-annotator reliability
of the annotation by calculating the ? score (Car-
letta, 1996). The figures are given in Table 1. The
category other contains all cases in which one of
the minor categories was selected. Each table cell
contains the percentage agreement and the ? value
for the respective category. The final column con-
tains the overall ? for the entire annotation.
The table clearly shows that the classification
of it in spoken dialog appears to be by no means
trivial: With one exception, ? for the category
normal is below .67, the threshold which is nor-
mally regarded as allowing tentative conclusions
(Krippendorff, 1980). The ? for the nonreferen-
tial sub-categories extrapos it and prop-it is also
very variable, the figures for the former being on
average slightly better than those for the latter,
but still mostly below that threshold. In view of
these results, it would be interesting to see simi-
lar annotation experiments on written texts. How-
ever, a study of the types of confusions that oc-
cur showed that quite a few of the disagreements
arise from confusions of sub-categories belonging
to the same super-category, i.e. referential resp.
nonreferential. That means that a decision on the
level of granularity that is needed for the current
work can be done more reliably.
The data used in the machine learning experi-
ments described in Section 4 is a gold standard
variant that the annotators agreed upon after the
annotation was complete. The distribution of the
five classes in the gold standard data is as follows:
normal: 588, vague: 48, discarded: 222, extra-
pos it: 71, and prop-it: 88.
52
normal vague discarded extrapos it prop-it other ?
Bed017 81.8% / .65 36.4% / .33 94.7% / .94 30.8% / .27 63.8% / .54 44.4% / .42 .62
Bmr001 88.5% / .69 23.5% / .21 93.6% / .92 50.0% / .48 40.0% / .33 0.0% / -.01 .63
Bns003 81.9% / .59 22.2% / .18 80.5% / .75 58.8% / .55 27.6% / .21 33.3% / .32 .55
Bro004 84.0% / .65 0.0% / -.05 89.9% / .86 75.9% / .75 62.5% / .59 0.0% / -.01 .65
Bro005 78.6% / .57 0.0% / -.03 88.0% / .84 60.0% / .58 44.0% / .36 25.0% / .23 .58
Table 1: Classification of it by two annotators in a corpus subset.
4 Automatic Classification
4.1 Training and Test Data Generation
4.1.1 Segmentation
We extracted all instances of it and the segments
(i.e. speaker units) they occurred in. This pro-
duced a total of 1.017 instances, 62.5% of which
were referential. Each instance was labelled as
ref or nonref accordingly. Since a single segment
does not adequately reflect the context of the it,
we used the segments? time information to join
segments to larger units. We adopted the concept
and definition of spurt (Shriberg et al, 2001), i.e.
a sequence of speech not interrupted by any pause
longer than 500ms, and joined segments with time
distances below this threshold. For each instance
of it, features were generated mainly on the basis
of this spurt.
4.1.2 Preprocessing
For each spurt, we performed the following pre-
processing steps: First, we removed all single
dashes (i.e. interruption points), non-lexicalised
filled pauses (like em and eh), and all word frag-
ments. This affected only the string representa-
tion of the spurt (used for pattern matching later),
so the information that a certain spurt position was
associated with e.g. an interruption point or a filled
pause was not lost.
We then ran a simple algorithm to detect di-
rect repetitions of 1 to up to 6 words, where re-
moved tokens were skipped. If a repetition was
found, each token in the first occurrence was
tagged as discarded. Finally, we also temporarily
removed potential discourse markers by matching
each spurt against a short list of expressions like
actually, you know, I mean, but also so and sort
of. This was done rather agressively and without
taking any context into account. The rationale for
doing this was that while discourse markers do
indeed convey important information to the dis-
course, they are not relevant for the task at hand
and can thus be considered as noise that can be re-
moved in order to make the (syntactic and lexical)
patterns associated with nonreferential it stand out
more clearly. For each spurt thus processed, POS
tags were obtained automatically with the Stan-
ford tagger (Toutanova et al, 2003). Although this
tagger is trained on written text, we used it without
any retraining.
4.1.3 Feature Generation
One question we had to address was which infor-
mation from the transcription we wanted to use.
One can assume that using information like sen-
tence breaks or interruption points should be ex-
pected to help in the classification task at hand.
On the other hand, we did not want our system
to be dependent on this type of human-added in-
formation. Thus, we decided to do several setups
which made use of this information to various de-
grees. Different setups differed with respect to the
following options:
-use eos information: This option controls the
effect of explicit end-of-sentence information in
the transcribed data. If this option is active, this
information is used in two ways: Spurt strings are
trimmed in such a way that they do not cross sen-
tence boundaries. Also, the search space for dis-
tance features is limited to the current sentence.
-use interruption points: This option controls
the effect of explicit interruption points. If this op-
tion is active, this information is used in a similar
way as sentence boundary information.
All of the features described in the following
were obtained fully automatically. That means
that errors in the shallow feature generation meth-
ods could propagate into the model that was
learned from the data. The advantage of this ap-
proach is, however, that training and test data are
homogeneous. A model trained on partly erro-
neous data is supposed to be more robust against
similarly noisy testing data.
The first group of features consists of 21 sur-
face syntactic patterns capturing the left and right
context of it. Each pattern is represented by a bi-
nary feature which has either the value match or
nomatch. This type of pattern matching is done
53
for two reasons: To get a simplified symbolic
representation of the syntactic context of it, and
to extract the other elements (nouns, verbs) from
its predicative context. The patterns are matched
using shallow (regular-expression based) methods
only.
The second group of features contains lexical
information about the predicative context of it. It
includes the verb that it is the grammatical sub-
ject resp. object of (if any). Further features are
the nouns that serve as the direct object (if it is
subject), and the noun resp. adjective complement
in cases where it appears in a copula construction.
All these features are extracted from the patterns
described above, and then lemmatized.
The third group of features captures the wider
context of it through distance (in tokens) to words
of certain grammatical categories, like next com-
plementizer, next it, etc.
The fourth group of features contains the fol-
lowing: oblique is a binary feature encoding
whether the it is preceeded by a preposition.
in seemlist is a feature that encodes whether or not
the verb that it is the subject of appears in the list
seem, appear, look, mean, happen, sound (from
Dimitrov et al (2002)). discarded is a binary fea-
ture that encodes whether the it has been tagged as
discarded during preprocessing. The features are
listed in Table 2. Features of the first group are
only given as examples.
4.2 Machine Learning Experiment
We then applied machine learning in order to build
an automatic classifier for detecting nonreferential
instances of it, given a vector of features as de-
scribed above. We used JRip, the WEKA4 reim-
plementation of Ripper (Cohen, 1995). All fol-
lowing figures were obtained by means of ten-fold
cross-validation. Table 3 contains all results dis-
cussed in what follows.
In a first experiment, we did not use either of
the two options described above, so that no in-
formation about interruption points or sentence
boundaries was available during training or test-
ing. With this setting, the classifier achieved a re-
call of 55.1%, a precision of 71.9% and a resulting
F-measure of 62.4% for the detection of the class
nonreferential. The overall classification accuracy
was 75.1%.
The advantage of using a machine learning sys-
4http://www.cs.waikato.ac.nz/ ml/
tem that produces human-readable models is that
it allows direct introspection of which of the fea-
tures were used, and to which effect. It turned out
that the discarded feature is very successful. The
model produced a rule that used this feature and
correctly identified 83 instances of nonreferential
it, while it produced no false positives. Similarly,
the seem list feature alone was able to correctly
identify 22 instances, producing nine false posi-
tives. The following is an example of a more com-
plex rule involving distance features, which is also
very successful (37 true positives, 16 false posi-
tives):
dist_to_next_to <= 8 and
dist_to_next_adj <= 4
==> class = nonref (53.0/16.0)
This rule captures the common pattern for ex-
traposition constructions like It is important to do
that.
The following rule makes use of the feature en-
coding the distance to the next complementizer
(14 true positives, five false positives):
obj_verb = null and
dist_to_next_comp <= 5
==> nonref (19.0/5.0)
The fact that these rules with these conditions
were learned show that the features found to be
most important for the detection of nonreferential
it in written text (cf. Section 2) are also highly rele-
vant for performing that task for spoken language.
We then ran a second experiment in which we
used sentence boundary information to restrict the
scope of both the pattern matching features and
the distance-related features. We expected this to
improve the performance of the model, as patterns
should apply less generously (and thus more ac-
curately), which could be expected to result in an
increase in precision. However, the second experi-
ment yielded a recall of 57.7%, a precision of only
70.1% and an F-measure of 63.3% for the detec-
tion of this class. The overall accuracy was 74.9%.
The system produced a mere five rules (compared
to seven before). The model produced the identi-
cal rule using the discarded-feature. The same ap-
plies to the seem list feature, with the difference
that both precision and recall of this rule were al-
tered: The rule now produced 23 true positives and
six false positives. The slightly higher recall of the
model using the sentence boundary information is
mainly due to a better coverage of the rule using
the features encoding the distance to the next to-
infinitive and the next adjective: it now produced
54
Syntactic Patterns
1. INF it do it
10. it BE adj it was easy
11. it BE obj it?s a simple question
13. it MOD-VERBS INF obj it?ll take some more time
20. it VERBS TO-INF it seems to be
Lexical Features
22. noun comp noun complement (in copula construction)
23. adj comp adjective complement (in copula construction)
24. subj verb verb that it is the subject of
25. prep preposition before indirect object
26. ind obj indirect object of verb that it is subject of
27. obj direct object of verb that it is subject of
28. obj verb verb that it is object of
Distance Features (in tokens)
29. dist to next adj distance to next adjective
30. dist to next comp distance to next complementizer (that,if,whether)
31. dist to next it distance to next it
32. dist to next nominal distance to next nominal
33. dist to next to distance to next to-infinitive
34. dist to previous comp distance to previous complementizer
35. dist to previous nominal distance to previous nominal
Other Features
36. oblique whether it follows a preposition
37. seem list whether subj verb is seem, appear, look, mean, happen, sound
38. discarded whether it has been marked as discarded (i.e. in a repetition)
Table 2: Our Features (selection)
57 true positives and only 30 false positives.
We then wanted to compare the contribution
of the sentence breaks to that of the interruption
points. We ran another experiment, using only the
latter and leaving everything else unaltered. This
time, the overall performance of the classifier im-
proved considerably: recall was 60.9%, precision
80.0%, F-measure 69.2%, and the overall accu-
racy was 79.6%. The resulting model was rather
complicated, including seven complex rules. The
increase in recall is mainly due to the following
rule, which is not easily interpreted:5
it_s = match and
dist_to_next_nominal >=21 and
dist_to_next_adj >=500 and
subj_verb = null
==> nonref (116.0/31.0)
The considerable improvement (in particular
in precision) brought about by the interruption
points, and the comparatively small impact of sen-
tence boundary information, might be explainable
in several ways. For instance, although sentence
boundary information allows to limit both the
search space for distance features and the scope of
pattern matching, due to the shallow nature of pre-
processing, what is between two sentence breaks
is by no means a well-formed sentence. In that
respect, it seems plausible to assume that smaller
5The value 500 is used as a MAX VALUE to indicate that
no match was found.
units (as delimited by interruption points) may be
beneficial for precision as they give rise to fewer
spurious matches. It must also be noted that inter-
ruption points do not mark arbitrary breaks in the
flow of speech, but that they can signal important
information (cf. Heeman & Allen (1999)).
5 Conclusion and Future Work
This paper presented a machine learning system
for the automatic detection of nonreferential it in
spoken dialog. Given the fact that our feature ex-
traction methods are only very shallow, the re-
sults we obtained are satisfying. On the one hand,
the good results that we obtained when utilizing
information about interruption points (P:80.0% /
R:60.9% / F:69.2%) show the feasibility of detect-
ing nonreferential it in spoken multi-party dialog.
To our knowledge, this task has not been tackled
before. On the other hand, the still fairly good
results obtained by only using automatically de-
termined features (P:71.9% / R:55.1% / F:62.4%)
show that a practically usable filtering compo-
nent for nonreferential it can be created even with
rather simple means.
All experiments yielded classifiers that are con-
servative in the sense that their precision is consid-
erably higher than their recall. This makes them
particularly well-suited as filter components.
For the coreference resolution system that this
55
P R F % Correct
None 71.9 % 55.1 % 62.4 % 75.1 %
Sentence Breaks 70.1 % 57.7 % 63.3 % 74.9 %
Interruption Points 80.0 % 60.9 % 69.2 % 79.6 %
Both 74.2 % 60.4 % 66.6 % 77.3 %
Table 3: Results of Automatic Classification Using Various Information Sources
work is part of, only the fully automatic variant is
an option. Therefore, future work must try to im-
prove its recall without harming its precision (too
much). One way to do that could be to improve the
recognition (i.e. correct POS tagging) of grammat-
ical function words (in particular complementizers
like that) which have been shown to be important
indicators for constructions with nonreferential it.
Other points of future work include the refinement
of the syntactic pattern features and the lexical fea-
tures. E.g., the values (i.e. mostly nouns, verbs,
and adjectives) of the lexical features, which have
been almost entirely ignored by both classifiers,
could be generalized by mapping them to common
WordNet superclasses.
Acknowledgements
This work has been funded by the Deutsche
Forschungsgemeinschaft (DFG) in the context of
the project DIANA-Summ (STR 545/2-1), and by
the Klaus Tschira Foundation (KTF), Heidelberg,
Germany. We thank our annotators Irina Schenk
and Violeta Sabutyte, and the three anonymous re-
viewers for their helpful comments.
References
Boyd, A., W. Gegg-Harrison & D. Byron (2005). Identifying
non-referential it: a machine learning approach incor-
porating linguistically motivated patterns. In Proceed-
ings of the ACL Workshop on Feature Selection for Ma-
chine Learning in NLP, Ann Arbor, MI, June 2005, pp.
40?47.
Byron, D. K. (2002). Resolving pronominal reference to ab-
stract entities. In Proc. of ACL-02, pp. 80?87.
Carletta, J. (1996). Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Clemente, J. C., K. Torisawa & K. Satou (2004). Improv-
ing the identification of non-anaphoric it using Support
Vector Machines. In International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications, Geneva, Switzerland.
Cohen, W. W. (1995). Fast effective rule induction. In
Proc. of the 12th International Conference on Machine
Learning, pp. 115?123.
Dimitrov, M., K. Bontcheva, H. Cunningham & D. Maynard
(2002). A light-weight approach to coreference resolu-
tion for named entities in text. In Proc. DAARC2.
Eckert, M. & M. Strube (2000). Dialogue acts, synchronising
units and anaphora resolution. Journal of Semantics,
17(1):51?89.
Evans, R. (2001). Applying machine learning toward an auto-
matic classification of It. Literary and Linguistic Com-
puting, 16(1):45 ? 57.
Heeman, P. & J. Allen (1999). Speech repairs, intonational
phrases, and discourse markers: Modeling speakers?
utterances in spoken dialogue. Computational Linguis-
tics, 25(4):527?571.
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke &
C. Wooters (2003). The ICSI Meeting Corpus. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, Hong Kong,
pp. 364?367.
Krippendorff, K. (1980). Content Analysis: An introduction
to its methodology. Beverly Hills, CA: Sage Publica-
tions.
Lappin, S. & H. J. Leass (1994). An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp. 104?111.
Paice, C. D. & G. D. Husk (1987). Towards the automatic
recognition of anaphoric features in English text: the
impersonal pronoun ?it?. Computer Speech and Lan-
guage, 2:109?132.
Quirk, R., S. Greenbaum, G. Leech & J. Svartvik (1991).
A Comprehensive Grammar of the English Language.
London, UK: Longman.
Shriberg, E., A. Stolcke & D. Baron (2001). Observations
on overlap: Findings and implications for automatic
processing of multi-party conversation. In Proceedings
of the 7th European Conference on Speech Communi-
cation and Technology (EUROSPEECH ?01), Aalborg,
Denmark, 3?7 September 2001, Vol. 2, pp. 1359?1362.
Strube, M. & C. Mu?ller (2003). A machine learning approach
to pronoun resolution in spoken dialogue. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan, 7?12 July
2003, pp. 168?175.
Toutanova, K., D. Klein & C. D. Manning (2003). Feature-
rich part-of-speech tagging with a cyclic dependency
network. In Proceedings of HLT-NAACL 03, pp. 252?
259.
56
Applying Co-Training to Reference Resolution
Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
mueller@eml.villa-bosch.de
Stefan Rapp
Sony International (Europe) GmbH
Advanced Technology Center Stuttgart
Heinrich-Hertz-Stra?e 1
70327 Stuttgart, Germany
rapp@sony.de
Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
strube@eml.villa-bosch.de
Abstract
In this paper, we investigate the practical
applicability of Co-Training for the task
of building a classifier for reference reso-
lution. We are concerned with the ques-
tion if Co-Training can significantly re-
duce the amount of manual labeling work
and still produce a classifier with an ac-
ceptable performance.
1 Introduction
A major obstacle for natural language processing
systems which analyze natural language texts or
utterances is the need to identify the entities re-
ferred to by means of referring expressions. Among
referring expressions, pronouns and definite noun
phrases (NPs) are the most prominent.
Supervised machine learning algorithms were
used for pronoun resolution with good results (Ge et
al., 1998), and for definite NPs with fairly good re-
sults (Aone and Bennett, 1995; McCarthy and Lehn-
ert, 1995; Soon et al, 2001). However, the defi-
ciency of supervised machine learning approaches is
the need for an unknown amount of annotated train-
ing data for optimal performance.
So, researchers in NLP began to experiment with
weakly supervised machine learning algorithms
such as Co-Training (Blum and Mitchell, 1998).
Among others Co-Training was applied to document
classification (Blum and Mitchell, 1998), named-
entity recognition (Collins and Singer, 1999), noun
phrase bracketing (Pierce and Cardie, 2001), and
statistical parsing (Sarkar, 2001). In this paper we
apply Co-Training to the problem of reference reso-
lution in German texts from the tourism domain in
order to provide answers to the following questions:
  Does Co-Training work at all for this task
(when compared to conventional C4.5 decision
tree learning)?
  How much labeled training data is required for
achieving a reasonable performance?
First, we discuss features that have been found to
be relevant for the task of reference resolution, and
describe the feature set that we are using (Section 2).
Then we briefly introduce the Co-Training paradigm
(Section 3), which is followed by a description of the
corpus we use, the corpus annotation, and the way
we prepared the data for using a binary classifier in
the Co-Training algorithm (Section 4). In Section 5
we specify the experimental setup and report on the
results.
2 Features for Reference Resolution
2.1 Previous Work
Driven by the necessity to provide robust systems
for the MUC system evaluations, researchers began
to look for those features which were particular im-
portant for the task of reference resolution. While
most features for pronoun resolution have been de-
scribed in the literature for decades, researchers only
recently began to look for robust and cheap features,
i.e., those which perform well over several domains
and can be annotated (semi-) automatically. Also,
the relative quantitative contribution of each of these
features came into focus only after the advent of
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 352-359.
                         Proceedings of the 40th Annual Meeting of the Association for
corpus-based and statistical methods. In the follow-
ing, we describe a few earlier contributions with re-
spect to the features used.
Decision tree algorithms were used for ref-
erence resolution by Aone and Bennett (1995,
C4.5), McCarthy and Lehnert (1995, C4.5) and
Soon et al (2001, C5.0). This approach requires
the definition of a set of training features de-
scribing pairs of anaphors and their antecedents.
Aone and Bennett (1995), working on reference
resolution in Japanese newspaper articles, use
66 features. They do not mention all of these
explicitly but emphasize the features POS-tag,
grammatical role, semantic class and distance.
The set of semantic classes they use appears to be
rather elaborated and highly domain-dependent.
Aone and Bennett (1995) report that their best
classifier achieved an F-measure of about 77% after
training on 250 documents. They mention that
it was important for the training data to contain
transitive positives, i.e., all possible coreference
relations within an anaphoric chain.
McCarthy and Lehnert (1995) describe a refer-
ence resolution component which they evaluated on
the MUC-5 English Joint Venture corpus. They dis-
tinguish between features which focus on individ-
ual noun phrases (e.g. Does noun phrase contain a
name?) and features which focus on the anaphoric
relation (e.g. Do both share a common NP?). It
was criticized (Soon et al, 2001) that the features
used by McCarthy and Lehnert (1995) are highly id-
iosyncratic and applicable only to one particular do-
main. McCarthy and Lehnert (1995) achieved re-
sults of about 86% F-measure (evaluated accord-
ing to Vilain et al (1995)) on the MUC-5 data set.
However, only a defined subset of all possible ref-
erence resolution cases was considered relevant in
the MUC-5 task description, e.g., only entity refer-
ences. For this case, the domain-dependent features
may have been particularly important, making it dif-
ficult to compare the results of this approach to oth-
ers working on less restricted domains.
Soon et al (2001) use twelve features (see Ta-
ble 1). They show a part of their decision tree in
which the weak string identity feature (i.e. iden-
tity after determiners have been removed) appears
to be the most important one. They also report
on the relative contribution of the features where
? distance in sentences between anaphor and antecedent
? antecedent is a pronoun?
? anaphor is a pronoun?
? weak string identity between anaphor and antecedent
? anaphor is a definite noun phrase?
? anaphor is a demonstrative pronoun?
? number agreement between anaphor and antecedent
? semantic class agreement between anaphor and an-
tecedent
? gender agreement between anaphor and antecedent
? anaphor and antecedent are both proper names?
? an alias feature (used for proper names and acronyms)
? an appositive feature
Table 1: Features used by Soon et al
the three features weak string identity, alias (which
maps named entities in order to resolve dates, per-
son names, acronyms, etc.) and appositive seem to
cover most of the cases (the other nine features con-
tribute only 2.3% F-measure for MUC-6 texts and
1% F-measure for MUC-7 texts). Soon et al (2001)
include all noun phrases returned by their NP iden-
tifier and report an F-measure of 62.6% for MUC-6
data and 60.4% for MUC-7 data. They only used
pairs of anaphors and their closest antecedents as
positive examples in training, but evaluated accord-
ing to Vilain et al (1995).
Cardie and Wagstaff (1999) describe an unsuper-
vised clustering approach to noun phrase corefer-
ence resolution in which features are assigned to sin-
gle noun phrases only. They use the features shown
in Table 2, all of which are obtained automatically
without any manual tagging.
? position (NPs are numbered sequentially)
? pronoun type (nom., acc., possessive, ambiguous)
? article (indefinite, definite, none)
? appositive (yes, no)
? number (singular, plural)
? proper name (yes, no)
? semantic class (based on WordNet: time, city, animal,
human, object; based on a separate algorithm: number,
money, company)
? gender (masculine, feminine, either, neuter)
? animacy (anim, inanim)
Table 2: Features used by Cardie and Wagstaff
The feature semantic class used by
Cardie and Wagstaff (1999) seems to be a
domain-dependent one which can only be
used for the MUC domain and similar ones.
Cardie and Wagstaff (1999) report a performance
of 53,6% F-measure (evaluated according to
Vilain et al (1995)).
2.2 Our Features
We consider the features we use for our weakly
supervised approach to be domain-independent.
We distinguish between features assigned to noun
phrases and features assigned to the potential coref-
erence relation. They are listed in Table 3 together
with their respective possible values. In the liter-
ature on reference resolution it is claimed that the
antecedent?s grammatical function and its realiza-
tion are important. Hence we introduce the features
ante gram func and ante npform. The identity in
grammatical function of a potential anaphor and an-
tecedent is captured in the feature syn par. Since
in German the gender and the semantic class do not
necessarily coincide (i.e. objects are not necessarily
neuter as in English) we also provide a semantic-
class feature which captures the difference between
human, concrete, and abstract objects. This basi-
cally corresponds to the gender attribute in English.
The feature wdist captures the distance in words be-
tween anaphor and antecedent, the feature ddist cap-
tures the distance in sentences, the feature mdist the
number of markables (NPs) between anaphor and
antecedent. Features like the string ident and sub-
string match features were used by other researchers
(Soon et al, 2001), while the features ante med and
ana med were used by Strube et al (2002) in order
to improve the performance for definite NPs. The
minimum edit distance (MED) computes the simi-
larity of strings by taking into account the minimum
number of editing operations (substitutions s, inser-
tions i, deletions d) needed to transform one string
into the other (Wagner and Fischer, 1974). The
MED is computed from these editing operations and
the length of the potential antecedent m or the length
of the anaphor n.
3 Co-Training
Co-Training (Blum and Mitchell, 1998) is a meta-
learning algorithm which exploits unlabeled in ad-
dition to labeled training data for classifier learn-
ing. A Co-Training classifier is complex in the sense
that it consists of two simple classifiers (most often
Naive Bayes, e.g. by Blum and Mitchell (1998) and
Pierce and Cardie (2001)). Initially, these classifiers
are trained in the conventional way using a small set
of size L of labeled training data. In this process,
each of the two classifiers is trained on a different
subset of features of the training data. These feature
subsets are commonly referred to as different views
that the classifiers have on the data, i.e., each classi-
fier describes a given instance in terms of different
features. The Co-Training algorithm is supposed to
bootstrap by gradually extending the training data
with self-labeled instances. It utilizes the two classi-
fiers by letting them in turn label the p best positive
and n best negative instances from a set of size P
of unlabeled training data (referred to in the litera-
ture as the pool). Instances labeled by one classifier
are then added to the other?s training data, and vice
versa. After each turn, both classifiers are re-trained
on their augmented training sets, and the pool is re-
filled with
	

unlabeled training instances
drawn at random. This process is repeated either for
a given number of iterations I or until all the unla-
beled data has been labeled. In particular the defi-
nition of the two data views appears to be a crucial
factor which can strongly influence the behaviour of
Co-Training. A number of requirements for these
views are mentioned in the literature, e.g., that they
have to be disjoint or even conditionally indepen-
dent (but cf. Nigam and Ghani (2000)). Another im-
portant factor is the ratio between p and n, i.e., the
number of positive and negative instances added in
each iteration. These values are commonly chosen
in such a way as to reflect the empirical class distri-
bution of the respective instances.
4 Data
4.1 Text Corpus
Our corpus consists of 250 short German texts (total
36924 tokens, 9399 NPs, 2179 anaphoric NPs) about
sights, historic events and persons in Heidelberg.
The average length of the texts was 149 tokens. The
texts were POS-tagged using TnT (Brants, 2000). A
basic identification of markables (i.e. NPs) was ob-
tained by using the NP-Chunker Chunkie (Skut and
Brants, 1998). The POS-tagger was also used for
assigning attributes to markables (e.g. the NP form).
The automatic annotation was followed by a man-
Document level features
1. doc id document number (1 . . . 250)
NP-level features
2. ante gram func grammatical function of antecedent (subject, object, other)
3. ante npform form of antecedent (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
4. ante agree agreement in person, gender, number
5. ante semanticclass semantic class of antecedent (human, concrete object, abstract object)
6. ana gram func grammatical function of anaphor (subject, object, other)
7. ana npform form of anaphor (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
8. ana agree agreement in person, gender, number
9. ana semanticclass semantic class of anaphor (human, concrete object, abstract object)
Coreference-level features
10. wdist distance between anaphor and antecedent in words (1 . . . n)
11. ddist distance between anaphor and antecedent in sentences (0, 1,  1)
12. mdist distance between anaphor and antecedent in markables (NPs) (1 . . . n)
13. syn par anaphor and antecedent have the same grammatical function (yes, no)
14. string ident anaphor and antecedent consist of identical strings (yes, no)
15. substring match one string contains the other (yes, no)
16. ante med minimum edit distance to anaphor:  ffA Machine Learning Approach to Pronoun Resolution in Spoken Dialogue
Michael Strube and Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
michael.strube|christoph.mueller  @eml.villa-bosch.de
Abstract
We apply a decision tree based approach
to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP-
and non-NP-antecedents. We present a set
of features designed for pronoun resolu-
tion in spoken dialogue and determine the
most promising features. We evaluate the
system on twenty Switchboard dialogues
and show that it compares well to Byron?s
(2002) manually tuned system.
1 Introduction
Corpus-based methods and machine learning tech-
niques have been applied to anaphora resolution in
written text with considerable success (Soon et al,
2001; Ng & Cardie, 2002, among others). It has
been demonstrated that systems based on these ap-
proaches achieve a performance that is comparable
to hand-crafted systems. Since they can easily be
applied to new domains it seems also feasible to
port a given corpus-based anaphora resolution sys-
tem from written text to spoken dialogue. This pa-
per describes the extensions and adaptations needed
for applying our anaphora resolution system (Mu?ller
et al, 2002; Strube et al, 2002) to pronoun resolu-
tion in spoken dialogue.
There are important differences between written
text and spoken dialogue which have to be accounted
for. The most obvious difference is that in spo-
ken dialogue there is an abundance of (personal and
demonstrative) pronouns with non-NP-antecedents
or no antecedents at all. Corpus studies have shown
that a significant amount of pronouns in spoken di-
alogue have non-NP-antecedents: Byron & Allen
(1998) report that about 50% of the pronouns in the
TRAINS93 corpus have non-NP-antecedents. Eck-
ert & Strube (2000) note that only about 45% of
the pronouns in a set of Switchboard dialogues have
NP-antecedents. The remainder consists of 22%
which have non-NP-antecedents and 33% without
antecedents. These studies suggest that the perfor-
mance of a pronoun resolution algorithm can be im-
proved considerably by enabling it to resolve also
pronouns with non-NP-antecedents.
Because of the difficulties a pronoun resolution
algorithm encounters in spoken dialogue, previous
approaches were applied only to tiny domains, they
needed deep semantic analysis and discourse pro-
cessing and relied on hand-crafted knowledge bases.
In contrast, we build on our existing anaphora res-
olution system and incrementally add new features
specifically devised for spoken dialogue. That way
we are able to determine relatively powerful yet
computationally cheap features. To our knowledge
the work presented here describes the first imple-
mented system for corpus-based anaphora resolution
dealing also with non-NP-antecedents.
2 NP- vs. Non-NP-Antecedents
Spoken dialogue contains more pronouns with non-
NP-antecedents than written text does. However,
pronouns with NP-antecedents (like 3rd pers. mas-
culine/feminine pronouns, cf. he in the example be-
low) still constitute the largest fraction of all coref-
erential pronouns in the Switchboard corpus.
In spoken dialogue there are considerable num-
bers of pronouns that pick up different kinds of
abstract objects from the previous discourse, e.g.
events, states, concepts, propositions or facts (Web-
ber, 1991; Asher, 1993). These anaphors then have
VP-antecedents (?it   ? in (B6) below) or sentential
antecedents (?that  ? in (B5)).
A1: . . . [he]  ?s nine months old. . . .
A2: [He]  likes to dig around a little bit.
A3: [His]  mother comes in and says, why did you let [him] 
[play in the dirt] ,
A:4 I guess [[he]  ?s enjoying himself]  .
B5: [That]  ?s right.
B6: [It] ?s healthy, . . .
A major problem for pronoun resolution in spo-
ken dialogue is the large number of personal and
demonstrative pronouns which are either not refer-
ential at all (e.g. expletive pronouns) or for which a
particular antecedent cannot easily be determined by
humans (called vague anaphors by Eckert & Strube
(2000)).
In the following example, the ?that  ? in utter-
ance (A3) refers back to utterance (A1). As for
the first two pronouns in (B4), following Eckert &
Strube (2000) and Byron (2002) we assume that re-
ferring expressions in disfluencies, abandoned utter-
ances etc. are excluded from the resolution. The
third pronoun in (B4) is an expletive. The pronoun
in (A5) is different in that it is indeed referential: it
refers back to?that  ? from (A3).
A1: . . . [There is a lot of theft, a lot of assault dealing with, uh,
people trying to get money for drugs.  ]
B2: Yeah.
A3: And, uh, I think [that  ]?s a national problem, though.
B4: It, it, it?s pretty bad here, too.
A5: [It  ]?s not unique . . .
Pronoun resolution in spoken dialogue also has
to deal with the whole range of difficulties that
come with processing spoken language: disfluen-
cies, hesitations, abandoned utterances, interrup-
tions, backchannels, etc. These phenomena have to
be taken into account when formulating constraints
on e.g. the search space in which an anaphor looks
for its antecedent. E.g., utterance (B2) in the previ-
ous example does not contain any referring expres-
sions. So the demonstrative pronoun in (A3) has to
have access not only to (B2) but also to (A1).
3 Data
3.1 Corpus
Our work is based on twenty randomly chosen
Switchboard dialogues. Taken together, the dia-
logues contain 30810 tokens (words and punctua-
tion) in 3275 sentences / 1771 turns. The annotation
consists of 16601 markables, i.e. sequences of words
and attributes associated with them. On the top level,
different types of markables are distinguished: NP-
markables identify referring expressions like noun
phrases, pronouns and proper names. Some of
the attributes for these markables are derived from
the Penn Treebank version of the Switchboard dia-
logues, e.g. grammatical function, NP form, gram-
matical case and depth of embedding in the syn-
tactical structure. VP-markables are verb phrases,
S-markables sentences. Disfluency-markables are
noun phrases or pronouns which occur in unfin-
ished or abandoned utterances. Among other (type-
dependent) attributes, markables contain a member
attribute with the ID of the coreference class they
are part of (if any). If an expression is used to re-
fer to an entity that is not referred to by any other
expression, it is considered a singleton.
Table 1 gives the distribution of the npform at-
tribute for NP-markables. The second and third row
give the number of non-singletons and singletons re-
spectively that add up to the total number given in
the first row.
Table 2 shows the distribution of the agreement
attribute (i.e. person, gender, and number) for the
pronominal expressions in our corpus. The left fig-
ure in each cell gives the total number of expres-
sions, the right figure gives the number of non-
singletons. Note the relatively high number of sin-
gletons among the personal and demonstrative pro-
nouns (223 for it, 60 for they and 82 for that). These
pronouns are either expletive or vague, and cause
the most trouble for a pronoun resolution algorithm,
which will usually attempt to find an antecedent
nonetheless. Singleton they pronouns, in particu-
lar, are typical for spoken language (as opposed to
defNP indefNP NNP prp prp$ dtpro
Total 1080 1899 217 1075 70 392
In coreference relation 219 163 94 786 56 309
Singletons 861 1736 123 289 14 83
Table 1: Distribution of npform Feature on Markables (w/o 1st and 2nd Persons)
3m 3f 3n 3p
prp 67 63 49 47 541 318 418 358
prp$ 18 15 14 11 3 3 35 27
dtpro 0 0 0 0 380 298 12 11
  85 78 63 58 924 619 465 396
Table 2: Distribution of Agreement Feature on Pronominal Expressions
written text). The same is true for anaphors with
non-NP-antecedents. However, while they are far
more frequent in spoken language than in written
text, they still constitute only a fraction of all coref-
erential expressions in our corpus. This defines an
upper limit for what the resolution of these kinds of
anaphors can contribute at all. These facts have to be
kept in mind when comparing our results to results
of coreference resolution in written text.
3.2 Data Generation
Training and test data instances were generated from
our corpus as follows. All markables were sorted
in document order, and markables for first and sec-
ond person pronouns were removed. The resulting
list was then processed from top to bottom. If the
list contained an NP-markable at the current posi-
tion and if this markable was not an indefinite noun
phrase, it was considered a potential anaphor. In
that case, pairs of potentially coreferring expressions
were generated by combining the potential anaphor
with each compatible1 NP-markable preceding2 it
in the list. The resulting pairs were labelled P if
both markables had the same (non-empty) value in
their member attribute, N otherwise. For anaphors
with non-NP-antecedents, additional training and
test data instances had to be generated. This process
was triggered by the markable at the current position
being it or that. In that case, a small set of poten-
tial non-NP-antecedents was generated by selecting
S- and VP-markables from the last two valid sen-
tences preceding the potential anaphor. The choice
1Markables are considered compatible if they do not mis-
match in terms of agreement.
2We disregard the phenomenon of cataphor here.
of the last two sentences was motivated pragmat-
ically by considerations to keep the search space
(and the number of instances) small. A sentence
was considered valid if it was neither unfinished
nor a backchannel utterance (like e.g. ?Uh-huh?,
?Yeah?, etc.). From the selected markables, inac-
cessible non-NP-expressions were automatically re-
moved. We considered an expression inaccessible
if it ended before the sentence in which it was con-
tained. This was intended to be a rough approxi-
mation of the concept of the right frontier (Webber,
1991). The remaining expressions were then com-
bined with the potential anaphor. Finally, the result-
ing pairs were labelled P or N and added to the in-
stances generated with NP-antecedents.
4 Features
We distinguish two classes of features: NP-level
features specify e.g. the grammatical function, NP
form, morpho-syntax, grammatical case and the
depth of embedding in the syntactical structure.
For these features, each instance contains one
value for the antecedent and one for the anaphor.
Coreference-level features, on the other hand, de-
scribe the relation between antecedent and anaphor
in terms of e.g. distance (in words, markables and
sentences), compatibility in terms of agreement and
identity of syntactic function. For these features,
each instance contains only one value.
In addition, we introduce a set of features which
is partly tailored to the processing of spoken dia-
logue. The feature ante exp type (17) is a rather
obvious yet useful feature to distinguish NP- from
non-NP-antecedents. The features ana np , vp and
NP-level features
1. ante gram func grammatical function of antecedent
2. ante npform form of antecedent
3. ante agree person, gender, number
4. ante case grammatical case of antecedent
5. ante s depth the level of embedding in a sentence
6. ana gram func grammatical function of anaphor
7. ana npform form of anaphor
8. ana agree person, gender, number
9. ana case grammatical case of anaphor
10. ana s depth the level of embedding in a sentence
Coreference-level features
11. agree comp compatibility in agreement between anaphor and antecedent
12. npform comp compatibilty in NP form between anaphor and antecedent
13. wdist distance between anaphor and antecedent in words
14. mdist distance between anaphor and antecedent in markables
15. sdist distance between anaphor and antecedent in sentences
16. syn par anaphor and antecedent have the same grammatical function (yes, no)
Features introduced for spoken dialogue
17. ante exp type type of antecedent (NP, S, VP)
18. ana np pref preference for NP arguments
19. ana vp pref preference for VP arguments
20. ana s pref preference for S arguments
21. mdist 3mf3p (see text)
22. mdist 3n (see text)
23. ante tfidf (see text)
24. ante ic (see text)
25. wdist ic (see text)
Table 3: Our Features
s pref (18, 19, 20) describe a verb?s preference for
arguments of a particular type. Inspired by the
work of Eckert & Strube (2000) and Byron (2002),
these features capture preferences for NP- or non-
NP-antecedents by taking a pronoun?s predicative
context into account. The underlying assumption is
that if a verb preceding a personal or demonstrative
pronoun preferentially subcategorizes sentences or
VPs, then the pronoun will be likely to have a non-
NP-antecedent. The features are based on a verb list
compiled from 553 Switchboard dialogues.3 For ev-
ery verb occurring in the corpus, this list contains
up to three entries giving the absolute count of cases
where the verb has a direct argument of type NP, VP
or S. When the verb list was produced, pronominal
arguments were ignored. The features mdist 3mf3p
and mdist 3n (21, 22) are refinements of the mdist
feature. They measure the distance in markables be-
tween antecedent and anaphor, but in doing so they
take the agreement value of the anaphor into ac-
count. For anaphors with an agreement value of 3mf
or 3p, mdist 3mf3p is measured as D = 1 + the num-
3It seemed preferable to compile our own list instead of us-
ing existing ones like Briscoe & Carroll (1997).
ber of NP-markables between anaphor and potential
antecedent. Anaphors with an agreement value of
3n, (i.e. it or that), on the other hand, potentially
have non-NP-antecedents, so mdist 3n is measured
as D + the number of anaphorically accessible4 S-
and VP-markables between anaphor and potential
antecedent.
The feature ante tfifd (23) is supposed to capture
the relative importance of an expression for a dia-
logue. The underlying assumption is that the higher
the importance of a non-NP expression, the higher
the probability of its being referred back to. For
our purposes, we calculated TF for every word by
counting its frequency in each of our twenty Switch-
board dialogues separately. The calculation of IDF
was based on a set of 553 Switchboard dialogues.
For every word, we calculated IDF as log(553/N   ),
with N   =number of documents containing the word.
For every non-NP-markable, an average TF*IDF
value was calculated as the TF*IDF sum of all words
comprising the markable, divided by the number of
4As mentioned earlier, the definition of accessibility of non-
NP-antecedents is inspired by the concept of the right frontier
(Webber, 1991).
words in the markable. The feature ante ic (24) as
an alternative to ante tfidf is based on the same as-
sumptions as the former. The information content of
a non-NP-markable is calculated as follows, based
on a set of 553 Switchboard dialogues: For each
word in the markable, the IC value was calculated
as the negative log of the total frequency of the word
divided by the total number of words in all 553 dia-
logues. The average IC value was then calculated as
the IC sum of all words in the markable, divided by
the number of words in the markable. Finally, the
feature wdist ic (25) measures the word-based dis-
tance between two expressions. It does so in terms
of the sum of the individual words? IC. The calcula-
tion of the IC was done as described for the ante ic
feature.
5 Experiments and Results
5.1 Experimental Setup
All experiments were performed using the decision
tree learner RPART (Therneau & Atkinson, 1997),
which is a CART (Breiman et al, 1984) reimple-
mentation for the S-Plus and R statistical comput-
ing environments (we use R, Ihaka & Gentleman
(1996), see http://www.r-project.org). We used the
standard pruning and control settings for RPART
(cp=0.0001, minsplit=20, minbucket=7). All results
reported were obtained by performing 20-fold cross-
validation.
In the prediction phase, the trained classifier is ex-
posed to unlabeled instances of test data. The classi-
fier?s task is to label each instance. When an instance
is labeled as coreferring, the IDs of the anaphor and
antecedent are kept in a response list for the evalua-
tion according to Vilain et al (1995).
For determining the relevant feature set we fol-
lowed an iterative procedure similar to the wrap-
per approach for feature selection (Kohavi & John,
1997). We start with a model based on a set of prede-
fined baseline features. Then we train models com-
bining the baseline with all additional features sep-
arately. We choose the best performing feature (f-
measure according to Vilain et al (1995)), adding
it to the model. We then train classifiers combining
the enhanced model with each of the remaining fea-
tures separately. We again choose the best perform-
ing classifier and add the corresponding new feature
to the model. This process is repeated as long as
significant improvement can be observed.
5.2 Results
In our experiments we split the data in three sets ac-
cording to the agreement of the anaphor: third per-
son masculine and feminine pronouns (3mf), third
person neuter pronouns (3n), and third person plural
pronouns (3p). Since only 3n-pronouns have non-
NP-antecedents, we were mainly interested in im-
provements in this data set.
We used the same baseline model for each data
set. The baseline model corresponds to a pronoun
resolution algorithm commonly applied to written
text, i.e., it uses only the features in the first two
parts of Table 3. For the baseline model we gener-
ated training and test data which included only NP-
antecedents.
Then we performed experiments using the fea-
tures introduced for spoken dialogue. The training
and test data for the models using additional features
included NP- and non-NP-antecedents. For each
data set we followed the iterative procedure outlined
in Section 5.1.
In the following tables we present the results of
our experiments. The first column gives the number
of coreference links correctly found by the classifier,
the second column gives the number of all corefer-
ence links found. The third column gives the total
number of coreference links (1250) in the corpus.
During evaluation, the list of all correct links is used
as the key list against which the response list pro-
duced by the classifier (cf. above) is compared. The
remaining three columns show precision, recall and
f-measure, respectively.
Table 4 gives the results for 3mf pronouns. The
baseline model performs very well on this data set
(the low recall figure is due to the fact that the 3mf
data set contains only a small subset of the coref-
erence links expected by the evaluation). The re-
sults are comparable to any pronoun resolution al-
gorithm dealing with written text. This shows that
our pronoun resolution system could be ported to the
spoken dialogue domain without sacrificing perfor-
mance.
Table 5 shows the results for 3n pronouns. The
baseline model does not perform very well. As men-
tioned above, for evaluating the performance of the
correct found total found total correct precision recall f-measure
baseline, features 1-16 120 150 1250 80.00 9.60 17.14
plus mdist 3mf3p 121 153 1250 79.08 9.68 17.25
Table 4: Results for Third Person Masculine and Feminine Pronouns (3mf)
correct found total found total correct precision recall f-measure
baseline, features 1-16 109 235 1250 46.38 8.72 14.68
plus none 97 232 1250 41.81 7.76 13.09
plus ante exp type 137 359 1250 38.16 10.96 17.03
plus wdist ic 154 389 1250 39.59 12.32 18.79
plus ante tfidf 158 391 1250 40.41 12.64 19.26
Table 5: Results for Third Person Neuter Pronouns (3n)
baseline model we removed all potential non-NP-
antecedents from the data. This corresponds to a
naive application of a model developed for written
text to spoken dialogue.
First, we applied the same model to the data set
containing all kinds of antecedents. The perfor-
mance drops somewhat as the classifier is exposed
to non-NP-antecedents without being able to differ-
entiate between NP- and non-NP-antecedents. By
adding the feature ante exp type the classifier is en-
abled to address NP- and non-NP-antecedents dif-
ferently, which results in a considerable gain in per-
formance. Substituting the wdist feature with the
wdist ic feature also improves the performance con-
siderably. The ante tfidf feature only contributes
marginally to the overall performance. ? These re-
sults show that it pays off to consider features par-
ticularly designed for spoken dialogue.
Table 6 presents the results for 3p pronouns,
which do not have non-NP-antecedents. Many of
these pronouns do not have an antecedent at all. Oth-
ers are vague in that human annotators felt them
to be referential, but could not determine an an-
tecedent. Since we did not address that issue in
depth, the classifier tries to find antecedents for these
pronouns indiscriminately, which results in rather
low precision figures, as compared to e.g. those for
3mf. Only the feature wdist ic leads to an improve-
ment over the baseline.
Table 7 shows the results for the combined clas-
sifiers. The improvement in f-measure is due to the
increase in recall while the precision shows only a
slight decrease.
Though some of the features of the baseline
model (features 1-16) still occur in the decision
tree learned, the feature ante exp type divides ma-
jor parts of the tree quite nicely (see Figure 1). Be-
low that node the feature ana npform is used to dis-
tinguish between negative (personal pronouns) and
potential positive cases (demonstrative pronouns).
This confirms the hypothesis by Eckert & Strube
(2000) and Byron (2002) to give high priority to
these features. The decision tree fragment in Figure
1 correctly assigns the P label to 23-7=16 sentential
antecedents.
split, n, loss, yval
* denotes terminal node
...
anteexptype=s,vp 1110 55 N
ananpform=prp 747,11 N *
ananpform=dtpro 363 44 N
anteexptype=vp 177 3 N *
anteexptype=s 186 41 N
udist>=1.5 95 14 N *
udist<1.5 91 27 N
wdistic<43.32 33 4 N *
wdistic>=43.32 58 23 N
anasdepth>=2.5 23 4 N *
anasdepth<2.5 35 16 N
wdistic>=63.62 24 11 N
wdistic<80.60 12 3 N *
wdistic>=80.60 12 4 P *
wdistic<63.62 11 3 P *
Figure 1: Decision Tree Fragment
However, the most important problem is the large
amount of pronouns without antecedents. The
model does find (wrong) antecedents for a lot of pro-
nouns which should not have one. Only a small frac-
tion of these pronouns are true expletives (i.e., they
precede a ?weather? verb or are in constructions like
?It seems that . . . ?. The majority of these cases are
referential, but have no antecedent in the data (i.e.,
correct found total found total correct precision recall f-measure
baseline, features 1-16 227 354 1250 64.12 18.16 28.30
plus wdist ic 230 353 1250 65.16 18.40 28.70
Table 6: Results for Third Person Plural Pronouns (3p)
correct found total found total correct precision recall f-measure
baseline, features 1-16 456 739 1250 61.71 36.48 45.85
combined 509 897 1250 56.74 40.72 47.42
Table 7: Combined Results for All Pronouns
they are vague pronouns).
The overall numbers for precision, recall and f-
measure are fairly low. One reason is that we did not
attempt to resolve anaphoric definite NPs and proper
names though these coreference links are contained
in the evaluation key list. If we removed them from
there, the recall of our experiments would approach
the 51% Byron (2002) mentioned for her system us-
ing only domain-independent semantic restrictions.
6 Comparison to Related Work
Our approach for determining the feature set for pro-
noun resolution resembles the so-called wrapper ap-
proach for feature selection (Kohavi & John, 1997).
This is in contrast to the majority of other work on
feature selection for anaphora resolution, which was
hardly ever done systematically. E.g. Soon et al
(2001) only compared baseline systems consisting
of one feature each, only three of which yielded an
f-measure greater than zero. Then they combined
these features and achieved results which were close
to the best overall results they report. While this tells
us which features contribute a lot, it does not give
any information about potential (positive or nega-
tive) influence of the rest. Ng & Cardie (2002) select
the set of features by hand, giving a preference to
high precision features. They admit that this method
is quite subjective.
Corpus-based work about pronoun resolution in
spoken dialogue is almost non-existent. However,
there are a few papers dealing with neuter pronouns
with NP-antecedents. E.g., Dagan & Itai (1991) pre-
sented a corpus-based approach to the resolution of
the pronoun it, but they use a written text corpus and
do not mention non-NP-antecedents at all. Paul et al
(1999) presented a corpus-based anaphora resolu-
tion algorithm for spoken dialogue. For their exper-
iments, however, they restricted anaphoric relations
to those with NP-antecedents.
Byron (2002) presented a symbolic approach
which resolves pronouns with NP- and non-NP-
antecedents in spoken dialogue in the TRAINS do-
main. Byron extends a pronoun resolution al-
gorithm (Tetrault, 2001) with semantic filtering,
thus enabling it to resolve anaphors with non-NP-
antecedents as well. Semantic filtering relies on
knowledge about semantic restrictions associated
with verbs, like semantic compatibility between sub-
ject and predicative noun or predicative adjective.
An evaluation on ten TRAINS93 dialogues with
80 3rd person pronouns and 100 demonstrative pro-
nouns shows that semantic filtering and the im-
plementation of different search strategies for per-
sonal and demonstrative pronouns yields a suc-
cess rate of 72%. As Byron admits, the ma-
jor limitation of her algorithm is its dependence
on domain-dependent resources which cover the
domain entirely. When evaluating her algorithm
with only domain-independent semantics, Byron
achieved 51% success rate. What is problematic
with her approach is that she assumes the input to
her algorithm to be only referential pronouns. This
simplifies the task considerably.
7 Conclusions and Future Work
We presented a machine learning approach to pro-
noun resolution in spoken dialogue. We built upon
a system we used for anaphora resolution in writ-
ten text and extended it with a set of features de-
signed for spoken dialogue. We refined distance
features and used metrics from information retrieval
for determining non-NP-antecedents. Inspired by
the more linguistically oriented work by Eckert &
Strube (2000) and Byron (2002) we also evaluated
the contribution of features which used the predica-
tive context of the pronoun to be resolved. However,
these features did not show up in the final models
since they did not lead to an improvement. Instead,
rather simple distance metrics were preferred. While
we were (almost) satisfied with the performance of
these features, the major problem for a spoken dia-
logue pronoun resolution algorithm is the abundance
of pronouns without antecedents. Previous research
could avoid dealing with this phenomenon by either
applying the algorithm by hand (Eckert & Strube,
2000) or excluding these cases (Byron, 2002) from
the evaluation. Because we included these cases
in our evaluation we consider our approach at least
comparable to Byron?s system when she uses only
domain-independent semantics. We believe that our
system is more robust than hers and that it can more
easily be ported to new domains.
Acknowledgements. The work presented here has
been partially funded by the German Ministry of
Research and Technology as part of the EMBASSI
project (01 IL 904 D/2) and by the Klaus Tschira
Foundation. We would like to thank Susanne
Wilhelm and Lutz Wind for doing the annota-
tions, Kerstin Schu?rmann, Torben Pastuch and Klaus
Rothenha?usler for helping with the data prepara-
tion.
References
Asher, Nicholas (1993). Reference to Abstract Objects in Dis-
course. Dordrecht, The Netherlands: Kluwer.
Breiman, Leo, Jerome H. Friedman, Charles J. Stone & R.A.
Olshen (1984). Classification and Regression Trees. Bel-
mont, Cal.: Wadsworth and Brooks/Cole.
Briscoe, Ted & John Carroll (1997). Automatic extraction
of subcategorization from corpora. In Proceedings of the
5th Conference on Applied Natural Language Processing,
Washington, D.C., 31 March ? 3 April 1997, pp. 356?363.
Byron, Donna K. (2002). Resolving pronominal reference to
abstract entities. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pp. 80?87.
Byron, Donna K. & James F. Allen (1998). Resolving demon-
strative pronouns in the TRAINS93 corpus. In New Ap-
proaches to Discourse Anaphora: Proceedings of the Sec-
ond Colloquium on Discourse Anaphora and Anaphor Res-
olution (DAARC2), pp. 68?81.
Dagan, Ido & Alon Itai (1991). A statistical filter for resolv-
ing pronoun references. In Y.A. Feldman & A. Bruckstein
(Eds.), Artificial Intelligence and Computer Vision, pp. 125?
135. Amsterdam: Elsevier.
Eckert, Miriam & Michael Strube (2000). Dialogue acts, syn-
chronising units and anaphora resolution. Journal of Seman-
tics, 17(1):51?89.
Ihaka, Ross & Robert Gentleman (1996). R: A language for
data analysis and graphics. Journal of Computational and
Graphical Statistics, 5:299?314.
Kohavi, Ron & George H. John (1997). Wrappers for fea-
ture subset selection. Artificial Intelligence Journal, 97(1-
2):273?324.
Mu?ller, Christoph, Stefan Rapp & Michael Strube (2002). Ap-
plying Co-Training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7?12 July 2002, pp.
352?359.
Ng, Vincent & Claire Cardie (2002). Improving machine learn-
ing approaches to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia, Penn., 7?12 July 2002, pp.
104?111.
Paul, Michael, Kazuhide Yamamoto & Eiichiro Sumita (1999).
Corpus-based anaphora resolution towards antecedent pref-
erence. In Proc. of the 37th ACL, Workshop Coreference and
Its Applications, College Park, Md., 1999, pp. 47?52.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong Lim
(2001). A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27(4):521?
544.
Strube, Michael, Stefan Rapp & Christoph Mu?ller (2002). The
influence of minimum edit distance on reference resolution.
In Proceedings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing, Philadelphia, Pa., 6?7
July 2002, pp. 312?319.
Tetrault, Joel R. (2001). A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Linguistics,
27(4):507?520.
Therneau, Terry M. & Elizabeth J. Atkinson (1997). An intro-
duction to recursive partitioning using the RPART routines.
Technical Report: Mayo Foundation. Distributed with the
RPART package.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly &
Lynette Hirschman (1995). A model-theoretic coreference
scoring scheme. In Proceedings of the 6th Message Under-
standing Conference (MUC-6), pp. 45?52. San Mateo, Cal.:
Morgan Kaufmann.
Webber, Bonnie L. (1991). Structure and ostension in the inter-
pretation of discourse deixis. Language and Cognitive Pro-
cesses, 6(2):107?135.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 109?112, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Flexible Stand-Off Data Model with Query Language
for Multi-Level Annotation
Christoph Mu?ller
EML Research gGmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
mueller@eml-research.de
Abstract
We present an implemented XML data model and a
new, simplified query language for multi-level an-
notated corpora. The new query language involves
automatic conversion of queries into the underly-
ing, more complicated MMAXQL query language.
It supports queries for sequential and hierarchical,
but also associative (e.g. coreferential) relations.
The simplified query language has been designed
with non-expert users in mind.
1 Introduction
Growing interest in richly annotated corpora is a
driving force for the development of annotation tools
that can handle multiple levels of annotation. We
find it crucial in order to make full use of the po-
tential of multi-level annotation that individual an-
notation levels be treated as self-contained modules
which are independent of other annotation levels.
This independence should also include the storing
of each level in a separate file. If these principles are
observed, annotation data management (incl. level
addition, removal and replacement, but also conver-
sion into and from other formats) is greatly facili-
tated.
The way to keep individual annotation levels in-
dependent of each other is by defining each with
direct reference to the underlying basedata, i.e. the
text or transcribed speech. Both sequential and hi-
erarchical (i.e. embedding or dominance) relations
between markables on different levels are thus only
expressed implicitly, viz. by means of the relations
of their basedata elements.
While it has become common practice to use
the stand-off mechanism to relate several annota-
tion levels to one basedata file, it is also not un-
common to find this mechanism applied for relating
markables to other markables (on a different or the
same level) directly, expressing the relation between
them explicitly. We argue that this is unfavourable
not only with respect to annotation data management
(cf. above), but also with respect to querying: Users
should not be required to formulate queries in terms
of structural properties of data representation that
are irrelevant for their query. Instead, users should
be allowed to relate markables from all levels in a
fairly unrestricted and ad-hoc way. Since querying is
thus considerably simplified, exploratory data analy-
sis of annotated corpora is facilitated for all users,
including non-experts.
Our multi-level annotation tool MMAX21
(Mu?ller & Strube, 2003) uses implicit relations
only. Its query language MMAXQL is rather
complicated and not suitable for naive users. We
present an alternative query method consisting of
a simpler and more intuitive query language and
a method to generate MMAXQL queries from the
former. The new, simplified MMAXQL can express
a wide range of queries in a concise way, including
queries for associative relations representing e.g.
coreference.
2 The Data Model
We propose a stand-off data model implemented in
XML. The basedata is stored in a simple XML file
1The current release version of MMAX2 can be downloaded
at http://mmax.eml-research.de.
109
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE words SYSTEM "words.dtd">
<words>
...
<word id="word_1064">My</word>
<word id="word_1065">,</word>
<word id="word_1066">uh</word>
<word id="word_1067">,</word>
<word id="word_1068">cousin</word>
<word id="word_1069">is</word>
<word id="word_1070">a</word>
<word id="word_1071">F</word>
<word id="word_1072">B</word>
<word id="word_1073">I</word>
<word id="word_1074">agent</word>
<word id="word_1075">down</word>
<word id="word_1076">in</word>
<word id="word_1077">Miami</word>
<word id="word_1078">.</word>
...
<word id="word_1085">she</word>
...
</words>
Figure 1: basedata file (extract)
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE markables SYSTEM "markables.dtd">
<markables xmlns="www.eml.org/NameSpaces/utterances">
...
<markable id="markable_116" span="word_1064..word_1078"/>
...
</markables>
Figure 2: utterances level file (extract)
which serves to identify individual tokens2 and as-
sociate an ID with each (Figure 1).
In addition, there is one XML file for each an-
notation level. Each level has a unique, descriptive
name, e.g. utterances or pos, and contains an-
notations in the form of <markable> elements.
In the most simple case, a markable only identifies
a sequence (i.e. span) of basedata elements (Figure
2).
Normally, however, a markable is also associated
with arbitrarily many user-defined attribute-value
pairs (Figure 3, Figure 4). Markables can also be
discontinuous, like markable 954 in Figure 4.
For each level, admissible attributes and their val-
ues are defined in a separate annotation scheme file
(not shown, cf. Mu?ller & Strube (2003)). Freetext
attributes can have any string value, while nominal
attributes can have one of a (user-defined) closed set
of possible values. The data model also supports
associative relations between markables: Markable
set relations associate arbitrarily many markables
with each other in a transitive, undirected way. The
coref class attribute in Figure 4 is an exam-
ple of how such a relation can be used to represent
a coreferential relation between markables (here:
markable 954 and markable 963, rest of set
2Usually words, but smaller elements like morphological
units or even characters are also possible.
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE markables SYSTEM "markables.dtd">
<markables xmlns="www.eml.org/NameSpaces/pos">
...
<markable id="markable_665" span="word_1064" pos="PRP$"/>
<markable id="markable_666" span="word_1065" pos=","/>
<markable id="markable_667" span="word_1066" pos="UH"/>
<markable id="markable_668" span="word_1067" pos=","/>
<markable id="markable_669" span="word_1068" pos="NN"/>
<markable id="markable_670" span="word_1069" pos="VBZ"/>
<markable id="markable_671" span="word_1070" pos="DT"/>
<markable id="markable_672" span="word_1071" pos="NNP"/>
<markable id="markable_673" span="word_1072" pos="NNP"/>
<markable id="markable_674" span="word_1073" pos="NNP"/>
<markable id="markable_675" span="word_1074" pos="NN"/>
<markable id="markable_676" span="word_1075" pos="IN"/>
<markable id="markable_677" span="word_1076" pos="IN"/>
<markable id="markable_678" span="word_1077" pos="NNP"/>
<markable id="markable_679" span="word_1078" pos="."/>
...
<markable id="markable_686" span="word_1085" pos="PRP"/>
...
</markables>
Figure 3: pos level file (extract)
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE markables SYSTEM "markables.dtd">
<markables xmlns="www.eml.org/NameSpaces/ref_exp">
...
<markable id="markable_953" span="word_1064" type="poss_det"/>
<markable id="markable_954" span="word_1064,word_1068" type="np"
coref_class="set_3"/>
<markable id="markable_955" span="word_1070..word_1074" type="np"/>
<markable id="markable_956" span="word_1071..word_1073" type="pn"/>
<markable id="markable_957" span="word_1077" type="pn"/>
...
<markable id="markable_963" span="word_1085" type="pron"
coref_class="set_3"/>
...
</markables>
Figure 4: ref exp level file (extract)
not shown). Markable pointer relations associate
with one markable (the source) one or more target
markables in an intransitive, directed fashion.
3 Simplified MMAXQL
Simplified MMAXQL is a variant of the MMAXQL
query language. It offers a simpler and more con-
cise way to formulate certain types of queries for
multi-level annotated corpora. Queries are automat-
ically converted into the underlying query language
and then executed. A query in simplified MMAXQL
consists of a sequence of query tokens which are
combined by means of relation operators. Each
query token queries exactly one basedata element
(i.e. word) or one markable.
3.1 Query Tokens
Basedata elements can be queried by matching reg-
ular expressions. Each basedata query token con-
sists of a regular expression in single quotes, which
must exactly match one basedata element. The query
?[Tt]he?
matches all definite articles, but not e.g. ether or
110
there. For the latter two words to also match, wild-
cards have to be used:
?.*[Tt]he.*?
Sequences of basedata elements can be queried by
simply concatenating several space-separated3 to-
kens. The query
?[Tt]he [A-Z].+?
will match sequences consisting of a definite article
and a word beginning with a capital letter.
Markables are the carriers of the actual annota-
tion information. They can be queried by means
of string matching and by means of attribute-value
combinations. A markable query token has the form
string/conditions
where string is an optional regular expression
and conditions specifies which attribute(s) the
markable should match. The most simple ?condi-
tion? is just the name of a markable level, which will
match all markables on that level. If a regular ex-
pression is also supplied, the query will return only
the matching markables. The query
[Aa]n?\s.*/ref exp4
will return all markables from the ref exp level
beginning with the indefinite article.
The conditions part of a markable query to-
ken can indeed be much more complex. A main
feature of simplified MMAXQL is that redundant
parts of conditions can optionally be left out, mak-
ing queries very concise. For example, the mark-
able level name can be left out if the name of the
attribute accessed by the query is unique across all
active markable levels. Thus, the query
/!coref class=empty
can be used to query markables from the ref exp
level which have a non-empty value in the
coref class attribute, granted that only one at-
tribute of this name exists.5 The same applies to the
names of nominal attributes if the value specified
in the query unambiguously points to this attribute.
Thus, the query
/pn
3Using the fact that meets is the default relation operator,
cf. Section 3.2.
4The space character in the regular expression must be
masked as \s because otherwise it will be interpreted as a query
token separator.
5If this condition does not hold, attribute names can be dis-
ambiguated by prepending the markable level name.
can be used to query markables from the pos level
which have the value pn, granted that there is ex-
actly one nominal attribute with the possible value
pn. Several conditions can be combined into one
query token. Thus, the query
/{poss det,pron},!coref class=empty
returns all markables from the ref exp level that
are either possessive determiners or pronouns and
that are part in some coreference set.6
3.2 Relation Operators
The whole point of querying corpora with multi-
level annotation is to relate markables from different
levels to each other. The reference system with re-
spect to which the relation between different mark-
ables is established is the sequence of basedata el-
ements, which is the same for all markables on all
levels. Since this bears some resemblance to differ-
ent events occurring in several temporal relations to
each other, we (like also Heid et al (2004), among
others) adopt this as a metaphor for expressing
the sequential and hierarchical relations between
markables, and we use a set of relation operators
that is inspired by (Allen, 1991). This set includes
(among others) the operators before, meets (de-
fault), starts, during/in, contains/dom,
equals, ends, and some inverse relations. The
following examples give an idea of how individual
query tokens can be combined by means of rela-
tion operators to form complex queries. The exam-
ple uses the ICSI meeting corpus of spoken multi-
party dialogue.7 This corpus contains, among oth-
ers, a segment level with markables roughly corre-
sponding to speaker turns, and a meta level contain-
ing markables representing e.g. pauses, emphases,
or sounds like breathing or mike noise. These two
levels and the basedata level can be combined to re-
trieve instances of you know that occur in segments
spoken by female speakers8 which also contain a
pause or an emphasis:
?[Yy]ou know? in (/participant={f.*} dom /{pause,emphasis})
6The curly braces notation is used to specify several OR-
connected values for a single attribute, while a comma outside
curly braces is used to AND-connect several conditions relating
to different attributes.
7Obtained from the LDC and converted into MMAX2 for-
mat, preserving all original information.
8The first letter of the participant value encodes the
speaker?s gender.
111
Relation operators for associative relations (i.e.
markable set and markable pointer) are nextpeer,
anypeer and nexttarget, anytarget, re-
spectively. Assuming the sample data from Section
2, the query
/ref_exp nextpeer:coref_class /ref_exp
retrieves pairs of anaphors (right) and their direct an-
tecedents (left). The query can be modified to
/ref_exp nextpeer:coref_class (/ref_exp equals /pron)
to retrieve only anaphoric pronouns and their direct
antecedents.
If a query is too complex to be expressed as a sin-
gle query token sequence, variables can be used to
store intermediate results of sub-queries. The fol-
lowing query retrieves pairs of utterances (incl. the
referring expressions embedded into them) that are
more than 30 tokens9 apart, and assigns the resulting
4-tuples to the variable $distant utts.
(/utterances dom /ref_exp) before:31- (/utterances dom /ref_exp)
-> $distant_utts
The next query accesses the second and last column
in the temporary result (by means of the zero-based
column index) and retrieves those pairs of anaphors
and their direct antecedents that occur in utterances
that are more than 30 tokens apart:
$distant_utts.1 nextpeer:coref_class $distant_utts.3
4 Related Work
In the EMU speech database system (Cassidy &
Harrington, 2001) the hierarchical relation between
levels has to be made explicit. Sequential and hi-
erarchical relations can be queried like with simpli-
fied MMAXQL, with the difference that e.g. for se-
quential queries, the elements involved must come
from the same level. Also, the result of a hierarchi-
cal query always only contains either the parent or
child element. The EMU data model supports an as-
sociation relation (similar to our markable pointer)
which can be queried using a => operator.
Annotation Graphs (Bird & Liberman, 2001)
identify elements on various levels as arcs connect-
ing two points on a time scale shared by all lev-
els. Relations between elements are thus also rep-
resented implicitly. The model can also express a
9A means to express distance in terms of markables is not
yet available, cf. Section 5.
binary association relation. The associated Annota-
tion Graph query language (Bird et al, 2000) is very
explicit, which makes it powerful but at the same
time possibly too demanding for naive users.
The NITE XML toolkit (Carletta et al, 2003) de-
fines a data model that is close to our model, al-
though it allows to express hierarchical relations ex-
plicitly. The model supports a labelled pointer re-
lation which can express one-to-many associations.
The associated query language NXT Search (Heid
et al, 2004) is a powerful declarative language for
querying diverse relations (incl. pointers), support-
ing quantification and constructs like forall and
exists.
5 Future Work
We work on support for queries like ?pairs of re-
ferring expressions that are a certain number of re-
ferring expressions apart?. We also want to include
wild cards and proximity searches, and support for
automatic markable creation from query results.
Acknowledgements
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany.
References
Allen, James (1991). Time and time again. International Jour-
nal of Intelligent Systems, 6(4):341?355.
Bird, Steven, Peter Buneman & Wang-Chiew Tan (2000). To-
wards a query language for annotation graphs. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation, Athens, Greece, 31
May-June 2, 2000, pp. 807?814.
Bird, Steven & Mark Liberman (2001). A formal framework for
linguistic annotation. Speech Communication, 33:23?60.
Carletta, Jean, Stefan Evert, Ulrich Heid, Jonathan Kilgour,
J. Robertson & Holger Voormann (2003). The NITE
XML toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instruments,
and Computers, 35:353?363.
Cassidy, Steve & Jonathan Harrington (2001). Multi-level anno-
tation in the EMU speech database management system.
Speech Communication, 33:61?78.
Heid, Ulrich, Holger Voormann, Jan-Torsten Milde, Ulrike Gut,
Katrin Erk & Sebastian Pado (2004). Querying both time-
aligned and hierarchical corpora with NXT search. In
Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, Lisbon, Portugal, 26-28
May, 2004, pp. 1455?1458.
Mu?ller, Christoph & Michael Strube (2003). Multi-level an-
notation in MMAX. In Proceedings of the 4th SIGdial
Workshop on Discourse and Dialogue, Sapporo, Japan,
4-5 July 2003, pp. 198?207.
112
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 816?823,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Resolving It, This, and That in Unrestricted Multi-Party Dialog
Christoph Mu?ller
EML Research gGmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
christoph.mueller@eml-research.de
Abstract
We present an implemented system for the
resolution of it, this, and that in tran-
scribed multi-party dialog. The system han-
dles NP-anaphoric as well as discourse-
deictic anaphors, i.e. pronouns with VP an-
tecedents. Selectional preferences for NP or
VP antecedents are determined on the basis
of corpus counts. Our results show that the
system performs significantly better than a
recency-based baseline.
1 Introduction
This paper describes a fully automatic system for
resolving the pronouns it, this, and that in unre-
stricted multi-party dialog. The system processes
manual transcriptions from the ICSI Meeting Cor-
pus (Janin et al, 2003). The following is a short
fragment from one of these transcripts. The letters
FN in the speaker tag mean that the speaker is a fe-
male non-native speaker of English. The brackets
and subscript numbers are not part of the original
transcript.
FN083: Maybe you can also read through the - all the text
which is on the web pages cuz I?d like to change the text
a bit cuz sometimes [it]1?s too long, sometimes [it]2?s too
short, inbreath maybe the English is not that good, so in-
breath um, but anyways - So I tried to do [this]3 today
and if you could do [it]4 afterwards [it]5 would be really
nice cuz I?m quite sure that I can?t find every, like, ortho-
graphic mistake in [it]6 or something. (Bns003)
For each of the six 3rd-person pronouns in the exam-
ple, the task is to automatically identify its referent,
i.e. the entity (if any) to which the speaker makes
reference. Once a referent has been identified, the
pronoun is resolved by linking it to one of its an-
tecedents, i.e. one of the referent?s earlier mentions.
For humans, identification of a pronoun?s referent
is often easy: it1, it2, and it6 are probably used to
refer to the text on the web pages, while it4 is prob-
ably used to refer to reading this text. Humans also
have no problem determining that it5 is not a normal
pronoun at all. In other cases, resolving a pronoun
is difficult even for humans: this3 could be used to
refer to either reading or changing the text on the
web pages. The pronoun is ambiguous because evi-
dence for more than one interpretation can be found.
Ambiguous pronouns are common in spoken dialog
(Poesio & Artstein, 2005), a fact that has to be taken
into account when building a spoken dialog pronoun
resolution system. Our system is intended as a com-
ponent in an extractive dialog summarization sys-
tem. There are several ways in which coreference in-
formation can be integrated into extractive summa-
rization. Kabadjov et al (2005) e.g. obtained their
best extraction results by specifying for each sen-
tence whether it contained a mention of a particular
anaphoric chain. Apart from improving the extrac-
tion itself, coreference information can also be used
to substitute anaphors with their antecedents, thus
improving the readability of a summary by minimiz-
ing the number of dangling anaphors, i.e. anaphors
whose antecedents occur in utterances that are not
part of the summary. The paper is structured as fol-
lows: Section 2 outlines the most important chal-
lenges and the state of the art in spoken dialog pro-
noun resolution. Section 3 describes our annotation
experiments, and Section 4 describes the automatic
816
dialog preprocessing. Resolution experiments and
results can be found in Section 5.
2 Pronoun Resolution in Spoken Dialog
Spoken language poses some challenges for pro-
noun resolution. Some of these arise from nonrefer-
ential resp. nonresolvable pronouns, which are im-
portant to identify because failure to do so can harm
pronoun resolution precision. One common type
of nonreferential pronoun is pleonastic it. Another
cause of nonreferentiality that only applies to spoken
language is that the pronoun is discarded, i.e. it is
part of an incomplete or abandoned utterance. Dis-
carded pronouns occur in utterances that are aban-
doned altogether.
ME010: Yeah. Yeah. No, no. There was a whole co- There
was a little contract signed. It was - Yeah. (Bed017)
If the utterance contains a speech repair (Heeman &
Allen, 1999), a pronoun in the reparandum part is
also treated as discarded because it is not part of the
final utterance.
ME10: That?s - that?s - so that?s a - that?s a very good question,
then - now that it - I understand it. (Bro004)
In the corpus of task-oriented TRAINS dialogs de-
scribed in Byron (2004), the rate of discarded pro-
nouns is 7 out of 57 (12.3%) for it and 7 out of
100 (7.0%) for that. Schiffman (1985) reports that
in her corpus of career-counseling interviews, 164
out of 838 (19.57%) instances of it and 80 out of
582 (13.75%) instances of that occur in abandoned
utterances.
There is a third class of pronouns which is referen-
tial but nonetheless unresolvable: vague pronouns
(Eckert & Strube, 2000) are characterized by having
no clearly defined textual antecedent. Rather, vague
pronouns are often used to refer to the topic of the
current (sub-)dialog as a whole.
Finally, in spoken language the pronouns it, this, and
that are often discourse deictic (Webber, 1991), i.e.
they are used to refer to an abstract object (Asher,
1993). We treat as abstract objects all referents of
VP antecedents, and do not distinguish between VP
and S antecedents.
ME013: Well, I mean there?s this Cyber Transcriber service,
right?
ME025: Yeah, that?s true, that?s true. (Bmr001)
Discourse deixis is very frequent in spoken dialog:
The rate of discourse deictic expressions reported in
Eckert & Strube (2000) is 11.8% for pronouns and
as much as 70.9% for demonstratives.
2.1 State of the Art
Pronoun resolution in spoken dialog has not received
much attention yet, and a major limitation of the few
implemented systems is that they are not fully au-
tomatic. Instead, they depend on manual removal
of unresolvable pronouns like pleonastic it and dis-
carded and vague pronouns, which are thus pre-
vented from triggering a resolution attempt. This
eliminates a major source of error, but it renders the
systems inapplicable in a real-world setting where
no such manual preprocessing is feasible.
One of the earliest empirically based works adress-
ing (discourse deictic) pronoun resolution in spo-
ken dialog is Eckert & Strube (2000). The au-
thors outline two algorithms for identifying the an-
tecedents of personal and demonstrative pronouns in
two-party telephone conversations from the Switch-
board corpus. The algorithms depend on two non-
trivial types of information: the incompatibility of
a given pronoun with either concrete or abstract an-
tecedents, and the structure of the dialog in terms of
dialog acts. The algorithms are not implemented,
and Eckert & Strube (2000) report results of the
manual application to a set of three dialogs (199 ex-
pressions, including other pronouns than it, this, and
that). Precision and recall are 66.2 resp. 68.2 for
pronouns and 63.6 resp. 70.0 for demonstratives.
An implemented system for resolving personal and
demonstrative pronouns in task-oriented TRAINS
dialogs is described in Byron (2004). The system
uses an explicit representation of domain-dependent
semantic category restrictions for predicate argu-
ment positions, and achieves a precision of 75.0 and
a recall of 65.0 for it (50 instances) and a precision
of 67.0 and a recall of 62.0 for that (93 instances)
if all available restrictions are used. Precision drops
to 52.0 for it and 43.0 for that when only domain-
independent restrictions are used.
To our knowledge, there is only one implemented
system so far that resolves normal and discourse de-
ictic pronouns in unrestricted spoken dialog (Strube
& Mu?ller, 2003). The system runs on dialogs from
the Switchboard portion of the Penn Treebank. For
817
it, this and that, the authors report 40.41 precision
and 12.64 recall. The recall does not reflect the ac-
tual pronoun resolution performance as it is calcu-
lated against all coreferential links in the corpus, not
just those with pronominal anaphors. The system
draws some non-trivial information from the Penn
Treebank, including correct NP chunks, grammati-
cal function tags (subject, object, etc.) and discarded
pronouns (based on the -UNF-tag). The treebank
information is also used for determining the acces-
sibility of potential candidates for discourse deictic
pronouns.
In contrast to these approaches, the work described
in the following is fully automatic, using only infor-
mation from the raw, transcribed corpus. No manual
preprocessing is performed, so that during testing,
the system is exposed to the full range of discarded,
pleonastic, and other unresolvable pronouns.
3 Data Collection
The ICSI Meeting Corpus (Janin et al, 2003) is
a collection of 75 manually transcribed group dis-
cussions of about one hour each, involving three
to ten speakers. A considerable number of partic-
ipants are non-native speakers of English, whose
proficiency is sometimes poor, resulting in disflu-
ent or incomprehensible speech. The discussions are
real, unstaged meetings on various, technical topics.
Most of the discussions are regular weekly meet-
ings of a quite informal conversational style, con-
taining many interrupts, asides, and jokes (Janin,
2002). The corpus features a semi-automatically
generated segmentation in which each segment is as-
sociated with a speaker tag and a start and end time
stamp. Time stamps on the word level are not avail-
able. The transcription contains capitalization and
punctuation, and it also explicitly records interrup-
tion points and word fragments (Heeman & Allen,
1999), but not the extent of the related disfluencies.
3.1 Annotation
The annotation was done by naive project-external
annotators, two non-native and two native speak-
ers of English, with the annotation tool MMAX21
on five randomly selected dialogs2. The annotation
1http://mmax.eml-research.de
2Bed017, Bmr001, Bns003, Bro004, and Bro005.
instructions were deliberately kept simple, explain-
ing and illustrating the basic notions of anaphora
and discourse deixis, and describing how markables
were to be created and linked in the annotation tool.
This practice of using a higher number of naive ?
rather than fewer, highly trained ? annotators was
motivated by our intention to elicit as many plau-
sible interpretations as possible in the presence of
ambiguity. It was inspired by the annotation ex-
periments of Poesio & Artstein (2005) and Artstein
& Poesio (2006). Their experiments employed up
to 20 annotators, and they allowed for the explicit
annotation of ambiguity. In contrast, our annota-
tors were instructed to choose the single most plau-
sible interpretation in case of perceived ambigu-
ity. The annotation covered the pronouns it, this,
and that only. Markables for these tokens were
created automatically. From among the pronomi-
nal3 instances, the annotators then identified normal,
vague, and nonreferential pronouns. For normal pro-
nouns, they also marked the most recent antecedent
using the annotation tool?s coreference annotation
function. Markables for antecedents other than it,
this, and that had to be created by the annotators
by dragging the mouse over the respective words
in the tool?s GUI. Nominal antecedents could be ei-
ther noun phrases (NP) or pronouns (PRO). VP an-
tecedents (for discourse deictic pronouns) spanned
only the verb phrase head, i.e. the verb, not the en-
tire phrase. By this, we tried to reduce the number
of disagreements caused by differing markable de-
marcations. The annotation of discourse deixis was
limited to cases where the antecedent was a finite or
infinite verb phrase expressing a proposition, event
type, etc.4
3.2 Reliability
Inter-annotator agreement was checked by comput-
ing the variant of Krippendorff?s ? described in Pas-
sonneau (2004). This metric requires all annotations
to contain the same set of markables, a condition
that is not met in our case. Therefore, we report
? values computed on the intersection of the com-
3The automatically created markables included all instances
of this and that, i.e. also relative pronouns, determiners, com-
plementizers, etc.
4Arbitrary spans of text could not serve as antecedents for
discourse deictic pronouns. The respective pronouns were to be
treated as vague, due to lack of a well-defined antecedent.
818
pared annotations, i.e. on those markables that can
be found in all four annotations. Only a subset of
the markables in each annotation is relevant for the
determination of inter-annotator agreement: all non-
pronominal markables, i.e. all antecedent markables
manually created by the annotators, and all referen-
tial instances of it, this, and that. The second column
in Table 1 contains the cardinality of the union of
all four annotators? markables, i.e. the number of all
distinct relevant markables in all four annotations.
The third and fourth column contain the cardinality
and the relative size of the intersection of these four
markable sets. The fifth column contains ? calcu-
lated on the markables in the intersection only. The
four annotators only agreed in the identification of
markables in approx. 28% of cases. ? in the five
dialogs ranges from .43 to .52.
| 1 ? 2 ? 3 ? 4 | | 1 ? 2 ? 3 ? 4 | ?
Bed017 397 109 27.46 % .47
Bmr001 619 195 31.50 % .43
Bns003 529 131 24.76 % .45
Bro004 703 142 20.20 % .45
Bro005 530 132 24.91 % .52
Table 1: Krippendorff?s ? for four annotators.
3.3 Data Subsets
In view of the subjectivity of the annotation task,
which is partly reflected in the low agreement even
on markable identification, the manual creation of a
consensus-based gold standard data set did not seem
feasible. Instead, we created core data sets from
all four annotations by means of majority decisions.
The core data sets were generated by automatically
collecting in each dialog those anaphor-antecedent
pairs that at least three annotators identified indepen-
dently of each other. The rationale for this approach
was that an anaphoric link is the more plausible the
more annotators identify it. Such a data set certainly
contains some spurious or dubious links, while lack-
ing some correct but more difficult ones. However,
we argue that it constitutes a plausible subset of
anaphoric links that are useful to resolve.
Table 2 shows the number and lengths of anaphoric
chains in the core data set, broken down accord-
ing to the type of the chain-initial antecedent. The
rare type OTHER mainly contains adjectival an-
tecedents. More than 75% of all chains consist of
two elements only. More than 33% begin with a
pronoun. From the perspective of extractive sum-
marization, the resolution of these latter chains is not
helpful since there is no non-pronominal antecedent
that it can be linked to or substituted with.
length 2 3 4 5 6 > 6 total
Bed017
NP 17 3 2 - 1 - 23
PRO 14 - 2 - - - 16
VP 6 1 - - - - 7
OTHER - - - - - - -
all 37 4 4 - 1 - 4680.44%
Bmr001
NP 14 4 1 1 1 2 23
PRO 19 9 2 2 1 1 34
VP 9 5 - - - - 14
OTHER - - - - - - -
all 42 18 3 3 2 3 7159.16%
Bns003
NP 18 3 3 1 - - 25
PRO 18 1 1 - - - 20
VP 14 4 - - - - 18
OTHER - - - - - - -
all 50 8 4 1 - - 6379.37%
Bro004
NP 38 5 3 1 - - 47
PRO 21 4 - 1 - - 26
VP 8 1 1 - - - 10
OTHER 2 1 - - - - 3
all 69 11 4 2 - - 8680.23%
Bro005
NP 37 7 1 - - - 45
PRO 15 3 1 - - - 19
VP 8 1 - 1 - - 10
OTHER 3 - - - - - 3
all 63 11 2 1 - - 7781.82%
?
NP 124 22 10 3 2 2 163
PRO 87 17 6 3 1 1 115
VP 45 12 1 1 - - 59
OTHER 5 1 - - - - 6
all 261 52 17 7 3 3 34376.01%
Table 2: Anaphoric chains in core data set.
4 Automatic Preprocessing
Data preprocessing was done fully automatically,
using only information from the manual tran-
scription. Punctuation signs and some heuristics
were used to split each dialog into a sequence
of graphemic sentences. Then, a shallow disflu-
ency detection and removal method was applied,
which removed direct repetitions, nonlexicalized
filled pauses like uh, um, interruption points, and
word fragments. Each sentence was then matched
against a list of potential discourse markers (actu-
ally, like, you know, I mean, etc.) If a sentence
contained one or more matches, string variants were
created in which the respective words were deleted.
Each of these variants was then submitted to a parser
trained on written text (Charniak, 2000). The vari-
ant with the highest probability (as determined by
the parser) was chosen. NP chunk markables were
created for all non-recursive NP constituents identi-
819
fied by the parser. Then, VP chunk markables were
created. Complex verbal constructions like MD +
INFINITIVE were modelled by creating markables
for the individual expressions, and attaching them
to each other with labelled relations like INFINI-
TIVE COMP. NP chunks were also attached, using
relations like SUBJECT, OBJECT, etc.
5 Automatic Pronoun Resolution
We model pronoun resolution as binary classifica-
tion, i.e. as the mapping of anaphoric mentions to
previous mentions of the same referent. This method
is not incremental, i.e. it cannot take into account
earlier resolution decisions or any other information
beyond that which is conveyed by the two mentions.
Since more than 75% of the anaphoric chains in our
data set would not benefit from incremental process-
ing because they contain one anaphor only, we see
this limitation as acceptable. In addition, incremen-
tal processing bears the risk of system degradation
due to error propagation.
5.1 Features
In the binary classification model, a pronoun is re-
solved by creating a set of candidate antecedents and
searching this set for a matching one. This search
process is mainly influenced by two factors: ex-
clusion of candidates due to constraints, and selec-
tion of candidates due to preferences (Mitkov, 2002).
Our features encode information relevant to these
two factors, plus more generally descriptive factors
like distance etc. Computation of all features was
fully automatic.
Shallow constraints for nominal antecedents include
number, gender and person incompatibility, embed-
ding of the anaphor into the antecedent, and coar-
gumenthood (i.e. the antecedent and anaphor must
not be governed by the same verb). For VP an-
tecedents, a common shallow constraint is that the
anaphor must not be governed by the VP antecedent
(so-called argumenthood). Preferences, on the other
hand, define conditions under which a candidate
probably is the correct antecedent for a given pro-
noun. A common shallow preference for nomi-
nal antecedents is the parallel function preference,
which states that a pronoun with a particular gram-
matical function (i.e. subject or object) preferably
has an antecedent with a similar function. The sub-
ject preference, in contrast, states that subject an-
tecedents are generally preferred over those with
less salient functions, independent of the grammat-
ical function of the anaphor. Some of our features
encode this functional and structural parallelism, in-
cluding identity of form (for PRO antecedents) and
identity of grammatical function or governing verb.
A more sophisticated constraint on NP an-
tecedents is what Eckert & Strube (2000) call I-
Incompatibility, i.e. the semantic incompatibility of
a pronoun with an individual (i.e. NP) antecedent.
As Eckert & Strube (2000) note, subject pronouns
in copula constructions with adjectives that can only
modify abstract entities (like e.g. true, correct, right)
are incompatible with concrete antecedents like car.
We postulate that the preference of an adjective to
modify an abstract entity (in the sense of Eckert &
Strube (2000)) can be operationalized as the condi-
tional probability of the adjective to appear with a
to-infinitive resp. a that-sentence complement, and
introduce two features which calculate the respec-
tive preference on the basis of corpus5 counts. For
the first feature, the following query is used:
# it (?s|is|was|were) ADJ to
# it (?s|is|was|were) ADJ
According to Eckert & Strube (2000), pronouns that
are objects of verbs which mainly take sentence
complements (like assume, say) exhibit a similar
incompatibility with NP antecedents, and we cap-
ture this with a similar feature. Constraints for
VPs include the following: VPs are inaccessible for
discourse deictic reference if they fail to meet the
right frontier condition (Webber, 1991). We use
a feature which is similar to that used by Strube
& Mu?ller (2003) in that it approximates the right
frontier on the basis of syntactic (rather than dis-
course structural) relations. Another constraint is
A-Incompatibility, i.e. the incompatibility of a pro-
noun with an abstract (i.e. VP) antecedent. Accord-
ing to Eckert & Strube (2000), subject pronouns in
copula constructions with adjectives that can only
modify concrete entities (like e.g. expensive, tasty)
are incompatible with abstract antecedents, i.e. they
5Based on the approx. 250,000,000 word TIPSTER corpus
(Harman & Liberman, 1994).
820
cannot be discourse deictic. The function of this
constraint is already covered by the two corpus-
based features described above in the context of I-
Incompatibility. Another feature, based on Yang
et al (2005), encodes the semantic compatibility
of anaphor and NP antecedent. We operationalize
the concept of semantic compatibility by substitut-
ing the anaphor with the antecedent head and per-
forming corpus queries. E.g., if the anaphor is ob-
ject, the following query6 is used:
# (V|Vs|Ved|Ving) (?|a|an|the|this|that) ANTE+
# (V|Vs|Ved|Ving) (?|the|these|those) ANTES
# (ANTE|ANTES)
If the anaphor is the subject in an adjective cop-
ula construction, we use the following corpus count
to quantify the compatibility between the predi-
cated adjective and the NP antecedent (Lapata et al,
1999):
# ADJ (ANTE|ANTES) + # ANTE (is|was) ADJ+
# ANTES (are|were) ADJ
# ADJ
A third class of more general properties of the po-
tential anaphor-antecedent pair includes the type of
anaphor (personal vs. demonstrative), type of an-
tecedent (definite vs. indefinite noun phrase, pro-
noun, finite vs. infinite verb phrase, etc.). Special
features for the identification of discarded expres-
sions include the distance (in words) to the closest
preceeding resp. following disfluency (indicated in
the transcription as an interruption point, word frag-
ment, or uh resp. um). The relation between po-
tential anaphor and (any type of) antecedent is de-
scribed in terms of distance in seconds7 and words.
For VP antecedents, the distance is calculated from
the last word in the entire phrase, not from the
phrase head. Another feature which is relevant for
dialog encodes whether both expressions are uttered
by the same speaker.
6V is the verb governing the anaphor. Correct inflected
forms were also generated for irregular verbs. ANTE resp.
ANTES is the singular resp. plural head of the antecedent.
7Since the data does not contain word-level time stamps, this
distance is determined on the basis of a simple forced align-
ment. For this, we estimated the number of syllables in each
word on the basis of its vowel clusters, and simply distributed
the known duration of the segment evenly on all words it con-
tains.
5.2 Data Representation and Generation
Machine learning data for training and testing was
created by pairing each anaphor with each of its
compatible potential antecedents within a certain
temporal distance (9 seconds for NP and 7 seconds
for VP antecedents), and labelling the resulting data
instance as positive resp. negative. VP antecedent
candidates were created only if the anaphor was ei-
ther that8 or the object of a form of do.
Our core data set does not contain any nonreferen-
tial pronouns, though the classifier is exposed to the
full range of pronouns, including discarded and oth-
erwise nonreferential ones, during testing. We try
to make the classifier robust against nonreferential
pronouns in the following way: From the manual
annotations, we select instances of it, this, and that
that at least three annotators identified as nonrefer-
ential. For each of these, we add the full range of
all-negative instances to the training data, applying
the constraints mentioned above.
5.3 Evaluation Measure
As Bagga & Baldwin (1998) point out, in an
application-oriented setting, not all anaphoric links
are equally important: If a pronoun is resolved to
an anaphoric chain that contains only pronouns, this
resolution can be treated as neutral because it has
no application-level effect. The common corefer-
ence evaluation measure described in Vilain et al
(1995) is inappropriate in this setting. We calculate
precision, recall and F-measure on the basis of the
following definitions: A pronoun is resolved cor-
rectly resp. incorrectly only if it is linked (directly
or transitively) to the correct resp. incorrect non-
pronominal antecedent. Likewise, the number of
maximally resolvable pronouns in the core data set
(i.e. the evaluation key) is determined by consider-
ing only pronouns in those chains that do not begin
with a pronoun. Note that our definition of precision
is stricter (and yields lower figures) than that ap-
plied in the ACE context, as the latter ignores incor-
rect links between two expressions in the response
8It is a common observation that demonstratives (in partic-
ular that) are preferred over it for discourse deictic reference
(Schiffman, 1985; Webber, 1991; Asher, 1993; Eckert & Strube,
2000; Byron, 2004; Poesio & Artstein, 2005). This preference
can also be observed in our core data set: 44 out of 59 VP an-
tecedents (69.49%) are anaphorically referred to by that.
821
if these expressions happen to be unannotated in the
key, while we treat them as precision errors unless
the antecedent is a pronoun. The same is true for
links in the response that were identified by less than
three annotators in the key. While it is practical to
treat those links as wrong, it is also simplistic be-
cause it does not do justice to ambiguous pronouns
(cf. Section 6).
5.4 Experiments and Results
Our best machine learning results were obtained
with the Weka9 Logistic Regression classifier.10 All
experiments were performed with dialog-wise cross-
validation. For each run, training data was created
from the manually annotated markables in four di-
alogs from the core data set, while testing was per-
formed on the automatically detected chunks in the
remaining fifth dialog. For training and testing, the
person, number11, gender, and (co-)argument con-
straints were used. If an anaphor gave rise to a pos-
itive instance, no negative training instances were
created beyond that instance. If a referential anaphor
did not give rise to a positive training instance (be-
cause its antecedent fell outside the search scope
or because it was removed by a constraint), no in-
stances were created for that anaphor. Instances for
nonreferential pronouns were added to the training
data as described in Section 5.2.
During testing, we select for each potential anaphor
the positive antecedent with the highest overall con-
fidence. Testing parameters include it-filter,
which switches on and off the module for the detec-
tion of nonreferential it described in Mu?ller (2006).
When evaluated alone, this module yields a preci-
sion of 80.0 and a recall of 60.9 for the detection
of pleonastic and discarded it in the five ICSI di-
alogs. For training, this module was always on.
We also vary the parameter tipster, which con-
trols whether or not the corpus frequency features
are used. If tipster is off, we ignore the corpus
frequency features both during training and testing.
We first ran a simple baseline system which re-
solved pronouns to their most recent compatible an-
tecedent, applying the same settings and constraints
9http://www.cs.waikato.ac.nz/ml/weka/
10The full set of experiments is described in Mu?ller (2007).
11The number constraint applies to it only, as this and that
can have both singular and plural antecedents (Byron, 2004).
as for testing (cf. above). The results can be found
in the first part of Table 3. Precision, recall and F-
measure are provided for ALL and for NP and VP
antecedents individually. The parameter tipster
is not available for the baseline system. The best
baseline performance is precision 4.88, recall 20.06
and F-measure 7.85 in the setting with it-filter
on. As expected, this filter yields an increase in pre-
cision and a decrease in recall. The negative effect
is outweighed by the positive effect, leading to a
small but insignificant12 increase in F-measure for
all types of antecedents.
Baseline Logistic Regression
Setting Ante P R F P R F
-it-filter
-tipster
NP 4.62 27.12 7.90 18.53 20.34 19.39?
VP 1.72 2.63 2.08 13.79 10.53 11.94
ALL 4.40 20.69 7.25 17.67 17.56 17.61?
+tipster
NP - - - 19.33 22.03 20.59???
VP - - - 13.43 11.84 12.59
ALL - - - 18.16 19.12 18.63??
+it-filter
-tipster
NP 5.18 26.27 8.65 17.87 17.80 17.83?
VP 1.77 2.63 2.12 13.12 10.53 11.68
ALL 4.88 20.06 7.85 16.89 15.67 16.26?
+tipster
NP - - - 20.82 21.61 21.21??
VP - - - 11.27 10.53 10.88
ALL - - - 18.67 18.50 18.58??
Table 3: Resolution results.
The second part of Table 3 shows the results of the
Logistic Regression classifier. When compared to
the best baseline, the F-measures are consistently
better for NP, VP, and ALL. The improvement is
(sometimes highly) significant for NP and ALL, but
never for VP. The best F-measure for ALL is 18.63,
yielded by the setting with it-filter off and
tipster on. This setting also yields the best F-
measure for VP and the second best for NP. The
contribution of the it-filter is disappointing: In both
tipster settings, the it-filter causes F-measure for
ALL to go down. The contribution of the corpus
features, on the other hand, is somewhat inconclu-
sive: In both it-filter settings, they cause an in-
crease in F-measure for ALL. In the first setting, this
increase is accompanied by an increase in F-measure
for VP, while in the second setting, F-measure for
VP goes down. It has to be noted, however, that
none of the improvements brought about by the it-
filter or the tipster corpus features is statistically sig-
nificant. This also confirms some of the findings of
Kehler et al (2004), who found features similar to
12Significance of improvement in F-measure is tested using
a paired one-tailed t-test and p <= 0.05 (?), p <= 0.01 (??),
and p <= 0.005 (???).
822
our tipster corpus features not to be significant for
NP-anaphoric pronoun resolution in written text.
6 Conclusions and Future Work
The system described in this paper is ? to our knowl-
edge ? the first attempt towards fully automatic res-
olution of NP-anaphoric and discourse deictic pro-
nouns (it, this, and that) in multi-party dialog. Un-
like other implemented systems, it is usable in a re-
alistic setting because it does not depend on manual
pronoun preselection or non-trivial discourse struc-
ture or domain knowledge. The downside is that,
at least in our strict evaluation scheme, the perfor-
mance is rather low, especially when compared to
that of state-of-the-art systems for pronoun resolu-
tion in written text. In future work, it might be
worthwhile to consider less rigorous and thus more
appropriate evaluation schemes in which links are
weighted according to how many annotators identi-
fied them.
In its current state, the system only processes man-
ual dialog transcripts, but it also needs to be eval-
uated on the output of an automatic speech recog-
nizer. While this will add more noise, it will also
give access to useful prosodic features like stress.
Finally, the system also needs to be evaluated extrin-
sically, i.e. with respect to its contribution to dialog
summarization. It might turn out that our system al-
ready has a positive effect on extractive summariza-
tion, even though its performance is low in absolute
terms.
Acknowledgments. This work has been funded
by the Deutsche Forschungsgemeinschaft as part of
the DIANA-Summ project (STR-545/2-1,2) and by
the Klaus Tschira Foundation. We are grateful to the
anonymous ACL reviewers for helpful comments
and suggestions. We also thank Ron Artstein for
help with significance testing.
References
Artstein, R. & M. Poesio (2006). Identifying reference to ab-
stract objects in dialogue. In Proc. of BranDial-06, pp.
56?63.
Asher, N. (1993). Reference to Abstract Objects in Discourse.
Dordrecht, The Netherlands: Kluwer.
Bagga, A. & B. Baldwin (1998). Algorithms for scoring coref-
erence chains. In Proc. of LREC-98, pp. 79?85.
Byron, D. K. (2004). Resolving pronominal reference to ab-
stract entities., (Ph.D. thesis). University of Rochester.
Charniak, E. (2000). A maximum-entropy-inspired parser. In
Proc. of NAACL-00, pp. 132?139.
Eckert, M. & M. Strube (2000). Dialogue acts, synchronis-
ing units and anaphora resolution. Journal of Semantics,
17(1):51?89.
Harman, D. & M. Liberman (1994). TIPSTER Complete
LDC93T3A. 3 CD-ROMS. Linguistic Data Consortium,
Philadelphia, Penn., USA.
Heeman, P. & J. Allen (1999). Speech repairs, intonational
phrases, and discourse markers: Modeling speakers? ut-
terances in spoken dialogue. Computational Linguistics,
25(4):527?571.
Janin, A. (2002). Meeting recorder. In Proceedings of the
Applied Voice Input/Output Society Conference (AVIOS),
San Jose, California, USA, May 2002.
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke &
C. Wooters (2003). The ICSI Meeting Corpus. In Pro-
ceedings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing, Hong Kong, pp. 364?
367.
Kabadjov, M. A., M. Poesio & J. Steinberger (2005). Task-
based evaluation of anaphora resolution: The case of
summarization. In Proceedings of the RANLP Workshop
on Crossing Barriers in Text Summarization Research,
Borovets, Bulgaria.
Kehler, A., D. Appelt, L. Taylor & A. Simma (2004). The
(non)utility of predicate-argument frequencies for pro-
noun interpretation. In Proc. of HLT-NAACL-04, pp. 289?
296.
Lapata, M., S. McDonald & F. Keller (1999). Determinants
of adjective-noun plausibility. In Proc. of EACL-99, pp.
30?36.
Mitkov, R. (2002). Anaphora Resolution. London, UK: Long-
man.
Mu?ller, C. (2006). Automatic detection of nonreferential it in
spoken multi-party dialog. In Proc. of EACL-06, pp. 49?
56.
Mu?ller, C. (2007). Fully automatic resolution of it, this, and
that in unrestricted multi-party dialog., (Ph.D. thesis).
Eberhard Karls Universita?t Tu?bingen, Germany. To ap-
pear.
Passonneau, R. J. (2004). Computing reliability for co-
reference annotation. In Proc. of LREC-04.
Poesio, M. & R. Artstein (2005). The reliability of anaphoric
annotation, reconsidered: Taking ambiguity into account.
In Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pp. 76?83.
Schiffman, R. J. (1985). Discourse constraints on ?it? and
?that?: A Study of Language Use in Career Counseling
Interviews., (Ph.D. thesis). University of Chicago.
Strube, M. & C. Mu?ller (2003). A machine learning approach to
pronoun resolution in spoken dialogue. In Proc. of ACL-
03, pp. 168?175.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly & L. Hirschman
(1995). A model-theoretic coreference scoring scheme.
In Proc. of MUC-6, pp. 45?52.
Webber, B. L. (1991). Structure and ostension in the interpre-
tation of discourse deixis. Language and Cognitive Pro-
cesses, 6(2):107?135.
Yang, X., J. Su & C. L. Tan (2005). Improving pronoun reso-
lution using statistics-based semantic compatibility infor-
mation. In Proc. of ACL-05, pp. 165?172.
823
Annotating Anaphoric and Bridging Relations with MMAX
Christoph Mu?ller and Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
fchristoph.mueller, michael.strubeg@eml.villa-bosch.de
Abstract
We present a tool for the annotation of
anaphoric and bridging relations in a cor-
pus of written texts. Based on differences as
well as similarities between these phenom-
ena, we define an annotation scheme. We
then implement the scheme within an anno-
tation tool and demonstrate its use.
1 Introduction
Anaphoric and bridging relations between discourse
entities are of major importance for establishing and
maintaining textual coherence. Consider the follow-
ing examples, taken from the Heidelberg Text Corpus
(HTC). The HTC is a collection of 577 short texts de-
scriptive of the city of Heidelberg, which have been
collected at our lab for a tourist information system in
the course of the DeepMap project (Malaka & Zipf,
2000).
Im Gegensatz zu anderen Sta?dten steht [das Heidelberger
Stadttheater] nicht an herausgehobener Stelle, sondern [es]
fu?gt sich in die Stra?enflucht ohne Vorplatz ein. [Der
Haupteingang] zeigt noch das alte Arkadenmotiv mit den
flachen Segmentbo?gen. (HTC002)
In contrast to other cities, [the Heidelberg theatre] is not
situated at a particularly exposed position, but [it] blends in
with the street. [The main entrance] still shows the original
motif of the arcades with the flat segments.
In the first sentence, [das Heidelberger Stadtthe-
ater] is introduced into the universe of discourse, and
then referred to anaphorically by means of the pro-
noun [es]. In the next sentence, there is a bridging re-
lation between the entities denoted by [der Hauptein-
gang] and [das Heidelberger Stadttheater]. 1
Note that in each example it is the fact that the sec-
ond discourse entity is either a definite NP or a pro-
noun that triggers the attempt to establish a relation
1Note that in this analysis (which is only one of two that
are possible here), [das Heidelberger Stadttheater] is at the
same time the antecedent to an anaphoric and a bridging ex-
pression.
to some previous entity: Normally, the use of a def-
inite NP respectively a pronoun presupposes that the
entity thus denoted has either already been introduced
into the universe of discourse or is assumed to be fa-
miliar to the reader. This is the case in the anaphori-
cally used pronoun in the first sentence. In the second
sentence, the relation between the two discourse enti-
ties [das Heidelberger Stadttheater] and [der Hauptein-
gang] is less obvious: The second one does not denote
the same entity as the first, but an entity that the second
one is a part of.
The ability to automatically resolve these kinds of
relations is an important feature of text understand-
ing systems. For both the training as well as the
evaluation of these systems, manually annotated cor-
pora are required. The creation of these corpora in
turn has a number of preliminaries. The phenom-
ena anaphor, bridging and the closely related concept
of antecedence need to be sufficiently defined (Sec-
tion 2). On the basis of this definition, an annotation
scheme needs to be developed (Section 3). Finally, an
annotation tool is required which implements the an-
notation scheme in a robust and efficient way. We re-
view a selection of existing tools, then present MMAX
(Multi-Modal Annotation in XML), our versatile Java
tool (Section 4), and we demonstrate how the annota-
tion scheme for anaphoric and bridging relations can
be implemented in MMAX (Section 5).
2 Definition
In general, anaphoric as well as bridging relations hold
between specifying expressions. These are those ex-
pressions that specify (i.e. are used to refer to) a
particular extra-linguistic entity. In what follows, we
briefly discuss the approach of (Vieira & Poesio, 2000)
and present our own definition. Since (Vieira & Poe-
sio, 2000) address the problem of bridging annotation,
they try to find an operational and easily applicable
definition. This is the main motivation for choosing
(Vieira & Poesio, 2000) (and not e.g. (Clark, 1975),
who introduced the term bridging) as the background
of our discussion. In the following discussion, two
features of pairs of specifying expressions will be im-
portant. The first one is cospecification (Sidner, 1983),
also known as coreference, a relation holding between
two or more specifying expressions which specify the
same extra-linguistic entity. The second important fea-
ture is identity of the head noun. This feature is appli-
cable to full NPs only and simply states that in a pair
of NPs the head of each is realized by the same noun.
Anaphor. In (Vieira & Poesio, 2000), only those
relations are classified as anaphoric in which both
cospecification and identity of the head noun is given. 2
Consequently, their rather strict definition contains but
one type, which the authors call direct anaphor. In
contrast to this, we take only cospecification to be a
necessary condition for anaphor. In case that iden-
tity of the head noun is also given, we call this direct
anaphor as well. We believe, however, that additional
sub-types of anaphor should be defined depending on
the type of the anaphoric expression. Along these
lines, we further distinguish pronominal anaphors, and
those in which the object specified by the second ex-
pression is standing in a conceptual IS-A (or hyponym-
hyperonym) relation to the object specified by the first
one. Consider the NP [das Bauwerk] in the follow-
ing example, which denotes a super-concept of [dem
Geba?ude].
Seit 1972 befindet sich das Romanische Seminar in [dem
Geba?ude Seminarstra?e 3]. [Das Bauwerk] wurde 1847
[...] errichtet. (HTC010)
Since 1972, the Romance Seminar is situated in [the build-
ing Seminarstra?e 3]. [The structure] was built [...] in
1847.
Bridging. Due to their strict definition of anaphor,
the term bridging is applied rather widely in (Vieira &
Poesio, 2000). First, those expressions are classified
as bridging which cospecify with their antecedent, but
which do so not by means of an identical, but a differ-
ent head noun. Finally, also non-cospecifying expres-
sions are classified as bridging as long as they stand
in some lexical relation (i.e. hyponymy, meronymy
or co-hyponymy) to their antecedent. The respec-
tive bridging sub-types are introduced by the authors
accordingly. Our approach, in contrast, classifies
as bridging only those expressions which are non-
cospecifying and which stand in some conceptual rela-
tion to their antecedent. At this time, we consider the
following relations to be relevant: cause-effect, part-
whole, and entity-attribute, for which we give the fol-
lowing examples:
Deshalb wurden [verschiedene Untersuchungen] [...]
vorgenommen. [Das Ergebnis] (cause-effect) war die Er-
stellung von Leitlinien fu?r die gestalterische Behandlung des
2It must be added here that (Vieira & Poesio, 2000) con-
sider definite NPs only, and in particular no pronouns.
[Hortus Palatinus]. Danach plante man, [die Zwischen-
terasse] (part-whole) [...] wiederherzustellen. (HTC113)
Therefore, [various examinations] [...] were conducted.
[The result] (cause-effect) was the production of guidelines
for the design of the [Hortus Palatinus]. After that, plans
were made to restore [...] [the middle terrace] (part-whole).
The Concept of Antecedence. In pairs of anaphoric
or bridging expressions, one member is regarded as
the antecedent of the other. In fact, the task of resolv-
ing such a relation is often identified with finding the
antecedent. While there are certainly cases where it is
possible to find exactly one antecedent to a given ex-
pression, there are also cases where this decision is not
obvious. This is true for anaphoric as well as bridging
antecedents: Consider the case of an NP that has been
introduced into the universe of discourse, and that is
referred back to anaphorically twice by means of a
pronoun. We believe that it is not at all clear what is to
be regarded as the antecedent of the second pronoun,
since one could argue for the NP on the grounds of its
semantic explicitness, or for the first pronoun on the
grounds of its being the more recent cospecifying en-
tity. The same is true with bridging expressions. In the
example in Section 1, one could regard the pronoun
[es] as the bridging antecedent of [der Haupteingang]
as well.
Preliminary Conclusion. Our definition of anaphor
and bridging developed so far can be summed up as
follows: Anaphoric relations can be observed between
cospecifying expressions. A pair of antecedent and
anaphor is always cospecifying, while on the other
hand, given a set of (more than two) cospecifying ex-
pressions, determining the antecedent to a given ex-
pression is not necessarily trivial. Anaphors can be
further differentiated according to the nature of the
lexical items taking part in the relation. Bridging re-
lations, in contrast, occur between non-cospecifying
expressions only. Here, the criterion for division into
sub-types is the conceptual relation between the enti-
ties denoted by the expressions taking part in the re-
lation. Finally, it is possible for an expression to be
antecedent to more than one anaphoric and / or bridg-
ing expressions at the same time, while we believe that
the reverse, i.e. one expression having more than one
antecedent, is not possible.
3 Annotation Scheme
The first step in the development of an annotation
scheme is the definition of the relevant markables, i.e.
the class of entities in the text between which the rela-
tions to be annotated can possibly occur. It is in terms
of these markables (with their attributes) and labelled
relations between markables that the annotation itself
is expressed.
In Section 2, we already roughly defined what
counts as a markable by stating that anaphoric and
bridging relations hold between specifying expres-
sions. To further distinguish, we introduce the at-
tribute np form which allows to differentiate between
the following subclasses: Proper noun, definite NP, in-
definite NP, personal pronoun, possessive pronoun and
demonstrative pronoun. In addition, other grammati-
cal features of markables, like agreement or the gram-
matical role they play, might also be of interest. We
capture these in two respective attributes, for which we
specify a closed list of possible values to be assigned
during annotation. These possible values are the com-
bination person/number/gender for the first and sub-
ject, object and other for the second attribute.
In a given pair of expressions it is the way in which
the second expression relates to the first one that de-
termines whether an anaphoric or a bridging relation
exists. It is natural, therefore, to represent this infor-
mation on this second markable and only there. More-
over, this is the only way to allow for the represen-
tation of cases in which one markable is antecedent
to several others. Since we rule out the possibility of
one markable being anaphor or bridging expression to
more than one antecedent, this information is easily
represented by means of an attribute which identifies
the markable as an anaphor or a bridging expression.
We add a further attribute for the respective relation?s
sub-type. For anaphoric expressions, the possible val-
ues for this attribute include direct, pronominal and
IS-A, and for bridging expressions part-whole, cause-
effect and entity-attribute, respectively.
Finally, the annotation of an anaphoric or bridg-
ing markable has to be complemented with informa-
tion on which markable is its antecedent. This can
be accomplished by supplying the markable with a
further attribute. However, selecting the correct an-
tecedent from several candidates can contain a consid-
erable amount of interpretation on the part of the anno-
tator. This is highly undesirable, because it is likely to
force arbitrary decisions which in turn can introduce
error and inconsistency into the annotation. It would
be preferable, therefore, if the explicit identification of
the antecedent would be optional. We do this by sup-
plying in our annotation scheme a means to represent
cospecification. With this additional representation,
the annotation of anaphoric relations in our annotation
scheme is a two-step process: Upon encountering an
anaphoric markable and setting its general attributes,
the markable is first annotated as being cospecifying
with all other markables already in this set of cospeci-
fying expressions. This is the only mandatory annota-
tion, and together with the information that the mark-
able is of the anaphoric type it perfectly well repre-
sents the anaphoric relation. The second, optional step
consists in the specification of the markable?s exact an-
tecedent. By separating the annotation of anaphoric
relations in this way, the concept antecedent becomes
free to be used only in those cases where it is both
relevant and unambiguously decidable. It is impor-
tant to note that no relevant information appears to be
lost here: Supplied that the linear order of markables
within the text is preserved, it should be possible to es-
tablish an antecedent to any anaphoric expression from
a set of cospecifying expressions annotated within the
scheme described above. Moreover, the important
task of evaluating the annotation scheme is not af-
fected either, because common evaluation algorithms
for anaphor annotations (Vilain et al, 1995) do not de-
pend on antecedence information, but treat anaphoric
expressions as cospecifying equivalence classes.
What is even more important is that by the same
means we can render optional the explicit specifica-
tion of bridging antecedents as well. Two cases can be
distinguished here: Whenever only a single candidate
for antecedence exists, specifying it is trivial. Thus,
the only cases where uncertainty as to the correct an-
tecedent of a bridging expression can arise appear to
be those in which multiple cospecifying candidates are
available. Since bridging (as we define it) is a relation
not between lexical items, but between extra-linguistic
entities, and since cospecification is a transitive rela-
tion, a bridging relation can be sufficiently expressed
by specifying any of the candidates. The major dif-
ference to the annotation of anaphoric relations is that
in case of bridging, the selection of an antecedent is
mandatory, but can be made at random, because what
is really selected is not the markable but the extra-
linguistic entity that it specifies.
4 Annotation Tool
This section deals with the question how the annota-
tion scheme developed in the previous section can be
implemented in a real annotation tool. Before present-
ing our own tool MMAX, we briefly review a selection
of already existing tools.
4.1 Existing Tools
The Discourse Tagging Tool (DTTool) (Aone & Ben-
nett, 1994) is a Tcl/Tk program for the annotation
and display of antecedent-anaphor relations in SGML-
encoded multilingual texts. While this field of applica-
tion makes it a potential candidate for the implementa-
tion of our scheme as well, this is not the case, mainly
because the tool lacks the possibility of assigning ar-
bitrary combinations of attributes and possible values
to markables, a feature that obviously is needed for the
representation of different types of relations.
CLinkA3 (Ora?san, 2000) is a more recent Java tool
3http://www.wlv.ac.uk/sles/compling/software/
for coreference annotation. In this case, it is a more
structural constraint which prevents our annotation
scheme from being implemented in this tool. This
constraint results from the fact that CLinkA was built
to implement the annotation scheme proposed in the
MUC-7 Coreference Task Definition (Hirschman &
Chinchor, 1997). In this scheme, cospecification is ex-
pressed in terms of antecedence only, a concept which
we have shown to be problematic, and which our anno-
tation scheme therefore avoids. Another problem with
CLinkA is that it does not seem to support additional
user-defined attributes either.
The Alembic Workbench4 is an annotation tool
which, among other tasks, directly supports cospecifi-
cation annotation. In contrast to DTTool and CLinkA,
it also allows for the extension of the tag set, so that
in principle the handling of different coreference phe-
nomena is possible. The tool (like the other two men-
tioned before) processes SGML files, into which an-
notation tags are inserted directly during annotation.
We regard this approach to annotation as a drawback,
because it mixes the basic data (i.e. the texts to be an-
notated) with the annotation itself. This can give rise
to problems, e.g. in cases where alternative annota-
tions of the same data are to be compared.
Referee, a Tcl/Tk program for coreference annota-
tion (DeCristofaro et al, 1999), is better in this respect
in that it writes the annotations to a separate file, leav-
ing the annotated text itself unaltered. The format of
this annotation file, however, is highly idiosyncratic,
rendering very difficult the subsequent analysis of the
annotation. Moreover, this tool also represents cospec-
ification in terms of antecedence only, making it im-
possible to annotate the former without specifying the
latter. On the other hand, Referee directly supports the
definition of user-definable attributes.
Finally, the MATE Workbench5 is the most ambi-
tious tool that we considered for the implementation
of our annotation scheme. It has been developed in
Java as a highly customizable tool for the XML-based
annotation of arbitrary and possibly non-hierarchical
levels of linguistic description. From a theoretical
point of view, the MATE Workbench would thus be
an ideal platform for the implementation of our anno-
tation scheme. In practical terms, however, we found
the performance of the program to be rather poor, ren-
dering it practically unusable as soon as a certain cor-
pus size was reached.
4.2 MMAX, an XML Annotation Tool
Since we found the existing tools that we considered
to be insufficient for the task of implementing our an-
notation scheme, we decided to develop our own tool.
4http://www.mitre.org/technology/alembic-workbench/
5http://mate.nis.sdu.dk
MMAX6 is written in Java for reasons of platform
independence. It processes XML-encoded text cor-
pora which make use of standoff annotation (Thomp-
son & McKelvie, 1997). Using this technique al-
lows us to keep apart the basic data and the annota-
tion. XML support in Java is realized by means of the
Apache7 implementations of an XML parser and XSL
stylesheet processor.
4.2.1 The Data Model
In MMAX, written texts are represented in XML in
terms of base-level and supra-base level elements. For
each of these element types, Document Type Defini-
tions (DTDs) exist which describe the structure of a
well-formed element. In the following, we give DTD
fragments and discuss their semantics.
A word is the most straightforward base level ele-
ment for a written text. Apart from the representation
of the word itself, each element of this type has an ID
attribute which serves to uniquely identify the word
within the text.
<!ELEMENT words (word*)>
<!ELEMENT word (#PCDATA)>
<!ATTLIST word id ID #REQUIRED>
The sequence of words which as a whole constitutes
the complete text can be divided with respect to two
criteria, a formal and a pragmatic one: Each word is
part of a particular (formally defined) text, which con-
sists of sentences, which in turn may be grouped into
paragraphs. Each sentence has an ID and a span8
attribute which is a pointer to a sequence of word el-
ements. In addition, every text can have an optional
headline, which consists of any number of sentences.
The formal structure of a text is described by the fol-
lowing DTD:
<!ELEMENT text ((headline?),
((paragraph+) | (sentence+)))>
<!ELEMENT headline (sentence*)>
<!ELEMENT paragraph (sentence*)>
<!ATTLIST paragraph id ID #REQUIRED>
<!ELEMENT sentence (EMPTY)>
<!ATTLIST sentence id ID #REQUIRED>
<!ATTLIST sentence span CDATA #REQUIRED>
In pragmatic terms, on the other hand, a text can be
regarded as a discourse, consisting of a series of dis-
course segments. Again, each discourse segment has
an ID and a span attribute, as well as a function at-
tribute indicative of its communicative function. This
pragmatic structure can be translated into a DTD as
follows:
<!ELEMENT discourse (discourse_segment*)>
<!ELEMENT discourse_segment (EMPTY)>
<!ATTLIST discourse_segment id ID #REQUIRED>
<!ATTLIST discourse_segment span CDATA #REQUIRED>
<!ATTLIST discourse_segment function CDATA #IMPLIED>
6http://www.eml.villa-bosch.de/english/research/nlp/
7http://www.apache.org
8We use our own attribute here instead of the href at-
tribute as defined in XPointer, because our element differs
from the latter both in semantics and implementation.
4.2.2 Markables
In MMAX, the XML elements representing mark-
ables possess a set of attributes which is only partly
pre-defined: A closed set of fixed system attributes
is complemented by an open set of user-definable at-
tributes which depend on the annotation scheme that
is to be implemented.
System Attributes. Each markable has an ID at-
tribute which uniquely identifies it. In addition, a span
attribute is needed as well which maps the markable
to one or more word elements. Finally, we intro-
duce a type attribute the meaning of which will be de-
scribed in the next subsection. Two additional system
attributes serve to express the relations between mark-
ables. We argue that two basic relations are sufficient
here.
The first is an unlabelled and unordered relation be-
tween arbitrarily many markables, which can be inter-
preted as set-membership, i.e. markables standing in
this relation to each other are interpreted as constitut-
ing a set. Note that the interpretation of this relation
is not pre-defined and needs to be specified within the
annotation scheme. In order to express a markable?s
membership in a certain set, a member attribute is in-
troduced which has as its value some string specifica-
tion. Set membership can thus be established/checked
by unifying/comparing the member attribute values of
two or more markables.
The second is a labelled and ordered relation be-
tween two markables, which is interpreted as one
markable pointing to the other. Note that here, again,
the nature of this pointing is not pre-defined. However,
there is a structural constraint imposed on the pointing
relation which demands that each markable can point
to at most one other markable. Since there is no con-
straint as to how many different markables can point
to another one, n:1 relations can be represented. A
pointer attribute is required for the expression of the
pointing relation. The range of possible values for this
attribute is the range of existing markables? IDs, with
the exception of the current markable itself.
The DTD fragment for markables and their system
attributes looks as follows:
<!ELEMENT markables (markable*)>
<!ATTLIST markable id ID #REQUIRED>
<!ATTLIST markable span CDATA #REQUIRED>
<!ATTLIST markable type CDATA #REQUIRED>
<!ATTLIST markable member CDATA #IMPLIED>
<!ATTLIST markable pointer IDREF #IMPLIED>
User-definable Attributes. It is by means of its
user-definable attributes that a markable obtains its
semantic interpretation within an annotation scheme.
But even within a single scheme, it may be required
to discriminate between different types of markables.
In MMAX, the type attribute is introduced for this pur-
pose. This attribute does not have any pre-defined pos-
Figure 1: Pair of bridging expression and antecedent
sible values. Instead, a list of these has to be supplied
by the annotation scheme. For each of these values,
in turn, a list of relevant attributes and possible values
has to be defined by the user. Depending on which of
the mutually exclusive type attributes is assigned to a
given markable during annotation, only the attributes
relevant to this type will be offered in a separate at-
tribute window for further specification.
5 Implementation
We utilize the system attribute type to discriminate be-
tween the three basic classes of markables, i.e. nor-
mal9, anaphoric and bridging ones. The respective
attributes and possible values for these mutually ex-
clusive markable types can directly be adopted from
the annotation scheme. Note that a subset of these
is in fact identical for each type (np form, agreement
and grammatical role), while other attributes? possible
values vary with the type of markable: For anaphoric
markables, e.g., the sub-types direct, pronominal and
IS-A are relevant, which make no sense for bridging
expressions, and vice versa. This is directly supported
by the adaptive attribute window. Figure 1 shows the
attribute window in response to the selection of a value
for the type attribute.
Cospecification between two or more markables is
expressed by means of an identical member attribute.
This value, though at this time realised as a string
of the form set XX only, can be interpreted as what
has been called universe entity, elsewhere, e.g. in
the Mate Dialogue Annotation Guidelines10. Adding
9Normal markables are those that are either not part of
any relation or serve as the antecedent only.
10http://www.ims.uni-stuttgart.de/projekte/mdag
Figure 2: Annotation of an anaphor
a markable to a set of cospecifying markables is ac-
complished in two steps: First, the set as a whole is
selected by left-clicking any of its members. As a re-
sult, all members are displayed in a different color, the
selected one in addition being highlighted. The mark-
able to be added is then right-clicked, and the desired
action chosen from a popup menu. Figure 2 shows this
situation. Note that the attribute window has changed
in response to the selection of the value anaphoric
for the type attribute. Specifying the antecedent to
an anaphoric expression is done as follows: First,
the anaphoric markable is selected by left-clicking it.
The desired antecedent is then right-clicked. Finally,
selecting the appropriate menu item from a popup
menu causes the anaphoric markable to point to its an-
tecedent. The antecedent and the anaphoric respec-
tively bridging expression are displayed in a differ-
ent colour whenever the latter is selected. Note that
by combination of the member and pointer attributes,
cospecification and bridging can be represented simul-
taneously, which may be needed in cases of long-
distance anaphor and short-distance bridging.
6 Outlook: Multi-modal Corpora
The definitions of the phenomena anaphor and bridg-
ing presented in this paper as well as the annotation
scheme developed were tailored mainly to the needs of
written texts. This bias is apparent both in the way we
defined markables as well as in the attributes that we
specified for them. In addition, our conception of an-
tecedence was also influenced by it in that the notion
of linear order (as can be observed between words in
a written text) was at least implicit in it. All phenom-
ena, however, are not limited to the domain of writ-
ten text: They also occur in spoken language and di-
alogue. What is even more important: The means by
which they are accomplished there include non-verbal
elements like gazes and in particular pointing gestures.
It is not at all clear yet if and how traditional defini-
tions of phenomena like anaphor or bridging can be
transferred to multi-modal corpora. In particular, phe-
nomena like cross-modal anaphor and bridging need
to be studied in more detail. As a step in this direc-
tion, we have already applied MMAX to the annota-
tion of cospecification in multi-modal corpora (Mu?ller
& Strube, 2001).
Acknowledgements. We thank Lutz Wind for giv-
ing feedback on previous versions of MMAX. We also
thank the two anonymous reviewers for their useful
comments. The work presented here has been partially
funded by the German Ministry of Research and Tech-
nology under grant 01 IL 904 D/2 (EMBASSI) and by
the Klaus Tschira Foundation.
References
Aone, C. & S. W. Bennett (1994). Discourse tagging tool
and discourse-tagged multilingual corpora. In Pro-
ceedings of the International Workshop on Sharable
Natural Language Resources (SNLR), Ikoma, Nara,
Japan, 10?11 August, 1994, pp. 71?77.
Clark, H. H. (1975). Bridging. In Proc. of TINLAP-1, pp.
169?174.
DeCristofaro, J., M. Strube & K. F. McCoy (1999). Build-
ing a tool for annotating reference in discourse. In
ACL ?99 Workshop on the Relationship between Dis-
course/Dialogue Structure and Reference, University
of Maryland, Maryland, 21 June, 1999, pp. 54?62.
Hirschman, L. & N. Chinchor (1997). MUC-7
Coreference Task Definition, http://www.
muc.saic.com/proceedings/.
Malaka, R. & A. Zipf (2000). Deep Map: Challenging
IT research in the framework of a tourist information
system. In Proceedings of the International Confer-
ence on Information and Communication Technologies
in Tourism (ENTER 2000), Barcelona, Spain, 26-28
April, 2000.
Mu?ller, C. & M. Strube (2001). MMAX: A tool for the an-
notation of multi-modal corpora. In Proceedings of
2nd IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Seattle, Wash., 5 August
2001.
Ora?san, C. (2000). ClinkA a coreferential links annotator. In
Proc. of LREC ?00, pp. 491?496.
Sidner, C. L. (1983). Focusing in the comprehension of defi-
nite anaphora. In M. Brady & R. Berwick (Eds.), Com-
putational Models of Discourse, pp. 267?330. Cam-
bridge, Mass.: MIT Press.
Thompson, H. S. & D. McKelvie (1997). Hyperlink seman-
tics for standoff markup of read-only documents. In
Proceedings of SGML Europe ?97, Barcelona, Spain,
May 1997.
Vieira, R. & M. Poesio (2000). An empirically-based system
for processing definite descriptions. Computational
Linguistics, 26(4):539?593.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly &
L. Hirschman (1995). A model-theoretic coreference
scoring scheme. In Proceedings fo the 6th Message
Understanding Conference (MUC-6), pp. 45?52.
The Influence of Minimum Edit Distance on Reference Resolution
Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
strube@eml.villa-bosch.de
Stefan Rapp
Sony International (Europe) GmbH
Advanced Technology Center Stuttgart
Heinrich-Hertz-Stra?e 1
70327 Stuttgart, Germany
rapp@sony.de
Christoph Mu?ller
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
mueller@eml.villa-bosch.de
Abstract
We report on experiments in reference res-
olution using a decision tree approach. We
started with a standard feature set used in
previous work, which led to moderate re-
sults. A closer examination of the perfor-
mance of the features for different forms
of anaphoric expressions showed good re-
sults for pronouns, moderate results for
proper names, and poor results for definite
noun phrases. We then included a cheap,
language and domain independent feature
based on the minimum edit distance be-
tween strings. This feature yielded a sig-
nificant improvement for data sets consist-
ing of definite noun phrases and proper
names, respectively. When applied to
the whole data set the feature produced a
smaller but still significant improvement.
1 Introduction
For the automatic understanding of written or spo-
ken natural language it is crucial to be able to iden-
tify the entities referred to by referring expressions.
The most common and thus most important types
of referring expressions are pronouns and definite
noun phrases (NPs). Supervised machine learning
algorithms have been used for pronoun resolution
(Ge et al, 1998) and for the resolution of definite
NPs (Aone and Bennett, 1995; McCarthy and Lehn-
ert, 1995; Soon et al, 2001). An unsupervised ap-
proach to the resolution of definite NPs was applied
by Cardie and Wagstaff (1999). However, though
machine learning algorithms may deduce to make
best use of a given set of features for a given prob-
lem, it is a linguistic question and a non-trivial task
to identify a set of features which describe the data
sufficiently.
We report on experiments in the resolution of
anaphoric expressions in general, including definite
noun phrases, proper names, and personal, posses-
sive and demonstrative pronouns. Based on the
work mentioned above we started with a feature set
including NP-level and coreference-level features.
Applied to the whole data set these features led
only to moderate results. Since the NP form of the
anaphor (i.e., whether the anaphoric expression is
realized as pronoun, definite NP or proper name) ap-
peared to be the most important feature, we divided
the data set into several subsets based on the NP
form of the anaphor. This led to the insight that the
moderate performance of our system was caused by
the low performance for definite NPs. We adopted
a new feature based on the minimum edit distance
(Wagner and Fischer, 1974) between anaphor and
antecedent, which led to a significant improvement
on definite NPs and proper names. When applied to
the whole data set the feature yielded a smaller but
still significant improvement.
In this paper, we first discuss features that have
been found to be relevant for the task of reference
resolution (Section 2). Then we describe our cor-
pus, the corpus annotation, and the way we prepared
the data for use with a binary machine learning clas-
sifier (Section 3). In Section 4 we first describe the
feature set used initially and the results it produced.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 312-319.
                         Proceedings of the Conference on Empirical Methods in Natural
We then introduce the minimum edit distance fea-
ture and give the results it yielded on different data
sets.
2 Features for Reference Resolution in
Previous Work
Driven by the necessity to provide robust systems
for the MUC system evaluations, researchers began
to look for those features which were particular im-
portant for the task of reference resolution. While
most features for pronoun resolution have been de-
scribed in the literature for decades, researchers only
recently began to look for robust and cheap features,
i.e., features which perform well over several do-
mains and can be annotated (semi-) automatically.
In the following, we describe a few earlier contri-
butions to reference resolution with respect to the
features used.
Decision tree algorithms were used for ref-
erence resolution by Aone and Bennett (1995,
C4.5), McCarthy and Lehnert (1995, C4.5) and
Soon et al (2001, C5.0). This approach requires
the definition of a set of features describing
pairs of anaphors and their antecedents, and col-
lecting a training corpus annotated with them.
Aone and Bennett (1995), working on reference
resolution in Japanese newspaper articles, use
66 features. They do not mention all of these
explicitly but emphasize the features POS-tag,
grammatical role, semantic class and distance.
The set of semantic classes they use appears to be
rather elaborated and highly domain-dependent.
Aone and Bennett (1995) report that their best
classifier achieved an F-measure of about 77% after
training on 250 documents. They mention that
it was important for the training data to contain
transitive positives, i.e., all possible coreference
relations within an anaphoric chain.
McCarthy and Lehnert (1995) describe a refer-
ence resolution component which they evaluated on
the MUC-5 English Joint Venture corpus. They dis-
tinguish between features which focus on individ-
ual noun phrases (e.g. Does noun phrase contain a
name?) and features which focus on the anaphoric
relation (e.g. Do both share a common NP?). It
was criticized (Soon et al, 2001) that the features
used by McCarthy and Lehnert (1995) are highly id-
iosyncratic and applicable only to one particular do-
main. McCarthy and Lehnert (1995) achieved re-
sults of about 86% F-measure (evaluated accord-
ing to Vilain et al (1995)) on the MUC-5 data set.
However, only a defined subset of all possible ref-
erence resolution cases was considered relevant in
the MUC-5 task description, e.g., only entity refer-
ences. For this case, the domain-dependent features
may have been particularly important, making it dif-
ficult to compare the results of this approach to oth-
ers working on less restricted domains.
Soon et al (2001) use twelve features (see Table
1). Soon et al (2001) show a part of their decision
tree in which the weak string identity feature (i.e.
identity after determiners have been removed) ap-
pears to be the most important one. They also report
on the relative contribution of the features where
the three features weak string identity, alias (which
maps named entities in order to resolve dates, per-
son names, acronyms, etc.) and appositive seem to
cover most of the cases (the other nine features con-
tribute only 2.3% F-measure for MUC-6 texts and
1% F-measure for MUC-7 texts). Soon et al (2001)
include all noun phrases returned by their NP iden-
tifier and report an F-measure of 62.6% for MUC-6
data and 60.4% for MUC-7 data. They only used
pairs of anaphors and their closest antecedents as
positive examples in training, but evaluated accord-
ing to Vilain et al (1995).
Cardie and Wagstaff (1999) describe an unsuper-
vised clustering approach to noun phrase corefer-
ence resolution in which features are assigned to
single noun phrases only. They use the features
shown in Table 2, all of which are obtained auto-
matically without any manual tagging. The feature
semantic class used by Cardie and Wagstaff (1999)
seems to be a domain-dependent one which can
only be used for the MUC domain and similar
ones. Cardie and Wagstaff (1999) report a perfor-
mance of 53,6% F-measure (evaluated according to
Vilain et al (1995)).
3 Data
3.1 Text Corpus
Our corpus consists of 242 short German texts (to-
tal 36924 tokens) about sights, historic events and
persons in Heidelberg. The average length is 151 to-
? distance in sentences between anaphor and antecedent
? antecedent is a pronoun?
? anaphor is a pronoun?
? weak string identity between anaphor and antecedent
? anaphor is a definite noun phrase?
? anaphor is a demonstrative pronoun?
? number agreement between anaphor and antecedent
? semantic class agreement between anaphor and antecedent
? gender agreement between anaphor and antecedent
? anaphor and antecedent are both proper names?
? an alias feature (used for proper names and acronyms)
? an appositive feature
Table 1: Features used by Soon et al
? position (NPs are numbered sequentially)
? pronoun type (nom., acc., possessive, ambiguous)
? article (indefinite, definite, none)
? appositive (yes, no)
? number (singular, plural)
? proper name (yes, no)
? semantic class (based on WordNet: time, city, animal,
human, object; based on a separate algorithm: num-
ber, money, company)
? gender (masculine, feminine, either, neuter)
? animacy (anim, inanim)
Table 2: Features used by Cardie and Wagstaff
kens. The texts were POS-tagged using TnT (Brants,
2000). A basic identification of markables (refer-
ring expressions, i.e. NPs) was obtained by using the
NP-Chunker Chunkie (Skut and Brants, 1998). The
POS-tagger was also used for assigning attributes
like e.g. the NP form to markables. The automatic
annotation was followed by a manual correction and
annotation phase in which the markables were anno-
tated with further tags (e.g. semantic class). In this
phase manual coreference annotation was performed
as well. In our annotation coreference is represented
in terms of a member attribute on markables. Mark-
ables with the same value in this attribute are con-
sidered coreferring expressions. The annotation was
performed by two students. The reliability of the an-
notations was checked using the kappa statistic (Car-
letta, 1996).
3.2 Data Generation
The problem of coreference resolution can easily be
formulated as a binary classification: Given a pair
of potential anaphor and potential antecedent, clas-
sify as positive if the antecedent is in fact the closest
antecedent, and as negative otherwise. In anaphoric
chains only the immediately adjacent pairs are clas-
sified as positive. We generated data suitable as in-
put to a machine learning algorithm from our corpus
using a straightforward algorithm which combined
potential anaphors and their potential antecedents.
We then applied the following filters to the resulting
pairs: Discard an antecedent-anaphor pair
  if the anaphor is an indefinite NP,
  if one entity is embedded into the other, e.g. if
the potential anaphor is the head of the poten-
tial antecedent NP (or vice versa),
  if both entities have different values in their se-
mantic class attributes1 ,
  if either entity has a value other than 3rd person
singular or plural in its agreement feature,
  if both entities have different values in their
agreement features2.
For some texts, these heuristics (which were ap-
plied to the entire corpus) reduced to up to 50%
the potential anaphor-antecedent pairs all of which
would have been negative cases. We consider the
cases discarded as irrelevant because they do not
contribute any knowledge for the classifier. After ap-
plication of the filters, the remaining candidate pairs
were labeled as follows:
  Pairs of anaphors and their direct (i.e. clos-
est) antecedents were labeled P. This means
that each anaphoric expression produced ex-
actly one positive instance.
  Pairs of anaphors and those non-antecedents
which occurred closer to the anaphor than the
direct antecedent were labeled N. The number
of negative instances that each expression pro-
duced thus depended on the number of non-
antecedents occurring between the anaphor and
the direct antecedent (or, the beginning of the
text if there was none).
Pairs of anaphors and non-antecedents which oc-
cured further away than the direct antecedent as well
as pairs of anaphors and non-direct (transitive) an-
tecedents were not considered in the data sets. This
produced 242 data sets with a total of 72093 in-
stances of potential antecedent-anaphor pairs.
4 Results
4.1 Our Features
The features for our study were selected according
to three criteria:
1This filter applies only if none of the expressions is a pro-
noun. Otherwise, filtering on semantic class is not possible be-
cause in a real-world setting, information about a pronoun?s se-
mantic class obviously is not available prior to its resolution.
2This filter applies only if the anaphor is a pronoun. This
restriction of the filter is necessary because German allows for
cases where an antecedent is referred back to by a non-pronoun
anaphor which has a different grammatical gender.
  relevance according to previous research,
  low annotation cost and/or high reliability of
automatic tagging,
  domain-independence.
We distinguish between features assigned to noun
phrases and features assigned to the potential coref-
erence relation. All features are listed in Table 3 to-
gether with their respective possible values.
The grammatical function of referring expres-
sions has often been claimed to be an important fac-
tor for reference resolution and was therefore in-
cluded (features 2 and 6). The surface realization
of referring expressions seems to have an influence
on coreference relations as well (features 3 and 7).
Since we use a German corpus and in this language
the gender and the semantic class do not necessar-
ily coincide (i.e., objects are not necessarily neuter
as they are in English) we also provide a semantic
class feature (5 and 9) which captures the difference
between human, concrete objects, and abstract ob-
jects. This basically corresponds to the gender at-
tribute in English, for which we introduced an agree-
ment feature (4 and 8). The feature wdist (10) cap-
tures the distance in words between anaphor and an-
tecedent, while the feature ddist (11) does the same
in terms of sentences and mdist (12) in terms of
markables. The equivalence in grammatical func-
tion between anaphor and potential antecedent is
captured in the feature syn par (13), which is true if
both anaphor and antecedent are subjects or both are
objects, and false in the other cases. The string ident
feature (14) appears to be of major importance since
it provides for high precision in reference resolution
(it almost never fails) while the substring match fea-
ture (15) could potentially provide better recall.
4.2 Baseline Results
Using the features of Table 3, we trained decision
tree classifiers using C5.0, with standard settings for
pre and post pruning. As several features are dis-
crete, we allowed the algorithm to use subsets of
feature values in questions such as ?Is ana npform in

PPER, PPOS, PDS  ??. We also let C5.0 construct
rules from the decision trees, as we found them to
give superior results. In our experiments, the value
Document level features
1. doc id document number (1 . . . 250)
NP-level features
2. ante gram func grammatical function of antecedent (subject, object, other)
3. ante npform form of antecedent (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
4. ante agree agreement in person, gender, number
5. ante semanticclass semantic class of antecedent (human, concrete object, abstract object)
6. ana gram func grammatical function of anaphor (subject, object, other)
7. ana npform form of anaphor (definite NP, indefinite NP, personal pronoun,
demonstrative pronoun, possessive pronoun, proper name)
8. ana agree agreement in person, gender, number
9. ana semanticclass semantic class of anaphor (human, concrete object, abstract object)
Coreference-level features
10. wdist distance between anaphor and antecedent in words (1 . . . n)
11. ddist distance between anaphor and antecedent in sentences (0, 1,  1)
12. mdist distance between anaphor and antecedent in markables (1 . . . n)
13. syn par anaphor and antecedent have the same grammatical function (yes, no)
14. string ident anaphor and antecedent consist of identical strings (yes, no)
15. substring match one string contains the other (yes, no)
Table 3: Our Features
of the ana semanticclass attribute was reset to miss-
ing for pronominal anaphors, because in a realistic
setting the semantic class of a pronoun obviously is
not available prior to its resolution.
Using 10-fold cross validation (with about 25
documents for each of the 10 bins), we achieved an
overall error rate of 1.74%. Always guessing the by
far more frequent negative class would give an er-
ror rate of 2.88% (70019 out of 72093 cases). The
precision for finding positive cases is 88.60%, the
recall is 45.32%. The equally weighted F-measure3
is 59.97%.
Since we were not satisfied with this result we
examined the performance of the features. Surpris-
ingly, against our linguistic intuition the ana npform
feature appeared to be the most important one. Thus,
we expected considerable differences in the perfor-
mance of our classifier with respect to the NP form
of the anaphor under consideration. We split the data
into subsets defined by the NP form of the anaphor
and trained the classifier on these data sets. The re-
sults confirmed that the classifier performed poorly
on definite NPs (defNP) and demonstrative pronouns
3computed as 
		
(PDS), moderately on proper names (NE) and quite
good on personal pronouns (PPER) and possessive
pronouns (PPOS) (the results are reported in Ta-
ble 4). As definite NPs account for 792 out of
2074 (38.19%) of the positive cases (and for 48125
(66.75%) of all cases), it is evident that the weak
performance for the resolution of definite NPs, es-
pecially the low recall of only 8.71% clearly impairs
the overall results. Demonstrative pronouns appear
only in 0.87% of the positive cases, so the inferior
performance is not that important. Proper names
(NE) however are more problematic, as they have to
be considered in 644 or 31.05% of the positive cases
(22.96% of all).
P R F
defNP 87.34% 8.71% 15.84%
NE 90.83% 50.78% 65.14%
PDS 25.00% 11.11% 15.38%
PPER 88.12% 78.07% 82.79%
PPOS 82.69% 87.31% 84.94%
all 88.60% 45.32% 59.97%
Table 4: Baseline results using features 2?15.
Antecedent Anaphor
?Philips? ?Kurfu?rst Philip?
?vier Schu?lern? ?die Schu?ler?
?die alte Universita?t? ?der alten Universita?t?
?im Studentenkarzer in der Augustinergasse? ?des Studentenkarzers?
?diese hervorragende Bibliothek? ?dieser Bibliothek?
Table 5: Anaphors and their direct antecedents
New coreference-level features
16. ante med minimum edit distance to anaphor
ffOntology-based Contextual Coherence Scoring
Robert Porzel Iryna Gurevych Christof E. Mu?ller
European Media Laboratory
Schloss-Wolfsbrunnenweg 31c
D-69118 Heidelberg
{porzel,gurevych,mueller2@eml.org}
Abstract
In this paper we present a contextual ex-
tension to ONTOSCORE, a system for
scoring sets of concepts on the basis of
an ontology. We apply the contextually
enhanced system to the task of scoring
alternative speech recognition hypothe-
ses (SRH) in terms of their semantic co-
herence. We conducted several annota-
tion experiments and showed that human
annotators can reliably differentiate be-
tween semantically coherent and incoher-
ent speech recognition hypotheses (both
with and without discourse context). We
also showed, that annotators can reliably
identify the overall best hypothesis from
a given n-best list. While the original
ONTOSCORE system correctly assigns the
highest score to 84.06% of the corpus,
the inclusion of the conceptual context in-
creases the number of correct classifica-
tions to yield 86.76%, given a baseline of
63.91% in both cases.
1 Introduction
Following Allen et al (2001), we can distinguish
between controlled and conversational dialogue sys-
tems. Since controlled and restricted interactions
between the user and the system increase recogni-
tion and understanding accuracy, such systems are
reliable enough to be deployed in various real world
applications, e.g. public transportation or cinema in-
formation systems. The more conversational a dia-
logue system becomes, the less predictable are the
users? utterances. Recognition and processing be-
come increasingly difficult and unreliable.
Today?s dialogue systems employ domain- and
discourse-specific knowledge bases, so-called on-
tologies, to represent the individual discourse enti-
ties as concepts as well as their relations to each
other. In this paper we employ an algorithm for mea-
suring the semantic coherence of sets of concepts us-
ing such an ontology and show how its performance
can be improved by means of an inclusion of the
conceptual context. Thereby creating a method for
scoring the contextual coherence of individual sets
of concepts.
In the following, we will show how the contex-
tual coherence measurement can be applied to esti-
mate how well a given speech recognition hypoth-
esis (SRH) fits with respect to the existing knowl-
edge representation and the given conceptual con-
text, thereby providing a mechanism that increases
the robustness and reliability of dialogue systems.
We can, therefore, show how the algorithm can be
successfully employed by a spoken dialogue system
to enhance the interface between automatic speech
recognition (ASR) and natural language understand-
ing (NLU).
In Section 2 we discuss the problem of scoring
and classifying SRHs in terms of their semantic co-
herence followed by a description of our annotation
experiments and the corresponding results in Sec-
tion 3. Section 4 contains a description of the kind
of knowledge representations and the algorithm em-
ployed by ONTOSCORE. In Section 5 we present the
contextually enhanced system. Evaluations of the
corresponding system for scoring SRHs are given in
Section 6. A conclusion and additional applications
are given in Section 7.
2 Semantic Coherence and Speech
Recognition Hypotheses
While a simple one-best hypothesis interface be-
tween ASR and NLU suffices for restricted dialogue
systems, more complex systems either operate on n-
best lists as ASR output or convert ASR word graphs
(Oerder and Ney, 1993) into n-best lists, given the
distribution of acoustic and language model scores
(Schwartz and Chow, 1990; Tran et al, 1996). For
example, in our data a user expressed the wish to get
from Cologne to Heidelberg and then to continue his
visit in Heidelberg, as:1
(1) ich
I
mo?chte
want
auf
on
dem
the
schnellsten
fastest
Weg
way
von
from
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
(2) wie
how
komme
can
ich
I
in
in
Heidelberg
Heidelberg
weiter.
continue.
Looking at the SRHs from the ensuing n-best list of
Example (1) we found that Example (1a) constituted
the best representation of the utterance, whereas
all others constituted less adequate representations
thereof.
(1a) ich
I
mo?chte
want
auf
on
schnellsten
fastest
Weg
way
von
from
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
(1b) ich
I
mo?chte
want
auf
on
schnellsten
fastest
Weg
way
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
(1c) ich
I
mo?chte
want
Folk
folk
Weg
way
von
from
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
(1d) ich
I
mo?chte
want
auf
on
schnellsten
fastest
Weg
way
vor
before
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
1All examples are displayed with the German original on top
and a glossed translation below.
(1e) ich
I
mo?chte
want
vor
before
schnellsten
fastest
Weg
way
von
from
Ko?ln
Cologne
nach
to
Heidelberg.
Heidelberg.
Facing multiple representations of a single utter-
ance consequently poses the question, which of the
different hypotheses corresponds most likely to the
user?s utterance. Several ways of solving this prob-
lem have been proposed and implemented in var-
ious systems. Frequently the scores provided by
the ASR system itself are used, e.g. acoustic and
language model probabilities. More recently also
scores provided by the NLU system have been em-
ployed, e.g. parsing scores (Engel, 2002) or dis-
course model scores (Pfleger et al, 2002). However,
these methods often assign very high scores to SRHs
which are semantically incoherent and low scores to
semantically coherent ones.
In the case of Example (1) all scores, i.e. the
acoustic, language model, parsing and the ON-
TOSCORE scores assign the highest score to Exam-
ple (1a) (see Table 2 for the actual numbers). SRH
1a can consequently be chosen as the best SRH.
As we will show in Section 6, the scoring of the
SRHs from Example (2) differs substantially, and
only the contextual coherence score manages to pick
an adequate SRH. The fact that neither of the other
scoring approaches systematically employs the sys-
tem?s knowledge of the domains at hand, can re-
sult in passing suboptimal SRHs through the system.
This means that, while there was a better represen-
tation of the actual utterance in the n-best list, the
NLU system is processing an inferior one, thereby
causing overall dialogue metrics, in the sense of
Walker et al (2000), to decrease. We, therefore,
propose an alternative way to rank SRHs on the ba-
sis of their contextual coherence, i.e. with respect
to a given ontology representing the domains of the
system and the given conceptual context.
3 Annotation Experiments
The experiments reported here are based on the
data collected in hidden-operator tests where sub-
jects were prompted to say certain inputs. We ob-
tained 232 dialogues, which were divided into 1479
audio files with single user utterances. Each ut-
terance corresponded to a single intention, e.g. a
route- or a sight information request. Firstly, all ut-
terances were also transcribed. Then the audio files
were sent to the speech recognizer. We logged the
speech recognition output, i.e. n-best lists of SRHs
for all utterances. A subset of the corpus was used to
log also the scores of the recognizer, parser and that
of OntoScore - including context-independent and
context-dependent semantic coherence scores. This
trial resulted in a sub-corpus of 552 utterances cor-
responding to 1,375 SRHs along with the respective
confidence scores.
We, then, conducted several annotation experi-
ments with a two-fold motivation. In the first place,
it was necessary to produce a hand-annotated corpus
to be used as a gold standard for the evaluation of
the contextual coherence scores. Furthermore, we
wanted to test whether human subjects were able
to annotate the data reliably according to our anno-
tation schemata. We had two annotators specially
trained for each of these particular annotation tasks.
In an earlier annotation experiment reported in
Gurevych et al (2002), the task of annotators was
to classify a subset of the corpus of SRHs as either
coherent or incoherent. Here we randomly mixed
SRHs in order to avoid contextual priming.2 In the
first new experiment, a sub-corpus of 552 utterances
was annotated within the discourse context, i.e. the
SRHs were presented in their original dialogue or-
der. For each SRH, a decision again had to be made
whether it is semantically coherent or incoherent
with respect to the best SRH representing the previ-
ous user utterance. Given a total of 1,375 markables,
the annotators reached an agreement of 79.71%, i.e.
1,096 markables.
In the second new annotation experiment, the an-
notators saw the SRHs together with the transcribed
user utterances. The task of annotators was to deter-
mine the best SRH from the n-best list of SRHs cor-
responding to a single user utterance. The decision
had to be made on the basis of several criteria. The
most important criteria was how well the SRH cap-
tures the intentional content of the user?s utterance.
2As reported elsewhere the resulting Kappa statistics (Car-
letta, 1996) over the annotated data yields ? = 0.7, which in-
dicates that human annotators can reliably distinguish between
coherent samples and incoherent ones.
If none of the SRHs captured the user?s intention ad-
equately, the decision had to be made by looking at
the actual word error rate. In this experiment the
inter-annotator agreement was 90.69%, i.e. 1,247
markables out of 1,375.3 Each corpus was then tran-
formed into an evaluation gold standard by means of
the annotators agreeing on a single solution for the
cases of disagreement.
The aim of the work presented here, then, was to
provide a knowledge-based score, that can be em-
ployed by any NLU system to select the best hypoth-
esis from a given n-best list. The corresponding ON-
TOSCORE system will be described below, followed
by its evaluation against the human gold standards.
4 The Knowledge Base and OntoScore
In this section, we provide a description of the
underlying algorithm and knowledge sources em-
ployed by the original ONTOSCORE system (in
press). It is important to note that the ontology
employed in this and the previous evaluations ex-
isted already and was crafted as a general knowl-
edge representation for various processing modules
within the system.4 Ontologies have traditionally
been used to represent general and domain specific
knowledge and are employed for various natural lan-
guage understanding tasks, e.g. semantic interpreta-
tion (Allen, 1987) and in spoken dialogue systems,
e.g. for discourse modeling, modality fusion and
dialogue management, see also Porzel et al (2003)
for an overview. ONTOSCORE offers an additional
way of employing ontologies, i.e. to use the knowl-
edge modeled therein as the basis for evaluating
the semantic coherence of sets of concepts. It can
be employed independently of the specific ontology
language used, as the underlying algorithm oper-
ates only on the nodes and named edges of the di-
rected graph represented by the ontology. The spe-
cific knowledge base, e.g. written in DAML+OIL
3A Kappa-statistic suitable for measuring the reliability of
annotations is not possible in this case. The Kappa-statistic is
class-based and cannot, therefore, be applied to the best SRH
labeling, due to the different number of SRHs in the n-best lists.
Therefore, we calculated the percentage of utterances, where
the annotators agreed on the best SRH.
4Alternative knowledge representations, such as WORD-
NET, could have been employed in theory as well, however
most of the modern domains of the system, e.g. electronic me-
dia or program guides, are not covered by WORDNET.
or OWL,5 is converted into a graph, consisting of
the class hierarchy, with each class corresponding to
a concept representing either an entity or a process
and their slots, i.e. the named edges of the graph cor-
responding to the class properties, constraints and
restrictions.
The ontology employed for the evaluation has
about 730 concepts and 200 relations. It includes
a generic top-level ontology whose purpose is to
provide a basic structure of the world, i.e. abstract
classes to divide the universe in distinct parts as re-
sulting from the ontological analysis.6 The model-
ing of Processes and Physical Objects as a kind of
event that is continuous and homogeneous in nature,
follows the frame semantic analysis used for gener-
ating the FRAMENET data (Baker et al, 1998). The
hierarchy of Processes is connected to the hierarchy
of Physical Objects via slot-constraint definitions.
See also (Gurevych et al, 2003b) for a further de-
scription of the ontology.
ONTOSCORE performs a number of processing
steps. A first preprocessing step is to convert each
SRH into a concept representation (CR). For that
purpose we augmented the system?s lexicon with
specific concept mappings. That is, for each entry in
the lexicon either zero, one or many corresponding
concepts where added. A simple vector of concepts -
corresponding to the words in the SRH for which en-
tries in the lexicon exist - constitutes each resulting
CR. All other words with empty concept mappings,
e.g. articles and aspectual markers, are ignored in
the conversion. Due to lexical ambiguity, i.e. the
one to many word - concept mappings, this process-
ing step yields a set I = {CR1, CR2, . . . , CRn} of
possible interpretations for each SRH.
ONTOSCORE converts the domain model, i.e. an
ontology, into a directed graph with concepts as
nodes and relations as edges. In order to find the
shortest path between two concepts, ONTOSCORE
employs the single source shortest path algorithm
of Dijkstra (Cormen et al, 1990). Thus, the minimal
paths connecting a given concept ci with every other
5DAML+OIL and OWL are frequently used knowl-
edge modeling languages originating in W3C and Seman-
tic Web projects. For more details, see www.w3c.org and
www.daml.org.
6The top-level was developed following the procedure out-
lined in Russell and Norvig (1995).
concept in CR (excluding ci itself) are selected, re-
sulting in an n? n matrix of the respective paths.
To score the minimal paths connecting
all concepts with each other in a given
CR, we adopted a method proposed by
Demetriou and Atwell (1994) to score the se-
mantic coherence of alternative sentence inter-
pretations against graphs based on the Longman
Dictionary of Contemporary English (LDOCE).
As defined by Demetriou and Atwell (1994),
R = {r1, r2, . . . , rn} is the set of direct relations
(both isa and semantic relations) that can connect
two nodes (concepts); and W = {w1, w2, . . . , wn}
is the set of corresponding weights, where the
weight of each isa relation is set to 0 and that of
each other relation to 1.
The algorithm selects from the set of all paths
between two concepts the one with the smallest
weight, i.e. the cheapest. The distances between all
concept pairs in CR are summed up to a total score.
The set of concepts with the lowest aggregate score
represents the combination with the highest seman-
tic relatedness. The ensuing distance between two
concepts, e.g. D(ci, cj) is, then, defined as the min-
imum score derived between ci and cj .
Demetriou and Atwell (1994) do not provide con-
crete evaluation results for the method. Also, their
algorithm only allows for a relative judgment stat-
ing which of a set of interpretations given a single
sentence is more semantically related.
Since our objective is to compute coherence
scores of arbitrary CRs on an absolute scale, certain
extensions were necessary. In this application the
CRs to be scored can differ in terms of their content,
the number of concepts contained therein and their
mappings to the original SRH. Moreover, in order to
achieve absolute values, the final score should be re-
lated to the number of concepts in an individual set
and the number of words in the original SRH. There-
fore, the results must be normalized in order to allow
for evaluation, comparability and clearer interpreta-
tion of the semantic coherence scores.
We modified the algorithm described above to
make it applicable and evaluatable with respect to
the task at hand as well as other possible tasks. The
basic idea is to calculate a score based on the path
distances in CR. Since short distances indicate co-
herence and many concept pairs in a given CR may
have no connecting path, we define the distance be-
tween two concepts ci and cj that are not connected
in the knowledge base as Dmax. This maximum
value can also serve as a maximum for long dis-
tances and can thus help to prune the search tree for
long paths. This constant has to be set according to
the structure of the knowledge base. For example,
employing the ontology described above, the max-
imum distance between two concepts does not ex-
ceed ten and we chose in that case Dmax = 10.
We can now define the semantic coherence score
for CR as the average path length between all con-
cept pairs in CR:
S(CR) =
?
ci,cj?CR,ci 6=cj D(ci, cj)
|CR|2 ? |CR|
Since the ontology is a directed graph, we have
|CR|2 ? |CR| pairs of concepts with possible di-
rected connections, i.e., a path from concept ci to
concept cj may be completely different to that from
cj to ci or even be missing. As a symmetric alter-
native, we may want to consider a path from ci to cj
and a path from cj to ci to be semantically equivalent
and thus model every relation in a bidirectional way.
We can then compute a symmetric score S?(CR) as
S?(CR) = 2
?
ci,cj?CR,i<j min(D(ci, cj)D(cj , ci))
|CR|2 ? |CR|
ONTOSCORE implements both options. As the
ontology currently employed features mostly unidi-
rectional relations we chose the S?(CR) function for
the evaluation, i.e. only the best path D(ci, cj) be-
tween a given pair of concepts, regardless of the di-
rection, is taken into account. A detailed description
of the original system can be found in (Gurevych et
al., 2003a).
5 Contextual Coherence Scoring
The contextually enhanced ONTOSCORE system
performs a number of additional processing steps,
each of them will be described below.
5.1 Scoring Conceptual Context
Representations
A necessary preprocessing step for the conceptual
context scoring of SRHs is to build a conceptual con-
text representation CR?(SRHn+1) resulting from a
pair of concept representations:
- a concept representation of the SRH to be
scored, i.e. CR(SRHn+1),
- and a concept representation of the preceding
utterance?s SRH, i.e. CR(SRHn).
For that purpose, the ONTOSCORE stores the best
concept representation from each dialogue turn as
CRbest(SRH). By the best CR we mean the in-
terpretation which received the highest score from
the ONTOSCORE system, from the list of alter-
native interpretations of the utterance. For ex-
ample CRbest for the utterance shown in Exam-
ple (1) is the CR of the SRH given in (1e), i.e.
{EmotionExperiencerSubjectProcess, Person, Two-
PointRelation, Route, Town, Town}.
To produce a conceptual context representation
for SRHn+1, we build a union of each of its possible
interpretations I = {CR1, CR2, . . . , CRn} with
the stored CRbest(SRHn) from the previous utter-
ance. This results in a contextually augmented new
set I ? = {CR?1, CR
?
2, . . . , CR
?
n} representing pos-
sible conceptual context interpretations of SRHn+1
as shown in Table 1.
I(SRHn+1) I ?(SRHn+1)
CR1 ? CRbest(SRHn) = CR?1
CR2 ? CRbest(SRHn) = CR?2
... ... ...
CRn ? CRbest(SRHn) = CR?n
Table 1: Creating conceptual context representations
If, however, the calculated score of CRbest(SRHn)
is below a certain threshold, meaning that even the
best prior hypothesis is most likely not semanti-
cally coherent, then CRbest(SRHn) = {?}. See
Section 6.2 for the corresponding numbers with re-
spect to the coherent versus incoherent classifica-
tion. Thusly, only if CRbest(SRHn) is empty then
solely the concept representations of SRHn+1 are
taken into account. This is, of course, also the case
at the first dialogue turn.
In order to score the alternative conceptual con-
text representations defined by I ?(SRHn+1), the
formula for S?(CR) is employed. This means that
we calculate a conceptual context coherence score
S? for each conceptual context representation CR?.
We also perform an inverse linear transformation of
the scores resulting in numbers from 0 to 1, so that
higher scores indicate better contextual coherence.
5.2 ONTOSCORE at Work
Looking at an example of ONTOSCORE at work, we
will examine the following discourse fragment con-
sisting of the two sequential utterances given in Ex-
ample (1) and (2). As shown in Table 2, in the case
of Example (1) all scores indicate the SRH given in
Example (1a) to be the best one.
SRH recognizer parser OntoScore
1a 1 1 .6
1b .74 .94 .6
1c .63 .94 .54
1d .78 .89 .54
1e .74 .88 .54
Table 2: The scores for the SRHs of Example (1).
Example (2) yields the following SRHs with the cor-
responding context-independent CRs and context-
dependent CR?s:
2a Rennen
Race
Lied
song
Comedy
comedy
Show
show
Heidelberg
Heidelberg
weiter.
continue.
CR{MusicPiece, Genre, Genre, Town}
CR?{MusicPiece, Genre, Genre, Town,
EmotionExperiencerSubjectProcess,Person,
TwoPointRelation, Route }
2b denn
then
wie
how
Comedy
comedy
Heidelberg
Heidelberg
weiter.
continue.
CR{Genre, Town}
CR?{Genre, Town,
EmotionExperiencerSubjectProcess,Person,
TwoPointRelation, Route }
2c denn
then
wie
how
Comedy
comedy
Show
show
weiter.
continue.
CR{Genre, Genre}
CR?{Genre, Genre,
EmotionExperiencerSubjectProcess, Person,
TwoPointRelation, Route }
2d denn
then
wie
how
Comedy
comedy
weiter.
continue.
CR{Genre}
CR?{Genre,
EmotionExperiencerSubjectProcess, Person,
TwoPointRelation, Route }
2e denn
then
wie
how
komme
can
ich
I
in
in
Heidelberg
Heidelberg
weiter.
continue.
CR{MotionDirectedTransliterated, Person,
Town}
CR?{MotionDirectedTransliterated, Person,
Town, EmotionExperiencerSubjectProcess,
TwoPointRelation, Route }
Adding the conceptual context we get the results
shown in Table 3 for Example (2):
SRH recognizer parser OntoScore
2a 1 .25 .32
2b .52 .2 .48
2c .34 .2 .39
2d .35 .12 0
2e .52 .08 .71
Table 3: The scores for the SRHs of Example 2.
As evident from Table 3, CR?best corresponds to Ex-
ample 2e. This means that 2e constitutes a more
contextually coherent concept structure than the al-
ternative SRHs. This SRH was also labeled both as
the best and as a coherent SRH by the annotators.
6 Evaluation
The ONTOSCORE software runs as a module in the
SMARTKOM multi-modal and multi-domain spoken
dialogue system (Wahlster et al, 2001). The sys-
tem features the combination of speech and gesture
as its input and output modalities. The domains of
the system include cinema and TV program infor-
mation, home electronic device control as well as
mobile services for tourists, e.g. tour planning and
sights information.
ONTOSCORE operates on n-best lists of SRHs
produced by the language interpretation module out
of the ASR word graphs. It computes a numerical
ranking of alternative SRHs and thus provides an
important aid to the spoken language understand-
ing component. More precisely, the task of ON-
TOSCORE in the system is to identify the best SRH
suitable for further processing and evaluate it in
terms of its contextual coherence against the domain
and discourse knowledge.
The ONTOSCORE module currently employs two
knowledge sources, an ontology (about 730 con-
cepts and 200 relations) and a lexicon (ca. 3.600
words) with word to concept mappings, covering the
respective domains of the system. The evaluation
of ONTOSCORE was carried out on a set of 95 di-
alogues. The resulting dataset contained 552 utter-
ances resulting in 1,375 SRHs, corresponding to an
average of 2.49 SRHs per user utterance. The corpus
had been annotated by humans subjects according to
two separate annotation schemata. The results of an-
notation experiments are reported in Section 3.
6.1 Identifying the Best SRH
The task of ONTOSCORE in our multimodal dia-
logue system is to determine the best SRH from
the n-best list of SRHs corresponding to a given
user utterance. The baseline for this evaluation
was computed by adding the individual ratios of ut-
terance/SRHs - corresponding to the likelihood of
guessing the best one in each individual case - and
dividing it by the number of utterances - yielding the
overall likelihood of guessing the best one 63.91%.
The accuracy of ONTOSCORE on this task
amounts to 86.76%. This means that in 86.76%
of all cases the best SRH defined by the human
gold standard is among the best scored by the ON-
TOSCORE module. The ONTOSCORE module with-
out the conceptual context feature yields the accu-
racy of only 84.06% on the same task. This suggests
that the overall results in identifying the best SRH
in the speech recognizer output can by improved by
taking the knowledge of conceptual context into ac-
count.
6.2 Classifying the SRHs as Semantically
Coherent versus Incoherent
For this evaluation we used the same corpus, where
each SRH was labeled as being either semantically
coherent versus incoherent with respect to the previ-
ous discourse context. We defined a baseline based
on the majority class, i.e. coherent, in the corpus,
63.05%. In order to obtain a binary classification
into semantically coherent and incoherent SRHs, a
cutoff threshold must be set.
Employing a cutoff threshold of 0.44, we find that
the contextually enhanced ONTOSCORE system cor-
rectly classifies 70.98% of SRHs in the corpus. This
indicates the improvement of 7.93% over the base-
line. We also conducted the same classification ex-
periment with ONTOSCORE without using the con-
ceptual context feature. In this case we obtained
69.96% accuracy.
From these results we can conclude that the task
of an absolute classification of coherent versus inco-
herent is substantially more difficult than that of de-
termining the best SRH, both for human annotators
(see Section 3) and for ONTOSCORE. Both human
and the system?s reliability is lower in the coherent
versus incoherent classification task, which allows
to classify zero, one or multiple SRHs from one ut-
terance as coherent or incoherent. In both tasks,
however, ONTOSCORE?s performance mirrors and
approaches human performance.
7 Concluding Remarks
The contextually enhanced ONTOSCORE system
described herein automatically performs ontology-
based scoring of sets of concepts which constitute
an adequate and suitable representation of a speech
recognition hypothesis and the prior conceptual con-
text. This conceptual context is an analogous con-
ceptual representation of the previous user utterance.
To date, the algorithm has been implemented in a
software which is employed by a multi-domain spo-
ken dialogue system and applied to the task of scor-
ing n-best lists of SRH, thus producing a score ex-
pressing how well a given SRH fits within the do-
main model and the given discourse. In the evalu-
ation of our system we employed an ontology that
was not designed for this task, but already existed as
the system?s internal knowledge representation. As
shown above, the inclusion of the conceptual dis-
course context yields an improvement of almost 3%
as compared to the context-independent system.
As future work we will examine how the com-
putation of a contextual coherence score, i.e. how
well a given SRH fits within the domain model
with respect to the previous discourse, can be em-
ployed to detect domain changes in complex multi-
modal and multi-domain spoken dialogue systems.
As one would expect, a contextual coherence score
as described above actually decreases when the user
changed from one domain to another, which most
likely also accounts for a set of the actual misclassi-
fications. As a future enhancement we will integrate
and evaluate an automatic domain change detection
function, which, if activated, will cause the system
to employ the context-independent scoring function.
Currently, we are also investigating whether the pro-
posed method can be applied to scoring sets of po-
tential candidates for resolving the semantic inter-
pretation of ambiguous, polysemous and metonymic
language use (Porzel and Gurevych, 2003). Addi-
tionally, As ontology building is constly, we exam-
ine the feasibility to employ alternative knowledge
sources, that are generated automatically from cor-
pora, e.g. via self organizing maps.
Acknowledgments
There work described herein was conducted within
the SmartKom project partly funded by the German
ministry of Research and Technology under grant
01IL95I7 and by the Klaus Tschira Foundation.
References
James F. Allen, Georga Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional system. In Proceedings of Intelligent User In-
terfaces, pages 1?8, Santa Fe, NM.
James F. Allen. 1987. Natural Language Understanding.
Menlo Park, Cal.: Benjamin Cummings.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Thomas H. Cormen, Charles E. Leiserson, and Ronald R.
Rivest. 1990. Introduction to Algorithms. MIT press,
Cambridge, MA.
George Demetriou and Eric Atwell. 1994. A seman-
tic network for large vocabulary speech recognition.
In Lindsay Evett and Tony Rose, editors, Proceedings
of AISB workshop on Computational Linguistics for
Speech and Handwriting Recognition, University of
Leeds.
Ralf Engel. 2002. SPIN: Language understanding for
spoken dialogue systems using a production system
approach. In Proceedings of ICSLP 2002.
Iryna Gurevych, Robert Porzel, and Michael Strube.
2002. Annotating the semantic consistency of speech
recognition hypotheses. In Proceedings of the Third
SIGdial Workshop on Discourse and Dialogue, pages
46?49, Philadelphia, USA, July.
Iryna Gurevych, Rainer Malaka, Robert Porzel, and
Hans-Peter Zorn. 2003a. Semantic coherence scoring
using an ontology. In Proceedings of the HLT-NAACL
Conference. to appear.
Iryna Gurevych, Robert Porzel, Elena Slinko, Nor-
bert Pfleger, Jan Alexandersson, and Stefan Merten.
2003b. Less is more: Using a single knowledge rep-
resentation in dialogue systems. In Proceedings of the
HLT-NAACL?03 Workshop on Text Meaning, Edmon-
ton, Canada.
Martin Oerder and Hermann Ney. 1993. Word
graphs: An efficient interface between continuous-
speech recognition and language understanding. In
ICASSP Volume 2, pages 119?122.
Norbert Pfleger, Jan Alexandersson, and Tilman Becker.
2002. Scoring functions for overlay and their ap-
plication in discourse processing. In KONVENS-02,
Saarbru?cken, September ? October.
Robert Porzel and Iryna Gurevych. 2003. Contextual
coherence in natural language processing. Modeling
and Using Context, Springer, LNCS:to appear.
Robert Porzel, Norbert Pfleger, Stefan Merten, Markus
Lo?ckelt, Iryna Gurevych, Ralf Engel, and Jan Alexan-
dersson. 2003. More on less: Further applications of
ontologies in multi-modal dialogue systems. In Pro-
ceedings of the IJCAI?03 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems, page to ap-
pear.
Stuart J. Russell and Peter Norvig. 1995. Artificial In-
telligence. A Modern Approach. Prentice Hall, Engle-
wood Cliffs, N.J.
Richard Schwartz and Ye-Lo Chow. 1990. The n-best
algorithm: an efficient and exact procedure for finding
the n most likely sentence hypotheses. In Proceedings
of ICASSP?90, Albuquerque, USA.
Bach-Hiep Tran, Frank Seide, Volker Steinbiss, Richard
Schwartz, and Ye-Lo Chow. 1996. A word graph
based n-best search in continuous speech recognition.
In Proceedings of ICSLP?96.
Wolfgang Wahlster, Norbert Reithinger, and Anselm
Blocher. 2001. SmartKom: Multimodal communi-
cation with a life-like character. In Proceedings of the
7th European Conference on Speech Communication
and Technology., pages 1547?1550.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards developing general model of
usability with PARADISE. Natural Language Enge-
neering, 6.
Multi-Level Annotation in MMAX
Christoph Mu?ller and Michael Strube
European Media Laboratory GmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
Christoph.Mueller, Michael.Strube  @eml.villa-bosch.de
Abstract
We present a light-weight tool for the an-
notation of linguistic data on multiple lev-
els. It is based on the simplification of an-
notations to sets of markables having at-
tributes and standing in certain relations
to each other. We describe the main fea-
tures of the tool, emphasizing its simplic-
ity, customizability and versatility.
1 Introduction
In recent years, the development and use of anno-
tation tools has been a recurrent topic in corpus-
based computational linguistics. Currently, special-
ized tools for the annotation of a wide range of phe-
nomena on different levels of linguistic description
are available. In the more recent of these tools,
principles of design and implementation are real-
ized which over the years have emerged as quasi-
standards:
 XML as data storage format,
 file-level separation of base data (i.e. the data
to be annotated) from the annotation, use of
stand-off annotation (Ide and Priest-Dorman,
1996),
 implementation in Java for the sake of platform
independence.
Most of the available tools handle well the phenom-
ena on the linguistic level they are intended for, be
it coreference, dialogue acts, or discourse structure,
to name just a few. The annotations they yield do
exist independently of each other and cannot easily
be combined or applied to the same language data.
This, however, would be highly desirable because it
would allow for simultaneous browsing and anno-
tating on several linguistic levels. In addition, anno-
tation tasks could be distributed to several research
groups with different expertise, with one group spe-
cializing in e.g. dialogue act tagging, another in
coreference annotation, and so on. After completion
of the individual annotation tasks, the annotations
could be combined into one multi-level annotation
that a single group could not have produced.
The MMAX1 tool presented in this paper is in-
tended as a light-weight and highly customizable
implementation of multi-level annotation of (poten-
tially multi-modal) corpora. It is based on the as-
sumption that any annotation can be simplified to
sets of so-called markables carrying attributes and
standing in certain relations to each other. Conse-
quently, all a tool has to supply is efficient low-level
support for the creation and maintenance of mark-
ables on different levels.
The remainder of this paper is structured as fol-
lows: In Section 2 we describe in more detail the
basic concepts underlying our approach. Section 3
describes how annotation (or coding) schemes can
be defined by the user and how they are enforced
by the tool during the annotation process. Section
4 deals with how our approach extends naturally to
cover multiple linguistic levels simultaneously. Sec-
tion 5 gives a detailed description of both the tool
1MultiModal Annotation in XML. The current release ver-
sion of the tool can be downloaded at http://www.eml.org/nlp.
itself and its Discourse API which offers high-level
Java access to annotated corpora in MMAX format.
In Section 6 we briefly discuss some related work.
2 Concepts
Linguistic annotation is the process and result of
(manually) adding new information to existing lan-
guage data. This existing data can consist of writ-
ten texts (e.g. a newspaper corpus), but also of spo-
ken language (which may even be multi-modal, i.e.
contain e.g. pointing gestures). Before it can be
annotated, this data must be converted into some
machine-readable format. In addition, some rudi-
mentary structure has to be imposed on it. What is
important for both of these preprocessing steps is
that they should not alter the original data in any
way. In particular, they should not introduce ar-
bitrary decisions or implicit assumptions. Instead,
a format should be created that is as simple and
theory-neutral as possible. In our approach, written
text is simply modelled as a sequence of sentence
elements, each of which spans a number of word
elements. For spoken language (or dialogues), se-
quences of turn elements are used, each of which
spans sequences of word elements2. Since the tok-
enization into words and the segmentation into sen-
tences or turns can be performed on a mere formal
(i.e. surface-based) level, we believe these elements
to be sufficiently objective to serve as the structure
for what we call annotation base data. This is in
contrast to e.g. utterance segmentation, which has
been shown to require a considerable amount of in-
terpretation from human subjects. Therefore, we do
not support utterance elements on the level of the
annotation base data, but regard utterance segmen-
tation as one possible level of annotation.
As for the XML implementation of the annotation
base data, we simply model sentence and turn ele-
ments as XML elements with the respective name
and with two obligatory attributes: The ID attribute
assigns a unique label to each element, and the span
attribute contains a (condensed) list of IDs of those
base data elements that the sentence or turn contains.
<sentence id="sentence_1"
span="word_1..word_8"/>
2For multi-modal dialogue, turns can contain gesture ele-
ments in addition to word elements.
The   turn  element may have an additional
speaker and number attribute.
<turn id="turn_1" span="word_1..word_7"
speaker="A" number="1"/>
Each word element in the base data is modelled as
a   word  XML element with an ID attribute as the
only obligatory one. The word itself is represented
as a text child of the   word  element. If the orig-
inal language data was spoken language, this is the
transcription of the originally spoken word. In this
case, the   word  element may also have an ad-
ditional starttime and endtime attribute relating the
word to a time line.
<word id="word_1" starttime="0.000"
endtime="0.7567">
This
</word>
All elements comprising a MMAX document are
stored in a sentences or turns file and a words file
(and an additional gestures file for multimodal dia-
logue). These files define the annotation base data
and are not supposed to be modifiable through the
annotation tool.
2.1 Markables
Markables are the sole carriers of annotation infor-
mation. The concept of markable is defined in for-
mal terms only, i.e. without any implicit semantics.
A markable is simply an abstract entity which ag-
gregates an arbitrary set of elements from the base
data. It does so by means of a list of IDs of word
elements (and/or gesture elements), which are inter-
preted as pointers to the respective elements. Obvi-
ously, the question which sequences of elements are
to be represented as markables depends on the lin-
guistic level or phenomenon one is interested in: In
the case of coreference annotation, markables would
identify referring expressions in the base data, be-
cause it is on this level that information has to be
added. If the task is dialogue act tagging, markables
would be used to represent utterances.
Markables are modelled as   markable  XML
elements which are similar to   sentence  and
  turn  elements in that they consist (in their most
basic form) mainly of an ID and a span attribute.
The latter attribute, however, can be more complex
since it can reference discontinuous (or fragmented)
sequences of base data elements.
<markable id="markable_1"
span="word_1..word_5,word_7" ... />
The placeholder dots in the example above are to in-
dicate that a markable can indeed have many more
attributes. These are described in sections 2.2 and
2.3. Markables pertaining to the same linguistic
level are stored together in a markables XML file.
In its header, this file contains a reference to an an-
notation scheme XML file (cf. Section 3).
2.2 Attributes
In order to really add information to the base data,
it is not sufficient for a markable to identify the set
of elements it aggregates. It also has to associate
some attributes with them. In our approach, mark-
ables can have arbitrarily many attributes in the form
of name-value pairs. At this time, two types of at-
tributes are supported: Nominal attributes can take
one of a closed set of values, freetext attributes can
take any string (or numerical) value. The attribute
names, types and possible values to be defined de-
pend on the nature of the markables for which they
are intended: In dialogue act tagging, markables
represent utterances, thus a nominal attribute dia-
logue act with possible values like initiation, re-
sponse, and preparation etc. would be relevant.
On the XML level, attributes are expressed in the
standard name="value" format on markable ele-
ments in the markables file. Note, however, that both
the type of the attributes and their possible values
(for nominal attributes) cannot be determined from
the markables file alone, but only with reference to
the annotation scheme (cf. Section 3) linked to it.
2.3 Relations
While markables and their attributes are sufficient
to add information to independent sequences of
base data elements, they cannot relate these to
each other for the expression of structural infor-
mation. Therefore, our approach is complemented
by a means to express relations between mark-
ables. Currently, attributes of type member-relation
and pointer-relation are supported. Attributes of
type member-relation express undirected relations
between arbitrary many markables. This relation
can be interpreted as set-membership, i.e. mark-
ables having the same value in an attribute of type
member-relation constitute an unordered set. At-
tributes of type pointer-relation, on the other hand,
express directed relations between single source
markables and arbitrarily many target markables. As
the name suggests, this relation can be interpreted
as the source markable pointing to its target mark-
able(s). It is important to note that member-relation
and pointer-relation are not attributes themselves.
Rather, they are types of attributes (like nominal and
freetext) which can be realized by attributes of arbi-
trary names. That means that for one markable, sev-
eral different attributes of type member- and pointer-
relation can be defined within the same annota-
tion scheme. The attribute type simply defines how
these attributes are interpreted. Like the concept
of markable itself, relations are also defined only
formally, i.e. without any semantic interpretation.
Like markables, relations can be associated with any
kind of semantic interpretation, depending on the
annotation task at hand: For coreference annota-
tion, it would be natural to use a member-relation
attribute coref class to model classes of coreferring
expressions. In addition, a (binary) pointer-relation
attribute antecedent could be employed to anno-
tate the direct antecedent of a coreferring expres-
sion. As another example, if the task is annotating
the predicate-argument structure of verbs, (binary)
pointer-relation attributes like subject, direct object
and indirect object could be used to link a verb to its
arguments.
On the XML level, relations are expressed like
normal attributes, with the only difference that their
values are (lists of) markable element IDs (pointer-
relation) or strings of the form set x (member-
relation).
<markable id="markable_2"
span="word_14..16"
coref_class="set_4"
antecedent="markable_1" ... />
3 Annotation Schemes
Even on the same linguistic level, not every at-
tribute or relation is applicable all the time or to
every kind of markable. In coreference annotation,
e.g., a markable that has been explicitly annotated
as discourse-initial should not be allowed to have
an antecedent attribute. Along the same lines, in
predicate-argument structure annotation, a so-called
weather-verb like ?rain? should not be allowed to
have a pointer to its subject. Restricting the avail-
ability of attributes to only those that make sense in
a particular situation is an important means to ensure
annotation quality and consistency.
Dependencies of this kind can best be captured
by formulating constraints on which attributes can
occur together or which are mutually exclusive. In
our approach, constraints of various types can be
modelled in the annotation scheme. Generally, an-
notation (or coding) schemes are of central im-
portance to any annotation task. They describe
which phenomena are to be annotated using which
set of attributes. Within the MMAX tool, annota-
tion scheme development has been of special im-
portance, because the expressiveness and the de-
gree of customizability of the annotation scheme
strongly determine how versatile and generally ap-
plicable the tool is. The mechanism for defin-
ing and handling annotation schemes described in
what follows has been developed in collaboration
with the Brazilian-French project COMMOn-REFs
(Unisinos, Sa?o Lepoldo-RS, Brazil; LORIA/INRIA,
Nancy, France) (Salmon-Alt and Vieira, 2002).
An annotation scheme defines all attributes (nom-
inal, freetext, member-relation and pointer-relation)
valid for a linguistic level. It specifies possible val-
ues for nominal attributes, and it identifies default
attribute values. Attributes can be either branch-
ing or non-branching: If an attribute is branching,
its current value influences which other attributes
are available. In a branching nominal attribute, at
least one of the possible values is associated with
a reference to one or more following attributes. In
a branching freetext, member-relation or pointer-
relation attribute, on the other hand, at most two
references to following attributes are possible, de-
pending on whether the attribute does or does not
have a value. Consider the following example (see
Figure 1 for an illustration): In recent work dealing
with pronoun resolution in spoken dialogue (Strube
and Mu?ller, 2003), different types of expressions
(noun phrases, verb phrases, whole utterances and
disfluencies) had to be annotated. They were distin-
guished by setting for each expression the appropri-
ate value in a nominal attribute called Expressions-
Type. Since noun phrases have different attributes
than e.g. verb phrases, the attribute ExpressionsType
was a branching one because each of its possible val-
ues referenced a partially3 different set of following
attributes: For noun phrases, the whole range of lin-
guistic features like case, grammatical role, seman-
tic role, etc. is relevant, while e.g. verb phrases and
utterances (for our purposes) needed only be distin-
guished according to their type (attributes VP Type
resp. Utt. Type). For unclassified expressions (none)
and disfluencies, on the other hand, no further at-
tributes were defined at all.
Comment
Exp. Type
none
vp uttnp
disfluency
Member Member Member
Pointer
Case
Gram. Role
Sem. Role
Pointer Pointer
VP Type Utt. Type
Figure 1: Annotation Scheme Diagram (Fragment)
Attributes that are referenced by branching at-
tributes (e.g. Member, Pointer, Case, VP Type) are
dependent in that they are only valid and accessible
if the corresponding value is selected on the branch-
ing attribute (i.e. ExpressionsType). Thus, the avail-
ability of attributes can effectively be constrained.
Since an attribute that is dependent on some other at-
tribute can itself be branching, arbitrarily many lev-
els of dependency are possible.
An annotation scheme of the above form can also
be described as an annotation tree, where each node
in the tree consists of a number of non-branching
and (optionally) one branching attribute. If a node
does have a branching attribute, the dependent at-
tributes it references can be seen as child nodes. If
a node does not have a branching attribute, it corre-
sponds to a leaf in the annotation tree.
3The Member and Pointer attribute applies to noun phrases,
verb phrases and utterances.
4 Levels
In Section 2, the linguistic levels of coreference, di-
alogue acts and predicate-argument structure were
used for illustrative purposes. It was demonstrated
how these different linguistic phenomena can be rep-
resented by means of a few simple concepts. The
following section deals with how the same concepts
lend themselves to the simultaneous representation
of multiple levels of linguistic description.
Among others, the following levels of linguistic
description could be envisaged:
 morpho-syntax,
 syntax,
 valency/predicate-argument structure,
 coreference,
 dialogue acts,
 prosody/intonation,

. . .
Relating e.g. the utterance level to the coreference
level could be done, for instance, in order to find out
whether utterance boundaries in spoken dialogues
can be used to narrow down the search space for an-
tecedents of discourse-deictic anaphors. Along sim-
ilar lines, the prosody or intonation level could pro-
vide relevant information as well.
Though it would be tempting to merge markable
files from different levels, this would have some se-
rious disadvantages. First of all, subsequent modifi-
cation or removal of a level would be cumbersome.
Moreover, alternative versions of the same level (e.g.
utterance segmentations performed by different an-
notators) cannot easily be compared without having
to duplicate the other levels. For these reasons, our
approach favours the separation of the different de-
scription levels to different markables files.
Since markables (as we define them) are not di-
rectly embedded into the base data, but reference
base data elements by means of their span attribute,
the simultaneous application of several description
levels is straightforward: Given some annotation
base data set, sets of markables pertaining to dif-
ferent description levels can simply be applied to it,
i.e. be allowed to access the base data elements they
reference, thus adding level by level of annotation.
Since markables on different levels are related only
indirectly by virtue of shared base data elements, is-
sues like overlap or discontinuous elements do not
arise. This is made possible through what can be
seen as a rigorous implementation of the principle of
stand-off annotation (Ide and Priest-Dorman, 1996;
Thompson and McKelvie, 1997).
5 MMAX
5.1 The Annotation Tool
The MMAX annotation tool is written in Java. XML
and XSL functionality is supplied by the Apache4
Xerces and Xalan engines. The Java executable of
the tool itself is very small (ca. 300 kb). Installing
the tool (under Windows or Linux) is done by simply
extracting a directory structure to the local hard disk;
no further installation is required. Figure 2 shows a
typical annotation situation with the most important
GUI elements being visible, i.e. (clockwise, begin-
ning in the upper left corner): the main annotation
window, the SearchWindow, and the AttributeWin-
dow. In the SearchWindow, a query for 3rd per-
son neuter personal and possessive pronouns with
oblique case is specified. Attributes can be queried
by either selecting the desired value from a list, or by
specifying a regular expression. The AttributeWin-
dow shows the annotation scheme described in Fig-
ure 1.
Up to now, MMAX has been used for the creation
of several annotated corpora, e.g. uni-modal text-
only corpora (Salmon-Alt and Vieira, 2002; Mu?ller
et al, 2002; Strube and Mu?ller, 2003) and multi-
modal human-machine corpora (Mu?ller and Strube,
2001; Rapp and Strube, 2002).
In order to minimize the tool?s system require-
ments and maximize its performance, we deliber-
ately chose to use a text-only display (as opposed to
an HTML display). This imposes a couple of restric-
tions with respect to user interaction. We distinguish
between the display of content-bearing vs. merely
layout information:
Content-bearing information is conveyed by
markables and their properties. Within MMAX, it
4http://www.apache.org
Figure 2: MMAX Screenshot
is visualized by means of text foreground and back-
ground colours and graphical arrows (for relations
between markables). User actions like selecting,
adding or deleting a markable, adding a relation be-
tween two markables, or modifying a markable?s at-
tributes change the content-bearing information and
thus require frequent display updates. The MMAX
display offers hard-coded (and highly optimized)
methods for manipulating text colour and for draw-
ing lines between markables. Thus we achieve very
good performance (i.e. low response time) for these
by far most frequent types of user interactions.
Layout information, on the other hand, contains
formal properties of the display only. It includes
mainly line breaks and indentations, but also font
style properties like bold, italic, and underlined.
Within MMAX, the XSL style sheet supplied in
the .MMAX project file is responsible for rendering
the display layout. By modifying this style sheet,
the user can customize the display, e.g. by insert-
ing pseudo-HTML tags like   bold  ...   /bold 
or   italic  ...   /italic  . During a MMAX
session, changes to the layout can only be made
by explicitly reinvoking the style sheet processor,
which, depending on the data and style sheet com-
plexity, can take several seconds. In contrast to
content-bearing information, however, layout infor-
mation is not expected to require frequent updates.
Utterance segmentation is one example of how the
display layout might change as a result of markables
being added to the annotation, i.e. if the user wishes
to have line breaks inserted directly after markables
respresenting utterances. This, however, can be per-
formed reasonably well if the user does not rebuild
the display after each single markable creation, but
only after each, say, five or ten.
A MMAX session is started by opening a
.MMAX project file. This file contains references
to all files comprising a MMAX document:
 one sentences or turns XML file,
 a words XML file (and/or a gestures file),
 a list of markables XML files,
 an XSL style sheet file for rendering the layout
of the MMAX display,
 an XML file specifying colour attributes for
rendering the appearance of markables depend-
ing on their content.
When a .MMAX project file is opened, the tool
first builds an XML DOM tree representation of the
information supplied in the base data files. For the
whole session, this tree serves as the read-only ?scaf-
fold? to which annotations (given in the form of one
or more markables files) are applied. Then, depend-
ing on which annotation levels the user chose to
view, information about markables from these lev-
els is added to the DOM tree as well. The DOM tree
is then submitted to the XSL style sheet for transfor-
mation into a single string, which is then converted
in a Java object of type StyledDocument. In the last
step, markables in the StyledDocument are coloured
according to their initial attributes, and the Styled-
Document is finally presented to the user by assign-
ing it to the MMAX display.
Users can explicitly activate markables on differ-
ent annotation levels. Only if a level is active, mark-
ables on this level are displayed and can be accessed
or modified. Users can select an active markable by
left-clicking it. If the click is ambiguous, a popup
menu is displayed containing all active markables in
the clicked position. In this menu, markables are
tagged with their respective level, so that users can
easily select markables from a particular level (with-
out having to temporarily deactivate all other levels).
Once a markable is selected, its attributes are
displayed in a separate AttributeWindow. In addi-
tion, if it has a non-empty value for some member-
relation or pointer-relation attribute, those are visu-
alized by means of arrows drawn on the MMAX dis-
play. The AttributeWindow has access to the anno-
tation scheme defined for the markable it currently
displays. This enables the AttributeWindow to per-
form a consistency check on each markable by try-
ing to ?reproduce? the annotation process that lead to
this markable having this set of attributes.5 It does
so by traversing the annotation tree, beginning at the
root, and recursively trying to match the attributes
of the markable to the attributes defined at the cur-
rent annotation tree node. If an attribute could be
matched, it is consumed, and the AttributeWindow
5Thanks to Caroline Varaschin Gasperin (Unisinos, Sa?o
Lepoldo-RS, Brazil) for providing some initial ideas on this.
is changed so that dependent attributes are accessi-
ble. If the matching process terminates before all
attributes have been consumed, an annotation error
or inconsistency has been detected. The same is true
if an undefined attribute value is found on the mark-
able. In both cases, a warning message is displayed
to the user. Within MMAX, the AttributeWindow is
the central location where the annotation scheme is
enforced. Figure 3 gives an idea of the internal rela-
tions between different MMAX components. Bold
boxes represent GUI elements.
Markable
Annotation Scheme
annotation tree
Valid path through
n 1
has
1
n
containsn
n
sets properties for
1
1
specifies
n
1
1
1
finds
displays
displays
11
specifies 1
n
complies with
1
1
1
defines
contains
1
Valid A?V pairs
Attribute Window
Search Window
Level
1
n
1
1
displays
Document
MMAX tool
Figure 3: MMAX Components
Creating a new markable works straightforwardly
by selecting a sequence of text on the display and
right-clicking it. If only one annotation level is ac-
tive, a pop-up menu with only one option, i.e. creat-
ing a markable on this level, will apear. Otherwise,
the menu will contain options for creating a mark-
able on any one currently active level.
When a newly created markable is selected, it
does not have any attributes except for those de-
fined on the root node of the annotation tree. The
AttributeWindow utilizes the order of the annotation
tree nodes to guide the user through the annotation
process, allowing only those attributes to be dis-
played that are valid in the current annotation situa-
tion. As an additional help, each attribute and each
value in the annotation scheme can have a textual de-
scription associated with it. During annotation, this
description will be displayed to the user when they
move the mouse over the corresponding item in the
AttributeWindow.
Creation and deletion of relations between mark-
ables is performed in two steps: First, the source
markable is selected as described above. Then
the target markable is selected by right-clicking it.
Then, another pop-up menu appears, the options
of which depend on which relations have been de-
fined for the source and target markable: If one or
more member-relation attributes are defined for both
markables, the user will have the option of adding
the target markable to the set of the source mark-
able (if it is already a member of one). If one or
more pointer-relation attributes are defined for the
source markable, the user will also have the option
of pointing to the target markable. Deleting relations
between markables works analogously. After each
modification, the display is refreshed in order to re-
flect changes to the selected markable?s attributes.
5.2 The Discourse API
The MMAX Discourse API6 is intended as a plat-
form for the exploitation and reuse of annotated doc-
uments in MMAX format. It maps the elements of
the base data and the markables to Java classes and
defines a set of basic operations to be performed
on them. The entire document is wrapped in a
Java Discourse object which serves as the single
entry point. The Discourse object itself is created
from the .MMAX project file by a DiscourseLoader
class which parses the XML files and resolves ref-
erences between elements. The result is a tree-like
structure which can be navigated by accessing ele-
ments on a particular level and retrieving their child
elements, which are Java objects themselves and
can thus be used as entry points to their child el-
ements as well. Consider the following example:
getSentenceCount(), when called on the Discourse
object, returns the number of sentences in the cur-
rent document. This number can be used to iter-
ate over all those elements by means of the getSen-
tence(position) method, which returns the sentence
at position as a Java Sentence object. Calling get-
WordCount() on this object returns the number of
word elements the current sentence contains. get-
Word(position) returns the word at position as a Java
Word object. These objects contain, among other
things, a getMarkables() method which returns a list
6This section is based on (Mu?ller and Strube, 2002) where
an earlier version of the MMAX Discourse API is described in
more detail.
of all markables (as Java Markable objects) a word
is part of. Alternatively, getMarkables(level) returns
only those markables on a particular level. On the
level of Markable objects, the API contains a set
of basic methods for e.g. retrieving attribute val-
ues. It also supports procedures like determining the
formal relation between markables (identity, embed-
ding, overlap, and the like).
6 Related Work
The work described in this paper is relevant for two
distinct yet related topics: Representation models
for linguistic data and development of annotation
tools proper.
6.1 Linguistic Representation Models
The Annotation Graph model (Bird and Liberman,
2001) is a current representation model for linguis-
tic data. Within this model, annotations are repre-
sented as labelled arcs connecting nodes on a com-
mon timeline. Each arc is associated with a particu-
lar type (like a phone, word, dialogue act, etc.), and
a set of attribute-value pairs. While they are sim-
ilar to MMAX markables in this respect, Annota-
tion Graphs are much more powerful since they can
model any phenomenon which can be mapped to se-
quentially aligned elements with a temporal exten-
sion. On the other hand, the dependence on time-
aligned data might make it more difficult to model
corpora without time stamps, like e.g. written text
corpora. In principle, however, our approach and
the Annotation Graph model serve rather different
purposes: the former has been primarily designed
as the internal representation format for the MMAX
tool, and turned out to be useful as an independent
representation model as well, while the ambition of
the latter has been to create a general purpose model
for the unification of diverse annotation system for-
mats. Due to their similarity, however, both models
are compatible with each other, and conversion from
one into the other should be possible.
6.2 Annotation Tools
The NITE7 (Bernsen et al, 2002) initiative is a
project in which a workbench for multi-level, cross-
level and cross-modality annotation of language data
7http://nite.nis.sdu.dk
is developed. It is comparable to our tool only in
that it explicitly addresses the simultaneous annota-
tion on different levels. It is, however, much more
ambitious than MMAX, both with respect to its in-
tended scope of functionality and the features it of-
fers for display customization. For instance, NITE
offers plug-ins for speech signal visualization and
even video annotation (Soria et al, 2002): The lat-
ter allows the user/annotator to insert information di-
rectly into the video data. In contrast to that, MMAX
only supports read-only access for playback of au-
dio (and possibly video) files associated with indi-
vidual sentences or turns in the base data. NITE is
even more advanced with respect to the display ca-
pabilities. Users have at their disposal not only plain
text elements, but more powerful devices like tables,
list, buttons and the like, which can be used to create
highly functional displays by means of XSL style
sheets. The downside, however, appears to be that
even minor changes to the elements displayed make
it necessary to reinvoke the style sheet processor,
which may become time-critical for long or more
complex documents. The NITE workbench, which
still appears to be in a demo or prototype stage, is
implemented in C++ and runs only on the Windows
platform. This decision might be motivated by per-
formance requirements resulting from the features
mentioned above.
Apart from NITE, a number of smaller and more
specialized tools for the annotation of individual lin-
guistic phenomena exist, many of which are publicly
available. The Linguistic Annotation website8 con-
tains pointers to a large number of those.
7 Conclusions
This paper presented the MMAX annotation tool
which is based on the following major considera-
tions. On the theoretical side, there is the simplifica-
tion of annotations to a set of simple concepts based
on the notion of markable. Markables are versatile
in the sense that almost any kind of annotation can
be expressed through them. In addition, arbitrarily
many markables can refer to the same sequence of
data without interfering with each other, even if they
are overlapping or discontinuous. This makes it pos-
sible to use them for annotation of various levels of
8http://www.ldc.upenn.edu/annotation
linguistic description simultaneously. Another theo-
retical issue in the design of MMAX is its ability to
express and enforce highly customizable annotation
schemes. On the practical side, a main design fea-
ture is the deliberate restriction of the display capa-
bilities. This, taken together with the rather simple
markable concept, made it possible to implement a
display which is quickly updatable and thus easily
and conveniently usable, even if more than one an-
notation level (i.e. markables file) is displayed at the
same time. The tool is implemented in Java, which
has the additional advantage of being platform in-
dependent and easily extensible. We believe that all
this taken together outweighs the disadvantages of a
slightly ?impoverished? display.
Acknowledgements. The work presented here has
been partially funded by the German Ministry of
Research and Technology as part of the EMBASSI
project (01 IL 904 D/2) and by the Klaus Tschira
Foundation. We would like to thank the researchers
from the COMMOn-REFs project, in particular Car-
oline Varaschin Gasperin, for their useful criticism
and ideas on improving MMAX.
References
Niels Ole Bernsen, Laila Dybkjaer, and Mykola Kolod-
nytsky. 2002. THE NITE WORKBENCH ? A tool
for the annotation of natural interactivity and multi-
modal data. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
Las Palmas, Canary Islands, Spain, 29-31 May, 2002,
pages 43?49.
Stephen Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech Commu-
nication, 33(1):23?60.
Nancy Ide and Greg Priest-Dorman. 1996. The corpus
encoding standard. http://www.cs.vassar.edu/CES.
Christoph Mu?ller and Michael Strube. 2001. MMAX:
A tool for the annotation of multi-modal corpora. In
Proceedings of 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Wash., 5 August 2001, pages 45?50.
Christoph Mu?ller and Michael Strube. 2002. An API
for discourse-level access to XML-encoded corpora.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Las Palmas, Ca-
nary Islands, Spain, 29-31 May, 2002, pages 26?30.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying Co-Training to reference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadelphia,
Penn., 7?12 July 2002, pages 352?359.
Stefan Rapp and Michael Strube. 2002. An iterative data
collection approach for multimodal dialogue systems.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Las Palmas, Ca-
nary Islands, Spain, 29-31 May, 2002, pages 661?665.
Susanne Salmon-Alt and Renata Vieira. 2002. Nomi-
nal expressions in multilingual corpora: Definites and
demonstratives. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation, Las Palmas, Canary Islands, Spain, 29-31 May,
2002, pages 1627?1634.
Claudia Soria, Niels Ole Bernsen, Niels Cade?e, Jean Car-
letta, Laila Dybkjaer, Stefan Evert, Ulrich Heid, Amy
Isard, Mykola Kolodnytsky, Christoph Lauer, Wolf-
gang Lezius, Lucas P.J.J. Noldus, Vito Pirrelli, Norbert
Reithinger, and Andreas Vo?gele. 2002. Advanced
tools for the study of natural interactivity. In Pro-
ceedings of the 3rd International Conference on Lan-
guage Resources and Evaluation, Las Palmas, Canary
Islands, Spain, 29-31 May, 2002, pages 357?363.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, 7?12 July 2003. To appear.
Henry S. Thompson and David McKelvie. 1997. Hy-
perlink semantics for standoff markup of read-only
documents. In Proceedings of SGML Europe ?97,
Barcelona, Spain, May 1997.
Representing and Accessing Multi-Level Annotations in MMAX2
Christoph Mu?ller
EML Research gGmbH
Villa Bosch
Schlo?-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
christoph.mueller@eml-research.de
1 Introduction
MMAX21 is a versatile, XML-based annotation
tool which has already been used in a variety of an-
notation projects. It is also the tool of choice in the
ongoing project DIANA-Summ, which deals with
anaphora resolution and its application to spoken
dialog summarization. The project uses the ICSI
Meeting Corpus (Janin et al, 2003), a corpus of
multi-party dialogs which contains a considerable
amount of simultaneous speech. It features a semi-
automatically generated segmentation in which
the corpus developers tried to track the flow of the
dialog by inserting segment starts approximately
whenever a person started talking. As a result, the
corpus has some interesting structural properties,
most notably overlap, that are challenging for an
XML-based representation format. The following
brief overview of MMAX2 focuses on this aspect,
using examples from the ICSI Meeting Corpus.
2 The MMAX2 Data Model
Like most current annotation tools, MMAX2
makes use of stand-off annotation. The base data
is represented as a sequence of <word> elements
with exactly one PCDATA child each. Normally,
these PCDATA children represent orthographical
words, but larger or smaller units (e.g. characters)
are also possible, depending on the required gran-
ularity of the representation.2
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE words SYSTEM "words.dtd">
<words>
...
<word id="word_3710">That</word>
<word id="word_3711">?s</word>
<word id="word_3712">just</word>
<word id="word_3713">a</word>
<word id="word_3714" meta="true">Pause</word>
<word id="word_3715">specification</word>
<word id="word_3716">for</word>
<word id="word_3717">the</word>
<word id="word_3718">X_M_L</word>
<word id="word_3719">Yep</word>
<word id="word_3720">.</word>
1http://mmax.eml-research.de
2The meta attribute in the example serves to distinguish
meta information from actual spoken words.
<word id="word_3721">format</word>
<word id="word_3722">.</word>
...
</words>
The order of the elements in the base data file is
determined by the order of the segments that they
belong to.
Annotations are represented in the form of
<markable> elements which reference one or
more base data elements by means of a span at-
tribute. Each markable is associated with exactly
one markable level which has a unique, descrip-
tive name and which groups markables that be-
long to the same category or annotation dimen-
sion. Each markable level is stored in a sepa-
rate XML file which bears the level name as an
XML name space. The ID of a markable must be
unique for the level that it belongs to. Markables
can carry arbitrarily many features in the common
attribute=value format. It is by means of
these features that the actual annotation informa-
tion is represented. For each markable level, ad-
missible attributes and possible values are defined
in an annotation scheme XML file (not shown).
These annotation schemes are much more power-
ful for expressing attribute-related constraints than
e.g. DTDs. The following first example shows the
result of the segmentation of the sample base data.
The participant attribute contains the identi-
fier of the speaker that is associated with the re-
spective segment.
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE markables SYSTEM "markables.dtd">
<markables xmlns="www.eml.org/NameSpaces/segment">
...
<markable id="markable_468"
span="word_3710..word_3714"
participant="me012"/>
<markable id="markable_469"
span="word_3715..word_3718"
participant="me012"/>
<markable id="markable_470"
span="word_3719..word_3720"
participant="mn015"/>
<markable id="markable_471"
span="word_3721..word_3722"
participant="me012"/>
...
</markables>
73
The next example contains markables representing
the nominal and verbal chunks in the sample base
data.
<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE markables SYSTEM "markables.dtd">
<markables xmlns="www.eml.org/NameSpaces/chunks">
...
<markable id="markable_7834"
span="word_3710"
type="demonstrative"/>
<markable id="markable_7835"
span="word_3711"
type="copula"/>
<markable id="markable_7836"
span="word_3713..word_3715"
type="nn"/>
<markable id="markable_7837"
span="word_3711..word_3715"
type="predication"
subject="markable_7834"/>
<markable id="markable_7838"
span="word_3717..word_3718,word_3721"
type="nn"/>
...
</markables>
The following basic facts about markables in
MMAX2 are worth noting:
1. Every markable is defined with reference to
the base data. Markables on the same or different
levels are independent and ignorant of each other,
and only related indirectly, i.e. by means of base
data elements that they have in common.3 Struc-
tural relations like embedding ([[?s] just [a speci-
fication]]) can only be determined with recourse
to the base data elements that each markable
spans. This lazy representation makes it simple
and straightforward to add markables and entire
markable levels to existing annotations. It is also a
natural way to represent non-hierarchical relations
like overlap between markables. For example, a
segment break runs through the nominal chunk
represented by markable markable 7836 ([a
specification]) in the example above. If the seg-
ment markables were defined in terms of the mark-
ables contained in them, this would be a prob-
lem because the nominal chunk crosses a segment
boundary. The downside of this lazy representa-
tion is that more processing is required for e.g.
querying, when the structural relations between
markables have to be determined.
2. Markables can be discontinuous. A markable
normally spans a sequence of base data elements.
Each connected subsequence of these is called a
markable fragment. A discontinuous markable is
one that contains more than one fragment, like
3Note that this merely means that markables are not
defined in terms of other markables, while they can in-
deed reference each other: In the above example, markable
markable 7837 ([?s just a specification]) uses an associa-
tive relation (in this case named subject) to represent a ref-
erence to markable markable 7834 ([That]) on the same
level. References to markables on other levels can be repre-
sented by prefixing the markable ID with the level name.
markable markable 7838 ([the XML format])
above. Actually, this markable exemplifies what
could be called discontinuous overlap because it
does not only cross a segment boundary, but it also
has to omit elements from an intervening segment
by another speaker.
3 Accessing Data From Within MMAX2
3.1 Visualization
When a MMAX2 document is currently loaded,
the main display contains the base data text plus
annotation-related information. This information
can comprise
? line breaks (e.g. one after each segment),
? markable feature?s values (e.g. the
participant value at the beginning
of each segment),
? literal text (e.g. a tab character after the
participant value),
? markable customizations, and
? markable handles.
The so-called markable customizations are in
charge of displaying text in different colors, fonts,
font styles, or font sizes depending on a mark-
able?s features. The order in which they are ap-
plied to the text is determined by the order of
the currently available markable levels. Mark-
able customizations are processed bottom-up, so
markable levels should be ordered in such a way
that levels containing smaller elements (e.g. POS
tags) should be on top of those levels contain-
ing larger elements (chunks, segments etc.). This
way, smaller elements will not be hidden by larger
ones.
When it comes to visualizing several, poten-
tially embedded or overlapping markables, the so-
called markable handles are of particular impor-
tance. In their most simple form, markable han-
dles are pairs of short strings (most often pairs
of brackets) that are displayed directly before and
after each fragment of a markable. When two
or more markables from different levels start at
the same base data element, the nesting order of
the markables (and their handles) is determined
on the basis of the order of the currently avail-
able markable levels. The color of markable han-
dles can also be customized depending on a mark-
able?s features. Figure 1 gives an idea of what the
74
Figure 1: Highlighted markable handles on a discontinuous (left) and an overlapping (right) markable.
MMAX2 main window can look like. Both han-
dles and text background for chunk markables
with type=predication are rendered in light
gray. Other handles are rendered in a darker color.
Markable handles are sensitive to mouse events:
resting the mouse pointer over a markable handle
will highlight all handles of the pertaining mark-
able. Reasonable use of markable customizations
and handles allows for convenient visualization of
even rather complex annotations.
3.2 Querying
MMAX2 includes a query console which can be
used to formulate simple queries using a special
multi-level query language called MMAXQL. A
query in MMAXQL consists of a sequence of
query tokens which describe elements (i.e. either
base data elements or markables) to be matched,
and relation operators which specify which rela-
tion should hold between the elements matched
by two adjacent query tokens. A single markable
query token has the form
string/conditions
where string is an optional regular expression
and conditions specifies which features(s) the
markable should match. The most simple condi-
tion is just the name of a markable level, which
will match all markables on that level. If a regular
expression is also supplied, the query will return
only the matching markables. The query
[Aa]n?\s.+/chunks
will return all markables from the chunks level
that begin with the indefinite article4. Markables
with particular features can be queried by specify-
ing the desired attribute-value combinations. The
4The space character in the regular expression must be
masked as \s because otherwise it will be interpreted as a
query token separator.
following query e.g. will return all markables from
the chunks level with a type value of either nn
or demonstrative:
/chunks.type={nn,demonstrative}
If a particular value is defined for exactly one at-
tribute on exactly one markable level only, both
the level and the attribute name can be left out in
a query, rendering queries very concise (cf. the ac-
cess to the meta level below).
Relation operators can be used to connect
two query tokens to form a complex query.
The set of supported sequential and hierarchi-
cal relation operators5 includes meets (default),
starts, starts with, in, dom, equals,
ends, ends with, overlaps right, and
overlaps left. Whether two markables stand
in a certain relation is determined with respect
to the base data elements that they span. In the
current early implementation, for all markables
(including discontinuous ones), only the first and
last base data element is considered. The re-
sult of a query can directly be used as the input
to another query. The following example gives
an idea of what a more complex query can look
like. The query combines the segment level, the
meta level (which contains markables represent-
ing e.g. pauses, emphases, or sounds like breath-
ing or mike noise), and the base data level to re-
trieve those instances of you know from the ICSI
Meeting corpus that occur in segments spoken by
female speakers6 which also contain a pause or an
emphasis (represented on the meta level):
?[Yy]ou know? in (/participant={f.*} dom /{pause,emphasis})
The next query shows how overlap can be han-
5Associative relations are not discussed here, (Mu?ller,
2005).
6The first letter of the participant value encodes the
speaker?s gender.
75
dled. It retrieves all chunk markables along with
their pertaining segments by getting two partial
lists and merging them using the operator or.
(/chunks in /segment) or (/chunks overlaps_right /segment)
4 Accessing Data by Means of XSL
MMAX2 has a built-in XSL style sheet proces-
sor7 that can be used from the console to read
a MMAX2 document and process it with a user-
defined XSL style sheet. The XSL processor pro-
vides some special extensions for handling stand-
off annotation as it is realized in MMAX2. In the
current beta implementation, only some basic ex-
tensions exist. The style sheet processor can be
called from the console like this:
org.eml.MMAX2.Process -in INFILE.mmax -style STYLEFILE.xsl
The root element of each MMAX2 document is
the words element, i.e. the root of the base data
file, which will be matched by the supplied XSL
style sheet?s default template. The actual process-
ing starts in the XSL template for the word ele-
ments, i.e. the root element?s children. A minimal
template looks like this:
<xsl:template match="word">
<xsl:text> </xsl:text>
<xsl:apply-templates
select="mmax:getStartedMarkables(@id)"
mode="opening"/>
<xsl:value-of select="text()"/>
<xsl:apply-templates
select="mmax:getEndedMarkables(@id)"
mode="closing"/>
</xsl:template>
The above template inserts a white space before
the current word and then calls an extension func-
tion that returns a NodeList containing all mark-
ables starting at the word. The template then
inserts the word?s text and calls another exten-
sion function that returns a NodeList of markables
ending at the word. The markables returned by
the two extension function calls are themselves
matched by XSL templates. A minimal template
pair for matching starting and ending markables
from the chunks level and enclosing them in
bold brackets (using HTML) looks like this:
<xsl:template match="chunks:markable" mode="opening">
<b>[</b>
</xsl:template>
<xsl:template match="chunks:markable" mode="closing">
<b>]</b>
</xsl:template>
Note how the markable level name (here:
chunks) is used as a markable name space
to control which markables the above templates
should match. The following templates wrap a
pair of <p> tags around each markable from
7Based on Apache?s Xalan
the segment level and adds the value of the
participant attribute to the beginning of each.
<xsl:template match="segment:markable" mode="opening">
<xsl:text disable-output-escaping="yes">&lt;p&gt;</xsl:text>
<b>
<xsl:value-of select="@participant"/>
</b>
</xsl:template>
<xsl:template match="segment:markable" mode="closing">
<xsl:text disable-output-escaping="yes">&lt;/p&gt;</xsl:text>
</xsl:template>
Creating HTML in this way can be useful for
converting a MMAX2 document with multiple
levels of annotation to a lean version for distrib-
ution and (online) viewing.
5 Conclusion
MMAX2 is a practically usable tool for multi-
level annotation. Its main field of application is
the manual creation of annotated corpora, which
is supported by flexible and powerful means of vi-
sualizing both simple and complex (incl. overlap-
ping) annotations. MMAX2 also features a sim-
ple query language and a way of accessing anno-
tated corpora by means of XSL style sheets. While
these two data access methods are somewhat lim-
ited in scope, they are still useful in practice. If
a query or processing task is beyond the scope of
what MMAX2 can do, its simple and open XML
data format allows for easy conversion into other
XML-based formats, incl. those of other annota-
tion and query tools.
Acknowledgements
This work has partly been funded by the Klaus
Tschira Foundation (KTF), Heidelberg, Germany,
and by the Deutsche Forschungsgemeinschaft
(DFG) in the context of the project DIANA-Summ
(STR 545/2-1).
References
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke & C. Wooters (2003). The ICSI Meeting Cor-
pus. In Proceedings of the IEEE International Con-
ference on Acoustics, Speech and Signal Processing,
Hong Kong, pp. 364?367.
Mu?ller, C. (2005). A flexible stand-off data model with
query language for multi-level annotation. In Pro-
ceedings of the Interactive Posters/Demonstrations
session at the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, Mi.,
25-30 June 2005, pp. pp. 109?112.
76
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1338?1347,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Study on the Semantic Relatedness of Query and Document Terms in
Information Retrieval
Christof M?uller and Iryna Gurevych
Ubiquitous Knowledge Processing (UKP) Lab
Computer Science Department
Technische Universit?at Darmstadt, Hochschulstra?e 10
D-64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de/
Abstract
The use of lexical semantic knowledge in
information retrieval has been a field of ac-
tive study for a long time. Collaborative
knowledge bases like Wikipedia and Wik-
tionary, which have been applied in com-
putational methods only recently, offer
new possibilities to enhance information
retrieval. In order to find the most bene-
ficial way to employ these resources, we
analyze the lexical semantic relations that
hold among query and document terms
and compare how these relations are repre-
sented by a measure for semantic related-
ness. We explore the potential of different
indicators of document relevance that are
based on semantic relatedness and com-
pare the characteristics and performance
of the knowledge bases Wikipedia, Wik-
tionary and WordNet.
1 Introduction
Today we face a rapidly growing number of elec-
tronic documents in all areas of life. This demands
for more effective and efficient ways of searching
these documents for information. Especially user-
generated content on the web is a growing source
of huge amounts of data that poses special diffi-
culties to IR. The precise wording is often difficult
to predict and current information retrieval (IR)
systems are mainly based on the assumption that
the meaning of a document can be inferred from
the occurrence or absence of terms in it. In or-
der to yield a good retrieval performance, i.e., re-
trieving all relevant documents without retrieving
non-relevant documents, the query has to be for-
mulated by the user in an appropriate way. Blair
and Maron (1985) showed that with larger grow-
ing document collections, it gets impossible for
the user to anticipate the terms that occur in all
relevant documents, but not in non-relevant ones.
The use of semantic knowledge for improving
IR by compensating non-optimal queries has been
a field of study for a long time. First experi-
ments on query expansion by Voorhees (1994) us-
ing lexical-semantic relations extracted from a lin-
guistic knowledge base (LKB), namely WordNet
(Fellbaum, 1998), showed sginificant improve-
ments in performance only for manually selected
expansion terms. The combination of Word-
Net with thesauri built from the underlying doc-
ument collections by Mandala et al (1998) im-
proved the performance on several test collec-
tions. Mandala et al (1998) identified missing
relations, especially cross part of speech relations
and insufficient lexical coverage as reasons for the
low performance improvement when using only
WordNet.
In recent work, collaborative knowledge bases
(CKB) like Wikipedia have been used in IR for
judging the document relevance by computing the
semantic relatedness (SR) of queries and docu-
ments (Gurevych et al, 2007; Egozi et al, 2008;
M?uller and Gurevych, 2008) and have shown
promising results. These resources have a high
coverage of general and domain-specific terms.
They are employed in several SR measures such
as Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007) that allow the cross part of
speech computation of SR and are not restricted to
standard lexical semantic relations.
The goal of this paper is to shed light on the
role of lexical semantics in IR and the way it
can improve the performance of retrieval systems.
There exist different kinds of resources for lexi-
cal semantic knowledge and different ways to em-
bed this knowledge into IR. Wikipedia and Wik-
tionary, which have been applied in computational
methods only recently, offer new possibilities to
enhance IR. They have already shown an excel-
lent performance in computing the SR of word
pairs (Strube and Ponzetto, 2006; Gabrilovich and
1338
Markovitch, 2007; Zesch et al, 2008). However,
it is not yet clearly understood, what the most ben-
eficial method is to employ SR using these re-
sources in IR. We therefore perform a compara-
tive study on an IR benchmark. We particularly
analyze the contribution of SR of query and docu-
ment terms to this task. To motivate those exper-
iments we first prove that there exists a vocabu-
lary gap in the test collection between queries and
documents and show that the gap can be reduced
by using lexical semantic knowledge. As the vo-
cabulary coverage of knowledge bases is a crucial
factor for being effective in IR, we compare the
coverage of Wikipedia, Wiktionary and WordNet.
We then analyze the lexical semantic relations that
hold among query and document terms and how
they are represented by the values of a SR mea-
sure. Finally, we explore the potential of different
SR-based indicators of document relevance.
The remainder of this paper is structured as fol-
lows: In Section 2 we give a short overview of the
LKBs and CKBs and the measure of SR we em-
ploy in this paper. The test collection we use in
our experiments is described in Section 3. In Sec-
tion 4 we analyze the vocabulary of the test collec-
tion and determine the coverage of the knowledge
bases. This is followed by the examination of lex-
ical semantic relations and the analysis of the SR
of query terms in relevant and non-relevant docu-
ments in Section 5.
2 Knowledge Sources and Semantic
Relatedness Measure
2.1 Linguistic Knowledge Bases
LKBs are mainly created by trained linguists fol-
lowing clearly defined guidelines. Therefore, their
content is typically of high quality. This labor and
cost intensive approach, however, yields a number
of disadvantages for LKBs:
? their coverage and size are limited;
? they lack domain-specific vocabulary;
? continuous maintenance is often not feasible;
? the content can quickly be out-dated;
? only major languages are typically supported.
The most common types of LKBs are (i) dic-
tionaries, which alphabetically list words and their
senses of a certain language along with their def-
initions and possibly some additional information
and (ii) thesauri, which group words with similar
meaning together and define further semantic rela-
tions between the words, e.g., antonymy. The most
widely used LKB is WordNet, which is a com-
bination of dictionary and thesaurus. Since the
hypernym and hyponym relations between noun
groups form an is-a hierarchy, WordNet can also
be seen as an ontology. The current version 3.0 of
WordNet, which we use in our experiments, con-
tains over 155,000 English words organized into
almost 118,000 so called synsets, i.e., groups of
synonymous words. WordNet covers mainly gen-
eral vocabulary terms and its strongest part is the
noun hierarchy.
2.2 Collaborative Knowledge Bases
Enabled by the development of Web 2.0 technol-
ogy and created by communities of volunteers,
CKBs have emerged as a new source of lexical
semantic knowledge in recent years. In contrast
to LKBs, they are created by persons with di-
verse personal backgrounds and fields of exper-
tise. CKBs have the advantage of being freely
available unlike many LKBs. However, the con-
tent of CKBs is mainly semi- or unstructured text
which initially requires the extraction of explicit
knowledge that can then be used in computational
methods.
One of the CKBs we use in this paper is
Wikipedia, a freely available encyclopedia. It cur-
rently contains more than 12 million articles in
265 languages. Besides articles, Wikipedia also
offers other forms of knowledge that can be used
in computational methods. This includes the hi-
erarchy of article categories (Strube and Ponzetto,
2006; Zesch et al, 2007) and links between ar-
ticles in the same language (Milne and Witten,
2008) and across languages (Sch?onhofen et al,
2007; Potthast et al, 2008; Sorg and Cimiano,
2008; M?uller and Gurevych, 2008). Due to its
encyclopedic character, Wikipedia contains many
named entities and domain-specific terms which
are not found in WordNet. In our experiments we
used the Wikipedia dump of February 6th, 2007.
The second CKB we use is Wiktionary which is
a multilingual dictionary and an affiliated project
of Wikipedia. It resembles WordNet by containing
synonym and hyponym information. It also con-
tains information usually not found in LKBs like
abbreviations, compounds, contractions, and the
etymology of words. The 171 language-specific
1339
editions of Wiktionary contain more than 5 mil-
lion entries. Note that each language-specific edi-
tion contains not only entries for words of that
particular language, but also for words of for-
eign languages. Wiktionary has been used in
IR (M?uller and Gurevych, 2008; Bernhard and
Gurevych, 2009) and other tasks like sentiment
analysis (Chesley et al, 2006) or ontology learn-
ing (Weber and Buitelaar, 2006). In our experi-
ments we used the Wiktionary dump of Oct 16,
2007.
2.3 Semantic Relatedness Measure
A wide range of methods for measuring the SR of
term pairs are discussed in the literature. In our
experiments, we employ ESA as it can be used
with all three knowledge bases in our experiments
and has shown an excellent performance in re-
lated work. ESA was introduced by Gabrilovich
and Markovitch (2007) employing Wikipedia as
a knowledge base. Zesch et al (2008) explored
its performance using Wiktionary and WordNet as
knowledge bases.
The idea of ESA is to express a term?s mean-
ing by computing its relation to Wikipedia articles.
Each article title in Wikipedia is referred to as a
concept and the article?s text as the textual repre-
sentation of this concept. A term is represented
as a high dimensional concept vector where each
value corresponds to the term?s frequency in the
respective Wikipedia article. The SR of two terms
is then measured by computing the cosine between
the respective concept vectors. When applying
ESA to Wiktionary and WordNet, each word and
synset entry, respectively, is referred to as a dis-
tinct concept, and the entry?s information
1
is used
as the textual representation of the concept.
In our experiments, we apply pruning meth-
ods as proposed by Gabrilovich and Markovitch
(2007) with the goal of reducing noise and com-
putational costs. Wikipedia concepts are not taken
into account where the respective Wikipedia arti-
cles have less than 100 words or fewer than 5 in- or
outlinks. For all three knowledge bases, concepts
are removed from a term?s concept vector if their
normalized values are below a predefined thresh-
old (empirically set to 0.01).
1
For WordNet, the glosses and example sentences of the
synsets are used. Wiktionary does not contain glosses for all
entries due to instance incompleteness. Therefore, a concate-
nation of selected information from each entry is used. See
Zesch et al (2008) for details.
Documents
Number of documents 319115
Number of unique terms 400194
Ave. document length 256.23
Queries
Number of queries 50
Number of unique terms 117
Ave. query length 2.44
Table 1: Statistics of the test data (after prepro-
cessing).
3 Data
For our study we use parts of the data from
the HARD track at the TREC 2003 conference
2
.
The document collection consists of newswire text
data in English from the year 1999, drawn from
the Xinhua News Service (People?s Republic of
China), the New York Times News Service, and
the Associated Press Worldstream News Service.
3
As we did not have access to the other document
collections in the track, we restrict our experi-
ments to the newswire text data.
From the 50 available topics of that track, we
use only the title field, which consists of a few
keywords describing the information need of a
user. Table 1 shows some descriptive statistics of
the documents and topics. The topics cover gen-
eral themes like animal protection, Y2K crisis or
Academy Awards ceremony. For the preprocess-
ing of topics and documents we use tokenization,
stopword removal and lemmatization employing
the TreeTagger (Schmid, 1994). In our study, we
rely on the relevance assessments performed at
TREC to distinguish between relevant and non-
relevant documents for each topic.
4 Vocabulary Mismatch
To confirm the intuition that there exists a vocab-
ulary mismatch between queries and relevant doc-
uments, we computed the overlap of the terms in
queries and relevant documents. The results are
shown in the column String-based in Table 2. Av-
eraged over all 50 topics, 35.5% of the relevant
documents do contain all terms of the query, and
86.5% contain at least one of the query terms.
However, this means that 13.5% of the relevant
documents do not contain any query term and
2
http://trec.nist.gov/
3
AQUAINT Corpus, Linguistic Data Consortium (LDC)
catalog number LDC2002T31
1340
Measure String-based SR-Wikipedia SR-Wiktionary SR-WordNet
Threshold 0.0 0.05 0.0 0.05 0.0 0.05
Ave. number of documents where all 35.5 91.2 72.2 82.6 65.2 74.8 50.8
query terms are matched (in %)
Ave. number of documents where at 86.5 100.0 99.1 97.7 97.2 94.9 92.9
least one query term is matched (in %)
Ave. number of query terms 55.8 95.6 84.0 87.0 76.8 79.2 65.7
matched per document (in%)
Table 2: Statistics about the matching of the terms of queries and relevant documents.
cannot be retrieved by simple string-matching re-
trieval methods. In average, each relevant docu-
ment matches 55.8% of the query terms. With an
average query length of 2.44 (see Table 1), this
means that in general, only one of two query terms
occurs in the relevant documents which signifi-
cantly lowers the probability of these documents
to have a high ranking in the retrieval result.
In a second experiment, we proved the effec-
tiveness of the SR measure and knowledge bases
in reducing the vocabulary gap by counting the
number of query terms that match the terms in
the relevant documents as string or are seman-
tically related to them. The results are shown
in Table 2 for the different knowledge bases in
the columns SR-Wikipedia, SR-Wiktionary and SR-
WordNet. In order to analyse the performance of
the SR measure when excluding very low SR val-
ues that might be caused by noise, we additionally
applied a threshold of 0.05, i.e. only values above
this threshold were taken into account. The SR
values range between 0 and 1. However, the ma-
jority of SR values lie between 0 and 0.1.
Without threshold, using Wikipedia as knowl-
edge base, in 91.2% of the relevant documents all
query terms were matched. For Wiktionary with
82.6% and WordNet with 74.8% the number is
lower, but still more than twice as high as for the
string-based matching. Wikipedia matches in all
relevant documents at least one query term. The
average number of query terms matched per doc-
ument is also increased for all three knowledge
bases. Applying a threshold of 0.05, the values de-
crease, but are still above the ones for string-based
matching.
The sufficient coverage of query and document
terms is crucial for the effectiveness of knowledge
bases in IR. It was found that LKBs do not nec-
essarily provide a sufficient coverage (Mandala et
al., 1998). Table 3 shows the amount of terms
in queries and documents that are contained in
Wikipedia, Wiktionary and WordNet. Wikipedia
SR- SR- SR-
Wikipedia Wiktionary WordNet
Queries
Percentage of queries where 98.0 78.0 62.0
all terms are covered
Percentage of 99.2 89.3 80.3
covered terms
Percentage of covered 99.1 88.9 80.3
unique terms
Ave. percentage of covered 99.6 89.2 80.1
terms per query
Ave. percentage of covered 99.6 89.2 80.1
unique terms per query
Documents
Percentage of documents where 7.9 0.3 0.2
all terms are covered
Percentage of 96.5 88.5 84.3
covered terms
Percentage of covered 34.5 12.9 10.0
unique terms
Ave. Percentage of terms 97.4 91.8 88.8
covered per document
Ave. percentage of covered 96.3 88.0 83.6
unique terms per document
Table 3: Statistics about the coverage of the
knowledge bases.
contains almost all query terms and also shows the
best coverage for the document terms, followed
by Wiktionary and WordNet. The values for all
three knowledge bases are all higher than 80% ex-
cept for the percentage of queries or documents
where all terms are covered and the number of
covered unique terms. The low percentage of cov-
ered unique document terms for even Wikipedia is
mostly due to named entities, misspellings, identi-
fication codes and compounds.
Judging from the number of covered query
and document terms alone, one would expect
Wikipedia to yield a better performance when ap-
plied in IR than Wiktionary and especially Word-
Net. The higher coverage of Wikipedia is due to its
nature of being an encyclopedia featuring arbitrar-
ily long articles whereas entries in WordNet, and
also Wiktionary, have a rather short length follow-
ing specific guidelines. The high coverage alone
is however not the only important factor for the ef-
fectiveness of a resource. It was shown by Zesch et
al. (2008) that Wiktionary outperforms Wikipedia
1341
in the task of ranking word pairs by their seman-
tic relatedness when taking into account only word
pairs that are covered by both resources.
5 Comparison of Semantic Relatedness
in Relevant and Non-Relevant
Documents
We have shown in Section 4 that a mismatch be-
tween the vocabulary of queries and relevant doc-
uments exists and that the SR measure and knowl-
edge bases can be used to address this gap. In or-
der to further study the SR of query and document
terms with the goal to find SR-based indicators for
document relevance, we created sets of relevant
and non-relevant documents and compared their
characteristic values concerning SR.
5.1 Document Selection
For analysing the impact of SR in the retrieval pro-
cess, we compare relevant and non-relevant docu-
ments that were assigned similar relevance scores
by a standard IR system. For the document selec-
tion we followed a method employed by Vechto-
mova et al (2005). We created two sets of docu-
ments for each topic: one for relevant and one for
non-relevant documents. We first retrieved up to
1000 documents for the topic using the BM25 IR
model
4
(Sp?arck Jones et al, 2000) as implemented
by Terrier
5
. The relevant retrieved documents con-
stituted the first set. For the second set we se-
lected for each relevant retrieved document a non-
relevant document which had the closest score to
the relevant document. After selecting an equal
number of relevant and non-relevant documents,
we computed the mean average and the standard
deviation for the scores of each set. If there was a
substantial difference between the values of more
than 20%, the sets were rearranged by exchang-
ing non-relevant documents or excluding pairs of
relevant and non-relevant documents. If this was
not possible, we excluded the corresponding topic
from the experiments.
Table 4 shows the statistics for the final sets.
From the original 50 topics, 13 were excluded for
the above stated reasons or because no relevant
documents were retrieved. The average length of
about 345 terms for relevant documents is almost
40% larger than the length of non-relevant docu-
4
We used the default values for the constants of the model
(k
1
= 1.2, b = 0.75).
5
http://ir.dcs.gla.ac.uk/terrier/
Rel. Nonrel. Diff. (%)
Number of queries 37 37 0
Number of documents 1771 1771 0
Mean BM25 6.388 6.239 2.39
document score
Stdev BM25 1.442 1.288 12.00
document score
Ave. query length 2.32 2.32 0
Ave. document length 345.22 248.89 38.70
Ave. query term in- 6.93 4.64 49.35
stances in documents
Table 4: Data characteristics that are independent
of the chosen knowledge base and threshold.
ments. Also the average number of query term in-
stances is 6.93 in cotrast to 4.64 for non-relevant
documents. The large difference of average doc-
ument length and query term instances suggests a
larger difference of the average relevance scores
than 20%. However, in the BM25 model the rele-
vance score is decreased with increasing document
length and additional occurrences of a query term
have little impact after three or four occurrences.
5.2 Types of Lexical Semantic Relations
The most common classical lexical semantic re-
lations between words are synonymy, hyponymy
and a couple of others. In order to analyze the
importance of these relations in the retrieval pro-
cess, we automatically annotated the relations that
hold between query and document terms using
WordNet. Table 5 shows the percentage of lex-
ical semantic relations between query and docu-
ment terms (normalized by the number of query
and document terms). The table also shows the
coverage of the relations by the SR measure, i.e.
the percentage of annotated relations for which
the SR measure computed a value above 0 or the
threshold 0.05, respectively. The percentage of re-
lation types in general is higher for relevant doc-
uments. Cohyponymy and synonymy are by far
the most frequently occurring relation types with
up to almost 6%. Hypernyms and hyponyms have
both a percentage of less than 1%. Holonymy and
meronymy do almost not occur.
When applying no threshold, the SR measure
covers up to 21% of the synonyms and cohy-
ponyms and up to 12% of the hyper- and hy-
ponyms in relevant documents. Using Wiktionary
as knowledge base, the SR measure shows a bet-
ter coverage than with Wikipedia. This is con-
sistent with the findings in Zesch et al (2008).
1342
SR-Wikipedia SR-Wiktionary SR-WordNet
Relation Type Percentage 0.0 0.05 0.0 0.05 0.0 0.05
Relevant Documents
synonymy 3.61 17.81 13.13 18.33 13.78 15.28 12.18
hypernymy 0.86 8.57 2.30 12.18 3.02 11.69 2.26
hyponymy 0.88 5.72 1.28 6.33 1.67 6.54 1.02
cohyponymy 5.64 19.49 10.49 21.04 10.05 16.85 8.14
holonymy 0.02 0.61 0.17 0.74 0.17 0.53 0.00
meronymy 0.07 1.94 0.78 2.23 0.74 1.88 0.76
non-classical ? 58.80 6.62 23.22 3.13 12.77 2.56
Non-Relevant Documents
synonymy 3.41 15.84 12.41 16.46 12.90 14.19 11.44
hypernymy 0.56 6.10 1.95 9.43 2.10 8.93 1.57
hyponymy 0.74 4.77 1.00 6.35 1.40 5.90 0.78
cohyponymy 5.42 17.42 9.91 19.23 9.71 15.38 7.66
holonymy 0.02 0.39 0.09 0.49 0.09 0.32 0.00
meronymy 0.10 1.88 0.55 1.84 0.65 1.57 0.57
non-classical ? 57.33 5.54 21.92 2.59 11.77 2.15
Table 5: Percentage of lexical semantic relations between query and document terms and their coverage
by SR scores above threshold 0.00 and 0.05 in percent.
The reason for this is the method for construct-
ing the textual representation of the concepts in
the SR measure, where synonyms and other re-
lated words are concatenated. Also SR-WordNet
outperforms Wikipedia for hypernymy and hy-
ponymy. In contrast to Wiktionary, no direct in-
formation about related words is used to construct
the textual representation of concepts. However,
the very short and specific representations are built
from glosses and examples which often contain
hypernym-hyponym pairs. As WordNet is used for
both, the automatic annotation of lexical seman-
tic relations and the computation of SR values, its
lower term coverage in general has not much im-
pact on this experiment, as only the relations be-
tween terms contained in WordNet are annotated.
More than half of the SR values using
Wikipedia are computed for term pairs which were
not annotated with a classical relation. This is de-
picted in Table 5 as non-classical relation. These
non-classical relations can be for example func-
tional relations (pencil and paper) (Budanitsky
and Hirst, 2006). However, as WordNet covers
only a small part of the terms in the test collec-
tion, some of the SR values refered to as non-
classical relations might actually be classical re-
lations. For Wiktionary and WordNet, the num-
ber of non-classical relations is much lower, due
to their smaller size and the way the textual rep-
resentations of concepts are constructed. In gen-
eral, the average number of SR scores for classical
and non-classical relations are almost consistently
higher for relevant documents which suggests that
the comparison of SR scores could be beneficial in
SR-Wikipedia SR-Wiktionary SR-WordNet
Relation Type 0.0 0.05 0.0 0.05 0.0 0.05
Relevant Documents
synonymy 0.362 0.371 0.372 0.374 0.366 0.368
hypernymy 0.021 0.021 0.021 0.019 0.016 0.019
hyponymy 0.017 0.021 0.008 0.012 0.007 0.015
cohyponymy 0.270 0.334 0.315 0.353 0.312 0.363
holonymy 0.001 0.000 0.001 0.000 0.000 0.000
meronymy 0.004 0.003 0.003 0.002 0.003 0.002
non-classical 0.045 0.356 0.098 0.491 0.205 0.599
Non-Relevant Documents
synonymy 0.344 0.348 0.349 0.350 0.343 0.344
hypernymy 0.027 0.030 0.025 0.029 0.022 0.028
hyponymy 0.019 0.029 0.012 0.023 0.009 0.025
cohyponymy 0.250 0.295 0.277 0.312 0.295 0.334
holonymy 0.001 0.000 0.000 0.000 0.000 0.000
meronymy 0.003 0.002 0.002 0.002 0.003 0.002
non-classical 0.041 0.374 0.103 0.538 0.222 0.643
Table 6: Average values of SR scores correspond-
ing to lexical semantic relations between query
and document terms above threshold 0.00 and 0.05
in percent.
the IR process.
When applying a threshold of 0.05, the most
visible effect is that the percentage of non-
classical relations is decreasing much stronger
than the percentage of classical relations. The
comparison of the average SR values for each re-
lation type in Table 6 confirms that this is due to
the fact that the SR measure assigns on average
higher values to the classical relations than to the
non-classical relations. After applying a thresh-
old of 0.05 the average SR values corresponding
to non-classical relations increase and are equal to
or higher than the values for classical relations.
The values for classical relations are in general
higher for relevant documents, whereas the values
for non-classical relations are lower.
1343
5.3 SR-based Indicators for Document
Relevance
For each topic and document in one of the sets we
computed the SR between the query and document
terms. We then computed the arithmetic mean of
the following characteristic values for each set: the
sum of SR scores, the number of SR scores, the
number of terms which are semantically related to
a query term and the average SR score. In order
to eliminate the difference in document length and
average number of query term instances between
the relevant and non-relevant sets, we normalized
all values, except for the average SR score, by the
document length and excluded the SR scores of
query term instances.
Figure 1 shows the average difference of these
values between relevant and non-relevant docu-
ment sets for SR-thresholds from 0 to 0.6 (step-
size=0.01). As the majority of the SR scores have
a low value, there is not much change for thresh-
olds above 0.5.
Except for the average SR score, the differences
have a peak at thresholds between 0.01 and 0.09
and decrease afterwards to a constant value. The
SR scores computed using Wikipedia show the
highest differences. Wiktionary and WordNet per-
form almost equally, but show lower differences
than Wikipedia, especially for the sum of scores.
All three knowledge bases show higher differences
for the number of scores and number of related
terms than for the sum of scores. The differences
at the peaks are statistically significant
6
, except for
the differences of the sum of scores for Wiktionary
and WordNet.
For the average SR score, the differences are
mostly negative at low thresholds and increase to a
low positive value for higher thresholds. A higher
number of very low SR values is computed for the
relevant documents, which causes the average SR
score to be lower than for the non-relevant docu-
ments at low thresholds.
Additionally, Figure 2 shows the percentage of
topics where the mean value of the relevant docu-
ment set is higher than the one of the non-relevant
document set. Wikipedia shows the highest per-
centage with about 86% for the number of scores
and the number of related terms. Wiktionary and
WordNet have a low percentage for the sum of
scores, but reach up to 75% for the number of
scores and the number of related terms.
6
We used the Wilcoxon test at a significance level of 0.05.
The analysis of the SR of query and document
terms shows that there are significant differences
for relevant and non-relevant documents that can
be measured by computing SR scores with any of
the three knowledge bases. Especially when us-
ing Wiktionary and WordNet, the number of SR
scores and the number of related terms might be
better indicators for the document relevance than
the sum of SR scores.
6 Conclusions
The vocabulary mismatch of queries and docu-
ments is a common problem in IR, which becomes
even more serious the larger the document collec-
tion grows. CKBs like Wikipedia and Wiktionary,
which have been applied in computational meth-
ods only recently, offer new possibilities to tackle
this problem. In order to find the most beneficial
way to employ these resources, we studied the se-
mantic relatedness of query and document terms
of an IR benchmark and compared the character-
istics and performance of the CKBs Wikipedia and
Wiktionary to the LKB WordNet.
We first proved that there exists a vocabulary
gap in the test collection between queries and doc-
uments and that it can be reduced by employing a
concept vector based measure for SR with any of
the three knowledge bases. Using WordNet to au-
tomatically annotate the lexical semantic relations
of query and document terms, we found that cohy-
ponymy and synonymy are the most frequent clas-
sical relation types. Although the percentage of
annotated relations for which also the SR measure
computed values above a predefined threshold was
at best 21%, the average number of SR scores for
classical and non-classical relations were almost
consistently higher for relevant documents.
Comparing the number and the value of SR
scores of query and document terms, a significant
difference between relevant and non-relevant doc-
uments was observed by using any of the three
knowledge bases. Although Wikipedia had the
best coverage of collection terms and showed the
best perfomance in our experiments, Wiktionary
and Wikipedia also seem to have a sufficient
size for being beneficial in IR. In comparison to
our previous work where the sum of SR scores
was used as an indicator for document relevance
(M?uller and Gurevych, 2008), the results suggest
that the number of SR scores and the number of
related terms might show a better performance, es-
1344
Figure 1: Differences between mean values of relevant and non-relevant document sets.
Figure 2: Percentage of topics where the mean value of the relevant document sets is higher than the one
of the non-relevant document sets.
1345
pecially for Wiktionary and WordNet.
In our future work, we plan to extend our analy-
sis to other test collections and to query expansion
methods in order to generalize our conclusions.
As the problem of language ambiguity has a high
impact on the use of SR measures, we will also
consider word sense disambiguation in our future
experiments.
Acknowledgments
This work was supported by the Volkswagen
Foundation as part of the Lichtenberg-Professor-
ship Program under grant No. I/82806 and by the
German Research Foundation under grant No. GU
798/1-3. We would like to thank Aljoscha Bur-
chardt for his helpful comments and the anony-
mous reviewers for valuable feedback on this pa-
per.
References
D. Bernhard and I. Gurevych. 2009. Combining
lexical semantic resources with question & answer
archives for translation-based answer finding. In
Proceedings of the Joint conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing, Singa-
pore, Aug.
D. C. Blair and M. E. Maron. 1985. An evaluation
of retrieval effectiveness for a full-text document-
retrieval system. Commun. ACM, 28(3):289?299.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based Measures of Semantic Distance.
Computational Linguistics, 32(1):13?47.
P. Chesley, B. Vincent, L. Xu, and R. Srihari. 2006.
Using Verbs and Adjectives to Automatically Clas-
sify Blog Sentiment. In Proceedings of AAAI-
CAAW-06.
O. Egozi, E. Gabrilovich, and S. Markovitch. 2008.
Concept-Based Feature Generation and Selection
for Information Retrieval. In Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelli-
gence, Chicago, IL.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Ex-
plicit Semantic Analysis. In Proceedings of The
Twentieth International Joint Conference for Artifi-
cial Intelligence, pages 1606?1611, Hyderabad, In-
dia.
I. Gurevych, C. M?uller, and T. Zesch. 2007. What to
be? - Electronic Career Guidance Based on Seman-
tic Relatedness. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1032?1039, Prague, Czech Republic,
June.
R. Mandala, T. Tokunaga, and H. Tanaka. 1998. The
Use of WordNet in Information Retrieval. In Sanda
Harabagiu, editor, Proceedings of the COLING-ACL
workshop on Usage of WordNet in Natural Lan-
guage Processing, pages 31?37. Association for
Computational Linguistics, Somerset, New Jersey.
D. Milne and I. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In Wikipedia and AI workshop at
the AAAI-08 Conference (WikiAI08), Chicago, USA.
C. M?uller and I. Gurevych. 2008. Using Wikipedia
and Wiktionary in Domain-Specific Information Re-
trieval. In F. Borri, A. Nardi, and C. Peters, edi-
tors, Working Notes for the CLEF 2008 Workshop,
Aarhus, Denmark, Sep.
M. Potthast, B. Stein, and M. Anderka. 2008. A
Wikipedia-Based Multilingual Retrieval Model. In
C. Macdonald, I. Ounis, V. Plachouras, I. Ruthven,
and R. W. White, editors, 30th European Conference
on IR Research, ECIR 2008, Glasgow, volume 4956
of LNCS, pages 522?530. Springer.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of Conference
on New Methods in Language Processing.
P. Sch?onhofen, I. Biro, A. A. Benczur, and K. Csalo-
gany. 2007. Performing Cross Language Retrieval
with Wikipedia. In Working Notes for the CLEF
2007 Workshop.
P. Sorg and P. Cimiano. 2008. Cross-lingual Informa-
tion Retrieval with Explicit Semantic Analysis. In
F. Borri, A. Nardi, and C. Peters, editors, Working
Notes for the CLEF 2008 Workshop, Aarhus, Den-
mark, Sep.
K. Sp?arck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: de-
velopment and comparative experiments. Informa-
tion Processing and Management, 36(6):779?808
(Part 1); 809?840 (Part 2).
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of AAAI, pages 1419?1424.
O. Vechtomova, M. Karamuftuoglu, and S. E. Robert-
son. 2005. A Study of Document Relevance and
Lexical Cohesion between Query Terms. In Pro-
ceedings of the Workshop on Methodologies and
Evaluation of Lexical Cohesion Techniques in Real-
World Applications (ELECTRA 2005), the 28th An-
nual International ACM SIGIR Conference, pages
18?25, Salvador, Brazil, August.
1346
E. M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In SIGIR ?94: Proceedings
of the 17th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 61?69, New York, NY, USA.
Springer-Verlag New York, Inc.
N. Weber and P. Buitelaar. 2006. Web-based Ontology
Learning with ISOLDE. In Proc. of the Workshop
on Web Content Mining with Human Language at
the International Semantic Web Conference, Athens
GA, USA, 11.
T. Zesch, I. Gurevych, and M. M?uhlh?auser. 2007.
Comparing Wikipedia and German Wordnet by
Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of HLT-NAACL, pages
205?208.
T. Zesch, C. M?uller, and I. Gurevych. 2008. Us-
ing Wiktionary for Computing Semantic Related-
ness. In Proceedings of the Twenty-Third AAAI Con-
ference on Artificial Intelligence, AAAI 2008, pages
(861?867), Chicago, Illinois, USA.
1347
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1032?1039,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
What to be? - Electronic Career Guidance Based on Semantic Relatedness
Iryna Gurevych, Christof Mu?ller and Torsten Zesch
Ubiquitous Knowledge Processing Group
Telecooperation, Darmstadt University of Technology
Hochschulstr. 10, 64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
{gurevych,mueller,zesch}@tk.informatik.tu-darmstadt.de
Abstract
We present a study aimed at investigating
the use of semantic information in a novel
NLP application, Electronic Career Guid-
ance (ECG), in German. ECG is formu-
lated as an information retrieval (IR) task,
whereby textual descriptions of professions
(documents) are ranked for their relevance
to natural language descriptions of a per-
son?s professional interests (the topic). We
compare the performance of two semantic
IR models: (IR-1) utilizing semantic relat-
edness (SR) measures based on either word-
net or Wikipedia and a set of heuristics,
and (IR-2) measuring the similarity between
the topic and documents based on Explicit
Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007). We evaluate the perfor-
mance of SR measures intrinsically on the
tasks of (T-1) computing SR, and (T-2) solv-
ing Reader?s Digest Word Power (RDWP)
questions.
1 Electronic Career Guidance
Career guidance is important both for the person in-
volved and for the state. Not well informed deci-
sions may cause people to drop the training program
they are enrolled in, yielding loss of time and finan-
cial investments. However, there is a mismatch bet-
ween what people know about existing professions
and the variety of professions, which exist in real-
ity. Some studies report that school leavers typi-
cally choose the professions known to them, such
as policeman, nurse, etc. Many other professions,
which can possibly match the interests of the person
very well, are not chosen, as their titles are unknown
and people seeking career advice do not know about
their existence, e.g. electronics installer, or chem-
ical laboratory worker. However, people are very
good at describing their professional interests in nat-
ural language. That is why they are even asked to
write a short essay prior to an appointment with a
career guidance expert.
Electronic career guidance is, thus, a supplement
to career guidance by human experts, helping young
people to decide which profession to choose. The
goal is to automatically compute a ranked list of pro-
fessions according to the user?s interests. A current
system employed by the German Federal Labour
Office (GFLO) in their automatic career guidance
front-end1 is based on vocational trainings, manu-
ally annotated using a tagset of 41 keywords. The
user must select appropriate keywords according to
her interests. In reply, the system consults a knowl-
edge base with professions manually annotated with
the keywords by career guidance experts. There-
after, it outputs a list of the best matching profes-
sions to the user. This approach has two significant
disadvantages. Firstly, the knowledge base has to
be maintained and steadily updated, as the number
of professions and keywords associated with them
is continuously changing. Secondly, the user has to
describe her interests in a very restricted way.
At the same time, GFLO maintains an extensive
database with textual descriptions of professions,
1http://www.interesse-beruf.de/
1032
called BERUFEnet.2 Therefore, we cast the prob-
lem of ECG as an IR task, trying to remove the
disadvantages of conventional ECG outlined above
by letting the user describe her interests in a short
natural language essay, called a professional profile.
Example essay translated to English
I would like to work with animals, to treat and look
after them, but I cannot stand the sight of blood and
take too much pity on them. On the other hand, I like
to work on the computer, can program in C, Python and
VB and so I could consider software development as an
appropriate profession. I cannot imagine working in a
kindergarden, as a social worker or as a teacher, as I
am not very good at asserting myself.
Textual descriptions of professions are ranked
given such an essay by using NLP and IR tech-
niques. As essays and descriptions of professions
display a mismatch between the vocabularies of top-
ics and documents and there is lack of contextual in-
formation, due to the documents being fairly short
as compared to standard IR scenarios, lexical se-
mantic information should be especially beneficial
to an IR system. For example, the profile can con-
tain words about some objects or activities related to
the profession, but not directly mentioned in the de-
scription, e.g. oven, cakes in the profile and pastries,
baker, or confectioner in the document. Therefore,
we propose to utilize semantic relatedness as a rank-
ing function instead of conventional IR techniques,
as will be substantiated below.
2 System Architecture
Integrating lexical semantic knowledge in ECG re-
quires the existence of knowledge bases encoding
domain and lexical knowledge. In this paper, we in-
vestigate the utility of two knowledge bases: (i) a
German wordnet, GermaNet (Kunze, 2004), and (ii)
the German portion of Wikipedia.3 A large body of
research exists on using wordnets in NLP applica-
tions and in particular in IR (Moldovan and Mihal-
cea, 2000). The knowledge in wordnets has been
typically utilized by expanding queries with related
terms (Vorhees, 1994; Smeaton et al, 1994), con-
cept indexing (Gonzalo et al, 1998), or similarity
measures as ranking functions (Smeaton et al, 1994;
Mu?ller and Gurevych, 2006). Recently, Wikipedia
2http://infobub.arbeitsagentur.de/
berufe/
3http://de.wikipedia.org/
has been discovered as a promising lexical seman-
tic resource and successfully used in such different
NLP tasks as question answering (Ahn et al, 2004),
named entity disambiguation (Bunescu and Pasca,
2006), and information retrieval (Katz et al, 2005).
Further research (Zesch et al, 2007b) indicates that
German wordnet and Wikipedia show different per-
formance depending on the task at hand.
Departing from this, we first compare two seman-
tic relatedness (SR) measures based on the informa-
tion either in the German wordnet (Lin, 1998) called
LIN, or in Wikipedia (Gabrilovich and Markovitch,
2007) called Explicit Semantic Analysis, or ESA.
We evaluate their performance intrinsically on the
tasks of (T-1) computing semantic relatedness, and
(T-2) solving Reader?s Digest Word Power (RDWP)
questions and make conclusions about the ability of
the measures to model certain aspects of semantic
relatedness and their coverage. Furthermore, we fol-
low the approach by Mu?ller and Gurevych (2006),
who proposed to utilize the LIN measure and a set
of heuristics as an IR model (IR-1).
Additionally, we utilize the ESA measure in a
semantic information retrieval model, as this mea-
sure is significantly better at vocabulary cover-
age and at modelling cross part-of-speech relations
(Gabrilovich and Markovitch, 2007). We compare
the performance of ESA and LINmeasures in a task-
based IR evaluation and analyze their strengths and
limitations. Finally, we apply ESA to directly com-
pute text similarities between topics and documents
(IR-2) and compare the performance of two seman-
tic IR models and a baseline Extended Boolean (EB)
model (Salton et al, 1983) with query expansion.4
To summarize, the contributions of this paper are
three-fold: (i) we present a novel system, utilizing
NLP and IR techniques to perform Electronic Career
Guidance, (ii) we study the properties and intrinsi-
cally evaluate two SR measures based on GermaNet
and Wikipedia for the tasks of computing seman-
tic relatedness and solving Reader?s Digest Word
Power Game questions, and (iii) we investigate the
performance of two semantic IR models in a task
based evaluation.
4We also ran experiments with Okapi BM25 model as im-
plemented in the Terrier framework, but the results were worse
than those with the EB model. Therefore, we limit our discus-
sion to the latter.
1033
3 Computing Semantic Relatedness
3.1 SR Measures
GermaNet based measures GermaNet is a Ger-
man wordnet, which adopted the major properties
and database technology from Princeton?s Word-
Net (Fellbaum, 1998). However, GermaNet dis-
plays some structural differences and content ori-
ented modifications. Its designers relied mainly on
linguistic evidence, such as corpus frequency, rather
than psycholinguistic motivations. Also, GermaNet
employs artificial, i.e. non-lexicalized concepts, and
adjectives are structured hierarchically as opposed
to WordNet. Currently, GermaNet includes about
40000 synsets with more than 60000 word senses
modelling nouns, verbs and adjectives.
We use the semantic relatedness measure by Lin
(1998) (referred to as LIN), as it consistently is
among the best performing wordnet based measures
(Gurevych and Niederlich, 2005; Budanitsky and
Hirst, 2006). Lin defined semantic similarity using a
formula derived from information theory. This mea-
sure is sometimes called a universal semantic sim-
ilarity measure as it is supposed to be application,
domain, and resource independent. Lin is computed
as:
simc1,c2 =
2 ? log p(LCS(c1, c2))
log p(c1) + log p(c2)
where c1 and c2 are concepts (word senses) corre-
sponding to w1 and w2, log p(c) is the information
content, andLCS(c1, c2) is the lowest common sub-
sumer of the two concepts. The probability p is com-
puted as the relative frequency of words (represent-
ing that concept) in the taz5 corpus.
Wikipedia based measures Wikipedia is a free
online encyclopedia that is constructed in a col-
laborative effort of voluntary contributors and still
grows exponentially. During this process, Wikipedia
has probably become the largest collection of freely
available knowledge. Wikipedia shares many of
its properties with other well known lexical seman-
tic resources (like dictionaries, thesauri, semantic
wordnets or conventional encyclopedias) (Zesch et
al., 2007a). As Wikipedia also models relatedness
between concepts, it is better suited for computing
5http://www.taz.de
semantic relatedness than GermaNet (Zesch et al,
2007b).
In very recent work, Gabrilovich and Markovitch
(2007) introduce a SR measure called Explicit Se-
mantic Analysis (ESA). The ESA measure repre-
sents the meaning of a term as a high-dimensional
concept vector. The concept vector is derived from
Wikipedia articles, as each article focuses on a cer-
tain topic, and can thus be viewed as expressing a
concept. The dimension of the concept vector is the
number of Wikipedia articles. Each element of the
vector is associated with a certain Wikipedia article
(or concept). If the term can be found in this article,
the term?s tfidf score (Salton and McGill, 1983) in
this article is assigned to the vector element. Oth-
erwise, 0 is assigned. As a result, a term?s con-
cept vector represents the importance of the term for
each concept. Semantic relatedness of two terms can
then be easily computed as the cosine of their corre-
sponding concept vectors. If we want to measure
the semantic relatedness of texts instead of terms,
we can also use ESA concept vectors. A text is rep-
resented as the average concept vector of its terms?
concept vectors. Then, the relatedness of two texts
is computed as the cosine of their average concept
vectors.
As ESA uses all textual information in Wikipedia,
the measure shows excellent coverage. Therefore,
we select it as the second measure for integration
into our IR system.
3.2 Datasets
Semantic relatedness datasets for German em-
ployed in our study are presented in Table 1.
Gurevych (2005) conducted experiments with two
datasets: i) a German translation of the English
dataset by Rubenstein and Goodenough (1965)
(Gur65), and ii) a larger dataset containing 350
word pairs (Gur350). Zesch and Gurevych (2006)
created a third dataset from domain-specific corpora
using a semi-automatic process (ZG222). Gur65 is
rather small and contains only noun-noun pairs con-
nected by either synonymy or hypernymy. Gur350
contains nouns, verbs and adjectives that are con-
nected by classical and non-classical relations (Mor-
ris and Hirst, 2004). However, word pairs for
this dataset are biased towards strong classical rela-
tions, as they were manually selected from a corpus.
1034
CORRELATION r
DATASET YEAR LANGUAGE # PAIRS POS SCORES # SUBJECTS INTER INTRA
Gur65 2005 German 65 N discrete {0,1,2,3,4} 24 .810 -
Gur350 2006 German 350 N, V, A discrete {0,1,2,3,4} 8 .690 -
ZG222 2006 German 222 N, V, A discrete {0,1,2,3,4} 21 .490 .647
Table 1: Comparison of datasets used for evaluating semantic relatedness in German.
ZG222 does not have this bias.
Following the work by Jarmasz and Szpakow-
icz (2003) and Turney (2006), we created a sec-
ond dataset containing multiple choice questions.
We collected 1072 multiple-choice word analogy
questions from the German Reader?s Digest Word
Power Game (RDWP) from January 2001 to De-
cember 2005 (Wallace and Wallace, 2005). We dis-
carded 44 questions that had more than one correct
answer, and 20 questions that used a phrase instead
of a single term as query. The resulting 1008 ques-
tions form our evaluation dataset. An example ques-
tion is given below:
Muffin (muffin)
a) Kleingeba?ck (small cake)
b) Spenglerwerkzeug (plumbing tool)
c) Miesepeter (killjoy)
d) Wildschaf (moufflon)
The task is to find the correct choice - ?a)? in this
case.
This dataset is significantly larger than any of the
previous datasets employed in this type of evalua-
tion. Also, it is not restricted to synonym questions,
as in the work by Jarmasz and Szpakowicz (2003),
but also includes hypernymy/hyponymy, and few
non-classical relations.
3.3 Analysis of Results
Table 2 gives the results of evaluation on the task
of correlating the results of an SR measure with hu-
man judgments using Pearson correlation. The Ger-
maNet based LIN measure outperforms ESA on the
Gur65 dataset. On the other datasets, ESA is better
than LIN. This is clearly due to the fact, that Gur65
contains only noun-noun word pairs connected by
classical semantic relations, while the other datasets
also contain cross part-of-speech pairs connected by
non-classical relations. The Wikipedia based ESA
measure can better capture such relations. Addition-
ally, Table 3 shows that ESA also covers almost all
GUR65 GUR350 ZG222
# covered word pairs 53 116 55
Upper bound 0.80 0.64 0.44
GermaNet Lin 0.73 0.50 0.08
Wikipedia ESA 0.56 0.52 0.32
Table 2: Pearson correlation r of human judgments
with SR measures on word pairs covered by Ger-
maNet and Wikipedia.
COVERED PAIRS
DATASET # PAIRS LIN ESA
Gur65 65 60 65
Gur350 350 208 333
ZG222 222 88 205
Table 3: Number of covered word pairs based on Lin
or ESA measure on different datasets.
word pairs in each dataset, while GermaNet is much
lower for Gur350 and ZG222. ESA performs even
better on the Reader?s Digest task (see Table 4). It
shows high coverage and near human performance
regarding the relative number of correctly solved
questions.6 Given the high performance and cover-
age of the Wikipedia based ESA measure, we expect
it to yield better IR results than LIN.
4 Information Retrieval
4.1 IR Models
Preprocessing For creating the search index for
IR models, we apply first tokenization and then re-
move stop words. We use a general German stop
6Values for human performance are for one subject. Thus,
they only indicate the approximate difficulty of the task. We
plan to use this dataset with a much larger group of subjects.
#ANSWERED #CORRECT RATIO
Human 1008 874 0.87
GermaNet Lin 298 153 0.51
Wikipedia ESA 789 572 0.72
Table 4: Evaluation results on multiple-choice word
analogy questions.
1035
word list extended with highly frequent domain spe-
cific terms. Before adding the remaining words to
the index, they are lemmatized. We finally split
compounds into their constituents, and add both,
constituents and compounds, to the index.
EB model Lucene7 is an open source text search
library based on an EB model. After matching the
preprocessed queries against the index, the docu-
ment collection is divided into a set of relevant and
irrelevant documents. The set of relevant documents
is, then, ranked according to the formula given in the
following equation:
rEB(d, q) =
nq?
i=1
tf(tq, d)?idf(tq)?lengthNorm(d)
where nq is the number of terms in the query,
tf(tq, d) is the term frequency factor for term tq
in document d, idf(tq) is the inverse document fre-
quency of the term, and lengthNorm(d) is a nor-
malization value of document d, given the number
of terms within the document. We added a simple
query expansion algorithm using (i) synonyms, and
(ii) hyponyms, extracted from GermaNet.
IR based on SR For the (IR-1) model, we uti-
lize two SR measures and a set of heuristics: (i)
the Lin measure based on GermaNet (LIN), and (ii)
the ESA measure based on Wikipedia (ESA-Word).
This algorithm was applied to the German IR bench-
mark with positive results by Mu?ller and Gurevych
(2006). The algorithm computes a SR score for each
query and document term pair. Scores above a pre-
defined threshold are summed up and weighted by
different factors, which boost or lower the scores for
documents, depending on howmany query terms are
contained exactly or contribute a high enough SR
score. In order to integrate the strengths of tradi-
tional IR models, the inverse document frequency
idf is considered, which measures the general im-
portance of a term for predicting the content of a
document. The final formula of the model is as fol-
lows:
rSR(d, q) =
?nd
i=1
?nq
j=1 idf(tq,j) ? s(td,i, tq,j)
(1 + nnsm) ? (1 + nnr)
7http://lucene.apache.org
where nd is the number of tokens in the document,
nq the number of tokens in the query, td,i the i-th
document token, tq,j the j-th query token, s(td,i, tq,j)
the SR score for the respective document and query
term, nnsm the number of query terms not exactly
contained in the document, nnr the number of query
tokens, which do not contribute a SR score above the
threshold.
For the (IR-2) model, we apply the ESA method
for directly comparing the query with documents, as
described in Section 3.1.
4.2 Data
The corpus employed in our experiments was built
based on a real-life IR scenario in the domain of
ECG, as described in Section 1. The document col-
lection is extracted from BERUFEnet,8 a database
created by the GFLO. It contains textual descrip-
tions of about 1,800 vocational trainings, and 4,000
descriptions of professions. We restrict the collec-
tion to a subset of BERUFEnet documents, consist-
ing of 529 descriptions of vocational trainings, due
to the process necessary to obtain relevance judg-
ments, as described below. The documents contain
not only details of professions, but also a lot of infor-
mation concerning the training and administrative
issues. We only use those portions of the descrip-
tions, which characterize the profession itself.
We collected real natural language topics by ask-
ing 30 human subjects to write an essay about their
professional interests. The topics contain 130 words,
on average. Making relevance judgments for ECG
requires domain expertise. Therefore, we applied an
automatic method, which uses the knowledge base
employed by the GFLO, described in Section 1. To
obtain relevance judgments, we first annotate each
essay with relevant keywords from the tagset of 41
and retrieve a ranked list of professions, which were
assigned one or more keywords by domain experts.
To map the ranked list to a set of relevant and ir-
relevant professions, we use a threshold of 3, as
suggested by career guidance experts. This setting
yields on average 93 relevant documents per topic.
The quality of the automatically created gold stan-
dard depends on the quality of the applied knowl-
edge base. As the knowledge base was created by
8http://berufenet.arbeitsamt.de/
1036
domain experts and is at the core of the electronic ca-
reer guidance system of the GFLO, we assume that
the quality is adequate to ensure a reliable evalua-
tion.
4.3 Analysis of Results
In Table 5, we summarize the results of the ex-
periments applying different IR models on the
BERUFEnet data. We build queries from natural
language essays by (QT-1) extracting nouns, verbs,
and adjectives, (QT-2) using only nouns, and (QT-
3) manually assigning suitable keywords from the
tagset with 41 keywords to each topic. We report the
results with two different thresholds (.85 and .98) for
the Lin model, and with three different thresholds
(.11, .13 and .24) for the ESA-Word models. The
evaluation metrics used are mean average precision
(MAP), precision after ten documents (P10), the
number of relevant returned documents (#RRD). We
compute the absolute value of Spearman?s rank cor-
relation coefficient (SRCC) by comparing the rele-
vance ranking of our system with the relevance rank-
ing of the knowledge base employed by the GFLO.
Using query expansion for the EB model de-
creases the retrieval performance for most configu-
rations. The SR based models outperform the EB
model in all configurations and evaluation metrics,
except for P10 on the keyword based queries. The
Lin model is always outperformed by at least one of
the ESA models, except for (QT-3). (IR-2) performs
best on longer queries using nouns, verbs, adjectives
or just nouns.
Comparing the number of relevant retrieved doc-
uments, we observe that the IR models based on SR
are able to return more relevant documents than the
EB model. This supports the claim that semantic
knowledge is especially helpful for the vocabulary
mismatch problem, which cannot be addressed by
conventional IR models. E.g., only SR-based mod-
els can find the job information technician for a pro-
file which contains the sentence My interests and
skills are in the field of languages and IT. The job
could only be judged as relevant, as the semantic
relation between IT in the profile and information
technology in the professional description could be
found.
In our analysis of the BERUFEnet results ob-
tained on (QT-1), we noticed that many errors were
due to the topics expressed in free natural language
essays. Some subjects deviated from the given task
to describe their professional interests and described
facts that are rather irrelevant to the task of ECG,
e.g. It is important to speak different languages in
the growing European Union. If all content words
are extracted to build a query, a lot of noise is intro-
duced.
Therefore, we conducted further experiments
with (QT-2) and (QT-3): building the query using
only nouns, and using manually assigned keywords
based on the tagset of 41 keywords. For example,
the following query is built for the professional pro-
file given in Section 1.
Keywords assigned:
care for/nurse/educate/teach; use/program computer;
office; outside: outside facilities/natural
environment; animals/plants
IR results obtained on (QT-2) and (QT-3) show
that the performance is better for nouns, and sig-
nificantly better for the queries built of keywords.
This suggests that in order to achieve high IR perfor-
mance for the task of Electronic Career Guidance,
it is necessary to preprocess the topics by perform-
ing information extraction to remove the noise from
free text essays. As a result of the preprocessing,
natural language essays should be mapped to a set
of keywords relevant for describing a person?s in-
terests. Our results suggest that the word-based se-
mantic relatedness IR model (IR-1) performs signif-
icantly better in this setting.
5 Conclusions
We presented a system for Electronic Career Guid-
ance utilizing NLP and IR techniques. Given a nat-
ural language professional profile, relevant profes-
sions are computed based on the information about
semantic relatedness. We intrinsically evaluated and
analyzed the properties of two semantic relatedness
measures utilizing the lexical semantic information
in a German wordnet and Wikipedia on the tasks of
estimating semantic relatedness scores and answer-
ing multiple-choice questions. Furthermore, we ap-
plied these measures to an IR task, whereby they
were used either in combination with a set of heuris-
tics or the Wikipedia based measure was used to di-
rectly compute semantic relatedness of topics and
1037
MODEL
(QT-1) NOUNS, VERBS, ADJ. (QT-2) NOUNS (QT-3) KEYWORDS
MAP P10 #RRD SRCC MAP P10 #RRD SRCC MAP P10 #RRD SRCC
EB .39 .58 2581 .306 .38 .58 2297 .335 .54 .76 2755 .497
EB+SYN .37 .56 2589 .288 .38 .57 2310 .331 .54 .73 2768 .530
EB+HYPO .34 .47 2702 .275 .38 .56 2328 .327 .47 .65 2782 .399
Lin .85 .41 .56 2787 .338 .40 .59 2770 .320 .59 .73 2787 .578
Lin .98 .41 .61 2753 .326 .42 .59 2677 .341 .58 .74 2783 .563
ESA-Word .11 .39 .56 2787 .309 .44 .63 2787 .355 .60 .77 2787 .535
ESA-Word .13 .38 .59 2787 .282 .43 .62 2787 .338 .62 .76 2787 .550
ESA-Word .24 .40 .60 2787 .259 .43 .60 2699 .306 .54 .73 2772 .482
ESA-Text .47 .62 2787 .368 .55 .71 2787 .462 .56 .74 2787 .489
Table 5: Information Retrieval performance on the BERUFEnet dataset.
documents. We experimented with three different
query types, which were built from the topics by:
(QT-1) extracting nouns, verbs, adjectives, (QT-2)
extracting only nouns, or (QT-3) manually assign-
ing several keywords to each topic from a tagset of
41 keywords.
In an intrinsic evaluation of LIN and ESA mea-
sures on the task of computing semantic relatedness,
we found that ESA captures the information about
semantic relatedness and non-classical semantic re-
lations considerably better than LIN, which operates
on an is-a hierarchy and, thus, better captures the in-
formation about semantic similarity. On the task of
solving RDWP questions, the ESA measure signif-
icantly outperformed the LIN measure in terms of
correctness. On both tasks, the coverage of ESA is
much better. Despite this, the performance of LIN
and ESA as part of an IR model is only slightly
different. ESA performs better for all lengths of
queries, but the differences are not as significant as
in the intrinsic evaluation. This indicates that the
information provided by both measures, based on
different knowledge bases, might be complementary
for the IR task.
When ESA is applied to directly compute seman-
tic relatedness between topics and documents, it out-
performs IR-1 and the baseline EB model by a large
margin for QT-1 and QT-2 queries. For QT-3, i.e.,
the shortest type of query, it performs worse than
IR-1 utilizing ESA and a set of heuristics. Also,
the performance of the baseline EB model is very
strong in this experimental setting. This result in-
dicates that IR-2 utilizing conventional information
retrieval techniques and semantic information from
Wikipedia is better suited for longer queries provid-
ing enough context. For shorter queries, soft match-
ing techniques utilizing semantic relatedness tend to
be beneficial.
It should be born in mind, that the construction
of QT-3 queries involved a manual step of assigning
the keywords to a given essay. In this experimen-
tal setting, all models show the best performance.
This indicates that professional profiles contain a lot
of noise, so that more sophisticated NLP analysis
of topics is required. This will be improved in our
future work, whereby the system will incorporate
an information extraction component for automat-
ically mapping the professional profile to a set of
keywords. We will also integrate a component for
analyzing the sentiment structure of the profiles. We
believe that the findings from our work on apply-
ing IR techniques to the task of Electronic Career
Guidance generalize to similar application domains,
where topics and documents display similar proper-
ties (with respect to their length, free-text structure
and mismatch of vocabularies) and domain and lex-
ical knowledge is required to achieve high levels of
performance.
Acknowledgments
This work was supported by the German Research
Foundation under grant ?Semantic Information Re-
trieval from Texts in the Example Domain Elec-
tronic Career Guidance?, GU 798/1-2. We are grate-
ful to the Bundesagentur fu?r Arbeit for providing
the BERUFEnet corpus. We would like to thank the
anonymous reviewers for valuable feedback on this
paper. We would also like to thank Piklu Gupta for
helpful comments.
1038
References
David Ahn, Valentin Jijkoun, Gilad Mishne, Karin
Mu?ller, Maarten de Rijke, and Stefan Schlobach.
2004. Using Wikipedia at the TREC QA Track. In
Proceedings of TREC 2004.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based Measures of Semantic Distance.
Computational Linguistics, 32(1).
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. In Proceedings of ACL, pages 9?16, Trento, Italy.
Christiane Fellbaum. 1998. WordNet An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of The
20th International Joint Conference on Artificial In-
telligence (IJCAI), Hyderabad, India, January.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of the Coling-
ACL ?98 Workshop Usage of WordNet in Natural Lan-
guage Processing Systems, Montreal, Canada, August.
Iryna Gurevych and Hendrik Niederlich. 2005. Comput-
ing semantic relatedness in german with revised infor-
mation content metrics. In Proceedings of ?OntoLex
2005 - Ontologies and Lexical Resources? IJCNLP?05
Workshop, pages 28?33, October 11 ? 13.
Iryna Gurevych. 2005. Using the Structure of a Concep-
tual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, pages 767?778, Jeju
Island, Republic of Korea.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In RANLP, pages 111?
120.
Boris Katz, Gregory Marton, Gary Borchardt, Alexis
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,
Ozlem Uzuner, and Angela Wilcox. 2005. External
knowledge sources for question answering. In Pro-
ceedings of the 14th Annual Text REtrieval Conference
(TREC?2005), November.
Claudia Kunze, 2004. Lexikalisch-semantische Wort-
netze, chapter Computerlinguistik und Sprachtech-
nologie, pages 423?431. Spektrum Akademischer
Verlag.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 296?
304. Morgan Kaufmann, San Francisco, CA.
Dan Moldovan and Rada Mihalcea. 2000. Using Word-
Net and lexical operators to improve Internet searches.
IEEE Internet Computing, 4(1):34?43.
Jane Morris and Graeme Hirst. 2004. Non-Classical
Lexical Semantic Relations. In Workshop on Com-
putational Lexical Semantics, Human Language Tech-
nology Conference of the North American Chapter of
the ACL, Boston.
Christof Mu?ller and Iryna Gurevych. 2006. Exploring
the Potential of Semantic Relatedness in Information
Retrieval. In Proceedings of LWA 2006 Lernen - Wis-
sensentdeckung - Adaptivita?t: Information Retrieval,
pages 126?131, Hildesheim, Germany. GI-Fachgruppe
Information Retrieval.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Gerard Salton andMichael J. McGill. 1983. Introduction
to Modern Information Retrieval. McGraw-Hill, New
York.
Gerard Salton, Edward Fox, and Harry Wu. 1983. Ex-
tended boolean information retrieval. Communication
of the ACM, 26(11):1022?1036.
Alan F. Smeaton, Fergus Kelledy, and Ruari O?Donell.
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding posting lists, query expansion with
WordNet and POS tagging of Spanish. In Proceedings
of TREC-4, pages 373?390.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Ellen Vorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th An-
nual ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 61?69.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Jan
2001?Dec 2005. Verlag Das Beste, Stuttgart.
Torsten Zesch and Iryna Gurevych. 2006. Automatically
Creating Datasets for Measures of Semantic Related-
ness. In Proceedings of the Workshop on Linguistic
Distances, pages 16?24, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007a. Analyzing and Accessing Wikipedia as a Lexi-
cal Semantic Resource. In Biannual Conference of the
Society for Computational Linguistics and Language
Technology, pages 213?221, Tuebingen, Germany.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007b. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multiple
Datasets. In Proceedings of NAACL-HLT.
1039
