FrameNet-based Semantic Parsing using Maximum Entropy Models 
Namhee Kwon 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology,  
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Sciences Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As part of its description of lexico-semantic 
predicate frames or conceptual structures, the 
FrameNet project defines a set of semantic 
roles specific to the core predicate of a 
sentence.  Recently, researchers have tried to 
automatically produce semantic interpretations 
of sentences using this information.  Building 
on prior work, we describe a new method to 
perform such interpretations.  We define 
sentence segmentation first and show how 
Maximum Entropy re-ranking helps achieve a 
level of 76.2% F-score (answer among top-
five candidates) or 61.5% (correct answer). 
1 Introduction 
To produce a semantic analysis has long been a 
goal of Computational Linguistics.  To do so, 
however, requires a representation of the semantics 
of each predicate.  Since each predicate may have a 
particular collection of semantic roles (agent, 
theme, etc.) the first priority is to build a collection 
of predicate senses with their associated role 
frames.  This task is being performed in the 
FrameNet project based on frame semantics 
(Fillmore, 1976). 
Each frame contains a principal lexical item as 
the target predicate and associated frame-specific 
roles, such as offender and buyer, called frame 
elements.  FrameNet I contains 1,462 distinct 
predicates (927 verbs, 339 nouns, 175 adjectives) 
in 49,000 annotated sentences with 99,000 
annotated frame elements.  Given these, it would 
be interesting to attempt an automatic sentence 
interpretation. 
We build semantic parsing based on FrameNet, 
treating it as a classification problem.  We split the 
problem into three parts: sentence segmentation, 
frame element identification for each segment, and 
semantic role tagging for each frame element.  In 
this paper, we provide a pipeline framework of 
these three phases, followed by a step of re-ranking 
from n-best lists of every phase for the final output.  
All classification and re-ranking are performed by 
Maximum Entropy. 
The top-five final outputs provide an F-score of 
76.2% for the correct frame element identification 
and semantic role tagging.  The performance of the 
single best output is 61.5% F-score. 
The rest of the paper is organized as follows: we 
review related work in Section 2, explain 
Maximum Entropy in Section 3, describe the 
detailed method in Section 4, show the re-ranking 
process in Section 5, and conclude in Section 6.   
2 Related Work 
The first work using FrameNet for semantic 
parsing was done by Gildea and Jurafsky (G & J, 
2002) using conditional probabilistic models.  
They divide the problem into two sub-tasks: frame 
element identification and frame element 
classification.  Frame element identification 
identifies the frame element boundaries in a 
sentence, and frame element classification 
classifies each frame element into its appropriate 
semantic role.  The basic assumption is that the 
frame element (FE) boundaries match the parse 
constituents, and both identification and 
classification are then done for each constituent1. 
In addition to the separate two phase model of 
frame element identification and role classification, 
they provide an integrated model that exhibits 
improved performance.  They define a frame 
element group (FEG) as a set of frame element 
roles present in a particular sentence.  By 
integrating FE identification with role labeling, 
allowing FEG priors and role labeling decision to 
affect the determination of next FE identification, 
they accomplish F-score of 71.9% for FE 
identification and 62.8% for both of FE 
identification and role labeling.  However, since 
this integrated approach has an exponential 
complexity in the number of constituents, they 
apply a pruning scheme of using only the top m 
                                                     
1 The final output performance measurement is limited 
to the number of parse constituents matching the 
frame element boundaries. 
hypotheses on the role for each constituent (m = 
10). 
Fleischman et al(FKH, 2003) extend G & J?s 
work and achieve better performance in role 
classification for correct frame element boundaries.  
Their work improves accuracy from 78.5% to 
84.7%.  The main reasons for improvement are 
first the use of Maximum Entropy and second the 
use of sentence-wide features such as Syntactic 
patterns and previously identified frame element 
roles.  It is not surprising that there is a 
dependency between each constituent?s role in a 
sentence and sentence level features reflecting this 
dependency improve the performance. 
In this paper, we extend our previous work 
(KFH) by adopting sentence level features even for 
frame element identification. 
3 Maximum Entropy 
ME models implement the intuition that the best 
model is the one that is consistent with the set of 
constraints imposed by the evidence, but otherwise 
is as uniform as possible (Berger et al 1996).  We 
model the probability of a class c given a vector of 
features x according to the ME formulation below: 
        )],(exp[1)|(
0
xcfZxcp ii
n
ix
?=?=  
Here xZ  is normalization constant, ),( xcfi  is a 
feature function which maps each class and vector 
element to a binary value, n is the total number of 
feature functions, and i?  is a weight for the 
feature function.  The final classification is just the 
class with the highest probability given its feature 
vector and the model.   
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in G & J?s 
model.  ME models are log-linear models in which 
feature functions map specific instances of features 
and classes to binary values.  Thus, ME is not here 
being used as another way to find weights for an 
interpolated model.  Rather, the ME approach 
provides an overarching framework in which the 
full distribution of classes (semantic roles) given 
features can be modeled. 
4 Model 
We define the problem into three subsequent 
processes (see Figure 1): 1) sentence segmentation 
2) frame element identification, and 3) semantic 
role tagging for the identified frame elements.  In 
order to use sentence-wide features for the FE 
identification, a sentence should have a single non-
overlapping constituent sequence instead of all the 
independent constituents.  Sentence segmentation 
is applied before FE identification for this purpose.  
For each segment the classification into FE or not 
is performed in the FE identification phase, and 
from the FE-tagged constituents the semantic role 
classification is applied in the role tagging phase. 
He got up, bent briefly over her hand.
(He) (got up) (bent) (briefly) (over her hand)
FE NO T FE FE
(He) (briefly) (over her hand)
Agent Manner Path
Input sentence
1) Sentence Segmentation: 
choose the highest constituents 
while separating target word 
2) Frame Element Boundary Identification:
apply ME classification to classify each segment 
into classes of FE (frame element), T (target), NO (none)
Extract the identified FEs:
choose segments that are identified as FEs
3) Semantic Role Tagging:
apply ME classification to classify each FE
Into classes of 120 semantic roles
Output role: Agent (He), Manner (briefly), Path (over her hand) 
for the target ?bent?
Fig. 1. The sequence of steps on a sample sentence. 
4.1 Sentence Segmentation 
The advantages of applying sentence 
segmentation before FE identification are 
considered in two ways.  First we can utilize 
sentence-wide features, and second the number of 
constituents as FE candidates is reduced, which 
reduces the convergence time in training. 
We segment a sentence with parse constituents2.  
During training, we split a sentence into true frame 
elements and the remainder.  After choosing frame 
elements as segments, we choose the highest level 
constituents in parse tree for other parts, and then 
make a complete sentence composed of a sequence 
of constituent segments.  During testing, we need 
to consider all combinations of various level 
constituents.  We know the given target word 
should be a separate segment because a target word 
is not a part of other FEs.  Since most frame 
elements tend to be among the higher levels of a 
parse tree, we decide to use the highest 
constituents while separating the target word.  
Figure 2 shows an example of the segmentation for 
                                                     
2 We use Michael Collins?s parser :  
http://www.cis.upenn.edu/~mcollins/ 
an actual sentence in FrameNet with the target 
word ?bent?.  
He got up bent briefly over her hand
PRP VBD RP
PRT
VBD RP IN PRP$ NN
VP
VP
VP
ADVP
NP
PP
NP
S
Fig. 2. A sample sentence segmentation: ?bent? is 
a target predicate in a sentence and the shaded 
constituent represents each segment. 
However, this segmentation for testing reduces 
the FE coverage of constituents, which means our 
FE classification performance is limited.  Table 1 
shows the FE coverage and the number of 
constituents for our development set.  The FE 
coverage of individual constituents (86.36%) 
means the accuracy of the parser.  This limitation 
and will be discussed in detail in Section 4.4. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  115,380 86.36 
Sentence 
segmentation 29,688 77.25 
Table 1.  The number of constituents and FE 
coverage for development set. 
4.2 Frame Element Identification 
Frame element identification is executed for the 
sequence of segments. For the example sentence in 
Figure 2, ?(He) (got up) (bent) (briefly) (over her 
hand)?, there are five segments and each segment 
has its own feature vector.  Maximum Entropy 
classification into the classes of FE, Target, or 
None is conducted for each.  Since the target 
predicate is given we don?t need to classify a target 
word into a class, but we do not exclude it from the 
segments because we want to get benefit of using 
previous segment?s features. 
The initial features are adopted from G & J and 
FKH, and most features are common to both of 
frame element identification and semantic role 
classification.  The features are: 
? Target predicate (target): The target 
predicate, the principal word in a sentence, is 
the feature that is provided by the user.  
Although there can be many predicates in a 
sentence, only one predicate is defined at a 
time. 
? Target identification (tar): The target 
identification is a binary value, indicating 
whether the given constituent is a target or not.  
Because we have a target word in a sequence 
of segments, we provide this information 
explicitly. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?over her hand? in a sentence of 
Figure 2 has a path PP?VP?VBD. 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree. 
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method3.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision if the head is 
proper noun is done by the part of speech tag 
in a parse tree.  
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other.  We follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.  
Generally, a grammatical function of external 
argument is a subject, and that of object 
argument is an object.  This feature is applied 
only to constituents whose phrase type is NP.  
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate and whether the constituent has the 
same parent as the target predicate or not. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (target, frame 
element, or none) is used to exploit the 
dependency between constituents.  During 
training, this information is provided by simply 
                                                     
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
looking at the true classes of the frame element 
occurring n-positions before the current 
element.  During testing, hypothesized classes 
of the n elements are used and Viterbi search is 
performed to find the most probable tag 
sequence for a sentence. 
The combination of these features is used in ME 
classification as feature sets.  The feature sets are 
optimized by previous work and trial and error 
experiments.  Table 2 shows the lists of feature sets 
for ?briefly? in a sentence of ?He got up, bent 
briefly over her hand?.  These feature sets contain 
the previous or next constituent?s features, for 
example, pt_-1 represents the previous 
constituent?s phrase type and lf_1 represents the 
next constituent?s logical function. 
Feature Set Example Functions 
f(c, target) f(c, ?bent?) = 1 
f(c, target, pt) f(c, ?bent?,ADVP) = 1 
f(c, target, pt, lf) f(c, ?bent?,ADVP,other) = 1 
f(c, pt, pos, voice) f(c, ADVP,after_yes,active) = 1 
f(c, pt, lf) f(c, ADVP,other) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, pt_-1, pos_-1,voice) f(c, VBD_-1,t_-1,active) = 1 
f(c, pt_1, pos_1, voice) f(c, PP_1,after_yes_1, active) = 1 
f(c, head) f(c, ?briefly?) = 1 
f(c, head, target) f(c, ?briefly?, ?bent?) = 1 
f(c, path) f(c, ADVP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, tar) f(c, 0) = 1 
f(c, c_-1) f(c, ?target?_-1) = 1 
f(c, c_-1,c_-2) f(c, ?target?_-1,?NO FE?_-2) = 1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?briefly? 
from the sample sentence in Fig.2 are shown. 
4.3 Semantic Role Classification 
The semantic role classification is executed only 
for the constituents that are classified into FEs in 
the previous FE identification phase.  Maximum 
Entropy classification is performed to classify each 
FE into classes of semantic roles. 
Most features from the frame element 
identification in Section 4.2 are still used, and two 
additional features are applied.  The feature sets 
are in Table 3. 
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, in the sentence from Figure 2, there 
are three frame elements, and the element 
?He? has order 0, while ?over her hand? has 
order 2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by looking at the phrase type and logical 
functions of each frame element in a sentence.  
For example, in the sentence from Figure 2, 
?He? is an external argument Noun Phrase, 
?bent? is a target predicate, and ?over her 
hand? is an external argument Prepositional 
Phrase.  Thus, the syntactic pattern associated 
with the sentence is [NP-ext, target, PP-ext]. 
Feature Sets 
f(c, target) f(r, head) 
f(r, target, pt) f(r, head, target) 
f(r, target, pt, lf) f(r, head, target, pt) 
f(r, pt, pos, voice) f(r, order, syn) 
f(r, pt, pos, voice, target) f(r,target, order, syn) 
f(r, r_-1) f(r,r_-1,r_-2) 
Table 3. Feature sets used in ME semantic role 
classification. 
4.4 Experiments and Results 
Since FrameNet II was published during our 
research, we continued using FrameNet I (120 
semantic role categories).  We can, therefore, 
compare our results with previous research by 
matching exactly the same data as used in G & J 
and FKH.  We thank Dan Gildea for providing the 
following data set: training (36,993 sentences / 
75,548 frame elements), development (4,000 
sentences / 8,167 frame elements), and held our 
test sets (3,865 sentences / 7,899 frame elements). 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  For testing, we use the YASMET 
MEtagger (Bender et al 2003) to perform the 
Viterbi search for choosing the most probable tag 
sequence for a sentence using the probabilities 
from training.  Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999).  The standard deviation of this distribution 
and the number of GIS iterations for training are 
optimized on development set for each experiment.  
Table 4 shows the performance for test set. The 
evaluation is done for individual frame elements. 
To segment a sentence before FE identification 
or role tagging improves the overall performance 
(from 57.6% to 60.0% in Table 4).  Since the 
segmentation reduces the FE coverage of segments, 
we conduct the experiment with the manually 
chosen segmentation to see how much the 
segmentation helps the performance.  Here, we 
extract segments from the parse tree constituents, 
so the FE coverage is 86% for test set, which 
maches the parsing accuracy.  Table 5 shows the 
performance of the frame element identification for 
test set:  F-score is 77.2% that is much better than 
71.7% of our automatic segmentation. 
FE identification FE identification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
separated 
model 
73.6 63.1 67.5 67.0 46.8 55.1 
FKH 
ME model 73.6 67.9 70.6 60.0 55.4 57.6 
Our model 
(segmentation 
+ ME 
classification) 
75.5 68.2 71.7 62.9 56.8 60.0 
Table 4. Performance comparison for test set. 
Precision Recall F-score 
82.1 72.9 77.2 
Table 5.  Result of frame element identification on 
manual segmentation of test set 
5 n-best Lists and Re-ranking 
As stated, the sentence segmentation improves 
the performance by using sentence-wide features, 
but it drops the FE coverage of constituents.  In 
order to determine a good segmentation for a 
sentence that does not reduce the FE coverage, we 
perform another experiment by using re-ranking.  
We obtain all possible segmentations for a given 
sentence, and conduct frame element identification 
and semantic role classification for all 
segmentations.  During both phases, we get n-best 
lists with Viterbi search, and finally choose the 
best output with re-ranking method.  Figure 3 
shows the overall framework of this task.  
5.1 Maximum Entropy Re-ranking 
We model the probability of output r given 
candidates? feature sets {x1 .. xt} where t is the total 
number of candidates and xj is a feature set of the 
jth candidate according to the following ME 
formulation: 
]})..{,(exp[1})..{|(
0
1 1?
=
=
n
i
tiixt
xxrfZxxrp ?
 
where Zx is a normalization factor, fi(r,{x1..xt}) is a 
feature function which maps each output and all 
candidates? feature sets to a binary value, n is the 
total number of feature functions, and ?i is the 
weight for a given feature function.  The weight ?i 
is associated with only each feature function while 
the weight in the ME classifier is associated with 
all possible classes as well as feature functions.  
The final decision is r having the highest 
probability of p(r|{x1..xt}) from t number of 
candidates. 
As a feature set for each candidate, we use the 
ME classification probability that is calculated 
during Viterbi search.  These probabilities are 
conditional probabilities given feature sets and 
these feature sets depend on the previous output, 
for example, semantic role tagging is done for the 
identified FEs in the previous phase.  For this 
reason, the product of these conditional 
probabilities is used as a feature set. 
    )|(*)|(*)|()|( ferpsegfepssegpsrp =  
where s is a given sentence, seg is a segmentation, 
fe is a frame element identification, and r is the 
final semantic role tagging.  p(fe|seg) and p(r|fe) 
are produced from the ME classification but 
p(seg|s) is computed by a heuristic method and a 
development set optimization experiment.  The 
adopted p(seg|s) is composed of p(each segment?s 
part of speech tag | target?s part of speech tag), 
p(the number of total segments in a sentence | total 
number of words in a sentence), and the average of 
each segment?s p(head word of FE | target). 
Two additional feature sets other than p(r|s) are 
applied to get slight improvement for re-ranking 
performance, which are average of p(parse tree 
depth of FE | target) and average of p(head word 
of FE | target). 
5.2 Experiments and Results 
We apply ME re-ranking in YASMET-ME 
package.  We train re-ranking model with 
development set after obtaining candidate lists for 
the set.  For a simple cross validation, the 
development set is divided into a sub-training set 
(3,200 sentences) and a sub-development set (800 
sentences) by selecting every fifth sentence.  
Training for re-ranking is executed with the sub- 
training set and optimization is done with the sub-
development set.  The final test is applied to test 
set. 
The possible number of segmentations is different 
depending on sentences, but the average number of 
segmentation lists is 15.24 for the development  set.  
For these segmentations, we compute 10-best5 lists 
for the FE identification and 10-best lists for the 
semantic role classification. 
                                                     
4  To reduce the number of different segmenations 
while not dropping the FE coverage, the segmentations 
having too many segments for a long sentence are 
excluded. 
5 The experiment showed 10-best lists outperformed 
other n-best lists where n is less than 10.  The bigger 
number was not tested because of  huge number of lists. 
 He craned over the balcony again but finally he seemed to sigh.
1. (He) (craned) (over) (the) (balcony) (again) (but) (finally) (he) (seemed) (to) (sigh).
?
6. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed) (to sigh).
7. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed to sigh).
?
11. (He) (craned) (over the balcony) (again) (but) (finally) (he) (seemed to sigh).
12. (He) (craned) (over the balcony) (again) (but) (finally he seemed to sigh).
Input sentence
Sentence Segmentation: segment a sentence into all possible combinations of constituents of a                                        
parse tree while separating target word (In this example, target word is ?craned?.)
Frame Element Identification:  apply ME classification to all segmentations and get n-best output 
classifying each segment into FE (frame element), T (target), or NO (none), then extract segments that are 
identified as frame elements
(1)
(2)
(4)
1.1 (He)
?
6.1 (He) (the balcony)
?
11.1 (He) (over the balcony)
?
12.1 (He) (over the balcony)
12.2 (He) 
12.3 (He) (over the balcony) (again)
..
(3)
Semantic Role Classification: apply ME classification into 120 semantic roles and get n-best output for each
1.1.1 Agent (He)
?.
6.1.1 Agent (He), BodyPart (the balcony)
?.
12.1.1 Agent (He), Goal (over the balcony)
12.1.2 Agent (He), Path (over the balcony)
12.1.3 Self-mover (He), Goal (over the balcony)
?.
Re-ranking : apply ME re-ranking and choose the best one from long lists
Final Output Agent (He), Path (over the balcony)
 
Fig. 3.  The framework of the re-ranking method with an actual system output.  (1) contains different number 
of segmentations depending on each sentence, (2) has mn number of lists when we obtain m possible 
segmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-best 
role classifications given mn lists (4) shows finally chosen output. 
Table 6 shows the performance of re-ranking.  
To evaluate the performance of top-n, the best 
tagging output for a sentence is chosen among n-
lists and the performance is computed for that list.  
The top-5 lists show two interesting points: one is 
that precision is very high, and the other is that F-
score including role tagging is not much different 
from F-score of only FE identification.  In other 
words, there are a few (not 120) confusing roles for 
a given frame element, and we have many frame 
elements that are not identified even in n-best lists. 
FE identification FE identification  & Role tagging Re-rank 
Prec Rec F Prec Rec F 
Top-1 77.4 66.0 71.2 66.7 57.0 61.5 
Top-2 81.8 69.2 75.0 75.6 64.0 69.4 
Top-5 86.8 72.4 78.0 83.7 69.9 76.2 
Table 6. Re-ranking performance for test set 
To improve our re-ranker, more features 
regarding these problems should be added, and a 
more principled method to obtain the probability of 
segmenations, p(seg) in Sectioin 5.1, needs to be 
investigated. 
Table 7 compares the final output with G & J?s 
best result.  Our model is slightly worse than their 
integrated model, but it supports much further 
experimentation in segmentation and re-ranking. 
FE identification FE indetification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
integrated 
model 
74.0 70.1 72.0 64.6 61.2 62.9 
Our 
model w/ 
re-ranking 
77.4 66.0 71.2 66.7 57.0 61.5 
Table 7. The final output for test set.  
6 Conclusion 
We describe a pipeline framework to analyze 
sentences into frame elements and semantic roles 
based on the FrameNet corpus.  The process 
includes four steps: sentence segmentation, FE 
identification, role classification, and final re-
ranking of the n-best outputs. 
In future work, we will investigate ways to 
reduce the gap between the five-best output 
performance and the single best output.  More 
features should be extracted to improve re-ranking 
accuracy.  Although the segmentation improves the 
performance, since the final output is dominated by 
the initial segmentation, we will explore a smart 
segmentation method, possibly one not even 
limited to constituents. 
In addition to the provided syntactic features, we 
will apply semantic features using ontology.  
Finally, the challenge is to apply this type of work 
to new predicates, ones not yet treated in 
FrameNet.  We are searching for methods to 
achieve this. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Language 
Processing. Proc. of EACL-2003. Budapest, 
Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
M. Collins. 1997. Three Generative, Lexicalized 
Models for Statistical Parsing. Proc. of the 35th 
Annual Meeting of the ACL. pages 16-23, 
Madrid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP) 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
K. Hacioglu, W. Ward. 2003. Target word 
detection and semantic role chunking using 
support vector machines. Proc. of HLT-NAACL 
2003, Edmonton, Canada. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
S. Pradhan, K. Haciolgu, W. Ward, J. Martin, D. 
Jurafsky. 2003. Semantic Role Parsing: Adding 
Semantic Structure to Unstructured Text. Proc of 
of the International Conference on Data Mining 
(ICDM-2003), Melbourne, FL  
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, Croatia 
Proceedings of NAACL HLT 2007, Companion Volume, pages 217?220,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Semi-Automatic Evaluation Scheme: Automated Nuggetization for 
Manual Annotation 
Liang Zhou, Namhee Kwon, and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{liangz, nkwon, hovy}@isi.edu 
 
Abstract 
In this paper we describe automatic in-
formation nuggetization and its applica-
tion to text comparison. More 
specifically, we take a close look at how 
machine-generated nuggets can be used to 
create evaluation material. A semi-
automatic annotation scheme is designed 
to produce gold-standard data with excep-
tionally high inter-human agreement.  
1 Introduction 
In many natural language processing (NLP) tasks, 
we are faced with the problem of determining the 
appropriate granularity level for information units. 
Most commonly, we use sentences to model indi-
vidual pieces of information. However, more NLP 
applications require us to define text units smaller 
than sentences, essentially decomposing sentences 
into a collection of phrases. Each phrase carries an 
independent piece of information that can be used 
as a standalone unit. These finer-grained informa-
tion units are usually referred to as nuggets.  
When performing within-sentence comparison 
for redundancy and/or relevancy judgments, with-
out a precise and consistent breakdown of nuggets 
we can only rely on rudimentary n-gram segmenta-
tions of sentences to form nuggets and perform 
subsequent n-gram-wise text comparison. This is 
not satisfactory for a variety of reasons. For exam-
ple, one n-gram window may contain several sepa-
rate pieces of information, while another of the 
same length may not contain even one complete 
piece of information.  
Previous work shows that humans can create 
nuggets in a relatively straightforward fashion. In 
the PYRAMID scheme for manual evaluation of 
summaries (Nenkova and Passonneau, 2004), ma-
chine-generated summaries were compared with 
human-written ones at the nugget level. However, 
automatic creation of the nuggets is not trivial. 
Hamly et al (2005) explore the enumeration and 
combination of all words in a sentence to create the 
set of all possible nuggets. Their automation proc-
ess still requires nuggets to be manually created a 
priori for reference summaries before any sum-
mary comparison takes place. This human in-
volvement allows a much smaller subset of phrase 
segments, resulting from word enumeration, to be 
matched in summary comparisons. Without the 
human-created nuggets, text comparison falls back 
to its dependency on n-grams. Similarly, in ques-
tion-answering (QA) evaluations, gold-standard 
answers use manually created nuggets and com-
pare them against system-produced answers bro-
ken down into n-gram pieces, as shown in 
POURPRE (Lin and Demner-Fushman, 2005) and 
NUGGETEER (Marton and Radul, 2006).  
A serious problem in manual nugget creation is 
the inconsistency in human decisions (Lin and 
Hovy, 2003). The same nugget will not be marked 
consistently with the same words when sentences 
containing multiple instances of it are presented to 
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower. In recent exercises of the 
PYRAMID evaluation, inconsistent nuggets are 
flagged by a tracking program and returned back to 
the annotators, and resolved manually.  
Given these issues, we address two questions in 
this paper: First, how do we define nuggets so that 
they are consistent in definition? Secondly, how do 
217
we utilize automatically extracted nuggets for vari-
ous evaluation purposes?  
2  Nugget Definition  
 
Based on our manual analysis and computational 
modeling of nuggets, we define them as follows:  
Definition:  
? A nugget is predicated on either an event  or 
an entity .  
? Each nugget consists of two parts: the an-
chor and the content.  
The anchor is either:  
? the head noun of the entity, or 
? the head verb of the event, plus the head 
noun of its associated entity (if more than 
one entity is attached to the verb, then its 
subject).  
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor 
may have several separate contents.  
When a nugget contains nested sentences, this 
definition is applied recursively. Figure 1 shows an 
example. Anchors are marked with square brack-
ets. If the anchor is a verb, then its entity attach-
ment is marked with curly brackets. If the sentence 
in question is a compound and/or complex sen-
tence, then this definition is applied recursively to 
allow decomposition. For example, in Figure 1, 
without recursive decomposition, only two nuggets 
are formed: 1) ?[girl] working at the bookstore in 
Hollywood?, and 2) ?{girl} [talked] to the diplo-
mat living in Britain?. In this example, recursive 
decomposition produces nuggets with labels 1-a, 1-
b, 2-a, and 2-b.  
2.1  Nugget Extraction 
We use syntactic parse trees produced by the 
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions 
for entities and events. For entity nuggets, we ex-
amine subtrees headed by ?NP?; for event nuggets, 
subtrees headed by ?VP? are examined and their 
corresponding subjects (siblings headed by ?NP?) 
are treated as entity attachments for the verb 
phrases.  
3  Utilizing Nuggets in Evaluations 
In recent QA and summarization evaluation exer-
cises, manually created nuggets play a determinate 
role in judging system qualities. Although the two 
task evaluations are similar, the text comparison 
task in summarization evaluation is more complex 
because systems are required to produce long re-
sponses and thus it is hard to yield high agreement 
if manual annotations are performed. The follow-
ing experiments are conducted in the realm of 
summarization evaluation.  
3.1  Manually Created Nuggets 
During the recent two Document Understanding 
Confereces (DUC-05 and DUC-06) (NIST, 2002?
2007), the PYRAMID framework (Nenkova and 
Passonneau, 2004) was used for manual summary 
evaluations. In this framework, human annotators 
select and highlight portions of reference summa-
ries to form a pyramid of summary content units 
(SCUs) for each docset. A pyramid is constructed 
from SCUs and their corresponding popularity 
scores?the number of reference summaries they 
appeared in individually. SCUs carrying the same 
information do not necessarily have the same sur-
face-level words. Annotators need to make the de-
cisions based on semantic equivalence among 
Figure 1. Nugget definition examples.  
Sentence:  
The girl working at the bookstore in Hollywood 
talked to the diplomat living in Britain.  
 
Nuggets are: 
1)  [girl] working at the bookstore in Holly-
wood 
a. [girl] working at the bookstore  
b. [bookstore] in Hollywood 
2) {girl} [talked] to the diplomat living in 
Britain 
a. {girl} [talked] to the diplomat 
b. [diplomat] living in Britian 
 
Anchors: 
1)  [girl] 
a. [girl] 
b. [bookstore] 
2) {girl} [talked]: talked is the anchor verb 
and girl is its entity attachment.  
a. {girl} [talked] 
b. [diplomat]  
218
various SCUs. To evaluate a peer summary from a 
particular docset, annotators highlight portions of 
text in the peer summary that convey the same in-
formation as those SCUs in previously constructed 
pyramids.  
3. 2  Automatically Created Nuggets 
We envisage the nuggetization process being 
automated and nugget comparison and aggregation 
being performed by humans. It is crucial to involve 
humans in the evaluation process because recog-
nizing semantically equivalent units is not a trivial 
task computationally. In addition, since nuggets are 
system-produced and can be imperfect, annotators 
are allowed to reject and re-create them. We per-
form record-keeping in the background on which 
nugget or nugget groups are edited so that further 
improvements can be made for nuggetization.  
The evaluation scheme is designed as follows: 
 
For reference summaries  (per docset):  
? Nuggets are created for all sentences;  
? Annotators will group equivalent nuggets.  
? Popularity scores are automatically assigned 
to nugget groups.  
For peer summaries :  
? Nuggets are created for all sentences;  
? Annotators will match/align peer?s nuggets 
with reference nugget groups.  
? Recall scores are to be computed.  
3. 3  Consistency in Human Involvement 
The process of creating nuggets has been auto-
mated and we can assume a certain level of consis-
tency based on the usage of the syntactic parser. 
However, a more important issue emerges. When 
given the same set of nuggets, would human anno-
tators agree on nugget group selections and their 
corresponding contributing nuggets? What levels 
of agreement and disagreement should be ex-
pected? Two annotators, one familiar with the no-
tion of nuggetization (C1) and one not (C2), 
participated in the following experiments.  
Figure 2 shows the annotation procedure for 
reference summaries. After two rounds of individ-
ual annotations and consolidations and one final 
round of conflict resolution, a set of gold-standard 
nugget groups is created for each docset and will 
be subsequently used in peer summary annotations. 
The first round of annotation is needed since one 
of the annotators, C2, is not familiar with nuggeti-
zation. After the initial introduction of the task, 
concerns and questions arisen can be addressed. 
Then the annotators proceed to the second round of 
annotation. Naturally, some differences and con-
flicts remain. Annotators must resolve these prob-
lems during the final round of conflict resolution 
and create the agreed-upon gold-standard data.   
Previous manual nugget annotation has used one 
annotator as the primary nugget creator and an-
other annotator as an inspector (Nenkova and Pas-
sonneau, 2004). In our annotation experiment, we 
encourage both annotators to play equally active 
roles. Conflicts between annotators resulting from 
ideology, comprehension, and interpretation differ-
ences helped us to understand that complete 
agreement between annotators is not realistic and 
not achievable, unless one annotator is dominant 
over the other. We should expect a 5-10% annota-
tion variation.  
In Figure 3, we show annotation comparisons 
from first to second round. The x -axis shows the 
nugget groups that C1 and C2 have agreed on. The 
y -axis shows the popularity score a particular nug-
get group received. Selecting from three reference 
summaries, a score of three for a nugget group in-
dicates it was created from nuggets in all three 
 
Figure 2. Reference annotation and gold-standard 
data creation.  
219
summaries. The first round initially appears suc-
cessful because the two annotators had 100% 
agreement on nugget groups and their correspond-
ing scores. However, C2, the novice nuggetizer, 
was much more conservative than C1, because 
only 10 nugget groups were created. The geometric 
mean of agreement on all nugget group assignment 
is merely 0.4786. During the second round, differ-
ences in group-score allocations emerge, 0.9192, 
because C2 is creating more nugget groups. The 
geometric mean of agreement on all nugget group 
assignment has been improved to 0.7465.  
After the final round of conflict resolution, 
gold-standard data was created. Since all conflicts 
must be resolved, annotators have to either con-
vince or be convinced by the other. How much 
change is there between an annotator?s second-
round annotation and the gold-standard? Geomet-
ric mean of agreement on all nugget group assign-
ment for C1 is 0.7543 and for C2 is 0.8099. 
Agreement on nugget group score allocation for 
C1 is 0.9681 and for C2 is 0.9333. From these fig-
ures, we see that while C2 contributed more to the 
gold-standard?s nugget group creations, C1 had 
more accuracy in finding the correct number of 
nugget occurrences in reference summaries. This 
confirms that both annotators played an active role. 
Using the gold-standard nugget groups, the annota-
tors performed 4 peer summary annotations. The 
agreement among peer summary annotations is 
quite high, at approximately 0.95. Among the four, 
annotations on one peer summary from the two 
annotators are completely identical.  
4  Conclusion 
In this paper we have given a concrete definition 
for information nuggets and provided a systematic 
implementation of them. Our main goal is to use 
these machine-generated nuggets in a semi-
automatic evaluation environment for various NLP 
applications. We took a close look at how this can 
be accomplished for summary evaluation, using 
nuggets created from reference summaries to grade 
peer summaries. Inter-annotator agreements are 
measured to insure the quality of the gold-standard 
data created. And the agreements are very high by 
following a meticulous procedure. We are cur-
rently preparing to deploy our design into full-
scale evaluation exercises.  
References 
Collins, M. 1999. Head-driven statistical models for 
natural language processing. Ph D Dissertation , Uni-
versity of Pennsylvania.  
Hamly, A., A. Nenkova, R. Passonneau, and O. Ram-
bow. 2005. Automation of summary evaluation by 
the pyramid method. In Proceedings of RANLP.  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. In 
Proceedings of NAACL-HLT. 
Lin, J. and D. Demner-Fushman. 2005. Automatically 
evaluating answers to definition questions. In Pro-
ceedings of HLT-EMNLP.  
Marton, G. and A. Radul. 2006. Nuggeteer: automatic 
nugget-based evaluation using description and judg-
ments. In Proceedings NAACL-HLT.  
Nenkova, A. and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. 
In Proceedings NAACL-HLT.  
NIST. 2001?2007. Document Understanding Confer-
ence. www-nlpir.nist.gov/projects/duc/index.html.  
 
 
Figure 3. Annotation comparisons from 1 st to 
2nd round.  
220
Maximum Entropy Models for FrameNet Classification  
 
Michael Fleischman, Namhee Kwon and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, nkwon, hovy }@ISI.edu 
  
Abstract 
The development of FrameNet, a large 
database of semantically annotated sen-
tences, has primed research into statistical 
methods for semantic tagging.  We ad-
vance previous work by adopting a 
Maximum Entropy approach and by using 
previous tag information to find the high-
est probability tag sequence for a given 
sentence.  Further we examine the use of 
sentence level syntactic pattern features to 
increase performance.  We analyze our 
strategy on both human annotated and 
automatically identified frame elements, 
and compare performance to previous 
work on identical test data.  Experiments 
indicate a statistically significant im-
provement (p<0.01) of over 6%. 
1 Introduction 
Recent work in the development of FrameNet, a 
large database of semantically annotated sentences, 
has laid the foundation for statistical approaches to 
the task of automatic semantic classification.   
The FrameNet project seeks to annotate a large 
subset of the British National Corpus with seman-
tic information.  Annotations are based on Frame 
Semantics (Fillmore, 1976), in which frames are 
defined as schematic representations of situations 
involving various frame elements such as partici-
pants, props, and other conceptual roles.   
In each FrameNet sentence, a single target 
predicate is identified and all of its relevant frame 
elements are tagged with their semantic role (e.g., 
Agent, Judge), their syntactic phrase type (e.g., 
NP, PP), and their grammatical function (e.g., ex-
ternal argument, object argument).  Figure 1 shows 
an example of an annotated sentence and its appro-
priate semantic frame.   
 
 
 
 
 
 
 
          She  clapped  her hands  in inspiration. 
Frame:        Body-Movement 
 
Frame Elements:   
Agent     Body Part Cause 
       -NP            -NP  -PP 
       -Ext             -Obj -Comp 
 
Figure 1.  Frame for lemma ?clap? shown with three 
core frame elements and a sentence annotated with ele-
ment type, phrase type, and grammatical function. 
 
As of its first release in June 2002, FrameNet 
has made available 49,000 annotated sentences.  
The release contains 99,000 annotated frame ele-
ments for 1462 distinct lexical predicates (927 
verbs, 339 nouns, and 175 adjectives). 
While considerable in scale, the FrameNet da-
tabase does not yet approach the magnitude of re-
sources available for other NLP tasks.  Each target 
predicate, for example, has on average only 30 sen-
tences tagged.  This data sparsity makes the task of 
learning a semantic classifier formidable, and in-
creases the importance of the modeling framework 
that is employed. 
2 Related Work 
To our knowledge, Gildea and Jurafsky (2002) 
is the only work to use FrameNet to build a statis-
tically based semantic classifier.  They split the 
problem into two distinct sub-tasks: frame element 
identification and frame element classification.  In 
the identification phase, syntactic information is 
extracted from a parse tree to learn the boundaries 
of the frame elements in a sentence.  In the classi-
fication phase, similar syntactic information is 
used to classify those elements into their semantic 
roles.   
In both phases Gildea and Jurafsky (2002) 
build a model of the conditional probabilities of the 
classification given a vector of syntactic features.  
The full conditional probability is decomposed into 
simpler conditional probabilities that are then in-
terpolated to make the classification.  Their best 
performance on held out test data is achieved using 
a linear interpolation model: 
where r is the class to be predicted, x is the vector 
of syntactic features, xi is a subset of those fea-
tures, ?i is the weight given to that subset condi-
tional probability (as determined using the EM 
algorithm), and m is the total number of subsets 
used.  Using this method, they report a test set ac-
curacy of 78.5% on classifying semantic roles and 
precision/recall scores of .726/.631 on frame ele-
ment identification.  
We extend Gildea and Jurafsky (2002)?s initial 
effort in three ways.  First, we adopt a maximum 
entropy (ME) framework in order to learn a more 
accurate classification model.  Second, we include 
features that look at previous tags and use previous 
tag information to find the highest probability se-
mantic role sequence for a given sentence.  Finally, 
we examine sentence-level patterns that exploit 
more global information in order to classify frame 
elements.  We compare the results of our classifier 
to that of Gildea and Jurafsky (2002) on matched 
test sets of both human annotated and automati-
cally identified frame elements.  
3 
                                                          
Semantic Role Classification 
Training (36,993 sentences / 75,548 frame ele-
ments), development (4,000 sentences / 8,167 
frame elements), and held out test sets (3,865 sen-
tences / 7,899 frame elements) were obtained in 
order to exactly match those used in Gildea and 
Jurafsky (2002)1 .  In the experiments presented 
below, features are extracted for each frame ele-
ment in a sentence and used to classify that ele-
ment into one of 120 semantic role categories.  The 
boundaries of each frame element are given based 
on the human annotations in FrameNet.  In Section 
4, experiments are performed using automatically 
identified frame elements. 
3.1 
                                                          
1 Data sets (including parse trees) were obtained from Dan 
Gildea via personal communication. 
Features 
For each frame element, features are extracted 
from the surface text of the sentence and from an 
automatically generated syntactic parse tree 
(Collins, 1997).  The features used are described 
below: 
 
)|()|(
0
i
m
i
i xrpxrp ?
=
= ? ? Target predicate (tar): Although there may 
be many predicates in a sentence with associ-
ated frame elements, classification operates on 
only one target predicate at a time.  The target 
predicate is the only feature that is not ex-
tracted from the sentence itself and must be 
given by the user.  Note that the frame which 
the target predicate instantiates is not given, 
leaving any word sense ambiguities to be han-
dled implicitly by the classifier.2 
? Phrase type (pt):  The syntactic phrase type of 
the frame element (e.g. NP, PP) is extracted 
from the parse tree of the sentence by finding 
the constituent in the tree whose boundaries 
match the human annotated boundaries of the 
element.  In cases where there exists no con-
stituent that perfectly matches the element, the 
constituent is chosen which matches the largest 
text span of the element and has the same left-
most boundary.  
? Syntactic head (head): The syntactic heads of 
the frame elements are extracted from the 
frame element?s matching constituent (as de-
scribed above) using a heuristic method de-
scribed by Michael Collins. 3   This method 
extracts the syntactic heads of constituents; 
thus, for example, the second frame element in 
Figure 1 has head ?hands,? while the third 
frame element has head ?in.? 
? Logical Function (lf): A simplification of the 
grammatical function annotation (see section 
1) is extracted from the parse tree.  Unlike the 
2 Because of the interaction of head word features with the 
target predicate, we suspect that ambiguous lexical items do 
not account for much error.  This question, however, will be 
addressed explicitly in future work. 
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
Table 1.  Feature sets used in ME frame element classifier.  Shows individual feature sets, example feature 
function from that set, and total number of feature functions in the set.  Examples taken from frame element 
?in inspiration,? shown in Figure 1. 
 Number Feature Set Example function Number of Functions 
in Feature Set 
0 f(r, tar) f(CAUSE, ?clap?)=1 6,518 
1 f(r, tar, pt) f(CAUSE, ?clap?, PP)=1 12,030 
2 f(r, tar, pt, lf) f(CAUSE, ?clap?, PP, other)=1 14,615 
3 f(r, pt, pos, voice) f(CAUSE, NP, ?clap?, active)=1 1,215 
4 f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, ?clap?)=1 15,602 
5 f(r ,head) f(CAUSE, ?in?)=1 18,504 
6 f(r, head, tar) f(CAUSE, ?in?, ?clap?)=1 38,223 
7 f(r, head, tar, pt) f(CAUSE, ?in?, ?clap?, PP)=1 39,740 
8 f(r, order, syn) f(CAUSE, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
13,228 
9 f(r, tar, order, syn) f(CAUSE, ?clap?, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
40,580 
10 f(r,r_-1) f(CAUSE, BODYPART)=1 1,158 
11 f(r,r_-1,r_-2) f(CAUSE, BODYPART, AGENT)=1 2,030 
Total Number of Features:  203,443 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
full grammatical function, the lf can have only 
one of three values: external argument, object 
argument, other.  A node is considered an ex-
ternal argument if it is an ancestor of an S 
node, an object argument if it is an ancestor of 
a VP node, and other for all other cases.  This 
feature is only applied to frame elements 
whose phrase type is NP.  
? Position (pos): The position of the frame ele-
ment relative to the target (before, after) is ex-
tracted based on the surface text of the 
sentence. 
? Voice (voice): The voice of the sentence (ac-
tive, passive) is determined using a simple 
regular expression passed over the surface text 
of the sentence. 
? Order (order): The position of the frame ele-
ment relative to the other frame elements in the 
sentence.  For example, in the sentence from 
Figure 1, the element ?She? has order=0, while 
?in inspiration? has order=2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern of the sentence is generated 
by looking at the phrase types and logical 
functions of each frame element in the sen-
tence.  For example, in the sentence: ?Alexan-
dra bent her head;? ?Alexandra? is an external 
argument Noun Phrase, ?bent? is a target 
predicate, and ?her head? is an object argu-
ment Noun Phrase.  Thus, the syntactic pattern 
associated with the sentence is [NP-ext, target, 
NP-obj].   
These syntactic patterns can be highly in-
formative for classification.  For example, in 
the training data, a syntactic pattern of [NP-
ext, target, NP-obj] given the predicate bend 
was associated 100% of the time with the 
Frame Element pattern: ?AGENT TARGET 
BODYPART.? 
? Previous role (r_n): Frame elements do not 
occur in isolation, but rather, depend very 
much on the other elements in a sentence.  
This dependency can be exploited in classifica-
tion by using the semantic roles of previously 
classified frame elements as features in the 
classification of a current element.  This strat-
egy takes advantage of the fact that, for exam-
ple, if a frame element is tagged as an AGENT 
it is highly unlikely that the next element will 
also be an AGENT. 
The previous role feature indicates the 
classification that the n-previous frame ele-
ment received.  During training, this informa-
tion is provided by simply looking at the true 
classes of the frame element occurring n posi-
tions before the target element.  During testing, 
hypothesized classes of the n elements are used 
and Viterbi search is performed to find the 
most probable tag sequence for a sentence. 
3.2 Maximum Entropy 
ME models implement the intuition that the best 
model will be the one that is consistent with the set 
of constrains imposed by the evidence, but other-
wise is as uniform as possible (Berger et al, 1996).  
We model the probability of a semantic role r 
given a vector of features x according to the ME 
formulation below: 
3.3 
3.4 
Experiments 
We present three experiments in which different 
feature sets are used to train the ME classifier.  The 
first experiment uses only those feature combina-
tions described in Gildea and Jurafsky (2002) (fea-
ture sets 0-7 from Table 1).  The second 
experiment uses a super set of the first and incor-
porates the syntactic pattern features described 
above (feature sets 0-9).  The final experiment uses 
the previous tags and implements Viterbi search to 
find the best tag sequence (feature sets 0-11). 
?
=
=
n
i
i
x
xrfZxrp
0
i )],(exp[1)|( ?  
Here Zx is a normalization constant, fi(r,x) is a fea-
ture function which maps each role and vector 
element (or combination of elements) to a binary 
value, n is the total number of feature functions, 
and ?i is the weight for a given feature function.  
The final classification is just the role with highest 
probability given its feature vector and the model. 
We further investigate the effect of varying two 
aspects of classifier training: the standard deviation 
of the Gaussian priors used for smoothing, and the 
number of sentences used for training.  To examine 
the effect of optimizing the standard deviation, a 
range of values was chosen and a classifier was 
trained using each value until performance on a 
development set ceased to improve.   
The feature functions that we employ can be 
divided into feature sets based upon the types and 
combinations of features on which they operate.  
Table 1 lists the feature sets that we use, as well as 
the number of individual feature functions they 
contain.  The feature combinations were chosen 
based both on previous work and trial and error.  In 
future work we will examine more principled fea-
ture selection techniques. 
To examine the effect of training set size on 
performance, five data sets were generated from 
the original set with 36, 367, 3674, 7349, and 
24496 sentences, respectively.  These data sets 
were created by going through the original set and 
selecting every thousandth, hundredth, tenth, fifth, 
and every second and third sentence, respectively.  
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in the Gildea 
and Jurafsky model.  ME models are log-linear 
models in which feature functions map specific 
instances of syntactic features and classes to binary 
values (e.g., if a training element has head=?in? 
and role=CAUSE, then, for that element, the feature 
function f(CAUSE, ?in?) will equal 1).  Thus, ME is 
not here being used as another way to find weights 
for an interpolated model.  Rather, the ME ap-
proach provides an overarching framework in 
which the full distribution of semantic roles given 
syntactic features can be modeled. 
 
 
 
 
 
 
 
 
 
 
 We train the ME models using the GIS algo-
rithm (Darroch and Ratcliff, 1972) as implemented 
in the YASMET ME package (Och, 2002).  We 
use the YASMET MEtagger (Bender et al, 2003) 
to perform the Viterbi search.  The classifier was 
trained until performance on the development set 
ceased to improve.  Feature weights were 
smoothed using Gaussian priors with mean 0 
(Chen and Rosenfeld, 1999).  The standard devia-
tion of this distribution was optimized on the de-
velopment set for each experiment. 
 
Classifier Performance on Test Set
81.7
83.6
84.7
78.5
76
78
80
82
84
86
G&J Exp 1 Exp 2 Exp 3
%
 C
or
re
ct
Figure 2.  Performance of models on test data using 
hand annotated frame element boundaries.  G&J refers 
to the results of Gildea and Jurafsky (2002).  Exp 1 in-
corporates feature sets 0-7 from Table 1; Exp 2 feature 
sets 0-9; Exp 3 features 0-11.  
Results  
Figure 2 shows the results of our experiments 
alongside those of (Gildea and Jurafsky, 2002) on 
identical held out test sets.  The difference in per-
formance between each classifier is statistically 
significant at (p<0.01) (Mitchell, 1997), with the 
exception of Exp 2 and Exp 3, whose difference is 
statistically significant at (p<0.05).   
 
Table 2.  Effect of different smoothing parameter (std. 
dev.) values on classification performance. 
Std. Dev. % Correct 
1 79.9 
2 82.1 
4 81.9 
 
Table 2 shows the effect of varying the stan-
dard deviation of the Gaussian priors used for 
smoothing in Experiment 1.  The difference in per-
formance between the classifiers trained using 
standard deviation 1 and 2 is statistically signifi-
cant at (p<0.01). 
 
10%
20%
30%
40%
50%
60%
70%
80%
90%
10 100 1000 10000 100000
# Sentences in Training
%
 C
or
re
ct
 
Figure 3.  Effect of training set size on semantic role 
classification. 
 
Figure 3 shows the change in performance as a 
function of training set size.  Classifiers were 
trained using the full set of features described for 
Experiment 3. 
Table 3 shows the confusion matrix for a subset 
of semantic roles.  Five roles were chosen for pres-
entation based upon their high contribution to clas-
sifier error.  Confusion between these five account 
for 27% of all errors made amongst the 120 possi-
ble roles.  The tenth role, other, represents the sum 
of the remaining 115 roles.  Table 4 presents ex-
ample errors for five of the most confused roles.   
3.5 Discussion 
It is clear that the ME models improve perform-
ance on frame element classification.  There are a 
number of reasons for this improvement. 
First, for this task the log-linear model employed 
in the ME framework is better than the linear 
interpolation model used by Gildea and Jurafsky.  
One possible reason for this is that semantic role 
classification benefits from the ME model?s bias 
for more uniform probability distributions that sat-
isfy the constraints placed on the model by the 
training data.   
Another reason for improved performance comes 
from ME?s simpler design.  Instead of having to 
worry about finding proper backoff strategies 
amongst distributions of features subsets, ME al-
lows one to include many features in a single 
model and automatically adjusts the weights of 
these features appropriately. 
 
Table 3.  Confusion matrix for five roles which contrib-
ute most to overall system error. Columns refer to ac-
tual role.  Rows refer to the model?s hypothesis.  Other 
refers to combination of all other roles. 
  Area Spkr Goal Msg Path Other Prec. 
Area 98  6  18 16 0.710 
Spkr  373  23  41 0.853 
Goal 11  431  28 50 0.828 
Msg  18 1 315  33 0.858 
Path 32  36  415 41 0.791 
Other 15 21 26 24 33 5784 0.979 
Recall 0.628 0.905 0.862 0.87 0.84 0.969   
 
Also, because the ME models find weights for 
many thousands of features, they have many more 
degrees of freedom than the linear interpolated 
models of Gildea and Jurafsky.  Although many 
degrees of freedom can lead to overfitting of the 
training data, the smoothing procedure employed 
in our experiments helps to counteract this prob-
lem.  As evidenced in Table 2, by optimizing the 
standard deviation used in smoothing the ME 
models are able to show significant increases in 
performance on held out test data. 
Finally, by including in our model sentence-
level pattern features and information about previ-
ous classes, global information can be exploited for 
improved classification.  The accuracy gained by 
including such global information confirms the 
intuition that the semantic role of an element is 
much related to the entire sentence of which it is a 
part. 
Having discussed the advantages of the models 
presented here, it is interesting to look at the errors 
that the system makes.  It is clear from the confu-
sion matrix in Table 3 that a great deal of the sys-
tem error comes from relatively few semantic 
roles.4  Table 4 offers some insight into why these 
errors occur.  For example, the confusions exem-
plified in 1 and 2 are both due to the fact that the 
particular phrases employed can be used in multi-
ple roles (including the roles hypothesized by the 
system).  Thus, while ?across the counter? may be 
considered a goal when one is talking about a per-
son and their head, the same phrase would be con-
sidered a path if one were talking about a mouse 
who is running.   
 
Table 4.  Example errors for five of the most often con-
fused semantic roles 
 Actual Proposed Example Sentence 
1 Goal Path The barman craned his head 
across the counter. 
2 Area Path Mr. Glass began hallucinating, 
throwing books around the 
classroom. 
3 Message Speaker Debate lasted until 20 Septem-
ber, opposition being voiced 
by a number of Italian and 
Spanish prelates. 
4 Addressee Speaker Furious staff claim they were 
even called in from holiday to 
be grilled by a specialist secu-
rity firm 
5 Reason Evaluee We cannot but admire the 
efficiency with which she 
took control of her own life. 
 
Examples 3 and 4, while showing phrases with 
similar confusions, stand out as being errors caused 
by an inability to deal with passive sentences.  
Such errors are not unexpected; for, even though 
the voice of the sentence is an explicit feature, the 
system suffers from the paucity of passive sen-
tences in the data (approximately 5%). 
Finally, example 5 shows an error that is based 
on the difficult nature of the decision itself (i.e., it 
is unclear whether ?the efficiency? is the reason for 
admiration, or what is being admired).  Often 
times, phrases are assigned semantic roles that are 
not obvious even to human evaluators.  In such 
cases it is difficult to determine what information 
might be useful for the system. 
Having looked at the types of errors that are 
common for the system, it becomes interesting to 
examine what strategy may be best to overcome 
such errors.  Aside from new features, one solution 
is obvious: more data.  The curve in Figure 2 
shows that there is still a great deal of performance 
to be gained by training the current ME models on 
more data.  The slope of the curve indicates that 
we are far from a plateau, and that even constant 
increases in the amount of available training data 
may push classifier performance above 90% accu-
racy.   
Having demonstrated the effectiveness of the 
ME approach on frame element classification 
given hand annotated frame element boundaries, 
we next examine the value of the approach given 
automatically identified boundaries. 
                                                          
4 
4.1 
Frame Element Identification 
Gildea and Jurafsky equate the task of locating 
frame element boundaries to one of identifying 
frame elements amongst the parse tree constituents 
of a given sentence.  Because not all frame element 
boundaries exactly match constituent boundaries, 
this approach can perform no better than 86.9% 
(i.e. the number of elements that match constitu-
ents (6864) divided by the total number of ele-
ments (7899)) on the test set.   
Features 
Frame element identification is a binary classifica-
tion problem in which each constituent in a parse 
tree is described by a feature vector and, based on 
that vector, tagged as either a frame element or not.  
In generating feature vectors we use a subset of the 
features described for role tagging as well as an 
additional path feature. 
 
 
Figure 4.  Generation of path features used in frame 
element tagging.  The path from the constituent ?in in-
spiration? to the target predicate ?clapped? is repre-
sented as the string PP?VP?VBD.  
 
Gildea and Jurafsky introduce the path feature 
in order to capture the structural relationship be-
tween a constituent and the target predicate.  The  4 44% of all error is due to confusion between only nine roles. 
Table 5.  Results of frame element identification.  G&J represents results reported in (Gildea and Jurafsky, 2002), 
ME results for the experiments reported here.  The second column shows precision, recall, and F-scores for the task 
of frame element identification, the third column for the combined task of identification and classification.   
FE ID only FE ID + FE Classification Method 
Precision Recall F-Score Precision Recall F-Score 
G&J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551 
ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576 
 
path of a constituent is represented by the nodes 
through which one passes while traveling up the 
tree from the constituent and then down through 
the governing category to the target.  Figure 4 
shows an example of this feature for a frame ele-
ment from the sentence presented in Figure 1. 
4.2 
4.3 
Experiments 
We use the ME formulation described in Section 
3.2 to build a binary classifier.  The classifier fea-
tures follow closely those used in Gildea and Juraf-
sky.  We model the data using the feature sets: f(fe, 
path), f(fe, path, tar), and f(fe, head, tar), where fe 
represents the binary classification of the constitu-
ent.  While this experiment only uses three feature 
sets, the heterogeneity of the path feature is so 
great that the classifier itself uses 1,119,331 unique 
binary features. 
With the constituents having been labeled, we 
apply the ME frame element classifier described 
above.  Results are presented using the classifier of 
Experiment 1, described in section 3.3. We then 
investigate the effect of varying the number of 
constituents used for training on identification per-
formance.  Five data sets of approximately 100,000 
10,000, 1,000, and 100 constituents were generated 
from the original set by random selection and used 
to train ME models as described above. 
Results 
Table 5 compares the results of Gildea and Juraf-
sky (2002) and the ME frame element identifier on 
both the task of frame element identification alone, 
and the combined task of frame element identifica-
tion and classification.  In order to be counted cor-
rect on the combined task, the constituent must 
have been correctly identified as a frame element, 
and then must have been correctly classified into 
one of the 120 semantic categories.   
Recall is calculated based on the total number 
of frame elements in the test set, not on the total 
number of elements that have matching parse con-
stituents.  Thus, the upper limit is 86.9%, not 
100%.  Precision is calculated as the number of 
correct positive classifications divided by the num-
ber of total positive classifications.  
The difference in the F-scores on the identifica-
tion task alone and on the combined task are statis-
tically significant at the (p<0.01) level 5 .  The 
accuracy of the ME semantic classifier on the 
automatically identified frame elements is 81.5%, 
not a statistically significant difference from its 
performance on hand labeled elements, but a statis-
tically significant difference from the classifier of 
Gildea and Jurafsky (2002) (p<0.01). 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
100 1000 10000 100000 1000000
# Constituents in Training
F-
Sc
or
e
 
Figure 5.  Effect of training set size on frame element 
boundary identification. 
 
Figure 5 shows the results of varying the train-
ing set size on identification performance.  For 
each data set, thresholds were chosen to maximize 
F-Score. 
4.4 
                                                          
Discussion 
It is clear from the results above that the perform-
ance of the ME model for frame element classifica-
tion is robust to the use of automatically identified 
frame element boundaries.  Further, the ME 
5 G&J?s results for the combined task were generated with a 
threshold applied to the FE classifier (Dan Gildea, personal 
communication).  This is why their precision/recall scores are 
dissimilar to their accuracy scores, as reported in section 3.  
Because the ME classifier does not employ a threshold, com-
parisons must be based on F-score. 
framework yields better results on the frame ele-
ment identification task than the simple linear in-
terpolation model of Gildea and Jurafsky.  This 
result is not surprising given the discussion in Sec-
tion 3.   
What is striking, however, is the drastic overall 
reduction in performance on the combined 
identification and classification task.  The 
bottleneck here is the identification of frame 
element boundaries.  Unlike with classification 
though, Figure 5 indicates that a plateau in the 
learning curve has been reached, and thus, more 
data will not yield as dramatic an improvement for 
the given feature set and model.   
5 Conclusion 
The results reported here show that ME models 
provide higher performance on frame element clas-
sification tasks, given both human and automati-
cally identified frame element boundaries, than the 
linear interpolation models examined in previous 
work.  We attribute this increase to the benefits of 
the ME framework itself, the incorporation of sen-
tence-level syntactic patterns into our feature set, 
and the use of previous tag information to find the 
most probable sequence of roles for a sentence.   
But perhaps most striking in our results are the 
effects of varying training set size on the perform-
ance of the classification and identification models.  
While for classification, the learning curve appears 
to be still increasing with training set size, the 
learning curve for identification appears to have 
already begun to plateau.  This suggests that while 
classification will continue to improve as the Fra-
meNet database gets larger, increased performance 
on identification will rely on the development of 
more sophisticated models. 
In future work, we intend to apply the lessons 
learned here to the problem of frame element iden-
tification.  Gildea and Jurafsky have shown that 
improvements in identification can be had by more 
closely integrating the task with classification (they 
report an F-Score of .719 using an integrated 
model).  We are currently exploring a ME ap-
proach which integrates these two tasks under a 
tagging framework.  Initial results show that sig-
nificant improvements can be had using techniques 
similar to those described above. 
Acknowledgments 
The authors would like to thank Dan Gildea who 
generously allowed us access to his data files and 
Oliver Bender for making the MEtagger software 
available.  Finally, we thank Franz Och whose help 
and expertise was invaluable. 
References 
O. Bender, K. Macherey, F. J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Lan-
guage Processing. Proc. of EACL-2003.  Buda-
pest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natu-
ral Language Processing. Computational Lin-
guistics, vol. 22, no. 1. 
S. F. Chen and R. Rosenfeld. 1999. A Gaussian 
prior for smoothing maximum entropy models. 
Technical Report CMUCS -99-108, Carnegie 
Mellon University 
M. Collins. 1997. Three generative, lexicalized 
models for statistical parsing.  Proc. of the 35th 
Annual Meeting of the ACL.  pages 16-23, Ma-
drid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals 
of Mathematical Statistics, 43:1470-1480. 
C. Fillmore 1976.  Frame semantics and the nature 
of language. Annals of the New York Academy 
of Sciences: Conference on the Origin and De-
velopment of Language and Speech, Volume 
280 (pp. 20-32).  
D. Gildea and D. Jurafsky. 2002.  Automatic La-
beling of Semantic Roles, Computational Lin-
guistics, 28(3) 245-288 14.  
T. Mitchell. 1997.  Machine Learning.  McGraw-
Hill International Editions, New York, NY. 
Pages 143-145. 
F.J. Och. 2002. Yet another maxent toolkit: 
YASMET. www-i6.informatik.rwth-
aachen.de/Colleagues/och/. 
 
 
SENSEVAL Automatic Labeling of Semantic Roles using Maximum Entropy 
Models 
Namhee Kwon  
Information Science Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology 
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Science Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As a task in SensEval-3, Automatic Labeling 
of Semantic Roles is to identify frame 
elements within a sentence and tag them with 
appropriate semantic roles given a sentence, a 
target word and its frame.  We apply 
Maximum Entropy classification with feature 
sets of syntactic patterns from parse trees and 
officially attain 80.2% precision and 65.4% 
recall.  When the frame element boundaries 
are given, the system performs 86.7% 
precision and 85.8% recall. 
1 Introduction 
The Automatic Labeling of Semantic Roles track 
in SensEval-3 focuses on identifying frame 
elements in sentences and tagging them with their 
appropriate semantic roles based on FrameNet1. 
For this task, we extend our previous work 
(Fleischman et el., 2003) by adding a sentence 
segmentation step and by adopting a few additional 
feature vectors for Maximum Entropy Model.  
Following the task definition, we assume the frame 
and the lexical unit of target word are known 
although we have assumed only the target word is 
known in the previous work. 
2 Model 
We separate the problem of FrameNet tagging 
into three subsequent processes: 1) sentence 
segmentation 2) frame element identification, and 
3) semantic role tagging.  We assume the frame 
element (FE) boundaries match the parse 
constituents, so we segment a sentence based on 
parse constituents.  We consider steps 2) and 3) as 
classification problems.  In frame element 
identification, we use a binary classifier to 
determine if each parse constituent is a FE or not, 
while, in semantic role tagging, we classify each 
                                                     
1 http://www.icsi.berkeley.edu/~framenet 
identified FE into its appropriate semantic role.2  
Figure 1 shows the sequence of steps. 
He fastened the panel from an old radio to the headboard wi th 
sticky tape and tied the driving wheel to Pete 's cardboard box 
wi th st ring
(He) (fastened the panel from an old radio to the headboard 
wi th sticky tape) (and) (t ied) (the driving wheel) ( to Pete 's 
cardboard box) (wi th s tring)
(He) (the driving wheel) (to Pete 's cardboard box) (wi th  string)
Agen t I tem Goal Connecto r
Input sentence
1) Sentence Segmentation: choose the highest 
consti tuen ts while separating targe t word 
2) Frame Element Identification: apply ME 
classification to classify each segment in t o classes of 
FE (frame element), T (target), NO (none) then ext ract 
iden ti fied f rame elements
3) Semantic Role Tagging: apply ME classification to 
classify each FE Into classes of various semantic roles
Output role: Agent (He), I tem ( the driving wheel), 
Goal ( to Pete?s cardboard box), Connector (wi th string)
Fig. 1. The sequence of steps on a sample sentence 
having a target word ?tied?. 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  We use the YASMET MEtagger (Bender et 
al. 2003) to perform the Viterbi search for 
choosing the most probable tag sequence for a 
sentence using the probabilities computed during 
training. Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999). 
2.1 Sentence Segmentation 
We segment a sentence into a sequence of non-
overlapping constituents instead of all individual 
constituents. There are a number of advantages to 
applying sentence segmentation before FE 
                                                     
2 We are currently ignoring null instantiations.  
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
boundary identification.  First, it allows us to 
utilize sentence-wide features for FE identification.  
The sentence-wide features, containing dependent 
information between frame element such as the 
previously identified class or the syntactic pattern, 
have previously been shown to be powerful 
features for role classification (Fleischman et al, 
2003).  Further, it allows us to reduce the number 
of candidate constituents for FE, which reduces the 
convergence time in training.  
The constituents are derived from a syntactic 
parse tree3.  Although we need to consider all 
combinations of various level constituents in a 
parse tree, we know the given target word should 
be a separate segment because a target word is not 
a part of other FEs.4  Since most frame elements 
tend to be in higher levels of the parse tree, we 
decide to use the highest constituents (the parse 
constituents having the maximum number of 
words) while separating the target word.  Figure 2 
shows an example of the segmentation for an 
actual sentence in FrameNet with the target word 
?tied?. 
  
He tied the to Pete box
PRP VBD DT TO NP NN
VP
VP
VP
NP
NP
PP
NP
S
?s
CC
and
PP
fastened the panel
from an old radio  
to the headboard  
with sticky tape
?.
VBG
driving wheel
NN
POSNNP
NN
cardboard
IN
with
NP
NN
string
Fig. 2. A sample sentence segmentation: ?tied? is 
the target predicate, and the shaded constituent 
represents each segment. 
However, this segmentation reduces the FE 
coverage of constituents (the number of 
constituents matching frame elements).  In Table 1, 
?individual constituents? means a list of all 
constituents, and ?Sentence segmentation? means a 
sequence of non-overlapping constituents that are 
taken in our work.  We can regard 85.8% as the 
accuracy of the parser. 
                                                     
3 We use Charniak parser :  
   http://www.cs.brown.edu/people/ec/#software 
4 Although 17% of constituents are both a target and a 
frame element, there is no case that a target is a part of a 
frame element. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  342,245 85.8 
Sentence 
segmentation 66,401 79.5 
Table 1.  FE coverage for the test set. 
2.2 Frame Element Identification 
Frame element identification is executed for 
segments to classify into the classes on FE, Target, 
or None.  When a constituent is both a target and a 
frame element, we set it as a frame element when 
training because we are interested in identifying 
frame elements not a target. 
The initial features are adopted from (Gildea and 
Juraksky 2002) and (Fleischman, Kwon, and Hovy 
2003), and a few additional features are also used.  
The features are: 
? Target predicate (target): The target is the 
principal lexical item in a sentence.  
? Target lexical name (lexunit): The formal 
lexical name of target predicate is the string of 
the original form of target word and 
grammatical type.  For example, when the 
target is ?tied?, the lexical name is ?tie.v?.   
? Target type (ltype): The target type is a part 
of lexunit representing verb, noun, or adjective. 
(e.g. ?v? for a lexunit ?tie.v?) 
? Frame name (frame): The semantic frame is 
defined in FrameNet with corresponding target. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?the driving wheel? in the sentence 
of Figure 2 has the path, NP?VP?VBD. 
? Partial path (ppath): The partial path is a 
variation of path, and it produces the same path 
as above if the constituent is under the same 
?S? as target word, if not, it gives ?nopath?.  
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method5.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision as to whether the 
head is a proper noun is made based on the 
part of speech tags used in the parse tree.  
                                                     
5 http://www.ai.mit.edu/people/mcollins/papers/heads 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree.  It is not the 
same as the manually defined PT in FrameNet. 
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other. When the constituent?s 
phrase type is NP, we follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.   
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (Target, FE, or 
None) is used to exploit the dependency 
between constituents.  During training, this 
information is provided by simply looking at 
the true class of the constituent occurring n-
positions before the target element.  During 
testing, the hypothesized classes are used for 
Viterbi search. 
Feature Set Example Functions 
f(c, lexunit) f(c, tie.v) = 1 
f(c, pt, pos, voice) f(c, NP,after,active) = 1 
f(c, pt, lf) f(c, ADVP,obj) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, head) f(c, wheel) = 1 
f(c, head, frame) f(c, wheel, Attaching) = 1 
f(c, path) f(c, NP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, target) f(c, tied) = 1 
f(c, ppath) f(c, NP?VP?VBD) = 1 
f(c, ppath, pos) f(c,NP?VP?VBD, after) = 1 
f(c, ppath_-1, pos_-1) f(c, VBD_-1,after) = 1 
f(c ,ltype,  ppath) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path_-1) f(c, v,VBD_-1) = 1 
f(c  frame) f(c, Attaching) = 1 
f(c, frame, c_-1) f(c, Attaching, T_-1) = 1 
f(c,frame, c_-2,c_-1) f(c, Attaching,NO_-2,T_-1)=1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?the driving 
wheel? from the sample sentence in Fig.2. 
The combinations of these features that are used 
in the ME model are shown in Table 2.  These 
feature sets contain the previous or next 
constituent?s features, for example, pt_-1 
represents the previous constituent?s phrase type 
and lf_1 represents the next constituent?s logical 
function.  
2.3 Semantic Role Classification 
Semantic role classification is executed only for 
the constituents that are classified into FEs in the 
previous FE identification phase by employing 
Maximum Entropy classification.   
In addition to the features in Section 2.2, two 
more features are applied.   
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, the sentence from Figure 2 has four 
frame elements, where the element ?He? has 
order 0, while ?with string? has order 3. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by considering the phrase type and logical 
functions of each frame element in the 
sentence.  In the example sentence in Figure 2, 
?He? is an external argument Noun Phrase, 
?tied? is a target predicate, and ?the driving 
wheel? is an object argument Noun Phrase.  
Thus, the syntactic pattern associated with the 
sentence is [NP-ext, target, NP-obj, PP-other, 
PP-other]. 
Table 3 shows the list of feature sets used for the 
ME role classification. 
Feature Set 
f(r, lexunit) f(r, pt, lf) 
f(r, target) f(r, pt_-1, lf_-1) 
f(r, pt, pos, voice) f(r, pt_1, lf_1) 
f(r, head) f(r, order, syn) 
f(r, head, lexunit) f(r, lexunit, order, syn) 
f(r, head, frame) f(r, pt, pos, voice, lexunit) 
f(r, frame, r_-1) f(r, frame, r_-2,r_-1) 
f(r, frame,r_-3, r_-2,r_-1) 
Table 3. Feature sets used in role classification. 
3 Results 
SensEval-3 provides the following data set: 
training set (24,558 sentences/ 51,323 frame 
elements/ 40 frames), and test set (8,002 sentences/ 
16,279 frame elements/ 40 frames). We submit two 
sets to SensEval-3, one (test A) is the output of all 
above processes (identifying frame elements and 
tagging them given a sentence), and the other (test 
B) is to tag semantic roles given frame elements.  
For test B, we attempt the role classification for all 
frame elements including frame elements not 
matching the parse tree constituents.  Although 
there are frame elements that have two different 
semantic roles, we ignore those cases and assign 
one semantic role per frame element.  This 
explains why test B shows 99% attempted frame 
elements.  The attempted number for test A is the 
number of frame elements identified by our system.  
Table 4 shows the official scores for these tests. 
Test Prec. Overlap Recall Attempted 
Test A 80.2 78.4 65.4 81.5 
Test B 86.7 86.6 85.8 99.0 
Table 4. Final SensEval-3 scores for the test set. 
In the official evaluation, the precision and recall 
are calculated by counting correct roles that 
overlap even in only one word with the reference 
set.  Overlap score shows how much of an actual 
FE is identified as an FE not penalizing wrongly 
identified part.  Since this evaluation is so lenient, 
we perform another evaluation to check exact 
matches. 
FE boundary 
Identification 
FE boundary 
Identification & 
Role labeling Method 
Prec Rec Prec Rec 
Test A 80.3 66.1 71.1 58.5 
Test B 100.0 99.0 86.7 85.8 
Table 5.  Exact match scores for the test set. 
4 Discussion and Conclusion 
Due to time limit, we?ve not done many 
experiments with different feature sets or 
thresholds in ME classification.  We expect that 
recall will increase with lower thresholds 
especially in lenient evaluation and the final 
performance will increase by optimizing 
parameters. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003.      Comparison of Alignment Templates 
and Maximum Entropy Models for Natural 
Language Processing. Proc. of EACL-2003. 
Budapest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proc. of NAACL-2000, Seattle, USA. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP), 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, 2003. Croatia. 
