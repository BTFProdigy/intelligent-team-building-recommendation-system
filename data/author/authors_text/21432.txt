Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1869?1879,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Unified Model for Topics, Events and Users on Twitter
Qiming Diao
Living Analytics Research Centre
School of Information System
Singapore Management University
qiming.diao.2010@smu.edu.sg
Jing Jiang
Living Analytics Research Centre
School of Information System
Singapore Management University
jingjiang@smu.edu.sg
Abstract
With the rapid growth of social media, Twitter
has become one of the most widely adopted
platforms for people to post short and instant
message. On the one hand, people tweets
about their daily lives, and on the other hand,
when major events happen, people also fol-
low and tweet about them. Moreover, people?s
posting behaviors on events are often closely
tied to their personal interests. In this paper,
we try to model topics, events and users on
Twitter in a unified way. We propose a model
which combines an LDA-like topic model and
the Recurrent Chinese Restaurant Process to
capture topics and events. We further propose
a duration-based regularization component to
find bursty events. We also propose to use
event-topic affinity vectors to model the asso-
ciation between events and topics. Our exper-
iments shows that our model can accurately
identify meaningful events and the event-topic
affinity vectors are effective for event recom-
mendation and grouping events by topics.
1 Introduction
Twitter is arguably the most popular microblog site
where people can post short, instant messages to
share with families, friends and the rest of the
world. For content analysis on Twitter, two impor-
tant concepts have been repeatedly visited: (1) Top-
ics. These are longstanding themes that many per-
sonal tweets revolve around. Example topics range
from music and sports to more serious ones like pol-
itics and religion. Much work has been done to an-
alyze topics on Twitter (Ramage et al, 2010; Hong
and Davison, 2010; Zhao et al, 2011; Lau et al,
2012). (2) Events. These are things that take place
at a certain time and attract many people?s short-
term attention in social media. Example events in-
clude concerts, sports games, scandals and elections.
Event detection on Twitter has been a hot research
topic in recent years (Petrovic? et al, 2010; Weng and
Lee, 2011; Becker et al, 2011; Diao et al, 2012; Li
et al, 2012).
The concepts of topics and events are orthogonal
in that many events fall under certain topics. For
example, concerts fall under the topic about mu-
sic. Furthermore, being social media, Twitter users
play important roles in forming topics and events on
Twitter. Each user has her own topic interests, which
influence the content of her tweets. Whether a user
publishes a tweet related to an event also largely de-
pends on whether her topic interests match the na-
ture of the event. Modeling the interplay between
topics, events and users can deepen our understand-
ing of Twitter content and potentially aid many pred-
ication and recommendation tasks. In this paper, we
aim to construct a unified model of topics, events
and users on Twitter. Although there has been a
number of recent studies on event detection on Twit-
ter, to the best of our knowledge, ours is the first that
links the topic interests of users to their tweeting be-
haviors on events.
Specifically, we propose a probabilistic latent
variable model that identifies both topics and events
on Twitter. To do so, we first separate tweets into
topic tweets and event tweets. The former are related
to a user?s personal life, such as a tweet complain-
ing about the traffic condition or wishing a friend
1869
happy birthday. The latter are about some major
global event interesting to a large group of people,
such as a tweet advertising a concert or comment-
ing on an election result. Although considering only
topic tweets and event tweets is a much simplified
view of the diverse range of tweets, we find it ef-
fective in finding meaningful topics and events. We
further use an LDA-like model (Blei et al, 2003) to
discover topics and the Recurrent Chinese Restau-
rant Process (Ahmed and Xing, ) to discover events.
Details are given in Section 3.1.
Our major contributions lie in two novel modi-
fications to the base model described above. The
first is a duration-based regularization component
that punishes long-term events (Section 3.2). Be-
cause events on Twitter tend to be bursty, this mod-
ification presumably can produce more meaningful
events. More specifically, we borrow the idea of us-
ing pseudo-observed variables to regularize graph-
ical models (Balasubramanyan and Cohen, 2013),
and carefully design the pseudo-observed variable
in our task to capture the burstiness of events. The
second modification is adding event-topic affinity
vectors inspired by PMF-based collaborative filter-
ing (Salakhutdinov and Mnih, 2008) (Section 3.3).
It uses the latent topics to explain users? preferences
of events and subsequently infers the association be-
tween topics and events.
We use a real Twitter data set consisting of 500
users to evaluate our model (Section 4). We find
that the model can discover meaningful topics and
events. Comparison with our base model and with
an existing model for event discovery on Twitter
shows that the two modifications are both effective.
The duration-based regularization helps find more
meaningful events; the event-topic affinity vectors
improve an event recommendation task and helps
produce a meaningful organization of events by top-
ics.
2 Related Work
Study of topics, events and users on Twitter is re-
lated to several branches of work. We review the
most interesting and relevant work below.
Event detection on Twitter: There have been quite
a few studies in this direction in recent years, in-
cluding both online detection (Sakaki et al, 2010;
Petrovic? et al, 2010; Weng and Lee, 2011; Becker et
al., 2011; Li et al, 2012) and offline detection (Diao
et al, 2012). Online detection is mostly concerned
with early detection of major events, so efficiency of
the algorithms is the main focus. These algorithms
do not aim to identify all relevant tweets, nor do
they analyze the association of events with topics. In
comparison, our work focuses on modeling topics,
events and users as well as their relation. Recently,
Petrovic? et al (2013) pointed out that Twitter stream
does not lead news stream for major news events, but
Twitter stream covers a much wider range of events
than news stream. Our work helps better understand
these additional events on Twitter and their relations
with users? topic interests. Our model bears similar-
ity to our earlier work (Diao et al, 2012), but we use
a non-parametric model (RCRP) to discover events
directly inside the probabilistic model.
Temporal topic modeling: A number of models
have been proposed for the temporal aspect of top-
ics (Blei and Lafferty, 2006; Wang and McCallum,
2006; Wang et al, 2007; Hong et al, 2011), but most
of them fix the number of topics. The Recurrent Chi-
nese Restaurant Process (Ahmed and Xing, ) was
proposed to model the life cycles of topics and al-
lows an infinite number of topics. It has later been
combined with LDA to model both topics and events
in news streams and social media streams (Ahmed
et al, 2011; Tang and Yang, 2012). Our work also
jointly models topics and events, but different from
previous work, we do not assume that every docu-
ment (tweet in our case) belongs to an event, which
is important because Twitter contains many personal
posts unrelated to major events.
Collaborative filtering with LDA: Part of our
model is inspired by work on collaborative fil-
tering based on probabilistic matrix factorization
(PMF) (Salakhutdinov and Mnih, 2008). Recently
there has been some work combining LDA with
PMF to recommend items with textual content such
as news articles and advertisements (Wang and Blei,
2011; Agarwal and Chen, 2010). They use topics to
interpret the latent structure of users and items. We
borrow their idea but our items are events, which are
not known and have to be discovered by our model.
1870
Figure 1: Plate notation for the whole model, in which pseudo-observed variables and distributions based on empirical
counts are shown as dotted nodes.
3 Our Model
In this section, we present our model for topics,
events and users on Twitter. We assume that we have
a stream of tweets which are divided into T epoches.
Let t ? {1, 2, . . . , T} be the index of an epoch.
Each epoch contains a set of tweets and each tweet
is a bag of words. We use wt,i,j ? {1, 2, . . . , V }
to denote the j-th word of the i-th tweet in the t-
th epoch, where V is the vocabulary size. The au-
thor of the i-th tweet in the t-th epoch (i.e. the
Twitter user who publishes the tweet) is denoted as
ut,i ? {1, 2, . . . , U}, where U is the total number of
Twitter users we consider.
We first present our base model in Section 3.1.
We then introduce a duration-based regularization
mechanism to ensure the burstiness of events in Sec-
tion 3.2. In Section 3.3 we discuss howwemodel the
relation between topics and events using event-topic
affinity vectors. Finally we discuss model inference
in Section 3.4.
3.1 The Base Model
Recall that our objective is to model topics, events,
users and their relations. As in many topic models,
our topic is a multinomial distribution over words,
denoted as ?a where a is a topic index. Each event is
also a multinomial distribution over words, denoted
as ?k where k is an event index. Because topics are
long-standing and stable, we fix the number of top-
ics to be A, where A can be tuned based on histor-
ical data. In contrast, events emerge and die along
the timeline. We therefore use a non-parametric
model called the Recurrent Chinese Restaurant Pro-
cess (RCRP) (Ahmed and Xing, ) to model the birth
and death of events. To model the relation between
users and topics, we assume each user u has a multi-
nomial distribution over topics, denoted as ?u.
As we have discussed, we separate tweets into two
categories, topic tweets and event tweets. Separa-
tion of these two categories is done through a latent
variable y sampled from a user-specific Bernoulli
distribution ?u. For topic tweets, the topic is sam-
pled from the corresponding user?s topic distribution
?u. For event tweets, the event is sampled accord-
ing to RCRP. We now briefly review RCRP. Gener-
ally speaking, RCRP assumes a Chinese Restaurant
Process (CRP) (Blackwell andMacQueen, 1973) for
items within an epoch and chains up the CRPs in ad-
jacent epochs along the timeline. Specifically, in our
case, the generative process can be described as fol-
lows. Tweets come in according to their timestamps.
In the t-th epoch, for the i-th tweet, we first flip a bi-
ased coin based on probability ?u to decide whether
this tweet is event-related. If it is, then we need to
decide which event it belongs to. It could be an ex-
isting event that has at least one related tweet in the
1871
previous epoch or the current epoch, or it could be a
new event. Let nk,t?1 denote the number of tweets
related to event k at the end of epoch (t ? 1). Let
n(i)k,t denote the number of tweets related to event k
in epoch t before the i-th tweet comes. Let Nt?1
denote the total number of event-related tweets in
epoch (t? 1) and N (i)t denote the number of event-
related tweets in epoch t before the i-th tweet. Then
RCRP assumes that the probability for the i-th tweet
to join event k is nk,t?1+n
(i)
k,t
Nt?1+N(i)t +?
and the probability
to start a new event is ?
Nt?1+N(i)t +?
, where ? is a
parameter. As we can see, RCRP naturally captures
the ?rich-get-richer? phenomenon in social media.
Finally we place Dirichlet and Beta priors on
the various parameters in our model. Formally,
the generative process of our base model is out-
lined in Figure 2, excluding the lines in bold and
blue. We also show the plate notation in Figure 1,
in which the Recurrent Chinese Restaurant Pro-
cess is represented as an infinite dynamic mixture
model (Ahmed and Xing, ) and ?rcrpt means the dis-
tribution on an infinite number of events in epoch t.
Dt is the total number of tweets (both event-related
and topic tweets), while Nt represents the number
event-related tweets in epoch t.
3.2 Regularization on Event Durations
As we have pointed out, events on Twitter tend to
be bursty, i.e. the duration of an event tends to
be short, but this characteristic is not captured by
RCRP. While there can be different ways to incor-
porate this intuition, here we adopt the idea of regu-
larization using pseudo-observed variables proposed
recently by Balasubramanyan and Cohen (2013).
We introduce a pseudo-observed binary variable rt,i
for each tweet, where the value of rt,i is set to 1
for all tweets. We assume that this variable is de-
pendent on the hidden variables y and s. Specif-
ically, if yt,i is 0, i.e. the tweet is topic-related,
then rt,i gets a value of 1 with probability 1. If
yt,i is 1, then we look at all the tweets that belong
to event st,i. Our goal is to make sure that this
tweet is temporally close to these other tweets. So
we assume that rt,i gets a value of 1 with proba-
bility exp(?
?T
t?=1,|t??t|>1 ?|t ? t?|nst,i,t?), where
nst,i,t? is the number of tweets in epoch t? that be-
? For each topic a = 1, . . . , A
- draw ?a ? Dirichlet(?)
? For each user u = 1, . . . , U
- draw ?u ? Dirichlet(?), ?u ? Beta(?)
? For each epoch t and tweet i
- draw yt,i ? Bernoulli(?ut,i)
- If yt,i = 0
* draw zt,i ? Multinomial(?ut,i)
* For each j, draw wt,i,j ? Multinomial(?zt,i)
- If yt,i = 1
* draw st,i from RCRP
* If st,i is a new event
. draw ?st,i ? Dirichlet(?)
. draw ?0st,i ? Gaussian(0, ?
?1)
. draw ?st,i ? Gaussian(0, ?
?1IA)
* draw rt,i ? Bernoulli(?st,i,t), where ?st,i,t =
exp(?
?T
t?=1,|t??t|>1 ?|t
? ? t|nst,i,t?)
* draw ct,i ? Gaussian(?0st,i +?
T
st,i ? z?ut,i , ?
?1)
* For each j, draw wt,i,j ? Multinomial(?st,i)
Figure 2: The generative process of our model, in which
the duration-based regularization (section 3.2) and the
event-topic affinity vector (section 3.3) are in blue and
bold lines.
long to event st,i and ? > 0 is a parameter. We can
see that when we factor in the generation of these
pseud-observed variables r, we penalize long-term
events and favor events whose tweets are concen-
trated along the timeline. Generation of these vari-
ables r is shown in bold and blue in Figure 2.
3.3 Event-Topic Affinity Vectors
So far in our model topics and events are not re-
lated. However, many events are highly related to
certain topics. For example, a concert is related to
music while a football match is related to sports. We
would like to capture these relations between top-
ics and events. One way to do it is to assume that
event tweets also have topical words sampled from
the event?s topic distribution, something similar to
the models by Ahmed et al (2011) and by Tang
and Yang (2012). However, our prelimiary exper-
iments show that this idea does not work well on
Twitter, mainly because tweets are too short. Here
we explore another approach inspired by recommen-
dation methods based on probabilistic matrix factor-
ization (Salakhutdinov and Mnih, 2008). The idea
is that when a user posts a tweet about an event, we
can treat the event as an item and this posting be-
1872
havior as adoption of the item. If we assume that the
adoption behavior is influenced by some latent fac-
tors, i.e. the latent topics, then basically we would
like the topic distribution of this user to be close to
that of the event.
Specifically, we assume that each event k has as-
sociated with it an A-dimensional vector ?k and a
parameter ?0k. The vector ?k represents the event?s
affinity to topics. ?0k is a bias term that represents
the inner popularity of an event regardless of its
affinity to any topic. We further assume that each
tweet has another pseudo-observed variable ct,i that
is set to 1. For topic tweets, ct,i gets a value of 1
with probability 1. For event tweets, ct,i is gener-
ated by a Gaussian distribution with mean equal to
?0st,i + ?st,i ? z?ut,i , where z?u is an A-dimensional
vector denoting the empirical topic distribution of
user u?s tweets. This treatment follows the practice
of fLDA by Agarwal and Chen (2010). Let C?u,a be
the number of tweets by user u assigned to topic a,
based on the values of the latent variables y and z.
Then
z?u,a =
C?u,a
?A
a?=1 C?u,a?
,
ct,i ?Gaussian(?0st,i + ?st,i ? z?ut,i , ?
?1),
where ? is a parameter. We generate ?k and ?0k using
Gaussian priors once event k emerges. The genera-
tion of the variables c is shown in bold and blue in
Figure 2.
3.4 Inference
We train the model using a stochastic EM sampling
scheme. In this scheme, we alternate between Gibbs
sampling and gradient descent. In the Gibbs sam-
pling part, we fix the values of ?0k and ?k for each
event k, and then we sample the latent variables yt,i
,zt,i and st,i for each tweet. In the gradient descent
part, we update the event-topic affinity vectors ?k
and the bias term ?0k of each event k by keeping the
assignment of the variables yt,i ,zt,i and st,i fixed.
For the Gibbs sampling part, we jointly sample
yt,i = 0, zt,i = a (topic tweet) and yt,i = 1, st,i = k
(event tweet) as follows:
Topic tweet:
p(yt,i = 0, zt,i = a|y?t,i, z?t,i,w, r, c, ut,i)
?
n(?)u,0 + ?
n(?)u,(.) + 2?
n(?)u,a + ?
n(?)u,(.) +A?
?V
v=1
?E(v)?1
i=0 (n
(?)
a,v + i+ ?)
?E(.)?1
i=0 (n
(?)
a,(.) + i+ V ?)
?
t?,i??Iu
N (ct?,i? |?0st?,i? + ?st?,i? ? z?
?
u, ??1)
N (ct?,i? |?0st?,i? + ?st?,i? ? z?u, ?
?1)
Event tweet:
p(yt,i = 1, st,i = k|y?t,i, z?t,i,w, r, c, ut,i)
?
n(?)u,1 + ?
n(?)u,(.) + 2?
1
N
(
nRCRPk,t N (ct,i|?0st,i + ?st,i ? z?u, ?
?1)
? exp(?
T
?
t?=1
|t??t|>1
?|t? t?|nk,t?)
)
?V
v=1
?E(v)?1
i=0 (n
(?)
k,v + i+ ?)
?E(.)?1
i=0 (n
(?)
k,(.) + i+ V ?)
in which,
nRCRPk,t =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(nk,t?1 + nk,t)
? nk,t+nk,t+1nk,t if nk,t?1 > 0, nk,t > 0,
nk,t?1 if nk,t?1 > 0, nk,t = 0,
nk,t+1 if nk,t+1 > 0, nk,t = 0,
? if k is a new event,
where we use u to represent ut,i. n(?)u,0 is the number
of topic tweets by user u while n(?)u,1 is the number
of event tweets by user u. They stem from integrat-
ing out the user?s Bernoulli distribution ?u. n(?)u,(.) is
the total number of tweets by user u. Similarly, n(?)u,a
is the number of tweets assigned to topic a for this
user, resulting from integrating out the user?s topic
distribution ?u. n(?)u,(.) is the same as n
(?)
u,0. E(v) is
the number of times word type v appears in the cur-
rent tweet, and E(.) is the total number of words in
the current tweet. n(?)a,v is the number of times word
type v is assigned to topic a, and n(?)a,(.) is the number
of words assigned to topic a. n(?)k,v is the number of
times word type v is assigned to event k, and n(?)k,(.)
is the total number of words assigned to event k.
These word counters stem form integrating out each
event?s word distribution and are set to zero when k
is a new event. Iu = {t?, i?|yt?,i? = 1, ut?,i? = u},
which is the set of event tweets published by user u,
and u represents ut,i for short. z??u is the empirical
1873
counting vector which considers the current tweet?s
topic assignment, while z?u and all other counters do
not consider the current tweet. Finally, N is a lo-
cal normalization factor for event tweets, which in-
cludes the RCRP, event-topic affinity and regulariza-
tion on event duration.
With the previous Gibbs sampling step, we can
get the assignment of variables yt,i ,zt,i and st,i.
Given the assignment, we use gradient descent to
update the values of the bias term ?0k and the event-
topic affinity vectors ?k for each current existing
event k. First, we can get the logarithm of the poste-
rior distribution:
lnP (y, z, s, r, c|w,u, all priors)
= constant ?
?
?
k=1
{ ?2(?
0
k
2 + ?k ? ?k)
+
U
?
u=1
nu,k
?
2 [1 ? (?
0
k + ?k ? z?u)]
2},
where nu,k is the number of event tweets about
event k published by user u. The derivative of the
logarithm of the posterior distribution with respect
to the bias term ?0k and the event-topic affinity vec-
tor ?k are as follows:
? lnP
??0k
= ???0k +
U
?
u=1
?nu,k[1 ? (?0k + ?k ? z?u)],
? lnP
??k
= ???k +
U
?
u=1
?nu,k[1 ? (?0k + ?k ? z?u)]z?u.
4 Experiment
4.1 Dataset and Experiment Setup
We evaluate our model on a Twitter dataset that con-
tains 500 users. These users are randomly selected
from a much larger pool of around 150K users based
in Singapore. Selecting users from the same coun-
try/city ensures that we find coherent and meaning-
ful topics and events. We use tweets published be-
tween April 1 and June 30, 2012 for our experi-
ments. For preprocessing, we use the CMU Twitter
POS Tagger1 to tag these tweets and remove those
non-standard words (i.e. words tagged as punc-
tuation marks, emoticons, urls, at-mentions, pro-
nouns, etc.) and stop words. We also remove tweets
1http://www.ark.cs.cmu.edu/TweetNLP/
with less than three words. After preprocessing, the
dataset contains 655,881 tweets in total.
Recall that our model is designed to identify top-
ics, events and their relations with users. We there-
fore would like to evaluate the quality of the iden-
tified topics and events as well as the usefulness of
the discovered topic distributions of users and event-
topic affinity vectors. Because our topic discovery
mechanism is fairly standard and a quick inspection
shows that the discovered topics are generally mean-
ingful and comparable to those discovered by stan-
dard LDA, here we do not focus on evaluation of
topics. In Section 4.2 we evaluate the quality of the
discovered events. In Section 4.3 we show how the
discovered event-topic affinity vectors can be useful.
For comparison, we consider an existing method
called TimeUserLDA introduced in our previous
work (Diao et al, 2012). TimeUserLDA also mod-
els topics and events by separating topic tweets from
event tweets. However, it groups event tweets into a
fixed number of bursty topics and then uses a two-
state machine in a postprocessing step to identify
events from these bursty topics. Thus, events are
not directly modeled within the generative process
itself. In contrast, events are inherent in our gener-
ative model. We do not compare with other event
detection methods because our objective is not on-
line event detection.
We also compare our final model with two de-
generate versions of it. We refer to the base model
described in Section 3.1 as Base and the model
with the duration-based regularization asBase+Reg.
Comparison with these two degenerate models al-
lows us to assess the effect of the two modifications
we propose. We refer to the final model with both
the duration-based regularization and the event-topic
affinity vectors as Base+Reg+Aff.
For the parameter setting, we empirically set A to
40, ? to 50A , ? to 1, ? to 0.01, ? to 1, ? to 10, ? to 1,
and the duration regularization parameter ? to 0.01.
When a new event k is created, the inner popular-
ity bias term ?0k is set to 1, and the factors in event-
topic affinity vectors ?k are all set to 0. We run the
stochastic EM sampling scheme for 300 iterations.
After Gibbs sampling assigns each variable a value
at the end of each iteration, we update the values of
?0k and ?k for the existing events using gradient de-
scent.
1874
Event Top words Duration Inner popularity (?0k)
debate caused by
Manda Swaggie
singapore, bieber, europe, amanda, justin, trending,
manda, hates, swaggie, hate
17 June - 19 June 0.9457
Indonesia tsunami tsunami, earthquake, indonesia, singapore, hit, warn-
ing, aceh, 8.9, safe, magnitude
10 April - 12 April 0.9439
SJ encore concert #ss4encore, cr, #ss4encoreday2, hyuk, 120526, super,
leader, changmin, fans, teuk
26 May - 28 May 0.8360
Mother?s Day day, happy, mother?s, mothers, love, mom, mum, ev-
eryday, mother, moms
11 May - 14 May 0.9370
April Fools? Day april, fools, day, fool, joke, prank, happy, today, trans,
fool?s
1 April - 3 April 0.9322
Table 1: The top-5 events identified by Base+Reg+Aff. We show the story name which is manually labeled, top ten
ranking words, lasting duration and the inner popularity (?0k) for each event.
4.2 Events
First we quantitatively evaluate the quality of the de-
tected events. Our model finds clusters of tweets
that represent events. We first assess whether these
events are meaningful. We then judge whether the
detected event tweets are indeed related to the corre-
sponding event.
Quality of Top Events
Method P@5 P@10 P@20 P@30
Base+Reg+Aff 1.000 1.000 0.950 0.900
Base+Reg 1.000 1.000 0.950 0.867
Base 0.000 0.200 0.250 0.367
TimeUserLDA 1.000 0.800 0.750 0.600
Table 2: Precision@K for the various methods.
Usually we are interested in the most popular
events on Twitter. We therefore assess whether the
top events are meaningful. For each method, we
rank the detected events based on the number of
tweets assigned to them and then pick the top-30
events for each method. We randomly mix these
events and ask two human judges to label them.
The judges are given 100 randomly selected tweets
for each event (or all tweets if an event contains
less than 100 tweets). The judges can use exter-
nal sources to help them. If an event is meaningful
based on the 100 sample tweets, a score of 1 is given.
Otherwise it is scored 0. The inter-annotator agree-
ment score is 0.744 using Cohen?s kappa, showing
substantial agreement. Finally we treat an event as
meaningful if both judges have scored it 1.
Table 2 shows the performance in terms of
precision@K, and Table 1 shows the top 5 events
of our model (i.e., Base+Reg+Aff). We have the
following findings from the results: (1) Our base
model performs quite poorly for the top events while
Base+Reg and Base+Reg+Aff perform much bet-
ter. This shows that the duration-based regular-
ization is critical in finding meaningful events. A
close examination shows that the base model clus-
ters many general topic tweets as events, such as
tweets about transportation and music and even
foursquare tweets. (2) TimeUserLDA performs well
for the very top events (P@5 and P@10) but its
performance drops for lower-ranked events (P@20
and P@30), similar to what was reported by Diao
et al (2012). A close examination shows that this
method is good at finding major events that do not
have strong topic association and thus attract most
people?s attention, e.g. earthquakes, but not good at
finding topic-oriented events such as some concerts
and sports games. This is because this method mixes
topics and events first and only detects events from
bursty topics in a second stage of postprocessing. In
contrast, our model performs well for topic-oriented
events. (3) The difference between Base+Reg and
Base+Reg+Aff is small, suggesting that the event-
topic affinity vectors are not crucial for event detec-
tion.
Precision of Event Tweets
Next, we evaluate the relevance of the detected
event tweets to each event. To make a fair compar-
ison, we select only the common events identified
by all the methods. We pick 3 out of 5 common
events shared by all methods within top-30 events
1875
Event TimeUserLDA Base Base+Reg Base+Reg+Aff
Father?s Day 0.61 0.63 0.71 0.72
debate caused by Manda Swaggie 0.73 0.74 0.84 0.80
Indonesia tsunami 0.75 0.75 0.82 0.80
Super Junior album release N/A 0.72 0.78 0.81
Table 3: Precision of the event tweets for the 4 common events.
(we pick ?Fathers? day? to represent public festi-
vals, and ignore the similar events ?Mothers? day?
and ?April fools?). We also pick one event shared
by three RCRP based models. We further ask one
of the judges to score the 100 tweets as either 1 or 0
based on their relevance to the event. The precision
of the 100 tweets for each event and each method is
shown in Table 3. We can see that again Base+Ref
and Base+Ref+Aff perform similarly, and both out-
perform the other two methods. We also take a
close look at the tweets and find that the false posi-
tives mislabeled by Base is mainly due to the long-
duration of the discovered events. For example, for
the event ?Super Junior album release,? Base finds
other music-related tweets surrounding the peak pe-
riod of the event itself.
In summary, our evaluation on event quality
shows that (1) Using the non-parametric RCRP
model to identify events within the generative
model itself is advantageous over TimeUserLDA,
which identifies events by postprocessing. (2) The
duration-based regularization is crucial for finding
more meaningful events.
4.3 Event-Topic Association
Besides event identification, our model also finds the
association between events and topics through the
event-topic affinity vectors. The discovered event-
topic association can potentially be used for various
tasks. Here we conduct two experiments to demon-
strate its usefulness.
Event Recommendation
Recall that to discover event-topic association, we
treat an event as an item and a tweet about the event
as indication of the user?s adoption of the item. Fol-
lowing this analogy with item recommendation, we
define an event recommendation task where the goal
is to recommend an event to users who have not
posted any tweet about the event but may potentially
be interested in the event. Intuitively, if a user?s topic
distribution is similar to the event-topic affinity vec-
tor of the event, then the user is likely to be inter-
ested in the event.
Specifically, we use the first two months? data
(April and May 2012) as training data to learn all
the users? topic distributions. We then use a ransom
subset of 250 training users and their tweets in June
to identify events in June as well as the event-topic
affinity vectors of these events. We pick 8 meaning-
ful events that are ranked high by all methods for
testing. For each event, we try to find among the
remaining 250 users those who may be interested
in the event and compare the results with ground
truth obtained by human judgment. Because it is
time consuming to obtain the ground truth for all 250
users, we randomly pick 100 of these 250 users for
testing purpose. For each test user and each event,
we manually inspect the user?s tweets around the
peak days of the event to judge whether she has com-
mented on the event. This is used as ground truth.
With our complete model Base+Reg+Aff, we can
simply rank the 100 test users in decreasing order of
?k ? z?u. For the other methods, because we do not
have any parameter that directly encodes event-topic
association, we cannot rank users based on how sim-
ilar their topic distributions are to the event?s affinity
to topics. We instead adopt a collaborative filtering
strategy and rank the test users by their similarity
with those training users who have tweeted about
the event. Specifically, each of these methods pro-
duces a topic distribution ?u for each user. In addi-
tion, for each test event these methods identify a list
of training users who have tweeted on it. By taking
the average topic distribution of these training users
and compute its cosine similarity with a test user?s
topic distribution, we can rank the 100 test users.
Since we have turned the recommendation task
into a ranking task, we use Average Precision, a
commonly used metric in information retrieval, to
compare the performance. Average Precision is the
1876
Event TimeUserLDA Base Base+Reg Base+Reg+Aff Inner popularity (?0k)
debate caused by Manda Swaggie 0.3533 0.3230 0.3622 0.2956 0.943
Father?s Day 0.3811 0.3525 0.3596 0.4362 0.917
Big Bang album release 0.1406 0.1854 0.1533 0.1902 0.893
City Harvest Church scandal N/A 0.2832 0.1874 0.3347 0.890
Alex Ong pushing an old lady N/A 0.1540 0.1539 0.1113 0.876
final episode of Super Spontan (reality show) N/A 0.0177 0.0331 0.2900 0.862
Super Junior album release N/A 0.0398 0.0330 0.5900 0.792
LionsXII 9-0 Sabah FA (soccer) 0.0711 0.1207 0.2385 0.3220 0.773
MAP N/A 0.1845 0.1901 0.3213
Table 4: For the 8 test events that happened in June 2012, we compute the Average Precision for each event. We also
show the Mean Average Precision (MAP) when applicable.
Topic Top words of the topic Related event Top words of the event
Food eat, food, eating, ice, hungry, din-
ner, cream, lunch, chicken, buy
Ben&Jerry free cone day free, cone, day, ben, jerry?s, today, b&j, zoo,
#freeconeday, singapore
Super Junior encore concert #ss4encore, cr, #ss4encoreday2, hyuk,
120526, super, leader, changmin, fans, teuk
Korean Music
music, big, cr, super, bang, junior,
love, concert, bank, album
Super Junior Shanghai concert #ss4shanghai, cr, 120414, donghae, eun-
hyuk, giraffe, solo, hyuk, ryeowook, shang-
hai
Super Junior Paris concert #ss4paris, cr, paris, super, 120406, ss4, ju-
nior, siwon, show, update
Malay aku, nak, tak, kau, ni, lah, tk, je,mcm, nk
final episode of Super Spontan zizan, johan, friendship, jozan, #superspon-
tan, skips, forever, real, juara, gonna
LionsXII 9-0 Sabah FA sabah, 9-0, #lionsxii, lions, singapore, 7-0,
amet, sucks, sabar, goal
Soccer win, game, man, chelsea, match,city, goal, good, united, team
Man City crowned English champions man, city, united, qpr, fuck, bored, lah, love,
glory, update
Table 5: Example topics and their corresponding correlated events.
average of the precision value obtained for the set
of top items existing after each relevant item is re-
trieved (Manning et al, 2008). We also rank the
8 events in decreasing order of their inner popular-
ity ?0k learned by our complete model. The results
are shown in Table 4. We have the following find-
ings from the table. (1) Our complete method out-
performs the other methods for 6 out of the 8 test
events, suggesting that with the inferred event-topic
affinity vectors we can do better event recommen-
dation. (2) The improvement brought by the event-
topic affinity vectors, as reflected in the difference in
Average Precision between Base+Reg+Aff and Base
(or Base+Reg) is more pronounced for events with
lower inner popularity. Recall that the inner popu-
larity of an event shows the inherent popularity of
an event regardless of its association with any topic,
that is, an event with high inner popularity attracts
attention of many people regardless of their topic
interests, while an event with low inner popularity
tends to attract attention of certain people with simi-
lar topic interests. The finding above suggests that
the event-topic affinity vectors are especially use-
ful for recommending events that attract only certain
people?s attention, such as those related to sports,
music, etc.
One may wonder for the events with low inner
popularity why we could not achieve the same ef-
fect by Base or Base+Reg where we consider the
topic similarity of test users with training users who
have tweeted about the event. Our close examina-
tion shows that for these events although Base and
Base+Reg may identify relevant event tweets with
decent precision, the users they identify who have
1877
tweeted about the event may not share similar topic
interests. As a result, when we average these users?
topic interests, we cannot obtain a clear skewed
topic distribution that explains the event?s affinity
to different topics. In contrast, Base+Reg+Aff ex-
plicitly models the event-topic affinity vector and
prefers to assign a tweet to an event if its author?s
topic distribution is similar to the event?s affinity
vector. Through the training iterations, the users
who have tweeted about an event as identified by
Base+Reg+Aff will gradually converge to share sim-
ilar topic distributions.
Grouping Events by Topics
Finally, we show that the event-topic affinity vec-
tors can also be used to group events by topics. This
can potentially be used to better organize and present
popular events in social media. In Table 5 we show
a few highly related events for a few popular topics
in our Twitter data set. Specifically given a topic a
we rank the meaningful events that contain at least
70 tweets based on ?k,a. We can see from the table
that the events are indeed related to the correspond-
ing topic. The event ?LionsXII 9-0 Sabah FA? is
particularly interesting in that it is highly related to
both the topic on Malay and the topic on soccer. (Li-
onsXII is a soccer team from Singapore and Sabah
FA is a soccer team from Malaysia.)
5 Conclusion
In this paper, we propose a unified model to study
topics, events and users jointly. The base of our
method is a combination of an LDA-like model and
the Recurrent Chinese Restaurant Process, which
aims to model users? longstanding personal topic in-
terests and events over time simultaneously. The Re-
current Chinese Restaurant Process is appealing in
the sense that it provides a principled dynamic non-
parametric model in which the number of events is
not fixed overtime. We further use a time duration-
based regularization to capture the fast emergence
and disappearance of events on Twitter, which is
effective to produce more meaningful events. Fi-
nally, we use an inner popularity bias parameter and
event-topic affinity vectors to interpret an event?s
inherent popularity and its affinity to different top-
ics. Our experiments quantitatively show that our
proposed model can effectively identify meaningful
events and accurately find relevant tweets for these
events. Furthermore, the event-topic association in-
ferred by our model can help an event recommenda-
tion task and organize events by topics.
6 Acknowledgments
This research is supported by the Singapore National
Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
References
Deepak Agarwal and Bee-Chung Chen. 2010. fLDA:
matrix factorization through latent Dirichlet aloca-
tion. In Proceedings of the third ACM international
conference on Web search and data mining, pages 91?
100.
Amr Ahmed and Eric P. Xing. Dynamic non-parametric
mixture models and the recurrent Chinese restaurant
process: with applications to evolutionary clustering.
In Proceedings of the SIAM International Conference
on Data Mining, SDM 2008.
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J. Smola, and Choon Hui Teo. 2011. Uni-
fied analysis of streaming news. In In Proceedings of
the 20th international conference on World wide web,
pages 267 ? 276.
Ramnath Balasubramanyan and William W. Cohen.
2013. Regularization of latent variable models to ob-
tain sparsity. In Proceedings of SIAM Conference on
Data Mining.
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Be-
yond trending topics: Real-world event identification
on twitter. In Fifth International AAAI Conference on
Weblogs and Social Media.
D. Blackwell and J. MacQueen. 1973. Ferguson distribu-
tions via Polya urn schemes. The Annals of Statistics,
pages 353?355.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, pages 113 ? 120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 536 ? 544.
1878
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the First Workshop on Social Media Analytics, pages
80?88.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent topic
model for multiple text streams. In Proceedings of
SIGKDD.
Jey Han Lau, Nigel Collier, and Timothy Baldwin. 2012.
On-line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference of on Computa-
tional Linguistics, pages 1519?1534.
Chenliang Li, Aixin Sun, and Anwitaman Datta. 2012.
Twevent: segment-based event detection from tweets.
In In Proceedings of the 21st ACM international con-
ference on Information and knowledge management,
pages 155 ? 164.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze, 2008. Introduction to Information Re-
trieval, chapter Evaluation in information retrieval.
Cambridge University Press.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to Twitter. In The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 181 ? 189.
Sas?a Petrovic?, Miles Osborne, Richard McCreadie,
RichardMacdonald, Richard Ounis, and Luke Shrimp-
ton. 2013. Can twitter replace newswire for breaking
news? In Proceedings of the 7th International Confer-
ence on Weblogs and Social Media.
Daniel Ramage, Susan T. Dumais, and Daniel J. Liebling.
2010. Characterizing microblogs with topic models.
In Proceedings of the 4th International Conference on
Weblogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on World Wide Web,
pages 851?860.
Ruslan Salakhutdinov and Andriy Mnih. 2008. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems, volume 20.
Xuning Tang and Christopher C. Yang. 2012. TUT:
a statistical model for detecting trends, topics and
user interests in social media. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 972 ? 981.
Chong Wang and David M. Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448 ? 456.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining., pages 424 ? 433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Proceedings of the 13th acm sigkdd
international conference on knowledge discovery and
data mining. In Proceedings of SIGKDD, pages 784 ?
793.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the Fifth International
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing Twitter and traditional media using topic
models. In Proceedings of the 33rd European Confer-
ence on IR Research, pages 338?349.
1879
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536?544,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Finding Bursty Topics from Microblogs
Qiming Diao, Jing Jiang, Feida Zhu, Ee-Peng Lim
Living Analytics Research Centre
School of Information Systems
Singapore Management University
{qiming.diao.2010, jingjiang, fdzhu, eplim}@smu.edu.sg
Abstract
Microblogs such as Twitter reflect the general
public?s reactions to major events. Bursty top-
ics from microblogs reveal what events have
attracted the most online attention. Although
bursty event detection from text streams has
been studied before, previous work may not
be suitable for microblogs because compared
with other text streams such as news articles
and scientific publications, microblog posts
are particularly diverse and noisy. To find top-
ics that have bursty patterns on microblogs,
we propose a topic model that simultaneous-
ly captures two observations: (1) posts pub-
lished around the same time are more like-
ly to have the same topic, and (2) posts pub-
lished by the same user are more likely to have
the same topic. The former helps find event-
driven posts while the latter helps identify and
filter out ?personal? posts. Our experiments
on a large Twitter dataset show that there are
more meaningful and unique bursty topics in
the top-ranked results returned by our mod-
el than an LDA baseline and two degenerate
variations of our model. We also show some
case studies that demonstrate the importance
of considering both the temporal information
and users? personal interests for bursty topic
detection from microblogs.
1 Introduction
With the fast growth of Web 2.0, a vast amount of
user-generated content has accumulated on the so-
cial Web. In particular, microblogging sites such
as Twitter allow users to easily publish short in-
stant posts about any topic to be shared with the
general public. The textual content coupled with
the temporal patterns of these microblog posts pro-
vides important insight into the general public?s in-
terest. A sudden increase of topically similar posts
usually indicates a burst of interest in some event
that has happened offline (such as a product launch
or a natural disaster) or online (such as the spread
of a viral video). Finding bursty topics from mi-
croblogs therefore can help us identify the most pop-
ular events that have drawn the public?s attention. In
this paper, we study the problem of finding bursty
topics from a stream of microblog posts generated
by different users. We focus on retrospective detec-
tion, where the text stream within a certain period is
analyzed in its entirety.
Retrospective bursty event detection from tex-
t streams is not new (Kleinberg, 2002; Fung et al,
2005; Wang et al, 2007), but finding bursty topic-
s from microblog steams has not been well studied.
In his seminal work, Kleinberg (2002) proposed a s-
tate machine to model the arrival times of documents
in a stream in order to identify bursts. This model
has been widely used. However, this model assumes
that documents in the stream are all about a given
topic. In contrast, discovering interesting topics that
have drawn bursts of interest from a stream of top-
ically diverse microblog posts is itself a challenge.
To discover topics, we can certainly apply standard
topic models such as LDA (Blei et al, 2003), but
with standard LDA temporal information is lost dur-
ing topic discovery. For microblogs, where posts are
short and often event-driven, temporal information
can sometimes be critical in determining the topic of
a post. For example, typically a post containing the
536
word ?jobs? is likely to be about employment, but
right after October 5, 2011, a post containing ?jobs?
is more likely to be related to Steve Jobs? death. Es-
sentially, we expect that on microblogs, posts pub-
lished around the same time have a higher probabil-
ity to belong to the same topic.
To capture this intuition, one solution is to assume
that posts published within the same short time win-
dow follow the same topic distribution. Wang et
al. (2007) proposed a PLSA-based topic model that
exploits this idea to find correlated bursty patterns
across multiple text streams. However, their model
is not immediately applicable for our problem. First,
their model assumes multiple text streams where
word distributions for the same topic are different
on different streams. More importantly, their model
was applied to news articles and scientific publica-
tions, where most documents follow the global top-
ical trends. On microblogs, besides talking about
global popular events, users also often talk about
their daily lives and personal interests. In order to
detect global bursty events from microblog posts, it
is important to filter out these ?personal? posts.
In this paper, we propose a topic model designed
for finding bursty topics from microblogs. Our mod-
el is based on the following two assumptions: (1) If
a post is about a global event, it is likely to follow
a global topic distribution that is time-dependent.
(2) If a post is about a personal topic, it is likely
to follow a personal topic distribution that is more
or less stable over time. Separation of ?global? and
?personal? posts is done in an unsupervised manner
through hidden variables. Finally, we apply a state
machine to detect bursts from the discovered topics.
We evaluate our model on a large Twitter dataset.
We find that compared with bursty topics discovered
by standard LDA and by two degenerate variations
of our model, bursty topics discovered by our model
are more accurate and less redundant within the top-
ranked results. We also use some example bursty
topics to explain the advantages of our model.
2 Related Work
To find bursty patterns from data streams, Kleinberg
(2002) proposed a state machine to model the ar-
rival times of documents in a stream. Different states
generate time gaps according to exponential density
functions with different expected values, and bursty
intervals can be discovered from the underlying state
sequence. A similar approach by Ihler et al (2006)
models a sequence of count data using Poisson dis-
tributions. To apply these methods to find bursty
topics, the data stream used must represent a single
topic.
Fung et al (2005) proposed a method that iden-
tifies both topics and bursts from document stream-
s. The method first finds individual words that have
bursty patterns. It then finds groups of words that
tend to share bursty periods and co-occur in the same
documents to form topics. Weng and Lee (2011)
proposed a similar method that first characterizes the
temporal patterns of individual words using wavelet-
s and then groups words into topics. A major prob-
lem with these methods is that the word clustering
step can be expensive when the number of bursty
words is large. We find that the method by Fung
et al (2005) cannot be applied to our dataset be-
cause their word clustering algorithm does not scale
up. Weng and Lee (2011) applied word clustering
to only the top bursty words within a single day, and
subsequently their topics mostly consist of two or
three words. In contrast, our method is scalable and
each detected bursty topic is directly associated with
a word distribution and a set of tweets (see Table 3),
which makes it easier to interpret the topic.
Topic models provide a principled and elegan-
t way to discover hidden topics from large docu-
ment collections. Standard topic models do not con-
sider temporal information. A number of temporal
topic models have been proposed to consider topic
changes over time. Some of these models focus on
the change of topic composition, i.e. word distri-
butions, which is not relevant to bursty topic detec-
tion (Blei and Lafferty, 2006; Nallapati et al, 2007;
Wang et al, 2008). Some other work looks at the
temporal evolution of topics, but the focus is not on
bursty patterns (Wang and McCallum, 2006; Ahmed
and Xing, 2008; Masada et al, 2009; Ahmed and X-
ing, 2010; Hong et al, 2011).
The model proposed by Wang et al (2007) is the
most relevant to ours. But as we have pointed out
in Section 1, they do not need to handle the sep-
aration of ?personal? documents from event-driven
documents. As we will show later in our experi-
ments, for microblogs it is critical to model users?
537
personal interests in addition to global topical trend-
s.
To capture users? interests, Rosen-Zvi et al
(2004) expand topic distributions from document-
level to user-level in order to capture users? specif-
ic interests. But on microblogs, posts are short and
noisy, so Zhao et al (2011) further assume that each
post is assigned a single topic and some words can
be background words. However, these studies do not
aim to detect bursty patterns. Our work is novel in
that it combines users? interests and temporal infor-
mation to detect bursty topics.
3 Method
3.1 Preliminaries
We first introduce the notation used in this paper and
formally formulate our problem. We assume that
we have a stream of D microblog posts, denoted as
d1, d2, . . . , dD. Each post di is generated by a user
ui, where ui is an index between 1 and U , and U is
the total number of users. Each di is also associat-
ed with a discrete timestamp ti, where ti is an index
between 1 and T , and T is the total number of time
points we consider. Each di contains a bag of word-
s, denoted as {wi,1, wi,2, . . . , wi,Ni}, where wi,j is
an index between 1 and V , and V is the vocabulary
size. Ni is the number of words in di.
We define a bursty topic b as a word distri-
bution coupled with a bursty interval, denoted as
(?b, tbs, tbe), where ?b is a multinomial distribution
over the vocabulary, and tbs and tbe (1 ? tbs ? tbe ? T )
are the start and the end timestamps of the bursty in-
terval, respectively. Our task is to find meaningful
bursty topics from the input text stream.
Our method consists of a topic discovery step and
a burst detection step. At the topic discovery step,
we propose a topic model that considers both users?
topical interests and the global topic trends. Burst
detection is done through a standard state machine
method.
3.2 Our Topic Model
We assume that there are C (latent) topics in the text
stream, where each topic c has a word distribution
?c. Note that not every topic has a bursty interval.
On the other hand, a topic may have multiple bursty
intervals and hence leads to multiple bursty topics.
We also assume a background word distribution ?B
that captures common words. All posts are assumed
to be generated from some mixture of these C + 1
underlying topics.
In standard LDA, a document contains a mixture
of topics, represented by a topic distribution, and
each word has a hidden topic label. While this is a
reasonable assumption for long documents, for short
microblog posts, a single post is most likely to be
about a single topic. We therefore associate a single
hidden variable with each post to indicate its topic.
Similar idea of assigning a single topic to a short se-
quence of words has been used before (Gruber et al,
2007; Zhao et al, 2011). As we will see very soon,
this treatment also allows us to model topic distribu-
tions at time window level and user level.
As we have discussed in Section 1, an importan-
t observation we have is that when everything else
is equal, a pair of posts published around the same
time is more likely to be about the same topic than a
random pair of posts. To model this observation, we
assume that there is a global topic distribution ?t for
each time point t. Presumably ?t has a high prob-
ability for a topic that is popular in the microblog-
sphere at time t.
Unlike news articles from traditional media,
which are mostly about current affairs, an important
property of microblog posts is that many posts are
about users? personal encounters and interests rather
than global events. Since our focus is to find popular
global events, we need to separate out these ?person-
al? posts. To do this, an intuitive idea is to compare
a post with its publisher?s general topical interests
observed over a long time. If a post does not match
the user?s long term interests, it is more likely re-
lated to a global event. We therefore introduce a
time-independent topic distribution ?u for each us-
er to capture her long term topical interests.
We assume the following generation process for
all the posts in the stream. When user u publishes
a post at time point t, she first decides whether to
write about a global trendy topic or a personal top-
ic. If she chooses the former, she then selects a topic
according to ?t. Otherwise, she selects a topic ac-
cording to her own topic distribution ?u. With the
chosen topic, words in the post are generated from
the word distribution for that topic or from the back-
ground word distribution that captures white noise.
538
1. Draw ?B ? Dirichlet(?), ? ? Beta(?), ? ?
Beta(?)
2. For each time point t = 1, . . . , T
(a) draw ?t ? Dirichlet(?)
3. For each user u = 1, . . . , U
(a) draw ?u ? Dirichlet(?)
4. For each topic c = 1, . . . , C,
(a) draw ?c ? Dirichlet(?)
5. For each post i = 1, . . . , D,
(a) draw yi ? Bernoulli(?)
(b) draw zi ? Multinomial(?ui) if yi = 0 or
zi ? Multinomial(?ti) if yi = 1
(c) for each word j = 1, . . . , Ni
i. draw xi,j ? Bernoulli(?)
ii. draw wi,j ? Multinomial(?B) if
xi,j = 0 or wi,j ? Multinomial(?zi)
if xi,j = 1
Figure 2: The generation process for all posts.
We use ? to denote the probability of choosing to
talk about a global topic rather than a personal topic.
Formally, the generation process is summarized in
Figure 2. The model is also depicted in Figure 1(a).
There are two degenerate variations of our model
that we also consider in our experiments. The first
one is depicted in Figure 1(b). In this model, we only
consider the time-dependent topic distributions that
capture the global topical trends. This model can be
seen as a direct application of the model by Wang
et al (2007). The second one is depicted in Fig-
ure 1(c). In this model, we only consider the users?
personal interests but not the global topical trends,
and therefore temporal information is not used. We
refer to our complete model as TimeUserLDA, the
model in Figure 1(b) as TimeLDA and the model in
Figure 1(c) asUserLDA. We also consider a standard
LDA model in our experiments, where each word is
associated with a hidden topic.
Learning
We use collapsed Gibbs sampling to obtain sam-
ples of the hidden variable assignment and to esti-
mate the model parameters from these samples. Due
to space limit, we only show the derived Gibbs sam-
pling formulas as follows.
First, for the i-th post, we know its publisher ui
and timestamp ti. We can jointly sample yi and zi
based on the values of all other hidden variables. Let
us use y to denote the set of all hidden variables y
and y?i to denote all y except yi. We use similar
symbols for other variables. We then have
p(yi = p, zi = c|z?i,y?i,x,w) ?
Mpi(p) + ?
Mpi(?) + 2?
?
M l(c) + ?
M l(?) + C?
?
?V
v=1
?E(v)?1
k=0 (M c(v) + k + ?)
?E(?)?1
k=0 (M c(?) + k + V ?)
, (1)
where l = ui when p = 0 and l = ti when p =
1. Here every M is a counter. Mpi(0) is the number
of posts generated by personal interests, while Mpi(1)
is the number of posts coming from global topical
trends. Mpi(?) = M
pi
0 + Mpi1 . M
ui
(c) is the number of
posts by user ui and assigned to topic c, and Mui(?) is
the total number of posts by ui. M ti(c) is the number
of posts assigned to topic c at time point ti, and M ti(?)
is the total number of posts at ti. E(v) is the number
of times word v occurs in the i-th post and is labeled
as a topic word, while E(?) is the total number of
topic words in the i-th post. Here, topic words refer
to words whose latent variable x equals 1. M c(v) is
the number of times word v is assigned to topic c,
and M c(?) is the total number of words assigned to
topic c. All the counters M mentioned above are
calculated with the i-th post excluded.
We sample xi,j for each word wi,j in the i-th post
using
p(xi,j = q|y, z,x?{i,j},w)
?
M?(q) + ?
M?(?) + 2?
?
M l(wi,j) + ?
M l(?) + V ?
, (2)
where l = B when q = 0 and l = zi when q = 1.
M?(0) and M
?
(1) are counters to record the numbers
of words assigned to the background model and any
topic, respectively, andM?(?) = M
?
(0)+M
?
(1). M
B
(wi,j)
is the number of times word wi,j occurs as a back-
ground word. M zi(wi,j) counts the number of times
word wi,j is assigned to topic zi, and M zi(?) is the to-
tal number of words assigned to topic zi. Again, all
counters are calculated with the current word wi,j
excluded.
539
Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical
trends. (c) A variation of our model where we only consider users? personal topical interests.
3.3 Burst Detection
Just like standard LDA, our topic model itself finds a
set of topics represented by ?c but does not directly
generate bursty topics. To identify bursty topics, we
use the following mechanism, which is based on the
idea by Kleinberg (2002) and Ihler et al (2006). In
our experiments, when we compare different mod-
els, we also use the same burst detection mechanism
for other models.
We assume that after topic modeling, for each dis-
covered topic c, we can obtain a series of counts
(mc1,mc2, . . . ,mcT ) representing the intensity of the
topic at different time points. For LDA, these
are the numbers of words assigned to topic c.
For TimeUserLDA, these are the numbers of posts
which are in topic c and generated by the global top-
ic distribution ?ti , i.e whose hidden variable yi is 1.
For other models, these are the numbers of posts in
topic c.
We assume that these counts are generated by two
Poisson distributions corresponding to a bursty state
and a normal state, respectively. Let ?0 denote the
expected count for the normal state and ?1 for the
bursty state. Let vt denote the state for time point t,
where vt = 0 indicates the normal state and vt = 1
indicates the bursty state. The probability of observ-
ing a count of mct is as follows:
p(mct |vt = l) =
e??l?m
c
t
l
mct !
,
where l is either 0 or 1. The state sequence
(v0, v1, . . . , vT ) is a Markov chain with the follow-
ing transition probabilities:
p(vt = l|vt?1 = l) = ?l,
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.750 N/A
TimeLDA 0.800 0.700 0.600 0.633
UserLDA 0.800 0.700 0.850 0.833
TimeUserLDA 1.000 1.000 0.900 0.800
Table 1: Precision at K for the various models.
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.700 N/A
TimeLDA 0.400 0.500 0.500 0.567
UserLDA 0.800 0.500 0.500 0.600
TimeUserLDA 1.000 0.900 0.850 0.767
Table 2: Precision at K for the various models after we
remove redundant bursty topics.
where l is either 0 or 1.
?0 and ?1 are topic specific. In our experiments,
we set ?0 = 1T
?
t mct , that is, ?0 is the average
count over time. We set ?1 = 3?0. For transition
probabilities, we empirically set ?0 = 0.9 and ?1 =
0.6 for all topics.
We can use dynamic programming to uncover the
underlying state sequence for a series of counts. Fi-
nally, a burst is marked by a consecutive subse-
quence of bursty states.
4 Experiments
4.1 Data Set
We use a Twitter data set to evaluate our models.
The original data set contains 151,055 Twitter users
based in Singapore and their tweets. These Twitter
users were obtained by starting from a set of seed
Singapore users who are active online and tracing
540
Bursty Period Top Words Example Tweets Label
Nov 29 vote, big, awards, (1) why didnt 2ne1 win this time! Mnet Asian
bang, mama, win, (2) 2ne1. you deserved that urgh! Music Awards
2ne1, award, won (3) watching mama. whoohoo (MAMA)
Oct 5 ? Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away! Steve Jobs
iphone, rip, world, (2) google founders: steve jobs was an inspiration! death
changed, 4s, siri (3) apple 4 life thankyousteve
Nov 1 ? Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me. slap her mum? girl slapping
slap, found, body, queen of cine? joke please can. mom
mom, singapore, steven (2) she slapped her mum and boasted about it on fb
(3) adelyn lives in woodlands , later she slap me how?
Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed. suicide near
slap, found, body, (2) another body found, in bedok reservoir? bedok reservoir
mom, singapore, steven (3) so many bodies found at bedok reservoir. alamak.
Oct 23 man, arsenal, united, (1) damn you man city! we will get you next time! football game
liverpool, chelsea, city, (2) wtf 90min goal!
goal, game, match (3) 6-1 to city. unbelievable.
Table 3: Top-5 bursty topics ranked by TimeUserLDA. The labels are manually given. The 3rd and the 4th bursty
topics come from the same topic but have different bursty periods.
Rank LDA UserLDA TimeLDA
1 Steve Jobs? death MAMA MAMA
2 MAMA football game MAMA
3 N/A #zamanprimaryschool MAMA
4 girl slapping mom N/A girl slapping mom
5 N/A iphone 4s N/A
Table 4: Top-5 bursty topics ranked by other models. N/A indicates a meaningless burst.
their follower/followee links by two hops. Because
this data set is huge, we randomly sampled 2892
users from this data set and extracted their tweets
between September 1 and November 30, 2011 (91
days in total). We use one day as our time window.
Therefore our timestamps range from 1 to 91. We
then removed stop words and words containing non-
standard characters. Tweets containing less than 3
words were also discarded. After preprocessing, we
obtained the final data set with 3,967,927 tweets and
24,280,638 tokens.
4.2 Ground Truth Generation
To compare our model with other alternative models,
we perform both quantitative and qualitative evalua-
tion. As we have explained in Section 3, each mod-
el gives us time series data for a number of topics,
and by applying a Poisson-based state machine, we
can obtain a set of bursty topics. For each method,
we rank the obtained bursty topics by the number
of tweets (or words in the case of the LDA model)
assigned to the topics and take the top-30 bursty top-
ics from each model. In the case of the LDA mod-
el, only 23 bursty topics were detected. We merged
these topics and asked two human judges to judge
their quality by assigning a score of either 0 or 1.
The judges are graduate students living in Singapore
and not involved in this project. The judges were
given the bursty period and 100 randomly selected
tweets for the given topic within that period for each
bursty topic. They can consult external resources to
help make judgment. A bursty topic was scored 1
if the 100 tweets coherently describe a bursty even-
t based on the human judge?s understanding. The
inter-annotator agreement score is 0.649 using Co-
hen?s kappa, showing substantial agreement. For
ground truth, we consider a bursty topic to be cor-
rect if both human judges have scored it 1. Since
some models gave redundant bursty topics, we al-
so asked one of the judges to identify unique bursty
541
topics from the ground truth bursty topics.
4.3 Evaluation
In this section, we show the quantitative evalua-
tion of the four models we consider, namely, LDA,
TimeLDA, UserLDA and TimeUserLDA. For each
model, we set the number of topics C to 80, ? to 50C
and ? to 0.01 after some preliminary experiments.
Each model was run for 500 iterations of Gibbs sam-
pling. We take 40 samples with a gap of 5 iterations
in the last 200 iterations to help us assign values to
all the hidden variables.
Table 1 shows the comparison between these
models in terms of the precision of the top-K result-
s. As we can see, our model outperforms all other
models for K <= 20. For K = 30, the UserLDA
model performs the best followed by our model.
As we have pointed out, some of the bursty topics
are redundant, i.e. they are about the same bursty
event. We therefore also calculated precision at K
for unique topics, where for redundant topics the one
ranked the highest is scored 1 and the other ones
are scored 0. The comparison of the performance
is shown in Table 2. As we can see, in this case,
our model outperforms other models with all K. We
will further discuss redundant bursty topics in the
next section.
4.4 Sample Results and Discussions
In this section, we show some sample results from
our experiments and discuss some case studies that
illustrate the advantages of our model.
First, we show the top-5 bursty topics discovered
by the TimeUserLDA model in Table 3. As we can
see, all these bursty topics are meaningful. Some of
these events are global major events such as Steve
Jobs? death, while some others are related to online
events such as the scandal of a girl boasting about
slapping her mother on Facebook. For comparison,
we also show the top-5 bursty topics discovered by
other models in Table 4. As we can see, some of
them are not meaningful events while some of them
are redundant.
Next, we show two case studies to demonstrate
the effectiveness of our model.
Effectiveness of Temporal Models: Both
TimeLDA and TimeUserLDA tend to group posts
published on the same day into the same topic. We
find that this can help separate bursty topics from
general ones. An example is the topic on the Circle
Line. The Circle Line is one of the subway lines of
Singapore?s mass transit system. There were a few
incidents of delays or breakdowns during the period
between September and November, 2011. We show
the time series data of the topic related to the Circle
Line of UserLDA, TimeLDA and TimeUserLDA in
Figure 3. As we can see, the UserLDA model de-
tects a much large volume of tweets related to this
topic. A close inspection tells us that the topic under
UserLDA is actually related to the subway systems
in Singapore in general, which include a few other
subway lines, and the Circle Line topic is merged
with this general topic. On the other hand, TimeL-
DA and TimeUserLDA are both able to separate the
Circle Line topic from the general subway topic be-
cause the Circle Line has several bursts. What is
shown in Figure 3 for TimeLDA and TimeUserLDA
is only the topic on the Circle Line, therefore the
volume is much smaller. We can see that TimeLDA
and TimeUserLDA show clearer bursty patterns than
UserLDA for this topic. The bursts around day 20,
day 44 and day 85 are all real events based on our
ground truth.
Effectiveness of User Models: We have stat-
ed that it is important to filter out users? ?person-
al? posts in order to find meaningful global events.
We find that our results also support this hypothesis.
Let us look at the example of the topic on the Mnet
Asian Music Awards, which is a major music award
show that is held by Mnet Media annually. In 2011,
this event took place in Singapore on November 29.
Because Korean pop music is very popular in Singa-
pore, many Twitter users often tweet about Korean
pop music bands and singers in general. All our top-
ic models give multiple topics related to Korean pop
music, and many of them have a burst on Novem-
ber 29, 2011. Under the TimeLDA and UserLDA
models, this leads to several redundant bursty top-
ics for the MAMA event ranked within the top-30.
For TimeUserLDA, however, although the MAMA
event is also ranked the top, there is no redundan-
t one within the top-30 results. We find that this is
because with TimeUserLDA, we can remove tweet-
s that are considered personal and therefore do not
contribute to bursty topic ranking. We show the top-
ic intensity of a topic about a Korean pop singer in
542
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 3: Topic intensity over time for the topic on the Circle Line.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 4: Topic intensity over time for the topic about a Korean pop singer. The dotted curves show the topic on Steve
Jobs? death.
Figure 4. For reference, we also show the intensity
of the topic on Steve Jobs? death under each mod-
el. We can see that because this topic is related to
Korean pop music, it has a burst on day 90 (Novem-
ber 29). But if we consider the relative intensity of
this burst compared with Steve Jobs? death, under
TimeLDA and UserLDA, this topic is still strong but
under TimeUserLDA its intensity can almost be ig-
nored. This is why with TimeLDA and UserLDA
this topic leads to a redundant burst within the top-
30 results but with TimeUserLDA the burst is not
ranked high.
5 Conclusions
In this paper, we studied the problem of finding
bursty topics from the text streams on microblogs.
Because existing work on burst detection from tex-
t streams may not be suitable for microblogs, we
proposed a new topic model that considers both the
temporal information of microblog posts and user-
s? personal interests. We then applied a Poisson-
based state machine to identify bursty periods from
the topics discovered by our model. We compared
our model with standard LDA as well as two de-
generate variations of our model on a real Twitter
dataset. Our quantitative evaluation showed that our
model could more accurately detect unique bursty
topics among the top ranked results. We also used
two case studies to illustrate the effectiveness of the
temporal factor and the user factor of our model.
Our method currently can only detect bursty top-
ics in a retrospective and offline manner. A more in-
teresting and useful task is to detect realtime bursts
in an online fashion. This is one of the directions we
plan to study in the future. Another limitation of the
current method is that the number of topics is pre-
determined. We also plan to look into methods that
allow appearance and disappearance of topics along
the timeline, such as the model by Ahmed and Xing
(2010).
Acknowledgments
This research is supported by the Singapore Nation-
al Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
543
restaurant process: with applications to evolutionary
clustering. In Proceedings of the SIAM International
Conference on Data Mining, pages 219?230.
Amr Ahmed and Eric P. Xing. 2010. Timeline: A dy-
namic hierarchical Dirichlet process model for recov-
ering birth/death and evolution of topics in text stream.
In Proceedings of the 26th Conference on Uncertainty
in Artificial Intelligence, pages 20?29.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd International
Conference on Machine Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In Proceedings of the 31st
International Conference on Very Large Data Bases,
pages 181?192.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov model. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent top-
ic model for multiple text streams. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 832?
840.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 207?216.
Jon Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proceedings of the 8th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 91?101.
Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu,
Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi
Oguri. 2009. Dynamic hyperparameter optimization
for bayesian topical trend analysis. In Proceedings of
the 18th ACM Conference on Information and knowl-
edge management, pages 1831?1834.
Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty,
and Kin Ung. 2007. Multiscale topic tomography. In
Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 520?529.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, pages
487?494.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 424?433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pattern-
s from coordinated text streams. In Proceedings of
the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 784?
793.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 579?586.
Jianshu Weng and Francis Lee. 2011. Event detection in
Twitter. In Proceedings of the 5th International AAAI
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Proceedings of the 33rd European confer-
ence on Advances in information retrieval, pages 338?
349.
544
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 33?41,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
A Unified Topic-Style Model for Online Discussions
Ying Ding, Jing Jiang, Qiming Diao
School of Information Systems
Singapore Management University
{ying.ding.2011, jingjiang, qiming.diao.2010}@smu.edu.sg
Abstract
Forums have become major places for
online communications for many years,
where people often share and express
opinions. We observe that, when editing
posts, while some people seriously state
their opinions, there are also many peo-
ple playing jokes and writing meaningless
posts on the discussed topics. We design
a unified probabilistic graphical model to
capture both topic-driven words and style-
driven words. The model can help us sepa-
rate serious and unserious posts/users and
identify slang words. An extensive set
of experiments demonstrates the effective-
ness of our model.
1 Introduction
With the fast growth of the popularity of online
social media, people nowadays are very used to
sharing their thoughts and interacting with their
friends on the Internet. Large online social net-
work sites such as Facebook, Twitter and Flickr
have attracted hundreds of millions of users. A-
mong these online social media platforms, forums
have always played an important role with its spe-
cial characteristics. Unlike personal blogs, forum-
s allow many users to engage in online conversa-
tions with a topic focus. Unlike Facebook, forums
are usually open to public and users who post in
forums do not need to reveal too much personal
information. Unlike Wikipedia or Freebase, fo-
rums encourage users to exchange not only factual
information but more importantly subjective opin-
ions. All these characteristics make online forums
a valuable source from which we can retrieve and
summarize the general public?s opinions about a
given topic. This is especially important for busi-
nesses who want to find out how their products
and services have been received and policy mak-
ers who are concerned about people?s opinions on
social issues.
While the freedom with which users can post in
online forums has promoted the popularity of on-
line forums, it has also led to the diversity in post
quality. There are posts which contribute positive-
ly to a discussion by offering relevant, serious and
meaningful opinions, but there are also many posts
which appear irrelevant, disrespectful or meaning-
less. These posts are uninformative, hard to con-
sume and sometimes even destructive. Let us look
at some examples. Table 1 shows two forum posts
in response to a piece of news about GDP bonuses
for senior civil servants in Singapore. We can see
that User A?s post is clearly written. User B?s post,
on the other hand, is hard to comprehend. We see
broken sentences, many punctuation marks such
as ??? and colloquial expressions such as ?ha.?
User B is not seriously contributing to the online
discussion but rather trying to make a joke of the
issue. Generally speaking, User B?s post is less
useful than User A?s post in helping us understand
the public?s response to the news.
Senior civil servants to get bumper
GDP bonuses
User A let us ensure this will be the LAST time
they accord themselves ceiling salary s-
cales and bonuses. i suspect MANY cit-
izens are eagerly looking forward to the
GE.
User B Fever night, fever night, fe..ver..
Fever like to do it
Got it?????? Ha..ha..ha...
Table 1: Two example online posts.
In this work, we opt for a fully unsupervised
approach to modeling this phenomenon in online
discussions. Our solution is based on the observa-
tion that the writing styles of serious posts and un-
serious posts are different, and the writing styles
are often characterized by the words used in the
posts. Moreover, the same user usually exhibits
33
User Post
User A
Re: Creativity, Art in the eyes of beholder. your
take?
The difference is, the human can get tired or
sick, and then it will affect his work, but the
robot can work 24 hours a day 365 days a year
and yet produce the same every time.
Re: Diesel oil spill turns Manila Bay red, poses
risk to health - ST
The question is, will this environmental haz-
ard turn up on the shores of it neighbors? And
maybe even affect Singapore waters?
User B
Re: Will PAP know who i vote in GE?
Hey! Who are you???
You make. ha..ha..ha.. he..he..he..
very angry lah
Re: Gender discrimination must end for Singa-
pore to flourish, says AWARE
Hao nan bu gen nu dou Let you win lah
ha..ha..ha..
Table 2: Sample posts of two example users.
the same writing style in most of his posts. For
example, Table 2 shows two example users, each
with two sample posts. We can see that their writ-
ing styles are consistent in the two posts. If we
treat each writing style as a latent factor associat-
ed with a word distribution, we can associate ob-
served words with the underlying writing styles.
However, not all words in a post are style-driven.
Many words in forum posts are chosen based on
the topic of the corresponding thread. Our model
therefore jointly considers both topics and writing
styles.
We apply our topic-style model to a real on-
line forum dataset from Singapore. By setting the
number of styles to two, we clearly find that one
writing style corresponds to the more serious posts
while the other corresponds to posts that are not so
serious. This topic-style model also automatically
learns a meaningful slang lexicon. Moreover, we
find that topics discovered by our topic-style mod-
el are more distinctive from each other than topics
produced by standard LDA.
Our contributions in this paper can be summa-
rized as follows: 1) We propose a principled topic-
style model to jointly model topics and writing
styles at the same time in online forums. 2) An
extensive set of experiments shows that our mod-
el is effective in separating the more serious posts
and unserious posts and identifying slang words.
2 Related Work
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) has been shown to be useful for many ap-
plications. Many extensions of LDA have been
designed for different tasks, which are not detailed
here. Our model is also an extension of LDA. We
introduce two types of word distributions, one rep-
resenting topics and the other representing writing
styles. We use switch variables to alternate be-
tween these two types of word distributions. We
also assume an author-level distribution over writ-
ing styles. It is worth pointing out that although
our model bears similarity to a number of oth-
er LDA extensions, our objectives are different
from existing work. E.g., the author topic mod-
el (Rosen-Zvi et al., 2004) also assumes an author-
level distribution over topics, but the author-level
distribution is meant to capture an author?s topical
interests. In contrast, our user-level distribution is
over writing styles and is meant to identify serious
versus unserious users. Similar to the models by
Mei et al. (2007) and Paul et al. (2010) , we also
use switch variables to alternate between different
types of word distributions, but our goal is to iden-
tify words associated with writing styles instead of
sentiment words or perspective words.
Another body of related research is around s-
tudying text quality, formality and sarcasm. Pitler
and Nenkova (2008) investigated different fea-
tures for text readability judgement and empirical-
ly demonstrated that discourse relations are high-
ly correlated with perceived readability. Brooke
et al. (2010) applied Latent Semantic Analysis
to determine the formality level of lexical items.
Agichtein et al. (2008) presented a general classifi-
cation framework incorporating community feed-
back to identify high quality content in social me-
dia. Davidov et al. (2010) proposed the first robust
algorithm for recognition of sarcasm. Gonz?alez-
Ib?a?nez et al. (2011) took a closer look at sarcasm
in Twitter messages and found that automatic clas-
sification can be as good as human classification.
All these studies mainly rely on supervised tech-
niques and human annotation needs to be done,
which is very time consuming. Our method is ful-
ly unsupervised, which can automatically uncover
different styles and separate serious posts from un-
serious posts.
Our work is also related to spam/spammer de-
tection in social media, which has been studied
over different platforms for a few years. Jindal
and Liu (2008) first studied opinion spam in on-
line reviews and proposed a classification method
for opinion spam detection. Bhattarai et al. (2009)
34
investigated different content attributes of com-
ment spam in the Blogsphere and built a detection
system with good performance based on these at-
tributes. Ding et al. (2013) proposed to utilize both
content and social features to detect spams in on-
line question answer website. Existing work on
spam detection need annotated data to learn the s-
pam features but our model does not as it is fully
unsupervised.
3 A Topic-Style Model
Writing styles can be reflected in many differen-
t ways. Besides choices of words or expression-
s, many other linguistic features such as sentence
length, sentence complexity and use of punctua-
tion marks may all be associated with one?s writ-
ing style. In this work, however, we try to take
an approach that does not rely on heavy linguistic
analysis or feature engineering. Part of the reason
is that we want our approach to be independent of
language, culture or social norms so that it is ro-
bust and can be easily applied to any online forum.
To this end, we represent a writing style simply
as a distribution over words, much like a topic in
LDA. We assume that there are S latent writing
styles shared by all users contributing to a forum.
Meanwhile, we also assume a different set of T
latent topics. We mix writing styles and topics to
explain the generation of words in forum posts.
A key assumption we have is that the same us-
er tends to maintain a consistent writing style, and
therefore we associate each user with a multinomi-
al distribution over our latent writing styles. This
is similar to associating a document with a distri-
bution over topics in LDA, where the assumption
is that a single document tends to have focused
topics. Another assumption of our model is that
each word in a post is generated from either the
background or a topic or a writing style, as deter-
mined by a binary switch variable.
3.1 Model Description
We now formally describe the topic-style model
we propose. The model is depicted in Figure 1.
We assume that there are T latent topics, where
?
t
is the word distribution for topic t. There are
S latent writing styles, where ?
s
is the word dis-
tribution for writing style s. There are E threads,
where each thread e has a topic distribution ?
e
, and
there are U users, where each user u has a writing
style distribution pi
u
.
Figure 1: Topic-Style Model
Notation Description
?, ?
E
, ?
U
,
?
B
, ?
T
, ?
S
Hyper-parameters of Dirichlet distributions
? A global multinomial distribution over
switching variables x
?
e
, pi
u
Thread-specific topic distributions and user-
specific style distributions
?
B
, ?
t
, ?
s
Word distributions of background, topics
and styles
x
e,p,n
,
y
e,p,n
,
z
e,p,n
Hidden variables: x
e,p,n
for switching,
y
e,p,n
for style of style words, z
e,p,n
for
topic of topic words
e, p, n Indices: e for threads, p for posts, n for
words
E,P
e
, U,
N
e,p
Number of threads, numbers of posts in
threads, number of users and numbers of
words in posts
S,K, V Numbers of styles, topics and word types
Table 3: Notation used in our model.
For each word in a post, first a binary switch
variable x is sampled from a global Bernoulli dis-
tribution parameterized by ?. If x = 0, we draw a
word from the background word distribution. Oth-
erwise, if x = 1, we draw a topic from the corre-
sponding thread?s topic distribution; if x = 2, we
draw a writing style from the corresponding user?s
writing style distribution. We then draw the word
from the corresponding word distribution.
The generative process of our model is de-
scribed as follows. The notation we use in the
model is also summarized in Table 3.
? Draw a global multinomial switching variable distribu-
tion ? ? Dirichlet(?).
? Draw a multinomial background word distribution
?
B
? Dirichlet(?
B
).
? For each topic t = 1, 2, . . . , T , draw a multinomial
topic-word distribution ?
t
? Dirichlet(?
T
).
? For each writing style s = 1, 2, . . . , S, draw a multi-
nomial style-word distribution ?
s
? Dirichlet(?
S
).
? For each user u = 1, 2, . . . , U , draw a multinomial
style distribution pi
u
? Dirichlet(?
u
).
? For each thread e = 1, 2, . . . , E
35
? draw a multinomial topic distribution ?
e
?
Dir(?
E
).
? for each post p = 1, 2, . . . , P
e
in the thread,
where u
e,p
? {1, 2, . . . , U} is the user who has
written the post
? for each word n = 1, 2, . . . , N
e,p
in the
thread, where w
e,p,n
? {1, 2, . . . , V } is the
word type
? draw x
e,p,n
? Multinomial(?).
? If x = 0, draw w
e,p,n
?
Multinomial(?
B
)
? If x = 1, draw y
e,p,n
?
Multinomial(pi
u
e,p
), and then draw
w
e,p,n
? Multinomial(?
y
e,p,n
).
? If x = 2, draw z
e,p,n
?
Multinomial(?
e
), and then draw
w
e,p,n
? Multinomial(?
z
e,p,n
).
3.2 Parameters Estimation
We use Gibbs sampling to estimate the parameters.
The sampling probability that assign the nth word
in post p of thread e to the background topic is as
follows:
P (x
e,p,n
= 0|W ,U ,X?i,Y ?i,Z?i)
?(? + n
0
)?
?
B
+ n
w
e,p,n
B
V ?
B
+ n
0
where n
0
is the number of words assigned as back-
ground words and n
w
e,p,n
B
is the number of times
word type of w
e,p,n
assigned to background. The
probability to assign this word to style s is as fol-
lows:
P (x
e,p,n
= 1, y
e,p,n
= s|W ,U ,X?i,Y ?i,Z?i)
?(? + n
1
)?
?
U
+ n
s
u
e,p
S?
U
+ n
?
u
e,p
?
?
S
+ n
w
e,p,n
s
V ?
S
+ n
?
s
where n
1
is the number of words assigned as style
words, n
?
u
e,p
and n
s
u
e,p
are the number of words
written by user u
e,p
and assigned as style words,
and the number of these words assigned to style
s, respectively. n
?
s
and n
w
e,p,n
s
are the number of
words assigned to style s and the number of times
word type of term w
e,p,n
assigned to style s. The
probability to assign this word topic t is as follows:
P (x
e,p,n
= 2, z
e,p,n
= t|W ,U ,X?i,Y ?i,Z?i)
?(? + n
2
)?
?
E
+ n
t
e
K?
E
+ n
?
e
?
?
T
+ n
w
e,p,n
t
V ?
T
+ n
?
t
where n
2
is the number of words assigned as topic
words, n
?
e
is the number of words in thread e as-
signed as topic words, n
t
e
is the number of words
in thread e assigned to topic t, n
?
t
is the number of
words assigned to topic t, and n
w
e,p,n
t
is the num-
ber of times word type of w
e,p,n
is assigned to top-
ic t.
After running Gibbs sampling for a number of
iterations, we can estimate the parameters based
on the sampled topic assignments. They can be
calculated by the equations below:
?
w
t
=
?
T
+ n
w
t
V ?
T
+ n
?
t
?
w
s
=
?
S
+ n
w
s
V ?
S
+ n
?
s
?
t
e
=
?
E
+ n
t
e
K?
E
+ n
?
e
?
s
u
=
?
U
+ n
s
u
S?
U
+ n
?
u
4 Experiment
4.1 Data Set and Experiment Setup
To evaluate our model, we use forum threads from
AsiaOne
1
, a popular online forum site in Singa-
pore. We crawled all the threads between January
2011 and June 2013 under a category called ?Sin-
gapore,? which is the largest category on AsiaOne.
In the preprocessing stage, we removed the URL-
s, HTML tags and tokenized the text. Emoticons
are kept in our data set as they frequently occur
and indicate users? emotions. All stop words and
words occurring less than 4 times are deleted. We
also removed users who have fewer than 8 post-
s and threads attracting fewer than 21 posts. The
detailed statistics of the processed dataset are giv-
en in Table 4.
#Users #Words #Tokens #Posts/User #Posts/Thread
580 29,619 2,940,886 205.3 69.5
Table 4: Detailed statistics of the dataset.
We fix the hyper-parameters ?, ?
E
, ?
U
, ?
T
and
?
S
to be 10, 1, 1, 0.01 and 0.01 respectively. we
set ?
B,v
to be H ? p
B
(v), where H is set to be 20
and p
B
(v) is the probability of word v as estimated
from the entire corpus. The number of topics K is
set to be 40 empirically.
4.2 Model Development
Before we evaluate the effectiveness of our model,
we first show how we choose the number of styles
to use. Note that although we are interested in sep-
arating serious and unserious posts, our model can
generally handle any arbitrary number of writing
styles. We therefore vary the number of writing
styles to see which number empirically gives the
most meaningful results.
Assuming that different styles are characterized
by words, we expect to see that the discovered
1
http://www.asiaone.com
36
2 3 4 5 6 7 8 912
13
14
15
16
17
18
19
Number of Styles
Aver
age 
Dive
rgen
ce
Figure 2: Average Divergence over different num-
bers of styles.
Style No. Top Words
2 Style 1 singapore, people, years, government
styles Style 2 BIGGRIN, TONGUE, lah, ha
3 Style 1 people, make, WINK, good
styles Style 2 singapore, years, government, mr
Style 3 BIGGRIN, TONGUE, lah, ha
4 Style 1 ha, lah, WINK, dont
styles Style 2 singapore, year, mr, years
Style 3 people, good, make, singapore
Style 4 BIGGRIN, TONGUE, EEK, MAD
Table 5: Sample style words
word distributions for different styles are very d-
ifferent from each other. To measure the distinc-
tion among a set of styles, we define a metric
called Average Divergence (AD) based on KL-
divergence. Average Divergence can be calculated
as follows.
AD(S) =
2
N(N ? 1)
?
i6=j
S
KL
(s
i
||s
j
),
where S is a set of style-word distributions, N is
the size of S and s
i
is the i-th distribution in S.
S
KL
(s
i
||s
j
) is the symmetric KL divergence be-
tween s
i
and s
j
(i.e., D
KL
(s
i
||s
j
) +D
KL
(s
j
||s
i
)).
The higher Average Divergence is, the more dis-
tinctive distributions in S are.
Figure 2 shows the Average Divergence over d-
ifferent numbers of styles. We can clearly see that
the Average Divergence reaches the highest value
when there are only two styles and decreases with
the increase of style number. This means the styles
are mostly distinct from each other when the num-
ber is 2 and their difference decreases when there
are more styles.
To get a better understanding of the differences
of using different numbers of styles, we compare
the top words in each style when the number of
styles is set to be 2, 3 and 4. The results are shown
in Table 5 where all uppercase words represent e-
moticons. From the top words of the first row, we
Serious Unserious
Style Style
singapore lah
people ha
years dont
government stupid
time leh
made ah
year lor
public liao
Table 6: Top words of different styles
can see that Style 1 is dominated by formal words
while Style 2 is dominated by emoticons like BIG-
GRIN and slang words like ?lah? and ?ha.? These
two styles are well distinguished from each other
and humans can easily tell the difference between
them. Also, Style 2 is an unserious style character-
ized by emoticons, slang and urban words. Table 6
shows the top words of these 2 styles excluding e-
moticons. From this table, we can observe that
Style 2 has many slang words with high probabil-
ity while top words in Style 1 are all very formal.
However, styles in the second and third rows of
Table 5 are not easily distinguishable from each
other. In these results, there often exist two styles
very similar to the styles in row 1 while the other
styles look like the combination of these two styles
and humans cannot tell their meanings very clear-
ly. Based on these observations, we fix the number
of styles to 2 in the following experiments.
0 2 4 6 8 10 12 14 16 180
0.05
0.1
0.15
0.2
0.25
Word Length
Prob
abilit
y
 
 Style 1Style 2
Figure 3: Word length distribution
One previous work uses word length as an in-
dicator of formality (Karlgren and Cutting, 1994).
Here, we borrow this idea and compare the word
length of Style 1 and Style 2. We calculate the
distributions of word length and show the results
in Figure 3. It shows that the majority of word-
s in Style 1 are longer compared with those in
Style 2. To have a quantitative view of the differ-
ence between the word lengths of these two styles,
we heuristically extract words labeled with Style 1
37
and Style 2 in our dataset in the final iteration of
Gibbs sampling and apply Mann-Whitney U test
on these two word length populations. The null
hypothesis that the two input populations are the
same is rejected at the 1% significance level. This
verifies the intuition that serious posts tend to use
longer words than unserious posts.
4.3 Post Identification
Our model can also be used to separate serious
posts and unserious posts. We treat this as a re-
trieval problem and use precision/recall for evalu-
ation.
We use a simple scoring function, which is the
proportion of words assigned to the unserious style
when we terminate the Gibbs sampling at the 800-
th iteration, to score each post. When applying this
method to our data, emoticons are all removed.
For comparison, we rank post according to the
number of emoticons inside a post as the baseline.
After getting the result of each method, we ask t-
wo annotators to label the first and last 50 posts
in the ranking list. The first 50 posts are used for
evaluation of unserious post retrieval and the last
50 post are used for evaluation of serious post re-
trieval. This evaluation is based on the assumption
that if a method can separate serious and unserious
posts very well, posts ranked at the top position
should be unserious ones and those ranked near
to the bottom should be serious ones. The results
are shown in Table 7 where our method is denot-
ed as TSM and the baseline method is denoted as
EMO. In serious post retrieval, the baseline have
a perfect performance and our method is compet-
itive. We can see that EMO has a perfect perfor-
mance in identifying serious posts. When posts
are ranked in reverse order according to the num-
ber of emoticons they contain, the last 50 ones do
not contain any emoticons. They can be regarded
as a random sample of posts without emoticons.
Compared with identifying serious posts, identi-
fying unserious posts looks much more difficult.
EMO?s poor performance on this task tells us that
emoticon is not a promising sign to detect unse-
rious posts. However, the word style a post uses
matters more, which also proves the value of our
proposed model.
4.4 User Identification
In this section, we evaluate the performance of
TSM on identifying serious and unserious users.
This identification task is very important as many
P@5 P@15 P@25 P@35
Serious
EMO 1.0 1.0 1.0 1.0
TSM 1.0 1.0 1.0 0.97
Unserious
EMO 0.4 0.67 0.64 0.6
TSM 1.0 0.93 0.96 0.97
Table 7: Precision for Serious and Unserious Post
Retrieval. P@N stands for the precision of the first
N results in ranking list.
P@5 P@15 P@25 P@35
Serious
Baseline 0.6 0.8 0.8 0.83
TSM 1.0 1.0 1.0 0.94
Unserious
Baseline 1.0 0.87 0.92 0.91
TSM 1.0 1.0 1.0 1.0
Table 8: Precision for serious and unserious user
retrieval.
research tasks such as opinion mining and expert
finding are more interested in the serious users.
We treat this task as a retrieval problem as well,
which means we will rank users by a scoring func-
tion and do evaluation on this ranking result.
We rank user according to their style distribu-
tion pi
u
and pick the first 50 and last 50 users for
evaluation. For each user, 10 posts are sampled
to be shown to the annotators. We mix these 100
users and ask two graduate students to do the an-
notations. The evaluation strategy is the same as
that in Section 4.3. We choose a simple base-
line which ranks users by the number of emoticons
they use per post. The evaluation result is shown
in Table 8 for serious and unserious user retrieval
respectively.
In both serious and unserious user retrieval
tasks, our method gets almost perfect perfor-
mance, which is better than the baseline. This
means the user style distributions learned by our
model can help separate serious and unserious
users.
4.5 Perplexity
Perplexity is a widely used criterion in statistical
natural language processing. It measures the pre-
dictive power of a model on unseen data, which
is algebraically equivalent to the inverse of the ge-
ometric mean per-word likelihood. A lower per-
plexity means the test data, which is unseen in the
training phase, can be generated by the model with
a higher probability. So it also indicates that the
model has a better generalization performance.
In this experiment, we leave 10% data for test-
ing and use the remaining 90% data for training.
We choose LDA as a baseline for comparison and
38
treat each thread as a document. The perplexity for
both models is calculated over different numbers
of topics, which ranges from 10 to 100. The result
is show in Figure 4. We can clearly see that our
proposed model has a substantially lower perplexi-
ty than LDA over different numbers of topics. This
proves that our model fits the forum discussion da-
ta better and has a stronger generalization power.
It also indicates that separating topic-driven words
and style-driven words can better fit the generation
of user generated content in forum discussions.
4.6 Topic Distinction
In traditional topic modeling, like LDA, all words
are regarded as topic-driven words, which are gen-
erated by mixture of topics. However, this may not
be true to user-generated content in online forum-
s as not all words are driven by discussed topics.
Take the following post for example:
? Okay lah. Let them be. I mean its their KKB
right? Let it rot lor.
In this post, the words ?lah? and ?lor? are not relat-
ed to the topics under discussion. They appear in
the post because the authors are used to using these
words, which means these words are style driven.
Style-driven words are related to a user?s charac-
teristics and should not be clustered into any top-
ic. Without separating these two types of words,
style-driven words may appear in different topics
and make topics less distinct to each other.
Figure 5 compares the Average Divergence
among discovered topics between TSM (Topic
Style model) and LDA over different numbers of
topics. We can clearly see that the Average Diver-
gence of TSM is substantially larger than that of L-
DA over different numbers of topics. This proves
that in TSM, the learned topics are more distinct
from each other. This is because LDA mixes these
two kinds of words, which introduces noise into
the learned topics and decreases their distinction
between each other. But topic driven words and
style driven words are well separated in TSM. Fig-
ure 5 also plots the Average Divergence between
the learned two styles, which is the curve denot-
ed by DIFF. We can see the AD between differ-
ent styles is even larger than that among topics in
TSM. Different topics may still have some over-
lap in frequently used words but styles may share
few words with each other. So AD of styles can
get higher value. This also proves the effective-
P@5 P@10 P@20 P@30 P@40 P@50
E 0 0.2 0.25 0.23 0.225 0.2
T 0.8 0.9 0.8 0.8 0.675 0.62
Table 9: Slang identification precision. E: Emoti-
con; T:TSM.
#Word/Post #Post
Formal User 34.9 158.3
Informal User 14.5 381
Table 10: Mean Value of average post length and
number of post for different type of users
ness of our model in identifying writing styles and
uncovering more distinct topics.
4.7 Discovering Slang
By looking at Table 5, we notice that the unse-
rious style contains many slang words with high
probability. This indicates that the unserious style
in the dataset we use is also characterized by slang
words. In this section, we will show the useful-
ness of our model in slang discovery. The base-
line method is denoted as Emoticon as it ranks
words according to their probability of occurring
in a post containing emoticons. We ask two Sin-
gaporean annotators to help us identify Singapore-
an slang in the top 50 words. The result is shown
in Table 9. It tells us the unserious style learned
in our model has very good performance in iden-
tifying local slang words. For people preferring
unserious writing style, they would write posts in
a very flexible way and use many informal words,
abbreviations and slang expressions. So our un-
serious style will be characterized by these slang
words and performs very well in identifying these
slang words.
4.8 Analysis of Users
In this subsection, we analyze users in our dataset
based on the result learned by TSM. Figure 8
shows the distribution of the histogram of serious
style probability. The majority of users have a
high serious style probability, which means most
users in our dataset are more eager to give serious
comments and express their opinions. This satis-
fies our observation that most people use forums
mainly to discuss and seek knowledge on differ-
ent topics and they are very eager to express their
thoughts in a serious way.
We heuristically split all users into two sets ac-
cording to user-style probability by setting 0.5 as
threshold. Users with probability of serious style
39
10 20 30 40 50 60 70 80 90 1004000
40504100
41504200
42504300
43504400
44504500
4550
Number of topics
Perple
xity
 
 TSMLDA
Figure 4: Perplexity
10 20 30 40 50 60 70 80 90 1001
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Number of topics
Log(AD
)
 
 
TSMLDADIFF
Figure 5: Average Divergence
Serious User Unserious User0
200400
600800
10001200
14001600
18002000
Numb
er of P
osts
Figure 6: Box plot of post number for serious
and unserious users
Serious User Unserious User0
50
100
150
200
Numb
er of W
ords P
er Pos
t
Figure 7: Box plot of average post length for se-
rious and unserious users
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
20
40
60
80
100
120
140
160
180
200
Seriouness Score
Numb
er of U
ser
Figure 8: Seriousness Score of Users
larger than 0.5 are regarded as serious users and
the remaining are unserious users. Next, we ex-
tract the number of posts each user edit and the
average number of words per post for each user
and compare the difference between these two us-
er sets. Figure 6 and Figure 7 show the box plots
of post number and average post length respective-
ly. We can see that serious users edit fewer posts
but use more words in each post. To see the dif-
ference between serious and unserious users more
clearly, we apply Mann-Whitney U test on the post
number populations and average post length pop-
ulations. The Mann-Whitney U test on both data
set reject the null hypothesis that two input popu-
lations are the same at the 1% significance level.
The mean value for post number and average post
length are also computed and shown in Table 10.
We can find that serious users tend to publish few-
er but longer posts than unserious users. This re-
sult is intuitive as serious users often spend more
effort editing their posts to express their opinions
more clearly. However, for unserious users, they
may just use a few words to play a joke or show
some emotions and they can post many posts with-
out spending too much time.
5 Conclusions
In this paper, we propose a unified probabilistic
graphical model, called Topic-Style Model, which
models topics and styles at the same time. Tra-
ditional topic modeling methods treat a corpus as
a mixture of topics. But user-generated content
in forum discussions contains not only words re-
lated to topics but also words related to different
writing styles. The proposed Topic-Style Model
can perform well in separating topic-driven word-
s and style-driven words. In this model, we as-
sume that writing style is a consistent writing pat-
tern a user will express in her posts across differ-
ent threads and use a latent variable at user lev-
el to capture the user specific preference of writ-
ing styles. Our model can successfully discover
writing styles which are different from each other
both in word distribution and formality. Words be-
longing to different writing styles and user specific
style distribution are captured by our model at the
same time. An extensive set of experiments shows
that our method has good performances in sepa-
rating serious and unserious posts and users. At
the same time, the model can identify slang words
with promising accuracy, which is proven by our
experiments. An analysis based on the learned
parameters in our model reveal the difference be-
tween serious and unserious users in average post
40
length and post number.
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining, pages 183?194, New York, NY,
USA. ACM.
Archana Bhattarai, Vasile Rus, and Dipankar Dasgup-
ta. 2009. Characterizing comment spam in the blo-
gosphere through content analysis. In Computation-
al Intelligence in Cyber Security, 2009. CICS?09.
IEEE Symposium on, pages 37?44. IEEE.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010.
Automatic acquisition of lexical formality. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 90?98,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhuoye Ding, Yeyun Gong, Yaqian Zhou, Qi Zhang,
and Xuanjing Huang. 2013. Detecting spammer-
s in community question answering. In Proceeding
of International Joint Conference on Natural Lan-
guage Processing, pages 118?126, Nagoya, Japan.
Association for Computational Linguistics.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and Ni-
na Wacholder. 2011. Identifying sarcasm in twitter:
A closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, pages 581?586, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the 2008 International
Conference on Web Search and Data Mining, WSD-
M ?08, pages 219?230, New York, NY, USA. ACM.
Jussi Karlgren and Douglass Cutting. 1994. Recogniz-
ing text genres with simple metrics using discrim-
inant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics - Volume 2,
pages 1071?1075, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of the 16th International Conference on
World Wide Web, WWW ?07, pages 171?180, New
York, NY, USA. ACM.
Michael J. Paul, ChengXiang Zhai, and Roxana Gir-
ju. 2010. Summarizing contrastive viewpoints in
opinionated text. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186?195, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of the
20th Conference on Uncertainty in Artificial Intelli-
gence, pages 487?494, Arlington, Virginia, United
States. AUAI Press.
41
