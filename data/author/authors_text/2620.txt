Comparing Corpora using Frequency Profiling 
Paul RAYSON 
Computing Deparanent, 
Lancaster University 
Lancaster, UK, 
paul@comp.lancs.ac.uk 
Roger GARSlDE 
Computing Department, 
Lancaster University 
Lancaster, UK, 
rgg@comp.lancs.ac.uk 
Abstract 
This paper describes a method of comparing 
corpora which uses frequency profiling. The 
method can be used to discover key words in 
the corpora which differentiate one corpus 
from another. Using annotated corpora, it 
can be applied to discover key grammatical 
or word-sense categories. This can be used 
as a quick way in to find the differences 
between the corpora and is shown to have 
applications in the study of social 
differentiation in the use of English 
vocabulary, profiling of learner English and 
document analysis in the software 
engineering process. 
1 Introduct ion 
Corpus-based techniques have increasingly been 
used to compare language usage in recent years. 
One of the largest early studies was the 
comparison of one million words of American 
English (the Brown corpus) with one million 
words of British English (the LOB corpus) by 
Hofland and Johansson (1982). A difference 
coefficient defined by Yule (1944) showed the 
relative frequency of a word in the two corpora. 
A statistical goodness-of-fit test, the Chi-squared 
test, was also used to compare word frequencies 
across the two corpora. They noted any resulting 
chi-squared values which indicated that a 
statistically significant difference at the 5%, 1%, 
or 0.1% level had been detected between the 
frequency of a word in American English and in 
British English. The null hypothesis of the test is 
that there is no difference between the observed 
frequencies. 
More recently, this size of corpus comparison 
is becoming the standard even for postgraduate 
studies with the increasing availability of 
corpora and reasoning that one million words 
gives sufficient evidence for higher frequency 
words. However, with the production of large 
corpora such as the British National Corpus 
(BNC) containing one hundred million words 
(Aston & Burnard, 1998), frequency 
comparisons are available across millions of 
words of text. There are two main types of 
corpus comparison: 
? comparison of a sample corpus to a large(r) 
corpus 
? comparison of two (roughly-) equal sized 
corpora 
In the first type, we refer to the large(r) corpus 
as a horrnative' corpus since it provides a text 
norm (or standard) against which we can 
compare. These two main types of comparison 
can be extended to the comparison of more than 
two corpora. For example, we may compare one 
normative corpus to several smaller corpora at 
the same time, or compare three or more equal 
sized corpora to each other. In general, however, 
this makes the results more difficult o interpret. 
There are also a number of issues which need 
to be considered when comparing two (or more) 
corpora: 
? representativeness 
? homogeneity within the corpora 
? comparability ofthe corpora 
? reliability of statsfical tests (for different sized 
corpora nd other factors) 
Representativeness (Biber, 1993) is a 
particularly important attribute for a normative 
corpus when comparing a sample corpus to a 
large normative corpus (such as the BNC) which 
contains ections from many different text types 
and domains. To be representative a corpus 
should contain samples of  all major text types 
(Leech, 1993) and if possible in some way 
proportional to their usage in ~very day 
language' (Clear, 1992). This first type of 
comparison is intended to discow~r features in 
the sample corpus with significantly different 
usage (i.e. frequency) to that found in ~eneral' 
language. 
The second type of comparison is one that 
views corpora as equals (as in the Brown and 
LOB comparison). It aims to discover features in 
the corpora that distinguish one tiom another. 
Homogeneity within each of the corpora is 
important here since we may find that the results 
reflect sections within one of the corpora which 
are unlike other sections in either of the corpora 
under consideration (Kilgarriff 1997). 
Comparability is of interest too, since the 
corpora should have been sampled for in the 
same way. In other words, the corpora should 
have been built using the same stratified 
sampling method and with, if  possible, 
randornised methods of sample selection. This is 
the case with Brown and LOB, since LOB was 
designed to be comparable tothe Brown corpus. 
The final issue, which has been addressed 
elsewhere, is the one regarding the reliability of 
the statistical tests in relation to the size of the 
corpora under consideration. Kilgarriff (1996) 
points out that in the Brown versus LOB 
comparison many eomrnon words are marked as 
having significant chi-squared values, and that 
because words are not selected at random in 
language we will always see a large number of 
differences in two such text collections. He 
selects the Mann-Whitney test that: uses ranks of 
frequency data rather than the frequency values 
themselves tocompute the statistic. However, he 
observes that even with the new test 60% of 
words are marked as significant. Ignoring the 
actual frequency of occurrence as in the Mann- 
Whitney test discards most of the evidence we 
have about he distribution of  words. The test is 
often used when comparing ordinal rating scales 
(Oakes 1998: 17). 
Dunning (1993) reports that we should not rely 
on the assumption of a normal distribution when 
performing statistical text analysis and suggests 
that parametric analysis based on the binomial or 
multinomial distributions i  a better alternative 
for smaller texts. The chi-squared value becomes 
unreliable when the expected frequency is less 
than 5 and possibly overestimates with high 
frequency words and when comparing a 
relatively small corpus to a much larger one. He 
proposes the log-likelihood ratio as an 
alternative to Pearson~ chi-squared test. For this 
reason, we chose to use the log-likelihood ratio 
in our work as described in the next section. In 
fact, Cressie and Read (1984) show that 
Pearson~ X 2 (chi-squared) and the likelihood 
ratio G 2 (Dunning~ log-likelihood) are two 
statistics in a continuum defined by the power- 
divergence family of statistics. They go on to 
describe this family in later work (1988, 1989) 
where they also make reference to the long and 
continuing discussion of the normal and chi- 
squared approximations for X 2 and G 2. 
We have applied the goodness-of-fit test for 
comparison of linguistically annotated corpora. 
The frequency distributions of part-of-speech 
and semantic tags are sharply different to words. 
In these comparisons, we are unlikely to observe 
rare events such as tags occurring once. 
However, much higher frequencies will occur 
and so the log-likelihood test is less likely to 
overestimate significance in these cases. 
2 Methodology 
The method is fairly simple and straightforward 
to apply. Given two corpora we wish to 
compare, we produce a frequency list for each 
corpus. Normally, this would be a word 
frequency list, but as described above and as 
with examples in the following application 
section, it can be a part-of-speech (POS) or 
semantic tag frequency list. However, let us 
assume for now that we are performing a 
comparison at the word levee For each word in 
the two frequency lists we calculate the log- 
likelihood (henceforth LL) statistic. This is 
performed by constructing a contingency table 
as in Table 1. 
i The application of this technique to POS or 
semantic tag frequency lists is achieved by 
constructing the contingency table with tag rather 
than word frequencies. 
Table 1 Contigency table for word frequencies 
CORPUS CORPUS TOTAL 
ONE TWO 
Freq a b a+b 
of word 
Freq c-a d-b c+d-a-b 
of other 
words 
TOTAL c d c+d 
Note that the value ~' corresponds to the 
number of words in corpus one, and ~' 
corresponds to the number of words in corpus 
two (1'4 values). The values ~' and b'are called 
the observed values (O). We need to calculate 
the expected values (E) according to the 
following formula: 
E i = 
i 
i 
In our case N1 = c, and N2 = d. So, for this 
word, E1 = c*(a+b) / (c+d) and E2 = d*(a+b) /
(c+d). The calculation for the expected values 
takes account of the size of the two corpora, so 
we do not need to normalise the figures before 
applying the formula. We can then calculate the 
log-likehood value according to this formula: 
-21n A = 2~ Oi In~-~ 
This equates to calculating LL as follows: 
LL = 2*((a*log (a/E1)) + (b*log (b/E2))) 
The word frequency list is then sorted by the 
resulting LL values. This gives the effect of 
placing the largest LL value at the top of the list 
representing the word which has the most 
significant relative frequency difference between 
the two corpora. In this way, we can see the 
words most indicative (or characteristic) of one 
corpus, as compared to the other corpus, at the 
top of the list. The words which appear with 
roughly similar relative frequencies in the two 
corpora ppear lower down the list. Note that we 
do not use the hypothesis-test by comparing the 
LL values to a chi-squared distribution table. As 
Kilgarriff & Rose (1998) note, even Pearson~ 
X 2 is suitable without the hypothesis-testing 
link: Given the non-random nature of words in 
a text, we are always likely to find frequencies 
of words which differ across any two texts, and 
the higher the frequencies, the more information 
the statistical test has to work with. Hence, it is 
at this point that the researcher must intervene 
and qualitatively examine examples of the 
significant words highlighted by this technique. 
We are not proposing a completely automated 
approach. 
3 Applications 
This method has already been applied to study 
social differentiation in the use of English 
vocabulary and profiling of learner English. In 
Rayson et al(1997), selective quantitative 
analyses of the demographically sampled spoken 
English component of the BNC were carried out. 
This is a subcorpus of circa 4.5 million words, in 
which speakers and respondents are identified 
by such factors as gender, age, social group and 
geographical region. Using the method, a 
comparison was performed of the vocabulary of 
speakers, highlighting those differences which 
are marked by a very high value of significant 
difference between different sectors of the 
corpus according to gender, age and social 
group. 
In Granger and Rayson (1998), two similar- 
sized corpora of native and non-native writing 
were compared at the lexical level. The corpora 
were analysed by a part-of-speech tagger, and 
this permitted a comparison at the major word- 
class level. The patterns of significant overuse 
and underuse for POS categories demonstrated 
that the learner data displayed many of the 
stylistic features of spoken rather than written 
English. 
The same technique has more recently been 
applied to compare corpora analysed at the 
semantic level in a systems engineering domain 
and this is the main focus of this section. The 
motivation for this work is that despite natural 
language's well-documented shortcomings as a 
medium for precise technical description, its use 
in software-intensive systems engineering 
remains inescapable. This poses many problems 
for engineers who must derive problem 
understanding and synthesise precise solution 
descriptions from free text. This is true both for 
the largely unstructured textual descriptions 
from which system requirements are derived, 
and for more formal documents, such as 
standards, which impose requirements on system 
development processes. We describe an 
experiment that has been carried out in the 
REVERE project (Rayson et al 2000) to 
investigate the use of probabilistic natural 
language processing techniques to provide 
systems engineering support. 
The target documents are field reports of a 
series of ethnographic studies at an air traffic 
conlxol (ATC) centre. This formed part of a 
study of ATC as an example of a system that 
supports collaborative user tasks (Bentley et al 
1992). The documents consist of both the 
verbatim transcripts of the ethnographerb 
observations and interviews with controllers, 
and of reports compiled by the ethnographer for 
later analysis by a multi-disciplinary team of 
social scientists and systems engineers. The field 
reports form an interesting study because they 
exhibit many characteristics typical of 
documents een by a systems engineer. The 
volume of the information is fairly high (103 
pages) and the documents are not structured in a 
way designed to help the extraction of 
requirements ( ay around business processes or 
system architecture). 
The text is analysed by a part-of-speech tagger, 
CLAWS (Garside and Smith, 1997), and a 
semantic analyser (Rayson and Wilson, 1996) 
which assigns semantic tags that represent the 
semantic field (word-sense) of words from a 
lexicon of single words and an idiom list of 
multi-word combinations (e.g. ~ a rule). These 
resources contain approximately 52,000 words 
and idioms. 
The normative corpus that we used was a 2.3 
million-word subset of the BNC derived from 
the transcripts of spoken English. Using this 
.corpus, the most over-represented sernanfie 
categories in the ATC field reports are shown in 
Table 2. The log-likelihood test is applied as 
described in the previous ection and represents 
the semantic tag's frequency deviation from the 
normative corpus. The higher the figure, the 
greater the deviation. 
Table 2. Over-represented categories in ATC 
field reports 
Log- Tag Word sense (examples 
likelihood from the text) 
3366 $7.1 power, organising 
(bontroller; ~hief) 
2578 M5 flying (lalane; Hight; 
t~irport) 
988 02 general objects (~trip; 
holder; tack) 
643 03 electrical equipment 
(radar; blip) 
535 Y1 science and technology 
('PH) 
449 W3 geographical terms 
(Pole Hill; Dish Sea) 
432 Q1.2 paper documents and 
writing (~vriting; 
~,vritten; hotes) 
372 N3.7 measurement (length; 
height; l:listance; 
levels; '1000ft) 
318 L1 life and living things 
(live) 
310 A 10 indicating actions 
(l~ointing', indicating; 
tlisplay) 
306 X4.2 mental objects 
(~ysterns; tlpproach; 
haode; tactical; 
larocedure) 
290 A4.1 kinds, groups (Sector; 
Sectors) 
With the exception of Y I (an anomaly caused 
by an interviewees initials being mistaken for 
the PH unit of acidity), all of these semantic 
categories include important objects, roles, 
functions, etc. in the ATC domain. The 
frequency with which some of these occur, such 
as M5 (flying), are uusurprising. Others are 
more revealing about the domain of ATC. 
Figure 1 shows some of the occurrences of the 
semantic category 02 (general objects). The 
important information extracted here is the 
importance of Mrips' (formally, 1light strips). 
These are small pieces of cardboard with printed 
flight details that are the most fundamental 
artefact used by the air traffic controllers to 
manage their air space. Examination of other 
words in this category also shows that flight 
4 
!i tO mqt"ll~ " 1250L' i  n red m a , t r ip  
'he :T..sle o f  I lm . . .  &:lU0t; Tht ,  ~t r lp  
~te? l  I~, 'the ~ pr in ted  tn  box 
~on prtn, t~ l  tn hot ' 6 ' o f  the strip 
=rr ' tw l  t i l e  over th=tbeo~n ( box 
iviousllJ only aA~'ozla,~te- :some .s'lwips 
~l  t tne  neor the call$tfln on a ~trtp  
much I~msier . lhermere  1.6 ~t r i~  
! re tor t  1.6 s t r ips  in one oF h i ,  melts 
i*Y , .thot ta lk ing  aml us ing on input 
~hat t~llctr~l md u~inl; an t r lmt  device 
: /Arawtt: the nicl~ l~hina ~,~:  ~"?rins 
, d '~ i l t t  o t  = time t r l s tm 
~ tomrds  ' ~l'le b0r t~ Of ~e~ :
? II " o f  the s t r ip  ( ~ te l  
( ~ le f t  } S t r lpssee~d br  
' A " ) ' th is  ~ (:l~'tcusl~ on ly  
~mett out of, pos i t ion  , and 2\[ got  
t;o tndtcote =nur.~,~L ~meed . < 
? oF ht= ~lm . .dBb. A ; 
dev ice  s ight  =t~o be ? but that  
retort  a im be , but  ~ the pr  
i=t the i r  F lex ib i l i ty  . ,~uot :  o 
Figure 1. Browsing the semantic ategory 02 
strips are held in tacks' to organise them 
according to (for example) aircraft time-of- 
arrival. 
Similarly, browsing the context for Q1.2 
(paper documents and writing) would allow us 
to discover that controllers annotate flight strips 
'to record deviations from flight plans, and L1 
(life, living things) would reveal that some strips 
are live; that is, they refer to aircraft currently 
traversing the contxoller's sector. Notice also that 
the semantic categories' deviation from the 
normative corpus can also be expected to reveal 
domain roles (actors). In this example, the 
frequency of $7.1 (power, organising) shows the 
importance of the roles of ~ontrollers' and 
~hiefs'. 
Using the frequency profiling method does not 
automate the task of identifying abstractions, 
much less does it produce fully formed 
requirements that can be pasted into a 
specification document. Instead, it helps the 
engineer quickly isolate potentially significant 
domain abstractions that require closer analysis. 
4 Conclusions 
reliability of the statistical tests (LL, Pearson~ 
X 2 and others) under the effects of corpus size, 
ratio of the corpora being compared and word 
(or tag) frequency. 
We do not propose a completely automated 
approach. The tools suggest a group of key items 
by decreasing order of significance which 
distinguish one corpus from another. It is then 
that the researcher should investigate 
occurrences of the significant items in the 
corpora using standard corpus techniques uch 
as KWIC (key-word in context). The reasons 
behind their significance can be discovered and 
explanations ought for the patterns displayed. 
By this process, we can compare the corpora 
under investigation and make hypotheses about 
the language use that they represent. 
Acknowledgements 
Our thanks go to Geoffrey Leech and the 
anonymous reviewers who commented on 
earlier versions of this paper. The REVERE 
project is supported under the EPSRC Systems 
Engineering for Business Process Change 
(SEBPC) programme, project number 
GR/MO4846. 
This paper has described a method of comparing 
corpora which uses frequency profiling. The 
method has been shown to discover key items in 
the corpora which differentiate one corpus from 
another. It has been applied at the word level, 
part-of-speech tag level, and semantic tag level. 
It can be used as a quick way in to find the 
differences between the corpora and is shown to 
have applications in the study of social 
differentiation i  the use of English vocabulary: 
profiling of learner English and document 
analysis in the software ngineering process. 
Future directions in which we aim to research 
include a more precise specification of the 
References 
Aston, G. and Burnard, L. (1998). The BNC 
Handbook: Exploring the British National Corpus 
with SARA, Edinburgh University Press. 
Bentley IL, Rodden T., Sawyer P., Sommerville I, 
Hughes J., Randall D., Shapiro D. (1992). 
Ethnographically-informed systems design for air 
traffic control, In Proceedings of Computer- 
Supported Cooperative Work (CSCW) '92, 
Toronto, November 1992. 
Biber, D. (1993). Representativeness in Corpus 
Design. Literary and Linguistic Computing, 8, 
Issue 4, Oxford University Press, pp. 243-257. 
Clear, J. (1992). Corpus sampling. In G. Leitner (ed.) 
New directions in English lang~aage corpora~ 
Mouton-de-Gruyter, Berlin, pp. 21 - 31. 
Cressie, N. and Read, T. R. C. (1984) Multinomial 
Goodness-of-Fit Tests. Journal of the Royal 
Statistical Society. Series B (Methodological), Vol. 
46, No. 3, pp. 440 - 464. 
Cressie, N. and Read, T. R. C. (1989). Pearson~ X 2 
and the Loglikelihood Ratio Statistic G2: A 
comparative review. International Statistical 
Review, 57, 1, Belfast University Press, N.I., pp. 
19--43. 
Dunning, T. (1993). Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics, 19, 1, March 1993, pp. 
61-74. 
Garside, R. and Smith, N. (199"7). A Hybrid 
Grammatical Tagger: CLAtVS4, in Garside, R., 
Leech, G., and McEnery, A. (eds.) Corpus 
Annotation: Linguistic Information from Computer 
Text Corpora, Longman, London. 
Granger, S. and Rayson, P. (1998). Automatic 
profiling of learner texts. In S. Granger (ed.) 
Learner English on Computer. Longman, London 
and New York, pp. 119-131. 
Hofland, K. and Johansson, S. (1982). Word 
frequencies in British and American English. The 
Norwegian Computing Centre for the Humanities, 
Bergen, Norway. 
Kilgarriff, A. (1996) Why chi-square doesn't work; 
and an improved LOB-Brown comparison. ALLC- 
ACH Conference, June 1996, Bergen, Norway. 
Kilgarriff, A. (1997). Using word frequency lists to 
measure corpus homogeneity and similarity 
between corpora. Proceedings 5th ACL workshop 
on very large corpora. Beijing and Hong Kong. 
Kilgarriff, A. and Rose, T. (1998). Measures for 
corpus similarity and homogeneity. In proceedings 
of the 3 m conference on Empirical Methods in 
Natural Language Processing, Granada, Spain, pp. 
46 - 52. 
Leech, G. (1993). 100 million words of English: a 
description of the background, nature and 
prospects of the British National Corpus project. 
English Today 33, Vol. 9, No. 1, Cambridge 
University Press. 
Oakes, M. P. (1998). Statistics for Corpus 
Linguistics. Edinburgh University Press, 
Edinburgh. 
Rayson, P., and Wilson, A. (1996). The ACAMRIT 
semantic tagging system: progress report, In L. J. 
Evett, and T. G. Rose (eds.) Language Engineering 
for Document Analysis and Recognition, LEDAR, 
AISB96 Workshop proceedings, pp 13-20. 
Brighton, England. 
Rayson, P., Leech, G., and Hodges, M. (1997). Social 
differentiation in the use of English vocabulary: 
some analyses of the conversational component of 
the British National Corpus. International Journal 
of Corpus Linguistics. 2 (1). pp. 133 - 152. John 
Benjamins, Amsterdam/Philadelphia. 
Rayson, P., Garside, R., and Sawyer, P. (2000). 
Assisting requirements engineering with semantic 
document analysis. In Proceedings of RIAO 2000 
(Recherche d'Inforrnafions Assisfie par Ordinateur, 
Computer-Assisted Information Retrieval) 
International Conference, Coll~ge de France, Paris, 
France, April 12-14, 2000. C.I.D., Paris, pp. 1363 - 
1371. 
Read, T. R. C. and Cressie, N. A. C. (1988). 
Goodness-of-fit s atistics for discrete multivariate 
data. Springer series in statistics. Springer-Vedag, 
New York. 
Yule, G. (1944). The Statistical Study of Literary 
Vocabulary. Cambridge University Press. 
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring MWE Compositionality Using Semantic Annotation 
Scott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 
 
1Computing Department 
Lancaster University 
Lancaster, UK 
{s.piao, p.rayson, r.garside}@lancaster.ac.uk 
2Dept. of Linguistics and EL 
Lancaster University 
Lancaster, UK 
{o.mudraya, a.wilson}@lancaster.ac.uk 
 
 
Abstract 
This paper reports on an experiment in 
which we explore a new approach to the 
automatic measurement of multi-word 
expression (MWE) compositionality. We 
propose an algorithm which ranks MWEs 
by their compositionality relative to a 
semantic field taxonomy based on the 
Lancaster English semantic lexicon (Piao 
et al, 2005a). The semantic information 
provided by the lexicon is used for meas-
uring the semantic distance between a 
MWE and its constituent words. The al-
gorithm is evaluated both on 89 manually 
ranked MWEs and on McCarthy et als 
(2003) manually ranked phrasal verbs. 
We compared the output of our tool with 
human judgments using Spearman?s 
rank-order correlation coefficient. Our 
evaluation shows that the automatic rank-
ing of the majority of our test data 
(86.52%) has strong to moderate correla-
tion with the manual ranking while wide 
discrepancy is found for a small number 
of MWEs. Our algorithm also obtained a 
correlation of 0.3544 with manual rank-
ing on McCarthy et als test data, which 
is comparable or better than most of the 
measures they tested. This experiment 
demonstrates that a semantic lexicon can 
assist in MWE compositionality meas-
urement in addition to statistical algo-
rithms. 
1 Introduction 
Over the past few years, compositionality and 
decomposability of MWEs have become impor-
tant issues in NLP research. Lin (1999) argues 
that ?non-compositional expressions need to be 
treated differently than other phrases in many 
statistical or corpus?based NLP methods?. Com-
positionality means that ?the meaning of the 
whole can be strictly predicted from the meaning 
of the parts? (Manning & Sch?tze, 2000). On the 
other hand, decomposability is a metric of the 
degree to which the meaning of a MWE can be 
assigned to its parts (Nunberg, 1994; Riehemann, 
2001; Sag et al, 2002). These two concepts are 
closely related. Venkatapathy and Joshi (2005) 
suggest that ?an expression is likely to be rela-
tively more compositional if it is decomposable?. 
While there exist various definitions for 
MWEs, they are generally defined as cohesive 
lexemes that cross word boundaries (Sag et al, 
2002; Copestake et al, 2002; Calzolari et al, 
2002; Baldwin et al, 2003), which include 
nominal compounds, phrasal verbs, idioms, col-
locations etc. Compositionality is a critical crite-
rion cutting across different definitions for ex-
tracting and classifying MWEs. While semantics 
of certain types of MWEs are non-compositional, 
like idioms ?kick the bucket? and ?hot dog?, 
some others can have highly compositional se-
mantics like the expressions ?traffic light? and 
?audio tape?. 
Automatic measurement of MWE composi-
tionality can have a number of applications. One 
of the often quoted applications is for machine 
translation (Melamed, 1997; Hwang & Sasaki, 
2005), in which non-compositional MWEs need 
special treatment. For instance, the translation of 
a highly compositional MWE can possibly be 
inferred from the translations of its constituent 
words, whereas it is impossible for non-
compositional MWEs, for which we need to 
identify the translation equivalent for the MWEs 
as a whole. 
In this paper, we explore a new method of 
automatically estimating the compositionality of 
MWEs using lexical semantic information, 
sourced from the Lancaster semantic lexicon 
(Piao et al, 2005a) that is employed in the 
USAS1 tagger (Rayson et al, 2004). This is a 
                                                 
1 UCREL Semantic Analysis System 
2
large lexical resource which contains nearly 
55,000 single-word entries and over 18,800 
MWE entries. In this lexicon, each MWE2 and 
the words it contains are mapped to their poten-
tial semantic categories using a semantic field 
taxonomy of 232 categories. An evaluation of 
lexical coverage on the BNC corpus showed that 
the lexical coverage of this lexicon reaches 
98.49% for modern English (Piao et al, 2004).  
Such a large-scale semantic lexical resource al-
lows us to examine the semantics of many 
MWEs and their constituent words conveniently 
without resorting to large corpus data. Our ex-
periment demonstrates that such a lexical re-
source provides an additional approach for auto-
matically estimating the compositionality of 
MWEs. 
One may question the necessity of measuring 
compositionality of manually selected MWEs. 
The truth is, even if the semantic lexicon under 
consideration was compiled manually, it does not 
exclusively consist of non-compositional MWEs 
like idioms. Built for practical discourse analysis, 
it contains many MWEs which are highly com-
positional but depict certain entities or semantic 
concepts. This research forms part of a larger 
effort to extend lexical resources for semantic 
tagging. Techniques are described elsewhere 
(e.g. Piao et al, 2005b) for finding new candi-
date MWE from corpora. The next stage of the 
work is to semi-automatically classify these can-
didates using an existing semantic field taxon-
omy and, to assist this task, we need to investi-
gate patterns of compositionality. 
2 Related Work  
In recent years, various approaches have been 
proposed to the analysis of MWE compositional-
ity. Many of the suggested approaches employ 
statistical algorithms. 
One of the earliest studies in this area was re-
ported by Lin (1999) who assumes that ?non-
compositional phrases have a significantly dif-
ferent mutual information value than the phrases 
that are similar to their literal meanings? and 
proposed to identify non-compositional MWEs 
in a corpus based on distributional characteristics 
of MWEs. Bannard et al (2003) tested tech-
niques using statistical models to infer the mean-
ing of verb-particle constructions (VPCs), focus-
                                                 
2 In this lexicon, many MWEs are encoded as templates, 
such as driv*_* {Np/P*/J*/R*} mad_JJ,  which represent 
variational forms of a single MWE, For further details, see 
Rayson et al, 2004.  
ing on prepositional particles. They tested four 
methods over four compositional classification 
tasks, reporting that, on all tasks, at least one of 
the four methods offers an improvement in preci-
sion over the baseline they used. 
McCarthy et al (2003) suggested that compo-
sitional phrasal verbs should have similar 
neighbours as for their simplex verbs. They 
tested various measures using the nearest 
neighbours of phrasal verbs and their simplex 
counterparts, and reported that some of the 
measures produced results which show signifi-
cant correlation with human judgments. Baldwin 
et al (2003) proposed a LSA-based model for 
measuring the decomposability of MWEs by ex-
amining the similarity between them and their 
constituent words, with higher similarity indicat-
ing the greater decomposability.  They evaluated 
their model on English noun-noun compounds 
and verb-particles by examining the correlation 
of the results with similarities and hyponymy 
values in WordNet. They reported that the LSA 
technique performs better on the low-frequency 
items than on more frequent items. Venkatapathy 
and Joshi (2005) measured relative composition-
ality of collocations having verb-noun pattern 
using a SVM (Support Vector Machine) based 
ranking function. They integrated seven various 
collocational and contextual features using their 
ranking function, and evaluated it against manu-
ally ranked test data. They reported that the SVM 
based method produces significantly better re-
sults compared to methods based on individual 
features. 
The approaches mentioned above invariably 
depend on a variety of statistical contextual in-
formation extracted from large corpus data. In-
evitably, such statistical information can be af-
fected by various uncontrollable ?noise?, and 
hence there is a limitation to purely statistical 
approaches. 
In this paper, we contend that a manually 
compiled semantic lexical resource can have an 
important part to play in measuring the composi-
tionality of MWEs. While any approach based on 
a specific lexical resource may lack generality, it 
can complement purely statistical approaches by 
importing human expert knowledge into the 
process. Particularly, if such a resource has a 
high lexical coverage, which is true in our case, 
it becomes much more useful for dealing with 
general English. It should be emphasized that we 
propose our semantic lexical-based approach not 
as a substitute for the statistical approaches. 
3
Rather we propose it as a potential complement 
to them.   
In the following sections, we describe our ex-
periment and explore this approach to the issue 
of automatic estimation of MWE compositional-
ity. 
3 Measuring MWE compositionality 
with semantic field information 
In this section, we propose an algorithm for 
automatically measuring MWE compositionality 
based on the Lancaster semantic lexicon. In this 
lexicon, the semantic field of each word and 
MWE is encoded in the form of semantic tags. 
We contend that the compositionality of a MWE 
can be estimated by measuring the distance be-
tween semantic fields of an MWE and its con-
stituent words based on the semantic field infor-
mation available from the lexicon. 
The lexicon employs a taxonomy containing 
21 major semantic fields which are further di-
vided into 232 sub-categories. 3  Tags are de-
signed to denote the semantic fields using letters 
and digits. For instance, tag N3.2 denotes the 
category of {SIZE} and Q4.1 denotes {media: 
Newspapers}. Each entry in the lexicon maps a 
word or MWE to its potential semantic field 
category/ies. More often than not, a lexical item 
is mapped to multiple semantic categories, re-
flecting its potential multiple senses. In such 
cases, the tags are arranged by the order of like-
lihood of meanings, with the most prominent one 
at the head of the list. For example, the word 
?mass? is mapped to tags N5, N3.5, S9, S5 and 
B2, which denote its potential semantic fields of 
{QUANTITIES},  {MEASUREMENT: 
WEIGHT}, {RELIGION AND SUPERNATU-
RAL}, {GROUPS AND AFFILIATION} and 
{HEALTH AND DISEASE}. 
 The lexicon provides direct access to the se-
mantic field information for large number of 
MWEs and their constituent words. Furthermore, 
the lexicon was analysed and classified manually 
by a team of linguists based on the analysis of 
corpus data and consultation of printed and elec-
tronic corpus-based dictionaries, ensuring a high 
level of consistency and accuracy of the semantic 
analysis.  
In our context, we interpret the task of measur-
ing the compositionality of MWEs as examining 
the distance between the semantic tag of a MWE 
and the semantic tags of its constituent words. 
                                                 
3 For the complete semantic tagset, see website: 
http://www.comp.lancs.ac.uk/ucrel/usas/ 
Given a MWE M and its constituent words wi (i 
= 1, .., n), the compositionality D can be meas-
ured by multiplying the semantic distance SD 
between M and each of its constituent words wi. 
In practice, the square root of the product is used 
as the score in order to reduce the range of actual 
D-scores, as shown below: 
 
(1)   ?
=
=
n
i
iwMSDMD
1
),()(  
 
where D-score ranges between [0, 1], with 1 in-
dicating the strongest compositionality and 0 the 
weakest compositionality. 
In the semantic lexicon, as the semantic in-
formation of function words is limited, they are 
classified into a single grammatical bin (denoted 
by tag Z5). In our algorithm, they are excluded 
from the measuring process by using a stop word 
list. Therefore, only the content constituent 
words are involved in measuring the composi-
tionality. Although function words may form an 
important part of many MWEs, such as phrasal 
verbs, because our algorithm solely relies on se-
mantic field information, we assume they can be 
ignored.  
 The semantic distance between a MWE and 
any of its constituent words is calculated by 
quantifying the similarity between their semantic 
field categories. In detail, if the MWE and a con-
stituent word do not share any of the major 21 
semantic domains, the SD is assigned a small 
value ?.4 If they do, three possible cases are con-
sidered: 
 
Case a. If they share the same tag, and the con-
stituent word has only one tag, then SD 
is one. 
Case b. If they share a tag or tags, but the con-
stituent words have multiple candidate 
tags, then SD is weighted using a vari-
able ? based on the position of the 
matched tag in the candidate list as well 
as the number of candidate tags. 
Case c. If they share a major category, but their 
tags fall into different sub-categories 
(denoted by the trailing digits following 
a letter), SD is further weighted using a 
                                                 
4 We avoid using zero here in order to avoid producing se-
mantic distance of zero indiscriminately when any one of 
the constituent words produces zero distance regardless of 
other constituent words. 
4
variable ? which reflects the difference 
of the sub-categories. 
With respect to weight ?, suppose L is the 
number of candidate tags of the constituent word 
under consideration, N is the position of the spe-
cific tag in the candidate list (the position starts 
from the top with N=1), then the weight ? is cal-
culated as 
 
(2)  
2
1
L
NL +?=? , 
 
where N=1, 2, ?, n and N<=L. Ranging between 
[1, 0), ? takes into account both the location of 
the matched tag in the candidate tag list and the 
number of candidate tags. This weight penalises 
the words having more candidate semantic tags 
by giving a lower value for their higher degree of 
ambiguity. As either L or N increases, the ?-
value decreases.       
Regarding the case c), where the tags share the 
same head letter but different digit codes, i.e. 
they are from the same major category but in 
different sub-categories, the weight ? is calcu-
lated based on the number of sub-categories they 
share. As we mentioned earlier, a semantic tag 
consists of an initial letter and some trailing dig-
its divided by points, e.g. S1.1.2 {RECIPROC-
ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-
SERVE} etc. If we let T1, T2 be a pair of semantic 
tags with the same initial letters, which have ki 
and kj trailing digit codes (denoting the number 
of sub-division layers) respectively and share n 
digit codes from the left, or from the top layer, 
then ? is calculated as follows: 
 
(3)   
k
n=? ; 
(4)   . ),max( ji kkk =
 
where ? ranges between (0, 1). In fact, the cur-
rent USAS taxonomy allows only the maximum 
three layers of sub-division, therefore ? has one 
of three possible scores: 0.500 (1/2), 0.333 (1/3) 
and 0.666 (2/3). In order to avoid producing zero 
scores, if the pair of tags do not share any digit 
codes except the head letter, then n is given a 
small value of 0.5. 
Combining all of the weighting scores, the 
semantic distance SD in formula (1) is calculated 
as follows: 
 
(5)  ( )
??
?
?
??
?
?
?
=
?
?
=
=
.  then   c), if
;  then   b), if
1;  then   a), if
;   then   matches,   tagno if
,
1
1
n
i
ii
n
i
iiwMSD
??
?
?
 
where ? is given a small value of 0.001 for our 
experiment5. 
Some MWEs and single words in the lexicon 
are assigned with combined semantic categories 
which are considered to be inseparable, as shown 
below: 
petrol_NN1 station_NN1 M3/H1 
where the slash means that this MWE falls under 
the categories of M3 {VEHICLES AND TRANS-
PORTS ON LAND} and H1 {ARCHITECTURE 
AND KINDS OF HOUSES AND BUILDINGS} 
at the same time. For such cases, criss-cross 
comparisons between all possible tag pairs are 
carried out in order to find the optimal match 
between the tags of the MWE and its constituent 
words. 
By way of further explanation, the word 
?brush? as a verb has candidate semantic tags of 
B4 {CLEANING AND PERSONAL CARE} and 
A1.1.1 {GENERAL ACTION, MAKING} etc. On 
the other hand, the phrasal verb ?brush down? 
may fall under either B4 category with the sense 
of cleaning or G2.2 category {ETHICS} with the 
sense of reprimand. When we apply our algo-
rithm to it, we get the D-score of 1.0000 for the 
sense of cleaning, indicating a high degree of 
compositionality, whereas we get a low D-score 
of 0.0032 for the sense of reprimand, indicating 
a low degree of compositionality. Note that the 
word ?down? in this MWE is filtered out as it is 
a functional word. 
The above example has only a single constitu-
ent content word. In practice, many MWEs have 
more complex structures than this example. In 
order to test the performance of our algorithm, 
we compared its output against human judgments 
of compositionality, as reported in the following 
section. 
4 Manually Ranking MWEs for 
Evaluation 
In order to evaluate the performance of our 
tool against human judgment, we prepared a list 
                                                 
5 As long as ? is small enough, it does not affect the ranking 
of D-scores. 
5
of 89 MWEs6 and asked human raters to rank 
them via a website. The list includes six MWEs 
with multiple senses, and these were treated as 
separate MWE. The Lancaster MWE lexicon has 
been compiled manually by expert linguists, 
therefore we assume that every item in this lexi-
con is a true MWE, although we acknowledge 
that some errors may exist. 
Following McCarthy et al?s approach, we 
asked the human raters to assign each MWE a 
number ranging between 0 (opaque) and 10 
(fully compositional). Both native and non-native 
speakers are involved, but only the data from 
native speakers are used in this evaluation. As a 
result, three groups of raters were involved in the 
experiment.  Group 1 (6 people) rated MWEs 
with indexes of 1-30, Group 2 (4 people) rated 
MWEs with indexes of 31-59 and Group 3 (five 
people) rated MWEs with indexes of 6-89. 
In order to test the level of agreement between 
the raters, we used the procedures provided in 
the 'irr' package for R (Gamer, 2005). With this 
tool, the average intraclass correlation coefficient 
(ICC) was calculated for each group of raters 
using a two-way agreement model (Shrout & 
Fleiss, 1979). As a result, all ICCs exceeded 0.7 
and were significant at the 95% confidence level, 
indicating an acceptable level of agreement be-
tween raters. For Group 1, the ICC was 0.894 
(95% ci = 0.807 < ICC < 0.948), for Group 2 it 
was 0.9 (95% ci=0.783<ICC<0.956) and for 
Group 3 it was 0.886 (95% ci =  0.762 < ICC < 
0.948). 
Based on this test, we conclude that the man-
ual ranking of the MWEs is reliable and is suit-
able to be used in our evaluation. Source data for 
the human judgements is available from our 
website in spreadsheet form7. 
5 Evaluation 
In our evaluation, we focused on testing the 
performance of the D-score against human rat-
ers? judgment on ranking different MWEs by 
their degree of compositionality, as well as dis-
tinguishing the different degrees of composition-
ality for each sense in the case of multiple tags.  
The first step of the evaluation was to imple-
ment the algorithm in a program and run the tool 
on the 89 test MWEs we prepared. Fig. 1 illus-
trates the D-score distribution in a bar chart. As 
shown by the chart, the algorithm produces a 
widely dispersed distribution of D-scores across 
                                                 
6 Selected at random from the Lancaster semantic lexicon. 
7 http://ucrel.lancs.ac.uk/projects/assist/ 
the sample MWEs, ranging from 0.000032 to 
1.000000. For example, the tool assigned the 
score of 1.0 to the FOOD sense and 0.001 to the 
THIEF senses of ?tea leaf? successfully distin-
guishing the different degrees of compositional-
ity of these two senses. 
 
MWE Compositionality Distribution
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
89 MWEs
D
-s
co
re
 
 
Fig 1: D-score distribution across 89 sample 
MWEs 
 
As shown in Fig. 1, some MWEs share the 
same scores, reflecting the limitation of the num-
ber of ranks that our algorithm can produce as 
well as the limited amount of semantic informa-
tion available from a lexicon. Nonetheless, the 
algorithm produced 45 different scores which 
ranked the MWEs into 45 groups (see the steps 
in the figure). Compared to the eleven scores 
used by the human raters, this provides a fine-
grained ranking of the compositionality.   
The primary issue in our evaluation is the ex-
tent to which the automatic ranking of the MWEs 
correlates with the manual ranking of them. As 
described in the previous section, we created a 
list of 89 manually ranked MWEs for this pur-
pose. Since we are mainly interested in the ranks 
rather than the actual scores, we examined the 
correlation between the automatic and manual 
rankings using Spearman?s correlation coeffi-
cient. (For the full ranking list, see Appendix). 
In the manually created list, each MWE was 
ranked by 3-6 human raters. In order to create a 
unified single test data of human ranking, we 
calculated the average of the human ranks for 
each MWE. For example, if two human raters 
give ranks 3 and 4 to a MWE, then its rank is 
(3+4)/2=3.5. Next, the MWEs are sorted by the 
averaged ranks in descending order to obtain the 
combined ranks of the MWEs. Finally, we sorted 
the MWEs by the D-score in the same way to 
obtain a parallel list of automatic ranks. For the 
calculation of Spearman?s correlation coefficient, 
if n MWEs are tied to a score (either D-score or 
the average manual ranks), their ranks were ad-
6
justed by dividing the sum of their ranks by the 
number of MWEs involved. Fig. 2 illustrates the 
correspondence between the adjusted automatic 
and manual rankings. 
 
Auto vs. Manual Ranks Comparison
(n=89, rho=0.2572)
0
20
40
60
80
100
0 20 40 60 80 100
auto ranks
m
an
ua
l r
an
ks
 
 
Fig. 2: Scatterplot of automatic vs. manual 
ranking. 
 
As shown in Fig. 2, the overall correlation seems 
quite weak. In the automatic ranking, quite a few 
MWEs are tied up to three ranks, illustrated by 
the vertically aligned points. The precise correla-
tion between the automatic and manual rankings 
was calculated using the function provided in R 
for Windows 2.2.1.  Spearman's rank correlation 
(rho) for these data was 0.2572 (p=0.01495), 
indicating a significant though rather weak posi-
tive relationship. 
In order to find the factors causing this weak 
correlation, we tested the correlation for those 
MWEs whose rank differences were less than 20, 
30, 40 and 50 respectively. We are interested to 
find out how many of them fall under each of the 
categories and which of their features affected 
the performance of the algorithm. As a result, we 
found 43, 54, 66 and 77 MWEs fall under these 
categories respectively, which yield different 
correlation scores, as shown in Table 1.  
 
numb of 
MWEs 
Percent 
(%) 
Rank 
diff 
rho-
score 
Sig. 
43 48.31 <20 0.9149 P<0.001 
54 60.67 <30 0.8321 P<0.001 
66 74.16 <40 0.7016 P<0.001 
77 86.52 <50 0.5084 P<0.001 
89 (total) 100.00 <=73 0.2572 P<0.02 
 
Table 1: Correlation coefficients corresponding 
different rank differences. 
 
As we expected, the rho decreases as the rank 
difference increases, but all of the four categories 
containing a total of 77 MWEs (86.52%) show 
reasonably high correlations, with the minimum 
score of 0.5084. 8 In particular, 66 of them 
(74.16%), whose ranking differences are less 
than 40, demonstrate a strong correlation with 
rho-score 0.7016, as illustrated by Fig. 3 
 
ScatterPlot of Auto vs. Man Ranks for 66 MWEs
(rank_diff < 40)
0
20
40
60
80
100
0 20 40 60 80 10auto ranks
m
an
 r
an
ks
0
 
 
Fig 3: ScatterPlot for 66 MWEs (rank_diff < 
40) which shows a strong correlation 
 
Our manual examination shows that the algo-
rithm generally pushes the highly compositional 
and non-compositional MWEs towards opposite 
ends of the spectrum of the D-score. For example, 
those assigned with score 1 include ?aid worker?, 
?audio tape? and ?unemployment figure?. On the 
other hand, MWEs such as ?tea leaf? (meaning 
thief), ?kick the bucket? and ?hot dog? are given 
a low score of 0.001. We assume these two 
groups of MWEs are generally treated as highly 
compositional and opaque MWEs respectively. 
However, the algorithm could be improved. A 
major problem found is that the algorithm pun-
ishes longer MWEs which contain function 
words. For example, ?make an appearance? is 
scored 0.000114 by the algorithm, but when the 
article ?an? is removed, it gets a higher score 
0.003608. Similarly, when the preposition ?up? 
is removed from ?keep up appearances?, it gets 
0.014907 compared to the original 0.000471, 
which would push up their rank much higher. To 
address this problem, the algorithm needs to be 
refined to minimise the impact of the function 
words to the scoring process. 
Our analysis also reveals that 12 MWEs with 
rank differences (between automatic and manual 
ranking) greater than 50 results in a degraded 
overall correlation. Table 2 lists these words, in 
which the higher ranks indicate higher composi-
tionality.  
 
                                                 
8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, 
0.6~0.8 and 0.8~1.0 indicate moderate, strong and very 
strong correlations respectively. 
7
MWE Sem. Tag9 Auto 
rank 
Manual 
rank 
plough into A9- 53.5 3 
Bloody Mary F2 53.5 2 
pillow fight K6 26 80.5 
lollipop lady M3/S2 70 15 
cradle snatcher S3.2/T3/S2 73.5 17.5 
go bananas X5.2+++ 65 8.5 
make an appearance S1.1.3+ 2 58.5 
keep up appearances A8/S1.1.1 4 61 
sandwich course P1 69 11.5 
go bananas B2-/X1 68 10 
Eskimo roll M4 71.5 5 
in other words Z4 12.5 83 
 
Table 2: Twelve MWEs having rank differences 
greater than 50. 
 
Let us take ?pillow fight? as an example. The 
whole expression is given the semantic tag K6, 
whereas neither ?pillow? nor ?fight? as individ-
ual word is given this tag. In the lexicon, ?pil-
low? is classified as H5 {FURNITURE AND 
HOUSEHOLD FITTINGS} and ?fight? is as-
signed to four semantic categories including S8- 
{HINDERING}, X8+ {HELPING}, E3- {VIO-
LENT/ANGRY}, and K5.1 {SPORTS}. For this 
reason, the automatic score of this MWE is as 
low as 0.003953 on the scale of [0, 1]. On the 
contrary, human raters judged the meaning of 
this expression to be fairly transparent, giving it 
a high score of 8.5 on the scale of [0, 10]. Similar 
contrasts occurred with the majority of the 
MWEs with rank differences greater than 50, 
which are responsible for weakening the overall 
correlation. 
Another interesting case we noticed is the 
MWE ?pass away?. This MWE has two major 
senses in the semantic lexicon L1- {DIE} and 
T2- {END} which were ranked separately. Re-
markably, they were ranked in the opposite order 
by human raters and the algorithm. Human raters 
felt that the sense DIE is less idiomatic, or more 
compositional, than END, while the algorithm 
indicated otherwise. The explanation of this 
again lies in the semantic classification of the 
lexicon, where ?pass? as a single word contains 
the sense T2- but not L1-. Consequently, the 
automatic score for ?pass away? with the sense 
                                                 
                                                
9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving 
possession), B2 (health and disease), F2 (drink), K6 (chil-
dren?s games and toys), M3 (land transport), M4 (swim-
ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-
ticipation), S2 (people), S3.2 (relationship), T3 (time: age), 
X1 (psychological actions), X5.2 (excited), Z4 (discourse 
bin) 
L1- is much lower (0.001) than that with the 
sense of T2- (0.007071). 
In order to evaluate our algorithm in compari-
son with previous work, we also tested it on the 
manual ranking list created by McCarthy et al
(2003).10 We found that 79 of the 116 phrasal 
verbs in that list are included in the Lancaster 
semantic lexicon. We applied our algorithm on 
those 79 items to compare the automatic ranks 
against the average manual ranks using the 
Spearman?s rank correlation coefficient (rho). As 
a result, we obtained rho=0.3544 with signifi-
cance level of p=0.001357. This result is compa-
rable with or better than most measures reported 
by McCarthy et al(2003). 
6 Discussion 
The algorithm we propose in this paper is dif-
ferent from previous proposed statistical methods 
in that it employs a semantic lexical resource in 
which the semantic field information is directly 
accessible for both MWEs and their constituent 
words. Often, typical statistical algorithms meas-
ure the semantic distance between MWEs and 
their constituent words by comparing their con-
texts comprising co-occurrence words in near 
context extracted from large corpora, such as 
Baldwin et als algorithm (2003). 
When we consider the definition of the com-
positionality as the extent to which the meaning 
of the MWE can be guessed based on that of its 
constituent words, a semantic lexical resource 
which maps MWEs and words to their semantic 
features provides a practical way of measuring 
the MWE compositionality. The Lancaster se-
mantic lexicon is one such lexical resource 
which allows us to have direct access to semantic 
field information of large number of MWE and 
single words. Our experiment demonstrates the 
potential value of such semantic lexical resources 
for the automatic measurement of MWE compo-
sitionality. Compared to statistical algorithms 
which can be affected by a variety of un-
controllable factors, such as size and domain of 
corpora, etc., an expert-compiled semantic lexi-
cal resource can provide much more reliable and 
?clean? lexical semantic information. 
However, we do not suggest that algorithms 
based on semantic lexical resources can substi-
tute corpus-based statistical algorithms. Rather, 
we suggest it as a complement to existing statis-
tical algorithms. As the errors of our algorithm 
 
10This list is available at website: 
http://mwe.stanford.edu/resources/  
8
reveal, the semantic information provided by the 
lexicon alone may not be rich enough for a very 
fine-grained distinction of MWE compositional-
ity. In order to obtain better results, this algo-
rithm needs to be combined with statistical tech-
niques. 
A limitation of our approach is language-
dependency. In order to port our algorithm to 
languages other than English, one needs to build 
similar semantic lexicon in those languages. 
However, similar semantic lexical resources are 
already under construction for some other lan-
guages, including Finnish and Russian (L?fberg 
et al, 2005; Sharoff et al, 2006), which will al-
low us to port our algorithm to those languages. 
7 Conclusion 
In this paper, we explored an algorithm based 
on a semantic lexicon for automatically measur-
ing the compositionality of MWEs. In our 
evaluation, the output of this algorithm showed 
moderate correlation with a manual ranking. We 
claim that semantic lexical resources provide 
another approach for automatically measuring 
MWE compositionality in addition to the exist-
ing statistical algorithms. Although our results 
are not yet conclusive due to the moderate scale 
of the test data, our evaluation demonstrates the 
potential of lexicon-based approaches for the 
task of compositional analysis. We foresee, by 
combining our approach with statistical algo-
rithms, that further improvement can be ex-
pected. 
8 Acknowledgement 
The work reported in this paper was carried 
out within the UK-EPSRC-funded ASSIST Pro-
ject (Ref. EP/C004574). 
References  
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, 
and Dominic Widdows. 2003. An Empirical Model 
of Multiword Expression Compositionality. In 
Proc. of the ACL-2003 Workshop on Multiword 
Expressions: Analysis, Acquisition and Treatment,  
pages 89-96, Sapporo, Japan. 
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proc. of the ACL2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pages 65?72, Sapporo. 
Nicoletta Calzolari, Charles Fillmore, Ralph Grish-
man, Nancy Ide, Alessandro Lenci, Catherine 
MacLeod, and Antonio Zampolli. 2002. Towards 
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the Third Interna-
tional Conference on Language Resources and 
Evaluation (LREC 2002), pages 1934?1940, Las 
Palmas, Canary Islands. 
Ann Copestake, Fabre Lambeau, Aline Villavicencio, 
Francis Bond, Timothy Baldwin, Ivan A. Sag, and 
Dan Flickinger. 2002. Multiword expressions: Lin-
guistic precision and reusability. In Proc. of the 
Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1941?
1947, Las Palmas, Canary Islands. 
Matthias  Gamer. 2005. The irr Package: Various Co-
efficients of Interrater Reliability and Agreement. 
Version 0.61 of 11 October 2005.  Available from:   
cran.r-project.org/src/contrib/Descriptions/irr.html 
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual 
Meeting of the ACL, pages 317?324, College Park, 
USA. 
Laura L?fberg, Scott Piao, Paul Rayson, Jukka-Pekka 
Juntunen, Asko Nyk?nen, and Krista Varantola. 
2005. A semantic tagger for the Finnish language. 
In Proc. of the Corpus Linguistics 2005 conference, 
Birmingham, UK. 
Christopher D. Manning and Hinrich Sch?tze. 2000. 
Foundations of Statistical Natural Language Proc-
essing. The MIT Press, Cambridge, Massachusetts. 
Diana McCarthy, Bill Keller, and John Carroll. 2003. 
Detecting a continuum of compositionality in 
phrasal verbs. In Proc. of the ACL-2003 Workshop 
on Multiword Expressions: Analysis, Acquisition 
and Treatment, pages 73?80, Sapporo, Japan. 
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing , Providence, USA. 
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 
1994. Idioms. Language, 70: 491?538. 
Scott S.L. Piao, Paul Rayson, Dawn Archer and Tony 
McEnery. 2004. Evaluating Lexical Resources for 
a Semantic Tagger. In Proc. of LREC-04, pages 
499?502, Lisbon, Portugal. 
Scott S.L. Piao, Dawn Archer, Olga Mudraya, Paul 
Rayson, Roger Garside, Tony McEnery and An-
drew Wilson. 2005a. A Large Semantic Lexicon 
for Corpus Annotation. In Proc. of the Corpus Lin-
guistics Conference 2005, Birmingham, UK. 
Scott S.L. Piao., Paul Rayson, Dawn Archer, Tony 
McEnery. 2005b. Comparing and combining a se-
mantic tagger and a statistical tool for MWE ex-
traction. Computer Speech and Language, 19, 4: 
378?397. 
9
Paul Rayson, Dawn Archer, Scott Piao, and Tony 
McEnery. 2004. The UCREL Semantic Analysis 
System. In Proc. of LREC-04 Workshop: Beyond 
Named Entity Recognition Semantic Labeling for 
NLP Tasks, pages 7?12, Lisbon, Portugal. 
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. the-
sis, Stanford University, Stanford. 
 Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann 
Copestake, and Dan Flickinger. 2002. Multiword 
Expressions: A Pain in the Neck for NLP. In Proc. 
of the 3rd International Conference on Intelligent 
Text Processing and Computational Linguistics 
(CICLing-2002), pages 1?15, Mexico City, Mexico. 
Neil J. Salkind. 2004. Statistics for People Who Hate 
Statistics. Sage: Thousand Oakes, US. 
Serge Sharoff, Bogdan Babych, Paul Rayson, Olga 
Mudraya and Scott Piao. 2006. ASSIST: Auto-
mated semantic assistance for translators. Proceed-
ings of EACL 2006, pages 139?142, Trento, Italy. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intra-
class Correlations: Uses in Assessing Rater Reli-
ability. Psychological Bulletin (2), 420?428. 
Sriram Venkatapathy and Aravind K. Joshi. 2005. 
Measuring the relative compositionality of verb-
noun (V-N) collocations by integrating features. In 
Proc. of Human Language Technology Conference 
and Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP 2005), pages 
899?906, Vancouver, Canada. 
 
Appendix: Manual vs. Automatic Ranks 
of Sample MWEs 
The table below shows the human and auto-
matic rankings of 89 sample MWEs. The MWEs 
are sorted in ascending order by manual average 
ranks. The top items are supposed to be the most 
compositional ones. For example, according to 
the manual ranking, facial expression is the most 
compositional MWE while tea leaf is the most 
opaque one. This table also shows that some 
MWEs are tied up with the same ranks. For the 
definitions of the full semantic tagset, see web-
site http://www.comp.lancs.ac.uk/ucrel/usas/. 
 
MWE Tag Sem tag Man 
rank 
Auto. 
rank 
facial expression B1 1 9 
aid worker S8/S2 2 4 
audio tape K3 3.5 4 
leisure activities K1 3.5 36.5 
advance warning T4/Q2.2 5 36.5 
living space H2 6 51 
in other words Z4 7 77.5 
unemployment fig-
ures 
I3.1/N5 8 4 
camera angle Q4.3 9.5 45 
pillow fight K6 9.5 64 
youth club S5/T3 11.5 4 
petrol station M3/H1 11.5 36.5 
palm tree L3 13 9 
rule book G2.1/Q4.1 14 4 
ball boy K5.1/S2.2 15 13 
goal keeper K5.1/S2 16.5 4 
kick in E3- 16.5 36.5 
ventilation shaft H2 18 47 
directory enquiries Q1.3 19 14 
phone box Q1.3/H1 21 18.5 
lose balance M1 21 53 
bend the rules A1.7 21 54.5 
big nose X7/X2.4 23 67 
quantity control N5/A1.7 24 11.5 
act of God S9 25 36.5 
air bag A15/M3 26 62.5 
mind stretching A12 27 59 
plain clothes B5 28 36.5 
keep up appearances A8/S1.1.1 29 86 
examining board P1 30 23 
open mind X6 31.5 49 
make an appearance S1.1.3+ 31.5 88 
cable television Q4.3 33 15 
king size N3.2 34 36.5 
action point X7 35 61 
keep tight rein on A1.7 36 28 
noughts and crosses K5.2 37 77.5 
tea leaf L3/F2 38 4 
single minded X5.1 39.5 77.5 
window dressing I2.2 39.5 77.5 
street girl G1.2/S5 42 36.5 
just over the horizon S3.2/S2.1 42 60 
pressure group T1.1.3 42 16.5 
air proof O4.1 44.5 57.5 
heart of gold S1.2.2 44.5 77.5 
lose heart X5.2 46 26 
food for thought X2.1/X5.1 47 89 
play part S8 48 68 
look down on S1.2.3 49 77.5 
arm twisting Q2.2 50 36.5 
take into account A1.8 51 69 
kidney bean F1 52 9 
come alive A3+ 53 52 
break new ground T3/T2 54 54 
make up to S1.1.2 55 65 
by virtue of C1 56.5 36.5 
snap shot A2.2 56.5 27 
pass away L1- 58 77.5 
long face E4.1 59 77.5 
bossy boots S1.2.3/S2 60 77.5 
plough into M1/A1.1.2 61 11.5 
kick in T2+ 62 50 
animal magnetism S1.2 63 55.5 
sixth former P1/S2 64 77.5 
pull the strings S7.1 65 62.5 
couch potato A1.1.1/S2 66 77.5 
think tank S5/X2.1 67 36.5 
come alive X5.2+ 68 24 
hot dog F1 69 77.5 
cheap shot G2.2-/Q2.2 70 66 
10
rock and roll K2 71 48 
bright as a button S3.2/T3/S2 72.5 87 
cradle snatcher X9.1+ 72.5 16.5 
alpha wave B1 74 77.5 
lollipop lady M3/S2 75 20 
pass away X5.2+ 76.5 57.5 
plough into T2- 76.5 36.5 
piece of cake P1 78.5 77.5 
sandwich course A12 78.5 21 
go bananas B2-/X1 80 22 
go bananas X5.2+++ 81.5 36.5 
go bananas E3- 81.5 25 
kick the bucket L1 83 77.5 
on the wagon F2 84 36.5 
Eskimo roll M4 85 18.5 
acid house K2 86 46 
plough into A9- 87 36.5 
Bloody Mary F2 88 36.5 
tea leaf G2.1-/S2mf 89 77.5 
 
11
