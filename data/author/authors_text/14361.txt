Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080?1089,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Latent-Variable Modeling of String Transductions
with Finite-State Methods?
Markus Dreyer and Jason R. Smith and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{markus,jsmith,jason}@cs.jhu.edu
Abstract
String-to-string transduction is a central prob-
lem in computational linguistics and natural
language processing. It occurs in tasks as di-
verse as name transliteration, spelling correc-
tion, pronunciation modeling and inflectional
morphology. We present a conditional log-
linear model for string-to-string transduction,
which employs overlapping features over la-
tent alignment sequences, and which learns la-
tent classes and latent string pair regions from
incomplete training data. We evaluate our ap-
proach on morphological tasks and demon-
strate that latent variables can dramatically
improve results, even when trained on small
data sets. On the task of generating mor-
phological forms, we outperform a baseline
method reducing the error rate by up to 48%.
On a lemmatization task, we reduce the error
rates in Wicentowski (2002) by 38?92%.
1 Introduction
A recurring problem in computational linguistics
and language processing is transduction of charac-
ter strings, e.g., words. That is, one wishes to model
some systematic mapping from an input string x to
an output string y. Applications include:
? phonology: underlying representation ? surface
representation
? orthography: pronunciation? spelling
? morphology: inflected form ? lemma, or differ-
ently inflected form
? fuzzy name matching (duplicate detection) and
spelling correction: spelling? variant spelling
?This work was supported by the Human Language Tech-
nology Center of Excellence and by National Science Founda-
tion grant No. 0347822 to the final author. We would also like
to thank Richard Wicentowski for providing us with datasets for
lemmatization, and the anonymous reviewers for their valuable
feedback.
? lexical translation (cognates, loanwords, translit-
erated names): English word? foreign word
We present a configurable and robust framework
for solving such word transduction problems. Our
results in morphology generation show that the pre-
sented approach improves upon the state of the art.
2 Model Structure
A weighted edit distance model (Ristad and Yian-
ilos, 1998) would consider each character in isola-
tion. To consider more context, we pursue a very
natural generalization. Given an input x, we evalu-
ate a candidate output y by moving a sliding window
over the aligned (x, y) pair. More precisely, since
many alignments are possible, we sum over all these
possibilities, evaluating each alignment separately.1
At each window position, we accumulate log-
probability based on the material that appears within
the current window. The window is a few charac-
ters wide, and successive window positions over-
lap. This stands in contrast to a competing approach
(Sherif and Kondrak, 2007; Zhao et al, 2007)
that is inspired by phrase-based machine translation
(Koehn et al, 2007), which segments the input string
into substrings that are transduced independently, ig-
noring context.2
1At the other extreme, Freitag and Khadivi (2007) use no
alignment; each feature takes its own view of how (x, y) relate.
2We feel that this independence is inappropriate. By anal-
ogy, it would be a poor idea for a language model to score a
string highly if it could be segmented into independently fre-
quent n-grams. Rather, language models use overlapping n-
grams (indeed, it is the language model that rescues phrase-
based MT from producing disjointed translations). We believe
phrase-based MT avoids overlapping phrases in the channel
model only because these would complicate the modeling of
reordering (though see, e.g., Schwenk et al (2007) and Casacu-
berta (2000)). But in the problems of section 1, letter reordering
is rare and we may assume it is local to a window.
1080
Figure 1: One of many possible alignment strings A for
the observed pair breaking/broke, enriched with latent
strings `1 and `2. Observed letters are shown in bold. The
box marks a trigram to be scored. See Fig. 2 for features
that fire on this trigram.
Joint n-gram models over the input and output di-
mensions have been used before, but not for mor-
phology, where we will apply them.3 Most notable
is the local log-linear grapheme-to-phoneme model
of Chen (2003), as well as generative models for
that task (Deligne et al (1995), Galescu and Allen
(2001), Bisani and Ney (2002)).
We advance that approach by adding new latent
dimensions to the (input, output) tuples (see Fig. 1).4
This enables us to use certain linguistically inspired
features and discover unannotated information. Our
features consider less or more than a literal n-gram.
On the one hand, we generalize with features that
abstract away from the n-gram window contents; on
the other, we specialize the n-gram with features that
make use of the added latent linguistic structure.
In section 5, we briefly sketch our framework for
concisely expressing and efficiently implementing
models of this form. Our framework uses familiar
log-linear techniques for stochastic modeling, and
weighted finite-state methods both for implementa-
tion and for specifying features. It appears general
enough to cover most prior work on word transduc-
tion. We imagine that it will be useful for future
work as well: one might easily add new, linguisti-
cally interesting classes of features, each class de-
fined by a regular expression.
2.1 Basic notation
We use an input alphabet ?x and output alphabet
?y. We conventionally use x ? ??x to denote the
input string and y ? ??y to denote the output string.
3Clark (2001) does use pair HMMs for morphology.
4Demberg et al (2007) similarly added extra dimensions.
However, their added dimensions were supervised, not latent,
and their model was a standard generative n-gram model whose
generalization was limited to standard n-gram smoothing.
There are many possible alignments between x
and y. We represent each as an alignment string
A ? ??
xy
, over an alignment alphabet of ordered
pairs, ?xy
def
= ((?x ? {})? (?y ? {}))? {(, )}.
For example, one alignment of x = breaking
with y = broke is the 9-character string A =
(b,b)(r,r)(e,o)(a, )(k,k)(,e)(i, )(n, )(g, ).
It is pictured in the first two lines of Fig. 1.
The remainder of Fig. 1 shows how we intro-
duce latent variables, by enriching the alignment
characters to be tuples rather than pairs. Let ?
def
=
(?xy ? ?`1 ? ?`2 ? ? ? ? ? ?`K ), where ?`i are al-
phabets used for the latent variables `i.
FSA and FST stand for ?finite-state acceptor? and
?finite-state transducer,? while WFSA and WFST
are their weighted variants. The ? symbol denotes
composition.
Let T be a relation and w a string. We write T [w]
to denote the image of w under T (i.e., range(w ?
T )), a set of 0 or more strings. Similarly, if W is a
weighted language (typically encoded by a WFSA),
we write W [w] to denote the weight of w in W .
Let pix ? ?? ? ??x denote the deterministic reg-
ular relation that projects an alignment string to its
corresponding input string, so that pix[A] = x. Sim-
ilarly, define piy ? ?? ? ??y so that piy[A] = y. Let
Axy be the set of alignment strings A compatible
with x and y; formally, Axy
def
= {A ? ?? : pix[A] =
x?piy[A] = y}. This set will range over all possible
alignments between x and y, and also all possible
configurations of the latent variables.
2.2 Log-linear modeling
We use a standard log-linear model whose features
are defined on alignment strings A ? Axy, allow-
ing them to be sensitive to the alignment of x and y.
Given a collection of features fi : ?? ? R with as-
sociated weights ?i ? R, the conditional likelihood
of the training data is
p?(y | x) =
?
A?Axy exp
?
i ?ifi(A)
?
y?
?
A?Axy?
exp
?
i ?ifi(A)
(1)
Given a parameter vector ?, we compute equa-
tion (1) using a finite-state machine. We define a
WFSA,U?, such thatU?[A] yields the unnormalized
probability u?(A)
def
= exp
?
i ?ifi(A) for any A ?
??. (See section 5 for the construction.) To obtain
1081
the numerator of equation (1), with its
?
A?Axy , we
sum over all paths in U? that are compatible with x
and y. That is, we build x ? pi?1x ? U? ? piy ? y and
sum over all paths. For the denominator we build the
larger machine x ? pi?1x ? U? and again compute the
pathsum. We use standard algorithms (Eisner, 2002)
to compute the pathsums as well as their gradients
with respect to ? for optimization (section 4.1).
Below, we will restrict our notion of valid align-
ment strings in ??. U? is constructed not to accept
invalid ones, thus assigning them probability 0.
Note that the possible output strings y? in the de-
nominator in equation (1) may have arbitrary length,
leading to an infinite summation over alignment
strings. Thus, for some values of ?, the sum in
the denominator diverges and the probability dis-
tribution is undefined. There exist principled ways
to avoid such ? during training. However, in our
current work, we simply restrict to finitely many
alignment strings (given x), by prohibiting as invalid
those with > k consecutive insertions (i.e., charac-
ters like (,a)).5 Finkel et al (2008) and others have
similarly bounded unary rule cycles in PCFGs.
2.3 Latent variables
The alignment between x and y is a latent ex-
planatory variable that helps model the distribution
p(y | x) but is not observed in training. Other latent
variables can also be useful. Morphophonological
changes are often sensitive to phonemes (whereas x
and y may consist of graphemes); syllable bound-
aries; a conjugation class; morpheme boundaries;
and the position of the change within the form.
Thus, as mentioned in section 2.1, we enrich the
alignment string A so that it specifies additional la-
tent variables to which features may wish to refer.
In Fig. 1, two latent strings are added, enabling the
features in Fig. 2(a)?(h). The first character is not
5We set k to a value between 1 and 3, depending on the tasks,
always ensuring that no input/output pairs observed in training
are excluded. The insertion restriction does slightly enlarge the
FSA U?: a state must keep track of the number of consecutive
 symbols in the immediately preceding x input, and for a few
states, this cannot be determined just from the immediately pre-
ceding (n ? 1)-gram. Despite this, we found empirically that
our approximation is at least as fast as the exact method of Eis-
ner (2002), who sums around cyclic subnetworks to numerical
convergence. Furthermore, our approximation does not require
us to detect divergence during training.
Figure 2: The boxes (a)-(h) represent some of the features
that fire on the trigram shown in Fig. 1. These features are
explained in detail in section 3.
just an input/output pair, but the 4-tuple (b,b,2,1).
Here, `1 indicates that this form pair (breaking /
broke) as a whole is in a particular cluster, or word
class, labeled with the arbitrary number 2. Notice in
Fig. 1 that the class 2 is visible in all local windows
throughout the string. It allows us to model how cer-
tain phenomena, e.g. the vowel change from ea to
o, are more likely in one class than in another. Form
pairs in the same class as the breaking / broke ex-
ample might include the following Germanic verbs:
speak, break, steal, tear, and bear.
Of course, word classes are latent (not labeled in
our training data). Given x and y, Axy will include
alignment strings that specify class 1, and others
that are identical except that they specify class 2;
equation (1) sums over both possibilities.6 In a valid
alignment stringA, `1 must be a constant string such
as 111... or 222..., as in Fig. 1, so that it spec-
ifies a single class for the entire form pair. See sec-
tions 4.2 and 4.3 for examples of what classes were
learned in our experiments.
The latent string `2 splits the string pair into num-
bered regions. In a valid alignment string, the re-
gion numbers must increase throughout `2, although
numbers may be skipped to permit omitted regions.
To guide the model to make a useful division into
regions, we also require that identity characters such
as (b,b) fall in even regions while change charac-
ters such as (e,o) (substitutions, deletions, or inser-
6The latent class is comparable to the latent variable on the
tree root symbol S in Matsuzaki et al (2005).
1082
tions) fall in odd regions.7 Region numbers must not
increase within a sequence of consecutive changes
or consecutive identities.8 In Fig. 1, the start of re-
gion 1 is triggered by e:o, the start of region 2 by
the identity k:k, region 3 by :e.
Allowing region numbers to be skipped makes it
possible to consistently assign similar labels to sim-
ilar regions across different training examples. Ta-
ble 2, for example, shows pairs that contain a vowel
change in the middle, some of which contain an ad-
ditional insertion of ge in the begining (verbinden
/ verbunden, reibt / gerieben). We expect the model
to learn to label the ge insertion with a 1 and vowel
change with a 3, skipping region 1 in the examples
where the ge insertion is not present (see section
4.2, Analysis).
In the next section we describe features over these
enriched alignment strings.
3 Features
One of the simplest ways of scoring a string is an n-
gram model. In our log-linear model (1), we include
ngram features fi(A), each of which counts the oc-
currences in A of a particular n-gram of alignment
characters. The log-linear framework lets us include
ngram features of different lengths, a form of back-
off smoothing (Wu and Khudanpur, 2000).
We use additional backoff features on alignment
strings to capture phonological, morphological, and
orthographic generalizations. Examples are found in
features (b)-(h) in Fig. 2. Feature (b) matches vowel
and consonant character classes in the input and
output dimensions. In the id/subst ngram feature,
we have a similar abstraction, where the character
classes ins, del, id, and subst are defined over in-
put/output pairs, to match insertions, deletions, iden-
tities (matches), and substitutions.
In string transduction tasks, it is helpful to in-
clude a language model of the target. While this
can be done by mixing the transduction model with
a separate language model, it is desirable to in-
clude a target language model within the transduc-
7This strict requirement means, perhaps unfortunately, that a
single region cannot accommodate the change ayc:xyz unless
the two y?s are not aligned to each other. It could be relaxed,
however, to a prior or an initialization or learning bias.
8The two boundary characters #, numbered 0 and max
(max=6 in our experiments), are neither changes nor identities.
tion model. We accomplish this by creating target
language model features, such as (c) and (g) from
Fig. 2, which ignore the input dimension. We also
have features which mirror features (a)-(d) but ig-
nore the latent classes and/or regions (e.g. features
(e)-(h)).
Notice that our choice of ? only permits mono-
tonic, 1-to-1 alignments, following Chen (2003).
We may nonetheless favor the 2-to-1 alignment
(ea,o) with bigram features such as (e,o)(a,). A
?collapsed? version of a feature will back off from
the specific alignment of the characters within a win-
dow: thus, (ea,o) is itself a feature. Currently, we
only include collapsed target language model fea-
tures. These ignore epsilons introduced by deletions
in the alignment, so that collapsed ok fires in a win-
dow that contains ok.
4 Experiments
We evaluate our model on two tasks of morphol-
ogy generation. Predicting morphological forms has
been shown to be useful for machine translation and
other tasks.9 Here we describe two sets of exper-
iments: an inflectional morphology task in which
models are trained to transduce verbs from one form
into another (section 4.2), and a lemmatization task
(section 4.3), in which any inflected verb is to be re-
duced to its root form.
4.1 Training and decoding
We train ? to maximize the regularized10 conditional
log-likelihood11
?
(x,y?)?C
log p?(y
? | x) + ||?||2/2?2, (2)
where C is a supervised training corpus. To max-
imize (2) during training, we apply the gradient-
based optimization method L-BFGS (Liu and No-
cedal, 1989).12
9E.g., Toutanova et al (2008) improve MT performance
by selecting correct morphological forms from a knowledge
source. We instead focus on generalizing from observed forms
and generating new forms (but see with rootlist in Table 3).
10The variance ?2 of the L2 prior is chosen by optimizing on
development data. We are also interested in trying an L1 prior.
11Alternatives would include faster error-driven methods
(perceptron, MIRA) and slower max-margin Markov networks.
12This worked a bit better than stochastic gradient descent.
1083
To decode a test example x, we wish to find
y? = argmaxy???y p?(y | x). Constructively, y? is the
highest-probability string in the WFSA T [x], where
T = pi?1x ?U??piy is the trained transducer that maps
x nondeterministically to y. Alas, it is NP-hard to
find the highest-probability string in a WFSA, even
an acyclic one (Casacuberta and Higuera, 2000).
The problem is that the probability of each string y
is a sum over many paths in T [x] that reflect differ-
ent alignments of y to x. Although it is straightfor-
ward to use a determinization construction (Mohri,
1997)13 to collapse these down to a single path per
y (so that y? is easily read off the single best path),
determinization can increase the WFSA?s size expo-
nentially. We approximate by pruning T [x] back to
its 1000-best paths before we determinize.14
Since the alignments, classes and regions are not
observed in C, we do not enjoy the convex objec-
tive function of fully-supervised log-linear models.
Training equation (2) therefore converges only to
some local maximum that depends on the starting
point in parameter space. To find a good starting
point we employ staged training, a technique in
which several models of ascending complexity are
trained consecutively. The parameters of each more
complex model are initialized with the trained pa-
rameters of the previous simpler model.
Our training is done in four stages. All weights
are initialized to zero. ? We first train only fea-
tures that fire on unigrams of alignment charac-
ters, ignoring features that examine the latent strings
or backed-off versions of the alignment characters
(such as vowel/consonant or target language model
features). The resulting model is equivalent to
weighted edit distance (Ristad and Yianilos, 1998).
? Next,15 we train all n-grams of alignment charac-
ters, including higher-order n-grams, but no backed-
off features or features that refer to latent strings.
13Weighted determinization is not always possible, but it is
in our case because our limit to k consecutive insertions guar-
antees that T [x] is acyclic.
14This value is high enough; we see no degradations in per-
formance if we use only 100 or even 10 best paths. Below that,
performance starts to drop slightly. In both of our tasks, our
conditional distributions are usually peaked: the 5 best output
candidates amass > 99% of the probability mass on average.
Entropy is reduced by latent classes and/or regions.
15When unclamping a feature at the start of stages ???, we
initialize it to a random value from [?0.01, 0.01].
13SIA. liebte, pickte, redete, rieb, trieb, zuzog
13SKE. liebe, picke, rede, reibe, treibe, zuziehe
2PIE. liebt, pickt, redet, reibt, treibt, zuzieht
13PKE.lieben, picken, reden, reiben, treiben, zuziehen
2PKE. abbrechet, entgegentretet, zuziehet
z. abzubrechen, entgegenzutreten, zuzuziehen
rP. redet, reibt, treibt, verbindet, u?berfischt
pA.geredet, gerieben, getrieben, verbunden, u?berfischt
Table 2: CELEX forms used in our experiments. Changes
from one form to the other are in bold (information not
given in training). The changes from rP to pA are very
complex. Note also the differing positions of zu in z.
? Next, we add backed-off features as well as all
collapsed features. ? Finally, we train all features.
In our experiments, we permitted latent classes 1?
2 and, where regions are used, regions 0?6. For
speed, stages ??? used a pruned ? that included
only ?plausible? alignment characters: a may not
align to b unless it did so in the trained stage-(1)
model?s optimal alignment of at least one training
pair (x, y?).
4.2 Inflectional morphology
We conducted several experiments on the CELEX
morphological database. We arbitrarily consid-
ered mapping the following German verb forms:16
13SIA ? 13SKE, 2PIE ? 13PKE, 2PKE ? z,
and rP ? pA.17 We refer to these tasks as 13SIA,
2PIE, 2PKE and rP. Table 2 shows some examples
of regular and irregular forms. Common phenomena
include stem changes (ei:ie), prefixes inserted af-
ter other morphemes (abzubrechen) and circumfixes
(gerieben).
We compile lists of form pairs from CELEX. For
each task, we sample 2500 data pairs without re-
placement, of which 500 are used for training, 1000
as development and the remaining 1000 as test data.
We train and evaluate models on this data and repeat
16From the available languages in CELEX (German, Dutch,
and English), we selected German as the language with the
most interesting morphological phenomena, leaving the mul-
tilingual comparison for the lemmatization task (section 4.3),
where there were previous results to compare with. The 4 Ger-
man datasets were picked arbitrarily.
17A key to these names: 13SIA=1st/3rd sg. ind. past;
13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.;
13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd pl. subjunct.
pres.; z=infinitive; rP=imperative pl.; pA=past part.
1084
Features Task
ng vc tlm tlm-coll id lat.cl. lat.reg. 13SIA 2PIE 2PKE rP
ngrams x 82.3 (.23) 88.6 (.11) 74.1 (.52) 70.1 (.66)
ngrams+x
x x 82.8 (.21) 88.9 (.11) 74.3 (.52) 70.0 (.68)
x x 82.0 (.23) 88.7 (.11) 74.8 (.50) 69.8 (.67)
x x x 82.5 (.22) 88.6 (.11) 74.9 (.50) 70.0 (.67)
x x x 81.2 (.24) 88.7 (.11) 74.5 (.50) 68.6 (.69)
x x x x 82.5 (.22) 88.8 (.11) 74.5 (.50) 69.2 (.69)
x x 82.4 (.22) 88.9 (.11) 74.8 (.51) 69.9 (.68)
x x x 83.0 (.21) 88.9 (.11) 74.9 (.50) 70.3 (.67)
x x x 82.2 (.22) 88.8 (.11) 74.8 (.50) 70.0 (.67)
x x x x 82.9 (.21) 88.6 (.11) 75.2 (.50) 69.7 (.68)
x x x x 81.9 (.23) 88.6 (.11) 74.4 (.51) 69.1 (.68)
x x x x x 82.8 (.21) 88.7 (.11) 74.7 (.50) 69.9 (.67)
ngrams+x
+latent
x x x x x x 84.8 (.19) 93.6 (.06) 75.7 (.48) 81.8 (.43)
x x x x x x 87.4 (.16) 93.8 (.06) 88.0 (.28) 83.7 (.42)
x x x x x x x 87.5 (.16) 93.4 (.07) 87.4 (.28) 84.9 (.39)
Moses3 73.9 (.40) 92.0 (.09) 67.1 (.70) 67.6 (.77)
Moses9 85.0 (.21) 94.0 (.06) 82.3 (.31) 70.8 (.67)
Moses15 85.3 (.21) 94.0 (.06) 82.8 (.30) 70.8 (.67)
Table 1: Exact-match accuracy and average edit distance (the latter in parentheses) versus the correct answer on the
German inflection task, using different combinations of feature classes. The label ngrams corresponds to the second
stage of training, ngrams+x to the third where backoff features may fire (vc = vowel/consonant, tlm = target LM, tlm-
coll = collapsed tlm, id = identity/substitution/deletion features), and ngrams+x+latent to the fourth where features
sensitive to latent classes and latent regions are allowed to fire. The highest n-gram order used is 3, except for Moses9
and Moses15 which examine windows of up to 9 and 15 characters, respectively. We mark in bold the best result for
each dataset, along with all results that are statistically indistinguishable (paired permutation test, p < 0.05).
the process 5 times. All results are averaged over
these 5 runs.
Table 1 and Fig. 3 report separate results after
stages ?, ?, and ? of training, which include suc-
cessively larger feature sets. These are respectively
labeled ngrams, ngrams+x, and ngrams+x+latent.
In Table 1, the last row in each section shows the
full feature set at that stage (cf. Fig. 3), while earlier
rows test feature subsets.18
Our baseline is the SMT toolkit Moses (Koehn et
al., 2007) run over letter strings rather than word
strings. It is trained (on the same data splits) to
find substring-to-substring phrase pairs and translate
from one form into another (with phrase reordering
turned off). Results reported as moses3 are obtained
from Moses runs that are constrained to the same
context windows that our models use, so the maxi-
mum phrase length and the order of the target lan-
guage model were set to 3. We also report results
using much larger windows, moses9 and moses15.
18The number k of consecutive insertions was set to 3.
Results. The results in Table 1 show that including
latent classes and/or regions improves the results
dramatically. Compare the last line in ngrams+x
to the last line in ngrams+x+latent. The accuracy
numbers improve from 82.8 to 87.5 (13SIA), from
88.7 to 93.4 (2PIE), from 74.7 to 87.4 (2PKE), and
from 69.9 to 84.9 (rP).19 This shows that error re-
ductions between 27% and 50% were reached. On
3 of 4 tasks, even our simplest ngrams method beats
the moses3 method that looks at the same amount of
context.20 With our full model, in particular using
latent features, we always outperform moses3?and
even outperform moses15 on 3 of the 4 datasets, re-
ducing the error rate by up to 48.3% (rP). On the
fourth task (2PIE), our method and moses15 are sta-
tistically tied. Moses15 has access to context win-
dows of five times the size than we allowed our
methods in our experiments.
19All claims in the text are statistically significant under a
paired permutation test (p < .05).
20This bears out our contention in footnote 2 that a ?segment-
ing? channel model is damaging. Moses cannot fully recover by
using overlapping windows in the language model.
1085
While the gains from backoff features in Table 1
were modest (significant gains only on 13SIA), the
learning curve in Fig. 3 suggests that they were help-
ful for smaller training sets on 2PKE (see ngrams vs
ngrams+x on 50 and 100) and helped consistently
over different amounts of training data for 13SIA.
Analysis. The types of errors that our system (and
the moses baseline) make differ from task to task.
Due to lack of space, we mainly focus on the com-
plex rP task. Here, most errors come from wrongly
copying the input to the output, without making a
change (40-50% of the errors in all models, except
for our model with latent classes and no regions,
where it accounts for only 30% of the errors). This
is so common because about half of the training ex-
amples contain identical inputs and outputs (as in
the imperative berechnet and the participle (ihr habt)
berechnet). Another common error is to wrongly as-
sume a regular conjugation (just insert the prefix ge-
at the beginning). Interestingly, this error by sim-
plification is more common in the Moses models
(44% of moses3 errors, down to 40% for moses15)
than in our models, where it accounts for 37% of
the errors of our ngrams model and only 19% if la-
tent classes or latent regions are used; however, it
goes up to 27% if both latent classes and regions
are used.21 All models for rP contain errors where
wrong analogies to observed words are made (ver-
schweisst/verschwissen in analogy to the observed
durchweicht/durchwichen, or bebt/geboben in anal-
ogy to hebt/gehoben). In the 2PKE task, most errors
result from inserting the zu morpheme at a wrong
place or inserting two of them, which is always
wrong. This error type was greatly reduced by la-
tent regions, which can discover different parame-
ters for different positions, making it easier to iden-
tify where to insert the zu.
Analysis of the 2 latent classes (when used) shows
that a split into regular and irregular conjugations
has been learned. For the rP task we compute,
for each data pair in development data, the poste-
rior probabilities of membership in one or the other
class. 98% of the regular forms, in which the past
participle is built with ge- . . . -t, fall into one class,
21We suspect that training of the models that use classes and
regions together was hurt by the increased non-convexity; an-
nealing or better initialization might help.
Figure 3: Learning curves for German inflection tasks,
13SIA (left) and 2PKE (right), as a function of the num-
ber of training pairs. ngrams+x means all backoff fea-
tures were used, ngrams+x+latent means all latent fea-
tures were used in addition. Moses15 examines windows
of up to 15 characters.
which in turn consists nearly exclusively (96%) of
these forms. Different irregular forms are lumped
into the other class.
The learned regions are consistent across different
pairs. On development data for the rP task, 94.3%
of all regions that are labeled 1 are the insertion se-
quence (,ge), region 3 consists of vowel changes
93.7% of the time; region 5 represents the typical
suffixes (t,en), (et,en), (t,n) (92.7%). In the
2PKE task, region 0 contains different prefixes (e.g.
entgegen in entgegenzutreten), regions 1 and 2 are
empty, region 3 contains the zu affix, region 4 the
stem, and region 5 contains the suffix.
The pruned alignment alphabet excluded a few
gold standard outputs so that the model contains
paths for 98.9%?99.9% of the test examples. We
verified that the insertion limit did not hurt oracle
accuracy.
4.3 Lemmatization
We apply our models to the task of lemmatization,
where the goal is to generate the lemma given an in-
flected word form. We compare our model to Wicen-
towski (2002, chapter 3), an alternative supervised
approach. Wicentowski?s Base model simply learns
how to replace an arbitrarily long suffix string of an
input word, choosing some previously observed suf-
fix? suffix replacement based on the input word?s
1086
Without rootlist (generation) With rootlist (selection)
Wicentowski (2002) This paper Wicentowski (2002) This paper
Lang. Base Af. WFA. n n+x n+x+l Base Af. WFA. n n+x n+x+l
Basque 85.3 81.2 80.1 91.0 (.20) 91.1 (.20) 93.6 (.14) 94.5 94.0 95.0 90.9 (.29) 90.8 (.31) 90.9 (.30)
English 91.0 94.7 93.1 92.4 (.09) 93.4 (.08) 96.9 (.05) 98.3 98.6 98.6 98.7 (.04) 98.7(.04) 98.7(.04)
Irish 43.3 - 70.8 96.8 (.07) 97.0 (.06) 97.8 (.04) 43.9 - 89.1 99.6 (.02) 99.6 (.02) 99.5 (.03)
Tagalog 0.3 80.3 81.7 80.5 (.32) 83.0 (.29) 88.6 (.19) 0.8 91.8 96.0 97.0 (.07) 97.2 (.07) 97.7 (.05)
Table 3: Exact-match accuracy and average edit distance (the latter in parentheses) on the 8 lemmatization tasks (2
tasks ? 4 languages). The numbers from Wicentowski (2002) are for his Base, Affix and WFAffix models. The
numbers for our models are for the feature sets ngrams, ngrams+x, ngrams+x+latent. The best result per task is in
bold (as are statistically indistinguishable results when we can do the comparison, i.e., for our own models). Corpus
sizes: Basque 5,842, English 4,915, Irish 1,376, Tagalog 9,479.
final n characters (interpolating across different val-
ues of n). His Affix model essentially applies the
Base model after stripping canonical prefixes and
suffixes (given by a user-supplied list) from the input
and output. Finally, his WFAffix uses similar meth-
ods to also learn substring replacements for a stem
vowel cluster and other linguistically significant re-
gions in the form (identified by a deterministic align-
ment and segmentation of training pairs). This ap-
proach is a bit like our change regions combined
with Moses?s region-independent phrase pairs.
We compare against all three models. Note that
Affix and WFAffix have an advantage that our mod-
els do not, namely, user-supplied lists of canonical
affixes for each language. It is interesting to see
how our models with their more non-committal tri-
gram structure compare to this. Table 3 reports re-
sults on the data sets used in Wicentowski (2002),
for Basque, English, Irish, and Tagalog. Follow-
ing Wicentowski, 10-fold cross-validation was used.
The columns n+x and n+x+l mean ngram+x and
ngram+x+latent, respectively. As latent variables,
we include 2 word classes but no change regions.22
For completeness, Table 3 also compares with Wi-
centowski (2002) on a selection (rather than genera-
tion) task. Here, at test time, the lemma is selected
from a candidate list of known lemmas, namely, all
the output forms that appeared in training data.23
These additional results are labeled with rootlist in
the right half of Table 3.
On the supervised generation task without rootlist,
22The insertion limit k was set to 2 for Basque and 1 for the
other languages.
23Though test data contained no (input, output) pairs from
training data, it reused many of the output forms, since many
inflected inputs are to be mapped to the same output lemma.
our models outperform Wicentowski (2002) by a
large margin. Comparing our results that use la-
tent classes (n+x+l) with Wicentowski?s best mod-
els we observe error reductions ranging from about
38% (Tagalog) to 92% (Irish). On the selection task
with rootlist, we outperform Wicentowski (2002) in
English, Irish, and Tagalog.
Analysis. We examined the classes learned on En-
glish lemmatization by our ngrams+x+latent model.
For each of the input/output pairs in development
data, we found the most probable latent class. For
the most part, the 2 classes are separated based on
whether or not the correct output ends in e. This
use of latent classes helped address many errors like
wronging / wronge or owed / ow). Such missing or
surplus final e?s account for 72.5% of the errors for
ngrams and 70.6% of the errors for ngrams+x, but
only 34.0% of the errors for ngrams+x+latent.
The test oracles are between 99.8% ? 99.9%, due
to the pruned alignment alphabet. As on the inflec-
tion task, the insertion limit does not exclude any
gold standard paths.
5 Finite-State Feature Implementation
We used the OpenFST library (Allauzen et al, 2007)
to implement all finite-state computations, using the
expectation semiring (Eisner, 2002) for training.
Our model is defined by the WFSA U?, which is
used to score alignment strings in ?? (section 2.2).
We now sketch how to construct U? from features.
n-gram construction The construction that we
currently use is quite simple. All of our current
features fire on windows of width ? 3. We build
a WFSA with the structure of a 3-gram language
1087
model over ??. Each of the |?|2 states remembers
two previous alignment characters ab of history; for
each c ? ?, it has an outgoing arc that accepts c (and
leads to state bc). The weight of this arc is the total
weight (from ?) of the small set of features that fire
when the trigram window includes abc. By conven-
tion, these also include features on bc and c (which
may be regarded as backoff features ?bc and ??c).
Since each character in ? is actually a 4-tuple, this
trigram machine is fairly large. We build it lazily
(?on the fly?), constructing arcs only as needed to
deal with training or test data.
Feature templates Our experiments use over
50,000 features. How do we specify these features
to the above construction? Rather than writing ordi-
nary code to extract features from a window, we find
it convenient to harness FSTs as a ?little language?
(Bentley, 1986) for specifying entire sets of features.
A feature template T is an nondeterministic FST
that maps the contents of the sliding window, such
as abc, to one or more features, which are also
described as strings.24 The n-gram machine de-
scribed above can compute T [((a?b)?c)?] to find
out what features fire on abc and its suffixes. One
simple feature template performs ?vowel/consonant
backoff?; e.g., it maps abc to the feature named
VCC. Fig. 2 showed the result of applying several
actual feature templates to the window shown in
Fig. 1. The extended regular expression calculus
provides a flexible and concise notation for writ-
ing down these FSTs. As a trivial example, the tri-
gram ?vowel/consonant backoff? transducer can be
described as T = V V V , where V is a transducer
that performs backoff on a single alignment charac-
ter. Feature templates should make it easy to experi-
ment with adding various kinds of linguistic knowl-
edge. We have additional algorithms for compiling
U? from a set of arbitrary feature templates,25 in-
cluding templates whose features consider windows
of variable or even unbounded width. The details are
beyond the scope of this paper, but it is worth point-
ing out that they exploit the fact that feature tem-
plates are FSTs and not arbitrary code.
24Formally, if i is a string naming a feature, then fi(A)
counts the number of positions in A that are immediately pre-
ceded by some string in T?1[i].
25Provided that the total number of features is finite.
6 Conclusions
The modeling framework we have presented here
is, we believe, an attractive solution to most string
transduction problems in NLP. Rather than learn the
topology of an arbitrary WFST, one specifies the
topology using a small set of feature templates, and
simply trains the weights.
We evaluated on two morphology generation
tasks. When inflecting German verbs we, even with
the simplest features, outperform the moses3 base-
line on 3 out of 4 tasks, which uses the same amount
of context as our models. Introducing more sophis-
ticated features that have access to latent classes and
regions improves our results dramatically, even on
small training data sizes. Using these we outper-
form moses9 and moses15, which use long context
windows, reducing error rates by up to 48%. On the
lemmatization task we were able to improve the re-
sults reported in Wicentowski (2002) on three out of
four tested languages and reduce the error rates by
38% to 92%. The model?s errors are often reason-
able misgeneralizations (e.g., assume regular con-
jugation where irregular would have been correct),
and it is able to use even a small number of latent
variables (including the latent alignment) to capture
useful linguistic properties.
In future work, we would like to identify a set of
features, latent variables, and training methods that
port well across languages and string-transduction
tasks. We would like to use features that look at
wide context on the input side, which is inexpen-
sive (Jiampojamarn et al, 2007). Latent variables
we wish to consider are an increased number of
word classes; more flexible regions?see Petrov et
al. (2007) on learning a state transition diagram for
acoustic regions in phone recognition?and phono-
logical features and syllable boundaries. Indeed, our
local log-linear features over several aligned latent
strings closely resemble the soft constraints used by
phonologists (Eisner, 1997). Finally, rather than de-
fine a fixed set of feature templates as in Fig. 2,
we would like to refine empirically useful features
during training, resulting in language-specific back-
off patterns and adaptively sized n-gram windows.
Many of these enhancements will increase the com-
putational burden, and we are interested in strategies
to mitigate this, including approximation methods.
1088
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proc. of CIAA, volume 4783.
Jon Bentley. 1986. Programming pearls [column]. Com-
munications of the ACM, 29(8), August.
Maximilian Bisani and Hermann Ney. 2002. Inves-
tigations on jointmultigram models for grapheme-to-
phoneme conversion.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications, volume 1891.
Francisco Casacuberta. 2000. Inference of finite-
state transducers by using regular grammars and mor-
phisms. In A.L. Oliveira, editor, Grammatical Infer-
ence: Algorithms and Applications, volume 1891.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proc. of Inter-
speech.
Alexander Clark. 2001. Learning morphology with Pair
Hidden Markov Models. In Proc. of the Student Work-
shop at the 39th Annual Meeting of the Association for
Computational Linguistics, Toulouse, France, July.
Sabine Deligne, Francois Yvon, and Fre?de?ric Bimbot.
1995. Variable-length sequence matching for phonetic
transcription using joint multigrams. In Eurospeech.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
In Proc. of ACL, Prague, Czech Republic, June.
Jason Eisner. 1997. Efficient generation in primitive Op-
timality Theory. In Proc. of ACL-EACL, Madrid, July.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. of ACL.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Dayne Freitag and Shahram Khadivi. 2007. A sequence
alignment model based on the averaged perceptron. In
Proc. of EMNLP-CoNLL.
Lucian Galescu and James F. Allen. 2001. Bi-directional
conversion between graphemes and phonemes using a
joint N-gram model.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proc. of NAACL-HLT, Rochester, New
York, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
of ACL, Companion Volume, Prague, Czech Republic,
June.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503?528.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. of ACL, Ann Arbor, Michigan, June.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2).
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learning
structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5).
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proc. of EMNLP-CoNLL, pages 430?438.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proc. of ACL, Prague, Czech
Republic, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
Columbus, Ohio, June.
Richard Wicentowski. 2002. Modeling and Learning
Multilingual Inflectional Morphology in a Minimally
Supervised Framework. Ph.D. thesis, Johns-Hopkins
University.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient training
methods for maximum entropy language modeling. In
Proc. of ICSLP, volume 3, Beijing, October.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Proc. of NAACL-HLT,
Rochester, New York, April.
1089
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403?411,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Parallel Sentences from Comparable Corpora using Document
Level Alignment
Jason R. Smith?
Center for Lang. and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
jsmith@cs.jhu.edu
Chris Quirk and Kristina Toutanova
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{chrisq,kristout}@microsoft.com
Abstract
The quality of a statistical machine transla-
tion (SMT) system is heavily dependent upon
the amount of parallel sentences used in train-
ing. In recent years, there have been several
approaches developed for obtaining parallel
sentences from non-parallel, or comparable
data, such as news articles published within
the same time period (Munteanu and Marcu,
2005), or web pages with a similar structure
(Resnik and Smith, 2003). One resource not
yet thoroughly explored is Wikipedia, an on-
line encyclopedia containing linked articles
in many languages. We advance the state
of the art in parallel sentence extraction by
modeling the document level alignment, mo-
tivated by the observation that parallel sen-
tence pairs are often found in close proximity.
We also include features which make use of
the additional annotation given by Wikipedia,
and features using an automatically induced
lexicon model. Results for both accuracy
in sentence extraction and downstream im-
provement in an SMT system are presented.
1 Introduction
For any statistical machine translation system, the
size of the parallel corpus used for training is a ma-
jor factor in its performance. For some language
pairs, such as Chinese-English and Arabic-English,
large amounts of parallel data are readily available,
but for most language pairs this is not the case. The
?This research was conducted during the author?s intern-
ship at Microsoft Research.
domain of the parallel corpus also strongly influ-
ences the quality of translations produced. Many
parallel corpora are taken from the news domain, or
from parliamentary proceedings. Translation qual-
ity suffers when a system is not trained on any data
from the domain it is tested on.
While parallel corpora may be scarce, compara-
ble, or semi-parallel corpora are readily available
in several domains and language pairs. These cor-
pora consist of a set of documents in two languages
containing similar information. (See Section 2.1
for a more detailed description of the types of non-
parallel corpora.) In most previous work on ex-
traction of parallel sentences from comparable cor-
pora, some coarse document-level similarity is used
to determine which document pairs contain paral-
lel sentences. For identifying similar web pages,
Resnik and Smith (2003) compare the HTML struc-
ture. Munteanu and Marcu (2005) use publication
date and vector-based similarity (after projecting
words through a bilingual dictionary) to identify
similar news articles.
Once promising document pairs are identified,
the next step is to extract parallel sentences. Usu-
ally, some seed parallel data is assumed to be avail-
able. This data is used to train a word align-
ment model, such as IBM Model 1 (Brown et al,
1993) or HMM-based word alignment (Vogel et al,
1996). Statistics from this word alignment model
are used to train a classifier which identifies bilin-
gual sentence pairs as parallel or not parallel. This
classifier is applied to all sentence pairs in docu-
ments which were found to be similar. Typically,
some pruning is done to reduce the number of sen-
403
tence pairs that need to be classified.
While these methods have been applied to news
corpora and web pages, very little attention has
been given to Wikipedia as a source of parallel sen-
tences. This is surprising, given that Wikipedia
contains annotated article alignments, and much
work has been done on extracting bilingual lexi-
cons on this dataset. Adafre and de Rijke (2006)
extracted similar sentences from Wikipedia article
pairs, but only evaluated precision on a small num-
ber of extracted sentences.
In this paper, we more thoroughly investigate
Wikipedia?s viability as a comparable corpus, and
describe novel methods for parallel sentence ex-
traction. Section 2 describes the multilingual re-
sources available in Wikipedia. Section 3 gives fur-
ther background on previous methods for parallel
sentence extraction on comparable corpora, and de-
scribes our approach, which finds a global sentence
alignment between two documents. In Section
4, we compare our approach with previous meth-
ods on datasets derived from Wikipedia for three
language pairs (Spanish-English, German-English,
and Bulgarian-English), and show improvements in
downstream SMT performance by adding the paral-
lel data we extracted.
2 Wikipedia as a Comparable Corpus
Wikipedia (Wikipedia, 2004) is an online collabo-
rative encyclopedia available in a wide variety of
languages. While the English Wikipedia is the
largest, with over 3 million articles, there are 24
language editions with at least 100,000 articles.
Articles on the same topic in different languages
are also connected via ?interwiki? links, which are
annotated by users. This is an extremely valuable
resource when extracting parallel sentences, as the
document alignment is already provided. Table
1 shows how many of these ?interwiki? links are
present between the English Wikipedia and the 16
largest non-English Wikipedias.
Wikipedia?s markup contains other useful indica-
tors for parallel sentence extraction. The many hy-
perlinks found in articles have previously been used
as a valuable source of information. (Adafre and
de Rijke, 2006) use matching hyperlinks to iden-
tify similar sentences. Two links match if the arti-
Figure 1: Captions for an image of a foil in English and
Spanish
cles they refer to are connected by an ?interwiki?
link. Also, images in Wikipedia are often stored
in a central source across different languages; this
allows identification of captions which may be par-
allel (see Figure 1). Finally, there are other minor
forms of markup which may be useful for finding
similar content across languages, such as lists and
section headings. In Section 3.3, we will explain
how features are derived from this markup.
2.1 Types of Non-Parallel Corpora
Fung and Cheung (2004) give a more fine-grained
description of the types of non-parallel corpora,
which we will briefly summarize. A noisy parallel
corpus has documents which contain many parallel
sentences in roughly the same order. Comparable
corpora contain topic aligned documents which are
not translations of each other. The corpora Fung
and Cheung (2004) examine are quasi-comparable:
they contain bilingual documents which are not
necessarily on the same topic.
Wikipedia is a special case, since the aligned
article pairs may range from being almost com-
pletely parallel (e.g., the Spanish and English en-
tries for ?Antiparticle?) to containing almost no par-
allel sentences (the Spanish and English entries for
?John Calvin?), despite being topic-aligned. It is
best characterized as a mix of noisy parallel and
comparable article pairs. Some Wikipedia authors
will translate articles from another language; others
404
French German Polish Italian Dutch Portuguese Spanish Japanese
496K 488K 384K 380K 357K 323K 311K 252K
Russian Swedish Finnish Chinese Norwegian Volapu?k Catalan Czech
232K 197K 146K 142K 141K 106K 103K 87K
Table 1: Number of aligned bilingual articles in Wikipedia by language (paired with English).
write the content themselves. Furthermore, even ar-
ticles created through translations may later diverge
due to independent edits in either language.
3 Models for Parallel Sentence Extraction
In this section, we will focus on methods for ex-
tracting parallel sentences from aligned, compara-
ble documents. The related problem of automatic
document alignment in news and web corpora has
been explored by a number of researchers, includ-
ing Resnik and Smith (2003), Munteanu and Marcu
(2005), Tillmann and Xu (2009), and Tillmann
(2009). Since our corpus already contains docu-
ment alignments, we sidestep this problem, and will
not discuss further details of this issue. That said,
we believe that our methods will be effective in cor-
pora without document alignments when combined
with one of the aforementioned algorithms.
3.1 Binary Classifiers and Rankers
Much of the previous work involves building a
binary classifier for sentence pairs to determine
whether or not they are parallel (Munteanu and
Marcu, 2005; Tillmann, 2009). The training data
usually comes from a standard parallel corpus.
There is a substantial class imbalance (O(n) pos-
itive examples, and O(n2) negative examples), and
various heuristics are used to mitigate this prob-
lem. Munteanu and Marcu (2005) filter out neg-
ative examples with high length difference or low
word overlap (based on a bilingual dictionary).
We propose an alternative approach: we learn
a ranking model, which, for each sentence in the
source document, selects either a sentence in the
target document that it is parallel to, or ?null?. This
formulation of the problem avoids the class imbal-
ance issue of the binary classifier.
In both the binary classifier approach and the
ranking approach, we use a Maximum Entropy
classifier, following Munteanu and Marcu (2005).
3.2 Sequence Models
In Wikipedia article pairs, it is common for par-
allel sentences to occur in clusters. A global sen-
tence alignment model is able to capture this phe-
nomenon. For both parallel and comparable cor-
pora, global sentence alignments have been used,
though the alignments were monotonic (Gale and
Church, 1991; Moore, 2002; Zhao and Vogel,
2002). Our model is a first order linear chain Condi-
tional Random Field (CRF) (Lafferty et al, 2001).
The set of source and target sentences are observed.
For each source sentence, we have a hidden vari-
able indicating the corresponding target sentence
to which it is aligned (or null). The model is simi-
lar to the discriminative CRF-based word alignment
model of (Blunsom and Cohn, 2006).
3.3 Features
Our features can be grouped into four categories.
Features derived from word alignments
We use a feature set inspired by (Munteanu and
Marcu, 2005), who defined features primarily based
on IBM Model 1 alignments (Brown et al, 1993).
We also use HMM word alignments (Vogel et al,
1996) in both directions (source to target and target
to source), and extract the following features based
on these four alignments:1
1. Log probability of the alignment
2. Number of aligned/unaligned words
3. Longest aligned/unaligned sequence of words
4. Number of words with fertility 1, 2, and 3+
We also define two more features which are in-
dependent of word alignment models. One is a
sentence length feature taken from (Moore, 2002),
1These are all derived from the one best alignment, and
normalized by sentence length.
405
which models the length ratio between the source
and target sentences with a Poisson distribution.
The other feature is the difference in relative doc-
ument position of the two sentences, capturing the
idea that the aligned articles have a similar topic
progression.
The above features are all defined on sentence
pairs, and are included in the binary classifier and
ranking model.
Distortion features
In the sequence model, we use additional dis-
tortion features, which only look at the difference
between the position of the previous and current
aligned sentences. One set of features bins these
distances; another looks at the absolute difference
between the expected position (one after the previ-
ous aligned sentence) and the actual position.
Features derived from Wikipedia markup
Three features are derived from Wikipedia?s
markup. The first is the number of matching links
in the sentence pair. The links are weighted by their
inverse frequency in the document, so a link that
appears often does not contribute much to this fea-
ture?s value. The image feature fires whenever two
sentences are captions of the same image, and the
list feature fires when two sentences are both items
in a list. These last two indicator features fire with
a negative value when the feature matches on one
sentence and not the other.
None of the above features fire on a null align-
ment, in either the ranker or CRF. There is also a
bias feature for these two models, which fires on all
non-null alignments.
Word-level induced lexicon features
A common problem with approaches for paral-
lel sentence classification, which rely heavily on
alignment models trained from unrelated corpora,
is low recall due to unknown words in the candi-
date sentence-pairs. One approach that begins to
address this problem is the use of self-training, as
in (Munteanu and Marcu, 2005). However, a self-
trained sentence pair extraction system is only able
to acquire new lexical items that occur in parallel
sentences. Within Wikipedia, many linked article
pairs do not contain any parallel sentences, yet con-
tain many words and phrases that are good transla-
tions of each other.
In this paper we explore an alternative approach
to lexicon acquisition for use in parallel sentence
extraction. We build a lexicon model using an ap-
proach similar to ones developed for unsupervised
lexicon induction from monolingual or compara-
ble corpora (Rapp, 1999; Koehn and Knight, 2002;
Haghighi et al, 2008). We briefly describe the lex-
icon model and its use in sentence-extraction.
The lexicon model is based on a probabilistic
modelP (wt|ws, T, S) wherewt is a word in the tar-
get language, ws is a word in the source language,
and T and S are linked articles in the target and
source languages, respectively.
We train this model similarly to the sentence-
extraction ranking model, with the difference that
we are aligning word pairs and not sentence pairs.
The model is trained from a small set of annotated
Wikipedia article pairs, where for some words in
the source language we have marked one or more
words as corresponding to the source word (in the
context of the article pair), or have indicated that the
source word does not have a corresponding transla-
tion in the target article. The word-level annotated
articles are disjoint from the sentence-aligned arti-
cles described in Section 4. The following features
are used in the lexicon model:
Translation probability. This is the translation
probability p(wt|ws) from the HMM word align-
ment model trained on the seed parallel data. We
also use the probability in the other direction, as
well as the log-probabilities in the two directions.
Position difference. This is the absolute value of
the difference in relative position of words ws and
wt in the articles S and T .
Orthographic similarity. This is a function of the
edit distance between source and target words. The
edit distance between words written in different al-
phabets is computed by first performing a determin-
istic phonetic translation of the words to a common
alphabet. The translation is inexact and this is a
promising area for improvement. A similar source
of information has been used to create seed lexicons
in (Koehn and Knight, 2002) and as part of the fea-
ture space in (Haghighi et al, 2008).
Context translation probability. This feature
looks at all words occurring next to word ws in the
406
article S and next to wt in the article T in a local
context window (we used one word to the left and
one word to the right), and computes several scor-
ing functions measuring the translation correspon-
dence between the contexts (using the IBM Model
1 trained from seed parallel data). This feature is
similar to distributional similarity measures used in
previous work, with the difference that it is limited
to contexts of words within a linked article pair.
Distributional similarity. This feature corre-
sponds more closely to context similarity measures
used in previous work on lexicon induction. For
each source headword ws, we collect a distribu-
tion over context positions o ? {?2,?1,+1,+2}
and context words vs in those positions based on a
count of times a context word occurred at that off-
set from a headword: P (o, vs|ws) ? weight(o) ?
C(ws, o, vs). Adjacent positions ?1 and +1 have
a weight of 2; other positions have a weight of 1.
Likewise we gather a distribution over target words
and contexts for each target headword P (o, vt|wt).
Using an IBM Model 1 word translation table
P (vt|vs) estimated on the seed parallel corpus,
we estimate a cross-lingual context distribution as
P (o, vt|ws) =
?
vs P (vt|vs) ? P (o, vs|ws). We de-
fine the similarity of a words ws and wt as one mi-
nus the Jensen-Shannon divergence of the distribu-
tions over positions and target words.2
Given this small set of feature functions, we
train the weights of a log-linear ranking model for
P (wt|ws, T, S), based on the word-level annotated
Wikipedia article pairs. After a model is trained,
we generate a new translation table Plex(t|s) which
is defined as Plex(t|s) ?
?
t?T,s?S P (t|s, T, S).
The summation is over occurrences of the source
and target word in linked Wikipedia articles. This
new translation table is used to define another
HMM word-alignment model (together with dis-
tortion probabilities trained from parallel data) for
use in the sentence extraction models. Two copies
of each feature using the HMM word alignment
model are generated: one using the seed data HMM
2We restrict our attention to words with ten or more occur-
rences, since rare words have poorly estimated distributions.
Also we discard the contribution from any context position and
word pair that relates to more than 1,000 distinct source or tar-
get words, since it explodes the computational overhead and
has little impact on the final similarity score.
model, and another using this new HMM model.
The training data for Bulgarian consisted of two
partially annotated Wikipedia article pairs. For
German and Spanish we used the feature weights
of the model trained on Bulgarian, because we did
not have word-level annotated Wikipedia articles.
4 Experiments
4.1 Data
We annotated twenty Wikipedia article pairs for
three language pairs: Spanish-English, Bulgarian-
English, and German-English. Each sentence
in the source language was annotated with pos-
sible parallel sentences in the target language
(the target language was English in all experi-
ments). The pairs were annotated with a quality
level: 1 if the sentences contained some parallel
fragments, 2 if the sentences were mostly paral-
lel with some missing words, and 3 if the sen-
tences appeared to be direct translations. In all
experiments, sentence pairs with quality 2 or 3
were taken as positive examples. The resulting
datasets are available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx.
For our seed parallel data, we used the Europarl
corpus (Koehn, 2005) for Spanish and German and
the JRC-Aquis corpus for Bulgarian, plus the article
titles for parallel Wikipedia documents, and trans-
lations available from Wiktionary entries.3
4.2 Intrinsic Evaluation
Using 5-fold cross-validation on the 20 document
pairs for each language condition, we compared the
binary classifier, ranker, and CRF models for paral-
lel sentence extraction. To tune for precision/recall,
we used minimum Bayes risk decoding. We define
the loss L(?, ?) of picking target sentence ? when
the correct target sentence is ? as 0 if ? = ?, ?
if ? = NULL and ? 6= NULL, and 1 otherwise.
By modifying the null loss ?, the precision/recall
trade-off can be adjusted. For the CRF model, we
used posterior decoding to make the minimum risk
decision rule tractable. As a summary measure of
the performance of the models at different levels of
recall we use average precision as defined in (Ido
3Wiktionary is an online collaborative dictionary, similar to
Wikipedia.
407
Language Pair Binary Classifier Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian 75.7 33.9 56.2 76.3 38.8 57.0 80.6 52.9 59.5
English-Spanish 90.4 81.3 87.6 93.4 81.0 84.5 94.7 87.6 90.2
English-German 61.8 9.4 27.5 66.4 25.7 42.4 78.9 52.2 54.7
Table 2: Average precision, recall at 90% precision, and recall at 80% precision for each model in all three language
pairs. In these experiments, the Wikipedia features and lexicon features are omitted.
Setting Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian
One Direction 76.3 38.8 57.0 80.6 52.9 59.5
Intersected 78.2 47.9 60.3 79.9 38.8 57.0
Intersected +Wiki 80.8 39.7 68.6 82.1 53.7 62.8
Intersected +Wiki +Lex 89.3 64.4 79.3 90.9 72.0 81.8
English-Spanish
One Direction 93.4 81.0 84.5 94.7 87.6 90.2
Intersected 94.3 82.4 89.0 95.4 88.5 91.8
Intersected +Wiki 94.5 82.4 89.0 95.6 89.2 92.7
Intersected +Wiki +Lex 95.8 87.4 91.1 96.4 90.4 93.7
English-German
One Direction 66.4 25.7 42.4 78.9 52.2 54.7
Intersected 71.9 36.2 43.8 80.9 54.0 67.0
Intersected +Wiki 74.0 38.8 45.3 82.4 56.9 71.0
Intersected +Wiki +Lex 78.7 46.4 59.1 83.9 58.7 68.8
Table 3: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all three
language pairs. ?+Wiki? indicates that Wikipedia features were used, and ?+Lex? means the lexicon features were
used.
et al, 2006). We also report recall at precision of
90 and 80 percent. Table 2 compares the different
models in all three language pairs.
In our next set of experiments, we looked at the
effects of the Wikipedia specific features. Since the
ranker and CRF are asymmetric models, we also
experimented with running the models in both di-
rections and combining their outputs by intersec-
tion. These results are shown in Table 3.
Identifying the agreement between two asym-
metric models is a commonly exploited trick else-
where in machine translation. It is mostly effec-
tive here as well, improving all cases except for
the Bulgarian-English CRF where the regression is
slight. More successful are the Wikipedia features,
which provide an auxiliary signal of potential par-
allelism.
The gains from adding the lexicon-based features
can be dramatic as in the case of Bulgarian (the
CRF model average precision increased by nearly
9 points). The lower gains on Spanish and German
may be due in part to the lack of language-specific
training data. These results are very promising and
motivate further exploration. We also note that this
is perhaps the first successful practical application
of an automatically induced word translation lexi-
con.
4.3 SMT Evaluation
We also present results in the context of a full ma-
chine translation system to evaluate the potential
utility of this data. A standard phrasal SMT sys-
tem (Koehn et al, 2003) serves as our testbed, us-
ing a conventional set of models: phrasal mod-
408
els of source given target and target given source;
lexical weighting models in both directions, lan-
guage model, word count, phrase count, distortion
penalty, and a lexicalized reordering model. Given
that the extracted Wikipedia data takes the standard
form of parallel sentences, it would be easy to ex-
ploit this same data in a number of systems.
For each language pair we explored two training
conditions. The ?Medium? data condition used eas-
ily downloadable corpora: Europarl for German-
English and Spanish-English, and JRC/Acquis for
Bulgarian-English. Additionally we included titles
of all linked Wikipedia articles as parallel sentences
in the medium data condition. The ?Large? data
condition includes all the medium data, and also in-
cludes using a broad range of available sources such
as data scraped from the web (Resnik and Smith,
2003), data from the United Nations, phrase books,
software documentation, and more.
In each condition, we explored the impact of in-
cluding additional parallel sentences automatically
extracted from Wikipedia in the system training
data. For German-English and Spanish-English,
we extracted data with the null loss adjusted to
achieve an estimated precision of 95 percent, and
for English-Bulgarian a precision of 90 percent. Ta-
ble 4 summarizes the characteristics of these data
sets. We were pleasantly surprised at the amount
of parallel sentences extracted from such a var-
ied comparable corpus. Apparently the average
Wikipedia article contains at least a handful of
parallel sentences, suggesting this is a very fertile
ground for training MT systems.
The extracted Wikipedia data is likely to make
the greatest impact on broad domain test sets ? in-
deed, initial experimentation showed little BLEU
gain on in-domain test sets such as Europarl, where
out-of-domain training data is unlikely to provide
appropriate phrasal translations. Therefore, we ex-
perimented with two broad domain test sets.
First, Bing Translator provided a sample of trans-
lation requests along with translations in German-
English and Spanish-English, which acted our stan-
dard development and test set. Unfortunately no
such tagged set was available in Bulgarian-English,
so we held out a portion of the large system?s train-
ing data to use for development and test. In each
language pair, the test set was split into a devel-
opment portion (?Dev A?) used for minimum error
rate training (Och, 2003) and a test set (?Test A?)
used for final evaluation.
Second, we created new test sets in each of
the three language pairs by sampling parallel sen-
tences from held out Wikipedia articles. To
ensure that this test data was clean, we man-
ually filtered the sentence pairs that were not
truly parallel and edited them as necessary to
improve adequacy. We called this ?Wikitest?.
This test set is available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx. Characteristics of these
test sets are summarized in Table 5.
We evaluated the resulting systems using BLEU-
4 (Papineni et al, 2002); the results are pre-
sented in Table 6. First we note that the extracted
Wikipedia data are very helpful in medium data
conditions, significantly improving translation per-
formance in all conditions. Furthermore we found
that the extracted Wikipedia sentences substantially
improved translation quality on held-out Wikipedia
articles. In every case, training on medium data
plus Wikipedia extracts led to equal or better trans-
lation quality than the large system alone. Further-
more, adding the Wikipedia data to the large data
condition still made substantial improvements.
5 Conclusions
Our first substantial contribution is to demonstrate
that Wikipedia is a useful resource for mining par-
allel data. The sheer volume of extracted parallel
sentences within Wikipedia is a somewhat surpris-
ing result in the light of Wikipedia?s construction.
We are also releasing several valuable resources to
the community to facilitate further research: man-
ually aligned document pairs, and an edited test
set. Hopefully this will encourage research into
Wikipedia as a resource for machine translation.
Secondly, we improve on prior pairwise mod-
els by introducing a ranking approach for sentence
pair extraction. This ranking approach sidesteps the
problematic class imbalance issue, resulting in im-
proved average precision while retaining simplicity
and clarity in the models.
Also by modeling the sentence alignment of the
articles globally, we were able to show a substan-
tial improvement in task accuracy. Furthermore a
409
German English Spanish English Bulgarian English
sentences 924,416 924,416 957,884 957,884 413,514 413,514
Medium types 351,411 320,597 272,139 247,465 115,756 69,002
tokens 11,556,988 11,751,138 18,229,085 17,184,070 10,207,565 10,422,415
sentences 6,693,568 6,693,568 7,727,256 7,727,256 1,459,900 1,459,900
Large types 1,050,832 875,041 1,024,793 952,161 239,076 137,227
tokens 100,456,622 96,035,475 155,626,085 137,559,844 29,741,936 29,889,020
sentences 1,694,595 1,694,595 1,914,978 1,914,978 146,465 146,465
Wiki types 578,371 525,617 569,518 498,765 107,690 74,389
tokens 21,991,377 23,290,765 29,859,332 28,270,223 1,455,458 1,516,231
Table 4: Statistics of the training data size in all three language pairs.
German English Spanish English Bulgarian English
Dev A sentences 2,000 2,000 2,000 2,000 2,000 2,000
tokens 16,367 16,903 24,571 21,493 39,796 40,503
Test A sentences 5,000 5,000 5,000 5,000 2,473 2,473
tokens 42,766 43,929 68,036 60,380 52,370 52,343
Wikitest sentences 500 500 500 500 516 516
tokens 8,235 9,176 10,446 9,701 7,300 7,701
Table 5: Statistics of the test data sets.
Language pair Training data Dev A Test A Wikitest
Spanish-English Medium 32.6 30.5 33.0
Medium+Wiki 36.7 (+4.1) 33.8 (+3.3) 39.1 (+6.1)
Large 39.2 37.4 38.9
Large+Wiki 39.5 (+0.3) 37.3 (-0.1) 41.1 (+2.2)
German-English Medium 28.7 26.6 13.0
Medium+Wiki 31.5 (+2.8) 29.6 (+3.0) 18.2 (+5.2)
Large 35.0 33.7 17.1
Large+Wiki 34.8 (-0.2) 33.9 (+0.2) 20.2 (+3.1)
Bulgarian-English Medium 36.9 26.0 27.8
Medium+Wiki 37.9 (+1.0) 27.6 (+1.6) 37.9 (+10.1)
Large 51.7 49.6 36.0
Large+Wiki 51.7(+0.0) 49.4 (-0.2) 39.5(+3.5)
Table 6: BLEU scores under various training and test conditions. The first column is from minimum error rate training;
the next two columns are on held-out test sets. For training data conditions including extracted Wikipedia sentences,
parenthesized values indicate absolute BLEU difference against the corresponding system without Wikipedia extracts.
small sample of annotated articles is sufficient to
train these global level features, and the learned
classifiers appear very portable across languages. It
is difficult to say whether such improvement will
carry over to other comparable corpora with less
document structure and meta-data. We plan to ad-
dress this question in future work.
Finally, initial investigations have shown that
substantial gains can be achieved by using an in-
duced word-level lexicon in combination with sen-
tence extraction. This helps address modeling word
pairs that are out-of-vocabulary with respect to the
seed parallel lexicon, while avoiding some of the
issues in bootstrapping.
410
References
S. F Adafre and M. de Rijke. 2006. Finding similar
sentences across multiple languages in wikipedia. In
Proceedings of EACL, pages 62?69.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL.
P. F Brown, V. J Della Pietra, S. A Della Pietra, and
R. L Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
P. Fung and P. Cheung. 2004. Multi-level bootstrap-
ping for extracting parallel sentences from a quasi-
comparable corpus. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 1051.
W. A Gale and K. W Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings
of the workshop on Speech and Natural Language,
pages 152?157.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Roy Bar-Haim Ido, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
the ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 127?133, Edmonton,
Canada, May.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit, volume 5.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Machine
Learning, pages 282?289.
R. C Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. Lecture Notes in Computer
Science, 2499:135?144.
D. S Munteanu and D. Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelpha, Pennsylva-
nia, USA.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of ACL.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
C. Tillmann and J. Xu. 2009. A simple sentence-level
extraction algorithm for comparable data. In Pro-
ceedings of HLT/NAACL, pages 93?96.
C. Tillmann. 2009. A Beam-Search extraction algo-
rithm for comparable data. In Proceedings of ACL,
pages 225?228.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841.
Wikipedia. 2004. Wikipedia, the free encyclopedia.
[Online; accessed 20-November-2009].
B. Zhao and S. Vogel. 2002. Adaptive parallel sentences
mining from web bilingual news collection. In Pro-
ceedings of the 2002 IEEE International Conference
on Data Mining, page 745. IEEE Computer Society.
411
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1374?1383,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Dirt Cheap Web-Scale Parallel Text from the Common Crawl
Jason R. Smith1,2
jsmith@cs.jhu.edu
Philipp Koehn3
pkoehn@inf.ed.ac.uk
Herve Saint-Amand3
herve@saintamh.org
Chris Callison-Burch1,2,5
ccb@cs.jhu.edu ?
Magdalena Plamada4
plamada@cl.uzh.ch
Adam Lopez1,2
alopez@cs.jhu.edu
1Department of Computer Science, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3School of Informatics, University of Edinburgh
4Institute of Computational Linguistics, University of Zurich
5Computer and Information Science Department, University of Pennsylvania
Abstract
Parallel text is the fuel that drives modern
machine translation systems. The Web is a
comprehensive source of preexisting par-
allel text, but crawling the entire web is
impossible for all but the largest compa-
nies. We bring web-scale parallel text to
the masses by mining the Common Crawl,
a public Web crawl hosted on Amazon?s
Elastic Cloud. Starting from nothing more
than a set of common two-letter language
codes, our open-source extension of the
STRAND algorithm mined 32 terabytes of
the crawl in just under a day, at a cost of
about $500. Our large-scale experiment
uncovers large amounts of parallel text in
dozens of language pairs across a variety
of domains and genres, some previously
unavailable in curated datasets. Even with
minimal cleaning and filtering, the result-
ing data boosts translation performance
across the board for five different language
pairs in the news domain, and on open do-
main test sets we see improvements of up
to 5 BLEU. We make our code and data
available for other researchers seeking to
mine this rich new data resource.1
1 Introduction
A key bottleneck in porting statistical machine
translation (SMT) technology to new languages
and domains is the lack of readily available paral-
lel corpora beyond curated datasets. For a handful
of language pairs, large amounts of parallel data
?This research was conducted while Chris Callison-
Burch was at Johns Hopkins University.
1github.com/jrs026/CommonCrawlMiner
are readily available, ordering in the hundreds of
millions of words for Chinese-English and Arabic-
English, and in tens of millions of words for many
European languages (Koehn, 2005). In each case,
much of this data consists of government and news
text. However, for most language pairs and do-
mains there is little to no curated parallel data
available. Hence discovery of parallel data is an
important first step for translation between most
of the world?s languages.
The Web is an important source of parallel
text. Many websites are available in multiple
languages, and unlike other potential sources?
such as multilingual news feeds (Munteanu and
Marcu, 2005) or Wikipedia (Smith et al, 2010)?
it is common to find document pairs that are di-
rect translations of one another. This natural par-
allelism simplifies the mining task, since few re-
sources or existing corpora are needed at the outset
to bootstrap the extraction process.
Parallel text mining from the Web was origi-
nally explored by individuals or small groups of
academic researchers using search engines (Nie
et al, 1999; Chen and Nie, 2000; Resnik, 1999;
Resnik and Smith, 2003). However, anything
more sophisticated generally requires direct access
to web-crawled documents themselves along with
the computing power to process them. For most
researchers, this is prohibitively expensive. As a
consequence, web-mined parallel text has become
the exclusive purview of large companies with the
computational resources to crawl, store, and pro-
cess the entire Web.
To put web-mined parallel text back in the
hands of individual researchers, we mine parallel
text from the Common Crawl, a regularly updated
81-terabyte snapshot of the public internet hosted
1374
on Amazon?s Elastic Cloud (EC2) service.2 Us-
ing the Common Crawl completely removes the
bottleneck of web crawling, and makes it possi-
ble to run algorithms on a substantial portion of
the web at very low cost. Starting from nothing
other than a set of language codes, our extension
of the STRAND algorithm (Resnik and Smith,
2003) identifies potentially parallel documents us-
ing cues from URLs and document content (?2).
We conduct an extensive empirical exploration of
the web-mined data, demonstrating coverage in
a wide variety of languages and domains (?3).
Even without extensive pre-processing, the data
improves translation performance on strong base-
line news translation systems in five different lan-
guage pairs (?4). On general domain and speech
translation tasks where test conditions substan-
tially differ from standard government and news
training text, web-mined training data improves
performance substantially, resulting in improve-
ments of up to 1.5 BLEU on standard test sets, and
5 BLEU on test sets outside of the news domain.
2 Mining the Common Crawl
The Common Crawl corpus is hosted on Ama-
zon?s Simple Storage Service (S3). It can be
downloaded to a local cluster, but the transfer cost
is prohibitive at roughly 10 cents per gigabyte,
making the total over $8000 for the full dataset.3
However, it is unnecessary to obtain a copy of the
data since it can be accessed freely from Amazon?s
Elastic Compute Cloud (EC2) or Elastic MapRe-
duce (EMR) services. In our pipeline, we per-
form the first step of identifying candidate docu-
ment pairs using Amazon EMR, download the re-
sulting document pairs, and perform the remain-
ing steps on our local cluster. We chose EMR be-
cause our candidate matching strategy fit naturally
into the Map-Reduce framework (Dean and Ghe-
mawat, 2004).
Our system is based on the STRAND algorithm
(Resnik and Smith, 2003):
1. Candidate pair selection: Retrieve candidate
document pairs from the CommonCrawl cor-
pus.
2. Structural Filtering:
(a) Convert the HTML of each document
2commoncrawl.org
3http://aws.amazon.com/s3/pricing/
into a sequence of start tags, end tags,
and text chunks.
(b) Align the linearized HTML of candidate
document pairs.
(c) Decide whether to accept or reject each
pair based on features of the alignment.
3. Segmentation: For each text chunk, perform
sentence and word segmentation.
4. Sentence Alignment: For each aligned pair of
text chunks, perform the sentence alignment
method of Gale and Church (1993).
5. Sentence Filtering: Remove sentences that
appear to be boilerplate.
Candidate Pair Selection We adopt a strategy
similar to that of Resnik and Smith (2003) for find-
ing candidate parallel documents, adapted to the
parallel architecture of Map-Reduce.
The mapper operates on each website entry in
the CommonCrawl data. It scans the URL string
for some indicator of its language. Specifically,
we check for:
1. Two/three letter language codes (ISO-639).
2. Language names in English and in the lan-
guage of origin.
If either is present in a URL and surrounded by
non-alphanumeric characters, the URL is identi-
fied as a potential match and the mapper outputs
a key value pair in which the key is the original
URL with the matching string replaced by *, and
the value is the original URL, language name, and
full HTML of the page. For example, if we en-
counter the URL www.website.com/fr/, we
output the following.
? Key: www.website.com/*/
? Value: www.website.com/fr/, French,
(full website entry)
The reducer then receives all websites mapped
to the same ?language independent? URL. If two
or more websites are associated with the same key,
the reducer will output all associated values, as
long as they are not in the same language, as de-
termined by the language identifier in the URL.
This URL-based matching is a simple and in-
expensive solution to the problem of finding can-
didate document pairs. The mapper will discard
1375
most, and neither the mapper nor the reducer do
anything with the HTML of the documents aside
from reading and writing them. This approach is
very simple and likely misses many good potential
candidates, but has the advantage that it requires
no information other than a set of language codes,
and runs in time roughly linear in the size of the
dataset.
Structural Filtering A major component of the
STRAND system is the alignment of HTML docu-
ments. This alignment is used to determine which
document pairs are actually parallel, and if they
are, to align pairs of text blocks within the docu-
ments.
The first step of structural filtering is to lin-
earize the HTML. This means converting its DOM
tree into a sequence of start tags, end tags, and
chunks of text. Some tags (those usually found
within text, such as ?font? and ?a?) are ignored
during this step. Next, the tag/chunk sequences
are aligned using dynamic programming. The ob-
jective of the alignment is to maximize the number
of matching items.
Given this alignment, Resnik and Smith (2003)
define a small set of features which indicate the
alignment quality. They annotated a set of docu-
ment pairs as parallel or non-parallel, and trained
a classifier on this data. We also annotated 101
Spanish-English document pairs in this way and
trained a maximum entropy classifier. However,
even when using the best performing subset of fea-
tures, the classifier only performed as well as a
naive classifier which labeled every document pair
as parallel, in both accuracy and F1. For this rea-
son, we excluded the classifier from our pipeline.
The strong performance of the naive baseline was
likely due to the unbalanced nature of the anno-
tated data? 80% of the document pairs that we
annotated were parallel.
Segmentation The text chunks from the previ-
ous step may contain several sentences, so before
the sentence alignment step we must perform sen-
tence segmentation. We use the Punkt sentence
splitter from NLTK (Loper and Bird, 2002) to
perform both sentence and word segmentation on
each text chunk.
Sentence Alignment For each aligned text
chunk pair, we perform sentence alignment using
the algorithm of Gale and Church (1993).
Sentence Filtering Since we do not perform any
boilerplate removal in earlier steps, there are many
sentence pairs produced by the pipeline which
contain menu items or other bits of text which are
not useful to an SMT system. We avoid perform-
ing any complex boilerplate removal and only re-
move segment pairs where either the source and
target text are identical, or where the source or
target segments appear more than once in the ex-
tracted corpus.
3 Analysis of the Common Crawl Data
We ran our algorithm on the 2009-2010 version
of the crawl, consisting of 32.3 terabytes of data.
Since the full dataset is hosted on EC2, the only
cost to us is CPU time charged by Amazon, which
came to a total of about $400, and data stor-
age/transfer costs for our output, which came to
roughly $100. For practical reasons we split the
run into seven subsets, on which the full algo-
rithm was run independently. This is different
from running a single Map-Reduce job over the
entire dataset, since websites in different subsets
of the data cannot be matched. However, since
the data is stored as it is crawled, it is likely that
matching websites will be found in the same split
of the data. Table 1 shows the amount of raw par-
allel data obtained for a large selection of language
pairs.
As far as we know, ours is the first system built
to mine parallel text from the Common Crawl.
Since the resource is new, we wanted to under-
stand the quantity, quality, and type of data that
we are likely to obtain from it. To this end, we
conducted a number of experiments to measure
these features. Since our mining heuristics are
very simple, these results can be construed as a
lower bound on what is actually possible.
3.1 Recall Estimates
Our first question is about recall: of all the pos-
sible parallel text that is actually available on the
Web, how much does our algorithm actually find
in the Common Crawl? Although this question
is difficult to answer precisely, we can estimate
an answer by comparing our mined URLs against
a large collection of previously mined URLs that
were found using targeted techniques: those in the
French-English Gigaword corpus (Callison-Burch
et al, 2011).
We found that 45% of the URL pairs would
1376
French German Spanish Russian Japanese Chinese
Segments 10.2M 7.50M 5.67M 3.58M 1.70M 1.42M
Source Tokens 128M 79.9M 71.5M 34.7M 9.91M 8.14M
Target Tokens 118M 87.5M 67.6M 36.7M 19.1M 14.8M
Arabic Bulgarian Czech Korean Tamil Urdu
Segments 1.21M 909K 848K 756K 116K 52.1K
Source Tokens 13.1M 8.48M 7.42M 6.56M 1.01M 734K
Target Tokens 13.5M 8.61M 8.20M 7.58M 996K 685K
Bengali Farsi Telugu Somali Kannada Pashto
Segments 59.9K 44.2K 50.6K 52.6K 34.5K 28.0K
Source Tokens 573K 477K 336K 318K 305K 208K
Target Tokens 537K 459K 358K 325K 297K 218K
Table 1: The amount of parallel data mined from CommonCrawl for each language paired with English.
Source tokens are counts of the foreign language tokens, and target tokens are counts of the English
language tokens.
have been discovered by our heuristics, though we
actually only find 3.6% of these URLs in our out-
put.4 If we had included ?f? and ?e? as identi-
fiers for French and English respectively, coverage
of the URL pairs would increase to 74%. How-
ever, we chose not to include single letter identi-
fiers in our experiments due to the high number of
false positives they generated in preliminary ex-
periments.
3.2 Precision Estimates
Since our algorithms rely on cues that are mostly
external to the contents of the extracted data
and have no knowledge of actual languages, we
wanted to evaluate the precision of our algorithm:
how much of the mined data actually consists of
parallel sentences?
To measure this, we conducted a manual anal-
ysis of 200 randomly selected sentence pairs for
each of three language pairs. The texts are het-
erogeneous, covering several topical domains like
tourism, advertising, technical specifications, fi-
nances, e-commerce and medicine. For German-
English, 78% of the extracted data represent per-
fect translations, 4% are paraphrases of each other
(convey a similar meaning, but cannot be used
for SMT training) and 18% represent misalign-
ments. Furthermore, 22% of the true positives
are potentially machine translations (judging by
the quality), whereas in 13% of the cases one of
the sentences contains additional content not ex-
4The difference is likely due to the coverage of the Com-
monCrawl corpus.
pressed in the other. As for the false positives,
13.5% of them have either the source or target
sentence in the wrong language, and the remain-
ing ones representing failures in the alignment
process. Across three languages, our inspection
revealed that around 80% of randomly sampled
data appeared to contain good translations (Table
2). Although this analysis suggests that language
identification and SMT output detection (Venu-
gopal et al, 2011) may be useful additions to the
pipeline, we regard this as reasonably high preci-
sion for our simple algorithm.
Language Precision
Spanish 82%
French 81%
German 78%
Table 2: Manual evaluation of precision (by sen-
tence pair) on the extracted parallel data for Span-
ish, French, and German (paired with English).
In addition to the manual evaluation of preci-
sion, we applied language identification to our
extracted parallel data for several additional lan-
guages. We used the ?langid.py? tool (Lui and
Baldwin, 2012) at the segment level, and report the
percentage of sentence pairs where both sentences
were recognized as the correct language. Table 3
shows our results. Comparing against our man-
ual evaluation from Table 2, it appears that many
sentence pairs are being incorrectly judged as non-
parallel. This is likely because language identifi-
cation tends to perform poorly on short segments.
1377
French German Spanish Arabic
63% 61% 58% 51%
Chinese Japanese Korean Czech
50% 48% 48% 47%
Russian Urdu Bengali Tamil
44% 31% 14% 12%
Kannada Telugu Kurdish
12% 6.3% 2.9%
Table 3: Automatic evaluation of precision
through language identification for several lan-
guages paired with English.
3.3 Domain Name and Topic Analysis
Although the above measures tell us something
about how well our algorithms perform in aggre-
gate for specific language pairs, we also wondered
about the actual contents of the data. A major
difficulty in applying SMT even on languages for
which we have significant quantities of parallel
text is that most of that parallel text is in the news
and government domains. When applied to other
genres, such systems are notoriously brittle. What
kind of genres are represented in the Common
Crawl data?
We first looked at the domain names which con-
tributed the most data. Table 4 gives the top five
domains by the number of tokens. The top two do-
main names are related to travel, and they account
for about 10% of the total data.
We also applied Latent Dirichlet Allocation
(LDA; Blei et al, 2003) to learn a distribution over
latent topics in the extracted data, as this is a pop-
ular exploratory data analysis method. In LDA
a topic is a unigram distribution over words, and
each document is modeled as a distribution over
topics. To create a set of documents from the ex-
tracted CommonCrawl data, we took the English
side of the extracted parallel segments for each
URL in the Spanish-English portion of the data.
This gave us a total of 444, 022 documents. In
our first experiment, we used the MALLET toolkit
(McCallum, 2002) to generate 20 topics, which
are shown in Table 5.
Some of the topics that LDA finds cor-
respond closely with specific domains,
such as topics 1 (blingee.com) and 2
(opensubtitles.org). Several of the topics
correspond to the travel domain. Foreign stop
words appear in a few of the topics. Since our sys-
tem does not include any language identification,
this is not surprising.5 However it does suggest an
avenue for possible improvement.
In our second LDA experiment, we compared
our extracted CommonCrawl data with Europarl.
We created a set of documents from both Com-
monCrawl and Europarl, and again used MAL-
LET to generate 100 topics for this data.6 We then
labeled each document by its most likely topic (as
determined by that topic?s mixture weights), and
counted the number of documents from Europarl
and CommonCrawl for which each topic was most
prominent. While this is very rough, it gives some
idea of where each topic is coming from. Table 6
shows a sample of these topics.
In addition to exploring topics in the datasets,
we also performed additional intrinsic evaluation
at the domain level, choosing top domains for
three language pairs. We specifically classified
sentence pairs as useful or boilerplate (Table 7).
Among our observations, we find that commer-
cial websites tend to contain less boilerplate ma-
terial than encyclopedic websites, and that the ra-
tios tend to be similar across languages in the same
domain.
FR ES DE
www.booking.com 52% 71% 52%
www.hotel.info 34% 44% -
memory-alpha.org 34% 25% 55%
Table 7: Percentage of useful (non-boilerplate)
sentences found by domain and language pair.
hotel.info was not found in our German-
English data.
4 Machine Translation Experiments
For our SMT experiments, we use the Moses
toolkit (Koehn et al, 2007). In these experiments,
a baseline system is trained on an existing parallel
corpus, and the experimental system is trained on
the baseline corpus plus the mined parallel data.
In all experiments we include the target side of the
mined parallel data in the language model, in order
to distinguish whether results are due to influences
from parallel or monolingual data.
5We used MALLET?s stop word removal, but that is only
for English.
6Documents were created from Europarl by taking
?SPEAKER? tags as document boundaries, giving us
208,431 documents total.
1378
Genre Domain Pages Segments Source Tokens Target Tokens
Total 444K 5.67M 71.5M 67.5M
travel www.booking.com 13.4K 424K 5.23M 5.14M
travel www.hotel.info 9.05K 156K 1.93M 2.13M
government www.fao.org 2.47K 60.4K 1.07M 896K
religious scriptures.lds.org 7.04K 47.2K 889K 960K
political www.amnesty.org 4.83K 38.1K 641K 548K
Table 4: The top five domains from the Spanish-English portion of the data. The domains are ranked by
the combined number of source and target tokens.
Index Most Likely Tokens
1 glitter graphics profile comments share love size girl friends happy blingee cute anime twilight sexy emo
2 subtitles online web users files rar movies prg akas dwls xvid dvdrip avi results download eng cd movie
3 miles hotels city search hotel home page list overview select tokyo discount destinations china japan
4 english language students details skype american university school languages words england british college
5 translation japanese english chinese dictionary french german spanish korean russian italian dutch
6 products services ni system power high software design technology control national applications industry
7 en de el instructions amd hyper riv saab kfreebsd poland user fr pln org wikimedia pl commons fran norway
8 information service travel services contact number time account card site credit company business terms
9 people time life day good years work make god give lot long world book today great year end things
10 show km map hotels de hotel beach spain san italy resort del mexico rome portugal home santa berlin la
11 rotary international world club korea foundation district business year global hong kong president ri
12 hotel reviews stay guest rooms service facilities room smoking submitted customers desk score united hour
13 free site blog views video download page google web nero internet http search news links category tv
14 casino game games play domaine ago days music online poker free video film sports golf live world tags bet
15 water food attribution health mango japan massage medical body baby natural yen commons traditional
16 file system windows server linux installation user files set debian version support program install type
17 united kingdom states america house london street park road city inn paris york st france home canada
18 km show map hotels hotel featured search station museum amsterdam airport centre home city rue germany
19 hotel room location staff good breakfast rooms friendly nice clean great excellent comfortable helpful
20 de la en le el hotel es het del und die il est der les des das du para
Table 5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely
tokens.
4.1 News Domain Translation
Our first set of experiments are based on systems
built for the 2012 Workshop on Statistical Ma-
chine Translation (WMT) (Callison-Burch et al,
2012) using all available parallel and monolingual
data for that task, aside from the French-English
Gigaword. In these experiments, we use 5-gram
language models when the target language is En-
glish or German, and 4-gram language models for
French and Spanish. We tune model weights using
minimum error rate training (MERT; Och, 2003)
on the WMT 2008 test data. The results are given
in Table 8. For all language pairs and both test
sets (WMT 2011 and WMT 2012), we show an
improvement of around 0.5 BLEU.
We also included the French-English Gigaword
in separate experiments given in Table 9, and Table
10 compares the sizes of the datasets used. These
results show that even on top of a different, larger
parallel corpus mined from the web, adding Com-
monCrawl data still yields an improvement.
4.2 Open Domain Translation
A substantial appeal of web-mined parallel data
is that it might be suitable to translation of do-
mains other than news, and our topic modeling
analysis (?3.3) suggested that this might indeed be
the case. We therefore performed an additional
set of experiments for Spanish-English, but we
include test sets from outside the news domain.
1379
Europarl CommonCrawl Most Likely Tokens
9 2975 hair body skin products water massage treatment natural oil weight acid plant
2 4383 river mountain tour park tours de day chile valley ski argentina national peru la
8 10377 ford mercury dealer lincoln amsterdam site call responsible affiliates displayed
7048 675 market services european competition small public companies sector internal
9159 1359 time president people fact make case problem clear good put made years situation
13053 849 commission council european parliament member president states mr agreement
1660 5611 international rights human amnesty government death police court number torture
1617 4577 education training people cultural school students culture young information
Table 6: A sample of topics along with the number of Europarl and CommonCrawl documents where
they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or
CommonCrawl, and some that are somewhat prominent in both.
WMT 11 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 30.46 29.96 30.79 32.41 16.12
+Web Data 30.92 30.51 31.05 32.89 16.74
WMT 12 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 29.25 27.92 32.80 32.83 16.61
+Web Data 29.82 28.22 33.39 33.41 17.30
Table 8: BLEU scores for several language pairs before and after adding the mined parallel data to
systems trained on data from WMT data.
WMT 11 FR-EN EN-FR
Baseline 30.96 30.69
+Web Data 31.24 31.17
WMT 12 FR-EN EN-FR
Baseline 29.88 28.50
+Web Data 30.08 28.76
Table 9: BLEU scores for French-English and
English-French before and after adding the mined
parallel data to systems trained on data from
WMT data including the French-English Giga-
word (Callison-Burch et al, 2011).
For these experiments, we also include training
data mined from Wikipedia using a simplified ver-
sion of the sentence aligner described by Smith
et al (2010), in order to determine how the ef-
fect of such data compares with the effect of web-
mined data. The baseline system was trained using
only the Europarl corpus (Koehn, 2005) as par-
allel data, and all experiments use the same lan-
guage model trained on the target sides of Eu-
roparl, the English side of all linked Spanish-
English Wikipedia articles, and the English side
of the mined CommonCrawl data. We use a 5-
gram language model and tune using MERT (Och,
Corpus EN-FR EN-ES EN-DE
News Commentary 2.99M 3.43M 3.39M
Europarl 50.3M 49.2M 47.9M
United Nations 316M 281M -
FR-EN Gigaword 668M - -
CommonCrawl 121M 68.8M 88.4M
Table 10: The size (in English tokens) of the train-
ing corpora used in the SMT experiments from Ta-
bles 8 and 9 for each language pair.
2003) on the WMT 2009 test set.
Unfortunately, it is difficult to obtain meaning-
ful results on some open domain test sets such as
the Wikipedia dataset used by Smith et al (2010).
Wikipedia copied across the public internet, and
we did not have a simple way to filter such data
from our mined datasets.
We therefore considered two tests that were
less likely to be problematic. The Tatoeba cor-
pus (Tiedemann, 2009) is a collection of example
sentences translated into many languages by vol-
unteers. The front page of tatoeba.org was
discovered by our URL matching heuristics, but
we excluded any sentence pairs that were found in
the CommonCrawl data from this test set.
1380
The second dataset is a set of crowdsourced
translation of Spanish speech transcriptions from
the Spanish Fisher corpus.7 As part of a re-
search effort on cross-lingual speech applications,
we obtained English translations of the data using
Amazon Mechanical Turk, following a protocol
similar to one described by Zaidan and Callison-
Burch (2011): we provided clear instructions,
employed several quality control measures, and
obtained redundant translations of the complete
dataset (Lopez et al, 2013). The advantage of
this data for our open domain translation test is
twofold. First, the Fisher dataset consists of con-
versations in various Spanish dialects on a wide
variety of prompted topics. Second, because we
obtained the translations ourselves, we could be
absolutely assured that they did not appear in some
form anywhere on the Web, making it an ideal
blind test.
WMT10 Tatoeba Fisher
Europarl 89/72/46/20 94/75/45/18 87/69/39/13
+Wiki 92/78/52/24 96/80/50/21 91/75/44/15
+Web 96/82/56/27 99/88/58/26 96/83/51/19
+Both 96/84/58/29 99/89/60/27 96/83/52/20
Table 11: n-gram coverage percentages (up to 4-
grams) of the source side of our test sets given our
different parallel training corpora computed at the
type level.
WMT10 Tatoeba Fisher
Europarl 27.21 36.13 46.32
+Wiki 28.03 37.82 49.34
+Web 28.50 41.07 51.13
+Both 28.74 41.12 52.23
Table 12: BLEU scores for Spanish-English be-
fore and after adding the mined parallel data to a
baseline Europarl system.
We used 1000 sentences from each of the
Tatoeba and Fisher datasets as test. For com-
parison, we also test on the WMT 2010 test
set (Callison-Burch et al, 2010). Following
Munteanu and Marcu (2005), we show the n-gram
coverage of each corpus (percentage of n-grams
from the test corpus which are also found in the
training corpora) in Table 11. Table 12 gives
end-to-end results, which show a strong improve-
ment on the WMT test set (1.5 BLEU), and larger
7Linguistic Data Consortium LDC2010T04.
improvements on Tatoeba and Fisher (almost 5
BLEU).
5 Discussion
Web-mined parallel texts have been an exclusive
resource of large companies for several years.
However, when web-mined parallel text is avail-
able to everyone at little or no cost, there will
be much greater potential for groundbreaking re-
search to come from all corners. With the advent
of public services such as Amazon Web Services
and the Common Crawl, this may soon be a re-
ality. As we have shown, it is possible to obtain
parallel text for many language pairs in a variety
of domains very cheaply and quickly, and in suf-
ficient quantity and quality to improve statistical
machine translation systems. However, our effort
has merely scratched the surface of what is pos-
sible with this resource. We will make our code
and data available so that others can build on these
results.
Because our system is so simple, we believe that
our results represent lower bounds on the gains
that should be expected in performance of systems
previously trained only on curated datasets. There
are many possible means through which the sys-
tem could be improved, including more sophisti-
cated techniques for identifying matching URLs,
better alignment, better language identification,
better filtering of data, and better exploitation of
resulting cross-domain datasets. Many of the com-
ponents of our pipeline were basic, leaving consid-
erable room for improvement. For example, the
URL matching strategy could easily be improved
for a given language pair by spending a little time
crafting regular expressions tailored to some ma-
jor websites. Callison-Burch et al (2011) gathered
almost 1 trillion tokens of French-English parallel
data this way. Another strategy for mining parallel
webpage pairs is to scan the HTML for links to the
same page in another language (Nie et al, 1999).
Other, more sophisticated techniques may also
be possible. Uszkoreit et al (2010), for ex-
ample, translated all non-English webpages into
English using an existing translation system and
used near-duplicate detection methods to find can-
didate parallel document pairs. Ture and Lin
(2012) had a similar approach for finding paral-
lel Wikipedia documents by using near-duplicate
detection, though they did not need to apply a full
translation system to all non-English documents.
1381
Instead, they represented documents in bag-of-
words vector space, and projected non-English
document vectors into the English vector space us-
ing the translation probabilities of a word align-
ment model. By comparison, one appeal of our
simple approach is that it requires only a table
of language codes. However, with this system
in place, we could obtain enough parallel data to
bootstrap these more sophisticated approaches.
It is also compelling to consider ways in which
web-mined data obtained from scratch could be
used to bootstrap other mining approaches. For
example, Smith et al (2010) mine parallel sen-
tences from comparable documents in Wikipedia,
demonstrating substantial gains on open domain
translation. However, their approach required seed
parallel data to learn models used in a classifier.
We imagine a two-step process, first obtaining par-
allel data from the web, followed by comparable
data from sources such as Wikipedia using mod-
els bootstrapped from the web-mined data. Such a
process could be used to build translation systems
for new language pairs in a very short period of
time, hence fulfilling one of the original promises
of SMT.
Acknowledgements
Thanks to Ann Irvine, Jonathan Weese, and our
anonymous reviewers from NAACL and ACL for
comments on previous drafts. The research lead-
ing to these results has received funding from the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 288487
(MosesCore). This research was partially funded
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence, and by
gifts from Google and Microsoft.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 17?53. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web text
mining for cross-language ir. In IN IN PROC. OF
RIAO, pages 62?77.
J. Dean and S. Ghemawat. 2004. Mapreduce: simpli-
fied data processing on large clusters. In Proceed-
ings of the 6th conference on Symposium on Opeart-
ing Systems Design & Implementation-Volume 6,
pages 10?10. USENIX Association.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Comput. Linguist., 19:75?102, March.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180. Association for Computa-
tional Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Edward Loper and Steven Bird. 2002. Nltk: the natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for
teaching natural language processing and computa-
tional linguistics - Volume 1, ETMTNLP ?02, pages
63?70. Association for Computational Linguistics.
Adam Lopez, Matt Post, and Chris Callison-Burch.
2013. Parallel speech, transcription, and translation:
The Fisher and Callhome Spanish-English speech
translation corpora. Technical Report 11, Johns
Hopkins University Human Language Technology
Center of Excellence.
Marco Lui and Timothy Baldwin. 2012. langid.py:
an off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
ACL ?12, pages 25?30. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
1382
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Comput. Linguist.,
31:477?504, December.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?99, pages 74?81, New York, NY,
USA. ACM.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In acl, pages 160?
167, Sapporo, Japan.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 527?534. As-
sociation for Computational Linguistics.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compara-
ble Corpora using Document Level Alignment. In
NAACL 2010.
Jo?rg Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
626?630, Montre?al, Canada, June. Association for
Computational Linguistics.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109. Association for Computational Linguistics.
Ashish Venugopal, Jakob Uszkoreit, David Talbot,
Franz J. Och, and Juri Ganitkevitch. 2011. Water-
marking the outputs of structured prediction with an
application in statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?11, pages
1363?1372. Association for Computational Linguis-
tics.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proc. of ACL.
1383
