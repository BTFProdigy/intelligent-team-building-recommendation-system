Subcategorization Acquisition and Evaluation for Chinese Verbs 
Xiwu Han, Tiejun Zhao, Haoliang Qi, Hao Yu 
Department of Computer Science,  
Harbin Institute of Technology, 150001 Harbin, China 
{hxw, tjzhao, qhl, yh}@mtlab.hit.edu.cn 
 
Abstract 
This paper describes the technology and an ex-
periment of subcategorization acquisition for 
Chinese verbs. The SCF hypotheses are gener-
ated by means of linguistic heuristic information 
and filtered via statistical methods. Evaluation 
on the acquisition of 20 multi-pattern verbs 
shows that our experiment achieved the similar 
precision and recall with former researches. Be-
sides, simple application of the acquired lexicon 
to a PCFG parser indicates great potentialities of 
subcategorization information in the fields of 
NLP. 
Credits 
This research is sponsored by National Natural 
Science Foundation (Grant No. 60373101 and 
603750 19), and High-Tech Research and Devel-
opment Program (Grant No. 2002AA117010-09). 
Introduction 
Since (Brent 1991) there have been a consider-
able amount of researches focusing on verb lexi-
cons with respective subcategorization informa-
tion specified both in the field of traditional lin-
guistics and that of computational linguistics. As 
for the former, subcategory theories illustrating 
the syntactic behaviors of verbal predicates are 
now much more systemically improved, e.g. 
(Korhonen 2001). And for auto-acquisition and 
relevant application, researchers have made great 
achievements not only in English, e.g. (Briscoe 
and Carroll 1997), (Korhonen 2003), but also in 
many other languages, such as Germany (Schulte 
im Walde 2002), Czech (Sarkar and Zeman 
2000), and Portuguese (Gamallo et. al 2002). 
However, relevant theoretical researches on 
Chinese verbs are generally limited to case gram-
mar, valency, some semantic computation theo-
ries, and a few papers on manual acquisition or 
prescriptive designment of syntactic patterns. 
Due to irrelevant initial motivations, syntactic 
and semantic generalizabilities of the consequent 
outputs are not in such a harmony that satisfies 
the description granularity for SCF (Han and 
Zhao 2004). The only auto-acquisition work for 
Chinese SCF made by (Han and Zhao 2004) de-
scribes the predefinition of 152 general frames 
for all verbs in Chinese, but that experiment is 
not based on real corpus. After observing and 
analyzing quantity of subcategory phenomena in 
real Chinese corpus in the People?s Daily 
(Jan.~June, 1998), we removed from Han & 
Zhao?s predefinition 15 SCFs that are actually 
similar derivants of others, and then with this 
foundation and linguistic rules from (Zhao 2002) 
as heuristic information we generated SCF hy-
potheses from the corpus of People?s Daily 
(Jan.~June, 1998), and statistically filtered the 
hypotheses into a Chinese verb SCF lexicon. As 
far as we know, this is the first attempt of Chi-
nese SCF auto-acquisition based on real corpus. 
In the rest of this paper, the second section de-
scribes a comprehensive system that builds verb 
SCF lexicons from large real corpus, the respec-
tive operating principles, and the knowledge 
coded in our SCF. The third section analyzed the 
acquired lexicon with two experiments: one 
evaluated the acquisition results of 20 verbs with 
multi syntactic patterns against manual gold 
standard; the other checked the performance of 
the lexicon when applied in a PCFG parser. The 
forth section compares and contrasts this research 
with related works done by others. And at last, 
Section 5 concludes our present achievements, 
disadvantages and possible future focuses. 
 
1   SCF Acquisition 
1.1 The Acquisition Method 
There are generally 4 steps in the process of our 
auto-acquisition experiment. First, the corpus is 
processed with a cascaded HMM parser; second, 
every possible local patterns for verbs are ab-
stracted; and then, the verb patterns are classified 
into SCF hypotheses according to the predefined 
set; at last, hypotheses are filtered statistically 
and the respective frequencies are also recorded. 
The actual application program consists of 6 
parts as shown in the following paragraphs. 
a. Segmenting and tagging: The raw cor-
pus is segmented into words and tagged 
with POS by the comprehensive seg-
menting and tagging processor devel-
oped by MTLAB of Computer 
Department in Harbin Institute of Tech-
nology. The advantage of the POS defi-
nition is that it describes some subsets of 
nouns and verbs in Chinese. 
b. Parsing: The tagged sentences are parsed 
with a cascaded HMM parser1, devel-
oped by MTLAB of HIT, but only the 
intermediate parsing results are used. 
The training set of the parser is 20,000 
sentences in the Chinese Tree Bank2 of 
(Zhao 2002). 
c. Error-driven correction: Some key errors 
occurring in the former two parts are 
corrected according to manually ob-
tained error-driven rules, which are gen-
erally about words or POS in the corpus. 
d. Pattern abstraction: Verbs with largest 
governing ranges are regarded as predi-
cates, then local patterns, previous 
phrases and respective syntactic tags are 
abstracted, and isolated parts are com-
bined, generalized or omitted according 
to basic phrase rules in (Zhao 2002). 
e. Hypothesis generation: Based on lin-
guistic restraining rules, e.g. no more 
than two NP?s occurring in a series and 
no more than three in one pattern, and 
no PP TP MP occurring with NP before 
any predicates (Han and Zhao 2004), the 
patterns are coordinated and classified 
into the predefined SCF groups. In this 
part, about 5% unclassifiable patterns 
are removed. 
                                                           
1 When evaluated on auto-tagged open corpus, the parser?s 
phrase precision if 62.3%, and phrase recall is 60.9% (Meng, 
2003). 
2 A sample of the tree bank or relevant introduction could be 
found at http://mtlab.hit.edu.cn. 
f. Hypothesis filtering: According to the 
statistical reliability of each type of the 
SCF hypotheses and the linguistic prin-
ciple that arguments occur more fre-
quently with predicates than adjuncts do, 
the hypotheses are filtered by means of 
statistical methods, in this paper which 
are binomial hypotheses testing (BHT) 
and maximum likelihood estimation 
(MLE). 
 
Table 1: An Example of Auto-acquisition 
 
No Actions Results 
a) Input ????????????
???????? 
b) Tag and 
parse 
BNP[BMP[?/m ?/q ]?/ng ]
? /p NDE[ ? ? ? /r ?
/usde ]BVP[ ? ? /vg ?
/vq ]BVP[ ? ? /vg ?
/ut ]NP[??/nc ?/usde ??
/ng ]?/wj 
c) Correct 
errors 
BNP[BMP[?/m ?/q ]?/ng ]
?/p NDE[???/r ?/usde ?
?/vg ?/vq ]BVP[??/vg ?
/LE ]NP[??/nc ?/usde ??
/ng ]?/wj 
d) Abstract 
patterns 
BNP PP BVP[vg LE ] NP 
e) Generate 
hypothesis
NP v NP ?01000? 
f) Filter hy-
potheses 
NP v NP {01111}3 
 
In Table 1, for example, when acquiring SCF 
information for ???? (prove) and a related sen-
tence in the corpus is a), our tagger and parser 
will return b), and error-driven correction will 
return c) with errors of NDE and the 1st BVP cor-
rected4. Since the governing range of ???? is 
larger than that of ???? (ask), the other verb in 
this sentence, the program abstracts its local pat-
tern BVP[vg LE] and previous phrase BNP, gen-
                                                           
3  {01000} projects to the Chinese syntactic mor-
phemes {?????????}, 1 means the SCF 
may occur with the respective morpheme, while 0 
may not (Han & Zhao, 2004). 
4 Note that not all errors in this example have been corrected, 
but this doesn?t affect further procession. Also, for defini-
tions of NDE and BVP see (Zhao, 2002). 
eralizes BNP and NDE as NP, combines the sec-
ond NP with isolated part ??/p? into PP, and 
returns d). Then the hypothesis generator returns 
e) as the possible SCF in which the verb may 
occurs. Actually in the corpus there are 621 hy-
pothesis tokens generated, and among them 92 
ones are of same arguments with e), and thus e) 
can pass the hypothesis testing (See also Section 
1.2), so we obtain one SCF for ???? as f). 
1.2 Filtering Methods 
In researches of subcategorization acquisition, 
statistical methods for hypothesis filtering mainly 
include the BHT, the Log Likelihood Ratio 
(LLR), the T-test and the MLE, and the most 
popular one is the BHT. Since (Brent 1993) be-
gan to use the method, most researchers have 
agreed that the BHT results in better precision 
and recall with SCF hypotheses of high, medium 
and low frequencies. Only (Korhonen 2001) re-
ports 11.9% total performance of the MLE better 
than the BHT. Therefore, we applied the two sta-
tistical methods in our present experiment. This 
subsection chiefly illustrates the expressions of 
our methods and definitions of parameters in 
them, while performance comparison of the two 
will be introduced in Section 3. 
When applying the BHT method, it is nec-
essary to determine the probability of the primi-
tive event. As for SCF acquisition, the co-
occurrence of one predefined SCF scfi with one 
verb v is the relevant primitive event, and the 
concerned probability is p(v|scfi) here. However, 
the aim of filtering is to rule out those unreliable 
hypotheses, so it is the probability that one primi-
tive event doesn't occur that is often used for 
SCF hypothesis testing, i.e. the error probability: 
pe(v|scfi) = 1 p(v|scfi). (Brent 1993) estimated pe 
according to the acquisition system?s perform-
ance, while (Briscoe and Carroll 1997) calculated 
pe from the distribution of SCF types in ANLT 
and SCF tokens in Susanne as shown in the fol-
lowing equation. 
 
Brent?s method mainly depends on the related 
corpus and processing program, which may 
cause intolerable errors. Briscoe and Carroll?s 
method draws on both linguistic and statistical 
information thus leading to comparatively stable 
estimation, and therefore has been used by many 
latter researches, e.g. (Korhonen 2001). But there 
is no MRD proper for Chinese SCF description 
so we estimated pe from the 1,775 common verbs 
and SCF tokens in the related corpus of 43,000 
sentences used by (Han and Zhao 2004). We 
formed the equation as follows: 
 
Then the number of all hypotheses about verb 
vj is recorded as n, and the number of those for 
scfi as m. According to Bernoulli theory, the 
probability P that an event with probability p ex-
actly happens m times out of n such trials is:  
 
And the probability that the event happens m or 
more times is: 
 
In turn, P(m+, n, pe) is the probability that scfi 
wrongly occurs m or more times with a verb that 
doesn't match it. Therefore, a threshold of 0.05 
on this probability will yield a 95% confidence 
that a high enough proportion of hypotheses for 
scfi have been observed for the verb legitimately 
to be assigned scfi (Korhonen 2001). 
The MLE method is closely related to the general 
performance of the concerned SCF acquisition 
system. First, we randomly draw from the ap-
plied corpus a training set, which is large enough 
so as to ensure similar SCF frequency distribu-
tion. Then, the frequency of scfi occurring with a 
verb vj is recorded and used to estimate the actual 
probability p(scfi| vj). Thirdly, an empirical 
threshold is determined, such that it ensures 
maximum value of F measure on the training set. 
Finally, the threshold is used to filter out those 
SCF hypotheses with low frequencies from the 
total set. 
2    Experimental Evaluation 
2.1   Acquisition Performance 
 
Using the previously described theory and tech-
nology we have acquired an SCF lexicon for 
3,558 common Chinese verbs from the corpus of 
People?s Daily (Jan.~June, 1998). In the lexicon 
the minimum number of SCF tokens for a verb is 
30, and the maximum is 20,000. In order to check 
the acquisition performance of the used system, 
we evaluated a part of the lexicon against a man-
ual gold standard. The testing set includes 20 
verbs of multi syntactic patterns, and for each 
verb there are 503~2,000 SCF tokens with the 
total number of 18,316 (See Table 2). Table 3 
gives the evaluation results for different filtering 
methods, including non-filtering 5 , BHT, and 
MLE with thresholds of 0.001, 0.005, 0.008 and 
0.01. We calculated the type precision and recall 
by the following expressions as (Korhonen 2001) 
did:  
 
In here, true positives are correct SCF types 
proposed by the system, false positives are incor-
rect SCF types proposed by system, and false 
negatives are correct SCF types not proposed by 
the system. 
 
Table 2: Verbs in the Testing Set6 
 
Verbs English Tokens Verbs English Tokens
? Read 503 ?? Hope 620 
?? Find 529 ? See 645 
?? Reckon 543 ?? Invest 679 
? Pull 544 ?? Know 722 
?? Report 612 ? Send 800 
?? Develop 1,006 ?? Set up 1,186
?? Behave 1,007 ?? Insist 1,200
?? Decide 1,038 ? Think 1,200
?? End 1,140 ?? Require 1,200
?? Begin 1142 ? Write 2,000
 
According to Table 3, all other filtering meth-
ods outperform non-filtering, and MLE is better 
than BHT. Among the four MLE thresholds, 
0.008 achieves the best comprehensive perform-
ance but its F-measure is only 0.74 larger than 
that of 0.01 while its precision drops by 2.4 per-
cent. Hence, we chose 0.01 as the threshold for 
the whole experiment with purpose to meet the 
practical requirement of high precision and to 
avoid possible over-fit phenomena. Finally, with 
a confidence of 95% we can estimate the general 
performance of the acquisition system with preci-
sion of  60.6% +/- 2.39%, and recall of 51.3%+/-
2.45%. 
                                                           
5 Non-filtering means filtering with a zero threshold or not 
filtering at all. This method is used as baseline here.  
6 The English meanings given here are not intended to cover 
the whole semantic range of the respective verbs, on the 
contrary they are just for readers? reference. 
 
Table 3: System Performance for Different 
 Filtering Methods 
 
          Measures
Methods Precision Recall F-measure
Non-filtering 37.43% 85.9% 52.14 
BHT 50% 57.2% 53.36 
0.001 39.2% 85.9% 53.83 
0.005 40.3% 83.33% 54.33 
0.008 58.2% 54.5% 56.3 MLE
0.01 60.6% 51.3% 55.56 
 
2.2    Task-oriented Evaluation 
In order to further analyze the practicability of 
the previously described technology, we per-
formed a simple task-oriented evaluation, apply-
ing the acquired SCF lexicon in a PCFG parser 
helping to choose from the n-best parsing results. 
The concerned parser was trained from 10,000 
manually parsed Chinese sentences7. In this ex-
periment there are 664 verbs and their SCF in-
formation involved. The open testing set consists 
of 1,500 sentences, for each of which the PCFG 
parser outputs 5-best parsing results. Then SCF 
hypotheses are generated for each result by 
means of the formerly mentioned technology. 
Finally, the maximum likelihood between hy-
potheses and those SCF types for the related verb 
in the lexicon is calculated in the following way:  
 
where i ? 5, hi is one of the hypotheses generated 
for the parsing results, and scfj is the jth SCF type 
for the concerned verb. This calculation keeps the 
likelihood between 0 and 1. The parsing result 
                                                           
7 These sentences and the testing corpus mentioned latter are 
all taken from the Chinese Tree Bank developed by MTLAB 
of HIT, and a sample may be downloaded at 
http://mtlab.hit.edu.cn. 
with maximum likelyhood is then regarded as the 
final choice. When two or more hypotheses hold 
the same likelihood, the one with larger or largest 
PCFG probability will be chosen.  
Table 4 shows the phrase-based and sentence-
based evaluation results for the parser without 
and with SCF heuristic information. There are 
three cased included: a) The output is one-best; b) 
The output is 5-best and the best evaluation result 
is recorded; c) The 5-best output is checked again 
for the best syntactic tree by means of SCF in-
formation. The phrased-based evaluation follows 
the popular method for evaluating a parser, while 
the sentence-based depends on the intersection of 
the parsed trees and those in the gold standard. 
Since the PCFG parser output at least one syntac-
tic tree for every sentence in our testing corpus, 
the sentence-based precision and recall are equal 
to each other. 
 
Table 4: Parsing Evaluation 
 
Phrase-based Sentence-
based 
Parsing  
Methods 
Precision Recall Precision  
= Recall 
One-best 57.5% 55% 13.64% 
5-best 65.28% 64.59% 26.2% 
With SCF 62.86% 62.1% 21.66% 
 
Table 4 shows that SCF information remarka-
bly improved the performance of the PCFG 
parser: the phrase-based precision increased by 
5.36% and recall by 7.1%, while the sentence-
based precision and recall both increased by 
8.04%. However, this doesn?t reach the upper 
limit of the 5-best. The possible reasons are: a) 
the our present SCF lexicon remains to be im-
proved; b) our method of applying SCF informa-
tion to the parser is too simple, e.g. probabilities 
of PCFG parsing results haven?t been exploited 
thoroughly. 
 
3 Related Works 
As far as we know, this is the first attempt to 
automatically acquire SCF information from real 
Chinese corpus and the first trial to apply SCF 
lexicon to a Chinese parser. Our research draws a 
lot on related works from international researches, 
and for the purpose of crosslingual processing, 
our research is kept in consistency with SCF 
conventions as much as possible. 
Due to linguistic differences, nevertheless, not 
all theories, methods or experiences could adapt 
to Chinese. Generally, there are four aspects that 
our research differs from those of other lan-
guages. First, the SCF formalization of most 
former researches follows the Levin style, in 
which most SCFs omit NP before predicates, 
while Chinese SCFs need to depict arguments 
occurring before verbs. Second, except (Sarkar 
and Zeman 2000), most former researches are 
based on manual SCF predefinition, while our 
predefined SCF set is statistically acquired (See 
Han and Zhao 2004). Third, involved parsers of 
former researches are mostly better than Chinese 
parsers to some degree. Forth, our SCF informa-
tion also includes 5 syntactic morphemes (See 
also Section 1.1). 
Meanwhile, the basic purpose for Chinese SCF 
acquisition is also to determine the subcategory 
features for a verb via its argument distributions 
and then apply the lexicon to NLP tasks. There-
fore, under similar cases the respective evalua-
tions are comparable. And Table 5 gives the 
comparison between our research and the best 
English results without semantic backoff 8  in 
(Korhonen 2001). 
 
Table 5: Performance Comparison Between 
Chinese and English Researches 
 
                Filtering 
Measures    Non BHT MLE
Ours 37.43% 50% 58.2%Precision Korhonen 24.3% 50.3% 74.8%
Ours 85.9% 57.2% 54.5%Recall Korhonen 83.5% 56.6% 57.8%
Ours 52.14 53.36 56.3F-
measure Korhonen 37.6 53.3 65.2
 
The comparison shows that our nonfiltering re-
sult is better than Korhonen?s, both BHT results 
are similar, while our MLE result is much worse 
                                                           
8 Semantic backoff is a method of generating SCF hypothe-
ses according to the semantic classification of the concerned 
verb. Note that this paper doesn?t involve verb meanings for 
generating hypotheses. Besides, though the evaluation for 
English SCF acquisition is the best, it?s not the newest. For 
the newest, please refer to (Korhonen 2003), in which the 
precision is 71.8% and recall is 34.5%. 
than Korhonen?s. That means our hypothesis 
generator performs well but our filtering method 
remains to be improved. According to the analy-
sis of relevant corpus, we found the main cause 
might be that low frequency SCF types account 
for 32% in our corpus while those in (Korhonen 
2001) sum to nearly 21%. 
Further more, (Briscoe and Carroll 1997) ap-
plied their acquired English SCF lexicon to an 
intermediate parser, and reported a 7% improve-
ment of both phrase-based precision and recall. 
Our application of SCF lexicon to a PCFG parser 
leads to 5.36% improvement for phrase-based 
precision, 7.1% for recall, and 8.04% for sen-
tence-based precision and recall. 
 
4    Conclusion 
 
This paper for the first time describes a largescale 
experiment of automatically acquiring SCF lexi-
con from real Chinese corpus. Perfor mance eva-
luation shows that our technology and acquiring 
program have achieved similar performance 
compared with former researches of other lan-
guages. And the application of the acquired lexi-
con to a PCFG parser indicates great potentiali-
ties of SCF information in the field of NLP. 
However, there is still a large gap between 
Chinese subcategorization works and those of o-
ther languages. Our future work will focus on the 
optimization of linguistic heuristic information 
and filtering methods, the application of semantic 
backoff, and the exploitation of SCF lexicon for 
other NLP tasks. 
References  
Brent, M. R. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association 
for Computational Linguistics, Berkeley, CA. 209-
214. 
Brent, M. 1993. From Grammar to Lexicon: un-
supervised learning of lexical syntax. Compu-
tational Linguistics 19.3. 243-262. 
Briscoe, Ted and John Carroll, 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied 
Natural Language Processing, Washington, DC. 
Dorr, B. J. Gina-Anne Levow, Dekang Lin, and Scott 
Thomas, 2000. Chinese-English Semantic Resource 
Construction, 2nd International Conference on 
Language Resources and Evaluation (LREC2000), 
Athens, Greece, pp. 757--760. 
Gamallo, P., Agustini, A. and Lopes Gabriel P., 2002. 
Using Co-Composition for Acquiring Syntactic 
and Semantic Subcategorisation, ACL-02.  
Han, Xiwu, Tiejun Zhao, 2004. FML-Based SCF Pre-
definition Learning for Chinese Verbs. Interna-
tional Joint Conference of NLP 2004. 
Jin, Guangjin, 2001. Semantic Computations for Mod-
ern Chinese Verbs. Beijing University Press, Bei-
jing. (in Chinese) 
Korhonen, Anna, 2001. Subcategorization Acquistion, 
Dissertation for Ph.D, Trinity Hall University of 
Cambridge. 29-77. 
Korhonen, Anna, 2003. Clustering Polysemic Sub-
categorization Frame Distributions Semantically. 
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pp. 64-71. 
Meng, Yao, 2003. Research on Global Chinese Pars-
ing Model and Algorithm Based on Maximum En-
tropy. Dissertation for Ph.D. Computer Department, 
HIT. 33-34. 
Sabine Shulte im Walde, 2002. Inducing German Se-
mantic Verb Classes from Purely Syntactic Sub-
categorization Information. Proceedings of the 40st 
ACL, pp. 223-230. 
Sarkar, A. and Zeman, D. 2000. Automatic Ex-
traction of Subcategorization Frames for 
Czech. In Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics, aarbrucken, Germany. 
Zhan Weidong, 2000. Valence Based Chinese Seman-
tic Dictionary, Language and Character Applica-
tions, Volume 1. (in Chinese) 
Zhao Tiejun, 2002. Knowledge Engineering Report 
for MTS2000.  
 
Coling 2010: Poster Volume, pages 701?709,
Beijing, August 2010
Reexamination on Potential for Personalization in Web Search 
Daren Li1  Muyun Yang1  Haoliang Qi2  Sheng Li1  Tiejun Zhao1 
 
1School of Computer Science 
Harbin Institute of Technology 
{drli|ymy|tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
 
2School of Computer Science 
Heilongjiang Institute of Technology 
haoliang.qi@gmail.com 
 
Abstract 
Various strategies have been proposed 
to enhance web search through utiliz-
ing individual user information. How-
ever, considering the well acknowl-
edged recurring queries and repetitive 
clicks among users, it is still an open 
issue whether using individual user in-
formation is a proper direction of ef-
forts in improving the web search. In 
this paper, we first quantitatively dem-
onstrate that individual user informa-
tion is more beneficial than common 
user information. Then we statistically 
compare the benefit of individual and 
common user information through 
Kappa statistic. Finally, we calculate 
potential for personalization to present 
an overview of what queries can bene-
fit more from individual user informa-
tion. All these analyses are conducted 
on both English AOL log and Chinese 
Sogou log, and a bilingual perspective 
statistics consistently confirms our 
findings. 
1 Introduction 
Most of traditional search engines are designed 
to return identical result to the same query 
even for different users. However, it has been 
found that majority of queries are quite ambi-
guous (Cronen-Townsend et al, 2002) as well 
as too short (Silverstein et al, 1999) to de-
scribe the exact informational needs of users. 
Different users may have completely different 
information needs under the same query (Jan-
sen et al, 2000). For example, when users is-
sue a query ?Java? to a search engine, their 
needs can be something ranging from a pro-
gramming language to a kind of coffee. 
In order to solve this problem, personalized 
search is proposed, which is a typical strategy 
of utilizing individual user information. Pitkow 
et al (2002) describe personalized search as 
the contextual computing approach which fo-
cuses on understanding the information con-
sumption patterns of each user, the various 
information foraging strategies and applica-
tions they employ, and the nature of the infor-
mation itself. After that, personalized search 
has gradually developed into one of the hot 
topics in information retrieval. As for various 
personalization models proposed recently, Dou 
et al (2007), however, reveal that they actually 
harms the results for certain queries while im-
proving others. This result based on a large-
scale experiment challenges not only the cur-
rent personalization methods but also the mo-
tivation to improve web search by the persona-
lized strategies. 
In addition, the studies on query logs rec-
orded by search engines consistently report the 
prevailing repeated query submissions by large 
number of users (Silverstein et al, 1999; Spink 
et al, 2001). It is reported that the 25 most fre-
quent queries from the AltaVista cover 1.5% 
of the total query submissions, despite being 
only 0.00000016% of unique queries (Silvers-
tein et al, 1999). As a result, the previous us-
ers? activities may serve as valuable informa-
tion, and technologies focusing on common 
701
user information, such as collaborative filter-
ing (or recommendation) may be a better reso-
lution to web search. Therefore, the justifica-
tion of utilizing individual user information 
deserves further discussion. 
To address this issue, this paper conducts a 
bilingual perspective of survey on two large-
scale query logs publically available: the AOL 
in English and the Sogou1 in Chinese. First we 
quantitatively investigate the evidences for 
exploiting common user information and indi-
vidual user information in these two logs. Af-
ter that we introduce Kappa statistic to meas-
ure the consistency of users? implicit relevance 
judgment inferred from clicks. It is tentatively 
revealed that using individual user information 
is what requires web search to face with after 
common user information is well exploited. 
Finally, we study the distribution of potential 
for personalization over the whole logs to gen-
erally disclose what kind of query deserves for 
individual user information. 
The remainder of this paper is structured as 
follows. Section 2 introduces previous me-
thods employing individual and common user 
information. In Section 3, we quantitatively 
compare the evidences for exploiting common 
user information and individual user informa-
tion. In Section 4, we introduce Kappa statistic 
to measure the consistency of users? clicks on 
the same query and try to statistically present 
the development direction of current web 
search. Section 5 figures out utilizing individu-
al user information as a research issue after 
well exploiting common user information. Sec-
tion 6 presents the potential for personalization 
curve, trying to outline which kind of queries 
benefit the most from individual user informa-
tion. Conclusions and future work are detailed 
in Section 7. 
2 Related Work 
With the rapid expansion of World Wide Web, 
it becomes more and more difficult to find re-
levant information through one-size-fits-all 
information retrieval service provided by clas-
sical search engines. Two kinds of user infor-
mation are mainly used to enhance search en-
                                                 
1 A famous Chinese search engine with a large number of 
Chinese web search users. 
gines: common user information and individu-
al user information. We separately review the 
previous works focusing on using these two 
kinds of information. 
Among various attempts to improve the per-
formance of search engine, collaborative web 
search is the one to take advantage of the repe-
tition of users? behaviors, which we call com-
mon user information. Since there is no unified 
definition on collaborative web search, in this 
paper, we believe that the collaborative web 
search assumes that community search activi-
ties can provide valuable search knowledge, 
and sharing this knowledge facilitates improv-
ing traditional search engine results (Smyth, 
2007). An important technique of collaborative 
web search is Collaborative Filtering (CF, also 
known as collaborative recommendation), in 
which, items are recommended to an active 
user based on historical co-occurrence data 
between users and items (Herlocker et al, 
1999). A number of researchers have explored 
algorithms for collaborative filtering and the 
algorithms can be categorized into two classes: 
memory-based CF and model-based CF. 
Memory-based CF methods apply a nearest-
neighbor-like scheme to predict a user?s rat-
ings based on the ratings given by like-minded 
users (Yu et al, 2004). The model-based ap-
proaches expand memory-based CF to build a 
descriptive model of group-based user prefe-
rences and use the model to predict the ratings. 
Examples of model-based approaches include 
clustering models (Kohrs et al, 1999) and as-
pect models (J. Canny, 2002). 
The other way to improve web search is per-
sonalized web search, focusing on learning the 
individual preferences instead of others? beha-
viors, which is called individual user informa-
tion. Early works learn user profiles from the 
explicit description of users to filter search re-
sults (Chirita et al, 2005). However, most of 
users are not willing to provide explicit feed-
back on search results and describe their inter-
ests (Carroll et al, 1987). Therefore, recent 
researches on the personalized search focus on 
modeling user preference from different types 
of implicit data, such as query history (Speretta 
et al, 2005), browsing history (Sugiyama et al, 
2004), clickthrough data (Sun et al, 2005), 
immediate search context (Shen et al, 2005) 
and other personal information (Teevan et al, 
702
2005). So far, there is still no proper compari-
son between the two solutions. It is still an 
open question which kind of information is 
more effective to build the web search model. 
Considering the difficulty in collecting pri-
vate information, using individual user infor-
mation seems less promising as the cost-
effective solution to web search. To address 
this issue, some researches about the value of 
personalization have been conducted. Teevan 
et al (2007) have done a ground breaking job 
to quantify the benefit for the search engines if 
search results were tailored to satisfy each user. 
The possible improvement by the personalized 
search, named potential for personalization, is 
measured by a gap between the relevance of 
individualized rankings and group ranking 
based on NDCG. However, it is less touched 
for the position of individual user information 
in contrast with common user information in 
large scale query log and how to balance the 
usage of common and individual information 
in information retrieval model. 
This paper tentatively examines individual 
user information against common user infor-
mation on two large-scale search engine logs 
in following aspects: the evidence from clicks 
on the same query, Kappa statistic for the 
whole queries, and overall distribution of que-
ries in terms of number of submissions and 
Kappa value. The bilingual statistics consis-
tently reveals the tendency of using individual 
user information as an equally important issue 
as (if not more than) using common user in-
formation) issue for researches on web search. 
3 Quantitative Evidences for Using 
Common or Individual User Infor-
mation 
To quantitatively investigate the value of 
common user information and individual user 
information in query log, we discriminate the 
evidence for using the two different types of 
user information as follows: 
(1) Evidence for using common user infor-
mation: if there were multiple users who have 
exactly the same click sets on one query, we 
suppose those clicks sets, together with the 
query, as the evidence for exploiting common 
user information. It is clear that such queries 
are able to be better responded with other?s 
search results. Note that common user infor-
mation is hard to be clearly defined, in order to 
simplify the quantitative statistics we give a 
strict definition. Further analysis will be shown 
in following sections. 
(2) Evidence for using individual user in-
formation: if a user?s click set on a query was 
not the same as any other?s, for that query, the 
search intent of the user who issue that query 
can be better inferred from his/her individual 
information than common user information. 
We suppose this kind of clicks, together with 
the related queries, as the evidence for exploit-
ing individual user information. 
Since users may have different search in-
tents when they issue the same query, a query 
can be an evidence for using both common and 
individual user information. In our statistics, if 
a query has both duplicate click sets and 
unique click set, the query is not only counted 
by the first category but also the second cate-
gory.  
The statistics of the two categories are con-
ducted in the query log of both English and 
Chinese search engines. We use a subset of 
AOL Query Log from March 1, 2006 to May 
31, 2006 and Sogou Query Log from March 1, 
2007 to March 31, 2007. The basic statistics of 
AOL and Sogou log are shown in Table 1. No-
tice that the queries in raw AOL and Sogou log 
without clicks are removed in this study. 
 
Item AOL Sogou 
#days 92 31 
#users 6,614,960 7,488,754 
#queries 7,840,348 8,019,229 
#unique queries 4,811,649 4,580,836 
#clicks 12,984,610 17,607,808 
Table 1: Basic statistics of AOL & Sogou log 
 
Table 2 summarizes the statistics of differ-
ent evidence categories over AOL and Sogou 
log. Note that click set refers to the set of 
clicks related to a query submission instead of 
a unique query. As for evidence for using 
common and individual user information, there 
is no clear distinction in terms of number of 
records, number of users in two logs. However, 
in terms of unique query and distinct click set, 
one can?t fail to find that evidence for using 
individual user information clearly exceeds  
703
Log 
The Condition Number 
Repeated queries Click Records User Unique Query 
Distinct 
Click Set 
AOL 
3,745,088 
(47.77% of total 
query submissions) 
Same 2,438,284 277,416 382,267 461,460 
Different 2,563,245 343,846 542,593 1,349,892 
Sogou 
4,252,167 
(53.02% of total 
query submissions) 
Same 2,469,363 1,380,951 228,315 358,346 
Different 5,481,832 1,545,817 752,047 2,171,872 
 
Table 2: Different click behaviors on repeated queries 
that for using common user information, espe-
cially in Sogou log. Therefore, though making 
use of common and individual user informa-
tion can address equally well for half users and 
half visits to the search engine, the fact that  
much more unique queries and click sets ac-
tually claims the significance of needing indi-
vidual user information to personalize web 
results. And methods exploiting individual us-
er information provide a much more challeng-
ing task in terms of problem space, though one 
may argue utilizing common user information 
is much easier to attack. 
4 Kappa Statistics for Individual and 
Common user information 
Section 3 has shown the evidence for using 
individual user information is prevailing than 
common user information in quantity for the 
unique queries in search engines. However, 
these counts deserve a further statistical cha-
racterization. In this section, we introduce 
Kappa statistic to depict the overall consisten-
cy of users? clicks in query logs. 
4.1 Kappa 
Kappa is a statistical measure introduced to 
access the agreement among different raters. 
There are two types of Kappa. One is Cohen?s 
Kappa (Cohen, 1960), which measures only 
the degree of agreement between two raters. 
The other is Fleiss?s Kappa (Fleiss, 1971), 
which generalizes Cohen?s Kappa to measure 
agreement among more than two raters, de-
noted as: 
e
e
P
PP
?
?=
1
?  
where, P is the probability that a randomly 
selected rater agree with another on a random-
ly selected subject. eP is the expected probabil-
ity of agreement if all raters made ratings by 
chance. If we use Kappa to measure the consis-
tency of relevance judgment by different raters, 
P can be interpreted as the probability that 
two random selected raters consistently rate a 
random selected search result as relevant or 
non-relevant one. Similarly, eP can also be 
construed as the expected probability of iden-
tical relevance judgment rated by different ra-
ters all by chance.  
Teevan et al (2008) used Fleiss?s Kappa to 
measure the inter-rater reliability of different 
raters? explicit relevance judgments. We ex-
pand their work and employ Fleiss?s Kappa to 
measure the consistency of implicit relevance 
judgments by users on the same query2. Here 
clicks are treated as a proxy for relevance: 
documents clicked by a user are judged as re-
levant and those not clicked as non-relevant 
(Teevan et al, 2008). As we all know that the 
result set of one query may change over time, 
so we select the longest time span to calculate 
Kappa value of a query, during which the re-
sult set of it preserves unchanged. From Kappa 
value of each query, we can statistically interp-
ret to which extent users share consistent intent 
on the same query according to Table 3 (Lan-
dis and Koch, 1977). Though the interpretation 
in Table 3 is not accepted with no doubt, it can 
give us an intuition about what extent of 
agreement consistency is. In other words, 
Kappa is a measure with statistical sense. 
Meanwhile, Kappa values of queries with  
                                                 
2 There may be more than two users who submitted the 
same query. 
704
  
                                       (a). AOL                                                                  (b). Sogou 
 
Figure 1: Number of unique queries and query submissions as a function of Kappa value. 
 
? Interpretation 
< 0 No agreement 
0.0 ? 0.20 Slight agreement 
0.21 ? 0.40 Fair agreement 
0.41 ? 0.60 Moderate agreement 
0.61 ? 0.80 Substantial agreement 
0.81 ? 1.00 Almost perfect agreement 
Table 3:  Kappa Interpretation 
 
various sizes of click sets are also comparable. 
That is also the reason we choose Kappa to 
measure consistency. 
4.2 Distribution of Kappa 
As introduced in Section 2, common user in-
formation is supposed to be the repetition of 
users? behaviors. We consider that the amount 
of repetition of users? clicks on one query is 
quantified by the consistency of its clicks. To 
statistically present the scale of repetition in 
current query log, we try to give an overview 
of consistency level of two commercial query 
logs. 
Figure 1 plots distribution of Kappa value of 
the two logs in the coordinate with logarithmic 
Y-axis. About 34.5% unique queries (44.0% 
query submissions) in AOL log and only 
13.9% unique queries (15.2% query submis-
sions) in Sogou log have high Kappa values 
above 0.6. According to Table 3, click sets of 
these queries can be regarded as somewhat 
consistent. These queries can be roughly re-
solved by using common user information. On 
the other hand, for the rest of queries which 
constitute majority of the logs, users? click sets 
are rather diversified, which are hard to be sa-
tisfied by returning the same result list to them. 
As a whole, the queries in both AOL and So-
gou can be characterized as less consistently in 
the clicks according to Kappa value, which is a 
statistical support for exploiting individual user 
information. 
5 Individual or Common user infor-
mation: A Tendency View 
The above analyses quantitative analyses have 
shown that the repetition of search is not the 
statistically dominant factor, with the impres-
sion that employing individual user informa-
tion is equally, if not more, important than 
common user information. This section tries to 
further reveal this issue so as to balance the 
position of individual user information and 
common user information from a research 
point. 
Intuitively, a query can be characterized by 
the number of people issuing it, i.e. query fre-
quency if we remove the resubmissions of one 
query by the same people. We try to depict the 
above mentioned query submissions and Kap-
pa values as a function of number of people 
who issue the queries in Figure 2. In Figure 2, 
different numbers of users who issue the same 
query are shown on the x-axis, and the y-axis 
represents the number of different entities (left 
scale) and the average Kappa value (right scale) 
of the queries. We find that the number of que-
ries becomes very small when the number of 
users in a group grows over 10, so we set a 
variant step length for them: with the length 
step of the group size falling between 2 and 10 
set as 1, between 11 and 100 as 10, between 
101 and 1000 as 100 and above 1000 as 1000. 
705
   
 
                                     (a). AOL                                                                      (b). Sogou 
 
Figure 2: Average Kappa value of queries as a function of number of people in a group who issue 
the same query (line) and the number of submissions of the queries issued by the same size of 
group (dark columns). 
 
According to Figure 2(a), Kappa values of 
the queries in AOL log with more than 20 us-
ers are above 0.6, which indicates rather con-
sistent clicks for them, accounting for about  
29.4% of all query submissions. While for 
those queries visited by less than 20 users, the 
Kappa value declines gradually from 0.6 with 
the drop of users. For these queries occupying 
majority of query submissions, exploiting in-
dividual user information is supposed to be a 
better solution since the clicks on them are ra-
ther individualized. 
According to Figure 2(b), though Kappa 
values of queries increase similarly with 
people submitting them in AOL, the overall 
consistency of the queries in Sogou log is 
much lower: with a Kappa value below 0.6 
even for the queries visited by a large number 
of users. This fact indicates that Chinese users 
may be less consistent in their search intents, 
or partially reflects that the Chinese as a non-
inflection language has more ambiguity, which 
can also be implied from Table 2. Therefore, 
individual user information may be more ef-
fective than common user information in So-
gou log. 
Summarized from Figure 2, it is sensible 
that common user information is appropriate 
for the queries in the right-most of X-axis. 
With most number of visiting people, such 
queries bear rather consistent clicks though 
covering only a small proportion of the distinct 
query set. Moving from the right to the left, we 
can find the majority of queries yield a less 
Kappa value, for which the individualized 
clicks require individual user information to 
meet the needs of each user. In this sense, how 
to exploit individual user information is pre-
destined as the next issue of information re-
trieval if common user information was to be 
well utilized. 
6 Queries for Personalization 
Since using individual user information is a 
non-negligible issue in IR research, a subse-
quent issue is what queries can benefit in what 
extent from individual user information. In this 
section, we try to give an overview for this 
issue via a measure named potential for perso-
nalization. 
6.1 Potential for Personalization 
Potential for personalization proposed by Tee-
van et al (2007) is used to measure the norma-
lized Discounted Cumulative Gain (NDCG) 
improvement between the best ranking of the 
results to a group and individuals. NDCG is a 
well-known measure of the quality of a search 
result (J?rvelin and Kek?l?inen, 2000). 
The best ranking of the results to a group is 
the ranking with highest NDCG based on re-
levance judgments of the users in the group. 
For the queries with explicit judgments, the 
best ranking can be generated as follows: re-
sults that all raters thought were relevant are 
ranked first, followed by those that most 
people thought were relevant but a few people 
thought were irrelevant, until the  results most 
people though were irrelevant. In other word,  
706
  
 
(a)  AOL                                                                    (b)   Sogou 
 
Figure 3: Number of unique queries and query submissions as a function of potential for 
personalization 
 
   
 
(a)  AOL                                                                       (b) Sogou 
 
Figure 4: The average NDCG of group best ranking as a function of number of people in group 
(solid line), combining with the distribution of  the number of unique queries issued by the same 
size of group (dark columns) 
 
the best ranking always tries to put the results 
that have the highest collective gain first to get 
the highest NDCG. 
The previous work has shown that the im-
plicit click-based potential for personalization 
is strongly related to variation in explicit 
judgments (J. Teevan et al, 2008). In this pa-
per, we continue using click-based potential 
for personalization to measure the variation. 
Assuming the clicked results as relevant, we 
can calculate the potential for personalization 
of each query over the web search query log to 
present what kind of query can benefit more 
from personalization. 
6.2 Potential for Personalization Distribu-
tion over Query Logs 
Teevan et al (2007) have depicted a potential 
for personalization curve based on explicit 
judgment to characterize the benefit that could 
be obtained by personalizing search results for 
each user. We continue using potential for per-
sonalization based on click-through to roughly 
reveal what kind of query can benefit more 
from personalization. 
First we investigate the number of unique 
queries with different potential for personaliza-
tion, which is shown in Figure 3. We find that 
there are about 53.9% unique queries in AOL 
log and 32.4% unique queries in Sogou log, 
whose potential for personalization is 0. For 
these queries, current web search is able to re-
turn perfect results to all users. However, for 
the rest of queries, even the best group ranking 
of results can?t satisfy everyone who issues the 
query. So these queries should be better served 
by individual user information, covering 
707
46.1% unique queries in AOL and 67.6% in 
Sogou. 
Then, in order to further interpret what kind 
of query individual user information is needed 
most, we further relate potential for personali-
zation to the number of users who submit the 
queries over AOL and Sogou query log as 
shown in Figure 4. For clarity?s sake, we also 
set the same step length as in Figure 2. 
According to Figure 4, the curve of potential 
for personalization is approximately U-shaped 
in both AOL log and Sogou Log. As the num-
ber of users in one group increases, perfor-
mance of the best non-personalized rankings 
first declines, then flattens out and finally 
promotes3. Note that the left part of the curve 
is very similar to what Teevan et al (2007) 
showed in their work. 
Again in Figure 4, the queries which have 
the most potential for personalization are the 
ones which are issued by more than 6 and less 
than 20 users in AOL log. While in Sogou log, 
the queries issued by more than 6 and less than 
4000 users have the most potential for persona-
lization. Such different findings are probably 
caused by the content of query. There are 
many recommended queries in the homepage 
of Sogou search engine, most of which are in-
formational query and clicked by a large num-
ber of users. Even when the size of group who 
issue the same query becomes very big, the 
query still has a wide variation of users? beha-
viors. So the consistency level of queries in 
Sogou log is much lower than the queries in 
AOL log at the same size of group.  
7 Conclusion and Future Work 
In this paper, we try to justify the position of 
individual user information comparing with 
common user information. It is shown that ex-
ploiting individual user information is a non-
trivial issue challenging the IR community 
through the analysis of both English and Chi-
nese large scale search logs. 
We first classify the repetitive queries into 2 
categories according to whether the corres-
ponding clicks are unique among different us-
ers. We find that quantitatively the queries and 
                                                 
3 Note that the different step length dims the actual U-
shape in the figure. 
clicks deserving for individual user informa-
tion is much bigger than those deserving for 
common user information. 
After that we use Kappa statistic to present 
that the overall consistency of query clicks re-
coded in search logs is pretty low, which statis-
tically reveals that the repetition is not the do-
minant factor and individual user information 
is more desired to enhance most queries in cur-
rent query log. 
We also explore the distribution of Kappa 
values over different numbers of users in the 
group who issue the same query, concluding 
that how to utilize individual user information 
to improve the performance of web search en-
gine is the next research issue confronted by 
the IR community when the repeated search of 
users are properly exploited.  
Finally, potential for personalization is cal-
culated over the two query logs to present an 
overview of what kind of queries that the op-
timal group-based retrieval model fails, which 
is supposed to benefit most from individual 
user information. 
One possible enrichment to this work may 
come from the employment of content analysis 
based on text processing techniques. The dif-
ferent clicks, which are the basis of our exami-
nation, may have similar or even exact content 
in their web pages. Though the manual check 
for a small scale sampling from the Sogou log 
yields less than 1% probability for such case, 
the content based examination will be definite-
ly more convincing than simple click counts. 
In addition, the queries for the two types of 
user information are not examined for their 
contents or the related information needs. Con-
tent analysis or linguistic view to these queries 
would be more informative. Both of these is-
sues are to be addressed in our future work. 
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project 
(Grant No.2006AA010108). The authors are 
grateful for the anonymous reviewers for their 
valuable comments. 
 
 
708
References 
Canny John. 2002. Collaborative filtering with pri-
vacy via factor analysis. In Proceedings of SI-
GIR? 02, pages 45-57. 
Carroll M. John and Mary B. Rosson. 1987. Para-
dox of the active user. Interfacing thought: cog-
nitive aspect of human-computer interaction, 
pages 80-111. 
Chirita A. Paul, Wofgang Nejdl, Raluca Paiu, and 
Christian Kohlschutter. 2005. Using odp metada-
ta to personalize search. In Proceedings of SI-
GIR ?05, pages 178-185. 
Cohen Jacob. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20: 37-46 
Dou Zhicheng, Ruihua Song, and Ju-Rong Wen. 
2007. A Large-scale Evaluation and Analysis of 
Personalized Search Strategies. In Proceedings 
of WWW ?07, pages 581-590. 
Fleiss L. Joseph. 1971. Measuring nominal scale 
agreement among many raters. Psychological 
Bulletin, 76(5): 378-382. 
Herlocker L. Jonathan, Joseph A. Konstan, Al 
Borchers, and John Riedl. 1999. An algorithmic 
framework for performing collaborative filtering. 
In Proceedings of SIGIR ?99, pages 230-237. 
Jansen J. Bernard, Amanda Spink, and Tefko Sara-
cevic. 2000. Real life, real users, and real needs: 
a study and analysis of user queries on the web. 
Information Processing and Management, pages 
207-227. 
J?rvelin Kalervo and Jaana Kek?l?inen. 2000. IR 
evaluation methods for retrieving highly relevant 
documents. In Proceedings of SIGIR ?00, pages 
41-48. 
Kohrs Arnd and Bernard Merialdo. 1999. Cluster-
ing for collaborative filtering applications. In 
Proceedings of CIMCA ?99, pages 199-204. 
Landis J. Richard and Gary. G. Koch. 1977. The 
mea-surement of observer agreement for cate-
gorical data. Biometrics 33: 159-174. 
Pitkow James, Hinrich Schutze, Todd Cass, Rob 
Cooley, Don Turnbull, Andy Edmonds, Eytan 
Adar and Thomas Breuel. 2002. Personalized 
search. ACM, 45(9):50-55. 
Shen Xuehua, Bin Tan and ChengXiang Zhai. 2005 
Implicit user modeling for personalized search. 
In Proceedings of CIKM ?05, pages 824-831. 
 
Silverstein Craig, Monika Henzinger, Hannes Ma-
rais and Michael Moricz. 1999. Analysis of a 
very large web search engine query log. SIGIR 
Forum, 33(1):6-12. 
Smyth Barry. 2007. A Community-Based Approach 
to Personalizing Web Search. IEEE Computer, 
40(8): 42-50. 
Speretta Mirco and Susan Gauch. Personalized 
Search based on user search histories. 2005. In 
Proceedings of WI ?05, pages 622-628. 
Spink Amanda, Dietmar Wolfram, Major Jansen, 
Tefko Saracevic. 2001. Searching the web: The 
public and their queries. Journal of the American 
Society for Information Science and Technology, 
52(3), 226-234 
Sugiyama Kazunari, Kenji Hatano, and Masatoshi 
Yoshikawa. 2004. Adaptive web search based on 
user profile constructed without any effort from 
users. In Proceedings of WWW ?04, pages 675-
684. 
Sun Jian-Tao, Hua-Jun Zeng, Huan Liu, Yuchang 
Lu and Zheng Chen. 2005. CubeSVD: a novel 
approach to personalized web search. In Pro-
ceedings of WWW?05, pages 382-390. 
Teevan Jaime, Susan T. Dumais, and Eric Horvitz. 
2005. Personalizing search via automated analy-
sis of interests and activities. In Proceedings of 
SIGIR ?05, pages 449-456. 
Teevan Jaime, Susan T. Dumais and Eric Horvitz. 
2007. Characterizing the value of personalizing 
search. In Proceedings of SIGIR ?07, pages 757-
758. 
Teevan Jaime, Susan T. Dumais and Daniel J. 
Liebling. 2008. To personalize or Not to Perso-
nalize: Modeling Queries with Variation in User 
Intent. In Proceedings of SIGIR ?08, pages 163-
170. 
Townsend Steve Cronen and W. Bruce Croft. 2002. 
Quantifying query ambiguity. In Proceedings of 
HLT ?02, pages 613-622. 
Yu Kai, Anton Schwaighofer, Volker Tresp, Xiao-
wei Xu, Hans-Peter Kriegel. 2004. Probabilistic 
Memory-based Collaborative Filtering. In IEEE 
Transactions on Knowledge and Data Engineer-
ing, pages 56-59. 
 
709
Coling 2010: Poster Volume, pages 1203?1210,
Beijing, August 2010
Utilizing Variability of Time and Term Content, within and across 
Users in Session Detection 
Shuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao1 
1Harbin Institute of Technology, 2Heilongjiang Institute of Technology 
{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
haoliang.qi@gmail.com 
Abstract 
In this paper, we describe a SVM classi-
fication framework of session detection 
task on both Chinese and English query 
logs. With eight features on the aspects 
of temporal and content information ex-
tracted from pairs of successive queries, 
the classification models achieve signifi-
cantly superior performance than the stat-
of-the-art method. Additionally, we find 
through ROC analysis that there exists 
great discrimination power variability 
among different features and within the 
same feature across different users. To 
fully utilize this variability, we build lo-
cal models for individual users and com-
bine their predictions with those from the 
global model. Experiments show that the 
local models do make significant im-
provements to the global model, although 
the amount is small. 
1 Introduction 
To provide users better experiences of search 
engines, inspecting users? activities and inferring 
users? interests are indispensible. Query logs rec-
orded by search engines serves well for these 
purposes. Query log conveys the user interest 
information in the form of slices of the query 
stream. Thus the task of session detection con-
sists in distinguishing slice that corresponds to a 
user interest from other ones, and thus this paper, 
we adopt the definition of a session following 
(Jansen et al, 2007): 
(A session is) a series of interactions by the us-
er toward addressing a single information need. 
This definition is equivalent to that of the 
?search goal? proposed by Jones and Klinkner 
(2008), which corresponds to an atomic infor-
mation need, resulting in one or more queries.  
This paper adopts a classification point of 
view to the task of session detection (Jones and 
Klinkner, 2008). Given a pair of successive que-
ries in a query log, we examine it in various 
viewpoints (i.e. features) such as time proximity 
and similarity of the content of the two queries to 
determine whether these two queries cross a bor-
der of a search session. In other words, we classi-
fy the gap between the two queries into two clas-
ses: session shift and session continuation. In 
practice, search goals in a search mission and 
different search missions could be intermingled, 
and increase the difficulty of correctly identify-
ing them. In this paper, we do not take this issue 
into account and simply treat all boundaries be-
tween intermingled search goals as session shifts. 
The chief advantage in this choice is that we will 
have the opportunity to make classification mod-
el working online without caching user?s queries 
that are pending to be assigned to a session. 
Various studies built accurate models in pre-
dicting session boundaries and in distinguishing 
intermingled sessions, and they are summarized 
in Section 2. However, none of these works ana-
lyzed the contribution of individual features from 
a user-oriented viewpoint, or evaluated a fea-
ture?s discrimination power in a general scenario 
independent of its usage, as this paper does by 
conducting ROC analyses. During these analyses, 
we found that the discrimination power of fea-
tures varies dramatically, and for different users, 
the discrimination power of a particular feature 
also does not remain constant.  
Thus, it is appealing to build local models for 
users with have sufficient size of training exam-
ples, and combine the local models? predictions 
with those made by the global model trained by 
the whole training data. However, few of previ-
1203
ous works build user-specific models for the sake 
of characterizing the variability in user?s search 
activities, except that of Murray et al (2006). To 
fully make use of these two aspects of variability, 
inspired by Murray et al, we build users? local 
models based on a much broader range of evi-
dences, and show that different local models vary 
to a great extent, and experiments show that the 
local models do make significant improvements 
to the global model, although the amount is small. 
The remainder of this paper is organized as 
follows: Section 2 summarizes the related work 
of the session detection task. In Section 3, we 
first describe our classification framework as 
well as the features utilized. Then we conduct 
various evaluations on both English and Chinese 
query logs. Section 4 introduces the approaches 
to building local models based on an analysis of 
the variability of the discrimination power of 
features, and combine predictions of local mod-
els with those of the global model. Section 5 dis-
cusses the experimental results and concludes 
this paper. 
2 Related Work 
The simplest method in session detection is 
defining a timeout threshold and marking any 
time gaps of successive queries that exceed the 
threshold as session shifts. The thresholds 
adopted in different studies were significantly 
different, ranging from 5 minutes to 30 minutes 
(Silverstein et al, 1999; He and G?ker, 2000; 
Radlinski and Joachims, 2005; Downey et al, 
2007). Other study suggested adopting a dynamic 
timeout threshold. Murray et al (2006) proposed 
a user-centered hierarchical agglomerative 
clustering algorithm to determine timeout 
threshold for each user dynamically, other than 
setting a fixed threshold. However, Jones and 
Klinkner (2008) pointed out that single timeout 
criterion is always of limited utility, whatever its 
length is, and incorporating timeout features with 
other various features achieved satisfactory 
classification accuracy.  
An effective approach to combining the time 
out features with various evidences for session 
detection is machine learning. He et al (2002) 
collected statistical information from human an-
notated query logs to predict the probability a 
?New? pattern indicates a session shift according 
to the time gap between successive queries. 
?zmutlu and colleagues re-examined He et al?s 
work, and explored other machine learning tech-
niques such as neural networks, multiple linear 
regression, Monte Carlo simulation, conditional 
probabilities (Gayo-Avello, 2009), and HMMs 
(?zmutlu, 2009). 
In recent studies, Jones and Klinkner (2008) 
built logistic regression models to identify search 
goals and missions, and tackled the intermingled 
search goal/mission issue by examining arbitrary 
pairs of queries in the query log. Another contri-
bution of Jones and Klinkner is that they made a 
thorough analysis of contributions of individual 
features. However, they explored the features? 
contributions from a feature selection point of 
view rather than from a user-oriented one, and 
thus failed to characterize the variability of the 
discrimination power of the features when ap-
plied to different users. 
3 Learning to Detect Session Shifts 
3.1 Feature Extraction 
We adopt eight features covering both the tem-
poral and the content aspect of pairs of succes-
sive queries. Most these features are commonly 
used by previous studies (He and G?ker, 2000; 
?zmutlu, 2006; Jones and Klinkner, 2008). 
However, in this paper, we will analyze their 
contributions to the resulted model in a quite dif-
ferent way from that in previous works. 
Let Q = (q1, q2, ? , qn) denote a query log.  
The features are extracted from every successive 
pair of queries (qi, qi+1). Table 1 summarizes the 
features we adopt. The normalization described 
in Table1 is done according to the type of the 
feature. Features describing characters are nor-
malized by the average length of the two queries, 
while those describing character-n-grams are 
normalized by the average size of the n-gram sets 
of the two queries. Character-n-grams (e.g. bi-
grams ?ca? and ?at? in ?cat?) are robust to dif-
ferent representations of the same topic (e.g. ?IR? 
as Information Retrieval) and typos (e.g. 
?speling? as ?spelling?), and serve as a simple 
stemming method. In practice, character-n-grams 
are accumulative, which means they consist of 
all m-grams with m ? n. 
The feature ?avg_ngram_distance?, a variant 
of the ?lexical distance? in (Gayo-Avello, 2009), 
is more complicated than to be described briefly. 
1204
Here we first define n-gram distance (ND) from 
qi to qj, which is formalized as follows: 
j
ji
ji n
n
ND
qin   gram--char. of #
qin occur   qin   gram--char. of #
1)qq( ?=?  
Note that character-n-grams are accumulative 
and there could be multiple occurrences of a 
character-n-gram in a query, so the number of a 
character-n-gram is the sum of that of all m-
grams with m ? n, and multiple occurrences are 
all considered. At last, the average of character-
n-gram distance (ACD) of the pair (qi, qi+1) is:  
2
)qq()qq(
)q,q( 111
iiii
ii
NDND
ACD
?+?
=
++
+
 
There are seven features describing the content 
aspect of a query pair, and they are more or less 
overlapped (e.g. edit_distance vs. common_char). 
However, we show in the next subsection that all 
these features are beneficial to the final perfor-
mance.  
Feature Description 
time_interval time interval between 
successive queries 
avg_ngram_ 
distance 
avg. of character-n-gram 
distances 
edit_disance normalized Levenshtein 
edit distance 
common_prefix normalized length of pre-
fix shared 
common_suffix normalized length of suf-
fix shared 
common_char normalized number of 
characters shared 
common_ngram normalized number of 
character-n-grams shared 
Jaccard_ngram Jaccard distance between 
character-n-gram sets 
Table 1. Features used in classification models 
3.2 Data Preparation 
The query logs we explored include an English 
search log tracked by AOL from Mar 1, 2006 to 
May, 31 2006 (Pass et al, 2006), and a Chinese 
search log tracked by Sogou.com, which is one 
of the major Chinese Search Engines, from Mar 
1, 2007 to Mar 31, 20071. We applied systematic 
sampling over the user space on the two logs, 
which yielded 223 users and 2809 users, corre-
sponding to 6407 and 6917 query instances re-
                                                 
1 http://www.sogou.com/labs/resources.html 
spectively2. Sampling over the user space instead 
of over the query space avoids the bias to the 
most active users who submit much more queries 
than average users. 
For each sampled dataset, we invited annota-
tors who are familiar with IR and search process 
to determine each pair of successive queries of 
interest is across the border of a session. We 
made trivial pre-split process under two rules: 
 Queries from different users are not in the 
same session. 
 Queries from different days are not in the 
same session.  
Table 2 shows some basic statistics of the an-
notated data set. During the annotation process, 
the annotators were guided to identify the user?s 
information need at the finest granularity ever 
possible, because we focus on the atomic infor-
mation needs as described in Section 1. Conse-
quently, the average numbers of queries in a ses-
sion in both query logs are lower than previous 
studies. 
 AOL log Sogou log 
Queries 6407 6917 
Sessions 4571 5726 
Queries per session 1.40 1.21 
Longest session 21 12 
Table 2. Summary of the annotation results in 
both query logs 
3.3 Learning Framework 
In this section we seek to build accurate global 
classification model based on the whole training 
data obtained in the previous sub-subsection for 
both the query logs. We built the models within 
SVM framework. The implementation of SVM 
we used is libSVM (Chang and Lin, 2001). For 
the sake of evaluations and of model integration 
in the next section, we set the prediction of SVM 
to be probability estimation of the test example 
being positive. All features were pre-scaled into 
[0, 1] interval. We adopted the polynomial kernel, 
and for both datasets, we exhaustively tried each 
of the subset of the eight features using 5-fold 
cross validation. We found that using all the 
eight features yielded the best classification ac-
curacy. Thus in the experiments in rest of this 
                                                 
2 The sampling schema and sample size was deter-
mined following (Gayo-Avello, 2009). 
1205
section and the next section, we adopt the entire 
feature set to build global classification models. 
There is one parameter to be determined for 
feature extraction: the length of character-n-
grams. The proper lengths on AOL log and 
Sogou log are different. We tried the length from 
1 to 9, and according to cross validation accuracy, 
we found the best lengths for the two logs as 6 
and 3 respectively. 
3.4 Experimental Results 
3.4.1 Baseline Methods 
We provide two base line methods for compari-
sons. The first method is the commonly used 
timeout methods. We tried different timeout 
thresholds from 5 minutes to 30 minutes with a 
step of 5 minutes, and found that for both query 
logs the 5 minutes? threshold yield the best over-
all performance.  
The second method achieved the best perfor-
mance on the AOL log (Gayo-Avello, 2009), 
which addresses the session detection problem 
using a geometric interpolation method, in com-
parison to previous studies on this query log. We 
re-implemented this method and evaluated it on 
both the datasets. Similarly, the best parameters 
for the two query logs are different, such as the 
length of a character-n-gram. We only report the 
performance with the best parameter settings. 
3.4.2 Analyzing the Performance  
We analyze the performance of the SVM models 
according to precision, recall, F1-mean and F1.5-
mean of predictions on session shift and continu-
ation against human annotation data. 
The F

-mean is defined as: 
RP
PR
+
+
=
2
2)1(
mean-F ?
?
?  
where P denotes precision and R denotes recall. 
He et al (2002) regards recall more important 
than precision, and set the value of   in F

-mean 
to 1.5. We also report performance under this 
measure. 
In addition to traditional precision / recall 
based measures, we also perform ROC (Receiver 
Operating Characteristic) analysis to determine 
the discrimination power of different methods. 
The best merit of ROC analysis is that given a 
reference set, which is usually the human annota-
tion results, it evaluates a set of indicator?s dis-
crimination power for arbitrary binary classifica-
tion problem independent of the critical value 
with which the class predictions are made.  
Specifically, in the context session detection, 
regardless of the critical value that splits the clas-
sifier outputs into positive ones and negative 
ones (e.g. the 5-minutes? timeout threshold and 
50% probability in SVM?s output), the ROC 
analysis provides the overall discrimination pow-
er evaluation of the output set of a certain meth-
od (by trying to set each output value as the criti-
cal value). For the baseline method by Gayo-
Avello, the core of the decision heuristics also 
had a critical value to be determined. For details, 
readers could refer to (Gayo-Avello, 2009).  
3.4.3 Precision, Recall, and F-means 
Before we examine the discrimination power of 
each session detection method?s output independ-
ent of the threshold value selected. In this sub-
subsection, we begin with a more traditional eval-
uation schema: setting a proper threshold to pro-
duce binary predictions. It is straightforward to set 
the threshold for SVM method to 50%, and as 
described in sub-subsection 3.1.1, the threshold 
for timeout method is 5 minutes. The threshold of 
Gayo-Avello?s method is implied in its heuristics. 
Table 3 and Table 4 show the experimental re-
sults on AOL log and Sogou log respectively. 
For each dataset, we performed 1000-times boot-
strap resampling, generating 1000 bootstrapped 
datasets with the same size as the original dataset. 
To test the statistical significance of performance 
differences, we adopted Wilcoxon signed-rank 
test on the performance measures computed from 
the 1000 bootstrapped dataset, and found com-
parisons between each pair of methods were all 
significant at 95% level. 
The results show that SVM method clearly 
outperforms the baseline methods, and timeout 
method performs poorly. It may be argued that 
the poor performance of timeout method is due 
to the improper threshold value chosen. In this 
case, the ROC analysis, which assesses the dis-
crimination power of a method?s output set inde-
pendent of the threshold value chosen, is more 
suitable for performance evaluation. 
Gayo-Avello method significantly outperforms 
the timeout method. But due to its heuristic na-
ture, it is less likely to do better than the super-
vised-learning methods, although it avoids the 
over fitting issue. The Gayo-Avello method?s 
unstable performance in predicting session con-
1206
tinuations implies that its heuristics did not gen-
eralize well to Chinese query logs. 
 Timeout Gayo-Avello SVM 
P 
shift 75.92 89.35 90.96 
cont. 63.05 85.32 92.06 
R 
shift 64.49 87.85 93.82 
cont. 74.77 87.08 88.50 
F1 
shift 69.74 88.60 92.37 
cont. 68.41 86.19 90.25 
F1.5 
shift 67.62 88.31 92.92 
cont. 70.72 86.53 89.57 
Table 3. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on AOL dataset.  
 Timeout Gayo-Avello SVM 
P 
shift 67.75 75.10 87.53 
cont. 52.82 83.51 81.62 
R 
shift 59.52 91.44 86.17 
cont. 61.53 58.84 83.33 
F1 
shift 63.37 82.47 86.85 
cont. 56.84 69.04 82.47 
F1.5 
shift 61.83 85.71 86.59 
cont. 58.56 64.72 82.80 
Table 4. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on Sogou dataset. 
3.4.4 ROC Analysis 
By setting certain threshold value, we analyzed 
the three method?s performance using precision / 
recall based measures. In this sub-subsection, we 
try to set each value in an output set as the 
threshold value, and evaluate the discrimination 
power of methods by the area under the ROC 
curve. 
Figure 1 shows the ROC curves of the SVM 
method and the two baseline methods: timeout 
and Gayo-Avello, for predicting session shifts. 
ROC curves for predicting session continuations 
are symmetric with respect to the reference line, 
so we omit them in the rest of this paper for the 
sake of space limit.  
The results show that SVM method clearly 
outperforms the baseline methods in the prospec-
tive of discrimination power, with ROC area 
0.9562 on AOL dataset and 0.9154 on Sogou 
dataset. The curves of the two baseline methods 
are clearly under that of SVM method. This 
means baseline methods can never achieve accu-
racy as high as SVM method w.r.t. a fixed false 
alarm (classification error) rate, nor false alarm 
rate as low as SVM method w.r.t. a fixed accura-
cy rate. Again, Gayo-Avello method significantly 
outperforms timeout method, while underper-
forms the SVM method. For the question in the 
previous sub-subsection, coinciding with previ-
ous studies (Murray et al, 2006; Jones and 
Klinkner, 2008), applying single timeout thresh-
old always yields limited discrimination power, 
wherever the operating point on ROC curve (i.e. 
threshold value) is set. 
4 Making Use of the Variability of Dis-
crimination Power 
In this section, we first analyze the amount of 
contribution that each feature makes and show 
that the contribution, i.e. the discrimination pow-
er of each feature varies dramatically across dif-
ferent users. Then, we propose an approach to 
making use of this variability. Finally through 
experimental results, we show that the proposed 
approach makes small, yet significant improve-
ments to the SVM method in Section 3. 
4.1 Variability of Discrimination Power 
The ROC analysis of individual feature provides 
adequate characterizations of the discrimination 
power of the feature. Another advantage of 
adopting ROC analysis is that the results are in-
dependent not only of the critical value, but also 
of the scale of the feature values.  
Figure 2 shows the ROC curves of all the eight 
features in both datasets. Note that some features 
are with a higher value indicating session contin-
uation rather than session shift, so their ROC 
curves are below the reference line. The feature 
?time_interval? behaves exactly the same as the 
timeout method in Figure 1. For the rest of the 
features, ?avg_ngram_distance?, ?common_ngram? 
and ?Jaccard_ngram? achieve the best discrimi-
nation powers, showing the character-n-gram 
representation is effective. The feature ?com-
mon_char? performs significantly better in 
Sogou dataset than in AOL dataset, because Chi-
nese characters convey much more information 
than English characters do. ?common_suffix? 
performing worse than ?common_prefix? reflects 
the custom of users. Users tend to add terms at 
the end of the query in a searching iteration, thus 
predicting session continuations by examining 
the common suffixes is problematic. 
1207
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.7707
Gayo-Avello ROC area: 0.9130
SVM ROC area: 0.9562
Reference
AOL
    
0.
00
0.
25
0.
50
1.
00
0.
75
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.6365
Gayo-Avello ROC area: 0.8463
SVM ROC area: 0.9154
Reference
Sogou
 
Figure 1. ROC analysis of SVM method and two baseline methods for predicting session shifts on 
both AOL and Sogou dataset. All comparisons between ROC areas within the same dataset are at 
least 95% statistically significant, because the corresponding confidence intervals do not overlap. 
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.7707
avg_ngram_distance ROC area: 0.9560
edit_disance ROC area: 0.8848
common_prefix ROC area: 0.2177
common_suffix ROC area: 0.2985
common_char ROC area: 0.1360
common_ngram ROC area: 0.0480
Jaccard_ngram ROC area: 0.0464
Reference
AOL
  
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.6365
avg_ngram_distance ROC area: 0.9108
edit_disance ROC area: 0.8333
common_prefix ROC area: 0.2449
common_suffix ROC area: 0.3745
common_char ROC area: 0.0922
common_ngram ROC area: 0.1018
Jaccard_ngram ROC area: 0.0965
Reference
Sogou
 
Figure 2. ROC analysis of individual features for predicting session shifts on both AOL and Sogou 
dataset. Note that some curves with similar ROC area values overlap each other. 
In spite of the discrimination power a feature 
has, its behavior on different users is worth-
while to be examined. For selecting users that 
have sufficient data to draw stable conclusions, 
we consider only users who issued more than 50 
queries in the datasets. Unfortunately, there are 
too few users (6 users) qualified in Sogou da-
taset, so we show only the statistics of ROC 
area values of each of the features in Table 5 
based on 37 users in AOL dataset. 
The statistics in Table 5 show that for differ-
ent users. Recall that in sub-subsection 3.3.2, a 
0.04 difference of ROC area make the perfor-
mance of the SVM method significantly better 
1208
than that of the Gayo-Avello?s method. Thus, 
the discrimination power of a feature is likely to 
vary significantly, because all the standard de-
viations are at 0.03 or even higher level. Espe-
cially, the minimum and maximum values show 
that for these users, some of the findings above 
from the whole dataset do not hold. This implies 
that it is likely more feasible to build specific 
local models for these users to make full use of 
the variability within the same feature. 
Feature avg. sdev. min. max. 
time_interval 0.780 0.088 0.476 0.912
avg_ngram_ 
distance 
0.954 0.034 0.861 1.000
edit_disance 0.883 0.056 0.733 0.990
common_prefix 0.224 0.069 0.099 0.327
common_suffix 0.299 0.113 0.064 0.578
common_char 0.143 0.082 0.037 0.493
common_ngram 0.051 0.037 0.000 0.187
Jaccard_ngram 0.049 0.036 0.000 0.173
Table 5. Average, standard deviation, minimum, 
and maximum ROC areas of individual features 
4.2 Building Local Models 
We built individual local models for each user 
that issued more than 50 queries in AOL dataset. 
We also performed 5-fold cross validations and 
set the prediction to be the probability estima-
tion of a test example being positive. The fea-
ture selection process showed again that all the 
eight features are beneficial, and none of them 
should be excluded. 
In each fold of cross validation, we per-
formed 90%-bagging on the training set 10 
times to get the variance estimations of the local 
model. For each example in the test set, we set 
the final output on it to be the average of the 10 
outputs, and recorded the standard deviation of 
the outputs on this example which is used dur-
ing the model combination. We also conducted 
the same process for the global model for the 
sake of combination process described below. 
4.3 Combing with the Global Model 
Since the predictions of both the local and the 
global models are probability estimations, it is 
reasonable to combine them using linear combi-
nation. For each example, there are two outputs 
Ol and Og coming from local and global models 
accordingly. For each example e of a user?s sub 
dataset U, we have the outputs Ol(e) and Og(e) 
as well as the normalized deviations Dl(e) and 
Dg(e) (by the largest deviation in U of the corre-
sponding models). The final output O(e) is de-
fined as: 
)()(
)()()()(
)(
eDeD
eOeDeOeD
eO
gl
glgl
+
?+?
=
 
 Global Local Combine 
P 
shift 90.48 88.53 90.43 
cont. 91.75 92.12 92.52 
R 
shift 93.94 94.44 94.56 
cont. 87.20 84.16 87.04 
F1 
shift 92.18 91.39 92.45 
cont. 89.41 87.96 89.69 
F1.5 
shift 92.85 92.54 93.25 
cont. 88.55 86.46 88.65 
Table 6. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of global model (bagging), 
local model (bagging) and combined model  
This combination process is similar to (Osl et 
al., 2008). Note that the more the deviation of a 
model is, the less feasible the corresponding 
model is. We compared the performance of 
three models: global model, local model, and 
combined model. The results are summarized in 
Table 6. All comparisons between different 
models are statistically significant at 95% level, 
based on the same bootstrapping settings in sub-
subsection 3.4.3. The combined model shows 
slight (may due to the inferior performance of 
the local model), yet significant improvement to 
the global model. In spite of the amount of the 
improvement, the local model did correct some 
errors of the global model. It may be not ac-
ceptable to build such an expensive combined 
model for a limited improvement. Nevertheless, 
the results do show that the variability across 
different users is exploitable. 
5 Discussion and Conclusion 
In this paper, we built a learning framework of 
detecting sessions which corresponds to user?s 
interest in a query log. We considered two as-
pect of a pair of successive queries: temporal 
aspect and content aspect, and designed eight 
features based on these two aspects, and the 
SVM models built with these features achieved 
satisfactory performance (92.37% F1-mean on 
session shift, 90.25% F1-mean on session con-
tinuation), significantly better than the best-ever 
approach on AOL query log. 
1209
The analysis of the features? discrimination 
power was conducted not only among different 
features, but also within the same feature when 
applied to different users in the query log. By 
analyzing the statistics of ROC area values of 
each of the features based on 37 users in AOL 
dataset, experimental results showed that there 
is considerable variability in both these aspects. 
To make full use of this variability, we built 
local models for individual user and combine 
the yielded predictions with those yielded by the 
global model. Experiments showed that the lo-
cal model did make significant improvements to 
the global model, although the amount was 
small (92.45% vs. 92.18% F1-mean on session 
shift, 89.69% vs. 89.41% F1-mean on session 
continuation). 
In future studies, we will explore other learn-
ing frameworks which better integrate the local 
model and the global model, and will try to ac-
quire more data to build local models. We will 
also analyze more deeply the characteristics of 
ROC analysis in the feature selection process.  
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project (Grant 
No.2006AA010108). The authors are grateful 
for the anonymous reviewers for their valuable 
comments. 
References 
Chang Chih-Chung and Chih-Jen Lin. 2001. 
LIBSVM : a library for support vector machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Downey Doug, Susan Dumais, and Eric Horvitz. 
2007. Models of searching and brows-
ing: languages, studies, and applications. In Pro-
ceedings of the 20th international joint conference 
on Artificial intelligence, pages 2740-2747, Hy-
derabad, India. 
Gayo-Avello Daniel. 2009. A survey on session de-
tection methods in query logs and a proposal for 
future evaluation, Information Science 
179(12):1822-1843. 
He Daqing and Ayse G?ker. 2000. Detecting Session 
Boundaries from Web User Logs. In BCS/IRSG 
22nd Annual Colloqui-um on Information Re-
trieval Research, pages 57-66.  
He Daqing, Ayse G?ke, and David J. Harper. 2002. 
Combining evidence for automatic web session 
identification. Information Processing and Man-
agement: an International Journal, 38(5):727-742. 
Jansen Bernard J., Amanda Spink, Chris Blakely, 
and Sherry Koshman. 2007. Defining a session on 
Web search engines: Research Articles. Journal of 
the American Society for Information Science and 
Technology, 58(6):862-871 
Jones Rosie and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical 
segmentation of search topics in query logs. In 
Proceedings of the 17th ACM conference on In-
formation and knowledge management, pages 
699-708, Napa Valley, California, USA. 
Murray G. Craig, Jimmy Lin, and Abdur Chowdhury. 
2007. Identification of user sessions with hierar-
chical agglomerative clustering. American Society 
for Information Science and Technology, 43(1):1-
9. 
Osl Melanie, Christian Baumgartner, Bernhard Tilg, 
and Stephan Dreiseitl. 2008. On the combination 
of logistic regression and local probability esti-
mates. In Proceedings of Third International Con-
ference on Broadband Communications, Infor-
mation Technology & Biomedical Applications, 
pages 124-128. 
?zmutlu Seda. 2006. Automatic new topic identifi-
cation using multiple linear regression. Infor-
mation Processing and Management: an Interna-
tional Journal, 42(4):934-950. 
?zmutlu Huseyin C. 2009. Markovian analysis for 
automatic new topic identification in search en-
gine transaction logs. Applied Stochastic Models 
in Business and Industry, 25(6):737-768. 
Pass Greg, Abdur Chowdhury, and Cayley Torgeson. 
2006. A picture of search. In Proceedings of the 
1st international conference on Scalable infor-
mation systems, Hong Kong. 
Radlinski Filip and Thorsten Joachims. 2005. Query 
chains: learning to rank from implicit feedback. In 
Proceedings of the eleventh ACM SIGKDD inter-
national conference on Knowledge discovery in 
data mining, pages 239-248, Chicago, Illinois, 
USA. 
Silverstein Craig, Hannes Marais, Monika Henzinger, 
and Michael Moricz. 1999. Analysis of a very 
large web search engine query log. ACM SIGIR 
Forum, 33(1):6-12. 
1210
