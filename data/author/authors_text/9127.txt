R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 885 ? 895, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Tense Tagging for Verbs in Cross-Lingual Context:  
A Case Study 
Yang Ye1 and Zhu Zhang2 
1
 Department of Linguistics, University of Michigan, USA 
2
 School of Information and Department of Electrical Engineering and Computer Science,  
University of Michigan, USA 
yye@umich.edu, zhuzhang@umich.edu 
Abstract. The current work applies Conditional Random Fields to the problem 
of temporal reference mapping from Chinese text to English text. The learning 
algorithm utilizes a moderate number of linguistic features that are easy and in-
expensive to obtain. We train a tense classifier upon a small amount of manu-
ally labeled data. The evaluation results are promising according to standard 
measures as well as in comparison with a pilot tense annotation experiment in-
volving human judges. Our study exhibits potential value for full-scale machine 
translation systems and other natural language processing tasks in a cross-
lingual scenario. 
1   Introduction 
Temporal resolution is a crucial dimension in natural language processing. The fact that 
tense does not necessarily exist as a grammatical category in many languages poses a 
challenge on cross-lingual applications, e.g. machine translation. The fact that English 
tenses and Chinese aspect markers align at word level on one hand and sub-word level 
on the other hand poses a challenge for temporal reference distinction translation in a 
statistical machine translation (MT) system. A word-based alignment algorithm will not 
be able to capture the temporal reference distinction when mapping between Chinese 
and English. Being able to successfully map the temporal reference distinction in Chi-
nese text through disparate features onto the most appropriate tenses for the parallel 
English text is an important criterion for good translation quality. Languages have vari-
ous levels of time reference distinction representation: some have finer grained tenses 
than others, as typological studies have shown. When facing the unbalanced levels of 
temporal reference distinction between a pair of languages, we have to optimize the 
mapping between the two temporal systems through intelligent learning. Most machine 
translation systems do not have a separate temporal reference resolution module, but if 
we can integrate a special module into them, the temporal reference resolution of the 
system could be corrected accordingly and yield a better translation. Other than machine 
translation, in cross-lingual question answering (CLQA) with English as the target lan-
guage, the ability to successfully formulate queries and maintain the temporal reference 
information in the original questions is desirable. 
886 Y. Ye and Z. Zhang 
2   Related Work 
2.1   Temporal Reference Modeling in Cross-Lingual Scenario 
The nature of being past, present or future is highly relative and hence the information 
contained in tenses is often referred to as temporal reference distinction. While there 
is a large body of research on temporal reference in formal semantics and logic as 
well as in other disciplines of Linguistics, works in cross-lingual temporal reference 
mapping remain inadequate.  
Campbell et. al. [1] proposed a language-neutral framework for representing se-
mantic tense. This framework is called the Language Neutral Syntax (LNS). Based on 
the observation that grammatical or morphological tenses in different languages do 
not necessarily mean the same thing, they interpret semantic tense to be largely a rep-
resentation of event sequence; their work did not attempt direct and explicit represen-
tations of tenses. The tense node in the LNS tree contains either global tense feature 
(also known as ?absolute tense?) or anchorable tense feature (also known as ?relative 
tense?). This work treated compound tenses as being represented by primary and sec-
ondary tense features. The tense in an embedded clause is anchored to the tense in the 
matrix clause. Campbell?s work attempted neither a strict nor a deep semantic repre-
sentation of tenses, but rather a syntactic representation that is language-neutral. In 
addition, similar to most of its peer works in tense modeling, it only attacked the 
problem in a scope of individual sentences. 
Pustejovsky et. al. [2] reported an annotation scheme, the TimeML metadata for 
markup of events and their anchoring in documents. The challenge of human labeling 
of links among eventualities were discussed to the full fledge in their paper showing 
that inter-annotator consistency for links is a hard-to-reach ideal. The automatic 
?time-stamping? was attempted earlier on a small sample of text in an earlier work of 
Mani [3]. The result was not particularly promising showing need for bigger size of 
training data as well as more predictive features, especially on the discourse level. At 
the word level, semantic representation of tenses could be approached in various ways 
depending on different applications. None of the previous works were designed par-
ticularly for cross-lingual temporal reference distinction mapping and the challenges 
of this mapping for some language pairs have not received full attention. 
2.2   Temporal Reference Mapping Between Chinese and English 
Since temporal reference distinction mapping is of particular interest of cross-lingual 
natural language processing tasks, the pilot works for tense classification in Chinese 
were naturally motivated by machine translation scenario. Olsen et. al. [4] attacked 
tense reconstructing for Chinese text in the scenario of Chinese to English MT. On 
top of the more overt features, their work made use of the telicity information en-
coded in the lexicons through the use of Lexical Conceptual Structures (LCS). Based 
on the dichotomy of grammatical aspect and lexical aspect, they proposed that past 
tense corresponds to the telic LCS which is either inherently telic or derived telic. 
While grammatical aspect markings supersede the LCS, in the absence of grammati-
cal aspect marking, verbs that have telic LCS are translated into past tense and present 
tense otherwise. This work, while pushing tense reconstruction one step further to-
wards the semantics embedded in the events, is subject to the risk of adopting one-to-
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 887 
one mapping between grammatical aspect markings and tenses hence oversimplifies 
the temporal reference situation in Chinese text. Additionally, their binary tense tax-
onomy is oversimplifying the rich temporal reference system that exists in Chinese. 
Li et. al. [5] proposed a computational model based on machine learning and het-
erogeneous collaborative bootstrapping for analyzing temporal relations in a Chinese 
multiple-clause sentence. The core model is a set of rules that map the combinational 
effects of a set of linguistic features to one class of temporal relations for one event 
pair. Their work showed promising results for combining machine learning algorithms 
and linguistic features to achieve temporal relation resolution, but did not directly ad-
dress cross-lingual temporal reference information mapping. The nature of the task 
they were attacking is B Series temporal resolution in Mctaggart?s terminology.   
3   Problem Definition 
3.1   The Taxonomy of Tenses 
In the current literature, the taxonomy of tenses typically includes the three basic 
tenses (present, past and future) plus their combination with the progressive and per-
fect grammatical aspects, because in English tense and aspect are morphologically 
merged. This yields a taxonomy of 13 tenses. We collapse these 13 tenses into a tax-
onomy of three classes: present, past and future. The reason for this collapse is two-
fold: linguistically, this three-class taxonomy conforms more strictly with the well  
defined tripartite temporal reference distinction [6]; and in practice, only nine tenses 
occurred  in  our  data  set:  simple past, simple future, simple present, present perfect, 
 
Fig. 1. Tense Taxonomy 
 
1. Present 
E,R,S 
2. Past 
E,R S 
3. Future 
E,R S 
Perfect 
4. 
E R,S 5. 
E 
R,S 
6. 
E SR 7. 
E 
R S 
8. 
E 
S R 
9. 
E 
S R 
Progressive 
E 
10. 
R,S 
E 
11. 
R S 
12. 
S R 
E
13. 
R S E 
888 Y. Ye and Z. Zhang 
present progressive, past perfect, past progressive, past future and present perfect pro-
gressive. Some tenses are very sparse in the data set yielding little value from the 
learning perspective. Figure 11 shows the tense taxonomy. In the graph, for each of 
the thirteen tenses, we provide the timeline representation for the configuration of the 
three time points under Reichenbachian system. E stands for the event time, R stands 
for the reference time and S stands for the speech time. It is observed that in terms of 
the relationship between the speech time and the event time, the thirteen tenses could 
be grouped into three categories: tense 1 and tense 5 have the event time overlapping 
with the speech time; tense 2, 4, 6, 7, 10, 11 and 13 have the event time being prior to 
the speech time; tense 3, 8, 9 and 12 have the event time being later than the speech 
time. These three categories form our collapsed tense taxonomy. 
3.2   Problem Formulation 
In general, the tense tagging problem for verbs can be formalized as a standard classi-
fication or labeling problem, in which we try to learn a classifier 
C: V?T 
where V is the set of verbs (each described by a feature vector), and T is the set of 
possible tense tags (defined by the taxonomy above). 
This is, however, a somewhat simplistic view of the picture. Just as temporal 
events are usually sequentially correlated, verbs in adjacent linguistic utterances are 
not independent. Therefore the problem should be further formalized as a sequential 
learning problem, where we try tag a sequence of verbs (V1, ?, Vn) with a sequence 
of tense tags (t1, ?, tn). This formalization shares similarities with many other prob-
lems inside and outside the computational linguistics community, such as information 
extraction from web pages, part-of-speech tagging, protein and DNA sequence analy-
sis, and computer intrusion detection. 
4   Data 
4.1   Data Summary 
We use 52 pairs of parallel Chinese-English articles from LDC release. The 52 Chi-
nese articles from Xinhua News Service consist of 20626 Chinese characters in total 
with each article containing between about 340 and 400 Chinese characters The Chi-
nese documents are in Chinese Treebank format with catalog number LDC2001T11. 
The parallel English articles are from Multiple-Translation Chinese (MTC) Corpus 
from LDC with catalog number LDC2002T01. We use the best human translations 
out of 10 translation teams2 as our gold-standard parallel English data. 
                                                          
1
  For tense 13, it is controversial whether the event time precedes or succeeds the speech time. 
(e.g. for ?I was going to ask him at that time?, it is not clear whether the asking event has 
happened by the speech time.) This graph only represents the authors? hunch about the tense 
taxonomy for this particular project. 
2
  Two LDC personnel, one a Chinese-dominant bilingual and the other an English-dominant bi-
lingual, performed this ranking. There was overall agreement on the ranking between the two 
and minor discrepancies were resolved through discussion and comparison of additional files. 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 889 
4.2   Obtaining Tense Tags from the Data 
The decision of the granularity level of the data points in the current project is a non-
trivial issue. Recently it has been argued that tense should be regarded as a category 
of the whole sentence, or in logical terms of the whole proposition, since it relates to 
the truth value of the proposition as a whole, rather than just some property of the 
verb. While we agree with this assertion, in the interest of focusing on our immediate 
goal of assigning an appropriate tense tag to the parallel verb in the target language, 
we adopt the more traditional analysis of tense as a category of the verb on the basis 
of its morphological attachment to the verb.  
There are a total of 1542 verbs in the 52 Chinese source articles. We manually 
aligned these verbs in the Chinese source article with their corresponding verbs in 
English; this yields a subset of 712 verbs out of the 1542 verbs being translated into 
English as verbs. We see a dramatic nominalization (i.e. verbal expressions in Chi-
nese are translated into nominal phrases in English) process in Chinese-to-English 
translation through the dramatic contrast between these two numbers. We excluded 
the verbs that are not translated as verbs into the parallel English text. This exclusion 
is based on the rationale that another choice of syntactic structure might retain the 
verbal status in the target English sentence, but the tense of those potential English 
verbs would be left to the joint decision of a set of disparate features. Those tenses are 
unknown in our training data. 
5   Tense Tagging by Learning 
5.1   Temporal Reference Distinction in Chinese Text 
Assigning accurate tense tags to the English verbs in Chinese-to-English Machine 
Translation is equivalent to understanding temporal reference distinction in the source 
Chinese text. Since there are no morphologically realized tenses in Chinese, the tempo-
ral reference distinction in Chinese is encoded in disparate linguistic features. Figure 2 
shows how various features in simple Chinese sentences jointly represent the temporal 
reference distinction information. For complex sentences with an embedding structure, 
these  features  will  behave  in  a more complicated way in that the anaphoric relations  
 
Sentence-
final modal 
particle 
   verb + Post-verbal marker + 
Temporal 
adverbial 
+ 
Lexical properties (Vendler, 1967): 
activity, state, achievement and 
accomplishment  
 
Fig. 2. Temporal Structure for a Simple Chinese Sentence 
890 Y. Ye and Z. Zhang 
between the reference time and speech time hold differently for main verbs and verbs 
in embedded structure. While world knowledge is beyond the scope of our computa-
tional capacity at this stage, we expect that the various linguistic features will be able 
to approximately reconstruct the temporal reference distinction for Chinese verbs. 
5.2   The Feature Space 
There are a big variety of heterogeneous features that contribute to the temporal refer-
ence semantics of Chinese verbs. Tenses in English, while manifesting temporal ref-
erence distinction, do not always reflect the distinction at the semantic level, as is 
shown in the sentence ?I will leave when he comes.? Hornstein [7] accounted for this 
type of phenomenon by proposing the Constraints on Derived Tense Structures. 
Hence the feature space we propose to use consists of the features that contribute to 
the semantic level temporal reference construction as well as those contributing to the 
tense generation from that semantic level. 
The feature space includes the following 11 features: 
feature1: whether the current sentence contains a temporal noun phrase, a 
 temporal location phrase or a temporal prepositional phrase; 
feature2: whether or not the current verb is in quoted speech; 
feature3: whether the current verb appears in relative clause or sentential  
     complement; 
feature4: whether or not the current verb is in news headlines; 
feature5: previous word's POS; 
feature6: current verb's POS, there are three types of verbs in the corpora: the  
regular verbs (VV); the copula ?shi4?3 (VC) and the verb ?you3? (VE); 
feature7: next word's POS; 
feature8: whether or not the verb is followed by the aspect marker ?le?; 
feature9: whether or not the verb is followed by the aspect marker ?zhe?; 
feature10: whether or not the verb is followed by the aspect marker ?guo?; 
feature11: whether or not the verb is a main verb; 
The above 11 features include lexical features as well as syntactic features. None 
of the above features is expensive to obtain. We aim to show that the temporal refer-
ence distinction classe, as a semantic feature of the verb, could be predicted by learn-
ing from inexpensive linguistic features that are easily available. Feature 11 is moti-
vated by the observation that tense in English is used to inform the reader (listener) of 
when the event associating with the main verb occurs with respect to the time of ut-
terance while the tense of an embedded verb does not necessarily indicate this rela-
tionship directly. In the current paper, we have a different definition for main verb: 
any verb that is not in embedded structure is treated as a main verb including those 
verbs appearing in adjunct clauses. 
5.3   Learning Algorithm: Conditional Random Field 
Conditional Random Fields (CRF) is a formalism well-suited for learning and predic-
tion on sequential data. It is a probabilistic framework proposed by Lafferty [8] for 
                                                          
3
  The digit at the end of the syllable here indicates the tone. ?Shi4? means ?be? and ?you3? 
means ?have?. 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 891 
labeling and segmenting structured data, such as sequences, trees and lattices. The 
conditional nature of CRFs relaxes the independence assumptions required by tradi-
tional Hidden Markov Models (HMMs); CRFs also avoid the label bias problem ex-
hibited by maximum entropy Markov models (MEMMs) and other conditional 
Markov models based on directed graphical models. CRFs have been shown to per-
form well on a number of real-world problems, in particular, NLP problems such as 
shallow parsing [9], table extraction [10], and named entity recognition [11]. 
For our experiments, we use the off-the-shelf implementation of CRFs provided by 
MALLET [12].  
6   Experiments and Evaluation 
6.1   Preliminary Experiment with Tense Annotation by Human Judges 
In order to evaluate the empirical challenge of tense generation in a Chinese-to-
English Machine Translation system, a pilot experiment of tense annotation for Chi-
nese text by native judges was carried. The annotation experiment was carried out on 
20 news articles from LDC Xinhua News release with category number 
LDC2001T11. The articles were divided into 4 groups with 5 articles in each group. 
For each group, three native Chinese speakers annotated the tense of the verbs in the 
articles. Prior to annotating the data, the judges underwent brief training during which 
they were asked to read an example of a Chinese sentence for each tense and make 
sure they understand the examples. During the annotation, the judges were asked to 
read whole articles first and then select a tense tag based on the context of each verb. 
In cases where the judges were unable to decide the tense of a verb, they were in-
structed to tag it as ?unknown?.  
Kappa scores were calculated for the three human judges? annotation results. 
Kappa score is the de facto standard for evaluating inter-judge agreement on tagging 
tasks. It is defined by the following formula (1), where P(A) is the observed agree-
ment among the judges and P(E) is the expected agreement: 
)(1
)()(
EP
EPAPk
?
?
=  (1) 
The annotation was originally carried out on the taxonomy of 13 tenses. We col-
lapsed these 13 tenses into three tenses as discussed in section 3.1. Table 1 summa-
rizes the kappa statistics for the human annotation results after we collapse the tenses: 
Table 1. Kappa Scores for Human Tense Annotation for Xinhua News on Collapsed Tense 
Classes 
 
Xinhua 
news 1 
Xinhua 
news 2 
Xinhua 
news 3 
Xinhua 
news 4 
Kappa score 
for 3 judges 0.409 0.440 0.317 0.325 
892 Y. Ye and Z. Zhang 
There are different interpretations as to what is a good level of agreement and what 
kappa scores are considered low. But generally, a kappa score of lower than 0.40 falls 
into the lower range of agreement4. Even if we consider the meta-linguistic nature of 
the task, the kappa scores we observe belong to the poor-fair range of agreement, il-
lustrating the challenge of temporal reference mapping across Chinese and English. 
The difficulty of tense classification demonstrated by these experiments with human 
judges provides an upper bound on the performance of automatic machine classifica-
tion. As challenging a task as it is, tense generation for English verbs in a Chinese-to-
English Machine Translation system must address this cross-lingual mapping problem 
in order to obtain an accurate translation result. 
6.2   Experimental Setup and Evaluation Metrics 
It is conceivable that the granularity of sequences may matter in learning from data 
with sequential relationship, and in the context of verb tense tagging, it naturally maps 
to the granularity of discourse. Based on this conjecture, we experiment with two dif-
ferent sequential granularities: 
? Sentence-level sequence: each sentence is treated as a sequence; 
? Paragraph-level sequence: each sentence is treated as a sequence, and there is no 
boundary between sentences within the paragraph. 
All results are obtained by 5-fold cross validation. The classifier?s performance is 
evaluated against the tenses from the best-ranked human translation parallel English 
text. 
To evaluate the performance of classifiers, we measure the standard classification 
accuracy where accuracy is defined as in equation (2): 
spredictionofnumbertotal
spredictioncorrectofnumber
accuracy =  (2) 
To measure how well the classifier does on each class respectively, we compute 
precision, recall, and F-measure, which are defined respectively in equation (3), (4) 
and (5): 
hitsofnumbertotal
hitscorrectofnumber
ecision =Pr  (3) 
hitlistperfectofsize
hitscorrectofnumber
call =Re  (4) 
callecision
callecision
measureF
RePr
RePr2
+
??
=?  (5) 
                                                          
4
 http://www.childrens-mercy.org/stats/definitions/kappa.htm 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 893 
6.3   Experimental Results 
The evaluation is carried on the collapsed tense taxonomy that consists of three tense 
classes: present, past and future. This collapse is motivated by two reasons: linguisti-
cally, this collapse reflects the accommodation of the ?gray area? that exists in the 13-
way tense taxonomy; practically, the collapse helps to alleviate the sparse data prob-
lem. Ideally, with a large enough data set that could cover the less-common tenses, 
the full-fledged tense taxonomy is desirable given that the ?gray area? could be ana-
lyzed and included into the evaluation. The CRF-based tense classifier yielded the 
performance in Table 2 and Table 3: 
Table 2. Sentence-level sequence: overall accuracy 58.21% 
 Precision Recall F-measure 
Present 42.50% 27.48% 32.07% 
Past 67.57% 79.55% 72.10% 
Future 29.66% 25.56% 21.56% 
Table 3. Paragraph-level sequence: overall accuracy 58.05% 
 Precision Recall F-measure 
Present 38.79% 32.44% 33.96% 
Past 69.12% 75.72% 71.59% 
Future 33.16% 30.25% 26.59% 
An accuracy of around 60% seems not satisfactory if viewed in isolation, but when 
contrasted with the kappa score of human tense annotating discussed above, the cur-
rent evaluation indicates promising results for our algorithm. Even though the human 
judges underwent only minimal training, their poor-to-fair kappa scores indicate that 
this is a very hard problem. Therefore while there is certainly room for improvement, 
the tagging performance of our algorithm is quite promising. 
It is noticed that the granularity of sequences does not seem to yield significantly 
different performance based on the current data. However, whether this is true in gen-
eral remains an open question. 
7   Discussions 
There are four important dimensions for any natural language processing tasks: 
? The data: ideally, the data used should be as representative as possible of a 
wide range of genres unless the target application is focused on a certain nar-
row domain; 
? The feature space: ideally, the features should be easily available and have 
wide coverage over the predicting space of the target problem; the more so-
894 Y. Ye and Z. Zhang 
phisticated and the more expensive the features are, the less we could claim to 
gain from the learning algorithms. 
? The learning algorithm: nowadays, various machine-learning algorithms have 
been proposed and applied in different natural language task domains. A learn-
ing algorithm should be chosen to appropriately explore the feature space. 
? The evaluation: ideally, evaluation from multiple perspectives is desired to re-
solve disagreements. 
Reflecting upon these dimensions for the current paper, from the data perspective, 
we focused on news report genre where the temporal thread progression is relatively 
simpler than many other genres. When facing temporal reference classification for 
more complicated genres, larger amounts of training data would be necessary for 
learning a more sophisticated classifier. Fortunately, the amount of accessible parallel 
data is growing and it is always possible to obtain the tense tags for the Chinese verbs 
automatically using an off-the-shelf aligning tool although this might introduce a cer-
tain amount of noise.  
As for the choice of the predicting features, the current project does not utilize any 
lexical semantic features owing to the limited lexical semantic knowledge resources 
for Chinese. We expect such knowledge resources, if available, would enhance the 
feature vector and boost the classification performance. Additionally, it is observed 
that for a Chinese-to-English MT system, tense generation in English is significantly 
subject to the syntactic constraints. Hence when integrating into a MT system, the 
current learning algorithm might have opportunity to employ additional features from 
other parts of the system, for example, syntactic features for English could be added 
to the current feature space. 
Regarding the choice of learning algorithm, we chose CRFs, a learning algorithm 
for sequential data, based on the fact that tenses for verbs in a certain discourse unit 
are not independent of each other.  
From the evaluation point of view, the current work evaluates the classifier against 
the tenses from a certain human translation team. The frequent disagreements among 
the human annotators illustrate the difficulty of constructing a gold standard against 
which to evaluate the performance of our classifier. Lastly, measuring BLEU score 
change brought about by integrating the current classifier into a statistical MT system 
would be desirable, such that we can better understand the practical implications of 
this study for MT systems. 
8   Conclusions and Future Work 
The current work has shown how a moderate set of shallow and inexpensive linguistic 
features can be combined with a standard machine learning algorithm for learning a 
tense classifier trained on a moderate number of data points, with promising results. A 
tense resolution module built upon the current framework could enhance a MT system 
with its temporal reference distinction resolution.  
Several issues to be explored in future work are the following: First, our current 
training corpus of Xinhua News articles is rather homogeneous, hence the classifier 
trained exclusively on this data set may not be robust when carried over to data from 
different source. This will become particularly important if we want to integrate the 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 895 
current work into a general-domain MT system. Secondly, related to the homogeneity 
of our training data, we only explored a limited number of features, while the feature 
space could be expanded to include a richer and wider scope. For example, discourse 
structure features have not been explored. Finally, we are very interested in evaluating 
our work against existing MT systems with regard to temporal mapping. 
References 
1. Campbell, R., Aikawa, T., Jiang, Z., Lozano, C., Melero, M and Wu, A.: A Language-
Neutral Representation of Temporal Information. In Proceedings of the Workshop on An-
notation Standards for Tempora Information in Natural Language, LREC 2002, Las Pal-
mas de Gran Canaria, Spain (2002) 13-21.  
2. Pustejovsky, J., Ingria, B., Sauri, R., Castano, J., Littman, J., Gaizauskas, R., Setzer, A., 
Katz, G. and Mani, I.: The Specification Language TimeML. In Mani, I., Pustejovsky, J., 
and Gaizauskas, R (eds.). (2004) The Language of Time: A Reader. Oxford University 
Press, to appear 
3. Mani, I.: "Recent Developments in Temporal Information Extraction (Draft)", In Nicolov, 
N., and Mitkov, R. Proceedings of RANLP'03, John Benjamins, to appear. 
4. Olson, M., Traum, D., Van-ess Dykema, C. and Weinberg, A.: Implicit Cues for Explicit 
Generation: Using Telicity as a Cue for Tense Structure in a Chinese to English MT System, 
in proceedings Machine Translation Summit VIII, Santiago de Compostela (Spain) (2001)  
5. Li, W., Wong, K. F., Hong, C. and Yuan, C.: Applying Machine Learning to Chinese 
Temporal Relation Resolution, Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (2004) 582-588 
6. Reichenbach, H.: Elements of Symbolic Logic, The Macmillan Company (1947) 
7. Dorr, B. J. and Gaasterland, T.: "Constraints on the Generation of Tense, Aspect, and Con-
necting Words from Temporal Expressions," Technical Report CS-TR-4391, UMIACS-TR-
2002-71, LAMP-TR-091, University of Maryland, College Park, MD (2002) 
8. Lafferty, J., McCallum, A. and Pereira, F.: Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In Proceedings of ICML-01, (2001) 282-289 
9. Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields, In Proceedings 
of the 2003 Human Language Technology Conference and North American Chapter of the 
Association for Computational Linguistics (HLT/NAACL-03) (2003) 
10. Pinto, D., McCallum, A., Lee, X. and Croft, W. B.: Table Extraction Using Conditional 
Random Fields. In Proceedings of the 26th Annual International ACM SIGIR Conference 
on Research and Development in Information Retrieval (SIGIR 2003) (2003)  
11. McCallum, A. and Li, W.: Early Results for Named Entity Recognition with Conditional 
Random Fields, Feature Induction and Web-Enhanced Lexicons. In Proceedings of the 
Seventh Conference on Natural Language Learning (CoNLL) (2003) 
12. McCallum, A. K.: MALLET: A Machine Learning for Language Toolkit 
http://mallet.cs.umass.edu. (2002) 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Latent Features in Automatic Tense Translation between Chinese and
English
Yang Ye?, Victoria Li Fossum?, Steven Abney ? ?
? Department of Linguistics
? Department of Electrical Engineering and Computer Science
University of Michigan
Abstract
On the task of determining the tense to use
when translating a Chinese verb into En-
glish, current systems do not perform as
well as human translators. The main focus
of the present paper is to identify features
that human translators use, but which are
not currently automatically extractable.
The goal is twofold: to test a particu-
lar hypothesis about what additional infor-
mation human translators might be using,
and as a pilot to determine where to focus
effort on developing automatic extraction
methods for features that are somewhat be-
yond the reach of current feature extrac-
tion. The paper shows that incorporating
several latent features into the tense clas-
sifier boosts the tense classifier?s perfor-
mance, and a tense classifier using only the
latent features outperforms one using only
the surface features. Our findings confirm
the utility of the latent features in auto-
matic tense classification, explaining the
gap between automatic classification sys-
tems and the human brain.
1 Introduction
Language speakers make two types of distinctions
about temporal relations: the first type of relation
is based on precedence between events and can be
expanded into a finer grained taxonomy as pro-
posed by (Allen, 1981). The second type of re-
lation is based on the relative positioning between
the following three time parameters proposed by
(Reichenbach, 1947): speech time (S), event time
(E) and reference time (R). In the past couple of
decades, the NLP community has seen an emer-
gent interest in the first type of temporal relation.
In the cross-lingual context, while the first type of
relationship can be easily projected across a lan-
guage pair, the second type of relationship is of-
ten hard to be projected across a language pair. In
contrast to this challenge, cross-lingual temporal
reference distinction has been poorly explored.
Languages vary in the granularity of their
tense and aspect representations; some have finer-
grained tenses or aspects than others. Tense gener-
ation and tense understanding in natural language
texts are highly dynamic and context-dependent
processes, since any previously established time
point or interval, whether explicitly mentioned in
the context or not, could potentially serve as the
reference time for the event in question. (Bruce,
1972) captures this nature of temporal reference
organization in discourse through a multiple tem-
poral reference model. He defines a set (S
1
, S
2
, ...,
Sn) that is an element of tense. S1 corresponds to
the speech time, Sn is the event time, and (Si, i=2,
..., n-1) stand for a sequence of time references
from which the reference time of a particular event
could come. Given the elusive nature of reference
time shift, it is extremely hard to model the ref-
erence time point directly in temporal information
processing. The above reasons motivate classify-
ing temporal reference distinction automatically,
using machine learning algorithms such as Con-
ditional Random Fields (CRFs).
Many researchers in Natural Language Process-
ing seem to believe that an automatic system does
not have to follow the mechanism of human brain
in order to optimize its performance, for example,
the feature space for an automatic classification
system does not have to replicate the knowledge
sources that human beings utilize. There has been
very little research that pursues to testify this faith.
The current work attempts to identify which
features are most important for tense generation
in Chinese to English translation scenario, which
can point to direction of future research effort for
automatic tense translation between Chinese and
English.
48
The remaining part of the paper is organized
as follows: Section 2 summarizes the significant
related works in temporal information annotation
and points out how this study relates to yet dif-
fers from them. Section 3 formally defines the
problem, tense taxonomy and introduces the data.
Section 4 discusses the feature space and proposes
the latent features for the tense classification task.
Section 5 presents the classification experiments
in Conditional Random Fields as well as Classifi-
cation Tree and reports the evaluation results. Sec-
tion 6 concludes the paper and section 7 points out
directions for future research.
2 Related Work
There is an extensive literature on temporal infor-
mation processing. (Mani, et al, 2005) provides
a survey of works in this area. Here, we high-
light several works that are closely related to Chi-
nese temporal information processing. (Li, 2001)
describes a model of mining and organizing tem-
poral relations embedded in Chinese sentences,
in which a set of heuristic rules are developed to
map linguistic patterns to temporal relations based
on Allen?s thirteen relations. Their work shows
promising results via combining machine learning
techniques and linguistic features for successful
temporal relation classification, but their work is
concerned with another type of temporal relation-
ship, namely, the precedence-based temporal rela-
tion between a pair of events explicitly mentioned
in text.
A significant work worth mentioning is (Olsen
et. al. 2001)?s paper, where the authors exam-
ine the determination of tense for English verbs
in Chinese-to-English translation. In addition to
the surface features such as the presence of aspect
markers and certain adverbials, their work makes
use of the telicity information encoded in the lexi-
cons through the use of Lexical Conceptual Struc-
tures (LCS). Based on the dichotomy of grammat-
ical aspect and lexical aspect, they propose that
past tense corresponds to the telic (either inher-
ently or derived) LCS. They propose a heuristic
algorithm in which grammatical aspect markings
supersede the LCS, and in the absence of gram-
matical aspect marking, verbs that have telic LCS
are translated into past tense and present tense oth-
erwise. They report a significant performance im-
provement in tense resolution from adding a verb
telicity feature. They also achieve better perfor-
mance than the baseline system using the telic-
ity feature alone. This work, while alerting re-
searchers to the importance of lexical aspectual
feature in determination of tense for English verbs
in Chinese-to-English machine translation, is sub-
ject to the risk of adopting a one-to-one mapping
between grammatical aspect markings and tenses
hence oversimplifies the temporal reference prob-
lem in Chinese text. Additionally, their binary
tense taxonomy is too coarse for the rich tempo-
ral reference system in Chinese.
(Ye, et al 2005) reported a tense tagging case
study of training Conditional Random Fields on
a set of shallow surface features. The low inter-
annotator agreement rate reported in the paper il-
lustrates the difficulty of tense tagging. Neverthe-
less, the corpora size utilized is too small with only
52 news articles and none of the latent features was
explored, so the evaluation result reported in the
paper leaves room for improvement.
3 Problem Definition
3.1 Problem Formulation
The problem we are interested in can be formal-
ized as a standard classification or labeling prob-
lem, in which we try to learn a classifier
C : V ? T (1)
where V is a set of verbs (each described by a
feature vector), and T is the set of possible tense
tags.
Tense and aspect are morphologically merged
in English and coarsely defined, there can be
twelve combinations of the simple tripartite tenses
(present, past and future) with the progressive and
perfect grammatical aspects. For our classification
experiments, in order to combat sparseness, we ig-
nore the aspects and only deal with the three sim-
ple tenses: present, past and future.
3.2 Data
We use 152 pairs of parallel Chinese-English arti-
cles from LDC release. The Chinese articles come
from two news sources: Xinhua News Service and
Zaobao News Service, consisting of 59882 Chi-
nese characters in total with roughly 350 charac-
ters per article. The English parallel articles are
from Multiple-Translation Chinese (MTC) Corpus
from LDC with catalog number LDC2002T01.
We chose to use the best human translation out
49
of 9 translation teams as our gold-standard par-
allel English data. The verb tenses are obtained
through manual alignment between the Chinese
source articles and the English translations. In or-
der to avoid the noise brought by errors and be fo-
cused on the central question we try to answer in
the paper, we did not use automatic tools such as
GIZA++ to obtain the verb alignments, which typ-
ically comes with significant amount of errors. We
ignore Chinese verbs that are not translated into
English as verbs because of ?nominalization? (by
which verbal expressions in Chinese are translated
into nominal phrases in English). This exclusion is
based on the rationale that another choice of syn-
tactic structure might retain the verbal status in the
target English sentence, but the tense of those po-
tential English verbs would be left to the joint de-
cision of a set of disparate features. Those tenses
are unknown in our training data. This preprocess-
ing yields us a total of 2500 verb tokens in our data
set.
4 Feature Space
4.1 Surface Features
There are many heterogeneous features that con-
tribute to the process of tense generation for Chi-
nese verbs in the cross-lingual situation. Tenses in
English, while manifesting a distinction in tempo-
ral reference, do not always reflect this distinction
at the semantic level, as is shown in the sentence ?I
will leave when he comes.? (Hornstein, 1990) ac-
counts for this phenomenon by proposing the Con-
straints on Derived Tense Structures. Therefore,
the feature space we use includes the features that
contribute to the semantic level temporal reference
construction as well as those contributing to tense
generation from that semantic level. The follow-
ing is a list of the surface features that are directly
extractable from the training data:
1. Feature 1: Whether the verb is in quoted
speech or not.
2. Feature 2: The syntactic structure in which
the current verb is embedded. Possible struc-
tures include sentential complements, rel-
ative clauses, adverbial clauses, appositive
clauses, and null embedding structure.
3. Feature 3: Which of the following signal
adverbs occur between the current verb
and the previous verb: yi3jing1(already),
ceng2jing1(once), jiang1(future tense
marker), zheng4zai4(progressive aspect
marker), yi4zhi2(have always been).
4. Feature 4: Which of the following aspect
markers occur between the current verb and
the subsequent verb: le0, zhe0, guo4.
5. Feature 5: The distance in characters between
the current verb and the previously tagged
verb (We descretize the continuous distance
into three ranges: 0 < distance < 5, 5 ?
distance < 10, or 10 ? distance <?).
6. Feature 6: Whether the current verb is in the
same clause as the previous verb.
Feature 1 and feature 2 are used to capture the
discrepancy between semantic tense and syntactic
tense. Feature 3 and feature 4 are clues or triggers
of certain aspectual properties of the verbs. Fea-
ture 5 and feature 6 try to capture the dependency
between tenses of adjacent verbs.
4.2 Latent Features
The bottleneck in Artificial Intelligence is the un-
balanced knowledge sources shared by human be-
ings and a computer system. Only a subset of
the knowledge sources used by human beings can
be formalized, extracted and fed into a computer
system. The rest are less accessible and are very
hard to be shared with a computer system. De-
spite their importance in human language process-
ing, latent features have received little attention in
feature space exploration in most NLP tasks be-
cause they are impractical to extract. Although
there have not yet been rigorous psycholinguis-
tic studies demonstrating the extent to which the
above knowledge types are used in human tempo-
ral relation processing, we hypothesize that they
are very significant in assisting human?s temporal
relation decision. Nevertheless, a quantitative as-
sessment of the utility of the latent features in NLP
tasks has yet to be explored. (Olsen, et al, 2001)
illustrates the value of latent features by showing
how the telicity feature alone can help with tense
resolution in Chinese to English machine transla-
tion. Given the prevalence of latent features in hu-
man language processing, in order to emulate hu-
man beings performance of the disambiguation, it
is crucial to experiment with the latent features in
automatic tense classification.
(Pustejovsky, 2004) discusses the four basic
problems in event-temporal identification:
50
??????????????????????????????????
??????????
He said that Henan Province not only possesses the hardwares necessary for foreign 
investment, but also has, on the basis of the State policies and Henan's specific 
conditions, formulated its own preferential policies.
? ?? ???? ??
N/A include subsume
?? ??
Figure 1: Temporal Relations between Adjacent Events
1. Time-stamping of events (identifying an
event and anchoring it in time)
2. Ordering events with respect to one another
3. Reasoning with contextually under-specified
temporal expressions
4. Reasoning about the persistence of events
(how long does an event or the outcome of
an event last?)
While time-stamping of the events and reason-
ing with contextually under-specified temporal ex-
pressions might be too informative to be features
in tense classification, information concerning or-
derings between events and persistence of events
are relatively easier to be encoded as features in
a tense classification task. Therefore, we exper-
iment with these two latent knowledge sources,
both of which are heavily utilized by human be-
ings in tense resolution.
4.3 Telicity and Punctuality Features
Following (Vendler, 1947), temporal information
encoded in verbs is largely captured by some in-
nate properties of verbs, of which telicity and
punctuality are two very important ones. Telic-
ity specifies a verb?s ability to be bound in a cer-
tain time span, while punctuality specifies whether
or not a verb is associated with a point event in
time. Telicity and punctuality prepare verbs to be
assigned different tenses when they enter the con-
text in the discourse. While it is true that isolated
verbs are typically associated with certain telicity
and punctuality features, such features are contex-
tually volatile. In reaction to the volatility exhib-
ited in verb telicity and punctuality features, we
propose that verb telicity and punctuality features
should be evaluated only at the clausal or senten-
tial level for the tense classification task. We man-
ually obtained these two features for both the En-
glish and the Chinese verbs. All verbs in our data
set were manually tagged as ?telic? or ?atelic?, and
?punctual? or ?apunctual?, according to context.
4.4 Temporal Ordering Feature
(Allen, 1981) defines thirteen relations that could
possibly hold between any pair of situations. We
experiment with six temporal relations which we
think represent the most typical temporal relation-
ships between two events. We did not adopt all
of the thirteen temporal relationships proposed by
Allen for the reason that some of them would re-
quire excessive deliberation from the annotators
and hard to implement. The six relationships we
explore are as follows:
1. event A precedes event B
2. event A succeeds event B
3. event A includes event B
4. event A subsumes event B
5. event A overlaps with event B
6. no temporal relations between event A and
event B
For each Chinese verb in the source Chinese
texts, we annotate the temporal relation between
the verb and the previously tagged verb as belong-
ing to one of the above classes. The annotation
of the temporal relation classes mimics a deeper
semantic analysis of the Chinese source text. Fig-
ure 1 illustrates a sentence in which each verb is
tagged by the temporal relation class that holds be-
tween it and the previous verb.
51
5 Experiments and Evaluation
5.1 CRF learning algorithms
Conditional Random Fields (CRFs) are a formal-
ism well-suited for learning and prediction on se-
quential data in many NLP tasks. It is a prob-
abilistic framework proposed by (Lafferty et al,
2001) for labeling and segmenting structured data,
such as sequences, trees and lattices. The condi-
tional nature of CRFs relaxes the independence as-
sumptions required by traditional Hidden Markov
Models (HMMs). This is because the conditional
model makes it unnecessary to explicitly represent
and model the dependencies among the input vari-
ables, thus making it feasible to use interacting and
global features from the input. CRFs also avoid
the label bias problem exhibited by maximum en-
tropy Markov models (MEMMs) and other con-
ditional Markov models based on directed graph-
ical models. CRFs have been shown to perform
well on a number of NLP problems such as shal-
low parsing (Sha and Pereira, 2003), table extrac-
tion (Pinto et al, 2003), and named entity recog-
nition (McCallum and Li, 2003). For our exper-
iments, we use the MALLET implementation of
CRF?s (McCallum, 2002).
5.2 Experiments
5.2.1 Human Inter-Annotator Agreement
All supervised learning algorithms require a
certain amount of training data, and the reliability
of the computational solutions is intricately tied
to the accuracy of the annotated data. Human an-
notations typically suffer from errors, subjectivity,
and the expertise effect. Therefore, researchers
use consistency checking to validate human an-
notation experiments. The Kappa Statistic (Co-
hen, 1960) is a standard measurement of inter-
annotator agreement for categorical data annota-
tion. The Kappa score is defined by the following
formula, where P(A) is the observed agreement
rate from multiple annotators and P(E) is the ex-
pected rate of agreement due to pure chance:
k = P (A)? P (E)
1? P (E)
(2)
Since tense annotation requires disambiguating
grammatical meaning, which is more abstract than
lexical meaning, one would expect the challenge
posed by human annotators in a tense annota-
tion experiment to be even greater than for word
sense disambiguation. Nevertheless, the tense an-
notation experiment carried as a precursor to our
tense classification task showed a kappa Statistic
of 0.723 on the full taxonomy, with an observed
agreement of 0.798. In those experiments, we
asked three bilingual English native speakers who
are fluent in Chinese to annotate the English verb
tenses for the first 25 Chinese and English parallel
news articles from our training data.
We could also obtain a measurement of reliabil-
ity by taking one annotator as the gold standard
at one time, then averaging over the precisions of
the different annotators across different gold stan-
dards. While it is true that numerically, precision
would be higher than Kappa score and seems to
be inflating Kappa score, we argue that the dif-
ference between Kappa score and precision is not
limited to one measure being more aggressive than
the other. Rather, the policies of these two mea-
surements are different. The Kappa score cares
purely about agreement without any consideration
of trueness or falseness, while the procedure we
described above gives equal weight to each anno-
tator being the gold standard, and therefore con-
siders both agreement and truthness of the annota-
tion. The advantage of the precision-based agree-
ment measurement is that it makes comparison of
the system performance accuracy to the human
performance accuracy more direct. The precision
under such a scheme for the three annotators is
80% on the full tense taxonomy.
5.2.2 CRF Learning Experiments
We train a tense classifier on our data set in two
stages: first on the surface features, and then on
the combined space of both surface features (dis-
cussed in 4.1) and latent features (discussed in 4.2-
4.4). It is conceivable that the granularity of se-
quences may matter in learning from data with se-
quential relationship, and in the context of verb
tense tagging, it naturally maps to the granularity
of discourse. (Ye, et al, 2005) shows that there
is no significant difference between sentence-level
sequences and paragraph-level sequences. There-
fore, we experiment with only sentence-level se-
quences.
5.2.3 Classification Tree Learning
Experiments
To verify the stability of the utility of the la-
tent features, we also experiment with classifica-
tion tree learning on the same features space as
52
Tense Precision Recall F
Present tense 0.662 0.661 0.627
Past tense 0.882 0.915 0.896
Future tense 0.758 0.487 0.572
Table 1: Evaluation Results for CRFs Classifier in Precision, Recall and F Using All Features
Surface Features Latent Features Surface and Latent Features
Accuracy for Training Data 79.3% 82.9% 85.9%
Table 2: Apparent Accuracy for the Training Data of the Classification Tree Classifiers
discussed above. Classification Trees are used
to predict membership of cases or objects in the
classes of a categorical dependent variable from
their measurements on one or more predictor vari-
ables. The main idea of Classification Tree is to
do a recursive partitioning of the variable space
to achieve good separation of the classes in the
training dataset. We use the Recursive Partition-
ing and Regression Trees(Rpart) package provided
by R statistical computing software for the imple-
mentation of classification trees. In order to avoid
over-fitting, we prune the tree by setting the min-
imum number of objects in a node to attempt a
split and the minimum number of objects in any
terminal node to be 10 and 3 respectively. In the
constructed classification tree when we use all fea-
tures including both surface and latent features,
the top split at the root node in the tree is based
on telicity feature of the English verb, indicating
the importance of telicity feature for English verb
among all of the features.
5.3 Evaluation Results
All results are obtained by 5-fold cross validation.
The classifier?s performance is evaluated against
the tenses from the best-ranked human-generated
English translation. To evaluate the performance
of the CRFs tense classifier, we compute the pre-
cision, recall, general accuracy and F, which are
defined as follow.
Accuracy =
nprediction
Nprediction
(3)
Recall = nhit
S
(4)
Precision = nhit
Nhit
(5)
F = 2? Precision ?Recall
Precision + Recall
(6)
where
1. Nprediction: Total number of predictions;
2. nprediction: Number of correct predictions;
3. Nhit: Total number of hits;
4. nhit: Number of correct hits;
5. S: Size of perfect hitlist;
From Table 1, we see that past tense, which oc-
curs most frequently in the training data, has the
highest precision, recall and F. Future tense, which
occurs least frequently, has the lowest F. Precision
and recall do not show clear pattern across differ-
ent tense classes.
Table 2 presents the apparent classification ac-
curacies for the training data, we see that latent
features still outperform the surface features. Ta-
ble 3 summarizes the general accuracies of the
tense classification systems for CRFs and Classifi-
cation Trees. The CRFs classifier and the Classifi-
cation Tree classifier demonstrate similar scales of
improvement from surface features, latent features
to both surface and latent features.
53
Methodology Surface Features Latent Features Surface and Latent Features
CRFs 75.8% 80% 83.4%
Classification Tree 74.1% 81% 84.5%
Table 3: Evaluations in General Accuracy
5.4 Baseline Systems
To better evaluate our tense classifiers, we provide
two baseline systems here. The first baseline sys-
tem is the tense resolution from the best ranked
machine translation system?s translation results in
the MTC corpus mentioned above. When evalu-
ated against the reference tense tags from the best
ranked human translation team, the best MT sys-
tem yields a accuracy of 47%. The second base-
line system is a naive system that assigns the most
frequent tense in the training data set, which in our
case is past tense, to all verbs in the test data set.
Given the fact that we are deadling with newswire
data, this baseline system yields a high baseline
system with an accuracy of 69.5%.
6 Discussion and Conclusions
To the best of our knowledge, the current paper
is the first work investigating the utility of latent
features in the task of machine-learning based au-
tomatic tense classification. We significantly out-
perform the two baseline systems as well as the
automatic tense classifier performance reported by
(Ye, et al, 2005) by 15% in general accuracy. A
crucial finding of our experiments is that utility of
only three latent features, i.e. verb telicity, verb
punctuality and temporal ordering between adja-
cent events, outperforms that of all the surface
linguistic features we discussed earlier in the pa-
per. While one might think that the lack of exist-
ing techonology of latent feature extraction would
discount research effort on latent features? utili-
ties, we believe that such efforts guide the research
community to determine where to focus effort on
developing automatic extraction methods for fea-
tures that are beyond the reach of current tech-
nologies. Such research effort will also help to
shed light on the enigmatic research question of
whether automatic NLP systems should take ef-
fort to make use of the features employed by hu-
man beings to optimize the system performance
and shorten the gap between the system and hu-
man brain. The results of the current paper point
to the fact that bottleneck of cross-linguistic tense
classification is acquisition and modeling of the
more latent linguistic knowledge. To our surprise,
CRF tense classifier performance is consistently
tied with classification tree tense classifier perfor-
mance in all of our experiments. One might expect
that CRFs would accurately capture sequential de-
pendencies among verbs. Reflecting upon the sim-
ilar evaluation results of the CRFs classifier and
the Classification Tree classifier, it is unlikely for
this to be due to the over-fitting of the Classifi-
cation Tree because of the pruning we did to the
Classification Trees. Therefore, we speculate that
the dependencies between the tense tags of verbs
in the texts may not be strong enough for CRFs
to outperform Classification Tree. This might also
be contributable to the built-in variable selection
procedures of Classification Trees, which makes
it more robust to interacting and interdependent
features. A confirmative explanation towards the
equal performances between the CRFs and the
Classification Tree classifiers requires more exper-
iments with other machine learning algorithms.
In conclusion, this paper makes the following
contributions:
1. It demonstrates that an accurate tense classi-
fier can be constructed automatically by com-
bining off-the-shelf machine learning tech-
niques and inexpensive linguistic features.
2. It shows that latent features (such as verb
telicity, verb punctuality and temporal order-
ing between adjacent events) have higher util-
ity in tense classification than the surface lin-
guistic features.
3. It reveals that the sequential dependency be-
tween tenses of adjacent verbs in the dis-
course may be rather weak.
54
7 Future Work
Temporal reference is a complicated semantic do-
main with rich connections among the disparate
features. We investigate three latent features:
telicity, punctuality, and temporal ordering be-
tween adjacent verbs. We summarize several in-
teresting questions for future research in this sec-
tion. First, besides the latent features we examined
in the current paper, there are other interesting
latent features to be investigated under the same
theme, e.g. classes of temporal expression associ-
ated with the verbs and causal relationships among
disparate events. Second, currently, the latent fea-
tures are obtained through manual annotation by
a single annotator. In an ideal situation, multi-
ple annotators are desired to provide the reliabil-
ity of the annotations as well as reduce the noise
in annotations. Thirdly, it would be interesting to
examine the utility of the same latent features for
classification in the opposite direction, namely, as-
pect marker classification for Chinese verbs in the
English-to-Chinese translation scenario. Finally,
following our discussion of the degree of depen-
dencies among verb tenses in the texts, it is desir-
able to study rigorously the dependencies among
tenses and aspect markers for verbs in extensions
of the current research.
References
James Allen. 1981. Towards a General Theory of Ac-
tion and Time. Artificial Intelligence, 23(2): 123-
160.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York, N.Y.
B. Bruce. 1972. A Model for Temporal Reference and
its Application in a Question Answering System, Ar-
tificial Intelligence. Vol. 3, No. 1, 1-25.
Inderjeet Mani, James Pustejovsky and Robert
Gaizauskas. 2005. The Language of Time, Oxford
Press.
Wenjie Li, Kam-Fai Wong, Caogui Hong, Chunfa
Yuan. 2004. Applying Machine Learning to Chinese
Temporal Relation Resolution. Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, 582-588.
Mari Olson, David Traum, Carol Van-ess Dykema,
and AmyWeinberg. 2001. Implicit Cues for Explicit
Generation: Using Telicity as a Cue for Tense Struc-
ture in a Chinese to English MT System. Proceed-
ings Machine Translation Summit VIII, Santiago de
Compostela, Spain.
Yang Ye, Zhu Zhang. 2005. Tense Tagging for Verbs in
Cross-Lingual Context: a Case Study. Proceedings
of IJCNLP 2005, 885-895
Norbert Hornstein. 1990. As Time Goes By: Tense and
Universal Grammar. The MIT Press.
James Pustejovsky, Robert Ingria, Roser Sauri, Jose
Castano, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, and Inderjeet Mani. 2004. The
Specification Language TimeML. The Language of
Time: A Reader. Oxford, 185-96.
Zeno Vendler. 1967. Verbs and Times. Linguistics in
Philosophy, 97-121.
Lafferty, J., McCallum, A. and Pereira, F. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-01, 282-289.
Sha, F. and Pereira, F. 2003. Shallow Parsing with
Conditional Random Fields. Proceedings of the
2003 Human Language Technology Conference and
North American Chapter of the Association for
Computational Linguistics (HLT/NAACL-03)
Pinto, D., McCallum, A., Lee, X. and Croft, W. B.
2003. Table Extraction Using Conditional Random
Fields. Proceedings of the 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 2003)
McCallum, A. and Li, W. 2003. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL)
McCallum, A. K. 2002. MALLET: A Ma-
chine Learning for Language Toolkit,
http://mallet.cs.umass.edu.
Jacob Cohen, 1960. A Coefficient of Agreement for
Nominal Scales, Educational and Psychological
Measurement, 20, 37-46.
Ross Ihaka and Robert Gentleman. 1996. R: A Lan-
guage for Data Analysis and Graphics, Journal
of Computational and Graphical Statistics, Vol. 5.
299?14.
55
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 13?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
How and Where do People Fail with Time: Temporal Reference Mapping
Annotation by Chinese and English Bilinguals
Yang Ye?, Steven Abney??
?Department of Linguistics
?Department of Electrical Engineering and Computer Science
University of Michigan
Abstract
This work reports on three human tense
annotation experiments for Chinese verbs
in Chinese-to-English translation scenar-
ios. The results show that inter-annotator
agreement increases as the context of the
verb under the annotation becomes in-
creasingly specified, i.e. as the context
moves from the situation in which the tar-
get English sentence is unknown to the
situation in which the target lexicon and
target syntactic structure are fully speci-
fied. The annotation scheme with a fully
specified syntax and lexicon in the tar-
get English sentence yields a satisfactorily
high agreement rate. The annotation re-
sults were then analyzed via an ANOVA
analysis, a logistic regression model and a
log-linear model. The analyses reveal that
while both the overt and the latent linguis-
tic factors seem to significantly affect an-
notation agreement under different scenar-
ios, the latent features are the real driving
factors of tense annotation disagreement
among multiple annotators. The analy-
ses also find the verb telicity feature, as-
pect marker presence and syntactic em-
bedding structure to be strongly associated
with tense, suggesting their utility in the
automatic tense classification task.
1 Introduction
In recent years, the research community has seen
a fast-growing volume of work in temporal infor-
mation processing. Consequently, the investiga-
tion and practice of temporal information anno-
tation by human experts have emerged from the
corpus annotation research. To evaluate automatic
temporal relation classification systems, annotated
corpora must be created and validated, which mo-
tivates experiments and research in temporal infor-
mation annotation.
One important temporal relation distinction that
human beings make is the temporal reference dis-
tinction based on relative positioning between the
following three time parameters, as proposed by
(Reichenbach, 1947): speech time (S), event time
(E) and reference time (R). Temporal reference
distinction is linguistically realized as tenses. Lan-
guages have various granularities of tense repre-
sentations; some have finer-grained tenses or as-
pects than others. This poses a great challenge to
automatic cross-lingual tense mapping. The same
challenge holds for cross-lingual tense annotation,
especially for language pairs that have dramati-
cally different tense strategies. A decent solution
for cross-lingual tense mapping will benefit a va-
riety of NLP tasks such as Machine Translation,
Cross-lingual Question Answering (CLQA), and
Multi-lingual Information Summarization. While
automatic cross-lingual tense mapping has re-
cently started to receive research attention, such
as in (Olsen,et al, 2001) and (Ye, et al, 2005),
to the best of our knowledge, human performance
on tense and aspect annotation for machine trans-
lation between English and Chinese has not re-
ceived any systematic investigation to date. Cross-
linguistic NLP tasks, especially those requiring a
more accurate tense and aspect resolution, await
a more focused study of human tense and aspect
annotation performance.
Chinese and English are a language pair in
which tense and aspect are represented at differ-
ent levels of units: one being realized at the word
level and the other at the morpheme level.
This paper reports on a series of cross-linguistic
tense annotation experiments between Chinese
and English, and provides statistical inference for
different linguistic factors via a series of statisti-
cal modeling. Since tense and aspect are mor-
phologically merged in English, tense annotation
13
discussed in this paper also includes elements of
aspect. We only deal with tense annotation in
Chinese-to-English scenario in the scope of this
paper.
The remaining part of the paper is organized
as follows: Section 2 summarizes the significant
related works in temporal information annotation
and points out how this study relates to yet differs
from them. Section 3 reports the details of three
tense annotation experiments under three scenar-
ios. Section 4 discusses the inter-judge agree-
ment by presenting two measures of agreement:
the Kappa Statistic and accuracy-based measure-
ment. Section 5 investigates and reports on the
significance of different linguistic factors in tense
annotation via an ANOVA analysis, a logistic re-
gression analysis and a log-linear model analysis.
Finally, section 6 concludes the paper and points
out directions for future research.
2 Related Work
There are two basic types of temporal location re-
lationships. The first one is the ternary classifica-
tion of past, present and future. The second one
is the binary classification of ?BEFORE? versus
?AFTER?. These two types of temporal relation-
ships are intrinsically related but each stands as a
separate issue and is dealt with in different works.
While the ?BEFORE? versus ?AFTER? relation-
ship can easily be transferred across a language
pair, the ternary tense taxonomy is often very hard
to transfer from one language to another.
(Wilson, et al, 1997) describes a multilin-
gual approach to annotating temporal information,
which involves flagging a temporal expression in
the document and identifying the time value that
the expression designates. Their work reports an
inter-annotator reliability F-measure of 0.79 and
0.86 respectively for English corpora.
(Katz, et al, 2001) describes a simple and gen-
eral technique for the annotation of temporal rela-
tion information based on binary interval relation
types: precedence and inclusion. Their annotation
scheme could benefit a range of NLP applications
and is easy to carry out.
(Pustejovsky et al, 2004) reports an annotation
scheme, the TimeML metadata, for the markup of
events and their anchoring in documents. The an-
notation schema of TimeML is very fine-grained
with a wide coverage of different event types, de-
pendencies between events and times, as well as
?LINK? tags which encode the various relations
existing between the temporal elements of a doc-
ument. The challenge of human labeling of links
among eventualities was discussed at great length
in their paper. Automatic ?time-stamping? was
attempted on a small sample of text in an earlier
work of (Mani, 2003). The result was not partic-
ularly promising. It showed the need for a larger
quantity of training data as well as more predictive
features, especially on the discourse level. At the
word level, the semantic representation of tenses
could be approached in various ways depending
on different applications. So far, their work has
gone the furthest towards establishing a broad and
open standard metadata mark-up language for nat-
ural language texts.
(Setzer, et al, 2004) presents a method of eval-
uating temporal order relation annotations and an
approach to facilitate the creation of a gold stan-
dard by introducing the notion of temporal clo-
sure, which can be deduced from any annotations
through using a set of inference rules.
From the above works, it can be seen that the
effort in temporal information annotation has thus
far been dominated by annotating temporal rela-
tions that hold entities such as events or times
explicitly mentioned in the text. Cross-linguistic
tense and aspect annotation has so far gone un-
studied.
3 Chinese Tense Annotation
Experiments1
In current section, we present three tense annota-
tion experiments with the following scenarios:
1. Null-control situation by native Chinese
speakers where the annotators were provided
with the source Chinese sentences but not the
English translations;
2. High-control situation by native English
speakers where the annotators were provided
with the Chinese sentences as well as English
translations with specified syntax and lexi-
cons;
3. Semi-control situation by native English
speakers where the annotators were allowed
to choose the syntax and lexicons for the En-
glish sentence with appropriate tenses;
1All experiments in the paper are approved by Behav-
ioral Sciences Institutional Review Board at the University
of Michigan, the IRB file number is B04-00007481-I.
14
3.1 Experiment One
Experiment One presents the first scenario of
tense annotation for Chinese verbs in Chinese-to-
English cross-lingual situation. In the first sce-
nario, the annotation experiment was carried out
on 25 news articles from LDC Xinhua News re-
lease with category number LDC2001T11. The ar-
ticles were divided into 5 groups with 5 articles in
each group. There are a total number of 985 verbs.
For each group, three native Chinese speakers who
were bilingual in Chinese and English annotated
the tense of the verbs in the articles independently.
Prior to annotating the data, the annotators under-
went brief training during which they were asked
to read an example of a Chinese sentence for each
tense and make sure they understand the exam-
ples. During the annotation, the annotators were
asked to read the whole articles first and then se-
lect a tense tag based on the context of each verb.
The tense taxonomy provided to the annotators in-
clude the twelve tenses that are different combi-
nations of the simple tenses (present, past and fu-
ture), the prograssive aspect and the perfect aspect.
In cases where the judges were unable to decide
the tense of a verb, they were instructed to tag it
as ?unknown?. In this experiment, the annotators
were asked to tag the tense for all Chinese words
that were tagged as verbs in the Penn Treebank
corpora. Conceivably, the task under the current
scenario is meta-linguistic in nature for the reason
that tense is an elusive notion for Chinese speak-
ers. Nevertheless, the experiment provides a base-
line situation for human tense annotation agree-
ment. The following is an example of the anno-
tation where the annotators were to choose an ap-
propriate tense tag from the provided tense tags:
((IP (NP-TPC (NP-PN (NR ??))(NP (NN ??)(NN??)))(LCP-TMP (NP (NT ?
?))(LC?)) (NP-SBJ (NP (PP (P ?)(NP (NN ?)))(NP (NN ??)))(NP (NN ?
?)))(VP (ADVP (AD ???)) (VP (VV??)))(PU?)) ) 
1. simple present tense
2. simple past tense
3. simple future tense
4. present perfect tense
5. past perfect tense
6. future perfect tense
7. present progressive tense
8. past progressive tense
9. future progressive
10. present perfect progressive
11. past perfect progressive
3.2 Experiment Two
Experiment Two was carried out using 25 news
articles from the parallel Chinese and English
news articles available from LDC Multiple Trans-
lation Chinese corpora (MTC catalog number
LDC2002T01). In the previous experiment, the
annotators tagged all verbs. In the current experi-
mental set-up, we preprocessed the materials and
removed those verbs that lose their verbal status in
translation from Chinese to English due to nom-
inalization. After this preprocessing, there was
a total of 288 verbs annotated by the annotators.
Three native speakers, who were bilingually fluent
in English and Chinese, were recruited to annotate
the tense for the English verbs that were translated
from Chinese. As in the previous scenario, the an-
notators were encouraged to pay attention to the
context of the target verb when tagging its tense.
The annotators were provided with the full taxon-
omy illustrated by examples of English verbs and
they worked independently. The following is an
example of the annotation where the annotators
were to choose an appropriate tense tag from the
provided tense tags:
?????????????????????????????????????
?????
According to statistics, the cities (achieve) a combined gross domestic product of RMB19 
billion last year, an increase of more than 90% over 1991 before their opening. 
A. achieves
B. achieved 
C. will achieve 
D. are achieving 
E. were achieving 
F. will be achieving 
G. have achieved 
H. had achieved 
I. will have achieved 
J. have been achieving
K. had been achieving
L. will have been achieving
M. would achieve
3.3 Experiment Three
Experiment Three was an experiment simulated
on 52 Xinhua news articles from the Multiple
Translation Corpus (MTC) mentioned in the pre-
vious section. Since in the MTC corpora, each
Chinese article is translated into English by ten
human translation teams, conceptually, we could
view these ten translation teams as different an-
notators. They were making decisions about ap-
propriate tense for the English verbs. These an-
notators differ from those in Experiment Two de-
scribed above in that they were allowed to choose
any syntactic structure and verb lexicon. This is
because they were performing tense annotation in
a bigger task of sentence translation. Therefore,
their tense annotations were performed with much
less specification of the annotation context. We
manually aligned the Chinese verbs with the En-
glish verbs for the 10 translation teams from the
MTC corpora and thus obtained our third source
of tense annotation results. For the Chinese verbs
15
that were not translated as verbs into English, we
assigned a ?Not Available? tag. There are 1505
verbs in total including the ones that lost their ver-
bal status across the language.
4 Inter-Judge Agreement
Researchers use consistency checking to validate
human annotation experiments. There are vari-
ous ways of performing consistency checking de-
scribed in the literature, depending on the scale of
the measurements. Each has its advantages and
disadvantages. Since our tense taxonomy is nomi-
nal without any ordinal information, Kappa statis-
tics measurement is the most appropriate choice to
measure inter-judge agreement.
4.1 Kappa Statistic
Kappa scores were calculated for the three human
judges? annotation results. The Kappa score is the
de facto standard for evaluating inter-judge agree-
ment on tagging tasks. It reports the agreement
rate among multiple annotators while correcting
for the agreement brought about by pure chance.
It is defined by the following formula, where P(A)
is the observed agreement among the judges and
P(E) is the expected agreement:
k =
P (A)? P (E)
1? P (E)
(1)
Depending on how one identifies the expected
agreement brought about by pure chance, there are
two ways to calculate the Kappa score. One is the
?Seigel-Castellian? Kappa discussed in (Eugenio,
2004), which assumes that there is one hypotheti-
cal distribution of labels for all judges. In contrast,
the ?Cohen? Kappa discussed in (Cohen, 1960),
assumes that each annotator has an individual dis-
tribution of labels. This discrepancy slightly af-
fects the calculation of P(E). There is no consen-
sus regarding which Kappa is the ?right? one and
researchers use both. In our experiments, we use
the ?Seigel-Castellian? Kappa.
The Kappa statistic for the annotation results of
Experiment One are 0.277 on the full taxonomy
and 0.37 if we collapse the tenses into three big
classes: present, past and future. The observed
agreement rate,that is, P(A), is 0.42.
The Kappa score for tense resolution from the
ten human translation teams for the 52 Xinhua
news articles is 0.585 on the full taxonomy; we
expect the Kappa score to be higher if we exclude
the verbs that are nominalized. Interestingly, the
Kappa score calculated by collapsing the 13 tenses
into 3 tenses (present, past and future) is only
slightly higher: 0.595. The observed agreement
rate is 0.72.
Human tense annotation in the Chinese-to-
English restricted translation scenario achieved a
Kappa score of 0.723 on the full taxonomy with an
observed agreement of 0.798. If we collapse sim-
ple past and present perfect, the Kappa score goes
up to 0.792 with an observed agreement of 0.893.
The Kappa score is 0.81 on the reduced taxonomy.
4.2 Accuracy
The Kappa score is a relatively conservative mea-
surement of the inter-judge agreement rate. Con-
ceptually, we could also obtain an alternative mea-
surement of reliability by taking one annotator as
the gold standard at one time and averaging over
the accuracies of the different annotators across
different gold standards. While it is true that nu-
merically, this would yield a higher score than the
Kappa score and seems to be inflating the agree-
ment rate, we argue that the difference between
the Kappa score and the accuracy-based measure-
ment is not limited to one being more aggressive
than the other. The policies of these two mea-
surements are different. The Kappa score is con-
cerned purely with agreement without any consid-
eration of truthfulness or falsehood, while the pro-
cedure we described above gives equal weights to
each annotator being the gold standard. Therefore,
it considers both the agreement and the truthful-
ness of the annotation. Additionally, the accuracy-
based measurement is the same measurement that
is typically used to evaluate machine performance;
therefore it gives a genuine ceiling for machine
performance.
The accuracy under such a scheme for the three
annotators in Experiment One is 43% on the full
tense taxonomy.
The accuracy under such a scheme for tense
generation agreement from three annotators in Ex-
periment Two is 80% on the full tense taxonomy.
The accuracy under such a scheme for the ten
translation teams in Experiment Three is 70.8% on
the full tense taxonomy.
Table 1 summarizes the inter-judge agreement
for the three experiments.
Examining the annotation results, we identified
the following sources of disagreement. While the
16
Agreement Exp 1 Exp 2 Exp 3
Kappa Statistic 0.277 0.723 0.585
Kappa Statistic 0.37 0.81 0.595
(Reduced Taxonomy)
Accuracy 43% 80% 70.8%
Table 1: Inter-Annotator Agreement for the Three
Tense Annotation Experiments
first two factors can be controlled for by a clearly
pre-defined annotation guideline, the last two fac-
tors are intrinsically rooted in natural languages
and therefore hard to deal with:
1. Different compliance with Sequence of Tense
(SOT) principle among annotators;
2. ?Headline Effect?;
3. Ambiguous POS of the ?verb?: sometimes it
is not clear whether a verb is adjective or past
participle. e.g. The Fenglingdu Economic
Development Zone is the only one in China
that is/was built on the basis of a small town.
4. Ambiguous aspectual property of the verb:
the annotator?s view with respect to whether
or not the verb is an atelic verb or a telic verb.
e.g. ?statistics showed/show......?
Put abstractly, ambiguity is an intrinsic property
of natural languages. A taxonomy allows us to
investigate the research problem, yet any clearly
defined discrete taxonomy will inevitably fail on
boundary cases between different classes.
5 Significance of Linguistic Factors in
Annotation
In the NLP community, researchers carry out an-
notation experiments mainly to acquire a gold
standard data set for evaluation. Little effort has
been made beyond the scope of agreement rate
calculations. We propose that not only does fea-
ture analysis for annotation experiments fall un-
der the concern of psycholinguists, it also merits
investigation within the enterprise of natural lan-
guage processing. There are at least two ways
that the analysis of annotation results can help
the NLP task besides just providing a gold stan-
dard: identifying certain features that are respon-
sible for the inter-judge disagreement and model-
ing the situation of associations among the differ-
ent features. The former attempts to answer the
Figure 1: Interaction between Aspect Marker and
Temporal Modifier
question of where the challenge for human classi-
fication comes from, and thereby provides an ex-
ternal reference for an automatic NLP system, al-
though not necessarily in a direct way. The latter
sheds light on the structures hidden among groups
of features, the identification of which could pro-
vide insights for feature selection as well as of-
fer convergent evidence for the significance of cer-
tain features confirmed from classification practice
based on machine learning.
In this section, we discuss at some length a fea-
ture analysis for the results of each of the anno-
tation experiments discussed in the previous sec-
tions and summarize the findings.
5.1 ANOVA analysis of Agreement and
Linguistic Factors in Free Translation
Tense Annotation
This analysis tries to find the relationship be-
tween the linguistic properties of the verb and the
tense annotation agreement across the ten different
translation teams in Experiment Three. Specifi-
cally, we use an ANOVA analysis to explore how
the overall variance in the inconsistency of the
tenses of a particular verb with respect to differ-
ent translation teams can be attributed to different
linguistic properties associated with the Chinese
verb. It is a three-way ANOVA with three linguis-
tic factors under investigation: whether the sen-
tence contains a temporal modifier or not; whether
the verb is embedded in a relative clause, a senten-
tial complement, an appositive clause or none of
the above; and whether the verb is followed by as-
pect markers or not. The dependent variable is the
inconsistency of the tenses from the teams. The
17
inconsistency rate is measured by the ratio of the
number of distinct tenses over the number of tense
tokens from the ten translation teams.
Our ANOVA analysis shows that all of the three
main effects, i.e. the embedding structures of the
verb (p  0.001), the presence of aspect markers
(p  0.01), and the presence of temporal mod-
ifiers (p < 0.05) significantly affect the rate of
disagreement in tense generation among the dif-
ferent translation teams. The following graphs
show the trend: tense generation disagreement
rates are consistently lower when the Chinese as-
pect marker is present, whether there is a temporal
modifier present or not (Figure 1). The model also
suggested that the presence of temporal modifiers
is associated with a lower rate of disagreement
for three embedding structures except for verbs in
sentential complements (Figure 2, 0: the verb is
not in any embedding structures; 1: the verb is
embedded in a relative clause; 2: the verb is em-
bedded in an appositive clause; 3: the verb is em-
bedded in sentential complement). Our explana-
tion for this is that the annotators receive varying
degrees of prescriptive writing training, so when
there is a temporal modifier in the sentence as a
confounder, there will be a larger number, a higher
incidence of SOT violations than when there is
no temporal modifier present in the sentence. On
top of this, the rate of disagreement in tense tag-
ging between the case where a temporal modifier
is present in the sentence and the case where it is
not depends on different types of embedding struc-
tures (Figure 2, p value < 0.05).
We also note that the relative clause embed-
ding structure is associated with a much higher
disagreement rate than any other embedding struc-
tures (Figure 3).
5.2 Logistic Regression Analysis of
Agreement and Linguistic Factors in
Restricted Tense Annotation
The ANOVA analysis in the previous section is
concerned with the confounding power of the
overt linguistic features. The current section ex-
amines the significance of the more latent fea-
tures on tense annotation agreement when the SOT
effect is removed by providing the annotators a
clear guideline about the SOT principle. Specif-
ically, we are interested in the effect of verb telic-
ity and punctuality features on tense annotation
agreement. The telicity and punctuality features
Figure 2: Interaction between the Temporal Mod-
ifier and the Syntactic Embedding Structure
were obtained through manual annotation based
on the situation in the context. The data are from
Experiment Two. Since there are only three an-
notators, the inconsistency rate we discussed in
5.1 would have insufficient variance in the current
scenario, making logistic regression a more appro-
priate analysis. The response is now binary being
either agreement or disagreement (including par-
tial agreement and pure disagreement). To avoid a
multi-colinearity problem, we model Chinese fea-
tures and English features separately. In order
to truly investigate the effects of the latent fea-
tures, we keep the overt linguistic features in the
model as well. The overt features include: type of
syntactic embedding, presence of aspect marker,
presence of temporal expression in the sentence,
whether the verb is in a headline or not, and the
presence of certain signal adverbs including ?yi-
jing?(already), ?zhengzai? (Chinese pre-verb pro-
gressive marker), ?jiang?(Chinese pre-verbal ad-
verb indicating future tense). We used backward
elimination to obtain the final model.
The result showed that punctuality is the only
factor that significantly affects the agreement rate
among multiple judges in both the model of En-
glish features and the model of Chinese features.
The significance level is higher for the punctuality
of English verbs, suggesting that the source lan-
guage environment is more relevant in tense gener-
ation. The annotators are roughly four times more
likely to fail to agree on the tense for verbs as-
sociated with an interval event. This supports the
hypothesis that human beings use the latent fea-
tures for tense classification tasks. Surprisingly,
the telicity feature is not significant at all. We sus-
18
Figure 3: Effect of Syntactic Embedding Structure
on Tense Annotation Disagreement
pect this is partly due to the correlation between
the punctuality feature and the telicity feature. Ad-
ditionally, none of the overt linguistic features is
significant in the presence of the latent features,
which implies that the latent features drive dis-
agreement among multiple annotators.
5.3 Log-linear Model Analysis of
Associations between Linguistic Factors
in Free Translation Tense Annotation
This section discusses the association patterns be-
tween tense and the relevant linguistic factors via
a log-linear model. A log-linear model is a special
case of generalized linear models (GLMs) and has
been widely applied in many fields of social sci-
ence research for multivariate analysis of categor-
ical data. The model reveals the interaction be-
tween categorical variables. The log-linear model
is different from other GLMs in that it does not
distinguish between ?response? and ?explanatory
variables?. All variables are treated alike as ?re-
sponse variables?, whose mutual associations are
explored. Under the log-linear model, the ex-
pected cell frequencies are functions of all vari-
ables in the model. The most parsimonious model
that produces the smallest discrepancy between
the expected cell and the observed cell frequen-
cies is chosen as the final model. This provides
the best explanation of the observed relationships
among variables.
We use the data from Experiment Two for the
current analysis. The results show that three lin-
guistic features under investigation are signifi-
cantly associated with tense. First, there is a strong
association between aspect marker presence and
tense, independent of punctuality, telicity feature
and embedding structure. Second, there is a strong
association between telicity and tense, indepen-
dent of punctuality, aspect marker presence and
punctuality feature. Thirdly, there is a strong as-
sociation between embedding structure and tense,
independent of telicity, punctuality feature and as-
pect marker presence. This result is consistent
with (Olsen, 2001), in that the lexical telicity fea-
ture, when used heuristically as the single knowl-
edge source, can achieve a good prediction of verb
tense in Chinese to English Machine Translation.
For example, the odds of the verb being atelic in
the past tense is 2.5 times the odds of the verb
being atelic in the future tense, with a 95% con-
fidence interval of (0.9, 7.2). And the odds of a
verb in the future tense having an aspect marker
approaches zero when compared to the odds of a
verb in the past tense having an aspect marker.
Putting together the pieces from the logistic
analysis and the current analysis, we see that an-
notators fail to agree on tense selection mostly
with apunctual verbs, while the agreed-upon tense
is jointly decided by the telicity feature, aspect
marker feature and the syntactic embedding struc-
ture that are associated with the verb.
6 Conclusions and Future Work
As the initial attempt to assess human beings?
cross-lingual tense annotation, the current paper
carries out a series of tense annotation experi-
ments between Chinese and English under differ-
ent scenarios. We show that even if tense is an
abstract grammatical category, multiple annotators
are still able to achieve a good agreement rate
when the target English context is fully specified.
We also show that in a non-restricted scenario,
the overt linguistic features (aspect markers, em-
bedding structures and temporal modifiers), can
cause people to fail to agree with each other signif-
icantly in tense annotation. These factors exhibit
certain interaction patterns in the decision mak-
ing of the annotators. Our analysis of the anno-
tation results from the scenario with a fully speci-
fied context show that people tend to fail to agree
with each other on tense for verbs associated with
interval events. The disagreement seems not to
be driven by the overt linguistic features such as
embedding structure and aspect markers. Lastly,
among a set of overt and latent linguistic features,
aspect marker presence, embedding structure and
19
the telicity feature exhibit the strongest association
with tense, potentially indicating their high utility
in tense classification task.
The current analysis, while suggesting certain
interesting patterns in tense annotation, could be
more significant if the findings could be replicated
by experiments of different scales on different data
sets. Furthermore, the statistical analysis could be
more finely geared to capture the more subtle dis-
tinctions encoded in the features.
AcknowledgementAll of the annotation exper-
iments in this paper are funded by Rackham Grad-
uate School?s Discretionary Funds at the Univer-
sity of Michigan.
References
Hans Reichenbach,1947. Elements of Symbolic Logic,
Macmillan, New York, N.Y.
Mari Olson, David Traum, Carol Van Ess-Dykema,
and AmyWeinberg, 2001. Implicit Cues for Explicit
Generation: Using Telicity as a Cue for Tense Struc-
ture in a Chinese to English MT System, Proceed-
ings Machine Translation Summit VIII, Santiago de
Compostela, Spain.
Yang Ye, Zhu Zhang, 2005. Tense Tagging for Verbs
in Cross-Lingual Context: A Case Study. Proceed-
ings of 2nd International Joint Conference in Natural
Language Processing (IJCNLP), 885-895.
George Wilson, Inderjeet Mani, Beth Sundheim, and
Lisa Ferro, 2001. A Multilingual Approach to An-
notating and Extracting Temporal Information, Pro-
ceedings of the ACL 2001 Workshop on Temporal
And Spatial Information Processing, 39th Annual
Meeting of ACL, Toulouse, 81-87.
Graham Katz and Fabrizio Arosio, 2001. The Annota-
tion of Temporal Information in Natural Language
Sentences, Proceedings of the ACL 2001 Workshop
on Temporal And Spatial Information Processing,
39th Annual Meeting of ACL, Toulouse, 104-111.
James Pustejovsky, Robert Ingria, Roser Sauri, Jose
Castano, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, and Inderjeet Mani. 2004. The
Specification Language TimeML. The Language of
Time: A Reader. Oxford, 185-96.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101.
Inderjeet Mani, 2003. Recent Developments in Tem-
poral Information Extraction. In Nicolov, N. and
Mitkov, R., editors, Proceedings of RANLP?03.
John Benjamins.
Andrea Setzer, Robert Gaizauskas, and Mark Hep-
ple, 2003. Using Semantic Inferences for Tem-
poral Annotation Comparison, Proceedings of the
Fourth International Workshop on Inference in
Computational Semantics (ICOS-4), INRIA, Lor-
raine, Nancy, France, September 25-26, 185-96.
Jacob Cohen, 1960. A Coefficient of Agreement for
Nominal Scales, Educational and Psychological
Measurement, 20, 37-46.
20
Proceedings of the Second Workshop on Statistical Machine Translation, pages 240?247,
Prague, June 2007. c?2007 Association for Computational Linguistics
Sentence Level Machine Translation Evaluation as a Ranking Problem: one
step aside from BLEU
Yang Ye
University of Michigan
yye@umich.edu
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Chin-Yew Lin
Microsoft Research Asia
cyl@microsoft.com
Abstract
The paper proposes formulating MT evalu-
ation as a ranking problem, as is often done
in the practice of assessment by human. Un-
der the ranking scenario, the study also in-
vestigates the relative utility of several fea-
tures. The results show greater correlation
with human assessment at the sentence level,
even when using an n-gram match score as
a baseline feature. The feature contributing
the most to the rank order correlation be-
tween automatic ranking and human assess-
ment was the dependency structure relation
rather than BLEU score and reference lan-
guage model feature.
1 Introduction
In recent decades, alongside the growing research
on Machine Translation (MT), automatic MT evalu-
ation has become a critical problem for MT system
developers, who are interested in quick turnaround
development cycles. The state-of-the-art automatic
MT evaluation is an n-gram based metric repre-
sented by BLEU (Papineni et al, 2001) and its vari-
ants. Ever since its creation, the BLEU score has
been the gauge of Machine Translation system eval-
uation. Nevertheless, the research community has
been largely aware of the deficiency of the BLEU
metric. BLEU captures only a single dimension
of the vitality of natural languages: a candidate
translation gets acknowledged only if it uses ex-
actly the same lexicon as the reference translation.
Natural languages, however, are characterized by
their extremely rich mechanisms for reproduction
via a large number of syntactic, lexical and semantic
rewriting rules. Although BLEU has been shown
to correlate positively with human assessments at
the document level (Papineni et al, 2001), efforts to
improve state-of-the-art MT require that human as-
sessment be approximated at sentence level as well.
Researchers report the BLEU score at document
level in order to combat the sparseness of n-grams
in BLEU scoring. But, ultimately, document-level
MT evaluation has to be pinned down to the gran-
ularity of the sentence. Unfortunately, the corre-
lation between human assessment and BLEU score
at sentence level is extremely low (Liu et al, 2005,
2006). While acknowledging the appealing simplic-
ity of BLEU as a way to access one perspective of an
MT candidate translation?s quality, we observe the
following facts of n-gram based MT metrics. First,
they may not reflect the mechanism of how human
beings evaluate sentence translation quality. More
specifically, optimizing BLEU does not guarantee
the optimization of sentence quality approved by hu-
man assessors. Therefore, BLEU is likely to have
a low correlation with human assessment at sen-
tence level for most candidate translations. Second,
it is conceivable that human beings are more reli-
able ranking the quality of multiple candidate trans-
lations than assigning a numeric value to index the
quality of the candidate translation even with signif-
icant deliberation. Consequently, a more intuitive
approach for automatic MT evaluation is to repli-
cate the quality ranking ability of human assessors.
Thirdly, the BLEU score is elusive and hard to in-
terpret; for example, what can be concluded for a
240
candidate translation?s quality if the BLEU score is
0.0168, particularly when we are aware that even
a human translation can receive an embarrassingly
low BLEU score? In light of the discussion above,
we propose an alternative scenario for MT evalua-
tion, where, instead of assigning a numeric score to
a candidate translation under evaluation, we predict
its rank with regard to its peer candidate translations.
This formulation of the MT evaluation task fills the
gap between an automatic scoring function and hu-
man MT evaluation practice. The results from the
current study will not only interest MT system eval-
uation moderators but will also inform the research
community about which features are useful in im-
proving the correlation between human rankings and
automatic rankings.
2 Problem Formulation
2.1 Data and Human Annotation Reliability
We use two data sets for the experiments:
the test data set from the LDC MTC corpus
(LDC2003T171) and the data set from the MT eval-
uation workshop at ACL052. Both data sets are for
Chinese-English language pairs and each has four
reference translations and seven MT system transla-
tions as well as human assessments for fluency and
adequacy on a scale of 1 to 5, with 5 indicating the
best quality. For the LDC2003T17 data, human as-
sessments exist for only three MT systems; for the
ACL05 workshop data, there are human assessments
for all seven MT systems. Table 1 summarizes the
information from these two data sets.
The Kappa scores (Cohen, 1960) for the human
assessment scores are negative, both for fluency and
adequacy, indicating that human beings are not con-
sistent when assigning quality scores to the candi-
date translations. We have much sympathy with a
concern expressed in (Turian, 2003) that ?Automatic
MT evaluation cannot be faulted for poor correlation
with the human judges, when the judges do not cor-
relate well each other.?To determine whether human
assessor might be more consistent when ranking
pairs of sentences, we examined the ?ranking con-
sistency score?of the human assessment data for the
LDC2003T17 data. For this consistency score, we
1http://www.ldc.upenn.edu/Catalog/
2http://www.isi.edu/? cyl/MTSE2005/
are only concerned with whether multiple judges are
consistent in terms of which sentence of the two sen-
tences is better: we are not concerned with the quan-
titative difference between judges. Since some sen-
tences are judged by three judges while others are
judged by only two judges, we calculated the consis-
tency scores under both circumstances, referred to as
?Consistent 2?and ?Consistent 3?in the following ta-
ble. For ?Consistent 2?, for every pair of sentences,
where sentence 1 is scored higher (or lower or equal)
than sentence 2 by both judges, then the two judges
are deemed consistent. For ?Consistent 3?, the pro-
portion of sentences that achieved the above consis-
tency from triple judges is reported. Additionally,
we also considered a consistency rate that excludes
pairs for which only one judge says sentence 1 is bet-
ter and the other judge(s) say(s) sentence 2 is better.
We call these ?Consistent 2 with tie?and ?Consistent
3 with tie?. From the rank consistency scores in Ta-
ble 2, we observe that two annotators are more con-
sistent with the relative rankings for sentence pairs
than with the absolute quality scores. This finding
further supports the task of ranking MT candidate
sentences as more reliable than the one of classify-
ing the quality labels.
2.2 Ranking Over Classification and
Regression
As discussed in the previous section, it is difficult for
human assessors to perform MT candidate transla-
tion evaluation with fine granularity (e.g., using real-
valued numeric score). But humans? assessments
are relatively reliable for judgments of quality rank-
ing using a coarser ordinal scale, as we have seen
above. Several approaches for automatically assign-
ing quality scores to candidate sentences are avail-
able, including classification, regression or ranking,
of which ranking is deemed to be a more appropri-
ate approach. Nominalize the quality scores and for-
mulating the task as a classification problem would
result in a loss of the ordinal information encoded
in the different scores. Additionally, the low Kappa
scores in the human annotation reliability analysis
reported above also confirms our previous specula-
tion that a classification approach is less appropriate.
Regression would be more reasonable than classifi-
cation because it preserves the ordinal information
in the quality labels, but it also inappropriately im-
241
Data Index MT Systems References Documents Sentences
LDC2003T17 7 4 100 878
ACL05 Workshop 7 4 100 919
Table 1: Data Sets Information
Inter-Judge Score Consistent
2
Consistent
3
Consistent
2 with Tie
Consistent
3 with Tie
Ranking Consistency Score 45.3% 23.4% 92.6% 87.0%
Table 2: Ranking Consisteny Scores for LDC2003T17 Data
poses interval scaling onto the quality labels. In
contrast, ranking considers only the relative rank-
ing information from human labels and does not im-
pose any extra information onto the quality labels
assigned by human beings.
The specific research question addressed in this
paper is three-fold: First, in addition to investigating
the correlation between automatic numeric scoring
and human assessments, is ranking of peer candidate
translations an alternative way of examining correla-
tion that better suits the state of affairs of human an-
notation? Second, if the answer to the above ques-
tion is yes, can better correlation be achieved with
human assessment under the new task scenario? Fi-
nally, in addition to n-gram matching, which other
knowledge sources can combat and even improve
the rank order correlation? The process of rank-
ing is a crucial technique in Information Retrieval
(IR) where search engines rank web pages depend-
ing on their relevance to a query. In this work, sen-
tence level MT evaluation is considered as a ranking
problem. For all candidate translations of the same
source Chinese sentence, we predict their transla-
tion quality ranks. We evaluate the ranker by Spear-
man?s rank order correlation coefficient between hu-
man ranks and predicted ranks described by the fol-
lowing formula (Siegel,1956):
r = 1? ( 6
?
D2
N(N2 ? 1)
) (1)
where D is the difference between each pair of ranks
and N is the number of candidates for ranking.
3 Related Works
Papineni et al(2001) pioneered the automatic MT
evaluation study, which scores translation quality via
n-gram matching between the candidate and refer-
ence translations. Following the growing awareness
of the deficiency of n-gram based automatic MT
evaluation, many studies attempted to improve upon
n-gram based metrics (Zhou et al, 2006; Liu, et
al., 2005,2006) as well as propose ways to evaluate
MT evaluation metrics (Lin, et al 2004). Previous
studies, however, have focused on MT evaluation at
the document level in order to fight n-gram sparse-
ness problem. While document level correlation
provides us with a general impression of the qual-
ity of an MT system, researchers desire to get more
informative diagnostic evaluation at sentence level
to improve the MT system instead of just an over-
all score that does not provide details. Recent years
have seen several studies investigating MT evalu-
ation at the sentence level (Liu et al, 2005,2006;
Quirk, 2004). The state-of-the-art sentence level
correlations reported in previous work between hu-
man assessments and automatic scoring are around
0.20. Kulesza et al(2004) applied Support Vec-
tor Machine classification learning to sentence level
MT evaluation and reported improved correlation
with human judgment over BLEU. However, the
classification taxonomy in their work is binary, be-
ing either machine translation or human translation.
Additionally, as discussed above, the inconsistency
from the human annotators weakens the legitimacy
of the classification approach. Gamon et al(2005)
reported a study of English to French sentence-level
MT evaluation without reference translations. In or-
der to improve on the correlation between human as-
sessments and the perplexity score alone, they com-
bined a perplexity score with a classification score
obtained from an SVM binary classifier distinguish-
ing machine-translated sentences from human trans-
242
lations. The results showed that even the combi-
nation of the above two scores cannot outperform
BLEU.
To sum up, very little consideration has been
taken in previous research as to which learning ap-
proach is better motivated and justified by the state
of affairs of human annotation reliability. Presum-
ably, research that endeavors to emulate human per-
formance on tasks that demontrate good inter-judge
reliability is most useful.
a learning approach that is better supported by
human annotation reliability can alleviate the noise
from human assessments and therefore achieve more
reliable correlations.
4 Experiments and Evaluation
4.1 Ranking SVM Learning Algorithm
Ranking peer candidate sentence translations is a
task in which the translation instances are classi-
fied into a number of ranks. This is a canonical or-
dinal regression scenario, which differs from stan-
dard classification and metric regression. For imple-
mentation, we use the Ranking SVM of SVMlight
(Joachims, 2004), which was originally developed
to rank the web pages returned upon a certain query
in search engines. Given an instance of a candidate
translation, Ranking SVM assigns it a score based
on:
U(x) = W Tx (2)
where W represents a vector of weights (Xu et al,
2005). The higher the value of U(x), the better x is as
a candidate translation. In an ordinal regression, the
values of U(x) are mapped into intervals correspond-
ing to the ordinal categories. An instance falling
into one interval is classified into the corresponding
translation quality. In ranking experiments, we use
the Ranking SVM scores to rank the candidate sen-
tences under evaluation.
4.2 Features
We experiment with three different knowledge
sources in our ranking experiments:
1. N-gram matching between the candidate trans-
lation and the reference translation, for which
we use BLEU scores calculated by the NIST
script with smoothing3 to avoid undefined log
probabilities for zero n-gram probabilities.
2. Dependency relation matching between the
candidate translation and the reference transla-
tion.
3. The log of the perplexity score of the candidate
translation, where the perplexity score is ob-
tained from a local language model trained on
all sentences in the four reference translations
using CMU SLM toolkit. The n-gram order is
the default trigram.
4.2.1 N-gram matching feature
N-gram matching is certainly an important cri-
terion in some cases for evaluating the translation
quality of a candidate translation. We use the BLEU
score calculated by the BLEU score script from
NIST for this feature.
As has been observed by many researchers,
BLEU fails to capture any non n-gram based match-
ing between the reference and candidate transla-
tions. We carried out a pair-wise experiment on
four reference translations from the LDC2003T17
test data, where we took one reference sentence as
the reference and the other three references as can-
didate translations. Presumably, since the candidate
sentences are near-optimal translations, the BLEU
scores obtained in such a way should be close to
1. But our analysis shows a mean BLEU of only
0.1456398, with a standard deviation of 0.1522381,
which means that BLEU is not very predictive of
sentence level evaluation. The BLEU score is, how-
ever, still informative in judging the average MT
system?s translation.
4.2.2 Dependency Structure Matching
Dependency relation information has been widely
used in Machine Translation in recent years. Fox
(2002) reported that dependency trees correspond
better across translation pairs than constituent trees.
The information summarization community has also
seen successful implementation of ideas similar to
the depedency structure. Zhou et al(2005) and Hovy
et al(2005) reported using Basic Elements (BE) in
text summarization and its evaluation. In the current
3We added an extremely small number to both matched n-
grams and total number of n-grams.
243
paper, we match a candidate translation with a ref-
erence translation on the following five dependency
structure (DS) types:
? Agent - Verb
? Verb - Patient
? Modified Noun - Modifier
? Modified Verb - Modifier
? Preposition - Object
Besides the consideration of the presence of cer-
tain lexical items, DS captures information as to
how the lexical items are assembled into a good sen-
tence. By using their dependency relation match for
ranking the quality of peer translations, we assume
that the dependency structure in the source language
should be well preserved in the target language and
that multiple translations of the same source sen-
tence should significantly share dependency struc-
tures. Liu et al(2005) make use of dependency
structure in sentence level machine translation eval-
uation in the form of headword chains, which are
lexicalized dependency relations. We propose that
unlexicalized dependency relations can also be in-
formative. Previous research has shown that key de-
pendency relations tend to have a strong correspon-
dence between Chinese and English (Zhou et al,
2001). More than 80 % of subject-verb, adjective-
noun and adverb-verb dependency relations were
able to be mapped, although verb-object DS map-
ping is weaker at a rate of 64.8%. In our paper, we
considered three levels of matching for dependency
relation triplets, where a triplet consists of the DS
type and the two lexical items as the arguments.
We used an in-house dependency parser to extract
the dependency relations from the sentences. Figure
1 illustrates how dependency relation matching can
go beyond n-gram matching. We calculated 15 DS
scores for each sentence correponding to the counts
of match for the 5 DS types at the 3 different levels.
4.2.3 Reference language model (RLM) feature
Statistical Language Modeling (SLM) is a key
component in Statistical Machine Translation. The
most dominant technology in SLM is n-gram mod-
els, which are typically trained on a large corpus
for applications such as SMT and speech recogni-
tion. Depending on the size of the corpora used
to train the language model, a language model can
Figure 1: Dependency Relation Matching Scheme
Figure 2: An Example - A Sentence Gets Credits for
Dependency Relation Matching
244
be tuned to reflect n-gram probabilities for both a
narrowed scope as well as a general scope covering
the distribution of n-gram probabilities of the whole
language. In the BLEU calculation, the candidate
sentence is evaluated against an extremely local lan-
guage model of merely the reference sentence. We
speculate that a language model that stands in be-
tween such an immediate local language model and
the large general English language model could help
capture the variation of lexical and even structural
selections in the translations by using information
beyond the scope of the local sentence. Addition-
ally, this language model could represent the style
of a certain group of translators in a certain domain
on the genre of news articles. To pursue such a lan-
guage model, we explore a language model that is
trained on all sentences in the four references. We
obtain the perplexity score of each candidate sen-
tence based on the reference language model. The
perplexity score obtained this way reflects the de-
gree to which a candidate translation can be gen-
erated from the n-gram probability distribution of
the whole collection of sentences in the four refer-
ences. It adds new information to BLEU because it
not only compares the candidate sentence to its cor-
responding reference sentence but also reaches out
to other sentences in the current document and other
documents on the same topics. We choose perplex-
ity over the language model score because the per-
plexity score is normalized with regard to the length
of the sentence; that is, it does not favor sentences of
relatively shorter length.
In our ranking experiments, for training, both the
seven MT translations and the four reference trans-
lations of the same source sentence are evaluated
as ?candidate? translations, and then each of these
eleven sentences is evaluated against the four ref-
erence sentences in turn. The BLEU score of each
of these sentences is calculated with multiple refer-
ences. Each dependency score is the average score
of the four references. For the reference language
model feature, the perplexity score is used for each
sentence.
Conceptually, the reference language model and
dependency structure features are more relevant to
the fluency of the sentence than to the adequacy.
Because the candidate sentences? adequacy scores
are based on arbitrary reference sentences out of the
Feature Set Mean Corr Corr Var
BLEU 0.3590644 0.0076498
DS 0.4002753 0.0061299
PERP 0.4273000 0.0014043
BLEU+DS 0.4128991 0.0027576
BLEU+PERP 0.4288112 0.0013783
PERP+DS 0.4313611 0.0014594
All 0.4310457 0.0014494
Table 3: Training and Testing on Within-year Data
(Test on 7 MT and 4 Human)
four references in the human assessment data, we
decided to focus on fluency ranking for this paper.
The ranking scenario and features can easily be gen-
eralized to adequacy evaluation: the full and partial
match dependency structure features are relevant to
adeqaucy too. The high correlation between ade-
quacy and fluency scores from human assessments
(both pearson and spearman correlations are 0.67)
also indicates that the same features will achieve im-
provements for adequacy evaluation.
4.3 Sentence Ranking on Within-year Data
In the first experiment, we performed the ranking
experiment on the ACL05 workshop data and test on
the same data set. We did three-fold cross-validation
on two different test scenarios. On the first sce-
nario, we tested the ranking models on the seven MT
system output sentences and the four human refer-
ence sentences. It is widely agreed upon among re-
searchers that a good evalutation metric should rank
reference translation as higher than machine trans-
lation (Lin et al, 2004). We include the four hu-
man reference sentences into the ranking to test the
ranker?s ability to discriminate optimal translations
from poor ones. For the second scenario, we test
the ranking models on only the seven MT system
output sentences. Because the quality differences
across the seven system translations are more subtle,
we are particularly interested in the ranking quality
on those sentences. Tables 3 and 4 summarize the
results from both scenarios.
The experimental results in the above tables con-
veyed several important messages: in the ranking
setup, for both the MT and human mixed output and
MT only output scenarios, we have a significantly
245
Feature Set Mean Corr Corr Var
BLEU 0.2913541 0.0324386
DS 0.3058766 0.0226442
PERP 0.2921684 0.0210605
BLEU+DS 0.315106 0.0206144
BLEU+PERP 0.2954833 0.0211094
PERP+DS 0.3067157 0.0217037
All 0.305248 0.0218777
Table 4: Training and Testing on Within-year Data
(Test on MT only)
improved correlation between human scoring and
automatic ranking at sentence level compared to the
state-of-the-art sentence level correlation for fluency
score of approximately 0.202 found previously (Liu
et al, 2006). When the ranking task is performed on
a mixture of MT sentences and human translations,
dependency structure and reference language model
perplexity scores sequentially improve on BLEU in
increasing the correlation. When the ranking task
is performed only on MT system output sentences,
dependency structure still significantly outperforms
BLEU in increasing the correlation, and the refer-
ence language model, even trained on a small num-
ber of sentences, demonstrates utility equal to that
of BLEU. The dependency structure feature proves
to have robust utility in informing fluency quality
in both scenarios, even with noise from the depen-
dency parser, likely because a dependency triplet
with inaccurate arguments is still rewarded as a type
match or partial match. Additionally, the feature is
reward-based and not penalty-based. We only re-
ward matches and do not penalize mismatches, such
that the impact of the noise from the MT system and
the dependency parser is weakened.
4.4 Sentence Ranking on Across-year Data
It is trivial to retrain the ranking model and test on
a new year?s data. But we speculate that a model
trained from a different data set can have almost the
same ranking power as a model trained on the same
data set. Therefore, we conducted an experiment
where we trained the ranking model on the ACL
2005 workshop data and test on the LDC2003T17
data. We do not need to retrain the ranking SVM
model; we only need to retrain the reference lan-
Feature Set Mean Corr Corr Var
BLEU 0.3133257 0.1957059
DS 0.4896355 0.0727430
PERP 0.4582005 0.0542485
BLEU+DS 0.4907745 0.0678395
BLEU+PERP 0.4577449 0.0563994
PERP+DS 0.4709567 0.0549708
All 0.4707289 0.0565538
Table 5: Training and Testing on Across-year Data
(test on 3 MT plus 1 human)
guage model on the multiple references from the
new year?s data to obtain the perplexity scores.
Because LDC2003T17 has human assessments for
only three MT systems, we test on the three system
outputs plus a human translation chosen randomly
from the four reference translations. The results in
Table 5 show an encouraging rank order correlation
with human assessments. Similar to training and
testing on within-year data, both dependency struc-
ture and perplexity scores achieve higher correlation
than the BLEU score. Combining BLEU and depen-
dency structure achieves the best correlation.
4.5 Document Level Ranking Testing
Previously, most researchers working on MT evalu-
ation studied the correlation between automatic met-
ric and human assessment on the granularity of the
document to mitigate n-gram sparseness. Presum-
ably, good correlation at sentence level should lead
to good correlation at document level but not vice
versa. Table 6 reports the correlations using the
model trained on the 2005 workshop data and tested
on the 100 documents of the LDC 2003 data. Com-
paring these correlations with the correlations re-
ported in the previous section, we see that using the
same model, the document level rank order corre-
lation is substantially higher than the sentence level
correlation, with the dependency structure showing
the highest utility.
5 Conclusion and Future Work
The current study proposes to formulate MT evalu-
ation as a ranking problem. We believe that a reli-
able ranker can inform the improvement of BLEU
for a better automatic scoring function. Ranking in-
246
Feature Set Mean Corr Corr Var
BLEU 0.543 0.0853
DS 0.685 0.0723
PERP 0.575 0.0778
BLEU+DS 0.639 0.0773
BLEU+PERP 0.567 0.0785
PERP+DS 0.597 0.0861
All 0.599 0.0849
Table 6: Document Level Ranking Testing Results
formation could also be integrated into tuning pro-
cess to better inform the optimization of weights of
the different factors for SMT models. Our ranking
experiments show a better correlation with human
assessments at sentence level for fluency score com-
pared to the previous non-ranking scenario, even
with BLEU as the baseline feature. On top of BLEU,
both the dependency structure and reference lan-
guage model have shown encouraging utility for dif-
ferent testing scenarios. Looking toward the fu-
ture work, more features could be explored, e.g., a
parsing-based score of each candidate sentence and
better engineering for dependency triplet extraction.
Additionally, the entire research community on MT
evaluation would benefit from a systematic and de-
tailed analysis of real data that can provide a quanti-
tative breakdown of the proportions of different ?op-
erations? needed to rewrite one sentence to another.
Such an effort will guide MT evaluation researchers
to decide which features to focus on.
References
J. Cohen, A Coefficient of Agreement for Nominal
Scales, Educational and Psychological Measurement,
20, 37-46, 1960.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-occurrence Statistics.
HLT, pages 128?132, 2002.
H. J. Fox, Phrasal Cohesion and Statistical Machine
Translation. EMNLP, 2002.
M. Gamon, et al, Sentence-level MT Evaluation without
Reference Translations: Beyond Language Modeling,
Proceedings of EAMT, 2005.
T. Joachims, Making Large-scale Support Vector Ma-
chine Learning Practical, in B. Scholkopf, C. Burges,
A. Smola. Advances in Kernel Methods: Support Vec-
tor Machines, MIT Press, Cambridge, MA, December,
1998.
A. Kulesza and S. M. Shieber, A Learning Approach to
Improving Sentence-Level MT Evaluation, 10th Inter-
national Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, 2004.
C. Lin, et al, ORANGE: a Method for Evaluating Au-
tomatic Evaluation Metrics for Machine Translation.
COLING, 2004.
C. Lin, et al, Automatic Evaluation of Machine Trans-
lation Quality Using Longest Common Subsequence
and Skip-Bigram Statistics, ACL, 2004.
D. Liu, et al, Syntactic Features for Evaluation of Ma-
chine Translation, ACLWorkshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, 2005.
D. Liu, et al, Stochastic Iterative Alignment for Ma-
chine Translation Evaluation, COLING/ACL Poster
Session, Sydney, 2006.
C. B. Quirk, Training a Sentence-Level Machine Trans-
lation Confidence Measure, In Proceedings of LREC,
2004.
E. Hovy, et al, Evaluating DUC 2005 using Basic El-
ements. Document Understanding Conference (DUC-
2005), 2005.
K. Papineni, et al, BLEU: a Method for Automatic Eval-
uation of Machine Translation, IBM research division
technical report, RC22176 (W0109-022), 2001.
S. Siegel and N.J. Catellan, Non-parametric Statistics for
the Behavioral Sciences, McGraw-Hill, 2nd edition,
1988.
M. Snover, et al, A Study of Translation Error Rate with
Targeted Human Annotation, LAMP-TR-126, CS-TR-
4755, UMIACS-TR-2005-58, University of Maryland,
2005.
J. Turian, et al, Evaluation of Machine Translation and
its Evaluation, MT Summit IX, 2003.
J. Xu, et al, Ranking Definitions with Supervised Learn-
ing Method, WWW?05 industry track, 811-819, 2005.
L. Zhou, et al, A BE-based Multi-document Summarizer
with Query Interpretation. Document Understanding
Conference (DUC-2005), 2005.
L. Zhou, C. Lin, E-evaluating Machine Translation Re-
sults with Paraphrase Support, EMNLP, 2006.
M. Zhou, C. Huang, Approach to the Chinese depen-
dency formalism for the tagging of corpus. Journal of
Chinese Information Processing, 8(3): 35-52, 1994.
247
