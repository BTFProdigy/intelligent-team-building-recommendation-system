A Framework for Fast Incremental
Interpretation during Speech Decoding
William Schuler?
University of Minnesota
Stephen Wu?
University of Minnesota
Lane Schwartz?
University of Minnesota
This article describes a framework for incorporating referential semantic information from a
world model or ontology directly into a probabilistic language model of the sort commonly
used in speech recognition, where it can be probabilistically weighted together with phonological
and syntactic factors as an integral part of the decoding process. Introducing world model
referents into the decoding search greatly increases the search space, but by using a single
integrated phonological, syntactic, and referential semantic language model, the decoder is able to
incrementally prune this search based on probabilities associated with these combined contexts.
The result is a single unified referential semantic probability model which brings several kinds
of context to bear in speech decoding, and performs accurate recognition in real time on large
domains in the absence of example in-domain training sentences.
1. Introduction
The capacity to rapidly connect language to referential meaning is an essential aspect
of communication between humans. Eye-tracking studies show that humans listening
to spoken directives are able to actively attend to the entities that the words in these
directives might refer to, even while the words are still being pronounced (Tanenhaus
et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002). This timely access to
referential information about input utterances may allow listeners to adjust their pref-
erences among likely interpretations of noisy or ambiguous utterances to favor those
that make sense in the current environment or discourse context, before any lower-level
disambiguation decisions have been made. This same capability in a spoken language
interface system could allow reliable human?machine interaction in the idiosyncratic
language of day-to-day life, populated with proper names of co-workers, objects, and
events not found in broad training corpora. When domain-specific training corpora are
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; swu@cs.umn.edu; lane@cs.umn.edu.
Submission received: 25 April 2007; revised submission received: 4 March 2008; accepted for publication:
2 June 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
not available, a referential semantic interface could still exploit its model of the world:
the data to which it is an interface, and patterns characterizing these data.
This article describes a framework for incorporating referential semantic informa-
tion from a world model or ontology directly into a statistical language model of the
sort commonly used in speech recognition, where it can be probabilistically weighted
together with phonological and syntactic factors as an integral part of the decoding
process. Introducing world model referents into the decoding search greatly increases
the search space, but by using a single integrated phonological, syntactic, and referential
semantic language model, the decoder is able to incrementally prune this search based
on probabilities associated with these combined contexts.
Semantic interpretation is defined dynamically in this framework, in terms of transi-
tions over time from less constrained referents to more constrained referents. Because it
is defined dynamically, interpretation in this framework can incorporate dependencies
on referential context?for example, constraining interpretations to a presumed set of
entities, or a presumed setting?which may be fixed prior to recognition, or dynam-
ically hypothesized earlier in the recognition process. This contrasts with other recent
systemswhich interpret constituents only given fixed inter-utterance contexts or explicit
syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist
et al 2007). Moreover, because it is defined dynamically, in terms of transitions, this
context-dependent interpretation framework can be directly integrated into a Viterbi
decoding search, like ordinary state transitions in a Hidden Markov Model. The result
is a single unified referential semantic probability model which brings several kinds
of referential semantic context to bear in speech decoding, and performs accurate
recognition in real time on large domains in the absence of example domain-specific
training sentences.
The remainder of this article is organized as follows: Section 2 will describe related
approaches to interleaving semantic interpretation with speech recognition. Section 3
will provide definitions for worldmodels used in semantic interpretation, and language
models used in speech decoding, which will form the basis of a referential semantic
language model, defined in Section 4. Then Section 5 will describe an evaluation of this
model in a sample spoken language interface application.
2. Related Work
Early approaches to incremental interpretation (Mellish 1985; Haddock 1989) apply
semantic constraints associated with each word in a sentence to progressively winnow
the set of individuals that could serve as referents in that sentence. These incrementally
constrained referents are then used to guide the syntactic analysis of the sentence, dis-
preferring analyses with empty interpretations in the current environment or discourse
context. Similar approaches were applied to broad-coverage text processing, querying a
large commonsense knowledge base as a world model (Martin and Riesbeck 1986). But
this winnowing is done deterministically, invoking default assumptions and potentially
exponential backtracking when default assumptions fail.
The idea of basing analysis decisions on constrained sets of referent individuals
was later extended to pursue multiple interpretations at once by exploiting polynomial
structure-sharing in a dynamic programming parser (Schuler 2001; DeVault and Stone
2003; Gorniak and Roy 2004; Aist et al 2007). The resulting shared interpretation is
similar to underspecified semantic representations (Bos 1996), except that the rep-
resentation mainly preserves syntactic ambiguity rather than semantic (e.g., quanti-
314
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
fier scoping) ambiguity, and the size complexity of the parser chart representation is
polynomially bounded. This approach was further extended to support hypothetical
referents (DeVault and Stone 2003), domains with continuous relations (Gorniak and
Roy 2004), and updates to the shared parser chart by components handling other levels
of linguistic analysis in parallel, during real-time recognition (Aist et al 2007).
The advantage of this use of the parser chart is that it allows a straightforward
mapping between syntax and semantics using familiar compositional semantic rep-
resentations. But the standard dynamic programming algorithm for parsing derives
its complexity bounds from the fact that each recognized constituent can be analyzed
independently of every other constituent. These independence assumptions must be
relaxed if dynamic context dependencies are to be applied across sibling constituents
(e.g., in the package data directory, open . . . , where the files to be opened should be
restricted to the contents of the package data directory). More importantly, from an
engineering perspective, the dynamic programming algorithm for parsing runs in cubic
time, not linear, which means this interpretation framework cannot be directly applied
to continuous audio streams. Interface systems therefore typically perform utterance
or sentence segmentation as a stand-alone pre-process, without integrating syntactic or
referential semantic dependencies into this decision.
Finally, some speech recognition systems employ inter-utterance context-dependent
language models that are pre-compiled into word n-grams for particular discourse or
environment states, and swapped out between utterances (Young et al 1989; Lemon
and Gruenstein 2004; Seneff et al 2004). But in some cases accurate interpretation will
require spoken language interfaces to exploit context continuously during utterance
recognition, not just between utterances. For example, the probability distribution over
the next word in the utterance go to the package data directory and get the . . . (or in the
package data directory get the . . . ) will depend crucially on the linguistic and environment
context leading up to this point: the meaning of package data directory in the first part of
this directive, as well as the objects that will be available once this part of the directive
has been carried out. Moreover, in rich environments pre-compilation to word n-grams
can be expensive, since all referents in the world model must be considered to build
accurate n-grams. This will not be practical if environments change frequently.
3. Background
In contrast to the approaches described in Section 2, this article proposes an incremental
interpretation framework which is entirely contained within a single-pass probabilistic
decoding search. Essentially, this approach directly integrates model theoretic seman-
tics, summarized in Section 3.1, with conventional probabilistic time-series models used
in speech recognition, summarized in Section 3.2.
3.1 Referential Semantics
Semantic interpretation requires a framework within which a speaker?s intended mean-
ings can be formalized. Sections 3.1.1 and 3.1.2 describe a model theoretic approach
to semantic interpretation that will later be extended in Section 4.1. The referential
states defined here will then be incorporated into a representation of nested syntactic
constituents in a hierarchic time-series model in Section 4.2. Some of the notation
introduced here is summarized later in Table 1 (Section 4).
315
Computational Linguistics Volume 35, Number 3
Figure 1
A subsumption lattice (laid on its side) over the power set of a domain containing three
individuals: ?1, ?2, and ?3. Subsumption relations are represented as gray arrows from supersets
(or super-concepts) to subsets (or sub-concepts).
3.1.1 Model Theory. The language model described in this article defines semantic ref-
erents in terms of a world model M. In model theory (Tarski 1933; Church 1940), a
world model is defined as a tuple M = ?E , ?? containing a domain of individuals E =
{?1, ?2, . . . } and an interpretation function ? to interpret expressions in terms of those
individuals. This interpretation function accepts expressions ? of various types: logical
statements, of simple type T (for example, the demo file is writable) which may be true
or false; references to individuals, of simple type E (for example, the demo file) which
may refer to any individual in the world model; or functors of complex type ??,??,
which take an argument of type ? and produce output of type ?. Functor expressions ?
of type ??,?? can be applied to other expressions ? of type ? as arguments to yield
expressions ?(?) of type ? (for example, writablemay take the demo file as an argument
and return true). By nesting functors, complex expressions can be defined, denoting
sets or properties of individuals: ?E, T? (for example, writable), relations over individual
pairs: ?E, ?E, T?? (for example, contains), or first-order functors over sets: ??E, T?, ?E, T??
(for example, a comparative adjective like larger).
3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take
sets as arguments) can be mapped to equivalent zero-order models (with functors
defined only on entities). This is generally motivated by a desire to allow sets of
individuals to be described in much the same way as individuals themselves (Hobbs
1985). Entities in a zero-order model M can be defined from individuals in a higher-
order model M? by mapping or reifying each set S = {?1, ?2, . . . } in P (EM? ) (or each
set of sets in P (P (EM? )), etc.) as an entity eS in a new domain EM.1 Relations l inter-
preted as zero-order functors inM can be defined directly from relations l? interpreted
as higher-order functors (over sets) in M? by mapping each instance of ?S1,S2? in
l?M? : P (EM? )?P (EM? ) to a corresponding instance of ?eS1 , eS2? in lM : EM?EM. Set
subsumption inM? can then be defined on entities made from reified sets inM, similar
to ?ISA? relations over concepts in knowledge representation systems (Brachman and
Schmolze 1985).
These subset or subsumption relations can be represented in a subsumption lattice,
as shown in Figure 1, with supersets to the left connecting to subsets to the right. This
representation will be used in Section 4 to define weighted transitions over first-order
referents in a statistical time-series model of interpretation.
1 Here, P (X) is the power set of X, containing the set of all subsets.
316
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
3.2 Language Modeling for Speech Recognition
The referential semantic language model described in this article is based on Hierar-
chic Hidden Markov Models (HHMMs), an existing extension of the standard Hidden
Markov Model (HMM) language modeling framework used in speech recognition,
which has been factored to represent hierarchic information about language structure
over time. This section will review HMMs (Section 3.2.1) and Hierarchic HMMs (Sec-
tions 3.2.2 and 3.2.3). This underlying framework will then be extended to include
random variables over semantic referents in Section 4.2.
3.2.1 HMMs and Language Models. The model described in this article is a specialization
of the HMM framework commonly used in speech recognition (Baker 1975; Jelinek,
Bahl, and Mercer 1975). HMMs characterize speech as a sequence of hidden states ht
(which may consist of speech sounds, words, or other hypothesized syntactic or se-
mantic information), and observed states ot (typically finite, overlapping frames of an
audio signal) at corresponding time steps t. A most-probable sequence of hidden states
h?1..T can then be hypothesized given any sequence of observed states o1..T, using Bayes?
Law (Equation 2) and Markov independence assumptions (Equation 3) to define the
full probability P(h1..T | o1..T ) as the product of a Language Model (LM) prior proba-
bility P(h1..T )
def
=
?
t P?LM (ht | ht?1) and an Acoustic Model (AM) likelihood probability
P(o1..T | h1..T )
def
=
?
t P?AM (ot | ht):
h?1..T = argmax
h1..T
P(h1..T | o1..T ) (1)
= argmax
h1..T
P(h1..T ) ? P(o1..T | h1..T ) (2)
def
= argmax
h1..T
T
?
t=1
P?LM (ht | ht?1) ? P?AM (ot | ht) (3)
The initial hidden state h0 may be defined as a constant.
2 HMM transitions can be
modeled using Weighted Finite State Automata (WFSAs), corresponding to regular
expressions. An HMM state ht may then be defined as a WFSA state, or a symbol
position in a corresponding regular expression.
3.2.2 Hierarchic HMMs. Language model transitions P?LM (?t |?t?1) over internally
structured hidden states ?t can be modeled using synchronized levels of stacked-
up component HMMs in an HHMM (Murphy and Paskin 2001), generalized here
as an abstract topology over unspecified random variables ? and ?. In this topol-
ogy, HHMM transition probabilities are calculated in two phases: a ?reduce? phase
(resulting in an intermediate, marginalized state ?t at time step t), in which compo-
nent HMMs may terminate; and a ?shift? phase (resulting in a modeled state ?t),
in which unterminated HMMs transition, and terminated HMMs are re-initialized
from their parent HMMs. Variables over intermediate and modeled states are factored
2 It is also common to define a prior distribution over initial states at h0, but this is not necessary here.
317
Computational Linguistics Volume 35, Number 3
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
?t = ??1t . . . ?
D
t ? (4)
?t = ??1t . . . ?
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?? and ?shift? ?? models:
P?LM (?t |?t?1) =
?
?t
P(?t |?t?1) ? P(?t |?t ?t?1) (6)
def
=
?
?1t ...?
D
t
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
)
?
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
)
(7)
with ?D+1t and ?
0
t defined as constants. In Viterbi (maximum likelihood) decoding, the
marginals (sums) in this equation may be approximated using an argmax operator. A
graphical representation of the dependencies in this model is shown in Figure 2.
3.2.3 Simple Hierarchic HMMs. The previous generalized definition can be considered a
template for factoring HMMs into synchronized levels, using ? and ? as parameters.
The specific Murphy?Paskin definition of HHMMs can then be considered a ?simple?
instantiation of this template using FSA states for ? and switching variables for ?. In
Section 4, this instantiation will be augmented (or further factored) to incorporate addi-
tional variables over semantic referents at each depth and time step, without changing
the overall topology of the model.
Figure 2
Graphical representation of a HHMMwith D = 3 hidden levels. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables
with observed values.
318
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
In simple HHMMs, each intermediate state variable ?dt is a boolean switching vari-
able f d?,t ? {0, 1} and each modeled state variable ?
d
t is a syntactic, lexical, or phonetic
FSA state qd?,t:
?dt = f
d
?,t (8)
?dt = q
d
?,t (9)
Instantiating ?? as ?Simple-?, f
d is deterministic: true (equal to 1) with probability 1 if
there is a transition at the level immediately below d and the stack element qd?,t?1 is a
final state, and false (equal to 0) with probability 1 otherwise:3
P?Simple-? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
def
=
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1 ?Final : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1?Final : [f
d
?,t= 1]
(10)
where f D+1?,t = 1 and q
0
?,t = ROOT.
Shift probabilities at each level (instantiating ?? as ?Simple-?) are defined using
level-specific transition ?Simple-Trans and expansion ?Simple-Init models:
P?Simple-? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
def
=
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [q
d
?,t= q
d
?,t?1]
if f d+1?,t = 1, f
d
?,t= 0 : P?Simple-Trans (q
d
?,t | q
d
?,t?1)
if f d+1?,t = 1, f
d
?,t= 1 : P?Simple-Init (q
d
?,t | q
d?1
?,t )
(11)
where f D+1?,t = 1 and q
0
?,t = ROOT. This model is conditioned on final-state switching
variables at and immediately below the current HHMM level: If there is no final state
immediately below the current level (the first case above), it deterministically copies the
current FSA state forward to the next time step; if there is a final state immediately below
the current level (the second case presented), it transitions the FSA state at the current
level, according to the distribution ?Simple-Trans; and if the state at the current level is
final (the third case presented), it re-initializes this state given the state at the level
above, according to the distribution ?Simple-Init. The overall effect is that higher-level
HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM
therefore behaves like a probabilistic implementation of a pushdown automaton (or
?shift?reduce? parser) with a finite stack, where the maximum stack depth is equal to
the number of levels in the HHMM hierarchy.
Like HMM states, the states at each level in a simple HHMM also correspond to
weighted FSA (WFSA) states or symbol positions in regular expressions, except that
some states can be nonterminal states, which introduce corresponding sub-expressions
or sub-WFSAs governing state transitions at the level below. The process of expanding
each nonterminal state qd?1?,t to a sub-expression or WFSA (with start state q
d
?,t) is
modeled in ?Simple-Init. Transitions to adjacent (possibly final) states within each
expression or WFSA are modeled in ?Simple-Trans.
3 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
319
Computational Linguistics Volume 35, Number 3
For example, a simple HHMMmay factor a language model into word (q1?,t), phone
(q2?,t), and subphone (q
3
?,t) levels, where a word state may be a single word, a phone state
may be a position in a sequence of phones corresponding to a word, and a subphone
state may be a position in a sequence of subphone states (e.g., onset, middle, and end)
corresponding to a phone. In this case, ?Simple-Init would define a prior model over
words at level 1, a pronunciation model of phone sequences for each word at level 2,
and a state-sequence model of subphone states for each phone at level 3; and?Simple-Trans
would define a word bigram model at level 1, and would deterministically advance
along phone and subphone sequences at levels 2 and 3 (Bilmes and Bartels 2005).
This hierarchy of regular expressions may also be viewed as a probabilistic im-
plementation of a cascaded FSA, used for modeling syntax in information extraction
systems such as FASTUS (Hobbs et al 1996).
4. A Referential Semantic Language Model
A referential semantic language model can now be defined as an instantiation of an
HHMM (as described in Section 3.2), interpreting directives in a reified world model
(as described in Section 3.1). This interpretation framework is novel in that it is defined
dynamically in terms of transitions over referential states?evocations of entity referents
from a (e.g., first-order) world model?stacked up in a Hierarchic HMM. This allows
(1) a straightforward fast implementation of semantic interpretation (as transition) that
is compatible with conventional time-series models used in speech recognition; and (2)
a broader notion of semantic composition that exploits referential context in time order
(from previous constituents to later constituents) as well as bottom-up (from component
constituents to composed constituents).
First, Section 4.1 will describe a definition of semantic constraints as transitions in
a time-series model. Then Section 4.2 will apply these transitions to nested referents
in a Hierarchic HMM. Section 4.3 will introduce a state-based syntactic representa-
tion to link this semantic representation with recognized words. Finally, Section 4.4
will demonstrate the expressive power of this model on some common linguistic
constructions.
Because this section combines notation from different theoretical frameworks (in
particular, from formal semantics and statistical time-series modeling), a notation
summary is provided in Table 1.
4.1 Dynamic Relations
Semantic interpretationmay be easily integrated into a probabilistic time-series model if
it is formulated as a type of transition, from source to destination referents of equivalent
type at adjacent time steps. In other words, while relations in an ordinary Montagovian
interpretation framework (Montague 1973) may be functions from entity referents to
truth value referents, all relations in the world model defined here must be transition
functions from entity referents to entity referents.
One-place properties l may be modeled in this system by defining transitions from
preceding, unconstrained referents to referents constrained by l. The unconstrained
referents can be thought of as context arguments: For example, in the context of the
set of user-writable files, a property like EXECUTABLE evokes the subset of writable
executables. In the subsumption lattice shown in Figure 1, this will define a rightward
transition from each set referent to some subset referent, labeled with the traversed
relation (see Figure 4 in Section 4.4).
320
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 1
Summary of notation used in Section 4.
Model theory (see Section 3.1)
M : a world model
EM : the domain of individuals in world modelM
? : an individual
?M : an interpretation function from logical symbols (e.g., relation labels)
to logical functions over individuals, sets of individuals, etc.
variables with asterisks : refer to an initial world model prior to reification
Type theory (see Section 3.1.1)
E : the type of an individual
T : the type of a truth value
??,?? : the type of a function from type ? to type ? (variables over types)
Set theory (see Section 4.1)
S : a set of individuals
R : a relation over tuples of individuals
Random variables (see Sections 3.2 and 4.2)
h : a hidden variable in a time-series model
o : an observed variable in a time-series model,
(in this case, a frame of the acoustical signal)
? : a complex variable occurring in the reduce phase of processing;
for example, composed of ?e?, f??
? : a complex variable occurring in the shift phase of processing;
for example, composed of ?e?, q??
f : a random variable over final state status; for example, with value 1 or 0
q : a random variable over FSA (syntax) states,
in this case compiled from regular expressions; for example, with value q1 or q2
e : a random variable over referent entities; for example, with value e{?1?2?3}
l : a random variable over relation labels; for example, with value EXECUTABLE
(see Section 4.1)
t : a time step, from 1 to the end of the utterance T
d : a depth level, from 1 to the maximum depth level D
? : a probability model mapping variable values to probabilities
(real numbers form 0.0 to 1.0)
L : functions from FSA (syntax) states to relation labels
variables in boldface : instances or values of a random variable
non-bold variables with single subscripts : are specific to a time step; for example, ?t
non-bold variables with double subscripts : are specific to a reduce or shift phase within
a time step; for example, e?,t, q?,t
non-bold variables with superscripts : are specific to a depth level; for example, ?dt , e
d
?,t
General n-ary semantic relations l in this framework are therefore formulated as a
type of multi-source transition, distinguishing one argument of an original, ordinary
relation l? as an output (destination) and leaving the rest as input (source); then intro-
ducing a context referent as an additional input. Instead of defining simple transition
arcs on a subsumption lattice, n-ary relations more accurately define hyperarcs, with
multiple source referents: zero or more conventional arguments and one additional con-
text referent, leading to a destination referent intersectively constrained to this context.
321
Computational Linguistics Volume 35, Number 3
This model of interpretation as transition also allows referential semantic con-
straints to be applied that occur prior to hypothesized constituents, in addition to those
that occur as arguments. For example, in the sentence go to the package data directory
and hide the executable file, the phrase go to the package data directory provides a powerful
constraint on the referent of the executable file, although it does not occur as an argument
sub-constituent of this noun phrase. In this framework, the referent of the package data
directory (as a set of files) can be passed as a context argument to intersectively constrain
the interpretation of the executable file.
Recall the definition in Section 3.1.2 of a zero-order model M with refer-
ents e{?1,?2,...} reified from sets of individuals {?1, ?2, . . . } in some original first- or
higher-order modelM?. The referential semantic language model described in this arti-
cle interacts with this reified world modelM through queries of the form lM(eS1 , eS2 ),
where l is a relation, eS1 is an argument referent, and eS2 is a context referent (or eS1 is a
context referent if there is no argument). Each query returns a destination referent eS
such that S is a subset of the context set in the original world model M?. These
context-dependent relations l inM are then defined in terms of corresponding ordinary
relations l? of various types in the original world modelM? as follows:
lM(eS1 , eS2 ) = eS s.t.
?
?
?
if l?M? is type ?E, T? : S = S1 ? l?M?
if l?M? is type ?E, ?E, T?? : S = S2 ? (S1 ? l?M? )
if l?M? is type ??E, T?, ?E, T?? : S = S2 ? l?M? (S1)
(12)
where relation products are defined to resemble matrix products:
S ? R = {??? | ???S, ???, ?????R} (13)
For example, a property like EXECUTABLE would ordinarily bemodeled as a functor
of type ?E, T?: given an individual, it would return true if the individual can be executed.
The first case in Equation (12) casts this as a transition from an argument set S1 to
the set of individuals within S1 that are executable. On the other hand, a relation like
CONTAINS would ordinarily be modeled as ?E, ?E, T??: given an individual and then
another individual, it would return true if the relation holds over the pair. The second
case in Equation (12) casts this as a transition from a set of containers S1, given a context
set S2, to the subset of this context that are contained by an individual in S1. Finally, a
first-order functor like LARGEST would ordinarily be modeled as ??E, T?, ?E, T??: given
a set of individuals and then another individual, it would return true if the individual
belongs to the (singleton) set of things that are the largest in the argument set. The last
case in Equation (12) casts this as a transition from a set S1, given a context set S2, to
the (singleton) subset of this context that are members of S1 and are larger than all other
individuals in S1. More detailed examples of each relation type in Equation (12) are
provided in Section 4.4.
Relations in this world model have the character of being context-dependent in the
sense that relations like CAPTAIN that are traditionally one-place (denoting a set of enti-
ties with rank captain) are now two-place, dependent on an argument superconcept in
the subsumption lattice. Relations can therefore be given different meanings at different
places in the world model: in the context of a particular football team, CAPTAIN will
refer to a particular player; in the context of a different team, it will refer to someone
else. One-place relations can still be defined using a subsumption lattice root concept
322
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
?? as a context argument of course, but this will increase the perplexity (number of
choices) at the root concept, making recognition less reliable.
In this definition, referents e are similar to the information states in Dynamic Predi-
cate Logic (Groenendijk and Stokhof 1991), except that only limited working memory
for information states is assumed, containing only one referent (or variable binding in
DPL terms) per HHMM level.
4.2 Referential Semantic HHMM
Like the simple HHMM described in Section 3.2.3, the referential semantic language
model described in this article (henceforth RSLM), is defined by instantiating the gen-
eral HHMM ?template? defined in Section 3.2.2. This RSLM instantiation incorporates
both the switching variables f ?{0, 1} and FSA state variables q of the simple HHMM,
and adds variables over semantic referents e to the ?reduce? and ?shift? phases at each
level. Thus, the RSLM decomposes each HHMM reduce variable ?dt into a joint variable
subsuming an intermediate referent ed?,t and a final-state switching variable f
d
?,t; and
decomposes each HHMM shift variable ?dt into a joint variable subsuming a modeled
referent ed?,t and an ordinary FSA state q
d
?,t:
?dt = ?e
d
?,t, f
d
?,t? (14)
?dt = ?e
d
?,t, q
d
?,t? (15)
A graphical representation of this referential semantic language model is shown in
Figure 3.
The intermediate referents ed?,t in this framework correspond to the traditional notion
of compositional semantics (Frege 1892), in which meanings of composed constituents
(at higher levels in the HHMM hierarchy) are derived from meanings of component
constituents (at lower levels in the hierarchy). However, in addition to the referents
Figure 3
A graphical representation of the dependencies in the referential semantic language model
described in this article (compare with Figure 2). Again, circles denote random variables
and edges denote conditional dependencies. Shaded circles denote random variables with
observed values.
323
Computational Linguistics Volume 35, Number 3
of their component constituents, the intermediate referents in this framework are also
constrained by the referents at the same depth in the previous time step?the referen-
tial context described in Section 4.1. The modeled referents ed?,t in this framework then
correspond to a snapshot at each time step of the referential state of the recognizer,
after all completed constituents have been composed (or reduced), and after any new
constituents have been introduced (or shifted).
Both intermediate and modeled referents are constrained by labeled relations l
in ?M associated with ordinary FSA states. Thus, relation labels are defined for ?re-
duce? and ?shift? HHMM operations via label functions L? and L?, respectively, which
map FSA states q to relation labels l.
Entity referents ed? at each reduce phase of this HHMM are constrained by the pre-
vious FSA state qdt-1 using a reduce relation l
d
?,t = L?(q
d
?,t-1), such that e
d
? = l
d
?M(e
d+1
? , e
d
t-1).
Reduce probabilities at each level (instantiating ?? as ?RSLM-?) are therefore:
4
P?RSLM-? (?
d
t |?
d+1
t ?
d
t-1?
d-1
t-1 )
def
=
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0] ? [e
d
?,t= e
d
?,t]
if f d+1?,t = 1, q
d
?,t-1 ?Final : [f
d
?,t= 0] ? [e
d
?,t= e
d+1
?,t ]
if f d+1?,t = 1, q
d
?,t-1?Final : [f
d
?,t= 1] ?
[ed?,t= l
d
?,tM(e
d+1
?,t , e
d-1
?,t-1)]
(16)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t-1) pro-
vides a non-trivial constraint only when qd?,t is a final state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e.
Entity referents ed?,t at each shift phase of this HHMM are constrained by the cur-
rent FSA state qd?,t using a shift relation l
d
?,t = L?(q
d
?,t), such that e
d
?,t = l
d
?,tM(e
d-1
?,t, e).
Shift probabilities at each level (instantiating ?? as ?RSLM-?) then generate relation
labels using a ?description? model ?Ref-Init, with referents e
d
?,t and state transitions q
d
?,t
conditioned on (or deterministically dependent on) these labels. The probability distri-
bution over modeled variables is therefore
P?RSLM-? (?
d
t |?
d+1
t ?
d
t ?
d
t-1?
d-1
t )
def
=
?
?
?
?
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? [q
d
?,t= q
d
?,t-1]
if f d+1?,t = 1, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? P?Syn-Trans (q
d
?,t | q
d
?,t-1)
if f d+1?,t = 1, f
d
?,t= 1 :
?
ld?,t
P?Ref-Init (l
d
?,t | e
d-1
?,t q
d-1
?,t)
?[ed?,t= l
d
?,tM(e
d-1
?,t, e)]
?P?Syn-Init (q
d
?,t | l
d
?,t q
d-1
?,t)
(17)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t) pro-
vides a non-trivial constraint only when qd?,t is an initial state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e. The probability models?Ref-Init and
?Syn-Init are induced from corpus observations or defined by hand.
The cases in this equation, conditioned on final-state switching variables f d+1?,t
and f d?,t, correspond to those in Equation (11) in Section 3.2.3. In the first case, where
there is no final state immediately below the current level, referents and FSA states are
simply propagated forward. In the second case, where there is a final state immediately
below the current level, referents are propagated forward and the FSA state is advanced
4 Again, [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
324
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
according to the distribution ?Syn-Trans. In the third case, where the current FSA state is
final and must be re-initialized, a new referent and FSA state are chosen by:
1. selecting, according to a ?description? model ?Ref-Init, a relation label l
d
?,t
with which to constrain the current referent,
2. deterministically generating a referent ed?,t given this label and the referent
at the level above, and
3. selecting, according to a ?lexicalization? model ?Syn-Init, an FSA state q
d
?,t
that is compatible with this label (i.e., has L?(q
d
?,t) = l
d
?,t).
4.3 Associating Semantic Relations with Syntactic Expressions
In this framework, semantic referents are constrained over time by instances of seman-
tic relations l? and l?. These relations are determined by instances of syntactic FSA
states q1, . . . ,qn, themselves expanded from higher-level FSA states q. These associa-
tions between syntactic and semantic random variable values can be represented in
expansion rules of the form
q  q1 . . . qn; with l? = L?(q1) and l? = L?(qn) (18)
where q1 . . . qn may be any regular expression initiating at state q1 and culminating at
(final) state qn. Note that regular expressions must therefore begin with shift relations
and end with reduce relations. This is in order to keep the syntactic and referential
semantic expansions synchronized.
These hierarchic regular expressions are defined to resemble expansion rules in
a context free grammar (CFG). However, unlike CFGs, HHMMs have memory limits
on nesting, in the form of a maximum depth D beyond which no expansion may take
place. As a result, the expressive power of an HHMM is restricted to the set of regular
languages, whereas CFGs may recognize the set of context-free languages; and HHMM
recognition is worst-case linear on the length of an utterance, whereas CFG recognition
is cubic.5 Similar limits have been proposed on syntax in natural languages, motivated
by limits on short term memory observed in humans (Miller and Chomsky 1963;
Pulman 1986). These have been applied to obtain memory-limited parsers (e.g., Marcus
1980), and depth-limited right-corner grammars that are equivalent to CFGs, except
that they restrict the number of internally recursive expansions allowed in recognition
(Schuler and Miller 2005).
4.4 Expressivity
The language model described herein defines referential semantics purely in terms of
HHMM shift and reduce operations over referent entities, made from reified sets of
individuals in some original world model. This section will show that this basic model
is sufficiently expressive to represent many commonly occurring linguistic phenomena,
5 When expressed as a function of the size of the grammar, HHMM recognition is asymptotically
exponential on D, whereas CFG recognition is cubic regardless of depth. In practice, however, exact
inference using either formalism is impractical, so approximate inference is used instead (e.g.,
maintaining a beam at each time step or at each constituent span in CFG parsing).
325
Computational Linguistics Volume 35, Number 3
Figure 4
A subsumption lattice (laid on its side, in gray) over the power set of a domain containing three
files: f1 (a writable executable), f2 (a read-only executable), and f3 (a read-only data file).
?Reference paths? made up of conjunctions of relations l (directed arcs, in black) traverse the
lattice from left to right toward the empty set, as referents (e{...}, corresponding to sets of files)
are incrementally constrained by intersection with each lM. (Some arcs are omitted for clarity.)
including intersective modifiers (e.g., adjectives like executable), multi-argument rela-
tions (e.g., prepositional phrases or relative clauses, involving trajector and landmark
referents), negation (as in the adverb not), and comparatives over continuous properties
(e.g., larger).
4.4.1 Properties. Properties (traditionally unary relations like EXECUTABLE or WRITABLE)
can be represented in theworldmodel as labeled edges lt from supersets et?1 to subsets et
defined by intersecting the set et?1 with the set ltM satisfying the property lt. Recall that
a reified world model can be cast as a subsumption lattice as described in Section 3.1.2.
The result of conjoining a property l with a context set e can therefore be found by
downward traversal of an edge in this lattice labeled l and departing from e.6
Thus, in Figure 4, the set of executables that are read-only would be reachable by
traversing a READ-ONLY relation from the set of executables, or by traversing an EX-
ECUTABLE relation from the set of read-only objects, or by a composed path READ-
ONLY?EXECUTABLE or EXECUTABLE?READ-ONLY from e. The resulting set may then
serve as context for subsequent traversals. Property relations may also result in self-
traversals (e.g., DATAFILE?READ-ONLY in Figure 4) or traversals to the empty set e?
(e.g., DATAFILE?WRITABLE). Property relations like EXECUTABLE can be defined using
the dynamic relations in the first case of Equation (12) in Section 4.1, which simply
ignore the non-context argument.
A general template for intersective nouns and modifiers can be expressed as a noun
phrase (NP) expansion using the following regular expression (where l? and l? indicate
relation labels constraining referents at the beginning and end of the NP):
NP ? Det
(
Adj
)?
Noun
(
PP |RC
)?
; with l? = IDENTITY and l? = IDENTITY (19)
6 Although properties (and later, n-ary relations) are defined in terms of an exponentially large
subsumption lattice, this lattice need not be an actual data structure. If the world model is queried from a
decoder trellis with a beam filter rather than from a complete search, only those lattice relations that are
phonologically, syntactically, and semantically most likely (in other words, those that are on this beam)
will be explored.
326
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in which referents are successively constrained by the semantics of relations associated
with adjective and noun expansions:
Adj ? executable; with l? = EXECUTABLE and l? = IDENTITY (20)
Noun ? executable; with l? = EXECUTABLE and l? = IDENTITY (21)
(and are also constrained by the prepositional phrase (PP) and relative clause (RC)
modifiers, as described below). Here the relation EXECUTABLE traverses from refer-
ent e{f1f2f3} to referent e{f1f2}, a subset of e{f1f2f3} satisfying EXECUTABLE
?
M? .
4.4.2 n-ary Relations. Sequences of properties (traditionally unary relations) can be inter-
preted as simple nonbranching paths from referent to referent in a subsumption lattice,
but higher-arity relations define more complex paths that fork and rejoin. For example,
the referent of the directory containing the executable in Figure 5 would be reachable only
by:
1. storing the original set of directories e{d1d2d3} as a top-level referent in the
HHMM hierarchy, then
2. traversing a CONTAIN relation departing e{d1d2d3} to obtain the contents of
those directories e{f2f3}, then
3. traversing an EXECUTABLE relation departing e{f2f3} to constrain this set to
the set of contents that are also executable: e{f2}, then
4. traversing the inverse CONTAIN? of relation CONTAIN to obtain the
containers of these executables, then constraining the original set of
directories e{d1d2d3} by intersection with this resulting set to yield the
directories containing executables: e{d2}.
This ?forking? of referential semantic paths is handled via syntactic recursion: one path is
explored by the recognizer while the other waits on the HHMM hierarchy (essentially
Figure 5
Reference paths for a relation containing in the directory containing the executable file. A reference
path forks to specify referents using a two-place relation CONTAIN in a domain of directories
d1, d2, d3 and files f1, f2, f3. Here, d2 contains f2 and d3 contains f3, and f1 and f2 are executable. The
ellipsis in the referent set indicates the presence of additional individuals that are not directories.
Again, subsumption is represented in gray and relations are represented in black. (Portions of
the complete subsumption lattice and relation graph are omitted for clarity.)
327
Computational Linguistics Volume 35, Number 3
functioning as a stack). A sample template for branching reduced relative clauses (or
prepositional phrases) that exhibit this forking behavior can be expressed as below:
RC ? containing NP; with l? = CONTAIN and l? = CONTAIN? (22)
where the inverse relation CONTAIN? is applied when the NP expansion concludes or
reduces (when the forked paths are re-joined). Relations like CONTAIN are covered in
the second case of Equation (12) in Section 4.1, which define transitions from sets of
individuals associated with one argument of an original relation CONTAIN? to sets of
individuals associated with the other argument of this relation, in the presence of a
context set, which is a superset of the destination. The calculation of semantic tran-
sition probabilities for n-ary relations thus resembles that for properties, except that
the probability term associated with the relation l? and the inverse relation l? would
depend on both context and argument referents (to its left and below it, in the HHMM
hierarchy).
Note that there is ultimately a singleton referent {f2} of the executable file in Figure 5,
even though there are two executable files in the world model used in these examples.
This illustrates an important advantage of a dynamic context-dependent (three referent)
model of semantic composition over the strict compositional (two referent) model. In a
dynamic context model, the executable file is interpreted in the context of the files that are
contained in a directory. In a strict compositional model, the executable file is interpreted
only in the context of fixed constraints covering the entire utterance, and the constraints
related to the relation containing are applied only to the directories. This means that a
generative model based on strict composition will assign some probability to an infi-
nitely recursive description the directories containing executables contained by directories . . .
In generation systems, this problem has been addressed by adding machinery to keep
track of redundancy (Dale and Haddock 1991). But in this framework, a description
model (?Ref-Init) which is sensitive to the sizes of its source referent and destination ref-
erent at the end of each departing labeled transition will be able to disprefer referential
transitions that attempt to constrain already singleton referents, or that provide only
trivial or vacuous (redundant) constraints in general. This solution is therefore more in
line with graph-basedmodels of generation (Krahmer, van Erk, and Verleg 2003), except
that the graphs proposed here are over reified sets rather than individuals, and the goal
is a generative probability model of language rather than generation per se.
4.4.3 Negation. Negation can be modeled in this framework as a relation between sets.
Although it does not require any syntactic memory, negation does require referential
semantic memory, in that the complement of a specified set must be intersected with
some initial context set. Files that are not writable must still be files after all; only the
writable portion of this description should be negated.
A regular expression for negation of adjectives is
Adj ? not Adj; with l? = IDENTITY and l? = NOT (23)
and is applied to a world model in Figure 6. Relations like NOT are covered in the third
case of Equation (12) in Section 4.1, which define transitions between sets in an original
relation NOT?.
4.4.4 Comparatives, Superlatives, and Subsective Modifiers. Comparatives (e.g., larger),
superlatives (e.g., largest), and subsective modifiers (e.g., large, relative to some context
328
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Figure 6
Reference paths for negation in files that are not writable, using a world model with files f1, f2,
and f3 of which only f1 is writable. The recognizer first forks a copy of the set of files {f1, f2, f3}
using the relation IDENTITY, then applies the adjective relation WRITABLE to yield {f1}. The
complement of this set {f2, f3, . . . } is then intersected with the stored top-level referent
set {f1, f2, f3} to produce the set of files that are not writable: {f2, f3}. Ellipses in referent sets
indicate the presence of additional individuals that are not files.
set) define relations from sets to sets, or from sets to individuals (singleton sets). They
can be handled in much the same way as negation. Here the context is provided from
previous words and from sub-structure, in contrast to DeVault and Stone (2003), which
define the context of a comparative either from fixed inter-utterance constraints or as the
referent of the portion of the noun phrase dominated by the comparative (in addition
to inter-utterance constraints). One advantage of dynamic (time-order) constraints is
that implicit comparatives (in the Clark directory, select the file that is larger, with no
complement) can be modeled with no additional machinery. If substructure context is
not needed, then no additional HHMM storage is necessary.
A regular expression for superlative adjectives is
Noun ? largest Noun; with l? = IDENTITY and l? = LARGEST (24)
and is applied to a world model in Figure 7. Relations like LARGEST are also covered
in the third case of Equation (12), which defines transitions between sets in an original
relation LARGEST?.
5. Evaluation in a Spoken Language Interface
Much of the motivation for this approach has been to develop a human-like model of
language processing. But there are practical advantages to this approach as well. One
of the main practical advantages of the referential semantic language model described
Figure 7
Reference paths for a comparative in the largest executable; this forks a copy of the referent set
{f1, f2, f3} using the relation IDENTITY, applies EXECUTABLE to the forked set to obtain {f1, f2},
and returns the referent {f2}with the largest file size using LARGEST.
329
Computational Linguistics Volume 35, Number 3
in this article is that it may allow spoken language interfaces to be applied to content-
creation domains that are substantially developed by individual users themselves. Such
domains may include scheduling or reminder systems (organizing items containing
idiosyncratic person or event names, added by the user), shopping lists (containing
idiosyncratic brand names, added by the user), interactive design tools (containing new
objects designed and named by the user), or programming interfaces for home or small
business automation (containing new actions, defined by the user). Indeed, computers
are frequently used for content creation as well as content browsing; there is every
reason to expect that spoken language interfaces will be used this way as well.
But the critical problem of applying spoken language interfaces to these kinds of
content-creation domains is that the vocabulary of possible proper names that users
may add or invent is vast. Interface vocabularies in such domains must allow new
words to be created, and once they are created, these new words must be incorpo-
rated into the recognizer immediately, so that they can be used in the current context.
The standard tactic of training language models on example sentences prior to use
is not practical in such domains?except for relatively skeletal abstractions, example
sentences will often not be available. Even very large corpora gleaned from Internet
documents are unlikely to provide reliable statistics for users? made-up names with
contextually appropriate usage, as a referential semantic language model provides.
Content-creation applications such as this may have considerable practical value as
a means of improving accessibility to computers for disabled users. These domains also
provide an ideal proving ground for a referential semantic language model, because
directives in these domains mostly refer to a world model that is shared by the user
and the interfaced application, and because the idiosyncratic language used in such
domains makes it more resistant to domain-independent corpus training than other
domains. In contrast, domains such as database query (e.g., of airline reservations),
dictation, or information extraction are less likely to benefit from a referential semantic
language model, because the world model in such domains is not shared by either the
speaker (in database query) or by the interfaced application (in dictation or information
extraction),7 or because these domains are relatively fixed, so the expense ofmaintaining
linguistic training corpora in these domains can often be justified.
This section will describe an evaluation of an implementation of the referential
semantic language model as a spoken language interface in a very basic content-
creation domain: that of a file organizer, similar to a Unix shell.8 The performance of the
model on this domain will be evaluated in large environments containing thousands
of entities; more than will fit on the beam used in the Viterbi decoding search in this
implementation.
The experiments described in Sections 5.1 through 5.8 were conducted to investigate
the effect on recognition time and accuracy of using a referential semantic language
model to recognize common types of queries, generated by an experimenter and read by
several speakers. A thorough evaluation of the possible coverage of this kind of system
on spontaneous input (e.g., in usability experiments) would require a rich syntactic
representation and attention to disfluencies and speech repairs which are beyond the
scope of this article (see Section 6).
7 Techniques based on abductive reasoning may mitigate this problem of incomplete model sharing
(Hobbs et al 1993), but this would require considerable extensions to the proposed model, and is
beyond the scope of this article.
8 This is also similar to a spoken language version of Wilensky?s Unix consultant (Wilensky, Arens, and
Chin 1984).
330
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
5.1 Ontology Navigation Test Domain
To evaluate the contribution to recognition accuracy of referential semantics over that of
syntax and phonology alone, a baseline (syntax only) and test (baseline plus referential
semantics) recognizer were run on sample ontology manipulation directives in a ?stu-
dent activities? domain. This domain has the form of a simple tree-like taxonomy, with
some cross-listings (for example, studentsmay be listed in homerooms and in activities).
Taxonomic ontologies (e.g., for organizing biological classifications or computer file
directories) can be mapped to reified world models of the sort described in Section 3.1.2.
Concepts C in such an ontology define sets of individuals described by that concept:
{?|C(?)}. Subconcepts C? of a concept C then define subsets of individuals: {?|C?(?)} ?
{?|C(?)}. These sets and subsets can be reified as referent entities and arranged on
a subsumption lattice as described in Section 3.1.2. A sample taxonomic ontology is
shown in Figure 8a (tilted on its side tomatch the subsumption lattices shown elsewhere
in this article). Thus defined, such ontologies can be navigated using referent transitions
described in Section 4.1 by entering concept referents via ?downward? (rightward in the
figure) transitions, and leaving concept referents via ?upward? (leftward) transitions.
For example, this ontology can be manipulated using directives such as:
(1) set Crookston campus homeroom two Clark to sports football captain
which are incrementally interpreted by transitioning down the subsumption lattice (e.g.,
from sports to football to captain) or forking to another part of the lattice (e.g., from Clark
to sports).
As an ontology like this is navigated in spoken language, there is a sense in which
other referents e? at the same level of the ontology as the most recently described refer-
ent e, or at higher levels of the ontology than the most recently described entity, should
be semantically accessible without restating the ontological context (the path from the
root concept e) shared by e
? and e. Thus, in the context of having recently referred
to someone in Homeroom 2 at a particular campus in a school activities database,
other students in the same homeroom or other activities at the same campus should
be accessible without giving an explicit back up directive at each branch in the ontology.
To see the value of implicit upward transitions, compare Example (1) to a directive that
makes upward transitions explicit using the keyword back (similar to ?..? in the syntax of
Unix paths) to exit the homeroom two and Clark folders:
(2) set Crookston campus homeroom two Clark to back back sports football captain
or if starting from the Duluth campus sports football directory:
(3) set back back back Crookston campus homeroom two Clark to back back sports
football captain
Instead of requiring explicit back keywords, these upward transitions can be implic-
itly composed with downward transitions, resulting in transitions from source eS1 to
destination eS via some ancestor eS0 :
UP-lM(eS1 , eS2 ) = eS s.t. ?eS0 S0?S1, S0?S, lM(eS0 , e) = eS (25)
The composed transition function finds a referent eS0 which subsumes both eS1 and eS,
then finds an ordinary (downward) transition l connecting eS0 to eS. The result is a UP-l
transition to every immediate child of an ancestor a referent (or in genealogical terms,
331
Computational Linguistics Volume 35, Number 3
Figure 8
Upward and downward transitions in a sample student activities world model. Downward
transitions (a) define basic sub-type relations. Upward transitions (b) relate sibling, ancestor, and
(great-great-...-)aunt/uncle concepts. The entire model is reachable from any given referent via
these two kinds of transitions.
to every sibling, ancestor, and sibling of ancestor), making these contextually salient
concepts immediately accessible without explicit back-stepping (see Figure 8b).
Downward transitions are ordinary properties, as defined in the first case of
Equation (12) in Section 4.1.
5.2 Scaling to Richer Domains
Although navigation in this domain is constrained to tree-like graphs, this domain tests
all of the features of a referential semantic language model that would be required
332
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in richer domains. As described in Section 4, rich domains (in particular, first-order
domains, in which users can describe sets of individuals as referents) are mapped to
transition edges on a simple graph, similar to the tree-like graphs used in this ontology.
In first-order domains, the size of this graph may be exponential on the number of
individuals in the world model. But once the number of referents exceeds the size of the
decoder beam, the time performance of the recognizer is constrained not by the number
of entities in the world model, but by the beam width and the number of outgoing
relations (labels) that can be traversed from each hypothesis. In a first-order system, just
as in the simple ontology navigation system evaluated here, this number of relations
is constrained to the set of words defined by the user up to that point. In both cases,
although the interface may be used to describe any one of an arbitrarily large set of
referents, the number of referents that can be evoked at the next time step is bounded by
a constant.
When this model is extended to first-order or continuous domains, the time re-
quired to calculate sets of individuals or hypothetical planner states that result from
a transition may be nontrivial, because it may not be possible in such domains to retain
the entire referent transition model in memory. In first-order domains, for example, this
may require evaluating certain binary relations over all pairs of individuals in the world
model, with time complexity proportional to the square of the size of the world model
domain. Fortunately the model described herein, like most generative languagemodels,
hypothesizes words before recognizing them. This means a recognizer based on this
model will be able to compute transitions thatmight follow a hypothesizedword during
the time that word is being recognized. If just the current set of possible transitions
is known (say, these have already been pre-fetched into a cache), the set of outgoing
transitions that will be required at some time following one of these current transitions
can be requested as soon as the beginning of this transition is hypothesized?as soon
as any word associated with this transition makes its way onto the decoder beam.
From this point, the recognizer will have the entire duration of the word to compute
(in parallel, in a separate thread, or on a separate server) the set of outgoing transitions
that may follow this word. In other words, the model described herein may be scaled to
richer domains because it is amenable to parallelization.
5.3 World Model
The student activities ontology used in this evaluation is a taxonomic world model
definedwith upward and downward transitions as described in Section 5.1. It organizes
extracurricular activities under subcategories (e.g., offense ? football ? sports), and
organizes students into homerooms, in which context they can be identified by a single
(first or last) name. Every student or activity is an entity e in the set of entities E , and
relations l are subcategory labels or student names.
5.3.1 World Model M240. In the original student activities world model M240, a total
of 240 entities were created in E : 158 concepts (groups or positions) and 82 instances
(students), each connected via a labeled arc from a parent concept.
Because a world model in this framework is a weighted set of labeled arcs, it is
possible to calculate a meaningful perplexity statistic for transitions in this model,
assuming all referents are equally likely to be a source. The perplexity of this world
model (the average number of departing arcs) is 16.79, after inserting ?UP? arcs as
described in Section 5.1.
333
Computational Linguistics Volume 35, Number 3
5.3.2 WorldModelM4175.An expanded version of the students ontology,M4175, includes
4,175 entities from 717 concepts and 3,458 instances. This model contains M240 as a
subgraph, so that the same directives may be used in either domain; but it expands
M240 from above, with additional campuses and schools, and below, with additional
students in each class. The perplexity of this world model was 37.77, after inserting
?UP? arcs as described in Section 5.1.
5.4 Test Corpus
A corpus of 144 test sentences (no training sentences) was collected from seven native
English speakers (5male, 2 female), whowere asked tomake specific edits to the student
activities ontology described previously. The subjects were all graduate students and
native speakers of English, from various parts of the United States. The edit directives
were recorded as isolated utterances, not as part of an interactive dialogue, and the
target concepts were identified by name in written prompts, so the corpus has much of
the character of read speech. The average sentence length in this collection is 7.17 words.
5.5 Acoustic Model
Baseline and test versions of this system were run using a Recurrent Neural Network
(RNN) acoustic model (Robinson 1994). This acoustic model performs competitively
with multi-state triphone models based on multivariate Gaussian mixtures, but has the
advantage of using only uniphones with single subphone states. As a result, less of the
HMM trellis beam is occupied with subphone variations, so that a larger number of
semantically distinct hypotheses may be considered at each frame.
Each model was evaluated using parameters trained from the TIMIT corpus of
read speech (Fisher et al 1987). This corpus yields several thousand examples for each
of the relatively small set of single-state uniphones used in the RNN model. Read
speech is also appropriate training data for this evaluation, because the test subjects
are constrained to perform fixed edit tasks given written prompts, and the number of
reasonable ways to perform these tasks is limited by the ontology, so hesitations and
disfluencies are relatively rare.
5.6 Phone and Subphone Models
The language model used in these experiments is decomposed into five hierarchic
levels, each with referent e and ordinary FSA state q components, as described in
Section 4.2. The top three levels of this model represent syntactic states as q (derived
from regular expressions defined in Section 4.3) and associated semantic referents as e.
The bottom two levels represent pronunciation and subphone states as q, and ignore e.
Transitions across pronunciation states are defined in terms of sequences of phones
associated with a word via a pronunciation model. The pronunciation model used in
these experiments is taken from the CMU ARPABET dictionary (Weide 1998). Transi-
tions across subphone states are defined in terms of sequences of subphones associated
with a phone. Because this evaluation used an acoustic model trained on the TIMIT
corpus (Fisher et al 1987), the TIMIT phone set was used as subphones. In most cases,
these subphones map directly to ARPABET phones, so each subphone HMM consists of
a single, final state; but in cases of plosive phones (B, D, G, K, P, and T), the subphone
HMM consists of a stop subphone (e.g., bcl) followed by a burst subphone (e.g., b).
334
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Referents are ignored in both the phone and subphone models, and therefore do not
need to be calculated.
State transitions within the phone level P?Pron-Trans (q
4
?,t | q
4
?,t?1) deterministically ad-
vance along a sequence of phones in a pronunciation; and initial phone sequences de-
pend on words in higher-level syntactic states q3?,t, via a pronunciation model ?Pron-Init:
P?RSLM-? (?
4
t |?
5
t ?
4
t ?
4
t?1 ?
3
t )
def
=
?
?
?
if f 5?,t= 0, f
4
?,t= 0 : [q
4
?,t= q
4
?,t?1]
if f 5?,t= 1, f
4
?,t= 0 : P?Pron-Trans (q
4
?,t | q
4
?,t?1)
if f 5?,t= 1, f
4
?,t= 1 : P?Pron-Init (q
4
?,t | q
3
?,t)
(26)
The student activities domain was developed with no synonymy?only one word de-
scribes each semantic relation. Alternate pronunciations are modeled using a uniform
distribution over all listed pronunciations.
Initialization and transition of subphone sequences depend on the phone at the
current time step and the subphone at the previous time step. This model was trained
directly using relative frequency estimation on the TIMIT corpus itself:
P?RSLM-? (?
5
t |?
6
t ?
5
t ?
5
t?1 ?
4
t )
def
= P?(q5?,t | q
4
?,t q
5
?,t?1) (27)
5.7 Syntax and Reference Models
The three upper levels of the HHMM comprise the syntactic and referential portion of
the language model. Concept error rate tests were performed on three baseline and test
versions of this portion of the language model, using the same acoustic, phone, and
subphone models, as described in Sections 5.5 and 5.6.
5.7.1 Language Model ?LM-Sem. First, the syntactic and referential portion of the language
model was implemented as described in Section 4.2. A subset of the regular expres-
sion grammar appears in Figure 9. Any nondeterminism resulting from disjunction or
Kleene-star repetition in the regular expressions was handled in?Syn-Trans using uniform
distributions over all available following states. Distributions over regular expression
expansions in ?Syn-Init were uniform over all available expansions. Distributions over
labels in?Ref-Init were also uniform over all labels departing the entity referent condition
that were compatible with the FSA state category generated by ?Syn-Init.
Figure 9
Sample grammar for student activities domain. Relations l?, l? = IDENTITY unless otherwise
specified.
335
Computational Linguistics Volume 35, Number 3
5.7.2 Language Model ?LM-NoSem. Second, in order to evaluate the contribution of refer-
ential semantics to recognition, a baseline version of the model was tested with all
relations defined to be equivalent to NIL, returning e at each depth and time step,
with all relation labels reachable in M from e. This has the effect of eliminating all
semantic constraints from the recognizer, while preserving the relation labels of the
original model as a resource from which to calculate concept error rate. The decoding
equations and grammar inModel?LM-NoSem are therefore the same as inModel?LM-Sem;
only the domain of possible referents is restricted.
Again, distributions over state transitions, expansions, and outgoing labels in
?Syn-Trans, ?Syn-Init, and ?Ref-Init are uniform over all available options.
5.7.3 Language Model ?LM-Trigram. Finally, the referential semantic language model (Lan-
guage Model ?LM-Sem) was compiled into a word trigram model, in order to test how
well the model would function as a pre-process to a conventional trigram-based speech
recognizer. This was done by iterating over all possible sequences of hidden state
transitions starting from every possible configuration of referents and FSA states on
a stack of depth D (where D = 3):
ht = ?wt?1,wt? (28)
P(ht | ht?1) = P(wt?1 wt |wt?2 wt?1) = P(wt |wt?2 wt?1) (29)
def
=
?
?t?2..t
?
wt?2,wt?1
P?Uniform (?t?2) ? [wt?2=W(q
1..D
?,t?2)]
?P?LM-Sem (?t?1 |?t?2) ? [wt?1=W(q
1..D
?,t?1)]
?P?LM-Sem (?t |?t?1) ? [wt=W(q
1..D
?,t )]
(30)
First, every valid combination of syntactic categories was calculated in a depth-
first search using ?LM-NoSem. Then every combination of three referents from M240 was
hypothesized as a possible referent configuration. A complete set of possible initial
values for ?t?2 was then filled with combinations from the set of syntactic category
configuration crossed with the set of referent configurations. From each possible ?t?2,
?LM-Sem was consulted to give a distribution over ?t?1 (assuming a word-level transition
occurs, with f 4?,t?1 = 1), and then again from each possible configuration of ?t?1 to give
a distribution over ?t (again assuming a word-level transition). The product of these
transition probabilities was then calculated and added to a trigram count, based on the
words wt?2, wt?1, and wt occurring in ?t?2, ?t?1, and ?t. These trigram counts were then
normalized over wt?2 and wt?1 to give P(wt |wt?2 wt?1).
5.8 Results
The following results report Concept Error Rate (CER), as the sum of the percentages of
insertions, deletions, and substitutions required to transform the most likely sequence
of relation labels hypothesized by the system into the hand-annotated transcript, ex-
pressed as a percentage of the total number of labels in the hand-annotated transcript.
Because there are few semantically unconstrained function words in this domain, this is
essentially word error rate, with a few multi-word labels (e.g., first chair, homeroom two)
concatenated together.
5.8.1 Language Model ?LM-Sem and World Model M240. Results using Language Model
?LM-Sem with the 240-entity world model (M240) show an overall 17.1% CER (Table 2).
336
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 2
Per-subject results for Language Model ?LM-Sem withM240.
subject % correct % substitute % delete % insert CER %
0 83.8 14.1 2.1 2.8 19.0
1 73.2 20.3 6.5 5.8 32.7
2 90.2 7.8 2.0 0.7 10.5
3 88.1 9.3 2.7 0.7 12.6
4 88.4 10.3 1.4 3.4 15.1
5 90.8 8.5 0.7 7.0 16.2
6 90.6 8.6 0.7 3.6 12.9
all 86.4 11.3 2.3 3.4 17.1
Table 3
Per-subject results for Language Model ?LM-Sem withM4175.
subject % correct % substitute % delete % insert CER %
0 85.2 14.1 0.7 2.1 16.9
1 70.6 25.5 3.9 7.2 36.6
2 86.9 9.2 3.9 3.9 17.0
3 86.8 11.3 2.0 2.0 15.2
4 83.6 14.4 2.1 6.9 23.3
5 89.4 9.9 0.7 3.5 14.1
6 89.9 9.4 0.7 5.0 15.1
all 84.5 13.5 2.1 4.4 19.9
Here the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 59.44%.
5.8.2 Language Model ?LM-Sem and World ModelM4175. With the number of entities (and
words) increased to 4,175 (M4175), the CER increases slightly to 19.9% (Table 3). Here
again, the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 62.24%. Here, the use of a
world model (Language Model ?LM-Sem) with no linguistic training data is comparable
to that reported for other large-vocabulary systems (Seneff et al 2004; Lemon and
Gruenstein 2004), which were trained on sample sentences.
5.8.3 LanguageModel?LM-NoSem with noWorldModel. In comparison, a baseline using only
the grammar and vocabulary from the students domainM240 without any world model
information and no linguistic training data (Language Model ?LM-NoSem) scores 43.5%
(Table 4).9 The sentence error rate for this experiment was 93.01%.
Ignoring the world model significantly raises error rates compared to Model
?LM-Sem (p < 0.01 using pairwise t-test against Language model ?LM-Sem with M240,
grouping scores by subject), suggesting that syntactic constraints are poor predictors of
9 Ordinarily a syntactic model would be interpolated with word n-gram probabilities derived from corpus
training, but in the absence of training sentences these statistics cannot be included.
337
Computational Linguistics Volume 35, Number 3
Table 4
Per-subject results for Language Model ?LM-NoSem.
subject % correct % substitute % delete % insert CER %
0 57.0 35.9 7.0 12.7 55.6
1 49.0 41.2 9.8 13.7 64.7
2 71.9 18.3 9.8 6.5 34.6
3 69.5 26.5 4.0 9.3 39.7
4 67.8 28.8 3.4 13.7 45.9
5 79.6 19.0 1.4 7.0 27.5
6 75.5 22.3 2.2 10.8 35.3
all 67.1 27.5 5.5 10.5 43.5
concepts without considering reference. But this is not surprising: because the grammar
by itself does not constrain the set of ontology labels that can be used to construct a
path, the perplexity of this model is 240 (reflecting a uniform distribution over nearly
the entire lexicon), whereas the perplexity ofM240 is only 16.79.
5.8.4 Language Model ?LM-Trigram and World Model M240. In order to test how well the
model would function as a pre-process to a conventional trigram-based speech recog-
nizer, the referential semantic languagemodel (LanguageModel?LM-Sem) was compiled
into a word trigram model. This word trigram language model (Language Model
?LM-Trigram), compiled from the referential semantic model (in the 240-entity domain),
shows a concept error rate of 26.6% on the students experiment (Table 5). The sentence
error rate for this experiment was 66.43%.
Using trigram context (Language Model ?LM-Trigram) similarly shows statistically
significant increases in error over Language Model ?LM-Sem with M240 (p = 0.01 using
pairwise t-test, grouping scores by subject), showing that referential context is also
more predictive than word n-grams derived from referential context. Moreover, the
compilation to trigrams required to build Language Model ?LM-Trigram is expensive
(requiring several hours of pre-processing) because it must consider all combinations
of entities in the world model. This would make the pre-compiled model impractical in
mutable domains.
Table 5
Per-subject results for Language Model ?LM-Trigram withM240.
subject % correct % substitute % delete % insert CER %
0 76.1 19.0 4.9 5.6 29.6
1 56.9 24.8 18.3 12.4 44.4
2 81.7 9.2 9.2 0.0 18.3
3 83.4 13.9 2.7 2.0 18.5
4 79.5 13.0 7.5 11.0 31.5
5 86.6 10.6 2.8 0.7 14.1
6 83.5 14.4 2.2 0.7 17.3
all 78.1 15.0 6.9 4.7 26.6
338
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 6
Experimental results with four model configurations.
experiment correct substitute delete insert CER
?LM-Sem,M240 86.4 11.3 2.3 3.4 17.1
?LM-Sem,M4175 84.5 13.5 2.1 4.4 19.9
?LM-NoSem 67.1 27.5 5.5 10.5 43.5
?LM-Trigram,M240 78.1 15.0 6.9 4.7 26.6
5.8.5 Summary of Results. Results in Table 6 summarize the results of the four
experiments.
Some of the erroneously hypothesized directives in this domain described im-
plausible edits: for example, making one student a subset of another student. Domain
information or meta-data could eliminate some of these kinds of errors, but in content-
creation applications it is not always possible to provide this information in advance;
and given the subtle nature of the effect of this information on recognition, it is not clear
that users would want to manage it themselves, or allow it to be automatically induced
without supervision.10 In any case, the comparison described in this section to a non-
semantic model?LM-NoSem suggests that the worldmodel by itself is able to apply useful
constraints in the absence of domain knowledge. This suggests that, in an interpolated
approach, direct world model information may relieve some of the burden on authored
or induced domain knowledge to perform robustly, so that this domain knowledge may
be authored more sparsely or induced more conservatively than it otherwise might.
All evaluations ran in real time on a 4-processor dual-core 2.6GHz server, with a
beam width of 1,000 hypotheses per frame. Differences in runtime performance were
minimal, even between the simple trigrammodel and HHMM-based referential seman-
tic language models. This was due to two factors:
1. All recognizers were run with the same beam width. Although it might
be possible to narrow the beam width to produce faster than real-time
performance for some models, widening the beam beyond 1,000 did not
return significant reductions in CER in the experiments described herein.
2. The implementation of the Viterbi decoder used in these experiments was
optimized to skip combinations of joint variable values that would result
in zero probability transitions (which is a reasonable optimization for any
factored time-series model), significantly decreasing runtime for HHMM
recognition.
5.8.6 Statistical Significance vs. Magnitude of Gain. The experiments described in this
article show a statistically significant increase in accuracy due to the incorporation of
referential semantic information into speech decoding. But these results should not be
interpreted to demonstrate any particular magnitude of error reduction (as might be
claimed for the introduction of head words into parsing models, for example).
10 Ehlen et al (2008) provide an example of a user interface for managing imperfect automatically-induced
information about task assignments from meeting transcripts, which is much more concrete than the kind
of domain knowledge inference considered here.
339
Computational Linguistics Volume 35, Number 3
First, this is because the acoustic model used in these experiments was trained on
a relatively small corpus (6,000 utterances), which introduces the possibility that the
acoustic model was under-trained. As a result, the error rates for both baseline and
test systems may be greater here than if a larger training corpus had been used, so the
performance gain due to the introduction of referential semantics may be overstated.
Second, these experiments were designed with relatively strong referential con-
straints (a tree-like ontology, with a perplexity of about 17 for M240) and relatively
weak syntactic constraints (allowing virtually any sequence of relation labels, with a
much higher perplexity of about 240), in order to highlight differences due to referential
semantics. In general use, recognition accuracy gains due to the incorporation of ref-
erential semantic information will depend crucially on the relative perplexity of the
referential constraints combined with syntactic constraints, compared to that of syntac-
tic constraints alone. This paper has argued that in content-creation applications this
difference can be manipulated and exploited?in fact, by reorganizing folders into a
binary branching tree (with perplexity 2), a user could achieve nearly perfect speech
recognition?but in applications involving fixed ontologies and purely hypothetical
directives, as in database query applications, gains may be minimal or nonexistant.
6. Conclusion and Future Work
This article has described a referential semantic language model that achieves recogni-
tion accuracy favorably comparable to a pre-compiled trigram baseline in user-defined
domains with no available domain-specific training corpora, through the use of ex-
plicit hypothesized semantic referents. This architecture requires that the interfaced
application make available a queryable world model, but the combined phonological,
syntactic, and referential semantic decoding process ensures the world model is only
queriedwhen necessary, allowing accurate real time performance even in large domains
containing several thousand entities.
The framework described in this article is defined over first-order sets (of individu-
als), making transition functions over referents equivalent to expressions in first-order
logic. This framework can be extended to model other kinds of references (e.g., to time
intervals or events) by casting them as individuals (Hobbs 1985).
The system as defined herein also has some ability to recognize referents con-
strained by quantifiers: for example, the directory containing two files. Because its referents
are reified sets, the system can naturally model relations that are sensitive to cardinality
(self-transitioning if the set has N or greater individuals, transitioning to e? otherwise).
But a dynamic view of the referential semantics of nested quantifiers requires referents
to be indexed to particular iterations of quantifiers at higher levels of nesting in the
HHMM hierarchy (corresponding to higher-scoping quantifiers). Extending the system
to dynamically interpret nested quantifiers therefore requires that all semantic opera-
tions preserve an ?iteration context? of nested outer-quantified individuals for each
inner-quantified individual. This is left for future work.
Some analyses of phenomena like intensional or non-inherent adjectives?for ex-
ample, toy in toy guns, which are not actually guns; or old in old friends, who are
not necessarily elderly (Peters and Peters 2000)?involve referents corresponding to
second-order sets (this allows these adjectives to be composed before being applied to
a noun: old but casual friend). Unfortunately, extending the framework described in this
article to use a similarly explicit representation of second- or higher-order sets would
be impractical. Not only would the number of possible second- or higher-order sets
be exponentially larger than the number of possible first-order sets (which is already
340
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
exponential on the number of individuals), but the length of the description of each
referent itself would be exponential on the number of individuals (whereas the list of
individuals describing a first-order referent is merely linear).
The definition of semantic interpretation as a transition function does support
interesting extensions to hypothetical reasoning and planning beyond the standard
closed-world model-theoretic framework, however. Recall the sentence go to the package
data directory and hide the executable files, or equivalently, in the package data directory,
hide the executable files, exemplifying the continuous context-sensitivity of the referential
semantic language model. Here, the system focuses on the contents of this directory
because a sequence of transitions resulting from the combined phonological, syntactic,
and referential semantic context of the sentence led it to this state. One may characterize
the referential semantic transitions leading to this state as a hypothetical sequence of
change directory actions moving the active directory of the interface to this directory (for
the purpose of understanding the consequences of the first part of this directive). The
hypothesized context of this directory is then a world state or planning state resulting
from these actions. Thus characterized, the referential semantic decoder is performing
a kind of statistical plan recognition (Blaylock and Allen 2005). By viewing referents
as world states, or as having world-state components, it would then be possible to use
logical conclusions of other types of actions as implicit constraints?e.g., unpack the tar
file and hide the executable [which will result from this unpacking]?without adding extra
functionality to the recognizer implementation. Similarly, referents for hypothetical
objects like the noun phrase a tar file in the directive create a tar file, are not part of the
world model when the user describes them.
Recognizing references to these hypothetical states and objects requires a capacity
to dynamically generate referents not in the current world model. The domain of
referents in this extended system is therefore unbounded. Fortunately, as mentioned
in Section 5.2, the number of referents that can be generated at each time step is still
bounded by a constant, equal to the recognizer?s beam width multiplied by the num-
ber of traversable relation labels. This means that distributions over outgoing relation
labels are still well-defined for each referential state. The only difference is that, when
modeling hypothetical referents, these distributions must be calculated dynamically.
Finally, this article has primarily focused on connecting an explicit representation
of referential semantics to speech recognition decisions. Ordinarily this is thought of
as being mediated by syntax, which is covered in this article only through a rela-
tively simple framework of bounded recursive HHMM state transitions. However, the
bounded HHMM representation used in this paper has been applied (without seman-
tics) to rich syntactic parsing as well, using a transformed grammar to minimize stack
usage to cases of center-expansion (Schuler et al 2008). Coverage experiments with this
transformed grammar demonstrated that over 97% of the large syntactically annotated
Penn Treebank (Marcus, Santorini, andMarcinkiewicz 1994) could be parsed using only
three elements of stack memory, with four elements giving over 99% coverage. This
suggests that the relatively tight bounds on recursion described in this paper might be
expressively adequate if syntactic states are defined using this kind of transform.
This transform model (again, without semantics) was then further applied to pars-
ing speech repairs, in which speakers repeat or edit mistakes in their directives: for
example, select the red, uh, the blue folder (Miller and Schuler 2008). The resulting system
models incomplete disfluent constituents using transitions associated with ordinary
fluent speech until the repair point (the uh in the example), then processes the speech
repair using only a small number of learned repair reductions. Coverage results for
the same transform model on the Penn Treebank Switchboard Corpus of transcribed
341
Computational Linguistics Volume 35, Number 3
spontaneous speech showed a similar three- to four-element memory requirement. If
this HHMM speech repair model were combined with the HHMM model of referen-
tial semantics described in this article, referents associated with ultimately disfluent
constituents could similarly be recognized using referential transitions associated with
ordinary fluent speech until the repair point, then reduced using a repair rule that
discards the referent. These results suggest that an HHMM-based semantic framework
such as the one described in this article may be psycholinguistically plausible.
Acknowledgments
The authors would like to thank the
anonymous reviewers for their input. This
research was supported by National Science
Foundation CAREER/PECASE award
0447685. The views expressed are not
necessarily endorsed by the sponsors.
References
Aist, Gregory, James Allen, Ellen Campana,
Carlos Gallo, Scott Stoness, Mary Swift,
and Michael Tanenhaus. 2007. Incremental
understanding in human?computer
dialogue and experimental evidence
for advantages over nonincremental
methods. In Proceedings of DECALOG,
pages 149?154, Trento.
Baker, James. 1975. The Dragon system: an
overivew. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bilmes, Jeff and Chris Bartels. 2005.
Graphical model architectures for speech
recognition. IEEE Signal Processing
Magazine, 22(5):89?100.
Blaylock, Nate and James Allen. 2005.
Recognizing instantiated goals using
statistical methods. In IJCAI Workshop
on Modeling Others from Observations
(MOO-2005), pages 79?86, Edinburgh.
Bos, Johan. 1996. Predicate logic unplugged.
In Proceedings of the 10th Amsterdam
Colloquium, pages 133?143, Amsterdam.
Brachman, Ronald J. and James G. Schmolze.
1985. An overview of the kl-one
knowledge representation system.
Cognitive Science, 9(2):171?216.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Church, Alonzo. 1940. A formulation of the
simple theory of types. Journal of Symbolic
Logic, 5(2):56?68.
Dale, Robert and Nicholas Haddock. 1991.
Content determination in the generation of
referring expressions. Computational
Intelligence, 7(4):252?265.
DeVault, David and Matthew Stone. 2003.
Domain inference in incremental
interpretation. In Proceedings of ICoS,
pages 73?87, Nancy.
Ehlen, Patrick, Matthew Purver, John
Niekrasz, Stanley Peters, and Kari Lee.
2008. Meeting adjourned: Off-line
learning interfaces for automatic meeting
understanding. In Proceedings of the
International Conference on Intelligent User
Interfaces, pages 276?284, Canary Islands.
Fisher, William M., Victor Zue, Jared
Bernstein, and David S. Pallet. 1987. An
acoustic?phonetic data base. Journal of the
Acoustical Society of America, 81:S92?S93.
Frege, Gottlob. 1892. Uber sinn und
bedeutung. Zeitschrift fur Philosophie und
Philosophischekritik, 100:25?50.
Gorniak, Peter and Deb Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
Groenendijk, Jeroen and Martin Stokhof.
1991. Dynamic predicate logic. Linguistics
and Philosophy, 14:39?100.
Haddock, Nicholas. 1989. Computational
models of incremental semantic
interpretation. Language and Cognitive
Processes, 4:337?368.
Hobbs, Jerry R. 1985. Ontological
promiscuity. In Proceedings of ACL,
pages 61?69, Chicago, IL.
Hobbs, Jerry R., Douglas E. Appelt,
John Bear, David Israel, Megumi
Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded
finite-state transducer for extracting
information from natural-language
text. In Yves Schabes, editor, Finite
State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Hobbs, Jerry R., Mark Stickel, Douglas E.
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63:69?142.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
342
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
statistical decoder for the recognition of
continuous speech. IEEE Transactions on
Information Theory, 21:250?256.
Krahmer, Emiel, Sebastiaan van Erk, and
Andre Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Lemon, Oliver and Alexander Gruenstein.
2004. Multithreaded context for
robust conversational interfaces:
Context-sensitive speech recognition and
interpretation of corrective fragments.
ACM Transactions on Computer-Human
Interaction, 11(3):241?267.
Marcus, Mitch. 1980. A Theory of Syntactic
Recognition for Natural Language. MIT
Press, Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martin, Charles and Christopher Riesbeck.
1986. Uniform parsing and inferencing
for learning. In Proceedings of AAAI,
pages 257?261, Philadelphia, PA.
Mellish, Chris. 1985. Computer Interpretation
of Natural Language Descriptions. Wiley,
New York.
Miller, George and Noam Chomsky. 1963.
Finitary models of language users. In
R. Luce, R. Bush, and E. Galanter, editors,
Handbook of Mathematical Psychology,
volume 2. John Wiley, New York,
pages 419?491.
Miller, Tim and William Schuler. 2008.
A unified syntactic model for parsing
fluent and disfluent speech. In
Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL ?08) pages 105?108,
Columbus, OH.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In J. Hintikka, J. M. E. Moravcsik,
and P. Suppes, editors, Approaches
to Natural Language. D. Riedel,
Dordrecht, pages 221?242. Reprinted in
R. H. Thomason ed., Formal Philosophy,
Yale University Press, New Haven,
CT, 1994.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Peters, Ivonne and Wim Peters. 2000.
The treatment of adjectives in simple:
Theoretical observations. In Proceedings
of LREC, paper # 366, Athens.
Pulman, Steve. 1986. Grammars, parsers
and memory limitations. Language and
Cognitive Processes, 1(3):197?225.
Robinson, Tony. 1994. An application of
recurrent nets to phone probability
estimation. In IEEE Transactions on
Neural Networks, 5:298?305.
Seneff, Stephanie, Chao Wang, Lee
Hetherington, and Grace Chung. 2004.
A dynamic vocabulary spoken dialogue
interface. In Proceedings of ICSLP,
pages 1457?1460, Jeju Island.
Schuler, William. 2001. Computational
properties of environment-based
disambiguation. In Proceedings of ACL,
pages 466?473, Toulouse.
Schuler, William, Samir AbdelRahman,
Tim Miller, and Lane Schwartz. 2008.
Toward a psycholinguistically-motivated
model of language. In Proceedings of
COLING, pages 785?792, Manchester, UK.
Schuler, William and Tim Miller. 2005.
Integrating denotational meaning into a
DBN language model. In Proceedings
of the 9th European Conference on Speech
Communication and Technology /
6th Interspeech Event (Eurospeech/
Interspeech?05), pages 901?904, Lisbon.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension.
Science, 268:1632?1634.
Tarski, Alfred. 1933. Prace Towarzystwa
Naukowego Warszawskiego, Wydzial III Nauk
Matematyczno-Fizycznych, 34. Translated as
?The concept of truth in formalized
languages?, in J. Corcoran, editor, Logic,
Semantics, Metamathematics: Papers from
1923 to 1938. Hackett Publishing Company,
Indianapolis, IN, 1983, pages 152?278.
Weide, R. L. 1998. Carnegie Mellon
University Pronouncing Dictionary v0.6d.
Available at www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Wilensky, Robert, Yigal Arens, and David
Chin. 1984. Talking to UNIX: An overview
of UC. Communications of the ACM,
27(6):574?593.
Young, S. L., A. G. Hauptmann, W. H. Ward,
E. T. Smith, and P. Werner. 1989. High
level knowledge sources in usable speech
recognition systems. Communications
of the ACM, 32(2):183?194.
343

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1189?1198,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Complexity Metrics in an Incremental Right-corner Parser
Stephen Wu Asaf Bachrach? Carlos Cardenas? William Schuler?
Department of Computer Science, University of Minnesota
? Unit de Neuroimagerie Cognitive INSERM-CEA
? Department of Brain & Cognitive Sciences, Massachussetts Institute of Technology
? University of Minnesota and The Ohio State University
swu@cs.umn.edu ?asaf@mit.edu ?cardenas@mit.edu ?schuler@ling.ohio-state.edu
Abstract
Hierarchical HMM (HHMM) parsers
make promising cognitive models: while
they use a bounded model of working
memory and pursue incremental hypothe-
ses in parallel, they still achieve parsing
accuracies competitive with chart-based
techniques. This paper aims to validate
that a right-corner HHMM parser is also
able to produce complexity metrics, which
quantify a reader?s incremental difficulty
in understanding a sentence. Besides
defining standard metrics in the HHMM
framework, a new metric, embedding
difference, is also proposed, which tests
the hypothesis that HHMM store elements
represents syntactic working memory.
Results show that HHMM surprisal
outperforms all other evaluated metrics
in predicting reading times, and that
embedding difference makes a significant,
independent contribution.
1 Introduction
Since the introduction of a parser-based calcula-
tion for surprisal by Hale (2001), statistical tech-
niques have been become common as models of
reading difficulty and linguistic complexity. Sur-
prisal has received a lot of attention in recent lit-
erature due to nice mathematical properties (Levy,
2008) and predictive ability on eye-tracking move-
ments (Demberg and Keller, 2008; Boston et al,
2008a). Many other complexity metrics have
been suggested as mutually contributing to reading
difficulty; for example, entropy reduction (Hale,
2006), bigram probabilities (McDonald and Shill-
cock, 2003), and split-syntactic/lexical versions of
other metrics (Roark et al, 2009).
A parser-derived complexity metric such as sur-
prisal can only be as good (empirically) as the
model of language from which it derives (Frank,
2009). Ideally, a psychologically-plausible lan-
guage model would produce a surprisal that would
correlate better with linguistic complexity. There-
fore, the specification of how to encode a syntac-
tic language model is of utmost importance to the
quality of the metric.
However, it is difficult to quantify linguis-
tic complexity and reading difficulty. The two
commonly-used empirical quantifications of read-
ing difficulty are eye-tracking measurements and
word-by-word reading times; this paper uses read-
ing times to find the predictiveness of several
parser-derived complexity metrics. Various fac-
tors (i.e., from syntax, semantics, discourse) are
likely necessary for a full accounting of linguis-
tic complexity, so current computational models
(with some exceptions) narrow the scope to syn-
tactic or lexical complexity.
Three complexity metrics will be calculated in
a Hierarchical Hidden Markov Model (HHMM)
parser that recognizes trees in right-corner form
(the left-right dual of left-corner form). This type
of parser performs competitively on standard pars-
ing tasks (Schuler et al, 2010); also, it reflects
plausible accounts of human language processing
as incremental (Tanenhaus et al, 1995; Brants and
Crocker, 2000), as considering hypotheses proba-
bilistically in parallel (Dahan and Gaskell, 2007),
as bounding memory usage to short-term mem-
ory limits (Cowan, 2001), and as requiring more
memory storage for center-embedding structures
than for right- or left-branching ones (Chomsky
and Miller, 1963; Gibson, 1998). Also, unlike
most other parsers, this parser preserves the arc-
eager/arc-standard ambiguity of Abney and John-
1189
son (1991). Typical parsing strategies are arc-
standard, keeping all right-descendants open for
subsequent attachment; but since there can be an
unbounded number of such open constituents, this
assumption is not compatible with simple mod-
els of bounded memory. A consistently arc-eager
strategy acknowledges memory bounds, but yields
dead-end parses. Both analyses are considered in
right-corner HHMM parsing.
The purpose of this paper is to determine
whether the language model defined by the
HHMM parser can also predict reading times ?
it would be strange if a psychologically plausi-
ble model did not also produce viable complex-
ity metrics. In the course of showing that the
HHMM parser does, in fact, predict reading times,
we will define surprisal and entropy reduction in
the HHMM parser, and introduce a third metric
called embedding difference.
Gibson (1998; 2000) hypothesized two types
of syntactic processing costs: integration cost, in
which incremental input is combined with exist-
ing structures; and memory cost, where unfinished
syntactic constructions may incur some short-term
memory usage. HHMM surprisal and entropy
reduction may be considered forms of integra-
tion cost. Though typical PCFG surprisal has
been considered a forward-looking metric (Dem-
berg and Keller, 2008), the incremental nature of
the right-corner transform causes surprisal and en-
tropy reduction in the HHMM parser to measure
the likelihood of grammatical structures that were
hypothesized before evidence was observed for
them. Therefore, these HHMM metrics resemble
an integration cost encompassing both backward-
looking and forward-looking information.
On the other hand, embedding difference is
designed to model the cost of storing center-
embedded structures in working memory. Chen,
Gibson, and Wolf (2005) showed that sentences
requiring more syntactic memory during sen-
tence processing increased reading times, and it
is widely understood that center-embedding incurs
significant syntactic processing costs (Miller and
Chomsky, 1963; Gibson, 1998). Thus, we would
expect for the usage of the center-embedding
memory store in an HHMM parser to correlate
with reading times (and therefore linguistic com-
plexity).
The HHMM parser processes syntactic con-
structs using a bounded number of store states,
defined to represent short-term memory elements;
additional states are utilized whenever center-
embedded syntactic structures are present. Simi-
lar models such as Crocker and Brants (2000) im-
plicitly allow an infinite memory size, but Schuler
et al (2008; 2010) showed that a right-corner
HHMM parser can parse most sentences in En-
glish with 4 or fewer center-embedded-depth lev-
els. This behavior is similar to the hypothesized
size of a human short-term memory store (Cowan,
2001). A positive result in predicting reading
times will lend additional validity to the claim
that the HHMM parser?s bounded memory cor-
responds to bounded memory in human sentence
processing.
The rest of this paper is organized as fol-
lows: Section 2 defines the language model of the
HHMM parser, including definitions of the three
complexity metrics. The methodology for evalu-
ating the complexity metrics is described in Sec-
tion 3, with actual results in Section 4. Further dis-
cussion on results, and comparisons to other work,
are in Section 5.
2 Parsing Model
This section describes an incremental parser in
which surprisal and entropy reduction are sim-
ple calculations (Section 2.1). The parser uses a
Hierarchical Hidden Markov Model (Section 2.2)
and recognizes trees in a right-corner form (Sec-
tion 2.3 and 2.4). The new complexity metric, em-
bedding difference (Section 2.5), is a natural con-
sequence of this HHMM definition. The model
is equivalent to previous HHMM parsers (Schuler,
2009), but reorganized into 5 cases to clarify the
right-corner structure of the parsed sentences.
2.1 Surprisal and Entropy in HMMs
Hidden Markov Models (HMMs) probabilistically
connect sequences of observed states ot and hid-
den states qt at corresponding time steps t. In pars-
ing, observed states are words; hidden states can
be a conglomerate state of linguistic information,
here taken to be syntactic.
The HMM is an incremental, time-series struc-
ture, so one of its by-products is the prefix prob-
ability, which will be used to calculate surprisal.
This is the probability that that words o1..t have
been observed at time t, regardless of which syn-
tactic states q1..t produced them. Bayes? Law and
Markov independence assumptions allow this to
1190
be calculated from two generative probability dis-
tributions.1
Pre(o1..t)=
?
q1..t
P(o1..t q1..t) (1)
def=
?
q1..t
t
?
?=1
P?A(q? | q??1)?P?B(o? | q? ) (2)
Here, probabilities arise from a Transition
Model (?A) between hidden states and an Ob-
servation Model (?B) that generates an observed
state from a hidden state. These models are so
termed for historical reasons (Rabiner, 1990).
Surprisal (Hale, 2001) is then a straightforward
calculation from the prefix probability.
Surprisal(t) = log2
Pre(o1..t?1)
Pre(o1..t)
(3)
This framing of prefix probability and surprisal in
a time-series model is equivalent to Hale?s (2001;
2006), assuming that q1..t ? Dt, i.e., that the syn-
tactic states we are considering form derivations
Dt, or partial trees, consistent with the observed
words. We will see that this is the case for our
parser in Sections 2.2?2.4.
Entropy is a measure of uncertainty, defined as
H(x) = ?P(x) log2 P(x). Now, the entropy Ht
of a t-word string o1..t in an HMM can be written:
Ht =
?
q1..t
P(q1..t o1..t) log2 P(q1..t o1..t) (4)
and entropy reduction (Hale, 2003; Hale, 2006) at
the tth word is then
ER(ot) = max(0, Ht?1 ? Ht) (5)
Both of these metrics fall out naturally from the
time-series representation of the language model.
The third complexity metric, embedding differ-
ence, will be discussed after additional back-
ground in Section 2.5.
In the implementation of an HMM, candidate
states at a given time qt are kept in a trel-
lis, with step-by-step backpointers to the highest-
probability q1..t?1.2 Also, the best qt are often kept
in a beam Bt, discarding low-probability states.
1Technically, a prior distribution over hidden states,
P(q0), is necessary. This q0 is factored and taken to be a de-
terministic constant, and is therefore unimportant as a proba-
bility model.
2Typical tasks in an HMM include finding the most likely
sequence via the Viterbi algorithm, which stores these back-
pointers to maximum-probability previous states and can
uniquely find the most likely sequence.
This mitigates the problems of large state spaces
(e.g., that of all possible grammatical derivations).
Since beams have been shown to perform well
(Brants and Crocker, 2000; Roark, 2001; Boston
et al, 2008b), complexity metrics in this paper
are calculated on a beam rather than over all (un-
bounded) possible derivations Dt. The equations
above, then, will replace the assumption q1..t ?Dt
with qt?Bt.
2.2 Hierarchical Hidden Markov Models
Hidden states q can have internal structure; in Hi-
erarchical HMMs (Fine et al, 1998; Murphy and
Paskin, 2001), this internal structure will be used
to represent syntax trees and looks like several
HMMs stacked on top of each other. As such, qt
is factored into sequences of depth-specific vari-
ables ? one for each of D levels in the HMM hi-
erarchy. In addition, an intermediate variable ft is
introduced to interface between the levels.
qt
def= ?q1t . . . qDt ? (6)
ft
def= ?f1t . . . fDt ? (7)
Transition probabilities P?A(qt | qt?1) over com-
plex hidden states qt are calculated in two phases:
? Reduce phase. Yields an intermediate
state ft, in which component HMMs may ter-
minate. This ft tells ?higher? HMMs to hold
over their information if ?lower? levels are in
operation at any time step t, and tells lower
HMMs to signal when they?re done.
? Shift phase. Yields a modeled hidden state qt,
in which unterminated HMMs transition, and
terminated HMMs are re-initialized from
their parent HMMs.
Each phase is factored according to level-
specific reduce and shift models, ?F and ?Q:
P?A(qt|qt?1) =
?
ft
P(ft|qt?1)?P(qt|ft qt?1) (8)
def=
?
f1..Dt
D
?
d=1
P?F(f
d
t |fd+1t qdt?1qd?1t?1 )
? P?Q(qdt |fd+1t fdt qdt?1qd?1t ) (9)
with fD+1t and q0t defined as constants. Note that
only qt is present at the end of the probability cal-
culation. In step t, ft?1 will be unused, so the
marginalization of Equation 9 does not lose any
information.
1191
. . .
. . .
. . .
. . .
f3t?1
f2t?1
f1t?1
q1t?1
q2t?1
q3t?1
ot?1
f3t
f2t
f1t
q1t
q2t
q3t
ot
(a) Dependency structure in the HHMM
parser. Conditional probabilities at a node are
dependent on incoming arcs.
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8
the
engineers
pulled
off an
engineering
trick
? ? ? ? ? ? ?
? ? vbd
VBD/PRT
? ? ?
dt
NP/NN
S/VP
S/VP
S/NP
S/NN
S/NN S
(b) HHMM parser as a store whose elements at each time step are listed
vertically, showing a good hypothesis on a sample sentence out of many
kept in parallel. Variables corresponding to qdt are shown.
S
NP
DT
the
NN
engineers
VP
VBD
VBD
pulled
PRT
off
NP
DT
an
NN
NN
engineering
NN
trick
(c) A sample sentence in CNF.
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NN
DT
the
NN
engineers
VBD
VBD/PRT
VBD
pulled
PRT
off
DT
an
NN
engineering
NN
trick
(d) The right-corner transformed version of (c).
Figure 1: Various graphical representations of HHMM parser operation. (a) shows probabilistic depen-
dencies. (b) considers the qdt store to be incremental syntactic information. (c)?(d) demonstrate the
right-corner transform, similar to a left-to-right traversal of (c). In ?NP/NN? we say that NP is the active
constituent and NN is the awaited.
The Observation Model ?B is comparatively
much simpler. It is only dependent on the syntac-
tic state at D (or the deepest active HHMM level).
P?B(ot | qt)
def= P(ot | qDt ) (10)
Figure 1(a) gives a schematic of the dependency
structure of Equations 8?10 for D = 3. Evalua-
tions in this paper are done with D = 4, following
the results of Schuler, et al (2008).
2.3 Parsing right-corner trees
In this HHMM formulation, states and dependen-
cies are optimized for parsing right-corner trees
(Schuler et al, 2008; Schuler et al, 2010). A sam-
ple transformation between CNF and right-corner
trees is in Figures 1(c)?1(d).
Figure 1(b) shows the corresponding store-
element interpretation3 of the right corner tree
in 1(d). These can be used as a case study to
see what kind of operations need to occur in an
3This is technically a pushdown automoton (PDA), where
the store is limited to D elements. When referring to direc-
tions (e.g., up, down), PDAs are typically described opposite
of the one in Figure 1(b); here, we push ?up? instead of down.
HHMM when parsing right-corner trees. There
is one unique set of HHMM state values for each
tree, so the operations can be seen on either the
tree or the store elements.
At each time step t, a certain number of el-
ements (maximum D) are kept in memory, i.e.,
in the store. New words are observed input, and
the bottom occupied element (the ?frontier? of the
store) is the context; together, they determine what
the store will look like at t+1. We can characterize
the types of store-element changes by when they
happen in Figures 1(b) and 1(d):
Cross-level Expansion (CLE). Occupies a new
store element at a given time step. For exam-
ple, at t=1, a new store element is occupied
which can interact with the observed word,
?the.? At t = 3, an expansion occupies the
second store element.
In-level Reduction (ILR). Completes an active
constituent that is a unary child in the right-
corner tree; always accompanied by an in-
level expansion. At t= 2, ?engineers? com-
pletes the active NP constituent; however, the
1192
level is not yet complete since the NP is along
the left-branching trunk of the tree.
In-level Expansion (ILE). Starts a new active
constituent at an already-occupied store ele-
ment; always follows an in-level reduction.
With the NP complete in t= 2, a new active
constituent S is produced at t=3.
In-level Transition (ILT). Transitions the store
to a new state in the next time step at the same
level, where the awaited constituent changes
and the active constituent remains the same.
This describes each of the steps from t=4 to
t=8 at d=1 .
Cross-level Reduction (CLR). Vacates a store
element on seeing a complete active con-
stituent. This occurs after t = 4; ?off?
completes the active (at depth 2) VBD con-
stituent, and vacates store element 2. This
is accompanied with an in-level transition at
depth 1, producing the store at t=5. It should
be noted that with some probability, complet-
ing the active constituent does not vacate the
store element, and the in-level reduction case
would have to be invoked.
The in-level/cross-level ambiguity occurs in the
expansion as well as the reduction, similar to Ab-
ney and Johnson?s arc-eager/arc-standard compo-
sition strategies (1991). At t=3, another possible
hypothesis would be to remain on store element
1 using an ILE instead of a CLE. The HHMM
parser, unlike most other parsers, will preserve this
in-level/cross-level ambiguity by considering both
hypotheses in parallel.
2.4 Reduce and Shift Models
With the understanding of what operations need to
occur, a formal definition of the language model is
in order. Let us begin with the relevant variables.
A shift variable qdt at depth d and time step t is
a syntactic state that must represent the active and
awaited constituents of right-corner form:
qdt
def= ?gAqdt , g
W
qdt
? (11)
e.g., in Figure 1(b), q12=?NP,NN?=NP/NN. Each g is
a constituent from the pre-right-corner grammar,
G.
Reduce variables f are then enlisted to ensure
that in-level and cross-level operations are correct.
fdt
def= ?kfdt , gfdt ? (12)
First, kfdt is a switching variable that differenti-
ates between ILT, CLE/CLR, and ILE/ILR. This
switching is the most important aspect of fdt , so
regardless of what gfdt is, we will use:
? fdt ? F0 when kfdt =0, (ILT/no-op)
? fdt ? F1 when kfdt =1, (CLE/CLR)
? fdt ? FG when kfdt ? G. (ILE/ILR)
Then, gfdt is used to keep track of a completely-
recognized constituent whenever a reduction oc-
curs (ILR or CLR). For example, in Figure 1(b),
after time step 2, an NP has been completely rec-
ognized and precipitates an ILR. The NP gets
stored in gf13 for use in the ensuing ILE instead
of appearing in the store-elements.
This leads us to a specification of the reduce and
shift probability models. The reduce step happens
first at each time step. True to its name, the re-
duce step handles in-level and cross-level reduc-
tions (the second and third case below):
P?F(f
d
t | fd+1t qdt?1qd?1t?1 )
def=
{
if fd+1t 6?FG : Jfdt =0K
if fd+1t ?FG, fdt ? F1 : P??F-ILR,d(fdt | qdt?1 qd?1t?1 )
if fd+1t ?FG, fdt ? FG : P??F-CLR,d(fdt | qdt?1 qd?1t?1 )
(13)
with edge cases q0t and fD+1t defined as appropri-
ate constants. The first case is just store-element
maintenance, in which the variable is not on the
?frontier? and therefore inactive.
Examining ?F-ILR,d and ?F-CLR,d, we see that
the produced fdt variables are also used in the ?if?
statement. These models can be thought of as
picking out a fdt first, finding the matching case,
then applying the probability models that matches.
These models are actually two parts of the same
model when learned from trees.
Probabilities in the shift step are also split into
cases based on the reduce variables. More main-
tenance operations (first case) accompany transi-
tions producing new awaited constituents (second
case below) and expansions producing new active
constituents (third and fourth case):
P?Q(q
d
t | fd+1t fdt qdt?1qd?1t )
def=
?
?
?
?
?
if fd+1t 6?FG : Jqdt = qdt?1K
if fd+1t ?FG, fdt ? F0 : P??Q-ILT,d(qdt | fd+1t qdt?1 qd?1t )
if fd+1t ?FG, fdt ? F1 : P??Q-ILE,d(qdt | fdt qdt?1 qd?1t )
if fd+1t ?FG, fdt ?FG : P??Q-CLE,d(qdt | qd?1t )
(14)
1193
FACTOR DESCRIPTION EXPECTED
Word order in
narrative
For each story, words were indexed. Subjects would tend to read faster later in a story. negative
slope
Reciprocal
length
Log of the reciprocal of the number of letters in each word. A decrease in the reciprocal
(increase in length) might mean longer reading times.
positive
slope
Unigram
frequency
A log-transformed empirical count of word occurrences in the Brown Corpus section of
the Penn Treebank. Higher frequency should indicate shorter reading times.
negative
slope
Bigram
probability
A log-transformed empirical count of two-successive-word occurrences, with Good-
Turing smoothing on words occuring less than 10 times.
negative
slope
Embedding
difference
Amount of change in HHMM weighted-average embedding depth. Hypothesized to in-
crease with larger working memory requirements, which predict longer reading times.
positive
slope
Entropy
reduction
Amount of decrease in the HHMM?s uncertainty about the sentence. Larger reductions
in uncertainty are hypothesized to take longer.
positive
slope
Surprisal ?Surprise value? of a word in the HHMM parser; models were trained on the Wall Street
Journal, sections 02?21. More surprising words may take longer to read.
positive
slope
Table 1: A list of factors hypothesized to contribute to reading times. All data was mean-centered.
A final note: the notation P??(? | ?) has been used
to indicate probability models that are empirical,
trained directly from frequency counts of right-
corner transformed trees in a large corpus. Alter-
natively, a standard PCFG could be trained on a
corpus (or hand-specified), and then the grammar
itself can be right-corner transformed (Schuler,
2009).
Taken together, Equations 11?14 define the
probabilistic structure of the HHMM for parsing
right-corner trees.
2.5 Embedding difference in the HHMM
It should be clear from Figure 1 that at any time
step while parsing depth-bounded right-corner
trees, the candidate hidden state qt will have a
?frontier? depth d(qt). At time t, the beam of
possible hidden states qt stores the syntactic state
(and a backpointer) along with its probability,
P(o1..t q1..t). The average embedding depth at a
time step is then
?EMB(o1..t) =
?
qt?Bt
d(qt) ?
P(o1..t q1..t)
?
q?t?Bt P(o1..t q
?
1..t)
(15)
where we have directly used the beam notation.
The embedding difference metric is:
EmbDiff(o1..t) = ?EMB(o1..t) ? ?EMB(o1..t?1)
(16)
There is a strong computational correspondence
between this definition of embedding difference
and the previous definition of surprisal. To see
this, we rewrite Equations 1 and 3:
Pre(o1..t)=
?
qt?Bt
P(o1..t q1..t) (1?)
Surprisal(t) = log2 Pre(o1..t?1) ? log2 Pre(o1..t)
(3?)
Both surprisal and embedding difference include
summations over the elements of the beam, and
are calculated as a difference between previous
and current beam states.
Most differences between these metrics are rel-
atively inconsequential. For example, the dif-
ference in order of subtraction only assures that
a positive correlation with reading times is ex-
pected. Also, the presence of a logarithm is rel-
atively minor. Embedding difference weighs the
probabilities with center-embedding depths and
then normalizes the values; since the measure is
a weighted average of embedding depths rather
than a probability distribution, ?EMB is not always
less than 1 and the correspondence with Kullback-
Leibler divergence (Levy, 2008) does not hold, so
it does not make sense to take the logs.
Therefore, the inclusion of the embedding
depth, d(qt), is the only significant difference
between the two metrics. The result is a met-
ric that, despite numerical correspondence to sur-
prisal, models the HHMM?s hypotheses about
memory cost.
3 Evaluation
Surprisal, entropy reduction, and embedding dif-
ference from the HHMM parser were evaluated
against a full array of factors (Table 1) on a cor-
pus of word-by-word reading times using a linear
mixed-effects model.
1194
The corpus of reading times for 23 native En-
glish speakers was collected on a set of four nar-
ratives (Bachrach et al, 2009), each composed of
sentences that were syntactically complex but con-
structed to appear relatively natural. Using Linger
2.88, words appeared one-by-one on the screen,
and required a button-press in order to advance;
they were displayed in lines with 11.5 words on
average.
Following Roark et al?s (2009) work on the
same corpus, reading times above 1500 ms (for
diverted attention) or below 150 ms (for button
presses planned before the word appeared) were
discarded. In addition, the first and last word of
each line on the screen were removed; this left
2926 words out of 3540 words in the corpus.
For some tests, a division between open- and
closed-class words was made, with 1450 and 1476
words, respectively. Closed-class words (e.g., de-
terminers or auxiliary verbs) usually play some
kind of syntactic function in a sentence; our evalu-
ations used Roark et al?s list of stop words. Open
class words (e.g., nouns and other verbs) more
commonly include new words. Thus, one may ex-
pect reading times to differ for these two types of
words.
Linear mixed-effect regression analysis was
used on this data; this entails a set of fixed effects
and another of random effects. Reading times y
were modeled as a linear combination of factors
x, listed in Table 1 (fixed effects); some random
variation in the corpus might also be explained by
groupings according to subject i, word j, or sen-
tence k (random effects).
yijk = ?0 +
m
X
?=1
??xijk? + bi + bj + bk + ? (17)
This equation is solved for each of m fixed-
effect coefficients ? with a measure of confidence
(t-value = ??/SE(??), where SE is the standard er-
ror). ?0 is the standard intercept to be estimated
along with the rest of the coefficients, to adjust for
affine relationships between the dependent and in-
dependent variables. We report factors as statisti-
cally significant contributors to reading time if the
absolute value of the t-value is greater than 2.
Two more types of comparisons will be made to
see the significance of factors. First, a model of
data with the full list of factors can be compared
to a model with a subset of those factors. This is
done with a likelihood ratio test, producing (for
mixed-effects models) a ?21 value and correspond-
ing probability that the smaller model could have
produced the same estimates as the larger model.
A lower probability indicates that the additional
factors in the larger model are significant.
Second, models with different fixed effects can
be compared to each other through various infor-
mation criteria; these trade off between having
a more explanatory model vs. a simpler model,
and can be calculated on any model. Here, we
use Akaike?s Information Criterion (AIC), where
lower values indicate better models.
All these statistics were calculated in R, using
the lme4 package (Bates et al, 2008).
4 Results
Using the full list of factors in Table 1, fixed-effect
coefficients were estimated in Table 2. Fitting the
best model by AIC would actually prune away
some of the factors as relatively insignificant, but
these smaller models largely accord with the sig-
nificance values in the table and are therefore not
presented.
The first data column shows the regression on
all data; the second and third columns divide the
data into open and closed classes, because an eval-
uation (not reported in detail here) showed statis-
tically significant interactions between word class
and 3 of the predictors. Additionally, this facil-
itates comparison with Roark et al (2009), who
make the same division.
Out of the non-parser-based metrics, word order
and bigram probability are statistically significant
regardless of the data subset; though reciprocal
length and unigram frequency do not reach signif-
icance here, likelihood ratio tests (not shown) con-
firm that they contribute to the model as a whole.
It can be seen that nearly all the slopes have been
estimated with signs as expected, with the excep-
tion of reciprocal length (which is not statistically
significant).
Most notably, HHMM surprisal is seen here to
be a standout predictive measure for reading times
regardless of word class. If the HHMM parser is
a good psycholinguistic model, we would expect
it to at least produce a viable surprisal metric, and
Table 2 attests that this is indeed the case. Though
it seems to be less predictive of open classes, a
surprisal-only model has the best AIC (-7804) out
of any open-class model. Considering the AIC
on the full data, the worst model with surprisal
1195
FULL DATA OPEN CLASS CLOSED CLASS
Coefficient Std. Err. t-value Coefficient Std. Err. t-value Coefficient Std. Err. t-value
(Intcpt) -9.340?10?3 5.347?10?2 -0.175 -1.237?10?2 5.217?10?2 -0.237 -6.295?10?2 7.930?10?2 -0.794
order -3.746?10?5 7.808?10?6 -4.797? -3.697?10?5 8.002?10?6 -4.621? -3.748?10?5 8.854?10?6 -4.232?
rlength -2.002?10?2 1.635?10?2 -1.225 9.849?10?3 1.779?10?2 0.554 -2.839?10?2 3.283?10?2 -0.865
unigrm -8.090?10?2 3.690?10?1 -0.219 -1.047?10?1 2.681?10?1 -0.391 -3.847?10+0 5.976?10+0 -0.644
bigrm -2.074?10+0 8.132?10?1 -2.551? -2.615?10+0 8.050?10?1 -3.248? -5.052?10+1 1.910?10+1 -2.645?
embdiff 9.390?10?3 3.268?10?3 2.873? 2.432?10?3 4.512?10?3 0.539 1.598?10?2 5.185?10?3 3.082?
etrpyrd 2.753?10?2 6.792?10?3 4.052? 6.634?10?4 1.048?10?2 0.063 4.938?10?2 1.017?10?2 4.857?
srprsl 3.950?10?3 3.452?10?4 11.442? 2.892?10?3 4.601?10?4 6.285? 5.201?10?3 5.601?10?4 9.286?
Table 2: Results of linear mixed-effect modeling. Significance (indicated by ?) is reported at p < 0.05.
(Intr) order rlngth ungrm bigrm emdiff entrpy
order .000
rlength -.006 -.003
unigrm .049 .000 -.479
bigrm .001 .005 -.006 -.073
emdiff .000 .009 -.049 -.089 .095
etrpyrd .000 .003 .016 -.014 .020 -.010
srprsl .000 -.008 -.033 -.079 .107 .362 .171
Table 3: Correlations in the full model.
(AIC=-10589) outperformed the best model with-
out it (AIC=-10478), indicating that the HHMM
surprisal is well worth including in the model re-
gardless of the presence of other significant fac-
tors.
HHMM entropy reduction predicts reading
times on the full dataset and on closed-class
words. However, its effect on open-class words is
insignificant; if we compare the model of column
2 against one without entropy reduction, a likeli-
hood ratio test gives ?21 = 0.0022, p = 0.9623
(the smaller model could easily generate the same
data).
The HHMM?s average embedding difference
is also significant except in the case of open-
class words ? removing embedding difference on
open-class data yields ?21 = 0.2739, p = 0.6007.
But what is remarkable is that there is any signifi-
cance for this metric at all. Embedding difference
and surprisal were relatively correlated compared
to other predictors (see Table 3), which is expected
because embedding difference is calculated like
a weighted version of surprisal. Despite this, it
makes an independent contribution to the full-data
and closed-class models. Thus, we can conclude
that the average embedding depth component af-
fects reading times ? i.e., the HHMM?s notion of
working memory behaves as we would expect hu-
man working memory to behave.
5 Discussion
As with previous work on large-scale parser-
derived complexity metrics, the linear mixed-
effect models suggest that sentence-level factors
are effective predictors for reading difficulty ? in
these evaluations, better than commonly-used lex-
ical and near-neighbor predictors (Pollatsek et al,
2006; Engbert et al, 2005). The fact that HHMM
surprisal outperforms even n-gram metrics points
to the importance of including a notion of sentence
structure. This is particularly true when the sen-
tence structure is defined in a language model that
is psycholinguistically plausible (here, bounded-
memory right-corner form).
This accords with an understated result of
Boston et al?s eye-tracking study (2008a): a
richer language model predicts eye movements
during reading better than an oversimplified one.
The comparison there is between phrase struc-
ture surprisal (based on Hale?s (2001) calculation
from an Earley parser), and dependency grammar
surprisal (based on Nivre?s (2007) dependency
parser). Frank (2009) similarly reports improve-
ments in the reading-time predictiveness of unlexi-
calized surprisal when using a language model that
is more plausible than PCFGs.
The difference in predictivity due to word class
is difficult to explain. One theory may be that
closed-class words are less susceptible to random
effects because there is a finite set of them for
any language, making them overall easier to pre-
dict via parser-derived metrics. Or, we could note
that since closed-class words often serve grammat-
ical functions in addition to their lexical content,
they contribute more information to parser-derived
measures than open-class words. Previous work
with complexity metrics on this corpus (Roark et
al., 2009) suggests that these explanations only ac-
count for part of the word-class variation in the
performance of predictors.
1196
Further comparsion to Roark et al will show
other differences, such as the lesser role of word
length and unigram frequency, lower overall cor-
relations between factors, and the greater predic-
tivity of their entropy metric. In addition, their
metrics are different from ours in that they are de-
signed to tease apart lexical and syntactic contri-
butions to reading difficulty. Their notion of en-
tropy, in particular, estimates Hale?s definition of
entropy on whole derivations (2006) by isolating
the predictive entropy; they then proceed to define
separate lexical and syntactic predictive entropies.
Drawing more directly from Hale, our definition
is a whole-derivation metric based on the condi-
tional entropy of the words, given the root. (The
root constituent, though unwritten in our defini-
tions, is always included in the HHMM start state,
q0.)
More generally, the parser used in these evalu-
ations differs from other reported parsers in that
it is not lexicalized. One might expect for this
to be a weakness, allowing distributions of prob-
abilities at each time step in places not licensed
by the observed words, and therefore giving poor
probability-based complexity metrics. However,
we see that this language model performs well
despite its lack of lexicalization. This indicates
that lexicalization is not a requisite part of syntac-
tic parser performance with respect to predicting
linguistic complexity, corroborating the evidence
of Demberg and Keller?s (2008) ?unlexicalized?
(POS-generating, not word-generating) parser.
Another difference is that previous parsers have
produced useful complexity metrics without main-
taining arc-eager/arc-standard ambiguity. Results
show that including this ambiguity in the HHMM
at least does not invalidate (and may in fact im-
prove) surprisal or entropy reduction as reading-
time predictors.
6 Conclusion
The task at hand was to determine whether the
HHMM could consistently be considered a plau-
sible psycholinguistic model, producing viable
complexity metrics while maintaining other char-
acteristics such as bounded memory usage. The
linear mixed-effects models on reading times val-
idate this claim. The HHMM can straightfor-
wardly produce highly-predictive, standard com-
plexity metrics (surprisal and entropy reduction).
HHMM surprisal performs very well in predicting
reading times regardless of word class. Our for-
mulation of entropy reduction is also significant
except in open-class words.
The new metric, embedding difference, uses the
average center-embedding depth of the HHMM
to model syntactic-processing memory cost. This
metric can only be calculated on parsers with an
explicit representation for short-term memory el-
ements like the right-corner HHMM parser. Re-
sults show that embedding difference does predict
reading times except in open-class words, yielding
a significant contribution independent of surprisal
despite the fact that its definition is similar to that
of surprisal.
Acknowledgments
Thanks to Brian Roark for help on the reading
times corpus, Tim Miller for the formulation of
entropy reduction, Mark Holland for statistical in-
sight, and the anonymous reviewers for their input.
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Asaf Bachrach, Brian Roark, Alex Marantz, Susan
Whitfield-Gabrieli, Carlos Cardenas, and John D.E.
Gabrieli. 2009. Incremental prediction in naturalis-
tic language processing: An fMRI study.
Douglas Bates, Martin Maechler, and Bin Dai. 2008.
lme4: Linear mixed-effects models using S4 classes.
R package version 0.999375-31.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
U. Patil, and Shravan Vasishth. 2008a. Parsing costs
as predictors of reading difficulty: An evaluation us-
ing the Potsdam Sentence Corpus. Journal of Eye
Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
1197
Evan Chen, Edward Gibson, and Florian Wolf. 2005.
Online syntactic storage costs in sentence com-
prehension. Journal of Memory and Language,
52(1):144?169.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
Delphine Dahan and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: A dynamical model
of saccade generation during reading. Psychological
Review, 112:777?813.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. Machine Learning, 32(1):41?62.
Stefan L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proc. Annual Meeting of the
Cognitive Science Society, pages 1139?1144.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30(4):609?642.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Scott A. McDonald and Richard C. Shillcock. 2003.
Low-level predictive inference in reading: The influ-
ence of transitional probabilities on eye movements.
Vision Research, 43(16):1735?1751.
George Miller and Noam Chomsky. 1963. Finitary
models of language users. In R. Luce, R. Bush,
and E. Galanter, editors, Handbook of Mathematical
Psychology, volume 2, pages 419?491. John Wiley.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Joakim Nivre. 2007. Inductive dependency parsing.
Computational Linguistics, 33(2).
Alexander Pollatsek, Erik D. Reichle, and Keith
Rayner. 2006. Tests of the EZ Reader model:
Exploring the interface between cognition and eye-
movement control. Cognitive Psychology, 52(1):1?
56.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. In-
tegration of visual and linguistic information in spo-
ken language comprehension. Science, 268:1632?
1634.
1198
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620?631,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Incremental Syntactic Language Models for Phrase-based Translation
Lane Schwartz
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
lane.schwartz@wpafb.af.mil
Chris Callison-Burch
Johns Hopkins University
Baltimore, MD USA
ccb@cs.jhu.edu
William Schuler
Ohio State University
Columbus, OH USA
schuler@ling.ohio-state.edu
Stephen Wu
Mayo Clinic
Rochester, MN USA
wu.stephen@mayo.edu
Abstract
This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.
1 Introduction
Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al,
This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.
1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.
Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1
As a workaround, parsers can rerank the translated
output of translation systems (Och et al, 2004).
On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al, 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.
The contributions of this work are as follows:
? A novel method for integrating syntactic LMs
into phrase-based translation (?3)
? A formal definition of an incremental parser for
1While not all languages are written left-to-right, we will
refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.
620
statistical MT that can run in linear-time (?4)
? Integration with Moses (?5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (?6)
2 Related Work
Neither phrase-based (Koehn et al, 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al, 2003) but found such restric-
tions failed to improve translation quality.
Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al, 2004; Galley et al, 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al, 2006; Huang
et al, 2006; Shen et al, 2008), tree-to-string (Liu
et al, 2006; Liu et al, 2007; Mi et al, 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeille? et al, 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al, 2006; Nesson et al, 2006; Zhang et al, 2007;
DeNeefe et al, 2007; DeNeefe and Knight, 2009;
Liu et al, 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al, 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al, 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.
Traditional approaches to language models in
speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al, 2005).
Hassan et al (2007) and Birch et al (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.
The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.
3 Parser as Syntactic Language Model in
Phrase-Based Translation
Parsing is the task of selecting the representation ??
(typically a tree) that best models the structure of
621
???????
?s?
??0
???????
?s? the
??11
???????
?s? that
??12
???????
?s? president
??13
. . .
???????
the president
??21
???????
that president
??22
???????
president Friday
??23
. . .
???????
president meets
??31
???????
Obama met
??32
. . .
Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Pra?sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state ??th . Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.
sentence e, out of all such possible representations
? . This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree ?? is taken to be:
?? = argmax
?
P(? | e) (1)
We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.
P(e) =
?
???
P(?, e) (2)
P(e) =
?
???
P(e | ?)P(?) (3)
3.1 Incremental syntactic language model
An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After
processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, ?t. The syntac-
tic language model probability of a partial sentence
e1...et is defined:
P(e1...et) =
?
???t
P(e1...et | ?)P(?) (4)
In practice, a parser may constrain the set of trees
under consideration to ??t, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function ? (Equation
6). The role of ? is explained in ?3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.
P(e1...et) ? P(?? t) =
?
???? t
P(e1...et | ?)P(?) (5)
?(et, ?? t?1)? ?? t (6)
622
3.2 Decoding in phrase-based translation
Given a source language input sentence f , a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation e? using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters ? (Och and Ney, 2002).
e? = argmax
e
exp(
?
j
?jhj(e,f)) (7)
Phrase-based translation constructs a set of trans-
lation options ? hypothesized translations for con-
tiguous portions of the source sentence ? from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.
3.3 Incorporating a Syntactic Language Model
Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function ? can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state ??t.
The hypothesis at the root of the translation lattice
is initialized with ?? 0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which ?? t?1 is stored. Given a new target language
word et and ?? t?1, the incremental parser?s transi-
tion function ? calculates ?? t. Figure 1 illustrates
S
NP
DT
The
NN
president
VP
VP
VB
meets
NP
DT
the
NN
board
PP
IN
on
NP
Friday
Figure 2: Sample binarized phrase structure tree.
S
S/NP
S/PP
S/VP
NP
NP/NN
DT
The
NN
president
VP
VP/NN
VP/NP
VB
meets
DT
the
NN
board
IN
on
NP
Friday
Figure 3: Sample binarized phrase structure tree af-
ter application of right-corner transform.
a sample phrase-based decoding lattice where each
translation lattice node is augmented with syntactic
language model state ??t.
In phrase-based translation, many translation lat-
tice nodes represent multi-word target language
phrases. For such translation lattice nodes, ? will
be called once for each newly hypothesized target
language word in the node. Only the final syntac-
tic language model state in such sequences need be
stored in the translation lattice node.
4 Incremental Bounded-Memory Parsing
with a Time Series Model
Having defined the framework by which any in-
cremental parser may be incorporated into phrase-
based translation, we now formally define a specific
incremental parser for use in our experiments.
The parser must process target language words
incrementally as the phrase-based decoder adds hy-
potheses to the translation lattice. To facilitate this
incremental processing, ordinary phrase-structure
trees can be transformed into right-corner recur-
623
r1t?1
r2t?1
r3t?1
s1t?1
s2t?1
s3t?1
r1t
r2t
r3t
s1t
s2t
s3t
et?1 et
. . .
. . .
. . .
. . .
Figure 4: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.
sive phrase structure trees using the tree transforms
in Schuler et al (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents c?/c?? consisting of an
?active? constituent c? lacking an ?awaited? con-
stituent c?? yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input ?meets the?.
A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in ?4.1 below.
The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents c?/c??.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.
4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses
This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, s?1..D1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T , using
HHMM state transition model ?A and observation
symbol model ?B (Rabiner, 1990):
s?1..D1..T
def
= argmax
s1..D1..T
T?
t=1
P?A(s
1..D
t | s
1..D
t?1 )?P?B(et | s
1..D
t )
(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model ?S) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model ?R):
P?A(s
1..D
t | s
1..D
t?1 )
def
=
?
r1t ..r
D
t
D?
d=1
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
? P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t ) (9)
Store elements are defined to contain only the
active (c?) and awaited (c??) constituent categories
necessary to compute an incomplete constituent
probability:
sdt
def
= ?c?, c??? (10)
Reduction states are defined to contain only the
complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a
flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):
rdt
def
= ?crdt , frdt ? (11)
The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
?S-E,d), transition along a sequence of store elements
624
s11
s21
s31
e1
t=1
r12
r22
r32
s12
s22
s32
e2
t=2
r13
r23
r33
s13
s23
s33
e3
t=3
r14
r24
r34
s14
s24
s34
e4
t=4
r15
r25
r35
s15
s25
s35
e5
t=5
r16
r26
r36
s16
s26
s36
e6
t=6
r17
r27
r37
s17
s27
s37
e7
t=7
r18
r28
r38
=DT
=NP/NN
=NP
=NN
=S/VP
=VB
=S/VP
=VP/NP
=DT
=VP/NN
=S/VP
=NN
=VP
=S/PP
=IN
=S/NP
=S
=NP
=The =president =meets =the =board =on =Friday
Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.
if no reduction has taken place (frdt =0; using depth-
specific store state transition model ?S-T,d): 2
P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
?
?
?
if frd+1t =1, frdt =1 : P?S-E,d(s
d
t | s
d?1
t )
if frd+1t =1, frdt =0 : P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
if frd+1t =0, frdt =0 : Js
d
t = s
d
t?1K
(12)
and possibly reduce a store element (terminate
a sequence) if the store state below it has re-
duced (frd+1t = 1; using depth-specific reduction
model ?R,d):
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if frd+1t =0 : Jr
d
t = r?K
if frd+1t =1 : P?R,d(r
d
t | r
d+1
t s
d
t?1 s
d?1
t?1 )
(13)
where r? is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of s0t and r
D+1
t .
Figure 5 illustrates this model in action.
These pushdown automaton operations are then
refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model ?S-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a
2An indicator function J?K is used to denote deterministic
probabilities: J?K = 1 if ? is true, 0 otherwise.
new incomplete constituent in the same store el-
ement) from awaited transitions (model ?S-T-W,d,
which involve no completion):
P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
{
if rdt 6=r? : P?S-T-A,d(s
d
t | s
d?1
t r
d
t )
if rdt =r? : P?S-T-W,d(s
d
t | s
d
t?1r
d+1
t )
(14)
P?R,d(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if crd+1t 6=xt : Jr
d
t = r?K
if crd+1t =xt : P?R-R,d(r
d
t | s
d
t?1s
d?1
t?1 )
(15)
These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities ?G-R,d and ?G-L,d: 3
3Model probabilities are also defined in terms of left-
progeny probability distribution E?G-RL?,d which is itself defined
in terms of PCFG probabilities:
E?G-RL?,d(c?
0
? c?0 ...)
def
=
?
c?1
P?G-R,d(c? ? c?0 c?1) (16)
E?G-RL?,d(c?
k
? c?0k0 ...)
def
=
?
c
?0k
E?G-RL?,d(c?
k?1
? c?0k ...)
?
?
c
?0k1
P?G-L,d(c?0k ? c?0k0 c?0k1) (17)
E?G-RL?,d(c?
?
? c?? ...)
def
=
??
k=0
E?G-RL?,d(c?
k
? c?? ...) (18)
E?G-RL?,d(c?
+
? c?? ...)
def
= E?G-RL?,d(c?
?
? c?? ...)
? E?G-RL?,d(c?
0
? c?? ...) (19)
625
???????
president meets
??31
. . .
???????
the board
??51
. . .
s13
s23
s33
e3
r14
r24
r34
s14
s24
s34
e4
r15
r25
r35
s15
s25
s35
e5=meets =the =board
Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state ??31 contains random variables
s1..33 ; likewise ??51 contains s
1..3
5 . The intervening random variables r
1..3
4 , s
1..3
4 , and r
1..3
5 are calculated by
transition function ? (Eq. 6, as defined by ?4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.
? for expansions:
P?S-E,d(?c??, c
?
??? | ??, c??)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? Jx?? = c??? = c??K (20)
? for awaited transitions:
P?S-T-W,d(?c?, c??1? | ?c
?
?, c??? c??0)
def
=
Jc? = c??K ?
P?G-R,d(c?? ? c??0 c??1)
E?G-RL?,d(c??
0
? c??0 ...)
(21)
? for active transitions:
P?S-T-A,d(?c??, c??1? | ??, c?? c??0)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? P?G-L,d(c?? ? c??0 c??1)
E?G-RL?,d(c?
+
? c??0 ...)
(22)
? for cross-element reductions:
P?R-R,d(c??,1 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
0
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(23)
? for in-element reductions:
P?R-R,d(c??,0 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
+
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(24)
We use the parser implementation of (Schuler,
2009; Schuler et al, 2010).
5 Phrase Based Translation with an
Incremental Syntactic Language Model
The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-
626
coder?s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state ??th . Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, ??th
contains those random variables s1..Dt that maintain
distributions over syntactic elements.
By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(??th) = P(e
h
1..t, s
1..D
1..t ),
where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).
During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.
When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a
syntactic language model state ??51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state ??31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.
In-domain Out-of-domain
LM WSJ 23 ppl ur-en dev ppl
WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41
Interpolated WSJ
5-gram + HHMM 209.13 225.48
Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10
Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05
Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.
Our syntactic language model is integrated into
the current version of Moses (Koehn et al, 2007).
6 Results
As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.
ppl = b
?logbP(e1...eT )
T (25)
We trained the syntactic language model from
?4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al, 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating
4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al, 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.
627
Sentence Moses +HHMM +HHMM
length beam=50 beam=2000
10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588
Figure 8: Mean per-sentence decoding time (in sec-
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.
HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.
We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.
In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.
7 Discussion
This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
Figure 9: Results for Ur-En devtest (only sentences
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.
priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.
We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.
The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al, 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.
The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.
A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.
628
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.
Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517?
558.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79?85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225?
231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452.
John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507?514.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232?241.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727?736.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541?548.
Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773?781.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
629
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273?
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968.
Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849?857.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80?87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105?112.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288?295.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26?33.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99?105.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675?685.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177?180.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704?711.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653?660.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192?199.
630
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833?
840.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128?137.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161?168.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172?181.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.
Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?279.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-
form. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344?352.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577?585.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189?208.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535?542.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
631
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 148?154, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
MayoClinicNLP?CORE: Semantic representations for textual similarity
Stephen Wu
Mayo Clinic
Rochester, MN 55905
wu.stephen@mayo.edu
Dongqing Zhu & Ben Carterette
University of Delaware
Newark, DE 19716
{zhu,carteret}@cis.udel.edu
Hongfang Liu
Mayo Clinic
Rochester, MN 55905
liu.hongfang@mayo.edu
Abstract
The Semantic Textual Similarity (STS) task
examines semantic similarity at a sentence-
level. We explored three representations of
semantics (implicit or explicit): named enti-
ties, semantic vectors, and structured vectorial
semantics. From a DKPro baseline, we also
performed feature selection and used source-
specific linear regression models to combine
our features. Our systems placed 5th, 6th, and
8th among 90 submitted systems.
1 Introduction
The Semantic Textual Similarity (STS) task (Agirre
et al, 2012; Agirre et al, 2013) examines semantic
similarity at a sentence-level. While much work has
compared the semantics of terms, concepts, or doc-
uments, this space has been relatively unexplored.
The 2013 STS task provided sentence pairs and a
0?5 human rating of their similarity, with training
data from 5 sources and test data from 4 sources.
We sought to explore and evaluate the usefulness
of several semantic representations that have had
recent significance in research or practice. First,
information extraction (IE) methods often implic-
itly consider named entities as ad hoc semantic rep-
resentations, for example, in the clinical domain.
Therefore, we sought to evaluate similarity based on
named entity-based features. Second, in many appli-
cations, an effective means of incorporating distri-
butional semantics is Random Indexing (RI). Thus
we consider three different representations possi-
ble within Random Indexing (Kanerva et al, 2000;
Sahlgren, 2005). Finally, because compositional
distributional semantics is an important research
topic (Mitchell and Lapata, 2008; Erk and Pado?,
2008), we sought to evaluate a principled compo-
sition strategy: structured vectorial semantics (Wu
and Schuler, 2011).
The remainder of this paper proceeds as follows.
Section 2 overviews our similarity metrics, and Sec-
tion 3 overviews the systems that were defined on
these metrics. Competition results and additional
analyses are in Section 4. We end with discussion
on the results in Section 5.
2 Similarity measures
Because we expect semantic similarity to be multi-
layered, we expect that we will need many similar-
ity measures to approximate human similarity judg-
ments. Rather than reinvent the wheel, we have cho-
sen to introduce features that complement existing
successful feature sets. We utilized 17 features from
DKPro Similarity and 21 features from TakeLab,
i.e., the two top-performing systems in the 2012 STS
task, as a solid baseline.
These are summarized in Table 1. We introduce 3
categories of new similarity metrics, 9 metrics in all.
2.1 Named entity measures
Named entity recognition provides a common ap-
proximation of semantic content for the informa-
tion extraction perspective. We define three simple
similarity metrics based on named entities. First,
we computed the named entity overlap (exact string
matches) between the two sentences, where NEk
was the set of named entities found in sentence
Sk. This is the harmonic mean of how closely S1
148
Table 1: Full feature pool in MayoClinicNLP systems. The proposed MayoClinicNLP metrics are meant to comple-
ment DKPro (Ba?r et al, 2012) and TakeLab (?Saric? et al, 2012) metrics.
DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)
n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlap
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlap
n-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlap
n-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlap
n-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlap
n-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlap
n-grams/WordNGramJaccardMeasure 4 stopword-filtered
t words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txt
t words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txt
t words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txt
esa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txt
esa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txt
t vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6b.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6d.txt
custom/SemanticVectorsSimilarityMeasure d200 wr6p.txt
n-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifference
n-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifference
n-grams/CharacterNGramMeasure 4 t other/NumbersSize
string/GreedyStringTiling 3 t other/NumbersOverlap
string/LongestCommonSubsequenceComparator t other/NumbersSubset
string/LongestCommonSubsequenceNormComparator t other/SentenceSize
string/LongestCommonSubstringComparator t other/CaseMatches
t other/StocksSize
t other/StocksOverlap
matches S2, and how closely S2 matches S1:
simneo(S1, S2) = 2 ? ?NE1 ?NE2??NE1? + ?NE2? (1)
Additionally, we relax the constraint of requiring
exact string matches between the two sentences by
using the longest common subsequence (Allison and
Dix, 1986) and greedy string tiling (Wise, 1996) al-
gorithms. These metrics give similarities between
two strings, rather than two sets of strings as we
have with NE1 and NE2. Thus, we follow previ-
ous work in greedily aligning these named entities
(Lavie and Denkowski, 2009; ?Saric? et al, 2012) into
pairs. Namely, we compare each pair (nei,1, nej,2)
of named entity strings in NE1 and NE2. The
highest-scoring pair is entered into a set of pairs, P .
Then, the next highest pair is added to P if neither
named entity is already in P , and discarded other-
wise; this continues until there are no more named
entities in either NE1 or NE2.
We then define two named entity aligning mea-
sures that use the longest common subsequence
(LCS) and greedy string tiling (GST) fuzzy string
matching algorithms:
simnea(S1, S2) =
?
(ne1,ne2)?P
f(ne1, ne2)
max (?NE1?, ?NE2?)
(2)
where f(?) is either the LCS or GST algorithm.
In our experiments, we performed named entity
recognition with the Stanford NER tool using the
standard English model (Finkel et al, 2005). Also,
we used UKP?s existing implementation of LCS and
GST (?Saric? et al, 2012) for the latter two measures.
2.2 Random indexing measures
Random indexing (Kanerva et al, 2000; Sahlgren,
2005) is another distributional semantics framework
for representing terms as vectors. Similar to LSA
(Deerwester et al, 1990), an index is created that
represents each term as a semantic vector. But
in random indexing, each term is represented by
an elemental vector et with a small number of
randomly-generated non-zero components. The in-
tuition for this means of dimensionality reduction is
that these randomly-generated elemental vectors are
like quasi-orthogonal bases in a traditional geomet-
ric semantic space, rather than, e.g., 300 fully or-
thogonal dimensions from singular value decompo-
sition (Landauer and Dumais, 1997). For a standard
model with random indexing, a contextual term vec-
tor ct,std is the the sum of the elemental vectors cor-
responding to tokens in the document. All contexts
for a particular term are summed and normalized to
produce a final term vector vt,std.
Other notions of context can be incorporated into
149
this model. Local co-occurrence context can be ac-
counted for in a basic sliding-window model by con-
sidering words within some window radius r (in-
stead of a whole document). Each instance of the
term t will have a contextual vector ct,win = et?r +
? + et?1 + et+1 +? + et+r; context vectors for each
instance (in a large corpus) would again be added
and normalized to create the overall vector vt,win.
A directional model doubles the dimensionality of
the vector and considers left- and right-context sepa-
rately (half the indices for left-context, half for right-
context), using a permutation to achieve one of the
two contexts. A permutated positional model uses a
position-specific permutation function to encode the
relative word positions (rather than just left- or right-
context) separately. Again, vt would be summed
and normalized over all instances of ct.
Sentence vectors from any of these 4 Random
Indexing-based models (standard, windowed, direc-
tional, positional) are just the sum of the vectors for
each term vS = ?t?S vt. We define 4 separate simi-
larity metrics for STS as:
simRI(S1, S2) = cos(vS1,vS2) (3)
We used the semantic vectors package (Widdows
and Ferraro, 2008; Widdows and Cohen, 2010) in
the default configuration for the standard model. For
the windowed, directional, and positional models,
we used a 6-word window radius with 200 dimen-
sions and a seed length of 5. All models were
trained on the raw text of the Penn Treebank Wall
Street Journal corpus and a 100,075-article subset of
Wikipedia.
2.3 Semantic vectorial semantics measures
Structured vectorial semantics (SVS) composes dis-
tributional semantic representations in syntactic
context (Wu and Schuler, 2011). Similarity met-
rics defined with SVS inherently explore the quali-
ties of a fully interactive syntax?semantics interface.
While previous work evaluated the syntactic contri-
butions of this model, the STS task allows us to eval-
uate the phrase-level semantic validity of the model.
We summarize SVS here as bottom-up vector com-
position and parsing, then continue on to define the
associated similarity metrics.
Each token in a sentence is modeled generatively
as a vector e? of latent referents i? in syntactic con-
text c? ; each element in the vector is defined as:
e?[i?] = P(x? ? lci?), for preterm ? (4)
where l? is a constant for preterminals.
We write SVS vector composition between two
word (or phrase) vectors in linear algebra form,1 as-
suming that we are composing the semantics of two
children e? and e? in a binary syntactic tree into
their parent e? :
e? = M? (L??? ? e?)? (L??? ? e?) ? 1 (5)
M is a diagonal matrix that encapsulates probabilis-
tic syntactic information; the L matrices are linear
transformations that capture how semantically rele-
vant child vectors are to the resulting vector (e.g.,
L??? defines the the relevance of e? to e?). These
matrices are defined such that the resulting e? is a
semantic vector of consistent P(x? ? lci?) probabil-
ities. Further detail is in our previous work (Wu,
2010; Wu and Schuler, 2011).
Similarity metrics can be defined in the SVS
space by comparing the distributions of the com-
posed e? vectors ? i.e., our similarity metric is
a comparison of the vector semantics at different
phrasal nodes. We define two measures, one cor-
responding to the top node c? (e.g., with a syntactic
constituent c? = ?S?), and one corresponding to the
left and right largest child nodes (e.g.,, c? = ?NP?
and c ?= ?VP? for a canonical subject?verb?object
sentence in English).
simsvs-top(S1, S2) = cos(e?(S1),e?(S2)) (6)
simsvs-phr(S1, S2) =max(
avgsim(e?(S1),e?(S2);e ?(S1),e ?(S2)),
avgsim(e?(S1),e ?(S2);e ?(S1),e?(S2))) (7)
where avgsim() is the harmonic mean of the co-
sine similarities between the two pairs of arguments.
Top-level similarity comparisons in (6) amounts to
comparing the semantics of a whole sentence. The
phrasal similarity function simsvs-phr(S1, S2) in (7)
thus seeks to semantically align the two largest sub-
trees, and weight them. Compared to simsvs-top,
1We define the operator ? as point-by-point multiplication
of two diagonal matrices and 1 as a column vector of ones, col-
lapsing a diagonal matrix onto a column vector.
150
the phrasal similarity function simsvs-phr(S1, S2) as-
sumes there might be some information captured in
the child nodes that could be lost in the final compo-
sition to the top node.
In our experiments, we used the parser described
in Wu and Schuler (2011) with 1,000 headwords
and 10 relational clusters, trained on the Wall Street
Journal treebank.
3 Feature combination framework
The similarity metrics of Section 2 were calculated
for each of the sentence pairs in the training set, and
later the test set. In combining these metrics, we ex-
tended a DKPro Similarity baseline (3.1) with fea-
ture selection (3.2) and source-specific models and
classification (3.3).
3.1 Linear regression via DKPro Similarity
For our baseline (MayoClinicNLPr1wtCDT), we
used the UIMA-based DKPro Similarity system
from STS 2012 (Ba?r et al, 2012). Aside from the
large number of sound similarity measures, this pro-
vided linear regression through the WEKA package
(Hall et al, 2009) to combine all of the disparate
similarity metrics into a single one, and some pre-
processing. Regression weights were determined on
the whole training set for each source.
3.2 Feature selection
Not every feature was included in the final linear re-
gression models. To determine the best of the 47
(DKPro?17, TakeLab?21, MayoClinicNLP?9) fea-
tures, we performed a full forward-search on the
space of similarity measures. In forward-search, we
perform 10-fold cross-validation on the training set
for each measure, and pick the best one; in the next
round, that best metric is retained, and the remaining
metrics are considered for addition. Rounds con-
tinue until all the features are exhausted, though a
stopping-point is noted when performance no longer
increases.
3.3 Subdomain source models and
classification
There were 5 sources of data in the training set:
paraphrase sentence pairs (MSRpar), sentence pairs
from video descriptions (MSRvid), MT evaluation
sentence pairs (MTnews and MTeuroparl) and gloss
pairs (OnWN). In our submitted runs, we trained
a separate, feature-selected model based on cross-
validation for each of these data sources. In train-
ing data on cross-validation tests, training domain-
specific models outperformed training a single con-
glomerate model.
In the test data, there were 4 sources, with 2
appearing in training data (OnWN, SMT) and 2
that were novel (FrameNet/Wordnet sense defini-
tions (FNWN), European news headlines (head-
lines)). We examined two different strategies for ap-
plying the 5-source trained models on these 4 test
sets. Both of these strategies rely on a multiclass
random forest classifier, which we trained on the 47
similarity metrics.
First, for each sentence pair, we considered the
final similarity score to be a weighted combination
of the similarity score from each of the 5 source-
specific similarity models. The combination weights
were determined by utilizing the classifier?s confi-
dence scores. Second, the final similarity was cho-
sen as the single source-specific similarity score cor-
responding to the classifier?s output class.
4 Evaluation
The MayoClinicNLP team submitted three systems
to the STS-Core task. We also include here a post-
hoc run that was considered as a possible submis-
sion.
r1wtCDT This run used the 47 metrics from
DKPro, TakeLab, and MayoClinicNLP as a
feature pool for feature selection. Source-
specific similarity metrics were combined with
classifier-confidence-score weights.
r2CDT Same feature pool as run 1. Best-match (as
determined by classifier) source-specific simi-
larity metric was used rather than a weighted
combination.
r3wtCD TakeLab features were removed from the
feature pool (before feature selection). Same
source combination as run 1.
r4ALL Post-hoc run using all 47 metrics, but train-
ing a single linear regression model rather than
source-specific models.
151
Table 2: Performance comparison.
TEAM NAME headlines rank OnWNrank FNWNrank SMT rank mean rank
UMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1
UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2
deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3
MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707
UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4
MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5
MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6
CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7
MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8
NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9
CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 10
4.1 Competition performance
Table 2 shows the top 10 runs of 90 submitted in
the STS-Core task are shown, with our three sys-
tems placing 5th, 6th, and 8th. Additionally, we can
see that run 4 would have placed 4th. Notice that
there are significant source-specific differences be-
tween the runs. For example, while run 4 is better
overall, runs 1?3 outperform it on all but the head-
lines and FNWN datasets, i.e., the test datasets that
were not present in the training data. Thus, it is
clear that the source-specific models are beneficial
when the training data is in-domain, but a combined
model is more beneficial when no such training data
is available.
4.2 Feature selection analysis
0 10 20 30 40
0.
60
0.
65
0.
70
0.
75
0.
80
0.
85
0.
90
Step
Pe
a
rs
o
n
?s
 C
or
re
la
tio
n 
Co
ef
fic
ie
nt
MSRpar
MSRvid
SMTeuroparl
OnWN
SMTnews
ALL
Figure 1: Performance curve of feature selection for
r1wtCDT, r2CDT, and r4ALL
Due to the source-specific variability among the
runs, it is important to know whether the forward-
search feature selection performed as expected. For
source specific models (runs 1 and 3) and a com-
bined model (run 4), Figure 1 shows the 10-fold
cross-validation scores on the training set as the next
feature is added to the model. As we would ex-
pect, there is an initial growth region where the first
features truly complement one another and improve
performance significantly. A plateau is reached for
each of the models, and some (e.g., SMTnews) even
decay if too many noisy features are added.
The feature selection curves are as expected. Be-
cause the plateau regions are large, feature selection
could be cut off at about 10 features, with gains in
efficiency and perhaps little effect on accuracy.
The resulting selected features for some of the
trained models are shown in Table 3.
4.3 Contribution of MayoClinicNLP metrics
We determined whether including MayoClinicNLP
features was any benefit over a feature-selected
DKPro baseline. Table 4 analyzes this question
by adding each of our measures in turn to a base-
line feature-selected DKPro (dkselected). Note that
this baseline was extremely effective; it would have
ranked 4th in the STS competition, outperforming
our run 4. Thus, metrics that improve this baseline
must truly be complementary metrics. Here, we see
that only the phrasal SVSmeasure is able to improve
performance overall, largely by its contributions to
the most difficult categories, FNWN and SMT. In
fact, that system (dkselected + SVSePhrSimilari-
tyMeasure) represents the best-performing run of
any that was produced in our framework.
152
Table 3: Top retained features for several linear regression models.
OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)
t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYT
t other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2
t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparator
esa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlap
t ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlap
n-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1
t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3
t ngram/BigramOverlap t other/SentenceSize t other/SentenceSize
string/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifference
string/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlap
custom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4
custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSize
custom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubset
custom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasure
custom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6p
esa/ESA WordNet
OnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionary
esa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparator
string/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparator
string/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filtered
string/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNet
string/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlap
word-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filtered
n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filtered
custom/SVSePhrSimilarityMeasure t ngram/UnigramOverlap
esa/ESA Wiktionary t ngram/BigramOverlap
esa/ESA WordNet t other/StocksSize
n-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlap
n-grams/WordNGramJaccardMeasure 1 t other/StocksOverlap
string/LongestCommonSubstringComparator
custom/RandomIndexingMeasure d200 wr6d
custom/RandomIndexingMeasure d200 wr0
Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all
sources.
headlines OnWN FNWN SMT mean
dkselected 0.70331 0.79752 0.38358 0.31744 0.571319
dkselected + SVSePhrSimilarityMeasure 0.70178 0.79644 0.38685 0.32332 0.572774
dkselected + RandomIndexingMeasure d200 wr0 0.70054 0.79752 0.38432 0.31615 0.570028
dkselected + SVSeTopSimilarityMeasure 0.69873 0.79522 0.38815 0.31723 0.569533
dkselected + RandomIndexingMeasure d200 wr6d 0.69944 0.79836 0.38416 0.31397 0.569131
dkselected + RandomIndexingMeasure d200 wr6b 0.69992 0.79788 0.38435 0.31328 0.568957
dkselected + RandomIndexingMeasure d200 wr6p 0.69878 0.79848 0.37876 0.31436 0.568617
dkselected + StanfordNerMeasure aligngst 0.69446 0.79502 0.38703 0.31497 0.567212
dkselected + StanfordNerMeasure overlap 0.69468 0.79509 0.38703 0.31466 0.567200
dkselected + StanfordNerMeasure alignlcs 0.69451 0.79486 0.38657 0.31394 0.566807
(dk + all custom) selected 0.70311 0.79887 0.37477 0.31665 0.570586
Also, we see some source-specific behavior. None
of our introduced measures are able to improve the
headlines similarities. However, random indexing
improves OnWN scores, several strategies improve
the FNWN metric, and simsvs-phr is the only viable
performance improvement on the SMT corpus.
5 Discussion
Mayo Clinic?s submissions to Semantic Textual
Similarity 2013 performed well, placing 5th, 6th,
and 8th among 90 submitted systems. We intro-
duced similarity metrics that used different means
to do compositional distributional semantics along
with some named entity-based measures, finding
some improvement especially for phrasal similar-
ity from structured vectorial semantics. Through-
out, we utilized forward-search feature selection,
which enhanced the performance of the models. We
also used source-based linear regression models and
considered unseen sources as mixtures of existing
sources; we found that in-domain data is neces-
sary for smaller, source-based models to outperform
larger, conglomerate models.
Acknowledgments
Thanks to the developers of the UKP DKPro sys-
tem and the TakeLab system for making their code
available. Also, thanks to James Masanz for initial
implementations of some similarity measures.
153
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics-Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 435?
440. Association for Computational Linguistics.
Scott Deerwester, Susan Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent se-
mantic analysis. In Proceedings of the 22nd annual
conference of the cognitive science society, volume
1036. Citeseer.
T.K. Landauer and S.T. Dumais. 1997. A Solution to
Plato?s Problem: The Latent Semantic Analysis The-
ory of Acquisition, Induction, and Representation of
Knowledge. Psychological Review, 104:211?240.
Alon Lavie and Michael J Denkowski. 2009. The meteor
metric for automatic evaluation of machine translation.
Machine translation, 23(2-3):105?115.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, OH.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering, TKE, vol-
ume 5.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Trevor Cohen. 2010. The seman-
tic vectors package: New algorithms and public tools
for distributional semantics. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, pages 9?15. IEEE.
D. Widdows and K. Ferraro. 2008. Semantic vec-
tors: a scalable open source package and online tech-
nology management application. Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), pages 1183?1190.
Michael J Wise. 1996. Yap3: Improved detection of sim-
ilarities in computer program and other texts. In ACM
SIGCSE Bulletin, volume 28, pages 130?134. ACM.
StephenWu andWilliam Schuler. 2011. Structured com-
position of semantic vectors. In Proceedings of the In-
ternational Conference on Computational Semantics.
Stephen Tze-Inn Wu. 2010. Vectorial Representations
of Meaning for a Computational Model of Language
Comprehension. Ph.D. thesis, Department of Com-
puter Science and Engineering, University of Min-
nesota.
154
Structured Composition of Semantic Vectors
Stephen Wu
Mayo Clinic
wu.stephen@mayo.edu
William Schuler
The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Distributed models of semantics assume that word meanings can be discovered from ?the com-
pany they keep.? Many such approaches learn semantics from large corpora, with each document
considered to be unstructured bags of words, ignoring syntax and compositionality within a docu-
ment. In contrast, this paper proposes a structured vectorial semantic framework, in which semantic
vectors are defined and composed in syntactic context. As such, syntax and semantics are fully
interactive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.
Evaluations show that using relationally-clustered headwords as a semantic space in this framework
improves on a syntax-only model in perplexity and parsing accuracy.
1 Introduction
Distributed semantic representations like Latent Semantic Analysis (Deerwester et al, 1990), probabilis-
tic LSA (Hofmann, 2001), Latent Dirichlet Allocation (Blei et al, 2003), or relational clustering (Taskar
et al, 2001) have garnered widespread interest because of their ability to quantitatively capture ?gist?
semantic content.
Two modeling assumptions underlie most of these models. First, the typical assumption is that words
in the same document are an unstructured bag of words. This means that word order and syntactic struc-
ture are ignored in the resulting vectorial representations of meaning, and the only relevant relationship
between words is the ?same-document? relationship. Second, these semantic models are not composi-
tional in and of themselves. They require some external process to aggregate the meaning representations
of words to form phrasal or sentential meaning; at best, they can jointly represent whole strings of words
without the internal relationships.
This paper introduces structured vectorial semantics (SVS) as a principled response to these weak-
nesses of vector space models. In this framework, the syntax?semantics interface is fully interactive:
semantic vectors exist in syntactic context, and any composition of semantic vectors necessarily pro-
duces a hypothetical syntactic parse. Since semantic information is used in syntactic disambiguation
(MacDonald et al, 1994), we would expect practical improvements in parsing accuracy by accounting
for the interactive interpretation process.
Others have incorporated syntactic information with vector-space semantics, challenging the bag-
of-words assumption. Syntax and semantics may be jointly generated with Bayesian methods (Griffiths
et al, 2005); syntactic structure may be coupled to the basis elements of a semantic space (Pado? and
Lapata, 2007); clustered semantics may be used as a pre-processing step (Koo et al, 2008); or, semantics
may be learned in some defined syntactic context (Lin, 1998). These techniques are interactive, but
their semantic models are not syntactically compositional (Frege, 1892). SVS is a generative model of
sentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but is
inherently compositional.
Mitchell and Lapata (2008) provide a general framework for semantic vector composition:
p = f(u,v,R,K) (1)
295
where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base,
and p is a resulting composed vector (or tensor). In this initial work of theirs, they leave out any notion
of syntactic context, focusing on additive and multiplicative vector composition (with some variations):
Add: p[i] = u[i] + v[i] Mult: p[i] = u[i] ? v[i] (2)
Since the structured vectorial semantics proposed here may be viewed within this framework, our dis-
cussion will begin from their definition in Section 2.1.
Erk and Pado??s (2008) model also fits inside Mitchell and Lapata?s framework, and like SVS, it in-
cludes syntactic context. Their semantic vectors use syntactic information as relations between multiple
vectors in arriving at a final meaning representation. The emphasis, however, is on selectional prefer-
ences of individual words; resulting representations are similar to word-sense disambiguation output,
and do not construct phrase-level meaning from word meaning. Mitchell and Lapata?s more recent work
(2009) combines syntactic parses with distributional semantics; but the underlying compositional model
requires (as other existing models would) an interpolation of the vector composition results with a sepa-
rate parser. It is thus not fully interactive.
Though the proposed structured vectorial semantics may be defined within Equation 1, the end output
necessarily includes not only a semantic vector, but a full parse hypothesis. This slightly shifts the focus
from the semantically-centered Equation 1 to an accounting of meaning that is necessarily interactive
(between syntax and semantics); vector composition and parsing are then twin lenses by which the pro-
cess may be viewed. Thus, unlike previous models, a unique phrasal vectorial semantic representation
is composed during decoding.
Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we have
chosen to report results on the well-understood dual problem of parsing. The structured vectorial seman-
tic framework subsumes variants of several common parsing algorithms, two of which will be discussed:
lexicalized parsing (Charniak, 1996; Collins, 1997, etc.) and relational clustering (akin to latent annota-
tions (Matsuzaki et al, 2005; Petrov et al, 2006; Gesmundo et al, 2009)). Because previous work has
shown that linguistically-motivated syntactic state-splitting already improves parses (Klein andManning,
2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitive
and intransitive verbs). This pessimistically isolates the contribution of semantics on parsing accuracy ?
it will only show parsing gains where semantic information does not overlap with distributional syntactic
information. Evaluations show that interactively considering semantic information with syntax has the
predicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity.
The remainder of this paper is organized as follows: Section 2 describes SVS as both vector compo-
sition and parsing; Section 3 shows how relational-clustering SVS subsumes PCFG-LAs; and Section 4
evaluates modeling assumptions and empirical performance.
2 Structured Vectorial Semantics
2.1 Vector Composition
We begin with some notation. This paper will use boldfaced uppercase letters to indicate matrices
(e.g., L), boldfaced lowercase letters to indicate vectors (e.g., e), and no boldface to indicate any single-
valued variable (e.g. i). Indices of vectors and matrices will be associated with semantic concepts
(e.g., i1, i2, . . .); variables over those indices are single-value (scalar) variables (e.g., i); the contents
of vectors and matrices can be accessed by index (e.g., e[i1] for a constant, e[i] for a variable). We will
also define an operation d(?), which lists the elements of a column vector on the diagonal of a diagonal
matrix, i.e., d(e)[i, i]=e[i]. Often, these variables will technically be functions with arguments written
in parentheses, producing vectors or matrices (e.g., L(l) produces a matrix based on the value of l).
As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate
our vectors for now. We can rewrite their equation (Equation 1) in SVS notation by following several
conventions. All semantic vectors have a fixed dimensionality and are denoted e; source vectors and the
296
a) e
(lMOD )S
e0
(lMOD )NP
e00
(lMOD )DT
the
e01
(lID )NN
engineers
pulled off ...
b)
e00 =
he
a
dw
o
rd
s
i u
i t
i p
truth???????
0
1
.1
???????
e01 =
he
a
dw
o
rd
s
i u
i t
i p
truth???????
0
0
1
???????
aT0 = [ .1 .1 .8 ]
L0?00(lMOD) =
pa
re
n
t0
i u
i t
i p
child 00
ip it iu
???????
0 .2 .8
0 0 1
.1 .4 .5
???????
M(lMOD?NP ? lMOD?DT lID?NN) =
pa
re
n
t0
i u
i t
i p
parent 0
ip it iu
???????
.2 0 0
0 .1 0
0 0 .4
???????
Figure 1: a) Syntax and semantics on a tree during decoding. Semantic vectors e are subscripted with the
node?s address. Relations l and syntactic categories c are constants for the example. b) Example vectors
and matrices needed for the composition of a vector at address 0 (Section 2.2.1).
target vector are differentiated by subscript; instead of context variables R and K we will use M and L:
e? = f(e?,e?,M,L) (3)
Syntactic context is in the form of grammar rules M that are aware of semantic concepts; semantic
knowledge is in the form of labeled dependency relationships between semantic concepts, L. Both of
these are present and explicitly modeled as matrices in SVS?s canonical form of vector composition:
e? = M ? d(L??? ? e?) ? d(L??? ? e?) ? 1 (4)
Here, M is a diagonal matrix that encapsulates probabilistic syntactic information, where the syntactic
probabilities depend on the semantic concept being considered. TheLmatrices are linear transformations
that capture how semantically relevant source vectors are to the resulting vector (e.g., L??? defines the
the relevance of e? to e?), with the intuition that two 1D vectors are under consideration and require
a 2D matrix to relate them. 1 is a vector of ones ? this takes a diagonal matrix and returns a column
vector corresponding to the diagonal elements.
Of note in this definition of f(?) is the presence of matrices that operate on distributed semantic
vectors. While it is widely understood that matrices can represent transformations, relatively few have
used matrices to represent the distributed, dynamic nature of meaning composition (see Rudolph and
Giesbrecht (2010) for a counterexample).
2.2 Syntax?Semantics Interface
This section aims to more thoroughly define the way in which the syntax and semantics interact during
structured vectorial semantic composition. SVS will specify this interface such that the composition of
semantic vectors is probabilistically consistent and subsumes parsing under various frameworks. Parsing
has at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins,
1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al,
2005) are clustered concepts that touch on both syntactic and semantic information at a node. Of course,
other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. In this light, semantic
concepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations on
grammar trees.
Let us introduce notation to make the connection with parsing and syntax explicit. This paper will
denote syntactic categories as c and string yields as x. The location of these variables in phrase structure
will be identified using subscripts that describe the path from the root to the constituent.1 Paths consist
of left and/or right branches (indicated by ?0?s and ?1?s, respectively, as in Figure 1a). Variables ?, ?,
and ? stand for whole paths; ? is the path of a composed vector; and  is the empty path at the root. The
yield x? is the observed (sub)string that eventually results from the progeny of c? . Multiple trees ?? can
be constructed at ? by stringing together grammar rules that are consistent with observed text.
1For simplicity, trees are assumed to be compiled into strictly binary-branching form.
297
2.2.1 Lexicalized Parsing
To illustrate the definitions and operations presented in this section, we start with the concrete ?semantic?
space of headwords (i.e., bilexical parsing) before moving on to a formal definition. Our example here
corresponds to the best parse of the first two words in Figure 1a. In this example domain, assume that
the semantic space of concept headwords is {ipulled, ithe, iunk}, abbreviated as {ip, it, iu} where the last
concept is a constant for infrequently-observed words. This semantic space becomes the indices of
semantic vectors; complete vectors e at each node of Figure 1a are shown in Figure 1b.
The tree in Figure 1a contains complete concept vectors e at each node, with corresponding indices
i. Values in these vectors (see Figure 1b) are probabilities, indicating the likelihood that a particular
concept summarizes the meaning below a node. For example, consider e00: it produces the yield below
address 00 (?the?) with probability 1, and iu may also produce ?the? with probability 0.1.
Not shown on the tree are the matrices in Figure 1b. In the parametrized matrix
M(lMOD?NP ? lMOD?DT lID?NN), each diagonal element corresponds to the hypothesized grammar rule?s
probability, given a headword. Similarly, the matrix L0?00(lMOD) is parametrized by the semantic context
lMOD ? here, lMOD represents a generalized ?modifier? semantic role. For the semantic concept ip at ad-
dress 0, the left-child modifier (address 00) could be semantic concept it with probability 0.2, or concept
iu with probability 0.8. Finally, by adding an identity matrix for L0?01(lID) (a ?head? semantic role) to
the quantities in Figure 1b, we would have all the components to construct the vector at address 0:
e0 =
???????
.2 0 0
0 .1 0
0 0 .4
????????????????????????????????????????????????
M
?d??
???????
0 .2 .8
0 0 1
.1 .4 .5
????????????????????????????????????????????????
L0?01
???????
0
1
.1
???????dcurly
e00
?
? ? d
?
?
???????
1 0 0
0 1 0
0 0 1
?????????????????????????????????
L0?01
???????
0
0
1
???????dcurly
e01
?
? ?
???????
1
1
1
???????dcurly
1
=
i u
i t
i p
truth???????
0
0
0.036
???????????????????????????????
e0
Since the vector was constructed in syntactic and semantic context, the tree structure shown (including
semantic relationships l) is implied by the context.
2.2.2 Probabilities in vectors and matrices
Formally defining the probabilities in Figure 1, SVS populates vectors and matrices by means of 5
probability models (models are denoted by ?), along with the process of composition:
Syntactic model M(lc?? lc? lc?)[i? , i?] =P?M(lci? ? lc? lc?)
Semantic model L???(l?)[i? , i?] =P?L(i? ? i? , l?)
Preterminal model e?[i?] =P?P-Vit(G)(x? ? lci?), for preterm ? (5)
Root const. model aT [i] =PpiG(lci)
Any const. model aT?[i?] =PpiG(lci?)
These probabilities are encapsulated into vectors and matrices using a convention: column indices of
vectors or matrices represent conditioned semantic variables, row indices represent modeled variables.
As an example, from Figure 1b, elements of L0?00(lMOD) represent the probability P?L(i00 ? i0, l00).
Thus, the conditioned variable i00 is shown in the figure as column indices, and the modeled i0 as
row indices. This convention applies to the M matrix as well. Recall that M is a diagonal matrix
? its rows and columns model the same variable. Thus, we could rewrite P?M(lci? ? lc? lc?) as
P?M(lci? ? lc? lc?, i?) to make a consistent probabilistic interpretation.
We have intentionally left out the probabilistic definition of normal (non-preterminal) nonterminals
P?Vit(G) , and the rationale for aT vectors. These are both best understood in the dual problem of parsing.
2.2.3 Vector Composition for Parsing
The vector composition of Equation 4 can be rewritten with all arguments and syntactic information as:
e? = M(lc? ? lc? lc?) ? d(L???(l?) ? e?) ? d(L???(l?) ? e?) ? 1 (4?)
298
a compact representation that masks the underlying consistent probability operations. This section will
expand the vector composition equation to show its equivalence to standard statistical parsing methods.
Let us say that e?[i?] = P(x? ? lci?), the probability of giving a particular yield given the present
distributed semantics. Recall that in matrix multiplication, there is a summation over the inner dimen-
sions of the multiplied objects; replacing matrices and vectors with their probabilistic interpretations and
summing in the appropriate places, each element of e? is then:
e?[i?] = P?M(lci? ? lc? lc?) ? ?
i?
P?L(i? ? i? , l?) ? P?Vit(G)(x? ? lci?)
? ?
i?
P?L(i? ? i? , l?) ? P?Vit(G)(x? ? lci?) (6)
This can be loosely considered the multiplication of the syntax (?M term), left-child semantics (first
sum), and right-child semantics (second sum). The only summations are between L and e, since all other
multiplications are between diagonal matrices (similar to pointwise multiplication).
We can simplify this probability expression by grouping ?M and ?L into a grammar rule
P?G(lci?? lci? lci?)def= P?M(lci?? lc? lc?) ? P?L(i? ? i? , l?) ? P?L(i? ? i? , l?), since they deal with every-
thing except the yield of the two child nodes. The summations are then pushed to the front:
e?[i?] = ?
i?,i?
P?G(lci? ? lci? lci?)?P?Vit(G)(x? ? lci?) ? P?Vit(G)(x? ? lci?) (7)
Thus, we have a standard chart-parsing probability P(x? ? lci?) ? with distributed semantic concepts ?
in each vector element.
The use of grammar rules necessarily builds a hypothetical subtree ?? . In a typical CKY algorithm,
the tree corresponding to the highest probability would be chosen; however, we have not defined how to
make this choice for vectorial semantics.
We will choose the best tree with probability 1.0, so we define a deterministic Viterbi probability
over candidate vectors (not concepts) and context variables:
P?Vit(G)(x? ? lce?)def= J e? = argmaxlce? (aT? e? ? PpiG(lcaT? ) ? P?Vit(G)(x ? lce?))K (8)
where J?K is an indicator function such that J?K=1 if ? is true, 0 otherwise. Intuitively, the process is as
follows: we construct the vector e? at a node, according to Eqn. 4?; we then weight this vector against
prior knowledge about the context aT? ; the best vector in context will be chosen (the argmax). Also, the
vector at a node comes with assumptions of what structure produced it. Thus, the last two terms in the
parentheses are deterministic models ensuring that the best subtree ?? is indeed the one generated.
Determining the root constituent of the Viterbi tree is the same process as choosing any other Viterbi
constituent, except that prior contextual knowledge gets its own probability model in aT . As before,
the most likely tree ?? is the tree that maximizes the probability at the root, and can be constructed
recursively from the best child trees. Importantly, ?? has an associated, sentential semantic vector which
may be construed as the composed semantic information for the whole parsed sentence. Similar phrasal
semantic vectors can be obtained anywhere on the parse chart.
These equations complete the linear algebraic definition of structured vectorial semantics.
3 SVS with Relational Clusters
3.1 Inducing Relational Clusters
Unlike many vector space models that are based on the frequencies of terms in documents, we may
consider frequencies of terms that occur in similar semantic relations (e.g., head lID or modifier lMOD).
Reducing the dimensionality of terms in a term?context matrix will result in relationally-clustered con-
cepts. From a parsing perspective, this amounts to latent annotations (Matsuzaki et al, 2005) in l-context.
299
Let us re-notate the headword-lexicalized version of SVS (the example in Section 2.2.1) using h
for headword semantics, and reserve i for relationally-clustered concepts. Treebank trees can be deter-
ministically annotated with headwords h and relations l by using head rules (Magerman, 1995). The 5
SVS models ?M, ?L, ?P-Vit(G), piG, and piG can thus be obtained by counting instances and normalizing.
Empirical probabilities of this kind are denoted with a tilde, whereas estimated models have a hat.
Concepts i in a distributed semantic representation, however, cannot be found from annotated trees
(see example concepts in Figure 2). Therefore, we use Expectation Maximization (EM) in a variant of
the inside-outside algorithm (Baker, 1979) to learn distributed-concept behavior. In the M-step, the data-
informed result of the E-step is used to update the estimates of ?M, ?L, and ?H (where ?H is a generlization
of ?P-Vit(G) to any nonterminal). These updated estimates are then plugged back in to the next E-step. The
two steps continually alternate until convergence or a maximum number of iterations.
E-step:
P?(i? , i?, i? ? lc? , lc?, lc?) = P??Out(lci? , lch?lch?) ? P??Ins(lch? ? lci?)P?(lch) (9)
E(lci? ,lci?,lci?) = P?(i? ,i?,i? ?lc? ,lc?,lc?) ?P?(lc? ,lc?,lc?)
M-step:
P??M(lci?  lc?, lc?) =
?i?,i? E(lci? , lci?, lci?)
?lci?,lci? E(lci? , lci?, lci?)
P??L(i? ? i? ; l?) =
?lc? ,c?,lci? E(lci? , lci?, lci?)
?lc? ,ci?,lci? E(lci? , lci?, lci?)
(10)
P??H(h? ? lci?) = E(lci? ,?,?)?h? E(lci? ,?,?)
Inside probabilities can be recursively calculated on training trees from the bottom up. These are
simply probability sums of all subsumed subtrees (Viterbi probabilities with sums instead of maxes).
Outside probabilities can also be recursively calculated from training trees, here from parent proba-
bilities. For a left child (the right-child case is similar):
P??Out(lci?, lch?lch?) =P??Out(lci? , lch?lch?) ? P??M(lci?  lc?, lc?)
? ?i? P??L(i? ? i? , l?) ? P??Ins(lch? ? lci?) ? P??L(i? ? i? , l?) (11)
Since outside probabilities signify everything but what is subsumed by the node, they carry a comple-
mentary set of information to inside probabilities. Thus, inside and outside probabilities together are a
natural way to produce parent and child clustered concepts.
3.2 Relational Semantic Clusters in Parsing
Section 2.2.2 listed the five probability models necessary for SVS. To define SVS with relational clusters,
the estimates in Equation 10 can be used for ?M and ?L.
The preterminal model is based on ?H, but it also includes some backoff for words that have not been
used as headwords. The other two models also fall out nicely from the algorithm, though they are not
explicitly estimated in EM. The prior probability at the root is just the base case for outside probabilities:
P?piG(lci)def= P??Out(lci, lch?lch) (12)
Prior probabilities at non-root constituents are estimated from the empirically-weighted joint probability.
P?piG(lci?)def= ?
lci?,lci?
P?(lci? , lci?, lci?) (13)
With these models, a relationally-clustered SVS parser is now defined.
300
Cluster i1
?announcement?
unk 0.362
was 0.173
reported 0.097
posted 0.036
earned 0.029
filed 0.024
were 0.022
had 0.020
told 0.013
approved 0.013
Cluster i5
?change in value?
rose 0.137
fell 0.124
unk 0.116
gained 0.063
dropped 0.051
attributed 0.051
jumped 0.046
added 0.041
lost 0.039
advanced 0.022
Cluster i7
?change possession?
unk 0.381
had 0.065
was 0.062
took 0.036
bought 0.027
completed 0.025
received 0.024
were 0.023
got 0.018
made 0.018
acquired 0.016
Figure 2: Example ?H clusters from 1,000 head-
words clustered into 10 referents, after 10 EM itera-
tions, for transitive past-tense verbs (VBD-argNP).
0 5 10 15 20 25 30 35 40
0
100
200
300
400
500
Sentence Length
A
ve
ra
ge
 P
ar
si
ng
 T
im
e 
(s)
 
 
Non?vectorized
Vectorized
Figure 3: Speed of relationally-clustered SVS
parsers, with and without vectorization.
4 Evaluation
Sections 02?21 of the Wall Street Journal (WSJ) corpus were used as training data; Section 23 was
used as test data with reported parsing results on sentences greater than length 40. Punctuation was
left in for all reported evaluations. Trees were binarized, and syntactic states were thoroughly split into
subcategorization classes. As previously discussed, unlike tests on state-of-the-art automatically state-
splitting parsers, this isolates the contribution of semantics. The baseline 83.57 F-measure is comparable
to Klein and Manning (2003) before the inclusion of head annotations.
Subsequently, each branch was annotated with a head relation lID or a modifier relation lMOD ac-
cording to a binarized version of headword percolation rules (Magerman, 1995; Collins, 1997), and the
headword was propagated up from its head constituent. The most frequent headwords (e.g., h1, . . . , h50)
were stored, and the rest were assigned a constant, ?unk? headword category.
From counts on the binary rules of these annotated trees, the ?M, ?L, ?P-Vit(G), piG, and piG proba-
bilities for headword-lexicalization SVS were obtained. Modifier relations lMOD were deterministically
augmented with their syntactic context; both c and l symbols appearing fewer than 10 times in the whole
corpus were assigned ?unknown? categories.
These lexicalized models served as a baseline, but the augmented trees from which they were derived
were also inputs to the EM algorithm in Section 3.1. Each parameter in the model or training algorithm
was examined, with ?I ?={1,5,10,15,20} clusters, random initialization from reproducible seeds, and a
varying numbers of EM iterations.
The implemented parser had few adjustments from a plain CKY parser other than these vectors. No
approximate inference was used, with no beam for candidate parses and no re-ranking.
4.1 Interpretable relational clusters
Figure 2 shows example clusters for one of the headword models used, where EM clustered 1,000 head-
words into 10 concepts in 10 iterations. The lists are parts of the P??H(h? ? lci?) model. As such, each of
the 10 clusters will only produce headwords in light of some syntactic constituent. The figure shows how
distributed concepts produce headwords for transitive past-tense verbs. Note that the probability distri-
butions for different headwords are quite uneven, again confirming that some clusters are more specific,
and others are more general.
Each cluster has been given a heading of its approximate meaning ? i5, for example, mostly picks
verbs that are ?change in value? events. With 10 clusters, we might not expect such fine-grained clusters,
since pLSA-related approaches typically use several hundred for such tasks. The syntactic context of
transitive (and therefore state-split) past-tense verbs allows for much finer-grained distinctions, which
are then predominantly semantic in nature.
301
a)
Sec. 23, length < 40 wds LR LP F
syntax-only baseline: 83.32 83.83 83.57
headword-lex. 10hw: 83.10 83.61 83.35
headword-lex. 50hw: 83.09 83.40 83.24
rel?n clust. 50hw10 clust: 83.67 84.13 83.90
b)
Sec. 23, length < 40 wds LR LP F
baseline1 clust 83.34 83.90 83.62
1000 hw5 clust, avg 83.85 84.23 84.04
1000 hw10 clust, avg 84.04 84.40 84.21
1000 hw15 clust, avg 84.15 84.38 84.26
1000 hw20 clust, avg 84.21 84.42 84.31
Table 1: a) Unsmoothed lexicalized CKY parsers versus 10 semantic clusters. Evaluations were run
with EM trained to 10 iterations. b) Average dependence of parsing performance on number of semantic
clusters. Averages are taken over different random seeds, with EM running 4 or 10 iterations.
4.2 Engineering considerations
We should note that relationally-clustered SVS is feasible with respect to random initialization and speed.
Four relationally-clustered SVS models (with 500 headwords clustered into 5 concepts) were trained,
each having a different random initialization. We found that the parsing F-score had a mean of 83.98
and a standard deviation of 0.21 across different initializations of the model. This indicates that though
there are significant difference between the models, they still outperform models without SVS (see next
section).
Also, it may seem slow to consider the set of semantic concepts and relations alongside syntax, at
least with respect to normal parsing. The definition of SVS in terms of vectors actually mitigates this
effect on WSJ Section 23, according to Figure 3. Since SVS is probabilistically consistent, the parser
could be defined without vectors, but this would have the ?non-vectorized? speed curve. The contiguous
storage and access of information in the ?vectorized? version leads to an efficient implementation.
4.3 Comparison to Lexicalization
One important comparison to draw here is between the effectiveness of semantic clusters versus
headword-lexicalization. For fair head-to-head comparison onWSJ Section 23, both models were vector-
ized and included no smoothing or backoff. Neither relational clusters nor lexicalization were optimized
with backoff or smoothing.
Table 1a shows precision, recall, and F-score for lexicalized models and for clustered semantic mod-
els. First, note that the 10-cluster model (in bold) improves on a syntax-only parser (top line), showing
that the semantic model is contributing useful information to the parsing task.
Next, compare the 50-headword, 10-cluster model (in bold) to the line above it. It is natural to
compare this model to the headword-lexicalized model with 50 headwords, since the same information
from the trees is available to both models. The relationally-clustered model outperforms the headword-
lexicalized model, showing that clustering the headwords actually improves their usefulness, despite the
fact that fewer referents are used in the actual vectors.
It is also interesting, then, to compare this 50-headword, 10-cluster model to a headword-lexicalized
model with 10 headwords. In this case, the possible size of the grammar is equal. Again, the relationally-
clustered model outperforms plain lexicalization. This indicates that the 10 clustered referents are much
more meaningful than 10 headword referents for the disambiguating of syntax.
4.4 Effect of Number of clusters
The final experiment on relational-clustering SVS was to determine whether performance would vary
with the number of clusters. Table 1b compares average performance (over different random initializa-
tions) for numbers of clusters from 1 (a syntax-equivalent case) to 20.
First, it should be noted that all of the relationally clustered models improved on the baseline. Ran-
dom initializations did not vary enough for these models to do worse than syntax alone. For each vec-
tor/domain size, in fact, the gains over syntax-only are substantial.
302
In addition, the table shows that average performance increases with the number of clusters. This
loosely positive slope means that EM is still finding useful parts of the semantic space to explore and
cluster, so that the clusters remain meaningful. However, the increase in performance with number of
clusters is likely to eventually plateau.
Maximum-accuracy models were also evaluated, since each model is a full-fledged parser. The best
20-referent model obtained an F score of 84.60%, beating the syntactic baseline by almost a full absolute
point. Thus, finding relationally-clustered semantic output also contributes to some significant parsing
benefit.
4.5 Perplexity
Finally, per-word perplexities were calculated for a syntactic model and for a 5-concept relationally-
clustered model. Specific to this evaluation, following Mitchell and Lapata (2009), only the top 20,000
words in WSJ Sections 02-21 were kept in training or test sentences, and the rest replaced with ?unk?;
numbers were replaced with ?num.?
Sec. 23, ?unk?+?num? Perplexity
syntax only baseline 428.94
rel?n clust. 1khw?005e 371.76
Table 2: Model fit as measured by perplexity.
Table 2 shows that adding semantic information greatly reduces perplexity. Since as much syntactic
information as possible (such as argument structure) has been pre-annotated onto trees, the isolated
contribution of interactive semantics improves on a syntax-only model model.
5 Conclusion
This paper has introduced a structured vectorial semantic (SVS) framework in which vector composition
and syntactic parsing are a single, interactive process. The framework thus fully integrates distributional
semantics with traditional syntactic models of language.
Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalization
SVS (bilexical parsing) and relational-clustering SVS (latent annotations). It was found that relationally-
clustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additional
clusters had a mildly positive effect. Additionally, perplexity results showed that the integration of
distributed semantics in relationally-clustered SVS improved the model over a non-interactive baseline.
It is hoped that this flexible framework will enable new generations of interactive interpretation
models that deal with the syntax?semantics interface in a plausible manner.
References
Baker, J. (1979). Trainable grammars for speech recognition. In D. Klatt and J. Wolf (Eds.), Speech
Communication Papers for the 97th Meeting of the Acoustical Society of America, pp. 547?550.
Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent dirichlet alocation. Journal of Machine Learning
Research 3, 993?1022.
Charniak, E. (1996). Tree-bank grammars. In Proceedings of the National Conference on Artificial
Intelligence, pp. 1031?1036.
Collins, M. (1997). Three generative, lexicalised models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computational Linguistics (ACL ?97).
303
Deerwester, S., S. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman (1990). Indexing by latent
semantic analysis. Journal of the American Society for Information Science 41(6), 391?407.
Erk, K. and S. Pado? (2008). A structured vector space model for word meaning in context. In Proceedings
of EMNLP 2008.
Frege, G. (1892). Uber sinn und bedeutung. Zeitschrift fur Philosophie und Philosophischekritik 100,
25?50.
Ge, R. and R. J. Mooney (2005). A statistical semantic parser that integrates syntax and semantics. In
Ninth Conference on Computational Natural Language Learning, pp. 9?16.
Gesmundo, A., J. Henderson, P. Merlo, and I. Titov (2009). A latent variable model of synchronous
syntactic-semantic parsing for multiple languages. In Proceedings of CoNLL, pp. 37?42. Association
for Computational Linguistics.
Griffiths, T. L., M. Steyvers, D. M. Blei, and J. B. Tenenbaum (2005). Integrating topics and syntax.
Advances in neural information processing systems 17, 537?544.
Hofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learn-
ing 42(1), 177?196.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Koo, T., X. Carreras, and M. Collins (2008). Simple semi-supervised dependency parsing. In Proceed-
ings of the 46th Annual Meeting of the ACL, Volume 8. Citeseer.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th International
Conference on Machine Learning, Volume 296304.
MacDonald, M. C., N. J. Pearlmutter, and M. S. Seidenberg (1994). The lexical nature of syntactic
ambiguity resolution. Psychological Review 101(4), 676?703.
Magerman, D. (1995). Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguistics (ACL?95), Cambridge, MA, pp. 276?283.
Matsuzaki, T., Y. Miyao, and J. Tsujii (2005). Probabilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 75?82. Association for
Computational Linguistics.
Mitchell, J. and M. Lapata (2008). Vector-based models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, pp. 236?244.
Mitchell, J. and M. Lapata (2009). Language Models Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 430?439.
Pado?, S. and M. Lapata (2007). Dependency-based construction of semantic space models. Computa-
tional Linguistics 33(2), 161?199.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein (2006). Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 44th Annual Meeting of the Association for Computational
Linguistics (COLING/ACL?06).
Rudolph, S. and E. Giesbrecht (2010). Compositional matrix-space models of language. In Proceedings
of ACL 2010. Association for Computational Linguistics.
Taskar, B., E. Segal, and D. Koller (2001). Probabilistic classification and clustering in relational data.
In IJCAI?01: Proceedings of the 17th international joint conference on Artificial intelligence, San
Francisco, CA, USA, pp. 870?876. Morgan Kaufmann Publishers Inc.
304
