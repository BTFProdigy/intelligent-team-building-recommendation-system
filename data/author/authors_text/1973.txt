  
Assigning Belief Scores to Names in Queries 
Christopher Dozier 
Research and Development  
Thomson Legal and Regulatory  
610 Opperman Drive 
Eagan, MN 55123,USA 
chris.dozier@westgroup.com  
   
ABSTRACT 
Assuming that the goal of a person name query is to find 
references to a particular person, we argue that one can derive 
better relevance scores using probabilities derived from a 
language model of personal names than one can using corpus 
based occurrence frequencies such as inverse document frequency 
(idf).   We present here a method of calculating person name 
match probability using a language model derived from a 
directory of legal professionals.  We compare how well name 
match probability and idf predict search precision of word 
proximity queries derived from names of  legal professionals and 
major league baseball players.  Our results show that name match 
probability is a better predictor of relevance than idf.  We also 
indicate how rare names with high match probability can be used 
as virtual  tags within a corpus to identify effective collocation 
features for person names within a professional class.     
1. INTRODUCTION 
Some of the most common types of queries submitted to search 
engines both on the internet and on proprietary text search 
systems consist simply of a person?s name.  To improve the way 
such queries are handled, it would be useful if search engines 
could estimate the likelihood or belief that a name contained in a 
document pertains to the name in the query.  Traditionally, 
relevance likelihood for name phrases has been based on inverse 
document frequency or idf, [3][4].  The idea behind this relevance 
estimate is that names which rarely occur in the corpus are 
thought to be more indicative of relevance than names that 
commonly occur.    
Assuming that the goal of a person name query is to find 
references to a particular person, we argue that one can derive 
better relevance scores using probabilities derived from a 
language model of personal names than one can using corpus 
based occurrence frequencies.  The reason for this is that finding 
references to a particular person in text is more dependent upon 
the relative rarity of the name with respect to the human 
population than it is on the rarity of the name within a corpus. 
To get an intuitive idea of this point, consider that, within a 
corpus of 27,000 Wall Street Journal articles published between 
January and August of the year 2000, the name ?Trent Lott? 
occurred in 80 documents while the name ?John Smith? occurred 
in 24.  All 80 references to ?Trent Lott? referred to the majority 
leader of the U.S. Senate, while ?John Smith? references mapped 
to 5 different people.  This is not surprising.  From our 
experience, we know that ?Trent Lott? is an uncommon name and 
?John Smith? is a common one. 
We present here evidence that name match probability based on a 
language model predicts relevance for name queries far better than 
idf.   It may be argued that idf was never intended to be used to 
measure the relative ambiguity of a name query.  However, idf is 
the standard measure used in probabilistic search engines to 
measure the degree of relevance terms and phrases within a 
collection have to the terms and phrases in queries, [1] [5].  For 
this reason, we take idf to be the standard against which to 
compare name match probability.   
Being able to predict relevance through name match probabilities 
enables us to do three things.   First, it tells us when we need to 
add information to the query to improve precision either by 
prompting the user for it or automatically expanding the query.  
Second, and perhaps more importantly, it enables us to use names 
with high match probabilities as virtual tags that can help us find 
useful collocation features to disambiguate names within a given 
class of names, such as the names of attorneys and judges.  For 
purposes of this paper, we define an ambiguous name as one 
likely to be shared by many people and an unambiguous name as 
one likely to apply to a single person or to only a few people.  
And third, match probability can be used as a feature within a 
name search operator to improve search precision.  
 
2. DESCRIPTION OF MATCH 
PROBABILITY CALCULATION FOR 
PERSON NAMES 
The motivation for our work is an effort to develop a name search 
operator to find attorneys and judges in the news.   In our 
particular application, we wish to allow users to search for 
newspaper references to attorneys and judges listed in a directory 
of U.S. legal professionals.  This directory contains the 
curriculum vitae of approximately one million people.  In this 
section, we show how we calculate person name match 
probability. 
To compute the probability of relevance or match probability for a 
name, we perform three steps. First, we compute a probability 
distribution for the first and last names in our name directory.  
This is our language model.  Second, we compute a name?s 
probability by multiplying its first name probability with its last 
 
 
 
  
name probability.  Third, we compute its match probability by 
taking the reciprocal of the product of the name probability and 
the size of the human population likely to be referenced in the 
corpus.  For our Wall Street Journal test corpus, we estimated this 
size to be approximately the size of the U.S. population or 300 
million. Formulas for the three steps are shown below. 
 
where F =  number of occurrences of first name, L = number of 
occurrences of last name, and N = number of names in the 
directory. 
 
(2) )_()_()( namelastPnamefirstPnameP ?=  
 
 (3)   ( ) 1)(
1)_(
+?
= namePHmatchnameP
 
 
 
where H = size of human population likely to be referenced by the 
collection. 
Example calculations for Trent Lott and John Smith are shown 
below in Table 1. 
In this example, the match probability for Trent Lott is 
approximately four orders of magnitude higher than the match 
probability for John Smith, while idf or document frequency 
suggests the likelihood of relevance for documents retrieved for 
John Smith is higher than for documents retrieved for Trent Lott.  
Both empirically and intuitively, match probability is a better 
predictor of relevance here than idf. 
 
 
3. EVALUATION OF NAME MATCH 
PROBABILITY VERSUS IDF   
To test our hypothesis that name match probability predicts 
relevance better than idf, we compared how well name queries 
with high match probabilities performed against name queries 
with high idf.  We performed two experiments.  In the first, we 
selected names of individuals in our legal directory.  In the 
second, we used the names of currently active major league 
baseball players. 
To conduct the first experiment, we labeled person names in a 
collection of 27,000 WSJ documents with a commercially 
available name tagging program.   We then extracted these names 
and created a merged list of names specified by first and last name 
and pulled from this list names that occurred within our legal 
directory.  We then sorted this list by name match probability and 
by document occurrence frequency (which is equivalent to idf) to 
create two lists.  We then binned the names in the name match 
probability list into sets that fell between the following probability 
ranges: 1.0-0.9, 0.9-0.8 ,0.8-0.7, 0.7-0.6, 0.6-0.5, 0.5-0.4, 0.4-0.3, 
0.3-0.2, 0.2-0.1, and 0.1-0.0.   We binned the names in the 
document frequency list into sets that fell into the following 
document occurrence frequencies: 1, 2, 3, 4, 5, 6, 7, 8, 9, and 
>=10. 
We then selected 50 names at random from each of these bins 
(except for bins associated with 0.8-0.7 and 0.7-0.6 probabilities 
which contained 42 and 31 names  respectively).  For each name 
selected, we identified the legal directory entry that was 
compatible with the name.  In most cases, only one legal directory 
entry was compatible with the name.  In some cases, multiple 
entries were compatible.   For example, the name ?Paul Brown? is 
compatible with 71 legal directory entries since there are 71 
people in the directory with the first name ?Paul? and the last 
name ?Brown?.  In these cases, we selected one of the entries at 
random.  
For each name in each bin, we found the set of documents in the 
WSJ collection that would be returned by the word proximity 
query ?First_name +2 Last_name?.   That is, the documents that 
contained the first name followed within two words by the last 
name. 
The search precision results for match probability and document 
frequency bins are shown in tables 2 and 3 below.  The search 
precision of each bin was the number of relevant documents 
returned by the names in the bin divided by the total number of 
documents returned.  The row labeled  ?Number Unique Names in 
Each Category? is a count of the number of unique first and last 
name pairs found within the WSJ collection for the probability 
and document frequency ranges indicated.  It was from these sets 
of names that we selected our queries. 
The results in tables 2 and 3 show that match probability does a 
better job of estimating relevance than idf. Table 2 shows that 
search precision goes up as match probability rises.  Table 3 
shows no apparent correspondence between document frequency 
and search precision. 
 
                                                            Table 1: Example Calculation of Match Probability 
Name P(first name) P(last name) P(name) P(name match) Doc Freq 
Trent Lott 0.000084 0.000048 0.00000000408 0.449371705 80 
John Smith 0.036409 0.006552 0.00023857 0.00001397 24 
 
 
(1) N
FnamefirstP =)_(
N
LnamelastP =)_(
  
 In the second experiment, we performed basically the same steps 
described above on the names of the 286 baseball players 
currently playing in the major leagues.  We assigned name match 
probabilities to these names using the language model we derived 
from the legal directory.  Of the 286 names, we found 82 that 
were compatible with one or more name instances in the WSJ 
collection. For all 82, we found the set of documents in the WSJ 
collection that would be returned by the word proximity query 
?First_name +2 Last_name?.   We then measured how frequently 
the documents returned for a particular word proximity query 
actually referenced the player with which the name query was 
paired.  As in the attorney and judge name experiment, name 
match probability predicted relevance more accurately than idf.  
The results for baseball player names are shown in tables 4 and 5 
above. 
Note that on average the search precision for baseball players was 
higher than for attorneys and judges.  This is due to the combined 
Table 2:  Search Precision At Different Match Probabilities for Names Compatible 
                                                  with Judge and Attorney Names for WSJ Collection 
Match Prob 
Range 
1.0 -
0.9 
0.9 ? 
0.8 
0.8 ? 
0.7 
0.7 ? 
0.6 
0.6 ? 
0.5 
0.5 ? 
0.4 
0.4 ? 
0.3 
0.3 ? 
0.2 
0.2 ? 
0.1 
0.1 ? 
0.0 
Search  
Precision  
0.835 0.754 0.595 0.677 0.596 0.708 0.628 0.544 0.520 0.12 
Number Unique 
Names in Each 
Category 
80 61 42 31 57 72 113 135 292 10758 
 
 
Table 3:  Search Precision At Different Document Occurrence Frequencies for Names Compatible 
                                                  with Judge and Attorney Names for WSJ Collection 
Doc Freq 
 
1 2 3 4 5 6 7 8 9 >=10 
Search 
Precision 
0.18 0.10 0.10 0.20 0.06 0.10 0.08 0.18 0.14 0.24 
Number Unique 
Names in Each 
Category 
7702 1946 703 374 224 145 95 75 55 322 
 
Table 4:  Search Precision At Different Match Probabilities for Names Compatible 
                                with Names of Major League Baseball Players for WSJ Collection 
Match Prob 
Range 
1.0 -
0.9 
0.9 ? 
0.8 
0.8 ? 
0.7 
0.7 ? 
0.6 
0.6 ? 
0.5 
0.5 ? 
0.4 
0.4 ? 
0.3 
0.3 ? 
0.2 
0.2 ? 
0.1 
0.1 ? 
0.0 
Search Precision  1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.939 0.633 
Number Unique 
Names in Each 
Category 
15 5 2 2 2 3 2 7 7 48 
 
 
Table 5:  Search Precision At Different Document Occurrence Frequencies for Names Compatible 
                              with Names of Major League Baseball Players for WSJ Collection 
Doc Freq 
 
1 2 3 4 5 6 7 8 9 >=10 
Search 
Precision 
0.888 0.882 0.952 1.0 0.75 0.666 1.0 NA 1.0 0.74 
Number Unique 
Names in Each 
Category 
45 17 7 3 4 6 2 0 1 8 
 
  
effects of there being far fewer baseball player names than 
attorney and judge names and the fact that the average probability 
of a baseball player being mentioned in the news is higher than 
the average probability for a judge or attorney being mentioned. 
 
 
 
 
 
 
 
 
4. USING RARE NAMES TO IDENTIFY 
SEARCH FEATURES 
An important use of name match probabilities is the identification 
of co-occurrence features in text that can serve to disambiguate 
name references.  If we know certain names in the corpora very 
probably refer to certain individuals listed in a professional 
directory, we can look for words that co-occur frequently with 
these names but infrequently with names in general.  These words 
are likely to work well at disambiguating references to names of 
low match probability.  
As an example of feature identification, consider the figures 1 and 
2 above.  In these figures, the word ?rare? stands for the 20% of 
names in the legal directory that have the highest match 
probability.  The phrase ?medium rare? stands for the next 20% 
and so on.  The word ?common? then stands for the 20% of 
names with the lowest match probability.  For each of the five 
categories of name rarity, the graphs in the figures show the 
probability of an appositive term occurring at a given word 
position relative to the position of a name. 
Figure 1 shows the probability of attorney appositive nouns such 
as ?attorney?, ?lawyer?, ?counsel?, or ?partner? occurring at 12 
different word positions around attorney names of varying degrees 
of rarity.  Position ?1 stands for the word position directly before 
the name.  Position +1 stands for the position directly after.  
Position ?2 stands for the word position two words in front of the 
name and so on.  Figure 2 shows the probability of judge 
appositive nouns such as ?judge? or ?justice? occurring around 
judge names. 
The graphs in figures 1 and 2 show that the probability of 
appositive terms occurring at particular word positions grows 
steadily as the name rarity increases. This demonstrates that 
appositive terms are good indicators for judge and attorney names 
within the WSJ collection.  The figures also shows the word 
positions in which we should look for appositive terms. 
Figure 1 shows that we should look for attorney appositives in 
word positions ?2, -1, +2, +4, and +5.  This makes intuitive sense 
because it accounts for sentence constructs such as those shown in 
table 6. 
 
 
 
 
 
 
 
 
 
 
 
The sudden drop off in appositive term probability at word 
position +1 also makes sense since an article, adjective, or other 
part of speech often occurs between a trailing appositive head 
noun and the proper noun it modifies.  The drop off at word 
position +3 is still something of a mystery and is not something 
we can explain at this time.   Since +3 behavior seems to have no 
linguistic basis that we can perceive, we do not rely on it in 
constructing our search operator.   
Figure 2 shows that we should look for judge appositives in word 
position -1.  This makes perfect sense since it accounts for 
constructs such as ? Judge William Rehnquist? and ?Justice 
Antonin Scalia?.  Figure 2 also suggests that using the -1 
appositive test should yield good search recall since the 
conditional probability for rare names is about 0.9. 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
-6 -5 -4 -3 -2 -1 1 2 3 4 5 6
rare
medium rare
medium
medium common
common
0
0.2
0.4
0.6
0.8
1
-6 -5 -4 -3 -2 -1 1 2 3 4 5 6
rare
medium rare
medium
medium
common
common
Figure1: Conditional probability of 
attorney terms by word position 
relative to name  
Figure2: Conditional probability of 
judge terms by word position relative 
to name  
      Table 6: Examples of Use of Attorney Term Near Attorney Name 
Relative 
Word 
Position 
Example sentence 
-2 Attorney General Janet Reno said today ?..   
-1 Attorney Jack Smith defended his client vigorously. 
+2 said Vicki Patton, senior attorney for Environmental 
Defense 
+4 said Jim Hahn, Los Angeles City Attorney 
+5 says Buck Chapoton, a prominent Washington tax 
attorney 
 
  
5. PRELIMINARY SEARCH OPERATOR 
EXPERIMENTS 
We are currently investigating what levels of  search precision and 
recall we can achieve with special attorney and judge name search 
operators using name rarity together with  co-occurrence features 
such as appositive, city, state, firm, and court terms.  Our 
preliminary results are encouraging. Initial experiments with the 
attorney search operator indicate we can achieve a nine fold 
improvement in search precision over simple word proximity 
searches over the WSJ collection while sacrificing 18% recall.  
Preliminary results are shown in table 7 below. We produced 
these results by selecting 677 attorney names at random from the 
legal directory that existed within the WSJ collection.  For each 
name, we ran word proximity searches using the first and last 
name of the lawyers and scored the results.  Using the scored 
results from 377 of the names, we then trained a special Bayesian 
based name operator that used first name, last name, city, state, 
firm, and name rarity information as sources of name match 
evidence. Finally we tested the word proximity operator 
performance against the special name operator using the 
remaining 300 names. 
Note that we have assumed above that word proximity searches 
yield 100% recall.  This is not wholly accurate since it does not 
account for nicknames, use of first name initials, and so on.  We 
plan to revise this recall estimate in the future, but for now we 
assume that a word proximity search on first and last name 
provides close to 100% recall in a collection such as the WSJ. 
 
 
 
 
 
 
 
6. FUTURE WORK 
We plan to complete development of search operators for attorney 
and judges that make use of the combined features of name rarity, 
appositives, city, state, firm, and court terms.  We plan to compare 
the performance of these operators against searches based on 
name indexes derived from combining MUC style extraction 
techniques and record linking techniques. [2] Our hope is that the 
search operators will perform at levels close to the indexed based 
searches so that we can avoid the operational costs of creating 
special name indexes. 
We plan to mine names from text using name rarity and seed 
appositive phrases.  For example, using a seed appositive phrase 
for a profession such as ?expert witness?, we plan to identify and 
extract a set of expert witness names.  From this initial set of 
names, we will identify rare names and use these to identify more 
appositive phrases.  Once the appositive phrases are identified, we 
plan to extract more names, then more appositive phrases, and so 
on until a stopping condition is reached.  In this manner, we hope 
to develop a technique to automatically extract name lists from 
text collections.   
Finally we plan to assess whether it is possible to develop similar 
name match probability calculations for other types of names such 
as company names, organization names, and product names. 
 
7. CONCLUSION 
Assuming that the goal of a person name query is to find 
references to a particular person, we have shown  that one can 
derive better relevance scores using probabilities derived from a 
language model of personal names than one can using corpus 
based occurrence frequencies. We presented here a method of 
calculating person name match probability using a language 
model derived from a directory of legal professionals.  We 
compared how well name match probability and idf predict search 
precision of word proximity queries derived from names of legal 
professionals and major league baseball players.  Our results 
showed that name match probability is a better predictor of 
relevance than idf.  We also indicated how rare names with high 
match probability can be used as virtual tags within a corpus to 
identify effective collocation features for person names within a 
professional class. 
8. REFERENCES 
[1] Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information 
Retrieval. ACM Press,  New York, 1999. 
[2] Dozier, C. and Haschart, R., "Automatic Extraction and 
Linking of Person Names in Legal Text" in Proceedings of RIAO 
'2000;  Content Based Multimedia Information Access. Paris, 
France. pp.1305-1321. 2000  
[3] de Lima, F. and Pedersen, J., Phrase Recognition and 
Expansion for Short, Precision-biased Queries based on a Query 
Log. In  Proc.of the 22nd Annual Int. ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp. 145 ?
152, Berkeley, California, USA, 1999. 
[4] Thompson, P. and Dozier, C., Name Searching and 
Information Retrieval. In Proc.of the 2nd Conference on 
Empirical Methods in NLP,  pp. 134 ?140, Providence, Rhode 
Island, 1997. 
[5] Turtle, H. and Croft, W.,  Inference Networks for Document 
Retrieval. In Proc.of the 13th Annual Int. ACM SIGIR Conference 
on Research and Development in Information Retrieval, pp. 1 ?
24, Brussels, Belgium, 1990. 
 
 
Table 7: Comparison of Performance of Word Proximity 
Search and Special Name Operator Searches for Attorney 
Names 
Search Method Precision Recall F-measure 
Word proximity 0.09 1.00 0.17 
Attorney Name Search 
Operator 
0.85 0.82 0.83 
 
A web application using RDF/RDFS for metadata navigation
Xi S. Guo, Mark Chaudhary, Christopher Dozier
Yogi Arumainayagam, Venkatesan Subramanian
Research & Development
Thomson Legal & Regulatory
610 Opperman Drive
Eagan, MN 55123, USA
xi.guo@thomson.com
Abstract
This paper describes using RDF/RDFS/XML to cre-
ate and navigate a metadata model of relationships
among entities in text. The metadata we create
is roughly an order of magnitude smaller than the
content being modeled, it provides the end-user
with context sensitive information about the hyper-
linked entities in focus. These entities at the core
of the model are originally found and resolved us-
ing a combination of information extraction and
record linkage techniques. The RDF/RDFS meta-
data model is then used to ?look ahead? and navi-
gate to related information. An RDF aware front-
end web application streamlines the presentation of
information to the end user.
1 Introduction
As an information provider, Thomson West stores
vast quantities of documents that are served up in
response to user queries. Determining the relation-
ships between entities of interest in these documents
can be a complex and time consuming part of end-
user research. Nor is this sort of information always
explicitly presented in the documents retrieved by
searches. Automating the process of discovery is
complicated by the need to uniquely identify and
resolve ambiguities and co-references between en-
tities.
Our system relies on various NLP techniques and
name/entity taggers to identify attorney and judge
names in news articles on WestlawTM . These
names are then tagged with unique reference iden-
tifiers that link them to their records in our legal di-
rectory. The relationships between these individuals
and other entities like their firm (or court name for
judges), and title of the document in which they are
found are stored as RDF metadata.
A simple representation of relationships among
these entities is shown in Figure 1. Documents
make references to attorneys. Using NLP tech-
niques, each occurrence is resolved to a unique ref-
erence identification. The metadata then allows us
Figure 1: Relationships between entities
to expose meaningful relationships among entities
in text. Storing this information as metadata in
the UI allows us to look ahead. Hovering over a
name, the end user is able to see which firms they
are affiliated with. The user is also able to look
ahead to see all the other documents that the per-
son occurs in. In addition, we also know which
firm each attorney works for and this relationship
allows us to see all the other attorneys who work for
the same firm. This information is not present in
any of the documents retrieved but is inferred from
our RDF/RDFS (Lassila, 2000), (Klyne and Carroll,
2004), (W3C, 1999), (W3C, 2004) metadata model.
The RDF/RDFS metadata model helps to dynami-
cally resolve relationship among entities during the
time of front end rendering. This system could be
extended to incorporate additional relationships be-
tween other kinds of data.
2 Architecture
Content in our architecture consists of plain text
news documents and RDF metadata. Both are
stored in an XML content repository. In addition
we also store Thomson West?s legal database of at-
torney profiles in the same repository as well. With
the content stored, we use a name/entity tagger
in combination with methods described in (Dozier
and Haschart, 2000) to link occurrences of attorney
Figure 2: High Level Architecture
names within the plain text news documents to their
database profile record.
There are several reasons that motivate us to build
this web application using RDF/RDFS. Firstly, our
existing data model put metadata and content in the
same data repository, the relationships or links are
embedded inside content. This makes it very diffi-
cult to build new business products since developers
have to write programs to look at content first, ex-
tract information out of it and then put this extracted
information somewhere to enable front-end render-
ing. The disadvantage of this approach is being able
to dynamically maintain the integrity of both data
repository and relationship repository in a rapidly
changing environment. Both of these repositories
need to be updated whenever any relationships get
updated. The use of RDF/RDFS separates relation-
ships from content so manipulation of metadata is
easier and less expensive.
RDF/RDFS?s ability to provide a data infrastruc-
ture for entities, relationships extracted from NLP
applications is the second reason for choosing it as
our data model. In our domain, we have different
kinds of entities embedded in news articles, law re-
views, legal cases etc. These entities include attor-
ney name, judge name, and law firm names. We
are interested in not only identifying them in con-
tent but also finding their relationships and linking
them together. RDF/RDFS allows us to accomplish
this.
Architecture for this application uses MVC
(Model View Controller) design pattern for separat-
ing graphical interface of one application from its
backend artifacts such as code and data. This classic
architectural design pattern provided the flexibility
to maintain multiple views of backend data.
2.1 RDF/RDFS/XML Data Model
Using the MVC design pattern, our data model rep-
resents data used by the application and the rules
for accessing this data. A RDF/RDFS/XML model
is created to represent the data and a set of APIs is
provided for data accessing purpose.
Our prototype contains 911274 legal profession-
als? profiles from West?s Legal Directory and 2000
news documents. The news documents are pre-
processed using our name entity tagger. The tagging
process is able to generate a list of people templates
that are then fed into an entity reference resolution
program. This allows us to resolve each extracted
name template to its specific record from West?s Le-
gal Directory.
Our data model environment contains separate
metadata and content repositories, the XML content
repository and the RDF metadata repository. We
convert the news articles to XML and load them to
XML content repository. Our search API features of
this repository allow us to perform full text search-
ing inside content. Each news article takes the form
of one XML document identified by a unique refer-
ence number. Names found inside these documents
by the name tagger are identified with xml elements.
Besides 2000 news articles, WLD legal profession-
als? profiles are also loaded to this content reposi-
tory with each profile also associated with a unique
identifying number.
Our RDF metadata repository employs on
RDF/RDFS model. A simple RDF schema formally
specifies groups of related resources and the rela-
tionships between these resources. Figure 3 demon-
strates three major RDF resources; Document, Peo-
ple and Organization. The Attorney and Judge re-
sources are subclasses of the People resource. Each
instance of these resources has a URI associated
with it. Resource related properties are also de-
fined in this schema. The ranges of some properties
of resources are themselves resources from other
domains. For example, resource Document has a
property PeopleInDocument. This property has its
domain in Document but its range is in the People
domain. The schema allows us to specify the data
model so our metadata navigation application could
follow relationship links specified in it. More de-
tails about this schema can be found in Appendix
A.
Based on this schema, the RDF metadata repos-
itory is built to represent the relationships among
Figure 3: RDF schema of the application
news articles, attorneys, judges, courts and law
firms. The metadata building process involves sev-
eral steps that are entity and relation extraction from
the tagged XML content repository, RDF metadata
generation, and RDF metadata loading. The end
result is an RDF metadata repository with full text
search capability. Figure 4 shows samples of a por-
tion of the metadata model depicting the occurrence
of two attorneys in a Wall Street Journal document.
During the time the metadata repository was
built, our schema was only used for data validation
purpose. Currently we are exploring one approach
that leverages the expressive power of logic pro-
gramming tool such as Prolog to navigate the RDF
schema graph; this schema navigation should be
able to enable automatic metadata collection about
particular concepts and then build corresponded
RDF metadata based upon.
Note that in this application, URIs (unique ref-
erence identification) are used extensively. Each
document in both content and metadata reposito-
ries has a unique number associated with it. This
unique number works as a unique resource link and
is utilized by the RDF documents in the metadata
repository. With this unique number, the RDF docu-
ment can then be linked to any xml or rdf document,
and even to elements inside these documents using
Figure 4: Sample RDF metadata
XPATH.
In the sample of the RDF data presented in Table
1, the WSJ document with URI ?WSJ210572229?
entitled ?Market on a High Wire? contains ref-
erences to two attorneys; Froehlich and Madden.
Figure 5: Small RDF Graph of one metadata sample
Froehlich has URI ?WLD0293087701? and Mad-
den has URI ?WLD0293086676?. The metadata
also contains the XPATH of the attorney names in-
side this WSJ document as well as the XPATH to
other properties of the document such as news title
and news content.
Figure 5 shows a small RDF graph gener-
ated from samples in Table 1. In this graph,
?WSJ210572229? and ?WLD0293087701? are two
major resources from two different domains. The
RDF properties of both resources point to each other
through predicates. These pointing edges represent
relationships among multiple entities and they form
the infrastructure for our navigational map that will
eventually be presented to end-user.
Besides metadata and content storage, the data
model in MVC also provides a set of APIs for ac-
cessing both metadata and content. In XML content
repository, APIs exist for single XML document re-
trieval by URI and full text search by user queries.
In the RDF metadata repository, APIs exist for sin-
gle RDF document retrieval by URI, RDF resource
link retrieval using ARP, an RDF parser from HP
and RDF metadata full text search.
2.2 Application Controller
The Controller in our MVC patterned application
contains our metadata navigation logic. The pur-
pose of this layer is to capture all requests from the
front view and to interact with the data model to
provide the data wanted by the end user.
The general scenario of our application starts out
with a user typing in queries. These queries are then
passed to the XML content repository which re-
turns matched search results with navigation meta-
data embedded inside. All of this metadata is gener-
ated through the controller layer that interacts with
both RDF and XML repository. The results then are
presented to the user who can click on entities of in-
terest (which are RDF resources) and thus navigate
through our metadata repository.
2.3 Front View
All information rendering happens in the front view
layer. This layer interacts with end users and speci-
fies how final data can be represented. Since back-
end data is either RDF or XML, we use XSLT to
convert this to HTML/JSP pages that work in the
front end browser.
Appendix B shows a snapshot of our application
depicting a single Wall Street Journal article con-
taining attorney names. The end user can roll over
this name link and using the pop-up menu, navigate
to other corresponding entities such as other news
documents that mention the same name, or law firm
this attorney is working in. This metadata-based
navigation is described in detail in next section.
3 Metadata based Navigation
By tagging entity information and resolving cross
document co-references for attorneys and judges,
we were able to identify all the documents a partic-
ular attorney or judge appeared in. The RDF meta-
data model goes a step further weaving together
the relationships between attorneys, judges, firms,
courts and the documents that reference them.
With the metadata model it now becomes easier
for the user to see all related information from any
particular node. The combination of information
extracted from documents with information from
authority files, gives us a dynamic view of rela-
tionships in the content that can answer questions
such as ?What other attorneys were mentioned in
the same article?? and ?Who else works at the same
firm as this attorney?? These relationships facilitate
navigation between related entities. Figure 6 shows
how the metadata model allows the user to navigate
from one related node to the next. Not only are we
able to tell the firm an attorney belongs to even if
that wasn?t specifically mentioned in the text of the
document, but we can also use the metadata model
to shift our focus onto the firm node and immedi-
ately see a list of other attorneys related to that firm.
Switching to any one of those nodes (attorneys) im-
mediately shows us articles related to the next attor-
ney. In a similar fashion we can move from judges
to courts and articles and back.
4 Conclusion
This application utilizes RDF/RDFS to build a data
model that allows for easy maintenance of reference
links embedded in content. This data model also fa-
cilitates development of metadata navigation. By
just looking through metadata repository, the appli-
cation can decide the best way to utilize rich infor-
mation buried inside content repository.
We feel that this application can be extended to
provide inferencing capability. The hard wiring of
the logic inside the metadata repository does not
currently provide any formalism to infer hidden re-
lationships from the facts. Implementing this infer-
encing mechanism would bring us closer to our se-
mantic web goal.
References
Christopher Dozier and Robert Haschart. 2000.
Automatic extraction and linking of person
names in legal text. Proceedings of RIAO-2000:
Recherche d?Informations Assiste par Ordina-
teur.
Graham Klyne and Jeremy J. Carroll.
2004. Resource description framework
Figure 6: Navigation between related metadata
(rdf): Concepts and abstract syntax.
http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Ora Lassila. 2000. The resource description frame-
work. IEEE Intelligent Systems, 15(6):67?69.
W3C. 1999. Resource description
framework (rdf) model and syntax.
http://www.w3.org/TR/1999/REC-rdf-syntax-
19990222/.
W3C. 2004. Rdf vocabulary descrip-
tion language 1.0: Rdf schema.
http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Appendix
A A RDF Schema for data model of our application
<?xml version=?1.0? encoding=?ISO-8859-1??>
<!DOCTYPE rdf:RDF [ <!ENTITY rdf ?http://www.w3.org/1999/02/22-rdf-syntax-ns#?>
<!ENTITY PeopleCite ?http://www.thomson.com/PeopleCite#?>
<!ENTITY rdfs ?http://www.w3.org/TR/1999/PR-rdf-schema-19990303#?>]>
<rdf:RDF xmlns:rdf=?&rdf;? xmlns:PeopleCite=?&PeopleCite;? xmlns:rdfs=?&rdfs;?>
<rdfs:Class rdf:about=?&PeopleCite;Document?>
<rdfs:subClassOf rdf:resource=?&rdfs;Resource?/>
</rdfs:Class>
<rdf:Property rdf:about=?&PeopleCite;ContentOfDocument?>
<rdfs:domain rdf:resource=?&PeopleCite;Document?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;DocumentURI?>
<rdfs:comment> The Unique Identication Number of each document </rdfs:comment>
<rdfs:domain rdf:resource=?&PeopleCite;Document?/>
<rdfs:range rdf:resource=?&rdfs;Literal?/>
</rdf:Property>
<rdfs:Class rdf:about=?&PeopleCite;WSJ?>
<rdfs:comment xml:space=?preserve?>
<![CDATA[<<Wall Street Journal>>News Data Repository]]>
</rdfs:comment>
<rdfs:subClassOf rdf:resource=?&PeopleCite;Document?/>
</rdfs:Class>
<rdf:Property rdf:about=?&PeopleCite;PersonInDocument?>
<rdfs:domain rdf:resource=?&PeopleCite;Document?/>
<rdfs:range rdf:resource=?&PeopleCite;Person?/>
</rdf:Property>
<rdfs:Class rdf:about=?&PeopleCite;Person?>
<rdfs:subClassOf rdf:resource=?&rdfs;Resource?/>
</rdfs:Class>
<rdfs:Class rdf:about=?&PeopleCite;Attorney?>
<rdfs:subClassOf rdf:resource=?&PeopleCite;Person?/>
</rdfs:Class>
<rdfs:Class rdf:about=?&PeopleCite;Judge?>
<rdfs:subClassOf rdf:resource=?&PeopleCite;Person?/>
</rdfs:Class>
<rdf:Property rdf:about=?&PeopleCite;PersonURI?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&rdfs;Literal?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;LastNameOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;FirstNameOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;MiddleNameOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;AflicationOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&PeopleCite;Organization?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;AddressOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;CitingDocumentOfPerson?>
<rdfs:domain rdf:resource=?&PeopleCite;Person?/>
<rdfs:range rdf:resource=?&PeopleCite;Document?/>
</rdf:Property>
<rdfs:Class rdf:about=?&PeopleCite;Organization?>
<rdfs:subClassOf rdf:resource=?&rdfs;Resource?/>
</rdfs:Class>
<rdf:Property rdf:about=?&PeopleCite;OrganizationURI?>
<rdfs:domain rdf:resource=?&PeopleCite;Organization?/>
<rdfs:range rdf:resource=?&rdfs;Literal?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;NameOfOrganization?>
<rdfs:domain rdf:resource=?&PeopleCite;Organization?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
<rdf:Property rdf:about=?&PeopleCite;AddressOfOrganization?>
<rdfs:domain rdf:resource=?&PeopleCite;Organization?/>
<rdfs:range rdf:resource=?&rdfs;Resource?/>
</rdf:Property>
</rdf:RDF>
B One snapshot of our metadata web application
Cross Document Co-Reference Resolution Applications for People in the Legal 
Domain
Christopher Dozier  
Research and Development  
Thomson Legal and Regulatory  
610 Opperman Drive 
Eagan, MN 55123,USA 
chris.dozier@thomson.com 
Thomas Zielund  
Research and Development  
Thomson Legal and Regulatory  
610 Opperman Drive 
Eagan, MN 55123,USA 
tom.zielund@thomson.com 
Abstract
By combining information extraction and re-
cord linkage techniques, we have created a re-
pository of references to attorneys, judges, and 
expert witnesses across a broad range of text 
sources.  These text sources include news, 
caselaw, law reviews, Medline abstracts, and 
legal briefs among others. We briefly describe 
our cross document co-reference resolution al-
gorithm and discuss applications these re-
solved references enable. Among these 
applications is one that shows summaries of 
relationships chains between individuals based 
on their document co-occurrence and cross 
document co-references.  
1 Introduction  
Attorneys, judges, and expert witnesses all play 
important roles in legal systems.  Judges decide 
cases.   Attorneys handle the legal needs of clients.  
Expert witnesses testify about complex facts and 
play an ever-increasing part in the settlement of 
cases.  An important part of an attorney?s prepara-
tion for litigation involves researching the back-
ground of the judge deciding the case, of attorneys 
representing the opposing side, and of testifying 
experts.  To help attorneys with this research need, 
we have created a system that automatically links 
across documents references to attorneys, judges, 
and expert witnesses. These documents include 
news articles, caselaw documents, law reviews, 
Medline abstracts, and legal briefs among others.   
Our method of creating cross document co-
references involves extracting from text MUC type 
templates for individuals and matching the tem-
plates to biographical profile records using a 
Bayesian based record linkage technique.  We have 
described this method in detail elsewhere (Dozier 
and Haschart, 2000; Dozier et al, 2003) and 
briefly describe it in section 2. 
The biographical records for attorneys, judges, 
and expert witnesses that we use for cross docu-
ment co-reference resolution have been created 
through a combination of automatic and manual 
techniques.  The basis of the biographical records 
for attorneys and judges comes from a manually 
created professional directory which is itself a 
product called the Westlaw Legal Directory. The 
biographical records for the expert witnesses were 
created through text mining.  We have described 
the text mining application for creation of the ex-
pert witness database in (Dozier er al., 2003) and 
describe it briefly in sections 3 and 4. 
A new application we have created from these 
cross document references involves creating sum-
maries of relationships between attorneys, judges, 
and expert witnesses.  This new application is dis-
cussed in section 5.
Since their deployment, these applications have 
created automatically over 7 million links between 
references to attorneys, judges, and expert wit-
nesses in various document collections and their 
respective biographical profiles. 
2 Cross Document Co-reference Resolu-
tion through Extraction and Linking
By combining information extraction techniques 
with record linkage techniques, we have been able 
to resolve cross document references for attorneys, 
judges, and expert witnesses.   Our basic technique 
involves extracting a template record for an indi-
vidual from a text document and matching the tem-
plate record to an authority file record.  Figure 1 
depicts how extraction and record linkage are 
combined to create cross document links. 
P e rson  T e m p la te
S tructu red
R e co rd
A u tho rity  P ro file
S truc tu re d  R e cord
R cord  lin ka ge
process
E x trac tio n  o f
te m p la te
record
T e xt D ocum e nt L in ka geR e co rd
C onvers ion  o f
S tru ctu red
R ecord  to T ext
P e rson  P ro file
D ocum en t
Figure 1: Cross Document Coreference Resolution
Process
The extraction portion of our system is similar to 
template extraction systems described in the Mes-
sage Understanding Conferences proceedings
(MUC-6, 1995) and elsewhere (Appelt et al,1993)
(Grishman,1997).  Our extraction process relies on
a finite state machine that identifies paragraphs in a 
document containing attorney, judge, or expert 
names and a semantic parser that extracts from the
paragraphs template information about each indi-
vidual named.
The record linkage portion of our system uses a
Bayesian network to match and link attorney,
judge, and expert templates to biographical re-
cords.  This network computes the probability that
a given biographical record matches the same per-
son specified in an extracted template.  To com-
pute this match likelihood, we treat first name,
middle name, last name, firm, city, state, court, and 
other information as independent pieces of match
evidence.  We compute the prior probability of a
match by calculating the probability that a ran-
domly selected biographical record will match a 
template. We then compute conditional match
probabilities for each piece of evidence using a 
manually tagged training set. For each piece of
evidence, we compute the conditional probability
that a biographical record matches a template when
that piece of evidence matches exactly, matches in 
a strong fuzzy way, matches in a weak fuzzy way,
is unknown, or mismatches.   We define what we 
mean by strong fuzzy and weak fuzzy in (Dozier 
and Haschart, 2000) and (Dozier et al, 2003).  But 
basically we mean that a piece of match evidence
matches in a fuzzy way when it is compatible with 
another piece of evidence but does not match ex-
actly.
We compute the match probability score for the 
records using the following form of Bayes? rule: 
P(M|E) is the probability that the template and 
authority records refer to the same person, given
match evidence E.
P(M) is the prior probability that the reference 
records refer to the same person. P(?M) is the 
prior probability that records do not refer to the 
same person.
P(Ei|M) is the conditional probability that the 
match variable Ei takes on a particular value given 
that the template and authority records match.  For 
example, if we let E1 stand for middle name match
evidence, the probability that the middle names in
the records match exactly given that the records
themselves match is P(E1=exact|M).
P(Ei|?M) is the conditional probability that Ei
takes on a particular value, given that the template
and authority records do not match.  For example,
if we let E1 stand for middle name match evidence, 
the probability that the middle names in the refer-
ence records match, given that the records them-
selves do not match, is P(E1=exact|? M).
A sample caselaw document with highlighted 
links to judges and attorneys is shown in figure 2.
Figure 3 shows caselaw documents linked to the
biographical record for attorney Gerry Spence.
3 Mining Authority Records From Text
For expert witnesses, we created an expert wit-
ness authority file of some 100,000 profiles, mined
from approximately 300,000 jury verdict and set-
tlement documents, using publicly available pro-
fessional license information, an expertise
taxonomy, and automatic text mining techniques. 
This directory can be browsed by area of expertise
as well as by location and name.  The profiles are
automatically linked to Medline abstracts, as well
as back to the relevant jury verdict and settlement 
documents. To the best of our knowledge, this is
the largest expert witness directory of its kind and 
the first to be built using automatic text mining
techniques.
Figure 4 shows the text mining process we used 
to create our authority file of expert witnesses from
jury verdict and settlement documents.
)|()()|()(
)|()(
)|(
??
?
??
 
i
i
i
i
i
i
MEPMPMEPMP
MEPMP
EMP
Figure 2: Caselaw Document. Underscored Names are Hyperlinked to Profiles.
Figure 3: Gerry Spence Authority Record with Coreference Links.  Listed in Left-hand Panel are Cases that 
Reference Gerry Spence. 
Merge Expert
Instances
Merge Professional
License Data
Expert Witness
Authority File
JVS document
collection
Assign Taxonomy
Label
Expert
Taxonomy
Professional
License
Records
Expert Witness
Instance
Records
Extract Experts
from JVS
Match JVS
Experts to Authority
File
Medline article
Figure 4: Text Mining of Expert Witness Profiles 
First, we extract references to expert witnesses
from court proceeding documents.  In our initial 
implementation of the system, we extracted
290,000 reference records to expert witnesses from
300,000 documents.  The method of extraction for
these reference records is essentially the same as
the method described in section 2 for template re-
cords. Reference records in fact are template re-
cords for experts. 
Second, we merge the expert witness reference
records together to create an expert witness profile 
file in which each particular expert is listed only 
once. Section 4 describes the merging process in 
more detail. 
Third, we add professional license information
into the expert witness profile records.  These re-
cords include license information from the Drug 
Enforcement Agency, which licenses health care
professionals to prescribe drugs, and from various
other professional licensing agencies.  To deter-
mine whether a license record and expert record 
refer to the same person, we again apply Bayesian
based record linkage.  The evidence we use to
match license records to profiles includes first
name, middle name, last name, name suffix, city-
state information, area of expertise, and name rar-
ity.
Fourth, we automatically assign each expert wit-
ness record one or more classification categories in
an expertise taxonomy.
Finally, we link court documents and Medline 
abstracts to expert witness profile records using 
Bayesian based record linkage for a third and 
fourth time.
Figure 5 below shows a jury verdict and settle-
ment document.  Note that the reference to expert
witness oncologist Arthur Ablin is highlighted. 
Figure 6 shows the authority file record for Arthur
Ablin to which the hypertext link in the jury ver-
dict and settlement document is linked.  Figure 7 
shows the cases Dr. Ablin has testified in as well 
as Medline articles Dr. Ablin has authored.
4 Merging Reference Records to Create 
Authority File 
To create our directory of expert profiles (i.e., 
expert authority file) from expert reference re-
cords, we need to create a set of records that list a
particular expert one and only one time. This 
means we need to merge together all the reference
records that pertain to each individual.
As a first step in merging the expert reference
records, we group the reference records into sets in
which each record in a group shares a common last
name.  By doing this, we avoid the computational
cost of comparing every expert witness reference
record to every other reference record.
To merge expert references together within the 
groups, we use the following greedy algorithm:
1. Select an unmerged expert reference record from
the group. Create an expert authority record from 
this record. Mark the expert reference record as 
merged.
2. Compare the new expert authority record to each 
unmerged reference record in the group.  In each
comparison, compute the probability that the ex-
pert in the authority record refers to the same indi-
vidual referenced in the reference record.  Use 
Bayesian matching to compute this match probabi-
lity.  If the match probability exceeds a match
threshold, mark the reference record as ?merged?.
Note that the match threshold probability is deter-
mined empirically from training data.
3. If any unmerged records remain in the group, 
then return to step 1 else halt.
Note that, at this stage, it is still possible for 
there to be duplicate records in the authority file, if 
two or more reference records pertain to the same 
individual but have variant last name spellings. 
While this is not common, it can happen when a 
source document contains a misspelled last name. 
To address this situation, we make a final pass over
the merged authority file and flag record pairs for 
manual review when the last names of two records 
are separated from each other by an edit distance
of two or less and when the records match in all 
other respects.
5 Relationship Summaries
Using the links between documents and entities 
such as judges, attorneys, and expert witnesses, we
are creating a new set of products that automati-
cally summarize relevant relationships among
these entities.  For example, by following co-
Figure 5: Jury Verdict and Settlement Document with Hyper Links to Attorneys, Judges, and Expert Wit-
nesses including Arthur Ablin. 
Figure 6: Profile of Oncology Expert Arthur Ablin That Was Created Through Text Mining.
Figure 7: Documents Pertaining to Arthur Ablin. 
reference and co-occurrence links, automatic re-
ports can be generated that show which attorneys
have hired which expert witnesses, which attorneys
have worked together, which attorneys have ap-
peared before which judges, which experts have 
co-authored papers, and so on. 
Figures 8, 9, and 10 show a summary of the rela-
tionships between U.S. Chief Justice William H.
Rehnquist and some other attorneys and judges
who have been identified in documents with him.
The summary shows some of the close working 
relationships one would expect to find for the chief 
justice.  Note, however, that these relationships
were discovered through cross-document co-
reference, not through an editorial process.
6 Discussion 
Our cross document co-reference resolution 
method works by attaching a person referenced in 
a document to a profile record that stands for a 
unique real world person outside the frame of the 
document.   Person references from multiple
documents that attach to the same profile are re-
solved by this common attachment.
In the case of attorneys and judges, we used an
existing directory of attorneys and judges as the 
source of our profile records.  In the case of ex-
perts, we created profiles by mining references 
from highly structured and trustworthy documents
such as jury verdict and settlement documents and 
professional license records.
When more than one person in a single docu-
ment can be attached to individual profiles and the 
relationship between the references in the docu-
ment can be also extracted, then a relationship re-
cord can also be created outside the frame of the
document and these relationships can then be
chained together to generate relational inferences.
This is the technology we are currently exploring
within the legal domain and beyond.
There is a popular belief that between any two
people, there are around six degrees of separation
or less.  Although (Dodds et al, 2003) and 
(Travers and Milgram, 1969) provide some em-
pirical evidence in support of this theory, the popu-
larity of the theory spawns more from a popular 
trivia game in which you attempt to find a path to 
Kevin Bacon from any Hollywood star through co-
starring roles (see oracleofbacon.org for an on-line 
demo).
Using the database of links built from cross 
document co-reference resolution on legal docu-
ments, a similar technique could be used to find
paths of co-occurrence between arbitrary pairs of 
U. S. legal professionals.  Although we do not pic-
ture any popular party games developing from this, 
such techniques could prove useful for activities 
such as detecting conflicts of interest among peo-
ple working on a legal matter. 
Judge Summary 
x Name: William H. Rehnquist
x From: United States Supreme Court
x Location: Washington, DC
Links from Other WestLaw Content
x Case Law: 847
x Oral Arguments: 10
Figure 8: Link Summary Statistics for William H. Rehnquist 
1 Judges Appearing with William H. Rehnquist 
Name Court Location
Co-
Appearances
Antonin Scalia United States Supreme Court Washington, DC 782
David H. Souter United States Supreme Court Washington, DC 769
John Paul Stevens United States Supreme Court Washington, DC 690
Anthony M. Kennedy United States Supreme Court Washington, DC 682
Clarence Thomas United States Supreme Court Washington, DC 621
Sandra Day O?Connor United States Supreme Court Washington, DC 596
Ruth Bader Ginsburg United States Supreme Court Washington, DC 534
Stephen G. Breyer United States Supreme Court Washington, DC 471
Byron R. White United States Supreme Court Washington, DC 145
Thurgood Marshall United States Supreme Court Washington, DC 48
10 most frequently co-appearing of 27 records displayed.
Figure 9: Judges Appearing with Justice William H. Rehnquist
Attorneys Appearing with William H. Rehnquist
Name Firm
1.1 Location Co-
Appearances
Edwin S. Kneedler Justice Dept. Solicitor General Washington, DC 40
Theodore B. Olson U.S. Department Of Justice Washington, DC 36
Michael R. Dreeben Justice Dept. Solicitor General Washington, DC 34
Lawrence G. Wallace Justice Dept. Solicitor General Washington, DC 26
Kent L. Jones Justice Dept. Solicitor General Washington, DC 25
James A. Feldman Justice Dept. Solicitor General Washington, DC 19
Warren Price, III Price, Okamoto, Himeno & Lum Honolulu, HI 18
Charles M. Oberly, III Oberly & Jennings, P.A. Wilmington, DE 18
Kenneth W. Starr Kirkland & Ellis Llp Washington, DC 16
Malcolm L. Stewart Justice Dept. Solicitor General Washington, DC 16
10 most frequently co-appearing of 1792 records displayed.
Figure 10: Attorneys Appearing with Justice William H. Rehnquist
We have described the precision and recall of the 
programs that create cross document co-references 
for attorneys and judges in (Dozier and Haschart, 
2000) and that create co-references for experts in 
(Dozier et al, 2003).  In general, for collections in 
which documents reference people within stereo-
typical syntax, attach location and job type infor-
mation to the person, and usually include the 
person?s full first name (e.g., attorney names and 
expert witness names in caselaw), we typically 
achieve better than 0.98 precision and better than 
0.95 recall.  For collections in which documents do 
not give full name or do not have highly stereo-
typical syntax, the precision and recall perform-
ance is worse.  For example, when tagging expert 
witness references in Medline, where an author?s 
first initial is given in place of the full form of the 
first name and the author?s job type must be in-
ferred from the topic of the scientific article, we 
currently achieve precision of  better than 0.95 and 
recall of around 0.60. 
7 Conclusion  
By combining information extraction and record 
linkage techniques, we have created a repository of 
references to attorneys, judges, and expert wit-
nesses across a broad range of text sources.  These 
text sources include news, caselaw, law reviews, 
Medline abstracts, and legal briefs among others. 
We briefly describe our cross document co-
reference resolution algorithm and discuss applica-
tions these resolved references enable. Among 
these applications is one that shows summaries of 
relationships chains between individuals based on 
their document co-occurrence and cross document 
co-references
References
Appelt, D. E., Hobbs, J. E., Bear, J., Israel, D., Ty-
son, M. (1993).  FASTUS: A Finite-State Proc-
essor for Information Extraction from Real-
World Text.  In Proceedings of the International 
Joint Conference on Artificial Intelligence, pages 
1172-1178. 
Dodds, P., Muhammed, R. & Watts, D.  An Ex-
perimental Study of Search in Global Social 
Networks.  Science, Vol 301, Issue 5634, 8 Au-
gust 2003, pages 827-829. 
Dozier, C. and Haschart, R., (2000) Automatic Ex-
traction and Linking of Person Names in Legal 
Text. In Proceedings of RIAO-2000 (Recherche 
d?Informations Assistee par Ordinator), pages
1305-1321, Paris, France.  
Dozier, C., Jackson. P., Guo, X., Chaudhary, M., 
and Arumainayagam, Y. (2003) Creation of an 
Expert Witness Database Through Text Mining. 
In Proceedings of the 9th International Confer-
ence on Artificial Intelligence and Law., pages 
177-184, Edinburgh, Scotland, UK. 
Grishman, R. (1997). Information Extraction: 
Techniques and Challenges. In Information Ex-
traction : A Multidisciplinary Approach to an 
Emerging Information Technology : Interna-
tional Summer School, Frascati, pages 13-27, 
Spring Verlag, Frascati, Italy.  
Travers, Jeffrey and Milgram, Stanley (1969). An 
Experimental Study of the Small World Prob-
lem. Sociometry, Volume 32, Issue 4 (Dec., 
1969), pages 425-443.  
Learning Transformation Rules
for Semantic Role Labeling
Ken Williams Christopher Dozier
Research & Development
Thomson Legal and Regulatory
Eagan, MN 55123 USA
{ken.williams|chris.dozier|andrew.mcculloh}@thomson.com
Andrew McCulloh
Abstract
This paper presents our work on Semantic Role
Labeling using a Transformation-Based Error-
Driven approach in the style of Eric Brill (Brill,
1995). Our approach achieved an overall F1
score of 43.48 on non-verb annotations. We
believe our approach is noteworthy because of
its novelty in this area and because it produces
short lists of human-understandable transfor-
mation rules as its output.
1 Introduction to Transformation-Based
Error-Driven Learning
For the 2004 Conference on Computational Natural Lan-
guage Learning (CoNLL), our team has applied the
methodology popularized by Eric Brill for part-of-speech
tagging and linguistic parsing (Brill, 1995; Brill, 1993).
In this methodology, illustrated in Figure 1, a system
learns a sequence of rules that best labels training data.
These rules are then used to annotate previously unseen
data.
According to (Brill, 1995), a Transformation-Based
Error-Driven learning application is defined by:
1. The initial annotation scheme
2. The space of allowable transformations
3. The iterative algorithm for choosing a transforma-
tion sequence
The initial annotation may be extremely simple. For
example, in a part-of-speech tagging task, the initial an-
notation may assign each token its most likely tag without
any regard to context (Brill, 1995).
The iterative learning algorithm typically consists of
simply searching for a rule that maximizes the increase
in some objective function using a greedy hill-climbing
Unannotated text
Initial
annotation
Learned 
annotation
Truth 
annotation
Learner
Transformation 
rules
Figure 1: Overview of general Transformation-Based
Error-Driven learning
strategy. For the CoNLL shared task, since participants
are evaluated by their F1 scores, it is reasonable to use
the F1 score as an objective function. We also imple-
mented some extensions to the hill-climbing strategy that
we describe in Section 2.3.
2 Experimental Setting
In our approach, we used three successive learning
stages?the first stage tags the verb region V, the second
tags the A0 and A1 arguments, and the third tags all re-
maining arguments. The output of each stage becomes
the initial annotation for the following stage. Therefore,
our system only defines an explicit initial annotation for
the verb-tagging phase: for each proposition, we initially
tag only the single token containing the verb as V.
The search for new transformation templates is ter-
minated when no new transformation can be found that
would improve the objective function by at least 0.03%.
2.1 Transformation templates
For the first stage, transformations are generated from the
following eight transformation templates:
Lengthen [shorten] the end of region1 V by one token if:
a,b) followed by chunk with tag=X
c,d) followed by token with POS2=X
e,f) followed by chunk with tag=X and token with
POS=Y
g,h) the verb token?s lemma is X
In this formulation, ?chunk? refers to the IOB2 chunks,
and ?clause? refers to the nested clause structure (S re-
gions) given as task input. ?Lemma? refers to the in-
finitive form of the verb, identified in the task input and
coreferenced in the PropBank data. X and Y are vari-
ables that range over all types of chunks, POS tags, or
lemmas. The rule-learning system must determine which
values for these variables will produce the most effec-
tive transformations. For example, a rule that the system
might produce from template ?e? is:
Lengthen the end of region V by one token if
the region is followed by chunk with tag=PRT
and token with POS=RP.
Based on the observation that all V regions in the train-
ing data were either one or two tokens in length, an ad-
ditional constraint was added to the first stage, requiring
that lengthening-rules only apply to regions of length one,
and shortening-rules only apply to regions of length two.
The second and third stages use a common set of
eleven transformation templates, but in the second stage
the learner is restricted to adding or altering only A0 and
A1 regions. The transformation templates are as follows:
A,B) If chunk with tag=X is followed [preceded] directly
by region with tag=Y , mark chunk as Z.
C,D) If token with POS=X is followed [preceded] di-
rectly by region with tag=Y , mark token as Z.
E,F) If chunk with tag=X is followed [preceded] (per-
haps indirectly) by region with tag=Y , mark chunk
as Z.
G,H) If region with tag=X is followed [preceded] by
chunk with tag=PP, which is in turn followed [pre-
ceded] by chunk with tag=Y , extend X forward
[backward] through Y .
1In this paper, we use the term ?region? to refer to a section
of corpus text that has been labeled in the output as a verb or
as a verb argument. We also use the term in rule definitions to
refer to the type of label assigned to that section of text.
2part-of-speech
I,J) If verb?s first token has POS=X [and is preceded by
POS=Y ], switch A0 and A1.
K) If region with tag=X is contained in a clause-
starting verb phrase, and this is preceded by a
clause-starting token with POS=Y , mark token Y as
Z.
Templates ?A-H? are meant to capture structural rela-
tionships among arguments, such as the fact that A1 re-
gions usually follow V regions, or that arguments may
consist of several NP chunks joined by PP chunks. Tem-
plates ?I? and ?J? were written to discover passive verb
relationships. Template ?K? was an explicit (admittedly
ad hoc) attempt to recognize R-A0 and R-A1 arguments.
To avoid creating tagged regions that overlap, we use
a first-tag-wins strategy: if a transformation would tag a
new region that overlaps an existing tagged region, the
new region is trimmed until any overlaps vanish.
Notice that unlike the templates in the first stage, these
templates make no reference to lexical information. In
particular, no rule takes advantage of PropBank data in its
tagging process3. We anticipate that using PropBank data
would potentially improve performance, but we have not
yet experimented with it. Also, without any lexical infor-
mation in these templates, we are capturing only general
patterns of argument structure within the training corpus,
not the statistical patterns of particular verb frames. In
future experiments we expect to incorporate lexical data
into transformation rules.
2.2 Arguments and clause structure
In order to gain some traction on the problem, we an-
alyzed the relationship between semantic arguments and
clause boundaries. To investigate this, we labeled each ar-
gument with the smallest clause containing it as a proper
subset. We then tallied the number of each type of ar-
gument labeled with the same clause as its verb, and the
number labeled with a different clause. The results are
shown in Table 1.
Note that for almost all argument types, the over-
whelming majority of arguments are found in the same
clause as the verb. This motivated us to add an additional
constraint to the transformation templates A-J: only cre-
ate arguments in the same clause as the verb. This sim-
plification necessarily will miss any legitimate arguments
outside the clause (most notably 20% of A0 arguments).
2.3 Reordering of learned rules
In observing the sequence of transformations learned by
the system, it became apparent that the system?s strict
3We actually do use PropBank in a limited way: no trans-
formation will assign an argument A0-A5 to a verb unless that
argument is listed for one of the verb?s senses in PropBank.
Same Different Percent
Tag clause clause same
A1 16896 1150 93.6%
A0 10134 2575 79.7%
A2 4022 201 95.2%
AM-TMP 3329 238 93.3%
AM-MOD 1752 1 99.9%
AM-ADV 1675 52 97.0%
AM-MNR 1277 60 95.5%
AM-LOC 1184 95 92.6%
AM-DIS 1042 35 96.8%
A3 758 26 96.7%
AM-NEG 687 0 100.0%
A4 625 1 99.8%
AM-PNC 432 14 96.9%
C-A1 313 129 70.8%
AM-CAU 261 22 92.2%
AM-DIR 228 3 98.7%
AM-EXT 150 2 98.7%
R-A0 10 728 1.4%
R-A1 7 353 1.9%
Table 1: Verb-argument clause agreement on training
data (arguments with fewer than 50 examples omitted)
greedy-hill-climbing strategy often learned a non-optimal
ordering of rules. This is because the system has no look-
ahead capability to check whether a sequence of multiple
rules applied in succession might produce a good final
result despite providing little or no initial improvement.
The addition of a look-ahead searcher has been sug-
gested (Brill, 1995), but we have not seen it implemented
in a research context, likely due to the fact that a straight-
forward implementation of the concept would at mini-
mum square the amount of time required for training.
Instead, we implemented a look-behind search strat-
egy, which allows rules to be reordered after discovery. It
is meant to address the case in which the system learns
a set of rules that each produce improvements in the tar-
get function, but interact with each other in a non-optimal
way. Whenever our system discovers a new rule, rather
than simply applying it and searching for the next rule,
it is allowed to try all permutations of the last n discov-
ered rules to see whether performance would improve by
using a different ordering. If so, the rules are re-ordered.
To our knowledge, this strategy has not been employed
in Transformation-Based Error-Driven learning settings.
In our experiments, the strategy discovered transforma-
tion sequences that better annotated the input data with-
out using more rules, and therefore seems to produce a
labeler less likely to overfit the training data. In our test-
ing, the technique seems to have increased the overall F1
score by between 0.5% and 1.0%?we caution, however,
Precision Recall F?=1
Overall 57.73% 34.35% 43.07
A0 60.92% 52.98% 56.67
A1 53.90% 45.18% 49.16
AM-MOD 99.81% 58.59% 73.83
AM-NEG 44.89% 77.29% 56.79
AM-TMP 38.34% 6.36% 10.92
R-A0 64.61% 76.69% 70.14
V 99.19% 99.19% 99.19
Table 2: Annotation agreement on training data (rows
with all-zero entries omitted)
that we have not undertaken a rigorous comparative study
of the technique.
3 Results
The quality of our transformation rules on the training set
is shown in Table 2, and the results on the test set are
shown in Table 3. The rules that generated these results
are shown in Table 4, along with the iterative F1 scores
on the training set as the rules are learned.
4 Discussion
First, note that only one rule was learned in the verb-
tagging phase: Lengthen region V if followed by chunk
with tag=PRT. With earlier releases of the data the sys-
tem did learn multiple rules, including lexically-based
rules, but in later releases only this one rule was learned.
Second, observe that the system actually did reorder
rules after discovering them, as evidenced by the non-
monotonic ?discovery order? column. To attain this re-
sult, we used a look-behind of 2, i.e. the last 3 rules
learned were candidates for reordering.
Third, several of the rules in the sequence are identical.
In some cases, this seems to be because multiple applica-
tions of a rule were necessary to achieve full results (e.g.
rule ?H?, which extended an A0 or A1 region through
joined NP chunks several times). In other cases, this
seems to be one rule re-applying itself after another rule
modified the results of its earlier application (e.g. rule
?E?, which was affected by applications of rule ?H?).
Finally, note that only 23 transformations were found.
The last few rules begin dealing with lesser-represented
argument types like R-A0 and AM-NEG, but many types
remain completely unaddressed by the system. We may
be able to increase performance on those types by adding
additional rule templates, or by decreasing the learning
termination threshold for the system. Rule ?K? was cre-
ated as an explicit attempt to recognize R-A0 and similar
argument types, and seems to have been reasonably suc-
cessful. There may be other relatively simple templates
we can create to recognize other arguments.
Precision Recall F?=1
Overall 58.08% 34.75% 43.48
A0 60.26% 52.73% 56.24
A1 54.53% 44.56% 49.05
A2 0.00% 0.00% 0.00
A3 0.00% 0.00% 0.00
A4 0.00% 0.00% 0.00
A5 0.00% 0.00% 0.00
AM-ADV 0.00% 0.00% 0.00
AM-CAU 0.00% 0.00% 0.00
AM-DIR 0.00% 0.00% 0.00
AM-DIS 0.00% 0.00% 0.00
AM-EXT 0.00% 0.00% 0.00
AM-LOC 0.00% 0.00% 0.00
AM-MNR 0.00% 0.00% 0.00
AM-MOD 100.00% 56.68% 72.35
AM-NEG 48.34% 80.31% 60.36
AM-PNC 0.00% 0.00% 0.00
AM-PRD 0.00% 0.00% 0.00
AM-TMP 40.48% 6.83% 11.68
R-A0 66.45% 64.78% 65.61
R-A1 0.00% 0.00% 0.00
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.21% 98.21% 98.21
Table 3: Results on test data
In future work, there are several avenues we would like
to explore. Our first-tag-wins assignment strategy men-
tioned above is not grounded in research into alternate
strategies, and in fact we have not yet tried any others.
We also experimented with isolating common verb
types into their own corpus?for example, if we train sep-
arately on the verb ?say,? which represents nearly 10% of
the target verbs in the training set and exhibits different
argument patterns from other verbs, we achieve an F1
value of about 82% on this subset using only five learned
rules. It may be possible to leverage this work by group-
ing other less common verbs by their VerbNet class(es).
5 Conclusion
We have described a Transformation-Based Error-Driven
learning approach to the CoNLL shared task on seman-
tic role labeling. Although we are relative newcomers to
this task and this approach has not to our knowledge been
applied to it before, we believe our results are of general
interest for the following reasons.
First, the learned output of the system is highly
Discovery
Rule Parameters order F1
initial annotation 0.00
a PRT 1 0.00
E NP, V, A0 2 20.20
I VBN 5 24.60
B NP, V, A1 3 31.60
B S, V, A1 4 35.82
H A0, NP 6 36.44
D NN, V, A1 9 36.67
D NNS, V, A1 10 36.86
E NP, V, A1 8 37.36
J VBZ, ? 11 37.46
E S, V, A1 7 38.55
H A1, NP 12 38.64
H A0, NP 13 38.71
H A1, NP 14 38.78
J VBD, ? 15 38.83
E S, V, A1 16 39.07
G A0, NP 17 39.09
H A1, NP 18 39.11
C MD, V, AM-MOD 19 41.23
K V, WDT, R-A0 20 41.78
K V, WP, R-A0 21 42.21
A ADVP, V, AM-TMP 22 42.46
C RB, V, AM-NEG 23 43.16
Table 4: Rules learned for semantic role labeling
scrutable, in the sense that the transformation rules can
easily be reviewed and understood by a human supervi-
sor. This may benefit real-world application of the tech-
nique as rules may be manually reordered, switched on
or off, or modified. It also allows a developer to closely
monitor changes in the system, creating new rules as he
or she identifies areas of the data that are being under-
served by the current set of transformation templates.
Second, as alluded to above, there are several appeal-
ing directions to direct future research, and we believe the
results obtained here can be significantly improved.
Third, we know of no previous work using our look-
behind reordering technique in conjunction with rule-
based learning, and the technique may have broad appli-
cability beyond semantic role labeling.
References
Eric Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, Philadelpha, PA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
