Cognate Mapping ? A Heuristic Strategy for the Semi-Supervised
Acquisition of a Spanish Lexicon from a Portuguese Seed Lexicon
Stefan Schulza,b Korne?l Marko?b,c Eduardo Sbrissiaa Percy Nohamaa Udo Hahnc
aMaster Program in Health Technology, Parana? Catholic University, Curitiba, Brazil
bDepartment of Medical Informatics, Freiburg University Hospital, Germany
cComputational Linguistics Research Group, Jena University, Germany
http://www.coling.uni-freiburg.de/
Abstract
We deal with the automated acquisition of a
Spanish medical subword lexicon from an al-
ready existing Portuguese seed lexicon. Using
two non-parallel monolingual corpora we de-
termined Spanish lexeme candidates from Por-
tuguese seed lexicon entries by heuristic cog-
nate mapping. We validated the emergent lex-
ical translation hypotheses by determining the
similarity of fixed-window context vectors on
the basis of Portuguese and Spanish text cor-
pora.
1 Introduction
Medical language presents a unique combination
of challenges for language engineering, with a fo-
cus on applications such as information retrieval,
text mining and information extraction. Document
collections ? on the Web or in clinical databases
? are usually very large and dynamic. In addi-
tion, medical document collections are truly multi-
lingual. Furthermore, the user population which ac-
cess medical documents are really diverse, ranging
from physicians and nurses to laypersons, who use
different jargons and sublanguages. Therefore, the
simplicity of the content representation of the docu-
ments, as well as automatically performed intra- and
interlingual lexical mappings or transformations of
equivalent expressions, become crucial issues for an
adequate machine support.
We respond to these challenges in terms of
the MORPHOSAURUS system (an acronym for
MORPHeme TheSAURUS). It is centered around a
new type of lexicon, in which the entries are sub-
words, i.e., semantically minimal, morpheme-style
units (Schulz and Hahn, 2000). Intralingual as well
as interlingual synonymy is then expressed by the
assignment of subwords to concept-like equivalence
classes. As subword equivalence classes abstract
away from subtle particularities within and between
languages, and reference to them is achieved via
a language-independent code system, they form an
interlingua characterized by semantic identifiers.
Compared to relationally richer, e.g., WORDNET
based, interlinguas as applied for cross-language in-
formation retrieval (CLIR) (Gonzalo et al, 1999;
Ruiz et al, 1999), we use a rather limited set of
semantic relations and pursue a more restrictive ap-
proach to synonymy. In particular, we restrict our-
selves to the specific sublanguage used in the con-
text of the medical domain. Our claim that this
interlingual approach is useful for the purpose of
cross-lingual text retrieval and categorization has al-
ready been experimentally supported (Schulz et al,
2002; Marko? et al, 2003).
The quality of cross-lingual indexing fundamen-
tally depends on the underlying lexicon and the-
saurus. Its manual construction and maintenance
is costly and error-prone. Therefore, machine-
supported lexical acquisition techniques increas-
ingly deserve attention. Whereas in the medical do-
main parallel corpora are only available for a lim-
ited number of language pairs, unrelated (i.e., non-
parallel, non-aligned) corpora might provide suffi-
cient evidence for cognate identification, at least in
languages which are closely related.
In this paper, we present the results of such an ex-
periment. We have chosen Spanish and Portuguese
as a pair of closely related languages. Both lan-
guages exhibit a high degree of similarity in their
lexical inventory, as well as in the rules govern-
ing word formation. Accordingly, a Portuguese na-
tive speaker is able to understand technical texts in
Spanish without much effort, and vice versa. In
both languages, there is also an increasing number
of electronic texts available, so that a cross-lingual
search interface would significantly improve the ac-
cessibility of domain relevant documents.
2 Lexicographic Aspects of
Morpho-Semantic Indexing
We briefly outline the lexicographic and semantic
aspects of our approach, called Morpho-Semantic
Indexing (henceforth, MSI), which translates source
documents (and queries) into an interlingual repre-
sentation in which their content is represented by
language-independent semantic descriptors.
2.1 Subwords as Lexicon Units
Our work is based on the assumption that neither
fully inflected nor automatically stemmed words
constitute the appropriate granularity level for lex-
icalized content description. Especially in scien-
tific and technical sublanguages, we observe a high
frequency of domain-specific and content-bearing
suffixes (e.g., ?-itis?, ?-ectomia? in the medical do-
main), as well as the tendency to construct ut-
terly complex word forms such as ?pseudo?hypo?
para?thyroid?ism?, ?gluco?corticoid?s?, or ?pan-
creat?itis?.1 In order to properly account for the
particularities of ?medical? morphology, we in-
troduced subwords (Schulz et al, 2002) as self-
contained, semantically minimal units and moti-
vated their existence by their usefulness for docu-
ment retrieval rather than by linguistic arguments.
The minimality criterion is quite difficult to de-
fine in a general way, but its implications can
be illustrated by the following example. Given
the text token ?diaphysis?, a linguistically plausible
morpheme decomposition would possibly lead to
?dia?phys?is?. From a medical perspective, a seg-
mentation into ?diaphys?is? seems much more rea-
sonable, because the linguistically canonical mor-
phological decomposition is far too fine-grained
and likely to create too many ambiguities. For in-
stance, comparable ?low-level? segmentations of se-
mantically unrelated tokens such as ?dia?lyt?ic?,
?phys?io?logy? lead to morpheme-style units ?dia?
and ?phys?, which unwarrantedly match segmenta-
tions such as ?dia?phys?is?, too. The (semantic)
self-containedness of the chosen subword is often
supported by the existence of a synonym, e.g., for
?diaphys? we have ?shaft?.
2.2 Subword Lexicon and Thesaurus
Subwords are assembled in a multilingual lexicon
and thesaurus, which contain subword entries, spe-
cial subword attributes and semantic relations be-
tween subwords. Up until now, the lexicon and
the thesaurus have both been constructed manually,
with the following considerations in mind:
? Subwords are entered, together with their at-
tributes such as language (English, German,
Portuguese) and subword type (stem, prefix,
suffix, invariant). Each lexicon entry is as-
signed a unique identifier representing one
synonymy class, the MORPHOSAURUS iden-
tifier (MID), which contains this entry as its
unique member.
1
??? denotes the concatenation operator.
? Synonymy classes which contain intralingual
synonyms and interlingual translations of sub-
words are fused. We restrict intra- and inter-
lingual semantic equivalence to the context of
medicine.
? Semantic links between synonymy classes are
added. We subscribe to a shallow approach
in which semantic relations are restricted to a
paradigmatic relation has-meaning, which re-
lates one ambiguous class to its specific read-
ings,2 and a syntagmatic relation expands-to,
which consists of predefined segmentations in
case of utterly short subwords.3
We refrain from introducing hierarchical rela-
tions between MIDs, because such links can be ac-
quired from domain-specific vocabularies, e.g., the
Medical Subject Headings (MESH, 2001).
Table 1 depicts how source documents (cf. the
first column with an English and Portuguese frag-
ment) are converted into an interlingual represen-
tation by a three-step procedure. First, each in-
put word is orthographically normalized in terms
of lower case characters and according to language-
specific rules for the transcription of diacritics (sec-
ond column). Next, words are segmented into se-
quences of semantically plausible sublexical items
according to the subwords listed in the lexicon (third
column). Finally, each meaning-bearing subword
is replaced by its language-independent semantic
identifier, the MID, which unifies intralingual and
interlingual (quasi-)synonyms. Then, the system
yields the interlingual output representation of the
system (fourth column).
The manual construction of the trilingual sub-
word lexicon and the subword thesaurus has con-
sumed, up until now, three and a half person
years. The project originally started from a bilin-
gual German-English lexicon, while the Portuguese
part was added in a later project phase. The com-
bined subword lexicon contains 58,479 entries,4
with 21,397 for English, 22,053 for German, and
15,029 for Portuguese.
Taking into account, on the one hand, the out-
standing importance of Spanish as a major Western
2For instance, {head} ? {zephal,kopf,caput,cephal,cabec,
cefal} OR {leader,boss,lider,chefe}
3For instance, {myalg} ? {muscle,muskel,muscul} ?
{schmerz, pain,dor}
4Just for comparison, the size of WORDNET assem-
bling the lexemes of general English in the 2.0 version is
on the order of 152,000 entries (http://www.cogsci.
princeton.edu/?wn/doc.shtml, last visited on Jan-
uary 3, 2004). Linguistically speaking, the entries are basic
forms of verbs, nouns, adjectives and adverbs.
Original Orthographic Morphological Semantic
Document Normalization Segmentation Normalization
High TSH values suggest
the diagnosis of primary
hypothyroidism while
a suppressed TSH level
suggests hyperthyroidism.
high tsh values suggest the
diagnosis of primary hy-
pothyroidism while a sup-
pressed tsh level suggests
hyperthyroidism.
high tsh value s suggest the
diagnos is of primar y hypo
thyroid ism while a sup-
press ed tsh level suggest s
hyper thyroid ism.
#up# tsh #value#
#suggest# #diagnost#
#primar# #small# #thyre#
#suppress# tsh #nivell#
#suggest# #up# #thyre# .
A presenc?a de valores
elevados de TSH sug-
ere o diagn o?stico de
hipotireoidismo prim a?rio,
enquanto n ??veis suprim-
idos de TSH sugerem
hipertireoidismo.
a presenca de valores ele-
vados de tsh sugere o diag-
nostico de hipotireoidismo
primario, enquanto niveis
suprimidos de tsh sugerem
hipertireoidismo.
a presenc a de valor es ele-
vad os de tsh suger e o di-
agnost ico de hipo tireoid
ismo primari o, enquanto
niveis suprimid os de tsh
suger em hiper tireoid ismo.
#actual# #value# #up#
tsh #suggest# #diagnost#
#small# #thyre# #primar#
, #nivell# #suppress# tsh
#suggest# #up# #thyre# .
Table 1: Morpho-Semantic Indexing Example for English (row 1) and Portuguese (row 2): The original document
(column 1) is orthographically transformed (column 2), segmented according to the subword lexicon (column 3), while
content-bearing subwords are mapped to MSI-specific equivalence classes whose identifiers (MIDs) are automatically
generated by the system (column 4). (Bold MIDs co-occur in both documents.)
language and, on the other hand, the close lexical
ties between Portuguese and Spanish as Romance
languages, we intended to augment the existing
MORPHOSAURUS system by Spanish as its fourth
language and at the same time reuse the knowledge
of Portuguese for the purpose of speeding up and
facilitating the Spanish lexicon acquisition.
3 Experiments
We use the following resources for the experiments:
? A Portuguese subword lexicon, as described in
the previous section.
? A manually created list of 842 Spanish affixes.
? Medical corpora for Spanish and Portuguese.
These corpora were compiled exploiting het-
erogeneous WWW sources. The size of the
acquired corpora amounts to 2,267,841 tokens
with 118,021 types for Spanish and 3,406,589
tokens with 133,146 types for Portuguese.
? Word frequency lists generated from these cor-
pora, for Spanish and Portuguese.
3.1 Spanish Subword Generation
In order to acquire a first-shot Spanish subword lex-
icon we designed the following lexeme generation
strategy: Using the Portuguese lexicon, identical
and similarly spelled Spanish subword candidates
(cognates) are generated. As an example, the Por-
tuguese word stem ?estomag? (?stomach?) is identi-
cal with its Spanish cognate. An example for a pair
of similar stems is ?mulher? (?woman?) (Portuguese)
vs. ?mujer? (Spanish). Similar subword candidates
Rule Portuguese Spanish
(P ? S) Example Example
qua ? cua quadr cuadr
eia ? ena veia vena
ss ? s fracass fracas
lh ? j mulher mujer
lh ? ll detalh detall
l ? ll lev llev
i ? y ensai ensay
f ? h formig hormig
+ca ? za cabeca cabeza
+o+ ? ue sort suert
... ... ...
Table 2: Sample of Portuguese-to-Spanish String Sub-
stitution Rules
were generated by applying a set of string substitu-
tion rules some of which are listed in Table 2. In to-
tal, we formulated 45 rules as a result of identifying
common-language Portuguese-Spanish cognates in
a commercial dictionary. Some of these substitu-
tion patterns cannot be applied to starting or end-
ing sequences of characters in the Portuguese source
subword. These regularities are captured by using a
wildcard (?+? in Table 2) representing at least one
arbitrary character.
First, for each Portuguese lexicon entry (n =
14,183 stems and invariants, excluding affixes),
all possible Spanish variant strings were generated
based upon the set of string substitution rules. This
led, on the average, to 9.53 Spanish variant hypothe-
ses per Portuguese subword entry (ranging from 5.3
variants for high-frequency four-character words to
355.2 for low-frequency 17-character words). All
these candidates were subsequently compared to
the Spanish word frequency list, we had previously
compiled from our Spanish text corpus. Wherever
a left-sided string match (in the case of stems) or
an exact one (in the case of invariants) occurred,
the matching string was listed as a potential Span-
ish cognate of the Portuguese subword it originated
from. Whenever several Spanish substitution alter-
natives for a Portuguese subword had to be con-
sidered (cognate ambiguity) that particular one was
chosen which had the closest relative distribution
in the corpus-derived Spanish word frequency list,
when compared to its Portuguese equivalent in the
Portuguese word list. As a result, we obtained a list
of tentative Spanish subwords each linked by the as-
sociated MIDs to its corresponding cognate in the
Portuguese lexicon.
Quantitatively, starting from 14,183 Portuguese
subwords, a total of 132,576 Spanish subword can-
didates were created using the string substitution
rules. Matching these Spanish candidates against
the Spanish corpus and allowing for a maximum of
one Spanish candidate per Portuguese subword, we
identified 11,206 tentative Spanish cognates (79%
of the Portuguese seed lexicon) which are linked to
a total of 8,992 MIDs from their Portuguese corre-
lates (hence, 2214 synonym relationships have also
been hypothesized). 2,977 generated items could
not be found in the Spanish corpus, at all.
3.2 Manual Semantic Validation
One of the authors evaluated manually a random
sample of 388 (3.5% of all generated) cognate pairs
in order to identify false friends, i.e., similar words
in different languages with different meanings. In
our sample we found, e.g., the Spanish candidate
*?crianz? for the Portuguese ?crianc? (the normal-
ized stem of ?crianc?a?; English: ?child?). The cor-
rect translation of Portuguese ?crianc? to Spanish,
however, would have been ?nin? (the stem of ?nin?o?),
whilst the Spanish ?crianz? refers to ?criac? (stem of
?criac?a?o? in Portuguese; English: ?breed?). Taking
these false friend errors into account, the automatic
generation of Portuguese-Spanish cognate pairs still
yields 89,4% accuracy.
Assuming then that approximately 1,188 false
friends are among the list of 11,206 generated
Spanish subword translations (10.6%), the question
arises how to distinguish false friends from true pos-
itives (cognates). Because a manual examination of
the entire candidate set is a tedious and still error-
prone work, we shifted our attention to automatic
semantic validation techniques.
3.3 Automatic Semantic Validation
In order to automatically validate all the generated
cognate pairs, we examined the local context in
which these cognates occur in non-parallel corpora
of both languages involved. The basic idea that
underlies this approach is that a subword that ap-
pears in a certain context should have a (true posi-
tive) cognate that occurs in a similar context, at least
when (very) large corpora are taken into account.
Cognate similarity can then be measured in terms
of context vector comparison (cf. also Rapp (1999)
or Koehn and Knight (2002)).
We therefore processed the Portuguese corpus us-
ing the morpho-semantic normalization routines as
discussed in Section 2. In the next step, we cre-
ated a context vector for each MID, the compo-
nents of which contained the relative frequencies of
co-occurring MIDs in a local window of four sub-
sequent, yet unordered MID units (a size also en-
dorsed by Rapp (1999)).
In order to compute the context vector for each
Spanish subword candidate, we then constructed
a seed lexicon with all the automatically created
Spanish subword candidates, together with the list
of Spanish affixes. Based on this lexicon, the Span-
ish corpus was morphologically normalized in the
same way, using the MIDs that were licensed by the
Portuguese cognates. For each of the candidate cog-
nate MIDs, we built a corresponding context vector.
We then measured the context similarity for each
MID considering its Portuguese source context and
the corresponding Spanish one. We chose two sim-
ilarity metrics, viz. the well-known cosine metric
(Salton and McGill, 1983) and an inverted, normal-
ized (within the interval [0,1]) variant of the city-
block metric (advocated by Rapp (1999) as an alter-
native that outperformed cosine in his experiments).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  500  1000  1500  2000  2500  3000  3500
Ve
ct
or
 S
im
ilia
rit
y
MID (n = 3809)
Cosine 
City-Block
Figure 1: Context Similarity of MIDs Representing Por-
tuguese and Spanish Sources
 1
 10
 100
 1000
 10000
 100000
 0  1000  2000  3000  4000  5000  6000  7000  8000  9000
Ab
so
lu
te
 F
re
qu
en
cy
 in
 C
or
pu
s 
(lo
g-s
ca
le)
MID (n = 8992)
Portuguese Corpus
Spanish Corpus
Figure 2: Distribution of MIDs in the Portuguese and
Spanish Corpora
Figure 1 depicts the resulting curves. Both met-
rics reveal almost the same characteristics. Only for
higher similarities, city-block allows a more fine-
grained distinction.
For 5,183 (57.6%) from 8,992 pairs of MIDs (one
from a ?Portuguese? vector, the other from a ?Span-
ish? vector), no vector similarity at all could be mea-
sured. We distinguish between the following cases:
? There was no MID occurrence in the Spanish
corpus.
? There was a MID occurrence in the Spanish
corpus, but none in the Portuguese one.
? The vectors were orthogonal, i.e., the contexts
did not overlap at all, although the MID oc-
curred in the Spanish corpus, as well as in the
Portuguese one. This can be interpreted in two
ways: For reasonably frequent MIDs (cf. Fig-
ure 2 for the distribution in the corpora) this is
the strongest evidence for false friends (formal
cognates which are not semantically related),
whereas for sparsely distributed MIDs, it does
hardly permit any valid judgment concerning
their status as false or true cognates.
On the other hand, 1,540 MID pairs (in the sense
from above) exceed similarity values of 0.2 (17.1%)
and 2,065 pairs still share values greater than 0.15
(23%). The obvious question is: What is an ade-
quate threshold?
Figures 3 and 4 convey an answer to this ques-
tion. Both figures are meant to illustrate the trade-
off when one increases the threshold for the similar-
ity of both vectors, the Portuguese and the Span-
ish one, for the MIDs under consideration. The
central notion in these two figures is that of Kept
Hypotheses, i.e., the proportion of MIDs for which
0
10
20
30
40
50
60
70
80
90
100
0.0 0.1 0.2 0.3 0.4 0.5%
 o
f A
ll G
en
er
at
ed
 M
ID
 H
yp
ot
he
se
s 
(n 
= 8
99
2)
Cosine Value Threshold
Kept Hypotheses
False Negatives
False Positives
Figure 3: The Effects of Applying a Threshold Value to
the Cosine Metrics for the Validation of MID Hypotheses
0
10
20
30
40
50
60
70
80
90
100
0.0 0.1 0.2 0.3 0.4 0.5%
 o
f A
ll G
en
er
at
ed
 M
ID
 H
yp
ot
he
se
s 
(n 
= 8
99
2)
City Block Value Threshold
Kept Hypotheses
False Negatives
False Positives
Figure 4: The Effects of Applying a Threshold Value
to the City-Block Metrics for the Validation of MID Hy-
potheses
the assignment of the underlying cognate is judged
as being semantically valid. When we consider all
(100%) of the generated MIDs (n=8,992) as valid
(hence, cosine and city-block are both zero), we
get 953 false positives (given our empirically de-
termined accuracy rate of 89.4%, and, hence, er-
ror rate of 10.6%) and, obviously, no false negative.
Alternatively, when we consider instead 50% of the
generated MIDs (n=4,496) as valid (with thresholds
for cosine set at 0.05 and for city-block at 0.035),
we get 297 (3.3%) false positives, and the num-
ber of false negatives increases at a level of 3,687
(around 41%, for both metrics). In order to reduce
the set of false friends to zero using the cosine met-
ric, 92.2% of all generated MID cognates will be
rejected by the automatic validation for manual re-
vision (analogously, the number of false negatives
will increase). Interestingly, the same procedure us-
ing the city-block metric will lead to a rejection rate
of 97%.
At a first glance, this seems to contradict the
statement of Rapp (1999), who found in a num-
ber of experiments that the city-block metric yields
the best results among others, viz. cosine and Jac-
card measure, Euclidean distance and scalar prod-
uct. However, his measures were taken to find the
most similar vector for a given word in order to au-
tomatically identify word translations. On the other
hand, in our experiments, we intended to express the
degree of similarity given a pair of cognates. We hy-
pothesized that the city-block metric allows a more
fine grained similarity judgment whilst others, e.g.,
cosine, the Jaccard and Dice coefficient, etc., which
only account for overlapping elements of a vector,
have a stronger demarcation power.
Summarizing, when we increase the similarity
thresholds, the number of MID hypotheses de-
creases as does the number of false positives (al-
ready at a rather low level), while the number of
false negatives increases almost inversely related to
the number of MID hypotheses. Therefore, it is up
to the lexicon engineer to determine the level of pre-
selection in these three dimensions. We also con-
clude from our experiments that a much larger cor-
pus is needed in order to collect reasonable context
evidence for the infrequent MIDs, in particular.
4 Related Work
The rise of the empirical paradigm in the field of
machine translation is, to a large degree, due to the
wide-spread availability of parallel corpora (Brown
et al, 1990). They also constitute an important re-
source for the automated acquisition of translational
lexicons (Turcato, 1998). Unfortunately, the limited
availability of parallel corpora (e.g., the Canadian
Hansard corpus of English and French parliament
debates) restricts this method to a few language
pairs, mostly focused on specific sublanguages (e.g.
politics, legislation, economy). Neither exist such
parallel corpora for the medical sublanguage, nor
for the particular language pair, Spanish and Por-
tuguese, we focus on in this work.
The acquisition of unrelated, albeit comparable
corpora is much easier. Rapp (1999) used unre-
lated parallel corpora in order to learn English and
German word-to-word translations. His approach
is based on similarity measures and context clues,
using a seed lexicon of trusted translations. Koehn
and Knight (2002) derived such a seed lexicon from
German-English cognates which were selected by
using string similarity criteria. An additional boost
can be achieved by retrieving content-related doc-
ument pairs using CLIR techniques (Utsuro et al,
2003). An alternative generative approach is pro-
posed by Barker and Sutcliffe (2000) who created
Polish cognate candidates out of an English wordlist
using a set of string mapping rules.
Pirkola et al (2003) used aligned translation dic-
tionaries as source data. Based on that, they created
an algorithm to automatically generate transforma-
tion rules from five different languages to English,
including Spanish. Applying a two-step technique
(translation rules and fuzzy n-gram matching), they
achieved 81.1% of average precision in a Spanish-
to-English context covering biomedical words only.
However, their evaluation metrics considerably dif-
fered from ours, since they considered multiple hy-
potheses.
Our work differs from these precursors in many
ways. First of all, due to domain and language re-
strictions the size of our corpora is much smaller
than the commonly used newspaper corpora. For
the same reasons, CLIR techniques for retrieving
comparable documents are not yet available (on the
contrary, the goal of our work is to provide re-
sources for a medical CLIR system). Thirdly, the
two languages are so similar that a high amount
of translations could already be acquired by apply-
ing string mapping rules (this approach to cognate
mapping has also been discussed by MacWhinney
(1995) for second language acquisition of human
learners). Finally, rather than acquiring bilateral
word translation, our focus lies on assigning sub-
words to interlingual semantic identifiers.
5 Conclusions and Further Work
In a first round of experiments, we have shown
that a considerable amount of Portuguese subwords
from the medical domain could be mapped to Span-
ish cognate stems applying simple string transfor-
mation rules. We then used the local context in
language-specific corpora in order to validate these
cognate pairs. However, our results also reveal the
limitations of such an approach, at least for infre-
quent stems, due to the small corpus size. Ac-
cordingly, for future experiments one has to pro-
vide much larger text corpora, paticularly in the next
steps of our experiments, in which the Spanish lexi-
con will be completed by subwords which cannot be
generated from their Portuguese translations. Here,
we will acquire new Spanish lexeme candidates by
automated stemming, and retrieve their Portuguese
translations by exploring their local context. This
requires, however, huge corpora, exceeding the cur-
rent ones by several orders of magnitude. Addition-
ally, their documents will have to be related using
clustering techniques. The usability of the resulting,
mainly automatically generated Spanish extension
of the MORPHOSAURUS lexicon for the purpose of
cross-language text retrieval can then be evaluated
in real CLIR experiments as previously done for
English, German and Portuguese (cf. Hahn et al
(2004)).
Acknowledgements.
This work was partly supported by the German
Research Foundation (DFG), grant KL 640/5-1,
and by the Brazilian National Council for Scien-
tific Research and Development (CNPq), grants
551277/01-7 and 550240/03-9.
References
Gosia Barker and Richard F. E. Sutcliffe. 2000. An
experiment in the semi-automatic identification
of false-cognates between English and Polish. In
Proceedings of the Irish Conference on Artificial
Intelligence and Cognitive Science.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Je-
linek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79?85.
Julio Gonzalo, Felisa Verdejo, and Irina Chugur.
1999. Using EUROWORDNET in a concept-
based approach to cross-language text retrieval.
Applied Artificial Intelligence, 13(7):647?678.
Udo Hahn, Korne?l Marko?, Michael Poprat, Ste-
fan Schulz, and Joachim Wermter. 2004. Cross-
ing languages in text retrieval via an interlingua.
In Proceedings of the 7th International RIAO?04
Conference, pages 82?99.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora.
In Unsupervised Lexical Acquisition: Proceed-
ings of the Workshop of the ACL Special Inter-
est Group on the Lexicon (SIGLEX), pages 9?16.
Association for Computational Linguistics.
Brian MacWhinney. 1995. Language-specific pre-
diction in foreign language learning. Language
Testing, 12(3):292?320.
Korne?l Marko?, Phillip Daumke, Stefan Schulz,
and Udo Hahn. 2003. Cross-language MESH
indexing using morpho-semantic normalization.
In AMIA?03 ? Proceedings of the 2003 Annual
Symposium of the American Medical Informatics
Association, pages 425?429. Philadelphia, PA:
Hanley & Belfus.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo,
Kari Visala, and Kalervo Ja?rvelin. 2003. Fuzzy
translation of cross-lingual spelling variants. In
SIGIR 2003 ? Proceedings of the 26th Annual In-
ternational ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
345?352. Toronto, Canada, 2003, New York, NY:
ACM.
Reinhard Rapp. 1999. Automatic identification
of word translations from unrelated English and
German corpora. In Proceedings of the 37th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 519?526. San Francisco,
CA: Morgan Kaufmann.
Miguel Ruiz, Anne Diekema, and Pa?raic Sheridan.
1999. CINDOR conceptual interlingua document
retrieval: TREC-8 evaluation. In Proceedings
of the 8th Text REtrieval Conference (TREC-8),
pages 597?606. National Institute of Standards
and Technology (NIST). NIST Special Publica-
tion, No. 500-246.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. New
York, NY: McGraw Hill.
MESH. 2001. Medical Subject Headings.
Bethesda, MD: National Library of Medicine.
Stefan Schulz and Udo Hahn. 2000. Morpheme-
based, cross-lingual indexing for medical docu-
ment retrieval. International Journal of Medical
Informatics, 59(3):87?99.
Stefan Schulz, Martin Honeck, and Udo Hahn.
2002. Biomedical text retrieval in languages with
a complex morphology. In Proceedings of the
ACL 2002 Workshop ?Natural Language Process-
ing in the Biomedical Domain?, pages 61?68.
New Brunswick, NJ: Association for Computa-
tional Linguistics (ACL).
Davide Turcato. 1998. Automaticaly creating bilin-
gual lexicons for machine translation from bilin-
gual text. In COLING/ACL?98 ? Proceedings of
the 36th Annual Meeting of the Association for
Computational Linguistics & 17th International
Conference on Computational Linguistics, vol-
ume 2, pages 1299?1306. San Francisco, CA:
Morgan Kaufmann.
Takehito Utsuro, Takashi Horiuchi, Takeshi
Hamamoto, Kohei Hino, and Takeaki Nakayama.
2003. Effect of cross-language IR in bilingual
lexicon acquisition from comparable corpora. In
EACL?03 ? Proceedings of the 11th Conference
of the European Chapter of the Association
for Computational Linguistics, pages 355?362.
Association for Computational Linguistics.
High-Performance Tagging on Medical Texts
Udo Hahn Joachim Wermter
Computerlinguistik, Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, D-07743 Jena, Germany
hahn@coling.uni-freiburg.de
Abstract
We ran both Brill?s rule-based tagger and TNT,
a statistical tagger, with a default German
newspaper-language model on a medical text
corpus. Supplied with limited lexicon re-
sources, TNT outperforms the Brill tagger with
state-of-the-art performance figures (close to
97% accuracy). We then trained TNT on a large
annotated medical text corpus, with a slightly
extended tagset that captures certain medical
language particularities, and achieved 98% tag-
ging accuracy. Hence, statistical off-the-shelf
POS taggers cannot only be immediately reused
for medical NLP, but they also ? when trained
on medical corpora ? achieve a higher perfor-
mance level than for the newspaper genre.
1 Introduction
The application of language technology in the med-
ical field, dubbed as medical language processing
(MLP), is gaining rapid recognition (for a survey,
cf. Friedman and Hripcsak (1999)). It is both im-
portant, because there is strong demand for all kinds
of computer support for health care and clinical ser-
vices, which aim at improving their quality and de-
creasing their costs, and challenging ? given the
miracles of medical sublanguage, the various text
genres one encounters and the enormous breadth of
expertise surfacing as medical terminology.
However, the development of human language
technology for written language material has, up un-
til now, almost exclusively focused on newswire or
newspaper genres. This is most prominently ev-
idenced by the PENN TREEBANK (Marcus et al,
1993). Its value as one of the most widely used
language resources mainly derives from two fea-
tures. First, it supplies everyday, non-specialist doc-
ument sources, such as the Wall Street Journal, and,
second, it contains value-added, viz. annotated, lin-
guistic data. Since the understanding of newspaper
material does not impose particular requirements on
its reader, other than the mastery of general English
and common-sense knowledge, it is easy for almost
everybody to deal with. This is essential for the ac-
complishment of the second task, viz. the annotation
and reuse of part-of-speech (POS) tags and parse
trees, as the result of linguistic analysis. With the
help of such resources, whole generations of state-
of-the-art taggers, chunkers, grammar and lexicon
learners have evolved.
The medical field poses new challenges. First,
medical documents exhibit a large variety of struc-
tural features not encountered in newspaper docu-
ments (the genre problem), and, second, the under-
standing of medical language requires an enormous
amount of a priori medical expertise (the domain
problem). Hence, the question arises, how portable
results are from the newspaper domain to the medi-
cal domain?
We will deal with these issues, focusing on the
portability of taggers, from two perspectives. We
first pick up off-the-shelf technology, in our case
the rule-based Brill tagger (Brill, 1995) and the
statistically-based TNT tagger (Brants, 2000), both
trained on newspaper data, and run it on medical
text data. One may wonder how the taggers trained
on newspaper language perform with medical lan-
guage. Furthermore, one may ask whether it is nec-
essary (and, if so, costly) to retrain these taggers
on a medical corpus, if one were at hand? These
questions seem to be of particular importance, be-
cause the use of off-the-shelf language technology
for MLP applications has recently been questioned
(Campbell and Johnson, 2001). Answers will be
given in Section 2.
Once a large annotated medical corpus becomes
available, additional questions can be tackled. Will
taggers, e.g., improve their performance substan-
tially when trained on medical data, or is this more
or less irrelevant? Also, if medical sublanguage par-
ticularities can already be identified on the level of
POS co-occurrences, would it be a good idea to en-
hance newspaper-oriented, general-purpose tagsets
with dedicated medical tags? Finally, does this ex-
tension have a bearing on the performance of tag-
ging medical documents and, if so, to what extent?
We will elaborate on these questions in Section 4.
2 Medical Tagging with Off-the-shelf
Technology
For the first series of experiments, we chose two
representatives of the currently prevailing data-
driven tagging approaches, Brill?s rule-based tagger
(Brill, 1995) and TNT, a statistical tagger (Brants,
2000). As we are primarily concerned with Ger-
man language input, for Brill?s tagger, originally
developed on English data, its German rule exten-
sion package was used. TNT, on the other hand,
is based on a statistical model and therefore is ba-
sically language-independent. It implements the
Viterbi algorithm for second-order Markov models
(Brants, 2000), in which states of the model repre-
sent tags and the output represents words. The best
POS tag for a given word is determined by the high-
est probability that it occurs with   previous tags.
Tags for unknown words are assigned by a proba-
bilistic suffix analysis; smoothing is done by linear
interpolation.
2.1 Experiment 1: Medical Tagging with
Standard Tagset Trained on NEGRA
The German default version of TNT was trained
on NEGRA, the largest publicly available manually
annotated German newspaper corpus (composed of
355,095 tokens and POS-tagged with the general-
purpose STTS tagset; cf. (Skut et al, 1997)). The
Brill tagger comes with an English default version
also trained on general-purpose language corpora
like the PENN TREEBANK (Marcus et al, 1993).
In order to compare the performance of both tag-
gers on German data, the Brill tagger was retrained
on the German NEGRA newspaper corpus, with pa-
rameters recommended in the training manual.
In a second round, we set aside a subset of a
newly developed German-language medical corpus
(21,000 tokens, with 1800 sentences). We here
refer to this text corpus as FRAMED  and de-
scribe its superset, FRAMED (Wermter and Hahn,
2004), in more depth in Section 4.1. Three human
taggers, trained on the STTS tagset and on guide-
lines used for tagging the NEGRA corpus, annotated
FRAMED  according to NEGRA standards. The
interrater reliability for this part of the manual anno-
tation was 96.7% (standard deviation: 0.6%), based
on a random sample of 2000 tokens (10% of the
evaluation corpus).
The performance of both taggers, TNT and Brill,
with their NEGRA newspaper-trained parameteriza-
tion was then measured on the FRAMED  corpus.
In addition, since both TNT and Brill allow the in-
clusion of an external backup lexicon, their perfor-
mance was also measured by plugging in two such
medical backups.
2.2 Results from Medical Tagging with
Standard Tagset Trained on NEGRA
We measured tagging accuracy by the ratio of the
number of correct POS assignments to text to-
kens (as defined by the gold standard, viz. the
manually annotated corpus) and the number of all
POS assignments to text tokens from the test set.
Table 1 reveals that the n-gram-based TNT tag-
ger outperforms the rule-based Brill tagger on the
FRAMED  medical corpus, both being trained on
the NEGRA newspaper corpus. The inclusion of a
small medical backup lexicon (composed of 171 en-
tries which account for the most frequently falsely
tagged tokens such as measure units, Latinate medi-
cal terms, abbreviations etc.) boosted TNT?s perfor-
mance to 96.7%, which is on a par with the state-of-
the-art performance of taggers on newspaper texts.
A much more comprehensive medical backup lex-
icon, which contained the first one plus the Ger-
man Specialist Lexicon, a very large repository of
domain-specific medical terms (totalling 95,969 en-
tries), much to our surprise had almost no effect on
improving the tagging results.
TNT BRILL
Default 95.2% 91.9%
+ Back-up Lexicon 1 96.7% 93.4%
+ Back-up Lexicon 2 96.8% 93.5%
Table 1: Tagging Accuracy (Training on NEGRA News-
paper Corpus; Evaluation on FRAMED 
		 Medical Cor-
pus)
The results for the German version of Brill?s tag-
ger, both its default version (91.9%) and the lexicon
add-on (93.4%), are still considerably better than
those of its default version reported by Campbell et
al. (Campbell and Johnson, 2001) for English med-
ical input (89.0%).
3 An Inquiry into Corpus Similarity
The fact that an n-gram-based statistical POS tagger
like TNT, trained on newspaper and tested on medi-
cal language data, falls 1.5% short of state-of-the-art
performance figures may at first come as a surprise.
It has been observed by (Campbell and Johnson,
2001) and (Friedman and Hripcsak, 1999), however,
that medical language shows less variation and com-
plexity than general, newspaper-style language. Our
second series of experiments, quantifying the gram-
matical differences/similarities between newspaper
and medical language on the TNT-relevant POS n-
gram level, may shed some explanatory light on the
tagger?s performance.
3.1 Experiment 2: Measuring Corpus
Similarity
For this purpose, we collected a large medical docu-
ment collection of mostly clinical texts (i.e., pathol-
ogy, histology and surgery reports, discharge sum-
maries). We refer to this collection (composed of
2480K tokens) as BIGMED. Next, we randomly
split BIGMED into six subsamples of NEGRA size
(355K tokens). This was meant to ensure a sta-
tistically sound comparability and to break up the
medical subgenres. The same procedure was re-
peated for a collection of German newspaper and
newswire texts collected from the Web. All twelve
samples (six medical ones, henceforth called MED,
and six newspaper ones, henceforth called NEWS,
also composed of 2480K tokens to ease partition-
ing) were then automatically tagged by TNT based
on its newspaper-trained parameterization.
Since NEGRA is the newspaper corpus on which
the default version of TNT was trained, its statistical
comparison with MED should elucidate the tagger?s
performance on medical texts without changing the
training environment. Moreover, a parallel compar-
ison with other newspaper texts (NEWS) may help
in further balancing these results. Because TNT is
a Markovian tagger based on tri-, bi- and unigram
POS sequences, the statistics were based on the POS
n-gram sequences in the different corpora. For this
purpose, we extracted all POS trigram, bigram and
unigram type sequences from NEGRA, MED, and
NEWS. Their numbers are reported in Table 2 (see
rows 1, 4 and 7). We then generated a distribution
of these types based on three ranges of occurrence
frequencies. The results are reported in Table 3.
We then determined how many POS n-gram types
were common between NEGRA and MED and com-
mon between NEGRA and NEWS (see Table 2, rows
2, 5 and 8). Each of these common POS n-gram
types was subjected to a    test in order to measure
whether their common occurrence in both corpora
was just random (null hypothesis) or whether that
particular n-gram was indicative of the similarity
between the two corpora (i.e., between NEGRA and
MED, on the one hand, and between NEGRA and
NEWS, on the other hand). This interpretation of   
statistics has already been evaluated against other
corpus similarity measures and was shown to per-
form best (Kilgarriff, 2001), assuming a non-normal
distribution (cf. also Table 3). The    metric sums
the differences between observed and expected val-
ues in all squares of the table and scales them by
the magnitude of the expected values. The number
of all common significant POS n-grams (i.e., those
whose critical values are greater than 3.841 for a
NEGRA MED NEGRA NEWS
POS trigram 13,045 9,232.9 13,045 13,709.2
types (217.5) (86.8)
common POS 7,130.3 (144.2) 9,992.0 (33.6)
trigram types:
ratio (in %) 54.7 (1.1) 77.2 (0.4) 76.6 (0.3) 72.9 (0.3)
 significant 2,793.8 (34.4) 1,202.0 (29.6)
common POS
trigram types ratio: 41.7% (1.3) ratio: 12.1% (0.3)
POS bigram 1,441 1,169.0 1,441 1,441.8
types (20.7) (14.8)
common POS 1,076.5 (14.3) 1,270.8 (9.3)
bigram types:
ratio (in %) 76.4 (1.0) 92.0 (0.5) 88.2 (0.6) 88.1 (0.4)
 significant 689.9 (5.5) 386.5 (12.2)
common POS
bigram types ratio: 64.2% (0.9) ratio: 30.4% (0.9)
POS unigram 55 52.7 55 55.0
types (0.5) (0.5)
common POS 51.3 (0.5) 53.7 (0.5)
unigram types
 significant 44.7 (0.8) 36.5 (2.4)
common POS
unigram types ratio: 87.0% (1.5) ratio: 68.1% (4.9)
Table 2: POS n-gram and  Comparsions between
NEGRA-MED and NEGRA-NEWS (deviation of means of
six MED and six NEWS samples in parentheses)
probability level of  = 0.05) is indicative of the
magnitude of corpus similarity. These results are
reported in Table 2 (see rows 3, 6 and 9).
3.2 Results from Measuring Corpus Similarity
As shown in Table 2 (rows 1, 4 and 7), the number
of unique POS n-gram types was considerably lower
in MED. Compared with NEGRA, MED had 29%
less trigram types, 19% less bigram types and 4%
less unigram types (i.e., POS tags), whereas NEWS
even had slightly more types at all n-gram levels.
This much lower number of MED POS trigram and
POS n-gram types appearing
 10 10-1000  1000
times times times
tri- NEGRA 9402 3610 33
grams MED 6571.2 (153) 2610.5 (66.5) 51.2 (0.8)
NEWS 9972.5 (69) 3698.7 (31.6) 38 (0.6)
bi- NEGRA 618 744 79
grams MED 503.5 (18.2) 590.5 (16.3) 75 (1.9)
NEWS 598.8 (14.) 762.2 (5.3) 80.8 (.6)
uni- NEGRA 4 18 33
grams MED 4.3 (0.8) 21.3 (0.8) 27 (0.6)
NEWS 2.8 (1.0) 17.5 (0.8) 34.7 (0.5)
Table 3: Three-part Distribution of POS n-gram Types
in NEGRA, MED and NEWS
n-grams common to NEGRA and MED n-grams common to NEGRA and NEWS
Top 5 ranked ADJD ADJA NN 3552.0 (399.4) FM FM FM 772.7 (356.5)
POS trigrams ADJA ADJA NN 2811.1 (262.7) $, ADJA NN 176.8 (10.5)
ADJA NN $. 1740.0 (175.7) NN $. $( 172.4 (20.4)
ADJA NN ART 1471.7 (145.9) VVINF $. $( 169.7 (16.6)
ADJA NN KON 1162.6 (73.3) $. $( ART 148.9 (12.2)
Top 5 ranked ADJA NN 5854.9 (454.5) FM FM 869.9 (470.1)
POS bigrams ADJD ADJA 4861.8 (577.5) $. $( 831.3 (71.1)
ADJA ADJA 3355.6 (290.6) $. XY 407.2 (5.6)
NE NE 2249.9 (99.4) $. PPER 245.1 (21.4)
APPR NE 1884.6 (111.5) NN $( 221.9 (27.5)
Top 5 ranked ADJA 10632.9 (946.5) $( 992.5 (72.6)
POS unigrams ADJD 5479.5 (439.2) FM 953.2 (450.8)
NE 5211.8 (216.7) PPER 365.1 (17.1)
PPER 2201.3 (97.8) XY 329.6 (39.9)
$( 1936.7 (170.0) NE 127.7 (33.8)
Table 4:   Top 5 Ranked POS Trigrams, Bigrams and Unigrams Common to NEGRA and MED and to NEGRA and
NEWS (standard deviation of means of occurrence frequencies in parentheses)
bigram types is also reflected in the three-part dis-
tribution in Table 3: The number of POS trigrams
occurring less than ten times is almost one third less
in MED than in NEGRA or in NEWS; similarly, but
less pronounced, this can be observed for POS bi-
grams. On the other hand, the number of trigram
types occurring more than 1000 times is even higher
for MED, and the number of bigram and unigram
types is about the same when scaled against the to-
tal number of types. This indicates a rather high
POS trigram and bigram type dispersion in newspa-
per corpora, whereas medical narratives appear to
be more homogeneous.
Table 2 (rows 2, 5 and 7) indicates that the num-
ber of POS trigram and bigram types common to
both corpora was much smaller for the NEGRA-
MED comparison than it was for NEGRA-NEWS. In
other words, more of the NEGRA POS n-gram types
appeared in the NEWS corpus as well, whereas far
less showed up in the MED corpus. At this level of
comparison, sublanguage differences clearly show
up. If, however, compared with the total number
of POS n-gram types in each corpus, the common
ones cover much more of the MED corpus than of
the NEGRA corpus. The coverage for NEGRA and
NEWS is about the same.
The number of common POS n-gram types that
are  

significant (Table 2: rows 3, 6, and 9) shows
the magnitude of corpus similarity. For the common
trigram types, it was almost four times higher in the
NEGRA-MED comparison than for NEGRA-NEWS;
for the common bigram types it was more than twice
as high, and for the unigram types 20% higher.
Finally, table 4 shows that the top-ranked POS tri-
grams, bigrams and unigrams common to NEGRA
and MED (columns 2 to 4) exhibit a strikingly dif-
ferent  

magnitude compared to those common to
NEGRA and NEWS (columns 5 to 7). This means
that, in regard to their top POS n-grams, NEGRA
and MED are highly similar, whereas NEGRA and
NEWS are less so. Interestingly, for each n-gram
the top 5 ranks remain unchanged across all six
NEGRA-MED comparisons, whereas they have a
different ranking in almost each of the six NEGRA-
NEWS comparisons. It seems as though the most
characteristic similarities between medical sublan-
guage and newspaper language are highly consistent
and predictable, whereas the intra-newspaper com-
parison shows weak and inconsistent similarities.
4 Tagging with Medical Resources
4.1 FRAMED, an Annotated Medical Corpus
FRAMED, the FReiburg Annotated MEDical cor-
pus (Wermter and Hahn, 2004), combines a vari-
ety of relevant medical text genres focusing on clin-
ical reports. The clinical text genres cover discharge
summaries, pathology, histology and surgery re-
ports. The non-clinical ones consist of medical ex-
pert texts (from a medical textbook) and health care
consumer texts taken from the Web. It has already
been mentioned that medical language, as used in
these clinical documents, has some unique proper-
ties not found in newspaper genres. Among these
features are the use of Latin and Greek terminol-
ogy (sometimes also mixed with the host language,
here German), various ad hoc forms for abbrevi-
ations and acronyms, a variety of (sometimes id-
iosyncratically used) measure units, enumerations,
and some others. These may not be marginal sub-
language properties and thus may have an impact on
the quality of tagging procedures. In order to test
this assumption, we enhanced the NEGRA-rooted
STTS tagset with three dedicated tags which capture
ubiquitous lexical properties of medical texts not
Training NEGRA FRAMED NEGRA FRAMED NEGRA FRAMED NEGRA FRAMED
Size % unknown words accuracy, unknown words only accuracy, known words only overall accuracy
5,000 40.3 (1.4) 40.8 (3.3) 74.9 (2.5) 81.1 (2.5) 96.3(0.4) 97.8 (0.7) 87.7 (1.2) 91.0 (1.5)
10,000 33.9 (0.6) 33.5 (3.2) 79.3 (1.2) 85.9 (2.0) 96.8 (0.3) 97.8 (0.4) 90.9 (0.5) 93.7 (1.1)
20,000 28.6 (1.0) 26.1 (2.2) 82.9 (1.1) 88.9 (1.6) 97.1 (0.3) 98.2 (0.2) 93.0 (0.3) 95.9 (0.6)
30,000 25.2 (1.0) 21.1 (1.6) 84.4 (1.1) 90.2 (1.2) 97.3 (0.4) 98.3 (0.2) 94.0 (0.3) 96.6 (0.4)
40,000 23.1 (0.9) 18.3 (1.6) 85.1 (1.1) 91.7 (1.7) 97.3 (0.2) 98.6 (0.3) 94.6 (0.4) 97.3 (0.5)
50,000 21.6 (1.0) 16.7 (1.8) 85.8 (1.2) 92.0 (1.8) 97.4 (0.2) 98.7 (0.3) 94.9 (0.4) 97.6 (0.5)
60,000 20.2 (0.9) 15.3 (1.8) 86.1 (1.3) 92.4 (1.7) 97.5 (0.2) 98.7 (0.3) 95.2 (0.4) 97.7 (0.5)
70,000 19.2 (1.0) 14.5 (1.9) 86.4 (1.7) 92.4 (2.0) 97.5 (0.3) 98.6 (0.4) 95.4 (0.4) 97.7 (0.7)
80,000 18.5 (0.9) 13.6 (1.6) 86.9 (1.4) 93.2 (2.1) 97.5 (0.2) 98.8 (0.3) 95.6 (0.4) 98.0 (0.5)
90,000 17.9 (1.3) 12.5 (1.7) 86.9 (1.3) 93.0 (1.9) 97.6 (0.3) 98.7 (0.3) 95.7 (0.3) 98.0 (0.4)
Table 5: Averaged Learning Curve Values for Different Training Sizes (standard deviation in parentheses)
covered by this general-purpose tagset, thus yield-
ing the STTS-MED tagset.1 Our three student anno-
tators then annotated the FRAMED medical corpus
with the extended STTS-MED tagset. The mean of
the inter-annotator consistency of this annotation ef-
fort was 98.4% (with a standard deviation of 0.6).
A look at the frequency ranking of the dedicated
medical tags shows that they bear some relevance in
annotating medical corpora. Out of the 54 tag types
occurring in the FRAMED corpus, ENUM is ranked
14, LATIN is ranked 19, and FDSREF is ranked 33.
In terms of absolute frequencies, all three additional
tags account for 1613 (out of 100,141) tag tokens
(ENUM: 866, LATIN: 560, FDSREF: 187). To test
the overall impact of these three additional tags, we
ran the default NEGRA-newspaper-based TNT on
our FRAMED medical corpus and compared the re-
sulting STTS tag assignments with those from the
extended STTS-MED tagset. The additional tags ac-
counted for only 24% of the differences between the
two assignments (1613/6685). Hence, their intro-
duction, by no means, fully explains any improved
tagging results (compared with the reduced newspa-
per tagset). The other sublanguage properties men-
tioned above (e.g., abbreviations, acronyms, mea-
sure units etc.) are already covered by the original
tagset.
4.2 Experiment 3: Re-Training TNT on
FRAMED
In a third series of experiments, we compared
TNT?s performance with respect to the general
newspaper language and the medical sublanguage.
For this purpose, the tagger was newly trained and
tested on a random sample (100,198 tokens) of the
NEGRA newspaper corpus with the standard STTS
tagset, and, in parallel, re-trained and tested on the
FRAMED medical corpus using STTS-MED, the ex-
tended medical tagset.
1The three tags are ?ENUM? (all sorts of enumerations),
?LATIN? (Latin forms in medical terms), and ?FDSREF? (ref-
erence patterns related to formal document structure).
For this evaluation, we used learning curve values
(see Table 5) that indicate the tagging performance
when using training corpora of different sizes. Our
experiments started with 5,000 tokens and ranged
to the size of the entire corpus (minus the test set).
At each size increment point, the overall accuracy,
as well as the accuracies for known and unknown
words were measured, while also considering the
percentage of unknown words.
The tests were performed on random partitions of
the corpora that use up to 90% as training set (de-
pending on the training size) and 10% as test set.
In this way, the test data was guaranteed to be un-
seen during training. This process was repeated ten
times, each time using a different 10% as the test
set, and the single outcomes were then averaged.
4.3 Results from Medical Tagging with
Medical Resources
Table 5 (columns 4-9) reveals that the FRAMED-
trained TNT tagger outperforms the NEGRA-trained
one at all training points and across all types of
accuracies we measured. Trained with the largest
possible training size (viz. 90,000 tokens), the tag-
ger?s overall accuracy for its FRAMED parametriza-
tion scores 98.0%, compared to 95.7% for its NE-
GRA parametrization. The performance differences
between FRAMED and NEGRA range between 2.3
(at training points 90,000 and 70,000) and 3.3 per-
centage points (at training point 5,000). The tag-
ging accuracy for known tokens is higher for both
FRAMED and NEGRA (with 98.7% and 97.6%, re-
spectively, at training point 90,000). The differ-
ences here are less pronounced, ranging from 1.0
to 1.3 percentage points.
By far the largest performance difference can be
observed with respect to the tagging accuracy for
unknown words (cf. Table 5 (columns 4 and 5)),
ranging from 5.8 (at training point 30,000) to 6.6
percentage points (at training points 10,000 and
40,000). The FRAMED-trained tagger scores above
90% in seven out of ten points and never falls be-
low 80%. The NEGRA-based tagger, on the other
hand, remains considerably below 90% at all points,
and even falls below 80% at the first two train-
ing points. This performance difference is clearly
one factor which contributes to the FRAMED tag-
ger?s superior results. The difference in the average
percentage of unknown words is the other dimen-
sion where both environments diverge (cf. Table 5,
columns 2 and 3). Whereas the percentage of un-
known words starts out to be equally high for low-
est training sizes (5,000 and 10,000), this rate drops
much faster for the FRAMED-trained tagger. At the
highest possible training point, only 12.5% of the
words are unknown, compared to still almost 18%
unknown to the NEGRA-trained tagger, resulting in
a 5.4 percentage point difference. Thus, both the
high tagging accuracy for unknown words and their
lower rate, in the first place, seem to be key for the
superior performance of the FRAMED-trained TNT
tagger.
5 Discussion
Campbell and Johnson (2001) have argued that
general-purpose off-the-shelf NLP tools are not
readily portable and extensible to the analysis of
medical texts. By evaluating the English version
of Brill?s rule-based tagger (Brill, 1995), they con-
clude that taggers trained on general-purpose lan-
guage resources, such as newspaper corpora, are not
suited to medical narratives but rather need timely
and costly retraining on manually tagged medical
corpora. Interestingly though, it has also been
observed (Friedman and Hripcsak, 1999; Camp-
bell and Johnson, 2001) that medical language
shows less variation and complexity than general,
newspaper-style language, thus exhibiting typical
properties of a sublanguage. Setting aside the dif-
ference in vocabulary between medical and non-
medical domains, the degradation in performance of
general-language off-the-shelf NLP tools for MLP
applications then seems counter-intuitive. Our first
and second series of experiments were meant to ex-
plain this puzzling state of affairs.
The results of these experiments shed a different
light on the portability and extensibility of off-the-
shelf NLP tools for the analysis of medical narra-
tives as was hypothesized by Campbell and Johnson
(2001). A statistical POS tagger like TNT, which
is trained on general-purpose language by default,
only falls 1.5% short of the state-of-the-art perfor-
mance in a medical environment. An easy-to-set-up
medical backup lexicon eliminates this difference
entirely. It appears that it is the underlying language
model which determines whether a POS tagger is
more or less suited to be portable to the medical do-
main, not the surface characteristics of medical sub-
language. Moreover, lexical backup facilities show
up as a significant asset to MLP. Much to our sur-
prise, a full-scale, carefully maintained lexicon did
not substantially improve the tagger?s performance
in comparison with a heuristically assembled brief
list of the most common tagging mistakes.
A reason for the statistical tagger?s outperfor-
mance may be derived from our comparative corpus
statistics, which was the focus of our second series
of experiments. Concerning POS n-grams, the data
points to a less varied and less complex grammar
of medical sublanguage(s). Not only is the number
of POS n-gram types much lower for medical nar-
ratives than for general-language newspaper texts,
but the distribution also favors high-occurring (more
than 1000 times) types in MED. Another indicator
of a simpler POS n-gram grammar in medical nar-
ratives is the fact that the absolute number of POS
n-gram types common to NEGRA and MED is much
lower than for NEGRA and NEWS. Scaled against
the total number of types in MED, however, the
common ones cover a bigger part of the medical nar-
ratives, whereas they cover less of NEGRA. For POS
trigrams, half of NEGRA is congruent with three
quarters of MED; for POS bigrams three quarters
of NEGRA is congruent with nine tenths of MED.
Common POS n-grams that are  

significant in-
dicate that two corpora are similar with respect to
them. Their number was significantly higher for the
NEGRA-MED comparison than for NEGRA-NEWS.
Hence, the congruency of a high proportion of POS
n-gram types between NEGRA and MED is not ac-
cidental. At the POS n-gram type level, this shows
a higher degree of similarity between NEGRA and
medical narratives than between NEGRA and other
newspaper texts. Furthermore, the high  

num-
bers for the top ranked POS n-grams indicate that
they are especially characteristic of the NEGRA-
MED similarity. Eight of the top-ranked trigrams
and bigrams can be identified as parts of a noun
phrase. All of them contain a prenominal adjective
(ADJA in Table 4), six a common noun (NN in Ta-
ble 4). The prenominal adjective is by far the most
characteristic POS unigram for medical-newspaper
inter-language similarity. None of these observa-
tions hold for newspaper intra-language similarity.
Our third series of experiments showed that
Markovian taggers like TNT improve their perfor-
mance substantially when trained on medical data.
Indeed, we were able to achieve a performance
boost which goes beyond current state-of-the-art
numbers. This seems to be even more notable inas-
much as the tagger?s retraining was done on a com-
paratively small-sized corpus (90,000 tokens).
These experiments suggest two explanations.
First, annotating medical texts with a medically en-
hanced tagset took care of medical sublanguage
properties not covered by general-purpose tagsets.
Second, several tagging experiments on newspaper
language, whether statistical (Ratnaparkhi, 1996;
Brants, 2000) or rule-based (Brill, 1995), report
that the tagging accuracy for unknown words is
much lower than the overall accuracy.2 Thus, the
lower percentage of unknown words in medical
texts seems to be a sublanguage feature beneficial
to POS taggers, whereas the higher proportion of
unknown words in newspaper language seems to be
a prominent source of tagging errors. This is wit-
nessed by the tagging accuracy for unknown words,
which is much higher for the FRAMED-trained tag-
ger than for the newspaper-trained one. For the
medical tagger, there is only a 5 percentage point
difference between overall and unknown word accu-
racy at training point 90,000, whereas, for the news-
paper tagger, this difference amounts to 8.8 percent-
age points. This may be interrelated with another
property of sublanguages, viz. their lower number
of word types: At each training point, the lexicon of
the FRAMED tagger is 20 percentage points smaller
than that of the newspaper tagger. TNT?s handling
of unknown words relies on the probability distri-
bution for a particular (formal) suffix of some fixed
length (cf. Brants (2000)). Thus, guessing an un-
known word?s category is easier on a small-sized
tagger lexicon, because there are less choices for the
POS category of a word with a paricular suffix.
Only recently has the accuracy of data-driven
POS taggers moved beyond the the ?97% barrier?
(derived from newspaper corpora). This was partly
achieved by computationally more expensive mod-
els than TNT?s efficienct unidirectional Markovian
one. For example, Gime?nez and Ma`rquez (2003)
report an accuracy of 97.13% for their SVM-based
power tagger. The best automatically learned POS-
tagging result reported so far (97.24%) is Toutanova
et al (2003)?s feature-based cyclic dependency net-
work tagger. Although reaching the 98% accuracy
level constitutes a breakthrough, it is of course con-
ditioned by the medical sublanguage we are work-
ing with. Still, the application of language technolo-
gies in certain sublanguage domains like medicine,
and more recently, genomics and biology, is gaining
rapid importance, and thus, our results also have to
be considered from this perspective.
2These authors report on differences between 7.7 and 11.5
percentage points.
6 Conclusions
We collected experimental evidence, contrary to re-
cent claims (Campbell and Johnson, 2001), that off-
the-shelf NLP tools can be applied to MLP in a
straightforward way. We explain this finding with
statistically significant POS n-gram type overlaps
of newspaper language and medical sublanguage,
which has not been recognized before.
To the best of our knowledge, this is the first tag-
ging study that reaches a 98% accuracy level for
a data-driven tagger (which must be distinguished
from linguistically backuped taggers which come
with ?heavy? parsing machinery (Samuelsson and
Voutilainen, 1997)). Still, we deal with a spe-
cialized sublanguage simpler in structure compared
with newspaper language, although we kept it di-
verse through the various text genres.
Acknowledgements. We would like to thank our students, Inka
Benthin, Lucas Champollion and Caspar Hasenclever, for their
excellent work as human taggers. This work was partly sup-
ported by DFG grant KL 640/5-1.
References
T. Brants. 2000. TNT: A statistical part-of-speech tag-
ger. In Proc. ANLP 2000, pages 224?231.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing. Computational
Linguistics, 21(4):543?565.
D. A. Campbell and S. B. Johnson. 2001. Comparing
syntactic complexity in medical and non-medical cor-
pora. In Proc. AMIA 2001, pages 90?94.
C. Friedman and G. Hripcsak. 1999. Natural language
processing and its future in medicine. Academic
Medicine, 74(8):890?895.
J. Gime?nez and L. Ma`rquez. 2003. Fast and accurate
part-of-speech tagging: The SVM approach revisited.
In Proc. of the Intl. Conf. on RANLP 2003.
A. Kilgarriff. 2001. Comparing corpora. International
Journal of Corpus Linguistics, 6(1):97?133.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The PENN TREEBANK. Computational Linguistics,
19(2):313?330.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP?96, pages
133?142.
C. Samuelsson and A. Voutilainen. 1997. Compar-
ing a linguistic and a stochastic tagger. In Proc.
ACL?97/EACL?97, pages 246?253.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Proc. ANLP 1997, pages 88?95.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc of HLT-NAACL
2003, pages 252?259.
Joachim Wermter and Udo Hahn. 2004. An annotated
German-language medical text corpus as language re-
source. In Proc 4th Intl LREC Conf. Lisbon, Portugal.
Collocation Extraction Based on Modifiability Statistics
Joachim Wermter Udo Hahn
Computerlinguistik, Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, D-07743 Jena, Germany
wermter@coling.uni-freiburg.de
Abstract
We introduce a new, linguistically grounded
measure of collocativity based on the property
of limited modifiability and test it on German
PP-verb combinations. We show that our mea-
sure not only significantly outperforms the stan-
dard lexical association measures typically em-
ployed for collocation extraction, but also yields
a valuable by-product for the creation of col-
location databases, viz. possible structural and
lexical attributes. Our approach is language-,
structure-, and domain-independent because it
only requires some shallow syntactic analysis
(e.g., a POS-tagger and a phrase chunker).
1 Introduction
Natural language is an open and very flexible com-
munication system. Syntax, of course, imposes con-
straints, e.g., on word order or the occurrence of par-
ticular phrasal types such as PPs or NPs, and lexi-
cal semantics imposes, e.g., selectional constraints
on conceptually permitted sorts or types within the
context of specific verbs or nouns. Nevertheless,
natural language speakers usually enjoy an enor-
mous degree of freedom to express the content they
want to convey in a great variety of linguistic forms.
There is, however, a significant subset of expres-
sions which do not share this rather free combinabil-
ity, so-called collocations. From a linguistic per-
spective, they can be characterized by at least three
recurrent and prominent properties (Manning and
Schu?tze, 1999):
  Non-(or limited) compositionality. The mean-
ing of a collocation is not a straightforward
composition of the meanings of its parts. For
example, the meaning of ?red tape? is com-
pletely different from the meaning of its com-
ponents.
  Non-(or limited) substitutability. The parts of
a collocation cannot be substituted by seman-
tically similar words. Thus, ?gut? in ?to spill
gut? cannot be substituted by ?intestine? (see
also Lin (1999)).
  Non-(or limited) modifiability. Many collo-
cations cannot be supplemented by additional
lexical material. For example, the noun in ?to
kick the bucket? cannot be modified as ?to kick
the

holey/plastic/water  bucket?.
Considering these observations, from a natu-
ral language processing perspective, collocations
should not enter, e.g., the standard syntax-semantics
pipeline so as to prevent compositional semantic
readings of expressions for which this is absolutely
not desired. Hence, collocations need to be identi-
fied as such and subsequently be blocked, e.g., from
compositional semantic interpretation.
In computational linguistics, a wide variety of
lexical association measures have been employed
for the task of (semi-)automatic collocation identifi-
cation and extraction. Almost all of these measures
can be grouped into one of the following three cate-
gories:
  frequency-based measures (e.g., based on ab-
solute and relative co-occurrence frequencies)
  information-theoretic measures (e.g., mutual
information, entropy)
  statistical measures (e.g., chi-square, t-test,
log-likelihood, Dice?s coefficient)
The corresponding metrics have been extensively
discussed in the literature both in terms of their
mathematical properties (Dunning, 1993; Manning
and Schu?tze, 1999) and their suitability for the
task of collocation extraction (see Evert and Krenn
(2001) and Krenn and Evert (2001) for recent eval-
uations). Typically, they are applied to a set of can-
didate lexeme pairs which were obtained from pre-
processors varying in linguistic sophistication.1 The
selected measure then assigns an association score
1On the low end, this may just be a preset numeric window
span. In order to reduce the noise among the candidates, how-
ever, more elaborate linguistic processing, such as POS tag-
ging, chunking, or even parsing, is increasingly being applied.
to each candidate pair, which is computed from its
joint and marginal frequencies, thus expressing the
strength of the hypothesis stating whether it consti-
tutes a collocation or not.
While these association measures have their sta-
tistical merits in collocation identification, it is in-
teresting to note that they have relatively little to do
with the linguistic properties (such as those men-
tioned at the beginning) which are typically associ-
ated with the notion of collocativity. Therefore, it
may be interesting to investigate whether there is a
way to implement a measure which directly incor-
porates linguistic criteria in the collocation identifi-
cation task, and even more important, whether such
a linguistically rooted approach would fare better in
comparison to some of the standard lexical associa-
tion measures.
In the following study, we will introduce such a
linguistic measure for identifying PP-verb colloca-
tions in German, which is based on the property of
non- or limited modifiability. To the best of our
knowledge, this is the first work to use this kind
of linguistic measure to acquire collocations auto-
matically. By contrasting our method to previous
studies which use the standard lexical association
measures, we intend to emphasize a more linguis-
tically inspired use of statistics in collocation min-
ing. Section 2 motivates our definition of the notion
of collocation and Section 3 describes our methods,
in particular the linguistically grounded collocation
extraction algorithm, and the experimental setup de-
rived from it. In Section 4 we present and discuss
the results of our experiments.
2 Kinds of Collocations
There have been various approaches to define the
notion of ?collocation?. This is by no means an easy
task, especially when it comes to defining the de-
marcation line between collocations and free word
combinations (modulo general syntactic and seman-
tic semantic constraints). We favor an approach
which draws this line on the semantic layer, viz. the
compositionality between the components of a lin-
guistic expression.
For this purpose, we distinguish between three
classes of collocations based on varying degrees of
semantic compositionality of the basic lexical enti-
ties involved:
1. Idiomatic Phrases. In this case, none of the
lexical components involved contribute to the
overall meaning in a semantically transpar-
ent way. The meaning of the expression is
metaphorical or figurative. For example, the
literal meaning of the German PP-verb combi-
nation ?[jemanden] auf die Schippe nehmen? is
?to take [someone] onto the shovel?. Its figura-
tive meaning is ?to lampoon somebody?.
2. Support Verb Constructions/Narrow Colloca-
tions. This second class contains expressions
in which at least one component contributes to
the overall meaning in a semantically transpar-
ent way and thus constitutes its semantic core.
For example, in the support verb construction
?zur Verfu?gung stellen? (literal: ?to put to avail-
abilty?; actual: ?to make available?), the noun
?Verfu?gung? is the semantic core of the expres-
sion, whereas the verb only has a support func-
tion with some impact on argument structure,
causativity or aktionsart. There are, however,
also narrow collocations in which the basic lex-
ical meaning of the verb is the semantic core:
For example, in ?aus eigener Tasche bezahlen?
(?to pay out of one?s own pocket?) the verb
?bezahlen? is the semantic core. What unifies
these two types is the fact that they function as
predicates.
3. Fixed Phrases. Here, all basic lexical mean-
ings of the components involved contribute to
the overall meaning in a semantically much
more transparent way. Still, they are not as
completely compositional as to classify them
as free word combinations. For example, all
the basic lexical meanings of the different lex-
ical components in ?im Koma liegen? (literal:
?to lie in coma?; actual: ?to be comatose?)
contribute to the overall meaning of the ex-
pression. Still, this is different from a com-
pletely compositional free word combination,
such as ?auf der Strasse gehen? (?to walk on
the street?).
Our goal is to consider all three types of col-
locations as a whole, i.e., we will not distinguish
between the three different kinds of collocations.
However, in order to focus our experiments, we will
concentrate on a particular surface pattern in which
they occur, viz. PP-verb collocations.
3 Methods and Experiments
3.1 Construction and Statistics of the Testset
We used a 114-million-word German-language
newspaper corpus extracted from the Web to ac-
quire candidate PP-verb collocations. The corpus
was first processed by means of the TNT part-
of-speech tagger (Brants, 2000). Then we ran a
sentence/clause recognizer and an NP/PP chunker,
both developed at the Text Knowledge Engineer-
ing Lab at Freiburg University, on the POS-tagged
corpus. From the XML-marked-up tree output, PP-
verb complexes were automatically selected in the
following way: Taking a particular PP node as
a fixed point, either the preceding or the follow-
ing sibling V node was taken.2 From such a PP-
verb combination, we extracted and counted both its
various heads, in terms of Preposition-Noun-Verb
(PNV) triples, and all its associated supplements,
i.e., here in this case any additional lexical material
which also occurs in the nominal group of the PP,
such as articles, adjectives, adverbs, cardinals, etc.3
The extraction of the associated supplements is es-
sential to the linguistic measure described in sub-
section 3.3 below.
In order to reduce the amount of candidates
for evaluation and to eliminate low-frequency data,
we only considered PNV-triples with frequency
  
. This was also motivated by the well-
known fact that collocations tend to have a higher
co-occurrence frequency than free word combina-
tions.4 Table 1 contains the data for the correspond-
ing frequency distributions.
frequency PP-verb combinations
candidate tokens candidate types
all 1,663,296 1,159,133

	
279,350 8,644
Table 1: Frequency distribution for PP-Verb tokens and
types for our 114-million-word newspaper corpus
3.2 Classification of the Testset
Three human judges manually classified the PP-
verb candidate types with
  
in regard to
whether they were a collocation or not. For this
purpose, they used a manual, in which the guide-
lines included the linguistic properties as described
in Section 1 and the three collocation classes identi-
fied in Section 2.
Among the 8,644 PP-verb candidate types, 1,180
(13.7%) were identified as true collocations. The
inter-annotator agreement was 94.8% (with a stan-
dard deviation of 2.1).
2The verbs in this study are restricted to main verbs and are
reduced to their base form after extraction.
3It should be noted that both heads and associated supple-
ments may of course vary depending on the particular linguistic
structure targeted for collocation extraction.
4Cf. also Evert and Krenn (2001) for empirical evidence
justifying the exclusion of low-frequency data.
3.3 The Linguistic Measure
The linguistic property around which we built our
measure for collocativity is the non- or limited mod-
ifiabilty of collocations with additional lexical ma-
terial (i.e., supplements). The underlying assump-
tion is that a PNV triple is less modifiable (and thus
more likely to be a collocation) if it has a lexical
supplement which, compared to all others, is par-
ticularly characteristic. We express this assump-
tion in the following way: Let  be the number
of distinct supplements of a particular PNV triple
( ffProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 89?96
Manchester, August 2008
Are Morpho-Syntactic Features More Predictive for
the Resolution of Noun Phrase Coordination Ambiguity
than Lexico-Semantic Similarity Scores?
Ekaterina Buyko and Udo Hahn
Jena University
Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
ekaterina.buyko|udo.hahn@uni-jena.de
Abstract
Coordinations in noun phrases often pose
the problem that elliptified parts have to
be reconstructed for proper semantic inter-
pretation. Unfortunately, the detection of
coordinated heads and identification of el-
liptified elements notoriously lead to am-
biguous reconstruction alternatives. While
linguistic intuition suggests that semantic
criteria might play an important, if not su-
perior, role in disambiguating resolution
alternatives, our experiments on the re-
annotated WSJ part of the Penn Treebank
indicate that solely morpho-syntactic crite-
ria are more predictive than solely lexico-
semantic ones. We also found that the
combination of both criteria does not yield
any substantial improvement.
1 Introduction
Looking at noun phrases such as
?cat and dog owner?
?novels and travel books?
their proper coordination reading (and asymmetric
distribution of coordinated heads) as
?cat owner? AND ?dog owner?
?novels? AND ?travel books?
seems to be licensed by the striking semantic sim-
ilarity between ?cat? and ?dog?, and ?novels? and
?books?, respectively. If this were a general rule,
then automatic procedures for the resolution of co-
ordination ambiguities had to rely on the a priori
provision of potentially large amounts of semantic
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
background knowledge to make this similarity ex-
plicit. Furthermore, any changes in languages or
domains where such resources were missing (or,
were incomplete) would severely hamper coordi-
nation analysis.
Indeed, previous research has gathered lot of
evidence that conjoined elements tend to be se-
mantically similar. The important role of seman-
tic similarity criteria for properly sorting out con-
juncts was first tested by Resnik (1999). He in-
troduced an information-content-based similarity
measure that uses WORDNET (Fellbaum, 1998) as
a lexico-semantic resource and came up with the
claim that semantic similarity is helpful to achieve
higher coverage in coordination resolution for co-
ordinated noun phrases of the form ?noun1 and
noun2 noun3? than similarity measures based on
morphological information only.
In a similar vein, Hogan (2007b) inspected
WORDNET similarity and relatedness measures
and investigated their role in conjunct identifi-
cation. Her data reveals that several measures
of semantic word similarity can indeed detect
conjunct similarity. For the majority of these
similarity measures, the differences between the
mean similarity of coordinated elements and non-
coordinated ones were statistically significant.
However, it also became evident that these were
only slight differences, and not all coordinated
heads were semantically related as evidenced, e.g.,
by ?work?/?harmony? in ?hard work and harmony?.
The significance tests did also not reveal particu-
larly useful measures for conjunct identification.
Rus et al (2002) in an earlier study presented an
alternative heuristics-based approach to conjunct
identification for coordinations of the form ?noun1
and noun2 noun3?. They exploit, e.g., look-ups
in WORDNET for a compound noun as a con-
89
cept, and for the sibling relation between nouns in
the coordination and report bracketing precision of
87.4% on 525 candidate coordinations. Although
the authors demonstrated that WORDNET was re-
ally helpful in coordination resolution, the eval-
uation was only conducted on compound nouns
extracted from WORDNET?s noun hierachy and,
furthermore, the senses of nouns were manually
tagged in advance for the experiments.
Despite this preference for semantic criteria, one
might still raise the question how far non-semantic
criteria might guide the resolution of noun phrase
coordination ambiguities, e.g., by means of the dis-
tribution of resolution alternatives in a large corpus
or plain lexical or morpho-syntactic criteria. This
idea has already been explored before by various
researchers from different methodological angles
including distribution-based statistical approaches
(e.g., Chantree et al (2005), Nakov and Hearst
(2005)), similarity-based approaches incorporat-
ing orthographical, morpho-syntactic, and syntac-
tic similarity criteria (e.g., Agarwal and Boggess
(1992), Okumura and Muraki (1994)), as well as
a combination of distribution information and syn-
tactic criteria (Hogan, 2007a).
Statistical approaches enumerate all candidate
conjuncts and calculate the respective likelihood
according to a distribution estimated on a cor-
pus. For the coordination ?movie and television
industry? the distributional similarity of ?movie?
and ?industry? and the collocation frequencies of
the pairs [?movie? - ?industry?] and [?television? -
?industry?] would be compared against each other.
However, for such an approach only an F-measure
under 50% was reported (Chantree et al, 2005).
Unsupervised Web-distribution-based algorithms
(Nakov and Hearst, 2005) achieved 80% on the
disambiguation of coordinations of the fixed form
?noun1 and noun2 noun3?. Hogan (2007a) pre-
sented a method for the disambiguation of noun
phrase coordination by modelling two sources of
information, viz. distribution-based similarity be-
tween conjuncts and the dependency between con-
junct heads. This method was incorporated in
Bikel?s parsing model (Bikel, 2004) and achieved
an increase in NP coordination dependency F-
score from 69.9% to 73.8%.
Similarity-based approaches consider those el-
ements of a coordination as conjuncts which are
most ?similar? under syntactic, morphological, or
even semantic aspects. Agarwal and Boggess
(1992) include in their NP coordination analysis
syntactic and some semantic information about
candidate conjuncts and achieve an accuracy boost
up to 82%. Okumura and Muraki (1994) estimate
the similarity of candidate conjuncts by means of
a similarity function which incorporates syntactic,
orthographical, and semantic information about
the conjuncts. The model provides about 75% ac-
curacy.
The resolution of coordination ambiguity can
also be tried at parsing time. Charniak and John-
son (2005), e.g., supply a discriminative reranker
that uses e.g., features to capture syntactic paral-
lelism across conjuncts. The reranker achieves an
F-score of 91%.
Recently, discriminative learning-based ap-
proaches were proposed, which exploit only lex-
ical, morpho-syntactic features and the symmetry
of conjuncts. Shimbo and Hara (2007) incorpo-
rate morpho-syntactic and symmetry features in
a discriminative learning model and end up with
57% F-measure on the GENIA corpus (Ohta et al,
2002). Buyko et al (2007) employ Conditional
Random Fields (Lafferty et al, 2001) and success-
fully tested this technique in the biomedical do-
main for the identification and resolution of ellipti-
fied conjuncts. They evaluate on the GENIA corpus
and report an F-score of 93% for the reconstruc-
tion of the elliptical conjuncts employing lexical
and morpho-syntactic criteria only. At least two
questions remain ? whether the latter approach
can achieve similar results in the newswire lan-
guage domain (and is thus portable), and whether
the incorporation of additional semantic criteria in
this approach might boost the resolution rate, or
not (and is thus possibly more parsimonious). The
latter question is the main problem we deal with in
this paper.
2 Data Sets for the Experiments
2.1 Coordination Annotation in the PENN
TREEBANK
For our experiments, we used the WSJ part of the
PENN TREEBANK (Marcus et al, 1993). Some re-
searchers (e.g., Hogan (2007a)) had recently found
several inconsistencies in its annotation of the
bracketing of coordinations in NPs. These bugs
were shown to pose problems for training and test-
ing of coordination resolution and parsing tools.
Fortunately, a re-annotated version has been pro-
vided by Vadas and Curran (2007), with a focus
90
on the internal structure of NPs. They added addi-
tional bracketing annotation for each noun phrase
in the WSJ section of the PENN TREEBANK as-
suming a right-bracketing structure in NPs. In ad-
dition, they introduced tags, e.g., ?NML? for ex-
plicitly marking any left-branching constituents as
in
(NP (NML (JJ industrial) (CC and) (NN food))
(NNS goods))
where ?industrial? and ?food? are conjuncts. In the
example
(NP (DT some) (NN food) (CC and) (NN house-
hold) (NNS goods))
the structure of the noun phrase is already cor-
rect and should not be annotated further, since
?household goods? is already right-most and is co-
ordinated with ?food?. Still, in the original PENN
TREEBANK annotation, we find annotations of
noun phrases such as
(NP (NN royalty) (CC and) (NP (NN rock)
(NNS stars)))
that remain unchanged after the re-annotation pro-
cess.
2.2 Coordination Corpus
We, first, extracted a set of 3,333 non-nested NP
coordinations involving noun compounds and one
conjunction, with a maximal number of nine nouns
(no prepositional phrases were considered). We
focused on two patterns in the re-annotated WSJ
portion:
(1) Noun phrases containing at least two nouns and
a conjunction as sister nodes as in
(NP (NML (NN movie) (CC and) (NN book))
(NNS pirates))
or in
(NP (DT some) (NN food) (CC and) (NN house-
hold) (NNS goods))
(2) Noun phrases containing at least two noun
phrases and a conjunction as sister nodes (as
they remained unchanged from the original PENN
TREEBANK version). Thereby, the second noun
phrase contains at least two nouns as sister nodes
as in
(NP (NP (NNP France)) (CC and) (NP (NNP
Hong) (NNP Kong)))
We removed from this original set NPs which
could not be reduced to the following pattern:1
1These are typically coordinations of the form ?(W )N1 and
N2?, e.g., ?government sources and lobbyists?, where W is a
sequence of i tokens (i ? 0). 646 coordinations of this type
occurred in the WSJ portion of the PTB.
(W ) N1 and (W ) N2 N3,
where (W ) is a sequence of i tokens with i ? 0 as
in ?street lampsN1 and ficusN2 treesN3?.
The remaining major data set (A) then contained
2,687 NP coordinations. A second data set (B)
was formed, which is a proper subset of A and
contained only those coordination structures that
match the following pattern:
(X) N1 and (W ) N2 N3,
where (X) is defined as a sequence of i tokens
(i ? 0) with all part-of-speech (POS) tags except
nouns and (W ) defined as above; e.g., ?a happy
catN1 and dogN2 ownerN3?. Test set B contains,
in our opinion, a selection of less ?hard? coordi-
nations from the set A, and includes 1,560 items.
All these patterns focus on three forms of con-
junctions, namely ?and?, ?or?, and ?but not?, which
connect two conjuncts (the extension of which
varies in our data from one up to maximally eight
tokens as in ?London?s ?Big Bang? 1986 deregu-
lation and Toronto?s ?Little Bang? the same year?.
The remainders from the conjunctions and the
two conjuncts in a coordinated NP are called
shared elements (e.g., ?owner? and ?a happy? in
the above example). It is evident that the correct
recognition of conjunct boundaries allows for the
proper identification of the shared elements.
Set A contains 1,455 coordinations where N1
and N3 are coordinated (e.g, ?food and household
goods?) and 1,232 coordinations where N1 and N2
are coordinated (e.g., ?cotton and acetate fibers?).
Set B consists of 643 coordinations where N1 and
N3 are coordinated and 917 coordinations where
N1 and N2 are coordinated.
The extracted data sets were converted into an
IO representation of tokens labeled as ?C? for con-
junct, ?CC? for conjunction, and ?S? for the shared
element(s). The noun phrase ?cotton and acetate
fibers?, e.g., is represented as a sequence ?C CC
C S?, while ?food and household goods? is repre-
sented as a sequence ?C CC C C?.
3 Methods
We here compare three different approaches to the
resolution of noun phrase coordination ambiguity,
viz. ones relying solely on morpho-syntactic infor-
mation, solely on lexico-semantic information, and
a cumulative combination of both. As far as se-
mantic information is concerned we make use of
various WORDNET similarity measures.
91
3.1 Baselines
We used three baselines for resolving noun phrase
coordination ambiguities ? one incorporating
only lexico-semantic information, the WordNet
Similarity baseline, and two alternative ones in-
corporating only morpho-syntactic and syntactic
parse information, the Number Agreement and the
Bikel Parser baseline, respectively.
3.1.1 WORDNET Similarity (WN) Baseline
Our lexico-semantic baseline comes with
WORDNET semantic similarity scores of puta-
tively coordinated nouns. For our experiments,
we used the implementation of WORDNET simi-
larity and relatedness measures provided by Ted
Pedersen.2 The following similarity measures
were considered: two measures based on path
lenghts between concepts (path and lch (Leacock
et al, 1998)), three measures based on informa-
tion content, i.e., corpus-based measures of the
specificity of a concept (res (Resnik, 1999), lin
(Lin, 1998), and jcn (Jiang and Conrath, 1997)).
Furthermore, we used two relatedness measures,
namely, lesk (Banerjee and Pedersen, 2003) and
vector (Patwardhan et al, 2003), which score the
similarity of the glosses of both concepts. We
applied these similarity measures to any pair of
putatively coordinated nouns in the noun phrases
from our data sets, A and B. To determine poten-
tial conjuncts we calculate two similarity scores
relative to the structures discussed in Section 2.2:
s1 = sim(N1,N2) and s2 = sim(N1,N3)
Our final score is the maximum over both scores
which is then the semantic indicator for the most
plausible resolution of the coordination.
3.1.2 Number Agreement (NA) Baseline
We compared here the number agreement
between selected nouns (see Resnik (1999)).
Accordingly, N1 and N2 are coordinated, if
number(N1) = number(N2) AND number(N1) 6=
number(N3), while N1 and N3 are coordinated, if
number(N1) = number(N3) AND number(N1) 6=
number(N2).
3.1.3 Post-Processing Heuristics
In the WN and NA baselines, after the detection
of coordinated elements we used simple heuris-
tics to tag the remaining part of the noun phrase.
If N1 and N2 were hypothesized to be coordi-
nated, then all tokens preceding N1 were tagged as
2http://www.d.umn.edu/?tpederse/
shared elements, N3 was tagged as shared element
as well, while all tokens between the conjunction
and N2 were tagged as conjuncts. For example,
in ?a happy dogN1 and catN2 ownerN3? we identify
?dog? and ?cat? as coordinated elements and tag ?a
happy? and ?owner? as shared elements. The final
resolution looks like ?S S C CC C S?. If N1 and N3
were hypothesized to be coordinated, then all other
elements except conjunctions were tagged as parts
of conjuncts, as well.
3.1.4 Bikel Parser (BP) Baseline
We used the well-known Bikel Parser (Bikel,
2004) in its original version and the one used by
Collins (2003). We trained both of them only
with NPs extracted from the re-annotated version
of WSJ (see Section 2) and converted the bracket-
ing output of the parsers to the IO representation
for NP coordinations for further evaluations.
3.2 Chunking of Conjuncts with CRFs
The approach to conjunct identification presented
by Buyko et al (2007) employs Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001),3 which
assign a label to each token of coordinated NPs
according to its function in the coordination: ?C?
for conjuncts, ?CC? for conjunctions, and ?S? for
shared elements. Since non-nested conjuncts can
be assumed to be in a sequential order, sequen-
tial learning approaches (instead of single position
classification approaches) seem appropriate here.
Buyko et al (2007) report an F-measure of 93%
on conjunct identification in the GENIA corpus.
They use a feature set including lexical (words),
and morpho-syntactic features (POS tags, morpho-
syntactic similarity of putative conjuncts), but ex-
clude any semantic criteria. The morpho-syntactic
similarity features were generated from a rule-
based approach to conjunct identification using the
maximal symmetry of conjuncts as constituted by
their respective POS annotation.
We here intend to apply this approach for resolv-
ing coordination ambiguities involving noun com-
pounds in the newswire language such as ?presi-
dent and chief executive?. This restricts the spec-
trum of considered coordinations in noun phrases
to more complicated cases than those considered
by Buyko et al (2007). We will thus test the vari-
ous resolution models under harder test conditions,
3They employ the linear-chain CRF implementation from
the MALLET toolsuite available at http://mallet.cs.
umass.edu/index.php/Main_Page
92
Feature Class Description
default feature prior probability distribution over the
possible argument labels
lexical word
morpho-
syntactic
the token?s POS tag; output labels of the
morpho-syntactic similarity (?C?,?CC?
and ?S?) (see Buyko et al (2007)); out-
put labels of the number agreement
baseline (?C?, ?CC? and ?S?)
semantic WN output labels of the WORDNET similar-
ity baseline (?C?, ?CC? and ?S?)
contextual conjunctions of all features of neighbor-
ing tokens (two tokens to the left and
one token to the right)
Table 1: Feature Classes Used for Conjunct Iden-
tification
since Buyko et al consider, e.g., adjective coordi-
nations in noun phrases such as ?positive and neg-
ative IL-2 regulator? that are predominant in the
biomedical language domain.
We also propose in this work an extension of the
feature space in terms of lexico-semantic features
(see Table 1), information that originates from sim-
ilarity computations on WORDNET data. Further-
more, we do not use orthographical features of the
original approach as they are well suited only for
the biomedical language domain.
4 Results and Error Analysis
To evaluate the different approaches to conjunct
identification, we used recall and precision scores
since they are well suited for the evaluation of seg-
mentation tasks. Two types of decisions were eval-
uated ? the assignment of ?C? labels denoting con-
juncts in terms of the F-measure, and (given the
tagged conjuncts) the accuracy of the complete co-
ordination resolution. A coordination is resolved
properly only, if all tokens of both conjuncts are
correctly identified.
We carried out a ten-fold cross-validation of all
ML-based methods (Bikel parser (Bikel, 2004) and
CRF-based conjunct identification (Buyko et al,
2007)). For the evaluation of the NA and WN base-
lines, we tested their performance on the complete
data sets, A and B (see Section 2).
As Table 2 depicts the NA baseline achieved an
accuracy of 28.4% on A (36.6% on B), the Bikel
parser reached 77.2% on A (73.4% in B), while
the WN baseline got in its best run (vector mea-
sure) an accuracy of 41.7% on A (49.6% on B).
These results already reveal that parsing almost
dramatically outperforms the coordination resolu-
tion based on the NA similarity by up to 35.5%
points. The results of the WN baseline indicate
that the best similarity measure for conjunct iden-
tification is the vector similarity (Patwardhan et
al., 2003) that scores the similarity between the
glosses of the concepts.
Our error analysis of the WN baseline on the
test set A reveals that its low accuracy has var-
ious reasons. First, about 37% of the coordina-
tions could not be resolved due to the absence of at
least one noun involved in the coordination from
the WORDNET. These coordinations usually in-
clude named entities such as person and organi-
zation names (e.g., ?brothers Francis and Gilbert
Gros?). These coverage gaps have clearly a nega-
tive effect on the resolution results for all WORD-
NET similarity measures.
To find out errors which are specific for the
considered similarity measures, we have chosen
the res measure and inspected the analysis results
on all noun phrases where nouns are covered by
WORDNET. The remaining set of coordinations
contains 1,740 noun phrases. 1,022 coordinations
(59%) of this set were completely resolved by the
WN baseline, while 1,117 coordinations (64% of
the remaining part, 41.5% of the test set A) could
be at least partly resolved. Obviously, the coordi-
nated heads are properly detected by the res mea-
sure but our heuristics for tagging the remaining
modifiers (see Subsection 3.1.3) fail to provide the
correct conjunct boundaries.
623 coordinations (36%) were mis-classified by
the res measure. A closer look at this data re-
veals two types of errors. The first and minor
type is the misleading selection of putatively co-
ordinated heads N1, N2, and N3. We presuppose
in the WN baseline that the heads appear right-
most in the noun phrase, although that is not al-
ways the case as illustrated by the phrase ?North-
ern California earthquake and Hurricane Hugo?.
The res measure detected correctly a higher sim-
ilarity between ?earthquake? (N1) and ?hurricane?
(N2), but ?Hugo? (N3) is a modifier of ?hurricane?.
Although the res measure works fine, the coordi-
nation cannot be properly resolved due to syntac-
tic reasons. In some cases, N2 is wrongly selected
as in ?life and health insurance operation? where
the WN baseline selects ?insurance? as right-most
noun (except the last noun ?operation?) and not
?health?.4
4
?turbineN2 ? is, however, correctly selected in ?steam tur-
bine and gas turbine plants?.
93
Set A Set B
Recall/Precision/F-Score Accuracy Recall/Precision/F-Score Accuracy
NA 32.7 / 75.9 / 45.7 28.4 41.6 / 83.9 / 55.6 36.6
Bikel 85.6 / 85.4 / 85.5 77.2 83.8 / 83.6 / 83.7 73.4
Bikel (Collins) 85.9 / 85.7 / 85.8 77.5 83.6 83.4 / 83.5 72.9
WN jcn 45.6 / 69.3 / 55.0 36.2 54.7 / 72.1 / 62.2 41.2
WN lch 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4
WN lesk 49.2 / 66.2 / 56.5 38.1 59.3 / 70.3 / 64.3 43.3
WN lin 44.7 / 69.9 / 54.6 35.5 53.5 / 72.6 / 61.6 40.5
WN res 45.9 / 71.8 / 56.0 37.4 55.7 / 75.8 / 64.2 43.8
WN path 48.7 / 70.8 / 57.7 39.2 57.8 / 74.1 / 65.0 44.4
WN vector 51.2 / 68.9 / 58.8 41.7 62.8 / 74.5 / 68.1 49.6
CRF (default), contextual 75.2 / 72.4 / 73.8 60.3 77.9 / 75.1 / 76.5 63.1
+ Lexical, morpho-syntactic 87.1 / 87.2 / 87.1 77.9 88.1/ 88.0 / 88.0 78.8
+ WN (lesk) 87.2 / 87.2 / 87.2 78.0 88.2/ 88.2 / 88.2 79.1
CRF (default), contextual + only WN (lesk) 79.3 / 78.4 / 78.9 64.8 81.2 / 80.6 / 80.9 66.9
CRF (default), contextual + only morpho-
syntactic
86.2 / 86.3 / 86.3 76.6 87.6 / 87.6 / 87.6 78.1
Table 2: F-measure of Conjunct Identification and Accuracy of Coordination Resolution on the WSJ
Section of the PENN TREEBANK Corpus
The second type of error comes as erroneous
classifications of the res measure such as in ?hos-
pitals and blood banks? where ?hospitals? and
?blood? have a higher similarity than ?hospitals?
and ?banks? although they are, in fact, coordi-
nated here. ?hotels and large restaurant chains?,
?records and music publishing?, ?chemicals and
textiles company? are other examples for the obser-
vation that the coordinated elements have a lower
similarity as non-coordinated ones.
We also carried out a ten-fold cross-validation of
the CRF-based approach for the conjunct identifi-
cation. First of all, the CRF-based approach (with
and without WN similarity) achieved the highest
accuracy score ? up to 78.0% on set A, and 79.1%
on B ? compared with all other approaches we
scrutinized on. We also tested the performance of
the original semantics-free approach and the ad-
ditional effects of the WORDNET similarity mea-
sures (see Table 2). Although the integration of se-
mantic information leads to a mild gain compared
with the original approach (up to 0.3% points, with
the lesk measure), the results indicate that no sub-
stantial benefit can be traced to semantic features.
We ran several tests with solely morpho-
syntactic features (as enumerated in Table 1) and
solely WN features, too. They reveal that solely
morpho-syntactic features are up to 11.8% points
more predictive than WN features. The best re-
sults were still achieved using the gloss-oriented
lesk measure (see Table 3).
The inspection of the errors types from the var-
ious runs is not fully conclusive though. After
adding WN features to both sets, we detected some
improvements for conjunct tagging with high WN
similarity. Some conjunct boundaries could be cor-
rected as in ?record and movie producer? where,
in the first run, ?producer? was tagged as a con-
junct and was corrected as being shared by inte-
grating WN features. But we also detected a de-
grading tagging behavior of conjuncts with WN
features where the WN similarity was not helpful
at all as in ?chairman and chief designer? where
?chairman? and ?chief? under the influence of WN
features were judged to be conjuncts. We found
out that the addition of WN features positively in-
fluences the classification of coordinations where
N1 and N2 are coordinated, while it increased er-
rors in the classification of coordinations where N1
and N3 are coordinated.
In addition, we calculated intersections between
the set A error data (unique) of the res WN base-
line (1400 phrases) and the error data of the CRF
approach without WN features (391), and the er-
ror data of the CRF approach with WN features
(385), respectively. These error data sets con-
tain noun phrases where coordinated heads could
not be properly detected. The set of the res WN
baseline and the set of the CRF approach with-
out WN features have an intersection of 230 in-
stances, where 138 instances could not be found
in the WORDNET. That means that for about 161
instances (59%) in the mis-classified data of the
CRF approach the additional WN features would
not be helpful. The intersection remains similar
(226 instances) between the set of the res WN base-
94
Default, Context, WN Recall Accu-
Lexical, Morpho- Sim Precision / racy
Syntactic Feats. F-Score
? 88.1/ 88.0 / 88.0 78.8
? jcn 87.9/ 87.8 / 87.9 78.3
? lch 87.9 / 87.9 / 87.9 78.5
? lesk 88.2/ 88.2 / 88.2 79.1
? lin 88.0 / 88.0 / 88.0 78.8
? res 88.2 / 88.2 / 88.2 79.0
? path 87.9 / 87.9 / 87.9 78.5
? vector 87.8 / 87.7 / 87.7 78.2
Table 3: Conjunct Identification ? Cross-
validation on the WSJ section of the PENN TREE-
BANK Corpus on Test Set B
line and the set of the CRF approach enriched with
WN features. The intersection between the errror
sets of the both CRF approaches includes 352 in-
stances. The integration of the WN features was
not helful for almost the complete error data from
the original CRF approach. We have previously
shown that the res WN baseline features correlate
with the correct label sequence for only 1,117 co-
ordinations (41.5%) of the complete evaluation set
A and the features thus do not seem to be effective
in our approach.
Furthermore, we evaluated the results of the
CRF approach only for the correct detection of
coordinated heads (see above for the res measure
and intersection counts) and disregarded the mod-
ifier classification. The results ? 85.3% on set A
and 85.4% on set B ? reveal that the classification
of modifiers is a major source of classification er-
rors. In both configurations the problematic noun
phrases are the ones with (e.g., adjectival) modi-
fiers. The boundaries of conjuncts are not properly
recognized in such noun phrases, as for example in
?American comic book and television series? where
the correct label sequence is ?S C C CC C S?, since
?American? is the shared modifier of ?book? and
?television?, while ?comic? just modifies ?book?.
As most adjectives appearing at the beginning
of the noun phrase as in ?medical products and
services company? tend to be used as shared modi-
fiers of coordinations in our data, this, erroneously,
leads to false taggings, e.g., ?personal? in ?per-
sonal computer and software design? as a shared
element. To cope adequately with modifiers we
need to integrate more appropriate features such
as collocation frequencies of modifiers and coor-
dinated heads. The detection of a higher collo-
cation frequency of ?personal computer? in com-
parison to ?personal software? (e.g., using the pro-
cedures proposed by Wermter and Hahn (2004))
would help tagging the conjunct boundaries.
5 Conclusions and Future Work
We investigated the problem of noun phrase co-
ordination resolution as a segmentation problem
among conjuncts involved in the coordination.
While resolving coordination ellipsis is often con-
sidered as a semantically constrained problem, we
wanted to assess a less ?costly? solution strategy,
namely relying on ?cheaper? to get syntactic crite-
ria as much as possible, though not sacrificing the
accurary of resolutions.
We, first looked at morpho-syntactic criteria
only and lexico-semantic criteria only, and then at
the combination of both approaches. The evalu-
ation results from a variety of experiments reveal
that the major part of ambiguous coordinations
can be resolved using solely morpho-syntactic fea-
tures. Surprising as it might be, the semantic in-
formation as derived from the WORDNET sim-
ilarity measures does not yield any further sub-
stantial improvement for our approach. This is
somehow counter-intuitive, but our findings, un-
like those from earlier studies which emphasized
the role of semantic criteria, are based on exten-
sive corpus data ? the PENN TREEBANK.
Results from our error analysis will guide future
work to further boost results. Particular empha-
sis will be laid on the integration of named en-
tity recognizers, collocation frequencies and dis-
tributional similarity data as also advocated by
Chantree et al (2005).
The presented sequential labeling-based ap-
proach to coordination resolution was here ap-
plied to the resolution of a special type of ambigu-
ous noun phrases. In general, this approach can
easily be applied to the resolution of other types
of coordinative structures in noun phrases as al-
ready presented in Buyko et al (2007). As far as
other phrasal types (e.g., verbal phrases) are con-
cerned, long-distance coordinations play a much
more prominent role. The token-based labeling ap-
proach may be thus substituted by a chunk-based
approach operating on sentences.
Acknowledgements
This research was partly funded by the German
Ministry of Education and Research within the
STEMNET project (01DS001A-C) and by the EC
within the BOOTSTREP project (FP6-028099).
95
References
Agarwal, R. and L. Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 15?21.
Newark, DE, USA, 28 June - 2 July 1992.
Banerjee, S. and T. Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In IJ-
CAI?03 ? Proceedings of the 18th International Joint
Conference on Artificial Intelligence, pages 805?
810. Acapulco, Mexico, August 9-15, 2003.
Bikel, D. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Buyko, E., K. Tomanek, and U. Hahn. 2007. Reso-
lution of coordination ellipses in biological named
entities using Conditional Random Fields. In PAC-
LING 2007 - Proceedings of the 10th Conference of
the Pacific Association for Computational Linguis-
tics, pages 163?171. Melbourne, Australia, Septem-
ber 19-21, 2007.
Chantree, F. A. Kilgarriff, A. de Roeck, and A. Willis.
2005. Disambiguating coordinations using word dis-
tribution information. In RANLP 2005 ? Proceed-
ings of the Intl. Conference on ?Recent Advances
in Natural Language Processing?, pages 144?151.
Borovets, Bulgaria, 21-23 September, 2005.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking.
In ACL?05 ? Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 173?180. Ann Arbor, MI, 25-30 June 2005.
Collins, M. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Fellbaum, C., editor. 1998. WORDNET: An Electronic
Lexical Database. MIT Press.
Hogan, D. 2007a. Coordinate noun phrase disam-
biguation in a generative parsing model. In ACL?07
? Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 680?
687. Prague, Czech Republic, June 28-29, 2007.
Hogan, D. 2007b. Empirical measurements of lexi-
cal similarity in noun phrase conjuncts. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics. Demo and Poster Ses-
sions, pages 149?152. Prague, Czech Republic, June
28-29, 2007.
Jiang, J. and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In ROCLING-X ? Proceedings of the 1997 Inter-
national Conference on Research in Computational
Linguistics. Taipei, Taiwan, August 22-24, 1997.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML-2001
? Proceedings of the 18th International Conference
on Machine Learning, pages 282?289. Williams
College, MA, USA, June 28 - July 1, 2001.
Leacock, C., M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WORDNET relations
for sense identification. Computational Linguistics,
24(1):147?165.
Lin, D. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304.
Madison, WI, USA, July 24-27, 1998.
Marcus, M., B. Santorini, and M.-A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The PENN TREEBANK. Computational Linguistics,
19(2):313?330.
Nakov, P. and M. Hearst. 2005. Using the Web as an
implicit training set: Application to structural ambi-
guity resolution. In HLT-EMNLP?05 ? Proceedings
of the 5th Human Language Technology Conference
and 2005 Conference on Empirical Methods in Nat-
ural Language Processing, pages 835?842. Vancou-
ver, B.C., Canada, October 6-8, 2005.
Ohta, T., Y. Tateisi, and J.-D. Kim. 2002. The GE-
NIA corpus: An annotated research abstract corpus
in molecular biology domain. In HLT 2002 ? Pro-
ceedings of the 2nd International Conference on Hu-
man Language Technology Research, pages 82?86.
San Diego, CA, USA, March 24-27, 2002.
Okumura, A. and K. Muraki. 1994. Symmetric pattern
matching analysis for English coordinate structures.
In ANLP 1994 ? Proceedings of the 4th Conference
on Applied Natural Language Processing, pages 41?
46. Stuttgart, Germany, 13-15 October 1994.
Patwardhan, S., S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In CICLing 2003 ? Proceed-
ings 4th Intl. Conference on Computational Linguis-
tics and Intelligent Text Processing, pages 241?257.
Mexico City, Mexico, February 16-22, 2003.
Resnik, P. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal
of Artificial Intelligence Research, 11:95?130.
Rus, V., D. Moldovan, and O. Bolohan. 2002. Bracket-
ing compound nouns for logic form derivation. In
FLAIRS 2002 ? Proceedings of the 15th Interna-
tional Florida Artificial Intelligence Research Soci-
ety Conference, pages 198?202. Pensacola Beach,
FL, USA, May 14-16, 2002.
Shimbo, M. and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In EMNLP-
CoNLL 2007 ? Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 610?619. Prague,Czech Republic,
June 28-29, 2007.
Vadas, D. and J. Curran. 2007. Adding noun phrase
structure to the PENN TREEBANK. In ACL?07 ? Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 240?247.
Prague, Czech Republic, June 28-29, 2007.
Wermter, J. and U. Hahn. 2004. Collocation extraction
based on modifiability statistics. In COLING 2004 ?
Proceedings of the 20th International Conference on
Computational Linguistics, pages 980?986. Geneva,
Switzerland, August 23-27, 2004.
96
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 486?495, Prague, June 2007. c?2007 Association for Computational Linguistics
An Approach to Text Corpus Construction which Cuts Annotation Costs
and Maintains Reusability of Annotated Data
Katrin Tomanek Joachim Wermter Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30
D-07743 Jena, Germany
{tomanek|wermter|hahn}@coling-uni-jena.de
Abstract
We consider the impact Active Learning
(AL) has on effective and efficient text cor-
pus annotation, and report on reduction rates
for annotation efforts ranging up until 72%.
We also address the issue whether a corpus
annotated by means of AL ? using a particu-
lar classifier and a particular feature set ? can
be re-used to train classifiers different from
the ones employed by AL, supplying alter-
native feature sets as well. We, finally, report
on our experience with the AL paradigm un-
der real-world conditions, i.e., the annota-
tion of large-scale document corpora for the
life sciences.
1 Introduction
The annotation of corpora has become a crucial pre-
requisite for NLP utilities which rely on (semi-) su-
pervised machine learning (ML) techniques. While
stability, by and large, has been reached for tagsets
up until the syntax layer, semantic annotations in
terms of (named) entities, semantic roles, proposi-
tions, events, etc. reveal a high degree of variability
due to the inherent domain-dependence of the under-
lying tagsets. This diversity fuels a continuous need
for creating semantic annotation data anew.
Accordingly, annotation activities will persist and
even increase in number as HLT is expanding on
various technical and scientific domains (e.g., the
life sciences) outside the classical general-language
newspaper genre. Since the provision of annota-
tions is a costly, labor-intensive and error-prone pro-
cess the amount of work and time this activity re-
quires should be minimized to the extent that corpus
data could still be used to effectively train ML-based
NLP components on them. The approach we ad-
vocate does exactly this and yields reduction gains
(compared with standard procedures) ranging be-
tween 48% to 72%, without seriously sacrificing an-
notation quality.
Various techniques to minimize the necessary
amount of annotated training material have al-
ready been investigated. In co-training (Blum and
Mitchell, 1998), e.g., from a small initial set of la-
beled data multiple learners mutually provide new
training material for each other by labeling unseen
examples. Pierce and Cardie (2001) have shown,
however, that for tasks which require large numbers
of labeled examples ? such as most NLP tasks ? co-
training might be inadequate because it tends to gen-
erate noisy data. Furthermore, a well compiled ini-
tial training set is a crucial prerequisite for success-
ful co-training. As another alternative for minimiz-
ing annotation work, active learning (AL) is based
on the idea to let the learner have control over the ex-
amples to be manually labeled so as to optimize the
prediction accuracy. Accordingly, AL aims at select-
ing those examples with high utility for the model.
AL (as well as semi-supervised methods) is typi-
cally considered as a learning protocol, i.e., to train
a particular classifier. In contrast, we here propose
to employ AL as a corpus annotation method. A
corpus built on these premises must, however, still
be reusable in a flexible way so that, e.g., train-
ing with modified or improved classifiers is feasible
and reasonable on AL-generated corpora. Baldridge
and Osborne (2004) have already argued that this is
a highly critical requirement because the examples
selected by AL are tuned to one particular classi-
fier. The second major contribution of this paper ad-
486
dresses this issue and provides empirical evidence
that corpora built with one type of classifier (based
on Maximum Entropy) can reasonably be reused by
another, methodologically related type of classifier
(based on Conditional Random Fields) without re-
quiring changes of the corpus data. We also show
that feature sets being used for training classifiers
can be enhanced without invalidating corpus annota-
tions generated on the basis of AL and, hence, with
a poorer feature set.
2 Related Work
There are mainly two methodological strands of
AL research, viz. optimization approaches which
aim at selecting those examples that optimize some
(algorithm-dependent) objective function, such as
prediction variance (Cohn et al, 1996), and heuris-
tic methods with uncertainty sampling (Lewis and
Catlett, 1994) and query-by-committee (QBC) (Se-
ung et al, 1992) just to name the most prominent
ones. AL has already been applied to several NLP
tasks, such as document classification (Schohn and
Cohn, 2000), POS tagging (Engelson and Dagan,
1996), chunking (Ngai and Yarowsky, 2000), statis-
tical parsing (Thompson et al, 1999; Hwa, 2000),
and information extraction (Lewis and Catlett, 1994;
Thompson et al, 1999).
In a more recent study, Shen et al (2004) consider
AL for entity recognition based on Support Vector
Machines. Here, the informativeness of an exam-
ple is estimated by the distance to the hyperplane of
the currently learned SVM. It is assumed that an ex-
ample which lies close to the hyperplane has high
chances to have an effect on training. This approach
is essentially limited to the SVM learning scheme as
it solely relies on SVM-internal selection criteria.
Hachey et al (2005) propose a committee-based
AL approach where the committee?s classifiers con-
stitute multiple views on the data by employing dif-
ferent feature subsets. The authors focus on (pos-
sible) negative side effects of AL on the annota-
tions. They argue that AL annotations are cogni-
tively more difficult to deal with for the annota-
tors (because of the increased complexity of the se-
lected sentences). Hence, lower annotation quality
and higher per-sentence annotation times might be a
concern.
There are controversial findings on the reusabil-
ity of data annotated by means of AL for the prob-
lem of parse tree selection. Whereas Hwa (2001) re-
ports positive results, Baldridge and Osborne (2004)
argue that AL based on uncertainty sampling may
face serious performance degradation when labeled
data is reused for training a classifier different from
the one employed during AL. For committee-based
AL, however, there is a lack of work on reusabil-
ity. Our experiments of committee-based AL for en-
tity recognition, however, reveal that for this task at
least, reusability can be guaranteed to a very large
extent.
3 AL for Corpus Annotation -
Requirements for Practical Use
AL frameworks for real-world corpus annotation
should meet the following requirements:
fast selection time cycles ? AL-based corpus an-
notation is an interactive process in which b
sentences are selected by the AL engine for hu-
man annotation. Once the annotated data is
supplied, the AL engine retrains its underly-
ing classifier(s) on all available annotations and
then re-classifies all unseen corpus items. After
that the most informative (i.e., deviant) b sen-
tences from the set of newly classified data are
selected for the next iteration round. In this ap-
proach the time needed to select the next exam-
ples (which is the idle time of the human an-
notators) has to be kept at an acceptable limit
of a few minutes only. There are various AL
strategies which ? although they yield theoreti-
cally near-optimal sample selection ? turn out
to be actually impractible for real-world use
because of excessively high computation times
(cf. Cohn et al (1996)). Thus, AL-based an-
notation should be based on a computationally
tractable and task-wise feasible and acceptable
selection strategy (even if this might imply a
suboptimal reduction of annotation costs).
reusability ? The examples AL selects for man-
ual annotation are dependent on the model be-
ing used, up to a certain extent (Baldridge and
Osborne, 2004). During annotation time, how-
ever, the best model might not be known and
487
model tuning (especially the choice of features)
is typically performed once a training corpus
is available. Hence, from a practical point of
view, the resulting corpus should be reusable
with modified classifiers as well.
adaptive stopping criterion ? An explicit and
adaptive stopping criterion which is sensitive
towards the already achieved level of quality of
the annotated corpus is clearly preferred over
stopping after an a priori fixed number of an-
notation iterations.
If these requirements, especially the first and the
second one, cannot be guaranteed for a specific an-
notation task one should refrain from using AL. The
efficiency of AL-driven annotation (in terms of the
time needed to compile high quality training mate-
rial) might be worse compared to the annotation of
randomly (or subjectively) selected examples.
4 Framework for AL-based Named Entity
Annotation
For named entity recognition (NER), each change
of the application domain requires a more or less
profound change of the types of semantic categories
(tags) being used for corpus annotation. Hence, one
may encounter a lack of training material for various
relevant (sub)domains. Once this data is available,
however, one might want to modify the features of
the final classifier with respect to the specific entity
types. Thus, a corpus annotated by means of AL has
to provide the flexibility to modify the features of
the final classifier.
To meet the requirements from above under the
constraints of a real-world annotation task, we
decided for QBC-based AL, a heuristic AL ap-
proach, which is computationally less complex and
resource-greedy than objective function AL meth-
ods (the latter explicitly quantify the differences be-
tween the current and an ideal classifier in terms
of some objective function). Accordingly, we ruled
out uncertainty sampling, another heuristic AL ap-
proach, because it was shown before that QBC is
more efficient and robust (Freund et al, 1997).
QBC is based on the idea to select those examples
for manual annotation on which a committee of clas-
sifiers disagree most in their predictions (Engelson
and Dagan, 1996). A committee consists of a num-
ber of k classifiers of the same type (same learning
algorithm, parameters, and features) but trained on
different subsets of the training data. QBC-based
AL is also iterative. In each AL round the com-
mittee?s k classifiers are trained on the already an-
notated data C, then a pool of unannotated data P
is predicted with each classifier resulting in n au-
tomatically labeled versions of P . These are then
compared according to their labels. Those with the
highest variance are selected for manual annotation.
4.1 Selection Strategy
In each iteration, a batch of b examples is selected
for manual annotation. The informativeness of an
example is estimated in terms of the disagreement,
i.e., the uncertainty among the committee?s classi-
fiers on classifying a particular example. This is
measured by the vote entropy (Engelson and Dagan,
1996), i.e., the entropy of the distribution of classi-
fications assigned to an example by the classifiers.
Vote entropy is defined on the token level t as:
Dtok(t) := ?
1
log k
?
li
V (li, t)
k log
V (li, t)
k
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. As (named) en-
tities often span more than a single text token we
consider complete sentences as a reasonable exam-
ple size unit1 for AL and calculate the disagreement
of a sentence Dsent as the mean vote entropy of
its single tokens. Since the vote entropy is mini-
mal when all classifiers agree in their vote, sentences
with high disagreement are preferred for manual an-
notation. With informed decisions of human anno-
tators made available, the potential for future dis-
agreement of the classifier committee on conflicting
instances should decrease. Thus, each AL iteration
selects the b sentences with the highest disagreement
to focus on the most controversial decision prob-
lems.
Besides informativeness, additional criteria can
be envisaged for the selection of examples, e.g., di-
1Sentence-level examples are but one conceivable grain size
? lower grains (such as clauses or phrases) as well as higher
grains (e.g., paragraphs or abstracts) are equally possible, with
different implications for the AL process.
488
feature class description
orthographical based on regular expressions (e.g. Has-
Dash, IsGreek, ...), token transforma-
tion rule: capital letters replaced by ?A?,
lowercase letters by ?a?, digits by ?0?,
etc. (e.g., IL2 ? AA0, have ? aaaa)
lexical and
morphological
prefix and suffix of length 3, stemmed
version of each token
syntactic the token?s part-of-speech tag
contextual features of neighboring tokens
Table 1: Features used for AL
versity of a batch and representativeness of the re-
spective example (to avoid outliers) (Shen et al,
2004). We experimented with these more sophis-
ticated selection strategies but preliminary experi-
ments did not reveal any significant improvement of
the AL performance. Engelson and Dagan (1996)
confirm this observation that, in general, different
(and even more refined) selection methods still yield
similar results. Moreover, strategies incorporating
more selection criteria often require more parame-
ters to be set. However, proper parametrization is
hard to achieve in real-world applications. Using
disagreement exclusively for selection requires only
one parameter, viz. the batch size b, to be specified.
4.2 Classifier and Features
For our AL framework we decided to employ a Max-
imum Entropy (ME) classifier (Berger et al, 1996).
We employ a rich set of features (see Table 1) which
are general enough to be used in most (sub)domains
for entity recognition. We intentionally avoided us-
ing features such as semantic triggers or external
dictionary look-ups because they depend a lot on
the specific subdomain and entity types being used.
However, one might add them to fin- tune the final
classifier, if needed. ME classifiers outperform their
generative counterparts (e.g., Na??ve Bayesian clas-
sifiers) because they can easily handle overlapping,
probably dependent features which might be con-
tained in rich feature sets. We also favored an ME
classifier over an SVM one because the latter is com-
putationally much more complex on rich feature sets
and multiple classes and is thus not so well suited for
an interactive process like AL.
It has been shown that Conditional Random
Fields (CRF) (Lafferty et al, 2001) achieve higher
performance on many NLP tasks, such as NER, but
on the other hand they are computionally more com-
plex than an ME classifier making them also im-
practical for the interactive AL process. Thus, in
our committee we employ ME classifiers to meet re-
quirement 1 (fast selection time cycles). However,
in the end we want to use the annotated corpora to
train a CRF and will thus examine the reusability
of such an ME-annotated AL corpus for CRFs (cf.
Subsection 5.2).
4.3 Stopping Criterion
A question hardly addressed up until now is when to
actually terminate the AL process. Usually, it gets
stopped when the supervized learning performance
of the specific classifier is achieved. The problem
with such an approach is, however, that in prac-
tice one does not know the performance level which
could possibly be achieved on an unannotated cor-
pus.
An apparent way to monitor the progress of the
annotation process is to periodically (e.g., after each
AL iteration) train a classifier on the data annotated
so far and evaluate it against some randomly se-
lected gold standard. When the relative performance
growth of each AL iteration falls below a certain
threshold this might be a good reason to stop the an-
notation. Though this is probably the most reliable
way, it is impractical for many scenarios since as-
sembling and manually annotating a representative
gold standard may already be quite a laborious task.
Thus, a measure from which we can predict the de-
velopment of the learning curve would be beneficial.
One way to achieve this goal is to monitor the rate
of disagreement among the different classifiers after
each iteration. This rate will descend as the classi-
fiers get more and more robust in their predictions
on unseen data. Thus, an average disagreement ap-
proaching zero can be interpreted as an indication
that additional annotations will not render any fur-
ther improvement. In our experiments, we will show
that this is a valid stopping criterion, indeed.
5 Experiments and Results
For our experiments, we specified the following
three parameters: the batch size b (i.e., the num-
ber of sentences to be selected for each AL itera-
tion), the size and composition of the initial train-
489
ing set, and the number of k classifiers in a com-
mittee. The smaller the batch size, the higher the
AL performance turns out to be. In the special case
of batch size of b = 1 only that example with the
highest disagreement is selected. This is certainly
impractical since after each AL iteration a new com-
mittee of classifiers has to be trained causing unwar-
ranted annotation idle time. We found b = 20 to
be a good compromise between the annotators? idle
time and AL performance. The initial training set
also contains 20 sentences which are randomly se-
lected though. Our committee consists of k = 3
classifiers, which is a good trade-off between com-
putational complexity and diversity. Although the
AL iterations were performed on the sentence level,
we report on the number of annotated tokens. Since
sentences may considerably vary in their length the
number of tokens constitutes a better measure for an-
notation costs.
We ran our experiments on two common entity-
annotated corpora from two different domains (see
Table 2). From the general-language newspaper do-
main, we used the English data set of the CoNLL-
2003 shared task (Tjong Kim Sang and De Meul-
der, 2003). It consists of a collection of newswire
articles from the Reuters Corpus,2 which comes
annotated with three entity types: persons, loca-
tions, and organizations. From the sublanguage
biology domain we used the oncology part of the
PENNBIOIE corpus which consists of some 1150
PubMed abstracts. Originally, this corpus contains
gene, variation event, and malignancy entity annota-
tions. Manual annotation after each AL round was
simulated by moving the selected sentences from
the pool of unannotated sentences P to the train-
ing corpus T . For our simulations, we built two
subcorpora by filtering out entity annotations: the
PENNBIOIE gene corpus (PBgene), including the
three gene entity subtypes generic, protein, and rna,
and the PENNBIOIE variation events corpus (PB-
var) corpus including the variation entity subtypes
type, event, location, state-altered, state-generic,
and state-original. We split all three corpora into
two subsets, viz. AL simulation data and gold stan-
dard data on which we evaluate3 a classifier in terms
2http://trec.nist.gov/
3We use a strict evaluation criterion which only counts exact
matches as true positives because annotations having incorrect
corpus data set sentences tokens
CONLL AL 14,040 203,617
3 entities Gold 3,453 46,435
PBGENE AL 10,050 249,490
3 entities Gold 1,114 27,563
PBVAR AL 10,050 249,490
6 entities Gold 1,114 27,563
Table 2: Corpora used in the Experiments
of f-score trained on the annotated corpus after each
AL iteration (learning curve). As far as the CoNLL
corpus is concerned, we have used CoNLL?s training
set for AL and CoNLL?s test set as gold standard. As
for PBgene and PBvar, we randomly split the cor-
pora into 90% for AL and 10% as gold standard.
In the following experiments we will refer to the
classifiers used in the AL committee as selectors,
and the classifier used for evaluation as the tester.
5.1 Efficiency of AL and the Applicability of
the Stopping Criterion
In a first series of experiments, we evaluated whether
AL-based annotations can significantly reduce the
human effort compared to the standard annotation
procedure where sentences are selected randomly
(or subjectively). We also show that disagreement
is an accurate stopping criterion. As described in
Section 4.2, we here employed a committee of ME
classifiers for AL; a CRF was used as tester for both
the AL and the random selection. Figures 1, 2, and 3
depict the learning curves for AL selection and ran-
dom selection (upper two curves) and the respective
disagreement curves (lower curve). The random se-
lection curves contained in these plots are averaged
over three random selection runs.
With AL, we get a maximum f-score of ? 84.5%
on the CoNLL corpus after about 118,000 tokens. At
about the same number of tokens the disagreement
curve drops down to values of around Dsent = 0.
Comparing AL and random selection, an f-score of
? 84% is reached after 86,000 and 165,000 tokens,
respectively, which means a reduction of annotation
costs of about 48%. On PBgene, the effect of AL is
comparable: a maximum value of 83.5% f-score is
reached first after about 124,000 tokens, a data point
where hardly any disagreement between the com-
mittee?s classifiers occurs. For, e.g., an f-score of
boundaries are insufficient for manual corpus annotation.
490
Figure 1: CoNLL Corpus: Learning/Disagreement Curves
Figure 2: PBgene Corpus: Learning/Disagreement Curves
Figure 3: PBvar Corpus: Learning/Disagreement Curves
corpus selection F tokens reduction
CONLL random 84.0 165,000
AL 84.0 86,000 ? 48%
PBGENE random 83.0 101,000
AL 83.0 213,000 ? 53%
PBVAR random 80.0 56,000
AL 80.0 200,000 ? 72%
Table 3: Reduction of Annotation Costs Achieved
with AL-based Annotation
83%, the annotation effort can be reduced by about
53% using AL. On PBvar, an f-score of about 80%
is reached after ? 56,000 tokens when using AL se-
lection, while 200,000 tokens are needed with ran-
dom selection. For this task, AL reduces the an-
notation effort by of 72%. Here, the disagreement
curve approaches values of zero after approximately
80,000 tokens. At about this point the learning curve
reaches its maximum of about 81% f-score. Ta-
ble 3 summarizes the reduction of annotation costs
achieved on all three corpora.
Comparing both PENNBIOIE simulations, obvi-
ously, the reduction of annotation costs through AL
is much higher for the variation type entities than for
the gene entities. We hypothesize this to be mainly
due to incomparable entity densities. Whereas the
gene entities are quite frequent (about 1.3 per sen-
tence on average), the variation entities are rather
sparse (0.62 per sentence on average) making it an
ideal playground for AL-based annotation. Our ex-
periments also reveal that disagreement approaching
values of zero is a valid stopping criterion. This is,
under all circumstances, definitely the point when
AL-based annotation should stop because then all
classifiers of the committee vote consistently. Any
further selection ? even though AL selection is used
? is then, actually, a random selection. If, due to
reasons whatsoever, further annotations are wanted,
a direct switch to random selection is advisable be-
cause this is computationally less expensive than
AL-based selection.
5.2 Reusability
To evaluate whether the proposed AL framework for
named entity annotation allows for flexible re-use
of the annotated data, we performed experiments
where we varied both the learning algorithms and
the features of the selectors.
491
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
Figure 4: Algorithm Flexibility on PBvar
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
AL (CRF committee)
AL (ME committee)
AL (NB committee)
random selection
Figure 5: AlgorithmFflexibility on CoNLL
First, we analyzed the effect of different proba-
bilistic classifiers as selectors on the resulting learn-
ing curve of the CRF tester. Figures 4 and 5 show
the learning curves on our original ME committee,
a CRF committee, and also a committee of Na??ve
Bayes (NB) classifiers. It is not surprising that self-
reuse (CRF selectors and CRF tester) yields the best
results. Switching from CRF selectors to ME selec-
tors has almost no negative effect. Even with a com-
mittee of NB selectors (an ML approach which is
essentially less well suited for the NER task), AL-
based selection is still substantially more efficient
than random selection on both corpora. This shows
that our approach to use the less complex ME clas-
sifiers for the AL selection process has the positive
effect of fast selection cycle times at almost no costs.
This is especially interesting as the performance of
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
all features
sub1
sub2
sub3
random selection
Figure 6: Feature Flexibility on PBvar
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
14012010080604020
F-
sc
or
e
K tokens
all features
sub1
sub2
sub3
random selection
Figure 7: Feature Flexibility on ConLL
an ME classifier trained in supervized manner on
the complete corpus is significantly worse (several
percentage points of f-measure) than a CRF. That
means, even though an ME classifier is less well
suited as the final classifier, it works well as a se-
lector for CRFs.4
Second, we ran experiments on selectors with
only some features and our CRF tester with all fea-
tures (cf. Table 1). Feature subset 1 (sub1) contains
all but the syntactic features. In the second subset
(sub2), also morphological and lexical features are
missing. The third set (sub3) only contains ortho-
graphical features. We ran an AL simulation for
4We have also conducted experiments where we varied the
learning algorithms of the tester (we experimented with NB,
ME, MEMM, and CRFs) ? with comparable results. In a real-
istic scenario, however, on would rather choose a CRF as final
tester over, e.g., a NB.
492
each feature subset with a committee of CRF se-
lectors.5 Figures 6 and 7 show the various learning
curves. Here we see that a corpus that was produced
with AL on sub1 can easily be re-used by a tester
with little more features. This is probably the most
realistic scenario: the core features are kept and
only a few specific features (e.g., POS, a dictionary
look-up, chunk information, etc.) are added. When
adding substantially more features to the tester than
were available during AL time, the respective learn-
ing curves drop down towards the learning curve for
random selection. But even with a selector which
has only orthographical features and a tester with
many more features ? which is actually quite an ex-
treme example and a rather unrealistic scenario for
a real-world application ? AL is more efficient than
random selection. However, the limits of reusability
are taking shape: on PBvar, the AL selection with
sub3 converges with the random selection curve af-
ter about 100,000 tokens.
5.3 Findings with Real AL Annotation
We currently perform AL entity mention annotations
for an information extraction project in the biomedi-
cal subdomain of immunogenetics. For this purpose,
we retrieved about 200,000 abstracts (? 2,000,000
sentences) as our document pool of unlabeled exam-
ples from PUBMED. By means of random subsam-
pling, only about 40,000 sentences are considered in
each round of AL selection. To regularly monitor
classifier performance, we also perform gold stan-
dard (GS) annotations on 250 randomly chosen ab-
stracts (? 2,200 sentences). In all our annotations of
different entity types so far, we found AL learning
curves similar to the ones reported in our simula-
tion experiments, with classifier performance level-
ling off at around 75% - 85% f-score (depending on
the entity type).
Our annotations also reveal that AL is especially
beneficial when entity mentions are very sparse.
Figure 8 shows the cumulated entity density on AL
and gold standard annotations of cytokine receptors
(specialized proteins for which we annotated six dif-
ferent entity subtypes) ? very sparse entity types
with less than one entity mention per PUBMED ab-
stract on the average. As can be seen, after 2,000
5Here, we employed CRF instead of ME selectors to isolate
the effect of feature re-usability.
 0
 500
 1000
 1500
 2000
 2500
 3000
 200  400  600  800  1000  1200  1400  1600  1800  2000
e
n
tit
y 
m
en
tio
ne
s
sentences
GS annotation
AL annotation
Figure 8: Cumulated Entity Density on AL and GS
Annotations of Cytokine Receptors
sentences the entity density in our AL corpus is al-
most 15 times higher than in our GS corpus. Such a
dense corpus may be more appropriate for classifier
training than a sparse one yielded by random or se-
quential annotations, which may just contain lots of
negative training examples. We have observed com-
parable effects with other entity types, too, and thus
conclude that the sparser entity mentions of a spe-
cific type are in texts, the more beneficial AL-based
annotation is. We report on other aspects of AL for
real annotation projects in Tomanek et al (2007).
6 Discussion and Conclusions
We have shown, for the annotation of (named) en-
tities, that AL is well-suited to speed up annotation
work under realistic conditions. In our simulations
we yielded gains (in the number of tokens) up to
72%. We collected evidence that an average dis-
agreement approaching zero may serve as an adap-
tive stopping criterion for AL-driven annotation and
that a corpus compiled by means of QBC-based AL
is to a large extent reusable by modified classifiers.
These findings stand in contrast to those supplied
by Baldridge and Osborne (2004) who focused on
parse selection. Their research indicates that AL on
selectors with different learning algorithms and fea-
ture sets then used by the tester can easily get worse
than random selection. They conclude that it might
not be be advisable to employ AL in environments
where the final classifier is not very stable.
Our evidence leads us to a re-assessment of AL-
493
based annotations. First, we employed a committee-
based (QBC) while Baldridge and Osborne per-
formed uncertainty sampling AL. Committee-based
approaches calculate the uncertainty on an exam-
ple in a more implicit way, i.e., by the disagree-
ment among the committee?s classifiers. With uncer-
tainty sampling, however, the labeling uncertainty
of one classifier is considered directly. In future
work we will directly compare QBC and uncertainty
sampling with respect to data reusability. Second,
whereas Baldridge and Osborne employed AL on a
scoring or ranking problem we focused on classifica-
tion problems. Further research is needed to inves-
tigate whether the problem class (classification with
a fixed and moderate number of classes vs. ranking
large numbers of possible candidates) is responsible
for limited data reusability.
On the basis of our experiments we stipulate that
the proposed AL approach might be applicable with
comparable results to a wider range of corpus anno-
tation tasks, which otherwise would require substan-
tially larger amounts of annotation efforts.
Acknowledgements
This research was funded by the EC within the
BOOTStrep project (FP6-028099), and by the Ger-
man Ministry of Education and Research within the
StemNet project (01DS001A to 1C).
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Dekang Lin
and Dekai Wu, editors, EMNLP 2004 ? Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing, pages 9?16. Barcelona, Spain,
July 25-26, 2004. Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT?98 ? Proceedings of the 11th Annual Confer-
ence on Computational Learning Theory, pages 92?
100. Madison, Wisconsin, USA, July 24-26, 1998.
New York, NY: ACM Press.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artifical Intelligence Research, 4:129?145.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL?96 ? Proceedings of the 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 319?326. University of California at Santa
Cruz, California, U.S.A., 24-27 June 1996. San Fran-
cisco, CA: Morgan Kaufmann.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL-2005 ? Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151. Ann Arbor, MI, USA, June
2005. Association for Computational Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In EMNLP/VLC-2000 ? Proceed-
ings of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 45?52. Hong Kong, China, Oc-
tober 7-8, 2000.. Association for Computational Lin-
guistics.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Walter Daelemans and Re?mi
Zajac, editors, CoNLL-2001 ? Proceedings of the
5th Natural Language Learning Workshop. Toulouse,
France, 6-7 July 2001. Association for Computational
Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In ICML-2001 ? Proceedings of the 18th In-
ternational Conference on Machine Learning, pages
282?289. Williams College, MA, USA, June 28 - July
1, 2001. San Francisco, CA: Morgan Kaufmann.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In William W. Cohen and Haym Hirsh, editors, ICML
?94: Proceedings of the 11th International Conference
on Machine Learning, pages 148?156. San Francisco,
CA: Morgan Kaufmann.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In ACL?00 ? Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 117?125. Hong Kong, China,
1-8 August 2000. San Francisco, CA: Morgan Kauf-
mann.
494
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Lillian Lee and Donna Harman, editors,
EMNLP 2001 ? Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1?9. Pittsburgh, PA, USA, June 3-4, 2001.
Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In ICML
?00: Proceedings of the 17th International Conference
on Machine Learning, pages 839?846. San Francisco,
CA: Morgan Kaufmann.
H. Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In COLT?92 ?
Proceedings of the 5th Annual Conference on Compu-
tational Learning Theory, pages 287?294. Pittsburgh,
PA, USA, July 27-29, 1992. New York, NY: ACM
Press.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In ACL?04 ?
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 589?596.
Barcelona, Spain, July 21-26, 2004. San Francisco,
CA: Morgan Kaufmann.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In ICML
?99: Proceedings of the 16th International Conference
on Machine Learning, pages 406?414. Bled, Slovenia,
June 1999. San Francisco, CA: Morgan Kaufmann.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared
task: Language-independent named entity recogni-
tion. In Walter Daelemans and Miles Osborne, edi-
tors, CoNLL-2003 ? Proceedings of the 7th Confer-
ence on Computational Natural Language Learning,
pages 142?147. Edmonton, Canada, 2003. Association
for Computational Linguistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Efficient annotation with the Jena ANnota-
tion Environment (JANE). In Proceedings of the ACL
2007 ?Linguistic Annotation Workshop ? A Merger of
NLPXML 2007 and FLAC 2007?. Prague, Czech Re-
public, June 28-29, 2007. Association for Computa-
tional Linguistics (ACL).
495
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 843?850, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Paradigmatic Modiability Statistics for the Extraction
of Complex Multi-Word Terms
Joachim Wermter
Jena University
Language & Information Engineering
JULIE Lab
Friedrich-Schiller-Universit?at Jena
F?urstengraben 30
D-07743 Jena, Germany
joachim.wermter@uni-jena.de
Udo Hahn
Jena University
Language & Information Engineering
JULIE Lab
Friedrich-Schiller-Universit?at Jena
F?urstengraben 30
D-07743 Jena, Germany
udo.hahn@uni-jena.de
Abstract
We here propose a new method which sets
apart domain-specific terminology from
common non-specific noun phrases. It
is based on the observation that termino-
logical multi-word groups reveal a con-
siderably lesser degree of distributional
variation than non-specific noun phrases.
We define a measure for the observable
amount of paradigmatic modifiability of
terms and, subsequently, test it on bigram,
trigram and quadgram noun phrases ex-
tracted from a 104-million-word biomedi-
cal text corpus. Using a community-wide
curated biomedical terminology system as
an evaluation gold standard, we show that
our algorithm significantly outperforms
a variety of standard term identification
measures. We also provide empirical ev-
idence that our methodolgy is essentially
domain- and corpus-size-independent.
1 Introduction
As we witness the ever-increasing proliferation of
volumes of medical and biological documents, the
available dictionaries and terminological systems
cannot keep up with this pace of growth and, hence,
become more and more incomplete. What?s worse,
the constant stream of new terms is increasingly get-
ting unmanageable because human curators are in
the loop. The costly, often error-prone and time-
consuming nature of manually identifying new ter-
minology from the most recent literature calls for
advanced procedures which can automatically assist
database curators in the task of assembling, updat-
ing and maintaining domain-specific controlled vo-
cabularies. Whereas the recognition of single-word
terms usually does not pose any particular chal-
lenges, the vast majority of biomedical or any other
domain-specific terms typically consists of multi-
word units.1 Unfortunately these are much more
difficult to recognize and extract than their singleton
counterparts. Moreover, although the need to assem-
ble and extend technical and scientific terminologies
is currently most pressing in the biomedical domain,
virtually any (sub-)field of human research/expertise
in which we deal with terminologically structured
knowledge calls for high-performance terminology
identification and extraction methods. We want to
target exactly this challenge.
2 Related Work
The automatic extraction of complex multi-word
terms from domain-specific corpora is already an
active field of research (cf., e.g., for the biomedi-
cal domain Rindflesch et al (1999), Collier et al
(2002), Bodenreider et al (2002), or Nenadic? et
al. (2003)). Typically, in all of these approaches
term candidates are collected from texts by vari-
ous forms of linguistic filtering (part-of-speech tag-
ging, phrase chunking, etc.), through which candi-
dates obeying various linguistic patterns are iden-
tified (e.g., noun-noun, adjective-noun-noun com-
binations). These candidates are then submitted to
frequency- or statistically-based evidence measures
1Nakagawa and Mori (2002) claim that more than 85% of
domain-specific terms are multi-word units.
843
(such as the C-value (Frantzi et al, 2000)), which
compute scores indicating to what degree a candi-
date qualifies as a term. Term mining, as a whole,
is a complex process involving several other com-
ponents (orthographic and morphological normal-
ization, acronym detection, conflation of term vari-
ants, term context, term clustering; cf. Nenadic? et al
(2003)). Still, the measure which assigns a termhood
value to a term candidate is the essential building
block of any term identification system.
For multi-word automatic term recognition
(ATR), the C-value approach (Frantzi et al, 2000;
Nenadic? et al, 2004), which aims at improving the
extraction of nested terms, has been one of the most
widely used techniques in recent years. Other po-
tential association measures are mutual information
(Damerau, 1993) and the whole battery of statisti-
cal and information-theoretic measures (t-test, log-
likelihood, entropy) which are typically employed
for the extraction of general-language collocations
(Manning and Schu?tze, 1999; Evert and Krenn,
2001). While these measures have their statistical
merits in terminology identification, it is interesting
to note that they only make little use of linguistic
properties inherent to complex terms.2
More linguistically oriented work on ATR by
Daille (1996) or on term variation by Jacquemin
(1999) builds on the deep syntactic analysis of term
candidates. This includes morphological and head-
modifier dependency analysis and thus presupposes
accurate, high-quality parsing which, for sublan-
guages at least, can only be achieved by a highly
domain-dependent type of grammar. As sublan-
guages from different domains usually reveal a high
degree of syntactic variability among each other
(e.g., in terms of POS distribution, syntactic pat-
terns), this property makes it difficult to port gram-
matical specifications to different domains.
Therefore, one may wonder whether there are
cross-domain linguistic properties which might be
beneficial to ATR and still could be accounted for
by only shallow syntactic analysis. In this paper,
we propose the limited paradigmatic modiability of
terms as a criterion which meets these requirements
and will elaborate on it in detail in Subsection 3.3.
2A notable exception is the C-value method which incorpo-
rates a term?s likelihood of being nested in other multi-word
units.
3 Methods and Experiments
3.1 Text Corpus
We collected a biomedical training corpus of ap-
proximately 513,000 MEDLINE abstracts using the
following query composed of MESH terms from
the biomedical domain: transcription factors, blood
cells and human.3 We then annotated the result-
ing 104-million-word corpus with the GENIA part-
of-speech tagger4 and identified noun phrases (NPs)
with the YAMCHA chunker (Kudo and Matsumoto,
2001). We restrict our study to NP recognition
(i.e., determining the extension of a noun phrase but
refraining from assigning any internal constituent
structure to that phrase), because the vast majority of
technical or scientific terms surface as noun phrases
(Justeson and Katz, 1995). We filtered out a num-
ber of stop words (determiners, pronouns, measure
symbols, etc.) and also ignored noun phrases with
coordination markers (?and?, ?or?, etc.).5
n-gram cut-off NP term candidates
length tokens types
no cut-off 5,920,018 1,055,820bigrams c ? 10 4,185,427 67,308
no cut-off 3,110,786 1,655,440trigrams c ? 8 1,053,651 31,017
no cut-off 1,686,745 1,356,547quadgrams c ? 6 222,255 10,838
Table 1: Frequency distribution for noun phrase term candi-
date tokens and types for the MEDLINE text corpus
In order to obtain the term candidate sets (see Ta-
ble 1), we counted the frequency of occurrence of
noun phrases in our training corpus and categorized
them according to their length. For this study, we re-
stricted ourselves to noun phrases of length 2 (word
bigrams), length 3 (word trigrams) and length 4
(word quadgrams). Morphological normalization of
term candidates has shown to be beneficial for ATR
(Nenadic? et al, 2004). We thus normalized the nom-
3MEDLINE (http://www.ncbi.nlm.nih.gov) is the
largest biomedical bibliographic database. For information re-
trieval purposes, all of its abstracts are indexed with a controlled
indexing vocabulary, the Medical Subject Headings (MESH,
2004).
4http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/postagger/
5Of course, terms can also be contained within coordinative
structures (e.g., ?B and T cell?). However, analyzing their in-
herent ambiguity is a complex syntactic operation, with a com-
paratively marginal benefit for ATR (Nenadic? et al, 2004).
844
inal head of each noun phrase (typically the right-
most noun in English) via the full-form UMLS SPE-
CIALIST LEXICON (UMLS, 2004), a large repository
of both general-language and domain-specific (med-
ical) vocabulary. To eliminate noisy low-frequency
data (cf. also Evert and Krenn (2001)), we defined
different frequency cut-off thresholds, c, for the bi-
gram, trigram and quadgram candidate sets and only
considered candidates above these thresholds.
3.2 Evaluating Term Extraction Quality
Typically, terminology extraction studies evaluate
the goodness of their algorithms by having their
ranked output examined by domain experts who
identify the true positives among the ranked can-
didates. There are several problems with such an
approach. First, very often only one such expert
is consulted and, hence, inter-annotator agreement
cannot be determined (as, e.g., in the studies of
Frantzi et al (2000) or Collier et al (2002)). Fur-
thermore, what constitutes a relevant term for a par-
ticular domain may be rather difficult to decide ?
even for domain experts ? when judges are just ex-
posed to a list of candidates without any further con-
text information. Thus, rather than relying on ad
hoc human judgment in identifying true positives in
a candidate set, as an alternative we may take al-
ready existing terminolgical resources into account.
They have evolved over many years and usually re-
flect community-wide consensus achieved by expert
committees. With these considerations in mind, the
biomedical domain is an ideal test bed for evaluat-
ing the goodness of ATR algorithms because it hosts
one of the most extensive and most carefully curated
terminological resources, viz. the UMLS METATHE-
SAURUS (UMLS, 2004). We will then take the mere
existence of a term in the UMLS as the decision cri-
terion whether or not a candidate term is also recog-
nized as a biomedical term.
Accordingly, for the purpose of evaluating the
quality of different measures in recognizing multi-
word terms from the biomedical literature, we as-
sign every word bigram, trigram, and quadgram in
our candidate sets (see Table 1) the status of being
a term (i.e., a true positive), if it is found in the
2004 edition of the UMLS METATHESAURUS.6 For
6We exclude UMLS vocabularies not relevant for molecular
biology, such as nursing and health care billing codes.
example, the word trigram ?long terminal repeat?
is listed as a term in one of the UMLS vocabular-
ies, viz. MESH (2004), whereas ?t cell response?
is not. Thus, among the 67,308 word bigram candi-
date types, 14,650 (21.8%) were identified as true
terms; among the 31,017 word trigram candidate
types, their number amounts to 3,590 (11.6%), while
among the 10,838 word quadgram types, 873 (8.1%)
were identified as true terms.7
3.3 Paradigmatic Modifiability of Terms
For most standard association measures utilized for
terminology extraction, the frequency of occurrence
of the term candidates either plays a major role
(e.g., C-value), or has at least a significant impact
on the assignment of the degree of termhood (e.g.,
t-test). However, frequency of occurrence in a train-
ing corpus may be misleading regarding the deci-
sion whether or not a multi-word expression is a
term. For example, taking the two trigram multi-
word expressions from the previous subsection, the
non-term ?t cell response? appears 2410 times in
our 104-million-word MEDLINE corpus, whereas
the term ?long terminal repeat? (long repeating se-
quences of DNA) only appears 434 times (see also
Tables 2 and 3 below).
The linguistic property around which we built our
measure of termhood is the limited paradigmatic
modiability of multi-word terminological units. A
multi-word expression such as ?long terminal re-
peat? contains three token slots in which slot 1 is
filled by ?long?, slot 2 by ?terminal? and slot 3 by
?repeat?. The limited paradigmatic modiability of
such a trigram is now defined by the probability with
which one or more such slots cannot be filled by
other tokens. We estimate the likelihood of preclud-
ing the appearance of alternative tokens in particular
slot positions by employing the standard combina-
tory formula without repetitions. For an n-gram (of
size n) to select k slots (i.e., in an unordered selec-
tion) we thus define:
C(n, k) = n!k!(n? k)! (1)
7As can be seen, not only does the number of candidate
types drop with increasing n-gram length but also the propor-
tion of true terms. In fact, their proportion drops more sharply
than can actually be seen from the above data because the vari-
ous cut-off thresholds have a leveling effect.
845
For example, for n = 3 (word trigram) and k = 1
and k = 2 slots, there are three possible selections
for each k for ?long terminal repeat? and for ?t cell
response? (see Tables 2 and 3). k is actually a place-
holder for any possible token (and its frequency)
which fills this position in the training corpus.
n-gram freq P -Mod (k=1,2)
long terminal repeat 434 0.03
k slots possible selections sel freq modsel
k = 1 k1 terminal repeat 460 0.940
long k2 repeat 448 0.970
long terminal k3 436 0.995
mod1 =0.91
k = 2 k1 k2 repeat 1831 0.23
k1 terminal k3 1062 0.41
long k2 k3 1371 0.32
mod2 =0.03
Table 2: P -Mod and k-modifiabilities for k = 1 and k = 2
for the trigram term ?long terminal repeat?
n-gram freq P -Mod (k=1,2)
t cell response 2410 0.00005
k slots possible selections sel freq modsel
k = 1 k1 cell response 3248 0.74
t k2 response 2665 0.90
t cell k3 27424 0.09
mod1 =0.06
k = 2 k1 k2 response 40143 0.06
k1 cell k3 120056 0.02
t k2 k3 34925 0.07
mod2 =0.00008
Table 3: P -Mod and k-modifiabilities for k = 1 and k = 2
for the trigram non-term ?t cell response?
Now, for a particular k (1 ? k ? n; n = length of
n-gram), the frequency of each possible selection,
sel, is determined. The paradigmatic modifiability
for a particular selection sel is then defined by the
n-gram?s frequency scaled against the frequency of
sel. As can be seen in Tables 2 and 3, a lower fre-
quency induces a more limited paradigmatic modifi-
ability for a particular sel (which is, of course, ex-
pressed as a higher probability value; see the column
labeled modsel in both tables). Thus, with s being
the number of distinct possible selections for a par-
ticular k, the k-modiability, modk, of an n-gram
can be defined as follows (f stands for frequency):
modk(n-gram) :=
s
?
i=1
f(n-gram)
f(seli, n-gram)
(2)
The paradigmatic modiability, P -Mod, of an n-
gram is the product of all its k-modifiabilities:8
P -Mod(n-gram) :=
n
?
k=1
modk(n-gram) (3)
Comparing the trigram P -Mod values for k =
1, 2 in Tables 2 and 3, it can be seen that the term
?long terminal repeat? gets a much higher weight
than the non-term ?t cell response?, although their
mere frequency values suggest the opposite. This is
also reflected in the respective list rank (see Subsec-
tion 4.1 for details) assigned to both trigrams by the
t-test and by our P -Mod measure. While ?t cell re-
sponse? has rank 24 on the t-test output list (which
directly reflects its high frequency), P -Mod assigns
rank 1249 to it. Conversely, ?long terminal repeat?
is ranked on position 242 by the t-test, whereas it
occupies rank 24 for P -Mod. In fact, even lower-
frequency multi-word units gain a prominent rank-
ing, if they exhibit limited paradigmatic modifiabil-
ity. For example, the trigram term ?porphyria cu-
tanea tarda? is ranked on position 28 by P -Mod,
although its frequency is only 48 (which results in
rank 3291 on the t-test output list). Despite its lower
frequency, this term is judged as being relevant for
the molecular biology domain.9 It should be noted
that the termhood values (and the corresponding list
ranks) computed by P -Mod also include k = 3 and,
hence, take into account a reasonable amount of fre-
quency load. As can be seen from the previous rank-
ing examples, still this factor does not override the
paradigmatic modifiability factors of the lower ks.
On the other hand, P -Mod will also demote true
terms in their ranking, if their paradigmatic modifi-
ability is less limited. This is particularly the case if
one or more of the tokens of a particular term often
occur in the same slot of other equal-length n-grams.
For example, the trigram term ?bone marrow cell?
occurs 1757 times in our corpus and is thus ranked
quite high (position 31) by the t-test. P -Mod, how-
ever, ranks this term on position 550 because the to-
8Setting the upper limit of k to n (e.g., n = 3 for trigrams)
actually has the pleasant side effect of including frequency in
our modifiability measure. In this case, the only possible selec-
tion k1k2k3 as the denominator of Formula (2) is equivalent to
summing up the frequencies of all trigram term candidates.
9It denotes a group of related disorders, all of which arise
from a deficient activity of the heme synthetic enzyme uropor-
phyrinogen decarboxylase (URO-D) in the liver.
846
ken ?cell? also occurs in many other trigrams and
thus leads to a less limited paradigmatic modifiabil-
ity. Still, the underlying assumption of our approach
is that such a case is more an exception than the rule
and that terms are linguistically more ?frozen? than
non-terms, which is exactly the intuition behind our
measure of limited paradigmatic modifiability.
3.4 Methods of Evaluation
As already described in Subsection 3.2, standard
procedures for evaluating the quality of termhood
measures usually involve identifying the true posi-
tives among a (usually) arbitrarily set number of the
m highest ranked candidates returned by a particu-
lar measure, a procedure usually carried out by a do-
main expert. Because this is labor-intensive (besides
being unreliable), m is usually small, ranging from
50 to several hundreds.10 By contrast, we choose
a large and already consensual terminology to iden-
tify the true terms in our candidate sets. Thus, we
are able to dynamically examine various m-highest
ranked samples, which, in turn, allows for the plot-
ting of standard precision and recall graphs for the
entire candidate set. We thus provide a more reli-
able evaluation setting for ATR measures than what
is common practice in the literature.
We compare our P -Mod algorithm against the
t-test measure,11 which, of all standard measures,
yields the best results in general-language collo-
cation extraction studies (Evert and Krenn, 2001),
and also against the widely used C-value, which
aims at enhancing the common frequency of occur-
rence measure by making it sensitive to nested terms
(Frantzi et al, 2000). Our baseline is defined by the
proportion of true positives (i.e., the proportion of
terms) in our bi-, tri- and quadgram candidate sets.
This is equivalent to the likelihood of finding a true
positive by blindly picking from one of the different
sets (see Subsection 3.2).
10Studies on collocation extraction (e.g., by Evert and Krenn
(2001)) also point out the inadequacy of such evaluation meth-
ods. In essence, they usually lead to very superficial judgments
about the measures under scrutiny.
11Manning and Schu?tze (1999) describe how this measure
can be used for the extraction of multi-word expressions.
4 Results and Discussion
4.1 Precision/Recall for Terminology Extraction
For each of the different candidate sets, we incre-
mentally examined portions of the ranked output
lists returned by each of the three measures we con-
sidered. The precision values for the various por-
tions were computed such that for each percent point
of the list, the number of true positives found (i.e.,
the number of terms) was scaled against the overall
number of candidate items returned. This yields the
(descending) precision curves in Figures 1, 2 and 3
and some associated values in Table 4.
Portion of Precision scores of measures
ranked list
considered P -Mod t-test C-value
1% 0.82 0.62 0.62
Bigrams 10% 0.53 0.42 0.41
20% 0.42 0.35 0.34
30% 0.37 0.32 0.31
baseline 0.22 0.22 0.22
1% 0.62 0.55 0.54
Trigrams 10% 0.37 0.29 0.28
20% 0.29 0.23 0.23
30% 0.24 0.20 0.19
baseline 0.12 0.12 0.12
1% 0.43 0.50 0.50
Quadgrams 10% 0.26 0.24 0.23
20% 0.20 0.16 0.16
30% 0.18 0.14 0.14
baseline 0.08 0.08 0.08
Table 4: Precision scores for biomedical term extraction at
selected portions of the ranked list
First, we observe that, for the various n-gram
candidate sets examined, all measures outperform
the baselines by far, and, thus, all are potentially
useful measures for grading termhood. Still, the
P -Mod criterion substantially outperforms all other
measures at almost all points for all n-grams exam-
ined. Considering 1% of the bigram list (i.e., the first
673 candidates) precision for P -Mod is 20 points
higher than for the t-test and the C-value. At 1%
of the trigram list (i.e., the first 310 candidates),
P -Mod?s lead is 7 points. Considering 1% of the
quadgrams (i.e., the first 108 candidates), the t-test
actually leads by 7 points. At 10% of the quadgram
list, however, the P -Mod precision score has over-
taken the other ones. With increasing portions of all
ranked lists considered, the precision curves start to
converge toward the baseline, but P -Mod maintains
a steady advantage.
847
 0
 0.2
 0.4
 0.6
 0.8
 1
100908070605040302010
Portion of ranked list (in %)
Precision: P-Mod
Precision: T-test
Precision: C-value
Recall: P-Mod
Recall: T-test
Recall: C-value
Base
Figure 1: Precision/Recall for bigram biomedical term extrac-
tion
 0
 0.2
 0.4
 0.6
 0.8
 1
100908070605040302010
Portion of ranked list (in %)
Precision: P-Mod
Precision: T-test
Precision: C-value
Recall: P-Mod
Recall: T-test
Recall: C-value
Base
Figure 2: Precision/Recall for trigram biomedical term ex-
traction
 0
 0.2
 0.4
 0.6
 0.8
 1
100908070605040302010
Portion of ranked list (in %)
Precision: P-Mod
Precision: T-test
Precision: C-value
Recall: P-Mod
Recall: T-test
Recall: C-value
Base
Figure 3: Precision/Recall for quadgram biomedical term ex-
traction
The (ascending) recall curves in Figures 1, 2 and
3 and their corresponding values in Table 5 indicate
which proportion of all true positives (i.e., the pro-
portion of all terms in a candidate set) is identified by
a particular measure at a certain point of the ranked
list. For term extraction, recall is an even better indi-
cator of a particular measure?s performance because
finding a bigger proportion of the true terms at an
early stage is simply more economical.
Recall Portion of Ranked List
scores of
measures P -Mod t-test C-value
0.5 29% 35% 37%
0.6 39% 45% 47%
Bigrams 0.7 51% 56% 59%
0.8 65% 69% 72%
0.9 82% 83% 85%
0.5 19% 28% 30%
Trigrams 0.6 27% 38% 40%
0.7 36% 50% 53%
0.8 50% 63% 66%
0.9 68% 77% 84%
0.5 20% 28% 30%
0.6 26% 38% 40%
Quadgrams 0.7 34% 49% 53%
0.8 45% 62% 65%
0.9 61% 79% 82%
Table 5: Portions of the ranked list to consider for selected
recall scores for biomedical term extraction
Again, our linguistically motivated terminology
extraction algorithm outperforms its competitors,
and with respect to tri- and quadgrams, its gain is
even more pronounced than for precision. In order to
get a 0.5 recall for bigram terms, P -Mod only needs
to winnow 29% of the ranked list, whereas the t-test
and C-value need to winnow 35% and 37%, respec-
tively. For trigrams and quadgrams, P -Mod only
needs to examine 19% and 20% of the list, whereas
the other two measures have to scan almost 10 ad-
ditional percentage points. In order to obtain a 0.6,
0.7, 0.8 and 0.9 recall, the differences between the
measures narrow for bigram terms, but they widen
substantially for tri- and quadgram terms. To obtain
a 0.6 recall for trigram terms, P -Mod only needs to
winnow 27% of its output list while the t-test and
C-value must consider 38% and 40%, respectively.
For a level of 0.7 recall, P -Mod only needs to an-
alyze 36%, while the t-test already searches 50% of
the ranked list. For 0.8 recall, this relation is 50%
848
(P -Mod) to 63% (t-test), and at recall point 0.9,
68% (P -Mod) to 77% (t-test). For quadgram term
identification, the results for P -Mod are equally su-
perior to those for the other measures, and at recall
points 0.8 and 0.9 even more pronounced than for
trigram terms.
We also tested the significance of differences for
these results, both comparing P -Mod vs. t-test and
P -Mod vs. C-value. Because in all cases the ranked
lists were taken from the same set of candidates (viz.
the set of bigram, trigram, and quadgram candidate
types), and hence constitute dependent samples, we
applied the McNemar test (Sachs, 1984) for statis-
tical testing. We selected 100 measure points in the
ranked lists, one after each increment of one percent,
and then used the two-tailed test for a confidence in-
terval of 95%. Table 6 lists the number of significant
differences for these measure points at intervals of
10 for the bi-, tri-, and quadgram results. For the bi-
gram differences between P -Mod and C-value, all
of them are significant, and between P -Mod and
t-test, all are significantly different up to measure
point 70.12 Looking at the tri- and quadgrams, al-
though the number of significant differences is less
than for bigrams, the vast majority of measure points
is still significantly different and thus underlines the
superior performance of the P -Mod measure.
# of # of significant differences comparing
measure P -Mod with
points t-test C-val t-test C-val t-test C-val
10 10 10 9 9 3 3
20 20 20 19 19 13 13
30 30 30 29 29 24 24
40 40 40 39 39 33 33
50 50 50 49 49 43 43
60 60 60 59 59 53 53
70 70 70 69 69 63 63
80 75 80 79 79 73 73
90 84 90 89 89 82 83
100 93 100 90 98 82 91
bigrams trigrams quadgrams
Table 6: Significance testing of differences for bi-, tri- and
quadgrams using the two-tailed McNemar test at 95% confi-
dence interval
12As can be seen in Figures 1, 2 and 3, the curves start to
merge at the higher measure points and, thus, the number of
significant differences decreases.
4.2 Domain Independence and Corpus Size
One might suspect that the results reported above
could be attributed to the corpus size. Indeed, the
text collection we employed in this study is rather
large (104 million words). Other text genres and do-
mains (e.g., clinical narratives, various engineering
domains) or even more specialized biological sub-
domains (e.g., plant biology) do not offer such a
plethora of free-text material as the molecular biol-
ogy domain. To test the effect a drastically shrunken
corpus size might have, we assessed the terminology
extraction methods for trigrams on a much smaller-
sized subset of our original corpus, viz. on 10 million
words. These results are depicted in Figure 4.
 0
 0.2
 0.4
 0.6
 0.8
 1
100908070605040302010
Portion of ranked list (in %)
Precision: P-Mod
Precision: T-test
Precision: C-value
Recall: P-Mod
Recall: T-test
Recall: C-value
Base
Figure 4: Precision/Recall for trigram biomedical term ex-
traction on the 10-million-word corpus (cutoff c ? 4, with
6,760 term candidate types)
The P -Mod extraction criterion still clearly out-
performs the other ones on that 10-million-word cor-
pus, both in terms of precision and recall. We also
examined whether the differences were statistically
significant and applied the two-tailed McNemar test
on 100 selected measure points. Comparing P -Mod
with t-test, most significant differences could be ob-
served between measure points 20 and 80, with al-
most 80% to 90% of the points being significantly
different. These significant differences were even
more pronounced when comparing the results be-
tween P -Mod and C-value.
5 Conclusions
We here proposed a new terminology extraction
method and showed that it significantly outperforms
849
two of the standard approaches in distinguishing
terms from non-terms in the biomedical literature.
While mining scientific literature for new termino-
logical units and assembling those in controlled vo-
cabularies is a task involving several components,
one essential building block is to measure the de-
gree of termhood of a candidate. In this respect, our
study has shown that a criterion which incorporates
a vital linguistic property of terms, viz. their lim-
ited paradigmatic modiability, is much more pow-
erful than linguistically more uninformed measures.
This is in line with our previous work on general-
language collocation extraction (Wermter and Hahn,
2004), in which we showed that a linguistically mo-
tivated criterion based on the limited syntagmatic
modifiability of collocations outperforms alternative
standard association measures as well.
We also collected evidence that the superiority of
the P -Mod method relative to other term extraction
approaches holds independent of the underlying cor-
pus size (given a reasonable offset). This is a crucial
finding because other domains might lack large vol-
umes of free-text material but still provide sufficient
corpus sizes for valid term extraction. Finally, since
we only require shallow syntactic analysis (in terms
of NP chunking), our approach might be well suited
to be easily portable to other domains. Hence, we
may conclude that, although our methodology has
been tested on the biomedical domain only, there are
essentially no inherent domain-specific restrictions.
Acknowledgements. This work was partly supported by
the European Network of Excellence ?Semantic Mining in
Biomedicine? (NoE 507505).
References
Olivier Bodenreider, Thomas C. Rindflesch, and Anita Burgun.
2002. Unsupervised, corpus-based method for extending a
biomedical terminology. In Stephen Johnson, editor, Pro-
ceedings of the ACL/NAACL 2002 Workshop on ?Natural
Language Processing in the Biomedical Domain?, pages 53?
60. Philadelphia, PA, USA, July 11, 2002.
Nigel Collier, Chikashi Nobata, and Jun?ichi Tsujii. 2002. Au-
tomatic acquisition and classification of terminology using a
tagged corpus in the molecular biology domain. Terminol-
ogy, 7(2):239?257.
Be?atrice Daille. 1996. Study and implementation of com-
bined techniques for automatic extraction of terminology. In
Judith L. Klavans and Philip Resnik, editors, The Balanc-
ing Act: Combining Statistical and Symbolic Approaches to
Language, pages 49?66. Cambridge, MA: MIT Press.
Fred J. Damerau. 1993. Generating and evaluating domain-
oriented multi-word terms from text. Information Process-
ing & Management, 29(4):433?447.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures. In
ACL?01/EACL?01 ? Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics and the
10th Conference of the European Chapter of the ACL, pages
188?195. Toulouse, France, July 9-11, 2001.
Katerina T. Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: The C-
value/NC-value method. International Journal on Digital
Libraries, 3(2):115?130.
Christian Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentations of term variation. In Proceedings of the 37rd
Annual Meeting of the Association for Computational Lin-
guistics, pages 341?348. College Park, MD, USA, 20-26
June 1999.
John S. Justeson and Slava M. Katz. 1995. Technical terminol-
ogy: Some linguistic properties and an algorithm for identi-
fication in text. Natural Language Engineering, 1(1):9?27.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In NAACL?01, Language Technologies
2001 ? Proceedings of the 2nd Meeting of the North Amer-
ican Chapter of the Association for Computational Linguis-
tics, pages 192?199. Pittsburgh, PA, USA, June 2-7, 2001.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. Cam-
bridge, MA; London, U.K.: Bradford Book & MIT Press.
Hiroshi Nakagawa and Tatsunori Mori. 2002. A simple but
powerful automatic term extraction method. In COMPU-
TERM 2002 ? Proceedings of the 2nd International Work-
shop on Computational Terminology, pages 29?35. Taipei,
Taiwan, August 31, 2002.
Goran Nenadic?, Irena Spasic?, and Sophia Ananiadou. 2003.
Terminology-driven mining of biomedical literature. Bioin-
formatics, 19(8):938?943.
Goran Nenadic?, Sophia Ananiadou, and John McNaught. 2004.
Enhancing automatic term recognition through recognition
of variation. In COLING 2004 ? Proceedings of the 20th In-
ternational Conference on Computational Linguistics, pages
604?610. Geneva, Switzerland, August 23-27, 2004.
Thomas C. Rindflesch, Lawrence Hunter, and Alan R. Aronson.
1999. Mining molecular binding terminology from biomed-
ical text. In AMIA?99 ? Proceedings of the Annual Sym-
posium of the American Medical Informatics Association,
pages 127?131. Washington, D.C., November 6-10, 1999.
Lothar Sachs. 1984. Applied Statistics: A Handbook of Tech-
niques. New York: Springer, 2nd edition.
MESH. 2004. Medical Subject Headings. Bethesda, MD: Na-
tional Library of Medicine.
UMLS. 2004. Unified Medical Language System. Bethesda,
MD: National Library of Medicine.
Joachim Wermter and Udo Hahn. 2004. Collocation extrac-
tion based on modifiability statistics. In COLING 2004 ?
Proceedings of the 20th International Conference on Com-
putational Linguistics, pages 980?986. Geneva, Switzerland,
August 23-27, 2004.
850
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 785?792,
Sydney, July 2006. c?2006 Association for Computational Linguistics
You Can?t Beat Frequency (Unless You Use Linguistic Knowledge) ?
A Qualitative Evaluation of Association Measures for
Collocation and Term Extraction
Joachim Wermter Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
D-07743 Jena, Germany
{wermter|hahn}@coling-uni-jena.de
Abstract
In the past years, a number of lexical
association measures have been studied
to help extract new scientific terminol-
ogy or general-language collocations. The
implicit assumption of this research was
that newly designed term measures involv-
ing more sophisticated statistical criteria
would outperform simple counts of co-
occurrence frequencies. We here explic-
itly test this assumption. By way of four
qualitative criteria, we show that purely
statistics-based measures reveal virtually
no difference compared with frequency
of occurrence counts, while linguistically
more informed metrics do reveal such a
marked difference.
1 Introduction
Research on domain-specific automatic term
recognition (ATR) and on general-language collo-
cation extraction (CE) has gone mostly separate
ways in the last decade although their underlying
procedures and goals turn out to be rather simi-
lar. In both cases, linguistic filters (POS taggers,
phrase chunkers, (shallow) parsers) initially col-
lect candidates from large text corpora and then
frequency- or statistics-based evidence or associa-
tion measures yield scores indicating to what de-
gree a candidate qualifies as a term or a colloca-
tion. While term mining and collocation mining,
as a whole, involve almost the same analytical pro-
cessing steps, such as orthographic and morpho-
logical normalization, normalization of term or
collocation variation etc., it is exactly the measure
which grades termhood or collocativity of a can-
didate on which alternative approaches diverge.
Still, the output of such mining algorithms look
similar. It is typically constituted by a ranked list
on which, ideally, the true terms or collocations
are placed in the top portion of the list, while the
non-terms / non-collocations occur in its bottom
portion.
While there have been lots of approaches to
come up with a fully adequate ATR/CE metric
(cf. Section 2), we have made observations in our
experiments that seem to indicate that simplicity
rules, i.e., frequency of occurrence is the dominat-
ing factor for the ranking in the result lists even
when much smarter statistical machinery is em-
ployed. In this paper, we will discuss data which
reveals that purely statistics-based measures ex-
hibit virtually no difference compared with fre-
quency of occurrence counts, while linguistically
more informed measures do reveal such a marked
difference ? for the problem of term and colloca-
tion mining at least.
2 Related Work
Although there has been a fair amount of work
employing linguistically sophisticated analysis of
candidate items (e.g., on CE by Lin (1998) and
Lin (1999) as well as on ATR by Daille (1996),
Jacquemin (1999), and Jacquemin (2001)), these
approaches are limited by the difficulty to port
grammatical specifications to other domains (in
the case of ATR) or by the error-proneness of
full general-language parsers (in the case of CE).
Therefore, most recent approaches in both areas
have backed off to more shallow linguistic filter-
ing techniques, such as POS tagging and phrase
chunking (e.g., Frantzi et al (2000), Krenn and
Evert (2001), Nenadic? et al (2004), Wermter and
Hahn (2005)).
785
After linguistic filtering, various measures
are employed in the literature for grading the
termhood / collocativity of collected candidates.
Among the most widespread ones, both for ATR
and CE, are statistical and information-theoretic
measures, such as t-test, log-likelihood, entropy,
and mutual information. Their prominence is
also reflected by the fact that a whole chapter of
a widely used textbook on statistical NLP (viz.
Chapter 5 (Collocations) in Manning and Schu?tze
(1999)) is devoted to them. In addition, the C-
value (Frantzi et al, 2000) ? basically a frequency-
based approach ? has been another widely used
measure for multi-word ATR. Recently, more lin-
guistically informed algorithms have been intro-
duced both for CE (Wermter and Hahn, 2004) and
for ATR (Wermter and Hahn, 2005), which have
been shown to outperform several of the statistics-
only metrics.
3 Methods and Experiments
3.1 Qualitative Criteria
Because various metrics assign a score to the can-
didates indicating as to what degree they qualify
as a collocation or term (or not), these candidates
should ideally be ranked in such a way that the
following two conditions are met:
? true collocations or terms (i.e., the true pos-
itives) are ranked in the upper portion of the
output list.
? non-collocations or non-terms (i.e., the true
negatives) are ranked in the lower part of the
output list.1
While a trivial solution to the problem might
be to simply count the number of occurrences of
candidates in the data, employing more sophis-
ticated statistics-based / information-theoretic or
even linguistically-motivated algorithms for grad-
ing term and collocation candidates is guided by
the assumption that this additional level of sophis-
tication yields more adequate rankings relative to
these two conditions.
Several studies (e.g., Evert and Krenn (2001),
Krenn and Evert (2001), Frantzi et al (2000),
Wermter and Hahn (2004)), however, have al-
ready observed that ranking the candidates merely
by their frequency of occurrence fares quite well
1Obviously, this goal is similar to ranking documents ac-
cording to their relevance for information retrieval.
compared with various more sophisticated as-
sociation measures (AMs such as t-test, log-
likelihood, etc.). In particular, the precision/recall
value comparison between the various AMs ex-
hibits a rather inconclusive picture in Evert and
Krenn (2001) and Krenn and Evert (2001) as to
whether sophisticated statistical AMs are actually
more viable than frequency counting.
Commonly used statistical significance testing
(e.g., the McNemar or the Wilcoxon sign rank
tests; see (Sachs, 1984)) does not seem to provide
an appropriate evaluation ground either. Although
Evert and Krenn (2001) and Wermter and Hahn
(2004) provide significance testing of some AMs
with respect to mere frequency counting for collo-
cation extraction, they do not differentiate whether
this is due to differences in the ranking of true pos-
itives or true negatives or a combination thereof.2
As for studies on ATR (e.g., Wermter and Hahn
(2005) or Nenadic? et al (2004)), no statistical test-
ing of the term extraction algorithms to mere fre-
quency counting was performed.
But after all, these kinds of commonly used sta-
tistical significance tests may not provide the right
machinery in the first place. By design, they are
rather limited (or focused) in their scope in that
they just check whether a null hypothesis can be
rejected or not. In such a sense, they do not pro-
vide a way to determine, e.g., to which degree of
magnitude some differences pertain and thus do
not offer the facilities to devise qualitative criteria
to test whether an AM is superior to co-occurrence
frequency counting.
The purpose of this study is therefore to postu-
late a set of criteria for the qualitative testing of
differences among the various CE and ATR met-
rics. We do this by taking up the two conditions
above which state that a good CE or ATR algo-
rithm would rank most of the true positives in a
candidate set in the upper portion and most of
the true negatives in the lower portion of the out-
put. Thus, compared to co-occurrence frequency
counting, a superior CE/ATR algorithm should
achieve the following four objectives:
2In particular Evert and Krenn (2001) use the chi-square
test which assumes independent samples and is thus not re-
ally suitable for testing the significance of differences of two
or more measures which are typically run on the same set
of candidates (i.e., a dependent sample). Wermter and Hahn
(2004) use the McNemar test for dependent samples, which
only examines the differences in which two metrics do not
coincide.
786
1. keep the true positives in the upper portion
2. keep the true negatives in the lower portion
3. demote true negatives from the upper portion
4. promote true positives from the lower por-
tion.
We take these to be four qualitative criteria by
which the merit of a certain AM against mere oc-
currence frequency counting can be determined.
3.2 Data Sets
For collocation extraction (CE), we used the data
set provided by Wermter and Hahn (2004) which
consists of a 114-million-word German newspa-
per corpus. After shallow syntactic analysis, the
authors extracted Preposition-Noun-Verb (PNV)
combinations occurring at least ten times and had
them classified by human judges as to whether
they constituted a valid collocation or not, re-
sulting in 8644 PNV-combinations with 13.7%
true positives. As for domain-specific automatic
term recognition (ATR), we used a biomedical
term candidate set put forth by Wermter and Hahn
(2005), who, after shallow syntactic analysis, ex-
tracted 31,017 trigram term candidates occurring
at least eight times out of a 104-million-word
MEDLINE corpus. Checking these term candi-
dates against the 2004 edition UMLS Metathe-
saurus (UMLS, 2004)3 resulted in 11.6% true pos-
itives. This information is summarized in Table 1.
Collocations Terms
domain newspaper biomedicine
language German English
linguistic type PP-Verb noun phrases
combinations (trigrams)
corpus size 114 million 104 million
cutoff 10 8
# candidates 8,644 31,017
# true positives 1,180 (13.7%) 3,590 (11.6%)
# true negatives 7,464 (86.3%) 27,427 (88.4%)
Table 1: Data sets for Collocation Extraction (CE) and Au-
tomatic Term Dioscovery (ATR)
3The UMLS Metathesaurus is an extensive and carefully
curated terminological resource for the biomedical domain.
3.3 The Association Measures
We examined both standard statistics-based and
more recent linguistically rooted association mea-
sures against mere frequency of occurrence count-
ing (henceforth referred to as Frequency). As the
standard statistical AM, we selected the t-test (see
also Manning and Schu?tze (1999) for a descrip-
tion on its use in CE and ATR) because it has
been shown to be the best-performing statistics-
only measure for CE (cf. Evert and Krenn (2001)
and Krenn and Evert (2001)) and also for ATR (see
Wermter and Hahn (2005)).
Concerning more recent linguistically grounded
AMs, we looked at limited syntagmatic modifia-
bility (LSM) for CE (Wermter and Hahn, 2004)
and limited paradigmatic modifiability (LPM) for
ATR (Wermter and Hahn, 2005). LSM exploits
the well-known linguistic property that colloca-
tions are much less modifiable with additional lex-
ical material (supplements) than non-collocations.
For each collocation candidate, LSM determines
the lexical supplement with the highest probabil-
ity, which results in a higher collocativity score for
those candidates with a particularly characteristic
lexical supplement. LPM assumes that domain-
specific terms are linguistically more fixed and
show less distributional variation than common
noun phrases. Taking n-gram term candidates, it
determines the likelihood of precluding the ap-
pearance of alternative tokens in various token slot
combinations, which results in higher scores for
more constrained candidates. All measures assign
a score to the candidates and thus produce a ranked
output list.
3.4 Experimental Setup
In order to determine any potential merit of the
above measures, we use the four criteria described
in Section 3.1 and qualitatively compare the differ-
ent rankings given to true positives and true neg-
atives by an AM and by Frequency. For this pur-
pose, we chose the middle rank as a mark to di-
vide a ranked output list into an upper portion and
a lower portion. Then we looked at the true pos-
itives (TPs) and true negatives (TNs) assigned to
these portions by Frequency and quantified, ac-
cording to the criteria postulated in Section 3.1,
to what degree the other AMs changed these rank-
ings (or not). In order to better quantify the de-
grees of movement, we partitioned both the upper
and the lower portions into three further subpor-
tions.
787
Association upper portion (ranks 1 - 4322) lower portion (ranks 4323 - 8644)
Measure 0% - 16.7% 16.7% - 33.3% 33.3% - 50% 50% - 66.7% 66.7% - 83.3% 83.3% - 100%
Criterion 1 Freq 545 (60.2%) 216 (23.9%) 144 (15.9%) 0 0 0
(905 TPs) t-test 540 (59.7%) 198 (21.9%) 115 (12.7%) 9 (1.0%) 12 (1.3%) 12 (1.3%)
LSM 606 (67.0%) 237 (26.2%) 35 (3.9%) 10 (1.1%) 12 (1.3%) 5 (0.6%)
Criterion 2 Freq 0 0 0 1361 (33.6%) 1357 (33.5%) 1329 (32.8%)
(4047 TNs) t-test 0 34 (0.8%) 613 (15.2%) 1121 (27.7%) 1100 (27.2%) 1179 (29.1%)
LSM 118 (2.9%) 506 (12.5%) 726 (17.9%) 808 (20.0%) 800 (19.8%) 1089 (26.9%)
Criterion 3 Freq 896 (26.2%) 1225 (35.9%) 1296 (37.9%) 0 0 0
(3417 TNs) t-test 901 (26.4%) 1243 (36.4%) 932 (27.3%) 95 (2.8%) 47 (1.4%) 199 (5.8%)
LSM 835 (24.4%) 1150 (33.7%) 342 (10.0%) 218 (6.4%) 378 (11.1%) 494 (14.5%)
Criterion 4 Freq 0 0 0 113 (41.1%) 85 (30.9%) 77 (28.0%)
(275 TPs) t-test 0 0 31 (11.3%) 88 (32.6%) 59 (21.5%) 95 (34.5%)
LSM 0 10 (3.6%) 144 (52.4%) 85 (30.9%) 27 (9.8%) 9 (3.3%)
Table 2: Results on the four qualitative criteria for Collocation Extraction (CE)
Association upper portion (ranks 1 - 15508) lower portion (ranks 15509 - 31017)
Measure 0% - 16.7% 16.7% - 33.3% 33.3% - 50% 50% - 66.7% 66.7% - 83.3% 83.3% - 100%
Criterion 1 Freq 1252 (50.7%) 702 (28.4%) 515 (20.9%) 0 0 0
(2469 TPs) t-test 1283 (52.0%) 709 (28.7%) 446 (18.1%) 13 (0.5%) 2 (0.1%) 16 (0.6%)
LPM 1346 (54.5%) 513 (20.8%) 301 (12.2%) 163 (6.6%) 95 (3.8%) 51 (2.1%)
Criterion 2 Freq 0 0 0 4732 (32.9%) 4822 (33.5%) 4833 (33.6%))
(14387 TNs) t-test 0 0 580 (4.0%) 4407 (30.6%) 4743 (33.0%) 4657 (32.4%)
LPM 1009 (7.0%) 1698 (11.8%) 2190 (15.2%) 2628 (18.3%) 3029 (21.1%) 3834 (26.6%)
Criterion 3 Freq 3917 (30.0%) 4467 (34.3%) 4656 (35.7%) 0 0 0
(13040 TNs) t-test 3885 (29.8%) 4460 (34.2%) 4048 (31.0%) 315 (2.4%) 76 (0.6%) 256 (2.0%)
LPM 2545 (19.5%) 2712 (20.8%) 2492 (19.1%) 2200 (16.9%) 1908 (14.6%) 1182 (9.1%)
Criterion 4 Freq 0 0 0 438 (39.1%) 347 (31.0%) 336 (30.0%)
(1121 TPs) t-test 0 0 97 (8.7%) 436 (38.9%) 348 (31.0%) 240 (21.4%)
LPM 268 (23.9%) 246 (21.9%) 188 (16.8%) 180 (16.1%) 137 (12.2%) 102 (9.1%)
Table 3: Results on the four qualitative criteria for Automatic Term Discovery (ATR)
4 Results and Discussion
The first two criteria examine how conservative an
association measure is with respect to Frequency,
i.e., a superior AM at least should keep the status-
quo (or even improve it) by keeping the true pos-
itives in the upper portion and the true negatives
in the lower one. In meeting criteria 1 for CE,
Table 2 shows that t-test behaves very similar to
Frequency in keeping roughly the same amount of
TPs in each of the upper three subportions. LSM
even promotes its TPs from the third into the first
two upper subportion (i.e., by a 7- and 2-point in-
crease in the first and in the second subportion as
well as a 12-point decrease in the third subportion,
compared to Frequency).
With respect to the same criterion for ATR (see
Table 3), Frequency and t-test again show quite
similar distributions of TPs in the top three sub-
portions. LPM, on the other hand, demonstrates a
modest increase (by 4 points) in the top upper sub-
portion, but decreases in the second and third one
so that a small fraction of TPs gets demoted to the
lower three subportions (6.6%, 3.8% and 2.1%).
Regarding criterion 2 for CE (see Table 2), t-
test?s share of TNs in the lower three subportions
is slightly less than that of Frequency, leading
to a 15-point increase in the adjacent third up-
per subportion. This local ?spilling over? to the
upper portion is comparatively small considering
the change that occurs with respect to LSM. Here,
TNs appear in the second (12.5%) and the third
(17.9%) upper subportions. For ATR, t-test once
more shows a very similar distribution compared
to Frequency, whereas LPM again promotes some
of its lower TNs into the upper subportions (7%,
11.8% and 15.2%).
Criteria 3 and 4 examine the kinds of re-
rankings (i.e., demoting upper portion TNs and
promoting lower portion TPs) which an AM needs
to perform in order to qualify as being superior to
Frequency. These criteria look at how well an AM
is able to undo the unfavorable ranking of TPs and
TNs by Frequency. As for criterion 3 (the demo-
tion of TNs from the upper portion) in CE, Table 2
shows that t-test is only marginally able to undo
the unfavorable rankings in its third upper sub-
portion (11 percentage points less of TNs). This
causes a small fraction of TNs getting demoted to
788
Rank in Frequency
R
an
k 
in
 L
SM
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
0% 16.7% 33.3% 50%
Figure 1: Collocations: True negatives moved from upper
to lower portion (LSM rank compared to Frequency rank)
Rank in Frequency
R
an
k 
in
 t?
te
st
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
0% 16.7% 33.3% 50%
Figure 2: Collocations: True negatives moved from upper
to lower portion (t-test rank compared to Frequency rank)
the lower three subportions (viz. 2.8%, 1.4%, and
5.8%).
A view from another angle on this rather slight
re-ranking is offered by the scatterplot in Figure
2, in which the rankings of the upper portion TNs
Rank in Frequency
R
an
k 
in
 L
PM
0% 16.7% 33.3% 50%
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
Figure 3: Terms: True negatives moved from upper to
lower portion (LPM rank compared to Frequency rank)
Rank in Frequency
R
an
k 
in
 t?
te
st
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
0% 16.7% 33.3% 50%
Figure 4: Terms: True negatives moved from upper to
lower portion (t-test rank compared to Frequency rank)
of Frequency are plotted against their ranking in
t-test. Here it can be seen that, in terms of the rank
subportions considered, the t-test TNs are concen-
trated along the same line as the Frequency TNs,
with only a few being able to break this line and
789
Rank in Frequency
R
an
k 
in
 L
SM
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
50% 66.7% 83.3% 100%
Figure 5: Collocations: True positives moved from lower
to upper portion (LSM rank compared to Frequency rank)
Rank in Frequency
R
an
k 
in
 t?
te
st
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
50% 66.7% 83.3% 100%
Figure 6: Collocations: True positives moved from lower
to upper portion (t-test rank compared to Frequency rank)
get demoted to a lower subportion.
A strikingly similar picture holds for this cri-
terion in ATR: as can be witnessed from Figure
4, the vast majority of upper portion t-test TNs is
stuck on the same line as in Frequency. The sim-
Rank in Frequency
R
an
k 
in
 L
PM
50% 66.7% 83.3% 100%
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
Figure 7: Terms: True positives moved from lower to upper
portion (LPM rank compared to Frequency rank)
Rank in Frequency
R
an
k 
in
 t?
te
st
10
0%
83
.3
%
66
.7
%
50
%
33
.3
%
16
.7
0%
50% 66.7% 83.3% 100%
Figure 8: Terms: True positives moved from lower to upper
portion (t-test rank compared to Frequency rank)
ilarity of t-test in both CE and ATR is even more
remarkable given the fact in the actual number of
upper portion TNs is more than four times higher
in ATR (13040) than in CE (3076). A look at the
actual figures in Table 3 indicates that t-test is even
790
less able to deviate from Frequency?s TN distribu-
tion (i.e., the third upper subportion is only occu-
pied by 4.7 points less TNs, with the other two
subportions essentially remaining the same as in
Frequency).
The two linguistically rooted measures, LSM
for CE and LPM for ATR, offer quite a different
picture regarding this criterion. With LSM, almost
one third (32%) of the upper portion TNs get de-
moted to the three lower portions (see Table 2);
with LPM, this proportion even amounts to 40.6%
(see Table 3). The scatterplots in Figure 1 and
Figure 3 visualize this from another perspective:
in particular, LPM completely breaks the original
Frequency ranking pattern and scatters the upper
portion TNs in almost all possible directions, with
the vast majority of them thus getting demoted to
a lower rank than in Frequency. Although LSM
stays more in line, still substantially more upper
portion TNs get demoted than with t-test.
With regard to Criterion 4 (the promotion of
TPs from the lower portion) in CE, t-test manages
to promote 11.3% of its lower portion TPs to the
adjacent third upper subportion, but at the same
time demotes more TPs to the third lower subpor-
tion (34.5% compared to 28% in Frequency; see
Table 2). Figure 6 thus shows the t-test TPs to
be a bit more dispersed in the lower portion. For
ATR, the t-test distribution of TPs differs even less
from Frequency. Table 3 reveals that only 8.7% of
the lower portion TPs get promoted to the adjacent
third upper portion. The staggered groupinlpr g of
lower portion t-test TPs (visualized in the respec-
tive scatterplot in Figure 8) actually indicates that
there are certain plateaus beyond which the TPs
cannot get promoted.
The two non-standard measures, LSM and
LPM, once more present a very different picture.
Regarding LSM, 56% of all lower portion TPs get
promoted to the upper three subportions. The ma-
jority of these (52.4%) gets placed the third upper
subportion. This can also be seen in the respective
scatterplot in Figure 5 which shows a marked con-
centration of lower portion TPs in the third upper
subportion. With respect to LPM, even 62.6% of
all lower portion TPs make it to the upper portions
? with the majority (23.9%) even getting promoted
to the first upper subportion. The respective scat-
terplot in Figure 7 additionally shows that this up-
ward movement of TPs, like the downward move-
ment of TNs in Figure 3, is quite dispersed.
5 Conclusions
For lexical processing, the automatic identifica-
tion of terms and collocations constitutes a re-
search theme that has been dealt with by employ-
ing increasingly complex probabilistic criteria (t-
test, mutual information, log-likelihood etc.). This
trend is also reflected by their prominent status in
standard textbooks on statistical NLP. The implicit
justification in using these statistics-only metrics
was that they would markedly outperform fre-
quency of co-occurrence counting. We devised
four qualitative criteria for explicitly testing this
assumption. Using the best performing standard
association measure (t-test) as a pars pro toto, our
study indicates that the statistical sophistication
does not pay off when compared with simple fre-
quency of co-occurrence counting.
This pattern changes, however, when proba-
bilistic measures incorporate additional linguistic
knowledge about the distributional properties of
terms and the modifiability properties of colloca-
tions. Our results show that these augmented met-
rics reveal a marked difference compared to fre-
quency of occurrence counts ? to a larger degree
with respect to automatic term recognition, to a
slightly lesser degree for collocation extraction.
References
Be?atrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. In Judith L. Klavans and Philip Resnik,
editors, The Balancing Act: Combining Statistical
and Symbolic Approaches to Language, pages 49?
66. Cambridge, MA: MIT Press.
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In ACL?01/EACL?01 ? Proceedings of the
39th Annual Meeting of the Association for Com-
putational Linguistics and the 10th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 188?195. Toulouse,
France, July 9-11, 2001. San Francisco, CA: Mor-
gan Kaufmann.
Katerina T. Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic recognition of multi-word
terms: The C-value/NC-value method. Interna-
tional Journal on Digital Libraries, 3(2):115?130.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37rd Annual Meeting of the Association
for Computational Linguistics, pages 341?348. Col-
lege Park, MD, USA, 20-26 June 1999. San Fran-
cisco, CA: Morgan Kaufmann.
791
Christian Jacquemin. 2001. Spotting and Discovering
Terms through NLP. Mass.: MIT Press.
Brigitte Krenn and Stefan Evert. 2001. Can we do bet-
ter than frequency? A case study on extracting pp-
verb collocations. In Proceedings of the ACL Work-
shop on Collocations. Toulouse, France.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In COLING/ACL?98 ? Pro-
ceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics & 17th In-
ternational Conference on Computational Linguis-
tics, volume 2, pages 768?774. Montre?al, Quebec,
Canada, August 10-14, 1998. San Francisco, CA:
Morgan Kaufmann.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 317?324. College Park,
MD, USA, 20-26 June 1999. San Francisco, CA:
Morgan Kaufmann.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. Cambridge, MA; London, U.K.: Bradford
Book & MIT Press.
Goran Nenadic?, Sophia Ananiadou, and John Mc-
Naught. 2004. Enhancing automatic term recog-
nition through recognition of variation. In COL-
ING Geneva 2004 ? Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 604?610. Geneva, Switzerland, August 23-27,
2004. Association for Computational Linguistics.
Lothar Sachs. 1984. Applied Statistics: A Handbook
of Techniques. New York: Springer, 2nd edition.
UMLS. 2004. Unified Medical Language System.
Bethesda, MD: National Library of Medicine.
Joachim Wermter and Udo Hahn. 2004. Collocation
extraction based on modifiability statistics. In COL-
ING Geneva 2004 ? Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
volume 2, pages 980?986. Geneva, Switzerland, Au-
gust 23-27, 2004. Association for Computational
Linguistics.
Joachim Wermter and Udo Hahn. 2005. Paradig-
matic modifiability statistics for the extraction of of
complex multi-word terms. In HLT-EMNLP?05 ?
Proceedings of the 5th Human Language Technol-
ogy Conference and 2005 Conference on Empiri-
cal Methods in Natural Language Processing, pages
843?850. Vancouver, Canada, October 6-8, 2005.
Association for Computational Linguistics.
792
Proceedings of ACL-08: HLT, pages 861?869,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-Task Active Learning for Linguistic Annotations
Roi Reichart1? Katrin Tomanek2? Udo Hahn2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem, Israel
{roiri|arir}@cs.huji.ac.il
2Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We extend the classical single-task active
learning (AL) approach. In the multi-task ac-
tive learning (MTAL) paradigm, we select ex-
amples for several annotation tasks rather than
for a single one as usually done in the con-
text of AL. We introduce two MTAL meta-
protocols, alternating selection and rank com-
bination, and propose a method to implement
them in practice. We experiment with a two-
task annotation scenario that includes named
entity and syntactic parse tree annotations on
three different corpora. MTAL outperforms
random selection and a stronger baseline, one-
sided example selection, in which one task is
pursued using AL and the selected examples
are provided also to the other task.
1 Introduction
Supervised machine learning methods have success-
fully been applied to many NLP tasks in the last few
decades. These techniques have demonstrated their
superiority over both hand-crafted rules and unsu-
pervised learning approaches. However, they re-
quire large amounts of labeled training data for every
level of linguistic processing (e.g., POS tags, parse
trees, or named entities). When, when domains
and text genres change (e.g., moving from common-
sense newspapers to scientific biology journal arti-
cles), extensive retraining on newly supplied train-
ing material is often required, since different do-
mains may use different syntactic structures as well
as different semantic classes (entities and relations).
? Both authors contributed equally to this work.
Consequently, with an increasing coverage of a
wide variety of domains in human language tech-
nology (HLT) systems, we can expect a growing
need for manual annotations to support many kinds
of application-specific training data.
Creating annotated data is extremely labor-
intensive. The Active Learning (AL) paradigm
(Cohn et al, 1996) offers a promising solution to
deal with this bottleneck, by allowing the learning
algorithm to control the selection of examples to
be manually annotated such that the human label-
ing effort be minimized. AL has been successfully
applied already for a wide range of NLP tasks, in-
cluding POS tagging (Engelson and Dagan, 1996),
chunking (Ngai and Yarowsky, 2000), statistical
parsing (Hwa, 2004), and named entity recognition
(Tomanek et al, 2007).
However, AL is designed in such a way that it se-
lects examples for manual annotation with respect to
a single learning algorithm or classifier. Under this
AL annotation policy, one has to perform a separate
annotation cycle for each classifier to be trained. In
the following, we will refer to the annotations sup-
plied for a classifier as the annotations for a single
annotation task.
Modern HLT systems often utilize annotations re-
sulting from different tasks. For example, a machine
translation system might use features extracted from
parse trees and named entity annotations. For such
an application, we obviously need the different an-
notations to reside in the same text corpus. It is not
clear how to apply the single-task AL approach here,
since a training example that is beneficial for one
task might not be so for others. We could annotate
861
the same corpus independently by the two tasks and
merge the resulting annotations, but that (as we show
in this paper) would possibly yield sub-optimal us-
age of human annotation efforts.
There are two reasons why multi-task AL, and
by this, a combined corpus annotated for various
tasks, could be of immediate benefit. First, annota-
tors working on similar annotation tasks (e.g., con-
sidering named entities and relations between them),
might exploit annotation data from one subtask for
the benefit of the other. If for each subtask a sepa-
rate corpus is sampled by means of AL, annotators
will definitely lack synergy effects and, therefore,
annotation will be more laborious and is likely to
suffer in terms of quality and accuracy. Second, for
dissimilar annotation tasks ? take, e.g., a compre-
hensive HLT pipeline incorporating morphological,
syntactic and semantic data ? a classifier might re-
quire features as input which constitute the output
of another preceding classifier. As a consequence,
training such a classifier which takes into account
several annotation tasks will best be performed on
a rich corpus annotated with respect to all input-
relevant tasks. Both kinds of annotation tasks, simi-
lar and dissimilar ones, constitute examples of what
we refer to as multi-task annotation problems.
Indeed, there have been efforts in creating re-
sources annotated with respect to various annotation
tasks though each of them was carried out indepen-
dently of the other. In the general language UPenn
annotation efforts for the WSJ sections of the Penn
Treebank (Marcus et al, 1993), sentences are anno-
tated with POS tags, parse trees, as well as discourse
annotation from the Penn Discourse Treebank (Milt-
sakaki et al, 2008), while verbs and verb arguments
are annotated with Propbank rolesets (Palmer et al,
2005). In the biomedical GENIA corpus (Ohta et
al., 2002), scientific text is annotated with POS tags,
parse trees, and named entities.
In this paper, we introduce multi-task active
learning (MTAL), an active learning paradigm for
multiple annotation tasks. We propose a new AL
framework where the examples to be annotated are
selected so that they are as informative as possible
for a set of classifiers instead of a single classifier
only. This enables the creation of a single combined
corpus annotated with respect to various annotation
tasks, while preserving the advantages of AL with
respect to the minimization of annotation efforts.
In a proof-of-concept scenario, we focus on two
highly dissimilar tasks, syntactic parsing and named
entity recognition, study the effects of multi-task AL
under rather extreme conditions. We propose two
MTAL meta-protocols and a method to implement
them for these tasks. We run experiments on three
corpora for domains and genres that are very differ-
ent (WSJ: newspapers, Brown: mixed genres, and
GENIA: biomedical abstracts). Our protocols out-
perform two baselines (random and a stronger one-
sided selection baseline).
In Section 2 we introduce our MTAL framework
and present two MTAL protocols. In Section 3 we
discuss the evaluation of these protocols. Section
4 describes the experimental setup, and results are
presented in Section 5. We discuss related work in
Section 6. Finally, we point to open research issues
for this new approach in Section 7.
2 A Framework for Multi-Task AL
In this section we introduce a sample selection
framework that aims at reducing the human anno-
tation effort in a multiple annotation scenario.
2.1 Task Definition
To measure the efficiency of selection methods, we
define the training quality TQ of annotated mate-
rial S as the performance p yielded with a reference
learner X trained on that material: TQ(X, S) = p.
A selection method can be considered better than an-
other one if a higher TQ is yielded with the same
amount of examples being annotated.
Our framework is an extension of the Active
Learning (AL) framework (Cohn et al, 1996)). The
original AL framework is based on querying in an it-
erative manner those examples to be manually anno-
tated that are most useful for the learner at hand. The
TQ of an annotated corpus selected by means of AL
is much higher than random selection. This AL ap-
proach can be considered as single-task AL because
it focuses on a single learner for which the examples
are to be selected. In a multiple annotation scenario,
however, there are several annotation tasks to be ac-
complished at once and for each task typically a sep-
arate statistical model will then be trained. Thus, the
goal of multi-task AL is to query those examples for
862
human annotation that are most informative for all
learners involved.
2.2 One-Sided Selection vs. Multi-Task AL
The naive approach to select examples in a multiple
annotation scenario would be to perform a single-
task AL selection, i.e., the examples to be annotated
are selected with respect to one of the learners only.1
In a multiple annotation scenario we call such an ap-
proach one-sided selection. It is an intrinsic selec-
tion for the reference learner, and an extrinsic selec-
tion for all the other learners also trained on the an-
notated material. Obviously, a corpus compiled with
the help of one-sided selection will have a good TQ
for that learner for which the intrinsic selection has
taken place. For all the other learners, however, we
have no guarantee that their TQ will not be inferior
than the TQ of a random selection process.
In scenarios where the different annotation tasks
are highly dissimilar we can expect extrinsic selec-
tion to be rather poor. This intuition is demonstrated
by experiments we conducted for named entity (NE)
and parse annotation tasks2 (Figure 1). In this sce-
nario, extrinsic selection for the NE annotation task
means that examples where selected with respect
to the parsing task. Extrinsic selection performed
about the same as random selection for the NE task,
while for the parsing task extrinsic selection per-
formed markedly worse. This shows that examples
that were very informative for the NE learner were
not that informative for the parse learner.
2.3 Protocols for Multi-Task AL
Obviously, we can expect one-sided selection to per-
form better for the reference learner (the one for
which an intrinsic selection took place) than multi-
task AL selection, because the latter would be a
compromise for all learners involved in the multi-
ple annotation scenario. However, the goal of multi-
task AL is to minimize the annotation effort over all
annotation tasks and not just the effort for a single
annotation task.
For a multi-task AL protocol to be valuable in a
specific multiple annotation scenario, the TQ for all
considered learners should be
1Of course, all selected examples would be annotated w.r.t.
all annotation tasks.
2See Section 4 for our experimental setup.
1. better than the TQ of random selection,
2. and better than the TQ of any extrinsic selec-
tion.
In the following, we introduce two protocols for
multi-task AL. Multi-task AL protocols can be con-
sidered meta-protocols because they basically spec-
ify how task-specific, single-task AL approaches can
be combined into one selection decision. By this,
the protocols are independent of the underlying task-
specific AL approaches.
2.3.1 Alternating Selection
The alternating selection protocol alternates one-
sided AL selection. In sj consecutive AL iterations,
the selection is performed as one-sided selection
with respect to learning algorithm Xj . After that,
another learning algorithm is considered for selec-
tion for sk consecutive iterations and so on. Depend-
ing on the specific scenario, this enables to weight
the different annotation tasks by allowing them to
guide the selection in more or less AL iterations.
This protocol is a straight-forward compromise be-
tween the different single-task selection approaches.
In this paper we experiment with the special case
of si = 1, where in every AL iteration the selection
leadership is changed. More sophisticated calibra-
tion of the parameters si is beyond the scope of this
paper and will be dealt with in future work.
2.3.2 Rank Combination
The rank combination protocol is more directly
based on the idea to combine single-task AL selec-
tion decisions. In each AL iteration, the usefulness
score sXj (e) of each unlabeled example e from the
pool of examples is calculated with respect to each
learner Xj and then translated into a rank rXj (e)
where higher usefulness means lower rank number
(examples with identical scores get the same rank
number). Then, for each example, we sum the rank
numbers of each annotation task to get the overall
rank r(e) = ?nj=1 rXj (e). All examples are sorted
by this combined rank and b examples with lowest
rank numbers are selected for manual annotation.3
3As the number of ranks might differ between the single an-
notation tasks, we normalize them to the coarsest scale. Then
we can sum up the ranks as explained above.
863
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
random selection
extrinsic selection (PARSE?AL)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
random selection
extrinsic selection (NE?AL)
Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntactic
parse annotation (right), using the WSJ corpus scenario
This protocol favors examples which are good for
all learning algorithms. Examples that are highly in-
formative for one task but rather uninformative for
another task will not be selected.
3 Evaluation of Multi-Task AL
The notion of training quality (TQ) can be used to
quantify the effectiveness of a protocol, and by this,
annotation costs in a single-task AL scenario. To ac-
tually quantify the overall training quality in a multi-
ple annotation scenario one would have to sum over
all the single task?s TQs. Of course, depending on
the specific annotation task, one would not want to
quantify the number of examples being annotated
but different task-specific units of annotation. While
for entity annotations one does typically count the
number of tokens being annotated, in the parsing
scenario the number of constituents being annotated
is a generally accepted measure. As, however, the
actual time needed for the annotation of one exam-
ple usually differs for different annotation tasks, nor-
malizing exchange rates have to be specified which
can then be used as weighting factors. In this paper,
we do not define such weighting factors4, and leave
this challenging question to be discussed in the con-
text of psycholinguistic research.
We could quantify the overall efficiency score E
of a MTAL protocol P by
E(P ) =
n
?
j=1
?j ? TQ(Xj , uj)
where uj denotes the individual annotation task?s
4Such weighting factors not only depend on the annotation
level or task but also on the domain, and especially on the cog-
nitive load of the annotation task.
number of units being annotated (e.g., constituents
for parsing) and the task-specific weights are defined
by ?j . Given weights are properly defined, such a
score can be applied to directly compare different
protocols and quantify their differences.
In practice, such task-specific weights might also
be considered in the MTAL protocols. In the alter-
nating selection protocol, the numbers of consecu-
tive iterations si each single task protocol can be
tuned according to the ? parameters. As for the
rank combination protocol, the weights can be con-
sidered when calculating the overall rank: r(e) =
?n
j=1 ?j ? rXj (e) where the parameters ?1 . . . ?n re-
flect the values of ?1 . . . ?n (though they need not
necessarily be the same).
In our experiments, we assumed the same weight
for all annotation schemata, thus simply setting si =
1, ?i = 1. This was done for the sake of a clear
framework presentation. Finding proper weights for
the single tasks and tuning the protocols accordingly
is a subject for further research.
4 Experiments
4.1 Scenario and Task-Specific Selection
Protocols
The tasks in our scenario comprise one semantic
task (annotation with named entities (NE)) and one
syntactic task (annotation with PCFG parse trees).
The tasks are highly dissimilar, thus increasing the
potential value of MTAL. Both tasks are subject to
intensive research by the NLP community.
The MTAL protocols proposed are meta-
protocols that combine the selection decisions of
the underlying, task-specific AL protocols. In
our scenario, the task-specific AL protocols are
864
committee-based (Freund et al, 1997) selection
protocols. In committee-based AL, a committee
consists of k classifiers of the same type trained
on different subsets of the training data.5 Each
committee member then makes its predictions on
the unlabeled examples, and those examples on
which the committee members disagree most are
considered most informative for learning and are
thus selected for manual annotation. In our scenario
the example grain-size is the sentence level.
For the NE task, we apply the AL approach of
Tomanek et al (2007). The committee consists of
k1 = 3 classifiers and the vote entropy (VE) (Engel-
son and Dagan, 1996) is employed as disagreement
metric. It is calculated on the token-level as
V Etok(t) = ?
1
log k
c
?
i=0
V (li, t)
k log
V (li, t)
k (1)
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. The sentence level
vote entropy V Esent is then the average over all to-
kens tj of sentence s.
For the parsing task, the disagreement score is
based on a committee of k2 = 10 instances of Dan
Bikel?s reimplementation of Collins? parser (Bickel,
2005; Collins, 1999). For each sentence in the un-
labeled pool, the agreement between the committee
members was calculated using the function reported
by Reichart and Rappoport (2007):
AF (s) = 1N
?
i,l?[1...N ],i6=l
fscore(mi, ml) (2)
Where mi and ml are the committee members and
N = k2?(k2?1)2 is the number of pairs of different
committee members. This function calculates the
agreement between the members of each pair by cal-
culating their relative f-score and then averages the
pairs? scores. The disagreement of the committee on
a sentence is simply 1 ? AF (s).
4.2 Experimental settings
For the NE task we employed the classifier described
by Tomanek et al (2007): The NE tagger is based on
Conditional Random Fields (Lafferty et al, 2001)
5We randomly sampled L = 34 of the training data to create
each committee member.
and has a rich feature set including orthographical,
lexical, morphological, POS, and contextual fea-
tures. For parsing, Dan Bikel?s reimplementation of
Collins? parser is employed, using gold POS tags.
In each AL iteration we select 100 sentences for
manual annotation.6 We start with a randomly cho-
sen seed set of 200 sentences. Within a corpus we
used the same seed set in all selection scenarios. We
compare the following five selection scenarios: Ran-
dom selection (RS), which serves as our baseline;
one-sided AL selection for both tasks (called NE-AL
and PARSE-AL); and multi-task AL selection with
the alternating selection protocol (alter-MTAL) and
the rank combination protocol (ranks-MTAL).
We performed our experiments on three dif-
ferent corpora, namely one from the newspaper
genre (WSJ), a mixed-genre corpus (Brown), and a
biomedical corpus (Bio). Our simulation corpora
contain both entity annotations and (constituent)
parse annotations. For each corpus we have a pool
set (from which we select the examples for annota-
tion) and an evaluation set (used for generating the
learning curves). The WSJ corpus is based on the
WSJ part of the PENN TREEBANK (Marcus et al,
1993); we used the first 10,000 sentences of section
2-21 as the pool set, and section 00 as evaluation set
(1,921 sentences). The Brown corpus is also based
on the respective part of the PENN TREEBANK. We
created a sample consisting of 8 of any 10 consec-
utive sentences in the corpus. This was done as
Brown contains text from various English text gen-
res, and we did that to create a representative sample
of the corpus domains. We finally selected the first
10,000 sentences from this sample as pool set. Every
9th from every 10 consecutive sentences package
went into the evaluation set which consists of 2,424
sentences. For both WSJ and Brown only parse an-
notations though no entity annotations were avail-
able. Thus, we enriched both corpora with entity
annotations (three entities: person, location, and or-
ganization) by means of a tagger trained on the En-
glish data set of the CoNLL-2003 shared task (Tjong
Kim Sang and De Meulder, 2003).7 The Bio corpus
6Manual annotation is simulated by just unveiling the anno-
tations already contained in our corpora.
7We employed a tagger similar to the one presented by Set-
tles (2004). Our tagger has a performance of ? 84% f-score on
the CoNLL-2003 data; inspection of the predicted entities on
865
is based on the parsed section of the GENIA corpus
(Ohta et al, 2002). We performed the same divi-
sions as for Brown, resulting in 2,213 sentences in
our pool set and 276 sentences for the evaluation set.
This part of the GENIA corpus comes with entity an-
notations. We have collapsed the entity classes an-
notated in GENIA (cell line, cell type, DNA, RNA,
protein) into a single, biological entity class.
5 Results
In this section we present and discuss our results
when applying the five selection strategies (RS, NE-
AL, PARSE-AL, alter-MTAL, and ranks-MTAL) to
our scenario on the three corpora. We refrain from
calculating the overall efficiency score (Section 3)
here due to the lack of generally accepted weights
for the considered annotation tasks. However, we
require from a good selection protocol to exceed the
performance of random selection and extrinsic se-
lection. In addition, recall from Section 3 that we
set the alternate selection and rank combination pa-
rameters to si = 1, ?i = 1, respectively to reflect a
tradeoff between the annotation efforts of both tasks.
Figures 2 and 3 depict the learning curves for
the NE tagger and the parser on WSJ and Brown,
respectively. Each figure shows the five selection
strategies. As expected, on both corpora and both
tasks intrinsic selection performs best, i.e., for the
NE tagger NE-AL and for the parser PARSE-AL.
Further, random selection and extrinsic selection
perform worst. Most importantly, both MTAL pro-
tocols clearly outperform extrinsic and random se-
lection in all our experiments. This is in contrast
to NE-AL which performs worse than random se-
lection for all corpora when used as extrinsic selec-
tion, and for PARSE-AL that outperforms the ran-
dom baseline only for Brown when used as extrin-
sic selection. That is, the MTAL protocols suggest a
tradeoff between the annotation efforts of the differ-
ent tasks, here.
On WSJ, both for the NE and the parse annotation
tasks, the performance of the MTAL protocols is
very similar, though ranks-MTAL performs slightly
better. For the parser task, up to 30,000 constituents
MTAL performs almost as good as does PARSE-
AL. This is different for the NE task where NE-AL
WSJ and Brown revealed a good tagging performance.
clearly outperforms MTAL. On Brown, in general
we see the same results, with some minor differ-
ences. On the NE task, extrinsic selection (PARSE-
AL) performs better than random selection, but it is
still much worse than intrinsic AL or MTAL. Here,
ranks-MTAL significantly outperforms alter-MTAL
and almost performs as good as intrinsic selection.
For the parser task, we see that extrinsic and ran-
dom selection are equally bad. Both MTAL proto-
cols perform equally well, again being quite similar
to the intrinsic selection. On the BIO corpus8 we ob-
served the same tendencies as in the other two cor-
pora, i.e., MTAL clearly outperforms extrinsic and
random selection and supplies a better tradeoff be-
tween annotation efforts of the task at hand than one-
sided selection.
Overall, we can say that in all scenarios MTAL
performs much better than random selection and ex-
trinsic selection, and in most cases the performance
of MTAL (especially but not exclusively, ranks-
MTAL) is even close to intrinsic selection. This is
promising evidence that MTAL selection can be a
better choice than one-sided selection in multiple an-
notation scenarios. Thus, considering all annotation
tasks in the selection process (even if the selection
protocol is as simple as the alternating selection pro-
tocol) is better than selecting only with respect to
one task. Further, it should be noted that overall the
more sophisticated rank combination protocol does
not perform much better than the simpler alternating
selection protocol in all scenarios.
Finally, Figure 4 shows the disagreement curves
for the two tasks on the WSJ corpus. As has already
been discussed by Tomanek and Hahn (2008), dis-
agreement curves can be used as a stopping crite-
rion and to monitor the progress of AL-driven an-
notation. This is especially valuable when no anno-
tated validation set is available (which is needed for
plotting learning curves). We can see that the dis-
agreement curves significantly flatten approximately
at the same time as the learning curves do. In the
context of MTAL, disagreement curves might not
only be interesting as a stopping criterion but rather
as a switching criterion, i.e., to identify when MTAL
could be turned into one-sided selection. This would
be the case if in an MTAL scenario, the disagree-
8The plots for the Bio are omitted due to space restrictions.
866
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
0.
85
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000
0.
55
0.
60
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 2: Learning curves for NE task on WSJ (left) and Brown (right)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000 35000
0.
65
0.
70
0.
75
0.
80
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 3: Learning curves for parse task on WSJ (left) and Brown (right)
ment curve of one task has a slope of (close to) zero.
Future work will focus on issues related to this.
6 Related Work
There is a large body of work on single-task AL ap-
proaches for many NLP tasks where the focus is
mainly on better, task-specific selection protocols
and methods to quantify the usefulness score in dif-
ferent scenarios. As to the tasks involved in our
scenario, several papers address AL for NER (Shen
et al, 2004; Hachey et al, 2005; Tomanek et al,
2007) and syntactic parsing (Tang et al, 2001; Hwa,
2004; Baldridge and Osborne, 2004; Becker and Os-
borne, 2005). Further, there is some work on ques-
tions arising when AL is to be used in real-life anno-
tation scenarios, including impaired inter-annotator
agreement, stopping criteria for AL-driven annota-
tion, and issues of reusability (Baldridge and Os-
borne, 2004; Hachey et al, 2005; Zhu and Hovy,
2007; Tomanek et al, 2007).
Multi-task AL is methodologically related to ap-
proaches of decision combination, especially in the
context of classifier combination (Ho et al, 1994)
and ensemble methods (Breiman, 1996). Those ap-
proaches focus on the combination of classifiers in
order to improve the classification error rate for one
specific classification task. In contrast, the focus of
multi-task AL is on strategies to select training ma-
terial for multi classifier systems where all classifiers
cover different classification tasks.
7 Discussion
Our treatment of MTAL within the context of the
orthogonal two-task scenario leads to further inter-
esting research questions. First, future investiga-
tions will have to focus on the question whether
the positive results observed in our orthogonal (i.e.,
highly dissimilar) two-task scenario will also hold
for a more realistic (and maybe more complex) mul-
tiple annotation scenario where tasks are more sim-
ilar and more than two annotation tasks might be
involved. Furthermore, several forms of interde-
pendencies may arise between the single annotation
tasks. As a first example, consider the (functional)
interdependencies (i.e., task similarity) in higher-
level semantic NLP tasks of relation or event recog-
nition. In such a scenario, several tasks including
entity annotations and relation/event annotations, as
well as syntactic parse data, have to be incorporated
at the same time. Another type of (data flow) inter-
867
10000 20000 30000 40000
0.
01
0
0.
01
4
0.
01
8
tokens
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
10000 20000 30000 40000
5
10
15
20
25
30
35
40
constituents
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 4: Disagreement curves for NE task (left) and parse task (right) on WSJ
dependency occurs in a second scenario where ma-
terial for several classifiers that are data-dependent
on each other ? one takes the output of another clas-
sifier as input features ? has to be efficiently anno-
tated. Whether the proposed protocols are beneficial
in the context of such highly interdependent tasks is
an open issue. Even more challenging is the idea
to provide methodologies helping to predict in an
arbitrary application scenario whether the choice of
MTAL is truly advantageous.
Another open question is how to measure and
quantify the overall annotation costs in multiple an-
notation scenarios. Exchange rates are inherently
tied to the specific task and domain. In practice, one
might just want to measure the time needed for the
annotations. However, in a simulation scenario, a
common metric is necessary to compare the perfor-
mance of different selection strategies with respect
to the overall annotation costs. This requires stud-
ies on how to quantify, with a comparable cost func-
tion, the efforts needed for the annotation of a textual
unit of choice (e.g., tokens, sentences) with respect
to different annotation tasks.
Finally, the question of reusability of the anno-
tated material is an important issue. Reusability in
the context of AL means to which degree corpora
assembled with the help of any AL technique can be
(re)used as a general resource, i.e., whether they are
well suited for the training of classifiers other than
the ones used during the selection process.This is
especially interesting as the details of the classifiers
that should be trained in a later stage are typically
not known at the resource building time. Thus, we
want to select samples valuable to a family of clas-
sifiers using the various annotation layers. This, of
course, is only possible if data annotated with the
help of AL is reusable by modified though similar
classifiers (e.g., with respect to the features being
used) ? compared to the classifiers employed for the
selection procedure.
The issue of reusability has already been raised
but not yet conclusively answered in the context of
single-task AL (see Section 6). Evidence was found
that reusability up to a certain, though not well-
specified, level is possible. Of course, reusability
has to be analyzed separately in the context of var-
ious MTAL scenarios. We feel that these scenarios
might both be more challenging and more relevant
to the reusability issue than the single-task AL sce-
nario, since resources annotated with multiple lay-
ers can be used to the design of a larger number of a
(possibly more complex) learning algorithms.
8 Conclusions
We proposed an extension to the single-task AL ap-
proach such that it can be used to select examples for
annotation with respect to several annotation tasks.
To the best of our knowledge this is the first paper on
this issue, with a focus on NLP tasks. We outlined
a problem definition and described a framework for
multi-task AL. We presented and tested two proto-
cols for multi-task AL. Our results are promising as
they give evidence that in a multiple annotation sce-
nario, multi-task AL outperforms naive one-sided
and random selection.
Acknowledgments
The work of the second author was funded by the
German Ministry of Education and Research within
the STEMNET project (01DS001A-C), while the
work of the third author was funded by the EC
within the BOOTSTREP project (FP6-028099).
868
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP?04, pages 9?16.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings of IJCAI?05, pages 991?996.
Daniel M. Bickel. 2005. Code developed at the Univer-
sity of Pennsylvania, http://www.cis.upenn.
edu/
?
dbikel/software.html.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL?96, pages 319?326.
Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of CoNLL?05, pages
144?151.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 16(1):66?75.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of ICML?01, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind K. Joshi. 2008. Sense annotation in the penn
discourse treebank. In Proceedings of CICLing?08,
pages 275?286.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL?00,
pages 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceedings
of HLT?02, pages 82?86.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Roi Reichart and Ari Rappoport. 2007. An ensemble
method for selection of high quality parses. In Pro-
ceedings of ACL?07, pages 408?415, June.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of JNLPBA?04, pages 107?110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceedings
of ACL?04, pages 589?596.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
Proceedings of ACL?02, pages 120?127.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL?03, pages 142?147.
Katrin Tomanek and Udo Hahn. 2008. Approximating
learning curves for active-learning-driven annotation.
In Proceedings of LREC?08.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains corpus reusabil-
ity of annotated data. In Proceedings of EMNLP-
CoNLL?07, pages 486?495.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
EMNLP-CoNLL?07, pages 783?790.
869
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1039?1047,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Active Learning for Sequence Labeling
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
While Active Learning (AL) has already
been shown to markedly reduce the anno-
tation efforts for many sequence labeling
tasks compared to random selection, AL
remains unconcerned about the internal
structure of the selected sequences (typ-
ically, sentences). We propose a semi-
supervised AL approach for sequence la-
beling where only highly uncertain sub-
sequences are presented to human anno-
tators, while all others in the selected se-
quences are automatically labeled. For the
task of entity recognition, our experiments
reveal that this approach reduces annota-
tion efforts in terms of manually labeled
tokens by up to 60 % compared to the stan-
dard, fully supervised AL scheme.
1 Introduction
Supervised machine learning (ML) approaches are
currently the methodological backbone for lots of
NLP activities. Despite their success they create a
costly follow-up problem, viz. the need for human
annotators to supply large amounts of ?golden?
annotation data on which ML systems can be
trained. In most annotation campaigns, the lan-
guage material chosen for manual annotation is se-
lected randomly from some reference corpus.
Active Learning (AL) has recently shaped as a
much more efficient alternative for the creation of
precious training material. In the AL paradigm,
only examples of high training utility are selected
for manual annotation in an iterative manner. Dif-
ferent approaches to AL have been successfully
applied to a wide range of NLP tasks (Engel-
son and Dagan, 1996; Ngai and Yarowsky, 2000;
Tomanek et al, 2007; Settles and Craven, 2008).
When used for sequence labeling tasks such as
POS tagging, chunking, or named entity recogni-
tion (NER), the examples selected by AL are se-
quences of text, typically sentences. Approaches
to AL for sequence labeling are usually uncon-
cerned about the internal structure of the selected
sequences. Although a high overall training util-
ity might be attributed to a sequence as a whole,
the subsequences it is composed of tend to ex-
hibit different degrees of training utility. In the
NER scenario, e.g., large portions of the text do
not contain any target entity mention at all. To
further exploit this observation for annotation pur-
poses, we here propose an approach to AL where
human annotators are required to label only uncer-
tain subsequences within the selected sentences,
while the remaining subsequences are labeled au-
tomatically based on the model available from the
previous AL iteration round. The hardness of sub-
sequences is characterized by the classifier?s con-
fidence in the predicted labels. Accordingly, our
approach is a combination of AL and self-training
to which we will refer as semi-supervised Active
Learning (SeSAL) for sequence labeling.
While self-training and other bootstrapping ap-
proaches often fail to produce good results on NLP
tasks due to an inherent tendency of deteriorated
data quality, SeSAL circumvents this problem and
still yields large savings in terms annotation de-
cisions, i.e., tokens to be manually labeled, com-
pared to a standard, fully supervised AL approach.
After a brief overview of the formal underpin-
nings of Conditional Random Fields, our base
classifier for sequence labeling tasks (Section 2),
a fully supervised approach to AL for sequence
labeling is introduced and complemented by our
semi-supervised approach in Section 3. In Section
4, we discuss SeSAL in relation to bootstrapping
and existing AL techniques. Our experiments are
laid out in Section 5 where we compare fully and
semi-supervised AL for NER on two corpora, the
newspaper selection of MUC7 and PENNBIOIE, a
biological abstracts corpus.
1039
2 Conditional Random Fields for
Sequence Labeling
Many NLP tasks, such as POS tagging, chunking,
or NER, are sequence labeling problems where a
sequence of class labels ~y = (y1, . . . ,yn) ? Yn
are assigned to a sequence of input units
~x = (x1, . . . ,xn) ? X n. Input units xj are usually
tokens, class labels yj can be POS tags or entity
classes.
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) are a probabilistic framework for label-
ing structured data and model P~?(~y|~x). We focus
on first-order linear-chain CRFs, a special form of
CRFs for sequential data, where
P~?(~y|~x) =
1
Z~?(~x)
? exp
(
n
?
j=1
m
?
i=1
?ifi(yj?1,yj ,~x, j)
)
(1)
with normalization factor Z~?(~x), feature functions
fi(?), and feature weights ?i.
Parameter Estimation. The model parameters
?i are set to maximize the penalized log-likelihood
L on some training data T :
L(T ) =
?
(~x,~y)?T
log p(~y|~x) ?
m
?
i=1
?2i
2?2 (2)
The partial derivations of L(T ) are
?L(T )
??i
= E?(fi) ? E(fi) ?
?i
?2 (3)
where E?(fi) is the empirical expectation of fea-
ture fi and can be calculated by counting the oc-
currences of fi in T . E(fi) is the model expecta-
tion of fi and can be written as
E(fi) =
?
(~x,~y)?T
?
~y ??Yn
P~?(~y
?|~x)?
n
?
j=1
fi(y?j?1, y?j , ~x,j) (4)
Direct computation of E(fi) is intractable due to
the sum over all possible label sequences ~y ? ? Yn.
The Forward-Backward algorithm (Rabiner, 1989)
solves this problem efficiently. Forward (?) and
backward (?) scores are defined by
?j(y|~x) =
?
y??T?1j (y)
?j?1(y?|~x) ? ?j(~x, y?, y)
?j(y|~x) =
?
y??Tj(y)
?j+1(y?|~x) ? ?j(~x, y, y?)
where ?j(~x,a,b) = exp
(
?m
i=1 ?ifi(a,b,~x, j)
)
,
Tj(y) is the set of all successors of a state y at a
specified position j, and, accordingly, T?1j (y) is
the set of predecessors.
Normalized forward and backward scores
are inserted into Equation (4) to replace
?
~y ??Yn P~?(~y
?|~x) so that L(T ) can be opti-
mized with gradient-based or iterative-scaling
methods.
Inference and Probabilities. The marginal
probability
P~?(yj = y
?|~x) = ?j(y
?|~x) ? ?j(y?|~x)
Z~?(~x)
(5)
specifies the model?s confidence in label y? at po-
sition j of an input sequence ~x. The forward
and backward scores are obtained by applying the
Forward-Backward algorithm on ~x. The normal-
ization factor is efficiently calculated by summing
over all forward scores:
Z~?(~x) =
?
y?Y
?n(y|~x) (6)
The most likely label sequence
~y ? = argmax
~y?Yn
exp
(
n
?
j=1
m
?
i=1
?ifi(yj?1,yj ,~x, j)
)
(7)
is computed using the Viterbi algorithm (Rabiner,
1989). See Equation (1) for the conditional prob-
ability P~?(~y
?|~x) with Z~? calculated as in Equa-
tion (6). The marginal and conditional probabili-
ties are used by our AL approaches as confidence
estimators.
3 Active Learning for Sequence Labeling
AL is a selective sampling technique where the
learning protocol is in control of the data to be
used for training. The intention with AL is to re-
duce the amount of labeled training material by
querying labels only for examples which are as-
sumed to have a high training utility. This section,
first, describes a common approach to AL for se-
quential data, and then presents our approach to
semi-supervised AL.
3.1 Fully Supervised Active Learning
Algorithm 1 describes the general AL framework.
A utility function UM(pi) is the core of each AL
approach ? it estimates how useful it would be for
1040
Algorithm 1 General AL framework
Given:
B: number of examples to be selected
L: set of labeled examples
P : set of unlabeled examples
UM: utility function
Algorithm:
loop until stopping criterion is met
1. learn modelM from L
2. for all pi ? P : upi ? UM(pi)
3. select B examples pi ? P with highest utility upi
4. query human annotator for labels of all B examples
5. move newly labeled examples from P to L
return L
a specific base learner to have an unlabeled exam-
ple labeled and, subsequently included in the train-
ing set.
In the sequence labeling scenario, such an ex-
ample is a stream of linguistic items ? a sentence
is usually considered as proper sequence unit. We
apply CRFs as our base learner throughout this pa-
per and employ a utility function which is based
on the conditional probability of the most likely
label sequence ~y ? for an observation sequence ~x
(cf. Equations (1) and (7)):
U~?(~x) = 1 ? P~?(~y
?|~x) (8)
Sequences for which the current model is least
confident on the most likely label sequence are
preferably selected.1 These selected sentences are
fully manually labeled. We refer to this AL mode
as fully supervised Active Learning (FuSAL).
3.2 Semi-Supervised Active Learning
In the sequence labeling scenario, an example
which, as a whole, has a high utility U~?(~x), can
still exhibit subsequences which do not add much
to the overall utility and thus are fairly easy for the
current model to label correctly. One might there-
fore doubt whether it is reasonable to manually la-
bel the entire sequence. Within many sequences
of natural language data, there are probably large
subsequences on which the current model already
does quite well and thus could automatically gen-
erate annotations with high quality. This might, in
particular, apply to NER where larger stretches of
sentences do not contain any entity mention at all,
or merely trivial instances of an entity class easily
predictable by the current model.
1There are many more sophisticated utility functions for
sequence labeling. We have chosen this straightforward one
for simplicity and because it has proven to be very effective
(Settles and Craven, 2008).
For the sequence labeling scenario, we accord-
ingly modify the fully supervised AL approach
from Section 3.1. Only those tokens remain to be
manually labeled on which the current model is
highly uncertain regarding their class labels, while
all other tokens (those on which the model is suf-
ficiently certain how to label them correctly) are
automatically tagged.
To select the sequence examples the same util-
ity function as for FuSAL (cf. Equation (8)) is ap-
plied. To identify tokens xj from the selected se-
quences which still have to be manually labeled,
the model?s confidence in label y?j is estimated by
the marginal probability (cf. Equation (5))
C~?(y
?
j ) = P~?(yj = y
?
j |~x) (9)
where y?j specifies the label at the respective po-
sition of the most likely label sequence ~y ? (cf.
Equation (7)). If C~?(y?j ) exceeds a certain con-fidence threshold t, y?j is assumed to be the correct
label for this token and assigned to it.2 Otherwise,
manual annotation of this token is required. So,
compared to FuSAL as described in Algorithm 1
only the third step step is modified.
We call this semi-supervised Active Learning
(SeSAL) for sequence labeling. SeSAL joins the
standard, fully supervised AL schema with a boot-
strapping mode, namely self-training, to combine
the strengths of both approaches. Examples with
high training utility are selected using AL, while
self-tagging of certain ?safe? regions within such
examples additionally reduces annotation effort.
Through this combination, SeSAL largely evades
the problem of deteriorated data quality, a limiting
factor of ?pure? bootstrapping approaches.
This approach requires two parameters to be set:
Firstly, the confidence threshold t which directly
influences the portion of tokens to be manually
labeled. Using lower thresholds, the self-tagging
component of SeSAL has higher impact ? presum-
ably leading to larger amounts of tagging errors.
Secondly, a delay factor d can be specified which
channels the amount of manually labeled tokens
obtained with FuSAL before SeSAL is to start.
Only with d = 0, SeSAL will already affect the
first AL iteration. Otherwise, several iterations of
FuSAL are run until a switch to SeSAL will hap-
pen.
2Sequences of consecutive tokens xj for which C~?(y?j ) ?
t are presented to the human annotator instead of single, iso-
lated tokens.
1041
It is well known that the performance of boot-
strapping approaches crucially depends on the size
of the seed set ? the amount of labeled examples
available to train the initial model. If class bound-
aries are poorly defined by choosing the seed set
too small, a bootstrapping system cannot learn
anything reasonable due to high error rates. If, on
the other hand, class boundaries are already too
well defined due to an overly large seed set, noth-
ing to be learned is left. Thus, together with low
thresholds, a delay rate of d > 0 might be crucial
to obtain models of high performance.
4 Related Work
Common approaches to AL are variants of the
Query-By-Committee approach (Seung et al,
1992) or based on uncertainty sampling (Lewis
and Catlett, 1994). Query-by-Committee uses a
committee of classifiers, and examples on which
the classifiers disagree most regarding their pre-
dictions are considered highly informative and
thus selected for annotation. Uncertainty sam-
pling selects examples on which a single classi-
fier is least confident. AL has been successfully
applied to many NLP tasks; Settles and Craven
(2008) compare the effectiveness of several AL
approaches for sequence labeling tasks of NLP.
Self-training (Yarowsky, 1995) is a form of
semi-supervised learning. From a seed set of la-
beled examples a weak model is learned which
subsequently gets incrementally refined. In each
step, unlabeled examples on which the current
model is very confident are labeled with their pre-
dictions, added to the training set, and a new
model is learned. Similar to self-training, co-
training (Blum and Mitchell, 1998) augments the
training set by automatically labeled examples.
It is a multi-learner algorithm where the learners
have independent views on the data and mutually
produce labeled examples for each other.
Bootstrapping approaches often fail when ap-
plied to NLP tasks where large amounts of training
material are required to achieve acceptable perfor-
mance levels. Pierce and Cardie (2001) showed
that the quality of the automatically labeled train-
ing data is crucial for co-training to perform well
because too many tagging errors prevent a high-
performing model from being learned. Also, the
size of the seed set is an important parameter.
When it is chosen too small data quality gets dete-
riorated quickly, when it is chosen too large no im-
provement over the initial model can be expected.
To address the problem of data pollution by tag-
ging errors, Pierce and Cardie (2001) propose cor-
rected co-training. In this mode, a human is put
into the co-training loop to review and, if neces-
sary, to correct the machine-labeled examples. Al-
though this effectively evades the negative side ef-
fects of deteriorated data quality, one may find the
correction of labeled data to be as time-consuming
as annotations from the scratch. Ideally, a human
should not get biased by the proposed label but
independently examine the example ? so that cor-
rection eventually becomes annotation.
In contrast, our SeSAL approach which also ap-
plies bootstrapping, aims at avoiding to deteriorate
data quality by explicitly pointing human annota-
tors to classification-critical regions. While those
regions require full annotation, regions of high
confidence are automatically labeled and thus do
not require any manual inspection. Self-training
and co-training, in contradistinction, select exam-
ples of high confidence only. Thus, these boot-
strapping methods will presumably not find the
most useful unlabeled examples but require a hu-
man to review data points of limited training util-
ity (Pierce and Cardie, 2001). This shortcoming is
also avoided by our SeSAL approach, as we inten-
tionally select informative examples only.
A combination of active and semi-supervised
learning has first been proposed by McCallum and
Nigam (1998) for text classification. Committee-
based AL is used for the example selection. The
committee members are first trained on the labeled
examples and then augmented by means of Expec-
tation Maximization (EM) (Dempster et al, 1977)
including the unlabeled examples. The idea is
to avoid manual labeling of examples whose la-
bels can be reliably assigned by EM. Similarly,
co-testing (Muslea et al, 2002), a multi-view AL
algorithms, selects examples for the multi-view,
semi-supervised Co-EM algorithm. In both works,
semi-supervision is based on variants of the EM
algorithm in combination with all unlabeled ex-
amples from the pool. Our approach to semi-
supervised AL is different as, firstly, we aug-
ment the training data using a self-tagging mech-
anism (McCallum and Nigam (1998) and Muslea
et al (2002) performed semi-supervision to aug-
ment the models using EM), and secondly, we op-
erate in the sequence labeling scenario where an
example is made up of several units each requiring
1042
a label ? partial labeling of sequence examples is
a central characteristic of our approach. Another
work also closely related to ours is that of Krist-
jansson et al (2004). In an information extraction
setting, the confidence per extracted field is cal-
culated by a constrained variant of the Forward-
Backward algorithm. Unreliable fields are high-
lighted so that the automatically annotated corpus
can be corrected. In contrast, AL selection of ex-
amples together with partial manual labeling of the
selected examples are the main foci of our work.
5 Experiments and Results
In this section, we turn to the empirical assessment
of semi-supervised AL (SeSAL) for sequence la-
beling on the NLP task of named entity recogni-
tion. By the nature of this task, the sequences ?
in this case, sentences ? are only sparsely popu-
lated with entity mentions and most of the tokens
belong to the OUTSIDE class3 so that SeSAL can
be expected to be very beneficial.
5.1 Experimental Settings
In all experiments, we employ the linear-chain
CRF model described in Section 2 as the base
learner. A set of common feature functions was
employed, including orthographical (regular ex-
pression patterns), lexical and morphological (suf-
fixes/prefixes, lemmatized tokens), and contextual
(features of neighboring tokens) ones.
All experiments start from a seed set of 20 ran-
domly selected examples and, in each iteration,
50 new examples are selected using AL. The ef-
ficiency of the different selection mechanisms is
determined by learning curves which relate the an-
notation costs to the performance achieved by the
respective model in terms of F1-score. The unit of
annotation costs are manually labeled tokens. Al-
though the assumption of uniform costs per token
has already been subject of legitimate criticism
(Settles et al, 2008), we believe that the number
of annotated tokens is still a reasonable approxi-
mation in the absence of an empirically more ade-
quate task-specific annotation cost model.
We ran the experiments on two entity-annotated
corpora. From the general-language newspaper
domain, we took the training part of the MUC7
corpus (Linguistic Data Consortium, 2001) which
incorporates seven different entity types, viz. per-
3The OUTSIDE class is assigned to each token that does
not denote an entity in the underlying domain of discourse.
corpus entity classes sentences tokens
MUC7 7 3,020 78,305
PENNBIOIE 3 10,570 267,320
Table 1: Quantitative characteristics of the chosen corpora
sons, organizations, locations, times, dates, mone-
tary expressions, and percentages. From the sub-
language biology domain, we used the oncology
part of the PENNBIOIE corpus (Kulick et al,
2004) and removed all but three gene entity sub-
types (generic, protein, and rna). Table 1 summa-
rizes the quantitative characteristics of both cor-
pora.4 The results reported below are averages of
20 independent runs. For each run, we randomly
split each corpus into a pool of unlabeled examples
to select from (90 % of the corpus), and a comple-
mentary evaluation set (10 % of the corpus).
5.2 Empirical Evaluation
We compare semi-supervised AL (SeSAL) with
its fully supervised counterpart (FuSAL), using
a passive learning scheme where examples are
randomly selected (RAND) as baseline. SeSAL
is first applied in a default configuration with a
very high confidence threshold (t = 0.99) with-
out any delay (d = 0). In further experiments,
these parameters are varied to study their impact
on SeSAL?s performance. All experiments were
run on both the newspaper (MUC7) and biological
(PENNBIOIE) corpus. When results are similar to
each other, only one data set will be discussed.
Distribution of Confidence Scores. The lead-
ing assumption for SeSAL is that only a small por-
tion of tokens within the selected sentences consti-
tute really hard decision problems, while the ma-
jority of tokens are easy to account for by the cur-
rent model. To test this stipulation we investigate
the distribution of the model?s confidence values
C~?(y
?
j ) over all tokens of the sentences (cf. Equa-
tion (9)) selected within one iteration of FuSAL.
Figure 1, as an example, depicts the histogram
for an early AL iteration round on the MUC7 cor-
pus. The vast majority of tokens has a confidence
score close to 1, the median lies at 0.9966. His-
tograms of subsequent AL iterations are very sim-
ilar with an even higher median. This is so because
4We removed sentences of considerable over and under
length (beyond +/- 3 standard deviations around the average
sentence length) so that the numbers in Table 1 differ from
those cited in the original sources.
1043
confidence score
fre
qu
en
cy
0.2 0.4 0.6 0.8 1.0
0
50
0
10
00
15
00
Figure 1: Distribution of token-level confidence scores in the
5th iteration of FuSAL on MUC7 (number of tokens: 1,843)
the model gets continuously more confident when
trained on additional data and fewer hard cases re-
main in the shrinking pool.
Fully Supervised vs. Semi-Supervised AL.
Figure 2 compares the performance of FuSAL and
SeSAL on the two corpora. SeSAL is run with
a delay rate of d = 0 and a very high confi-
dence threshold of t = 0.99 so that only those
tokens are automatically labeled on which the cur-
rent model is almost certain. Figure 2 clearly
shows that SeSAL is much more efficient than
its fully supervised counterpart. Table 2 depicts
the exact numbers of manually labeled tokens to
reach the maximal (supervised) F-score on both
corpora. FuSAL saves about 50 % compared to
RAND, while SeSAL saves about 60 % compared
to FuSAL which constitutes an overall saving of
over 80 % compared to RAND.
These savings are calculated relative to the
number of tokens which have to be manually la-
beled. Yet, consider the following gedanken ex-
periment. Assume that, using SeSAL, every sec-
ond token in a sequence would have to be labeled.
Though this comes to a ?formal? saving of 50 %,
the actual annotation effort in terms of the time
needed would hardly go down. It appears that
only when SeSAL splits a sentence into larger
Corpus Fmax RAND FuSAL SeSAL
MUC7 87.7 63,020 36,015 11,001
PENNBIOIE 82.3 194,019 83,017 27,201
Table 2: Tokens manually labeled to reach the maximal (su-
pervised) F-score
0 10000 30000 50000
0.
60
0.
70
0.
80
0.
90
MUC7
manually labeled tokens
F?
sc
or
e
SeSAL
FuSAL
RAND
0 10000 30000 50000
0.
60
0.
70
0.
80
0.
90
PennBioIE
manually labeled tokens
F?
sc
or
e
SeSAL
FuSAL
RAND
Figure 2: Learning curves for Semi-supervised AL (SeSAL),
Fully Supervised AL (FuSAL), and RAND(om) selection
well-packaged, chunk-like subsequences annota-
tion time can really be saved. To demonstrate that
SeSAL comes close to this, we counted the num-
ber of base noun phrases (NPs) containing one or
more tokens to be manually labeled. On the MUC7
corpus, FuSAL requires 7,374 annotated NPs to
yield an F-score of 87%, while SeSAL hit the
same F-score with only 4,017 NPs. Thus, also in
terms of the number of NPs, SeSAL saves about
45% of the material to be considered.5
Detailed Analysis of SeSAL. As Figure 2 re-
veals, the learning curves of SeSAL stop early (on
MUC7 after 12,800 tokens, on PENNBIOIE after
27,600 tokens) because at that point the whole cor-
pus has been labeled exhaustively ? either manu-
ally, or automatically. So, using SeSAL the com-
plete corpus can be labeled with only a small
fraction of it actually being manually annotated
(MUC7: about 18%, PENNBIOIE: about 13%).
5On PENNBIOIE, SeSAL also saves about 45% com-
pared to FuSAL to achieve an F-score of 81%.
1044
Table 3 provides additional analysis results on
MUC7. In very early AL rounds, a large ratio of
tokens has to be manually labeled (70-80 %). This
number decreases increasingly as the classifier im-
proves (and the pool contains fewer informative
sentences). The number of tagging errors is quite
low, resulting in a high accuracy of the created cor-
pus of constantly over 99 %.
labeled tokens
manual automatic ? AR (%) errors ACC
1,000 253 1,253 79.82 6 99.51
5,000 6,207 11,207 44.61 82 99.27
10,000 25,506 34,406 28.16 174 99.51
12,800 57,371 70,171 18.24 259 99.63
Table 3: Analysis of SeSAL on MUC7: Manually and auto-
matically labeled tokens, annotation rate (AR) as the portion
of manually labeled tokens in the total amount of labeled to-
kens, errors and accuracy (ACC) of the created corpus.
The majority of the automatically labeled to-
kens (97-98 %) belong to the OUTSIDE class.
This coincides with the assumption that SeSAL
works especially well for labeling tasks where
some classes occur predominantly and can, in
most cases, easily be discriminated from the other
classes, as is the case in the NER scenario. An
analysis of the errors induced by the self-tagging
component reveals that most of the errors (90-
100 %) are due to missed entity classes, i.e., while
the correct class label for a token is one of the
entity classes, the OUTSIDE class was assigned.
This effect is more severe in early than in later AL
iterations (see Table 4 for the exact numbers).
labeled error types (%)
corpus tokens errors E2O O2E E2E
MUC7 10,000 75 100 ? ?
70,000 259 96 1.3 2.7
Table 4: Distribution of errors of the self-tagging component.
Error types: OUTSIDE class assigned though an entity class
is correct (E2O), entity class assigned but OUTSIDE is cor-
rect (O2E), wrong entity class assigned (E2E).
Impact of the Confidence Threshold. We also
ran SeSAL with different confidence thresholds t
(0.99, 0.95, 0.90, and 0.70) and analyzed the re-
sults with respect to tagging errors and the model
performance. Figure 3 shows the learning and er-
ror curves for different thresholds on the MUC7
corpus. The supervised F-score of 87.7% is only
reached by the highest and most restrictive thresh-
old of t = 0.99. With all other thresholds, SeSAL
0 2000 6000 10000
0.
60
0.
70
0.
80
0.
90
learning curves
manually labeled tokens
F?
sc
or
e
t=0.99
t=0.95
t=0.90
t=0.70
0 20000 40000 60000
0
50
0
10
00
20
00
error curves
all labeled tokens
e
rr
o
rs
t=0.99
t=0.95
t=0.90
t=0.70
Figure 3: Learning and error curves for SeSAL with different
thresholds on the MUC7 corpus
stops at much lower F-scores and produces labeled
training data of lower accuracy. Table 5 contains
the exact numbers and reveals that the poor model
performance of SeSAL with lower thresholds is
mainly due to dropping recall values.
threshold F R P Acc
0.99 87.7 85.9 89.9 99.6
0.95 85.4 82.3 88.7 98.8
0.90 84.3 80.6 88.3 98.1
0.70 69.9 61.8 81.1 96.5
Table 5: Maximum model performance on MUC7 in terms of
F-score (F), recall (R), precision (P) and accuracy (Acc) ? the
labeled corpus obtained by SeSAL with different thresholds
Impact of the Delay Rate. We also measured
the impact of delay rates on SeSAL?s efficiency
considering three delay rates (1,000, 5,000, and
10,000 tokens) in combination with three confi-
dence thresholds (0.99, 0.9, and 0.7). Figure 4 de-
picts the respective learning curves on the MUC7
corpus. For SeSAL with t = 0.99, the delay
1045
0 5000 10000 15000 20000
0.
60
0.
70
0.
80
0.
90
threshold 0.99
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.877
0 5000 10000 15000 20000
0.
60
0.
70
0.
80
0.
90
threshold 0.9
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.843
F=0.877
0 2000 6000 10000
0.
60
0.
70
0.
80
0.
90
threshold 0.7
manually labeled tokens
F?
sc
or
e
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=69.9
F=0.877
Figure 4: SeSAL with different delay rates and thresholds on MUC7. Horizontal lines mark the supervised F-score (upper line)
and the maximal F-score achieved by SeSAL with the respective threshold and d = 0 (lower line).
has no particularly beneficial effect. However,
in combination with lower thresholds, the delay
rates show positive effects as SeSAL yields F-
scores closer to the maximal F-score of 87.7%,
thus clearly outperforming undelayed SeSAL.
6 Summary and Discussion
Our experiments in the context of the NER
scenario render evidence to the hypothesis that
the proposed approach to semi-supervised AL
(SeSAL) for sequence labeling indeed strongly re-
duces the amount of tokens to be manually anno-
tated ? in terms of numbers, about 60% compared
to its fully supervised counterpart (FuSAL), and
over 80% compared to a totally passive learning
scheme based on random selection.
For SeSAL to work well, a high and, by this,
restrictive threshold has been shown to be crucial.
Otherwise, large amounts of tagging errors lead to
a poorer overall model performance. In our ex-
periments, tagging errors in such a scenario were
OUTSIDE labelings, while an entity class would
have been correct ? with the effect that the result-
ing models showed low recall rates.
The delay rate is important when SeSAL is run
with a low threshold as early tagging errors can
be avoided which otherwise reinforce themselves.
Finding the right balance between the delay factor
and low thresholds requires experimental calibra-
tion. For the most restrictive threshold (t = 0.99)
though such a delay is unimportant so that it can
be set to d = 0 circumventing this calibration step.
In summary, the self-tagging component of
SeSAL gets more influential when the confidence
threshold and the delay factor are set to lower val-
ues. At the same time though, under these con-
ditions negative side-effects such as deteriorated
data quality and, by this, inferior models emerge.
These problems are major drawbacks of many
bootstrapping approaches. However, our experi-
ments indicate that as long as self-training is cau-
tiously applied (as is done for SeSAL with restric-
tive parameters), it can definitely outperform an
entirely supervised approach.
From an annotation point of view, SeSAL effi-
ciently guides the annotator to regions within the
selected sentence which are very useful for the
learning task. In our experiments on the NER sce-
nario, those regions were mentions of entity names
or linguistic units which had a surface appearance
similar to entity mentions but could not yet be cor-
rectly distinguished by the model.
While we evaluated SeSAL here in terms of
tokens to be manually labeled, an open issue re-
mains, namely how much of the real annotation
effort ? measured by the time needed ? is saved
by this approach. We here hypothesize that hu-
man annotators work much more efficiently when
pointed to the regions of immediate interest in-
stead of making them skim in a self-paced way
through larger passages of (probably) semantically
irrelevant but syntactically complex utterances ?
a tiring and error-prone task. Future research is
needed to empirically investigate into this area and
quantify the savings in terms of the time achiev-
able with SeSAL in the NER scenario.
Acknowledgements
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Roman Klin-
ger (Fraunhofer SCAI) for fruitful discussions.
1046
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT?98 ?
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92?100.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL?96 ? Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics, pages 319?326.
T. Kristjansson, A. Culotta, and P. Viola. 2004. Inter-
active information extraction with constrained Con-
ditional Random Fields. In AAAI?04 ? Proceed-
ings of 19th National Conference on Artificial Intel-
ligence, pages 412?418.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. T. Mc-
Donald, M. S. Palmer, and A. I. Schein. 2004. Inte-
grated annotation for biomedical information extrac-
tion. In Proceedings of the HLT-NAACL 2004 Work-
shop ?Linking Biological Literature, Ontologies and
Databases: Tools for Users?, pages 61?68.
J. D. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional Random Fields: Probabilistic models
for segmenting and labeling sequence data. In
ICML?01 ? Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
D. D. Lewis and J. Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In
ICML?94 ? Proceedings of the 11th International
Conference on Machine Learning, pages 148?156.
Linguistic Data Consortium. 2001. Message Under-
standing Conference (MUC) 7. LDC2001T02. FTP
FILE. Philadelphia: Linguistic Data Consortium.
A. McCallum and K. Nigam. 1998. Employing EM
and pool-based Active Learning for text classifica-
tion. In ICML?98 ? Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 350?
358.
I. A. Muslea, S. Minton, and C. A. Knoblock. 2002.
Active semi-supervised learning = Robust multi-
view learning. In ICML?02 ? Proceedings of the
19th International Conference on Machine Learn-
ing, pages 435?442.
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: Cost-efficient resource usage for base noun
phrase chunking. In ACL?00 ? Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117?125.
D. Pierce and C. Cardie. 2001. Limitations of co-
training for natural language learning from large
datasets. In EMNLP?01 ? Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?9.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
B. Settles and M. Craven. 2008. An analysis of Active
Learning strategies for sequence labeling tasks. In
EMNLP?08 ? Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1069?1078.
B. Settles, M. Craven, and L. Friedland. 2008. Active
Learning with real annotation costs. In Proceedings
of the NIPS 2008 Workshop on ?Cost-Sensitive Ma-
chine Learning?, pages 1?10.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT?92 ? Proceedings of
the 5th Annual Workshop on Computational Learn-
ing Theory, pages 287?294.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains corpus reusability of an-
notated data. In EMNLP-CoNLL?07 ? Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 486?495.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL?95 ?
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
196.
1047
BioNLP 2007: Biological, translational, and clinical language processing, pages 193?194,
Prague, June 2007. c?2007 Association for Computational Linguistics
Quantitative Data on Referring Expressions in Biomedical Abstracts
Michael Poprat Udo Hahn
Jena University Language and Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{poprat|hahn}@coling-uni-jena.de
Abstract
We report on an empirical study that deals
with the quantity of different kinds of refer-
ring expressions in biomedical abstracts.
1 Problem Statement
One of the major challenges in NLP is the resolu-
tion of referring expressions. Those references can
be established by repeating tokens or by pronom-
inal, nominal and bridging anaphora. Experimen-
tal results show that pronominal anaphora are eas-
ier to resolve than nominal ones because the resolu-
tion of nominal anaphora requires an IS-A-taxonomy
as knowledge source. The resolution of bridging
anaphora, however, proves to be awkward because
encyclopedic knowledge is necessary.1 But in prac-
tice, are all of these phenomena equally important?
A look at the publications reveals that a compre-
hensive overview of the quantity and distribution
of referring expressions in biomedical abstracts is
still missing. Nevertheless, some scattered data can
be found: Castan?o et al (2002) state that 60 of
100 anaphora are nominal anaphora. Sanchez et al
(2006) confirm this proportion (24 pronominal and
50 nominal anaphora in 74 anaphoric expressions).
Kim and Park (2004), however, detect 53 pronomi-
nal and 26 nominal anaphora in 87 anaphoric expres-
sions. But Gawronska and Erlendsson (2005), on the
other hand, claim that pronominal anaphora are rare
and nominal anaphora are predominant. Studies on
bridging anaphora in the biomedical domain are re-
1However, even the resolution of pronouns can benefit from
extra-textual information (Castan?o et al, 2002).
ally still missing. Only Cimiano (2003) states that
10% of definite descriptions are bridging anaphora.
This contradictoriness and the lack of statistics on
referring expressions induced us to collect our own
data in order to obtain a consistent and meaningful
overview. This picture helps to decide where to start
if one wants to build a resolution component for the
biomedical domain.
2 Empirical Study
For our study we selected articles from MEDLINE
for stem cell transplantation and gene regulation.
Out of these articles, 11 stem cell abstracts and 9
gene regulation abstracts (? 12,000 tokens) were an-
notated by a team of one biologist and one computa-
tional linguist. The boundaries for annotations were
neither limited to nominal phrases (NPs) nor on their
heads because NPs in biomedical abstracts are of-
ten complex and hide relations between nouns (e.g.,
a ?p53 protein? is a protein called ?p53?, a ?p53
gene? is a gene that codes the ?p53 protein? and a
?p53 mutation? is a mutation in the ?p53 gene?).
Furthermore, we annotated anaphoric expressions
referring to biomedical entities and to processes.
We distinguished the following referring ex-
pressions: As repetitions, we counted string-
identical, string-variants and abbreviated token se-
quences in NPs, identical in their meaning (e.g.
?Mesenchymal stem cells? - ?MSCs? - ?MSC in-
hibitory effect?). For the time being, modifiers have
not been considered. Anaphora comprise pronom-
inal2, nominal (IS-A relations, e.g., ?B-PLL? IS-
2Without ?we? as it always refers to the authors.
193
Type of Referring Expression Number
Repetitions 388
Pronominal Anaphora 48 (sent. internal)6 (sent. external)
Nominal Anaphora 79
Bridging Anaphora 42
Subgrouping Anaphora 91
all 654
Table 1: Number of Referring Expressions
A ?B-cell malignancy?) and bridging anaphora (all
other semantic relations, e.g., ?G(1) progression?
PART-OF-PROCESS ?M-G(1) transition?). Further-
more, we detected a high number of subgrouping
anaphora that often occur when a group of enti-
ties (e.g., ?Vascular endothelial growth factor re-
ceptors?) are mentioned first and certain subgroups
(e.g., ?VEGFR1? etc.) are discussed later.
In our abstracts we detected 654 referring expres-
sions (see Table 1). Repetitions are predominant
with 59%. Within the group of 266 anaphora, sub-
grouping anaphora contributed with 34%, nominal
anaphora with 30%, pronominal anaphora with 20%
and bridging anaphora with only 16%. The most
common bridging relations were PART-OF-AMOUNT
(14) and PART-OF (11). The remaining 17 are held
by 8 other semantic relations such as RESULTS-
FROM, MUTATED-FROM, etc.
3 Open Issues and Conclusion
In biomedical abstracts we are confronted with nu-
merous repetitions, mainly containing biomedical
entities. Their reference resolution within an ab-
stract seems to be easy at first glance by just com-
paring strings and detecting acronyms. Some exam-
ples will show that this is tricky, though: In ?The
VEGFR3-transfected ECs exhibited high expression
level of LYVE-1.?, this statement on ECs only holds
if the modifier ?VEGFR3-transfected? is taken into
account. Furthermore, transfected ECs are not iden-
tical with non-transfected ECs which would be the
result if considering NP heads only. But not ev-
ery modifier influences an identity relation. For ex-
ample, the purification in ?. . . when priming with
purified CD34(+) cells? has no influence on the
CD34(+) cells and statements about these cells keep
their generality. A classification of such modifiers
adding information with or without influencing the
semantics of the modified expression must be made.
Hence, we have to be careful with assumed repeti-
tions and we have to handle all kinds of modifiers.
In this study we present the first comprehensive
overview of various kinds of referring expressions
that occur in biomedical abstracts. Although our
corpus is still small, we could observe the strong
tendency that repetitions play a major role (20 per
abstract). Anaphora occur less frequently (13 per
abstract). For a sound semantic interpretation, both
types must be handled. For knowledge-intensive
anaphora resolution, the existing biomedical re-
sources must be reviewed for adequacy. To the best
of our knowledge, although dominant in our study,
subgrouping anaphora have not been considered in
any anaphora resolution systems and suitable reso-
lution strategies must be found. The annotation pro-
cess (with more than one annotation team) will be
continued. The main result of this study, however,
is the observation that modifiers play an important
role for referencing. Their treatment for semantic
interpretation requires further investigations.
Acknowledgements: We thank Belinda Wu?rfel for
her annotation work. This study was funded by
the EC (BOOTStrep, FP6-028099), and by the Ger-
man Ministry of Education and Research (StemNet,
01DS001A - 1C).
References
J. Castan?o, J. Zhang, and J. Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In Proc. of the
Symp. on Reference Resolution for NLP.
P. Cimiano. 2003. On the research of bridging refer-
ences within information extraction systems. Diploma
thesis, University of Karlsruhe.
B. Gawronska and B. Erlendsson. 2005. Syntactic,
semantic and referential patterns in biomedical texts:
Towards in-depth comprehension for the purpose of
bioinformatics. In Proc. of the 2nd Workshop on Nat-
ural language Understanding and Cognitive science,
pages 68?77.
J.-J. Kim and J. C. Park. 2004. BioAR: Anaphora reso-
lution for relating protein names to proteome database
entries. In Proc. of the ACL 2004: Workshop on Ref-
erence Resolution and its Applications, pages 79?86.
O. Sanchez, M. Poesio, M. A. Kabadjov, and R. Tesar.
2006. What kind of problems do protein interactions
raise for anaphora resolution? A preliminary analysis.
In Proc. of the 2nd SMBM 2006, pages 109?112.
194
Proceedings of the Linguistic Annotation Workshop, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Efficient Annotation with the Jena ANnotation Environment (JANE)
Katrin Tomanek Joachim Wermter Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30
D-07743 Jena, Germany
{tomanek|wermter|hahn}@coling-uni-jena.de
Abstract
With ever-increasing demands on the diver-
sity of annotations of language data, the
need arises to reduce the amount of efforts
involved in generating such value-added lan-
guage resources. We introduce here the Jena
ANnotation Environment (JANE), a platform
that supports the complete annotation life-
cycle and allows for ?focused? annotation
based on active learning. The focus we pro-
vide yields significant savings in annotation
efforts by presenting only informative items
to the annotator. We report on our experi-
ence with this approach through simulated
and real-world annotations in the domain of
immunogenetics for NE annotations.
1 Introduction
The remarkable success of machine-learning meth-
ods for NLP has created, for supervised approaches
at least, a profound need for annotated language cor-
pora. Annotation of language resources, however,
has become a bottleneck since it is performed, with
some automatic support (pre-annotation) though, by
humans. Hence, annotation is a time-costly and
error-prone process.
The demands for annotated language data is in-
creasing at different levels. After the success in syn-
tactic (Penn TreeBank (Marcus et al, 1993)) and
propositional encodings (Penn PropBank (Palmer et
al., 2005)), more sophisticated semantic data (such
as temporal (Pustejovsky et al, 2003) or opinion an-
notations (Wiebe et al, 2005)) and discourse data
(e.g., for anaphora resolution (van Deemter and Kib-
ble, 2000) and rhetorical parsing (Carlson et al,
2003)) are being generated. Once the ubiquitous
area of newswire articles is left behind, different do-
mains (e.g., the life sciences (Ohta et al, 2002)) are
yet another major concern. Furthermore, any new
HLT application (e.g., information extraction, doc-
ument summarization) makes it necessary to pro-
vide appropriate human annotation products. Be-
sides these considerations, the whole field of non-
English languages is desperately seeking to enter
into enormous annotation efforts, at virtually all en-
coding levels, to keep track of methodological re-
quirements imposed by such resource-intensive re-
search activities.
Given this enormous need for high-quality anno-
tations at virtually all levels the question turns up
how to minimize efforts within an acceptable qual-
ity window. Currently, for most tasks several hun-
dreds of thousands of text tokens (ranging between
200,000 to 500,000 text tokens) have to be scruti-
nized unless valid tagging judgments can be learned.
While significant time savings have already been re-
ported on the basis of automatic pre-tagging (e.g.,
for POS and parse tree taggings in the Penn Tree-
Bank (Marcus et al, 1993), or named entity taggings
for the Genia corpus (Ohta et al, 2002)), this kind of
pre-processing does not reduce the number of text
tokens actually to be considered.
We have developed the Jena ANnotation Environ-
ment (JANE) that allows to reduce annotation ef-
forts by means of the active learning (AL) approach.
Unlike random or sequential sampling of linguistic
items to be annotated, AL is an intelligent selective
9
sampling strategy that helps reduce the amount of
data to be annotated substantially at almost no loss
in annotation effectiveness. This is achieved by fo-
cusing on those items particularly relevant for the
learning process.
In Section 2, we review approaches to annota-
tion cost reduction. We turn in Section 3 to the de-
scription of JANE, our AL-based annotation system,
while in Section 4 we report on the experience we
made using the AL component in NE annotations.
2 Related Work
Reduction of efforts for training (semi-) supervised
learners on annotated language data has always been
an issue of concern. Semi-supervised learning pro-
vides methods to bootstrap annotated corpora from a
small number of manually labeled examples. How-
ever, it has been shown (Pierce and Cardie, 2001)
that semi-supervised learning is brittle for NLP tasks
where typically large amounts of high quality anno-
tations are needed to train appropriate classifiers.
Another approach to reducing the human labeling
effort is active learning (AL) where the learner has
direct influence on the examples to be manually la-
beled. In such a setting, those examples are taken
for annotation which are assumed to be maximally
useful for (classifier) training. AL approaches have
already been tried for different NLP tasks (Engelson
and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky,
2000), though such studies usually report on simula-
tions rather than on concrete experience with AL for
real annotation efforts. In their study on AL for base
noun phrase chunking, Ngai and Yarowsky (2000)
compare the costs of rule-writing with (AL-driven)
annotation to compile a base noun phrase chunker.
They conclude that one should rather invest human
labor in annotation than in rule writing.
Closer to our concerns is the study by Hachey et
al. (2005) who apply AL to named entity (NE) an-
notation. There are some differences in the actual
AL approach they chose, while their main idea, viz.
to apply committee-based AL to speed up real anno-
tations, is comparable to our work. They report on
negative side effects of AL on the annotations and
state that AL annotations are cognitively more diffi-
cult for the annotators to deal with (because the sen-
tences selected for annotation are more complex).
As a consequence, diminished annotation quality
and higher per-sentence annotation times arise in
their experiments. By and large, however, they con-
clude that AL selection should still be favored over
random selection because the negative implications
of AL are easily over-compensated by the signifi-
cant reduction of sentences to be annotated to yield
comparable classifier performance as under random
sampling conditions.
Whereas Hatchey et al focus only on one group
of entity mentions (viz. four entity subclasses of the
astrophysics domain), we report on broader experi-
ence when applying AL to annotate several groups
of entity mentions in biomedical subdomains. We
also address practical aspects as to how create the
seed set for the first AL round and how one might
estimate the efficiency of AL. The immense sav-
ings in annotation effort we achieve here (up to
75%) may mainly depend on the sparseness of many
entity types in biomedical corpora. Furthermore,
we here present a general annotation environment
which supports AL-driven annotations for most seg-
mentation problems, not just for NE recognition.
In contrast, annotation editors, such as e.g. Word-
Freak1, typically offer facilities for supervised cor-
rection of automatically annotated text. This, how-
ever, is very different from the AL approach.
3 JANE ? Jena ANnotation Environment
JANE, the Jena ANnotation Environment, supports
the whole annotation life-cycle including the com-
pilation of annotation projects, annotation itself (via
an external editor), monitoring, and the deploy-
ment of annotated material. In JANE, an annota-
tion project consists of a collection of documents
to be annotated, an associated annotation schema
? a specification of what has to be annotated in
which way, according to the accompanying annota-
tion guidelines ? a set of configuration parameters,
and an annotator assigned to it.
We distinguish two kinds of annotation projects:
A default project, on the one hand, contains a prede-
fined and fixed collection of naturally occurring doc-
uments which the annotator handles independently
of each other. In an active learning project, on the
other hand, the annotator has access to exactly one
1http://wordfreak.sourceforge.net
10
(AL-computed pseudo) document at a time. After
such a document has completely been annotated, a
new one is dynamically constructed which contains
those sentences for annotation which are the most
informative ones for training a classifier. Besides
annotators who actually do the annotation, there
are administrators who are in charge of (annota-
tion) project management, monitoring the annota-
tion progress, and deployment, i.e., exporting the
data to other formats.
JANE consists of one central component, the an-
notation repository, where all annotation and project
data is stored centrally, two user interfaces, namely
one for the annotators and one for the administra-
tor, and the active learning component which inter-
actively generates documents to speed up the anno-
tation process. All components communicate with
the annotation repository through a network socket
? allowing JANE to be run in a distributed envi-
ronment. JANE is largely platform-independent be-
cause all components are implemented in Java. A
test version of JANE may be obtained from http:
//www.julielab.de.
3.1 Active Learning Component
One of the most established approaches to active
learning is based on the idea to build an ensemble
of classifiers from the already annotated examples.
Each classifier then makes its prediction on all unla-
beled exampels. Examples on which the classifiers
in the ensemble disagree most in their predictions
are considered informative and are thus requested
for labeling. Obviously, we can expect that adding
these examples to the training corpus will increase
the accuracy of a classifier trained on this data (Se-
ung et al, 1992). A common metric to estimate
the disagreement within an ensemble is the so-called
vote entropy, the entropy of the distribution of labels
li assigned to an example e by the ensemble of k
classifiers (Engelson and Dagan, 1996):
D(e) = ? 1log k
?
li
V (li, e)
k log
V (li, e)
k
Our AL component employs such an ensemble-
based approach (Tomanek et al, 2007). The ensem-
ble consists of k = 3 classifiers2. AL is run on the
2Currently, we incorporate as classifiers Naive Bayes, Max-
imum Entropy, and Conditional Random Fields.
sentence level because this is a natural unit for many
segmentation tasks. In each round, b sentences with
the highest disagreement are selected.3 The pool of
(available) unlabeled examples can be very large for
many NLP tasks; for NE annotations in the biomedi-
cal domain we typically download several hundreds
of thousands of abstracts from PUBMED.4 In or-
der to avoid high selection times, we consider only
a (random) subsample of the pool of unlabeled ex-
amples in each AL round. Both the selection size b
(which we normally set to b = 30), the composition
of the ensemble, and the subsampling ratio can be
configured with the administration component.
AL selects single, non-contiguous sentences from
different documents. Since the context of these sen-
tences is still crucial for many (semantic) annota-
tion decisions, for each selected sentence its origi-
nal context is added (but blocked from annotation).
When AL selection is finished, a new document is
compiled from these sentences (including their con-
texts) and uploaded to the annotation repository. The
annotator can then proceed with annotation.
Although optimized for NE annotations, the AL
component may ? after minor modifications of the
feature sets being used by the classifiers ? also be ap-
plied to other segmentation problems, such as POS
or chunk annotations.
3.2 Administration Component
Administering large-scale annotation projects is a
challenging management task for which we supply
a GUI (Figure 1) to support the following tasks:
User Management Create accounts for adminis-
trators and annotators.
Creation of Projects The creation of an annota-
tion project requires a considerable number of doc-
uments and other files (such as annotation schema
definitions) to be uploaded to the annotation reposi-
tory. Furthermore, several parameters, especially for
AL projects have to be set appropriately.
Editing a Project The administrator can reset a
project (especially when guidelines change, one
3Here, the vote entropy is calculated separately for each to-
ken. The sentence-level vote entropy is then the average over
the respective token sequence.
4http://www.ncbi.nlm.nih.gov/
11
Figure 1: Administration GUI: frame in foreground shows actions that can be performed on an AL project.
might want to start the annotation process anew,
i.e., delete all previous annotations but keep the rest
of the project unchanged), delete a project, copy a
project (which is helpful when several annotators la-
bel the same documents to check the applicability of
the guidelines by inter-annotator agreement calcula-
tion), and change several AL-specific settings.
Monitoring the Annotation Process The admin-
istrator can check which documents of an annotation
project have already been annotated, how long anno-
tation took on the average, when an annotator logged
in last time, etc. Furthermore, the progress of AL
projects can be visualized by learning and disagree-
ment curves and an enumeration of the number of
(unique) entities found so far.
Inter-Annotator Agreement For related projects
(projects sharing the same annotation schema and
documents to be annotated) the degree to which
several annotators mutually agree in their annota-
tions can be calculated. Such an inter-annotator
agreement (IAA) is common to estimate the quality
and applicability of particular annotation guidelines
(Kim and Tsujii, 2006). Currently, several IAA met-
rics of different strictness for NE annotations (and
other segmentation tasks) are incorporated.
Deployment The annotation repository stores the
annotations in a specific XML format (see Sec-
tion 3.3). For deployment, the annotations may be
needed in a different format. Currently, the admin-
istration GUI basically supports export into the IOB
format. Only documents marked by the annotators
as ?completely annotated? are considered.
3.3 Annotation Component
As the annotators are rather domain experts (in our
case graduate students of biology or related life sci-
ences) than computer specialists, we wanted to make
life for them as easy as possible. Hence, we pro-
vide a separate GUI for the annotators. After log-in
the annotator is given an overview of his/her annota-
tion projects along with a short description. Double
clicking on a project, the annotators get a list with
all documents in this project. Documents have dif-
ferent flags (raw, in progress, done) to indicate the
current annotation state as set by each annotator.
Annotation itself is done with MMAX, an external
annotation editor (Mu?ller and Strube, 2003), which
can be customized with respect to the particular an-
notation schema. The document to be annotated, the
annotations, and the configuration parameters are
stored in separate XML files. Our annotation repos-
itory reflects this MMAX-specific data structure.
Double clicking on a specific document directly
opens MMAX for annotation. During annotation,
the annotation GUI is locked to ensure data in-
12
tegrity. When working on an AL project, the anno-
tator can start the AL selection process (which then
runs on a separate high-performance machine) after
having finished the annotation of the current docu-
ment. During the AL selection process (it usually
takes up to several minutes) the current project is
blocked. However, meanwhile the annotator can go
on annotating other projects.
3.4 Annotation Repository
The annotation repository is the heart of our annota-
tion environment. All project, user, and annotation
relevant data is stored here centrally. This is a cru-
cial design criterion because it lets the administrator
access (e.g., for backup or deployment) all annota-
tions from one central site. Furthermore, the anno-
tators do not have to care about how to shift the an-
notated documents to the managerial staff. All state
information related to the entire annotation cycle is
recorded and kept centrally in this repository.
The repository is realized as a relational database5
reflecting largely the data structure of MMAX. Both,
the GUIs and the AL component, communicate with
the repository via the JDBC network driver. Thus,
each component can be run on a different machine
as long as it has a network connection to the annota-
tion repository. This has two main advantages: First,
annotators can work remotely (e.g., from home or
from a physically dislocated lab). Second, resource-
intensive tasks, e.g., AL selection, can be run on sep-
arate machines to which the annotators normally do
not have access. The components communicate with
each other only through the annotation repository. In
particular, there is no direct communication between
the annotation GUI and the AL component.
4 Experience with Real-World Annotations
We are currently conducting NE annotations for
two large-scale information extraction and seman-
tic retrieval projects. Both tasks cover two non-
overlapping biomedical subdomains, viz. one in the
field of hematopoietic stem cell transplantation (im-
munogenetics), the other in the area of gene regu-
lation. Entity types of interest are, e.g., cytokines
and their receptors, antigens, antibodies, immune
5We chose MYSQL, a fast and reliable open source database
with native Java driver support
cells, variation events, chemicals, blood diseases,
etc. In this section, we report on our actual ex-
perience and findings in annotating entity mentions
(drawing mainly on our work in the immunogenetics
subdomain) with JANE, with a focus on methodolog-
ical issues related to active learning.
In the biomedical domain, there is a vast amount
of unlabeled material available for almost any topic
of interest. The most prominent source is probably
PUBMED, a literature database which currently in-
cludes over 16 million citations, mostly abstracts,
from MEDLINE and other life science sources. We
used MESH terms6 and publication date ranges7 to
select relevant documents from the immunogenet-
ics subdomain. Thus, we retrieved about 200,000
abstracts (? 2,000,000 sentences) as our document
pool of unlabeled examples for immunogenetics.
Through random subsampling, only about 40,000
sentences are considered for AL selection.
For several of our entity annotations, we did both
an active learning (AL) annotation and a gold stan-
dard (GS) annotation. The latter is performed in
the default project mode on 250 abstracts randomly
chosen from the entire document pool. We asked
different annotators to annotate the same (subset of
the) GS to calculate inter-annotator agreement in or-
der to make sure that our annotation guidelines were
non-ambiguous. Furthermore, as the annotation pro-
ceeds, we regularly train a classifier on the AL an-
notations and evaluate it against the GS annotations.
From this learning curve, we can estimate the poten-
tial gain of further AL annotation rounds and decide
when to stop AL annotation.
4.1 Reduction of Annotation Effort through AL
In real-world AL annotation projects, the amount of
cost reduction is hard to estimate properly. We have
thus extensively simulated and tested the gain in the
reduction of annotation costs of our AL component
on available entity annotations of the biomedical do-
main (GENIA8 and PENNBIOIE9) and the general-
6MESH (http://www.nlm.nih.gov/mesh/) is the
U.S. National Library of Medicine?s controlled vocabulary used
for indexing PUBMED articles.
7Typically, articles published before 1990 are not considered
to contain relevant information for molecular biology.
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://bioie.ldc.upenn.edu/
13
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1000  2000  3000  4000  5000  6000  7000  8000
F-
sc
or
e
sentences
reduction of annotation costs (75%)
AL selection
random selection
Figure 2: Learning curves for AL and random selec-
tion on variation event entity mentions.
language newspaper domain (English data set of the
CoNLL-2003 shared task (Tjong Kim Sang and De
Meulder, 2003)). As a metric for annotation costs
we here consider the number of sentences to be an-
notated such that a certain F-score is reached with
our NE tagger.10 We therefore compare the learning
curves of AL and random selection. On almost ev-
ery scenario, we found that AL yields cost savings
of about 50%, sometimes even up to 75%.
As an example, we report on our AL simula-
tion on the PENNBIOIE corpus for variation events.
These entity mentions include the following six sub-
classes: type, event, original state, altered state,
generic state, and location. The learning curves
for AL and random selection are shown in Figure
2. Using random sampling, an F-score of 80% is
reached by random selection after ? 8,000 sentences
(200,000 tokens). In contrast, AL selection yields
the same F-score after ? 2,000 sentences (46,000
tokens). This amounts to a reduction of annotation
costs on the order of 75%.
Our real-world annotations revealed that AL is
especially beneficial when entity mentions are very
sparsely distributed in the texts. After an initializa-
tion phase needed by AL to take off (which can con-
siderably be accelerated when one carefully selects
the sentences of the first AL round, see Section 4.2),
AL selects, by and large, only sentences which con-
tain at least one entity mention of the type of inter-
10The named enatity tagger used throughout in this section
is based on Conditional Random Fields and similar to the one
presented by (Settles, 2004).
 0
 500
 1000
 1500
 2000
 2500
 3000
 200  400  600  800  1000  1200  1400  1600  1800  2000
e
n
tit
y 
m
en
tio
ns
sentences
AL annotation
GS annotation
Figure 3: Cumulated entity density on AL and GS
annotations of cytokine receptors.
est. In contrast, random selection (or in real anno-
tation projects: sequential annotations of abstracts
as in our default project mode), may lead to lots of
negative training examples with no entity mentions
of interest. When there is no simulation data at hand,
the entity density of AL annotations (compared with
the respective GS annotation) is a good estimate of
the effectiveness of AL.
Figure 3 depicts such a cumulated entity density
plot on AL and GS annotations of subtypes of cy-
tokine receptors, really very sparse entity types with
one entity mention per PUBMED abstract on the av-
erage. The 250 abstracts of the GS annotation only
contain 193 cytokine receptor entity mentions. AL
annotation of the same number of sentences resulted
in 2,800 annotated entity mentions of this type. The
entity density in our AL corpus is thus almost 15
times higher than in our GS corpus. Such a dense
corpus is certainly much more appropriate for clas-
sifier training due to the tremendous increase of pos-
itive training instances. We observed comparable ef-
fects with other entity types as well, and thus con-
clude that the sparser entity mentions of a specific
type are in texts, the more benefical AL-based anno-
tation actually is.
4.2 Mind the Seed Set
For AL, the sentences to be annotated in the first AL
round, the seed set, have to be manually selected. As
stated above, the proper choice of this set is crucial
for efficient AL based annotation. One should def-
initely refrain from a randomly generated seed set
14
? especially, when sparse entity mentions are anno-
tated ? because it might take quite a while for AL to
take off. If, in the worst case, the seed set contains
no entity mentions of interest, AL based annotation
resembles (for several rounds in the beginning until
incidentally some entity mentions are found) a ran-
dom selection ? which is, as shown in Section 4.1,
suboptimal. Figure 4 shows the simulated effect of
three different seed sets on variation event annota-
tion (PENNBIOIE). In the tuned seed set, each sen-
tence contains at least one variation entity mention.
On this seed, AL performs significantly better than
the randomly assembled seed or the seed with no en-
tity mentions at all. Of course, in the long run, the
three curves converge. Given this evidence, we stip-
ulate that the sparser an entity type is11 or the larger
the document pool to be selected from is, the later
the point of convergence and, thus, the more rele-
vant an effective seed set is.
We developed a useful three-step heuristic to
compile effective seed sets without excessive man-
ual work. In the first step, a list is compiled
comprised of as many entity mentions (of inter-
est to the current annotation project) as possible.
In knowledge- and expert-intensive domains such
as molecular biology, this can either be done by
consulting a domain expert or by harvesting entity
mentions from online resources (such as biological
databases).12 In a second step, the compiled list
is matched against each sentence of the document
pool. Third, a ranking procedure orders the sen-
tences (in descending order) according to the num-
ber of diverse matches of entity mentions. This en-
sures that textual mentions of all items from the list
are included in the seed set. Depending on the vari-
ety and density of the specific entity types, our seed
sets typically consist of 200 to 500 sentences.
4.3 Portability of Corpora
While we are working in the field of immunogenet-
ics, the PENNBIOIE corpus focuses on the subdo-
main of oncogenetics and provides a sound annota-
11Variation events are not as sparse in PENNBIOIE as, e.g.,
cytokine receptors in our subdomain. Actually, there is a varia-
tion entity in almost every second sentence.
12In an additional step, some spelling variations of such en-
tity mentions could automatically be generated.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  100  200  300  400  500
F-
sc
or
e
sentences
random seed set
tuned seed set
seed set with no entities
Figure 4: Effect of different seed sets for AL on vari-
ation event annotation.
tion of these entity mentions (PBVAR).13 We did a
GS annotation on 250 randomly chosen abstracts (?
2,000 sentences/65,000 tokens) from our document
pool applying PENNBIOIE?s annotation guidelines
for variation events to the subdomain of immuno-
genetics (IMVAR-Gold). We then evaluated how
well our entity tagger trained on PBVAR would do
on this data. Surprisingly, the performance was dra-
matically low, viz. 31.2% F-score.14
Thus, we did further variation event annotations
for the immunogenetics domain with AL: We anno-
tated ? 58,000 tokens (IMVAR-AL). We trained our
entity tagger on this data and evaluated the tagger on
both IMVAR-Gold and PBVAR. Table 1 summarizes
the results. We conclude that porting training cor-
pora, even from one related subdomain into another,
is only possible to a very limited extent. This may be
because current NE taggers (ours, as well) make ex-
tensive use of lexical features. However, the results
also reveal that annotations made by AL may be
more robust when ported to another domain: a tag-
ger trained on IMVAR-AL still yields about 62.5%
F-score on PBVAR, whereas training the tagger on
the respective GS annotation (IMVAR-Gold), only
about half the performance is yielded (35.8%).
13Although oncogenetics and immunogenetics are different
subdomains, they share topical overlaps ? in particular, with
respect to the types of relevant variation entity mentions (such
as ?single nucleotide polymorphism?, ?translocation?, ?in-frame
deletion?, ?substitution?, etc.). Hence, at least at this level the
two subdomains are related.
14Note that in a 10-fold cross-validation on PBVAR our entity
tagger yielded about 80% F-score.
15
evaluation data
training data PBVAR IMVAR-Gold
PBVAR
(? 200.000 tokens) ? 80% 31.2%
IMVAR-AL
(58.251 tokens) 62.5% 70.2%
IMVAR-Gold
(63.591 tokens) 35.8% ?
Table 1: Corpus portability: PENNBIOIE?s variation
entity annotations (PBVAR) vs. ours for immuno-
genetics (IMVAR-AL and -Gold).
5 Conclusion and Future Work
We introduced JANE, an annotation environment
which supports the whole annotation life-cycle from
annotation project compilation to annotation deploy-
ment. As one of its major contributions, JANE al-
lows for focused annotation based on active learn-
ing, i.e., it automatically presents sentences for an-
notation which are of most use for classifier training.
We have shown that porting annotated training
corpora, even from one subdomain to another and
thus related to a good extent, may severely degrade
classifier performance. Thus, generating new an-
notation data will increasingly become important,
especially under the prospect that there are more
and more real-world information extraction projects
for different (sub)domains and languages. We have
shown that focused, i.e., AL-driven, annotation is a
reasonable choice to significantly reduce the effort
needed to create such annotations ? up to 75% in a
realistic setting. Furthermore, we have highlighted
the positive effects of a high-quality seed set for AL
and outlined a general heuristic for its compilation.
At the moment, the AL component may be used
for most kinds of segmentation problems (e.g. POS
tagging, text chunking, entity recognition). Future
work will focus on the extension of the AL compo-
nent for relation encoding as required for corefer-
ences or role and propositional information.
Acknowledgements
We thank Alexander Klaue for implementing the
GUIs. This research was funded by the EC within
the BOOTStrep project (FP6-028099), and by the
German Ministry of Education and Research within
the StemNet project (01DS001A to 1C).
References
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 2003.
Building a discourse-tagged corpus in the framework of
Rhetorical Structure Theory. In J. van Kuppevelt and R.
Smith, editors, Current Directions in Discourse and Dia-
logue, pp. 85?112. Kluwer.
Sean Engelson and Ido Dagan. 1996. Minimizing manual an-
notation cost in supervised training from corpora. In Proc.
of ACL 1996, pp. 319?326.
B. Hachey, B. Alex, and M. Becker. 2005. Investigating the
effects of selective sampling on the annotation task. In Proc.
of CoNLL-2005, pp. 144?151.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proc. of EMNLP/VLC-2000, pp. 45?52.
Jin-Dong Kim and Jun?ichi Tsujii. 2006. Corpora and their
annotation. In S. Ananiadou and J. McNaught, editors, Text
Mining for Biology and Biomedicine, pp. 179?211. Artech.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
C. Mu?ller and M. Strube. 2003. Multi-level annotation in
MMAX. In Proc. of the 4th SIGdial Workshop on Discourse
and Dialogue, pp. 198?207.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proc. of ACL 2000, pp. 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The GE-
NIA corpus: An annotated research abstract corpus in molec-
ular biology domain. In Proc. of HLT 2002, pp. 82?86.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proc. of EMNLP 2001, pp. 1?9.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew See,
Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth
Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003.
The TIMEBANK corpus. In Proc. of the Corpus Linguistics
2003 Conference, pp. 647?656.
Burr Settles. 2004. Biomedical named entity recognition using
conditional random fields and rich feature sets. In Proc. of
JNLPBA 2004, pp. 107?110.
H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky.
1992. Query by committee. In Proc. of COLT 1992, pp.
287?294.
Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction
to the CONLL-2003 shared task: Language-independent
named entity recognition. In Proc. of CoNLL 2003, pp. 142?
147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An
approach to downsizing annotation costs and maintaining
corpus reusability. In Proc of EMNLP-CoNLL 2007.
Kees van Deemter and Rodger Kibble. 2000. On coreferring:
Coreference in MUC and related annotation schemes. Com-
putational Linguistics, 26(4):629?637.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3):165?210.
16
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 31?39,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building a BIOWORDNET by Using WORDNET?s Data Formats
and WORDNET?s Software Infrastructure ? A Failure Story
Michael Poprat Elena Beisswanger
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
D-07743 Jena, Germany
{poprat,beisswanger,hahn}@coling-uni-jena.de
Udo Hahn
Abstract
In this paper, we describe our efforts to build
on WORDNET resources, using WORDNET
lexical data, the data format that it comes with
and WORDNET?s software infrastructure in
order to generate a biomedical extension of
WORDNET, the BIOWORDNET. We began
our efforts on the assumption that the soft-
ware resources were stable and reliable. In
the course of our work, it turned out that this
belief was far too optimistic. We discuss the
stumbling blocks that we encountered, point
out an error in the WORDNET software with
implications for research based on it, and con-
clude that building on the legacy of WORD-
NET data structures and its associated soft-
ware might preclude sustainable extensions
that go beyond the domain of general English.
1 Introduction
WORDNET (Fellbaum, 1998) is one of the most au-
thoritative lexical resources for the general English
language. Due to its coverage ? currently more than
150,000 lexical items ? and its lexicological rich-
ness in terms of definitions (glosses) and semantic
relations, synonymy via synsets in particular, it has
become a de facto standard for all sorts of research
that rely on lexical content for the English language.
Besides this perspective on rich lexicological
data, over the years a software infrastructure has
emerged around WORDNET that was equally ap-
proved by the NLP community. This included,
e.g., a lexicographic file generator, various editors
and visualization tools but also meta tools rely-
ing on properly formated WORDNET data such as
a library of similarity measures (Pedersen et al,
2004). In numerous articles the usefulness of this
data and software ensemble has been demonstrated
(e.g., for word sense disambiguation (Patwardhan
et al, 2003), the analysis of noun phrase conjuncts
(Hogan, 2007), or the resolution of coreferences
(Harabagiu et al, 2001)).
In our research on information extraction and text
mining within the field of biomedical NLP, we sim-
ilarly recognized an urgent need for a lexical re-
source comparable to WORDNET, both in scope and
size. However, the direct usability of the original
WORDNET for biomedical NLP is severely ham-
pered by a (not so surprising) lack of coverage of the
life sciences domain in the general-language English
WORDNET as was clearly demonstrated by Burgun
and Bodenreider (2001).
Rather than building a BIOWORDNET by hand,
as was done for the general-language English
WORDNET, our idea to set up a WORDNET-style
lexical resource for the life sciences was different.
We wanted to link the original WORDNET with
various biomedical terminological resources vastly
available in the life sciences domain. As an obvious
candidate for this merger, we chose one of the ma-
jor high-coverage umbrella systems for biomedical
ontologies, the OPEN BIOMEDICAL ONTOLOGIES
(OBO).1 These (currently) over 60 OBO ontologies
provide domain-specific knowledge in terms of hi-
erarchies of classes that often come with synonyms
and textual definitions for lots of biomedical sub-
domains (such as genes, proteins, cells, sequences,
1http://www.bioontology.org/
repositories.html#obo
31
etc.).2 Given these resources and their software in-
frastructure, our plan was to create a biomedically
focused lexicological resource, the BIOWORDNET,
whose coverage would exceed that of any of its com-
ponent resources in a so far unprecedented man-
ner. Only then, given such a huge combined re-
source advanced NLP tasks such as anaphora res-
olution seem likely to be tackled in a feasible way
(Hahn et al, 1999; Castan?o et al, 2002; Poprat and
Hahn, 2007). In particular, we wanted to make di-
rect use of available software infrastructure such as
the library of similarity metrics without the need for
re-programming and hence foster the reuse of exist-
ing software as is.
We began our efforts on the assumption that the
WORDNET software resources were stable and reli-
able. In the course of our work, it turned out that this
belief was far too optimistic. We discuss the stum-
bling blocks that we encountered, point out an er-
ror in the WORDNET software with implications for
research based on it, and conclude that building on
the legacy of WORDNET data structures and its as-
sociated software might preclude sustainable exten-
sions that go beyond the domain of general English.
Hence, our report contains one of the rare failure sto-
ries (not only) in our field.
2 Software Around WORDNET Data
While the stock of lexical data assembled in the
WORDNET lexicon was continuously growing over
time,3 its data format and storage structures, the so-
called lexicographic file, by and large, remained un-
altered (see Section 2.1). In Section 2.2, we will deal
with two important software components with which
the lexicographic file can be created and browsed.
Over the years, together with the continuous exten-
sion of the WORDNET lexicon, a lot of software
tools have been developed in various programming
languages allowing browsing and accessing WORD-
NET as well as calculating semantic similarities on
it. We will discuss the most relevant of these tools
in Section 2.3.
2Bodenreider and Burgun (2002) point out that the structure
of definitions in WORDNET differ to some degree from more
domain-specialized sources such as medical dictionaries.
3The latest version 3.0 was released in December 2006
2.1 Lexicon Organization of WORDNET and
Storage in Lexicographic Files
At the top level, WORDNET is organized accord-
ing to four parts of speech, viz. noun, verb, adjec-
tive and adverb. The most recent version 3.0 cov-
ers more than 117,000 nouns, 11,500 verbs, 21,400
adjectives and 4,400 adverbs, interlinked by lexical
relations, mostly derivations. The basic semantic
unit for all parts of speech are sets of synonymous
words, so-called synsets. These are connected by
different semantic relations, imposing a thesaurus-
like structure on WORDNET. In this paper, we dis-
cuss the organization of noun synsets in WORDNET
only, because this is the relevant part of WORD-
NET for our work. There are two important seman-
tic relation types linking noun synsets. The hyper-
nym / hyponym relation on which the whole WORD-
NET noun sense hierarchy is built links more spe-
cific to more general synsets, while the meronym /
holonym relation describes partonomic relations be-
tween synsets, such as part of the whole, member of
the whole or substance of the whole.
From its very beginning, WORDNET was built
and curated manually. Lexicon developing experts
introduced new lexical entries into WORDNET,
grouped them into synsets and defined appropriate
semantic and lexical relations. Since WORDNET
was intended to be an electronic lexicon, a data
representation format had to be defined as well.
When the WORDNET project started more than two
decades ago, markup languages such as SGML or
XML were unknown. Because of this reason, a
rather idiosyncratic, fully text-based data structure
for these lexicographic files was defined in a way to
be readable and editable by humans ? and survived
until to-day. This can really be considered as an
outdated legacy given the fact that the WORDNET
community has been so active in the last years in
terms of data collection, but has refrained from
adapting its data formats in a comparable way to
to-day?s specification standards. Very basically,4
each line in the lexicographic file holds one synset
that is enclosed by curly brackets. Take as an
example the synset for ?monkey?:
4A detailed description can be found in the WORDNET
manual wninput(5WN), available from http://wordnet.
princeton.edu/man/wninput.5WN.
32
{ monkey, primate,@ (any of various
long-tailed primates (excluding the
prosimians)) }
Within the brackets at the first position synonyms
are listed, separated by commas. In the exam-
ple, there is only one synonym, namely ?monkey?.
The synonyms are followed by semantic relations to
other synsets, if available. In the example, there is
only one hypernym relation (denoted by ?@?) point-
ing to the synset ?primate?. The final position is
reserved for the gloss of the synset encapsulated in
round brackets. It is important to notice that there
are no identifiers for synsets in the lexicographic file.
Rather, the string expressions themselves serve as
identifiers. Given the fundamental idea of synsets ?
all words within a synset mean exactly the same in
a certain context ? it is sufficient to relate one word
in the synset in order to refer to the whole synset.
Still, there must be a way to deal with homonyms,
i.e., lexical items which share the same string, but
have different meanings. WORDNET?s approach to
distinguish different senses of a word is to add num-
bers from 0 to 15, called lexical identifiers. Hence,
in WORDNET, a word cannot be more than 16-fold
ambiguous. This must be kept in mind when one
wants to build a WORDNET for highly ambiguous
sublanguages such as the biomedical one.
2.2 Software Provided with WORDNET
To guarantee fast access to the entries and their rela-
tions, an optimized index file must be created. This
is achieved through the easy-to-use GRIND software
which comes with WORDNET. It simply consumes
the lexicographic file(s) as input and creates two
plain-text index files,5 namely data and index.
Furthermore, there is a command line tool, WN, and
a graphical browser, WNB, for data visualization that
require the specific index created by GRIND (as all
the other tools that query the WORDNET data do as
well). These tools are the most important (and only)
means of software support for WORDNET creation
by checking the syntax as well as allowing the (man-
ual) inspection of the newly created index.
5Its syntax is described in http://wordnet.
princeton.edu/man/wndb.5WN.
2.3 Third-Party WORDNET Tools
Due to the tremendous value of WORDNET for the
NLP and IR community and its usefulness as a
resource for coping with problems requiring mas-
sive amounts of lexico-semantic knowledge, the
software-developing community was and continues
to be quite active. Hence, in support of WORDNET
several APIs and software tools were released that
allow accessing, browsing and visualizing WORD-
NET data and measuring semantic similarity on the
base of the WORDNET?s lexical data structures.6
The majority of these APIs are maintained well
and kept up to date, such as JAWS7 and JWNL,8
and enable connecting to the most recent ver-
sion of WORDNET. For the calculation of vari-
ous similarity measures, the PERL library WORD-
NET::SIMILARITY initiated and maintained by Ted
Pedersen9 can be considered as a de facto stan-
dard and has been used in various experimental set-
tings and applications. This availability of well-
documented and well-maintained software is defi-
nitely a strong argument to rely on WORDNET as
a powerful lexico-semantic knowledge resource.
3 The BIOWORDNET Initiative
In this section, we describe our approach to extend
WORDNET towards the biomedical domain by in-
corporating terminological resources from the OBO
collection. The most obvious problems we faced
were to define a common data format and to map
non-compliant data formats to the chosen one.
3.1 OBO Ontologies
OBO is a collection of publicly accessible biomed-
ical ontologies.10 They cover terms from
many biomedical subdomains and offer structured,
domain-specific knowledge in terms of classes
(which often come with synonyms and textual defi-
nitions) and class hierarchies. Besides the hierarchy-
defining relation is-a, some OBO ontologies provide
6For a comprehensive overview of available WORDNET
tools we refer to WORDNET?s ?related project? website (http:
//wordnet.princeton.edu/links).
7http://engr.smu.edu/
?
tspell/
8http://jwordnet.sourceforge.net/
9http://wn-similarity.sourceforge.net/
10http://www.bioontology.org/
33
WordNet
Index
{ histoblast, simple_col...
{  laborinth_support ing .. .
{ structural_cell, cell_by...
{  mesangial_phagocyte, . . .
{ ito_cell, perisinusoida_ ...
{  . . .  }
   ...
OBO ontology 
in OWL-format
extracted data BioWordNet
lexicographic
fi le
Step 1 :
data  ex t rac t ion
from OBO
Step 2:  
convers ion to  WordNet
lexicographic f i le
 fo rmat
Step 3:  
bui ld ing WordNet  index 
using ?grind?
WordNet Browser
WordNet APIInformation Retrieval
Similarity MeasuringAnaphora Resolution
Document Clustering
Step 4:  
B ioWordNet
 index can be 
used by var ious 
sof tware  
components  
and APIs . . .
Step 5:  
. . .  and fur ther  be processed 
in  NLP compontents
BioWordNet
index f i le
IR and NLP 
applications
Figure 1: From OBO ontologies to BIOWORDNET? towards a domain-specific WORDNET for biomedicine
additional semantic relation types such as sequence-
of or develops-from to express even more complex
and finer-grained domain-specific knowledge. The
ontologies vary significantly in size (up to 60,000
classes with more than 150,000 synonyms), the
number of synonyms per term and the nature of
terms.
The OBO ontologies are available in various for-
mats including the OBO flat file format, XML and
OWL. We chose to work with the OWL version for
our purpose,11 since for the OWL language also ap-
propriate tools are available facilitating the extrac-
tion of particular information from the ontologies,
such as taxonomic links, labels, synonyms and tex-
tual definitions of classes.
3.2 From OBO to BIOWORDNET
Our plan was to construct a BIOWORDNET by con-
verting, in the first step, the OBO ontologies into a
WORDNET hierarchy of synsets, while keeping to
the WORDNET lexicographic file format, and build-
ing a WORDNET index. As a preparatory step, we
defined a mapping from the ontology to WORDNET
items as shown in Table 1.
The three-stage conversion approach is depicted
in Figure 1. First, domain specific terms and tax-
11http://www.w3.org/TR/owl-semantics/
OBO ontology BIOWORDNET
ontology class synset
class definition synset gloss
class name word in synset
synonym of class name word in synset
Ci is-a Cj Si hyponym of Sj
Cj has-subclass Ci Sj hypernym of Si
Table 1: Mapping between items from OBO and from
BIOWORDNET (Ci and Cj denote ontology classes, Si
and Sj the corresponding BIOWORDNET synsets)
onomic links between terms were extracted sepa-
rately from each of the OBO ontologies. Then
the extracted data was converted according to the
syntax specifications of WORDNET?s lexicographic
file. Finally for each of the converted ontologies the
WORDNET-specific index was built using GRIND.
Following this approach we ran into several prob-
lems, both regarding the WORDNET data structure
and the WORDNET-related software that we used
for the construction of the BIOWORDNET. Con-
verting the OBO ontologies turned out to be cum-
bersome, especially the conversion of the CHEBI
ontology12 (long class names holding many special
characters) and the NCI thesaurus13 (large number
12http://www.ebi.ac.uk/chebi/
13http://nciterms.nci.nih.gov/
34
of classes and some classes that also have a large
number of subclasses). These and additional prob-
lems will be addressed in more detail in Section 4.
4 Problems with WORDNET?s Data
Format and Software Infrastructure
We here discuss two types of problems we found
for the data format underlying the WORDNET lex-
icon and the software that helps building a WORD-
NET file and creating an index for this file. First,
WORDNET?s data structure puts several restrictions
on what can be expressed in a WORDNET lexicon.
For example, it constrains lexical information to a
fixed number of homonyms and a fixed set of rela-
tions. Second, the data structure imposes a number
of restrictions on the string format level. If these
restrictions are violated the WORDNET processing
software throws error messages which differ consid-
erably in terms of informativeness for error tracing
and detection or even do not surface at all at the lex-
icon builder?s administration level.
4.1 Limitations of Expressiveness
The syntax on which the current WORDNET lex-
icographic file is based imposes severe limitations
on what can be expressed in WORDNET. Although
these limitations might be irrelevant for representing
general-language terms, they do affect the construc-
tion of a WORDNET-like resource for biomedicine.
To give some examples, the WORDNET format al-
lows a 16-fold lexical ambiguity only (lexical IDs
that are assigned to ambiguous words are restricted
to the numbers 0-15, see Section 2). This forced us
to neglect some of the OBO ontology class names
and synonyms that were highly ambiguous.14
Furthermore, the OBO ontologies excel in a richer
set of semantic relations than WORDNET can of-
fer. Thus, a general problem with the conversion
of the OBO ontologies into WORDNET format was
that except from the taxonomic is-a relation (which
corresponds to the WORDNET hyponym relation)
and the part-of relation (which corresponds to the
WORDNET meronym relation) all remaining OBO-
specific relations (such as develops-from, sequence-
of, variant-of and position-of ) could not be rep-
14This is a well-known limitation that is already mentioned
in the WORDNET documentation.
resented in the BIOWORDNET. The structure of
WORDNET neither contains such relations nor is
it flexible enough to include them so that we face
a systematic loss of information in BIOWORDNET
compared to the original OBO ontologies. Al-
though these restrictions are well-known, their re-
moval would require extending the current WORD-
NET data structure fundamentally. This, in turn,
would probably necessitate a full re-programming of
all of WORDNET-related software.
4.2 Limitations of Data Format and Software
When we tried to convert data extracted from the
OBO ontologies into WORDNET?s lexicographic
file format (preserving its syntactic idiosyncrasies
for the sake of quick and straightforward reusability
of software add-ons), we encountered several intri-
cacies that took a lot of time prior to building a valid
lexicographic file.
First, we had to replace 31 different charac-
ters with unique strings such as ?(? with ?-LRB-
? and ?+? with ?-PLU-? before GRIND was able
to process the lexicographic file. The reason is
that many of such special characters occurring
in domain specific terms, especially in designa-
tors of chemical compounds such as ?methyl es-
ter 2,10-dichloro-12H-dibenzo(d,g)(1,3)dioxocin-6-
carboxylic acid? (also known as ?treloxinate? with
the CAS registry number 30910-27-1), are reserved
symbols in the WORDNET data formatting syntax.
If these characters are not properly replaced GRIND
throws an exact and useful error message (see Table
2, first row).
Second, we had to find out that we have to replace
all empty glosses by at least one whitespace charac-
ter. Otherwise, GRIND informs the user in terms of
a rather cryptic error message that mentions the po-
sition of the error though not its reason (see Table 2,
second row).
Third, numbers at the end of a lexical item need to
be escaped. In WORDNET, the string representation
of an item is used as its unique identifier. To dis-
tinguish homonyms (words with the same spelling
but different meaning, such as ?cell? as the func-
tional unit of all organisms, on the one hand, and
as small compartment, on the other hand) accord-
ing to the WORDNET format different numbers from
0 to 15 (so-called lexical IDs) have to be appended
35
Problem Description Sample Error Message Usefulness of Er-
ror Message
Problem Solution
illegal use of key characters noun.cell, line 7: Illegal
character %
high replace illegal characters
empty gloss sanity error - actual pos
2145 != assigned pos
2143!
moderate add gloss consisting of at least
one whitespace character
homonyms (different words
with identical strings)
noun.rex, line 5: Syn-
onym ?electrochem-
ical reaction? is not
unique in file
high distinguish word senses by
adding lexical identifiers (use
the numbers 1-15)
lexical ID larger than 15 noun.rex, line 4: ID must
be less than 16: cd25
high quote trailing numbers of
words, only assign lexical
identifiers between 1-15, omit
additional word senses
word with more than 425
characters
Segmentation fault (core
dumped)
low omit words that exceed the max-
imal length of 425 characters
synset with more than 998
direct hyponymous synsets
Segmentation fault (core
dumped)
low omit some hyponymous synsets
or introduce intermediate
synsets with a limited number
of hyponymous synsets
no query result though the
synset is in the index, access
software crashes
none ? not known
Table 2: Overview of the different kinds of problems that we encountered when creating a BIOWORDNET keeping to
the WORDNET data structure and the corresponding software. Each problem description is followed by a sample error
message that GRIND had thrown, a statement about how useful the error message was to detect the source of the error
and a possible solution for the problems, if available. The last row documents a special experience with data viewers
for data from the NCI thesaurus.
to the end of each homonym. If in a lexicographic
file two identical strings occur that have not been as-
signed different lexical identifiers (it does not mat-
ter whether this happens within or across synsets)
GRIND emits an error message that mentions both,
the position and the lexical entry which caused this
error (cf. Table 2, third row).
Numbers that appear at the end of a lexical item as
an integral part of it (such as ?2? in ?IL2?, a special
type of cytokine (protein)) have to be escaped in or-
der to avoid their misinterpretation as lexical identi-
fiers. This, again, is a well-documented shortcoming
of WORDNET?s data specification rules.
In case such numbers are not escaped prior to pre-
senting the lexicographic file to GRIND the word
closing numbers are always interpreted as lexical
identifiers. Closing numbers that exceed the num-
ber 15 cause GRIND to throw an informative error
message (see Table 2, fourth row).
4.3 Undocumented Restrictions and
Insufficient Error Messages
In addition to the more or less documented re-
strictions of the WORDNET data format mentioned
above we found additional restrictions that lack doc-
umentation up until now, to the best of our knowl-
edge.
First, it seems that the length of a word is re-
stricted to 425 characters. If a word in the lexico-
graphic file exceeds this length, GRIND is not able to
create an index and throws an empty error message,
namely the memory error ?segmentation fault? (cf.
Table 2, fifth row). As a consequence of this restric-
tion, some very long CHEBI class names could not
have been included in the BIOWORDNET.
Second, it seems that synsets are only allowed to
group up to 988 direct hyponymous synsets. Again,
GRIND is not able to create an index, if this restric-
tion is not obeyed and throws the null memory er-
36
ror message ?segmentation fault? (cf. Table 2, sixth
row). An NCI thesaurus class that had more than
998 direct subclasses thus could not have been in-
cluded in the BIOWORDNET.
Due to insufficient documentation and utterly
general error messages the only way to locate the
problem causing the ?segmentation fault? errors was
to examine the lexicographic files manually. We had
to reduce the number of synset entries in the lexico-
graphic file, step by step, in a kind of trial and error
approach until we could resolve the problem. This
is, no doubt, a highly inefficient and time consum-
ing procedure. More informative error messages of
GRIND would have helped us a lot.
4.4 Deceptive Results from WORDNET
Software and Third-Party Components
After getting rid of all previously mentioned errors,
valid index files were compiled. It was possible to
access these index files using the WORDNET query-
ing tools WN and WNB, indicating the index files
were ?valid?. However, when we tried to query
the index file that was generated by GRIND for the
NCI thesaurus we got strange results. While WN
did not return any query results, the browser WNB
crashed without any error message (cf. Table 2, sev-
enth row). The same holds for the Java APIs JAWS
and JWNL.
Since a manual examination of the index file re-
vealed that the entries that we were searching for, in
fact, were included in the file, some other, up to this
step unknown error must have prevented the soft-
ware tools from finding the targeted entries. Hence,
we want to point out that although we have exam-
ined this error for the NCI thesaurus only, the risk
is high that this ?no show? error is likely to bias
any other application as well which makes use of
the the same software that we grounded our ex-
periments on. Since the NCI thesaurus is a very
large resource, even worse, further manual error
search is nearly impossible. At this point, we
stopped our attempt building a WORDNET resource
for biomedicine based on the WORDNET formatting
and software framework.
5 Related Work
In the literature dealing with WORDNET and its
structures from a resource perspective (rather than
dealing with its applications), two directions can
be distinguished. On the one hand, besides the
original English WORDNET and the various vari-
ant WORDNETs for other languages (Vossen, 1998),
extensions to particular domains have already been
proposed (for the medical domain by Buitelaar and
Sacaleanu (2002) and Fellbaum et al (2006); for the
architectural domain Bentivogli et al (2004); and
for the technical report domain by Vossen (2001)).
However, none of these authors neither mentions im-
plementation details of the WORDNETs or perfor-
mance pitfalls we have encountered, nor is supple-
mentary software pointed out that might be useful
for our work.
On the other hand, there are suggestions concern-
ing novel representation formats of next-generation
WORDNETs. For instance in the BALKANET
project (Tufis? et al, 2004), an XML schema plus
a DTD was proposed (Smrz?, 2004) and an editor
called CISDIC with basic maintenance functionali-
ties and consistency check was released (Hora?k and
Smrz?, 2004). The availability of APIs or software to
measure similarity though remains an open issue.
So, our approach to reuse the structure and the
software for building a BIOWORDNET was moti-
vated by the fact that we could not find any al-
ternatives coming with a software ensemble as de-
scribed in Section 2. Against all expectations, we
did not manage to reuse the WORDNET data struc-
ture. However, there are no publications that report
on such difficulties and pitfalls we were confronted
with.
6 Discussion and Conclusion
We learnt from our conversion attempt that the cur-
rent WORDNET representation format of WORD-
NET suffers from several limitations and idiosyn-
crasies that cannot be by-passed by a simple, yet
ad hoc work-around. Many of the limitations and
pitfalls we found limiting (in the sense what can be
expressed in WORDNET) are due to the fact that its
data format is out-of-date and not really suitable for
the biomedical sublanguage. In addition, though we
do not take into doubt that the WORDNET software
37
works fine for the official WORDNET release, our
experiences taught us that it fails or gives limited
support in case of building and debugging a new
WORDNET resource. Even worse, we have evidence
from one large terminological resource (NCI) that
WORDNET?s software infrastructure (GRIND) ren-
ders deceptive results.
Although WORDNET might no longerbe the one
and only lexical resource for NLP each year a con-
tinuously strong stream of publications on the use of
WORDNET illustrates its importance for the com-
munity. On this account we find it remarkable that
although improvements in content and structure of
WORDNET have been proposed (e.g., Boyd-Graber
et al (2006) propose to add (weighted) connec-
tions between synsets, Oltramari et al (2002) sug-
gest to restructure WORDNET?s taxonomical struc-
ture, and Mihalcea and Moldovan (2001) recom-
mend to merge synsets that are too fine-grained)
to the best of our knowledge, no explicit proposals
have been made to improve the representation for-
mat of WORDNET in combination with the adaption
of the WORDNET-related software.
According to our experiences the existing WORD-
NET software is hardly (re)usable due to insufficient
error messages that the software throws and limited
documentation. From our point of view it would be
highly preferable if the software would be improved
and made more user-supportive (more meaningful
error messages would already improve the useful-
ness of the software). In terms of the actual rep-
resentation format of WORDNET we found that us-
ing the current format is not only cumbersome and
error-prone, but also limits what can be expressed in
a WORDNET resource.
From our perspective this indicates the need for
a major redesign of WORDNET?s data structure
foundations to keep up with the standards of to-
day?s meta data specification languages (e.g., based
on RFD (Graves and Gutierrez, 2006), XML or
OWL (Lu?ngen et al, 2007)). We encourage the re-
implementation of WORDNET resources based on
such a state-of-the-art markup language (for OWL in
particular a representation of WORDNET is already
available, cf. van Assem et al (2006)). Of course, if
a new representation format is used for a WORDNET
resource also the software accessing the resource has
to be adapted to the new format. This may require
substantial implementation efforts that we think are
worth to be spent, if the new format overcomes the
major problems that are due to the original WORD-
NET format.
Acknowledgments
This work was funded by the German Ministry
of Education and Research within the STEMNET
project (01DS001A-C) and by the EC within the
BOOTSTREP project (FP6-028099).
References
Luisa Bentivogli, Andrea Bocco, and Emanuele Pianta.
2004. ARCHIWORDNET: Integrating WORDNET
with domain-specific knowledge. In Petr Sojka, Karel
Pala, Christiane Fellbaum, and Piek Vossen, editors,
GWC 2004 ? Proceedings of the 2nd International
Conference of the Global WordNet Association, pages
39?46. Brno, Czech Republic, January 20-23, 2004.
Olivier Bodenreider and Anita Burgun. 2002. Character-
izing the definitions of anatomical concepts in WORD-
NET and specialized sources. In Proceedings of the 1st
International Conference of the Global WordNet Asso-
ciation, pages 223?230. Mysore, India, January 21-25,
2002.
Jordan Boyd-Graber, Christiane Fellbaum, Daniel Osh-
erson, and Robert Schapire. 2006. Adding dense,
weighted connections to WORDNET. In Petr Sojka,
Key-Sun Choi, Christiane Fellbaum, and Piek Vossen,
editors, GWC 2006 ? Proceedings of the 3rd Inter-
national WORDNET Conference, pages 29?35. South
Jeju Island, Korea, January 22-26, 2006.
Paul Buitelaar and Bogdan Sacaleanu. 2002. Extend-
ing synsets with medical terms WORDNET and spe-
cialized sources. In Proceedings of the 1st Interna-
tional Conference of the Global WordNet Association.
Mysore, India, January 21-25, 2002.
Anita Burgun and Olivier Bodenreider. 2001. Compar-
ing terms, concepts and semantic classes in WORD-
NET and the UNIFIED MEDICAL LANGUAGE SYS-
TEM. In Proceedings of the NAACL 2001 Workshop
?WORDNET and Other Lexical Resources: Applica-
tions, Extensions and Customizations?, pages 77?82.
Pittsburgh, PA, June 3-4, 2001. New Brunswick, NJ:
Association for Computational Linguistics.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature. In
Proceedings of The International Symposium on Ref-
erence Resolution for Natural Language Processing.
Alicante, Spain, June 3-4, 2002.
Christiane Fellbaum, Udo Hahn, and Barry Smith. 2006.
Towards new information resources for public health:
38
From WORDNET to MEDICAL WORDNET. Journal
of Biomedical Informatics, 39(3):321?332.
Christiane Fellbaum, editor. 1998. WORDNET: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press.
Alvaro Graves and Caludio Gutierrez. 2006. Data rep-
resentations for WORDNET: A case for RDF. In Petr
Sojka, Key-Sun Choi, Christiane Fellbaum, and Piek
Vossen, editors, GWC 2006 ? Proceedings of the 3rd
International WORDNET Conference, pages 165?169.
South Jeju Island, Korea, January 22-26, 2006.
Udo Hahn, Martin Romacker, and Stefan Schulz. 1999.
Discourse structures in medical reports ? watch out!
The generation of referentially coherent and valid text
knowledge bases in the MEDSYNDIKATE system. In-
ternational Journal of Medical Informatics, 53(1):1?
28.
Sanda M. Harabagiu, Ra?zvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL?01, Language Tech-
nologies 2001 ? Proceedings of the 2nd Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 1?8. Pittsburgh, PA,
USA, June 2-7, 2001. San Francisco, CA: Morgan
Kaufmann.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In ACL?07 ?
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 680?687.
Prague, Czech Republic, June 28-29, 2007. Strouds-
burg, PA: Association for Computational Linguistics.
Ales? Hora?k and Pavel Smrz?. 2004. New features of
wordnet editor VisDic. Romanian Journal of Infor-
mation Science and Technology (Special Issue), 7(1-
2):201?213.
Harald Lu?ngen, Claudia Kunze, Lothar Lemnitzer, and
Angelika Storrer. 2007. Towards an integrated
OWL model for domain-specific and general language
WordNets. In Attila Tana?cs, Dora? Csendes, Veronika
Vincze, Christiane Fellbaum, and Piek Vossen, editors,
GWC 2008 ? Proceedings of the 4th Global WORD-
NET Conference, pages 281?296. Szeged, Hungary,
January 22-25, 2008.
Rada Mihalcea and Dan Moldovan. 2001.
EZ.WORDNET: Principles for automatic genera-
tion of a coarse grained WORDNET. In Proceedings
of the 14th International Florida Artificial Intelli-
gence Research Society (FLAIRS) Conference, pages
454?458.
Alessandro Oltramari, Aldo Gangemi, Nicola Guarino,
and Claudio Madolo. 2002. Restructuring WORD-
NET?s top-level. In Proceedings of ONTOLEX 2002
@ LREC 2002.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Alexander F.
Gelbukh, editor, CICLing 2003 ? Computational Lin-
guistics and Intelligent Text Processing. Proceedings
of the 4th International Conference, volume 2588 of
Lecture Notes in Computer Science, pages 241?257.
Mexico City, Mexico, February 16-22, 2003. Berlin
etc.: Springer.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WORDNET::Similarity: Measuring the
relatedness of concepts. In AAAI?04 ? Proceedings
of the 19th National Conference on Artificial Intelli-
gence & IAAI?04 ? Proceedings of the 16th Innova-
tive Applications of Artificial Intelligence Conference,
pages 1024?1025. San Jose?, CA, USA, July 25-29,
2004. Menlo Park, CA; Cambridge, MA: AAAI Press
& MIT Press.
Michael Poprat and Udo Hahn. 2007. Quantitative data
on referring expressions in biomedical abstracts. In
BioNLP at ACL 2007 ? Proceedings of the Workshop
on Biological, Translational, and Clinical Language
Processing, pages 193?194. Prague, Czech Republic,
June 29, 2007. Stroudsburg, PA: Association for Com-
putational Liguistics.
Pavel Smrz?. 2004. Quality control and checking for
wordnets development: A case study of BALKANET.
Romanian Journal of Information Science and Tech-
nology (Special Issue), 7(1-2):173?181.
D. Tufis?, D. Christea, and S. Stamou. 2004. BALKA-
NET: Aims, methods, results and perspectives. a gen-
eral overview. Romanian Journal of Information Sci-
ence and Technology (Special Issue), 7(1-2):9?43.
Mark van Assem, Aldo Gangemi, and Guus Schreiber.
2006. Conversion of WORDNET to a standard
RDF/OWL representation. In LREC 2006 ? Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation. Genoa, Italy, May 22-28,
2006. Paris: European Language Resources Associa-
tion (ELRA), available on CD.
Piek Vossen, editor. 1998. EUROWORDNET: A Mul-
tilingual Database with Lexical Semantic Networks.
Dordrecht: Kluwer Academic Publishers.
Piek Vossen. 2001. Extending, trimming and fusing
WORDNET for technical documents. In Proceedings
of the NAACL 2001 Workshop ?WORDNET and Other
Lexical Resources: Applications, Extensions and Cus-
tomizations?. Pittsburgh, PA, June 3-4, 2001. New
Brunswick, NJ: Association for Computational Lin-
guistics.
39
Proceedings of the Workshop on BioNLP, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Feasible and Robust is the Automatic Extraction of Gene Regulation
Events ? A Cross-Method Evaluation under Lab and Real-Life Conditions
Udo Hahn1 Katrin Tomanek1 Ekaterina Buyko1 Jung-jae Kim2 Dietrich Rebholz-Schuhmann2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{udo.hahn|katrin.tomanek|ekaterina.buyko}@uni-jena.de
2EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{kim|rebholz}@ebi.ac.uk
Abstract
We explore a rule system and a machine learn-
ing (ML) approach to automatically harvest
information on gene regulation events (GREs)
from biological documents in two different
evaluation scenarios ? one uses self-supplied
corpora in a clean lab setting, while the other
incorporates a standard reference database of
curated GREs from REGULONDB, real-life
data generated independently from our work.
In the lab condition, we test how feasible
the automatic extraction of GREs really is
and achieve F-scores, under different, not di-
rectly comparable test conditions though, for
the rule and the ML systems which amount
to 34% and 44%, respectively. In the REGU-
LONDB condition, we investigate how robust
both methodologies are by comparing them
with this routinely used database. Here, the
best F-scores for the rule and the ML systems
amount to 34% and 19%, respectively.
1 Introduction
The extraction of binary relations from biomedical
text has caught much attention in the recent years.
Progress on this and other tasks has been monitored
in challenge competitions such as BIOCREATIVE I
and II,1 which dealt with gene/protein names and
and protein-protein interaction.
The BIOCREATIVE challenge and other related
ones have shown at several occasions that partici-
pants continue to use two fundamentally different
1http://biocreative.sourceforge.net/
systems: symbolic pattern-based systems (rule sys-
tems), on the one hand, and feature-based statisti-
cal machine learning (ML) systems, on the other
hand. This has led to some rivalry with regard to the
interpretation of their performance data, the costs
of human efforts still required and their scalability
for the various tasks. While rule systems are of-
ten hand-crafted and fine-tuned to a particular ap-
plication (making a major manual rewrite often nec-
essary when the application area is shifted), ML
systems are trained automatically on manually an-
notated corpora, i.e., without manual intervention,
and thus have the advantage to more easily adapt to
changes in the requested identification tasks. Time
costs (human workload) are thus shifted from rule
design and adaptation to metadata annotation.
Text mining systems as usually delivered by
BioNLP researchers render biologically relevant en-
tities and relations on a limited set of test documents
only. While this might be sufficient for the BioNLP
community, it is certainly insufficient for bioinfor-
maticians and molecular biologists since they re-
quire large-scale data with high coverage and reli-
ability. For our analysis, we have chosen the topic
of gene regulatory events in E. coli, which is a do-
main of very active research and grand challenges.2
Currently the gold standard of the existing body of
knowledge of such events is represented by the fact
database REGULONDB.3 Its content has been man-
2The field of gene regulation is one of the most prominent
topics of research and often mentioned as one of the core fields
of future research in molecular biology (cf, e.g., the Grand
Challenge I-2 described by Collins et al (2003)).
3http://regulondb.ccg.unam.mx/
37
ually gathered from the scientific literature and de-
scribes the curated computational model of mecha-
nisms of transcriptional regulation in E. coli. Having
this gold standard in mind, we face the challenging
task to automatically reproduce this content from the
available literature, to enhance this content with re-
liable additional information and to update this re-
source as part of a regular automatic routine.
Hence, we first explore the feasibility and per-
formance of a rule-based and an ML-based system
against special, independently created corpora that
were generated to enable measurements under clean
experimental lab conditions. This part, due to dif-
ferent experimental settings, is not meant as a com-
parison between both approaches though. We then
move to the even more demanding real-life scenario
where we evaluate and compare these solutions for
the identification of gene regulatory events against
the REGULONDB data resource. This approach tar-
gets the robustness of the proposed text mining so-
lutions from the perspectives of completeness, cor-
rectness and novelty of the generated results.
2 Related Work
Considering relation extraction (RE) in the biomed-
ical domain, there are only few studies which deal
primarily with gene regulation. Yang et al (2008)
focus on the detection of sentences that contain
mentions of transcription factors (proteins regulat-
ing gene expression). They aim at the detection
of new transcription factors, while relations are not
taken into account. In contrast, S?aric? et al (2004)
extract gene regulatory networks and achieve in the
RE task an accuracy of up to 90%. They disregard,
however, ambiguous instances, which may have led
to the low recall around 20%. The Genic Interaction
Extraction Challenge (Ne?dellec, 2005) was orga-
nized to determine the state-of-the-art performance
of systems designed for the detection of gene regula-
tion interactions. The best system achieved a perfor-
mance of about 50% F-score. The results, however,
have to be taken with care as the LLL corpus used in
the challenge is of extremely limited size.
3 Extraction of Gene Regulation Events
Gene regulation is a complex cellular process that
controls the expression of genes. These genes are
then transcribed into their RNA representation and
later translated into proteins, which fulfill various
tasks such as maintaining the cell structure, enabling
the generation of energy and interaction with the en-
vironment.
The analysis of the gene regulatory processes is
ongoing research work in molecular biology and af-
fects a large number of research domains. In par-
ticular the interpretation of gene expression profiles
from microarray analyses could be enhanced using
our understanding of gene regulation events (GREs)
from the literature.
We approach the task of the automatic extraction
of GREs from literature from two different method-
ological angles. On the one hand, we provide a set of
hand-crafted rules ? both for linguistic analysis and
conceptual inference (cf. Section 3.1), the latter be-
ing particularly helpful in unveiling only implicitly
stated biological knowledge. On the other hand, we
supply a machine learning-based system for event
extraction (cf. Section 3.2). No regularities are spec-
ified a priori by a human although, at least in the su-
pervised scenario we have chosen, this approach re-
lies on training data supplied by human (expert) an-
notators who provide sufficiently many instances of
ground truth decisions from which regularities can
automatically be learnt. At the level of system per-
formance, rules tend to foster precision at the cost
of recall and ML systems tend to produce inverse
figures, while there is no conclusive evidence for or
against any of these two approaches.
The extraction of GREs, independent of the ap-
proach one subscribes to, is a complex problem
composed of a series of subtasks. Abstracting away
from lots of clerical and infrastructure services (e.g.,
sentence splitting, tokenization) at the core of any
GRE extraction lie the following basic steps:
? the identification of pairs of gene mentions as
the arguments of a relation ? the well-known
named entity recognition and normalization
task,
? the decision whether the entity pair really con-
stitutes a relation,
? and the identification of the roles of the argu-
ments in the relation which implicitly amounts
to characterize each argument as either agent or
patient.
38
3.1 Rule-based Extraction
The rule-based system extracts GREs from text em-
ploying logical inference. The motivation of using
inference is that the events under scrutiny are often
expressed in text in either a compositional or an in-
complete way. We address this issue by composi-
tionally representing textual semantics and by log-
ically inferring implicit meanings of text over the
compositional representation of textual semantics.
Entity Identification. The system first recognizes
named entities of the types that can be participants of
the target events. We have collected 15,881 E. coli
gene/protein and operon names from REGULONDB
and UNIPROT. Most of the gene/protein names are
associated with UNIPROT identifiers. An operon in
prokaryotes is a DNA sequence with multiple genes
whose expression is controlled by a shared promoter
and which thus express together. We have mapped
the operon names to corresponding gene sets.
Named entity recognition relies on the use of dic-
tionaries. If the system recognizes an operon name,
it then associates the operon with its genes. The
system further recognizes multi-gene object names
(e.g., ?acrAB?), divides them into individual gene
names (e.g., ?acrA?, ?acrB?) and associates the gene
names with the multi-gene object names.
Relation Identification. The system then iden-
tifies syntactic structures of sentences in an in-
put corpus by utilizing the ENJU parser (Sagae et
al., 2007). The ENJU parser generates predicate-
argument structures, and the system converts them
into dependency structures.
The system then analyzes the semantics of the
sentences by matching syntactic-semantic patterns
to the dependency structures. We constructed 1,123
patterns for the event extraction according to the fol-
lowing workflow. We first collected keywords re-
lated to gene regulation, from GENE ONTOLOGY,
INTERPRO, WORDNET, and several papers about
information extraction from biomedical literature
(Hatzivassiloglou and Weng, 2002; Kim and Park,
2004; Huang et al, 2004). Then we collected sub-
categorization frames for each keyword and created
patterns for the frames manually.
Each pattern consists of a syntactic pattern and
a semantic pattern. The syntactic patterns com-
ply with dependency structures. The system tries
to match the syntactic patterns to the dependency
structures of sentences in a bottom-up way, consid-
ering syntactic and semantic restrictions of syntac-
tic patterns. Once a syntactic pattern is successfully
matched to a sub-tree of the available dependency
structure, its corresponding semantic pattern is as-
signed to the sub-tree as one of its semantics. The
semantic patterns are combined according to the de-
pendency structures to form a compositional seman-
tic structure.
The system then performs logical inference over
the semantic structures by using handcrafted infer-
ence rules and extracts target information from the
results of the inference. We have manually created
28 inference rules that reflect the knowledge of the
gene regulation domain. Only relations where the
identified agent is one of those known TFs are kept,
while all others are discarded.
3.2 Generic, ML-based Extraction
Apart from the already mentioned clerical pre-
processing steps, the ML-based extraction of GREs
requires several additional syntactic processing
steps including POS-tagging, chunking, and full
dependency- and constituency-based parsing.4
Entity Identification. To identify gene names in
the documents, we applied GENO, a multi-organism
gene name recognizer and normalizer (Wermter
et al, 2009) which achieved a top-rank perfor-
mance of 86.4% on the gene normalization task
of BIOCREATIVE-II. GENO recognizes gene men-
tions by means of an ML-based named entity tag-
ger trained on publicly available corpora. Subse-
quently, it attempts to map all identified mentions to
organism-specific UNIPROT5 identifiers. Mentions
that cannot be mapped are discarded; only success-
fully mapped mentions are kept. We utilized GENO
in its original version, i.e., without special adjust-
ments to the E. coli organism. However, only those
mentions detected to be genes of E. coli were fed
into the relation extraction component.
4These tasks were performed with the OPENNLP tools
(http://opennlp.sourceforge.net/) and the
MST parser (http://sourceforge.net/projects/
mstparser), both retrained on biomedical corpora.
5http://www.uniprot.de
39
Relation Identification. The ML-based approach
to GRE employs Maximum Entropy models and
constitutes and extension of the system proposed by
Buyko et al (2008) as it also makes use of depen-
dency parse information including dependency tree
level features (Katrenko and Adriaans, 2006) and
shortest dependency path features (Kim et al, 2008).
In short, the feature set consists of:
? word features (covering words before, after and
between both entity mentions);
? entity features (accounting for combinations of
entity types, flags indicating whether mentions
have an overlap, and their mention level);
? chunking and constituency-based parsing fea-
tures (concerned with head words of the
phrases between two entity mentions; this class
of features exploits constituency-based parsing
as well and indicates, e.g., whether mentions
are in the same NP, PP or VP);
? dependency parse features (analyzing both the
dependency levels of the arguments as dis-
cussed by Katrenko and Adriaans (2006) and
dependency path structure between the argu-
ments as described by Kim et al (2008));
? and relational trigger (key)words (accounting
for the connection of trigger words and men-
tions in a full parse tree).
An advantage of ML-based systems is that they
allow for thresholding. To achieve higher recall
values for our system, we may set the confidence
threshold for the negative class (i.e., a pair of en-
tity mentions does not constitute a relation) to values
> 0.5. Clearly, this is at the cost of precision as the
system more readily assigns the positive class.
4 Intrinsic Evaluation of Feasibility
The following two sections aim at evaluating the
rule-based and ML-based GRE extraction systems.
The systems are first ?intrinsically? evaluated, i.e.,
in a cross-validation manner on corpora annotated
with respect to GREs. Second, in a more realistic
scenario, both systems were evaluated against REG-
ULONDB, a database collecting knowledge about
gene regulation in E. coli. This scenario tests which
part of manually accumulated knowledge about gene
regulation in E. coli can automatically be identified
by our systems and at what level of quality.
4.1 Rule-based system
Corpus. For the training and evaluation of the
rule-based system, we annotated 209 MEDLINE ab-
stracts with three types of events: specific events
of gene transcription regulation, general events of
gene expression regulation, and physical events of
binding of transcription factors to gene regulatory
regions. Strictly speaking, only the first type is rele-
vant to REGULONDB. However, biologists often re-
port gene transcription regulation events in the sci-
entific literature as if they are gene expression regu-
lation events, which is a generalization of gene tran-
scription regulation, or the binding event, which it-
self is insufficient evidence for gene transcription
regulation. The two latter types may indicate that
the full-texts contain evidence of the first type.
We asked two curators to annotate the abstracts.
Curator A was trained with example annotations and
interactive discussions. Curator B was trained only
with example annotations and guidelines. For cross-
checking of annotations, we asked them to annotate
an unseen corpus of 97 abstracts and found that Cu-
rator A made 10.8% errors, misjudging three event
additions and, in the other 14 errors, mistaking in
annotating event types, event attributes, and pas-
sage boundaries, while Curator B made 32.4% er-
rors as such. This result indicates that the annotation
of GREs requires intensive and interactive training.
The curators have discussed and agreed on the final
release of the corpora.6
Results. The system has successfully extracted 79
biologically meaningful events among them (21.1%
recall) and incorrectly produced 15 events (84.0%
precision) which constitutes an overall F-score of
33.6%. Among the 79 events, the system has cor-
rectly identified event types of 39 events (49.4% pre-
cision), polarity of 46 events (58.2% precision), and
directness of 51 events (64.6% precision). Note that
the system employed a fully automatic module for
named entity recognition. The event type recogni-
tion is impaired, because it often fails to recognize
6The resultant annotated corpora are available at http://
www.ebi.ac.uk/?kim/eventannotation/.
40
the specific event type of transcription regulation,
but only identifies the general event type of gene ex-
pression regulation due to the lack of identified evi-
dence.
4.2 ML-based system
GeneReg corpus. The GENEREG corpus (Buyko
et al, 2008) constitutes a selection of 314 MED-
LINE abstracts related to gene regulation in E. coli.
These abstracts were randomly drawn from a set of
32,155 selected by MESH term queries from MED-
LINE using keywords such as Escherichia coli, Gene
Expression and Transcription Factors. These 314
abstracts were manually annotated for named enti-
ties involved in gene regulatory processes (such as
transcription factor, including co-factors and regu-
lators, and genes) and pairwise relations between
transcription factors (TFs) and genes, as well as trig-
gers (e.g., clue verbs) essential for the description of
gene regulation relations. As for the relation types,
the GENEREG corpus distinguishes between (a) un-
specified regulation of gene expression, (b) positive,
and (c) negative regulation of gene expression. Out
of the 314 abstracts a set of 65 were randomly se-
lected and annotated by a second annotator to iden-
tify inter-annotator agreement (IAA) values. For the
task of correct identification of the pair of interacting
named entities in gene regulation processes, an IAA
of 78.4% (R), 77.3% (P ), 77.8% (F) was measured ,
while 67% (R), 67.9% (P), 67.4% (F) were achieved
for the identification of interacting pairs plus the 3-
way classification of the interaction relation.
Experimental Setting. The ML-based extraction
system merges all of the above mentioned three
types (unspecific, negative and positive) into one
common type ?relation of gene expression?. So, it
either finds that there is a relation of interest be-
tween a pair of gold entity mentions or not. We
evaluated our system by a 5-fold cross-validation on
the GENEREG corpus. The fold splits were done
on the abstract-level to avoid the otherwise unrealis-
tic scenario where a system is trained on sentences
from an abstract and evaluated on other sentences
but from the same abstract (Pyysalo et al, 2008).
As our focus here is only on the performance of the
GRE extraction component, gold entity mentions as
annotated in the respective corpus were used.
Results. For the experimental settings given
above, the system achieved an F-score of 42% with
a precision of 59% and a recall of 33%. Increasing
the confidence threshold for the negative class in-
creases recall as shown for two different thresholds
in Table 1. As expected this is at the cost of preci-
sion. It shows, that using an extremely high thresh-
old of 0.95 results in a dramatically increased recall
of 73% compared to 33% with the default threshold.
Although at the cost of diminished precision of 32%
compared to originally 59%, the lifted threshold in-
creases the overall F-score (44%) by 2 points.
threshold R P F
default (0.5) 0.33 0.59 0.42
0.80 0.54 0.43 0.48
0.95 0.73 0.32 0.44
Table 1: Different confidence thresholds for the ML-
based system achieved by intrinsic evaluation
5 Extrinsic Evaluation of Robustness
REGULONDB is the primary and largest reference
database providing manually curated knowledge of
the transcriptional regulatory network of E. coli
K12. On K12, approximately for one-third of K12?s
genes, information about their regulation is avail-
able. REGULONDB is updated with content from
recent research papers on this issue. While REG-
ULONDB contains much more, for this paper our
focus was solely on REGULONDB?s information
about gene regulation events in E. coli. In the fol-
lowing, the term REGULONDB refers to this part of
the REGULONDB database. REGULONDB includes
e.g., the following information for each regulation
event: regulatory gene (the ?agent? in such an event,
a transcription factor), the regulated gene (the ?pa-
tient?), the regulatory effect on the regulated gene
(activating, suppression, dual, unknown), and evi-
dence that supports the existence of the regulatory
interaction.
Evaluation against REGULONDB constitutes a
real-life scenario. Thus, the complete extraction sys-
tems were run, including gene name recognition and
normalization as well as relation detection. Hence,
the systems? overall recall values are highly affected
by the gene name identification. REGULONDB is
here taken as a ?true? gold standard and thus as-
41
sumed to be correct and exhaustive with respect to
the GREs contained. As, however, every manu-
ally curated database is likely to be incomplete and
might contain some errors, we supplement our eval-
uation against REGULONDB with a manual analy-
sis of false positives errors caused by our system (cf.
Section 5.4).
5.1 Evaluation Scenario and Experimental
Settings
To evaluate our extraction systems against REG-
ULONDB we first processed a set of input docu-
ments (see below), collected all unique gene reg-
ulation events extracted and compared this set of
events against the full set of known events in REG-
ULONDB. A true positive (TP) hit is obtained, when
an event found automatically corresponds to one in
REGULONDB, i.e., having the same agent and pa-
tient. The type of regulation is not considered. A
false positive (FP) hit is counted, if an event was
found which does not occur in the same way in
REGULONDB, i.e., either patient or agent (or both)
are wrong. False negatives (FN) are those events
covered by REGULONDB but not found by a sys-
tem automatically. From these hit values, standard
precision, recall, and F-score values are calculated.
Of course, the systems? performance largely depend
on the size of the base corpus collection processed.
Thus, for both systems and all three document sets
we got separate performance scores.
Table 2 gives an overview to the document col-
lections used for evaluating the robustness of our
systems: The ?ecoli-tf? variants are documents fil-
tered both with E. coli TF names and with relevance
to E. coli. Abstracts are taken from Medline cita-
tions, while full texts are from a corpus of different
biomedical journals. The third document set, ?regu-
lon ra?, is a set containing abstracts from the REG-
ULONDB references.
name type # documents
ecoli-tf.abstracts abstract 4,347
ecoli-tf.fulltext full texts 1,812
regulon ra abstracts 2,704
Table 2: Document sets for REGULONDB evaluation
5.2 Rule-based-System
Table 3 shows the evaluation results of the rule-
based system against REGULONDB. Though the
system distinguishes the three types of events, we
have considered them all as events of gene tran-
scription regulation for the evaluation. For instance,
the system has extracted 718 unique events with
single-unit participants (i.e., excluding operons), not
considering event types and attributes (e.g., polar-
ity), from the ?ecoli-tf.fulltext? corpus. Among the
events, 347 events are found in Regulon (9.7% re-
call, 48.3% precision). If we only consider the
events that are specifically identified as gene tran-
scription regulation, the system has extracted 379
unique events among which 201 are also found in
Regulon (5.6% recall, 53.0% precision).
participant document set R P F
single-unit ecoli-tf.abstracts 0.09 0.60 0.15
multi-unit ecoli-tf.abstracts 0.24 0.61 0.34
single-unit ecoli-tf.fulltext 0.10 0.48 0.16
multi-unit ecoli-tf.fulltext 0.25 0.49 0.33
single-unit regulon ra 0.07 0.73 0.13
multi-unit regulon ra 0.18 0.70 0.28
Table 3: Results of evaluation against REGULONDB of
rule-based system.
When we split multi-unit participants into individ-
ual genes, the rule-based system shows better per-
formance, as shown in Table 3 with the participant
type ?multi-unit?. This may indicate that the gene
regulatory events of E. coli are often described as
interactions of operons. At best, the system shows
34% F-score with the ?ecoli-tf.abstracts? corpus.
5.3 ML-based System
The ML-based system was designed to recognize
all types of gene regulation events. REGULONDB,
however, contains only the subtype, i.e., regulation
of transcription. Thus, the ML-based system was
evaluated against REGULONDB in two modes: by
default, all events extracted by the systems are con-
sidered; in the ?TF-filtered? mode, only relations
with an agent from the list of all known TFs in E.
coli are considered (as done for the rule-based sys-
tem by default). Thus, comparing to the rule-based
system, only the results obtained in the ?TF-filtered?
mode should be considered.
42
5.3.1 Raw performance scores
The results for the ML-based system are shown in
Table 4. Recall values here range between 7 and
10%, while precision is between 29 and 78% de-
pending on both the document set as well as the
application of the TF filter. The low recall of the
ML-based system is partially due to the fact that the
system does not recognize multi-gene object names
(e.g., ?acrAB?), in this configuration the recall is
similar to the recall of the rule-based system in a
?single-unit modus? (see Table 3).
mode document set R P F
TF-filtered ecoli-tf.abstracts 0.09 0.70 0.16
default ecoli-tf.abstracts 0.09 0.45 0.15
TF-filtered ecoli-relevant.fulltext 0.10 0.54 0.17
default ecoli-relevant.fulltext 0.10 0.29 0.15
TF-filtered regulon ra 0.07 0.78 0.13
default regulon ra 0.07 0.47 0.12
Table 4: Results of evaluation against REGULONDB of
ML-based system
As already shown in the intrinsic evaluation,
application of different confidence thresholds in-
creases the recall of the ML-based system. This was
also done for the evaluation against REGULONDB.
Table 5 shows the impact of increased confidence
thresholds for the negative class on the ?regulon ra?
set for the ?TF-filtered? evaluation mode. Given an
extremely high threshold of 0.95, the recall is in-
creased from 7 to 11% which constitutes a relative
increase of over 60%. Precision obviously drops,
however, the overall F-score has improved from 13
to 19%. These results emphasize that an ML-based
system has an important handle which allows to ad-
just recall according to the application needs.
threshold R P F
default (0.5) 0.07 0.78 0.13
0.8 0.09 0.70 0.16
0.95 0.11 0.63 0.19
Table 5: Different confidence thresholds for the ML-
based system tested on the ?regulon ra? set
5.4 Manual analysis of false positives
REGULONDB was taken as an absolute gold stan-
dard in this evaluation. If a system correctly extracts
an event which is not contained in REGULONDB
for some reason, this constitutes a FP. Moreover, all
kinds of error (e.g., agent and patient mixed up) were
subsumed as FP errors. To analyze the cause and
distribution of FPs in more detail, a manual analysis
of the FP errors was performed and original FP hits
were assigned to one out of four FP error categories:
Cat1: Not a GRE This is really an FP error, as the
extracted relation does not at all constitute a
gene regulation event.
Cat2a: GRE but other than transcription
Unlike REGULONDB which contains only one
subtype of GREs, namely transcriptions, the
ML-based system identifies all kinds of GREs.
Therefore, the ML-based system clearly
identifies events which cannot be contained in
REGULONDB and, therefore, are not really
FPs.
Cat 3: Partially correct transcription event This
category deals with incorrect arguments of
GREs. We distinguish three types of FPs: (a)
the patient and the agent role are interchanged,
(b) the patient is wrong, while the agent is
right, and (c) the agent is wrong, while the
patient is right. In all these three cases, though
errors were committed human curators might
find the partially incorrect information useful
to speed up the curation process.
Cat4: Relation missing in REGULONDB Those
are relations which should be contained in
REGULONDB but are missing for some
reason. The agent is a correctly identified
transcription factor and the sentence contains
a mention of a transcription event. There are
several reasons why this relation was not found
in REGULONDB as we will discuss in the
following.
Table 6 shows the results of the manual FP anal-
ysis of the ML-based system (no TF filter applied)
on the ?ecoli-tf-abstracts? and ?ecoli-tf-fulltexts?.
It shows that the largest source of error is due
to Cat1, i.e., an identified relation is completely
wrong. As fulltext documents are generally more
complex, the relative amount of this kind of errors
is higher here than on abstracts (54.5 % compared
43
category abstracts (%) fulltexts (%)
Cat 1 44.5 54.5
Cat 2 11.2 10.9
Cat 3a 3.8 3.9
Cat 3b 8.5 4.4
Cat 3c 8.2 5.4
Cat 4 23.8 21.0
Table 6: Manual analysis of false positive errors (FP).
Percentages of FPs by category are reported on ?ecoli-tf-
abstracts? and ?ecoli-tf-fulltexts?
to 44.5 %). However, on abstracts and fulltexts, a
bit more than 10 % of the FP are because the sys-
tem found too general GREs which, by definition,
are not contained in REGULONDB (Cat2). Iden-
tified GREs that were partially correct constitute
20.5 % (abstracts) or 13.7 % (fulltexts) of the FP er-
rors (Cat3).
Finally, 23.8% and 21.0% of the FPs for abstracts
and fulltext, respectively, are correct transcription
events but could not be found in REGULONDB
(Cat4). This is due to several reasons. For instance,
identified gene names were incorrectly normalized
so that they could not be found in REGULONDB,
REGULONDB curators have not yet added a relation
or simply overlooked it; relations are correctly iden-
tified as such in the narrow context of a paragraph of
a document but were actually of speculative nature
only (this includes relations whose status is unsure,
often indicated by ?likely? or ?possibly?).
Summarizing, the manual FP analysis shows that
about 50% of all FPs are not completely erroneous.
These numbers must clearly be kept in mind when
interpreting the raw numbers (especially for preci-
sion) reported on in the previous subsection.
5.5 Integration of text mining results
We have integrated the results of the two different
text mining systems and found that both systems are
complementary to each other such that their result
sets do not heavily overlap. For instance, from the
?ecoli-tf.abstract? corpus, the rule-based system ex-
tracts 992 events, while the ML-based system ex-
tracts 705 events. For the integration, we have con-
sidered only the events whose participants are as-
sociated with UNIPROT identifiers. Among the ex-
tracted events, only 285 events are extracted by both
systems. We might speculate that the overlapping
events are more reliable than the rest of the extracted
events. It also leaves 71.3% of the results from
the rule-based system and 59.6% of results from the
ML-based system as unique contributions from each
of the approaches for the integration.
6 Conclusions
We have explored a rule-based and a machine
learning-based approach to the automatic extrac-
tion of gene regulation events. Both approaches
were evaluated under well-defined lab conditions us-
ing self-supplied corpora, and under real-life condi-
tions by comparing our results with REGULONDB,
a well-curated reference data set. While the re-
sults for the first evaluation scenario are state of the
art, performance figures in the real-life scenario are
not so shiny (the best F-scores for the rule-based
and the ML-based system are on the order of 34%
and 19%, respectively). This holds, in particular,
for the comparison with the work of Rodr??guez-
Penagos et al (2007). Still, at least the ML-based
approach is much more general than the very specifi-
cally tuned manual rule set from Rodr??guez-Penagos
et al (2007) and has potential for increases in perfor-
mance. Also, this has been the first extra-mural eval-
uation of automatically generating content for REG-
ULONDB.
Still, the analysis of false positives reveals that
the strict criteria we applied for our evaluation may
appear in another light for human curators. Con-
founded agents and patients (21% on the abstracts,
14% on full texts) and information not contained in
REGULONDB (24% on the abstracts, 21% on full
texts) might be useful from a heuristic perspective to
focus on interesting data during the curation process.
Acknowledgements
This work was funded by the EC within the BOOT-
Strep (FP6-028099) and the CALBC (FP7-231727)
projects. We want to thank Tobias Wagner (Centre
for Molecular Biomedicine, FSU Jena) for perform-
ing the manual FP analysis.
44
References
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the 3rd Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), pages 21?28.
Francis Collins, Eric Green, Alan Guttmacher, and Mark
Guyer. 2003. A vision for the future of genomics re-
search. Nature, 422(6934 (24 Feb)):835?847.
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction pat-
terns from published text articles. International Jour-
nal of Medical Informatics, 67:19?32.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and P. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency trees.
In KDECB 2006 ? Knowledge Discovery and Emer-
gent Complexity in Bioinformatics, pages 61?80.
Jung-jae Kim and Jong C. Park. 2004. BioIE: retar-
getable information extraction and ontological anno-
tation of biological interactions from the literature.
Journal of Bioinformatics and Computational Biology,
2(3):551?568.
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Learning
language in logic - genic interaction extraction LLL?
2005, pages 31?37.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, 9(3), April.
Carlos Rodr??guez-Penagos, Heladia Salgado, Irma
Mart??nez-Flores, and Julio Collado-Vides. 2007. Au-
tomatic reconstruction of a bacterial regulatory net-
work using natural language processing. BMC Bioin-
formatics, 8(293).
Kenji Sagae, Yusuke Miyao, and Junichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Annual Meeting of Association for Computational
Linguistics, pages 624?631.
Jasmin S?aric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Hui Yang, Goran Nenadic, and John Keane. 2008. Iden-
tification of transcription factor contexts in literature
using machine learning approaches. BMC Bioinfor-
matics, 9(Supplement 3: S11).
45
Proceedings of the Workshop on BioNLP: Shared Task, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Event Extraction from Trimmed Dependency Graphs
Ekaterina Buyko, Erik Faessler, Joachim Wermter and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
{ekaterina.buyko|erik.faessler|joachim.wermter|udo.hahn}@uni-jena.de
Abstract
We describe the approach to event extrac-
tion which the JULIELab Team from FSU
Jena (Germany) pursued to solve Task 1 in
the ?BioNLP?09 Shared Task on Event Ex-
traction?. We incorporate manually curated
dictionaries and machine learning method-
ologies to sort out associated event triggers
and arguments on trimmed dependency graph
structures. Trimming combines pruning ir-
relevant lexical material from a dependency
graph and decorating particularly relevant lex-
ical material from that graph with more ab-
stract conceptual class information. Given
that methodological framework, the JULIELab
Team scored on 2nd rank among 24 competing
teams, with 45.8% precision, 47.5% recall and
46.7% F1-score on all 3,182 events.
1 Introduction
Semantic forms of text analytics for the life sciences
have long been equivalent with named entity recog-
nition and interpretation, i.e., finding instances of se-
mantic classes such as proteins, diseases, or drugs.
For a couple of years, this focus has been comple-
mented by analytics dealing with relation extraction,
i.e., finding instances of relations which link one or
more (usually two) arguments, the latter being in-
stances of semantic classes, such as the interaction
between two proteins (PPIs).
PPI extraction is a complex task since cascades
of molecular events are involved which are hard to
sort out. Many different approaches have already
been tried ? pattern-based ones (e.g., by Blaschke
et al (1999), Hakenberg et al (2005) or Huang et
al. (2004)), rule-based ones (e.g., by Yakushiji et al
(2001), ?Saric? et al (2004) or Fundel et al (2007)),
and machine learning-based ones (e.g., by Katrenko
and Adriaans (2006), S?tre et al (2007) or Airola et
al. (2008)), yet without conclusive results.
In the following, we present our approach to solve
Task 1 within the ?BioNLP?09 Shared Task on Event
Extraction?.1 Task 1 ?Event detection and charac-
terization? required to determine the intended rela-
tion given a priori supplied protein annotations. Our
approach considers dependency graphs as the cen-
tral data structure on which various trimming oper-
ations are performed involving syntactic simplifica-
tion but also, even more important, semantic enrich-
ment by conceptual overlays. A description of the
component subtasks is provided in Section 2, while
the methodologies intended to solve each subtask
are discussed in Section 3. The system pipeline for
event extraction reflecting the task decomposition is
described in Section 4, while Section 5 provides the
evaluation results for our approach.
2 Event Extraction Task
Event extraction is a complex task that can be sub-
divided into a number of subtasks depending on
whether the focus is on the event itself or on the ar-
guments involved:
Event trigger identification deals with the large
variety of alternative verbalizations of the same
event type, i.e., whether the event is expressed in
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
19
a verbal or in a nominalized form (e.g., ?A is ex-
pressed? and ?the expression of A? both refer to the
same event type, viz. expression(A)). Since the
same trigger may stand for more than one event type,
event trigger ambiguity has to be resolved as well.
Event trigger disambiguation selects the correct
event name from the set of alternative event triggers.
Event typing, finally, deals with the semantic
classification of a disambiguated event name and the
assignment to an event type category.2
Argument identification is concerned with find-
ing all necessary participants in an event, i.e., the
arguments of the relation.
Argument typing assigns the correct semantic
category (entity class) to each of the determined par-
ticipants in an event (which can be considered as in-
stances of that class).
Argument ordering assigns each identified par-
ticipant its functional role within the event, mostly
Agent (and Patient/Theme).
The sentence ?Regulation of jun and fos gene ex-
pression in human monocytes by the macrophage
colony-stimulating factor?, e.g., contains mentions
of two Gene Expression events with respective
THEME arguments ?jun? and ?fos?, triggered in the
text by the literal phrase ?gene expression?.
Task 1 of the ?BioNLP?09 Shared Task on Event
Extraction? was defined in such a way as to iden-
tify a proper relation (event) name and link it with
its type, plus one or more associated arguments de-
noting proteins. To focus on relation extraction only
no automatic named entity recognition and interpre-
tation had to be performed (subtask ?argument typ-
ing? from above); instead candidate proteins were
already pre-tagged. The complexity of Task 1 was
raised by the condition that not only proteins were
allowed to be arguments but also were events.
3 Event Extraction Solution
Our event extraction approach is summarized in Fig-
ure 1 and consists of three major streams ? first, the
detection of lexicalized event triggers (cf. Section
3.1), second, the trimming of dependency graphs
which involves pruning irrelevant and semantically
enriching relevant lexical material (cf. Section 3.2),
2In our approach, event trigger disambiguation already im-
plies event typing.
Pre-processing
Argument Identi f icat ion with
Ensemble of Classifiers
Event Detection
Post-processing
     Trimming of
Dependency Graphs
  Typing of Putative
    Event Triggers
Figure 1: General Architecture of the Event Extraction
Solution of the JULIELab Team.
and, third, the identification of arguments for the
event under scrutiny (cf. Section 3.3). Event typ-
ing results from proper event trigger identification
(see Section 3.1.2), which is interlinked with the out-
come of the argument identification. We talk about
putative triggers because we consider, in a greedy
manner, all relevant lexical items (see Section 3.1.1)
as potential event triggers which might represent an
event. Only those event triggers that can eventually
be connected to arguments, finally, represent a true
event. To achieve this goal we preprocessed both the
original training and test data such that we enrich the
original training data with automatically predicted
event triggers in order to generate more negative ex-
amples for a more effective learning of true events.3
3.1 Event Trigger Identification
Looking at the wide variety of potential lexicalized
triggers for an event, their lacking discriminative
power relative to individual event types and their
inherent potential for ambiguity,4 we decided on
a dictionary-based approach whose curation princi-
ples are described in Section 3.1.1. Our disambigua-
tion policy for the ambiguous lexicalized event trig-
3Although the training data contains cross-sentence event
descriptions, our approach to event extraction is restricted to
the sentence level only.
4Most of the triggers are neither specific for molecular event
descriptions, in general, nor for a special event type. ?Induc-
tion?, e.g., occurs 417 times in the training data. In 162 of these
cases it acts as a trigger for Positive regulation, 6 times as a
trigger for Transcription, 8 instances trigger Gene expression,
while 241 occurrences do not trigger an event at all.
20
gers assembled in this suite of dictionaries, one per
event type, is discussed in Section 3.1.2.
3.1.1 Manual Curation of the Dictionaries
We started collecting our dictionaries from the
original GENIA event corpus (Kim et al, 2008a).
The extracted event triggers were then automatically
lemmatized5 and the resulting lemmata were subse-
quently ranked by two students of biology according
to their predictive power to act as a trigger for a par-
ticular event type. This expert assessment led us to
four trigger groups (for each event type these groups
were determined separately):
(1) Triggers are important and discriminative for
a specific event type. This group contains event trig-
gers such as ?upregulate? for Positive regulation.
(2) Triggers are important though not fully dis-
criminative for a particular event type; yet, this defi-
ciency can be overcome by other lexical cues within
the context of the same sentence. This group with in-
context disambiguators contains lexical items such
as ?proteolyse? for Protein catabolism.
(3) Triggers are non-discriminative for an event
type and even cannot be disambiguated by linguistic
cues within the context of the same sentence. This
group contains lexical items such as ?presence? for
Localization and Gene expression.
(4) Triggers are absolutely non-discriminative for
an event. This group holds general lexical triggers
such as ?observe?, ?demonstrate? or ?function?.
The final dictionaries used for the detection of
putative event triggers are a union of the first two
groups. They were further extended by biologists
with additional lexical material of the first group.
The dictionaries thus became event type-specific ?
they contain all morphological forms of the original
lemma, which were automatically generated using
the Specialist NLP Tools (2008 release).
We matched the entries from the final set of dic-
tionaries with the shared task data using the Ling-
pipe Dictionary Chunker.6 After the matching pro-
cess, some cleansing had to be done.7
5We used the lemmatizer from the Specialist NLP Tools
(http://lexsrv3.nlm.nih.gov/SPECIALIST/
index.html, 2008 release).
6http://alias-i.com/lingpipe/
7Event triggers were removed which (1) were found within
sentences without any protein annotations, (2) occurred within
3.1.2 Event Trigger Disambiguation
Preliminary experiments indicated that the dis-
ambiguation of event triggers might be beneficial
for the overall event extraction results since events
tend to be expressed via highly ambiguous triggers.
Therefore, we performed a disambiguation step pre-
ceding the extraction of any argument structures.
It is based on the importance of an event trig-
ger ti for a particular event type T as defined by
Imp(tTi ) := f(t
T
i )
P
i f(tTi )
, where f(tTi ) is the frequency
of the event trigger ti of the selected event type T
in a training corpus divided by the total amount of
all event triggers of the selected event type T in
that training corpus. The frequencies are measured
on stemmed event triggers. For example, Imp for
the trigger stem ?depend? amounts to 0.013 for the
event type Positive regulation, while for the event
type Regulation it yields 0.036 . If a text span con-
tains several event triggers with the same span off-
set, the event trigger with max(Imp) is selected and
other putative triggers are discarded. The trigger
stem ?depend? remains thus only for Regulation.
3.2 Trimming Dependency Graphs
When we consider event (relation) extraction as a se-
mantic interpretation task, plain dependency graphs
as they result from deep syntactic parsing might not
be appropriate to directly extract semantic informa-
tion from. This is due to two reasons - they contain
a lot of apparently irrelevant lexical nodes (from the
semantic perspective of event extraction) and they
also contain much too specific lexical nodes that
might better be grouped and further enriched se-
mantically. Trimming dependency graphs for the
purposes of event extraction, therefore, amounts to
eliminate semantically irrelevant and to semantically
enrich relevant lexical nodes (i.e., overlay with con-
cepts). This way, we influence the final representa-
tion for the machine learners we employ (in terms of
features or kernel-based representations) ? we may
avoid an overfitting of the feature or kernel spaces
with syntactic and lexical data and thus reduce struc-
tural information in a linguistically motivated way.
a longer event trigger, (3) overlapped with a longer trigger of
the same event type, (4) occurred inside an entity mention an-
notation.
21
3.2.1 Syntactic Pruning
Pruning targets auxiliary and modal verbs which
govern the main verb in syntactic structures such as
passives, past or future tense. We delete the aux-
iliars/modals as govenors of the main verbs from
the dependency graph and propagate the semantics-
preserving dependency relations of these nodes di-
rectly to the main verbs. Adhering to the depen-
dency tree format and labeling conventions set up
for the 2006 and 2007 CONLL shared tasks on de-
pendency parsing main verbs are usually connected
with the auxiliar by the VC dependency relation (see
Figure 2). Accordingly, in our example, the verb
?activate? is promoted to the ROOT in the depen-
dency graph and governs all nodes that were origi-
nally governed by the modal ?may?.
Figure 2: Trimming of Dependency Graphs.
3.2.2 Conceptual Decoration
Lexical nodes in the (possibly pruned) depen-
dency graphs deemed to be important for argument
extraction were then enriched with semantic class
annotations, instead of keeping the original lexical
(stem) representation (see Figure 2). The rationale
behind this decision was to generate more powerful
kernel-based or features representations (see Section
3.3.2 and 3.3.1).
The whole process is based on a three-tier task-
specific semantic hierarchy of named entity classes.
The top rank is constituted by the equivalent classes
Transcription factor, Binding site, and Promoter.
The second rank is occupied by MESH terms, and
the third tier assembles the named entity classes
Gene and Protein. Whenever a lexical item is cat-
egorized by one of these categories, the associated
node in the dependency graph is overlaid with that
category applying the ranking in cases of conflicts.
We also enriched the gene name mentions with
their respective Gene Ontology Annotations from
GOA.8 For this purpose, we first categorized GO
terms both from the ?molecular function? and from
the ?biological process? branch with respect to
their matching event type, e.g., Phosphorylation
or Positive regulation. We then mapped all gene
name mentions which occurred in the text to their
UNIPROT identifier using the gene name normalizer
GENO (Wermter et al, 2009). This identifier links a
gene with a set of (curated) GO annotations.
In addition, we inserted semantic information in
terms of the event trigger type and the experimen-
tal methods. As far as experimental methods are
concerned, we extracted all instances of them an-
notated in the GENIA event corpus. One student
of biology sorted the experimental methods relative
to the event categories under scrutiny. For example
?affinity chromatography? was assigned both to the
Gene expression and to the Binding category. For
our purposes, we only included those GO annota-
tions and experimental methods which matched the
event types to be identified in a sentence.
3.3 Argument Identification and Ordering
The argument identification task can be subdivided
into three complexity levels. Level (1) incorpo-
rates five event types (Gene expression, Transcrip-
tion, Protein catabolism, Localization, Phosphory-
lation) which involve a single participant with a
THEME role only. Level (2) is concerned with one
event type (Binding) that provides an n-ary argument
structure where all arguments occupy the THEME(n)
role. Level (3) comprises three event types (Posi-
tive regulation, Negative regulation, or an unspeci-
fied Regulation) that represent a regulatory relation
between the above-mentioned event classes or pro-
teins. These events have usually a binary structure,
with a THEME argument and a CAUSE argument.
For argument extraction, we built sentence-wise
pairs of putative triggers and their putative argu-
ment(s), the latter involving ontological informa-
tion about the event type. For Level (1), we built
pairs only with proteins, while for Level (3) we al-
8http://www.ebi.ac.uk/GOA
22
lowed all events as possible arguments. For Level
(2), Binding events, we generated binary (trigger,
protein) pairs as well as triples (trigger, protein1,
protein2) to adequately represent the binding be-
tween two proteins.9 Pairs of mentions not con-
nected by a dependency path could not be detected.
For the argument extraction we chose two ma-
chine learning-based approaches, feature-based and
a kernel-based one, as described below.10
3.3.1 Feature-based Classifier
We distinguished three groups of features. First,
lexical features (covering lexical items before, af-
ter and between both mentions (of the event trigger
and an argument) as described by Zhou and Zhang
(2007)); second, chunking features (concerned with
head words of the phrases between two mentions as
described by Zhou and Zhang (2007)); third, de-
pendency parse features (considering both the se-
lected dependency levels of the arguments (parents
and least common subsumer) as discussed by Ka-
trenko and Adriaans (2006), as well as a shortest de-
pendency path structure between the arguments as
used by Kim et al (2008b) for walk features).
For the feature-based approach, we chose the
Maximum Entropy (ME) classifier from MALLET.11
3.3.2 Graph Kernel Classifier
The graph kernel uses a converted form of depen-
dency graphs in which each dependency node is rep-
resented by a set of labels associated with that node.
The dependency edges are also represented as nodes
in the new graph such that they are connected to the
nodes adjacent in the dependency graph. Subgraphs
which represent, e.g., the linear order of the words
in the sentence can be added, if required. The entire
graph is represented in terms of an adjacency matrix
which is further processed to contain the summed
weights of paths connecting two nodes of the graph
(see Airola et al (2008) for details).
9We did not account for the binding of more than two pro-
teins as this would have led to a combinatory explosion of pos-
sible classifications.
10In our experiments, we used full conceptual overlaying
(see Section 3.2) for the kernel-based representation and partial
overlaying for the dependency parse features (only gene/protein
annotation was exploited here). Graph representations allow for
many semantic labels to be associated with a node.
11http://mallet.cs.umass.edu/index.php/
Main_Page
Figure 3: Graph Kernel Representation for a Trimmed
Dependency Graph ? (1) original representation, (2)
representation without graph dependency edge nodes
(weights (0.9, 0.3) taken from Airola et al (2008)).
For our experiments, we tried some variants of the
original graph kernel. In the original version each
dependency graph edge is represented as a node.
That means that connections between graph token
nodes are expressed through graph dependency edge
nodes (see Figure 3; (1)). To represent the connec-
tions between original tokens as direct connections
in the graph, we removed the edge nodes and each
token was assigned the edge label (its dependency
label; see Figure 3; (2)). Further variants included
encodings for (1) the shortest dependency path (sp)
between two mentions (argument and trigger)12 (2)
the complete dependency graph (sp-dep), and (3) the
complete dependency graph and linear information
(sp-dep-lin) (the original configuration from Airola
et al (2008)).
For the graph kernel, we chose the LibSVM
(Chang and Lin, 2001) Support Vector Machine as
classifier.
3.4 Postprocessing
The postprocessing step varies for the three different
Levels (see Section 3.3). For every event trigger of
Level (1) (e.g., Gene expression), we generate one
event per relation comprising a trigger and its argu-
ment. For Level (2) (Binding), we create a Binding
event with two arguments only for triples (trigger,
protein1, protein2). For the third Level, we create
for each event trigger and its associated arguments
e = n ? m events, for n CAUSE arguments and m
THEME arguments.
12For Binding we extracted the shortest path between two
protein mentions if we encounter a triple (trigger, protein1,
protein2).
23
4 Pipeline
The event extraction pipeline consists of two ma-
jor parts, a pre-processor and the dedicated event
extractor. As far as pre-processing is concerned,
we imported the sentence splitting, tokenization and
GDep parsing results (Sagae and Tsujii, 2007) as
prepared by the shared task organizers for all data
sets (training, development and test). We processed
this data with the OpenNLP POS tagger and Chun-
ker, both re-trained on the GENIA corpus (Buyko et
al., 2006). Additionally, we enhanced the original
tokenization by one which includes hyphenization
of lexical items such as in ?PMA-dependent?. 13
The data was further processed with the gene nor-
malizer GENO(Wermter et al, 2009) and a num-
ber of regex- and dictionary-based entity taggers
(covering promoters, binding sites, and transcrip-
tion factors). We also enriched gene name men-
tions with their respective Gene Ontology annota-
tions (see Section 3.2.2). The MESH thesaurus (ex-
cept chemical and drugs branch) was mapped on the
data using the Lingpipe Dictionary Chunker.14
After preprocessing, event extraction was started
distinguishing between the event trigger recognition
(cf. Section 3.1), the trimming of the dependency
graphs (cf. Section 3.2), and the argument extrac-
tion proper (cf. Section 3.3).15 We determined in
our experiments on the development data the perfor-
mance of every classifier type and its variants (for
the graph kernel), and of ensembles of the most per-
formant (F-Score) graph kernel variant and an ME
model.16 We present here the argument extraction
configuration used for the official run.17 For the
prediction of Phosphorylation, Localization, Pro-
tein catabolism types we used the graph kernel in
13This tokenization is more advantageous for the detection
of additional event triggers as it allows to generate depen-
dency relations from hyphenated terms. For example, in ?PMA-
dependent?, ?PMA? will be a child of ?dependent? linked by
the AMOD dependency relation, and ?dependent? receives the
original dependency relation of the ?PMA-dependent? token.
14http://alias-i.com/lingpipe/
15For the final configurations of the graph kernel, we opti-
mized the C parameter in the spectrum between 2?3 and 23 on
the final training data for every event type separately.
16In the ensemble configuration we built the union of positive
instances.
17We achieved with this configuration the best performance
on the development set.
its ?sp without dependency-edge-nodes? configura-
tion, while for the prediction of Transcription and
Gene expression events we used an ensemble of the
graph kernel in its ?sp with dependency-edge-nodes?
variant, and an ME model. For the prediction of
Binding we used an ensemble of the graph kernel
(?sp-dep with dependency-edge-nodes?) and an ME
model. For the prediction of regulatory events we
used ME models for each regulatory type.
5 Results
The baseline against which we compared our ap-
proach can be captured in a single rule. We extract
for every pair of a putative trigger and a putative ar-
gument the shortest dependency path between them.
If the shortest dependency path does not contain any
direction change, i.e., the argument is either a direct
child or a direct parent of the trigger, and if the path
does not contain any other intervening event trig-
gers, the argument is taken as the THEME role.
We performed evaluations on the shared task de-
velopment and test set. Our baseline achieved com-
petitive results of 36.0% precision, 34.0% recall,
35.0% F-score on the development set (see Table
1), and 30.4% precision, 35.7% recall, 32,8% F-
score on the test set (see Table 2). In particular
the one-argument events, i.e., Gene expression, Pro-
tein catabolism, Phosphorylation are effectively ex-
tracted with an F-score around 70.0%. More com-
plex events, in particular events of Level (3), i.e.,
(Regulation) were less properly dealt with because
of their strong internal complexity.
Event Class gold recall prec. F-score
Localization 53 75.47 30.30 43.24
Binding 248 33.47 20.80 25.66
Gene expression 356 76.12 75.07 75.59
Transcription 82 68.29 40.58 50.91
Protein catabolism 21 76.19 66.67 71.11
Phosphorylation 47 76.60 72.00 74.23
Regulation 169 14.20 15.09 14.63
Positive regulation 617 15.40 20.83 17.71
Negative regulation 196 11.73 13.22 12.43
TOTAL 1789 36.00 34.02 34.98
Table 1: Baseline results on the shared task development
data. Approximate Span Matching/Approximate Recur-
sive Matching.
24
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 42.53 44.85 43.66 174 42.53 44.85 43.66
Binding 347 32.28 37.09 34.51 398 44.22 58.28 50.29
Gene expression 722 61.36 80.55 69.65 722 61.36 80.55 69.65
Transcription 137 39.42 35.06 37.11 137 39.42 35.06 37.11
Protein catabolism 14 71.43 66.67 68.97 14 71.43 66.67 68.97
Phosphorylation 135 65.93 90.82 76.39 135 65.93 90.82 76.39
EVT-TOTAL 1529 51.14 60.90 55.60 1580 53.54 65.89 59.08
Regulation 291 9.62 11.72 10.57 338 9.17 12.97 10.75
Positive regulation 983 10.38 11.33 10.83 1186 14.67 19.33 16.68
Negative regulation 379 14.25 19.22 16.36 416 14.18 21.00 16.93
REG-TOTAL 1653 11.13 12.96 11.98 1940 13.61 18.59 15.71
ALL-TOTAL 3182 30.36 35.72 32.82 3520 31.53 41.05 35.67
Table 2: Baseline results on the shared task test data. Approximate Span Matching/Approximate Recursive Matching
(columns 3-5). Event decomposition, Approximate Span Matching/Approximate Recursive Matching (columns 7-9).
The event extraction approach, in its final config-
uration (see Section 4), achieved a performance of
50.4% recall, 45.8% precision and 48.0% F-score on
the development set (see Table 4), and 45.8% recall,
47.5% precision and 46.7% F-score on the test set
(see Table 3). This approach clearly outperformed
the baseline with an increase of 14 percentage points
on the test data. In particular, the events of Level (2)
and (3) were more properly dealt with than by the
baseline. In the event decomposition mode (argu-
ment detection is evaluated in a decomposed event)
we achieved a performance of 49.4% recall, 56.2%
precision, and 52.6% F-score (see Table 3).
Our experiments on the development set showed
that the combination of the feature-based and the
graph kernel-based approach can boost the results up
to 6 percentage points F-score (for the Binding event
type). It is interesting that the combination for Bind-
ing increased recall without dropping precision. The
original graph kernel approach for Binding events
performs with 38.3% recall, 27.9% precision and
32.3% F-score on the development set. The com-
bined approach comes with a remarkable increase
of 14 percentage points in recall. The combination
could also boost the recall of the Gene expression
and Transcription by 15 percentage points and 5 per-
centage points, respectively, without seriously drop-
ping the precision (4 points for every type). For
the other event types, no improvements were found
when we combined both approaches.
5.1 Error Discussion
One expert biologist analyzed 30 abstracts randomly
extracted from the development error data. We de-
termined seven groups of errrors based on this anal-
ysis. The first group contains examples for which
an event should be determined, but a false argument
was found (e.g., Binding arguments were not prop-
erly sorted, or correct and false arguments were de-
tected for the same trigger) (44 examples). The sec-
ond group comprised examples where no trigger was
found (23 examples). Group (3) stands for cases
where no events were detected although a trigger
was properly identified (14 examples). Group (4)
holds examples detected in sentences which did not
contain any events (12 examples). Group (5) lists bi-
ologically meaningful analyses, actually very close
to the gold annotation, especially for the cascaded
regulatory events (12 examples), while Group (6) in-
corporates examples of a detected event with incor-
rect type (1 example). Group (7) gathers misleading
gold annotations (10 examples).
This assessment clearly indicates that a major
source of errors can be traced to the level of argu-
ment identification, in particular for Binding events.
The second major source has its offspring at the
level of trigger detection (we ignored, for exam-
ple, triggers such as ?in the presence of ?, ?when?,
?normal?). About 10% of the errors are due to a
slight difference between extracted events and gold
events. For example, in the phrase ?role for NF-
kappaB in the regulation of FasL expression? we
25
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 43.68 77.55 55.88 174 43.68 77.55 55.88
Binding 347 49.57 35.25 41.20 398 63.57 54.88 58.91
Gene expression 722 64.82 80.27 71.72 722 64.82 80.27 71.72
Transcription 137 35.77 62.03 45.37 137 35.77 62.03 45.37
Protein catabolism 14 78.57 84.62 81.48 14 78.57 84.62 81.48
Phosphorylation 135 76.30 91.15 83.06 135 76.30 91.15 83.06
EVT-TOTAL 1529 57.49 63.97 60.56 1580 60.76 71.27 65.60
Regulation 291 31.27 30.13 30.69 338 35.21 37.54 36.34
Positive regulation 983 34.08 37.18 35.56 1186 40.64 49.33 44.57
Negative regulation 379 40.37 31.16 35.17 416 42.31 39.11 40.65
REG-TOTAL 1653 35.03 34.18 34.60 1940 40.05 44.55 42.18
ALL-TOTAL 3182 45.82 47.52 46.66 3520 49.35 56.20 52.55
Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate
Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match-
ing/Approximate Recursive Matching (columns 7-9).
could not extract the gold event Regulation of Regu-
lation (Gene expression (FasL)) associated with the
trigger ?role?, but we were able to find the (inside)
event Regulation (Gene expression (FasL)) associ-
ated with the trigger ?regulation?. Interestingly, the
typing of events is not an error source in spite of
the simple disambiguation approach. Still, our dis-
ambiguation strategy is not appropriate for the anal-
ysis of double-annotated triggers such as ?overex-
pression?, ?transfection?, etc., which are annotated
as Gene expression and Positive regulation and are
a major source of errors in Group (2). As Group
(6) is an insignificant source of errors in our ran-
domly selected data, we focused our error analysis
on the especially ambiguous event type Transcrip-
tion. We found from 34 errors that 14 of them were
due to the disambiguation strategy (in particular for
triggers ?(gene) expression? and ?induction?).
6 Conclusion
Our approach to event extraction incorporates man-
ually curated dictionaries and machine learning
methodologies to sort out associated event triggers
and arguments on trimmed dependency graph struc-
tures. Trimming combines pruning irrelevant lexi-
cal material from a dependency graph and decorat-
ing particularly relevant lexical material from that
graph with more abstract conceptual class informa-
tion. Given that methodological framework, the
JULIELab Team scored on 2nd rank among 24 com-
Event Class gold recall prec. F-score
Localization 53 71.70 74.51 73.08
Binding 248 52.42 29.08 37.41
Gene expression 356 75.28 81.46 78.25
Transcription 82 60.98 73.53 66.67
Protein catabolism 21 90.48 79.17 84.44
Phosphorylation 47 82.98 84.78 83.87
Regulation 169 37.87 36.78 37.32
Positive regulation 617 34.36 35.99 35.16
Negative regulation 196 41.33 33.61 37.07
TOTAL 1789 50.36 45.76 47.95
Table 4: Event extraction results on the shared task
development data of the official run of the JULIELab
Team. Approximate Span Matching/Approximate Recur-
sive Matching.
peting teams, with 45.8% precision, 47.5% recall
and 46.7% F1-score on all 3,182 events.
7 Acknowledgments
We wish to thank Rico Landefeld for his technical
support, Tobias Wagner and Rico Pusch for their
constant help and great expertise in biological is-
sues. This research was partially funded within the
BOOTSTREP project under grant FP6-028099 and
the CALBC project under grant FP7-231727.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008. A
26
graph kernel for protein-protein interaction extraction.
In Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 1?9.
Christian Blaschke, Miguel A. Andrade, Christos Ouzou-
nis, and Alfonso Valencia. 1999. Automatic ex-
traction of biological information from scientific text:
Protein-protein interactions. In ISMB?99 ? Proceed-
ings of the 7th International Conference on Intelligent
Systems for Molecular Biology, pages 60?67.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically adapting an NLP
core engine to the biology domain. In Proceedings
of the Joint BioLINK-Bio-Ontologies Meeting. A Joint
Meeting of the ISMB Special Interest Group on Bio-
Ontologies and the BioLINK Special Interest Group on
Text Data M ining in Association with ISMB, pages
65?68. Fortaleza, Brazil, August 5, 2006.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007. Relex-relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jo?rg Hakenberg, Ulf Leser, Conrad Plake, Harald Kirsch,
and Dietrich Rebholz-Schuhmann. 2005. LLL?05
challenge: Genic interaction extraction - identifica-
tion of language patterns based on alignment and finite
state automata. In Proceedings of the 4th Learning
Language in Logic Workshop (LLL05), pages 38?45.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and Pieter W. Adriaans. 2006. Learn-
ing relations from biomedical corpora using depen-
dency trees. In Karl Tuyls, Ronald L. Westra, Yvan
Saeys, and Ann Nowe?, editors, KDECB 2006 ? Knowl-
edge Discovery and Emergent Complexity in Bioin-
formatics. Revised Selected Papers of the 1st Inter-
national Workshop., volume 4366 of Lecture Notes
in Computer Science, pages 61?80. Ghent, Belgium,
May 10, 2006. Berlin: Springer.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008a.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007. Syn-
tactic features for protein-protein interaction extrac-
tion. In Christopher J. O. Baker and Jian Su, editors,
LBM 2007, volume 319, pages 6.1?6.14.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and par ser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
Jasmin ?Saric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomed-
ical papers using a full parser. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Kevin Lauderdale,
and Teri E. Klein, editors, PSB 2001 ? Proceedings
of the 6th Pacific Symposium on Biocomputing, pages
408?419. Maui, Hawaii, USA. January 3-7, 2001. Sin-
gapore: World Scientific Publishing.
Guodong Zhou and Min Zhang. 2007. Extracting re-
lation information from text documents by exploring
various types of knowledge. Information Processing
& Management, 43(4):969?982.
27
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 9?17,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On Proper Unit Selection in Active Learning:
Co-Selection Effects for Named Entity Recognition
Katrin Tomanek1? Florian Laws2? Udo Hahn1 Hinrich Schu?tze2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
2Institute for Natural Language Processing, Universita?t Stuttgart, Germany
{fl|hs999}@ifnlp.org
Abstract
Active learning is an effective method for cre-
ating training sets cheaply, but it is a biased
sampling process and fails to explore large
regions of the instance space in many appli-
cations. This can result in a missed cluster
effect, which signficantly lowers recall and
slows down learning for infrequent classes.
We show that missed clusters can be avoided
in sequence classification tasks by using sen-
tences as natural multi-instance units for label-
ing. Co-selection of other tokens within sen-
tences provides an implicit exploratory com-
ponent since we found for the task of named
entity recognition on two corpora that en-
tity classes co-occur with sufficient frequency
within sentences.
1 Introduction
Active learning (AL) has been shown to be an effec-
tive approach to reduce the amount of data needed
to train an accurate statistical classifier. AL selects
highly informative examples from a pool of unla-
beled data and prompts a human annotator for the
labels of these examples. The newly labeled exam-
ples are added to a training set used to build a statis-
tical classifier. This classifier is in turn used to assess
the informativeness of further examples. Thus, a
select-label-retrain loop is formed that quickly se-
lects hard to classify examples, honing in on the de-
cision boundary (Cohn et al, 1996).
A fundamental characteristic of AL is the fact that
it constitutes a biased sampling process. This is so
? Both authors contributed equally to this work.
by design, but the bias can have an undesirable con-
sequence: partial coverage of the instance space. As
a result, classes or clusters within classes may be
completely missed, resulting in low recall or slow
learning progress. This has been called the missed
cluster effect (Schu?tze et al, 2006). While AL has
been studied for a range of NLP tasks, the missed
cluster problem has hardly been addressed.
This paper studies the missed class effect, a spe-
cial case of the missed cluster effect where complete
classes are overlooked by an active learner. The
missed class effect is the result of insufficient ex-
ploration before or during a mainly exploitative AL
process. In AL approaches where exploration is only
addressed by an initial seed set, poor seed set con-
struction gives rise to the missed class effect.
We focus on the missed class effect in the con-
text of a common NLP task: named entity recogni-
tion (NER). We show that for this task the missed
class effect is avoided by increasing the sampling
granularity from single-instance units (i.e., tokens)
to multi-instance units (i.e., sentences). For AL ap-
proaches to NER, sentence selection recovers better
from unfavorable seed sets than token selection due
to what we call the co-selection effect. Under this
effect, a non-targeted entity class co-occurs in sen-
tences that were originally selected because of un-
certainty on tokens of a different entity class.
The rest of the paper is structured as follows: Sec-
tion 2 introduces the missed class effect in detail.
Experiments which demonstrate the co-selection ef-
fect achieved by sentence selection for NER are de-
scribed in Section 3 and their results presented in
Section 4. We draw conclusions in Section 5.
9
2 The Missed Class Effect
This section first describes the missed class ef-
fect. Then, we discuss several factors influencing
this effect, focusing on co-selection, a natural phe-
nomenon in common NLP applications of AL.
2.1 Sampling bias and misguided AL
The distribution of the labeled data points obtained
with an active learner deviates from the true data
distribution. While this sampling bias is intended
and accounts for the effectiveness of AL, it also
poses challenges as it leads to classifiers that per-
form poorly in some regions, or clusters, of the ex-
ample space. In the literature, this phenomenon has
been described as the missed cluster effect (Schu?tze
et al, 2006; Dasgupta and Hsu, 2008)
In this context, we must distinguish between ex-
ploration and exploitation. By design, AL is a
highly exploitative strategy: regions around decision
boundaries are inspected thoroughly so that decision
boundaries are learned well, but regions far from any
of the initial decision boundaries remain unexplored.
An exploitative sampling approach thus has to be
combined with some kind of exploratory strategy to
make sure the example space is adequately covered.
A common approach is to start an AL process with
an initial seed set that accounts for the exploration
step. However, a seed set which is not represen-
tative of the example space may completely mis-
guide AL ? at least when no other explorative tech-
niques are applied as a remedy. While approaches
to balancing exploration and exploitation (Baram et
al., 2003; Dasgupta and Hsu, 2008; Cebron and
Berthold, 2009) have been discussed, we here fo-
cus on a ?pure? AL scenario where exploration takes
only place in the beginning by a seed set. In sum-
mary, the missed clusters are the result of a sce-
nario where poor exploration is combined with ex-
clusively exploitative sampling.
Why is AL an exploitative sampling strategy? AL
selects data points based on the confidence of the ac-
tive learner. Assume an initial seed set that does not
contain examples of a specific cluster. This leads to
an initial active learner that is mistakenly overconfi-
dent about the class membership of instances in this
missed cluster. Far away from the decision bound-
ary, the active learner assumes a high confidence for
A
B
C
(a)
A
B
C
(b)
Figure 1: Illustration of the missed cluster effect in a 1-
d scenario. Shaded points are contained in the seed set,
vertical lines are final decision boundaries, and dashed
rectangles mark the explored regions
all instances in that cluster, even if they are in fact
misclassified. Consequently, the active learner will
fail to select these instances for long until some re-
direction impulse is received (if at all).
To give an example, let us consider a simple 1-
d toy scenario with examples from three clusters A,
B, and C as shown in Figure 1. In scenario (a), AL
is started from a seed set including one example of
clusters A and B only. In subsequent rounds, AL
will select examples in these clusters only (shown as
the dashed box in the figure). Examples in cluster
C are ignored as they are far from the initial deci-
sion boundary. Eventually, a decision boundary is
fixed as shown by the vertical line which indicates
that this AL process has completely overlooked ex-
amples from cluster C.
Assuming that the examples fall in two classes
X1 = {A ? C} and X2 = {B} the learned clas-
sifier has low recall for class X1 and relatively low
precision for class X2 as it erroneously assigns ex-
amples of cluster C to class X2. In a related sce-
nario with three classes X1 = {A}, X2 = {B}, and
X3 = {C} this would even mean that the classifier
is not at all aware about the third class resulting in
the missed class problem.
A more representative seed set circumvents this
problem. Given a seed set including one example
of each cluster, AL might find a second decision
boundary1 between clusters B and C because it is
now aware of examples from C. Figure 1(b) shows
a possible result of AL on this seed set.
The missed cluster effect can be understood as
the generalized problem. A special case of it is the
1Assuming a classifier that can learn several boundaries.
10
missed class effect as shown in the previous exam-
ple. In general, it has the same causes (insufficient
exploration and misguided exploitation), but is eas-
ier to test. Often we know (at least the number of) all
classes under scrutiny, while we usually cannot as-
sume all clusters in the feature space to be known. In
this paper, we focus on the missed class effect, i.e.,
scenarios where classes are overlooked by a mis-
guided AL process resulting in a slow (active) learn-
ing progress.
2.2 Factors influcencing the missed class effect
AL in a practical scenario is subject to several fac-
tors which mitigate or intensify the missed class ef-
fect described before. In the following, we describe
three such factors, with a special focus on the co-
selection effect, which we claim to significantly mit-
igate the missed class effect in a specific type of NLP
tasks, sequence learning problems such as NER or
POS tagging.
Class imbalance Many studies on AL for NLP
tasks assume that AL is started from a randomly
drawn seed set. Such a seed set can be problem-
atic when the class distribution in the data is highly
skewed. In this case, ?rare? classes might not be
represented in the seed set, increasing the chance to
completely miss out such a class using AL. When
classes are relatively frequent, an active learner ?
even when started from an unfavorable seed set ?
might still mistake an example of one class for an
uncertain example of a different class and conse-
quently select it. Thereby, it can acquire information
about the former class ?by accident? leading to sud-
den and rapid discovery of the newly-found class.
However, in the case of extreme class imbalance this
is very unlikely. Severe class imbalance intensifies
the missed cluster effect.
Similarity of considered classes If, e.g., two of
the classes to be learned, say Xi and Xj , are harder
to discriminate than others, or if the data contains
lots of noise, an active learner is more likely to select
some instances of Xi if at least its ?similar? coun-
terpart Xj was represented in the seed set. Hence,
it may mistake the instances of Xi and Xj before it
has acquired enough information to discriminate be-
tween them. So, under certain situations similarity
of classes can mitigate the missed class effect.
The co-selection effect Many NLP tasks are se-
quence learning problems including, e.g., POS tag-
ging, and named entity recognition. Sequences are
consecutive text tokens constituting linguistically
plausible chunks, e.g., sentences. Algorithms for se-
quence learning obviously work on sequence data,
so respective AL approaches need to select complete
sequences instead of single text tokens (Settles and
Craven, 2008). Furthermore, sentence selection has
been preferred over token selection in other works
with the argument that the manual annotation of sin-
gle, possibly isolated tokens is almost impossible or
at least extremely time-consuming (Ringger et al,
2007; Tomanek et al, 2007).
Within such sequences, instances of different
classes often co-occur. Thus, an active learner that
selects uncertain examples of one class gets exam-
ples of a second class as an unintended, yet pos-
itive side effect. We call this the co-selection ef-
fect. As a result, AL for sequence labeling is not
?pure? exploitative AL, but implicitly comprises an
exploratory aspect which can substantially reduce
the missed class problem. In scenarios where we
cannot hope for such a co-selection, we are much
more likely to have decreased AL performance due
to missed clusters or classes.
3 Experiments
We ran several experiments to investigate how the
sampling granularity, i.e. the size of the selection
unit, influences the missed class effect. AL based
on token selection (T-AL) is compared to AL based
on sentence selection (S-AL). Although our experi-
ments are certainly also subject to the other factors
mitigating the missed class effect (e.g. similarity of
classes), the main focus of the experiments is on the
co-selection effect that we expected to observe in
S-AL. Several scenarios of initial exploration were
simulated by seed sets of different characteristics.
The experiments were run on synthetic and real data
in the context of named entity recognition (NER).
3.1 Classifiers and active learning setup
The active learning approach used for both S-AL
and T-AL is based on uncertainty sampling (Lewis
and Gale, 1994) with the margin metric (Schein and
Ungar, 2007) as uncertainty measure. Let c and c?
11
be the two most likely classes predicted for token
xj with p?c,xj and p?c?,xj being the associated class
probabilities. The per-token margin is calculated as
M = |p?c,xj ? p?c?,xj |.
For T-AL, the sampling granularity is the token,
while in S-AL, complete sentences are selected. For
S-AL, the margins of all tokens in a sentence are
averaged and the aggregate margin is used to select
sentences. We chose this uncertainty measure for S-
AL for better comparison with T-AL. In either case,
examples (tokens or sentences) with a small margin
are preferred for selection. In every iteration, a batch
of examples is selected: 20 sentences for S-AL, 200
tokens for T-AL.
Bayesian logistic regression as implemented in
the BBR classification package (Genkin et al, 2007)
with out-of-the-box parameter settings was used as
base learner for T-AL. For S-AL, a linear-chain
Conditional Random Field (Lafferty et al, 2001) is
employed as implemented in MALLET (McCallum,
2002). Both base learners employ standard features
for NER including the lexical token itself, various
orthographic features such as capitalization, the oc-
currence of special characters like hyphens, and con-
text information in terms of features of neighboring
tokens to the left and right of the current token.
3.2 Data sets
We used three data sets in our experiments. Two of
them (ACE and PBIO) are standard data sets. The
third (SYN) is a synthetic set constructed to have
specific characteristics. For simplicity, we consider
only scenarios with two entity classes, a majority
class (MAJ) and a minority class (MIN). We dis-
carded all other entity annotations originally con-
tained in the corpus assigning the OUTSIDE class.2
The first data set (PBIO) is based on the annota-
tions of the PENNBIOIE corpus for biomedical en-
tity extraction (Kulick et al, 2004). As PENNBIOIE
makes fine-grained and subtle distinctions between
various subtypes of classes irrelevant for this study,
we combined several of the original classes into two
entity classes: The majority class consists of the
three original classes ?gene-protein?, ?gene-generic?,
and ?gene-rna?. The minority class consists of
the original and similar classes ?variation-type? and
2The OUTSIDE class marks that a token is not part of an
named entity.
?variation-event?. All other entity labels were re-
placed by the OUTSIDE class.
The second data set (ACE) is based on the
newswire section of the ACE 2005 Multilingual
Training Corpus (Walker et al, 2006). We chose
the ?person? class as majority class and the ?organi-
zation? class as the minority class. Again, all other
classes are mapped to OUTSIDE.
The synthetic data set (SYN) was constructed by
combining the sentences from the original ACE and
PENNBIOIE corpora. The ?person? class consti-
tutes the minority class, the very similar classes
?malignancy? and ?malignancy-type? were merged
to form the majority class. All other class la-
bels were set to OUTSIDE. SYN?s construction
was motivated by the following characteristics of
the new data set which would make the appear-
ance of the missed class effect very likely for
insufficient exploration scenarios:
(i) absence of inner-sentence entity class correlation
to ensure that sentences contain either mentions of
only a single entity class or no mentions at all.
(ii) marked entity class imbalance between the ma-
jority and minority classes
(iii) dissimilar surface patterns of entity mentions of
the two entity classes with the rationale that class
similarity will be low.
Table 1 summarizes characteristics of the data
sets. While SYN exhibits high imbalance (e.g., 1:9.4
on the token level), PBIO and ACE are moderately
skewed. In PBIO, the number of sentences contain-
ing any entity mention is relatively high compared
to ACE or SYN. For our experiments, the corpora
were randomly split in a pool for AL and a test set
for performance evaluation.
Inner-sentence entity class co-occurrence We
have described co-selection as a potential mitigat-
ing factor for the missed class effect in Section 2.
For this effect to occur, there must be some corre-
lation between the occurrence of entity mentions of
the MAJ class with those from MIN.
Table 2 shows correlation statistics based on the
?2 measure. We found strong correlation in all three
corpora3: For ACE and PBIO, the correlation is pos-
itive; for SYN it is negative so when a sentence in
SYN contains a majority class entity mention, it is
3All correlations are statistically significant (p < 0.01).
12
PBIO ACE SYN
sentences (all) 11,164 2,642 13,804
sentences (MAJ) 7,075 767 5,667
sentences (MIN) 2,156 974 974
MIN-MAJ ratio 1 : 3.3 1 : 1.3 1 : 5.8
tokens (all) 277,053 66,752 343,773
tokens (MAJ) 17,928 2,008 18,959
tokens (MIN) 4,079 1,822 2,008
MIN-MAJ ratio 1 : 4.4 1 : 1.1 1 : 9.4
Table 1: Characteristics of the data sets; ?sentences
(MAJ)?, e.g., specifies the number of sentences contain-
ing mentions of the majority class.
PBIO ACE SYN
?2 132.34 6.07 727
P (MIN |MAJ) 0.26 0.31 0.0
Table 2: Co-occurrence of entity classes in sentences
highly unlikely that it also contains a minority entity.
In fact, it is impossible by construction of the data
set. Further, this table shows the probability that a
sentence containing the majority class also contains
the minority class. As expected, this is exactly 0 for
SYN, but significantly above 0 for PBIO and ACE.
3.3 Seed sets
Selection of an appropriate seed set for the start of an
AL process is important to the success of AL. This is
especially relevant in the case of imbalanced classes
because a typically small random sample will pos-
sibly not contain any example of the rare class. We
constructed different types of seed sets (whose nam-
ing intentionally reflects the use of the entity classes
from Section 3.2) to simulate different scenarios of
ill-managed initial exploration. All seed sets have
a size of 20 sentences. The RANDOM set was ran-
domly sampled, the MAJ set is made of sentences
containing at least one majority class entity, but no
minority class entity. Accordingly, MIN is densely
populated with minority entities. Finally, OUTSIDE
contains only sentences without entity mentions.
One could think of the OUTSIDE and MAJ seed
sets of cases where a random seed set selection has
unluckily produced an especially bad seed set. MIN
serves to demonstrate the opposite case. For each
type of seed set, we sampled ten independent ver-
sions to calculate averages over several AL runs.
3.4 Cost measure
The success of AL is usually measured as reduc-
tion of annotation effort according to some cost mea-
sure. Traditionally, the most common cost measure
considers a unit cost per annotated token, which fa-
vors AL systems that select individual tokens. In
a real annotation setting, however, it is unnatural,
and therefore hard for humans to annotate single,
possibly isolated tokens, leading to bad annotation
quality (Hachey et al, 2005; Ringger et al, 2007).
When providing context, the question arises whether
the annotator can label several tokens present in the
context (e.g., an entire multi-token entity or even
the whole sentence) at little more cost than anno-
tating a single token. Thus, assigning a linear cost
of n to a sentence where n is the sentence?s length
in tokens seems to unfairly disadvantage sentence-
selection AL setups.
However, more work is needed to find a more re-
alistic cost measure. At present there is no other
generally accepted cost measure than unit cost per
token, so we report costs using the token measure.
4 Results
This section presents the results of our experiments
on the missed class effect in two different AL
scenarios, i.e., sentence selection (S-AL) and to-
ken selection (T-AL). The AL runs were stopped
when convergence on the minority class F-score was
achieved. This was done because early AL iterations
before the convergence point are most important and
representative for a real-life scenario where the pool
is extremely large, so that absolute convergence of
the classifier?s performance will never be reached.
The learning curves in Figures 2, 3, and 4 reveal
general characteristics of S-AL compared to T-AL.
For S-AL, the number of tokens on the x-axis is the
total number of tokens in the sentences labeled so
far. While S-AL generally yields higher F-scores, T-
AL converges much earlier when counted in terms
of tokens. The reason for this is that T-AL can se-
lect uncertain data more specifically. In contrast, S-
AL also selects tokens that the classifier can already
classify reliably ? these tokens are selected because
they co-occur in a sentence that also contains an un-
certain token. Whether T-AL is really more efficient
clearly depends on the cost-metric applied (cf. Sec-
13
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with OUTSIDE seed
0 5000 10000 150000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with OUTSIDE seed
0 5000 10000 150000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with OUTSIDE seed
0 10000 30000 50000 700000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 2: Results on SYN corpus for token selection (a,b,c) and sentence selection (d,e,f)
tion 3.4). Since the focus of this paper is on compar-
ing the missed class effect in a sentence and a token
selection AL setting (T-AL and S-AL) we apply the
straight-forward token measure.
4.1 The pathological case
Figure 2 shows results on the SYN corpus for T-AL
(upper row) and S-AL (lower row). Figures 2(a)
and 2(d) show the minority and majority class learn-
ing curves for a single run starting from the OUT-
SIDE seed set, which was particularly problematic
on SYN. (We show single runs to give a better pic-
ture of what happens during the selection process.)
The figures show that for both AL scenarios, the
OUTSIDE seed set caused the active learner to focus
exclusively on the majority class and to completely
ignore the minority class for many AL iterations (al-
most 30,000 tokens for S-AL and over 4,000 tokens
for T-AL). Had we stopped the AL process before
this turning point, the classifier?s performance on
the majority entity class would have been reason-
ably high while the minority class would not have
been learned at all ? which is precisely the defini-
tion of an (initially) missed class.
Figures 2(b) and 2(e) show the corresponding
mean margin plots of these AL runs, indicating the
confidence of the classifier on each class. The mean
margin is calculated as the average margin over to-
kens in the remaining pool, separately for each true
class label.4 As expected, the active learner is over-
confident but wrong on instances of the minority
class (assigning them to the OUTSIDE class, we
assume). Only after some time, margin scores on
minority class tokens start decreasing. This hap-
pens because from time to time minority class ex-
amples are mistakenly considered as majority class
examples with low confidence and thus selected by
accident. Lowered minority class confidence then
causes the selection of further minority class exam-
ples, resulting in a turning point with a steep slope
of the minority class learning curve.
Consequences of seed set selection We compare
the minority class learning curves for all types of
4Note that in a real, non-simulation active learning task, the
true class labels would be unknown.
14
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
F?score
MIN classMAJ class
(a) T-AL learning curve, single run
with MAJ seed
0 5000 10000 15000 200000.5
0.6
0.7
0.8
0.9
1.0
tokens
mean
 margin
MIN classMAJ class
(b) T-AL mean margin curve, single
run with MAJ seed
0 5000 10000 15000 200000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(c) T-AL learning curves, minority
class, all seeds, 10 runs
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
per clas
s F?sco
re
MIN classMAJ class
(d) S-AL learning curve, single run
with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
1.0
tokens
mean
 margin
MIN classMAJ class
(e) S-AL mean margin curve, single
run with MAJ seed
0 10000 20000 30000 40000 500000.0
0.2
0.4
0.6
0.8
tokens
minority
 class F
?score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(f) S-AL learning curves, minority
class, all seeds, 10 runs
Figure 3: Results on PBIO corpus for token selection (a,b,c) and sentence selection (d,e,f)
seed sets and for random selection (cf. Figures 2(c)
and 2(f)), now averaged over 10 runs. On S-AL all
but the MIN seed set were inferior to random selec-
tion. Even the commonly used random seed set se-
lection is problematic because the minority class is
so rare that there are random seed sets without any
example of the minority class.
On T-AL, all seed sets are better than random se-
lection. This, however, is because random selec-
tion is an extremely weak baseline for T-AL due to
the token distribution (cf. Table 1). Still, the RAN-
DOM, MAJ, and OUTSIDE seed sets are signifi-
cantly worse than a seed set which covers the minor-
ity class well. Note that the majority class learning
curves are relatively invariant against different seed
sets. The minority class seed set does have some
negative impact on initial learning progress on the
majority class (not shown here), but the impact is
rather small. Because of the higher frequency of
the majority class, the classifier soon finds major-
ity class examples to compensate for the seed set by
chance or class similarity.
4.2 Missed class effect mitigated by co-selection
Results on PBIO corpus On the PBIO corpus,
where minority and majority class entity mentions
naturally co-occur on the sentence level, we get
a different picture. Figure 3 shows the learning
(3(a), 3(d)) and mean margin (3(b), 3(e)) curves for
the MAJ seed set. T-AL still exhibits the missed
class effect on this seed set. The minority class
learning curve again has a delayed slope and high
mean margin scores of minority tokens at the be-
ginning, resulting in insufficient selection and slow
learning. S-AL, on the other hand, does not re-
ally suffer from the missed class effect: minor-
ity class entity mentions are co-selected in sen-
tences which were chosen due to uncertainty on
majority class tokens. Minority class mean mar-
gin scores quickly fall, reinforcing selection for mi-
nority class entities. Learning curves for minority
and majority classes run approximately in parallel.
Figure 3(f) shows that all seed sets perform quite
similar for S-AL. MIN unsurprisingly is a bit better.
With the other seed sets, S-AL performance is com-
15
0 2000 4000 6000 80000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(a) T-AL
0 5000 15000 250000.0
0.2
0.4
0.6
0.8
tokens
minority 
class F?
score
MIN seedMAJ seedOUTSIDE seedRANDOM seedrandom selection
(b) S-AL
Figure 4: Minority class learning curves for all seeds on
ACE averaged over 10 runs
parable to random selection. On the PBIO corpus,
random selection is a strong baseline as almost every
sentence contains an entity mention ? which is not
the case for SYN and ACE (cf. Table 1). As there is
no co-selection effect for T-AL, the MAJ and OUT-
SIDE seed sets also here are subject to the missed
class problem (Figure 3(c)), although not as severely
as on the SYN corpus.
Results on ACE corpus Figure 4 shows learning
curves averaged over 10 runs on ACE. Overall, the
missed class effect is less pronounced on ACE com-
pared to PBIO. Still, co-selection avoids a good por-
tion of the missed class effect on S-AL ? all seed
sets yield results much better than random selection
right from the beginning.
On T-AL, the OUTSIDE seed set has a marked
negative effect. However, while different seed
sets still have visible differences in learning perfor-
mance, the magnitude of the effect is smaller than
on PBIO. It is difficult to find the exact reasons
in a non-synthetic, natural language corpus where a
lot of different effects are intermingled. One might
assume higher class similarity between the major-
ity (?persons?) and the minority (?organizations?)
classes on the ACE corpus than, e.g., on the PBIO
corpus. Moreover, there is hardly any imbalance
in frequency between the two entity classes on the
ACE corpus. We briefly discussed such influencing
factors possibly mitigating the missed class effect in
Section 2.2.
4.3 Discussion
To summarize, on a synthetic corpus (SYN) the
missed class effect can be well studied in both
AL scenarios, i.e., S-AL and T-AL. Moving from
a relatively controlled, synthetic corpus (extreme
class imbalance, no inner-sentence co-occurrence
between entity classes, quite different entity classes)
to more realistic corpora, effects generally mix a bit
due to different degrees of class imbalance and prob-
ably higher similarity between entity classes.
Our experiments unveil that co-selection in S-AL
effectively helps avoid dysfunctional classifiers that
insufficiently explore the instance space due to a
disadvantageous seed set. In contrast, AL based
on token-selection (T-AL) cannot recover from in-
sufficient exploration as easy as AL with sentence-
selection and is thus more sensitive to the missed
class effect.
5 Conclusion
We have shown that insufficient exploration in the
initial stages of active learning gives rise to regions
of the sample space that contain missed classes that
are incorrectly classified. This results in low clas-
sification performance and slow learning progress.
Comparing two sampling granularities, tokens vs.
sentences, we found that the missed class effect is
more severe when isolated tokens instead of sen-
tences are selected for labeling.
The missed class problem in sequence classifica-
tion tasks can be avoided using sentences as natural
multi-instance units for selection and labeling. Us-
ing multi-instance units, co-selection of other tokens
within sentences provides an implicit exploratory
component. This solution is effective if classes co-
occur sufficiently within sentences which is the case
for many real-life entity recognition tasks.
While other work has proposed sentence selection
in AL for sequence labeling as a means to ease and
speed up annotation, we have gathered here addi-
tional motivation from the perspective of robustness
of learning. Future work will compare the beneficial
effect introduced by co-selection with other forms of
exploration-enabled active learning.
Acknowledgements
The first and the third author were funded by the
German Ministry of Education and Research within
the StemNet project (01DS001A-C) and by the EC
within the BOOTStrep project (FP6-028099).
16
References
Yoram Baram, Ran El-Yaniv, and Kobi Luz. 2003. On-
line choice of active learning algorithms. In ICML
?03: Proceedings of the 20th International Conference
on Machine Learning, pages 19?26.
Nicolas Cebron and Michael R. Berthold. 2009. Active
learning for object classification: From exploration to
exploitation. Data Mining and Knowledge Discovery,
18(2):283?299.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In ICML ?08: Proceed-
ings of the 25th International Conference on Machine
Learning, pages 208?215.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan T. McDonald, Martha S. Palmer, and Andrew Ian
Schein. 2004. Integrated annotation for biomedical
information extraction. In Proceedings of the HLT-
NAACL 2004 Workshop ?Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users?,
pages 61?68.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ?01: Proceedings of the 18th International
Conference on Machine Learning, pages 282?289.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Hinrich Schu?tze, Emre Velipasaoglu, and Jan Pedersen.
2006. Performance thresholding in practical text clas-
sification. In CIKM ?06: Proceedings of the 15th ACM
International Conference on Information and Knowl-
edge Management, pages 662?671.
Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks. In
EMNLP ?08: Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 1070?1079.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
17
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112?115,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Timed Annotations ? Enhancing MUC7 Metadata
by the Time It Takes to Annotate Named Entities
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We report on the re-annotation of selected
types of named entities from the MUC7
corpus where our focus lies on record-
ing the time it takes to annotate these
entities given two basic annotation units
? sentences vs. complex noun phrases.
Such information may be helpful to lay
the empirical foundations for the develop-
ment of cost measures for annotation pro-
cesses based on the investment in time for
decision-making per entity mention.
1 Introduction
Manually supplied annotation metadata is at the
heart of (semi)supervised machine learning tech-
niques which have become very popular in NLP
research. At their flipside, they create an enor-
mous bottleneck because major shifts in the do-
main of discourse, the basic entities of interest, or
the text genre often require new annotation efforts.
But annotations are costly in terms of getting well-
trained and intelligible human resources involved.
Surprisingly, cost awareness has not been a pri-
mary concern in most of the past linguistic anno-
tation initiatives. Only recently, annotation strate-
gies (such as Active Learning (Cohn et al, 1996))
which strive for minimizing the annotation load
have gained increasing attention. Still, when it
comes to the empirically plausible assessment of
annotation costs even proponents of Active Learn-
ing make overly simplistic and empirically ques-
tionable assumptions, e.g., the uniformity of an-
notation costs over the number of linguistic units
(e.g., tokens) to be annotated.
We here consider the time it takes to annotate
a particular entity mention as a natural indicator
of effort for named entity annotations. In order to
lay the empirical foundations for experimentally
grounded annotation cost models we couple com-
mon named entity annotation metadata with a time
stamp reflecting the time measured for decision
making.1
Previously, two studies ? one dealing with POS
annotation (Haertel et al, 2008), the other with
named entity and relation annotation (Settles et al,
2008) ? have measured the time needed to anno-
tate sentences on small data sets and attempted to
learn predictive models of annotation cost. How-
ever, these data sets do not meet our requirements
as we envisage a large, coherent, and also well-
known newspaper entity corpus extended by an-
notation costs on a fine-grained level. Especially
size and coherence of such a corpus are not only
essential for building accurate cost models but also
as a reference baseline for cost-sensitive annota-
tion strategies. Moreover, the annotation level for
which cost information is available is crucial be-
cause document- or sentence-level data might be
too coarse for several applications. Accordingly,
this paper introduces MUC7T , our extension to the
entity annotations of the MUC7 corpus (Linguis-
tic Data Consortium, 2001) where time stamps are
added to two levels of annotation granularity, viz.
sentences and complex noun phrases.
2 Corpus Annotation
2.1 Annotation Task
Our annotation initiative constitutes an extension
to the named entity annotations (ENAMEX) of the
English part of the MUC7 corpus covering New
York Times articles from 1996. ENAMEX annota-
tions cover three types of named entities, viz. PER-
SONS, LOCATIONS, and ORGANIZATIONS. We in-
structed two human annotators, both advanced stu-
dents of linguistics with good English language
skills, to re-annotate the MUC7 corpus for the
ENAMEX subtask. To be as consistent as possi-
1These time stamps should not be confounded with the an-
notation of temporal expressions (TIMEX in MUC7, or even
more advanced metadata using TIMEML for the creation of
the TIMEBANK (Pustejovsky et al, 2003)).
112
ble with the existing MUC7 annotations, the an-
notators had to follow the original guidelines of
the MUC7 named entity task. For ease of re-
annotation, we intentionally ignored temporal and
number expressions (TIMEX and NUMEX).
MUC7 covers three distinct document sets for
the named entity task. We used one of these sets
to train the annotators and develop the annotation
design, and another one for our actual annotation
initiative which consists of 100 articles reporting
on airplane crashes. We split lengthy documents
(27 out of 100) into halves to fit on the annota-
tion screen without the need for scrolling. Further-
more, we excluded two documents due to over-
length which would have required overly many
splits. Our final corpus contains 3,113 sentences
(76,900 tokens) (see Section 3.1 for more details).
Time-stamped ENAMEX annotation of this cor-
pus constitutes MUC7T , our extension of MUC7.
Annotation time measurements were taken on two
syntactically different annotation units of single
documents: (a) complete sentences and (b) com-
plex noun phrases. The annotation task was de-
fined such as to assign an entity type label to each
token of an annotation unit. Sentence-level anno-
tation units where derived by the OPENNLP2 sen-
tence splitter. The use of complex noun phrases
(CNPs) as an alternative annotation unit is mo-
tivated by the fact that in MUC7 the syntactic
encoding of named entity mentions basically oc-
curs through nominal phrases. CNPs were derived
from the sentences? constituency structure using
the OPENNLP parser (trained on PENNTREE-
BANK data) to determine top-level noun phrases.
To avoid overly long phrases, CNPs dominating
special syntactic structures, such as coordinations,
appositions, or relative clauses, were split up at
discriminative functional elements (e.g., a relative
pronoun) and these elements were eliminated. An
evaluation of our CNP extractor on ENAMEX an-
notations in MUC7 showed that 98.95% of all en-
tities where completely covered by automatically
identified CNPs. For the remaining 1.05% of the
entity mentions, parsing errors were the most com-
mon source of incomplete coverage.
2.2 Annotation and Time Measurement
While the annotation task itself was ?officially?
declared to yield only annotations of named en-
tity mentions within the different annotation units,
2http://opennlp.sourceforge.net
we were primarily interested in the time needed
for these annotations. For precise time measure-
ments, single annotation examples were shown to
the annotators, one at a time. An annotation exam-
ple consists of the chosen MUC7 document with
one annotation unit (sentence or CNP) selected
and highlighted. Only the highlighted part of the
document could be annotated and the annotators
were asked to read only as much of the context sur-
rounding the annotation unit as necessary to make
a proper annotation decision. To present the an-
notation examples to annotators and allow for an-
notation without extra time overhead for the ?me-
chanical? assignment of entity types, our annota-
tion GUI is controlled by keyboard shortcuts. This
minimizes annotation time compared to mouse-
controlled annotation such that the measured time
reflects only the amount of time needed for taking
an annotation decision.
In order to avoid learning effects at the annota-
tors? side on originally consecutive syntactic sub-
units, we randomly shuffled all annotation exam-
ples so that subsequent annotation examples were
not drawn from the same document. Hence, an-
notation times were not biased by the order of ap-
pearance of the annotation examples.
Annotators were given blocks of either 500
CNP- or 100 sentence-level annotation examples.
They were asked to annotate each block in a single
run under noise-free conditions, without breaks
and disruptions. They were also instructed not to
annotate for too long stretches of time to avoid tir-
ing effects making time measurements unreliable.
All documents were first annotated with respect
to CNP-level examples within 2-3 weeks, with
only very few hours per day of concrete annota-
tion work. After completion of the CNP-level an-
notation, the same documents had to be annotated
on the sentence level as well. Due to randomiza-
tion and rare access to surrounding context during
the CNP-level annotation, annotators credibly re-
ported that they had indeed not remembered the
sentences from the CNP-level round. Thus, the
time measurements taken on the sentence level do
not seem to exhibit any human memory bias.
Both annotators went through all annotation ex-
amples so that we have double annotations of the
complete data set. Prior to coding, they indepen-
dently got used to the annotation guidelines and
were trained on several hundred examples. For the
annotators? performance see Section 3.2.
113
3 Analysis
3.1 Corpus Statistics
Table 1 summarizes statistics on the time-stamped
MUC7 corpus. About 60% of all tokens are cov-
ered by CNPs (45,097 out of 76,900 tokens) show-
ing that sentences are made up from CNPs to a
large extent. Still, removing the non-CNP to-
kens markedly reduces the amount of tokens to
be considered for entity annotation. CNPs cover
slightly less entities (3,937) than complete sen-
tences (3,971), a marginal loss only.
sentences 3,113
sentence tokens 76,900
chunks 15,203
chunk tokens 45,097
entity mentions in sentences 3,971
entity mentions in CNPs 3,937
sentences with entity mentions 63%
CNPs with entity mentions 23%
Table 1: Descriptive statistics of time-stamped MUC7 corpus
On the average, sentences have a length of
24.7 tokens, while CNPs are rather short with
3.0 tokens, on the average. However, CNPs vary
tremendously in their length, with the shortest
ones having only one token and the longest ones
(mostly due to parsing errors) spanning over 30
(and more) tokens. Figure 1 depicts the length
distribution of sentences and CNPs showing that
a reasonable portion of CNPs have less than five
tokens, while the distribution of sentence lengths
almost follows a normal distribution in the interval
[0, 50]. While 63% of all sentences contain at least
one entity mention, only 23% of CNPs contain en-
tity mentions. These statistics show that CNPs are
generally rather short and a large fraction of CNPs
does not contain entity mentions at all. We may
hypothesize that this observation will be reflected
by annotation times.
sentence length
tokens
freq
uenc
y
0 20 40 60 80
0
100
300
500
CNP length
tokens
freq
uenc
y
0 5 10 15 20
0
200
0
600
0
Figure 1: Length distribution of sentences and CNPs
3.2 Annotation Performance
To test the validity of the guidelines and the gen-
eral performance of our annotators A and B, we
compared their annotation results on 5 blocks of
sentence-level annotation examples created dur-
ing training. Annotation performance was mea-
sured in terms of Cohen?s kappa coefficient ? on
the token level and entity-segment F -score against
MUC7 annotations. The annotators achieved
?A = 0.95 and ?B = 0.96, and FA = 0.92
and FB = 0.94, respectively.3 Moreover, they ex-
hibit an inter-annotator agreement of ?A,B = 0.94
and an averaged mutual F-score of FA,B = 0.90.
These numbers reveal that the task is well-defined
and the annotators have sufficiently internalized
the annotation guidelines to produce valid results.
Figure 2 shows the annotators? scores against
the original MUC7 annotations for the 31 blocks
of sentence-level annotations (3,113 sentences)
which range from ? = 0.89 to ? = 0.98. Largely,
annotation performance is similar for both anno-
tators and shows that they consistently found a
block either rather hard or easy to annotate. More-
over, annotation performance seems stationary ?
no general trend in annotation performance over
time can be observed.
l l l l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l l
l
l l
l l
l l
l l
0 5 10 15 20 25 300
.80
0.85
0.90
0.95
1.00
sentence?level annotation
blocks
kapp
a l
l l l l l l
l
l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l annotator Aannotator B
Figure 2: Average kappa coefficient per block
3.3 Time Measurements
Figure 3 shows the average annotation time per
block (CNPs and sentences). Considering the
CNP-level annotations, there is a learning effect
for annotator B during the first 9 blocks. Af-
ter that, both annotators are approximately on a
par regarding the annotation time. For sentence-
level annotations, both annotators again yield sim-
ilar annotation times per block, without any learn-
ing effects. Similar to annotation performance,
3Entity-specific F-scores against MUC7 annotations for
A and B are 0.90 and 0.92 for LOCATION, 0.92 and 0.93
for ORGANIZATION, and 0.96 and 0.98 for PERSON, respec-
tively.
114
l l l l l
l l l
l l
l
l l l l l l
l
l l
l l l l
l
l l l
l
l
0 5 10 15 20 25 30
1.0
1.2
1.4
1.6
1.8
2.0
CNP?level annotation
blocks
sec
onds
l
l l
l
l l
l
l l
l l
l l l l l l
l l l l
l
l
l
l
l
l annotator Aannotator B
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l l
l
l l
l
l
0 5 10 15 20 25 30
4.0
4.5
5.0
5.5
6.0
sentence?level annotation
blocks
sec
onds
l
l l l
l
l
l
l
l
l
l
l
l l
l
l
l l l l
l
l
l l
l l
l
l
l
l
l annotator Aannotator B
Figure 3: Average annotation times per block
analysis of annotation time shows that the annota-
tion behavior is largely stationary (excluding first
rounds of CNP-level annotation) which allows sin-
gle time measurements to be interpreted indepen-
dently of previous time measurements. Both, time
and performance plots exhibit that there are blocks
which were generally harder or easier than other
ones because both annotators operated in tandem.
3.4 Easy and Hard Annotation Examples
As we have shown, inter-annotator variation of
annotation performance is moderate. Intra-block
performance, in contrast, is subject to high vari-
ance. Figure 4 shows the distribution of annota-
tor A?s CNP-level annotation times for block 20.
A?s average annotation time on this block amounts
to 1.37 seconds per CNP, the shortest time be-
ing 0.54, the longest one amounting 10.2 seconds.
The figure provides ample evidence for an ex-
tremely skewed time investment for coding CNPs.
A preliminary manual analysis revealed CNPs
with very low annotation times are mostly short
and consist of stop words and pronouns only, or
CNP?level annotation
annotation time
freq
uenc
y
2 4 6 8 10
05
0
150
250
Figure 4: Distribution of annotation times in one block
are otherwise simple noun phrases with a sur-
face structure incompatible with entity mentions
(e.g., all tokens are lower-cased). Here, humans
can quickly exclude the occurrence of entity men-
tions which results in low annotation times. CNPs
which took desparately long (more than 6 seconds)
were outliers indicating distraction or loss of con-
centration. Times between 3 and 5 seconds were
basically caused by semantically complex CNPs.
4 Conclusions
We have created a time-stamped version of MUC7
entity annotations, MUC7T , on two levels of anno-
tation granularity ? sentences and complex noun
phrases. Especially the phrase-level annotations
allow for fine-grained time measurement. We will
use this corpus for studies on (time) cost-sensitive
Active Learning. MUC7T can also be used to de-
rive or learn accurate annotation cost models al-
lowing to predict annotation time on new data. We
are currently investigating causal factors of anno-
tation complexity for named entity annotation on
the basis of MUC7T .
Acknowledgements
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Oleg Lichten-
wald (JULIE Lab) for implementing the noun
phrase extractor for our experiments.
References
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proceedings of the ACL-08: HLT, Short
Papers, pages 65?68.
Linguistic Data Consortium. 2001. Message Under-
standing Conference 7. LDC2001T02. FTP file.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of the Corpus Linguistics
2003 Conference, pages 647?656.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS?08 Workshop on Cost Sensitive
Learning, pages 1?10.
115
An Empirical Assessment of Semantic Interpretation 
Mart in  Romacker  &: Udo  Hahn 
Text Understanding Lab, \ [ -~ Group, 
Freiburg University, Freiburg, D-79085, Germany 
{mr, hahn}~coling, uni-freiburg, de 
Abst rac t  
We introduce a framework for semantic interpreta- 
tion in which dependency structures are mapped to 
conceptual representations based on a parsimonious 
set of interpretation schemata. Our focus is on the 
empirical evaluation of this approach to semantic in- 
terpretation, i.e., its quality in terms of recall and 
precision. Measurements are taken with respect o 
two real-world omains, viz. information technology 
test reports and medical finding reports. 
1 Introduction 
Semantic interpretation has been an actively investi- 
gated issue on the research agenda of the logic-based 
paradigm of NLP in the late eighties (e.g., Charniak 
and Goldman (1988), Moore (1989), Pereira and 
Pollack (1991)). With the emergence of empirical 
methodologies in the early nineties, attention has al- 
most completely shifted away from this topic. Since 
then, semantic issues have mainly been dealt with 
under a lexical perspective, viz. in terms of the res- 
olution of lexico-semantie ambiguities (e.g., Schfitze 
(1998), Pedersen and Bruce (1998)) and the gener- 
ation of lexical hierarchies from large text corpora 
(e.g., Li and Abe (1996), Hirakawa et al (1996)) 
massively using statistical techniques. 
The research on semantic interpretation that was 
conducted in the pre-empiricist age of NLP was 
mainly driven by an interest in logical formalisms 
as carriers for appropriate semantic representations 
of NL utterances. With this representational bias, 
computational matters - -  how can semantic repre- 
sentation structures be properly derived from parse 
trees for a large variety of linguistic phenomena? - -
became a secondary issue. In particular, this re- 
search lacked entirely quantitative data reflecting 
the accuracy of the proposed semantic interpreta- 
tion mechanisms on real-world language data. 
One might be tempted to argue that recent eval- 
uation efforts within the field of information extrac- 
tion (IE) systems (Chinchor et al, 1993) are going to 
remedy this shortcoming. Given, however, the fixed 
number of knowledge templates and the restricted 
types of entities, locations, and events they encode 
as target information to be extracted, one readily re- 
alizes that such an evaluation framework provides, 
at best, a considerably biased, overly selective test 
environment for judging the understanding potential 
of text analysis ystems which are not tuned for this 
special application. 
On the other hand, the IE experiments clearly in- 
dicate the need for a quantitative assessment of the 
interpretative p rformance of natural anguage un- 
derstanding systems. We will focus on this challenge 
and propose such a general evaluation framework. 
We first outline the model of semantic interpretation 
underlying our approach and then focus on 'its em- 
pirical assessment for two basic syntactic structures 
of the German language, viz. genitives and auxiliary 
constructions, in two domains. 
2 The  Bas ic  Mode l  for  Semant ic  
Interpretation 
The problem of semantic interpretation can be de- 
scribed as the mapping from syntactic to semantic 
(or conceptual) representation structures. In our ap- 
proach, the syntactic representation structures are 
given as dependency graphs (Hahn et al, 1994). Un- 
like constituency-based syntactic descriptions, de- 
pendency graphs consist of lexical nodes only, and 
these nodes are connected by vertices, each one of 
which is labeled by a particular dependency relation 
(cf. Figure 1). 
For the purpose of semantic interpretation, de- 
pendency graphs can be decomposed into semanti- 
cally interpretable subgraphs3 Basically, two types 
of semantically interpretable subgraphs can be dis- 
tinguished. The first one consists of lexical nodes 
which are labeled by content words only (lexical in- 
stances of verbs, nouns, adjectives or adverbs) and 
which are directly linked by a single dependency re- 
lation of any type whatsoever. Such a subgraph is 
illustrated in Figure 1 by 8peicher- genatt - Com- 
puters. The second type of subgraph is also delim- 
ited by labels of content words but, in addition, a 
series of n -- 1... 4 intermediary lexical nodes may 
1This notion and all subsequent criteria for interpretation 
are  formally described in Romacker et al (1999). 
327 
pro  : 
kann subject: L _ 
/ Der  Computers  . e rwe i te r t  
I spec i f ie r /~  I 
I des * m~ject :  
SDRAM-Modu len  
/The  memory, o f  the computer  -- can  -- with SDRAM-modu les  -- ex tended -- be\]  
The memory of the computer can be extended with SDRAM-modules 
Figure 1: Dependency Graph for a Sample Sentence 
appear between these content words, all of which are 
labeled by non-content words (such as auxiliary or 
modal verbs, prepositions). Hence, in contrast o 
direct linkage we speak here of indirect linkage be- 
tween content words. Such a subgraph, with two 
intervening non-content words - the modal "kann" 
and the auxiliary "werden" -, is given in Figure 1 by 
Spe ieher -  subject - kann-  verbpart - werden-  
verbpart - erweitert .  Another subgraph with just 
one intervening non-content word - the preposition 
"mit" - is illustrated by erwe i te r t -  ppadjunct - mit  
- pobject - SDRAM-Modu len .  From these consid- 
erations follows that, e.g., the subgraph spanned by 
Spe ieher  and SDRAM-Modu len  does not form a 
semantically interpretable subgraph, since the con- 
tent word erwe i te r t  intervenes on the linking path. 
Our approach to semantic interpretation sub- 
scribes to the principles of locality and composition- 
ality. It operates on discrete and well-defined units 
(subgraphs) of the parse tree, and the results of se- 
mantic interpretation are incrementally combined by 
fusing semantically interpretable subgraphs. 
As semantic target language we have chosen the 
framework of KL-ONE-type description logics (DL) 
(Woods and Schmohe, 1992). Since these logics are 
characterized by a settheoretical semantics we stay 
on solid formal ground. Fhrthermore, we take ad- 
vantage of the powerful inference ngine of DL sys- 
tems, the description classifier, which turns out to be 
essential for embedded reasoning during the seman- 
tic interpretation process. By equating the semantic 
representation language with the conceptual one, we 
follow arguments discussed by Allen (1993). 
The basic idea for semantic interpretation is as 
follows: Each lexical surface form of a content word 
is associated with a set of concept identifiers repre- 
senting its (different) lexical meanings. This way, 
lexical ambiguity is accounted for. These concep- 
tual correlates are internal to the domain knowledge 
base, where they are described by a list of attributes 
or conceptual roles, and corresponding restrictions 
on permitted attribute values or role fillers are asso- 
ciated with them. 
, , ~s -~0~iN0-~0~Y f .  ' - l 
@ / 
!XTENS I 0N-PATIENT 
EXTENSION. 04 ~ r , 
k~ MODALITY L ....... , 
Figure 2: Concept Graph for a Sample Sentence 
As an example, consider the description for the 
concept COMPUTER-SYSTEM. It may be character- 
ized by a set of roles, such as HAS-HARD-DISK Or HAS- 
WORKING-MEMORY, with corresponding restrictions 
on the concept ypes of potential role fillers. HAS- 
WORKING-MEMORY, e.g., sanctions only fillers of 
the concept ype MEMORY. These conceptual con- 
straints are used for semantic filtering, i.e., for the 
elimination of syntactically admissible dependency 
graphs which, nevertheless, do not have a valid se- 
mantic interpretation. 
Semantic interpretation, in effect, boils down to 
finding appropriate conceptual relations in the do- 
main knowledge that link the conceptual correlates 
of the two content words spanning the semanti- 
cally interpretable subgraph, irrespective of whether 
a direct or an indirect linkage holds at the syn- 
tactic level. Accordingly, Figure 2 depicts the se- 
mantic/conceptual interpretation of the dependency 
structure given in Figure 1. Instances represent- 
ing the concrete discourse entities and events in 
the sample sentence are visualized as solid rectan- 
gles containing a unique identifier (e.g., COMPUTER- 
SYSTEM.02). Labeled and directed edges indicate 
instance roles. Dashed rectangles characterize sym- 
bols used as makers for tense and modality. 2 
Note that in Figure 2 each tuple of content words 
which configures a minimal subgraph in Figure 1 
has already received an interpretation i terms of a 
relation linking the conceptual correlates. For exam- 
ple, Spe icher -  genatt - Computers  (cf. Figure 1, 
box 1) is mapped to COMPUTER-SYSTEM.02 HAS- 
WORKING-MEMORY MEMORY.01 (cf. Figure 2, box 
1). However, the search for a valid conceptual rela- 
tion is not only limited to a simple one-link slot-filler 
structure. We rather may determine conceptual re- 
lation paths between conceptual correlates of lexical 
items, the length of which may be greater than 1. 
2We current ly do not further interpret the information con- 
tained in tense or modal i ty  markers. 
328 
VerbTrans iliary 
<subject: {agent patient}> 
<dirobject: {patient co-patient}>i 
erweitern (extend) werden_.passive 
<{patient co-patient}> 
Lexeme 
Nominal Pre ~osition 
Noun _ Pronoun: <genitive attribute:~ > :: 
Speicher (memory) mlt (with) 
<{has-part instrument ...}> 
Figure 3: Fragment of the Lexeme Class Hierarchy 
(Thus, the need for role composition in the DL lan- 
guage becomes evident.) The directed search in the 
concept graph of the domain knowledge requires o- 
phisticated structural and topological constraints to 
be manageable at all. These constraints are encap- 
sulated in a special path finding and path evaluation 
algorithm specified in Markert and Hahn (1997). 
Besides these conceptual constraints holding in 
the domain knowledge, we further attempt to reduce 
the search space for finding relation paths by two 
kinds of syntactic riteria. First, the search may be 
constrained by the type of dependency relation hold- 
ing between the content words of the currently con- 
sidered semantically interpretable subgraph (direct 
linkage), or it may be constrained by the intervening 
lexical material, viz. the non-content words (indirect 
linkage). Each of these syntactic onstraints has an 
immediate mapping to conceptual ones. 
For some dependency configurations, however, no 
syntactic onstraints may apply. Such a case of un- 
constrained semantic interpretation (e.g., for geni- 
tive attributes directly linked by the genatt relation) 
leads to an exhaustive directed search in the knowl- 
edge base in order to find all conceptually compati- 
ble role fillings among the two concepts involved. 
Syntactic restrictions on semantic interpretation 
either come from lexeme classes or concrete lexemes. 
They are organized in terms of the lexeme class hi- 
erarchy superimposed on the fully lexicalized epen- 
dency grammar we use (Hahn et al, 1994). In the 
fragment depicted in Figure 3, the lexeme class of 
transitive verbs, VERBTRANS, requires that when- 
ever a subject dependency relation is encountered, 
semantic interpretation is constrained to the con- 
ceptual roles AGENT or PATIENT and all their sub- 
relations (such as EXTENSION-PATIENT). All other 
conceptual roles are excluded from the subsequent 
semantic interpretation. Exploiting the property in- 
heritance mechanisms provided by the hierarchic or- 
ganization of the lexicalized ependency grammar, 
all concrete lexemes ubsumed by the lexeme class 
VERBTRANS, like "erweitern" (extend), inherit the 
corresponding constraint. However, there are lexeme 
classes uch as NOUN which do not render any con- 
straints for dependency relations uch as evidenced 
by gen\[itive\] att\[ribute\] (cf. Fig. 3). 
It may even happen that such restrictions can only 
be attached to concrete lexemes in order to avoid 
overgeneralization. Fortunately, we observed that 
this only happened to be the case for closed-class, 
i.e., non-content words. Accordingly, in Figure 3 
the preposition "with" is characterized by the con- 
straint hat only the conceptual roles HAS-PART, IN- 
STRUMENT, etc. must be taken into consideration for 
semantic interpretation. 
Since the constraints at the lexeme class or the lex- 
eme level are hard-wired in the class hierarchy, we 
refer to the mapping of dependency relations (or id- 
iosyncratic lexemes) to a set of conceptual relations 
(expanded to their transitive closure) as static inter- 
pretation. In contradistinction, the computation of 
relation paths for tuples of concepts during the sen- 
tence analysis process is called dynamic interpreta- 
tion, since the latter process incorporates additional 
conceptual constraints on the fly. 
The above-mentioned conventions allow the 
specification of high-level semantic interpretation 
schemata covering a large variety of different syntac- 
tic constructions by a single schema. For instance, 
each syntactic onstruction for which no conceptual 
constraints apply (e.g., the interpretation of geni- 
tives, most adjectives, etc.) receives its semantic 
interpretation by instantiating the same interpreta- 
tion schema (Romacker et al, 1999). The power of 
this approach comes from the fact that these high- 
level schemata re instantiated in the course of the 
parsing process by exploiting the dense specifications 
of the inheritance hierarchies both at the grammar 
level (the lexeme class hierarchy), as well as the con- 
ceptual evel (the concept and role hierarchies). 
We currently supply up to ten semantic interpre- 
tation schemata for declaratives, relatives, and pas- 
329 
sives at the clause level, complement subcategoriza- 
tion via PPs, auxiliaries, all tenses at the VP level, 
pre- and and postnominal modifiers at the NP level, 
and anaphoric expressions. We currently do not ac- 
count for control verbs (work in progress), coordina- 
tion and quantification. 
3 The  Eva luat ion  o f  Semant ic  
In terpretat ion  
In this section, we want to discuss, for two particular 
types of German language phenomena, the adequacy 
of our approach in the light of concrete language 
data taken from the two corpora we work with. This 
part of the enterprise, the empirical assessment ofse- 
mantic interpretation, is almost entirely neglected in 
the literature (for two notable exceptions, cf. Bon- 
nema et al (1997) and Bean et al (1998)). 
Though similarities exist (viz. dealing with the 
performance of NLP systems in terms of their abil- 
ity to generate semantic/conceptual structures), the 
semantic interpretation (SI) task has to be clearly 
distinguished from the information extraction (IE) 
task and its standard evaluation settings (Chinchor 
et al, 1993). In the IE task, a small subset of the 
templates from the entire domain is selected into 
which information from the texts are mapped. Also, 
the design of these templates focus on particularly 
interesting facets (roles, in our terminology), so that 
an IE system does not have to deal with the full 
range of qualifications that might occur - -  even re- 
lating to relevant, selected concepts. Note that in 
any case, a priori relevance decisions limit the range 
of a posteriori fact retrieval. 
The SI task, however, is far less restricted. We 
here evaluate the adequacy of the conceptual rep- 
resentation structures relating, in principle (only re- 
stricted, of course, by the limits of the knowledge ac- 
quisition devices), to the entire domain of discourse, 
with all qualifications mentioned in a text. Whether 
these are relevant or not for a particular application 
has to be determined by subsequent data/knowledge 
cleansing. In this sense, semantic interpretation 
might deliver the raw data for transformation i to 
appropriate IE target structures. Only because 
of feasibility reasons, the designers of IE systems 
equate IE with SI. The cross-linking of IE and SI 
tasks, however, bears the risk of having to determine, 
in advance, what will be relevant or not for later re- 
trieval processes, assumptions which are likely to be 
flawed by the dynamics of domains and the unpre- 
dictability of the full range of interests of prospective 
users. 
3.1 Methodo log ica l  Issues 
Our methodology to deal with the evaluation of se- 
mantic interpretation is based on a triple division of 
test conditions. The first category relates to checks 
whether so-called static constraints, effected by the 
mapping from a single dependency relation to one 
or more conceptual relations, are valid (cf. Figure 3 
for restrictions of this type). Second, one may in- 
vestigate the appropriateness of the results from the 
search of the domain knowledge base, i.e., whether a
relation between two concepts can be determined at 
all, and, if so, whether that relation (or role chain) 
is adequate. The conceptual constraints which come 
into play at this stage of processing are here referred 
to as dynamic onstraint propagation, since they are 
to be computed on the fly, while judging the valid- 
ity of the role chain in question. 3 Third, interactions 
between the above-mentioned static constraints and 
dynamic constraint propagation may occur. This 
is the case for the interpretation of auxiliaries or 
prepositions, where intervening lexical material and 
associated constraints have to be accounted for si- 
multaneously. 
In our evaluation study, we investigated the effects 
of category II and category III phenomena by consid- 
ering genitives and modal as well as auxiliary verbs, 
respectively. The knowledge background is consti- 
tuted by a domain ontology that is divided into an 
upper generic part (containing about 1,500 concepts 
and relations) and domain-specific extensions. We 
here report on the two specialized omains we deal 
with - -  a hardware-biased information technology 
(IT) domain model and an ontology covering parts 
of anatomical medicine (MED). Each of these two 
domain models adds roughly about 1,400 concepts 
and relations to the upper model. Corresponding 
lexeme entries in the lexicon provide linkages to the 
entire ontology. In order to avoid error chaining, we 
always assume a correct parse to be delivered for the 
semantic interpretation process. 
We took a random selection of 54 texts (compris- 
ing 18,500 words) from the two text corpora, viz. 
IT test reports and MEDical finding reports. For 
evaluation purposes (cf. Table 1), we concentrated 
on the interpretation of genitives (as an instance of 
direct linkage; GEN) and on the interpretation of 
periphrastic verbal complexes, i.e., passive, tempo- 
ral and modal constructions (as instances of indirect 
linkage; MODAUX). 
The choice of these two grammatical patterns al- 
lows us to ignore the problems caused by syntac- 
tic ambiguity, since in our data no structural am- 
3Note that computations at the domain knowledge level 
which go beyond mere type checking are usually located out- 
side the scope the semantic onsiderations. This is due to 
the fact that encyclopedic knowledge and its repercussions on
the understanding process are typically not considered part 
of the semantic interpretation task proper. While this may 
be true from a strict linguistic point of view, from the com- 
putational perspective of NLP this position cannot seriously 
be maintained. Even more so, when semantic and conceptual 
representations are collapsed. 
330 
biguities occurred. If one were to investigate the 
combined effects of syntactic ambiguity and seman- 
tic interpretation the evaluation scenario had to be 
changed. Methodologically, the first step were to ex- 
plore the precision of a semantic interpretation task 
without structural ambiguities (as we do) and then, 
in the next step, incorporate the treatment of syn- 
tactic ambiguities (e.g., by semantic filtering devices, 
cf. Bonnema et al (1997)). 
Several guidelines were defined for the evaluation 
procedure. A major issue dealt with the correctness 
of a semantic interpretation. In cases with interpre- 
tation, we considered a semantic interpretation to 
be a correct one, if the conceptual relation between 
the two concepts involved was considered adequate 
by introspection (otherwise, incorrect). This qualifi- 
cation is not as subjective as it may sound, since we 
applied really strict conditions adjusted to the fine- 
grained domain knowledge. 4 Interpretations were 
considered to be correct in those cases which con- 
tained exactly one relation, as well as cases of se- 
mantical/conceptual ambiguities (up to three read- 
ings, the most), presumed the relation set contained 
the correct one. 5 A special case of incorrectness, 
called nil, occurred when no relation path could be 
determined though the two concepts under scrutiny 
were contained in the domain knowledge base and 
an interpretation should have been computed. 
We further categorized the cases where the sys- 
tem failed to produce an interpretation due to at 
least one concept specification missing (with respect 
to the two linked content words in a semantically 
interpretable subgraph). In all those cases with- 
out interpretation, insufficient coverage of the upper 
model was contrasted with that of the two domain 
models in focus, MED and IT, and with cases in 
which concepts referred to other domains, e.g., fash- 
ion or food. Ontological subareas that could nei- 
ther be assigned to the upper model nor to partic- 
ular domains were denoted by phrases referring to 
time (e.g., "the beginning of the year"), space (e.g., 
4The major i ty  of cases were easy to judge. For instance, 
"the infiltration of the stroma" resulted in a correct reading 
- STROMA being the PATIENT of the INF ILTRAT ION event - ,  
as well as in an incorrect one - being the AGENT of the IN- 
FILTRATION. Among the incorrect semant ic  interpretat ions we 
also categorized, e.g., the interpretat ion of the expression "the 
prices of the manufacturers" as a conceptual inkage from 
PRICE via PRICE-OF to PRODUCT via HAS-MANUFACTURER to 
MANUFACTURER (this type of role chaining can be considered 
an intr iguing example of the embedded reasoning performed 
by the description logic inference ngine), since it did not ac- 
count for the interpretation that MANUFACTURERS fix PRICES 
as part of their market ing strategies. After all, correct inter- 
pretat ions always boiled down to entirely evident cases, e.g., 
HARD-DISK PART-OF COMPUTER. 
5At the level of semantic interpretation, the notion of se- 
mant ic  ambiguity relates to the fact that  the search algor i thm 
for valid conceptual relation paths retrieves more than a single 
relation (chain). 
"the surface of the storage medium"), and abstract 
notions (e.g., "the acceptance of IT technology"). 
Finally, we further distinguished evaluative xpres- 
sions (e.g., "the advantages ofplasma display") from 
figurative language, including idiomatic expressions 
(e.g., "the heart of the notebook"). 
At first glance, the choice of genitives may appear 
somewhat rivial. From a syntactic point of view, 
genitives are directly linked and, indeed, constitute 
an easy case to deal with at the dependency level. 
From a conceptual perspective, however, they pro- 
vide a real challenge. Since no static constraints are 
involved in the interpretation ofgenitives (cf. Figure 
3, lexeme class NOUN) and, hence, no prescriptions 
of (dis)allowed conceptual relations are made, an un- 
constrained search (apart from connectivity condi- 
tions imposed on the emerging role chains) of the 
domain knowledge base is started. Hence, the main 
burden rests on the dynamic constraint processing 
part of semantic interpretation, i.e., the path find- 
ing procedure muddling through the complete do- 
main knowledge base in order to select he adequate 
conceptual reading(s). Therefore, genitives make a 
strong case for test category II mentioned above. 
Dependency graphs involving modal verbs or aux- 
iliaries are certainly more complex at the syntac- 
tic level, since the corresponding semantically in- 
terpretable subgraphs may be composed of up to 
six lexical nodes. However, all intervening non- 
content-word nodes accumulate constraints for the 
search of a valid relation for semantic interpretations 
and, hence, allows us to test category III phenom- 
ena. The search space is usually pruned, since only 
those relations that are sanctioned by the interven- 
ing nodes have to be taken into consideration. 
3.2 Eva luat ion  Data  
We considered a total of almost 250 genitives in all 
these texts, from which about 59%/33% (MED/IT) 
received an interpretation. 6 Out of the total loss due 
to incomplete conceptual coverage, 56%/58% (23 of 
41 genitives/57 of 98 genitives) can be attributed to 
insufficient coverage of the domain models. Only the 
remaining 44%/42% are due to the residual factors 
listed in Table 1. 
In our sample, the number of syntactic onstruc- 
tions containing modal verbs or auxiliaries amout to 
292 examples. Compared to genitives, we obtained 
a more favorable recall for both domains: 66% for 
MED and 40% for IT. As for genitives, lacking in- 
terpretations, in the majority of cases, can be at- 
tributed to insufficient conceptual coverage. For the 
IT domain, however, a dramatic increase in the num- 
ber of missing concepts is due to gaps in the upper 
model (78 or 63%) indicating that a large number of 
6Confidence intervals at a 95% reliability level are given in 
brackets in Table 1. 
331 
MED-GEN IT-GEN MED-MODAUX IT-MODAUX 
# texts 29 25 29 25 
# words 4,300 14,200 4,300 14,200 
recall 57% 31% 66% 40% 
precision 97% 94% 95% 85% 
100 # occurrences ... 
? . .  w i th  in terpretat ion  
\[confidence intervals\] 
correct (single reading) 
? correct (multiple readings) 
incorrect 
nil 
? . .  w i thout  in terpretat ion  
domain model (MED/IT) 
59 (59%) 
\[48%-67%1 
53 (53%) 
4 (4%) 
0 
2 
41 (41%) 
23 (23%) 
147 
49 (33%) 
\[24%-41%\] 
28 (19%) 
18 (12%) 
3 
0 
98 (67%) 
57 (39%) 
58 
40 (69%) 
\[56%-81%\] 
38 (66%) 
0 (0%) 
0 
2 
18 (31%) 
11 (19%) 
upper model 
other domains 
? time 
?. space 
?. abstracta, generics 
?. evaluative xpressions 
.. figurative language 
. . . . . . .  miscellaneous 
3 
0 
0 
7 
11 
0 
1 
0 
23 
4 
15 
8 
12 
8 
17 
1 
234 
Ill (47%) 
\[40%-53%\] 
88 (38%) 
6 (3%) 
14 
3 
123 (53%) 
42 (34%) 
78 
0 
I 
5 
16 
3 
24 
3 
Table 1: Empirical Results for the Semantic Interpretation of Genitives (GEN) and Modal Verbs and Aux- 
iliaries (MODAUX) in the IT and MED domains 
essential concepts for verbs were not modeled. Also, 
figurative speech plays a more important role in IT 
with 24 occurrences. Both observations mirror the 
fact that IT  reports are linguistically far less con- 
strained and are rhetorically more advanced than 
their MED counterparts. 
Another interesting observation which is not made 
explicit in Table 1 concerns the distribution of modal 
verbs and auxiliaries. In MED, we encountered 57 
passives and just one modal verb and no temporal 
auxiliaries, i.e., our data are in line with prevailing 
findings about the basic patterns of medical sublan- 
guage (Dunham, 1986). For the IT  domain, cor- 
responding occurrences were far less biased, viz. 80 
passives, 131 modal verbs, and 23 temporal auxil- 
iaries. Finally, for the two domains 25 samples con- 
tained both modal verbs and auxiliaries, thus form- 
ing semantically interpretable subgraphs with four 
word nodes. 
One might be tempted to formulate a null hy- 
pothesis concerning the detrimental impact of the 
length of semantically interpretable subgraphs (i.e., 
the number of intervening lexical nodes carrying 
non-content words) on the quality of semantic inter- 
pretation. In order to assess the role of the length 
of the path in a dependency graph, we separately 
investigated the results for these subclasses of com- 
bined verbal complexes? From the entire four-node 
set (cf. Table 2) with 25 occurrences (3 for MED and 
22 for IT), 16 received an interpretation (3 for MED, 
13 for IT). While we neglect the MED data due to 
the small absolute numbers, the IT domain revealed 
MED IT 
4-nodes 4-nodes 
recall - 59% 
precision - 85% 
# occurrences ... 3 22 
? .. with interpretat ion 3 13 
. . . . . . .  correct 3 11 
Table 2: Interpretation Results for Semantically In- 
terpretable Graphs Consisting of Four Nodes 
59% recall and 85% precision. If we compare this 
to the overall figures for recall (40%) and precision 
(85%), the data might indicate a gain in recall for 
longer subgraphs, while precision keeps stable. 
The results we have worked out are just a first step 
into a larger series of broader and deeper evaluation 
efforts. The concrete values we present, sobering as 
they may be for recall (57%/31% for genitives and 
66%/40% for modal verbs and auxiliaries), encour- 
aging, however, for precision (97%/94% for genitives 
and 95%/85% for modal verbs and auxiliaries), can 
only be interpreted relative to other data still lacking 
on a broader scale. 
As with any such evaluation, idiosyncrasies of the 
coverage of the knowledge bases are inevitably tied 
with the results and, thus, put limits on too far- 
reaching generalizations. However, our data reflect 
the intention to submit a knowledge-intensive text 
understander to a realistic, i.e., conceptually un- 
constrained and therefore "unfriendly" test environ- 
ment. 
332 
Judged from the figures of our recall data, there 
is no doubt, whatsoever, that conceptual coverage 
of the domain constitutes the bottleneck for any 
knowledge-based approach to NLP. ~ Sublanguage 
differences are also mirrored systematically in these 
data, since medical texts adhere more closely to well- 
established concept taxonomies and writing stan- 
dards than magazine articles in the IT domain. 
4 Re la ted  Work  
After a period of active research within the logic- 
based paradigm (e.g., Charniak and Goldman 
(1988), Moore (1989), Pereira and Pollack (1991)), 
work on semantic interpretation has almost ceased 
with the emergence of the empiricist movement in 
NLP (cf. Bos et al (1996) for one of the more recent 
studies dealing with logic-based semantic interpreta- 
tion in the framework of the VERBMOBIL project). 
Only few methodological proposals for semantic 
computations were made since then (e.g., higher- 
order colored unification as a mechanism to avoid 
over-generation i herent to unconstrained higher- 
order unification (Gardent and Kohlhase, 1996)). 
An issue which has lately received more focused at- 
tention are ways to cope with the tremendous com- 
plexity of semantic interpretations in the light of an 
exploding number of (scope) ambiguities. Within 
the underspecification framework of semantic repre- 
sentations, e.g., DSrre (1997) proposes a polynomial 
algorithm which constructs packed semantic repre- 
sentations directly from parse forests. 
All the previously mentioned studies (with the ex- 
ception of the experimental setup in DSrre (1997)), 
however, lack an empirical foundation of their var- 
ious claims. Though the MUC evaluation rounds 
(Chinchor et al, 1993) yield the flavor of an empiri- 
cal assessment of semantic structures, their scope is 
far too limited to count as an adequate valuation 
platform for semantic interpretation. Nirenburg et 
al. (1996) already criticize the 'black-box' architec- 
ture underlying MUC-style evaluations, which pre- 
cludes to draw serious conclusions from the short- 
comings of MUC-style systems as far as single lin- 
guistic modules are concerned. More generally, in 
this paper the rationale underlying size (of the lex- 
icons, knowledge or rule bases) as the major assess- 
ment category is questioned. Rather dimensions re- 
lating to the depth and breadth of the knowledge 
sources involved in complex system behavior should 
be taken more seriously into consideration. This is 
exactly what we intended to provide in this paper. 
As far as evaluation studies are concerned ealing 
with the assessment of semantic interpretations, few 
7At least for the medical domain, we are currently actively 
pursuing research on the semiautomatic creation of large-scale 
ontologies from weak knowledge sources (medical terminolo- 
gies); cf. Schulz and Hahn (2000). 
have been carried out, some of which under severe 
restrictions. For instance, Bean et al (1998) nar- 
row semantic interpretation down to a very limited 
range of spatial relations in anatomy, while Gomez et 
al. (1997) bias the result by preselecting only those 
phrases that were already covered by their domain 
models, thus optimizing for precision while shunting 
aside recall considerations. 
A recent study by Bonnema et al (1997) comes 
closest o a serious confrontation with a wide range 
of real-world ata (Dutch dialogues on a train travel 
domain). This study proceeds from a corpus of 
annotated parse trees to which are assigned type- 
logical formulae which express the corresponding se- 
mantic interpretation. The goal of this work is to 
compute the most probable semantic interpretation 
for a given parse tree. Accuracy (i.e., precision) is 
rather high and ranges between 89,2%-92,3% de- 
pending on the training size and depth of the parse 
tree. Our accuracy criterion is weaker (the intended 
meaning must be included in the set of all read- 
ings), which might explain the slightly higher rates 
we achieve for precision. However, this study does 
not distinguish between different syntactic onstruc- 
tions that undergo semantic interpretation, or does 
it consider the level of conceptual interpretation (we 
focus on) as distinguished from the level of semantic 
interpretation to which Bonnema et al refer. 
5 Conc lus ions  
The evaluation of the quality and adequacy of se- 
mantic interpretation data is still in its infancy. Our 
approach which confronts semantic interpretation 
devices with a random sample of textual real-world 
data, without intentionally constraining the selec- 
tion of these language data, is a real challenge for 
the proposed methodology and it is unique in its 
experimental rigor. 
However, our work is just a step in the right di- 
rection rather than giving a complete picture or al- 
lowing final conclusions. Two reasons may be given 
for the lack of such experiments. First, interest in 
the deeper conceptual aspects of text interpretation 
has ceased in the past years, with almost all efforts 
devoted to robust and shallow syntactic processing 
of large data sets. This also results in a lack of so- 
phisticated semantic and conceptual specifications, 
in particular, for larger text analysis systems. Sec- 
ond, providing a gold standard for semantic inter- 
pretation is, in itself, an incredibly underconstrained 
and time-consuming process for which almost no re- 
sources have been allocated in the NLP community 
up to now. 
Acknowledgements .  We want to thank the mem- 
bers of the ~-~ group for close cooperation. Martin Ro- 
macker is supported by a grant from DFG (Ha 2097/5-1). 
333 
References  
James F. Allen. 1993. Natural language, knowledge 
representation, and logical form. In M. Bates and 
R. M. Weischedel, editors, Challenges in Natural 
Language Processing, pages 146-175. Cambridge: 
Cambridge University Press. 
Carol A. Bean, Thomas C. Rindflesch, and 
Charles A. Sneiderman. 1998. Automatic seman- 
tic interpretation ofanatomic spatial relationships 
in clinical text. In Proceedings of the 1998 AMIA 
Annual Fall Symposium., pages 897-901. Orlando, 
Florida, November 7-11, 1998. 
Remko Bonnema, Rens Bod, and Remko Scha. 1997. 
A DOP model for semantic interpretation. In Pro- 
ceedings of the 35th Annual Meeting of the Asso- 
ciation for Computational Linguistics ~ 8th Con- 
ference of the European Chapter of the ACL, pages 
159-167. Madrid, Spain, July 7-12, 1997. 
Johan Bos, BjSrn Gamb~ick, Christian Lieske, 
Yoshiki Mori, Manfred Pinkal, and Karsten 
Worm. 1996. Compositional semantics in VERB- 
MOBIL. In COLING'96 - Proceedings of the 16th 
International Conference on Computational Lin- 
guistics, pages 131-136. Copenhagen, Denmark, 
August 5-9, 1996. 
Eugene Charniak and Robert Goldman. 1988. A 
logic for semantic interpretation. In Proceedings 
of the 26th Annual Meeting of the Association for 
Computational Linguistics, pages 87-94. Buffalo, 
New York, U.S.A., 7-10 June 1988. 
Nancy Chinchor, Lynette Hirschman, and David D. 
Lewis. 1993. Evaluating message understanding 
systems: an analysis of the third Message Un- 
derstanding Conference (MUC-3). Computational 
Linguistics, 19(3):409-447. 
Jochen DSrre. 1997. Efficient construction of un- 
derspecified semantics under massive ambiguity. 
In Proceedings of the 35th Annual Meeting of the 
Association for Computational Linguistics ~ 8th 
Conference of the European Chapter of the A CL, 
pages 386-393. Madrid, Spain, July 7-12, 1997. 
George Dunham. 1986. The role of syntax in the 
sublanguage of medical diagnostic statements. In 
R. Grishman and R. Kittredge, editors, Analyz- 
ing Language in Restricted Domains: Sublanguage 
Description and Processing, pages 175-194. Hills- 
dale, NJ & London: Lawrence Erlbaum. 
Claire Gardent and Michael Kohlhase. 1996. 
Higher-order coloured unification and natural an- 
guage semantics. In ACL'96 - Proceedings of the 
34th Annual Meeting of the Association for Com- 
putational Linguistics, pages 1-9. Santa Cruz, 
California, U.S.A., 24-27 June 1996. 
Fernando Gomez, Carlos Segami, and Richard 
Hull. 1997. Determining prepositional attach- 
ment, prepositional meaning, verb meaning and 
thematic roles. Computational Intell., 13(1):1-31. 
Udo Hahn, Susanne Schacht, and Norbert Br5ker. 
1994. Concurrent, object-oriented natural lan- 
guage parsing: the PARSETALK model. Inter- 
national Journal of Human-Computer Studies, 
41(1/2):179-222. 
Hideki Hirakawa, Zhonghui Xu, and Kenneth Haase. 
1996. Inherited feature-based similarity measure 
based on large semantic hierarchy and large text 
corpus. In COLING'96 - Proceedings of the 16th 
International Conference on Computational Lin- 
guistics, pages 508-513. Copenhagen, Denmark, 
August 5-9, 1996. 
Hang Li and Naoki Abe. 1996. Clustering words 
with the MDL principle. In COLING'96 - Pro- 
ceedings of the 16th International Conference on 
Computational Linguistics, pages 4-9. Copen- 
hagen, Denmark, August 5-9, 1996. 
Katja Markert and Udo Hahn. 1997. On the in- 
teraction of metonymies and anaphora. In IJ- 
CA I '97-  Proceedings of the 15th International 
Joint Conference on Artificial Intelligence, pages 
1010-1015. Nagoya, Japan, August 23-29, 1997. 
Robert C. Moore. 1989. Unification-based seman- 
tic interpretation. In Proceedings of the 27th An- 
nual Meeting of the Association for Computa- 
tional Linguistics, pages 33-41. Vancouver, B.C., 
Canada, 26-29 June 1989. 
Sergei Nirenburg, Kavi Mahesh, and Stephen Beale. 
1996. Measuring semantic coverage. In COL- 
ING'96 - Proceedings of the 16th International 
Conference on Computational Linguistics, pages 
83-88. Copenhagen, Denmark, August 5-9, 1996. 
Ted Pedersen and Rebecca Bruce. 1998. Knowledge 
lean word-sense disambiguation. In AAAI'98 - 
Proceedings of the 15th National Conference on 
Artificial Intelligence, pages 800-805. Madison, 
Wisconsin, July 26-30, 1998. 
Fernando C.N. Pereira and Martha E. Pollack. 1991. 
Incremental interpretation. Artificial Intelligence, 
50(1):37-82. 
Martin Romacker, Katja Markert, and Udo Hahn. 
1999. Lean semantic interpretation. In IJCAI'99 
- Proceedings of the 16th International Joint Con- 
ference on Artificial Intelligence, pages 868-875. 
Stockholm, Sweden, July 31 - August 6, 1999. 
Stefan Schulz and Udo Hahn. 2000. Knowledge n- 
gineering by large-scale knowledge reuse: experi- 
ence from the medical domain. In Proceedings of 
the 7th International Conference on Principles of 
Knowledge Representation and Reasoning. Breck- 
enridge, CO, USA, April 12-15, 2000. 
Hinrich Schiitze. 1998. Automatic word sense 
discrimination. Computational Linguistics, 
24(1):97-124. 
William A. Woods and James G. Schmolze. 1992. 
The KL-ONE family. Computers ~ Mathematics 
with Applications, 23(2/5):133-177. 
334 
An Integrated Model of Semantic and Conceptual Interpretation 
fl'Oln Dependency Structures 
Udo Hahn Mart in  Romacker 
I~,: .... Groul), Text Understanding Lab, 
Fre, iburg University, D-79085 Freilmrg, Germany 
ht tp  ://www. coling, uni-freiburg, de/ 
Abstract  
\Ve propose a two-layered model for computing se- 
mantic and (:onceI)tual interpretations from del)en- 
dency struetm'es. Abstract interl)retatio,t s(:hemata 
generate semantic interpretations of 'minimal' de- 
pendency sul)gral)hs , while production rules whose 
sl)eeitieation is rooted ill ontologi(:al categories de- 
rive a canonical con(:eptual i ~terl)retation from se- 
lna.ntic int;ert)retal;ion sl;ruel;ures. Configm'ational 
descriptions of del)endeney gral)hs increase the lin- 
guistic generality of interl)rel,ation s(:hemata, while 
interfimillg sehemata nd t)ro(htcl;ions t() lexi(:al and 
COll(:el)l:tl~ll (;lass hierarchies re(ht(:es the amount and 
complexity of semantic sl)e(:iti(:atJons. 
1 Introduct ion 
The syntax/semanti(:s interface has always t)een a 
matter of com:ern  for (:OllStit; l lell(:y-l)ased feal; l lre 
grammar theories (of., e.g., Creary an(1 Pollard 
( \ ]9S5) ,  ~.'~ooIx, ( i \[989), \ ] )ah'y l l l l ) le  (1.992), Wedekind 
and Kaplan (1993)). Within the dependen(:y gram- 
lllal' COllllllllllil;y: f3r loss at, te, ntion has 1)een l)ai(t t() 
l;his tel)it:. As a (:onse(tuen(:e, ther(~ is no consensus 
how syntactic del)en(lency structures might l)e a(t- 
e(tuately transl'()rm(~d into semanti(: interl)rel;aiions 
(el., naji~:ova (:t987), Milward (1992), Lombardo et 
al. (1998) for alt;ernative proposals). 
In this paper, we introduce, a two-layered inter- 
pretation model. In a first; pass, dependency graph 
structures which result fl'om in(:remental parsing are 
immediately submitted to a semantic intcrl)reta- 
tion process. Such a process is triggered by gen- 
eral schemata whenever a semantically interl)retable 
subgraph of a syntactic dependency gral)h t)ecomes 
ava.ilable (el. Section 3). As a result, lexical items 
and the del)endency relations holding 1)etwe.en them 
are directly mat)ped to associated concel)tual enti- 
ties mid relations at; the level of semantic represen- 
tation (cf. Sections 4 mtd 5). In a subsequent steI), 
the (quasi-inferential) iml)lications of the knowledge 
representation structures emerging from the seman- 
tic interpretation stet) are accounted for l)y a pro- 
eess we here refer to as concepl, 'aal interpretation. 
The (:orresl)onding ot)erations relate to the (:(mcel)- 
tua\] representation level only and are triggered by 
a variety of production rules rooted in ontological 
categories in order to generate a canonical concep- 
tual representation f the parsed sentence (el. Sec- 
tion 6). This second level of interpretation is usually 
not taken into consideration by computational mod- 
els of semantic interpretation, either constituency- 
based nor (lei)endency-based ones, although it turns 
out to crucial for natural anguage ',,ndeTwtandin.q. 
2 Grammar  and Concept Knowledge 
Grammatical knowledge for syntactic analysis is 
t)ased on a fully texicalized dependency gl'alllltla\] 
(Itahn et al, 1994). ()ur preference for dependency 
structures is motivated, among other things, t)y the 
observation that the corresl)ondence of det)endency 
relations (holding between lexical items) to con(:ep- 
tual relations (holding between the, concepts they 
denote) is much closer than t.'or constituency-based 
grammars (Ilajicova, 1987). Ilence, a dependency- 
based al)i)roach cases inherently the descrit)tion of 
the regularities mMerlying selnantic inl;erl)retation. 
In this lexicalized el)endeney framework, lexeme 
sl)eeitications form the leaf nodes of a lexicon I)AG, 
which are further al)sl:racted in terms of lexeme 
class specilical;ions at different levels of generalit:y 
(of. Figure 1), This lea& to a lexeme (:lass hier- 
archy, which consists of lexeme (:lass names 142 := 
{VE1HIAL, VEllBINTII.ANS, NOMINAL~ NOUN~ ...} a l ld  
a subsuinption relation i.saw = {(VEI{IIINTI{ANS, 
gexenle 
Verba l  ... Preposi l ion ... Nomina l  
Verbhltrans Auxiliary Pronoun Noun 
Df'J. '= 1,,.101 , Dg'""'":= \[ \] 
D"'I~J:= { \] i D- v''n'm .= \[ \] 
I I)PP'O:= \[sul o, dirobj, imlirobj} 
VelbTrallS u,erde*t passive Sl~eicherOncmory) 
::lglir?bj:- \[di,'ol,j} R+:= \[patten! CO-l,ali,'nt } 
:. qdm,l,j:= \[ } mit (with) 
)i~:lt!rll (deliver) R+ := \[has-part instrmm,nt co-patient ..} 
Figure l: Fragnmnt of the Lexeme Class Hierarchy 
271 
propo: ____~ 
ka~n sublCCt: ~- -  ~_~ovorbpar t  
Fesl latte S p ~ t t :  werden 
v~orDpa rt. 
Die Compellers pps~ferl spec:~. ~ ~..~.att 
des mit yon pobjo~,:'?"~ "-~o~jo~t 
350Mhz-OPU Transtec 
7he hard disk of flw ?vmlputer with 3.$OMhz-CPU call - b)' 7ra/t.~lrt:- be delivrred. 
Figure 2: A Sample Dependen(:y Grat)h 
VERBAL), (NOUN, NOMINAL) ,  . . .}  C "l/V X \]/V. Inher -  
i tance  of grammar knowledge is based on the idea 
that constraints are attached to the most general 
lexeme classes to which they apply, leaving room for 
more and more specific (possibly, even idiosyncratic) 
specifications when one descends this hierarchy. 
A dependency grammar captures binary con- 
straints between a syntactic head (e.g., a noun) and 
one of its possible modifiers (e.g., a determiner or 
an adjective). In order to establish a dependency 
relation ~ C :D := {specifier, s~fl~iect, dirobject, ...} 
between a head and a Inodifier, lexenle-class-specific 
constraints oil word order, comt)atit)ility of Inor- 
phosyntactic features and senmntic integrity must 
be fiflfilled. Figure 2 depicts a dependency graph in 
which word nodes are given in bold face and depen- 
dency relations are indicated I)y labeled edges. 
Conceptual knowledge of the underlying domain is 
expressed in terms of a KL-ONE-like knowledge rep- 
resentation language (Woods and Schmolze, 1992). 
The domain ontology consists of a set of concei)t 
names 9 r := {COMPANY,  tIAI~I)-.DI.g\[<, ...} and a sub- 
sunlI)tion relation i saf  = {(HAIiD-DISK, NTOHACF- 
DFVICF), (TRANS'rI.:(~', COMPANY), ...} C ~P x 
Y.  The set of re lat ion nantes ?g : - -  {IIAS-t'All;F~ 
I )EI,  IVE I~-AGENT,  . . .}  denotes con(:eptual relations 
which are also organized in a subsmnption hierarchy 
isa?z = {(ttAS-IIARI)-DISK, ItAS-PItYSICAL-PAVI'), 
( I lAS -PHYS ICAL-PAR.T ,  I tA.q-PAltSI ' ) ,  . . .} .1 Examples 
of emerging concept mid relation hierarchies are de- 
picted in Figure 3 (right box). 
Ill our at)proach, the representation languages for 
semantics and domain knowledge coincide (for argu- 
ments supporting this view, el. Allen (1993)). Link- 
ing lexical items and concet)tual entities proceeds 
as follows: Upon entering the parsing process, each 
lexical itern w that has a (:onceptual correlate C in 
the domain knowledge base, w.C E Y (mostly verbs, 
Ilouns and adjectives), gets immediately instantiated 
in the knowledge base, such that for any instance Iw, 
initially, 2 type(I,,) = w.C holds (e.g., w = :'Fest- 
platte", I,,, = HAI{D-Dlst,:.2, w.C = type(HaI~.D- 
DISK.2) = HARD-DISK). If several conceptual cor- 
relates exist, either due to honmnymy or polysemy, 
I Atl sllbstllnpl;ioIl ,elations, isaw, isa:v, and isw~, arc 
considered to be transitive and reflexive. 
2For instance, anaphora might necessitate changes of this 
initial reference assignment, el. Strube and Ilahn (1999). 
i Syntactic Level -> Conceptual Level 
I 10op-'o,~): 
!subl\[ecl\]'~-------------t-~aecnt Action C,'dcgory 
I)l.'.~',d?lfe.c't\]~paticnt~,s.a..< is-at: i~-~?: 
dirobjfect\] ~ ' \ \  \[CAT-Vrans fct'-no,)d 
: I1" oo-pa,i . 
has-part \ ~k ..... :C' - -  Persal 
gen\[itive\]att\[ribute\] I " \ \ l ,  divery 
\ l  delivcr.aealt 
vcrbpart II instrumcnt "},,a;,,2J',,:ti2 '~"""~' 
ppatt\[ributc\] II destination ~ ?--! ..... P ...... -- I deliver-recipient 
spcc\[ificr\] / I  
l ,  : . . . .  ~ L 
Figure 3: Relating Grammatical ( eft box) and Con- 
ceptual I(nowledge (right box) 
each lexical ambiguity is processed independent.ly 
within set)arate (:ontext partitions of the km)wledge 
base (Romacker and Hahn, 2000a). 
3 Interpretable Subgraphs 
II1 the parse tree from Figure 2, we can distinguish 
lexical nodes that have a conceptual correlate (e.g., 
"Fcstplatte" relating to HAl{D-DIsK, "gelicfert " re- 
lating to DEIAVh;RY) from others that do not have 
Such a correlate (e.g., "mit" (with), "yon" (by)). Se- 
mantic interpretation capitalizes oil this distinction 
in order to tind adequate conceptual relations be- 
tween the corresl)onding concept, insta.nces: 
D i rect  L inkage.  If two word no(tes with (:oncet)- 
tual correlates are linked by a single depen(ten(:y re- 
lation, a direct linkage is given. Such a subgraph 
can immediately be interpreted in ternis of a (:on- 
ceptual relation licensed by the correspondiitg de- 
l)endency relation. This is illustrated in Figure 2 by 
the direct linkage between "Festplatte" (hard disk) 
and "Computers" via the gen\[itive\]att\[ribut(~\] rela- 
tion, which gets nmpped to tile IIARD-DISK-OF role 
linking the corresponding con(:eptual correlates, viz. 
HARD-DISK.2 alld C()MPUTER-,~YSTFM.4, respec- 
tively (see Figure 4). This interpretation uses only 
knowledge about the concet)tual correlates and the 
linking dependency relation. 
Ind i rect  L inkage. If two word nodes with con- 
ceptual correlates are linked via a series of depen- 
Delivery.10 
. . . . . .  c~. . . . . .  Transtec.9 
de|lver -agent !  
. . . .  - - -41~- - -  Hm'd-Disk.2 ! deliver -pattellt 
\[ - - (~}- -  COlllpllie\[ Syst('lll.4 
hard-disk-of 
\[ (Ji 350Mhz CPU.6 
hai-qm 
Figure 4: Semantic Interpretation of the Depen- 
dency Graph fl'om Figure '2 
272 
dency relations and none of tile il ltervening no(les 
have a conceptual correlate, an indirect linkage is 
given. For such a "nfinimal" sut)grat)h, semantic in- 
tert)retal;ion is made dependent on lexical informa- 
tion from the, intervening nodes, as well as knowledge 
aboul; the conceptual correlates and del)cn(lency i'e- 
lations. Figure 2 il lustrates such a eontiguration 
lay tile linkage between "Com,putcr.s" and "350Mhz- 
CPU"  via the intervening node "mit" (will O and 
the pI)att\[ributc\] mid l>objc, ct relations, the re, suit <)f 
which is a conceptual linkage between COMI'UTFI/.- 
SYSTI,'M.4 and 350N/IIIZ-Cl'U.6 via the relation HAS- 
CI'U in Figure 4. 
In oMer to increase tile generality and to t)reserve 
the simi)licity of semantic interpretat ion we intro- 
duce a generalization of the notion of del)emlency 
relation such that it incorporates direct as well as 
indirect linkage: Two content words (nouns, adjec- 
tives, adverbs or flfll verl)s) stand in a 'me.dialed syn- 
tactic relation, if one can t)ass from one word to 
the other along the connecting edges of the (tcl)en- 
(tency gral)h wit;hour traversing word nodes other 
than t)repositions, modal or auxil iary verl)s (i.e., el- 
ements of eh)sed woM classes). In Figure 2, e.g., 
the tuples ( "Fcstplattc.", "Co'm,p',,ter.s ") or ( "6'om,- 
putcrs", "350Mh, z-CPU") stand in mediated syntac- 
t i t  relations, whereas, e.g,, the tuple ("Fest, plattc", 
"Tra'nstcc") does not, since the comiecting 1)ath COlt- 
rains "gelicfc.rt" (dclivcrcd), a (;Ollt;elll; word. 
This leads to the following detinition: Let w and 
'w' be l, wo  (:onl;enI; words  in a senten( ; ( ;  ,5'. 1111 addi- 
l;i(-)n, lel; 'w2 , . . .  , 'wn-1 E S ('11, ~ 2) l)e l / re l )os i l ; i ons  , 
auxil iary or modal verl)s, and wl := 'w an(1 w,  := 
'u/. Then we say thaJ; 'w and 'w' st:and in a 'nt(:diat('d 
syntactic r(:lat, ion, iff there exists an inde.x I C {1, 
. . . ,  n} so that the following two (:on(litions hold: 
1. 'wi is modifier of'wi+l for i C {1, . . . ,  l-1}; 
2. wi is head of wi+\] for i E {I , . . . ,  n- l}. 
We call a subgral)h identitie(1 t)y su(:h a s(!rics 'w j, 
. . .  , w ,  a, sc'm, anl, ically intcrFrc, tabh', .s"u, bgraph, of 1;11(,' 
dependency graph of S. The, detinMon of a medi- 
ated syntactic relation encoml)asses the notion of a 
direct linkage (n := 2, so t;hat an empty set of in- 
tervening nodes emerges). The special eases 1 := 1 
and I := n yield an ascending and descending series 
of head-modif ier relations, resl)ectively. 
4 Semant ic  In terpretat ion  Mode l  
The model of semantic intert)re.tatiol~ we l)rOl)OSe 
(:Oral)rises two (:onsl,raint layers. First, sl, atic (:on- 
straints fl)r semmltic intert)retation derived from di- 
rectly mapping dependency relations to conceptual 
roles, and, second, a search of the knowledge t)ase 
which dynamically takes these static constraints into 
account. The translation from the syntacti(" to the 
semantic level is achieved in a strictly COml)ositional 
way l)y incrementally c()mbining the conc(;ptual l"(;I)- 
resentations of semantically interl)retat)le sul)gral)hs 
until the entire del)endeney graph ix processed. 
S ta t i c  Const ra in ts .  Intert)retation 1)rocedures 
operat ing on semantical ly interi)retable, subgrai)hs 
may inherit restrictions from the tyl)e of dependency 
relations or from the lexical material  they incor- 
1)orate. Constraint knowledge from the g;ranmmr 
level comes in two varieties, viz. via a positive list,, 
Icx v.l D4 . . . .  , and a negative list,, D\]  (''~'~'?l, of det)endcncy 
relations, tronl which adlnitted as well as excluded 
concel)tual relations, //,~ and /~,_, resl)ectively, are 
derived 1)y a silnple static symbol mal)l)ing. 
Knowledge at)out D_~ .......... I and Dj  c*val is part of 
the valen(:y sl)e(;itications. It is encoded at the level 
of lexeme classes l/V, such that lcxval C )IV x "D. By 
way of 1)rol)erty inheritance this knowledge is passed 
on to :ill subsumed lexical classes and instances. 
For insta.nce (of. Figure l), the lexeme class of in- 
transit ive verbs, VEI IBINTI /ANS G ~/~), d()\[ i i les fo r  igs 
sul).icct valency D ~! v''W"rr' .......... kiccO := {sul).icct} 
~lll(l \ ] )  {r, crbinlrans, sul, jccl.) _ ' := (/), whereas for 1)ret)osi- 
? I )  (vcrbinlrans, plmdj) tiolml a(liuncts we i'equire ~.~_ := 0 
anti D (,,~,.I,h,,. .. .... m,a4i) := {sul)ject, dirob.ject, in- 
dirol)ject}. All these constl'ainl;s arc inherited t)y 
the lexenm class VIgtlBTI/AN,S. \?e thell  distinguish 
tin'co t)asic cases how (:orrespmMing constraints may 
alib, ct semantic interl)rctation 1)roc(!sses: 
I. Knowledge availalfle f lom ,qy'lfl;ax deter'minas 
tim st;mind;it interl)retation , if Dq{ c~:v"! ~- 0 and 
D\ ]  ......... t = ~ (e.g., the subject of a verb). 
2. Knowledge availal)le from syntax vt',stricLs' t.he 
semanl;i(-intcrpr(;l.a.tion~ if' Dq{ (':''''''t = ~ mM 
1)\] .......... t ?_ 0 (e.g., for 1)reposit.ional djuncts). 
3. if' Dq{ ....... I = 0 all(1 D / ........ 1 = (/), l ie  syntact i ( :  
(:(mStl'aints ap1)ly and semanl.ic interl)re, l;ation 
pr()(-eeds e, ntire, ly concept-driven; i.e., it relies 
on domain knowledge only (e.g., for genitives).a 
hl order to transfer syntacl;ic constraints to l;}le 
eoncel)tual level, we define i: 7) --+ 2 "n, a mal)t)ing 
fi'onl det)endency relations onto sets of concel)tual 
relations. Some of these mal)l)ings are already de- 
I)icted in Figure 3 (e.g., i(subjecO := (aC~NT, PA- 
TIENT}). For del)en(lency relations 5 E "D that can- 
not })e linke, d ;I priori to a concel)tual relation (e.g., 
g(,n\[itiv@,tt\[ributeJ), we require i(5) := 0. 
The (:oncel)tual restrictions, R+ and/~,_, must be 
(:Oinlmt.ezl from Dq{ <':vat and D_ h,xvaz, respectively, 
1)y al)l)lying the interi)retation flmction i to each el- 
ement of the corresponding sets. This leads us to 
1~+ := {,  I :': e V,!  ........... z/~ y C i(:,,)} ~uld \]~ := 
{V I :" e n2 '* ' "  a , e i(:,')}. 
a\Ve hawe current ly  ilo eml)irical evidence for the fourth 
possible (:as('., where 1) lca'wd ~k 0 and 1) lcxval ~ O. 
273 
Dynamic Constraint Processing. Semantic 
interpretation implies a search in the knowledge base 
which takes the constraints into account that de- 
rive fl'om a particular dependency parse tree. Two 
sorts of knowledge then lmve to be combined - -  first, 
a pair of concepts for which a connecting relation 
path has to be determined; second, conceptual COl> 
straints on permitted and excluded conceptual rela- 
tions when connected relations are being computed. 
The first constraint ype ineorporates the content 
words linked by the semantically interpretable sub- 
graph, the latter accounts for the pm'ticular depen- 
dency relation(s) holding between them. Schema (1) 
describes the most general mapping fl'om the coil- 
ceptual correlates, h.Cfrom and 11t.Cto~ ill ~ of tile 
two syntactically linked lexical items, h and m, re- 
spectively, to connected relation paths 12~o,,.. 
)c x 2 ~ x 2 v? x .7 -~ 2/~ .... 
";: (CI,.o,,~ , 12+, 12- ,  C,o) ~ 12~o~,, (1) 
A connected relation path rel~o,, C R .... is defined 
by: 
relcon((rt,..., rn)) :?:;' Vi C {1, ..., n - 1} : 
A relation path is called connected, if for all its n 
constituent, noncomposite relations ri the concept 
type of the domain of the relation ri+l subsumes 
the concept ype of the range of the relation ri. 
To compute a semantic interpretatiol~, si triggers 
a search through the knowledge base and identifies 
all commeted relation paths from Cf~.o,, to Cto. Due 
to potential conceptual ambiguities in interpreting 
syntactic relations, more than one such path may 
exist (hence, we mat) to the power set of 12~o,~). In 
order to constrain connectivity, si takes into con- 
sideration all conceptual relations 12+ C Td a priori 
permitted for semantic interpretation, as well as all 
relations R_ C 7~ a priori excluded. Both of them re- 
flect the constraints set 11 t) by particular (lel)endency 
relations or non-content words figuring as lexical re- 
lators of content words. Thus, tel G R~o,, holds, if 
tel is a connected relation path from Cf,.o,,~ to Cto, 
obeying the restrictions imposed by 12+ and 12_. 
If the fllnction si returns the empty set (i.e., no 
valid interpretation can be comtmted), no depen- 
dency relation will be estat)lished. Otherwise, tbr 
all resulting relation paths RELi E I~con an asser- 
tional axiom of tile form (h.Cfro,n I~.ELi m.Cto) is 
added to the knowledge base, where RELi denotes 
tile i th reading. If i > 1, conceptual ambiguities oc- 
cur, resolution strategies for which m'e described ill 
Romaclmr and Hahn (2000a). 
To match a concept definition C against ile con- 
straints imposed by 12+ and 12_, we define the func- 
tion get-roles(C) =: CR, where C12 denotes the set 
of conceptual roles associated with C, which are then 
used as starting points for the path search. For ease 
and generality of st)ecification , R+ and li~_ consist of 
the most general conceptual relations only. Hence, 
the concrete conceptual roles CR and the general 
ones in R+ and R_ may not always be compatible. 
So prior to semantic interpretation, we expand I2+ 
and R_ into their transitive closures, incort)orating 
all their subrelations in the relation hierarchy. Thus, 
12; :-- { ,.* e I ,' 12+ : is,  ,. }. R*_ is 
correspondingly defined. 12+ restricts the search to 
relations contained in C12 rq 12"4-, iffR+ is not empty 
(otherwise, all elements of C12 are allowed), whereas 
12_ allows only for relations in C12 \ 12"_. 
5 A Sample  Semant ic  In terpretat ion  
Whenever a semantically interpretable subgraph is 
complete, selnantic interpretation gets started inl- 
mediately. As an example, we will consider a case of 
indirect linkage, as illustrated by the occurrence of 
auxiliary and modal verbs within a passive clause. 
When interpreting indirect syntactic relations, in- 
fl)rmation not only about content word nodes but 
also about intervening noncontent word nodes be- 
comes available. This way, further static constraints 
are imposed on R+ (and 12_) in terms of a list RI~.~. 
C T~. of permitted conceptual relations. This infor- 
mation is always specified at the lexcme level. Since 
12t~:~ relates to closed-class i|;ems only, the required 
nmnber of specifications i easy to survey. 
In our example (ef. Figure 2), the content words 
"Festplatte" (h, ard disk) and "flcliefcrt" (dclivered) 
are linked by a mediating modal verb ("lvann" (can)) 
and a passive auxiliary (%;erden" (bq ..... i,,c)). The 
semantic interI)retation schema tbr passive auxil- 
im'ies (2) addresses the concept ype of the instance 
fbr their syntactic subject, Cs,o,j = t!/pe(I.~ul, j) = 
HARI)-DISK, and that tbr their verbpart, C,,e,,1,pa,.t 
= type(Iv~:,.bv~rt) = DELIVERY. The relation be- 
tween these two, however, is determined by Rva**a,~:~. 
:= {PATIENT, CO-PATIENT}, constraint knowledge 
which resides in the lexeme specification for %mr- 
den" as passive auxiliary (of. Figure 1). 
: , , 12 o,,, (2) 
With sia,,x(DELIVEI{Y, {PATIENT, CO-PATIENT}, 
~, HAIl.D-DISK), we get the concet)tual relation 
DEH\ Ea-IATmNT (of. Figure 3), since HM{D-DISK 
is subsumed by PI~ODUCT and, thus, a legal filler of 
DELIVER-PATIENT C 12passaux" 
6 Conceptua l  In terpretat ion  
Conceptual interpretation uses a production rule sys- 
tem (Yen et al, 1991.) which accounts for charac- 
teristic patterns of assertions that result froth the 
semantic interpretation process. While the outcome 
of semantic intert)retation (cf. Figure 4) still adheres 
274 
to the surfa(:e fOl'Ill ()t' Ill(} parse(l sent(m('('. (:(m(:el)- 
tual i(ltt?l'pl(~tali()ll abstla(:ts away' fr()ill 1 }IOS(~ Slll'faC(} 
1)\]1(UH)I11(~I1~1 ~11((\[ CI'O,~/L(~a ~l ; l IO l ' I l I~d iz (~d '~ ( :a l tO l l i ca l  
(:OllCel)tual ret)resentatiolt of lhe inlmt, as need(~(t, 
e.g., for mfiformly queryilig th(} kl(owlr;dge lmse. 
As an exanq)le of such inferences consider Figure 
5. with the I)ELI\:I't/S relation linking TI/ANS'II.'(;.9, 
a haMware supplier, aud H..\IiI)-I)I.sI,:.2. By (:om- 
imting a (:on(:eptual relation representing the m> 
de(lying A(:TION TI/AN.qTE(:.!) and tIAItl)-I)I.qK.2 
are integrated in a n(n'malized (:(m(:ept graph. Note 
that the corl'eSlmnding lexi(:al it(!ins, "Tran.s'tec" and 
'q+stplatte" (hard disk), are not linked via a me- 
diated synta.cti(: relation in Figure 2. tlence, we 
lnay (:learly discern semanti(: interl)retation , which 
operates on sinqh, semantically interl)retabl(~' sub- 
graphs only, from concel)tual interi)retation , where 
the inferenc(>llased interl)retati(m of relationshil)s 
among different sift)graphs (:onles int(~ I)lay. 
An ind(~t}(uldent level for (:(m(:ei)tual imerpreta- 
(tOll also bo.came a necessity due to analyti(: consid- 
erations. Often the to(:al constraints for (:onc(~t)tual 
roles of ACTION, ~T..VI'E, or EVENT concepts (:armor 
be formulated restrictive enough for the semantic 
interpretation process. For exanlple, the. (xmceptual 
correlate of the verb "possess" does not impose any 
restri(:tion on its I'ATIENT role (linked to the siltlj(}(:t 
(lotmn(tency relation in a ,~enlantically into.rpretal)lc' 
sul)grat)\[ O. I{ather, restric.tions apply to l(roperly 
relatin9 the filM of the. PATIENT slot with that of 
the CO-PATIENT slot (dirobjoct, at lh0. do.1)enden(:y 
h.'vel). C, oncet)tual interl)retation rules are a means 
to further constrain these :cont(.~xt--sensitive ' aslm('ts 
of the interpretation I)rocess. 
Sin(:(} verbs play a prominent role in dcI)emlency 
gratmnars, the production rule system for con(:el> 
tual interpretation is ba.qed ut)on the conceptual COl'- 
relat, es of verbs (h/}ncetiwth verb concepts) in the 
k(lowl(.,(lg(.' base. Ditfer(}nt views are defined for ve,b 
con(:epts t) 3' using three ~d)st;racti(m dinmnsi(ms. 
First, verb concel)tS are classified, ac(:()rding to 
the set of thematic roles riley supply, as ACTION, 
~TA'I'E or Pt(OCES$. I)EIAVH{Y, e.g., is assigned to 
:\(TI'ION, Sill(;(} both A(;ENT and I'ATIENT form part 
of the concept detinition (of. Figure 3, right box). 
The second level of al~straction (:ol).sists (~f (:at:ego- 
r izations which reflect a ('omnmn core meaning. The 
upnlost conceptual node in this hierarchy is CATI.;- 
(;()I/Y. \]-)HAVH/Y, e.g., is considered as a C(}IIcopi 
whi(:h repres(}l~tS he ACTION of transfering a GOOl) 
"l'l'al IS;t ec.9 
i ?:~:, I la l 'd- i ) isk.2 
deliverl \[ . . . . .  I.l . . . .  ('OIllplll el" Sysielln.4 
hard di,k-ur \[ ~I~:: , 35(Mhz-CPU 6 
has-cpu 
Figure 5: A Sample Concel~tual Interllretati(m of the 
1)op(m(lency Graph fl'om Figure 2 
t() a customer. All verb (:(mcet)ts belonging t() this 
Ci ( l ; (~ro l 'y  HI'O S l l l )S ( l l l lOd  })y the correst)onding COl> 
COl)t. (2AT--TIIAN.qFI.;II..G()OD. (\Ve here make llse 
of nnfll, ilfle inheritance ln(!(:hanisnls.) 
Finally, every verb con(:et)t is linked to s(-)Ill(~ 
\-Iqt/I/-~IOI)F.I, \])I'LI\;EI/X/or ally other \,el'l) concept 
()f the C_-\T--TII.\NSI.'EI/-(~OOl) category is a con-. 
stiLu(,.nt i)hase of the BI;Y-ANI)-,~I.;IdJ-~\[OI)EL. "l\]() 
gem'.ralize appr()t)riately fr(nn individual verbs, vml) 
cat.egorie:; were extracted from ore text corpora that 
further refin(~ a large-scale taxononly for Gernmn 
verbs (Balhner and Brelmenstuhl, 1986). hi this 
\v(M(, a total  of about  20,000 verbs \\;ere subsumed 
by 700 categories to reflect a semantic generalization 
ill l:erms of a hie.rarchy of verb categories. 
The production rules for cont:el)tual interim;Cation 
operate on this calegorial hierarchy. Every verb con- 
Cel)t in the hierarchy is a sul)con(:ept of exactly one 
(:ateg()ry in the knowledge base. Whe,mver the pre- 
c(mditions of ;111 imert)retati(m rule are fulfilled, a 
concel)tual interl)l'(.'tat, iolx ix con(puled. 
Coil(:el)tual and semanti(: int(U'l)retation d('.pend 
on each other, since the bast(: interpretation schema 
(of. expression (1) in Section 4) is supplied with ac- 
tual t)aramete.rs frolll t)rodu(:tion rules. We there- 
fore may de.line another specialization of the basic 
int(.wl)retation schenla for (;on(:el)tllal interpretation 
sico,,,,. In particular, path searches are triggere(l 
that are re.~;ia'i(:ted by a positive list l'end(~red l)y the 
apl)licable production rule. 
For our ,~ample sent(}n(:e (of. Figures 2 and 4), the 
/:oncel)tual (:orr(,latc' for the verb "delivers" (DI,:- 
IAVEH.Y) is a sul)concept of A(:TIt)N. Addition- 
ally, l)l.'l.IVl.:l/Y is a mlbconcei)t of the (:ategory 
CAT--Tt/:\';SFI.'I~-Go()I) ((:f. Figure. 3). The. corre- 
sponding (:on(:el)tual interl)retation rule is given in 
Fi{{(lro, (J. Wht}llever }111 instance of /,he category 
(~A'I'-TItAN,qFEIt-.(~OOI) is encounter(,.d and \[)oth its 
:\(IENT and I'ATIF.NT r(}los ale. filled, relation paths 
are (:omput;ed from th(; types of the two instances 
involved, a and p, reslm(:tively. For each relation 
found by the search algorithnl (l~\]'2L in Figure 6), 
a correspondil~g assertion is added to the knowledg(.' 
base (TELL  in Figure 6). In the examt>le, the in- 
terl)retatioll s(:hmna is inst;mMated with the 4-tuple 
((:OMI':\NY, {TI~:\NSI"I':I~S-(;O()D}, {}, H.M~I/-I)IsK) 
resulting in t, he comfmtation of {I)I:.LIVFA/S} as the 
in'Olw.r re.lation link (of. Figure 5), since it is a sub- 
ro.lation of TI/AN.qFEI/S-(;OOI). 
EXISTS v, a, p: 
t; : CAT-TRANSH.:R-(~OOI)F1 
t' A ( IENT ,'1 M o PAT I I , 'NT  p 
IF .~i ........ (tUp,'(,z), {T,..~NSF,.:RS-GOO,)}, {}, tVp,~(p)) ? 
TItEN 
I{EL := .,;i ........ (t!jpc(~), {TII~.NSI.'I.:ItS-C;OOl)}, {}, type(p)) 
TELl, o, Itl,;l~ p FOR,AId; Itl,:l, ~ HEL 
Figure 6: Samt~le (~onceptual Imo.rpr0.tati(m I/,ule 
275 
7 Evaluat ion 
We evaluated this approach to senmntie intert)reta- 
tion on a random selection of 54 texts (comprising 
18,500 words) from two text corpora, viz. consumer 
product test reports and medical finding reports. 
For evaluation purposes, we concentrated on the in- 
terpretation of genitives (as an instance of direct 
linkage) and on the interpreta.tion of t)eriphrastic 
verbal complexes, i.e., passive, temporal and modal 
constructions (as instances of indirect linkage). 
The underlying ontology consists of an upper 
generic part (containing about 1,500 concepts and 
relations) and domain-specific extensions relating to 
information technology (IT) and (parts of) mmtomi- 
cal medicine (MED). Each of these two dolnain mod- 
els adds about 1,400 concepts and relations to the 
upper model. Corresponding lexeme entries in the 
lexicon provide linkages to the entire ontology. 
We considered a total of 247 genitives in the smn- 
ple. Recall was higher for medical texts (57%) than 
for IT documents (31%), though, in general, rather 
low. However, precision peaked at 97% and 94% fo," 
medical and IT texts, respectively. The mnnt)er of 
syntactic onstructions with modal verbs or auxil- 
iaries amour to 292 exmnples. Compared to geni- 
tives, we obtained a slightly more favorable recall 
for both doinains 66% tbr MED, 40% for IT - - ,  
while precision dropped slightly to 95% and 85% for 
nmdical and IT documents, respectively. 4 
As with any such evaluation, idiosyncrasies of the 
(;overage of the knowledge bases are inevitably tied 
with the results and, thus, put limits on too far- 
reaching generalizations. However, our data reflect 
the intention to submit a knowledge-intensive text 
un(lerstmlder to a realistic, i.e., conceptually un- 
constrained and therefore "unfriendly" test environ- 
ment. Judged from tile figures of our recall data, 
there is no doubt, whatsoever, that conceptual cov- 
erage of the domain constitutes the bottleneck for 
any knowledge-based approach to NLP. s Sublan- 
guage differences are also mirrored systematically in 
these data, since medical texts adhere more closely 
to well-established concept axonomies and writing 
standards than magazine articles ill the IT domain, 
whose rhetorical styles vary to a larger degree. 
8 Re lated  Work 
The standard way of deriving a semantic interpre- 
tation for constituency-based grammars is to assign 
each syntactic rule one or more semantic interpreta- 
tion rules (e.g., van Eijck and Moore (1992)), and to 
4A more detailed presentation of this evaluation study is 
given in Romacker and Itahn (20001)). 
5For the medical domain at least, we are currently actively 
pursuing research on the semiautomatic creation of large-scale 
ontologies from weak knowledge sources, viz. medical termi- 
nologies; cf. Schulz and Hahn (2000). 
determine the meaning of the syntactic head fl'om its 
constituents. This approach as also been adopted 
in the few explicit attempts at incorporating seman- 
tic interpretation i to a dependency grmninar fi'mne- 
work (Milward, 1992; Lombardo et al, 1998). There 
are no constraints on how to design and orgmfize this 
rule set despite those that are imI)lied by the choice 
of the semantic theory. In particular, abstraction 
mechanisms (going beyond the level of sortal tax- 
onomies for semantic labels, cf., e.g., Creary and 
Pollard (1985)), such as property inheritance, de- 
faults, are lacking. Accordingly, the number of rules 
increases rapidly and easily reaches orders of sev- 
eral lmndreds in a real-world setting (Bean cta l . ,  
1998). As an alternative, we provide a small set 
of gencric semantic interpretation schemata (by the 
order of 10) and conceptual interpretation rules (by 
the order of 30 for 200 verb concepts) instead of 
assigning specific interpretation rules to each gram- 
mar item (in our case, single lexemes), and incor- 
porate inheritance-based abstraction in the use of 
these schemata during the intert)retation process in 
the knowledge base. We clearly want to point out 
that while this rule system covers a wide variety 
of standard syntactic constructions (such as gent- 
tives, prepositional phrases, various tense and modal 
forms), it currently does not account fbr quantifica- 
tional issues (like scope ambiguities) for which en- 
tirely logic-based approach (Charniak and Goldman, 
1988; Moore, 1989; Pereira and Pollack, 1991) pro- 
vide quite sophisticated solutions. 
Sondheilner et al (1984) and Itirst (1988) treat 
semantic interpretation as a direct mapt)ing front 
syntactic to conceptual rel)resentations. They also 
shm:e with us tim representation f doinain knowl- 
edge using Kl,-ONE-style terminological languages, 
and, hence, they nmke heavy use of property inher- 
itance (or typing) inechanisms. The main diflbrence 
to our approach lies in the status of the semantic 
rules. Sondheimer et al (1984) attach single in- 
terpretatiotl rules to each r'olc (filler) and, hence, 
have to provide utterly detailed specifications re- 
flecting the idiosyncrasies of each semantically rele- 
vant (role) attachment. Property inheritmme comes 
only into play when the selection of alternative se- 
mantic rules is constrained to the one(s) inherited 
from the most specific case frame. In a similar way, 
Hirst (1988) uses strong typing at the coueeptual 
object level only, while we use it simultaneously at 
the grmnmar and the domain knowledge level for the 
processing of semantic schemata. 
9 Conclus ions 
We introduced an al)proach to the design of com- 
pact, yet highly expressive senmntic interpretation 
sdmmata. They derive their power from two sources. 
First;, the organization of grammar and domain 
276 
knowledge, its well as semantic interpretation mcch- 
alliSllIS: al'e lmsed on inheritance principles. Soc- 
ou(1, interpretation schemata l)stract from 1)articu- 
lar linguistic phenomena (spe('ilic lexical items, lex- 
eme classes or dependency relations) ill terms of gen- 
eral contiguration l)atterns in (tepo.nden(-y gral)hs. 
Underlying these design decisions is a strict sep- 
aration of linguistic and eouceptual knowledge. A 
clearly defined interface is provided which a.llows 
these st)ecitications to nmke reference to line-grained 
hierarchical knowledge, no ma.tter whether it is of 
gramnmtical or conceptual origin. The interface is 
divided into two levels. O11o nmkes use of static, 
high-level (:onstraints ut)l)lied l)y the nlal)l)ing of 
syntactic to conceptual roles or sut/1)lied its the 
meaning of closed word classes. The other uses these 
constraints in a dynanfic search through a knowl- 
edge base, that is l/arametrized by few and simI)le 
schenmta. Finally, at the level of conceptual in- 
terprotatiou inferences emerging fl'om senmntic rq)- 
resentations are COml)uted by a s(.'l; of t)roductious 
which make reference to a verbcategorial hierarchy. 
Also since the numl)er of s(:hentata at the semantic 
description layer remains ratho.r sulall, their o.x('.cu- 
tion is easy to tra(:e and thus SUl)l)orts the main- 
tenanco of largo-scale NLP systenls. The high ab- 
straction level 1)rovided by inheritance-based seman- 
tic sl)ecilications allows easy 1)orting across (liflhr- 
ent al)l)lication domains. Our exl)orienco resls on 
reusing the set of s(;mald;ie sehenmta once deveh)t/ed 
for the information technoh)gy domain in the nm(ti- 
ca\] domain without further (:hallges. 
A(-knowledgments.  Wc want to thank the members 
of the l~{group for close (;OOl)eration. M. Romacker was 
SUl)t)ortcd by a grant fi'om DFC (Ita 2097/5-11. 
Re ferences  
J. Allen. 1993. Natural language, knowledge rep- 
resentation, and logical tbrm. In M. Bates and 
R. Weischedel, editors, Challc'nges in Natural Lan- 
guage Proccssinfl, pages 146 175. Cambridge Uni- 
versity Press. 
T. Balhn(?r and W. Brenncnstuhl. 1986. Deutsche 
Vcrbcn. Einc ,sprach, analytisch, c Untcrsuchunfl des 
deutschcn Verbwortschatzcs. Tiil)ingen: G. Narr. 
C. Bean, T. Rindtlesch, and C. Sneidernlan. 1998. 
Automatic semantic intert)retation of anatonfic 
spatial relationshil)s in clinical text. Ill Prec. 1998 
AMIA Annual Fall Symposium, pages 897 901. 
E. Charniak and 1{. Goldman. 1988. A logic for se- 
nmntic interl)retation. In Prec. of the 261h Annual 
Meeting of the. A CL, 1)ages 87-94. 
L. Crea.ry and C. Pollard. 1985. A computational 
semantics for natural language. 111 Prec. of the 
23rd Annual Mcctinfl of the ACL, pages 172 179. 
M. Dalrymple. 1992. Categorial semantics tbr LFG. 
In COLING'92 --- Proceedings of th, c I5th, \[sic! 
ldth,\] International Conference, pages 212-218. 
U. Itahn, S. Sohacht, and N. BrSker. 1994. Con- 
current, object-oriented natural anguage t)arsing: 
the PAI{SE'I'AIA{ model. International Journal of 
II'aman-Computcr Studies, 41(1/2):179 222. 
E. Hajieova. 1987. Linguistic meaning as related to 
syntax and to senmntic interpretation. Ii1 M. Na- 
gao, editor, Language and Artificial Intelligence, 
pages 327-351. North-Holland. 
G. Hirst. 1988. Senlantic intert/retation and ambi- 
guity. Artificial Intelligence, 34(21:131-177. 
V. Lombardo, L. Lesmo, L. Perraris, and C. Sei- 
donari. 1998. Incremental interpretation and lex- 
icalized grmnmar. In CogSci'98 - Proceedings of 
the 20th Annual Conference, pages 621-626. 
D. Milward. 1992. Dynamics, dependency grammar 
and incremental inte.rl)retation, ht COLING'92 
15vccedinfls of the 15th, \[sic! l~th\] International 
Coufcrcncc, pages 1095 1099. 
R. Moore.. 1989. Unitication-based semantic in- 
terl)r(;tation. In Proceedings of the 27th Annual 
Meeting of th, e A CL, pago, s 33--4:1. 
F. Pereira and M. Pollack. 1991. Incremental inter- 
pretation. Artificial Intelligence, 50(11:37-82. 
M. \]~,mmck(?r and U. tIahu. 2000a. Coping with dif- 
ti?rent ypes of anlbiguity using a uniform context 
haudling nmchanism. In Applications of Natural 
Lang'aaflc to h~formation Systcm.s. PTvccedinfls of 
the 5th, NLDB Confc~w~,cc. 
M. l{omacker and U. Italm. 2000b. An empMcal 
assessment of scnlantic interpretation. Ill P~vC. of 
the 6th Applied Nat'aral Language Processing Con- 
fc,'re',,cc, ?4 1.st Co'nfercncc of the, North American 
Chapter of the A CL, pages 327 334. 
S. $chlflZ and U. lIahn. 2000. Knowledge ngineer- 
ing I)y large-scale knowledge rollS(?: OXl)eriem:e 
from the inedical donm.in. In KR '200(\] Prec. ~\[t\[t 
Inter'national Conference., l)ages 601-610. 
N. Sondheimer, R. Weischedel, and R. Bobrow. 
1984. Semantic interpretation using KL-ONI,'. In 
COLING'84 Prec. IOth \]?tl. Coufcrcucc ~ 22rid 
Annual Mooting of the, ACL, pages 101 107. 
M. Strube and \[J. Ilahn. 1999. Nmetional centering: 
grounding referential coherence in information 
structure. Computational Linguistics, 25 (3) :309 
344. 
a. van Eijck and R. Moore. 1992. Semantic rules 
for English. In H. Alshawi, editor, The Core Lan- 
flua.qc Engine, pages 83-115. MIT Press. 
J. Wedekind and R. Kaplan. 1993. \[Type-driven se- 
mantic interpretation of f-structures << .1, W > 
,< R ,K  >>\]. In EACL'93 Proc. 6th, Conf. Eu- 
ropean Chapter of the ACL, pages 404 d11. 
W. Woods and J. Schmolze. 1992. The KL-ONE 
fiunily. Computers "U Mathematics with Applica- 
tions, 2312/51:133 177. 
J. Yen, R. Neches, and R. MacGregor. 1991. 
CLASP: integrating term subsuml/tion systems 
and l)roduction systems. IEEE Transactions on 
Knowlcd.qc and Data Engineering, 3(1):25--32. 
277 
The SynDiKATe Text Knowledge Base Generator
Udo Hahn
Text Knowledge Engineering Lab
Albert-Ludwigs-Universita?t Freiburg
D-79085 Freiburg, Germany
hahn@coling.uni-freiburg.de
Martin Romacker
Text Knowledge Engineering Lab
Albert-Ludwigs-Universita?t Freiburg
D-79085 Freiburg, Germany
romacker@coling.uni-freiburg.de
ABSTRACT
SynDiKATe comprises a family of text understanding sys-
tems for automatically acquiring knowledge from real-world
texts, viz. information technology test reports and medical
nding reports. Their content is transformed to formal rep-
resentation structures which constitute corresponding text
knowledge bases. SynDiKATe's architecture integrates re-
quirements from the analysis of single sentences, as well as
those of referentially linked sentences forming cohesive texts.
Besides centering-based discourse analysis mechanisms for
pronominal, nominal and bridging anaphora, SynDiKATe
is supplied with a learning module for automatically boot-
strapping its domain knowledge as text analysis proceeds.
1. INTRODUCTION
The SynDiKATe system belongs to the broad family of
information extraction (IE) systems [1]. Signicant progress
has been made already, as current IE systems provide robust
shallow text processing such that frame-style templates are
lled with factual information about particular entities (lo-
cations, persons, event types, etc.) from the analyzed doc-
uments. Nevertheless, typical MUC-style systems are also
limited in several ways. They provide no inferencing ca-
pabilities which allow substantial reasoning about the tem-
plate llers (hence, their understanding depth is low), and
their potential to deal with textual phenomena is highly
constrained, if it is available at all. Also novel and unex-
pected though potentially relevant information which does
not match given template structures is hard to account for,
since system designers commit to a xed collection of do-
main knowledge templates (i.e., they have no concept learn-
ing facilities).
With SynDiKATe, we are addressing these shortcomings
and aim at a more sophisticated level of knowledge acqui-
sition from real-world texts. The documents we deal with
are technical narratives in German language taken from two
domains, viz. test reports from the information technology
(IT) domain as processed by the itSynDiKATe system [8],
.
and nding reports from a medical subdomain (MED), the
framework of the medSynDiKATe system [10, 9]. Our rst
goal is to extract conceptually and inferentially richer forms
of knowledge than those captured by standard IE systems
such as evaluative assertions and comparisons [25, 24], tem-
poral [26] and spatial information [22]. Second, we also
want to dynamically enhance the set of knowledge templates
through incremental taxonomy learning devices [12] so that
the information extraction capability of the system is in-
creased in a bootstrapping manner. Third, SynDiKATe is
particularly sensitive to the treatment of textual reference
relations [27, 6, 14]. The capability to properly deal with
various forms of anaphora is a prerequisite for the sound-
ness and validity of the knowledge bases we create as a re-
sult of the text understanding process and likewise for the
feasibility of sophisticated retrieval and question answering
applications based on the acquired text knowledge.
2. SYSTEM ARCHITECTURE
The overall architecture of SynDiKATe, an acronymwhich
stands for \Synthesis of Distributed Knowledge Acquired
from Texts", is summarized in Figure 1. Incoming texts, T
i
,
are mapped into corresponding text knowledge bases, TKB
i
,
which contain a representation of T
i
's content. This knowl-
edge base platform may feed various information services,
such as inferentially supported question answering (fact re-
trieval), text passage retrieval or text summarization [7].
2.1 Sentence-Level Understanding
Grammatical knowledge for syntactic analysis is based
on a fully lexicalized dependency grammar [11], we refer to
as Lexicon in Figure 1. Basic word forms (lexemes) con-
stitute the leaf nodes of the lexicon tree, which are further
abstracted in terms of a hierarchy of lexeme class speci-
cations at dierent levels of generality. The Generic Lexi-
con in Figure 1 contains lexical material which is domain-
independent (lexemes such as move, with, or month), while
domain-specic extensions are kept in specialized lexicons
serving the needs of particular subdomains, e.g., IT (hard
disk, color printer, etc.) or MED (gastritis, surface mucus,
etc.). Dependency grammars capture binary valency con-
straints between a syntactic head (e.g., a noun) and possi-
ble modiers (e.g., a determiner or an adjective). To estab-
lish a dependency relation between a head and a modier,
all the lexicalized constraints on word order, compatibility
of morphosyntactic features, and semantic criteria must be
fullled. This leads to a strictly local computation scheme
which inherently lends itself to robust partial parsing [5].
Figure 1: Architecture of a SynDiKATe System
Conceptual knowledge about the dierent domains is
expressed in a Kl-One-like description logic language [28].
Corresponding to the division at the lexical level, the on-
tologies we provide are split up between one that is used by
all applications, the Upper Ontology, while several dedicated
ontologies account for the conceptual requirements of par-
ticular domains, e.g., IT (HardDisk, ColorPrinter, etc.)
or MED (Gastritis, SurfaceMucus, etc.).
Semantic knowledge accounts for emerging conceptual
relations between conceptual items according to those de-
pendency relations that are established between their cor-
responding lexical items. Semantic interpretation schemata
mediate between both levels in a way as abstract and general
as possible [20]. These schemata are applied to semantically
interpretable subgraphs which are, from a semantic point of
view, \minimal" subgraphs of the incrementally built depen-
dency graph. Their bounding nodes contain content words
(i.e., nouns, verbs, and adjectives, all of which have a concep-
tual correlate in the domain ontologies), while all possibly
intervening nodes (zero up to four) contain only noncon-
tent words (such as prepositions, articles, auxiliaries, etc.,
all of which have no conceptual correlates). Semantic in-
terpretation schemata are fully embedded in the knowledge
representation model and system (cf. Figure 1).
The ParseTalk system, which comprises the lexicalized
grammar and associated dependency parser, is embedded in
an object-oriented computation model. So, the dependency
relations are computed by lexical objects, so-called word ac-
tors, through strictly local message passing, only involving
the lexical items they represent. To illustrate how a de-
pendency relation is established computationally, we give a
sketch of the basic protocol for incremental parsing [5]:
 After a word has been read from textual input by
the WordScanner (step A
1
in Figure 1), its associated
lexeme (specied in the Lexicon) is identied (step
A
2
) and a corresponding word actor gets initialized
(step B
1
). As all content words are directly linked to
the conceptual system, each lexical item w that has a
conceptual correlate C in the domain knowledge base
(step A
3
) gets instantiated in the text knowledge base
(step B
2
). The lexical item Festplatte (hard disk)
with the conceptual correlate Hard-Disk is instanti-
ated, e.g., by Hard-Disk.3, the particular item being
talked about in a given text.
1
 For integration in the parse tree, the newly created
word actor searches its head (alternatively, its modi-
er) by sending parallel requests for dependential gov-
ernment to its left context (step C). The search space
is restricted, since these requests are propagated up-
wards only along the `right shoulder' of the depen-
dency graph constructed so far. All word actors ad-
dressed this way check, in parallel, whether their va-
lency restrictions, i.e., grammatical and conceptual con-
straints, are met by the requesting word actor. Step
D simulates a conceptual check in the text knowledge
base, step E illustrates a test in the discourse memory.
 If all required constraints are fullled by one of the
targeted word actors, an immediate semantic interpre-
tation is performed. This usually alters the conceptual
representation structures by way of slot lling (step F ).
Semantic interpretation consists of nding a relational
link between the conceptual correlates of the two content
words bounding the associated semantically interpretable
subgraph. The linkage may either be constrained by depen-
dency relations (e.g., the subject: relation of a transitive
verb such as \sell" may only be interpreted conceptually
in terms of agent or patient roles), by intervening lexical
material (e.g., some prepositions impose special role con-
straints, such as mit (with) does in terms of has-part or
instrument roles), or it may be constrained by concep-
tual criteria only (as with the genitive: dependency rela-
tion, which unlike subject: imposes no additional selective
conceptual constraints for interpretation). The correspond-
ing knowledge about these language-specic constraints is
densely encoded in the Lexicon class hierarchy, an approach
which heavily relies on the property inheritance mechanisms
inherent to the object-oriented paradigm.
2.2 Text-Level Understanding
2.2.1 Referential Text Phenomena
The textual phenomena we deal with in SynDiKATe es-
tablish referential links between consecutive utterances in a
coherent text such as illustrated by three possible continua-
tions of sentence (1), with three dierent forms of extrasen-
tential anaphora:
(1) Compaq verkauft ein Notebook mit einer Festplatte, die
von Seagate hergestellt wird.
(Compaq sells a notebook with a hard disk that is man-
ufactured by Seagate.)
(2) Pronominal Anaphora:
Es ist mit einer Pentium-III-CPU ausgestattet.
(It comes with a Pentium-III CPU.)
(3) Nominal Anaphora:
Der Rechner ist mit einer Pentium-III-CPU ausgestattet.
(The machine comes with a Pentium-III CPU.)
(4) Functional Anaphora:
Der Arbeitsspeicher kann auf 96 MB erweitert werden.
(The main memory can be expanded up to 96MB.)
1
Due to the recognition of referential relations at the text
level of analysis this instantiation might be readjusted by
subsequent coreference declarations (cf. Section 2.2).
Compaq sells a Notebook with a hard disk that is manufactured by Seagate.
3
ein
4
2
5
1
Compaq
subject:
propo:
verkauft
die
,
delimiter:
subject:
relative:specifier:
Notebook
hergestellt
von
Seagate
ppadjunct:
pobject:
wird
ppatt:
pobject:
object:
mit
specifier:
Festplatte
.
einer
verbpart:
Figure 2: Dependency Parse for Sentence (1)
Figure 3: Conceptual Interpretation for Sentence (1)
The results of sentence-level analysis for sentence (1) are
given in Figure 2, which contains a syntactic dependency
graph (together with ve congurations of semantically in-
terpretable subgraphs), and Figure 3, which displays its con-
ceptual representation. For text-level analysis, pronominal
anaphora still heavily depend on grammatical conditions {
the agreement of the antecedent (\Notebook") and the pro-
noun (\Es" (it)) in gender and number; also conceptual cri-
teria apply insofar as a potential antecedent must t the
conceptual role (or case frame) restrictions when it is in-
tegrated in governing structures, say, the head verb of the
clause. In general, however, the inuence of grammatical
criteria gradually diminishes for other types of text phe-
nomena, while the inuence of conceptual criteria increases.
For nominal anaphora, number constraints are still valid,
while a generalization relation between the anaphoric noun
(\Rechner" (machine)) and its proper antecedent (\Note-
book") must hold, in addition. In the case of functional
anaphora, no grammar constraints at all apply, while quite
sophisticated conceptual role path conditions come into play,
e.g., \Arbeitsspeicher" (main memory) being a constituent
physical part of \Notebook".
The problems text phenomena cause are of vital impor-
tance for the adequacy of the representation structures re-
sulting from text processing, and are centered around the no-
tions of incomplete, invalid and incoherent knowledge bases.
Incomplete knowledge bases emerge when references to
already established discourse entities are simply not recog-
nized, as in the case of pronominal anaphora. Consider the
reference relationship between the pronoun \Es" (it) in sen-
tence (2) which refers to the noun phrase \ein Notebook"
(a notebook) in sentence (1). The occurrence of the pro-
noun is not reected at the conceptual level, since pronouns
(as noncontent words) do not have conceptual correlates.
Hence, an incomplete concept graph emerges as shown in
Figure 4 | the referent for the pronoun \Es" (it), Note-
book.2, is not linked to Pentium-III-CPU.6. An adequate
treatment with a properly resolved anaphor is shown in Fig-
ure 6, where the representation of the relevant portions of
sentence (1) is linked to the one of sentence (2), in particu-
Figure 4: Unresolved Pronominal Anaphor, Sentence (2)
Figure 5: Unresolved Nominal Anaphor, Sentence (3)
Figure 6: Resolved Anaphors, Sentences (1) and (2)/(3)
lar by determining the equip-patient role between Equip.7
and the proper referent, Notebook.2.
Invalid knowledge bases emerge when each entity which
has a dierent denotation at the text surface is treated as
a formally distinct conceptual item at the symbol level of
knowledge representation, although all dierent denotations
refer literally to the same conceptual entity. This is the
case for nominal anaphora, an example of which is given by
the reference relation between the noun phrase \Der Rech-
ner" (the machine) in sentence (3) and the noun phrase \ein
Notebook" (a notebook) in sentence (1). An invalid referen-
tial description appears in Figure 5, where Computer.5 is
introduced as a new entity in the discourse, whereas Figure
6 shows the valid conceptual representation capturing the
intended meaning at the representation level, viz. maintain-
ing Notebook.2 as the proper referent (note that pronom-
inal as well as nominal anaphora are two equivalent ways to
corefer to the discourse entity denoted by Notebook.2).
Finally, incoherent knowledge bases emerge when entities
which are linked by nontaxonomic conceptual relations at
the knowledge level occur in a text such that an implicit
reference to these relations can be made in the text source.
Unlike the previously discussed cases of coference, these
relations have to be made explicit at the symbol level of
the targeted text knowledge base by a search for connect-
ing paths between the concepts involved [6]. This is the
basic scenario for functional (or bridging) anaphora. Con-
sider, e.g., the relationship holding between the noun phrase
\Der Arbeitsspeicher" (the main memory) in sentence (4),
which refers to the noun phrase \ein Notebook" (a note-
book) in sentence (1). In Figure 8 the relational link miss-
ing in Figure 7 between Main-Memory.8 and Notebook.2
is established (via a has-part-type relation, viz. has-main-
memory), and, hence, representational coherence at the sym-
bol level of knowledge representation is preserved.
Figure 7: Unresolved Functional Anaphor, Sentence (4)
Figure 8: Resolved Functional Anaphor, Sentences (1)
and (4)
Disregarding textual phenomena will cause dysfunctional
system behavior. A query Q such as
Q : (retrieve ?x (Computer ?x))
A-: (|I| Notebook.2, |I| Computer.5)
A+: (|I| Notebook.2)
triggers a search for all instances of Computer in the text
knowledge base. Given an invalid knowledge base (cf. Fig-
ures 3 and 5), the incorrect answer (A-) contains two entities,
viz. Notebook.2 and Computer.5 | both are in the ex-
tension of the concept Computer. If, however, a valid text
knowledge base such as the one in Figure 6 or 8 is given,
only the correct answer, Notebook.2, is inferred (A+).
Rendering also quantitative substance to our claims, we
analyzed a randomly chosen sample of 100 reports on his-
tological ndings with approximately 14,000 text tokens [9].
In IT texts, (pro)nominal anaphora and functional anaphora
occur at an almost balanced rate [27]. In the medical texts,
however, functional anaphora turn out to be the major glue
for establishing local coherence, while anaphora, pronomi-
nal anaphora in particular, play a far less important role
than in other text genres. The high proportion of func-
tional anaphora (45%) [42%-48%]
2
and the remarkable rate
of nominal (34%) [31%-37%] compared to extrasentential
pronominal anaphora (2%) [1%-3%] is clearly an indication
of the primary orientation in medical texts to convey facts
in a very compact manner. Two consequences can be drawn
from this observation. First, resolution procedures for func-
tional anaphora { supplementing well-researched procedures
for (pro)nominal anaphora { have to be provided urgently
(cf. [6] for a fully worked out approach). Second, functional
anaphora presuppose a considerable amount of deep back-
ground knowledge, with emphasis on partonomic reasoning
[13], supplementing well-known principles of taxonomic rea-
soning for text understanding.
2
For all percentage numbers 95% condence intervals are
supplied in square brackets.
2.2.2 Centering Model for Anaphora Resolution
In order to avoid the emergence of incomplete, invalid and
incoherent text knowledge bases we consider discourse enti-
ties for establishing reference relations with upcoming items
from the textual input at a local [27] and at a global level
[14] of cohesion. To preserve adequate text representation
structures a centering mechanism is used. The discourse en-
tities which occur in an utterance U
i
constitute its set of
forward-looking centers, C
f
(U
i
). The elements in C
f
(U
i
)
are ordered to reect relative prominence in U
i
in the sense
that the most highly ranked element of C
f
(U
i
) is the most
likely antecedent of an anaphoric expression in U
i+1
, while
the remaining elements are ordered according to decreasing
preference for establishing referential links.
While it is usually assumed (for the English language,
in particular) that grammatical roles are the major deter-
minant for the ranking on the C
f
[4], we claim that for
German { a language with relatively free word order { it
is the functional information structure of the sentence [27].
Accordingly, the constraints on the ordering of entries in
C
f
(U
i
) prefer hearer-old (either evoked or unused) elements
in an utterance (i.e., those that can be related to previ-
ously introduced discourse elements or generally accessi-
ble world knowledge) over mediated (inferrable) ones, while
these are preferred over hearer-new (brand-new) elements
for anaphora resolution. If two elements belong to the same
category, then preference is dened in terms of linear prece-
dence of the discourse units in the source text.
When we apply these criteria to sentence (1), Table 1 de-
picts the resulting order of forward-looking centers in C
f
(S
1
).
Since we have no discourse-bound elements in the rst sen-
tence, textual precedence applies exclusively to the ordering
of the center list items. Only nouns and their conceptual
correlates are taken into consideration. The tuple notation
takes the conceptual correlate of the lexical item in the text
knowledge base in the rst place, while the lexical surface
form appears in the second place.
(1) Cf: [Compaq: Compaq, Notebook.2: Notebook,
Hard-Disk.3: Festplatte, Seagate: Seagate]
Table 1: Centering Data for Sentence (1)
Processing of the centering list C
f
(S
1
) for sentence (3)
until the generalization constraint is fullled, nally, results
in a query whether Notebook is subsumed by Computer,
the conceptual correlate of the lexical item \Rechner". As
this relationship obviously holds, in the conceptual represen-
tation structure of sentence (3) (cf. Figure 5) Computer.5,
the literal instance identier, is declared coreferent toNote-
book.2, the referentially valid identier. Instead of having
two unlinked sentence graphs, Figures 3 and 5, the reference
resolution for (pro)nominal anaphora leads to joining them
in a common valid text graph (Figure 6). In particular,
Notebook.2 links to the relation equip-patient, formerly
occupied by Computer.5. The corresponding centering list
at the end of the analysis of sentence (3) is provided in Table
2 (C
f
(S
1
) has been updated to reect the consumption of
the antecedent, Notebook.2, in the processing of C
f
(S
3
)).
(1) Cf: [Compaq: Compaq, Notebook.2: Notebook,
Hard-Disk.3: Festplatte, Seagate: Seagate]
(3) Cf: [Notebook.2: Rechner,
Pentium-III-CPU.6: Pentium-III-CPU]
Table 2: Centering Data for Sentences (1) and (3)
2.3 Textual Learning
The approach to learning new concepts as a result of text
understanding builds on two dierent sources of evidence |
the prior knowledge of the domain the texts are about, and
grammatical constructions in which unknown lexical items
occur in the texts. The architecture of SynDiKATe's con-
cept learning component is depicted in Figure 9.
linguistic
quality
labels
conceptual
labels
Hypothesis
space-p
Hypothesis
Qualifier
quality
space-q
Quality Machine
1
2
space-1
space-i
space-n
Hypothesis
Hypothesis
Hypothesis
Language Processor
dependency parse graph
text knowledge base
Figure 9: SynDiKATe's Learning Component
The ParseTalk system generates dependency parse graphs.
The kinds of syntactic constructions (e.g., genitive, apposi-
tive, comparative), in which unknown lexical items appear,
are recorded and later assessed relative to the credit they
lend to a particular concept hypothesis, e.g., high for ap-
positives (\the notebook X"), lower for genitives (\Compaq's
X"). The conceptual interpretation of parse trees involving
unknown lexical items in the text knowledge base leads to the
deduction of concept hypotheses. These are further enriched
by conceptual annotations which reect structural patterns
of consistency, mutual justication, analogy, etc. relative to
already available concept descriptions in the text knowledge
base or other hypothesis spaces. Both kinds of evidence, in
particular their predictive `goodness' for the learning task,
are represented by corresponding sets of linguistic and con-
ceptual quality labels.
Alternative concept hypotheses for each unknown lexi-
cal item are organized in terms of corresponding hypothesis
spaces, each of which holds a dierent conceptual reading.
An inference engine embedded in the terminological system,
the so-called quality machine, determines the overall credi-
bility of single concept hypotheses by taking the available set
of quality labels for each hypothesis into account. The qual-
ier, a terminological classier extended by an evaluation
metric for quality classes, computes a preference ranking of
those hypotheses which remain valid after the text has been
processed completely (cf. [12] for details).
3. COVERAGE AND EVALUATION
SynDiKATe's coverage varies considerably depending on
the target domain. The generic lexicon currently includes
3,000 entries, the IT lexicon adds 5,000, while the MED
lexicon contributes 70,000 entries each. The Upper Ontology
contains 1,200 concepts and roles, to which the IT ontology
adds 3,000 and the MED ontology contributes 240,000 items.
The IT domain was chosen as a testbed that can be ex-
tended on demand. The MED domain, however, is subject
to ontology engineering eorts on a larger scale. In order
to cope with the enormous knowledge engineering require-
ments, we semi-automatically transformed large portions of
a semantically weak, yet high-volume medical terminology
(UMLS) to a very large terminological knowledge base [21].
Admittedly, SynDiKATe has not yet undergone a thor-
ough empirical evaluation in one of the envisaged applica-
tion dimensions. We have, however, carefully evaluated its
subcomponents. The results can be summarized as follows:
Sentence Parsing. We compared a standard active chart
parser with full backtracking capabilities with the parser
of SynDiKATe, which is characterized by limited memo-
ization and restricted backtracking capabilities, using the
same grammar specications. On average, SynDiKATe's
parser exhibits a linear time complexity the factor of which
is dependent on ambiguity rates of input sentences. The
active chart parser runs into exponential time complexity
whenever it encounters extragrammatical or ungrammatical
input, since then it conducts an exhaustive search of the en-
tire parse space. The loss of structural descriptions due to
the parser's incompleteness amounts to 10% compared with
the complete, though intractable parser [5].
Text Parsing. While with respect to resolution capac-
ity (eectiveness) no signicant dierences could be deter-
mined, the functional centering model we propose outper-
forms the best-known centering algorithms by a rate of 50%
with respect to a measure of computation costs which con-
siders \cheap" and \expensive" transitional moves between
utterances to assess a text's coherency. Hence, the proce-
dure we propose is more e?cient [27].
Semantic Interpretation. Our group has been pioneer-
ing work on the empirical evaluation of meaning representa-
tions. We assessed the quality and coverage of semantic in-
terpretation for randomly sampled texts in the two domains
we consider. While recall was rather low (57% for MED, 31%
for IT), precision peaked at 97% and 94%, respectively [19].
\Heavy" Semantics. We can deal with intricate seman-
tic phenomena for which we have provided the rst empirical
evaluation data available at all. This relates to the resolu-
tion of metonymies, where we have determined a gain in
eectiveness that amounts to 16% compared with the best
procedures known so far [16], as well as it relates to compar-
atives and evaluative assertions, where gains in eectiveness
were almost tripled [25].
Concept Learning. The performance of the concept
learning component has been compared to standard learning
mechanisms based on the terminological classier available
in any sort of description logics systems. Our data indicate
an increase of performance of 8% (87% accuracy, while that
of standard classiers is on the order of 79%) [12].
Evaluating a text knowledge acquisition rather than an IE
system poses hard methodological problems [2]. The main
reason being that a gold standard for comparison | what
constitutes a canonical, commonly agreed upon interpreta-
tion of the content of a text? | is hard to establish, even
for technical texts. A follow-up problem is constituted by
the lack of a signicant amount of annotated text knowl-
edge bases on which comparative analyses might be assessed.
MUC-style evaluation metrics, e.g., have already been qual-
ied not to adequately reect the functionality of less con-
strained text understanders [29].
4. CONCLUSIONS
A major hypothesis underlying the design of SynDiKATe
is that ignoring the referential relations between adjacent
utterances will lead to referentially incomplete, invalid, or
incoherent text knowledge bases. We determine plausible
discourse units for reference resolution using the centering
model. This allows us to deal with various forms of pronom-
inal, nominal and functional anaphora in a uniform way.
In order to establish local coherence at the text represen-
tation level, single discourse entities related by anaphoric
expressions have to be conceptually linked. We claim that
only sophisticated knowledge representation languages with
powerful terminological reasoning capabilities, such as those
from the KL-ONE family, are able to deal with the full range
of challenges of referentially adequate text understanding, in
particular considering nominal and functional anaphora.
These two types of anaphora pose an enormous burden
on the availability of rich domain knowledge. We respond
to this challenge in two ways. In a large-scale knowledge
engineering eort, we semi-automatically transform a se-
mantically weak though huge thesaurus-style medical knowl-
edge source into a terminological knowledge base. If such a
human-made resource is missing, we turn to a purely auto-
matic approach of bootstrapping a given domain knowledge
base as part of on-going text understanding processes.
The depth of understanding we provide comes closest to
systems such as Scisor [18], Tacitus [15] or Pundit/Kernel
[17], but SynDiKATe's knowledge acquisition strategies or
learning capabilities have no counterpart there. Text under-
standers which incorporate learning components are even
rarer but systems such as Snowy [3] or Wrap-Up [23] ei-
ther have a very narrow domain theory and lack robustness
for dealing with unseen input eectively, or fail to account
for a wide range of referential text phenomena, respectively.
5. ACKNOWLEDGMENTS
The development of the SynDiKATe system has been sup-
ported by various grants from Deutsche Forschungsgemeinschaft
under Ha 2097/*. SynDiKATe would not have come to existence
without the exciting contributions and enthusiasm of current and
former members of the group, in particular, Steen Staab,
Katja Markert, Michael Strube, Martin Romacker, Stefan Schulz,
Klemens Schnattinger, Norbert Broker, Peter Neuhaus, Susanne
Schacht, Manfred Klenner, and Holger Schauer.
6. REFERENCES
[1] Jim Cowie and Wendy Lehnert. Information extraction.
Communications of the ACM, 39(1):80{91, 1996.
[2] Carol Friedman and George Hripcsak. Evaluating natural
language processors in the clinical domain. Methods of
Information in Medicine, 37(4/5):334{344, 1998.
[3] Fernando Gomez and Carlos Segami. The recognition and
classication of concepts in understanding scientic texts.
Journal of Experimental and Theoretical Articial
Intelligence, 1(1):51{77, 1989.
[4] Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
Centering: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203{225, 1995.
[5] Udo Hahn, Norbert Broker, and Peter Neuhaus. Let's
ParseTalk: Message-passing protocols for object-oriented
parsing. In H. Bunt and A. Nijholt, editors, Advances in
Probabilistic and other Parsing Technologies, pages
177{201. Kluwer, 2000.
[6] Udo Hahn, Katja Markert, and Michael Strube. A
conceptual reasoning approach to textual ellipsis. In
Proceedings of the ECAI'96, pages 572{576, 1996.
[7] Udo Hahn and Ulrich Reimer. Knowledge-based text
summarization: Salience and generalization operators for
knowledge base abstraction. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization, pages
215{232. MIT Press, 1999.
[8] Udo Hahn and Martin Romacker. Content management in
the SynDiKATe system: How technical documents are
automatically transformed to text knowledge bases. Data &
Knowledge Engineering, 35(2):137{159, 2000.
[9] Udo Hahn, Martin Romacker, and Stefan Schulz. Discourse
structures in medical reports { watch out! The generation
of referentially coherent and valid text knowledge bases in
the medSynDiKATe system. International Journal of
Medical Informatics, 53(1):1{28, 1999.
[10] Udo Hahn, Martin Romacker, and Stefan Schulz. How
knowledge drives understanding: Matching medical
ontologies with the needs of medical language processing.
Articial Intelligence in Medicine, 15(1):25{51, 1999.
[11] Udo Hahn, Susanne Schacht, and Norbert Broker.
Concurrent, object-oriented natural language parsing: The
ParseTalk model. International Journal of
Human-Computer Studies, 41(1/2):179{222, 1994.
[12] Udo Hahn and Klemens Schnattinger. Towards text
knowledge engineering. In Proceedings of the AAAI'98,
pages 524{531, 1998.
[13] Udo Hahn, Stefan Schulz, and Martin Romacker.
Partonomic reasoning as taxonomic reasoning in medicine.
In Proceedings of the AAAI'99, pages 271{276, 1999.
[14] Udo Hahn and Michael Strube. Centering in-the-large:
Computing referential discourse segments. In Proceedings of
the ACL'97/EACL'97, pages 104{111, 1997.
[15] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and
Paul Martin. Interpretation as abduction. Articial
Intelligence, 63(1/2):69{142, 1993.
[16] Katja Markert and Udo Hahn. On the interaction of
metonymies and anaphora. In Proceedings of the IJCAI'97,
pages 1010{1015, 1997.
[17] Martha S. Palmer, Rebecca J. Passonneau, Carl Weir, and
Tim Finin. The Kernel text understanding system.
Articial Intelligence, 63(1/2):17{68, 1993.
[18] Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. Information
extraction and text summarization using linguistic
knowledge acquisition. Information Processing &
Management, 25(4):419{428, 1989.
[19] Martin Romacker and Udo Hahn. An empirical assessment
of semantic interpretation. In Proceedings of the NAACL
2000, pages 327{334, 2000.
[20] Martin Romacker, Katja Markert, and Udo Hahn. Lean
semantic interpretation. In Proceedings of the IJCAI'99,
pages 868{875, 1999.
[21] Stefan Schulz and Udo Hahn. Knowledge engineering by
large-scale knowledge reuse: Experience from the medical
domain. In Proceedings of KR 2000, pages 601{610, 2000.
[22] Stefan Schulz, Udo Hahn, and Martin Romacker. Modeling
anatomical spatial relations with description logics. In
Proceedings of the AMIA 2000, pages 779{783, 2000.
[23] Stephen Soderland and Wendy Lehnert. Wrap-up: A
trainable discourse module for information extraction.
Journal of Articial Intelligence Research, 2:131{158, 1994.
[24] Steen Staab and Udo Hahn. Comparatives in context. In
Proceedings of the AAAI'97, pages 616{621, 1997.
[25] Steen Staab and Udo Hahn. \Tall", \good", \high" {
compared to what? In Proceedings of the IJCAI'97, pages
996{1001, 1997.
[26] Steen Staab and Udo Hahn. Scalable temporal reasoning.
In Proceedings of the IJCAI'99, pages 1247{1252, 1999.
[27] Michael Strube and Udo Hahn. Functional centering:
Grounding referential coherence in information structure.
Computational Linguistics, 25(3):309{344, 1999.
[28] William A. Woods and James G. Schmolze. The Kl-One
family. Computers & Mathematics with Applications,
23(2/5):133{177, 1992.
[29] P. Zweigenbaum, J. Bouaud, B. Bachimont, J. Charlet, and
J.-F. Boisvieux. Evaluating a normalized conceptual repre-
sentation produced from natural language patient discharge
summaries. In Proceedings of the AMIA'97, pages 590{594.
Biomedical Text Retrieval in Languages with a Complex Morphology
Stefan Schulz a Martin Honeck a Udo Hahn b
a Department of Medical Informatics, Freiburg University Hospital
http://www.imbi.uni-freiburg.de/medinf
b Text Knowledge Engineering Lab, Freiburg University
http://www.coling.uni-freiburg.de
Abstract
Document retrieval in languages with
a rich and complex morphology ? par-
ticularly in terms of derivation and
(single-word) composition ? suffers from
serious performance degradation with the
stemming-only query-term-to-text-word
matching paradigm. We propose an
alternative approach in which morpholog-
ically complex word forms are segmented
into relevant subwords (such as stems,
named entities, acronyms), and subwords
constitute the basic unit for indexing and
retrieval. We evaluate our approach on a
large biomedical document collection.
1 Introduction
Morphological alterations of a search term have a
negative impact on the recall performance of an
information retrieval (IR) system (Choueka, 1990;
Ja?ppinen and Niemisto?, 1988; Kraaij and Pohlmann,
1996), since they preclude a direct match between
the search term proper and its morphological vari-
ants in the documents to be retrieved. In order to
cope with such variation, morphological analysis
is concerned with the reverse processing of inflec-
tion (e.g., ?search   ed?, ?search   ing?)1 , derivation
(e.g., ?search   er? or ?search   able?) and composi-
tion (e.g., German ?Blut   hoch   druck? [?high blood
pressure?]). The goal is to map all occurring mor-
phological variants to some canonical base form ?
e.g., ?search? in the examples from above.
The efforts required for performing morphologi-
cal analysis vary from language to language. For
English, known for its limited number of inflec-
tion patterns, lexicon-free general-purpose stem-
1
?  ? denotes the string concatenation operator.
mers (Lovins, 1968; Porter, 1980) demonstrably im-
prove retrieval performance. This has been reported
for other languages, too, dependent on the general-
ity of the chosen approach (Ja?ppinen and Niemisto?,
1988; Choueka, 1990; Popovic and Willett, 1992;
Ekmekc?ioglu et al, 1995; Hedlund et al, 2001;
Pirkola, 2001). When it comes to a broader scope
of morphological analysis, including derivation and
composition, even for the English language only re-
stricted, domain-specific algorithms exist. This is
particularly true for the medical domain. From an
IR view, a lot of specialized research has already
been carried out for medical applications, with em-
phasis on the lexico-semantic aspects of dederiva-
tion and decomposition (Pacak et al, 1980; Norton
and Pacak, 1983; Wolff, 1984; Wingert, 1985; Du-
jols et al, 1991; Baud et al, 1998).
While one may argue that single-word com-
pounds are quite rare in English (which is not the
case in the medical domain either), this is certainly
not true for German and other basically aggluti-
native languages known for excessive single-word
nominal compounding. This problem becomes even
more pressing for technical sublanguages, such as
medical German (e.g., ?Blut   druck   mess   gera?t?
translates to ?device for measuring blood pressure?).
The problem one faces from an IR point of view is
that besides fairly standardized nominal compounds,
which already form a regular part of the sublanguage
proper, a myriad of ad hoc compounds are formed
on the fly which cannot be anticipated when formu-
lating a retrieval query though they appear in rele-
vant documents. Hence, enumerating morphological
variants in a semi-automatically generated lexicon,
such as proposed for French (Zweigenbaum et al,
2001), turns out to be infeasible, at least for German
and related languages.
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 61-68.
                         Proceedings of the Workshop on Natural Language Processing in
Furthermore, medical terminology is character-
ized by a typical mix of Latin and Greek roots with
the corresponding host language (e.g., German), of-
ten referred to as neo-classical compounding (Mc-
Cray et al, 1988). While this is simply irrelevant
for general-purpose morphological analyzers, deal-
ing with such phenomena is crucial for any attempt
to cope adequately with medical free-texts in an IR
setting (Wolff, 1984).
We here propose an approach to document re-
trieval which is based on the idea of segment-
ing query and document terms into basic subword
units. Hence, this approach combines procedures for
deflection, dederivation and decomposition. Sub-
words cannot be equated with linguistically signif-
icant morphemes, in general, since their granular-
ity may be coarser than that of morphemes (cf. our
discussion in Section 2). We validate our claims in
Section 4 on a substantial biomedical document col-
lection (cf. Section 3).
2 Morphological Analysis for Medical IR
Morphological analysis for IR has requirements
which differ from those for NLP proper. Accord-
ingly, the decomposition units vary, too. Within
a canonical NLP framework, linguistically signif-
icant morphemes are chosen as nondecomposable
entities and defined as the smallest content-bearing
(stem) or grammatically relevant units (affixes such
as prefixes, infixes and suffixes). As an IR alterna-
tive, we here propose subwords (and grammatical
affixes) as the smallest units of morphological anal-
ysis. Subwords differ from morphemes only, if the
meaning of a combination of linguistically signifi-
cant morphemes is (almost) equal to that of another
nondecomposable medical synonym. In this way,
subwords preserve a sublanguage-specific compos-
ite meaning that would get lost, if they were split up
into their constituent morpheme parts.
Hence, we trade linguistic atomicity against med-
ical plausibility and claim that the latter is ben-
eficial for boosting the system?s retrieval perfor-
mance. For instance, a medically justified mini-
mal segmentation of ?diaphysis? into ?diaphys   is?
will be preferred over a linguistically motivated one
(?dia   phys   is?), because the first can be mapped
to the quasi-synonym stem ?shaft?. Such a mapping
would not be possible with the overly unspecific
morphemes ?dia? and ?phys?, which occur in nu-
merous other contexts as well (e.g.?dia   gnos   is?,
?phys   io   logy?). Hence, a decrease of the preci-
sion of the retrieval system would be highly likely
due to over-segmentation of semantically opaque
compounds. Accordingly, we distinguish the fol-
lowing decomposition classes:
Subwords like  ?gastr?, ?hepat?, ?nier?, ?leuk?, ?di-
aphys?,  are the primary content carriers in a
word. They can be prefixed, linked by infixes, and
suffixed. As a particularity of the German medical
language, proper names may appear as part of com-
plex nouns (e.g., ?Parkinson   verdacht? [?suspicion
of Parkinson?s disease?]) and are therefore included
in this category.
Short words, with four characters or less, like
 ?ion?, ?gene?, ?ovum?  , are classified separately ap-
plying stricter grammatical rules (e.g., they cannot
be composed at all). Their stems (e.g., ?gen? or ?ov?)
are not included in the dictionary in order to pre-
vent false ambiguities. The price one has to pay for
this decision is the inclusion of derived and com-
posed forms in the subword dictionary (e.g., ?an-
ion?,?genet?,?ovul?).
Acronyms such as  ?AIDS?, ?ECG?,  and ab-
breviations (e.g., ?chron.? [for ?chronical?], ?diabet.?
[for ?diabetical?]) are nondecomposable entities in
morphological terms and do not undergo any further
morphological variation, e.g., by suffixing.
Prefixes like  ?a-?, ?de-?, ?in-?, ?ent-?, ?ver-?,
?anti-?,  precede a subword.
Infixes (e.g., ?-o-? in ?gastr   o   intestinal?, or
?-s-? in ?Sektion   s   bericht? [?autopsy report?]) are
used as a (phonologically motivated) ?glue? between
morphemes, typically as a link between subwords.
Derivational suffixes such as  ?-io-?, ?-ion-?,
?-ie-?, ?-ung-?, ?-itis-?, ?-tomie-?,  usually follow
a subword.
Inflectional suffixes like  ?-e?, ?-en?, ?-s?, ?-idis?,
?-ae?, ?-oris?,  appear at the very end of a com-
posite word form following the subwords or deriva-
tional suffixes.
Prior to segmentation a language-specific ortho-
graphic normalization step is performed. It maps
German umlauts ?a??, ?o??, and ?u?? to ?ae?, ?oe?, and
?ue?, respectively, translates ?ca? to ?ka?, etc. The
morphological segmentation procedure for German
in January 2002 incorporates a subword dictionary
composed of 4,648 subwords, 344 proper names,
and an affix list composed of 117 prefixes, 8 in-
fixes and 120 (derivational and inflectional) suffixes,
making up 5,237 entries in total. 186 stop words are
not used for segmentation. In terms of domain cov-
erage the subword dictionary is adapted to the ter-
minology of clinical medicine, including scientific
terms, clinicians? jargon and popular expressions.
The subword dictionary is still in an experimental
stage and needs on-going maintenance. Subword
entries that are considered strict synonyms are as-
signed a shared identifier. This thesaurus-style ex-
tension is particularly directed at foreign-language
(mostly Greek or Latin) translates of source lan-
guage terms, e.g., German ?nier? EQ Latin ?ren? (EQ
English ?kidney?), as well as at stem variants.
The morphological analyzer implements a simple
word model using regular expressions and processes
input strings following the principle of ?longest
match? (both from the left and from the right). It per-
forms backtracking whenever recognition remains
incomplete. If a complete recognition cannot be
achieved, the incomplete segmentation results, nev-
ertheless, are considered for indexing. In case the
recognition procedure yields alternative complete
segmentations for an input word, they are ranked ac-
cording to preference criteria, such as the minimal
number of stems per word, minimal number of con-
secutive affixes, and relative semantic weight.2
3 Experimental Setting
As document collection for our experiments we
chose the CD-ROM edition of MSD, a German-
language handbook of clinical medicine (MSD,
1993). It contains 5,517 handbook-style articles
(about 2.4 million text tokens) on a broad range of
clinical topics using biomedical terminology.
In our retrieval experiments we tried to cover a
wide range of topics from clinical medicine. Due to
the importance of searching health-related contents
both for medical professionals and the general pub-
lic we collected two sets of user queries, viz. expert
queries and layman queries.
2A semantic weight  =2 is assigned to all subwords and
some semantically important suffixes, such as ?-tomie? [?-tomy?]
or ?-itis?;  =1 is assigned to prefixes and derivational suffixes;
 =0 holds for inflectional suffixes and infixes.
Expert Queries. A large collection of multi-
ple choice questions from the nationally standard-
ized year 5 examination questionnaire for medical
students in Germany constituted the basis of this
query set. Out of a total of 580 questions we se-
lected 210 ones explicitly addressing clinical issues
(in conformance with the range of topics covered
by MSD). We then asked 63 students (between the
3rd and 5th study year) from our university?s Med-
ical School during regular classroom hours to for-
mulate free-form natural language queries in order
to retrieve documents that would help in answering
these questions, assuming an ideal search engine.
Acronyms and abbreviations were allowed, but the
length of each query was restricted to a maximum
of ten terms. Each student was assigned ten topics
at random, so we ended up with 630 queries from
which 25 were randomly chosen for further consid-
eration (the set contained no duplicate queries).
Layman Queries. The operators of a German-
language medical search engine (http://www.
dr-antonius.de/) provided us with a set of
38,600 logged queries. A random sample (  =400)
was classified by a medical expert whether they con-
tained medical jargon or the wording of laymen.
Only those queries which were univocally classified
as layman queries (through the use of non-technical
terminology) ended up in a subset of 125 queries
from which 27 were randomly chosen for our study.
The judgments for identifying relevant documents
in the whole test collection (5,517 documents) for
each of the 25 expert and 27 layman queries were
carried out by three medical experts (none of them
was involved in the system development). Given
such a time-consuming task, we investigated only
a small number of user queries in our experiments.
This also elucidates why we did not address inter-
rater reliability. The queries and the relevance judg-
ments were hidden from the developers of the sub-
word dictionary.
For unbiased evaluation of our approach, we used
a home-grown search engine (implemented in the
PYTHON script language). It crawls text/HTML
files, produces an inverted file index, and assigns
salience weights to terms and documents based on
a simple tf-idf metric. The retrieval process relies
on the vector space model (Salton, 1989), with the
cosine measure expressing the similarity between a
query and a document. The search engine produces
a ranked output of documents.
We also incorporate proximity data, since this in-
formation becomes particularly important in the seg-
mentation of complex word forms. So a distinc-
tion must be made between a document containing
?append   ectomy? and ?thyroid   itis? and another
one containing ?append   ic   itis? and ?thyroid   ec-
tomy?. Our proximity criterion assigns a higher
ranking to adjacent and a lower one to distant search
terms. This is achieved by an adjacency offset,
	

, which is added to the cosine measure of each
document. For a query  consisting of  terms,
 , the minimal distance between a
pair of terms in a document, ( Coling 2010: Poster Volume, pages 1247?1255,
Beijing, August 2010
A Comparison of Models for Cost-Sensitive Active Learning
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
http://www.julielab.de
Abstract
Active Learning (AL) is a selective sam-
pling strategy which has been shown to
be particularly cost-efficient by drastically
reducing the amount of training data to be
manually annotated. For the annotation
of natural language data, cost efficiency
is usually measured in terms of the num-
ber of tokens to be considered. This mea-
sure, assuming uniform costs for all to-
kens involved, is, from a linguistic per-
spective at least, intrinsically inadequate
and should be replaced by a more ade-
quate cost indicator, viz. the time it takes
to manually label selected annotation ex-
amples. We here propose three differ-
ent approaches to incorporate costs into
the AL selection mechanism and evaluate
them on the MUC7T corpus, an extension
of the MUC7 newspaper corpus that con-
tains such annotation time information.
Our experiments reveal that using a cost-
sensitive version of semi-supervised AL,
up to 54% of true annotation time can be
saved compared to random selection.
1 Introduction
Active Learning (AL) is a selective sampling strat-
egy for determining those annotation examples
which are particularly informative for classifier
training, while discarding those that are already
easily predictable for the classifier given previous
training experience. While the efficiency of AL
has already been shown for many NLP tasks based
on measuring the number of tokens or sentences
that are saved in comparison to random sampling
(e.g., Engelson and Dagan (1996), Tomanek et al
(2007) or Settles and Craven (2008)), it is obvious
that just counting tokens under the assumption of
uniform annotation costs for each token is empir-
ically questionable, from a linguistic perspective,
at least.
As an alternative, we here explore annotation
costs that incur for AL based on an empirically
more plausible cost metric, viz. the time it takes
to annotate selected linguistic examples. We in-
vestigate three approaches to incorporate costs
into the AL selection mechanism by modifying
the standard (fully supervised) mode of AL and
a non-standard semi-supervised one according to
cost considerations. The empirical backbone of
this comparison is constituted by MUC7T , a re-
annotation of a part of the MUC7 newspaper
corpus that contains annotation time information
(Tomanek and Hahn, 2010).
2 Active Learning
Unlike random sampling, AL is a selective sam-
pling technique where the learner is in control of
the data to be chosen for training. By design, the
intention behind AL is to reduce annotation costs,
usually considered as the amount of labeled train-
ing material required to achieve a particular target
performance of the model. The latter is yielded
by querying labels only for those examples which
are assumed to have a high training utility. In this
section, we introduce different AL frameworks ?
the default, fully supervised AL approach (Sec-
tion 2.1), as well as a semi-supervised variant of
it (Section 2.2). In Section 2.3 we then propose
three methods how these approaches to AL can be
made cost-sensitive without further modifications.
1247
2.1 Fully Supervised AL (FuSAL)
As we consider AL for the NLP task of Named
Entity Recognition (NER), some design decisions
have to be made. Firstly, the selection granular-
ity is set to complete sentences ? a reasonable lin-
guistic annotation unit which still allows for fairly
precise selection. Second, a batch of examples in-
stead of a single example is selected per AL iter-
ation to reduce the computational overhead of the
sampling process.
We base our approach to AL on Conditional
Random Fields (CRFs), which we employ as base
learners (Lafferty et al, 2001). For observation
sequences ~x = (x1, . . . , xn) and label sequences
~y = (y1, . . . , yn), a linear-chain CRF is defined as
P?(~y|~x) =
1
Z?(~x)
?
n?
i=1
exp
k?
j=1
?jfj
(
yi?1, yi, ~x, i
)
where Z?(~x) is the normalization factor, and k
feature functions fj(?) with feature weights ? =
(?1, . . . , ?k) appear.
The core of any AL approach is a utility func-
tion u(p, ?) which estimates the informativeness
of each example p, a complete sentence p = (~x),
drawn from the pool P of all unlabeled examples,
for model induction. For our experiments, we em-
ploy two alternative utility functions which have
produced the best results in previous experiments
(Tomanek, 2010, Chapter 4). The first utility func-
tion is based on the confidence of a CRF model ?
in the predicted label sequence ~y? which is given
by the probability distribution P?(~y?|~x). The util-
ity function based on this probability boils down
to
uLC(p, ?) = 1? P?(~y ?|~x)
so that sentences for which the predicted label se-
quence ~y? has a low probability is granted a high
utility. Instead of calculating the model?s con-
fidence on the complete sequence, we might al-
ternatively calculate the model?s confidence in its
predictions on single tokens. To obtain an overall
confidence for the complete sequence, the aver-
age over the single token-confidence values can be
computed by the marginal probability P?(yi|~x).
Now that we are calculating the confidence on the
token level, we might also obtain the performance
of the second best label and calculate the margin
between the first and second best label as a con-
fidence score so that the final utility function is
obtained by
uMA(p, ?) = ?
1
n
n?
i=1
(
max
y?inY
P?(yi = y?|~x)?
max
y??inY
y? 6=y??
P?(yi = y??|~x)
)
Algorithm 1 formalizes our AL framework.
Depending on the utility function, the best b ex-
amples are selected per round, manually labeled,
and then added to the set of labeled data L which
feeds the classifier for the next training round.
Algorithm 1 NER-specific AL Framework
Given:
b: number of examples to be selected in each iteration
L: set of labeled examples l = (~x, ~y) ? Xn ? Yn
P: set of unlabeled examples p = (~x) ? Xn
T (L): a learning algorithm
u(p, ?): utility function
Algorithm:
loop until stopping criterion is met
1. learn model: ? ? T (L)
2. sort p ? P: let S ? (p1, . . . , pm) : u(pi, ?) ?
u(pi+1, ?), i ? [1,m], p ? P
3. select b examples pi with highest utility from S: B ?
{p1, . . . , pb}, b ? m, pi ? S
4. query labels for all p ? B: B? ? {l1, . . . , lb}
5. L ? L ? B?, P ? P \ B
return L? ? L and ?? ? T (L?)
The specification is still not cost-sensitive as the
selection of examples depends only on the utility
function. Using uLC will result in a reduction of
the number of examples (i.e., sentences) selected
irrespective of the sentence length so that a model
learns the most from it. As a result, we observed
that the selected sentences are quite long which
might even cause higher annotation costs per sen-
tence (Tomanek, 2010, Chapter 4). As for uMA
there is at least a slight normalization sensitive
to costs since the sum over all token-level utility
scores is normalized by the length of the selected
sentence.
1248
2.2 Semi-supervised AL (SeSAL)
Tomanek and Hahn (2009) extendeded this stan-
dard fully supervised AL framework by a semi-
supervised variant (SeSAL). The selection of sen-
tences is performed in a standard manner, i.e.,
similarly to the procedure in Algorithm 1. How-
ever, once selected, rather than manually annotat-
ing the complete sentence, only (uncertain) sub-
sequences of each selected sentence are manually
labeled, while the remaining (certain) ones are au-
tomatically annotated using the current version of
the classifier.
After the selection of an informative example
p = (~x) with ~x = (x1, . . . , xn), the subsequences
~x? = (xa, . . . , xb), 1 ? a ? b ? n, with low local
uncertainty have to be identified. For reasons of
simplicity, only sequences of length 1, i.e., single
tokens, are considered. For a token xi from a se-
lected sequence ~x the model?s confidence C?(y?i )
in label y?i is estimated. Token-level confidence
for a CRF is calculated as the marginal probabil-
ity so that
C?(y?i ) = P?(yi = y?i |~x)
where y?i specifies the label at the respective posi-
tion of the predicted label sequence ~y ? (the one
which is obtained by the Viterbi algorithm). If
C?(y?i ) exceeds a confidence threshold t, y?i is as-
signed as the putatively correct label. Otherwise,
manual annotation of this token is required.
Employing SeSAL, savings of over 80 % of the
tokens compared to random sampling are reported
by Tomanek and Hahn (2009). Even when com-
pared to FuSAL, still 60 % of the number of to-
kens are eliminated. A crucial question, however,
not answered in these experiments, is whether this
method actually reduces the overall annotation ex-
penses in time rather than just in the number of to-
kens. Also SeSAL does not incorporate labeling
costs in the selection process.
2.3 Cost-Sensitive AL (CoSAL)
In this section, we turn to an extension of FuSAL
and SeSAL which incorporates cost sensitivity
into the AL selection process (CoSAL). Three
different approaches of CoSAL will be explored.
The challenge we now face is that two contradic-
tory criteria ? utility and costs ? have to be bal-
anced.
2.3.1 Cost-Constrained Sampling
CoSAL can be realized in the most straight-
forward way by simply constraining the sampling
to a particular maximum cost cmax per example.
Therefore, in a pre-processing step all examples
p ? P for which cost(p) > cmax are removed from
P . The unmodified NER-specific AL framework
can then be applied.
An obvious shortcoming of Cost-Constrained
Sampling (CCS) is that it precludes any form of
compensation between utility and costs. Thus, an
exceptionally useful example with a cost factor
slightly above cmax will be rejected. Another crit-
ical issue is how to fix cmax. If chosen too low,
the pre-filtering of P results in a much too strong
restriction of selection options when only few ex-
amples remain inside P . If chosen too high, the
cost constraint becomes ineffective.
2.3.2 Linear Rank Combination
A general solution to fit different criteria into
a single one is by way of linear combination.
If, however, different units of measurement are
used, a transformation function for the alignment
of benefit, or utility, and costs must be found. This
can be difficult to determine. In our scenario, ben-
efits measured by utility scores and costs mea-
sured in seconds are clearly incommensurable. As
it is not immediately evident how to express utility
in monetary terms (or vice versa), we transform
utility and cost information into ranks R(u(p, ?))
andR?(cost(p)) instead. As for utility, higher util-
ity leads to higher ranks. As for costs, lower costs
lead to higher ranks. The linear rank combination
(LRK) is defined as
?LRK(~v(p)) = ?R
(
u(p, ?)
)
+(1??)R?
(cost(p))
where ? is a weighting term. In a CoSAL sce-
nario, where utility is the primary criterion, ? >
0.5 seems a reasonable choice. Alternatively, as
costs and utility are contradictory, allowing equal
influence for both criteria, as with ? = 0.5, it
may be difficult to find appropriate examples in
a medium-sized corpus. Thus, the choice of ? de-
pends on size and diversity with respect to combi-
nations of utility and costs within P .
1249
2.3.3 Benefit-Cost Ratio
Our third approach to CoSAL is based on the
Benefit-Cost Ratio (BCR). Given equal units of
measurement for benefits and costs, the benefit-
cost ratio indicates whether a scenario is profitable
(ratio > 1). BCR can also be applied when units
are incommensurable and a transformation func-
tion is available, as is the case for the combination
of utility and cost. This holds as long as bene-
fit and costs can be expressed in the same units
by a linear transformation function, i.e., u(p, ?) =
? ? cost(p) + b. If such a transformation function
exists, one can refrain from finding proper values
for the above variables b and ? and instead calcu-
late BCR as
?BCR(p) =
u(p, ?)
cost(p)
Since annotation costs are usually expressed on
a linear scale, this is also required for utility, if
we want to use BCR. But when utility is based
on model confidence as we do it here, this prop-
erty gets lost.1 Hence a non-linear transforma-
tion function is needed to fit the scales of utility
and costs. Assuming a linear relationship between
utility and costs, BCR has already been applied
by Haertel et al (2008) and Settles et al (2008).
Our approach provides a crucial extension as we
explicitly consider scenarios where such a linear
relationship is not given and a non-linear transfor-
mation function is required instead.
In a direct comparison of LRK with BCR, LRK
may be used when such a transformation function
would be needed but is unknown and hard to find.
Choosing LRK over BCR is also motivated by
findings in the context of data fusion in informa-
tion retrieval where Hsu and Taksa (2005) remark
that, given incommensurable units and scales, one
would do better when ranks rather than the actual
scores or values were combined.
3 Experiments
In the following, we study possible benefits of
CoSAL, relative to FuSAL and SeSAL, in the
1Though normalized to [0, 1], confidence estimates, es-
pecially for sequence classification, are often not on a linear
scale so that confidence values that are twice as high do not
necessarily mean that the benefit in training a model on such
an example is doubled.
light of real annotation times as a cost measure
(instead of the standard, yet inadequate one, viz.
the number of tokens being selected). Such timing
data is available in the MUC7T corpus (Tomanek
and Hahn, 2010), a re-annotation of the MUC7
corpus containing the ENAMEX types (persons,
locations, and organizations) and a time stamp re-
flecting the time it took annotators to decide on
each entity type. The MUC7T corpus contains
3,113 sentences (76,900 tokens).
The results we report on are averaged over 20
independent runs. For each run, we split the
MUC7T corpus randomly into a pool to select
from (90%) and an evaluation set (10%). AL was
started from a random seed set of 20 sentences.
As utility scores to estimate benefits we applied
uMA and uLC as defined in Section 2.1.
The plots in the following sections depict costs
in terms of annotation time (in seconds) relative
to annotation quality (expressed via F1-scores).
Learning curves are only shown for early AL it-
erations. Later on, in the convergence phase, due
to the two conflicting criteria now considered si-
multaneously, selection options become more and
more scarce so that CoSAL necessarily performs
sub-optimally.
3.1 Parametrization of CoSAL Approaches
Preparatory experiments were run to analyze how
different parameters affected different CoSAL set-
tings. For the CCS and LRK experiments, we
used the uLC utility function.
For CCS, we tested three cmax values, viz. 7.5,
10, and 15, to determine the maximum perfor-
mance attainable on MUC7T when only examples
below the chosen threshold were included. Our
choices of the maximum were based on the dis-
tributions of annotation times over the sentences
(see Figure 1) where 7.5s marks the 75% quantile
and 15s is just above the 90% quantile. For 7.5s,
we peaked at Fmax = 0.84, for 10s at Fmax =
0.86, and for 15s at Fmax = 0.88. Figure 2
(top) shows the learning curves of CoSAL with
CCS and different cmax values. With cmax = 15,
as could be expected from the boxplot in Fig-
ure 1, no difference can be observed compared
to cost-insensitive FuSAL. CCS with lower val-
ues for cmax stagnates at the maximum perfor-
1250
seconds
freq
uen
cy
0 5 10 15 20 25 30
0
200
400
600
800
Figure 1: Distribution of annotation times per sen-
tence in MUC7T .
mance reported above, but still improves upon
cost-insensitive FuSAL in early AL iterations.
At some point in time all economical exam-
ples, with costs below cmax but high utility, have
been consumed from the corpus. Even in a cor-
pus much larger than MUC7T this effect will only
occur with some delay. Indeed, any choice of a re-
strictive value for cmax will cause similar exhaus-
tion effects. Unfortunately, it is unclear how to
tune cmax suitably in a real-life annotation sce-
nario where pretests for maximum performance
for a particular cmax are not possible. For further
experiments, we chose cmax = 10.
For LRK, we tested three different weights ?,
viz. 0.5, 0.75, and 0.9. Figure 2 (bottom) shows
their effects on the learning curves. Similar ten-
dencies as for cmax for CCS can be observed.
With ? = 0.9, CoSAL does not fall below default
FuSAL, at least in the observed range. A lower
weight of ? = 0.75 results in larger improve-
ments in earlier AL iterations but then falls back
to FuSAL and in later AL iterations (not shown
here) even below FuSAL. If the time parameter
is granted too much influence, as with ? = 0.5,
performance even drops to random selection level.
This might also be due to corpus exhaustion. For
further experiments, we chose ? = 0.75 because
of its potential to improve upon FuSAL in early
iterations.
For BCR with uMA, we change this utility func-
tion to n ? uMA to compensate for the normaliza-
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for CCS
seconds
F?s
cor
e
CCS 15s
CCS 10s
CCS 7.5s
FuSAL : uLC
RS
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for LRK
seconds
F?s
cor
e
LRK 0.9
LRK 0.75
LRK 0.5
FuSAL : uLC
RS
Figure 2: Different parameter settings for CCS
and LRK based on FuSAL with uLC as utility
function. FuSAL: uLC refers to cost-insensitive
FuSAL, CCS and LRK to the cost-sensitive ver-
sions of FuSAL with the respective parameters.
tion by token length which is otherwise already
contained in uMA(n is the length of the respective
sentence). For uLC, the preparatory experiments
already showed that this utility function does not
behave on a linear scale. This is so because uLC is
based on P?(~y|~x) for confidence estimation of the
complete label sequence ~y. Hence, a uLC score
twice as high does not indicate doubled benefit for
classifier training. Thus, we need a non-linear cal-
ibration function to transform uLC into a proper
utility estimator on a linear scale so that BCR can
be applied.
To determine such a non-linear calibration
function, the true benefit of an example p would
1251
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
lll
l
ll
l
l
l
l
l
l
l
l
l
l
0.5 0.6 0.7 0.8 0.9 1.0
0
2
4
6
8
uLC
n?
u M
A
corr: 0.6494
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
ll
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l l
l
l l
l
ll
l
l
l
l
l
l l
ll
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l l
l
l
l
l l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
0e+00 1e+08 2e+08 3e+08 4e+08 5e+08
0
2
4
6
8
e??uLC
n?
u M
A
corr: 0.8959
Figure 3: Scatter plots for (a) uLC versus n?uMA and (b) e??uLC versus n?uMA
be needed. In the absence of such informa-
tion, we consider n ? uMA as a good approxima-
tion. To identify the relationship between uLC and
n ? uMA, we trained a model on a random subsam-
ple from P ? ? P and used this model to obtain
the scores for uLC and n ? uMA for each example
from the test set T .2 Figure 3 (left) shows a scat-
ter plot of these scores which provides ample evi-
dence that the relationship between uLC and ben-
efit is indeed non-linear. As calibration function
for uLC we propose f(p) = e??uLC(p). Experi-
mentally, we determined ? = 20 as a good value.
Figure 3 (right) reveals that e??uLC(p) is a better
utility estimator; the correlation with n ? uMA is
now corr = 0.8959 and the relationship is close
to being linear.
In Figure 4, learning curves for BCR with the
utility function uLC and the calibrated function
e??uLC(p) are compared. BCR with the uncali-
brated utility function uLC fails miserably (the
performance falls even below random selection).
This adds credibility to our claim that while uLC
may be appropriate for ranking examples (as for
standard, cost-insensitive AL), it is inappropriate
for estimating true benefit/utility which is needed
when costs are to be incorporated with the BCR
method. BCR with the calibrated utility e??uLC(p),
in contrast, outperforms cost-insensitive FuSAL.
For further experiments with BCR, we either ap-
ply n?uMA or e??uLC(p) as utility functions.
2We experimented with different sizes forP ?, with almost
identical results.
1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
parameter test for BCR
seconds
F?s
cor
e
BCR : e20?uLC
BCR : uLC
FuSAL : uLC
RS
Figure 4: Different parameter settings for BCR
3.2 Comparison of CoSAL Approaches
We compared all three approaches to CoSAL in
the parametrization chosen above for the utility
functions uMA and uLC. Learning curves are
shown in Figure 5. Improvements over cost-
insensitive AL are only achieved in early AL iter-
ations up to 2,500s (for CoSAL based on uMA) or
4,000s (for CoSAL based on uLC) of annotation
time. This exclusiveness of early improvements
can be explained by the size of the corpus and, by
this, the limited number of good selection options.
Since AL selects with respect to two conflicting
criteria, the pool P should be much larger to in-
crease the chance for examples that are favorable
with respect to both criteria.
1252
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uMA
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : n ? uMA
FuSAL : uMA
RS
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
utility function : uLC
seconds
F?s
cor
e
CCS (10s)
LRK (0.75)
BCR : e20?uLC
FuSAL : uLC
RS
Figure 5: Comparison of CoSAL approaches for the utility functions uMA and uLC. Baseline given by
random selection (RS) and standard FuSAL with either uMA or uLC.
Improvements for CoSAL based on uLC are
generally higher than for uMA. Moreover, cost-
insensitive AL based on uLC does not exhibit any
normalization where, in contrast, uMA is normal-
ized at least to the number of tokens per example.
In CoSAL, both uLC and uMA are normalized by
costs, which is methodologically a more substan-
tial enhancement for uLC than for uMA.
For CoSAL based on uMA we cannot proclaim
a clear winner among the different approaches.
All three CoSAL approaches improve upon cost-
insensitive AL. For CoSAL based on uLC, LRK
performs best, while CCS and BCR perform simi-
larly well. Given this result, we might prefer LRK
or CCS over BCR. A disadvantage of the first two
approaches is that they require corpus-specific pa-
rameters which may be difficult to find for a new
learning problem for which no data for experi-
mentation is at hand. Though not the best per-
former, BCR does not require further parametriza-
tion and appears more appropriate for real-life an-
notation projects ? as long as utility is an appro-
priate estimator for benefit. CoSAL with BCR has
already been studied by Settles et al (2008). They
also applied a utility function based on sequence-
confidence estimation which presumably, as with
our uLC utility function, is not a good benefit esti-
mator. The fact that Settles et al did not explicitly
treat this issue might explain why cost-sensitive
AL based on BCR often performed worse than
cost-insensitive AL in their experiments.
3.3 CoSAL Applied to SeSAL
We looked at a cost-sensitive version of SeSAL by
applying the cost-sensitive FuSAL approach to-
gether with BCR and the transformation function
for the utility as discussed above. On top of this
selection, we ran the standard SeSAL approach ?
only tokens below a confidence threshold were se-
lected for annotation. The following experiments
are all based on the uLC utility function (and the
transformation function of it).
Figure 6 depicts learning curves for cost-
insensitive and cost-sensitive SeSAL and FuSAL
which reveal that cost-sensitive SeSAL consid-
0 1000 2000 3000 4000 5000 6000
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
seconds
F?s
cor
e
SeSAL BCR
FuSAL BCR
SeSAL
FuSAL
RS
Figure 6: Cost-sensitive (BCR variants) vs. cost-
insensitive FuSAL and SeSAL with uLC as utility
function.
1253
erably outperforms cost-sensitive FuSAL. Cost-
sensitive SeSAL attains a target performance of
F=0.85 with only 2806s, while cost-sensitive
FuSAL needs 3410s, and random selection con-
sumes over 6060s. Thus, cost-sensitive SeSAL
here reduces true annotation time by about 54 %
compared to random selection, whereas cost-
sensitive FuSAL reduces annotation time by only
44 %.
4 Related Work
Although the reduction of data acquisition costs
that result from human labeling efforts have al-
ways been the main driver for AL studies, cost-
sensitive AL is a new branch of AL. In an early
study on cost metrics for AL, Becker and Osborne
(2005) examined whether AL, while decreasing
the sample size on the one hand, on the other
hand increased annotation efforts. For a real-
world AL annotation project, they demonstrated
that the actual sampling efficiency measure for
an AL approach depends on the cost metric be-
ing applied. In a companion paper, Hachey et al
(2005) studied how sentences selected by AL af-
fected the annotators? performance both in terms
of the time needed and the annotation accuracy
achieved. They found that selectively sampled ex-
amples are, on the average, more difficult to anno-
tate than randomly sampled ones. This observa-
tion, for the first time, questioned the widespread
assumption that all annotation examples can be as-
signed a uniform cost factor.
Making a standard AL approach cost-sensitive
by normalizing utility in terms of annotation time
has been proposed before by Haertel et al (2008),
Settles et al (2008), and Donmez and Carbonell
(2008). CoSAL based on the net-benefit (costs
subtracted from utility) was proposed by Vijaya-
narasimhan and Grauman (2009) for object recog-
nition in images and Kapoor et al (2007) for voice
message classification.
5 Conclusions
We investigated three approaches to incorporate
the notion of cost into the AL selection mecha-
nism, including a fixed maximal cost budget per
example, a linear rank combination to express net-
benefit, and a benefit-cost ratio. The cost metric
we applied was the time needed by human coders
for annotating particular annotation examples.
Among the three approaches to cost-sensitive
AL, we see a slight advantage for benefit cost ra-
tios in real-world settings because they do not re-
quire additional corpus-specific parametrization,
once a proper calibration function is found.
Another observation is that advantages of
the three cost-sensitive AL models over cost-
insensitive ones consistently occur only in early
iteration rounds ? a result we attribute to corpus
exhaustion effects since cost-sensitive AL selects
for two criteria (utility and cost) and thus requires
a extremely large pool to be able to pick up really
advantageous examples. Consequently, applied
to real-world annotation settings where the pools
may be extremely large, we expect cost-sensitive
approaches to be even more effective in terms of
the reduction of annotation time.
To be applicable in real-world scenarios, anno-
tation costs which, in our experiments, were di-
rectly traceable in the MUC7T corpus have to be
estimated since they are not known prior to anno-
tation. In Tomanek et al (2010), we investigated
the reading behavior during named entity annota-
tion using eye-tracking technology. With the in-
sights gained from this study on crucial factors in-
fluencing annotation time we were able to induce
such a much needed predictive model of annota-
tion costs. In future work, we plan to incorporate
this empirically founded cost model into our ap-
proaches to cost-sensitive AL and to investigate
whether our positive findings can be reproduced
with estimated costs as well.
Acknowledgements
This work was partially funded by the EC within
the CALBC (FP7-231727) project.
References
Becker, Markus and Miles Osborne. 2005. A two-
stage method for active learning of statistical gram-
mars. In IJCAI?05 ? Proceedings of the 19th Inter-
national Joint Conference on Artificial Intelligence,
pages 991?996. Edinburgh, Scotland, UK, July 31 -
August 5, 2005.
1254
Donmez, Pinar and Jaime Carbonell. 2008. Proactive
learning: Cost-sensitive active learning with mul-
tiple imperfect oracles. In CIKM?08 ? Proceed-
ing of the 17th ACM conference on Information
and Knowledge Management, pages 619?628. Napa
Valley, CA, USA, October 26-30, 2008.
Engelson, Sean and Ido Dagan. 1996. Minimizing
manual annotation cost in supervised training from
corpora. In ACL?96 ? Proceedings of the 34th An-
nual Meeting of the Association for Computational
Linguistics, pages 319?326. Santa Cruz, CA, USA,
June 24-27, 1996.
Hachey, Ben, Beatrice Alex, and Markus Becker.
2005. Investigating the effects of selective sampling
on the annotation task. In CoNLL?05 ? Proceed-
ings of the 9th Conference on Computational Natu-
ral Language Learning, pages 144?151. Ann Arbor,
MI, USA, June 29-30, 2005.
Haertel, Robbie, Kevin Seppi, Eric Ringger, and James
Carroll. 2008. Return on investment for active
learning. In Proceedings of the NIPS 2008 Work-
shop on Cost-Sensitive Machine Learning. Whistler,
BC, Canada, December 13, 2008.
Hsu, Frank and Isak Taksa. 2005. Comparing rank and
score combination methods for data fusion in infor-
mation retrieval. Information Retrieval, 8(3):449?
480.
Kapoor, Ashish, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learning
with decision-theoretic active learning. In IJCAI?07
? Proceedings of the 20th International Joint Con-
ference on Artifical Intelligence, pages 877?882.
Hyderabad, India, January 6-12, 2007.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289. Williamstown, MA, USA, June
28 - July 1, 2001.
Settles, Burr and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP?08 ? Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1069?1078. Waikiki, Hon-
olulu, Hawaii, USA, October 25-27, 2008.
Settles, Burr, Mark Craven, and Lewis Friedland.
2008. Active learning with real annotation costs. In
Proceedings of the NIPS 2008 Workshop on Cost-
Sensitive Machine Learning. Whistler, BC, Canada,
December 13, 2008.
Tomanek, Katrin and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
ACL/IJCNLP?09 ? Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natu-
ral Language Processing, pages 1039?1047. Singa-
pore, August 2-7, 2009.
Tomanek, Katrin and Udo Hahn. 2010. Annotation
time stamps: Temporal metadata from the linguistic
annotation process. In LREC?10 ? Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation. La Valletta, Malta, May 17-
23, 2010.
Tomanek, Katrin, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains cor-
pus reusability of annotated data. In EMNLP-
CoNLL?07 ? Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Language Learning,
pages 486?495. Prague, Czech Republic, June 28-
30, 2007.
Tomanek, Katrin, Udo Hahn, Steffen Lohmann, and
Ju?rgen Ziegler. 2010. A cognitive cost model of an-
notations based on eye-tracking data. In ACL?10 ?
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics. Uppsala,
Sweden, July 11-16, 2010.
Tomanek, Katrin. 2010. Resource-Aware Annotation
through Active Learning. Ph.D. thesis, Technical
University of Dortmund.
Vijayanarasimhan, Sudheendra and Kristen Grauman.
2009. What?s it going to cost you? predicting ef-
fort vs. informativeness for multi-label image anno-
tations. CVPR?09 ? Proceedings of the 2009 IEEE
Computer Vision and Pattern Recognition Confer-
ence.
1255
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 982?992,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Evaluating the Impact of Alternative Dependency Graph Encodings on
Solving Event Extraction Tasks
Ekaterina Buyko and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
{ekaterina.buyko|udo.hahn}@uni-jena.de
Abstract
In state-of-the-art approaches to information
extraction (IE), dependency graphs constitute
the fundamental data structure for syntactic
structuring and subsequent knowledge elicita-
tion from natural language documents. The
top-performing systems in the BioNLP 2009
Shared Task on Event Extraction all shared the
idea to use dependency structures generated
by a variety of parsers ? either directly or
in some converted manner ? and optionally
modified their output to fit the special needs
of IE. As there are systematic differences be-
tween various dependency representations be-
ing used in this competition, we scrutinize on
different encoding styles for dependency in-
formation and their possible impact on solv-
ing several IE tasks. After assessing more
or less established dependency representations
such as the Stanford and CoNLL-X dependen-
cies, we will then focus on trimming opera-
tions that pave the way to more effective IE.
Our evaluation study covers data from a num-
ber of constituency- and dependency-based
parsers and provides experimental evidence
which dependency representations are partic-
ularly beneficial for the event extraction task.
Based on empirical findings from our study
we were able to achieve the performance of
57.2% F-score on the development data set of
the BioNLP Shared Task 2009.
1 Introduction
Relation and event extraction are among the most
demanding semantics-oriented NLP challenge tasks
(both in the newspaper domain such as for ACE1, as
well as in the biological domain such as for BioCre-
ative2 or the BioNLP Shared Task3), comparable in
terms of analytical complexity with recent efforts di-
rected at opinion mining (e.g., NTCIR-74 or TREC
Blog tracks5) or the recognition of textual entail-
ment.6 The most recent BioNLP 2009 Shared Task
on Event Extraction (Kim et al, 2009) required, for
a sample of 260 MEDLINE abstracts, to determine
all mentioned events ? to be chosen from a given
set of nine event types, including ?Localization?,
?Binding?, ?Gene Expression?, ?Transcription?,
?Protein Catabolism?, ?Phosphorylation?, ?Posi-
tive Regulation?, ?Negative Regulation?, and (un-
specified) ?Regulation? ? and link them appropri-
ately with a priori supplied protein annotations. The
demands on text analytics to deal with the complex-
ity of this Shared Task in terms of relation diversity
and specificity are unmatched by former challenges.
For relation extraction in the biomedical domain
(the focus of our work), a stunning convergence
towards dependency-based syntactic representation
structures is witnessed by the performance results
of the top-performing systems in the BioNLP?09
1http://papers.ldc.upenn.edu/LREC2004/
ACE.pdf
2http://biocreative.sourceforge.net/
3www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
SharedTask/
4http://research.nii.ac.jp/ntcir/
workshop/OnlineProceedings7/pdf/revise/
01-NTCIR-OV-MOAT-SekiY-revised-20081216.
pdf
5http://trec.nist.gov/data/blog08.html
6http://pascallin.ecs.soton.ac.uk/
Challenges/RTE/
982
Shared Task on Event Extraction.7 Regarding the
fact that dependency representations were always
viewed as a vehicle to represent fundamental seman-
tic relationships already at the syntactic level, this
is not a great surprise. Yet, dependency grammar
is not a monolithic, consensually shaped and well-
defined linguistic theory. Accordingly, associated
parsers tend to vary in terms of dependency pairing
or structuring (which pairs of words join in a depen-
dency relation?) and dependency typing (how are
dependency relations for a particular pair labelled?).
Depending on the type of dependency theory or
parser being used, various representations emerge
(Miyao et al, 2007). In this paper, we explore these
different representations of the dependency graphs
and try, first, to pinpoint their effects on solving the
overall event extraction task and, second, to further
enhance the potential of JREX, a high-performance
relation and event extractor developed at the JULIE
Lab (Buyko et al, 2009).
2 Related Work
In the biomedical domain, the focus has largely been
on binary relations, in particular protein-protein
interactions (PPIs). Accordingly, the biomedi-
cal NLP community has developed various PPI-
annotated corpora (e.g., LLL (Ne?dellec, 2005),
AIMED (Bunescu et al, 2005), BIOINFER (Pyysalo
et al, 2007)). PPI extraction does clearly not count
as a solved problem, and a deeper look at its bio-
logical and representational intricacies is certainly
worthwhile. The GENIA event corpus (Kim et al,
2008) and the BioNLP 2009 Shared Task data (Kim
et al, 2009) contain such detailed annotations of
PPIs (amongst others).
The BioNLP Shared Task was a first step towards
the extraction of specific pathways with precise in-
formation about the molecular events involved. In
that task, 42 teams participated and 24 of them sub-
mitted final results. The winner system, TURKU
(Bjo?rne et al, 2009), achieved with 51.95% F-score
the milestone result in that competition followed by
the JULIELab system (Buyko et al, 2009) which
peaked at 46.7% F-score. Only recently, an ex-
tension of the TURKU system, the TOKYO system,
7www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
SharedTask/results/results-master.html
has been realized (Miwa et al, 2010). TOKYO sys-
tem?s event extraction capabilities are based on the
TURKU system, yet TURKU?s manually crafted rule
system for post-processing and the combination of
extracted trigger-argument relations is replaced by
a machine learning approach in which rich features
collected from classification steps for triggers and
arguments are re-combined. TOKYO achieves an
overall F-score of 53.29% on the test data, thus out-
performing TURKU by 1.34 percentage points.
The three now top-performing systems, TOKYO,
TURKU and JULIELab, all rely on dependency
graphs for solving the event extraction tasks. While
the TURKU system exploits the Stanford dependen-
cies from the McClosky-Charniak parser (Charniak
and Johnson, 2005), and the JULIELab system uses
the CoNLL-like dependencies from the GDep parser
(Sagae and Tsujii, 2007),8 the TOKYO system over-
lays the Shared Task data with two parsing represen-
tations, viz. Enju PAS structure (Miyao and Tsujii,
2002) and GDep parser dependencies. Obviously,
one might raise the question as to what extent the
performance of these systems depends on the choice
of the parser and its output representations. Miyao
et al (2008) already assessed the impact of different
parsers for the task of biomedical relation extraction
(PPI). Here we perform a similar study for the task
of event extraction and focus, in particular, on the
impact of various dependency representations such
as Stanford and CoNLL?X dependencies and addi-
tional trimming procedures.
For the experiments on which we report here, we
performed experiments with the JULIELab system.
Our main goal is to investigate into the crucial role
of proper representation structures for dependency
graphs so that the performance gap from Shared
Task results between the best-performing TOKYO
system and the JULIELab system be narrowed.
3 Event Extraction
3.1 Objective
Event extraction is a complex task that can be sub-
divided into a number of subtasks depending on
8The GDep parser has been trained on the GENIA Tree-
bank pre-official version of the version 1.0 converted with the
script available from http://w3.msi.vxu.se/
?
nivre/
research/Penn2Malt.html
983
whether the focus is on the event itself or on the ar-
guments involved:
Event trigger identification deals with the large
variety of alternative verbalizations of the same
event type, e.g., whether the event is expressed in a
verbal or in a nominalized form (?A is expressed?
as well as ?the expression of A? both refer to the
same event type, viz. Expression(A)). Since the
same trigger may stand for more than one event type,
event trigger ambiguity has to be resolved as well.
Event trigger disambiguation selects the correct
event name from the set of alternative event triggers.
Argument identification is concerned with find-
ing all necessary participants in an event, i.e., the
arguments of the relation.
Argument ordering assigns each identified par-
ticipant its functional role within the event, mostly
Agent and Patient.
3.2 JULIELab System
The JULIELab solution can best be characterized as
a single-step learning approach for event detection
as the system does not separate the overall learn-
ing task into independent event trigger and event
argument learning subtasks.9 The JULIELab sys-
tem incorporates manually curated dictionaries and
machine learning (ML) methodologies to sort out
associated event triggers and arguments on depen-
dency graph structures. For argument extraction, the
JULIELab system uses two ML-based approaches,
a feature-based and a kernel-based one. Given
that methodological framework, the JULIELab team
scored on 2nd rank among 24 competing teams, with
45.8% precision, 47.5% recall and 46.7% F-score on
all 3,182 events. After the competition, this system
was updated and achieved 57.6% precision, 45.7%
recall and 51.0% F-score (Buyko et al, 2010) using
modified dependency representations from the MST
parser (McDonald et al, 2005). In this study, we
perform event extraction experiments with various
dependency representations that allow us to measure
their effects on the event extraction task and to in-
crease the overall JULIELab system performance in
terms of F-score.
9The JULIELab system considers all relevant lexical items as
potential event triggers which might represent an event. Only
those event triggers that can eventually be connected to argu-
ments, finally, represent a true event.
4 Dependency Graph Representations
In this section, we focus on representation formats
of dependency graphs. In Section 4.1, we introduce
fundamental notions underlying dependency pars-
ing and consider established representation formats
for dependency structures as generated by various
parsers. In Section 4.2, we account for selected trim-
ming operations for dependency graphs to ease IE.
4.1 Dependency Structures: Representation
Issues
Dependency parsing, in the past years, has increas-
ingly been recognized as an alternative to long-
prevailing constituency-based parsing approaches,
particularly in semantically-oriented application
scenarios such as information extraction. Yet even
under purely methodologically premises, it has
gained wide-spread attention as witnessed by recent
activities performed as part of the ?CoNLL Shared
Tasks on Multilingual Dependency Parsing? (Buch-
holz and Marsi, 2006).
In a nutshell, in dependency graphs of sentences,
nodes represent single words and edges account for
head-modifier relations between single words. De-
spite this common understanding, concrete syntactic
representations often differ markedly from one de-
pendency theory/parser to the other. The differences
fall into two main categories: dependency pairing or
structuring (which pairs of words join in a depen-
dency relation?) and dependency typing (how are
dependency relations for a particular pair labelled?).
The CoNLL?X dependencies, for example, are
defined by 54 relation types,10 while the Stanford
scheme (de Marneffe et al, 2006) incorporates 48
types (so called grammatical relations or Stanford
dependencies). The Link Grammar Parser (Sleator
and Temperley, 1991) employs a particularly fine-
grained repertoire of dependency relations adding
up to 106 types, whereas the well-known MINIPAR
parser (Lin, 1998) relies on 59 types. Differences in
dependency structure are at least as common as dif-
ferences in dependency relation typing (see below).
10Computed by using the conversion script on WSJ
data (accessible via http://nlp.cs.lth.se/
pennconverter/; see also Johansson and Nugues (2007)
for additional information). From the GENIA corpus, using this
script, we could only extract 29 CoNLL dependency relations.
984
Figure 1: Example of CoNLL 2008 dependencies, as used in most of the native dependency parsers.
Figure 2: Stanford dependencies, basic conversion from Penn Treebank.
In general, dependency graphs can be generated
by syntactic parsers in two ways. First, native de-
pendency parsers output CoNLL?X or Stanford de-
pendencies dependent on which representation for-
mat they have been trained on.11 Second, in a deriva-
tive dependency mode, the output of constituency-
based parsers, e.g., phrase structure representations,
is subsequently converted either into CoNLL?X or
Stanford dependencies using Treebank conversion
scripts (see below). In the following, we provide
a short description of these two established depen-
dency graph representations:
? CoNLL?X dependencies (CD). This depen-
dency tree format was used in the CoNLL?X
Shared Tasks on multi-lingual dependency
parsing (see Figure 1). It has been adopted
by most native dependency parsers and was
originally obtained from Penn Treebank (PTB)
trees using constituent-to-dependency conver-
sion (Johansson and Nugues, 2007). It differs
slightly in the number and types of dependen-
cies being used from various CoNLL rounds
(e.g., CoNLL?08 provided a dependency type
for representing appositions).12
? Stanford dependencies (SD). This format was
proposed by de Marneffe et al (2006) for
11We disregard in this study other dependency representa-
tions such as MINIPAR and LINK GRAMMAR representations.
12For the differences between CoNLL?07 and CoNLL?08 rep-
resentations, cf. http://nlp.cs.lth.se/software/
treebank_converter/
semantics-sensitive applications using depen-
dency representations, and can be obtained us-
ing the Stanford tools13 from PTB trees. The
Stanford format is widely used in the biomed-
ical domain (e.g., by Miyao et al (2008) or
Clegg and Shepherd (2005)).
There are systematic differences between
CoNLL?X and Stanford dependencies, e.g., as far
as the representation of passive constructions, the
position of auxiliary and modal verbs, or coordi-
nation representation is concerned. In particular,
the representation of the passive construction and
the role of the auxiliary verb therein may have
considerable effects for semantics-sensitive tasks.
While in SD the subject of the passive construction
is represented by a special nsubj dependency
label, in CD we find the same subject label as for
active constructions SUB(J). On CoNLL?08 data,
the logical subject is marked by the LGS depen-
dency edge that connects the passive-indicating
preposition ?by? with the logical subject of the
sentence.
The representation of active constructions are
similar in CD and SD though besides the role of
auxiliary and modal verbs. In the Stanford de-
pendency representation scheme, rather than taking
auxiliaries to be the heads in passive or tense con-
structions, main verbs are assigned this grammatical
function (see Figure 2). The CoNLL?X represen-
13Available from nlp.stanford.edu/software/
lex-parser.shtml
985
Figure 3: Noun phrase representation in
CoNLL?X dependency trees.
Figure 4: Trimming procedure noun phrase on
CoNLL?X dependency trees.
tation scheme is completely different in that auxil-
iaries ? much in common with standard dependency
theory ? are chosen to occupy the role of the gov-
ernor (see Figure 1). From the perspective of rela-
tion extraction, however, the Stanford scheme is cer-
tainly closer to the desired predicate-argument struc-
ture representations than the CoNLL scheme.
4.2 Dependency Graph Modifications in Detail
Linguistic intuition suggests that the closer a depen-
dency representation is to the format of the targeted
semantic representation, the more likely will it sup-
port the semantic application. This idea is directly
reflected in the Stanford dependencies which narrow
the distance between nodes in the dependency graph
by collapsing procedures (the so-called collapsed
mode of phrase structure conversion). An example
of collapsing is the conversion of ?expression nmod????
in pmod???? cells? to ?expression prep in????? cells?. An ex-
tension of collapsing is the re-structuring of coor-
dinations with sharing the dependency relations of
conjuncts (the so-called ccprocessed mode of phrase
structure conversion).
According to the Stanford scheme, Buyko et al
(2009) proposed collapsing scenarios on CoNLL?X
dependency graphs. Their so-called trimming op-
erations treat three syntactic phenomena, viz. coor-
dinations (coords), auxiliaries/modals (auxiliaries),
and prepositions (preps). For coordinations, they
propagate the dependency relation of the first con-
junct to all the other conjuncts within the coordi-
nation. For auxiliaries/modals, they prune the aux-
iliaries/modals as governors from the dependency
graph and propagate the dependency relations of
these nodes to the main verbs. Finally, for preposi-
tions, they collapse a pair of typed dependencies into
a single typed dependency (as illustrated above).
For the following experiments, we extended the
trimming procedures and propose the re-structuring
of noun phrases with action adjectives to make the
dependency representation even more compact for
semantic interpretation. The original dependency
representation of the noun phrase selects the right-
most noun as the head of the NP and thus all re-
maining elements are its dependents (see Figure 3).
For the noun phrases containing action adjectives
(mostly verb derivations) this representation does
not reflect the true semantic relations between the
elements. For example, in ?IL-10 mediated expres-
sion? it is ?IL-10? that mediates the expression.
Therefore, we re-structure the dependency graph by
changing the head of ?IL-10? from ?expression?
to ?mediated?. Our re-coding heuristics selects,
first, all the noun phrases containing action adjec-
tives ending with ?-ed?, ?-ing?, ?-ible? suffixes and
with words such as ?dependent?, ?specific?, ?like?.
In the second step, we re-structure the noun phrase
by encoding the adjective as the head of all the nouns
preceding this adjective in the noun phrase under
scrutiny (see Figure 4).
5 Experiments and Results
In this section, we describe the experiments and
results related to event extraction tasks based on
alternative dependency graph representations. For
our experiments, we selected the following top-
performing parsers ? the first three phrase structure
based and thus the origin of derivative dependency
structures, the last three fully dependency based for
making native dependency structures available:
? C+J, Charniak and Johnson?s reranking parser
(Charniak and Johnson, 2005), with the WSJ-
trained parsing model.
? M+C, Charniak and Johnson?s reranking parser
(Charniak and Johnson, 2005), with the self-
trained biomedical parsing model from Mc-
Closky (2010).
986
? Bikel, Bikel?s parser (Bikel, 2004) with the
WSJ-trained parsing model.
? GDep (Sagae and Tsujii, 2007), a native depen-
dency parser.
? MST (McDonald et al, 2005), another native
dependency parser.
? MALT (Nivre et al, 2007), yet another native
dependency parser.
The native dependency parsers were re-trained on
the GENIA Treebank (Tateisi et al, 2005) conver-
sions.14 These conversions,15 i.e., Stanford basic,
CoNLL?07 and CoNLL?08 were produced with the
currently available conversion scripts. For the Stan-
ford dependency conversion, we used the Stanford
parser tool,16 for CoNLL?07 and CoNLL?08 we used
the treebank-to-CoNLL conversion scripts17 avail-
able from the CoNLL?X Shared Task organizers.
The phrase structure based parsers were applied
with already available models, i.e., the Bikel and
C+J parsers as trained on the WSJ corpus, and
M+C as trained on the GENIA Treebank corpus.
For our experiments, we converted the prediction
results of the phrase structure based parsers into
five dependency graph representations, viz. Stanford
basic, Stanford collapsed, Stanford ccprocessed,
CoNLL?07 and CoNLL?08, using the same scripts
as for the conversion of the GENIA Treebank.
The JULIELab event extraction system was re-
trained on the Shared Task data enriched with differ-
ent outputs of syntactic parsers as described above.
The results for the event extraction task are repre-
sented in Table 1. Due to the space limitation of
this paper we provide the summarized results of im-
portant event extraction sub-tasks only, i.e., results
for basic events (Gene Expression, Transcription,
Localization, Protein Catabolism) are summarized
14For the training of dependency parsers, we used from the
available Stanford conversion variants only Stanford basic. The
collapsed and ccprocessed variants do not provide dependency
trees and are not recommended for training native dependency
parsers.
15We used the GENIA Treebank version 1.0, available from
www-tsujii.is.s.u-tokyo.ac.jp
16http://nlp.stanford.edu/software/
lex-parser.shtml
17http://nlp.cs.lth.se/software/treebank_
converter/
under SVT-TOTAL; regulatory events are summa-
rized under REG-TOTAL; the overall extraction re-
sults are listed in ALL-TOTAL (see Table 1).
Obviously, the event extraction system trained on
various dependency representations indeed produces
truly different results. The differences in terms of F-
score come up to 2.4 percentage points for the SVT-
TOTAL events (cf. the MALT parser, difference
between SD basic (75.6% F-score) and CoNLL?07
(78.0% F-score)), up to 3.6 points for REG-TOTAL
(cf. the M+C parser, difference between SD ccpro-
cessed (40.9% F-score) and CoNLL?07 (44.5% F-
score)) and up to 2.5 points for ALL-TOTAL (cf.
the M+C parser, difference between SD ccprocessed
(52.8% F-score) and CoNLL?07 (55.3% F-score)).
The top three event extraction results on the de-
velopment data based on different syntactic parsers
results are achieved with M+C parser ? CoNLL?07
representation (55.3% F-score), MST parser ?
CoNLL?08 representation (54.6% F-score) and
MALT parser ? CoNLL?08 representation (53.8%
F-score) (see Table 1, ALL-TOTAL). Surprisingly,
both the CoNLL?08 and CoNLL?07 formats clearly
outperform Stanford representations on all event ex-
traction tasks. Stanford dependencies seem to be
useful here only in the basic mode. The collapsed
and ccprocessed modes produce even worse results
for the event extraction tasks.
Our second experiment focused on trimming op-
erations on CoNLL?X dependency graphs. Here
we performed event extraction after the trimming of
the dependency trees as described in Section 4.2 in
different modes: coords ? re-structuring coordina-
tions; preps ? collapsing of prepositions; auxiliaries
? propagating dependency relations of auxiliars and
modals to main verbs; noun phrase ? re-structuring
noun phrases containing action adjectives. Our sec-
ond experiment showed that the extraction of se-
lected events can profit in particular from the trim-
ming procedures coords and auxiliaries, but there is
no evidence for a general trimming configuration for
the overall event extraction task.
In Table 2 we summarize the best configurations
we found for the events in focus. It is quite evi-
dent that the CoNLL?08 and CoNLL?07 dependen-
cies modified for auxiliaries and coordinations are
the best configurations for four events (out of nine).
For three events no modifications are necessary and
987
Parser SD basic SD collapsed SD ccprocessed CoNLL?07 CoNLL?08
SVT-TOTAL
R P F R P F R P F R P F R P F
Bikel 70.5 75.5 72.9 70.7 74.5 72.5 71.6 73.5 72.5 69.4 75.9 72.5 69.7 75.7 72.6
C+J 73.0 77.4 75.1 73.2 77.3 75.2 72.8 77.2 75.0 73.5 78.3 75.8 73.0 77.9 75.4
M+C 76.4 78.0 77.2 76.4 77.6 77.0 76.4 77.2 76.8 76.4 79.0 77.7 76.6 79.3 77.9
GDEP 77.1 77.5 77.3 N/A N/A N/A N/A N/A N/A 72.5 80.2 76.1 72.6 77.2 74.8
MALT 73.1 78.2 75.6 N/A N/A N/A N/A N/A N/A 75.9 80.3 78.0 73.7 78.2 75.9
MST 76.4 78.5 77.4 N/A N/A N/A N/A N/A N/A 74.8 78.4 76.6 76.7 80.8 78.7
REG-TOTAL
R P F R P F R P F R P F R P F
Bikel 35.3 40.6 37.8 33.8 40.3 36.8 34.3 39.6 36.8 33.9 39.2 36.3 34.0 41.0 37.2
C+J 36.2 41.8 38.8 37.3 41.8 39.4 36.5 41.9 39.0 38.1 43.9 40.8 37.4 44.0 40.4
M+C 39.4 45.5 42.3 38.8 45.3 41.8 38.5 43.7 40.9 41.9 47.4 44.5 40.1 47.9 43.7
GDEP 39.6 42.8 41.6 N/A N/A N/A N/A N/A N/A 38.4 43.7 40.9 39.8 44.4 42.0
MALT 38.8 44.3 41.4 N/A N/A N/A N/A N/A N/A 39.0 44.3 41.5 39.2 46.4 42.5
MST 39.5 43.6 41.4 N/A N/A N/A N/A N/A N/A 39.6 45.6 42.4 40.6 45.8 43.0
ALL-TOTAL
R P F R P F R P F R P F R P F
Bikel 47.4 51.5 49.4 46.3 50.8 48.5 46.9 50.2 48.5 44.8 50.7 47.6 44.7 51.8 48.0
C+J 49.3 53.8 51.5 49.6 52.8 51.2 49.0 53.0 50.9 50.3 54.4 52.3 49.5 54.3 51.8
M+C 52.3 56.4 54.3 51.8 55.7 53.7 51.3 54.3 52.8 53.2 57.5 55.3 52.2 58.2 55.0
GDEP 52.7 54.5 53.6 N/A N/A N/A N/A N/A N/A 50.6 55.2 52.8 51.3 55.0 53.1
MALT 50.4 54.7 52.4 N/A N/A N/A N/A N/A N/A 51.5 56.0 53.7 51.2 56.8 53.8
MST 52.3 54.8 53.5 N/A N/A N/A N/A N/A N/A 51.7 56.4 53.9 52.4 56.9 54.6
Table 1: Results on the Shared Task development data for Event Extraction Task. Approximate Span Match-
ing/Approximate Recursive Matching.
Event Class Best Parser Best Configuration R P F
Gene Expression MST CoNLL?08, auxiliaries, coords 79.5 81.8 80.6
Transcription MALT CoNLL?07, auxiliaries, coords 67.1 75.3 71.0
Protein Catabolism MST CoNLL?08, preps 85.7 100 92.3
Phosphorylation MALT CoNLL?08 80.9 88.4 84.4
Localization MST CoNLL?08, auxiliaries 81.1 87.8 84.3
Binding MST CoNLL?07, auxiliaries, coords, noun phrase 51.2 51.0 51.1
Regulation MALT CoNLL?07, auxiliaries, coords 30.8 49.5 38.0
Positive Regulation M+C CoNLL?07 43.0 49.9 46.1
Negative Regulation M+C CoNLL?07 49.5 45.3 47.3
Table 2: Best Configurations for Dependency Representations for Event Extraction Task on the development data.
Binding R P F
CoNLL?07 47.3 46.8 47.0
CoNLL?07 auxiliaries, coords 46.8 48.1 47.4
CoNLL?07 auxiliaries, coords, noun phrase 51.2 51.0 51.1
Table 3: Effects of trimming of CoNLL dependencies on the Shared Task development data for Binding events. Ap-
proximate Span Matching/Approximate Recursive Matching. The data was processed by the MST parser.
988
JULIELab JULIELab TOKYO System
(M+C, CoNLL?08) Final Configuration
Event Class gold R P F R P F R P F
Gene Expression 356 79.2 80.3 79.8 79.5 81.8 80.6 78.7 79.5 79.1
Transcription 82 59.8 72.0 65.3 67.1 75.3 71.0 65.9 71.1 68.4
Protein Catabolism 21 76.2 88.9 82.0 85.7 100 92.3 95.2 90.9 93.0
Phosphorylation 47 83.0 81.2 82.1 80.9 88.4 84.4 85.1 69.0 76.2
Localization 53 77.4 74.6 75.9 81.1 87.8 84.3 71.7 82.6 76.8
SVT-TOTAL 559 76.4 79.0 77.7 78.2 82.6 80.3 77.3 77.9 77.6
Binding 248 45.6 45.9 45.8 51.2 51.0 51.1 50.8 47.6 49.1
EVT-TOTAL 807 66.9 68.7 67.8 69.9 72.5 71.2 69.1 68.1 68.6
Regulation 169 32.5 46.2 38.2 30.8 49.5 38.0 36.7 46.6 41.1
Positive regulation 617 42.3 49.0 45.4 43.0 49.9 46.1 43.9 51.9 47.6
Negative regulation 196 48.5 44.0 46.1 49.5 45.3 47.3 38.8 43.9 41.2
REG-TOTAL 982 41.9 47.4 44.5 42.2 48.7 45.2 41.7 49.4 45.2
ALL-TOTAL 1789 53.2 57.5 55.3 54.7 60.0 57.2 54.1 58.7 56.3
Table 4: Results on the Shared Task development data. Approximate Span Matching/Approximate Recursive Match-
ing.
JULIELab JULIELab TOKYO system
(Buyko et al, 2010) Final Configuration
Event Class gold R P F R P F R P F
Gene Expression 722 66.3 79.6 72.4 67.0 77.2 71.8 68.7 79.9 73.9
Transcription 137 33.6 61.3 43.4 35.0 60.8 44.4 54.0 60.7 57.1
Protein Catabolism 14 71.4 90.9 80.0 71.4 90.9 80.0 42.9 75.0 54.6
Phosphorylation 135 80.0 85.0 82.4 80.7 84.5 82.6 84.4 69.5 76.3
Localization 174 47.7 93.3 63.1 45.4 90.8 60.5 47.1 86.3 61.0
SVT-TOTAL 1182 61.4 80.3 69.6 61.8 78.2 69.0 65.3 76.4 70.4
Binding 347 47.3 52.4 49.7 47.3 52.2 49.6 52.2 53.1 52.6
EVT-TOTAL 1529 58.2 73.1 64.8 58.5 71.7 64.4 62.3 70.5 66.2
Regulation 291 24.7 40.5 30.7 26.8 38.2 31.5 28.9 39.8 33.5
Positive Regulation 983 35.8 45.4 40.0 34.8 45.8 39.5 38.0 48.3 42.6
Negative Regulation 379 37.2 39.7 38.4 37.5 40.9 39.1 35.9 47.2 40.8
REG-TOTAL 1653 34.2 43.2 38.2 34.0 43.3 38.0 35.9 46.7 40.6
ALL-TOTAL 3182 45.7 57.6 51.0 45.8 57.2 50.9 48.6 59.0 53.3
Table 5: Results on the Shared Task test data. Approximate Span Matching/Approximate Recursive Matching.
989
only one event profits from trimming of prepositions
(Protein Catabolism). Only the Binding event prof-
its significantly from noun phrase modifications (see
Table 3). The increase in F-score for trimming pro-
cedures is 4.1 percentage points for Binding events.
In our final experiment we connected the best con-
figurations for each of the BioNLP?09 events as pre-
sented in Table 2. The overall event extraction re-
sults of this final configuration are presented in Ta-
bles 4 and 5. We achieved an increase of 1.9 per-
centage points F-score in the overall event extrac-
tion compared to the best-performing single parser
configuration (M+C, CoNLL?07) (see Table 4, ALL-
TOTAL). The reported results on the development
data outperform the results of the TOKYO system by
2.6 percentage points F-score for all basic events in-
cluding Binding events (see Table 4, EVT-TOTAL)
and by 0.9 percentage points in the overall event ex-
traction task (see Table 4, ALL-TOTAL).
On the test data we achieved an F-score similar
to the current JULIELab system trained on modified
CoNLL?07 dependencies from the MST parser (see
Table 5, ALL-TOTAL).18 The results on the offi-
cial test data reveal that the performance differences
between various parsers may play a much smaller
role than the proper choice of dependency represen-
tations.19 Our empirical findings that the best per-
formance results could only be achieved by event-
specific dependency graph configurations reveal that
the syntactic representations of different semantic
events vary considerably at the level of dependency
graph complexity and that the automatic prediction
of such syntactic structures can vary from one de-
pendency parser to the other.
6 Discussion
The evaluation results from Table 1 show that an in-
creased F-score is basically due to a better perfor-
mance in terms of precision. For example, the M+C
evaluation results in the Stanford basic mode pro-
vide an increased precision by 2 percentage points
compared to the Stanford ccprocessed mode. There-
fore, we focus here on the analysis of false positives
18The current JULIELab system uses event-specific trimming
procedures on CoNLL?07 dependencies determined on the de-
velopment data set (see Buyko et al (2010)).
19Trimmed CoNLL dependencies are used in both system
configurations.
that the JULIELab system extracts in various modes.
For the first analysis we took the outputs of the
systems based on the M+C parsing results. We
scrutinized on the Stanford basic and ccprocessed
false positives (fps) and we compared the occur-
rences of dependency labels in two data sets, namely
the intersection of false positives from both sys-
tem modes (set A) and the false positives produced
only by the system with a worse performance (set
B, ccprocessed mode). About 70% of all fps are
contained in set A. Our analysis revealed that some
dependency labels have a higher occurrence in set
B, e.g., nsubjpass, prep on, prep with,
prep in, prep for, prep as. Some depen-
dency labels occur only in set B such as agent,
prep unlike, prep upon. It seems that col-
lapsing some prepositions, such as ?with?, ?in?,
?for?, ?as?, ?on?, ?unlike?, ?upon?, does not have
a positive effect on the extraction of argument struc-
tures. In a second step, we compared the Stan-
ford basic and CoNLL?07 false positive sets. The
fps of both systems have an intersection of about
70%. We also compared the intersection of fps
between two outputs (set A) and the set of addi-
tional fps of the system with worse results (Stan-
ford basic mode, set B). The dependency labels such
as abbrev, dep, nsubj, nsubjpass have
a higher occurrence in set B than in set A. This anal-
ysis renders evidence that the distinction of nsubj
and nsubjpass does not seem to have been prop-
erly learned for event extraction.
For the second analysis round we took the out-
puts of the MST parsing results. As in the previ-
ous experiments, we compared false positives from
two mode outputs, here the CoNLL?07 mode and
the CoNLL?07 modified for auxiliaries and coor-
dinations mode. The fps have an intersection of
75%. The dependency labels such as VC, SUBJ,
COORD, and IOBJ occur more frequently in the ad-
ditional false positives from the CoNLL?07 mode
than in the intersection of false positives from both
system outputs. Obviously, the trimming of auxil-
iary and coordination structures has a direct positive
effect on the argument extraction reducing false pos-
itive numbers especially with corresponding depen-
dency labels in shortest dependency paths.
Our analysis of false positives shows that the dis-
tinction between active and passive subject labels,
990
abbreviation labels, as well as collapsing preposi-
tions in the Stanford dependencies, could not have
been properly learned, which consequently leads to
an increased rate of false positives. The trimming
of auxiliary structures and the subsequent coordina-
tion collapsing on CoNLL?07 dependencies has in-
deed event-specific positive effects on the event ex-
traction.
The main focus of this work has been on the eval-
uation of effects of different dependency graph rep-
resentations on the IE task achievement (here the
task of event extraction). But we also targeted the
task-oriented evaluation of top-performing syntactic
parsers. The results of this work indicate that the
GENIA-trained parsers, i.e., M+C parser, the MST,
MALT and GDep, are a reasonable basis for achiev-
ing state-of-the art performance in biomedical event
extraction.
But the choice of the most suitable parser should
also take into account its performance in terms of
parsing time. Cer et al (2010) and Miyao et al
(2008) showed in their experiments that native de-
pendency parsers are faster than constituency-based
parsers. When it comes to scaling event extraction
to huge biomedical document collections, such as
MEDLINE, the selection of a parser is mainly in-
fluenced by its run-time performance. MST, MALT
and GDep parsers or the M+C parser with reduced
reranking (Cer et al, 2010) would thus be an appro-
priate choice for large-scale event extraction under
these constraints.20
7 Conclusion
In this paper, we investigated the role different de-
pendency representations may have on the accom-
plishment of the event extraction task as exemplified
by biological events. Different representation for-
mats (mainly, Stanford vs. CoNLL) were then ex-
perimentally compared employing different parsers
(Bikel, Charniak+Johnson, GDep, MST, MALT),
both constituency based (for the derivative depen-
dency mode) as well as dependency based (for
the native dependency mode), considering different
training scenarios (newspaper vs. biology domain).
From our experiments we draw the conclusion
20For large-scale experiments an evaluation of the M+C with
reduced reranking should be provided.
that the dependency graph representation has a cru-
cial impact on the level of achievement of IE task
requirements. Surprisingly, the CoNLL?X depen-
dencies outperform the Stanford dependencies for
four from six parsers. With additionally trimmed
CoNLL?X dependencies we could achieve an F-
score of 50.9% on the official test data and an F-
score of 57.2% on the official development data of
the BioNLP Shared Task on Event Extraction (see
Table 5, ALL-TOTAL).
Acknowledgements
This research was partially funded by the BOOT-
STREP project under grant FP6-028099 within the
6th Framework Programme (EC), by the CALBC
project under grant FP7-231727 within the 7th
Framework Programme (EC), and by the JENAGE
project under grant 0315581D from the German
Ministry of Education and Research (BMBF) as part
of the GERONTOSYS funding initiative. We also
want to thank Kenji Sagae (Institute for Creative
Technologies, University of Southern California) for
kindly providing the models of the GDEP parser.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings BioNLP 2009. Compan-
ion Volume: Shared Task on Event Extraction, pages
10?18. Boulder, Colorado, USA, June 4-5, 2009.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on multilingual dependency parsing.
In CoNLL-X ? Proceedings of the 10th Conference
on Computational Natural Language Learning, pages
149?164, New York City, N.Y., June 8-9, 2006.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani, and
Yuk Wah Wong. 2005. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139?155.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings BioNLP 2009.
Companion Volume: Shared Task on Event Extrac-
991
tion, pages 19?27. Boulder, Colorado, USA, June 4-5,
2009.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2010. Syntactic simplification and se-
mantic enrichment - Trimming dependency graphs for
event extraction. Computational Intelligence, 26(4).
Daniel Cer, Marie-Catherine de Marneffe, Dan Jurafsky,
and Chris Manning. 2010. Parsing to Stanford De-
pendencies: Trade-offs between speed and accuracy.
In LREC?2010 ? Proceedings of the 7th International
Conference on Language Resources and Evaluation,
pages 1628?1632. Valletta, Malta, May 19-21, 2010.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In ACL?05 ? Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 173?180. Ann Arbor, MI, USA, 25-30,
June, 2005.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating Treebank parsers on a biomedical
corpus. In Proceedings of the ACL 2005 Workshop on
Software, pages 14?33. Ann Arbor, MI, USA, June 30,
2005.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC?2006 ? Proceedings of the 5th International
Conference on Language Resources and Evaluation,
pages 449?454. Genoa, Italy, 24-26 May 2006.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007 ? Proceedings of the 16th Nordic
Conference of Computational Linguistics, pages 105?
112. Tartu, Estonia, May 24-25, 2007.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings BioNLP 2009. Companion Volume: Shared
Task on Event Extraction, pages 1?9. Boulder, Col-
orado, USA, June 4-5, 2009.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the LREC?98 Workshop
on the Evaluation of Parsing Systems, pages 48?56.
Granada, Spain, 28-30 May 1998.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT/EMNLP
2005 ? Proceedings of the Human Language Tech-
nology Conference and the Conference on Empirical
Methods in Natural Language Processing, pages 523?
530. Vancouver, B.C., Canada, October 6-8, 2005.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology, 8:131?146.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In HLT 2002 ?
Proceedings of the 2nd International Conference on
Human Language Technology Research, pages 292?
297. San Diego, CA, USA, March 24-27, 2002.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of the GEAF 2007
Workshop, CSLI Studies in Computational Linguistics
Online, page 21 pages.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL 2008 ? Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 46?54. Columbus,
Ohio, USA, June 15-20, 2008.
Claire Ne?dellec. 2005. Learning Language in Logic:
Genic interaction extraction challenge. In Proceedings
LLL-2005 ? 4th Learning Language in Logic Work-
shop, pages 31?37. Bonn, Germany, August 7, 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2007.
MALTPARSER: A language-independent system for
data-driven dependency parsing. Natural Language
Engineering, 13(2):95?135.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. BIOINFER: A corpus for informa-
tion extraction in the biomedical domain. BMC Bioin-
formatics, 8(50).
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In EMNLP-CoNLL 2007 ? Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing and the Conference on Computa-
tional Natural Language Learning, pages 1044?1050,
Prague, Czech Republic, June 28-30, 2007.
Daniel Sleator and Davy Temperley. 1991. Parsing En-
glish with a link grammar. Technical report, Depart-
ment of Computer Science, CMU.
Yuka Tateisi, Akane Yakushiji, and Jun?ichi Tsujii. 2005.
Syntax annotation for the GENIA corpus. In IJC-
NLP 2005 ? Proceedings of the 2nd International Joint
Conference on Natural Language Processing, pages
222?227. Jeju Island, Korea, October 11-13, 2005.
992
Book Review
Introduction to Linguistic Annotation and Text Analytics
Graham Wilcock
(University of Helsinki)
Princeton, NJ: Morgan & Claypool (Synthesis Lectures on Human Language
Technologies, edited by Graeme Hirst, volume 2, No. 1), 2009, x+149 pp;
paperbound, ISBN 978-1-59829-738-6, $40.00; ebook, ISBN 978-1-59829-739-3,
$30.00 or by subscription
Reviewed by
Udo Hahn
Friedrich-Schiller-Universita?t Jena
Looking for a book that lucidly sorts out XML, XSLT and XMI, GATE and UIMA,
WordFreak, OpenNLP, and the Stanford NLP Toolkit? Looking for meticulous guid-
ance via batteries of stylesheets and shell scripts, but also keen on exploiting special-
purpose rule languages such as JAPE, or pre-configured text analytics engines such as
ANNIE? Looking for a coherent set of exercises (covering different technical angles and
varying in task complexity) and a series of illustrative screenshots that break down the
understanding of annotation workflows into easy-to-digest atomic thematic chunks?
Preferring the hands-on ?how-to? over dry and winding theory debates and formally
basedmethodological discussions? Interested to get in touch (again) with Shakespeare?s
Sonnet 130 (the text example running throughout the entire book)? Well, Graham
Wilcock?s Introduction to Linguistic Annotation and Text Analyticsmight be a perfect match
to enjoy all this.
This volume, the third published lecture in Morgan & Claypool?s Synthesis Lectures
on Human Language Technologies series, consists of six chapters. The first one lays the
XML-focused meta language foundations and provides additional insights into XML
parsing and validation tools, as well as format-switching XML transformation routines
using XSL Transformations (XSLT). Chapter 4 continues this technical thread as it elab-
orates on frameworks for interchanging annotations between different formats using
XSLT, as well as the UML-based XMLMetadata Interchange (XMI) format, an emerging
standard to support the interchange of annotations produced by different tools. In-
troducing the WordFreak annotation tool, the second chapter sheds light on relevant
linguistic annotation layers ranging from formal sentence splitting and tokenization
via part-of-speech tagging, syntactic constituency parsing, and semantic predicate?
argument analysis up to discourse phenomena such as co-reference resolution. Al-
though this chapter focuses primarily on manual annotation (there is also a subsection
at the end where WordFreak is linked with OpenNLP), the following one features
automatic statistical annotation procedures. Easy-to-plug-in OpenNLP is contrasted here
with stand-alone Stanford NLP tools at all major levels of the linguistic food chain,
namely, sentence and token splitting, chunking and parsing, named entity recognition,
and co-reference resolution. Increasing the level of abstraction at the systems level, the
author then advances to comparing GATE and UIMA, two alternative architectures for
text analytics. His emphasis is on the proper configuration and task-specific customiza-
tion (e.g., by introducing the JAPE rule language in GATE and the regex annotator in
UIMA, both serving to improve named entity recognition). Again, practical exercises
Computational Linguistics Volume 36, Number 4
are discussed in detail running through all levels of linguistic processing (integrating
gazetteers/dictionaries, POS tagging, NP chunking and full parsing, named entity
recognition, co-reference resolution, etc.). The final chapter concludes with a survey
of commercially available tools doing text analytics (e.g., alias-i?s LingPipe or IBM?s
LanguageWare). Furthermore, an advanced treatment of named entity recognition (for
job titles) and co-reference resolution is provided using the open-source frameworks
of rule-based GATE and UIMA solutions (both incorporating customized dictionaries)
and statistically based OpenNLP.
The book assumes little prior knowledge, although regular expression, JAVA, and
XML basics will certainly be helpful. It is targeted at undergraduate level (not necessar-
ily computer science) students who wish to gather experience in reusing, modifying,
and customizing existing linguistic annotation tools and text analytics architectures.
Throughout the book, the author directs the reader to open-source, freely available,
platform-independent, and easily downloadable software resources and repositories.
So students find themselves embedded in a stimulating playground where tooling is
the message.
The book is comprehensively written, well-structured, and very easy to follow,
in particular when students have the exercises run simultaneously on their personal
machines. The author has a clearly designed didactical master plan in mind which is
realized by a large number of exercises with increasing, but never particularly high,
complexity (see, e.g., the fourth chapter that deals with a nice set of transformation
problems involvingWordFreak?OpenNLP [XML?plain text], GATE?WordFreak [XML?
XML], and WordFreak?UIMA [XML?XMI] tool [language] pairs). These exercises are
reasonably chosen and solutions are technically well prepared and exhaustively ex-
plained with an admirable degree of clarity.
However, the more advanced and experienced reader might find the continuous
pampering by way of overly fine-grained thematic exposition and visualization a bit
excessive, perhaps even wearisome. There is, for example, also no division into easy,
medium, and hard problems to offer challenging tasks at different levels of students?
understanding. If the book is used in the context of more advanced teaching, it should
be complemented by much more technical standard reference textbooks providing
complementary theoretical background information on empirical NLP, (supervised)
machine learning methods for NLP, computational corpus linguistics, and so forth. Yet,
for getting used to mapping routines, workflows, and software underlying linguistic
meta data annotation, this tutorial fills certainly a gap for students who come across
these topics for the first time and enjoy the all-embracing tutorial approach of the author.
Any real complaints? Just a minor remark: all (URL) references are almost unread-
able (Web links were not removed from the printed version of the book).
There is also a dedicated Web site which contains copies of the material from this
book.1
This book review was edited by Pierre Isabelle.
Udo Hahn is Professor of Computational Linguistics and Language Technology at Friedrich-
Schiller-Universita?t Jena (Germany) and Head of the Jena University Language & Information
Engineering (JULIE) Lab. He works on text analytics (semantic search technologies, information
extraction, text mining, and text summarization), with focus on biomedical language process-
ing. Hahn?s address is Computerlinguistik, Friedrich-Schiller-Universita?t Jena, D-07743 Jena,
Germany; e-mail: udo.hahn@uni-jena.de; URL: http://www.julielab.de/.
1 http://sites.morganclaypool.com/wilcock.
766
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1158?1167,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Cognitive Cost Model of Annotations Based on Eye-Tracking Data
Katrin Tomanek
Language & Information
Engineering (JULIE) Lab
Universita?t Jena
Jena, Germany
Udo Hahn
Language & Information
Engineering (JULIE) Lab
Universita?t Jena
Jena, Germany
Steffen Lohmann
Dept. of Computer Science &
Applied Cognitive Science
Universita?t Duisburg-Essen
Duisburg, Germany
Ju?rgen Ziegler
Dept. of Computer Science &
Applied Cognitive Science
Universita?t Duisburg-Essen
Duisburg, Germany
Abstract
We report on an experiment to track com-
plex decision points in linguistic meta-
data annotation where the decision behav-
ior of annotators is observed with an eye-
tracking device. As experimental con-
ditions we investigate different forms of
textual context and linguistic complexity
classes relative to syntax and semantics.
Our data renders evidence that annotation
performance depends on the semantic and
syntactic complexity of the decision points
and, more interestingly, indicates that full-
scale context is mostly negligible ? with
the exception of semantic high-complexity
cases. We then induce from this obser-
vational data a cognitively grounded cost
model of linguistic meta-data annotations
and compare it with existing non-cognitive
models. Our data reveals that the cogni-
tively founded model explains annotation
costs (expressed in annotation time) more
adequately than non-cognitive ones.
1 Introduction
Today?s NLP systems, in particular those rely-
ing on supervised ML approaches, are meta-data
greedy. Accordingly, in the past years, we have
witnessed a massive quantitative growth of anno-
tated corpora. They differ in terms of the nat-
ural languages and domains being covered, the
types of linguistic meta-data being solicited, and
the text genres being served. We have seen large-
scale efforts in syntactic and semantic annotations
in the past related to POS tagging and parsing,
on the one hand, and named entities and rela-
tions (propositions), on the other hand. More re-
cently, we are dealing with even more challeng-
ing issues such as subjective language, a large
variety of co-reference and (e.g., RST-style) text
structure phenomena, Since the NLP community
is further extending their work into these more and
more sophisticated semantic and pragmatic analyt-
ics, there seems to be no end in sight for increas-
ingly complex and diverse annotation tasks.
Yet, producing annotations is pretty expensive.
So the question comes up, how we can rationally
manage these investments so that annotation cam-
paigns are economically doable without loss in an-
notation quality. The economics of annotations are
at the core of Active Learning (AL) where those
linguistic samples are focused on in the entire doc-
ument collection, which are estimated as being
most informative to learn an effective classifica-
tion model (Cohn et al, 1996). This intentional
selection bias stands in stark contrast to prevailing
sampling approaches where annotation examples
are randomly chosen.
When different approaches to AL are compared
with each other, or with standard random sam-
pling, in terms of annotation efficiency, up until
now, the AL community assumed uniform annota-
tion costs for each linguistic unit, e.g. words. This
claim, however, has been shown to be invalid in
several studies (Hachey et al, 2005; Settles et al,
2008; Tomanek and Hahn, 2010). If uniformity
does not hold and, hence, the number of annotated
units does not indicate the true annotation efforts
required for a specific sample, empirically more
adequate cost models are needed.
Building predictive models for annotation costs
has only been addressed in few studies for now
(Ringger et al, 2008; Settles et al, 2008; Arora
et al, 2009). The proposed models are based
on easy-to-determine, yet not so explanatory vari-
ables (such as the number of words to be anno-
tated), indicating that accurate models of anno-
tation costs remain a desideratum. We here, al-
ternatively, consider different classes of syntac-
tic and semantic complexity that might affect the
cognitive load during the annotation process, with
1158
the overall goal to find additional and empirically
more adequate variables for cost modeling.
The complexity of linguistic utterances can be
judged either by structural or by behavioral crite-
ria. Structural complexity emerges, e.g., from the
static topology of phrase structure trees and pro-
cedural graph traversals exploiting the topology
of parse trees (see Szmrecsa?nyi (2004) or Cheung
and Kemper (1992) for a survey of metrics of this
type). However, structural complexity criteria do
not translate directly into empirically justified cost
measures and thus have to be taken with care.
The behavioral approach accounts for this prob-
lem as it renders observational data of the an-
notators? eye movements. The technical vehicle
to gather such data are eye-trackers which have
already been used in psycholinguistics (Rayner,
1998). Eye-trackers were able to reveal, e.g.,
how subjects deal with ambiguities (Frazier and
Rayner, 1987; Rayner et al, 2006; Traxler and
Frazier, 2008) or with sentences which require
re-analysis, so-called garden path sentences (Alt-
mann et al, 2007; Sturt, 2007).
The rationale behind the use of eye-tracking de-
vices for the observation of annotation behavior is
that the length of gaze durations and behavioral
patterns underlying gaze movements are consid-
ered to be indicative of the hardness of the lin-
guistic analysis and the expenditures for the search
of clarifying linguistic evidence (anchor words) to
resolve hard decision tasks such as phrase attach-
ments or word sense disambiguation. Gaze dura-
tion and search time are then taken as empirical
correlates of linguistic complexity and, hence, un-
cover the real costs. We therefore consider eye-
tracking as a promising means to get a better un-
derstanding of the nature of the linguistic annota-
tion processes with the ultimate goal of identifying
predictive factors for annotation cost models.
In this paper, we first describe an empirical
study where we observed the annotators? reading
behavior while annotating a corpus. Section 2
deals with the design of the study, Section 3 dis-
cusses its results. In Section 4 we then focus on
the implications this study has on building cost
models and compare a simple cost model mainly
relying on word and character counts and addi-
tional simple descriptive characteristics with one
that can be derived from experimental data as pro-
vided from eye-tracking. We conclude with ex-
periments which reveal that cognitively grounded
models outperform simpler ones relative to cost
prediction using annotation time as a cost mea-
sure. Based on this finding, we suggest that cog-
nitive criteria are helpful for uncovering the real
costs of corpus annotation.
2 Experimental Design
In our study, we applied, for the first time ever to
the best of our knowledge, eye-tracking to study
the cognitive processes underlying the annotation
of linguistic meta-data, named entities in particu-
lar. In this task, a human annotator has to decide
for each word whether or not it belongs to one of
the entity types of interest.
We used the English part of the MUC7 corpus
(Linguistic Data Consortium, 2001) for our study.
It contains New York Times articles from 1996 re-
porting on plane crashes. These articles come al-
ready annotated with three types of named entities
considered important in the newspaper domain,
viz. ?persons?, ?locations?, and ?organizations?.
Annotation of these entity types in newspaper
articles is admittedly fairly easy. We chose this
rather simple setting because the participants in
the experiment had no previous experience with
document annotation and no serious linguistic
background. Moreover, the limited number of
entity types reduced the amount of participants?
training prior to the actual experiment, and posi-
tively affected the design and handling of the ex-
perimental apparatus (see below).
We triggered the annotation processes by giving
our participants specific annotation examples. An
example consists of a text document having one
single annotation phrase highlighted which then
had to be semantically annotated with respect to
named entity mentions. The annotation task was
defined such that the correct entity type had to be
assigned to each word in the annotation phrase. If
a word belongs to none of the three entity types a
fourth class called ?no entity? had to be assigned.
The phrases highlighted for annotation were
complex noun phrases (CNPs), each a sequence of
words where a noun (or an equivalent nominal ex-
pression) constitutes the syntactic head and thus
dominates dependent words such as determin-
ers, adjectives, or other nouns or nominal expres-
sions (including noun phrases and prepositional
phrases). CNPs with even more elaborate inter-
nal syntactic structures, such as coordinations, ap-
positions, or relative clauses, were isolated from
1159
their syntactic host structure and the intervening
linguistic material containing these structures was
deleted to simplify overly long sentences. We also
discarded all CNPs that did not contain at least
one entity-critical word, i.e., one which might be a
named entity according to its orthographic appear-
ance (e.g., starting with an upper-case letter). It
should be noted that such orthographic signals are
by no means a sufficient condition for the presence
of a named entity mention within a CNP.
The choice of CNPs as stimulus phrases is mo-
tivated by the fact that named entities are usually
fully encoded by this kind of linguistic structure.
The chosen stimulus ? an annotation example with
one phrase highlighted for annotation ? allows for
an exact localization of the cognitive processes
and annotation actions performed relative to that
specific phrase.
2.1 Independent Variables
We defined two measures for the complexity of
the annotation examples: The syntactic complex-
ity was given by the number of nodes in the con-
stituent parse tree which are dominated by the an-
notation phrase (Szmrecsa?nyi, 2004).1 According
to a threshold on the number of nodes in such a
parse tree, we classified CNPs as having either
high or low syntactic complexity.
The semantic complexity of an annotation ex-
ample is based on the inverse document frequency
df of the words in the annotation phrase according
to a reference corpus.2 We calculated the seman-
tic complexity score of an annotation phrase as
max i 1df (wi) , where wi is the i-th word of the anno-
tation phrase. Again, we empirically determined a
threshold classifying annotation phrases as having
either high or low semantic complexity. Addition-
ally, this automatically generated classification
was manually checked and, if necessary, revised
by two annotation experts. For instance, if an an-
notation phrase contained a strong trigger (e.g., a
social role or job title, as with ?spokeswoman? in
the annotation phrase ?spokeswoman Arlene?), it
was classified as a low-semantic-complexity item
even though it might have been assigned a high
inverse document frequency (due to the infrequent
word ?Arlene?).
1Constituency parse structure was obtained from the
OPENNLP parser (http://opennlp.sourceforge.
net/) trained on PennTreeBank data.
2We chose the English part of the Reuters RCV2 corpus
as the reference corpus for our experiments.
Two experimental groups were formed to study
different contexts. In the document context con-
dition the whole newspaper article was shown as
annotation example, while in the sentence context
condition only the sentence containing the annota-
tion phrase was presented. The participants3 were
randomly assigned to one of these groups. We de-
cided for this between-subjects design to avoid any
irritation of the participants caused by constantly
changing contexts. Accordingly, the participants
were assigned to one of the experimental groups
and corresponding context condition already in the
second training phase that took place shortly be-
fore the experiment started (see below).
2.2 Hypotheses and Dependent Variables
We tested the following two hypotheses:
Hypothesis H1: Annotators perform differently
in the two context conditions.
H1 is based on the linguistically plausible
assumption that annotators are expected to
make heavy use of the surrounding context
because such context could be helpful for the
correct disambiguation of entity classes. Ac-
cordingly, lacking context, an annotator is ex-
pected to annotate worse than under the con-
dition of full context. However, the availabil-
ity of (too much) context might overload and
distract annotators, with a presumably nega-
tive effect on annotation performance.
Hypothesis H2: The complexity of the annota-
tion phrases determines the annotation per-
formance.
The assumption is that high syntactic or se-
mantic complexity significantly lowers the
annotation performance.
In order to test these hypotheses we collected data
for the following dependent variables: (a) the an-
notation accuracy ? we identified erroneous enti-
ties by comparison with the original gold annota-
tions in the MUC7 corpus, (b) the time needed per
annotation example, and (c) the distribution and
duration of the participants? eye gazes.
320 subjects (12 female) with an average age of 24 years
(mean = 24, standard deviation (SD) = 2.8) and normal or
corrected-to-normal vision capabilities took part in the study.
All participants were students with a computing-related study
background, with good to very good English language skills
(mean = 7.9, SD = 1.2, on a ten-point scale with 1 = ?poor?
and 10 = ?excellent?, self-assessed), but without any prior
experience in annotation and without previous exposure to
linguistic training.
1160
2.3 Stimulus Material
According to the above definition of complex-
ity, we automatically preselected annotation ex-
amples characterized by either a low or a high de-
gree of semantic and syntactic complexity. After
manual fine-tuning of the example set assuring an
even distribution of entity types and syntactic cor-
rectness of the automatically derived annotation
phrases, we finally selected 80 annotation exam-
ples for the experiment. These were divided into
four subsets of 20 examples each falling into one
of the following complexity classes:
sem-syn: low semantic/low syntactic complexity
SEM-syn: high semantic/low syntactic complexity
sem-SYN: low semantic/high syntactic complexity
SEM-SYN: high semantic/high syntactic complexity
2.4 Experimental Apparatus and Procedure
The annotation examples were presented in a
custom-built tool and its user interface was kept
as simple as possible not to distract the eye move-
ments of the participants. It merely contained one
frame showing the text of the annotation example,
with the annotation phrase being highlighted. A
blank screen was shown after each annotation ex-
ample to reset the eyes and to allow a break, if
needed. The time the blank screen was shown was
not counted as annotation time. The 80 annotation
examples were presented to all participants in the
same randomized order, with a balanced distribu-
tion of the complexity classes. A variation of the
order was hardly possible for technical and ana-
lytical reasons but is not considered critical due to
extensive, pre-experimental training (see below).
The limitation on 80 annotation examples reduces
the chances of errors due to fatigue or lack of at-
tention that can be observed in long-lasting anno-
tation activities.
Five introductory examples (not considered in
the final evaluation) were given to get the subjects
used to the experimental environment. All anno-
tation examples were chosen in a way that they
completely fitted on the screen (i.e., text length
was limited) to avoid the need for scrolling (and
eye distraction). The position of the CNP within
the respective context was randomly distributed,
excluding the first and last sentence.
The participants used a standard keyboard to as-
sign the entity types for each word of the annota-
tion example. All but 5 keys were removed from
the keyboard to avoid extra eye movements for fin-
ger coordination (three keys for the positive en-
tity classes, one for the negative ?no entity? class,
and one to confirm the annotation). Pre-tests had
shown that the participants could easily issue the
annotations without looking down at the keyboard.
We recorded the participant?s eye movements
on a Tobii T60 eye-tracking device which is in-
visibly embedded in a 17? TFT monitor and com-
paratively tolerant to head movements. The partic-
ipants were seated in a comfortable position with
their head in a distance of 60-70 cm from the mon-
itor. Screen resolution was set to 1280 x 1024 px
and the annotation examples were presented in the
middle of the screen in a font size of 16 px and a
line spacing of 5 px. The presentation area had no
fixed height and varied depending on the context
condition and length of the newspaper article. The
text was always vertically centered on the screen.
All participants were familiarized with the
annotation task and the guidelines in a pre-
experimental workshop where they practiced an-
notations on various exercise examples (about 60
minutes). During the next two days, one after the
other participated in the actual experiment which
took between 15 and 30 minutes, including cali-
bration of the eye-tracking device. Another 20-30
minutes of training time directly preceded the ex-
periment. After the experiment, participants were
interviewed and asked to fill out a questionnaire.
Overall, the experiment took about two hours for
each participant for which they were financially
compensated. Participants were instructed to fo-
cus more on annotation accuracy than on annota-
tion time as we wanted to avoid random guess-
ing. Accordingly, as an extra incentive, we re-
warded the three participants with the highest an-
notation accuracy with cinema vouchers. None of
the participants reported serious difficulties with
the newspaper articles or annotation tool and all
understood the annotation task very well.
3 Results
We used a mixed-design analysis of variance
(ANOVA) model to test the hypotheses, with the
context condition as between-subjects factor and
the two complexity classes as within-subject fac-
tors.
3.1 Testing Context Conditions
To test hypothesis H1 we compared the number
of annotation errors on entity-critical words made
1161
above before anno phrase after below
percentage of participants looking at a sub-area 35% 32% 100% 34% 16%
average number of fixations per sub-area 2.2 14.1 1.3
Table 1: Distribution of annotators? attention among sub-areas per annotation example.
by the annotators in the two contextual conditions
(complete document vs. sentence). Surprisingly,
on the total of 174 entity-critical words within
the 80 annotation examples, we found exactly the
same mean value of 30.8 errors per participant in
both conditions. There were also no significant
differences in the average time needed to annotate
an example in both conditions (means of 9.2 and
8.6 seconds, respectively, with F (1, 18) = 0.116,
p = 0.74).4 These results seem to suggest that it
makes no difference (neither for annotation accu-
racy nor for time) whether or not annotators are
shown textual context beyond the sentence that
contains the annotation phrase.
To further investigate this finding we analyzed
eye-tracking data of the participants gathered for
the document context condition. We divided the
whole text area into five sub-areas as schemat-
ically shown in Figure 1. We then determined
the average proportion of participants that directed
their gaze at least once at these sub-areas. We con-
sidered all fixations with a minimum duration of
100 ms, using a fixation radius (i.e., the smallest
distance that separates fixations) of 30 px and ex-
cluded the first second (mainly used for orientation
and identification of the annotation phrase).
Figure 1: Schematic visualization of the sub-areas
of an annotation example.
Table 1 reveals that on average only 35% of the
4In general, we observed a high variance in the number of
errors and time values between the subjects. While, e.g., the
fastest participant handled an example in 3.6 seconds on the
average, the slowest one needed 18.9 seconds; concerning
the annotation errors on the 174 entity-critical words, these
ranged between 21 and 46 errors.
participants looked in the textual context above the
annotation phrase embedding sentence, and even
less perceived the context below (16%). The sen-
tence parts before and after the annotation phrase
were, on the average, visited by one third (32%
and 34%, respectively) of the participants. The
uneven distribution of the annotators? attention be-
comes even more apparent in a comparison of the
total number of fixations on the different text parts:
14 out of an average of 18 fixations per example
were directed at the annotation phrase and the sur-
rounding sentence, the text context above the an-
notation chunk received only 2.2 fixations on the
average and the text context below only 1.3.
Thus, the eye-tracking data indicates that the
textual context is not as important as might have
been expected for quick and accurate annotation.
This result can be explained by the fact that par-
ticipants of the document-context condition used
the context whenever they thought it might help,
whereas participants of the sentence-context con-
dition spent more time thinking about a correct an-
swer, overall with the same result.
3.2 Testing Complexity Classes
To test hypothesis H2 we also compared the av-
erage annotation time and the number of errors
on entity-critical words for the complexity subsets
(see Table 2). The ANOVA results show highly
significant differences for both annotation time
and errors.5 A pairwise comparison of all sub-
sets in both conditions with a t-test showed non-
significant results only between the SEM-syn and
syn-SEM subsets.6
Thus, the empirical data generally supports hy-
pothesis H2 in that the annotation performance
seems to correlate with the complexity of the an-
notation phrase, on the average.
5Annotation time results: F (1, 18) = 25, p < 0.01 for
the semantic complexity and F (1, 18) = 76.5, p < 0.01
for the syntactic complexity; Annotation complexity results:
F (1, 18) = 48.7, p < 0.01 for the semantic complexity and
F (1, 18) = 184, p < 0.01 for the syntactic complexity.
6t(9) = 0.27, p = 0.79 for the annotation time in the
document context condition, and t(9) = 1.97, p = 0.08 for
the annotation errors in the sentence context condition.
1162
experimental complexity e.-c. time errors
condition class words mean SD mean SD rate
sem-syn 36 4.0s 2.0 2.7 2.1 .075
document SEM-syn 25 9.2s 6.7 5.1 1.4 .204
condition sem-SYN 51 9.6s 4.0 9.1 2.9 .178
SEM-SYN 62 14.2s 9.5 13.9 4.5 .224
sem-syn 36 3.9s 1.3 1.1 1.4 .031
sentence SEM-syn 25 7.5s 2.8 6.2 1.9 .248
condition sem-SYN 51 9.6s 2.8 9.0 3.9 .176
SEM-SYN 62 13.5s 5.0 14.5 3.4 .234
Table 2: Average performance values for the 10 subjects of each experimental condition and 20 anno-
tation examples of each complexity class: number of entity-critical words, mean annotation time and
standard deviations (SD), mean annotation errors, standard deviations, and error rates (number of errors
divided by number of entity-critical words).
3.3 Context and Complexity
We also examined whether the need for inspect-
ing the context increases with the complexity of
the annotation phrase. Therefore, we analyzed the
eye-tracking data in terms of the average num-
ber of fixations on the annotation phrase and on
its embedding contexts for each complexity class
(see Table 3). The values illustrate that while the
number of fixations on the annotation phrase rises
generally with both the semantic and the syntactic
complexity, the number of fixations on the context
rises only with semantic complexity. The num-
ber of fixations on the context is nearly the same
for the two subsets with low semantic complexity
(sem-syn and sem-SYN, with 1.0 and 1.5), while
it is significantly higher for the two subsets with
high semantic complexity (5.6 and 5.0), indepen-
dent of the syntactic complexity.7
complexity fix. on phrase fix. on context
class mean SD mean SD
sem-syn 4.9 4.0 1.0 2.9
SEM-syn 8.1 5.4 5.6 5.6
sem-SYN 18.1 7.7 1.5 2.0
SEM-SYN 25.4 9.3 5.0 4.1
Table 3: Average number of fixations on the anno-
tation phrase and context for the document condi-
tion and 20 annotation examples of each complex-
ity class.
These results suggest that the need for context
mainly depends on the semantic complexity of the
annotation phrase, while it is less influenced by its
syntactic complexity.
7ANOVA result of F (1, 19) = 19.7, p < 0.01 and sig-
nificant differences also in all pairwise comparisons.
phrase antecedent
Figure 2: Annotation example with annotation
phrase and the antecedent for ?Roselawn? in the
text (left), and gaze plot of one participant show-
ing a scanning-for-coreference behavior (right).
This finding is also qualitatively supported by
the gaze plots we generated from the eye-tracking
data. Figure 2 shows a gaze plot for one partici-
pant that illustrates a scanning-for-coreference be-
havior we observed for several annotation phrases
with high semantic complexity. In this case, words
were searched in the upper context, which accord-
ing to their orthographic signals might refer to a
named entity but which could not completely be
resolved only relying on the information given by
the annotation phrase itself and its embedding sen-
tence. This is the case for ?Roselawn? in the an-
notation phrase ?Roselawn accident?. The con-
text reveals that Roselawn, which also occurs in
the first sentence, is a location. A similar proce-
dure is performed for acronyms and abbreviations
which cannot be resolved from the immediate lo-
cal context ? searches mainly visit the upper con-
text. As indicated by the gaze movements, it also
became apparent that texts were rather scanned for
hints instead of being deeply read.
1163
4 Cognitively Grounded Cost Modeling
We now discuss whether the findings on dependent
variables from our eye-tracking study are fruitful
for actually modeling annotation costs. There-
fore, we learn a linear regression model with time
(an operationalization of annotation costs) as the
dependent variable. We compare our ?cognitive?
model against a baseline model which relies on
some simple formal text features only, and test
whether the newly introduced features help predict
annotation costs more accurately.
4.1 Features
The features for the baseline model, character- and
word-based, are similar to the ones used by Ring-
ger et al (2008) and Settles et al (2008).8 Our
cognitive model, however, makes additional use
of features based on linguistic complexity, and in-
cludes syntactic and semantic criteria related to the
annotation phrases. These features were inspired
by the insights provided by our eye-tracking ex-
periments. All features are designed such that they
can automatically be derived from unlabeled data,
a necessary condition for such features to be prac-
tically applicable.
To account for our findings that syntactic and
semantic complexity correlates with annotation
performance, we added three features based on
syntactic, and two based on semantic complex-
ity measures. We decided for the use of multiple
measures because there is no single agreed-upon
metric for either syntactic or semantic complex-
ity. This decision is further motivated by find-
ings which reveal that different measures are often
complementary to each other so that their combi-
nation better approximates the inherent degrees of
complexity (Roark et al, 2007).
As for syntactic complexity, we use two mea-
sures based on structural complexity including (a)
the number of nodes of a constituency parse tree
which are dominated by the annotation phrase
(cf. Section 2.1), and (b) given the dependency
graph of the sentence embedding the annotation
phrase, we consider the distance between words
for each dependency link within the annotation
phrase and consider the maximum over such dis-
8In preliminary experiments our set of basic features com-
prised additional features providing information on the usage
of stop words in the annotation phrase and on the number
of paragraphs, sentences, and words in the respective annota-
tion example. However, since we found these features did not
have any significant impact on the model, we removed them.
tance values as another metric for syntactic com-
plexity. Lin (1996) has already shown that human
performance on sentence processing tasks can be
predicted using such a measure. Our third syn-
tactic complexity measure is based on the prob-
ability of part-of-speech (POS) 2-grams. Given
a POS 2-gram model, which we learned from
the automatically POS-tagged MUC7 corpus, the
complexity of an annotation phrase is defined by
?n
i=2 P (POSi|POSi?1) where POSi refers to the
POS-tag of the i-th word of the annotation phrase.
A similar measure has been used by Roark et al
(2007) who claim that complex syntactic struc-
tures correlate with infrequent or surprising com-
binations of POS tags.
As far as the quantification of semantic com-
plexity is concerned, we use (a) the inverse docu-
ment frequency df (wi) of each word wi (cf. Sec-
tion 2.1), and a measure based on the semantic
ambiguity of each word, i.e., the number of mean-
ings contained in WORDNET,9 within an annota-
tion phrase. We consider the maximum ambigu-
ity of the words within the annotation phrase as
the overall ambiguity of the respective annotation
phrase. This measure is based on the assumption
that annotation phrases with higher semantic am-
biguity are harder to annotate than low-ambiguity
ones. Finally, we add the Flesch-Kincaid Read-
ability Score (Klare, 1963), a well-known metric
for estimating the comprehensibility and reading
complexity of texts.
As already indicated, some of the hardness of
annotations is due to tracking co-references and
abbreviations. Both often cannot be resolved lo-
cally so that annotators need to consult the con-
text of an annotation chunk (cf. Section 3.3).
Thus, we also added features providing informa-
tion whether the annotation phrases contain entity-
critical words which may denote the referent of an
antecedent of an anaphoric relation. In the same
vein, we checked whether an annotation phrase
contains expressions which can function as an ab-
breviation by virtue of their orthographical appear-
ance, e.g., consist of at least two upper-case letters.
Since our participants were sometimes scanning
for entity-critical words, we also added features
providing information on the number of entity-
critical words within the annotation phrase. Ta-
ble 4 enumerates all feature classes and single fea-
tures used for determining our cost model.
9http://wordnet.princeton.edu/
1164
Feature Group # Features Feature Description
characters (basic) 6 number of characters and words per annotation phrase; test whether
words in a phrase start with capital letters, consist of capital letters only,
have alphanumeric characters, or are punctuation symbols
words 2 number of entity-critical words and percentage of entity-critical words
in the annotation phrase
complexity 6 syntactic complexity: number of dominated nodes, POS n-gram proba-
bility, maximum dependency distance;
semantic complexity: inverse document frequency, max. ambiguity;
general linguistic complexity: Flesch-Kincaid Readability Score
semantics 3 test whether entity-critical word in annotation phrase is used in docu-
ment (preceding or following current phrase); test whether phrase con-
tains an abbreviation
Table 4: Features for cost modeling.
4.2 Evaluation
To test how well annotation costs can be mod-
eled by the features described above, we used the
MUC7T corpus, a re-annotation of the MUC7 cor-
pus (Tomanek and Hahn, 2010). MUC7T has time
tags attached to the sentences and CNPs. These
time tags indicate the time it took to annotate the
respective phrase for named entity mentions of the
types person, location, and organization. We here
made use of the time tags of the 15,203 CNPs in
MUC7T . MUC7T has been annotated by two an-
notators (henceforth called A and B) and so we
evaluated the cost models for both annotators. We
learned a simple linear regression model with the
annotation time as dependent variable and the fea-
tures described above as independent variables.
The baseline model only includes the basic feature
set, whereas the ?cognitive? model incorporates all
features described above.
Table 5 depicts the performance of both mod-
els induced from the data of annotator A and B.
The coefficient of determination (R2) describes
the proportion of the variance of the dependent
variable that can be described by the given model.
We report adjusted R2 to account for the different
numbers of features used in both models.
model R2 on A?s data R2 on B?s data
baseline 0.4695 0.4640
cognitive 0.6263 0.6185
Table 5: Adjusted R2 values on both models and
for annotators A and B.
For both annotators, the baseline model is sig-
nificantly outperformed in terms of R2 by our
?cognitive? model (p < 0.05). Considering the
features that were inspired from the eye-tracking
study, R2 is increased from 0.4695 to 0.6263 on
the timing data of annotator A, and from 0.464 to
0.6185 on the data of annotator B. These numbers
clearly demonstrate that annotation costs are more
adequately modelled by the additional features we
identified through our eye-tracking study.
Our ?cognitive? model now consists of 21 co-
efficients. We tested for the significance of this
model?s regression terms. For annotator A we
found all coefficients to be significant with respect
to the model (p < 0.05), for annotator B all coeffi-
cients except one were significant. Figure 6 shows
the coefficients of annotator A?s ?cognitive? model
along with the standard errors and t-values.
5 Summary and Conclusions
In this paper, we explored the use of eye-tracking
technology to investigate the behavior of human
annotators during the assignment of three types of
named entities ? persons, organizations and loca-
tions ? based on the eye-mind assumption. We
tested two main hypotheses ? one relating to the
amount of contextual information being used for
annotation decisions, the other relating to differ-
ent degrees of syntactic and semantic complex-
ity of expressions that had to be annotated. We
found experimental evidence that the textual con-
text is searched for decision making on assigning
semantic meta-data at a surprisingly low rate (with
1165
Feature Group Feature Name/Coefficient Estimate Std. Error t value Pr(>|t|)
(Intercept) 855.0817 33.3614 25.63 0.0000
characters (basic) token number -304.3241 29.6378 -10.27 0.0000
char number 7.1365 2.2622 3.15 0.0016
has token initcaps 244.4335 36.1489 6.76 0.0000
has token allcaps -342.0463 62.3226 -5.49 0.0000
has token alphanumeric -197.7383 39.0354 -5.07 0.0000
has token punctuation -303.7960 50.3570 -6.03 0.0000
words number tokens entity like 934.3953 13.3058 70.22 0.0000
percentage tokens entity like -729.3439 43.7252 -16.68 0.0000
complexity sem compl inverse document freq 392.8855 35.7576 10.99 0.0000
sem compl maximum ambiguity -13.1344 1.8352 -7.16 0.0000
synt compl number dominated nodes 87.8573 7.9094 11.11 0.0000
synt compl pos ngram probability 287.8137 28.2793 10.18 0.0000
syn complexity max dependency distance 28.7994 9.2174 3.12 0.0018
flesch kincaid readability -0.4117 0.1577 -2.61 0.0090
semantics has entity critical token used above 73.5095 24.1225 3.05 0.0023
has entity critical token used below -178.0314 24.3139 -7.32 0.0000
has abbreviation 763.8605 73.5328 10.39 0.0000
Table 6: ?Cognitive? model of annotator A.
the exception of tackling high-complexity seman-
tic cases and resolving co-references) and that an-
notation performance correlates with semantic and
syntactic complexity.
The results of these experiments were taken as
a heuristic clue to focus on cognitively plausi-
ble features of learning empirically rooted cost
models for annotation. We compared a simple
cost model (basically taking the number of words
and characters into account) with a cognitively
grounded model and got a much higher fit for the
cognitive model when we compared cost predic-
tions of both model classes on the recently re-
leased time-stamped version of the MUC7 corpus.
We here want to stress the role of cognitive evi-
dence from eye-tracking to determine empirically
relevant features for the cost model. The alterna-
tive, more or less mechanical feature engineering,
suffers from the shortcoming that is has to deal
with large amounts of (mostly irrelevant) features
? a procedure which not only requires increased
amounts of training data but also is often compu-
tationally very expensive.
Instead, our approach introduces empirical,
theory-driven relevance criteria into the feature
selection process. Trying to relate observables
of complex cognitive tasks (such as gaze dura-
tion and gaze movements for named entity anno-
tation) to explanatory models (in our case, a time-
based cost model for annotation) follows a much
warranted avenue in research in NLP where fea-
ture farming becomes a theory-driven, explanatory
process rather than a much deplored theory-blind
engineering activity (cf. ACL-WS-2005 (2005)).
In this spirit, our focus has not been on fine-
tuning this cognitive cost model to achieve even
higher fits with the time data. Instead, we aimed at
testing whether the findings from our eye-tracking
study can be exploited to model annotation costs
more accurately.
Still, future work will be required to optimize
a cost model for eventual application where even
more accurate cost models may be required. This
optimization may include both exploration of ad-
ditional features (such as domain-specific ones)
as well as experimentation with other, presum-
ably non-linear, regression models. Moreover,
the impact of improved cost models on the effi-
ciency of (cost-sensitive) selective sampling ap-
proaches, such as Active Learning (Tomanek and
Hahn, 2009), should be studied.
1166
References
ACL-WS-2005. 2005. Proceedings of the ACL Work-
shop on Feature Engineering for Machine Learn-
ing in Natural Language Processing. accessible
via http://www.aclweb.org/anthology/
W/W05/W05-0400.pdf.
Gerry Altmann, Alan Garnham, and Yvette Dennis.
2007. Avoiding the garden path: Eye movements
in context. Journal of Memory and Language,
31(2):685?712.
Shilpa Arora, Eric Nyberg, and Carolyn Rose?. 2009.
Estimating annotation cost for active learning in a
multi-annotator environment. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 18?26.
Hintat Cheung and Susan Kemper. 1992. Competing
complexity metrics and adults? production of com-
plex sentences. Applied Psycholinguistics, 13:53?
76.
David Cohn, Zoubin Ghahramani, and Michael Jordan.
1996. Active learning with statistical models. Jour-
nal of Artificial Intelligence Research, 4:129?145.
Lyn Frazier and Keith Rayner. 1987. Resolution of
syntactic category ambiguities: Eye movements in
parsing lexically ambiguous sentences. Journal of
Memory and Language, 26:505?526.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL 2005 ? Proceedings of
the 9th Conference on Computational Natural Lan-
guage Learning, pages 144?151.
George Klare. 1963. The Measurement of Readability.
Ames: Iowa State University Press.
Dekang Lin. 1996. On the structural complexity of
natural language sentences. In COLING 1996 ? Pro-
ceedings of the 16th International Conference on
Computational Linguistics, pages 729?733.
Linguistic Data Consortium. 2001. Message Under-
standing Conference (MUC) 7. Philadelphia: Lin-
guistic Data Consortium.
Keith Rayner, Anne Cook, Barbara Juhasz, and Lyn
Frazier. 2006. Immediate disambiguation of lex-
ically ambiguous words during reading: Evidence
from eye movements. British Journal of Psychol-
ogy, 97:467?482.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 126:372?422.
Eric Ringger, Marc Carmen, Robbie Haertel, Kevin
Seppi, Deryle Lonsdale, Peter McClanahan, James
Carroll, and Noel Ellison. 2008. Assessing the
costs of machine-assisted corpus annotation through
a user study. In LREC 2008 ? Proceedings of the 6th
International Conference on Language Resources
and Evaluation, pages 3318?3324.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Proceed-
ings of the Workshop on BioNLP 2007: Biological,
Translational, and Clinical Language Processing,
pages 1?8.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In
Proceedings of the NIPS 2008 Workshop on Cost-
Sensitive Machine Learning, pages 1?10.
Patrick Sturt. 2007. Semantic re-interpretation and
garden path recovery. Cognition, 105:477?488.
Benedikt M. Szmrecsa?nyi. 2004. On operationalizing
syntactic complexity. In Proceedings of the 7th In-
ternational Conference on Textual Data Statistical
Analysis. Vol. II, pages 1032?1039.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
ACL 2009 ? Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCNLP of the AFNLP,
pages 1039?1047.
Katrin Tomanek and Udo Hahn. 2010. Annotation
time stamps: Temporal metadata from the linguistic
annotation process. In LREC 2010 ? Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation.
Matthew Traxler and Lyn Frazier. 2008. The role of
pragmatic principles in resolving attachment ambi-
guities: Evidence from eye movements. Memory &
Cognition, 36:314?328.
1167
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 235?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Proposal for a Configurable Silver Standard
Udo Hahn, Katrin Tomanek, Elena Beisswanger and Erik Faessler
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
http://www.julielab.de
Abstract
Among the many proposals to promote al-
ternatives to costly to create gold stan-
dards, just recently the idea of a fully au-
tomatically, and thus cheaply, to set up sil-
ver standard has been launched. However,
the current construction policy for such a
silver standard requires crucial parameters
(such as similarity thresholds and agree-
ment cut-offs) to be set a priori, based on
extensive testing though, at corpus com-
pile time. Accordingly, such a corpus is
static, once it is released. We here propose
an alternative policy where silver stan-
dards can be dynamically optimized and
customized on demand (given a specific
goal function) using a gold standard as an
oracle.
1 Introduction
Training natural language systems which rely on
(semi-)supervised machine learning algorithms,
or measuring the systems? performance requires
some standardized ground truth from which one
can learn or against which one evaluate, respec-
tively. Usually, a manually crafted gold stan-
dard is provided that is generated by human lan-
guage or domain experts after lots of iterative,
guideline-based training rounds. This procedure is
expensive, slow and yields only small, yet highly
trustable, amounts of meta data ? because human
experts are in the loop.
In the CALBC project,1 an alternative ap-
proach is currently under investigation (Rebholz-
Schuhmann et al, 2010a). The basic idea is to
generate the much needed ground truth automati-
cally. This is achieved by letting a flock of named
entity taggers run on a corpus, without impos-
ing any restriction on the type(s) being annotated.
1http://www.calbc.eu
The (most likely) heterogeneous results are auto-
matically homogenized subsequently, thus yield-
ing a consensus-based, machine-generated ground
truth. Considering the possible benefits (e.g., the
positive experience from boosting-style machine
learners (Freund, 1990)), but also being aware of
the possible drawbacks (varying quality of the dif-
ferent systems, skewed coverage of entity types,
different types of guidelines on which they were
trained, etc.), the CALBC consortium refers to
the outcome of this process as a silver standard
(Rebholz-Schuhmann et al, 2010a). This proce-
dure is inexpensive, fast, yields huge amounts of
meta data ? because computers are in the loop ?
but after all its applicability and validity has yet to
be determined experimentally.
The first silver standard corpus (SSC) that came
out of the CALBC project was generated by the
four main partners? named entity taggers.2 The
various contributions covered, among others, an-
notations for genes and proteins, chemicals, dis-
eases, etc (Rebholz-Schuhmann et al, 2010b). Af-
ter the submission of their runs, the SSC was gen-
erated by, first, harmonizing stretches of text in
terms of entity mention identification and, second,
by mapping these normalized mentions to agreed-
upon type systems (such as the MESH Semantic
Groups as described by Bodenreider and McCray
(2003) for entity type normalization). Basically,
the harmonization steps included rules when en-
tity mentions were considered to match or overlap
(using a cosine-based similarity criterion) and en-
tity types referred to the same class. For consensus
generation, finally, simple rules for majority votes
were established.
The CALBC consortium is fully aware of the
fact that the value of an SSC can only be assessed
2The CALBC consortium consists the Rebholz Group
from EBI (Hinxton, U.K.), the Biosemantics Group from
Erasmus (Rotterdam, The Netherlands), the JULIE Lab (Jena,
Germany), and LINGUAMATICS (Cambridge, U.K.).
235
by comparing, e.g., systems trained on such a sil-
ver standard with systems trained on a gold stan-
dard (preferably, though not necessarily, one that
is a subset of the document set which makes up the
SSC).
In the absence of such a gold standard, the
CALBC consortium has spent enormous efforts to
find out the most reasonable parameter settings
for, e.g., the cosine threshold (setting similar men-
tions apart from dissimilar ones) or the consen-
sus constraint (where a certain number of entity
types equally assigned by different taggers makes
one type the consensual silver one and discards all
alternative annotations). Once these criteria are
made effective, the SSC is completely fixed.
As an alternative, we are looking for a more
flexible solution. Our investigation was fuelled by
the following observations:
? The idiosyncrasies of guidelines (on which
(some) taggers were trained) do not necessar-
ily lead to semantically totally different enti-
ties although they differ literally to some de-
gree. Some guidelines prefer, e.g., ?human
IL-7 protein?, others favor ?human IL-7?,
and some lean towards ?IL-7?. As the cosine
measure tends to penalize a pair such as ?hu-
man IL-7 protein? and ?IL-7?, we intended
to avoid such a prescriptive mode and just
look at the type assignment for single tokens
as (parts of) entity mentions. thus avoiding
inconclusive mention boundary discussions.
? While we were counting, for all tokens of
the document set, the votes a single token re-
ceived from different taggers in terms of an-
notating this token with respect to some type,
we generated confidence data for meta data
assignments. Incorporating the distribution
of confidence values into the configuration
process, this allows us to get rid of a pri-
ori fixed majority criteria (e.g., two or three
out of five systems must agree on this token)
which are hard to justify in an absolute way.
Summarizing, we believe that the nature of di-
verging tasks to be solved, the levels of entity type
specificity to be reached, the sort of guidelines be-
ing preferred, etc. should allow prospective users
of a silver standard to customize one on their own
and not stick to one that is already prefabricated
without concrete application in mind.3
3There may be tasks where a ?long? entity such as ?hu-
As such an enterprise would be quite arbitrary
without a reference standard, we even go one step
further. We determine the suitability of, say, dif-
ferent voting scores and varying lexical extensions
of mentions by comparison to a gold standard so
that the ?optimal? configuration of a silver stan-
dard, given a set of goal-derived requirements,
can be automatically learned. In real-world ap-
plications, such gold standard annotations would
be delivered only for a fraction of the documents
contained in the entire corpus being tagged by a
flock of taggers. The gold standard is used to op-
timize parameters which are subsequently applied
to the aggregation of automatically annotated data.
Note that the gold standard is used for optimiza-
tion only, not for training. We call such a flexible,
dynamically adjustable silver standard a config-
urable Silver Standard Corpus (conSSC). In a sec-
ond step, we split the various conSSCs, re-trained
our NER tagger on these data sets and, by compar-
ison with the gold standard, were able to identify
the optimal conSSC for this task (which is not the
one (SSC I) made available by the CALBC consor-
tium for the first challenge round).4
2 Optimizing Silver Standards
In this section, we describe the constituent param-
eters of a wide spectrum of SSCs. Mostly, these
parameters were taken over from the design of the
SSC as developed by the CALBC project members.
Differing from that fixed SSC, we investigate the
impact of different parameter settings on the con-
struction of a collection of SSCs, and, first, eval-
uate their direct usefulness on a gold standard for
protein-gene annotations. Second, we also assess
their indirect usefulness by training NER classi-
fiers on these SSCs and evaluate the NERs? perfor-
mance on the gold standard. Thus, our approach
is entirely data-driven without the need for human
intervention in terms of choosing suitable param-
eter settings.
Technically, we first aggregate the votes from
the flock of taggers (in our experiments, we used
the four taggers from the CALBC project members
plus a second tagger of one of the members) for
each text token (for confidence-based decisions)
or at the entity level (for cosine-based decisions),
then we determine the confidence values of these
man IL-7 protein? may be appropriate, while for another task
a short one such as ?IL-7? is entirely sufficient.
4http://www.ebi.ac.uk/Rebholz-srv/
CALBC/challenge.html
236
aggregated votes, and, finally, we compute the
similarity of the various SSCs with the gold stan-
dard data in terms of F-scores (both exact and open
boundaries) and accuracy on the token level.
2.1 Calibrating Consensus
The metrical interpretation of consensus will be
based on thresholded votes for semantic groups at
the token level (cf. Section 2.1.1) and a cosine-
based measure to determine contiguous stretches
of entity mentions in the text (cf. Section 2.1.2).
2.1.1 Type Confidence and Type Voting
For each text token, we determine the entity type
assignment as generated by each NER tagger
which is part of the flock of CALBC taggers.5 We
count and aggregate these votes such that each en-
tity type has an associated type count value.
We then compute the ratio of systems agree-
ing on the same single type assignment and call
this the confidence attributed to a particular type
for some token. The confidence value will sub-
sequently be interpreted against the confidence
threshold [0, 1] that defines a measure of certainty
a type assignment should have in order to be ac-
cepted as consensual.
2.1.2 Cosine-based Similarity of Phrasal
Entity Mentions
As the above policy of token-wise annotation de-
couples contiguous entity mentions spanning over
more than one token, we also want to restitute this
phrasal structure. This is achieved by constructing
contiguous sequences of tokens that characterize a
phrasal entity mention at the text level to which the
same type label has been assigned. Since differ-
ent taggers tend to identify different spans of text
for the same entity type (as shown in the exam-
ple from Section 1) we have to account for similar
phrasal forms of named entity mentions.
This is achieved by constructing vectors which
represent entity mentions and by computing the
cosine between the different entity mention vec-
tors. Let E1 = T1T2T3 be an entity mention com-
prised of three tokens T1 to T3. Let E2 = T2T3 be
5Due to time constraints when we performed our experi-
ments, we make an extremely simplifying assumption: From
the whole range of possible entity types NER taggers may as-
sign to some token (cf. (Bodenreider and McCray, 2003)) we
have chosen the PRotein/GEne group for testing. Still, this
assumption does not do harm to the core of our hypotheses.
See also our discussion in Section 5.
an entity mention overlapping with E1 in the to-
kens T2 and T3. To decide whether E1 and E2 are
considered similar, we first construct two vectors
representing the entity mentions:
v(E1) = (f1, f2, f3)T
with fi = IDF (Ti) being the inverse document
frequency of the token Ti. We compute the in-
verse document frequency of tokens based on the
corpus which is subject to analysis. Analogously,
we construct the vector for E2
v(E2) = (0, f2, f3)T
filling in a zero for the IDF of T1 since it is not
covered by E2. The entity mentions E1 and E2
are considered equal or similar, if the cosine of
the two vectors is greater or equal a given thresh-
old, cos(v(E1), v(E2)) ? threshold.6 We then
compute the number of systems considering an en-
tity annotation as similar in the manner described
above. The annotation is accepted and thus en-
tered into the SSC, if a particular number of sys-
tems agree on one annotation. This approach was
previously developed by the CALBC project part-
ners (Rebholz-Schuhmann et al, 2010a).
The number of agreeing systems and the thresh-
old are the free parameters of this method and thus
subject to optimization.
2.2 Optimization of Silver Standard Corpora
In the experiments described in the next section,
we will consider alternative parametrizations for
Silver Standard Corpora, i.e., the required confi-
dence threshold or cosine threshold and the num-
ber of agreeing systems. We will then discuss two
variants for optimizing this collection of SSCs.
The first one directly uses the gold standard for op-
timization. The task will be to find that particular
parameter setting for an SSC which best fits the
data contained in the gold standard. Once these
parameters are determined they can be applied to
the complete CALBC document set (composed of
100,000 documents) to produce the final, quasi-
optimal SSC.
In another variant, we insert a classifier into this
loop. First, we train a classifier on a particular
6For final corpus creation, it must be decided which of the
matching entity mentions is entered into the reference SSC,
e.g. the longest or shortest entity annotation. In our exper-
iments, we always chose the shortest entity mention. How-
ever, preliminary experiments showed that the differences to
taking the longest entity mention were marginal.
237
SSC that is built from a particular parameter com-
bination. Next, this classifier is tested against the
gold standard. This is iterated through all parame-
ter combinations. Obviously, the best performing
classifier relative to the gold standard selects the
optimal SSC.
3 Experimental Setting
3.1 Gold Standard
We generated a new broad-coverage corpus com-
posed of 3,236 MEDLINE abstracts (35,519 sen-
tences or 941,890 tokens) dealing with gene
and protein mentions. Altogether, it comprises
57,889 named entity type annotations annotated
by one expert biologist. We created this new re-
source to have a consistent and (as far as pos-
sible) subdomain-independent protein-annotated
corpus.7
MEDLINE abstracts were annotated with (pro-
tein coding) genes, mRNAs and proteins. A
distinction was made between dedicated proteins
as they are recorded in the protein database
UNIPROT,8 protein complexes consisting of sev-
eral protein subunits (e.g., IL-2 receptor consist-
ing of ?, ?, and ? chain), and protein families or
groups (e.g., ?transcription factors?). Also enu-
merations of proteins and protein variants were an-
notated. Discontinuous annotations were avoided
as well as nested annotations (annotations embed-
ded in other annotations). However, gene/protein
mentions nested in terms other than gene/protein
mentions were annotated (e.g., protein mentions
nested in protein function descriptions such as
?ligase? in ?ligase activity?). Modifiers such as
species designators were excluded from annota-
tions whenever possible. Gene segments or pro-
tein fragments were also not annotated.
For our experiments, we did not distinguish be-
tween the different annotation classes (see Table
1) but merged all available annotations into one
class, viz. PRotein/GEne (PRGE).
3.2 Automatic Annotation of the Gold Standard
We then asked all four sites participating in the
CALBC project to automatically annotate the given
gold standard (made available without gold data,
7We are aware of other gene/protein-annotated corpora
such as PENNBIOIE (http://bioie.ldc.upenn.
edu/) or GENIA (http://www-tsujii.is.s.
u-tokyo.ac.jp/GENIA/home/wiki.cgi) that will
have to be taken into account in future studies as well.
8http://www.uniprot.org/
semantic type description
T028 Gene or Genome
T086 Nucleotide Sequence
T087 Amino Acid Sequence,
Amino Acid, Peptide
T116 Protein
T126 Enzyme
T192 Receptor
Table 1: Semantic types defining the PRGE group
(semantic type codes refer to the UMLS).
of course) using the same type of named entity tag-
ging machinery as was used to annotate CALBC?s
canonical SSC. The performance results of each
group?s system evaluated against the gold standard
are reported in Table 2. The data of each system
constitute the reference data sets and raw data for
all subsequent experiments on the configuration
and optimization of the silver standard.
The resulting raw material does thus not only
contain gene/protein annotations but also any
other entity types as supplied by the partners.
For our experiments on the gold standard, how-
ever, only the entity types subsumed by the PRGE
group (see Table 1) were considered and annota-
tions of all other types were discarded. The def-
inition of the PRGE group is identical to the one
proposed by Rebholz-Schuhmann et al (2010a).
For the experiments, the specific semantic types
(e.g., the UMLS concepts)9 were not considered,
only the semantic group PRGE was.
3.3 Evaluation Metrics
The following metrics were used to evaluate how
good the silver standard(s) fit(s) the provided gold
standard:
? segment-level recall, precision, and F-score
values with exact boundaries, the standard
way to evaluate NER taggers,
? segment-level recall, precision, and F-score,
but with relaxed boundary constraints. This
means that two entity mentions are consid-
ered to match when they overlap with at least
one token and have the same entity type as-
signed to them,
? accuracy measured on the token level.
These metrics can be considered as optimization
criteria.
9http://www.nlm.nih.gov/research/umls/
238
3.4 Tokenization
The CALBC partners? data do not necessarily
come with tokenization information and, more-
over, different partners/systems might have differ-
ent tokenizations. Since a common ground for
comparison is thus lacking we added a new, con-
sistent tokenization based on the JULIE Lab tok-
enizer (Tomanek et al, 2007b). This tokenizer is
optimized for biomedical documents with intrinsic
focus to keep complex biological terminological
units (such as ?IL-2?) unsegmented, but to split
up tokens that are not terminologically connected
(such as dividing ?IL-2-related? up into ?IL-2?,
?-? and ?related?). As a matter of fact, entity
boundaries do not necessarily coincide with token
boundaries. Our solution to this problem is as fol-
lows: Whenever a token partially overlaps with an
entity name, the full form of that token is consid-
ered to be associated with this entity. All data on
which we report here (silver and gold standards)
obey to this tokenization scheme.
3.5 Parameters Being Tested
The following parameter settings were considered
in our experiments:
? Four different values for confidence thresh-
olds indicating that 20% (0.2), 40% (0.4),
60% (0.6) or 80% (0.8) of all taggers agreed
on the same type annotation, viz. PRGE,
? Five different values for cosine thresholds
to identify overlapping entity mentions, viz.
(0.7, 0.8, 0.9, 0.95, 0.975), and two different
values for the number n of agreeing taggers,
viz. n ? 2 and n ? 3,
? Two tagger crowd scenarios, viz. one where
all five systems were involved, the other
where subsets of cardinality 2 of these
crowds were re-combined.10
4 Results
As already described in Section 2.2, we performed
two types of experiments. In the first experiment
(Section 4.1), we intend to find proper calibrations
of parameters for an optimal SSC as described in
Section 3.5. In the second experiment (Section
4.2), we incorporate an extrinsic task, training an
NER classifier on different parameter settings, as
a selector for the optimal SSC.
10We refrained from also testing combinations of 3 and 4
systems due to time constraints.
4.1 Intrinsic Calibration of Parameters
Full Merger of All Taggers. In this scenario,
we tested the merged results of the entire crowd of
CALBC taggers when compared to the gold stan-
dard and determined their performance scores (see
Table 3). We will discuss the results with respect
to the overlapping F-score, if not explicitly stated
otherwise.
Looking at the results of the runs involving dif-
ferent cosine thresholds, we witness a systematic
drawback when more than two systems are re-
quired to agree. Although precision is boosted in
this setting, recall is decreasing strongly which re-
sults in overall lower F-scores. When only two
systems are required to agree a comparatively
higher recall comes at the cost of lower preci-
sion. Yet, the F-score (both under exact as well
as overlap conditions) is always superior (ranging
between 75% and 73%) when compared to the 3-
agreement scenario. Note that the 2-agreement
condition for the highest threshold being tested
yields, without exception, better scores than the
best single system (cf. Table 2).
The best performing run in terms of F-score for
the confidence method results from a threshold of
0.2 with an F-score of 76%. Note that this F-
score lies 4 percentage points above the best per-
formance of a single system (cf. Table 2).
A threshold of 0.2 with five contributing sys-
tems results in a union of all annotations. Conse-
quently, this run benefits from a high recall com-
pared with the other runs. However, the run ex-
hibits the lowest precision rating (both for the ex-
act and overlap condition), which is due to the low
threshold being chosen. As can also be seen with
the confidence method at a threshold of 0.80, a
very high precision can be reached (99%) but at
the cost of extremely low recall.11 The methods
performing best in terms of overlapping F-score
also perform best in terms of exact F-score.
Selected Tagger Combinations: Twin Taggers.
In this scenario, we evaluated all twin combina-
tions of taggers against the gold standard regard-
ing the confidence criterion. In Table 4 we contrast
the two best performing and the two worst per-
forming tagger pairs for the confidence method.
The table reveals that there are some cases where
the taggers seem to complement each other, e.g.,
the twins SYS-1 and SYS-3, as well as SYS-3 and
11Exactly these kinds of alternatives offer flexibility for
choosing the most appropriate SSC given a specific task.
239
exactR exactP exactF overlapR overlapP overlapF systems
0.55 0.74 0.63 0.63 0.84 0.72 SYS-1
0.36 0.53 0.43 0.46 0.68 0.55 SYS-2
0.48 0.77 0.59 0.59 0.95 0.72 SYS-3
0.44 0.83 0.58 0.49 0.91 0.64 SYS-4
0.34 0.61 0.44 0.41 0.74 0.53 SYS-5
Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (best
performance scores in bold face). Measurements are taken both for exact as well as overlapping recall
(R), precision (P) and F-score (F).
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.53 0.71 0.61 0.66 0.87 0.75 0.70 2.00
cosine 0.93 0.40 0.79 0.53 0.49 0.96 0.65 0.70 3.00
cosine 0.94 0.54 0.71 0.61 0.65 0.87 0.74 0.80 2.00
cosine 0.93 0.41 0.80 0.54 0.48 0.95 0.64 0.80 3.00
cosine 0.94 0.54 0.72 0.62 0.65 0.86 0.74 0.90 2.00
cosine 0.93 0.41 0.81 0.54 0.48 0.95 0.64 0.90 3.00
cosine 0.94 0.54 0.73 0.62 0.64 0.86 0.74 0.95 2.00
cosine 0.93 0.41 0.83 0.55 0.47 0.95 0.63 0.95 3.00
cosine 0.94 0.55 0.75 0.64 0.64 0.86 0.73 0.97 2.00
cosine 0.93 0.42 0.85 0.56 0.47 0.95 0.63 0.97 3.00
confidence 0.95 0.58 0.73 0.65 0.68 0.85 0.76 0.20
confidence 0.94 0.44 0.83 0.58 0.50 0.94 0.66 0.40
confidence 0.93 0.32 0.88 0.47 0.35 0.97 0.52 0.60
confidence 0.91 0.16 0.91 0.27 0.17 0.99 0.30 0.80
Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param-
eter setting in bold face). Parameters: threshold (confidence or cosine) and number of agreeing systems
(agr. systems).
SYS-4. In both cases, a confidence threshold of
0.2 yields the best F-score. Additionally, these F-
scores (81% and 78%) are even higher than the
single system?s F-scores (+9% up to +14%). This
comes with a significant increase in recall over
both systems (+13% to +28%) though at the cost
of lowered precision relative to the system with
the higher precision (?1% to ?10%). These re-
sults also outperform the best results of the exper-
imental runs where all systems were involved (see
Table 3). This indicates that a subset of all systems
might yield a better SSC than a combination of all
systems? outputs.
4.2 Extrinsic Calibration of Parameters
We employed a standard named entity tagger to as-
sess the impact of the different merging strategies
on a scenario near to a real-world application.12
12This tagger is based on Conditional Random Fields (Laf-
ferty et al, 2001) and employs a standard feature set used for
Each SSC variant (and thus each parameter com-
bination) was evaluated with this tagger in a 10-
fold cross validation. The SSC and the gold corpus
were split into ten parts of equal size. Nine parts of
the SSC constituted the training data of one cross
validation round, the corresponding tenth part of
the gold standard was used for evaluation. This
way, we tested how adequate a merged corpus was
with respect to the training of a classifier. Because
the cross validation has been very time consum-
ing, we did not consider specific combinations of
systems but always merged the annotations of all
five systems. The results are displayed in Table 5.
Interestingly, the highest recall, precision, and
F-score values (both for the exact and overlap con-
dition) are shared by the same parameter combi-
nations which also performed best in Section 4.1.
Hence, the use of a named entity tagger supports
the evaluation results when comparing the various
biomedical entity recognition (Settles, 2004).
240
ACC exactR exactP exactF overlapR overlapP overlapF systems threshold
0.95 0.62 0.69 0.65 0.76 0.85 0.81 SYS-1 + SYS-3 0.20
0.92 0.22 0.69 0.34 0.26 0.81 0.39 SYS-2 + SYS-5 0.60
0.95 0.55 0.75 0.63 0.67 0.91 0.78 SYS-3 + SYS-4 0.20
0.92 0.30 0.85 0.45 0.34 0.94 0.50 SYS-4 + SYS-5 0.60
Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs
obtained by the confidence method.
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.46 0.69 0.56 0.58 0.86 0.69 0.70 2.00
cosine 0.93 0.32 0.77 0.45 0.39 0.94 0.55 0.70 3.00
cosine 0.94 0.46 0.69 0.56 0.57 0.86 0.69 0.80 2.00
cosine 0.93 0.32 0.78 0.46 0.39 0.94 0.55 0.80 3.00
cosine 0.94 0.46 0.70 0.56 0.57 0.85 0.68 0.90 2.00
cosine 0.93 0.32 0.79 0.46 0.38 0.93 0.54 0.90 3.00
cosine 0.94 0.47 0.71 0.56 0.56 0.85 0.68 0.95 2.00
cosine 0.93 0.33 0.80 0.47 0.38 0.93 0.54 0.95 3.00
cosine 0.94 0.47 0.73 0.57 0.56 0.85 0.67 0.97 2.00
cosine 0.93 0.33 0.82 0.47 0.38 0.93 0.54 0.97 3.00
confidence 0.94 0.50 0.72 0.59 0.60 0.85 0.70 0.20
confidence 0.93 0.36 0.82 0.50 0.41 0.93 0.56 0.40
confidence 0.92 0.25 0.87 0.39 0.28 0.95 0.43 0.60
confidence 0.91 0.12 0.89 0.20 0.12 0.96 0.22 0.80
Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.
Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).
SSCs directly to the gold standard corpus. How-
ever, this result may be due to our particular exper-
imental setting and should not be taken as a gen-
eral rule. Instead, this issue should be studied on
additional gold standard corpora (cf. Section 5).
5 Discussion and Conclusions
The experiments reported in this paper strengthen
the empirical basis of the novel idea of a silver
standard corpus (SSC). While the originators of
the SSC have come up with a fixed SSC, our ex-
periments show that different parametrizations of
SSCs allow to dynamically configure or select an
optimal one given a gold standard for comparison
during this optimization.
Our experimental data reveals that the boosting
hypothesis (the combination of several classifiers
outperforms weaker single ones in terms of perfor-
mance) is confirmed for complete mergers as well
as selected twin pairs of taggers. We also have
evidence that boosting within the SSC paradigm
tends to increase precision whereas it seems to de-
crease recall. This general observation becomes
stronger and stronger when the size of the commit-
tees (i.e., the number of submitting classifiers) in-
creases. It is also particularly interesting that both
the intrinsic evaluation (groups of classifiers vs.
gold standard), as well as the extrinsic evaluation
of SSCs (groups of classifiers trained and tested on
mutually exclusive partitions of the gold standard)
reveal parallel patterns in terms of performance ?
this indicates a surprising level of stability of the
entire SSC approach.
In our view, the strongest finding from our ex-
periments is the possibility to calibrate an SSC ac-
cording to requirements derived from the goal of
annotation campaigns. In particular, one can adapt
parameters to a specific use case, e.g., building a
corpus with high precision when compared to the
gold standard. Through the evaluation of the pa-
rameter space, one can assess the costs of reach-
ing a specific goal. For instance, a precision of
99% can be reached, yet at the cost of the F-score
plunging to 30%; only slightly lowering the preci-
sion to 97% boosts the F-score by 22 points (see
last two rows in Table 3).
241
Also, when increasingly more annotation sets
become available (e.g., through the CALBC chal-
lenges) the problem of adversarial or extremely
bad performing systems is no longer a pressing is-
sue since with the optimization approach such sys-
tems are automatically sorted out when optimizing
over the set of possible system combinations.
While our experiments are but a first step to-
wards the consolidation of the SSC paradigm
some obvious limitations of our work have to be
overcome:
? experiments with different gold standards
have to be run as one might hypothesize that
different gold standards require different pa-
rameter settings for the optimal SSC,
? experiments with different NER taggers have
to be run (e.g., we plan to use an NER tag-
ger which prefers recall over precision, while
the one used for these experiments generally
yields higher precision than recall scores),
? test with crowds of taggers which generate
higher recall than precision.13
In our approach, a gold standard is needed to
find good parameters to build an SSC. A ques-
tion not addressed so far is how huge such a gold
standard must be to offer an appropriate size for
the optimization step. Finally, it might be particu-
larly rewarding to join efforts in reducing the de-
velopment costs for such a gold standards ? Active
Learning (e.g., Tomanek et al (2007a)) might be
one promising approach to break this bottleneck.
Since effective calibration of SSCs is in need of
reasonably sized and densely populated gold stan-
dards, by combining these lines of research we
claim that additional benefits for SSCs become vi-
able.
6 Acknowledgments
We wish to thank Kerstin Hornbostel for stim-
ulating and corrective remarks on the biological
grounding of this investigation. This research was
partially funded by the EC?s 7th Framework Pro-
gramme within the CALBC project (FP7-231727)
and the GERONTOSYS research initiative from the
13We used a gold standard in which some unusual entities
(e.g., protein families) had been annotated for which most
named entity taggers have not been trained. This might also
explain the generally overall low recall among the crowd of
taggers yielded in our experiments.
German Federal Ministry of Education and Re-
search (BMBF) under grant 0315581D within the
JENAGE project.
References
Olivier Bodenreider and Alexa T. McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36(6):414?432.
Yoav Freund. 1990. Boosting a weak learning algo-
rithm by majority. In COLT?90 ? Proceedings of the
3rd Annual Workshop on Computational Learning
Theory, pages 202?216.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik van Mulligen, Ning Kang, Jan Kors,
David Milward, Peter Corbett, Ekaterina Buyko,
Elena Beisswanger, and Udo Hahn. 2010a. CALBC
Silver Standard Corpus. Journal of Bioinformatics
and Computational Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik M. van Mulligen, Ning Kang, Jan
Kors, Peter Milward, David Corbett, Ekaterina
Buyko, Katrin Tomanek, Elena Beisswanger, and
Udo Hahn. 2010b. The CALBC Silver Standard
Corpus for biomedical named entities: A study in
harmonizing the contributions from four indepen-
dent named entity taggers. In LREC 2010 ? Pro-
ceedings of the 7th International Conference on
Language Resources and Evaluation.
Burr Settles. 2004. Biomedical named entity recog-
nition using conditional random fields and rich fea-
ture sets. In NLPBA/BioNLP 2004 ? COLING
2004 International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions, pages 107?110.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007a. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In EMNLP-CoNLL?07
? Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Language Learning, pages 486?
495.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007b. A reappraisal of sentence and token splitting
for life sciences documents. In K. A. Kuhn, J. R.
Warren, and T. Y. Leong, editors, MEDINFO?07 ?
Proceedings of the 12th World Congress on Medical
Informatics, number 129 in Studies in Health Tech-
nology and Informatics, pages 524?528. IOS Press.
242
