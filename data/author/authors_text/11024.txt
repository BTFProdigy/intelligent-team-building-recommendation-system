Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
Hacking Wikipedia for Hyponymy Relation Acquisition
Asuka Sumida Kentaro Torisawa
Japan Advanced Institute of Science and Technology
1-1 Asahidai, Nomi-shi, Ishikawa-ken, 923-1211 JAPAN
{a-sumida,torisawa}@jaist.ac.jp
Abstract
This paper describes a method for extract-
ing a large set of hyponymy relations from
Wikipedia. The Wikipedia is much more con-
sistently structured than generic HTML doc-
uments, and we can extract a large number of
hyponymy relations with simple methods. In
this work, we managed to extract more than
1.4 ? 10
6 hyponymy relations with 75.3%
precision from the Japanese version of the
Wikipedia. To the best of our knowledge, this
is the largest machine-readable thesaurus for
Japanese. The main contribution of this paper
is a method for hyponymy acquisition from
hierarchical layouts in Wikipedia. By us-
ing a machine learning technique and pattern
matching, we were able to extract more than
6.3 ? 10
5 relations from hierarchical layouts
in the Japanese Wikipedia, and their precision
was 76.4%. The remaining hyponymy rela-
tions were acquired by existing methods for
extracting relations from definition sentences
and category pages. This means that extrac-
tion from the hierarchical layouts almost dou-
bled the number of relations extracted.
1 Introduction
The goal of this study has been to automatically ex-
tract a large set of hyponymy relations, which play a
critical role in many NLP applications, such as Q&A
systems (Fleischman et al, 2003). In this paper, hy-
ponymy relation is defined as a relation between a hy-
pernym and a hyponym when ?the hyponym is a (kind
of) hypernym.?1.
1This is a slightly modified definition of the one in (Miller
et al, 1990). Linguistic literature, e.g. (A.Cruse, 1998), dis-
tinguishes hyponymy relations, such as ?national university? and
?university?, and concept-instance relations, such as ?Tokyo Uni-
versity? and ?university?. However, we regard concept-instance
Currently, most useful sources of hyponymy re-
lations are hand-crafted thesauri, such as WordNet
(Fellbaum, 1998). Such thesauri are highly reliable,
but their coverage is not large and the costs of ex-
tension and maintenance is prohibitively high. To re-
duce these costs, many methods have been proposed
for automatically building thesauri (Hearst, 1992; Et-
zioni et al, 2005; Shinzato and Torisawa, 2004; Pan-
tel and Pennacchiotti, 2006). But often these meth-
ods need a huge amount of documents and compu-
tational resources to obtain a reasonable number of
hyponymy relations, and we still do not have a the-
saurus with sufficient coverage.
In this paper, we attempt to extract a large num-
ber of hyponymy relations without a large document
collection or great computational power. The key
idea is to focus on Wikipedia2, which is much more
consistently organized than normal documents. Ac-
tually, some studies have already attempted to ex-
tract hyponymy relations or semantic classifications
from Wikipedia. Hyponymy relations were extracted
from definition sentences (Herbelot and Copestake,
2006; Kazama and Torisawa, 2007). Disambiguation
of named entities was also attempted (Bunescu and
Pasca, 2006). Category pages were used to extract
semantic relations (Suchanek et al, 2007). Lexical
patterns for semantic relations were learned (Ruiz-
Casado et al, 2005).
The difference between our work and these at-
tempts is that we focus on the hierarchical layout of
normal articles in Wikipedia. For instance, the ar-
ticle titled ?Penguin? is shown in Fig. 1(b). This
article has a quite consistently organized hierarchi-
cal structure. The whole article is divided into the
sections ?Anatomy?, ?Mating habits?, ?Systematics
and evolution?, ?Penguins in popular culture? and so
on. The section ?Systematics and evolution? has the
relations as a part of hyponymy relations in this paper because we
think the distinction is not crucial for many NLP applications.
2http://ja.wikipedia.org/wiki
883
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s.
== Anatomy ==
== Mating habits ==
==Systematics and evolution==
===Systematics=== 
* Aptenodytes
**[[Emperor Penguin]]
** [[King Penguin]]
* Eudyptes
== Penguins in popular culture == 
== Book ==
* Penguins
* Penguins of the World  
== Notes ==
* Penguinone
* the [[Penguin missile]]
[[Category:Penguins]]
[[Category:Birds]]
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
(a) The source code of the ar-
ticle ?Penguin? in ?Wikipedia
Penguin
(b) The example of the arti-
cle ?Penguin? in Wikipedia
Penguins
Anatomy
Mating_habits Systematics_and_evolution
Systematics
Aptenodytes
Emperor_PenguinKing_Penguin
Eudyptes
Penguins_in_
popular culture
Book
Penguins Penguins_of_
the_World
Notes
Penguinone the_Penguin
_missile
(c) The displayed page of the article ?Penguin? in Wikipedia
Figure 1: The example of a Wikipedia article
subsection ?Systematics?, which is further divided to
?Aptenodytes?, ?Eudyptes? and so on. Some of such
section-subsection relations can be regarded as valid
hyponymy relations. In the article about ?Penguin?,
relations such as the one between ?Aptenodytes? and
?Emperor Penguin? and the one between ?Book? and
?Penguins of the World? are valid hyponymy rela-
tions. The main objective of this work is to develop a
method to extract only such hyponymy relations.
The rest of the paper is organized as follows. We
first explain the structure of Wikipedia in Section 2.
Next, we introduce our method in Section 3. Some
alternative methods are presented in Section 4. We
then show the experimental results in Section 5.
2 The Structure of Wikipedia
The Wikipedia is built on the MediaWiki software
package3. MediaWiki interprets the source code
written in the MediaWiki syntax to produce human-
readable web pages. For example, Fig. 1(b) is a result
of interpreting the source code in Fig. 1(a). An impor-
tant point is that the MediaWiki syntax is stricter than
the HTML syntax and usage of the syntax in most
Wikipedia articles are constrained by editorial policy.
This makes it easier to extract information from the
Wikipedia than from generic HTML documents.
3http://www.mediawiki.org/wiki/MediaWiki
Usually, a Wikipedia article starts with a definition
sentence, such as ?Penguins are a group of aquatic,
flightless birds? in Fig. 1(a). Then, the hierarchical
structure marked in the following manner follows.
Headings Headings describe the subject of a para-
graph. See line 2-5, 10-11, 14 of Fig. 1(a).
Headings are marked up as ?=+title=+? in the
MediaWiki syntax, where title is a subject of
the paragraph. Note that ?+? here means a fi-
nite number of repetition of symbols. ?=+sec-
tion=+? means that ?=section=?, ?==section==?
and ?===section===? are legitimate mark up in
the Wikipedia syntax. We use this ?+? notation
in the following explanation as well.
Bulleted lists Bulleted lists are lists of unordered
items. See line 6-9, 12-13, 15-16 of Fig. 1. Bul-
leted lists are marked as ?*+title? in the Medi-
aWiki syntax, where title is a subject of a listed
item.
Ordered lists Ordered lists are lists of numbered
items. Ordered lists are marked up as ?#+title?
in MediaWiki syntax, where title is a subject of
a numbered item.
Definition lists Definition lists contain terms and its
definitions. Our method focuses only on the
terms. Definition lists are marked as ?;title?
where title is a term.
The basic hierarchical structure of aWikipedia arti-
cle is organized by a pre-determined ordering among
the above items. For instance, a bulleted list item
is assumed to occupy a lower position in the hierar-
chy than a heading item. In general, items occupy
a higher position in the order of headings, definition
lists, bulleted lists, and ordered lists. In addition, re-
call that headings, bullet list and ordered list allowed
the repetitions of symbols ?=?, ?*? and ?#?. The
number of repetition indicates the position in the hi-
erarchy and the more repetition the item contains, the
lower the position occupied by the item becomes. For
instance, ?==Systematics and evolution==? occupies
a higher position than ?===Systematics===? as illus-
trated in Fig. 1(a) (b).
Then, it is easy to extract a hierarchical structure
based on the order among the mark-up items by pars-
ing the source code of an article. Fig. 1(c) illustrates
the hierarchical structure extracted from the source
code in Fig. 1(a).
884
3 Proposed Method
This section describes our method for extracting
hyponymy relations from hierarchical structures in
Wikipedia articles. The method consists of three
steps:
Step 1 Extract hyponymy relation candidates from
hierarchical structures in the Wikipedia.
Step 2 Select proper hyponymy relations by apply-
ing simple patterns to the extracted candidates.
Step 3 Select proper hyponymy relations from the
candidates by using a machine learning tech-
nique.
Each step is described below.
3.1 Step 1: Extracting Relation Candidates
The Step 1 procedure extracts the title of a marked-up
item and a title of its (direct) subordinate marked-up
item as a hyponymy relation for each marked-up item.
For example, given the hierarchy in Fig. 1(c), the
Step1 procedure extracted hyponymy relation can-
didates such as ?Aptenodytes/Emperor Penguin?and
?Book/Penguins of the World?. (Note that we de-
note hyponymy relations or their candidates as ?hy-
pernym/hyponym? throughout this paper.) However,
these relation candidates include many wrong hy-
ponymy relations such as ?Penguins in popular cul-
ture/Book?. Steps 2 and 3 select proper relations from
the output of the Step 1 procedure.
3.2 Step 2: Selecting Hyponymy Relations by
Simple Patterns
Step 2 selects plausible hyponymy relations by ap-
plying simple patterns to hyponymy relation can-
didates obtained in Step 1. This is based on our
observation that if a hypernym candidate matches
a particular pattern, it is likely to constitute a cor-
rect relation. For example, in Japanese, if a hy-
pernym candidate is ? omona X (Popular or typ-
ical X)?, X is likely to be a correct hypernym of
the hyponym candidates that followed it in the arti-
cle. Fig.2 shows a Japanese Wikipedia article about
a zoo that includes ?omona doubutsu (Popular
animals)?, ? Mazeran Pengin (Magellanic Pen-
guin)?, ?Raion (Lion)? and so on. From this ar-
ticle, the Step 1 procedure extracts a hyponymy re-
lation candidate ?Popular Animals/Magellanic Pen-
guin?, and the Step 2 procedure extracts ?Ani-
mals/Magellanic Penguin? after matching ?Popular?
Magellanic Penguin
Lion
Hokkaido Brown Bear
Popular animals
Figure 2: Example for Step2
Xno ichiran(list of X), Xichiran(list of
X), Xsyousai(details of X), Xrisuto(X list),
daihyoutekinaX(typical X), daihyouX(typical X),
syuyounaX(popular or typical X), omonaX(popular
or typical X), syuyouX(popular or typical X),
kihontekinaX(basic X), kihon(basic X),
chomeinaX(notable X), ookinaX(large X),
omonaX(popular or typical X), ta noX(other X),
ichibuX(partial list of X)
Figure 3: Patterns for Step 2
to the hypernym candidate and removing the string
?Popular? from the candidate. Fig. 3 lists all the pat-
terns we used. Note that the non-variable part of the
patterns is removed from the matched hypernym can-
didates.
3.3 Step 3: Selecting Proper Hyponymy
Relations by Machine Learning
The Step 3 procedure selects proper hyponymy rela-
tions from the relation candidates that do not match
the patterns in Step 2. We use Support Vector Ma-
chines (SVM) (Vapnik, 1998) for this task. For each
hyponymy relation candidate, we firstly apply mor-
phological analysis and obtain the following types of
features for each hypernym candidate and hyponym
candidate, and append them into a single feature vec-
tor, which is given to the classifier.
POS We found that POS tags are useful clues for
judging the validity of relations. For instance, if a
hypernym includes proper nouns (and particularly to-
ponyms), it is unlikely to constitute a proper relation.
We assigned each POS tag a unique dimension in the
feature space and if a hypernym/hyponym consists of
a morpheme with a particular POS tag, then the cor-
responding element of the feature vector was set to
one. When hypernyms/hyponyms are multiple mor-
pheme expressions, the feature vectors for every mor-
pheme were simply summed. (The obtained feature
vector works as disjunction of each feature vector.)
An important point is that, since the last morpheme of
hypernyms/hyponyms works as strong evidence for
the validity of relations, the POS tag of the last mor-
pheme was mapped to the dimension that is different
from the POS tags of the other morphemes.
885
MORPH Morphemes themselves are also mapped
to a dimension of the feature vectors. The last
morphemes are also mapped to dimensions that
are different from those of the other morphemes.
This feature is used for recognizing particular mor-
phemes that strongly suggest the validity of hy-
ponymy relations. For instance, if the morpheme
?zoku (genus)? comes in the end of the hyper-
nym, the relation is likely to be valid, as exem-
plified by the relation ?koutei pengin zoku
(Aptenodytes genus)/koutei pengin (Emperor
Penguin)?.
EXP Expressions of hypernym/hyponym candi-
dates themselves also give a good clue for judging
the validity of the relation. For instance, there are
typical strings that can be the title of a marked-up
item but cannot be a proper hypernym or a proper
hyponym. Examples of these strings include ?Back-
ground? and ?Note?. By mapping each expression to
an element in a feature vector and setting the element
to one, we can prevent the candidates containing such
expressions from being selected by the classifier.
ATTR We used this type of features according to
our observation that if a relation candidate includes an
attribute, it is a wrong relation. The attributes of an
object can be defined as ?what we want to know about
the object?. For instance, we regard ?Anatomy? as at-
tributes of creatures in general, and the relation such
as ?Penguin/Anatomy? cannot be regarded as proper
hyponymy relations. To set up this type of features,
we automatically created a set of attributes and the
feature was set to one if the hypernym/hyponym is
included in the set. The attribute set was created in
the following manner. We collected all the titles of
the marked-up items from all the articles, and counted
the occurrences of each title. If a title appears more
than one time, then it was added to the attribute set.
Note that this method relies on the hypothesis that
the same attribute is used in articles about more than
one object (e.g., ?Penguin? and ?Sparrow? ) belong-
ing to the same class (e.g., ?animal?). (Actually, in
this counting of titles, we excluded the titles of items
in the bulleted lists and the ordered lists in the bottom
layer of the hierarchical structures. This is because
these items are likely to constitute valid hyponymy
relations. We also excluded that match the patterns
in Fig. 3.) As a result, we obtained the set of 40,733
attributes and the precision of a set was 73% accord-
ing to the characterization of attributes in (Tokunaga
et al, 2005).
LAYER We found that if a hyponymy relation is
extracted from the bottom of the hierarchy, it tends
to be a correct relation. For example, in Fig. 1(c),
the hyponymy relation ?Penguin/Anatomy? which is
extracted from the top of hierarchy is wrong, but the
hyponymy relation ?Aptenodytes/Emperor Penguin ?
which is extracted from the bottom of the layer is cor-
rect. To capture this tendency, we added the mark that
marks up a hypernym and a hyponym to the features.
Each mark is mapped to a dimension in the feature
vector, and the corresponding element was set to one
if a hypernym/hyponym candidate appears with the
mark.
As the final output of our method, we merged the
results of Steps 2 and 3.
4 Alternative Methods
This section describes existing methods for acquiring
hyponymy relations from theWikipedia. We compare
the results of these methods with the output of our
method in the next section.
4.1 Extraction from Definition Sentences
Definition sentences in the Wikipedia article were
used for acquiring hyponymy relations by (Kazama
and Torisawa, 2007) for named entity recognition.
Their method is developed for the English version of
the Wikipedia and required some modifications to the
Japanese version. These modification was inspired by
Tsurumaru?s method (Tsurumaru et al, 1986).
Basically, definition sentences have forms similar
to ?hyponym word wa hypernym word no isshu de
aru(hyponym is a kind of hypernym)? in dictionaries
in general, and contain hyponymy relations in them.
In the Wikipedia, such sentences usually come just
after the titles of articles, so it is quite easy to recog-
nize them. To extract hyponymy relations from def-
inition sentences, we manually prepared 1,334 pat-
terns, which are exemplified in Table 4, and applied
them to the first sentence.
4.2 Extraction from Category Pages
Suchanek et al (Suchanek et al, 2007) extracted
hyponymy relations from the category pages in the
Wikipedia using WordNet information. Although we
cannot use WordNet because there is no Japanese
version of WordNet, we can apply their idea to the
Wikipedia only.
The basic idea is to regard the pairs of the category
name provided in the top of a category page and the
886
hyponym wa.*hypernym no hitotsu.
(hyponym is one of hypernym)
hyponym wa .*hypernym no daihyoutekina mono dearu.
(hyponym is a typical hypernym)
hyponym wa.*hypernym no uchi no hitotsu.
(hyponym is one of hypernym)
Note that hyponym and hypernym match only with
NPs.
Figure 4: Examples of patterns for definition sen-
tences
items listed in the page as hyponymy relation.
Thus, the method is quite simple. But the relations
extracted by this are not limited to hyponymy rela-
tions, unfortunately. For instance, the category page
?football? includes ?football team?. Such loosely as-
sociated relations are harmful for obtaining precise
relations. Suchanek used WordNet to prevent such re-
lations from being included in the output. However,
we could not develop such a method because of the
lack of a Japanese WordNet.
5 Experiments
For evaluating our method, we used the Japanese
version of Wikipedia from March 2007, which in-
cludes 820,074 pages4. Then, we removed ?user
pages?,?special pages?, ?template pages?, ?redirec-
tion pages?, and ?category pages? from it.
In Step 3, we used TinySVM5 with polynomial ker-
nel of degree 2 as a classifier. From the relation can-
didates given to the Step 3 procedure, we randomly
picked up 2,000 relations as a training set, and 1,000
relations as a development set. We also used the mor-
phological analyzer MeCab 6 in Step 3.
Table 1 summarizes the performance of our
method. Each row of the table shows A) the pre-
cision of the hyponymy relations, B) the number of
the relations, and C) the expected number of correct
relations estimated from the precision and the num-
ber of the extracted relations, after each step of the
procedure. Note that Step 2? indicates the hyponymy
relation candidates that did not match the pattern in
Fig.3 and that were given to the Step 3 procedure.
The difference between Step 2? and Step 3 indicates
the effect of our classifier. Step 2&3 is the final result
obtained by merging the results of Step 2 and Step 3.
As the final output, we obtained more than 6.3 ? 105
4This pages include ?incomplete pages? that are not counted
in the number of pages presented in the top page of the
Wikipedia.
5http://chasen.org/ taku/software/TinySVM/index.html
6http://mecab.sourceforge.net
Table 1: Performance of each step
Precision # of rels. estimated # of
correct rels.
Step 1 44% 2,768,856 1,218,296
Step 2 71.5% 221,605 158,447
Step 2? 40.0% 2,557,872 1,023,148
Step 3 78.1% 416,858 325,670
Step 2 & 3 76.4% 633,122 484,117
aatisuto / erubisu puresurii
Artist / Elvis Presley
sakura / someiyoshino
Cherry Blossom / Yoshino Cherry
heiya / nakagawa heiya
Plain / Nakagawa Plain
ikou oyobi kenzoubutsu / tsuki no piramiddo
Ruins and buildings / the Pyramid of the Moon
suponsaa / genzai?
Sponsors / Present?
shutsuen sakuhin / taidan go?
Art work / After leaving a group?
?*? indicates an incorrectly recognized relation.
Figure 5: Examples of acquired hyponymy relations
relations and their precision was 76.4%. Note that
the precision was measured by checking 200 random
samples for each step except for Step 3 and Step 2&3,
for which the precision was obtained in a way de-
scribed later. Note that all the numbers were obtained
after removing duplicates in the relations. Example
of the relations recognized by Step 2 or Step 3 are
shown in Fig. 5.
Table 2 shows the effect of each type of features in
Step 3. Each row indicates the precision, recall and
F-measure against 400 samples that are randomly se-
lected from the relation candidates given to Step 3,
when we removed a type of features from feature vec-
tor and when we used all the types. (The 400 sam-
ples included 142 valid relations.) We can see that all
types except for LAYER contributed to an improve-
ment of the F-measure. When the LAYER features
were removed, the F-measure was improved to 1.1
but the precision was on an unacceptable level (55%)
and cannot be used in actual acquisition.
Table 3 summarizes the statistics of all the methods
for acquisition from Wikipedia. It shows A) the pre-
Table 2: Effect of each features in Step3
Feature Type a Precision Recall F-measure
-POS 60.0% 57.0% 58.4
-MORPH 85.0% 47.8% 61.2
-EXP 82.2% 35.9% 50.0
-ATTR 79.7% 47.1% 59.2
-LAYER 55.0% 76.7% 64.1
ALL 78.1% 52.8% 63.0
887
Table 3: The result for extracting hyponymy relations
from definition sentences, category structures,and hi-
erarchy structures
# of # of correct
Precision rels. rels.
Hierarchy (Proposed) 76.4 % 633,122 484,117
Definition snts 77.5% 220,892 171,191
Category 70.5% 596,463 420,506
Total 75.3% 1,426,861 1,075,814
cision of the relations (200 random samples), B) the
number of relations, and C) the expected number of
correct relations estimated from the precision and the
number of extracted relations. We obtained 1.4? 106
hyponymy relations without duplication in total with
75.3% precision from definition sentences, category
structures, and hierarchical structures. They covered
6.6 ? 10
5 distinct hyponyms and 1.0 ? 105 distinct
hypernyms. Note that the number of duplicated rela-
tions in these results was just 23,616. This suggests
that we could extract different types of hyponymy re-
lations from each of these methods.
6 Conclusion
This paper described a method for extracting a large
set of hyponymy relations from the hierarchical struc-
tures of articles in Wikipedia. We could extract
633,122 relations from hierarchical layouts in the
Japanese Wikipedia and their precision was 76.4%.
Combining with existing methods that extract rela-
tions from definition sentences and category struc-
tures, we were able to extract 1,426,861 relations with
75.3% precision in total without duplication. To the
best of our knowledge, this is the largest machine-
readable thesaurus for Japanese available.
References
D. A.Cruse. 1998. Lexical Semantics. Cambridge Text-
books in Linguistics.
Razvan C. Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the EACL,
pages 9?16.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artif. Intell., 165(1):91?
134.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: Answering questions before they are asked.
In ACL2003, pages 1?7.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th International Conference on Computational Lin-
guistics, pages 539?545.
Aurelie Herbelot and Ann Copestake. 2006. Acquiring
ontological relationships from wikipedia using rmrs. In
Proceedings of the ISWC 2006 Workshop on Web Con-
tent Mining with Human Language Technologies.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 698?707.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduc-
tion to wordnet: An on-line lexical database. In Journal
of Lexicography, pages 235?244.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvesting
semantic relations. In ACL ?06 : Proceedings of the 21st
International Conference on Computational Linguistics
and the 44th annual meeting of the ACL, pages 113?
120.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic extraction of semantic rela-
tionships for wordnet by means of pattern learning from
wikipedia. In NLDB, pages 67?79.
Keiji Shinzato and Kentaro Torisawa. 2004. Acquiring
hyponymy relations from web documents. In HLT-
NAACL ?04 : Proceedings of Human Language Tech-
nology Conference/North American chapter of the As-
sociation for Computational Linguistics annual meet-
ing, pages 73?80.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge unifying wordnet and wikipedia. In WWW ?07 :
Proceedings of the 16th International World Wide Web
Conference.
Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Tori-
sawa. 2005. Automatic discovery of attribute words
fromweb documents. In IJCNLP 2005, pages 106?118.
Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida. 1986.
An attempt to automatic thesaurus construction from
an ordinary japanese language dictionary. In Proceed-
ings of the 11th conference on Computational linguis-
tics, pages 445?447.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
888
