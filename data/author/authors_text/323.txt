Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 160?167,
New York, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Parsing using Bayes Point Machines
Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052
simonco@microsoft.com
Anthony Aue
Microsoft Research
One Microsoft Way
Redmond, WA 98052
anthaue@microsoft.com
Kevin Duh
Dept. of Electrical Eng.
Univ. of Washington
Seattle, WA 98195
duh@ee.washington.edu
Eric Ringger
Computer Science Dept.
Brigham Young Univ.
Provo, UT 84602
ringger@cs.byu.edu
Abstract
We develop dependency parsers for Ara-
bic, English, Chinese, and Czech using
Bayes Point Machines, a training algo-
rithm which is as easy to implement as
the perceptron yet competitive with large
margin methods. We achieve results com-
parable to state-of-the-art in English and
Czech, and report the first directed depen-
dency parsing accuracies for Arabic and
Chinese. Given the multilingual nature of
our experiments, we discuss some issues
regarding the comparison of dependency
parsers for different languages.
1 Introduction
Dependency parsing is an alternative to constituency
analysis with a venerable tradition going back at
least two millenia. The last century has seen at-
tempts to formalize dependency parsing, particu-
larly in the Prague School approach to linguistics
(Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words directly
modify other words. Unlike constituency analysis,
there are no intervening non-lexical nodes. We use
the terms child and parent to denote the dependent
term and the governing term respectively.
Parsing has many potential applications, rang-
ing from question answering and information re-
trieval to grammar checking. Our intended ap-
plication is machine translation in the Microsoft
Research Treelet Translation System (Quirk et al,
2005; Menezes and Quirk, 2005). This system ex-
pects an analysis of the source language in which
words are related by directed, unlabeled dependen-
cies. For the purposes of developing machine trans-
lation for several language pairs, we are interested in
dependency analyses for multiple languages.
The contributions of this paper are two-fold: First,
we present a training algorithm called Bayes Point
Machines (Herbrich et al, 2001; Harrington et al,
2003), which is as easy to implement as the per-
ceptron, yet competitive with large margin meth-
ods. This algorithm has implications for anyone
interested in implementing discriminative training
methods for any application. Second, we develop
parsers for English, Chinese, Czech, and Arabic and
probe some linguistic questions regarding depen-
dency analyses in different languages. To the best of
our knowledge, the Arabic and Chinese results are
the first reported results to date for directed depen-
dencies. In the following, we first describe the data
(Section 2) and the basic parser architecture (Section
3). Section 4 introduces the Bayes Point Machine
while Section 5 describes the features for each lan-
guage. We conclude with experimental results and
discussions in Sections 6 and 7.
2 Data
We utilize publicly available resources in Arabic,
Chinese, Czech, and English for training our depen-
dency parsers.
For Czech we used the Prague Dependency Tree-
bank version 1.0 (LDC2001T10). This is a corpus
of approximately 1.6 million words. We divided
the data into the standard splits for training, devel-
160
opment test and blind test. The Prague Czech De-
pendency Treebank is provided with human-edited
and automatically-assigned morphological informa-
tion, including part-of-speech labels. Training and
evaluation was performed using the automatically-
assigned labels.
For Arabic we used the Prague Arabic De-
pendency Treebank version 1.0 (LDC2004T23).
Since there is no standard split of the data into
training and test sections, we made an approxi-
mate 70%/15%/15% split for training/development
test/blind test by sampling whole files. The Ara-
bic Dependency Treebank is considerably smaller
than that used for the other languages, with approx-
imately 117,000 tokens annotated for morphologi-
cal and syntactic relations. The relatively small size
of this corpus, combined with the morphological
complexity of Arabic and the heterogeneity of the
corpus (it is drawn from five different newspapers
across a three-year time period) is reflected in the
relatively low dependency accuracy reported below.
As with the Czech data, we trained and evaluated us-
ing the automatically-assigned part-of-speech labels
provided with the data.
Both the Czech and the Arabic corpora are anno-
tated in terms of syntactic dependencies. For En-
glish and Chinese, however, no corpus is available
that is annotated in terms of dependencies. We there-
fore applied head-finding rules to treebanks that
were annotated in terms of constituency.
For English, we used the Penn Treebank version
3.0 (Marcus et al, 1993) and extracted dependency
relations by applying the head-finding rules of (Ya-
mada and Matsumoto, 2003). These rules are a
simplification of the head-finding rules of (Collins,
1999). We trained on sections 02-21, used section
24 for development test and evaluated on section
23. The English Penn Treebank contains approxi-
mately one million tokens. Training and evaluation
against the development test set was performed us-
ing human-annotated part-of-speech labels. Evalu-
ation against the blind test set was performed us-
ing part-of-speech labels assigned by the tagger de-
scribed in (Toutanova et al, 2003).
For Chinese, we used the Chinese Treebank ver-
sion 5.0 (Xue et al, 2005). This corpus contains
approximately 500,000 tokens. We made an approx-
imate 70%/15%/15% split for training/development
test/blind test by sampling whole files. As with the
English Treebank, training and evaluation against
the development test set was performed using
human-annotated part-of-speech labels. For evalu-
ation against the blind test section, we used an im-
plementation of the tagger described in (Toutanova
et al, 2003). Trained on the same training section
as that used for training the parser and evaluated on
the development test set, this tagger achieved a to-
ken accuracy of 92.2% and a sentence accuracy of
63.8%.
The corpora used vary in homogeneity from the
extreme case of the English Penn Treebank (a large
corpus drawn from a single source, the Wall Street
Journal) to the case of Arabic (a relatively small
corpus?approximately 2,000 sentences?drawn from
multiple sources). Furthermore, each language
presents unique problems for computational analy-
sis. Direct comparison of the dependency parsing
results for one language to the results for another
language is therefore difficult, although we do at-
tempt in the discussion below to provide some basis
for a more direct comparison. A common question
when considering the deployment of a new language
for machine translation is whether the natural lan-
guage components available are of sufficient quality
to warrant the effort to integrate them into the ma-
chine translation system. It is not feasible in every
instance to do the integration work first and then to
evaluate the output.
Table 1 summarizes the data used to train the
parsers, giving the number of tokens (excluding
traces and other empty elements) and counts of sen-
tences.1
3 Parser Architecture
We take as our starting point a re-implementation
of McDonald?s state-of-the-art dependency parser
(McDonald et al, 2005a). Given a sentence x, the
goal of the parser is to find the highest-scoring parse
y? among all possible parses y ? Y :
y? = arg max
y?Y
s(x, y) (1)
1The files in each partition of the Chinese and Arabic data
are given at http://research.microsoft.com/?simonco/
HLTNAACL2006.
161
Language Total Training Development Blind
Tokens Sentences Sentences Sentences
Arabic 116,695 2,100 446 449
Chinese 527,242 14,735 1,961 2,080
Czech 1,595,247 73,088 7,319 7,507
English 1,083,159 39,832 1,346 2,416
Table 1: Summary of data used to train parsers.
For a given parse y, its score is the sum of the scores
of all its dependency links (i, j) ? y:
s(x, y) = ?
(i,j)?y
d(i, j) = ?
(i,j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a head-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i, j) is further decomposed as the weighted
sum of its features f(i, j).
This parser architecture naturally consists of three
modules: (1) a decoder that enumerates all possi-
ble parses y and computes the argmax; (2) a train-
ing algorithm for adjusting the weights w given the
training data; and (3) a feature representation f(i, j).
Two decoders will be discussed here; the training al-
gorithm and feature representation are discussed in
the following sections.
A good decoder should satisfy several proper-
ties: ideally, it should be able to search through all
valid parses of a sentence and compute the parse
scores efficiently. Efficiency is a significant issue
since there are usually an exponential number of
parses for any given sentence, and the discrimina-
tive training methods we will describe later require
repeated decoding at each training iteration. We re-
implemented Eisner?s decoder (Eisner, 1996), which
searches among all projective parse trees, and the
Chu-Liu-Edmonds? decoder (Chu and Liu, 1965;
Edmonds, 1967), which searches in the space of
both projective and non-projective parses. (A pro-
jective tree is a parse with no crossing dependency
links.) For the English and Chinese data, the head-
finding rules for converting from Penn Treebank
analyses to dependency analyses creates trees that
are guaranteed to be projective, so Eisner?s algo-
rithm suffices. For the Czech and Arabic corpora,
a non-projective decoder is necessary. Both algo-
rithms are O(N3), where N is the number of words
in a sentence.2 Refer to (McDonald et al, 2005b)
for a detailed treatment of both algorithms.
4 Training: The Bayes Point Machine
In this section, we describe an online learning al-
gorithm for training the weights w. First, we ar-
gue why an online learner is more suitable than a
batch learner like a Support Vector Machine (SVM)
for this task. We then review some standard on-
line learners (e.g. perceptron) before presenting the
Bayes Point Machine (BPM) (Herbrich et al, 2001;
Harrington et al, 2003).
4.1 Online Learning
An online learner differs from a batch learner in that
it adjusts w incrementally as each input sample is
revealed. Although the training data for our pars-
ing problem exists as a batch (i.e. all input sam-
ples are available during training), we can apply
online learning by presenting the input samples in
some sequential order. For large training set sizes,
a batch learner may face computational difficulties
since there already exists an exponential number of
parses per input sentence. Online learning is more
tractable since it works with one input at a time.
A popular online learner is the perceptron. It ad-
justs w by updating it with the feature vector when-
ever a misclassification on the current input sample
occurs. It has been shown that such updates con-
verge in a finite number of iterations if the data is lin-
early separable. The averaged perceptron (Collins,
2002) is a variant which averages the w across all
iterations; it has demonstrated good generalization
especially with data that is not linearly separable,
as in many natural language processing problems.
2The Chu-Liu-Edmonds? decoder, which is based on a maxi-
mal spanning tree algorithm, can run in O(N2), but our simpler
implementation of O(N3) was sufficient.
162
Recently, the good generalization properties of Sup-
port Vector Machines have prompted researchers to
develop large margin methods for the online set-
ting. Examples include the margin perceptron (Duda
et al, 2001), ALMA (Gentile, 2001), and MIRA
(which is used to train the parser in (McDonald et al,
2005a)). Conceptually, all these methods attempt to
achieve a large margin and approximate the maxi-
mum margin solution of SVMs.
4.2 Bayes Point Machines
The Bayes Point Machine (BPM) achieves good
generalization similar to that of large margin meth-
ods, but is motivated by a very different philoso-
phy of Bayesian learning or model averaging. In
the Bayesian learning framework, we assume a prior
distribution over w. Observations of the training
data revise our belief of w and produce a poste-
rior distribution. The posterior distribution is used
to create the final wBPM for classification:
wBPM = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D) wi (3)
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the expec-
tation taken with respect to this distribution. The
term |V (D)| is the size of the version space V (D),
which is the set of weights wi that is consistent with
the training data (i.e. the set of wi that classifies the
training data with zero error). This solution achieves
the so-called Bayes Point, which is the best approx-
imation to the Bayes optimal solution given finite
training data.
In practice, the version space may be large, so we
approximate it with a finite sample of size I . Further,
assuming a uniform prior over weights, we get the
following equation:
wBPM = Ep(w|D)[w] ?
I?
i=1
1
I wi (4)
Equation 4 can be computed by a very simple al-
gorithm: (1) Train separate perceptrons on different
random shuffles of the entire training data, obtaining
a set of wi. (2) Take the average (arithmetic mean)
of the weights wi. It is well-known that perceptron
training results in different weight vector solutions
Input: Training set D = ((x1, y1), (x2, y2), . . . , (xT , yT ))
Output: wBPM
Initialize: wBPM = 0
for i = 1 to I; do
Randomly shuffle the sequential order of samples in D
Initialize: wi = 0
for t = 1 to T; do
y?t = wi ? xt
if (y?t != yt) thenwi = wi + ytxt
done
wBPM = wBPM + 1Iwidone
Figure 1: Bayes Point Machine pseudo-code.
if the data samples are presented sequentially in dif-
ferent orders. Therefore, random shuffles of the data
and training a perceptron on each shuffle is effec-
tively equivalent to sampling different models (wi)
in the version space. Note that this averaging op-
eration should not be confused with ensemble tech-
niques such as Bagging or Boosting?ensemble tech-
niques average the output hypotheses, whereas BPM
averages the weights (models).
The BPM pseudocode is given in Figure 1. The
inner loop is simply a perceptron algorithm, so the
BPM is very simple and fast to implement. The
outer loop is easily parallelizable, allowing speed-
ups in training the BPM. In our specific implemen-
tation for dependency parsing, the line of the pseu-
docode corresponding to [y?t = wi ? xt] is replaced
by Eq. 1 and updates are performed for each in-
correct dependency link. Also, we chose to average
each individual perceptron (Collins, 2002) prior to
Bayesian averaging.
Finally, it is important to note that the definition of
the version space can be extended to include weights
with non-zero training error, so the BPM can handle
data that is not linearly separable. Also, although we
only presented an algorithm for linear classifiers (pa-
rameterized by the weights), arbitrary kernels can be
applied to BPM to allow non-linear decision bound-
aries. Refer to (Herbrich et al, 2001) for a compre-
hensive treatment of BPMs.
5 Features
Dependency parsers for all four languages were
trained using the same set of feature types. The
feature types are essentially those described in (Mc-
Donald et al, 2005a). For a given pair of tokens,
163
where one is hypothesized to be the parent and the
other to be the child, we extract the word of the par-
ent token, the part of speech of the parent token, the
word of the child token, the part of speech of the
child token and the part of speech of certain adjacent
and intervening tokens. Some of these atomic fea-
tures are combined in feature conjunctions up to four
long, with the result that the linear classifiers de-
scribed below approximate polynomial kernels. For
example, in addition to the atomic features extracted
from the parent and child tokens, the feature [Par-
entWord, ParentPOS, ChildWord, ChildPOS] is also
added to the feature vector representing the depen-
dency between the two tokens. Additional features
are created by conjoining each of these features with
the direction of the dependency (i.e. is the parent to
the left or right of the child) and a quantized measure
of the distance between the two tokens. Every token
has exactly one parent. The root of the sentence has
a special synthetic token as its parent.
Like McDonald et al we add features that con-
sider the first five characters of words longer than
five characters. This truncated word crudely approx-
imates stemming. For Czech and English the addi-
tion of these features improves accuracy. For Chi-
nese and Arabic, however, it is clear that we need a
different backoff strategy.
For Chinese, we truncate words longer than a sin-
gle character to the first character.3 Experimental
results on the development test set suggested that an
alternative strategy, truncation of words longer than
two characters to the first two characters, yielded
slightly worse results.
The Arabic data is annotated with gold-standard
morphological information, including information
about stems. It is also annotated with the output
of an automatic morphological analyzer, so that re-
searchers can experiment with Arabic without first
needing to build these components. For Arabic, we
truncate words to the stem, using the value of the
lemma attribute.
All tokens are converted to lowercase, and num-
bers are normalized. In the case of English, Czech
and Arabic, all numbers are normalized to a sin-
3There is a near 1:1 correspondence between characters
and morphemes in contemporary Mandarin Chinese. However,
most content words consist of more than one morpheme, typi-
cally two.
gle token. In Chinese, months are normalized to a
MONTH token, dates to a DATE token, years to a
YEAR token. All other numbers are normalized to a
single NUMBER token.
The feature types were instantiated using all or-
acle combinations of child and parent tokens from
the training data. It should be noted that when the
feature types are instantiated, we have considerably
more features than McDonald et al For example,
for English we have 8,684,328 whereas they report
6,998,447 features. We suspect that this is mostly
due to differences in implementation of the features
that backoff to stems.
The averaged perceptrons were trained on the
one-best parse, updating the perceptron for every
edge and averaging the accumulated perceptrons af-
ter every sentence. Experiments in which we up-
dated the perceptron based on k-best parses tended
to produce worse results. The Chu-Liu-Edmonds al-
gorithm was used for Czech. Experiments with the
development test set suggested that the Eisner de-
coder gave better results for Arabic than the Chu-
Liu-Edmonds decoder. We therefore used the Eisner
decoder for Arabic, Chinese and English.
6 Results
Table 2 presents the accuracy of the dependency
parsers. Dependency accuracy indicates for how
many tokens we identified the correct head. Root ac-
curacy, i.e. for how many sentences did we identify
the correct root or roots, is reported as F1 measure,
since sentences in the Czech and Arabic corpora can
have multiple roots and since the parsing algorithms
can identify multiple roots. Complete match indi-
cates how many sentences were a complete match
with the oracle dependency parse.
A convention appears to have arisen when report-
ing dependency accuracy to give results for English
excluding punctuation (i.e., ignoring punctuation to-
kens in the output of the parser) and to report results
for Czech including punctuation. In order to facil-
itate comparison of the present results with previ-
ously published results, we present measures includ-
ing and excluding punctuation for all four languages.
We hope that by presenting both sets of measure-
ments, we also simplify one dimension along which
published results of parse accuracy differ. A direct
164
Including punctuation Excluding punctuation
Language Dependency Root Complete Dependency Root Complete
Accuracy Accuracy Match Accuracy Accuracy Match
Arabic 79.9 90.0 9.80 79.8 87.8 10.2
Chinese 71.2 66.2 17.5 73.3 66.2 18.2
Czech 84.0 88.8 30.9 84.3 76.2 32.2
English 90.0 93.7 35.1 90.8 93.7 37.6
Table 2: Bayes Point Machine accuracy measured on blind test set.
comparison of parse results across languages is still
difficult for reasons to do with the different nature
of the languages, the corpora and the differing stan-
dards of linguistic detail annotated, but a compar-
ison of parsers for two different languages where
both results include punctuation is at least preferable
to a comparison of results including punctuation to
results excluding punctuation.
The results reported here for English and Czech
are comparable to the previous best published num-
bers in (McDonald et al, 2005a), as Table 3 shows.
This table compares McDonald et al?s results for an
averaged perceptron trained for ten iterations with
no check for convergence (Ryan McDonald, pers.
comm.), MIRA, a large margin classifier, and the
current Bayes Point Machine results. To determine
statistical significance we used confidence intervals
for p=0.95. For the comparison of English depen-
dency accuracy excluding punctuation, MIRA and
BPM are both statistically significantly better than
the averaged perceptron result reported in (McDon-
ald et al, 2005a). MIRA is significantly better
than BPM when measuring dependency accuracy
and root accuracy, but BPM is significantly better
when measuring sentences that match completely.
From the fact that neither MIRA nor BPM clearly
outperforms the other, we conclude that we have
successfully replicated the results reported in (Mc-
Donald et al, 2005a) for English.
For Czech we also determined significance using
confidence intervals for p=0.95 and compared re-
sults including punctuation. For both dependency
accuracy and root accuracy, MIRA is statisticallty
significantly better than averaged perceptron, and
BPM is statistically significantly better than MIRA.
Measuring the number of sentences that match com-
pletely, BPM is statistically significantly better than
averaged perceptron, but MIRA is significantly bet-
ter than BPM. Again, since neither MIRA nor BPM
outperforms the other on all measures, we conclude
that the results constitute a valiation of the results
reported in (McDonald et al, 2005a).
For every language, the dependency accuracy of
the Bayes Point Machine was greater than the ac-
curacy of the best individual perceptron that con-
tributed to that Bayes Point Machine, as Table 4
shows. As previously noted, when measuring
against the development test set, we used human-
annotated part-of-speech labels for English and Chi-
nese.
Although the Prague Czech Dependency Tree-
bank is much larger than the English Penn Treebank,
all measurements are lower than the corresponding
measurements for English. This reflects the fact that
Czech has considerably more inflectional morphol-
ogy than English, leading to data sparsity for the lex-
ical features.
The results reported here for Arabic are, to our
knowledge, the first published numbers for depen-
dency parsing of Arabic. Similarly, the results for
Chinese are the first published results for the depen-
dency parsing of the Chinese Treebank 5.0.4 Since
the Arabic and Chinese numbers are well short of
the numbers for Czech and English, we attempted
to determine what impact the smaller corpora used
for training the Arabic and Chinese parsers might
have. We performed data reduction experiments,
training the parsers on five random samples at each
size smaller than the entire training set. Figure 2
shows the dependency accuracy measured on the
complete development test set when training with
samples of the data. The graph shows the average
4(Wang et al, 2005) report numbers for undirected depen-
dencies on the Chinese Treebank 3.0. We cannot meaningfully
compare those numbers to the numbers here.
165
Language Algorithm DA RA CM
English Avg. Perceptron 90.6 94.0 36.5
(exc punc) MIRA 90.9 94.2 37.5
Bayes Point Machine 90.8 93.7 37.6
Czech Avg. Perceptron 82.9 88.0 30.3
(inc punc) MIRA 83.3 88.6 31.3
Bayes Point Machine 84.0 88.8 30.9
Table 3: Comparison to previous best published results reported in (McDonald et al, 2005a).
Arabic Chinese Czech English
Bayes Point Machine 78.4 83.8 84.5 91.2
Best averaged perceptron 77.9 83.1 83.5 90.8
Worst averaged perceptron 77.4 82.6 83.3 90.5
Table 4: Bayes Point Machine accuracy vs. averaged perceptrons, measured on development test set, ex-
cluding punctuation.
dependency accuracy for five runs at each sample
size up to 5,000 sentences. English and Chinese
accuracies in this graph use oracle part-of-speech
tags. At all sample sizes, the dependency accu-
racy for English exceeds the dependency accuracy
of the other languages. This difference is perhaps
partly attributable to the use of oracle part-of-speech
tags. However, we suspect that the major contribu-
tor to this difference is the part-of-speech tag set.
The tags used in the English Penn Treebank encode
traditional lexical categories such as noun, prepo-
sition, and verb. They also encode morphological
information such as person (the VBZ tag for exam-
ple is used for verbs that are third person, present
tense?typically with the suffix -s), tense, number
and degree of comparison. The part-of-speech tag
sets used for the other languages encode lexical cat-
egories, but do not encode morphological informa-
tion.5 With small amounts of data, the perceptrons
do not encounter sufficient instances of each lexical
item to calculate reliable weights. The perceptrons
are therefore forced to rely on the part-of-speech in-
formation.
It is surprising that the results for Arabic and Chi-
nese should be so close as we vary the size of the
5For Czech and Arabic we followed the convention estab-
lished in previous parsing work on the Prague Czech Depen-
dency Treebank of using the major and minor part-of-speech
tags but ignoring other morphological information annotated on
each node.
training data (Figure 2) given that Arabic has rich
morphology and Chinese very little. One possible
explanation for the similarity in accuracy is that the
rather poor root accuracy in Chinese indicates parses
that have gone awry. Anecdotal inspection of parses
suggests that when the root is not correctly identi-
fied, there are usually cascading related errors.
Czech, a morphologically complex language in
which root identification is far from straightfor-
ward, exhibits the worst performance at small sam-
ple sizes. But (not shown) as the sample size in-
creases, the accuracy of Czech and Chinese con-
verge.
7 Conclusions
We have successfully replicated the state-of-the-art
results for dependency parsing (McDonald et al,
2005a) for both Czech and English, using Bayes
Point Machines. Bayes Point Machines have the ap-
pealing property of simplicity, yet are competitive
with online wide margin methods.
We have also presented first results for depen-
dency parsing of Arabic and Chinese, together with
some analysis of the performance on those lan-
guages.
In future work we intend to explore the discrim-
inative reranking of n-best lists produced by these
parsers and the incorporation of morphological fea-
tures.
166
60
65
70
75
80
85
90
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000Sample size
Depen
dency
 Accu
racy
EnglishChineseArabicCzech
Figure 2: Dependency accuracy at various sample
sizes. Graph shows average of five samples at each
size and measures accuracy against the development
test set.
Acknowledgements
We would like to thank Ryan McDonald, Otakar
Smrz? and Hiroyasu Yamada for help in various
stages of the project.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Processing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
R. O. Duda, P. E. Hart, and D. G. Stork. 2001. Pattern
Classification. John Wiley & Sons, Inc.: New York.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of COLING 1996, pages 340?345.
Claudio Gentile. 2001. A new approximate maximal
margin classification algorithm. Journal of Machine
Learning Research, 2:213?242.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, pages 245?278.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Assocation for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. Technical Report MS-CIS-05-11, Dept. of
Computer and Information Science, Univ. of Pennsyl-
vania.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Arul Menezes and Chris Quirk. 2005. Microsoft re-
search treelet translation system: IWSLT evaluation.
In Proceedings of the International Workshop on Spo-
ken Language Translation.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd annual meet-
ing of the Association for Computational Linguistics.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe structurale.
Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 152?159.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
167
English-Japanese Example-Based Machine Translation Using Abstract 
Linguistic Representations 
 
Chris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirk  
and Hisami Suzuki 
Natural Language Processing Group, Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.com 
 
 
Abstract 
This presentation describes an example- 
based English-Japanese machine trans- 
lation system in which an abstract 
linguistic representation layer is used to 
extract and store bilingual translation 
knowledge, transfer patterns between 
languages, and generate output strings. 
Abstraction permits structural neutral-
izations that facilitate learning of trans-
lation examples across languages with 
radically different surface structure charac-
teristics, and allows MT development to 
proceed within a largely language- 
independent NLP architecture. Com-
parative evaluation indicates that after 
training in a domain the English-Japanese 
system is statistically indistinguishable 
from a non-customized commercially 
available MT system in the same domain. 
Introduction 
In the wake of the pioneering work of Nagao 
(1984), Brown et al (1990) and Sato and 
Nagao (1990), Machine Translation (MT) 
research has increasingly focused on the issue 
of how to acquire translation knowledge from 
aligned parallel texts. While much of this 
research effort has focused on acquisition of 
correspondences between individual lexical 
items or between unstructured strings of words, 
closer attention has begun to be paid to the 
learning of structured phrasal units: Yamamoto 
and Matsumoto (2000), for example, describe a 
method for automatically extracting correspon-
dences between dependency relations in 
Japanese and English. Similarly, Imamura 
(2001a, 2001b) seeks to match corresponding 
Japanese and English phrases containing 
information about hierarchical structures, 
including partially completed parses. 
 
Yamamoto and Matsumoto (2000) explicitly 
assume that dependency relations between 
words will generally be preserved across 
languages. However, when languages are as 
different as Japanese and English with respect 
to their syntactic and informational structures, 
grammatical or dependency relations may not 
always be preserved: the English sentence ?the 
network failed? has quite a different 
grammatical structure from its Japanese 
translation equivalent ??????????
???? ?a defect arose in the network.? One 
issue for example-based MT, then, is to capture 
systematic divergences through generic 
learning applicable to multiple language pairs. 
 
In this presentation we describe the MSR-MT 
English-Japanese system, an example-based 
MT system that learns structured phrase-sized 
translation units. Unlike the systems discussed 
in Yamamoto and Matsumoto (2000) and 
Imamura (2001a, 2001b), MSR-MT places the 
locus of translation knowledge acquisition at a 
greater level of abstraction than surface 
relations, pushing it into a semantically- 
motivated layer called LOGICAL FORM (LF) 
(Heidorn 2000; Campbell & Suzuki 2002a, 
2002b). Abstraction has the effect of 
neutralizing (or at least minimizing) differences 
in word order and syntactic structure, so that 
mappings between structural relations 
associated with lexical items can readily be 
acquired within a general MT architecture. 
 
In Section 1 below, we present an overview of 
the characteristics of the system, with special 
reference to English-Japanese MT. Section 2 
discusses a class of structures learned through 
phrase alignment, Section 3 presents the results 
of comparative evaluation, and Section 4 some 
factors that contributed to the evaluation results. 
Section 5 addresses directions for future work. 
 
  
1 The MSR-MT System 
The MSR-MT English-Japanese system is a 
hybrid example-based machine translation 
system that employs handcrafted broad- 
coverage augmented phrase structure grammars 
for parsing, and statistical and heuristic 
techniques to capture translation knowlege and 
for transfer between languages. The parsers are 
general purpose: the English parser, for 
example, forms the core of the grammar 
checkers used in Microsoft Word (Heidorn 
2000). The Japanese grammar utilizes much of 
the same codebase, but contains language- 
specific grammar rules and additional features 
owing to the need for word-breaking in 
Japanese (Suzuki et al 2000; Kacmarcik et al 
2000). These parsers are robust in that if the 
analysis grammar fails to find an appropriate 
parse, it outputs a best-guess ?fitted? parse. 
 
System development is not confined to 
English-Japanese: MSR-MT is part of a 
broader natural language processing project 
involving three Asian languages (Japanese, 
Chinese, and Korean) and four European 
languages (English, French, German, and 
Spanish). Development of the MSR-MT 
systems proceeds more or less simultaneously 
across these languages and in multiple 
directions, including Japanese-English. The 
Spanish-English version of MSR-MT has been 
described in Richardson et al 2001a, Richardson 
et al2001b, and the reader is referred to these 
papers for more information concerning 
algorithms employed during phrase alignment. 
A description of the French-Spanish MT 
system is found in Pinkham & Smets. 2002. 
 
1.1 Training Data 
MSR-MT requires that a large corpus of 
aligned sentences be available as examples for 
training. For English-Japanese MT, the system 
currently trains on a corpus of approximately 
596,000 pre-aligned sentence pairs. About 
274,000 of these are sentence pairs extracted 
from Microsoft technical documentation that 
had been professionally translated from 
English into Japanese. The remaining 322,000 
are sentence examples or sentence fragments 
extracted from electronic versions of student 
dictionaries.1  
1.2  Logical Form 
MSR-MT employs a post-parsing layer of 
semantic representation called LOGICAL FORM 
(LF) to handle core components of the 
translation process, namely acquisition and 
storage of translation knowledge, transfer 
between languages, and generation of target 
output. LF can be viewed as a representation of 
the various roles played by the content words 
after neutralizing word order and local 
morphosyntactic variation (Heidorn 2000; 
Campbell & Suzuki 2002a; 2002b). These can 
be seen in the Tsub (Typical Subject) and Tobj 
(Typical Object) relations in Fig. 1 in the 
sentence ?Mary eats pizza? and its Japanese 
counterpart. The graphs are simplified for 
expository purposes. 
Although our hypothesis is that equivalent 
sentences in two languages will tend to 
resemble each other at LF more than they do in 
the surface parse, we do not adopt a na?ve 
reductionism that would attempt to make LFs 
completely identical. In Fig. 2, for example, the 
LFs of the quantified nouns differ in that the 
Japanese LF preserves the classifier, yet are 
similar enough that learning the mapping 
between the two structures is straightforward. 
It will be noted that since the LF for each 
language stores words or morphemes of that 
language, this level of representation is not in 
any sense an interlingua. 
 
                                                   
1 Kodansha?s Basic English-Japanese Dictionary, 
1999; Kenkyusha?s New College Japanese-English 
Dictionary, 4th Edition, 1995 ; and Kenkyusha?s 
New College English-Japanese Dictionary, 6th 
Edition, 1994. 
 
 
Fig. 1  Canonical English and Japanese 
Logical Forms 
 
 
1.3  Mapping Logical Forms 
In the training phase, MSR-MT learns transfer 
mappings from the sentence-aligned bilingual 
corpus. First, the system deploys the 
general-purpose parsers to analyze the English 
and Japanese sentence pairs and generate LFs 
for each sentence. In the next step, an LF 
alignment algorithm is used to match source 
language and target language LFs at the 
sub-sentence level. 
 
The LF alignment algorithm first establishes 
tentative lexical correspondences between 
nodes in the source and target LFs on the basis 
of lexical matching over dictionary information 
and approximately 31,000 ?word associations,? 
that is, lexical mappings extracted from the 
training corpora using statistical techniques 
based on mutual information (Moore 2001). 
From these possible lexical correspondences, 
the algorithm uses a small grammar of 
(language-pair-independent) rules to align LF 
nodes on lexical and structural principles. The 
aligned LF pairs are then partitioned into 
smaller aligned LF segments, with individual 
node mappings captured in a relationship we 
call ?sublinking.? Finally, the aligned LF 
segments are filtered on the basis of frequency, 
and compiled into a database known as a 
Mindnet. (See Menezes & Richardson 2001 for a 
detailed description of this process.) 
 
The Mindnet is a general-purpose database of 
semantic information (Richardson et al 1998) 
that has been repurposed as the primary 
repository of translation information for MT 
applications. The process of building the 
Mindnet is entirely automated; there is no 
human vetting of candidate entries. At the end 
of a typical training session, 1,816,520 transfer 
patterns identified in the training corpus may 
yield 98,248 final entries in the Mindnet. Only 
the output of successful parses is considered 
for inclusion, and each mapping of LF 
segments must have been encountered twice in 
the corpus before it is incorporated into the 
Mindnet. 
 
In the Mindnet, LF segments from the source 
language are represented as linked to the 
corresponding LF segment from the target 
languages. These can be seen in Figs. 3 and 4, 
discussed below in Section 2. 
1.4  Transfer and Generation 
At translation time, the broad-coverage source 
language parser processes the English input 
sentence, and creates a source-language LF. 
This LF is then checked against the Mindnet 
entries. 2  The best matching structures are 
extracted and stitched together determinist-
ically into a new target-language ?transferred 
LF? that is then submitted to the Japanese 
system for generation of the output string. 
 
The generation module is language-specific 
and used for both monolingual generation and 
MT. In the context of MT, generation takes as 
input the transferred LF and converts it into a 
basic syntactic tree. A small set of heuristic 
rules preprocesses the transferred LF to 
?nativize? some structural differences, such as 
pro-drop phenomena in Japanese. A series of 
core generation rules then applies to the LF tree, 
transforming it into a Japanese sentence string. 
Generation rules operate on a single tree only, 
are application-independent and are developed 
in a monolingual environment (see Aikawa et 
al. 2001a, 2001b for further details.) 
Generation of inflectional morphology is also 
handled in this component. The generation 
component has no explicit knowledge of the 
source language. 
 
2 Acquisition of Complex Structural 
Mappings 
The generalization provided by LF makes it 
possible for MSR-MT to handle complex 
structural relations in cases where English and 
Japanese are systematically divergent. This is 
                                                   
2 MSR-MT resorts to lexical lookup only when a 
term is not found in the Mindnet. The handcrafted 
dictionary is slated for replacement by purely 
statistically generated data.  
 
 
Fig. 2  Cross-Linguistic Variation in Logical 
Form 
 
illustrated by the sample training pair in the 
lefthand column of Table 1. In Japanese, 
inanimate nouns tend to be avoided as subjects 
of transitive verbs; the word ?URL?, which is 
subject in the English sentence, thus 
corresponds to an oblique relation in the 
Japanese. (The Japanese sentence, although a 
natural and idiomatic translation of the English,  
is literally equivalent to ?one can access public 
folders with this URL.?)   
 
Nonetheless, mappings turn out to be learnable 
even where the information is structured so 
radically differently. Fig. 3 shows the Mindnet 
entry for ?provide,? which is result of training 
on sentence pairs like those in the lefthand 
column of Table 1. The system learns not only 
the mapping between the phrase ?provide 
access? and the potential form of ???? 
?access?, but also the crucial sublinking of the 
Tsub node of the English sentence and the node 
headed by ?  (underspecified for semantic 
role) in the Japanese. At translation time the 
system is able to generalize on the basis of the 
functional roles stored in the Mindnet; it can 
substitute lexical items to achieve a relatively 
natural translation of similar sentences such as 
shown in the right-hand side of Table 1.  
Differences of the kind seen in Fig 3 are 
endemic in our Japanese and English corpora. 
Fig. 4 shows part of the example Mindnet entry 
for the English word ?fail? referred to in the 
Introduction, which exhibits another mismatch 
in grammatical roles somewhat similar to that 
in observed in Fig. 3. Here again, the lexical 
matching and generic alignment heuristics have 
allowed the match to be captured into the 
Mindnet. Although the techniques employed 
may have been informed by analysis of 
language-specific data, they are in principle of 
general application. 
 
 
3 Evaluation 
In May 2002, we compared output of the 
MSR-MT English-Japanese system with a 
commercially available desktop MT system.3 
                                                   
3 Toshiba?s The Honyaku Office v2.0 desktop MT 
system was selected for this purpose. The Honyaku 
is a trademark of the Toshiba Corporation. Another 
desktop system was also considered for evaluation; 
however, comparative evaluation with that system 
indicated that the Toshiba system performed 
marginally, though not significantly, better on our 
technical documentation.  
 
Training Data Translation Output  
This URL provides access to public folders. 
 
This computer provides access to the internet. 
 
?? URL ?????? ????? 
????????? 
????????????????? 
????????? 
 
Table 1.  Sample Input and Output 
Fig. 3.  Part of the Mindnet Entry for ?provide? 
 
 
Fig. 4.  Part of the Mindnet Entry for ?fail? 
 
A total of 238 English-Japanese sentence pairs 
were randomly extracted from held-out 
software manual data of the same kinds used 
for training the system. 4  The Japanese 
sentences, which had been translated by human 
translators, were taken as reference sentences 
(and were assumed to be correct translations). 
The English sentences were then translated by 
the two MT systems into Japanese for blind 
evaluation performed by seven outside vendors 
unfamiliar with either system?s characteristics. 
 
No attempt was made to constrain or modify 
the English input sentences on the basis of 
length or other characteristics. Both systems 
provided a translation for each sentence.5  
 
For each of the Japanese reference sentences, 
evaluators were asked to select which 
translation was closer to the reference sentence. 
A value of +1 was assigned if the evaluator 
considered MSR-MT output sentence better 
and ?1 if they considered the comparison 
system better. If two translated sentences were 
considered equally good or bad in comparison 
                                                   
4  250 sentences were originally selected for 
evaluation; 12 were later discarded when it was 
discovered by evaluators that the Japanese reference 
sentences (not the input sentences) were defective 
owing to the presence of junk characters (mojibake) 
and other deficiencies.  
5 In MSR-MT, Mindnet coverage is sufficiently 
complete with respect to the domain that an 
untranslated sentence normally represents a 
complete failure to parse the input, typically owing 
to excessive length. 
to the reference, a value of 0 was assigned. On 
this metric, MSR-MT scored slightly worse 
than the comparison system rating of ?0.015. 
At a two-way confidence measure of +/?0.16, 
the difference between the systems is 
statistically insignificant. By contrast, an 
earlier evaluation conducted in October 2001 
yielded a score of ?0.34 vis-?-vis the 
comparison system. 
 
In addition, the evaluators were asked to rate 
the translation quality on an absolute scale of 1 
through 4, according to the following criteria: 
 
1. Unacceptable: Absolutely not comprehen- 
sible and/or little or no information trans- 
ferred accurately. 
2. Possibly Acceptable: Possibly compre- 
hensible (given enough context and/or 
time to work it out); some information 
transferred accurately. 
3. Acceptable: Not perfect, but definitely 
comprehensible, and with accurate transfer 
of all important information. 
4. Ideal: Not necessarily a perfect translation, 
but grammatically correct, and with all 
information accurately transferred. 
 
On this absolute scale, neither system 
performed exceptionally well: MSR-MT scored 
an average 2.25 as opposed to 2.32 for the 
comparison system. Again, the difference 
between the two is statistically insignificant. It 
should be added that the comparison presented 
here is not ideal, since MSR-MT was trained 
principally on technical manual sentences, 
 Evaluation 
Date 
Transfers 
per Sentence 
Nodes  
Per Transfer
 
 Oct. 2001 5.8 1.6  
 May 2002 6.7 2.0  
Table 2. Number of Transfers and Nodes Transferred per Sentence 
 
 Evaluation Date Word Class Total From 
Mindnet 
From 
Dictionary
Untranslated  
 Prepositions 410 17.1% 77.1% 5.9%  
 
Oct. 2001  
(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%  
 Prepositions 842 61.9% 37.5% 0.6%  
 
May 2002 
(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%  
Table 3.  Sources of Different Word Classes at Transfer 
 
while the comparison system was not 
specifically tuned to this corpus. Accordingly 
the results of the evaluation need to be 
interpreted narrowly, as demonstrating that:  
l  A viable example-based English-Japanese 
MT system can be developed that applies 
general-purpose alignment rules to semantic 
representations; and  
l  Given general-purpose grammars, a 
representation of what the sentence means, 
and suitable learning techniques, it is 
possible to achieve in a domain, results 
analogous with those of a mature 
commercial product, and within a relatively 
short time frame. 
4 Discussion 
It is illustrative to consider some of the factors 
that contributed to these results. Table 2 shows 
the number of transfers per sentence and the 
number of LF nodes per transfer in versions of 
the system evaluated in October 2001 and May 
2002. Not only is the MSR-MT finding more 
LF segments in the Mindnet, crucially the 
number of nodes transferred has also grown. 
An average of two connected nodes are now 
transferred with each LF segment, indicating 
that the system is increasingly learning its 
translation knowledge in terms of complex 
structures rather than simple lexical 
correspondences. 
 
It has been our experience that the greater 
MSR-MT?s reliance on the Mindnet, the better 
the quality of its output. Table 2 shows the 
sources of selected word classes in the two 
systems. Over time, reliance on the Mindnet 
has increased overall, while reliance on 
dictionary lookup has now diminished to the 
point where, in the case of content words, it 
should be possible to discard the handcrafted 
dictionary altogether and draw exclusively on 
the contextualized resources of the Mindnet 
and statistically-generated lexical data. Also 
striking in Table 2 is the gain shown in 
preposition handling: a majority of English 
prepositions are now being transferred only in 
the context of LF structures found in the 
Mindnet. 
 
The important observation underlying the gains 
shown in these tables is that they have 
primarily been obtained either as the result of 
LF improvements in English or Japanese (i.e., 
from better sentence analysis or LF 
construction), or as a result of generic 
improvements to the algorithms that map 
between LF segments (notably better 
coindexation and improved learning of 
mappings involving lexical attributes). In the 
latter case, although certain modifications may 
have been driven by phenomena observed 
between Japanese and English, the heuristics 
apply across all seven languages on which our 
group is currently working. Adaptation to the 
case of Japanese-English MT usually takes the 
form of loosening rather than tightening of 
constraints.  
 
 
5 Future Work 
Ultimately it is probably desirable that the 
system?s mean absolute score should approach 
3 (Acceptable) within the training domain: this 
is a high quality bar that is not attained by 
off-the-shelf systems. Much of the work will be 
of a general nature: improving the parses and 
LF structures of source and target languages 
will bring automatic benefits to both alignment 
of structured phrases and runtime translation. 
For example, efforts are currently underway to 
redesign LF to better represent scopal 
properties of quantifiers and negation 
(Campbell & Suzuki 2002a, 2002b). 
 
Work to improve the quality of alignment and 
transfer is ongoing within our group. In 
addition to improvement of alignment itself, 
we are also exploring techniques to ensure that 
the transferred LF is consistent with known 
LFs in the target language, with the eventual 
goal of obviating the need for heuristic rules 
used in preprocessing generation. Again, these 
improvements are likely to be system-wide and 
generic, and not specific to the 
English-Japanese case. 
 
 
Conclusions 
Use of abstract semantically-motivated 
linguistic representations (Logical Form) 
permits MSR-MT to align, store, and translate 
sentence patterns reflecting widely varying 
syntactic and information structures in 
Japanese and English, and to do so within the 
framework of a general-purpose NLP 
architecture applicable to both European 
languages and Asian languages. 
 
Our experience with English-Japanese example 
based MT suggests that the problem of MT 
among Asian languages may be recast as a 
problem of implementing a general represen- 
tation of structured meaning across languages 
that neutralizes differences where possible, and 
where this is not possible, readily permits 
researchers to identify general-purpose 
techniques of bridging the disparities that are 
viable across multiple languages. 
 
Acknowledgements 
We would like to thank Bill Dolan and Rich 
Campbell for their comments on a draft of this 
paper. Our appreciation also goes to the 
members of the Butler Hill Group for their 
assistance with conducting evaluations. 
References  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001a. Multilingual sentence generation. In 
Proceedings of 8th European Workshop on 
Natural Language Generation, Toulouse, France.  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001b. Sentence generation for multilingual 
machine translation. In Proceedings of the MT 
Summit VIII, Santiago de Compostela, Spain.  
Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D. 
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, 
and P. S. Roossin. 1990. A statistical approach to 
machine translation. Computational Linguistics, 
16(2): 79-85. 
Campbell, R. and H. Suzuki. 2002a. Language- 
neutral representation of syntactic structure. In 
Proceedings of the First International Workshop 
on Scalable Natural Language Understanding 
(SCANALU 2002), Heidelberg, Germany. 
Campbell, R. and H. Suzuki. 2002b. Language- 
Neutral Syntax: An Overview. Microsoft Research 
Techreport: MSR-TR-2002-76. 
Heidorn, G. 2000. Intelligent writing assistance. In 
R. Dale, H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker, New York. 
pp. 181-207.  
Imamura, K. 2001a. Application of translation 
knowledge acquired by ierarchical phrase 
alignment. In Proceedings of TMI.  
Imamura, K. 2001b. Hierarchical phrase alignment 
harmonized with parsing. In Proceedings of 
NLPRS, Tokyo, Japan, pp 377-384.  
Kacmarcik, G., C. Brockett, and H. Suzuki. 2000. 
Robust segmentation of Japanese text into a 
lattice for parsing. In Proceedings of COLING 
2000, Saarbrueken, Germany, pp. 390-396. 
Menezes, A. and S. D. Richardson. 2001. A 
best-first alignment algorithm for automatic 
extraction of transfer mappings from bilingual 
corpora. In Proceedings of the Workshop on 
Data-driven Machine Translation at 39th Annual 
Meeting of the Association for Computational 
Linguistics, Toulouse, France, pp. 39-46. 
Moore, R. C. 2001. Towards a simple and accurate 
statistical approach to learning translation 
relationships among words," in Proceedings, 
Workshop on Data-driven Machine Translation, 
39th Annual Meeting and 10th Conference of the 
European Chapter, Association for 
Computational Linguistics, Toulouse, France, pp. 
79-86. 
Nagao, M. 1984. A framework of a mechanical 
translation between Japanese and English by 
analogy principle. In A. Elithorn. and R. Bannerji 
(eds.) Artificial and Human Intelligence.  Nato 
Publications. pp. 181-207.   
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro. 2001. Rapid Assembly of a Large-scale 
French-English MT system. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Pinkham, J., and M. Smets. 2002. Machine 
translation without a bilingual dictionary. In 
Proceedings of the 9th International Conference 
on Theoretical and Methodological Issues in 
Machine Translation. Kyoto, Japan, pp. 146-156. 
Richardson, S. D., W. B. Dolan, A. Menezes, and M. 
Corston-Oliver. 2001. Overcoming the 
customization bottleneck using example-based 
MT. In Proceedings, Workshop on Data-driven 
Machine Translation, 39th Annual Meeting and 
10th Conference of the European Chapter, 
Association for Computational Linguistics. 
Toulouse, France, pp. 9-16. 
Richardson, S. D., W. B. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298.  
Richardson, S. D., W. B. Dolan, and L. 
Vanderwende. 1998 MindNet: Acquiring and 
structuring semantic information from text, 
ACL-98. pp. 1098-1102. 
Sato, S. and Nagao M. 1990. Toward 
memory-based translation. In Proceedings of 
COLING 1990, Helsinki, Finland, pp. 247-252. 
Suzuki, H., C. Brockett, and G. Kacmarcik. 2000. 
Using a broad-coverage parser for word-breaking 
in Japanese. In Proceedings of COLING 2000, 
Saarbrueken, Germany, pp. 822-827.  
Yamamoto K., and Y Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. In 
Proceedings of COLING 2000, Saarbrueken, 
Germany, pp. 933-939.  
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 57?64,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic identification of sentiment vocabulary: exploiting low associa-
tion with known sentiment terms 
 
 
Michael Gamon Anthony Aue 
Natural Language Processing Group Natural Language Processing Group 
Microsoft Research Microsoft Research 
mgamon@microsoft.com anthaue@microsoft.com 
 
 
 
 
Abstract 
We describe an extension to the technique 
for the automatic identification and label-
ing of sentiment terms described in Tur-
ney (2002) and Turney and Littman 
(2002). Their basic assumption is that 
sentiment terms of similar orientation 
tend to co-occur at the document level. 
We add a second assumption, namely that 
sentiment terms of opposite orientation 
tend not to co-occur at the sentence level. 
This additional assumption allows us to 
identify sentiment-bearing terms very re-
liably. We then use these newly identified 
terms in various scenarios for the senti-
ment classification of sentences. We show 
that our approach outperforms Turney?s 
original approach. Combining our ap-
proach with a Naive Bayes bootstrapping 
method yields a further small improve-
ment of classifier performance. We finally 
compare our results to precision and recall 
figures that can be obtained on the same 
data set with labeled data. 
1 Introduction 
The field of sentiment classification has received 
considerable attention from researchers in recent 
years (Pang and Lee 2002, Pang et al 2004, Tur-
ney 2002, Turney and Littman 2002, Wiebe et al 
2001, Bai et al 2004, Yu and Hatzivassiloglou 
2003 and many others). The identification and 
classification of sentiment constitutes a problem 
that is orthogonal to the usual task of text classifi-
cation. Whereas in traditional text classification the 
focus is on topic identification, in sentiment classi-
fication the focus is on the assessment of the 
writer?s sentiment toward the topic. 
Movie and product reviews have been the main 
focus of many of the recent studies in this area 
(Pang and Lee 2002, Pang et al 2004, Turney 
2002, Turney and Littman 2002). Typically, these 
reviews are classified at the document level, and 
the class labels are ?positive? and ?negative?. In 
this work, in contrast, we narrow the scope of in-
vestigation to the sentence level and expand the set 
of labels, making a threefold distinction between 
?positive?, ?neutral?, and ?negative?. The narrow-
ing of scope is motivated by the fact that for realis-
tic text mining on customer feedback, the 
document level is too coarse, as described in Ga-
mon et al (2005). The expansion of the label set is 
also motivated by real-world concerns; while it is a 
given that review text expresses positive or nega-
tive sentiment, in many cases it is necessary to also 
identify the cases that don?t carry strong expres-
sions of sentiment at all. 
Traditional approaches to text classification re-
quire large amounts of labeled training data. Ac-
quisition of such data can be costly and time-
consuming. Due to the highly domain-specific na-
ture of the sentiment classification task, moving 
from one domain to another typically requires the 
acquisition of a new set of training data. For this 
reason, unsupervised or very weakly supervised 
methods for sentiment classification are especially 
57
desirable.1 Our focus, therefore, is on methods that 
require very little data annotation. 
We describe a method to automatically identify 
the sentiment vocabulary in a domain. This method 
rests on three special properties of the sentiment 
domain: 
1. the presence of certain words can serve as 
a proxy for the class label 
2. sentiment terms of similar orientation tend 
to co-occur 
3. sentiment terms of opposite orientation 
tend to not co-occur at the sentence level. 
Turney (2002) and Turney and Littman (2002) 
exploit the first two generalizations for unsuper-
vised sentiment classification of movie reviews. 
They use the two terms excellent and poor as seed 
terms to determine the semantic orientation of 
other terms. These seed terms can be viewed as 
proxies for the class labels ?positive? and ?nega-
tive?, allowing for the exploitation of otherwise 
unlabeled data: Terms that tend to co-occur with 
excellent in documents tend to be of positive orien-
tation, and vice versa for poor. Turney (2002) 
starts from a small (2 word) set of terms with 
known orientation (excellent and poor). Given a set 
of terms with unknown sentiment orientation, Tur-
ney (2002) then uses the PMI-IR algorithm (Tur-
ney 2001) to issue queries to the web and 
determine, for each of these terms, its pointwise 
mutual information (PMI) with the two seed words 
across a large set of documents. Term candidates 
are constrained to be adjectives, which tend to be 
the strongest bearers of sentiment. The sentiment 
orientation (SO) of a term is then determined by 
the difference between its association (PMI) with 
the positive seed term excellent and its association 
with the negative seed term poor. The resulting list 
of terms and associated sentiment orientations can 
then be used to implement a classifier: semantic 
orientation of the terms in a document of unknown 
sentiment is added up, and if the overall score is 
positive, the document is classified as being of 
positive sentiment, otherwise it is classified as 
negative. 
Yu and Hatzivassiloglou (2003) extend this ap-
proach by (1) applying it at the sentence level (in-
stead of the document-level), (2) taking into 
account non-adjectival parts-of-speech, and (3) 
                                                          
1
 For domain-specificity of sentiment classification see Eng-
str?m (2004) and Aue and Gamon (2005). 
using larger sets of seed words. Their classification 
goal also differs from Turney?s: it is to distinguish 
opinion sentences from factual statements. 
Turney et al?s approach is based on the assump-
tion that sentiment terms of similar orientation tend 
to co-occur in documents. Our approach takes ad-
vantage of a second assumption: At the sentence 
level, sentiment terms of opposite orientation tend 
not to co-occur. This is, of course, an assumption 
that will only hold in general, with exceptions. Ba-
sically, the assumption is that sentences of the fol-
lowing form: 
I dislike X. 
I really like X. 
are more frequent than ?mixed sentiment? sen-
tences such as 
I dislike X but I really like Y. 
It has been our experience that this generaliza-
tion does hold often enough to be useful. 
We propose to utilize this assumption to identify 
a set of sentiment terms in a domain. We select the 
terms that have the lowest PMI scores on the sen-
tence level with respect to a set of manually se-
lected seed words. If our assumption about low 
association at the sentence level is correct, this set 
of low-scoring terms will be particularly rich in 
sentiment terms. We can then use this newly iden-
tified set to: 
(1) use Turney?s method to find the orienta-
tion for the terms and employ the terms 
and their scores in a classifier, and 
(2) use Turney?s method to find the orienta-
tion for the terms and add the new terms 
as additional seed terms for a second it-
eration 
As opposed to Turney (2002), we do not use the 
web as a resource to find associations, rather we 
apply the method directly to in-domain data. This 
has the disadvantage of not being able to apply the 
classification to any arbitrary domain. It is worth 
noting, however, that even in Turney (2002) the 
choice of seed words is explicitly motivated by 
domain properties of movie reviews. 
In the remainder of the paper we will describe 
results from various experiments based on this as-
sumption. We also show how we can combine this 
method with a Naive Bayes bootstrapping ap-
proach that takes further advantage of the unla-
beled data (Nigam et al 2000). 
58
2 Data 
For our experiments we used a set of car reviews 
from the MSN Autos web site. The data consist of 
406,818 customer car reviews written over a four-
year period. Aside from filtering out examples con-
taining profanity, the data was not edited. The re-
views range in length from a single sentence (56% 
of all cases) to 50 sentences (a single review). Less 
than 1% of reviews contain ten or more sentences. 
There are almost 900,000 sentences in total. When 
customers submitted reviews to the website, they 
were asked for a recommendation on a scale of 1 
(negative) to 10 (positive). The average score was 
very high, at 8.3, yielding a strong skew in favor of 
positive class labels. We annotated a randomly-
selected sample of 3,000 sentences for sentiment. 
Each sentence was viewed in isolation and classi-
fied as positive, negative or neutral. The neutral 
category was applied to sentences with no dis-
cernible sentiment, as well as to sentences that ex-
pressed both positive and negative sentiment. 
Three annotators had pair-wise agreement scores 
(Cohen?s Kappa score, Cohen 1960) of 70.10%, 
71.78% and 79.93%, suggesting that the task of 
sentiment classification on the sentence level is 
feasible but difficult even for people. This set of 
data was split into a development test set of 400 
sentences and a blind test set of 2600 sentences. 
Sentences are represented as vectors of binary 
unigram features. The total number of observed 
unigram features is 72988. In order to restrict the 
number of features to a manageable size, we disre-
gard features that occur less than 10 times in the 
corpus. With this restriction we obtain a reduced 
feature set of 13317 features. 
3 Experimental Setup 
Our experiments were performed as follows: We 
started with a small set of manually-selected and 
annotated seed terms. We used 4 positive and 6 
negative seed terms. We decided to use a few more 
negative seed words because of the inherent posi-
tive skew in the data that makes the identification 
of negative sentences particularly hard. The terms 
we used are: 
 positive: negative: 
 good  bad 
 excellent lousy 
 love  terrible 
 happy  hate 
   suck 
   unreliable 
There was no tuning of the set of initial seed 
terms; the 10 words were originally chosen intui-
tively, as words that we observed frequently when 
manually inspecting the data. 
We then used these seed terms in two basic 
ways: (1) We used them as seeds for a Turney-
style determination of the semantic orientation of 
words in the corpus (semantic orientation, or SO 
method). As mentioned above, this process is 
based on the assumption that terms of similar ori-
entation tend to co-occur. (2) We used them to 
mine sentiment vocabulary from the unlabeled data 
using the additional assumption that sentiment 
terms of opposite orientation tend not to co-occur 
at the sentence level (sentiment mining, or SM 
method). This method yields a set of sentiment 
terms, but no orientation for that set of terms. We 
continue by using the SO method to find the se-
mantic orientation for this set of sentiment terms, 
effectively using SM as a feature selection method 
for sentiment terminology. 
Pseudo-code for the SO and SM approaches is 
provided in Figure 1 and Figure 2. As a first step 
for both SO and SM methods (not shown in the 
pseudocode), PMI needs to be calculated for each 
pair (f, s) of feature f and seed word s over the col-
lection of feature vectors. 
 
Figure 1: SO method for determining semantic orienta-
tion 
59
 Figure 2: SM method for mining sentiment terms 
In the first scenario (using straightforward SO), 
features F range over all observed features in the 
data (modulo the aforementioned count cutoff of 
10). In the second scenario (SM + SO), features F 
range over the n% of features with the lowest PMI 
scores with respect to any of the seed words that 
were identified using the sentiment mining tech-
nique in Figure 2. 
The result of both SO and SM+SO is a list of 
unigram features which have an associated seman-
tic orientation score, indicating their sentiment ori-
entation: the higher the score, the more ?positive? 
a term, and vice versa. 
This list of features and associated scores can be 
used to construct a simple classifier: for each sen-
tence with unknown sentiment, we take the sum of 
the semantic orientation scores for all of the uni-
grams in that sentence. This overall score deter-
mines the classification of the sentence as 
?positive?, ?neutral? or ?negative? as shown in 
Figure 3. 
Scoring and classifying sentence vectors:
(1) assigning a sentence score:
FOREACH feature f in sentence vector v:
Score(v) = Score(v) + SO(f)
(2) assigning a class label based on the sentence score:
IF Score(v) > threshold1:
Class(v) = ?positive? 
ELSE IF Score(v) < threshold1 AND Score(v) > threshold2:
Class(v) = ?neutral?
ELSE
Class(v) = ?negative?
 
Figure 3: Using SO scores for sentence scoring and 
classification 
The two thresholds used in classification need to 
be determined empirically by taking the distribu-
tion of class values in the corpus into account. For 
our experiments we simply took the distribution of 
class labels in the 400 sentence development test 
set as an approximation of the overall class label 
distribution: we determined that distribution to be 
15.5% for negative sentences, 21.5% for neutral 
sentences, and 63.0% for positive sentences. 
Scores for all sentence vectors in the corpus are 
then collected using the scoring part of the algo-
rithm in Figure 3. The scores are sorted and the 
thresholds are determined as the cutoffs for the top 
63% and bottom 15.5% of scores respectively. 
4 Results 
4.1. Comparing SO and SM+SO 
In our first set of experiments we manipulated the 
following parameters: 
1. the choice of SO or SM+SO method 
2. the choice of n when selecting the n% se-
mantic terms with lowest PMI score in the 
SM method 
The tables below show the results of classifying 
sentence vectors using the unigram features and 
associated scores produced by SO and SO+SM. 
We used the 2,600-sentence manually-annotated 
test set described previously to establish these 
numbers. Since the data exhibit a strong skew in 
favor of the positive class label, we measure per-
formance not in terms of accuracy but in terms of 
average precision and recall across the three class 
labels, as suggested in (Manning and Sch?tze 
2002). 
 Avg precision Avg recall 
SO  0.4481 0.4511 
Table 1: Using the SO approach. 
Table 1 shows results of using the SO method 
on the data. Table 2 presents the results of combin-
ing the SM and SO methods for different values of 
n. The best results are shown in boldface. 
As a comparison between Table 1 and Table 2 
shows, the highest average precision and recall 
scores were obtained by combining the SM and SO 
methods. Using SM as a feature selection mecha-
nism also reduces the number of features signifi-
cantly. While the SO method employed on 
sentence-level vectors uses 13,000 features, the 
best-performing SM+SO combination uses only 
20% of this feature set, indicating that SM is in-
deed effective in selecting the most important sen-
timent-bearing terms. 
60
We also determined that the positive impact of 
SM is not just a matter of reducing the number of 
features. If SO - without the SM feature selection 
step - is reduced to a comparable number of fea-
tures by taking the top features according to abso-
lute score, average precision is at 0.4445 and 
average recall at 0.4464. 
N=10 N=20 N=30 N=40 N=50  
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
SM+SO 
SO from 
docu-
ment 
level 
0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475 
Table 2: combining SM and SO. 
Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms 
excellent, terrible, broke, junk, alright, bargain, 
grin, highest, exceptional, exceeded, horrible, 
loved, waste, ok, death, leaking, outstanding, 
cracked, rebate, warped, hooked, sorry, refuses, 
excellant, satisfying, died, biggest, competitive, 
delight, avoid, awful, garbage, loud, okay, com-
petent, upscale, dated, mistake, sucks, superior, 
high, kill, neither 
excellent, happy, stylish, sporty, smooth, love, 
quiet, overall, pleased, plenty, dependable, solid, 
roomy, safe, good, easy, smaller, luxury, comfort-
able, style, loaded, space, classy, handling, joy, 
small, comfort, size, perfect, performance, room, 
choice, recommended, package, compliments, 
awesome, unique, fun, holds, comfortably, ex-
tremely, value, free, satisfied, little, recommend, 
limited, great, pleasure 
Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms 
alternative, wont, below, surprisingly, main-
tained, choosing, comparing, legal, vibration, 
seemed, claim, demands, assistance, knew, engi-
neering, accelleration, ended, salesperson, per-
formed, started, midsize, site, gonna, lets, plugs, 
industry, alternator, month, told, vette, 180, 
powertrain, write, mos, walk, causing, lift, es, 
segment, $250, 300m, wanna, february, mod, 
$50, nhtsa, suburbans, manufactured, tiburon, 
$10, f150, 5000, posted, tt, him, saw, jan,  
condition, very, handles, milage, definitely, defi-
nately, far, drives, shape, color, price, provides, 
options, driving, rides, sports, heated, ride, sport, 
forward, expected, fairly, anyone, test, fits, stor-
age, range, family, sedan, trunk, young, weve, 
black, college, suv, midsize, coupe, 30, shopping, 
kids, player, saturn, bose, truck, town, am, leather, 
stereo, car, husband 
Table 3: the top 100 terms identified by SM and SO
Table 3 shows the top 100 terms that were identi-
fied by each SM and SO methods. The terms are 
categorized into sentiment-bearing and non-
sentiment bearing terms by human judgment. The 
two sets seem to differ in both strength and orien-
tation of the identified terms. The SM-identified 
words have a higher density of negative terms (22 
out of 43 versus 2 out of 49 for the SO-identified 
terms). The SM-identified terms also express sen-
timent more strongly, but this conclusion is more 
tentative since it may be a consequence of the 
higher density of negative terms. 
4.2. Multiple iterations: increasing the 
number of seed features by SM+SO 
In a second set of experiments, we assessed the 
question of whether it is possible to use multiple 
iterations of the SM+SO method to gradually build 
the list of seed words. We do this by adding the top 
n% of features selected by SM, along with their 
orientation as determined by SO, to the initial set 
of seed words. The procedure for this round of ex-
periments is as follows: 
? take the top n% of features identified by 
SM (we used n=1 for the reported re-
61
sults, since preliminary experiments 
with other values for n did not improve 
results) 
? perform SO for these features to deter-
mine their orientation 
? take the top 15.5% negative and top 
63% positive (according to class label 
distribution in the development test set) 
of the features and add them as nega-
tive/positive seed features respectively 
This iteration increases the number of seed fea-
tures from the original 10 manually-selected fea-
tures to a total of 111 seed features. 
With this enhanced set of seed features we then 
re-ran a subset of the experiments in Table 2. Re-
sults are shown in Table 4. Increasing the number 
of seed features through the SM feature selection 
method increases precision and recall by several 
percentage points. In particular, precision and re-
call for negative sentences are boosted. 
 
Avg 
precision 
Avg 
recall 
SM + SO, n=10, 
SO from document vectors 0.4826 0.48.76 
SM + SO, n=30, 
SO from document vectors 0.4957 0.4995 
SM + SO, n=50, 
SO from document vectors 0.4914 0.4952 
Table 4: Using 2 iterations to increase the seed feature 
set 
We also confirmed that these results are truly at-
tributable to the use of the SM method for the first 
iteration. If we take an equivalent number of fea-
tures with strongest semantic orientation according 
to the SO method and add them to the list of seed 
features, our results degrade significantly (the re-
sulting classifier performance is significantly dif-
ferent at the 99.9% level as established by the 
McNemar test). This is further evidence that SM is 
indeed an effective method for selecting sentiment 
terms. 
4.3. Using the SO classifier to bootstrap a 
Naive Bayes classifier 
In a third set of experiments, we tried to improve 
on the results of the SO classifier by combining it 
with the bootstrapping approach described in (Ni-
gam et al 2000). The basic idea here is to use the 
SO classifier to label a subset of the data DL. This 
labeled subset of the data is then used to bootstrap 
a Naive Bayes (NB) classifier on the remaining 
unlabeled data DU using the Expectation Maximi-
zation (EM) algorithm: 
(1) An initial naive Bayes classifier with 
parameters ? is trained on the docu-
ments in DL. 
(2) This initial classifier is used to estimate 
a probability distribution over all classes 
for each of the documents in DU. (E-
Step) 
(3) The labeled and unlabeled data are then 
used to estimate parameters for a new 
classifier. (M-Step) 
Steps 2 and 3 are repeated until convergence is 
achieved when the difference in the joint probabil-
ity of the data and the parameters falls below the 
configurable threshold ? between iterations. An-
other free parameter, ?, can be used to control how 
much weight is given to the unlabeled data. 
For our experiments we used classifiers from the 
best SM+SO combination (2 iterations at n=30) 
from Table 4 above to label 30% of the total data. 
Table 5 shows the average precision and recall 
numbers for the converged NB classifier.2 In addi-
tion to improving average precision and recall, the 
resulting classifier also has the advantage of pro-
ducing class probabilities instead of simple scores.3 
 Avg 
precision 
Avg 
recall 
Bootstrapped NB 
classifier 0.5167 0.52 
Table 5: Results obtained by bootstrapping a NB classi-
fier 
4.4. Results from supervised learning: 
using small sets of labeled data 
Given infinite resources, we can always annotate 
enough data to train a classifier using a supervised 
algorithm that will outperform unsupervised or 
weakly-supervised methods. Which approach to 
take depends entirely on how much time and 
money are available and on the accuracy require-
ments for the task at hand. 
                                                          
2
 In this experiment, ? was set to 0.1 and ? was set to 0.05. 
3
 We also experimented with labeling the whole data set with the best of our SO 
score classifiers, and then training a linear Support Vector Machine classifier on 
the data. The results were considerably worse than any of the reported numbers, 
so they are not included in this paper. 
62
To help situate the precision and recall numbers 
presented in the tables above, we trained Support 
Vector Machines (SVMs) using small amounts of 
labeled data. SVMs were trained with 500, 1000, 
2000, and 2500 labeled sentences. Annotating 
2500 sentences represents approximately eight per-
son-hours of work. The results can be found in Ta-
ble 5. We were pleasantly surprised at how well 
the unsupervised classifiers described above per-
form in comparison to state-of-the-art supervised 
methods (albeit trained on small amounts of data). 
Labeled ex-
amples 
Avg. Preci-
sion 
Avg. Recall 
500 .4878 .4967 
1000 .5161 .5105 
2000 .5297 .5256 
2500 .5017 .5083 
Table 6: Average precision and recall for SVMs for 
small numbers of labeled examples  
4.5. Results on the movie domain 
We also performed a small set of experiments on 
the movie domain using Pang and Lee?s 2004 data 
set. This set consists of 2000 reviews, 1000 each of 
very positive and very negative reviews. Since this 
data set is balanced and the task is only a two-way 
classification between positive and negative re-
views, we only report accuracy numbers here. 
 accuracy Training data 
Turney 
(2002) 66% unsupervised 
Pang & Lee 
(2004) 87.15% supervised 
Aue & Ga-
mon (2005) 91.4% supervised 
SO 73.95% unsupervised 
SM+SO to 
increase seed 
words, then 
SO 
74.85% weakly super-
vised 
Table 7: Classification accuracy on the movie review 
domain 
Turney (2002) achieves 66% accuracy on the 
movie review domain using the PMI-IR algorithm 
to gather association scores from the web. Pang 
and Lee (2004) report 87.15% accuracy using a 
unigram-based SVM classifier combined with sub-
jectivity detection. Aue and Gamon (2005) use a 
simple linear SVM classifier based on unigrams, 
combined with LLR-based feature reduction, to 
achieve 91.4% accuracy. Using the Turney SO 
method on in-domain data instead of web data 
achieves 73.95% accuracy (using the same two 
seed words that Turney does). Using one iteration 
of SM+SO to increase the number of seed words, 
followed by finding SO scores for all words with 
respect to the enhanced seed word set, yields a 
slightly higher accuracy of 74.85%. With addi-
tional parameter tuning, this number can be pushed 
to 76.4%, at which point we achieve statistical sig-
nificance at the 0.95 level according to the McNe-
mar test, indicating that there is more room here 
for improvement. Any reduction of the number of 
overall features in this domain leads to decreased 
accuracy, contrary to what we observed in the car 
review domain. We attribute this observation to the 
smaller data set. 
5 Discussion 
5.1 A note on statistical significance 
We used the McNemar test to assess whether two 
classifiers are performing significantly differently. 
This test establishes whether the accuracy of two 
classifiers differs significantly - it does not guaran-
tee significance for precision and recall differ-
ences. For the latter, other tests have been 
proposed (e.g. Chinchor 1995), but time con-
straints prohibited us from implementing any of 
those more computationally costly tests. 
For the results presented in the previous sections 
the McNemar test established statistical signifi-
cance at the 0.99 level over baseline (i.e. the SO 
results in Table 1) for the multiple iterations results 
(Table 4) and the bootstrapping approach (Table 
5), but not for the SM+SO approach (Table 2). 
5.2 Future work 
This exploratory set of experiments indicates a 
number of interesting directions for future work. A 
shortcoming of the present work is the manual tun-
ing of cutoff parameters. This problem could be 
alleviated in at least two possible ways: 
First, using a general combination of the ranking 
of terms according to SM and SO. In other words, 
calculate the semantic weight of a term as a com-
bination of SO and its rank in the SM scores. 
63
Secondly, following a suggestion by an anony-
mous reviewer, the Naive Bayes bootstrapping ap-
proach could be used in a feedback loop to inform 
the SO score estimation in the absence of a manu-
ally annotated parameter tuning set. 
5.3 Summary 
Our results demonstrate that the SM method can 
serve as a valid tool to mine sentiment-rich vo-
cabulary in a domain. SM will yield a list of terms 
that are likely to have a strong sentiment orienta-
tion. SO can then be used to find the polarity for 
the selected features by association with the senti-
ment terms of known polarity in the seed word list. 
Performing this process iteratively by first enhanc-
ing the set of seed words through SM+SO yields 
the best results. While this approach does not com-
pare to the results that can be achieved by super-
vised learning with large amounts of labeled data, 
it does improve on results obtained by using SO 
alone. 
We believe that this result is relevant in two re-
spects. First, by improving average precision and 
recall on the classification task, we move closer to 
the goal of unsupervised sentiment classification. 
This is a very important goal in itself given the 
need for ?out of the box? sentiment techniques in 
business intelligence and the notorious difficulty of 
rapidly adapting to a new domain (Engstr?m 2004, 
Aue and Gamon 2005). Second, the exploratory 
results reported here may indicate a general source 
of information for feature selection in natural lan-
guage tasks: features that have a tendency to be in 
complementary distribution (especially in smaller 
linguistic units such as sentences) may often form 
a class that shares certain properties. In other 
words, it is not only the strong association scores 
that should be exploited but also the particularly 
weak (negative) associations. 
References  
Anthony Aue and Michael Gamon (2005): ?Customiz-
ing Sentiment Classifiers to a New Domain: A Case 
Study. Under review. 
Xue Bai, Rema Padman, and Edoardo Airoldi. (2004). 
Sentiment Extraction from Unstructured Text Using 
Tabu Search-Enhanced Markov Blanket. In: Proceed-
ings of the International Workshop on Mining for 
and from the Semantic Web (MSW 2004), pp 24-35. 
Nancy A. Chinchor (1995): Statistical significance of 
MUC-6 results. Proceedings of the Sixth Message 
Understanding Conference, pp. 39-44. 
J. Cohen (1960): ?A coefficient of agreement for nomi-
nal scales.? In: Educational and Psychological meas-
urements 20, pp. 37?46 
Charlotta Engstr?m. 2004. Topic dependence in Senti-
ment Classification. MPhil thesis, University of 
Cambridge. 
Michael Gamon, Anthony Aue, Simon Corston-Oliver, 
and Eric Ringger. (2005): ?Pulse: Mining Customer 
Opinions from Free Text?. Under review. 
Christopher D. Manning and Hinrich Sch?tze (2002): 
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, London. 
Kamal Nigam, Andrew McCallum, Sebastian Thrun and 
Tom Mitchell (2000): Text Classification from La-
beled and Unlabeled Documents using EM. In: Ma-
chine Learning 39 (2/3), pp. 103-134. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan 
(2002): ?Thumbs up? Sentiment Classification using 
Machine Learning Techniques?. Proceedings of 
EMNLP 2002, pp. 79-86. 
Bo Pang and Lillian Lee. (2004). A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of 
ACL 2004, pp.217-278. 
Peter D. Turney (2001): ?Mining the Web for Syno-
nyms: PMI-IR versus LSA on TOEFL.? In Proceed-
ings of the Twelfth European Conference on 
Machine Learning, pp. 491-502. 
Peter D. Turney (2002): ?Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised classi-
fication of reviews?. In: Proceedings of ACL 2002, 
pp. 417-424. 
Peter D. Turney and M. L. Littman (2002): ?Unsuper-
vised Learning of Semantic Orientation from a Hun-
dred-Billion-Word Corpus.? Technical report ERC-
1094 (NRC 44929), National Research Council of 
Canada. 
Janyce Wiebe, Theresa Wilson and Matthew Bell 
(2001): ?Identifying Collocations for Recognizing 
Opinions?. In: Proceedings of the ACL/EACL Work-
shop on Collocation. 
Hong Yu and Vasileios Hatzivassiloglou (2003): ?To-
wards Answering opinion Questions: Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion Sentences?. In: Proceedings of EMNLP 
2003. 
64
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 196?200, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing with Reference to Slovene, Spanish and Swedish
Simon Corston-Oliver
Natural Language Processing
Microsoft Research
One Microsoft Way
Redmond WA 98052
simonco@microsoft.com
Anthony Aue
Natural Language Processing
Microsoft Research
One Microsoft Way
Redmond WA 98052
anthaue@microsoft.com
Abstract
We describe a parser used in the CoNLL
2006 Shared Task, ?Multingual Depen-
dency Parsing.? The parser first identi-
fies syntactic dependencies and then labels
those dependencies using a maximum en-
tropy classifier. We consider the impact of
feature engineering and the choice of ma-
chine learning algorithm, with particular
focus on Slovene, Spanish and Swedish.
1 Introduction
The system that we submitted for the CoNLL 2006
Shared Task, ?Multingual Dependency Parsing,?
(Buchholz et al, 2006) is a two stage pipeline. The
first stage identifies unlabeled directed dependen-
cies using an extension of the parser described in
(Corston-Oliver et al, 2006). The second stage is a
maximum entropy classifier that labels the directed
dependencies. The system was trained on the twelve
obligatory languages, as well as the optional lan-
guage, Bulgarian (Hajic? et al, 2004; Simov et al,
2005; Simov and Osenova, 2003; Chen et al, 2003;
Bo?hmova? et al, 2003; Kromann, 2003; van der Beek
et al, 2002; Brants et al, 2002; Kawata and Bar-
tels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006;
Civit Torruella and Mart?? Anton??n, 2002; Nilsson et
al., 2005; Oflazer et al, 2003; Atalay et al, 2003).
Table 1 presents the results of the system de-
scribed in the current paper on the CoNLL shared
task, including the optional evaluation on Bulgar-
ian. For Slovene, we ranked second with a labeled
Language Unlabeled Labeled
Attachment Attachment
Arabic 78.40 63.53
Bulgarian 90.09 83.36
Chinese 90.00 79.92
Czech 83.02 74.48
Danish 87.94 81.74
Dutch 74.83 71.43
German 87.20 83.47
Japanese 92.84 89.95
Portugese 88.96 84.59
Slovene 81.77 72.42
Spanish 84.87 80.36
Swedish 89.54 79.69
Turkish 73.11 61.74
Table 1: Results on CoNLL 2006 shared task.
dependency accuracy of 72.42%. This was not sta-
tistically significantly different from the top-ranked
score of 73.44%. For Spanish, our labeled depen-
dency accuracy of 80.36% is within 0.1% of the
third-ranked score of 80.46%. Our unlabeled de-
pendency accuracy for Swedish was the best of all
the systems at 89.54%. Our labeled accuracy for
Swedish, however, at 79.69%, fell far short of the
third-best score of 82.31%. We therefore focus on
Swedish when considering the impact of our choice
of learning algorithm on our label accuracy.
2 Data
We divided the shared data into training and devel-
opment test sets, using larger development test sets
196
for the languages supplied with more data. The de-
velopment test set consisted of 250 sentences for
Arabic, Slovene, Spanish and Turkish, 500 sen-
tences for Danish and Portuguese, and 1,000 sen-
tences for the other languages.
3 The Parser
The baseline parser predicts unlabeled directed de-
pendencies. As described in (Corston-Oliver et al,
2006), we reimplemented the parser described in
(McDonald et al, 2005) and validated their results
for Czech and English.
The parser finds the highest-scoring parse y?
among all possible parses y ? Y for a given sen-
tence:
y? = argmax
y?Y
s(y) (1)
The score s of a given parse y is the sum of the
scores of all the dependency links (i,j) ? y:
s(y) =
?
(i,j)?y
d(i, j) =
?
(i,j)?y
w ? f(i, j) (2)
where the link (i,j) indicates a parent-child depen-
dency between the token at position i and the token
at position j. The score d(i, j) of each dependency
link (i,j) is further decomposed as the weighted sum
of its features f(i, j).
To set w, we trained twenty averaged perceptrons
on different shuffles of the training data, using the
development test set to determine when the percep-
trons had converged. The averaged perceptrons were
then combined to make a Bayes Point Machine (Har-
rington et al, 2003). At both training and run time,
edges are scored independently, and Eisner?s O(N3)
decoder (Eisner, 1996) is used to find the optimal
parse. This decoder produces only projective analy-
ses, although it does allow for analyses with multiple
roots.
The features used for scoring the edges prior to
applying Eisner?s algorithm are extracted from each
possible parent-child dependency. The features in-
clude the case-normalized original form and lemma1
of each token , the part of speech (POS) tag of each
token, the POS tag of each intervening token and
1If no lemma was specified, we truncated the original form
by taking the first two characters for Chinese words consisting
of two characters or more and the first five characters for words
consisting of five characters or more in the other languages.
of each token to the left and right of the parent and
child. Additional features are created by combining
these atomic features, as described in (McDonald et
al., 2005). All features are in turn combined with
the direction of attachment and the distance between
tokens. Distance was discretized, with individual
buckets for distances 0-4, a single bucket for 5-9,
and a single bucket for 10+. In sections 3.1 and 3.2
we discuss the feature engineering we performed.
3.1 Part of Speech Features
We experimented with using the coarse POS tag and
the fine POS tag. In our official submission, we
used fine POS tags for all languages except Dutch
and Turkish. For Dutch and Turkish, using the fine
POS tag resulted in a reduction in unlabeled depen-
dency accuracy of 0.12% and 0.43% respectively
on the development test sets, apparently because of
the sparsity of the fine POS tags. For German and
Swedish, the fine and coarse POS tags are the same
so using the fine POS tag had no effect. For other
languages, using the fine POS tag showed modest
improvements in unlabeled dependency accuracy.
For Swedish, we performed an additional manipu-
lation on the POS tags, normalizing the distinct POS
tags assigned to each verbal auxiliary and modal to
a single tag ?aux?. For example, in the Swedish
data all inflected forms of the verb ?vara? (?be?) are
tagged as AV, and all inflected forms of the modal
?ma?ste? (?must?) are tagged as MV. This normaliza-
tion caused unlabeled dependency accuracy on the
Swedish development set to improve from 89.23%
to 89.45%.
3.2 Features for Root Identification
Analysis of the baseline parser?s errors suggested
the need for additional feature types to improve the
identification of the root of the sentence. In particu-
lar, the parser was frequently making errors in iden-
tifying the root of periphrastic constructions involv-
ing an auxiliary verb or modal and a participle. In
Germanic languages, for example, the auxiliary or
modal typically occurs in second position in declar-
ative main clauses or in initial position in cases of
subject-aux inversion. We added a collection of fea-
tures intended to improve the identification of the
root. The hope was that improved root identifica-
tion would have a positive cascading effect in the
197
identification of other dependencies, since a failure
to correctly identify the root of the sentence usually
means that the parse will have many other errors.
We extracted four feature types, the original form
of the first and last tokens in the sentence and the
POS of the first and last tokens in the sentence.
These features were intended to identify declarative
vs. interrogative sentences.
For each child and parent token being scored, we
also noted the following four features: ?child/parent
is first non-punctuation token in sentence?,
?child/parent is second non-punctuation token in
sentence?. The features that identify the second
token in the sentence were intended to improve
the identification of verb-second phenomena. Of
course, this is a linguistic oversimplification. Verb-
second phenomena are actually sensitive to the order
of constituents, not words. We therefore added four
feature types that considered the sequence of POS
tags to the left of the child or parent if they occurred
within ten tokens of the beginning of the sentence
and the sequence of POS tags to the right of the
child or parent if they occurred within ten tokens of
the end of the sentence.
We also added features intended to improve the
identification of the root in sentences without a fi-
nite verb. For example, the Dutch training data
contained many simple responses to a question-
answering task, consisting of a single noun phrase.
Four simple features were used ?Child/Parent is the
leftmost noun in the sentence?, ?Child/Parent is a
noun but not the leftmost noun in the sentence?.
These features were combined with an indicator
?Sentence contains/does not contain a finite verb?.
Child or parent tokens that were finite verbs were
flagged as likely candidates for being the root of
the sentence if they were the leftmost finite verb in
the sentence and not preceded by a subordinating
conjunction or relative pronoun. Finite verbs were
identified by POS tags and morphological features,
e.g. in Spanish, verbs without the morphological
feature ?mod=n? were identified as finite, while in
Portuguese the fine POS tag ?v-fin? was used.
Similarly, various sets of POS tags were used to
identify subordinating conjunctions or relative pro-
nouns for different languages. For example, in Bul-
garian the fine POS tag ?pr? (relative pronoun) and
?cs? (subordinating conjunction) were used. For
Dutch, the morphological features ?onder?, ?betr?
and ?voorinf? were used to identify subordinating
conjunctions and relative pronouns.
These features wreaked havoc with Turkish, a
verb-final language. For certain other languages,
dependency accuracy measured on the develop-
ment test set improved by a modest amount, with
more dramatic improvements in root accuracy (F1
measure combining precision and recall for non-
punctuation root tokens).
Since the addition of these features had been mo-
tivated by verb-second phenomena in Germanic lan-
guages, we were surprised to discover that the only
Germanic language to demonstrate a marked im-
provement in unlabeled dependency accuracy was
Danish, whose accuracy on the development set rose
from 87.51% to 87.72%, while root accuracy F1
rose from 94.12% to 94.72%. Spanish showed a
modest improvement in unlabeled dependency accu-
racy, from 85.08% to 85.13%, but root F1 rose from
80.08% to 83.57%.
The features described above for identifying the
leftmost finite verb not preceded by a subordinat-
ing conjunction or relative pronoun did not im-
prove Slovene unlabeled dependency accuracy, and
so were not included in the set of root-identifying
features in our Slovene CoNLL submission. Closer
examination of the Slovene corpus revealed that pe-
riphrastic constructions consisting of one or more
auxiliaries followed by a participle were annotated
with the participle as the head, whereas for other
languages in the shared task the consensus view ap-
pears to be that the auxiliary should be annotated
as the head. Singling out the leftmost finite verb in
Slovene when a participle ought to be selected as the
root of the sentence is therefore counter-productive.
The other root identification features did improve
root F1 in Slovene. Root F1 on the development test
set rose from 45.82% to 46.43%, although overall
unlabeled dependency accuracy on the development
test set fell slightly from 80.24% to 79.94%.
3.3 Morphological Features
As the preceding discussion shows, morphological
information was occasionally used to assist in mak-
ing finer-grained POS distinctions than were made
in the POS tags, e.g., for distinguishing subordi-
nating vs. coordinating conjunctions. Aside from
198
these surgical uses of the morphological information
present in the CoNLL data, morphology was not ex-
plicitly used by the baseline parser. For example,
there were no features that considered subject-verb
agreement nor agreement of an adjective with the
number or lexical gender of the noun it modified.
However, it is possible that morphological informa-
tion influenced the training of edge weights if the
information was implicit in the POS tags.
4 The Dependency Labeler
4.1 Classifier
We used a maximum entropy classifier (Berger et al,
1996) to assign labels to the unlabeled dependen-
cies produced by the Bayes Point Machine. We used
the same training and development test split that was
used to train the dependency parser. We chose to use
maximum entropy classifiers because they can be
trained relatively quickly while still offering reason-
able classification accuracy and are robust in the face
of large numbers of superfluous features, a desirable
property given the requirement that the same parser
handle multiple languages. Furthermore, maximum
entropy classifiers provide good probability distribu-
tions over class labels. This was important to us be-
cause we had initially hoped to find the optimal set
of dependency labels for the children of a given node
by modeling the probability of each set of labels
conditioned on the lemma and POS of the parent.
For example, labeling each dependant of a parent
node independently might result in three OBJECT
relations dependent on a single verb; modeling sets
of relations ought to prevent this. Unfortunately, this
approach did not outperform labeling each node in-
dependently.
Therefore, the system we submitted labeled each
dependency independently, using the most probable
label from the maximum entropy classifier. We have
noted in previous experiments that our SVM imple-
mentation often gives better one-best classification
accuracy than our maximum entropy implementa-
tion, but did not have time to train SVM classifiers.
To see how much the choice of classification al-
gorithm affected our official results, we trained a lin-
ear SVM classifier for Swedish after the competition
had ended, tuning parameters on the development
test set. As noted in section 1, our system scored
highest for Swedish in unlabeled dependency accu-
racy at 89.54% but fell well short of the third-ranked
system when measuring labeled dependency accu-
racy. Using an SVM classifier instead of a maxi-
mum entropy classifier, Swedish label accuracy rose
from 82.33% to 86.06%, and labeled attachment ac-
curacy rose from 79.69% to 82.95%, which falls
between the first-ranked score of 84.58% and the
second-ranked score of 82.55%. Similarly, Japanese
label accuracy rose from 93.20% to 93.96%, and
labeled attachment accuracy rose from 89.95% to
90.77% when we replaced a maximum entropy clas-
sifier with an SVM. This labeled attachment result
of 90.77% is comparable to the official second place
result of 90.71% for Japanese. We conclude that a
two stage pipeline such as ours, in which the sec-
ond stage labels dependencies in isolation, is greatly
impacted by the choice of classifier.
4.2 Features Used for Labeling
We extracted features from individual nodes in the
dependency tree, parent-child features and features
that took nodes other than the parent and child into
account.
The features extracted from each individual par-
ent and child node were the original surface form,
the lemma (see footnote 1 above), the coarse and fine
POS tags and each morphological feature.
The parent-child features are the direction of
modification, the combination of the parent and
child lemmata, all combinations of parent and child
lemma and coarse POS tag (e.g. child lemma com-
bined with coarse POS tag of the parent) and all pair-
wise combinations of parent and child morphology
features (e.g. parent is feminine and child is plural).
Additional features were verb position (whether
the parent or child is the first or last verb in the sen-
tence), coarse POS and lemma of the left and right
neighbors of the parent and child, coarse POS and
lemma of the grandparent, number and coarse POS
tag sequence of siblings to the left and to the right of
the child, total number of siblings of the child, num-
ber of tokens governed by child, whether the par-
ent has a verbal ancestor, lemma and morphological
features of the verb governing the child (if any), and
coarse POS tag combined with relative offset of each
sibling (e.g., the sibling two to the left of the child is
a determiner).
199
For Slovene, the label accuracy using all of the
features above was 81.91%. We retrained our max-
imum entropy classifier by removing certain classes
of features in order to determine their contribu-
tion. Removing the weight features caused a notable
drop, with label accuracy on the development test set
falling 0.52% to 81.39%. Removing the grandpar-
ent features (but including weight features) caused
an even greater drop of 1.03% to 80.88%. One place
where the grandparent features were important was
in distinguishing between Adv and Atr relations. It
appears that the relation between a noun and its gov-
erning preposition or between a verb and its govern-
ing conjunction is sensitive to the part of speech of
the grandparent. For example, we observed a num-
ber of cases where the relation between a noun and
its governing preposition had been incorrectly la-
beled as Adv when it should have been Atr. The
addition of grandparent features allowed the classi-
fier to make the distinction by looking at the POS of
the grandparent; when the POS was noun, the clas-
sifier tended to correctly choose the Atr label.
5 Conclusion
We have described a two stage pipeline that first pre-
dicts directed unlabeled dependencies and then la-
bels them. The system performed well on Slovene,
Spanish and Swedish. Feature engineering played
an important role both in predicting dependencies
and in labeling them. Finally, replacing the maxi-
mum entropy classifier used to label dependencies
with an SVM improves upon our official results.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. of the Tenth Conf. on Com-
putational Natural Language Learning (CoNLL-X).
SIGNLL.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency parsing
using bayes point machines. In Proc. of HLT-NAACL
2006.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proceedings of Seventh
Pacific-Asia Conference on Knowledge Discovery and
Data Mining, pages 241?252.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
of the Assocation for Computational Linguistics.
200
