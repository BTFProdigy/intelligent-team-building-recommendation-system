Applying an NVEF Word-Pair Identifier to  
the Chinese Syllable-to-Word Conversion Problem 
 
Jia-Lin Tsai 
Intelligent Agent Systems Lab. 
Institute of Information Science, Academia Sinica, 
Nankang, Taipei, Taiwan, R.O.C. 
tsaijl@iis.sinica.edu.tw 
Wen-Lian Hsu 
Intelligent Agent Systems Lab. 
Institute of Information Science, Academia Sinica, 
Nankang, Taipei, Taiwan, R.O.C. 
hsu@iis.sinica.edu.tw 
 
Abstract  
Syllable-to-word (STW) conversion is important 
in Chinese phonetic input methods and speech 
recognition. There are two major problems in 
the STW conversion: (1) resolving the ambigu-
ity caused by homonyms; (2) determining the 
word segmentation. This paper describes a 
noun-verb event-frame (NVEF) word identifier 
that can be used to solve these problems effec-
tively. Our approach includes (a) an NVEF 
word-pair identifier and (b) other word identifi-
ers for the non-NVEF portion.  
Our experiment showed that the NVEF 
word-pair identifier is able to achieve a 99.66% 
STW accuracy for the NVEF related portion, 
and by combining with other identifiers for the 
non-NVEF portion, the overall STW accuracy is 
96.50%.  
The result of this study indicates that the NVEF 
knowledge is very powerful for the STW con-
version. In fact, numerous cases requiring dis-
ambiguation in natural language processing fall 
into such ?chicken-and-egg? situation. The 
NVEF knowledge can be employed as a general 
tool in such systems for disambiguating the 
NVEF related portion independently (thus 
breaking the chicken-and-egg situation) and 
using that as a good fundamental basis to treat 
the remaining portion. This shows that the 
NVEF knowledge is likely to be important for 
general NLP. To further expand its coverage, we 
shall extend the study of NVEF to that of other 
co-occurrence restrictions such as noun-noun 
pairs, noun-adjective pairs and verb-adverb pairs. 
We believe the STW accuracy can be further 
improved with the additional knowledge. 
1. Introduction 
More than 100 Chinese input methods have been 
created in the past [1-6]. Currently, the most 
popular input method is based on phonetic 
symbols. Phonetic input method requires little 
training because Chinese are taught to write the 
corresponding pinyin syllable of each Chinese 
character in primary school. Since there are 
more than 13,000 distinct Chinese characters 
(with around 5400 commonly-used), but only 
1,300 distinct syllables, the homonym problem 
is quite severe in phonetic input method. 
Therefore, an intelligent syllable-to-word (STW) 
conversion for Chinese is very important. A 
comparable (but easier) problem to the STW 
conversion in English is the word-sense 
disambiguation. 
There are basically two approaches for the STW 
conversion: (a) the linguistic approach based on 
syntax parsing or semantic template matching 
[3,4,7,8] and (b) the statistical approach based 
on the n-gram model where n is usually 2 or 3 
[9-12]. The linguistic approach is more laborious 
but the end result can be more user friendly. On 
the other hand, the statistical approach is less 
labor intensive, but its power is dependent on 
training corpus and it usually does not provide 
deep semantic information. Our approach adopts 
the semantically oriented NVEF word-pairs (to 
be defined formally in Section 2.1) plus other 
statistical methods so that not only the result 
makes sense semantically, but the model is also 
fully automatic provided that enough NVEFs 
have already been collected.  
According to the studies in [13], good syllable 
sequence segmentation is crucial for the STW 
conversion. For example, consider the syllable 
sequence ?zhe4 liang4 che1 xing2 shi3 shun4 
chang4? of the Chinese sentence ??????
?? (This car moves well).? By dictionary, the 
two possible segmentation results are (we use 
?/? to indicate syllable word boundary) 
(1) ?zhe4/liang4/che1 xing2/shi3/shun4 chang4?  
(2) ?zhe4/liang4/che1/xing2 shi3/shun4 chang4? 
using the longest-syllabic-word-first strategy 
[14]. The two ambiguous portions are /che1 
xing2/shi3/ (/{?? ,??}/{?,?,?,?,?,? ,
?}/) and /che1/xing2 shi3/ (/{?,?,? }/{??,
??}/), respectively. In this case, if the system 
has the information that ??-??(car, move)? is 
a permissible NVEF word-pair and its 
corresponding syllable-pair ?che1-xing2shi3? 
has been collected, then the correct segmentation 
and word-pair ?? -?? (che1-xing2shi3)? of 
this syllable sequence can be determined 
simultaneously.  
Since NVEF word-pairs are usually the key 
features of a sentence, if identified correctly, 
they become good reference words for the 
n-gram models to predict the remaining 
unconverted syllables. We [15] showed that the 
knowledge of NVEF sense-pairs and their 
corresponding NVEF word-pairs (NVEF 
knowledge) are useful for effectively resolving 
word sense ambiguity and getting highly 
accurate word-segmentation for those 
ambiguous NVEF word-pairs in Chinese.  
In this paper, we shall show that the NVEF 
knowledge can be used effectively in the STW 
conversion for Chinese. Section 2 describes our 
approach. The experimental result is presented 
in Section 3. Directions for future research will 
be discussed in section 4. 
2 . Development of an NVEF-based Word 
Identifier 
Hownet [16] is adopted as the system?s 
word-sense dictionary, which provides the 
knowledge of Chinese lexicon (58,541 words), 
parts-of-speech (POS) and word senses. We 
have integrated Chinese words in Hownet, 
Sinica corpus [17], Cilin (tong2yi4ci2ci2lin2??
? ? ? ? ?) [18], Chinese dictionary 
(guo2yu2ci2dian3??????) [19] and Chinese 
word lists in [20] into a commonly-used 
machine-readable dictionary (MRD) called 
common MRD, which provides the knowledge of 
Chinese lexicon (in which the top 60,000 words 
are selected from the list of 252,307 words in 
descending order of word frequency), word 
frequencies and syllable words. The syllable of 
each word in common MRD was translated by 
the inversed process of phoneme-to-character 
system presented in [4,8]. Word frequency is 
computed according to a fixed size training 
corpus consisting of 4,539,624 Chinese 
sentences obtained from the on-line United 
Daily News [21] (during the period of 17 
January, 2001 to 30 December, 2001).  
2.1 Definition of the NVEF Sense-Pair, 
Word-Pair and Syllable Word-Pair 
The sense of a word is defined as its DEF 
(concept definition) in Hownet. Table 1 lists 
three different senses of the Chinese word 
??(Che/car/turn).? In Hownet, the DEF of a 
word consists of its main feature and secon-
dary features. For example, in the DEF 
?character|? ? , surname|? , human|? , 
ProperName|?? of the word ??(Che),? the 
first item ?character|??? is the main fea-
ture, and the remaining three items, ?sur-
name|? ,? ?human|? ,? and ?ProperName|
?,? are its secondary features. The main 
feature in Hownet can inherit features in the 
hypernym-hyponym hierarchy. There are 
approximately 1,500 features in Hownet. 
Each feature is called a sememe, which re-
fers to a smallest semantic unit that cannot 
be further reduced. 
 
Table 1. Three different senses of the Chinese 
word ??(Che/car/turn)? 
Word POS/Sense (i.e. DEF in Hownet)  
? Che N/character|?? ,surname|?,human|?,ProperName|? 
? car N/LandVehicle|? 
? turn V/cut|?? 
The Hownet dictionary used in this study con-
tains 58,541 words, in which there are 33,264 
nouns, 16,723 verbs and 16,469 senses (includ-
ing 10,011 noun-senses and 4,462 verb-senses). 
In our experiment, we have also added the DEFs 
for those words not in Hownet. 
A permissible NV word-pair such as ??-??
(car-move)? is called a noun-verb event-frame 
(NVEF) word-pair. According to the sense of the 
word ??(Che/car/turn)? and the word ???
(move),? the only permissible NV sense-pair for 
the NV word-pair ?? - ?? (car, move)? is 
?LandVehicle| ? ?-?VehicleGo| ? .? We call 
such a permissible NV sense-pair an NVEF 
sense-pair. Note that an NVEF sense-pair is a 
class that includes the NVEF word-pair instance 
??-??? as well as the corresponding NVEF 
syllable word-pair ?che1-xing2 shi3.? 
2.2 Definition of the NVEF KR-Tree 
A knowledge representation tree (KR-tree) of 
NVEF sense-pairs is shown in Fig.1. There are 
two types of nodes in the KR-tree: concept 
nodes and function nodes. Concept nodes refer 
to words and features in Hownet. Function 
nodes are used to define the relationships 
between their parent and children concept nodes. 
If a concept node A is the child of another 
concept node B, then A is a subclass of B. 
Following this convention, we shall omit the 
function node ?subclass? between A and B. 
Noun-sense class is divided into 15 subclasses 
according to their main features. They are 
bacteria, animal, human, plant, artifact, natural, 
event, mental, phenomena, shape, place, location, 
time, abstract and quantity.  
Three function nodes are used in the KR-tree as 
shown in Fig. 1: 
 
Figure 1. An illustration of the KR-tree using ???
?(artifact)? as an example noun-sense subclass.  
(1) Major-Event (????): The content of its 
parent node represents a noun-sense subclass, 
and the content of its child node represents a 
verb-sense subclass. A noun-sense subclass 
and a verb-sense subclass linked by a 
Major-Event function node is an NVEF 
subclass sense-pair, such as ?&LandVehicle|
? ? and ?=VehcileGo|?? in Fig. 1. To 
describe various relationships between 
noun-sense and verb-sense subclasses, we 
have designed three subclass sense-symbols, 
in which ?=? means ?exact,? ?&? means 
?like,? and ?%? means ?inclusive.? An 
example using these symbols is given below. 
Given three senses S1, S2 and S3 defined by a 
main feature A and three secondary features 
B, C and D, let S1 = A, B, C, D,             
S2 = A, B, and S3 = A, C, D. Then, we have 
that sense S2 is in the ?=A,B? exact-subclass; 
senses S1 and S2 are in the ?&A,B? 
like-subclass; and senses S1 S2, and S3 are in 
the ?%A? inclusive-subclass. 
(2) Word-Instance (?? ): The content of its 
children are the words belonging to the sense 
subclass of its parent node. These words are 
learned automatically by the NVEF 
sense-pair identifier according to sentences 
under the Test-Sentence nodes. 
(3) Test-Sentence (???): Its content includes 
several selected test sentences in support of 
its corresponding NVEF subclass sense-pair. 
2.3 An NVEF Word-Pair Identifier 
We [15] have developed an NVEF sense-pair 
identifier for word-sense disambiguation (WSD). 
This sense-pair identifier is based on the NVEF 
KR-tree and the techniques of longest syllabic 
NVEF-word-pair first (LS-NVWF) and 
exclusion word list (EWL) checking. By 
modifying this identifier, we obtain our NVEF 
word-pair identifier described below. 
Step 1. Input a syllable sequence. 
Step 2. Generate all possible NV word-pairs 
whose corresponding NV syllable 
word-pairs are found in the input 
sequence. Exclude certain NV word-pairs 
based on EWL checking. 
Step 3. Check each NV word-pair to see if its 
corresponding NV sense-pairs (there can 
be several such pairs) can be matched to 
an NVEF subclass sense-pair in the 
KR-tree. If one such NV sense-pair 
matches an NVEF subclass sense-pair in 
the KR-tree, then this permissible NVEF 
sense-pairs and their corresponding NVEF 
word-pairs can be used for the sentence. 
Resolve conflicts using the LS-NVWF 
strategy. 
Step 4. Arrange all remaining permissible 
NVEF sense-pairs and their corresponding 
NVEF word-pairs in a sentence-NVEF 
tree. If no NVEF word-pair can be 
identified from the input sequence, a null 
sentence-NVEF tree will be produced. 
A system overview of the NVEF word-pair 
identifier is given in Fig. 2. The output of this 
NVEF word-pair identifier is called a 
sentence-NVEF tree, as shown in Fig. 3. 
 
NVEF word-
pair identifier
KR tree
sentence-NVEF treeinput syllables
Hownet
LS-NVWF & EWL checking
Figure 2. A system overview of the NVEF word-pair 
identifier. 
 
 
Figure 3. A sentence-NVEF tree for the Chinese 
syllables ?yin3 liao4 li3 han2 you3 bu4 ming2 wu4 
zhi2(?????????|There are uncertain 
matters in the drink).?  
2.4 A Word Identifier for the non-NVEF 
portion 
To supplement the NVEF word-pair identifier 
for the portion of syllable sequence that is not 
converted by the NVEF knowledge, a separate 
word identifier is developed. A system overview 
of the identifier for the NVEF portion and 
non-NVEF portion is given in Fig. 4. Our word 
identifier for the non-NVEF portion includes 
four sub-identifiers whose details are given be-
low: 
(1) Number-classifier-noun phrase (NCN 
phrase) identifier: There are many specific 
linguistic units, such as names, addresses, 
determinatives-measure compounds (DM) etc. in 
syllables which need to be recognized in order to 
supplement the NVEF word-pair identifier 
(which works in a top-down fashion) Although 
the number of these linguistic units are infinite, 
they can be recognized by finite regular 
expressions [22]. Following this fact and 
Chinese grammar, we have developed an NCN 
phrase identifier to identify phrases consisting of 
the numbers, classifiers, and nouns, in particular, 
the commonly-used number-classifier-noun 
syllable pattern, such as syllables ?yi1 bai3 wu3 
shi2 ge4 guan1 zhong4 (???????|one 
hundred and fifty audience).?  
To develop this identifier, we first divide the 
related words in Hownet into three subclasses 
for the construction of the NCN phrase, i.e. 
numbers (the POS is ?NUM?), classifiers (the 
POS is ?CLAS?) and nouns (the POS is ?N?.) 
Secondly, to enrich the knowledge in Hownet, 
12 new numbers and 172 new classifiers are 
added into the original Hownet. Then we create 
a table listing 13,366 classifier-noun word-pairs 
(CN word-pairs) and their corresponding CN 
syllable word-pairs, such as ?ge4-guan1 zhong4 
( ? - ? ? ).? This table is called the CN 
word-pair list, which is generated by training 
corpus (Monosyllabic nouns are not considered 
in this table). 
NVEF word-
pair identifier
KR tree
segmented sentence and a
sentence-NVEF tree
unsegmented
syllable input
Hownet
NCN word
identifier
specific-
monosyllabic
word identifier
unique-syllabic
word identifier
common MRD training corpus
non-unique-
syllabic word
identifier
Figure 4. A system overview of the NVEF-based 
word identifier. 
Now, take the syllables ?yin1 yue4 ting1 you3 
yi1 bai3 wu3 shi2 ge4 guan1 zhong4 (????
???????|There are one hundred and 
fifty audiences in concert hall)? as an example. 
The NCN phrase identifier will first identify the 
words of number syllables ?yi1 bai3 wu3 
shi2( ? ? ? ? |one hundred and fifty)? 
combined by combining two matching number 
syllables ?yi1 bai3( ?? |one hundred)? and 
?wu3 shi2(?? |fifty).? Secondly, if the number 
of characters of the recognized number syllables 
is greater than 1, the NCN word identifier will 
continue on checking the following syllables 
with the CN word-pair list. In this case, since the 
following syllables ?ge4 guan1 zhong4? are 
found in the CN word-pair list, it will be 
identified as CN word-pair ??-??.? 
(2) Specific-monosyllabic word identifier: 
When a monosyllabic word in Hownet has 
exactly one POS, and that POS is in the set 
{ADJ (adjective), ADV (adverb), AUX 
(auxiliary), COOR (coordinator), CONJ 
(conjunctive), PREP (preposition), STRU 
(structure word)}, we call this word a 
specific-monosyllabic word. There are 525 
specific-monosyllabic words found in the used 
Hownet. 
Consider the following monosyllabic word ??
|already(yi3).? We shall use the POS 
information of polysyllabic words immediately 
preceding and following this word to decide if 
?yi3? should be identified as ?? (already)?. 
According to the training corpus, the top 3 
preferred POSs of words following ? ?
(already)? are V (verb), ADV (adverb) and ADJ 
(adjective). Therefore, the top 3 preferred POSs 
of syllable words following ?yi3? should also be 
V, ADV and ADJ provided that ??(already)? is 
to be identified. The top 3 preferred POSs of 
syllable words preceding and following a 
specific-monosyllabic word will be called the 
top 3 preceding and following preferred POSs, 
respectively. 
Now, consider the syllable sequence ?gong1 
cheng2 yi3 wan2 cheng2 le5 (??????
|The project has been done)? as an example. 
First, by checking syllable-by-syllable from left 
to right, our algorithm recognizes that there is a 
specific-monosyllabic word ?yi3? in this 
sentence. Then, it will use the 
longest-syllabic-word-first strategy to identify 
the syllable word ?wan2 cheng2? following 
?yi3? and the syllable word ?gong1 cheng2? 
preceding ?yi3?. It will check whether at least 
one of the distinct POSs of the preceding and 
following syllable words are within the set of 
top 3 preceding and following preferred POSs of 
?yi3?, respectively . Since this is indeed the case, 
the word ??? will be identified. 
After the input syllables have been processed by 
the NVEF word-pair identifier, the NCN word 
identifier, and the specific-monosyllabic word 
identifier, the remaining unconverted syllables 
will be segmented in a right-to-left fashion using 
the LS-NVWF strategy in the following process. 
(3) Unique-syllabic word identifier: When 
a given syllable word maps to exactly one word 
in the common MRD, we call the mapped word 
in MRD a unique-syllabic word, e.g. the word 
? ? ? ? /yin1 yue4 hui4/?. These 
unique-syllabic words will be identified directly 
from right to left. 
(4) Non-unique-syllabic word identifier: 
This identifier is used at the very end to deal 
with those remaining unconverted syllables. It is 
an n-gram based approach. Define the NVEF 
frequency to be the number of sentences 
including a given NVEF word-pair in the 
training corpus. First of all, the identifier will 
select, from the sentence-NVEF tree, the NVEF 
word-pair having the largest NVEF frequency as 
the main NVEF word-pair. Recall that the 
unconverted syllables have been segmented by 
the longest-syllable-word-first strategy from 
right to left. Finally, it will convert each 
segmented syllable word to its corresponding 
word by the following steps: (a) find all 
distinctly mapped words of a given syllable 
word from the common MRD, (b) compute the 
co-occurrence frequency of each mapped word 
with the key NVEF word-pair one-by-one in 
descending order of mapped words? frequencies, 
(c) whenever the co-occurrence frequency is 
greater than 0, then convert the given syllable 
word to this mapped word, (d) if all the 
computed co-occurrence frequencies in step (b) 
are 0, the given syllable word will be convert to 
its mapped word with the largest word 
frequency. 
Take the non-unique syllable word ?jin4? in 
Table A1 as example, the list of its mapped 
words in descending order of word frequency 
were ??(enter)/212,481?, ??(near)/115,913?, 
??(exhaustive)/58,387? , ??(forbid)/17,702?, 
??(strongly)/8,089?, ??(Jin Dynasty)/4,524?, 
?? (soak)/1,677? , ? ? (cinder)/722? , ? ?
(Jin)/114? and ? ? (red silk)/41.? Since the 
co-occurrence frequency of the mapped words 
with the key NVEF word-pair ??? - ??
(locale, enter)? is first greater than 0 at the word 
??(near)?, the non-unique syllabic word ?jin4? 
will be converted to the word ??.? 
3. Experimental Results 
Define the STW accuracy to be the ratio of the # 
of correct characters identified over the total # of 
characters. We use the inverse translator of 
phoneme-to-character system in [3] to convert a 
test sentence into a syllable sequence, then apply 
our STW algorithm to convert this syllable 
sequence back to characters and calculate its 
accuracy.  
If a sentence contains an NVEF word-pair, this 
sentence is called an NVEF identified sentence. 
Since the purpose of this study is to demonstrate 
the effect of applying NVEF word-pair identifier 
to the STW conversion, we shall focus on 
converting NVEF identified sentences. 
10,000 NVEF identified sentences are randomly 
selected from the test sentences in the KR-tree to 
be the closed test set; and another 10,000 
sentences are randomly selected from Sinica 
corpus to be the open test set. Note that 
sentences in open test set are not necessarily 
NVEF identified sentences. 
The results of the STW experiment are shown in 
Table 2 listed in three columns: (1) the NVEF 
word-pair identifier; (2) the other four 
sub-identifiers for the non-NVEF protion; and (3) 
the combination of (1) and (2).  
 
Table 2. The results of the STW experiment. 
    (1)       (2)    (3)  
Closed test 99.76%    94.65% 97.10% 
Open test 99.55%    93.64% 95.97% 
Average  99.66%    94.08% 96.50% 
  
For more details, the accuracies of the four 
identifiers in Section 2.4 are listed in Table 3 
below. 
 
Table 3. The STW accuracies of the four 
sub-identifiers for the non-NVEF portion 
(1) (2) (3) (4)  
Closed test   100.00% 94.68% 97.45% 89.01% 
Open test     97.25% 94.02% 97.37% 86.10% 
Average       98.31% 94.32% 97.41% 87.35% 
4. Conclusions and Directions for Future 
Research 
In this paper, we have applied an NVEF 
word-pair identifier to the Chinese STW 
conversion problem and obtained excellent rates 
as shown in Table 2. The knowledge used in this 
study includes: (1) the NVEF knowledge, (2) the 
CN word-pair list, (3) the top 3 preferred POSs 
following or preceding the 
specific-monosyllabic words, (4) the 
unique-syllabic word list and (5) the 
co-occurrence frequency of words with a 
selected key NVEF word-pairs. Besides the 
NVEF knowledge in (1), which can be (and has 
been) generated semi-automatically, the other 
knowledge can all be trained automatically. 
Our database for the NVEF knowledge has not 
been completed at the time of this writing. The 
NVEFs are constructed by selecting a 
noun-sense in Hownet and searching for 
meaningful verb-sense associated with it. 
Currently, only 66.34% (=6,641/10,011) of the 
noun-senses in Hownet have been considered in 
the NVEF knowledge construction. This results 
in 167,203 NVEF subclass sense-pairs and 
317,820 NVEF word-pairs created in the 
KR-tree. In the training corpus, about 50% of 
the sentences includes at least one NVEF 
word-pair in it.  
Based on this experiment, we find that the 
NVEF-based approach has the potential to 
provide the following information for a given 
syllable sequence: (1) well-segmented Chinese 
sentence, (2) sentence-NVEF tree including 
main verbs, nouns, NVEF word-pairs, NVEF 
sense-pairs, NVEF phrase-boundaries, and (3) 
the CN word-pairs. This information will likely 
be useful for general NLP, especially for 
sentence understanding.  
The NVEF knowledge is a general linguistic 
key-feature for sentence analysis. We are 
encouraged to note that the NVEF knowledge 
can achieve a high STW accuracy of 99.66% for 
the NVEF related portion. Our NVEF word 
identifier can be easily integrated with other 
existing STW conversion systems by using the 
NVEF word identifier as a first round filter, 
namely, identifying words in the NVEF related 
portion (thus, providing a good fundamental 
basis) and leaving the remaining unconverted 
syllables to other systems with a good potential 
to enhance their accuracies. 
We shall continue our work on covering all the 
noun-senses in Hownet for the NVEF 
knowledge construction. This procedure can 
now be done fully automatically with 99.9% of 
confidence. The study of NVEF will also be 
extended to that of other co-occurrence 
restrictions such as noun-noun (NN) pairs, 
noun-adjective (NA) pairs and verb-adverb (ND) 
pairs. Note, however, that the study of these 
latter pairs will be much simpliefied once the 
key-feature NVEFs of a sentence have been 
correctly extracted. We shall also try to improve 
our NVEF-based approach for the STW 
conversion and further extend it to speech 
recognition. 
The results in [15] indicate that the NVEF 
knowledge can also be used effectively for word 
sense disambiguation. In the future, we shall 
apply the NVEF knowledge to other fields of 
NLP, in particular, document classification, 
information retrieval, question answering and 
speech understanding. 
References 
1. Huang, J. K. 1985. The Input and Output of Chinese 
and Japanese Characters. IEEE Computer, 18(1):18-24. 
2. Chang, J.S., S.D. Chern and C.D. Chen. 1991. Conver-
sion of Phonemic -Input to Chinese Text Through Con-
straint Satisfaction. Proceedings of ICCPOL'91, 30-36. 
3. Hsu, W. L. and K.J. Chen. 1993. The Semantic Analy-
sis in GOING - An Intelligent Chinese Input System. 
Proceedings of the Second Joint Conference of Com-
putational Linguistics, Shiamen, 338-343. 
4. Hsu, W. L. and Y.S. Chen. 1999. On Pho-
neme-to-Character Conversion Systems in Chinese 
Processing. Journal of Chinese Institute of Engineers, 
5:573-579. 
5. Lua, K.T. and K.W. Gan. 1992. A Touch-Typing Pin-
yin Input System. Computer Processing of Chinese and 
Oriental Languages, 6:85-94. 
6. Sproat, R. 1990. An Application of Statistical Optimi-
zation with Dynamic Programming to Phone-
mic-Input-to-Character Conversion for Chinese. Pro-
ceedings of ROCLING III, 379-390. 
7. Chen, B., H. M. Wang and L. S. Lee. 2000. Retrieval of 
broadcast news speech in Mandarin Chinese collected 
in Taiwan using syllable -level statistical characteristics. 
Proceedings of the 2000 International Conference on 
Acoustics Speech and Signal Processing. 
8. Hsu, W. L. 1994. Chinese parsing in a pho-
neme-to-character conversion system based on seman-
tic pattern matching. Computer Processing of Chinese 
and Oriental Languages, 8(2):227-236. 
9. Kuo, J. J. 1995. Phonetic -input-to-character conversion 
system for Chinese using syntactic connection table and 
semantic distance. Computer Processing and Oriental 
Languages, 10(2):195-210. 
10.  Lin, M. Y. and W. H. Tasi. 1987. ?Removing the am-
biguity of phonetic Chinese input by the re laxation 
technique,? Computer Processing and Oriental Lan-
guages, 3(1):1-24. 
11.  Gu, H. Y., C. Y. Tseng and L. S. Lee. 1991. Markov 
modeling of mandarin Chinese for decoding the pho-
netic sequence into Chinese characters. Computer 
Speech and Language, 5(4):363-377. 
12.  Ho, T. H., K. C. Yang, J. S. Lin and L. S. Lee. 1997. 
Integrating long-distance language mode ling to pho-
netic-to-text conversion. Proceedings of ROCLING X 
International Conference on Computational Linguistics, 
287-299. 
13.  Fong, L. A. and K.H. Chung. 1994. Word Segmenta-
tion for Chinese Phonetic Symbols. Proceedings of In-
ternational Computer Symposium, 911-916. 
14.  Chen, C. G., K. J. Chen and L. S. Lee. 1986. A model 
for Lexical Analysis and Parsing of Chinese Sentences. 
Proceedings of 1986 International Conference on Chi-
nese Computing, Singapore, 33-40. 
15.  Tsai, J. L, W. L. Hsu and J. W. Su. 2002. Word sense 
disambiguation and sense-based NV event-frame iden-
tifier. Computational Linguistics and Chinese Lan-
guage Processing, 7(1):29-46. 
16.  Dong, Z. and Q. Dong, Hownet, 
http://www.keenage.com/ 
17.  CKIP. 1995. Technical Report no. 95-02, the content 
and illustration of Sinica corpus of Academia Sinica. 
Institute of Information Science, Academia Sinica, 
http://godel.iis.sinica.edu.tw/CKIP/r_content.html 
18.  Mei, J. et al 1982. Tong2Yi4Ci2Ci2Lin2 ?????
??, Shanghai Dictionary Press. 
19.  Taiwan?s Ministry of Education. 1998. 
Guo2Yu2Ci2Dian3 (Electronic Chinese Dictio nary),  
http://www.edu.tw/mandr/clc/dict/ 
20.  Tsai, C. T. (2001) A Review of Chinese Word Lists 
Accessible on the Internet. Chih-Hao Tsai Research 
Page  
http://www.geocities.com/hao510/wordlist/. 
21.  On-Line United Daily News, 
http://udnnews.com/NEWS/ 
22. Huang, C. R. et al 1996. Readings in Chinese Natural 
Language Processing. Journal of Chinese Linguistics, 
9:1-174.
 
SOAT: A Semi-Automatic Domain Ontology Acquisition Tool  
from Chinese Corpus 
Shih-Hung WU 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
shwu@iis.sinica.edu.tw 
Wen-Lian HSU 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
hsu@iis.sinica.edu.tw 
 
Abstract  
In this paper, we focus on the domain ontology 
acquisition from Chinese corpus by extracting 
rules designed for Chinese phrases. These rules 
are noun sequences with part-of-speech tags. 
Experiments show that this process can construct 
domain ontology prototypes efficiently and 
effectively. 
1. Introduction 
Domain ontology is important for large-scale 
natural language application systems such as 
speech recognition (Flett & Brown 2001), 
question answering (QA), knowledge 
management and organization memory 
(KM/OM), information retrieval, machine 
translation (Guarino 1998), and grammar 
checking systems (Bredenkamp 2000). With the 
help of domain ontology, software systems can 
perform better in understanding natural language. 
However, building domain ontology is laborious 
and time consuming. 
Previous works suggest that ontology 
acquisition is an iterative process which includes 
keyword collection as well as structure 
reorganization. The ontology will be revised, 
refined, and filled in detail during iteration. (Noy 
and McGuinness 2001) For example (Hearst 
1992), in order to find a hyponym of a keyword, 
the human editor must observe sentences 
containing this keyword and its related 
hyponyms. The editor then deduces rules for 
finding more hyponyms of this keyword. As 
such cycle iterates, the editor refines the rules to 
obtain better quality pairs of keyword-hyponyms. 
In this work we try to speed up the above 
labor-intensive approach by designing 
acquisition rules that can be applied recursively. 
A human editor only has to verify the results of 
the acquisition.  
The extraction rules we specified are templates 
of part-of-speech (POS) tagged phrase structure. 
Parsing a phrase by POS tags (Abney 1991) is a 
well-known shallow parsing technique, which 
provides the natural language processing 
function for different natural language 
applications including ontology acquisition 
(Maedche and Staab 2000). 
In previous works (Hsu et al 2001), we have 
constructed a knowledge representation 
framework, InfoMap, to integrate various 
linguistic knowledge, commonsense knowledge 
and domain knowledge. InfoMap is designed to 
perform natural language understanding. It has 
been applied to many application domains, such 
as QA system and KM/OM (Wu et al 2002) and 
has obtained encouraging results. An important 
characteristic of InfoMap is to extract events 
from a sentence by capturing the topic words, 
usually noun-verb (NV) pairs or noun-noun (NN) 
pairs, which is defined in domain ontology. We 
design the SOAT as a semi-automatic domain 
ontology acquisition tool following the ontology 
framework, InfoMap. 
We shall review the InfoMap ontology 
framework in section 2. The domain ontology 
acquisition process and extraction rules will be 
discussed in Section 3. Experimental results are 
reported in section 4. We conclude our work in 
Section 5. 
2. The InfoMap Framework 
Gruber defines an ontology to be a description 
of concepts and relationships (Gruber 1993). 
Our knowledge representation scheme, InfoMap, 
can serve as an ontology framework. InfoMap 
provides the knowledge necessary for 
understanding natural language related to a 
 
certain knowledge domain. Thus, we need to 
integrate various linguistic knowledge, 
commonsense knowledge and domain 
knowledge in making inferences. 
2.1 The Structure of InfoMap 
InfoMap consists of domain concepts and their 
associated attributes, activities, etc., which are 
its related concepts. Each of the concepts forms 
a tree-like taxonomy. InfoMap defines 
?reference? nodes to connect nodes on different 
branches, thereby integrating these concepts into 
a semantic network.  
InfoMap not only classifies concepts, but also 
classifies the relationships among concepts. 
There are two types of nodes in InfoMap: 
concept nodes and function nodes. The root 
node of a domain is the name of the domain. 
Following the root node, topics are found in this 
domain that may be of interest to users. These 
topics have sub-categories that list related 
sub-topics in a recursive fashion. 
2.2 Function Nodes in InfoMap 
InfoMap uses function nodes to label different 
relationships among related concept nodes. The 
basic function nodes are: category, attribute, 
synonym, and activity, which are described 
below. 
1. Category: Various ways of dividing up a 
concept A. For example, for the concept of 
?people?, we can divide it into young, mid-age 
and old people according to ?age?. Another 
way is to divide it into men and women 
according to ?sex?, or rich and poor people 
according to ?wealth?, etc. For each such 
partition, we shall attach a ?cause?. Each such 
division can be regarded as an angle of 
viewing concept A. 
2. Attribute: Properties of concept A. For 
example, the attributes of a human being can 
be the organs, the height, the weight, hobbies, 
etc. 
3. Associated activity: Actions that can be 
associated with concept A. For example, if A is 
a ?car?, then it can be driven, parked, raced, 
washed, repaired, etc. 
4. Synonym: Expressions that are synonymous 
to concept A in the context. 
2.3 The Contextual View of InfoMap 
Generally speaking, an ontology consists of 
definitions of concepts, relations and axioms. A 
well known ontology, WordNet (Miller 1990), 
has the following features: hypernymy, 
hyponymy, antonymy, semantic relationship, 
and synset. Comparing with the globlal view of 
concepts in WordNet, InfoMap defines category, 
event, atttibute, and synonym in a more 
contextual fashion. For example, the synonym of 
a concept in InfoMap is valid only in this 
particular context. This is very different from the 
synset in WordNet. Each node B underneath a 
function node (synonym, attribute, activity or 
category) of A can be treated as a related concept 
of A and can be further expanded by describing 
other relations pertaining to B. However, the 
relations for B described therein will be ?limited 
under the context of A?. For example, if A is 
?organization? and B is the ?facility? attribute of 
A, then underneath the node B we shall list those 
facilities one can normally find in an 
organization, whereas for the ?facility? attribute 
of ?hotel?, we shall only list those existing 
facilities in hotel. 
2.4 The Inference Engine of InfoMap 
The kernel program can map a natural language 
sentence into a set of nodes and uses the edited 
knowledge to recognize the events in the user?s 
sentences. Technically, InfoMap matches a 
natural language sentence to a collection of 
concept notes. There is a firing mechanism that 
finds nodes in InfoMap relevent to the input 
sentence. Suppose we want to find the event of 
the following sentence: ?How do I invest in 
stocks?? and the interrogative word ?how? can 
fire the word ?method?. Then along the path 
from ?method? to ?stock? the above sentence 
has fired the concepts ?stock? and ?invest?. 
Thus, the above sentence will correspond to the 
path:  
stock - event - invest - attribute - method 
Given enough knowledge about the events 
related to the main concept, InfoMap can be 
used to parse Chinese sentences. Readers can 
refer to (Hsu et al 2001) for a thorough 
description of InfoMap. 
 
3. Automatic Domain Ontology Acquisition 
To build an ontology for a new domain, we need 
to collect domain keywords and find the 
relationships among them. An acquisition 
process, SOAT, is designed that can construct a 
new ontology through domain corpus. Thus, 
with little human intervention, SOAT can build 
a prototype of the domain ontology. 
As described in previous sections, InfoMap 
consists of two major relations among concepts, 
i.e., Taxonomic relations (category and synonym) 
and Non-taxonomic relations (attribute and 
event). We defined sentence templates, which 
consists of patterns of keywords and variables, 
to capture these relations. 
3.1 Description of SOAT 
Given the domain corpus with the POS tag, our 
SOAT can be described as follows. 
Input: domain corpus with the POS tag 
Output: domain ontology prototype 
Steps: 
1 Select a keyword (usually the name of the 
domain) in the corpus as the seed to form 
a potential root set R 
2 Begin the following recursive process:  
2.1 Pick a keyword A as the root from R 
2.2 Find a new related keyword B of the root 
A by extraction rules and add it into the 
domain ontology according to the rules.   
2.3 If there is no more related keywords, 
remove A from R 
2.4 Put B into the potential root set 
2.5 Repeat step 2, until either R becomes 
empty or the total number of nodes 
generated exceeds a prescribed threshold. 
We find that most of the domain keywords are 
not in the dictionary. So the traditional TF/IDF 
method would fail. Instead, we use the high 
frequency new words discovered by PAT-tree as 
the seeds. Ideally, SOAT can generate an 
domain ontology prototype automatically. 
However, the extraction rules need to be refined 
and updated by a human editor. The details of 
SOAT extraction rules are in Section 3.2. 
3.2 The Extraction Rules of SOAT 
The extraction rules in Tables 1, 2, 3 and 4, 
consists of a specific noun as the root, and the 
POS tags of the neighboring words. A rule is a 
linguistic template for finding keywords related 
to the root. The target of extraction is usually a 
word or a compound word, which has strong 
semantic links to the root. Our rules are 
especially effective in identifying essential 
compound words for a specific domain. 
We use POS tags defined by CKIP (CKIP 1993), 
in which Na is the generic noun, Nb is the 
proper noun, and Nc is the toponym. Generally, 
an Na can be a subject or an object in a sentence, 
including concrete noun and abstract noun, such 
as ?cloth?, ?table?, ?tax?, and ?technology?. An 
Nc is the name of a place. Readers can refer to 
CKIP (CKIP 1993) for more information about 
the POS tag. In our experiment, we focus on Na 
and Nc, because the topics that we are interested 
in usually fall in these two categories. The 
extraction rules of finding categorical (taxonomy) 
relationships from a given Na (or Nc) are in 
Table 1 (and 3). The rules of finding attribute 
(non-taxonomy) relationships from a given Na 
(or Nc) are in Table 2 (and 4). 
Table 1. Category extraction rules of an Na noun  
Extraction rule Extraction 
target 
Example 
A+Na?root? A ???A????Na? 
Na+Na?root? Na ???Na????Na?
Nb+Na?root? Nb ???Nb????Na? 
Nc+Na?root? Nc ???Nc????Na?
Ncd+Na?root? Ncd  
VH+Na?root? VH ???VH????Na?
Nc+Nc+Na
?root? 
Nc+Nc ???Nc????Nc?
???Na? 
Na+Na+Na
?root? 
Na+Na ????Na????Na?
???Na? 
VH+Na+Na
?root? 
VH+Na ???VH????Na?
???Na? 
Table 2. Attribute extraction rules of an Na noun 
Extraction rule Extraction 
target 
Example 
Na?root?+Na Na ???Na????Na?
Na?root?+Nc Nc ???Na????Nc?
Na?root?+ DE 
+Na 
Na ???Na??(DE)??
??Na? 
Table 3. Category extraction rules of an Nc noun  
Extraction rule Extraction 
target 
Example 
A+Nc?root? A ???A????Nc Root? 
Na+Nc?root? Na ???Na????Nc Root? 
Nb+Nc?root? Nb ???Nb????Nc Root? 
Nc+Nc?root? Nc ???Nc????Nc Root? 
Ncd+Nc?root? Ncd  
VH+Nc?root? VH ???VH????Nc 
 
Root? 
Na+Nb+Nc
?root? 
Na+Nb ???Na?????Nb?
???Nc Root? 
Nb+Na+Nc
?root? 
Nb+Na ???Nb????Na?
???Nc Root? 
Nb+VH+Nc
?root? 
Nb+VH  
Nc+A+Nc?root? Nc+A ???Nc????A??
??Nc? 
Nc+FW+Nc
?root? 
Nc+FW  
Nc+Na+Nc
?root? 
Nc+Na ????Nc????Na?
???Nc Root? 
Nc+Nb+Nc
?root? 
Nc+Nb ???Nc????Nb?
???Nc Root? 
Nc+VC+Nc
?root? 
Nc+VC ???Nc????VC?
???Nc Root? 
Nc+Nc+Na+Nc
?root? 
Nc+Nc+Na ???Nc????Nc?
???Na????Nc Root? 
Nc+Nc+VC+Nc
?root? 
Nc+Nc+VC ???Nc????Nc?
???VC????Nc 
Root? 
Table 4. Attribute extraction rules of an Nc noun 
Extraction rule Extraction 
target 
Example 
Nc?root?+Na Na ??????Nc???
?Na? 
Nc?root?+Nc Nc ??????Nc????
?Nc? 
Nc ? root ?
+Nc+Nc 
Nc+Nc ??????Nc????
?Nc???????Nc? 
Nc ? root ?
+DE+Na 
Na ??????Nc??(DE)
????Na? 
4. Discussion 
Li and Thompson (1981) describe Mandarin 
Chinese as a Topic-prominent language in which 
the subject or the object is not as obvious as in 
other languages. Therefore, the highly precise 
shallow parsing result (Munoz et al 1999) on 
NN and SV pairs in English is probably not 
applicable to Chinese. 
4.1 The Experiment of Extraction Rate 
To test the qualitative and quantitative 
performance of SOAT, we design two 
experiments. We construct three domain 
ontology prototypes for three different domains 
and corpora. Table 5 shows the result in which 
the frequently asked questions (FAQs) for stocks 
are taken from test sentences of the financial QA 
system. The university and bank corpora are 
collected from the CKIP corpus (CKIP 1995). 
We select sentences containing the keyword 
?University? or ?Bank? as the domain corpora. 
The results in Table 5 show that SOAT can 
capture related keywords and the relationships 
among them from limited sentences very 
efficiently without using the frequency. 
Table 5. The Extraction Rate in Different Domains 
Domains  
Stock University Bank 
Corpus FAQ 
question 
CKIP 
corpus 
CKIP 
corpus 
Sentences : S 3385 3526 785 
Extrated Nodes : N 1791 2800 120 
Extration Rate : N/S 0.53 0.79 0.15 
4.2 Results from Different Corpora 
We select three different corpora from different 
information resources in the ?network? domain. 
The first corpus is a collection of FAQ sentences 
about computer network. The second corpus is a 
collection of sentences containing the keyword 
?network? from the CKIP corpus. The third 
corpus is the collection of sentences from 
Windows 2000 online help document. To reduce 
the cost of human verification, we limit the size 
of corpus to 275 sentences. The result in Table 6 
shows that there is a trade-off between 
extraction rate and the accuracy rate.  
Table 6. The extraction and accuracy rate of three 
corpora in the same domain 
 Network Domain 
Corpus FAQs  CKIP 
corpus 
Online help 
documents 
Sentences : S 275 275 275 
Extracted Nodes : N 25 180 73 
Extraction Rate : N/S 0.09 0.65 0.27 
Human verified: H 19 25 45 
Accuracy rate : H/N 0.76 0.14 0.62 
4.3 The Advantage of a Semi-Automatic 
Domain Ontology Editor for QA System 
SOAT can help in QA system ontology editing. 
In our experience, a trained knowledgeable 
editor can compile about 100 FAQs into our 
ontology manually per day. On the other hand, 
with the help of SOAT, a knowledgeable editor 
can edit on the average 4 categories, 25 
attributes and 42 activities that SOAT extracted. 
 
The quantity is estimated on 4*(25+42)=268 
FAQ query concepts at least. Thus, the 
productivity of using SOAT is approximated 
268% times. It is obvious that SOAT can help 
reducing the cost of building a new domain 
ontology. 
5. Conclusion 
We present a semi-automatic process of domain 
ontology acquisition from domain corpus. The 
ontology schema we used is general enough for 
different applications and specific enough for the 
task of understanding the Chinese natural 
language. The main objective of the research is to 
extract useful relationships from domain articles 
to construct domain ontology prototypes in a 
semi-automatic fashion. The SOAT extraction 
rules we developed can identify keywords with 
strong semantic links, especially those compound 
words in the domain. 
We have discussed how to extract related NN 
pairs in Section 3 for SOAT. However, the 
extraction rules for NN pairs do not apply for NV 
pairs. In the future we shall follow the approach 
in (Tsai et al 2002) to extract the relationships 
between nouns and its related verbs.  
The main restriction of SOAT is that the quality 
of the corpus must be very high, namely, the 
sentences are accurate and abundant enough to 
include most of the important relationships to be 
extracted.  
References  
Abney, S.P. (1991), Parsing by chunks. In Berwick, 
R.C., Abney, S.P. and Tenny, C. (ed.), 
Principle-based parsing: Computation and 
Psycholinguistics, pp. 257-278. Kluwer, 
Dordrecht. 
Bredenkamp, A., Crysmann, B., and Petrea, M. 
(2000), Looking for Errors: A declarative 
formalism for resource-adaptive language 
checking, Proceedings of the 2nd International 
Conference on Language Resources and 
Evaluation, Athens, Greece. 
CKIP (1993), Chinese Part-of-speech analysis, 
Technical Report 93-05, Academia Sinica, 
Taipei. 
CKIP (1995), A Description to the Sinica Corpus, 
Technical Report 95-02, Academia Sinica, 
Taipei. 
Flett, A. and Brown, M. (2001), 
Enterprise-standard Ontology Environments for 
Intelligent E-Business,  Proceedings of 
IJCAI-01 Workshop on E-Business & the 
Intelligent Web, Seattle, USA. 
Gruber, T.R. (1993), A translation approach to 
portable ontologies. Knowledge Acquisition, 
5(2), pp. 199-220, 1993. 
Guarino, N. (1998), Formal Ontology and 
Information Systems, Proceedings of the 1st 
International Conference on Formal Ontologies 
in Information Systems, FOIS'98, Trento, Italy, 
pp. 3-15. IOS Press. 
Hearst, M.A. (1992), Automatic acquisition of 
hyponyms from large text corpora. In 
COLING-92, pp. 539-545. 
Hsu, W.L., Wu, S.H. and Chen, Y.S. (2001), Event 
Identification Based On The Information Map - 
InfoMap, in symposium NLPKE of the IEEE 
SMC Conference, Tucson Arizona, USA. 
Li, C.N. and S.A. Thompson (1981), Mandarin 
Chinese: a functional reference grammar, 
University of California press. 
Maedche, A. and Staab, S. (2000), Discovering 
Conceptual Relations from Text. In: Horn, W. 
(ed.): ECAI 2000. Proceedings of the 14th 
European Conference on Artificial Intelligence, 
IOS Press, Amsterdam.  
Munoz, M., Punyakanok, V., Roth, D., Zimak, D. 
(1999), A Learning Approach to Shallow 
Parsing, Proceedings of EMNLP-WVLC'99. 
Noy, N.F. and McGuinness D.L. (2001), Ontology 
Development 101: A Guide to Creating Your 
First Ontology, SMI technical report 
SMI-2001-0880, Stanford Medical Informatics. 
Tsai, J. L, Hsu, W.L. and Su, J.W. (2002), Word 
sense disambiguation and sense-based NV 
event-frame identifier. Computational 
Linguistics and Chinese Language Processing, 
7(1), pp. 1-18. 
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L. 
(2002), FAQ-centered Organizational Memory, 
in Matta, N. and Dieng-Kuntz, R. (ed.), 
Knowledge Management and Organizational 
Memories, Kluwer Academic Publishers. 
 
Using Maximum Entropy to Extract Biomedical Named Entities
without Dictionaries
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu
Institute of Information Science, Academia Sinica
Nankang, Taipei, Taiwan 115
{thtsai, cwwu, hsu}@iis.sinica.edu.tw
Abstract
Current NER approaches include:
dictionary-based, rule-based, or ma-
chine learning. Since there is no
consolidated nomenclature for most
biomedical NEs, most NER systems
relying on limited dictionaries or rules
do not perform satisfactorily. In this
paper, we apply Maximum Entropy
(ME) to construct our NER framework.
We represent shallow linguistic infor-
mation as linguistic features in our ME
model. On the GENIA 3.02 corpus, our
system achieves satisfactory F-scores
of 74.3% in protein and 70.0% overall
without using any dictionary. Our
system performs significantly better
than dictionary-based systems. Using
partial match criteria, our system
achieves an F-score of 81.3%. Using
appropriate domain knowledge to
modify the boundaries, our system has
the potential to achieve an F-score of
over 80%.
1 Introduction
Biomedical literature available on the web has ex-
perienced unprecedented growth in recent years.
Therefore, demand for efficiently processing
these documents is increasing rapidly. There has
been a surge of interest in mining biomedical
literature. Some possible applications for such
efforts include the reconstruction and prediction
of pathways, establishing connections between
genes and disease, finding the relationships be-
tween genes, and much more.
Critical tasks for biomedical literature min-
ing include named entity recognition (NER), to-
kenization, relation extraction, indexing and cate-
gorization/clustering (Cohen and Hunter, 2005).
Among these technologies, NER is most fun-
damental. It is defined as recognizing objects
of a particular class in plain text. Depending
on required application, NER can extract objects
ranging from protein/gene names to disease/virus
names.
In general, biomedical NEs do not follow any
nomenclature (Shatkay and Feldman, 2003) and
can comprise long compound words and short ab-
breviations (Pakhomov, 2002). Some NEs con-
tain various symbols and other spelling variations.
On average, any NE of interest has five synonyms.
Biomedical NER is a challenging problem. There
are many different aspects to deal with. For ex-
ample, one can have unknown acronyms, abbre-
viations, or words containing hyphens, digits, let-
ters, and Greek letters; Adjectives preceding an
NE may or may not be part of that NE depend-
ing on the context and applications; NEs with the
same orthographical features may fall into differ-
ent categories; An NE may also belong to mul-
tiple categories intrinsically; An NE of one cate-
gory may contain an NE of another category in-
side it.
To tackle these challenges, researchers use
three main approaches: dictionary-based, rule-
based, and machine learning. In biomedical do-
main, there are more and more well-curated re-
sources, including lexical resources such as Lo-
268
cusLink (Maglott, 2002) and ontologies such as
MeSH (NLM, 2003). One might think that
dictionary-based systems relying solely on these
resources could achieve satisfactory performance.
However, according to (Pakhomov, 2002), they
typically perform quite poorly, with average re-
call rates in the range of only 10-30%. Rule-based
approaches, on the other hand, are more accurate,
but less portable across domains. Therefore, we
chose the machine learning approach.
Various machine learning approaches such as
ME (Kazama et al, 2002), SVM (Kazama et al,
2002; Song et al, 2004), HMM (Zhao, 2004) are
applied to NER. In this paper, we chose ME as
our framework since it is much easier to represent
various features in such a framework. In addi-
tion, ME models are flexible enough to capture
many correlated features, including overlapping
and non-independent features. We can thus use
multiple features with more ease than on an HMM
system. ME-based tagger, in particular, excel at
solving sequence tagging problems such as POS
tagging (Ratnaparkhi, 1997), general English
NER (Borthwick, 1999), and Chunking (Koeling,
2000).
In this paper, we describe how to construct a
ME-based framework that can exploit shallow lin-
guistic information in the recognition of biomed-
ical named entities. Hopefully, our experience
in integrating these features may prove useful for
those interested in constructing machine learning
based NER system.
2 Maximum Entropy Based Tagger
2.1 Formulation
In the Biomedical NER problem, we regard each
word in a sentence as a token. Each token is asso-
ciated with a tag that indicates the category of the
NE and the location of the token within the NE,
for example, B c, I c where c is a category, and
the two tags denote respectively the beginning to-
ken and the following token of an NE in category
c. In addition, we use the tag O to indicate that a
token is not part of an NE. The NER problem can
then be phrased as the problem of assigning one
of 2n + 1 tags to each token, where n is the num-
ber of NE categories. For example, one way to
tag the phrase ?IL-2 gene expression, CD28, and
NF-kappa B? in a paper is [B-DNA, I-DNA, O, O,
B-protein, O, O, B-protein, I-protein].
2.2 Maximum Entropy Modeling
ME is a flexible statistical model which assigns
an outcome for each token based on its history
and features. ME computes the probability p(o|h)
for any o from the space of all possible outcomes
O, and for every h from the space of all possi-
ble histories H . A history is all the condition-
ing data that enables one to assign probabilities
to the space of outcomes. In NER, history can
be viewed as all information derivable from the
training corpus relative to the current token. The
computation of p(o|h) in ME depends on a set of
binary-valued features, which are helpful in mak-
ing predictions about the outcome. For instance,
one of our features is: when all alphabets of the
current token are capitalized, it is likely to be part
of a biomedical NE. Formally, we can represent
this feature as follows:
f(h, o) =
?
??
??
1 : if W0-AllCaps(h)=true
and o=B-protein
0 : otherwise
(1)
Here, W0-AllCaps(h) is a binary function that
returns the value true if all alphabets of the cur-
rent token in the history h are capitalized. Given a
set of features and a training corpus, the ME esti-
mation process produces a model in which every
feature fi has a weight ?i. From (Berger et al,
1996), we can compute the conditional probabil-
ity as:
p(o|h) =
1
Z(h)
?
i
?fi(h,o)i (2)
Z(h) =
?
o
?
i
?fi(h,o)i (3)
The probability is given by multiplying the
weights of active features (i.e., those fi(h, o) =
1). The weight ?i is estimated by a procedure
called Generalized Iterative Scaling (GIS) (Dar-
roch and Ratcliff, 1972). This method improves
the estimation of weights iteratively. The ME esti-
mation technique guarantees that, for every fea-
ture fi, the expected value of ?equals the empirical
expectation of ?in the training corpus.
269
As noted in (Borthwick, 1999), ME allows
users to focus on finding features that character-
izes the problem while leaving feature weight as-
signment to the ME estimation routine. When
new features, e.g., syntax features, are added to
ME, users do not need to reformulate the model as
in the HMM model. The ME estimation routine
can automatically calculate new weight assign-
ments. More complete discussions of ME includ-
ing a description of the MEs estimation proce-
dure and references to some of the many success-
ful computational linguistics systems using ME
can be found in the following introduction (Rat-
naparkhi, 1997).
2.3 Decoding
After having trained an ME model and assigned
the proper weights ?to each feature fi, decoding
(i.e., marking up) a new piece of text becomes
simple. First, the ME module tokenizes the text.
Then, for each token, we check which features are
active and combine ?i of the active features ac-
cording to Equation 2. Finally, the probability of
a tag sequence y1...yn given a sentence w1...wn
is approximated as follows:
p(o1...on|w1...wn) ?
n?
j=1
p(oj |hj) (4)
where hj is the context for word wj . The tag-
ger uses beam search to find the most probable
sequence given the sentence. Sequences contain-
ing invalid subsequences are filtered out. For in-
stance, the sequence [B-protein, I-DNA] is in-
valid because it does not contain an ending token
and these two tokens are not in the same cate-
gory. Further details on the beam search can be
found in http://www-jcsu.jesus.cam.
ac.uk/?tdk22/project/beam.html.
3 Linguistic Features
3.1 Orthographical Features
Table 1 lists some orthographical features used
in our system. In our experience, ALLCAPS,
CAPSMIX, and INITCAP are more useful than
others.
Table 1: Orthographical features
Feature name Regular Expression
INITCAP [A-Z].*
CAPITALIZED [A-Z][a-z]+
ALLCAPS [A-Z]+
CAPSMIX .*[A-Z][a-z].* |
.*[a-z][A-Z].*
ALPHANUMERIC .*[A-Za-z].*[0-9].* |
.*[0-9].*[A-Za-z].*
SINGLECHAR [A-Za-z]
SINGLEDIGIT [0-9]
DOUBLEDIGIT [0-9][0-9]
INTEGER -?[0-9]+
REAL -?[0-9][.,]+[0-9]+
ROMAN [IVX]+
HASDASH .*-.*
INITDASH -.*
ENDDASH .*-
PUNCTUATION [,.;:?!-+]
QUOTE [???]
3.2 Context Features
Words preceding or following the target word
may be useful for determining its category. Take
the sentence ?The IL-2 gene localizes to bands
BC on mouse Chromosome 3? for example. If the
target word is ?IL-2,? the following word ?gene?
will help ME to distinguish ?IL-2 gene? from the
protein of the same name. Obviously, the more
context words analyzed the better and more pre-
cise the results. However, widening the context
window quickly leads to an explosion of the num-
ber of possibilities to calculate. In our experience,
a suitable window size is five.
3.3 Part-of-speech Features
Part of speech information is quite useful for iden-
tifying NEs. Verbs and prepositions usually indi-
cate an NEs boundaries, whereas nouns not found
in the dictionary are usually good candidates for
named entities. Our experience indicates that five
is also a suitable window size. The MBT POS
tagger (Daelemans et al, 1996) is used to provide
POS information. We trained it on GENIA 3.02p
and achieves 97.85% accuracy.
3.4 Word Shape Features
NEs in the same category may look similar (e.g.,
IL-2 and IL-4). So we have come up with sim-
ple way to normalize all similar words. Accord-
ing to our method, capitalized characters are all
replaced by ?A?, digits are all replaced by ?0?,
270
Table 2: Basic statistics for the data set
Data # abs # sen # words
GENIA 3.02 2,000 18,546 472,006 (236.00/abs)
non-English characters are replaced by ? ? (un-
derscore), and non-capitalized characters are re-
placed by ?a?. For example, Kappa-B will be nor-
malized as ?Aaaaa A?. To further normalize these
words, we shorten consecutive strings of iden-
tical characters to one character. For example,
?Aaaaa A? is normalized to ?Aa A?.
3.5 Prefix and Suffix Features
Some prefixes and suffixes can provide good
clues for classifying named entities. For example,
words which end in ?ase? are usually proteins. In
our experience, the acceptable length for prefixes
and suffixes is 3-5 characters.
4 Experiment
4.1 Datasets
In our experiment, we use the GENIA version
3.02 corpus (Kim et al, 2003). Its basic statis-
tics is summarized in Table 2. Frequencies for all
NE classes in it are showed in Table 3.
4.2 Results
In Table 4, one can see that F-scores for protein
and cell-type are comparably high. We believe
this is because protein and cell type are among
the top three most frequent categories in the train-
ing set (as shown in Table 3). One notices, how-
ever, that although DNA is the second most fre-
quent category, it does not have a high F-score.
We think this discrepancy is due to the fact that
DNA names are commonly used in proteins, caus-
ing a substantial overlap between these two cate-
gories. RNAs performance is comparably low be-
cause its training set is much smaller than those
of other categories. Cell lines performance is the
lowest since it overlaps heavily with cell type and
its training set is also very small.
In Table 5, one can see that, using the par-
tial matching criterion, the precision rates, recall
rates, and F-scores of protein names are all over
85%. The overall F-Score is 81.3%. The table
also shows that 83.9% of our systems suggestions
Table 4: NER performance of each NE category
on the GENIA 3.02 data (10-fold CV)
NE category Precision Recall F-score
protein 74.1 74.5 74.3
DNA 65.9 54.4 59.6
RNA 75.3 48.0 58.6
cell line 65.4 51.4 57.6
cell type 72.3 69.1 70.7
Overall 72.0 67.9 70.0
Table 5: Partial matching performance on the GE-
NIA 3.02 corpus (10-fold CV)
NE category Precision Recall F-score
protein 85.3 85.5 85.4
DNA 80.3 66.3 72.7
RNA 84.0 53.0 65.0
cell line 80.9 63.3 71.1
cell type 83.1 79.4 81.2
Overall 83.9 78.9 81.3
correctly identify at least one part of an NE, and
that our system tags at least one part of 78.9%
of all NEs in the test corpus. The precision rate in
all categories is over 80%, showing that , by using
appropriate post-processing methods, our system
can achieve high precision in all NE categories.
In Table 6, we compare our system with two
dictionary-based systems. One exploits hand-
crafted rules based on heuristics and protein name
dictionaries (Seki and Mostafa, 2003). We de-
note this system as ?rule + dictionary?. The other
system (Tsuruoka and Tsujii, 2004) has two con-
figurations: the first one exploits patterns to de-
tect protein names and their fragments, which
is denoted as ?dictionary expansion?; the sec-
ond one further applies naive Bayes filters to ex-
clude erroneous detections, which is denoted as
?dictionary expansion + filters?. One can see
that our system performs better than these dic-
tionary/heuristic systems by a wide margin. The
basic ?rule + dictionary? system achieves only
54.4% recall. By expanding the original dic-
tionary (?dictionary expansion?), they improve
the recall rate to 68.1%. After applying post
processing filters (?dictionary expansion + fil-
ters?), the recall rate dropped slightly, but preci-
sion increased by 25.7%. Still, our system per-
forms better than the best dictionary-based system
by 7.6%.
271
Table 3: Frequencies for NEs in each data set
Data protein DNA RNA cell type cell line All
GENIA 3.02 30,269 9,533 951 6,718 3,830 51,301
Table 6: Performance comparison between sys-
tems with and w/o dictionaries in extracting pro-
tein names on the GENIA 3.02 data
System Precision Recall F-score
our system 74.1 74.5 74.3
rule + dictionary 42.6 54.4 47.8
dictionary expansion 46.0 68.1 54.8
dictionary expansion + filters 71.7 62.3 66.6
5 Analysis and discussion
Recognition disagreement between our system
and GENIA is caused by the following two fac-
tors: Annotation problems:
1. Preceding adjective problem
Some descriptive adjectives are annotated as
parts of the following NE, but some are not.
2. Nested NEs
In GENIA, we found that in some instances
only embedded NEs are annotated while in
other instances, only the outside NE is an-
notated. However, according to the GENIA
tagging guidelines, the outside NE should be
tagged. For example, in 59 instances of the
phrase ?IL-2 gene?, ?IL-2? is tagged as a
protein 13 times, while in the other 46 it is
tagged as a DNA. This irregularity can con-
fuse machine learning based systems.
3. Cell-line/cell-type confusion
NEs in the cell line class are from certain cell
types. It is difficult even for an expert to dis-
tinguish them.
System recognition errors:
1. Misclassification
Some protein molecules or regions are mis-
classified as DNA molecules or regions.
These errors may be solved by exploiting
more context information.
2. Coordinated phrases
In GENIA, most conjunction phrases are
tagged as single NEs. However, conjunc-
tion phrases are usually composed of several
NEs, punctuation, and conjunctions such as
?and?, ?or? and ?but not?. Therefore, our
system sometimes only tags one of these NE
components. For example, in the phrase ?c-
Fos and c-Jun family members?, only ?c-
Jun family members? is tagged as a protein
by our system, while in GENIA, the whole
phrase is tagged as a protein.
3. False positives
Some entities appeared without accompany-
ing a specific name, for example, only men-
tion about ?the epitopes? rather than which
kind of epitopes. The GENIA corpus tends
to ignore these entities, but their contexts are
similar to the entities with specific names,
therefore, our system sometimes incorrectly
recognizes them as an NE.
6 Conclusion
Our system successfully integrates linguistic fea-
tures into the ME framework. Without using
any biomedical dictionaries, our system achieves
a satisfactory F-score of 74.3% in protein and
70.0% overall. Our system performs significantly
better than dictionary-based systems. Using par-
tial match criteria, our system achieves an F-score
of 81.3%. That means, with appropriate bound-
ary modification algorithms (with domain knowl-
edge), our system has the potential to achieve an
F-score of over 80%.
It is still difficult to recognize long, compli-
cated NEs and to distinguish between two over-
lapping NE classes, such as cell-line and cell-
type. This is because biomedical texts have com-
plicated syntax and involve more expert knowl-
edge than general domain news articles. An-
other serious problem is annotation inconsistency,
which confuses machine learning models and
makes evaluation difficult. Certain errors, such as
those in boundary identification, are more tolera-
ble if the main purpose is to discover relationships
272
between NEs.
In the future, we will exploit more linguistic
features such as composite features and external
features. Finally, to reduce human annotation ef-
fort and to alleviate the scarcity of available anno-
tated corpora, we will develop machine learning
techniques to learn from Web corpora in different
biomedical domains.
Acknowledgements
We are grateful for the support of National Sci-
ence Council under GRANT NSC94-2752-E-
001-001.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computer Linguistics, 22:39?
71.
A. Borthwick. 1999. A Maximum Entropy Approach
to Named Entity Recognition. Phd thesis, New York
University.
K. Bretonnel Cohen and Lawrence Hunter. 2005. Nat-
ural language processing and systems biology. In
W. Dubitzky and F. Azuaje, editors, Artificial In-
telligence and Systems Biology, Springer Series on
Computational Biology. Springer.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In E. Ejerhed and I. Da-
gan, editors, Fourth Workshop on Very Large Cor-
pora, pages 14?27.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematicl Statistics, 43:1470?1480.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning support vector machines for biomedical
named entity recognition. In ACL-02 Workshop on
Natural Language Processing in Biomedical Appli-
cations.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. Genia corpus - a semanti-
cally annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1).
Rob Koeling. 2000. Chunking with maximum en-
tropy models. In CoNLL-2000.
D. Maglott. 2002. Locuslink: a directory of genes. In
NCBI Handbook, pages 19?1 to 19?16.
NLM. 2003. Mesh: Medical subject headings.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical text. In the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
A. Ratnaparkhi. 1997. A simple introduction to maxi-
mum entropy models for natural language process-
ing. Technical Report Techical Report 97-08, Insti-
tute for Research in Cognitive Science University
of Pennsylvania.
Kazuhiro Seki and Javed Mostafa. 2003. An approach
to protein name extraction using heuristics and a
dictionary. In ASIST 2003.
Hagit Shatkay and Ronen Feldman. 2003. Min-
ing the biomedical literature in the genomic era:
an overview. Journal of Computational Biology,
10(6):821?855.
Yu Song, Eunju Kim, Gary Geunbae Lee, and
Byoung-kee Yi. 2004. Posbiotm-ner in the shared
task of bionlp/nlpba 2004. In the Joint Workshop on
Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), Geneva, Switzer-
land.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. Im-
proving the performance of dictionary-based ap-
proaches in protein name recognition. Journal of
Biomedical Informatics, 37(6):461?470.
Shaojun Zhao. 2004. Named entity recognition in
biomedical texts using an hmm model. In COL-
ING 2004 International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
273
Learning Patterns from the Web to Translate Named Entities for
Cross Language Information Retrieval
Yu-Chun Wang?? Richard Tzong-Han Tsai?? Wen-Lian Hsu?
?Institute of Information Science, Academia Sinica, Taiwan
?Department of Eletrical Engineering, National Taiwan University, Taiwan
?Department of Computer Science and Engineering, Yuan Ze University, Taiwan
albyu@iis.sinica.edu.tw
thtsai@saturn.yzu.edu.tw
hsu@iis.sinica.edu.tw
?corresponding author
Abstract
Named entity (NE) translation plays
an important role in many applications.
In this paper, we focus on translating
NEs from Korean to Chinese to improve
Korean-Chinese cross-language informa-
tion retrieval (KCIR). The ideographic
nature of Chinese makes NE translation
difficult because one syllable may map to
several Chinese characters. We propose
a hybrid NE translation system. First,
we integrate two online databases to ex-
tend the coverage of our bilingual dic-
tionaries. We use Wikipedia as a trans-
lation tool based on the inter-language
links between the Korean edition and
the Chinese or English editions. We
also use Naver.com?s people search en-
gine to find a query name?s Chinese or
English translation. The second compo-
nent is able to learn Korean-Chinese (K-
C), Korean-English (K-E), and English-
Chinese (E-C) translation patterns from
the web. These patterns can be used to
extract K-C, K-E and E-C pairs from
Google snippets. We found KCIR per-
formance using this hybrid configura-
tion over five times better than that
a dictionary-based configuration using
only Naver people search. Mean average
precision was as high as 0.3385 and recall
reached 0.7578. Our method can han-
dle Chinese, Japanese, Korean, and non-
CJK NE translation and improve perfor-
mance of KCIR substantially.
1 Introduction
Named entity (NE) translation plays an impor-
tant role in machine translation, information re-
trieval, and question answering. It is a chal-
lenging task because, although there are many
online bilingual dictionaries, they usually lack
domain specific words or NEs. Furthermore,
new NEs are generated everyday, but bilingual
dictionaries cannot update their contents fre-
quently. Therefore, it is necessary to construct
a named entity translation (NET) system.
Economic ties between China and Korea have
become closer as China has opened its mar-
kets further, and demand for the latest news
and information from China continues to grow
rapidly in Korea. One key way to meet this
demand is to retrieve information written in
Chinese by using Korean queries, referred to
as Korean-Chinese cross-language information
retrieval (KCIR). The main challenge involves
translating NEs because they are usually the
main concepts of queries. In (Chen et al, 1998),
the authors romanized Chinese NEs and selected
their English transliterations from English NEs
extracted from the Web by comparing their
phonetic similarities with Chinese NEs. Yaser
Al-Onaizan (Al-Onaizan and Knight, 2002)
281
transliterated an NE in Arabic into several can-
didates in English and ranked the candidates by
comparing their counts in several English cor-
pora. Unlike the above works, whose target lan-
guages are alphabetic, in K-C translation, the
target language is Chinese, which uses an ideo-
graphic writing system. Korean-Chinese NET
is much more difficult than NET considered in
previous works because, in Chinese, one sylla-
ble may map to tens or hundreds of characters.
For example, if an NE written in Korean com-
prises three syllables, there may be thousands of
possible translation candidates in Chinese.
In this paper, we propose an effective hybrid
NET method which can help improve perfor-
mance of cross-language information retrieval
systems. We also describe the construction of
a Korean-Chinese CLIR system able to evaluate
the effectiveness of our NE translation method.
2 Difficulties in Korean-Chinese
Named Entity Translation for IR
2.1 Korean NET
Most Korean NEs originate from Hanja. There-
fore, the most straightforward way to translate
a Korean name into Chinese is to use its Hanja
equivalent. Take the name of Korea?s president,
?x4? (No Mu-hyeon), as an example. We
can directly convert it to its Hanja equivalent:
??fI? (Lu Wu-Xuan). Or in the case of the
city name ???? (Pusan/?q/Fu-shan) and
the company name ??1? (Samsung/	/San-
xing), Chinese also presents Hanja equivalents.
If the Hanja name is unknown, the name is
translated character by character. Each Hangul
character is basically translated into a corre-
sponding Hanja character. For example, the
name of the Korean actor ?px1? (Cho In-
seong) is usually translated as ???? (Zhao
Ren-cheng) because ?p? is mapped to ???, ?x?
mapped to ???, and ?1? mapped to ??. How-
ever, that translation may differ from the per-
son?s given Hanja name.
For native Korean NEs which have no cor-
responding Hanja characters, we must turn to
transliteration or convention. Take the name of
South Korea?s capital ??? (Seoul) as an ex-
ample. Before 2005, Chinese media and govern-
ment used the old Hanja name of the city ?"??
(Han-cheng), which was used during Joseon dy-
nasty (A.D. 1392?1910). However, after 2005,
Chinese switched to using the transliteration
??>? (Shou-er) instead of ?"?? at the re-
quest of the Seoul Metropolitan Government.
This example illustrate how more than one Chi-
nese translation for a Korean name is possible,
a phenomenon which, at times, makes Korean-
Chinese information retrieval more difficult.
2.2 Chinese NET
To translate a Chinese NE written in Hangul,
we begin by considering the two C-K NET ap-
proaches. The older is based on the Sino-Korean
pronunciation and the newer on the Mandarin.
For example, ??c? (Taiwan) used to be
transliterated solely as ? ?? (Dae-man). How-
ever, during the 1990s, transliteration based on
Mandarin pronunciation became more popular.
Presently, the most common transliteration for
??c? is ??tD? (Ta-i-wan), though the Sino-
Korean-based ? ?? is still widely used. For
Chinese personal names, both ways are used.
For example, the name of Chinese actor Jackie
Chan (??? Cheng-long) is variously translit-
erated as ?1?? Seong-ryong (Sino-Korean)
and ???? Cheong-rung (Mandarin).
Translating Chinese NEs by either method is
a major challenge because each Hangul charac-
ter may correspond to several different Chinese
characters that have similar pronunciations in
Korean. This results in thousands of possible
combinations of Chinese characters, making it
very difficult to choose the most widely used one
one.
2.3 Japanese NET
Japanese NEs may contain Hiraganas,
Katakanas, or Kanjis. For each character
type, J-C translation rules may be similar to
or very different from K-C translation rules.
Some of these rules are based on Japanese
pronunciation, while some are not. For NEs
composed of all Kanjis, their Chinese transla-
tions are generally exactly the same as their
Kanji written forms. In contrast, Japanese NEs
282
are transliterated into Hangul characters. Take
??K? (Nagoya) for example. Its Chinese
translation ??K? is exactly the same as
its Kanji written form, while its pronuncia-
tion (Ming Gu Wu) is very different from its
Japanese pronunciation. This is different from
its Korean translation, ???|? (Na go ya).
In this example, we can see that, because the
translation rules in Chinese and Korean are
different, it is ineffective to utilize phonetic sim-
ilarity to find the Chinese translation equivalent
to the Korean translation.
2.4 Non-CJK NET
In both Korean and Chinese, transliteration
methods are mostly used to translate non-CJK
NEs. Korean uses the Hangul alphabet for
transliteration. Because of the phonology of
Korean, some phonemes are changed during
translation because the language lacks these
phonemes. (Oh, 2003; Lee, 2003) In contrast,
Chinese transliterates each syllable in a NE into
Chinese characters with similar pronunciation.
Although there are some conventions for select-
ing the transliteration characters, there are still
many possible transliterations since so many
Chinese characters have the same pronunciation.
For instance, the name ?Greenspan? has sev-
eral Chinese transliterations, such as ?[????
(Ge-lin-si-ban) and ?[??X? (Ge-lin-si-pan).
In summary, it is difficult to match a non-CJK
NE transliterated from Korean with its Chinese
transliteration due to the latter?s variations.
3 Our Method
In this section, we describe our Korean-Chinese
NE translation method for dealing with the
problems described in Section 2. We either
translate NE candidates from Korean into Chi-
nese directly, or translate them into English first
and then into Chinese. Our method is a hybrid
of two components: extended bilingual dictio-
naries and web-based NET.
3.1 Named Entity Candidate Selection
The first step is to identify which words in a
query are NEs. In general, Korean queries are
composed of several eojeols, each of which is
composed of a noun followed by the noun?s post-
position, or a verb stem followed by the verb?s
ending. We remove the postposition or the end-
ing to extract the key terms, and then select per-
son name candidates from the key terms. Next,
the maximum matching algorithm is applied to
further segment each term into words in the
Daum Korean-Chinese bilingual dictionary1. If
the length of any token segmented from a term
is 1, the term is regarded as an NE to be trans-
lated.
3.2 Extension of Bilingual Dictionaries
Most NEs are not included in general bilingual
dictionaries. We adopt two online databases
to translate NEs: Wikipedia and Naver people
search.
3.2.1 Wikipedia
In Wikipedia, each article has an inter-
language link to other language editions, which
we exploit to translate NEs. Each NE candidate
is first sent to the Korean Wikipedia, and the
title of the matched article?s Chinese version is
treated as the NE?s translation in Chinese. How-
ever, if the article lacks a Chinese version, we use
the English edition to acquire the NE?s transla-
tion in English. The English translation is then
transliterated into Chinese by the method de-
scribed in Section 3.3.3.
3.2.2 Naver People Search Engine
Most NEs are person names that cannot all
be covered by the encyclopedia. We use Naver
people search engine to extend the coverage of
person names. Naver people search is a transla-
tion tool that maintains a database of famous
people?s basic profiles. If the person is from
CJK, the search engine returns his/her name in
Chinese; otherwise, it returns the name in En-
glish. In the former case, we can adopt the re-
turned name directly, but in the latter, we need
to translate the name into Chinese. The trans-
lation method is described in Section 3.3.3.
1http://cndic.daum.net
283
3.3 Translation Pattern from the Web
Obviously, the above methods cannot cover all
possible translations of NEs. Therefore, we pro-
pose a pattern-based method to find the trans-
lation from the Web. Since the Chinese transla-
tions of some NEs cannot be found by patterns,
we find their Chinese translations indirectly by
first finding their English translations and then
finding the Chinese translations. Therefore,
we must generate K-C patterns to extract K-C
translation pairs, as well as K-E and E-C pat-
terns to extract K-E and E-C pairs, respectively.
3.3.1 Translation Pattern Learning
Our motivation is to learn patterns for ex-
tracting NEs written in the source language and
their equivalents in the target language from
the Web. First, we need to prepare the train-
ing set. To generate K-C and K-E patterns,
we collect thousands of NEs that originated in
Korean, Chinese, Japanese, or non-CJK lan-
guages from Dong-A Ilbo (a South Korean news-
paper). Then, all the Korean NEs are translated
into Chinese manually. NEs from non-CJK lan-
guages are also translated into English. To gen-
erate E-C patterns, we collect English NEs from
the MUC-6 and MUC-7 datasets and translate
them into Chinese manually.
We submit each NE in the source language
(source NE) and its translation in the target lan-
guage as a query to Google search engine. For
instance, the Korean NE ?Tt ??? and its
translation ?Major League? are first composed
as a query ?+Bjs$o?? + Major League?,
which is then sent to Google. The search en-
gine will return the relevant web documents with
their snippets. We collect the snippets in the
top 20 pages and we break them into sentences.
Only the sentences that contain at least one
source NE and its translation are retained.
For each pair of retained sentences, we apply
the Smith-Waterman local alignment algorithm
to find the longest common string, which is then
added to the candidate pattern pool. During the
alignment process, positions where the two in-
put sequences share the same word are counted
as a match. The following is an example of a pair
of sentences that contains ?Tt ??? and its
English translation, ?Major League?:
? ?Tt ??(Major League)??@?
? ]? ??\ ?<????
? ??m Tt ??(Major League)?,?
After alignment, the pattern is generated as:
<Korean NE>(<English Translation>)?
This pattern generation process is repeated for
each NE-translation pair.
3.3.2 Translation Pattern Filtering
After learning the patterns, we have to filter
out some ineffective patterns. First, we send
a Korean NE, such as ?Tt ???, to re-
trieve the snippets in the top 50 pages. Then,
we apply all the patterns to extract the trans-
lations from the snippets. The correct rate of
each translation pattern is calculated as follows:
CorrectRate = Ccorrect/Call, where Ccorrect is
the total number of correct translations ex-
tracted by the pattern and Call is the total num-
ber of translations extracted by the pattern.
If the correct rate of the pattern is below the
threshold ? , the pattern will be dropped.
3.3.3 Pattern-Based NET
The translations of some NEs, especially from
CJK, can be found comparatively easily from
the Web. However, for other NEs, especially
from non-CJK, this is not the case. There-
fore, we split the translation process into two
stages: the first translates the NE into its En-
glish equivalent, and the second translates the
English equivalent into Chinese.
To find an NE?s Chinese translation, we first
apply the translation patterns to extract possi-
ble Chinese translations. If its Chinese transla-
tion cannot be found, the K-E patterns are used
to find its English translation instead. If its En-
glish translation can be found, the E-C patterns
are then used to find its Chinese translation.
4 System Description
We construct a Korean-Chinese cross language
information retrieval (KCIR) system to deter-
mine how our person name translation methods
affect KCIR?s performance. A Korean query is
284
translated into Chinese and then used to retrieve
Chinese documents. The following sections de-
scribe the four stages of our KCIR system. We
use an example query, ?T??X??,??, 
?? (Kosovo?s situation, NATO, UN), to demon-
strate the work flow of our system.
4.1 Query Processing
Unlike English, Korean written texts do not
have word delimiters. Spaces in Korean sen-
tences separate eojeols. First, the postposition
or verb ending in each eojeol is removed. In our
example query, we remove the possessive post-
position ?X? at the end of the first eojeol. Then,
NE candidates are selected using the method de-
scribed in Section 3.1. ?T??? (Kosovo) is
recognized as an NE, and other terms ????
(situation), ???? (NATO), and ? ?? (UN)
are general terms because they can be found in
the bilingual dictionary.
4.1.1 Query Translation
Terms not selected as NE candidates are sent
to the online Daum Korean-Chinese dictionary
and Naver Korean-Chinese dictionary2 to get
their Chinese translations. In our example, the
terms ???? (situation), ???? (NATO), and
? ?? (UN) can be correctly translated into
Chinese by the bilingual dictionaries as ??K?
(situation), ?'lDT? (NATO), and
?o? (UN), respectively.
We employ Wikipedia, Naver people search,
and the pattern-based method simultaneously to
translate the NE candidate ?T??? (Kosovo).
Up to now, there is no article about Kosovo in
Korean Wikipedia. Naver people search does
not contain an article either because it is not a
person name. Meanwhile, since the K-C transla-
tion patterns cannot extract any Chinese trans-
lations, the K-E patterns are used to get the En-
glish translations, such as ?Kosovo?, ?Cosbo?,
and ?Kosobo?. The E-C patterns are then em-
ployed to get the Chinese translation from the
three English translations. Among them, only
Chinese translations for ?Kosovo? can be found
because the other two are either wrong or rarely
2http://cndic.naver.com
used translations. The Chinese translations ex-
tracted by our patterns are ??"+? (Ke-suo-
fu), ??"? (Ke-suo-fu), and ??"?? (Ke-
suo-wuo). They are all correct transliterations.
4.2 Term Disambiguation
A Hangul word might have many meanings. Be-
sides, sometimes the translation patterns might
extract wrong translations of the NE. This phe-
nomenon causes ambiguities during information
retrieval and influence the performance of IR sig-
nificantly. To solve this problem, we adopt the
mutual information score (MI score) to evaluate
the co-relation between a translation candidate
tcij for a term qti and all translation candidates
for all the other terms in Q; tcij ?s MI score given
Q is calculated as follows:
MI score(tcij |Q) =
|Q|
?
x=1,x6=i
Z(qtx)
?
y=1
Pr(tcij , tcxy)
Pr(tcij)Pr(tcxy)
where Z(qtx) is the number of translation can-
didates of the x-th query term qtx; tcxy is y-
th translation candidate for qtx; Pr(tcij , tcxy) is
the probability that tcij and tcxy co-occur in
the same sentence; and Pr(tcij) is the proba-
bility of tcij . Next, we compute the ratio of
the each candidate?s score over the highest can-
didate?s score as follows: ScoreRatio(tcij) =
MI score(tcij |Q)/MI score(tcih|Q), where tcih is
the candidate with highest MI score from the
qti. If the candidate?s score ratio is below the
threshold ?MI, the candidate will be discarded.
Here, we use the above example to illustrate
the term disambiguation mechanism. For the
given English term ?Kosovo?, the MI scores of
??"+?, ??"?, and ??"?? are computed;
??"? achieves the highest score, while the
score ratio of the other two candidates are much
lower than the threshold. Thus, only ??"?
is treated as Kosovo?s translation and used to
build the final Chinese query to perform the IR.
4.3 Indexing and Retrieval Model
We use the Lucene information retrieval engine
to index all documents and the bigram index
based on Chinese characters. The Okapi BM25
function (Robertson et al, 1996) is used to score
285
a retrieved document?s relevance. In addition,
we employ the following document re-ranking
function (Yang et al, 2007):
?
(
?K
i=1 df(t, di)? f(i))/K
DF (t, C)/R
?
?
|t|
df(t, di) =
{
1 t ? di
0 t /? di
,
where di is the ith document; R is the total num-
ber of documents in the collection C; DF (t, C)
is the number of documents containing a term t
in C; and |t| is t?s length, f(i) = 1sqrt(i) .
5 Evaluation and Analysis
To evaluate our KCIR system, we use the topic
and document collections of the NTCIR-5 CLIR
tasks (Kishida et al, 2005). The document
collection is the Chinese Information Retrieval
Benchmark (CIRB) 4.0, which contains news
articles published in four Taiwanese newspa-
pers from 2000 to 2001. The topics have four
fields: title, description, narration, and con-
centrate words. We use 50 topics provided by
NTCIR-5 and use the title field as the input
query because it is similar to queries input to
search engines.
We construct five runs as follows:
? Baseline: using a Korean-Chinese
dictionary-based translation.
? Baseline+Extended Dictionaries only:
the baseline system plus the extended dic-
tionaries translation.
? Baseline+NET Methods: the baseline
system plus our NET methods, namely,
Wikipedia, Naver people search, and the
pattern-based method.
? Google Translation: using the Google
translation tool.
? Chinese monolingual: using the Chinese
versions of the topics given by NTCIR.
We use the Mean Average Precision (MAP)
and Recall (Saracevic et al, 1988) to evaluate
the performance of IR. NTCIR provides two
Table 1: Evaluation Results
Run MAP RecallRigid Relax Rigid Relax
Baseline 0.0553 0.0611 0.2202 0.2141
Baseline+extended
dictionaries
0.1573 0.1751 0.5706 0.5489
Baseline+NET 0.2576 0.2946 0.7255 0.7103
Google translation 0.1340 0.1521 0.5254 0.5149
Chinese mono 0.2622 0.3019 0.7705 0.7452
kinds of relevance judgments: Rigid and Re-
lax. A document is rigid-relevant if it is highly
relevant to the topic; and relax-relevant if it is
highly relevant or partially relevant to the topic.
Table 1 shows that our method improves
KCIR substantially. Our method?s performance
is about five times better than that of the base-
line system and very close to that of Chinese
monolingual IR. Wikipedia translation improves
the performance, but not markedly because
Wikipedia cannot cover some NEs. Google
translation is not very satisfactory either, since
many NEs cannot be translated correctly.
To evaluate our NE translation method, we
create two additional datasets. The first dataset
contains all the 30 topics with NEs in NTCIR-
5. To further investigate the effectiveness of
our method for queries containing person names,
which are the most frequent NEs, we construct
a second dataset containing 16 topics with per-
son names in NTCIR-5. We compare the per-
formance of our method on KCIR with that of
Chinese monolingual IR on these two datasets.
The results are shown in Tables 2 and 3.
5.1 Effectiveness of Extended Dict
We adopt two online dictionaries to extend
our bilingual dictionaries: Wikipedia and Naver
people search engine. Wikipedia is an effective
tool for translating well-known NEs. In the test
topics, NEs like ?@|?(Kim Jong-il, North
Korea?s leader), ????(Taliban), ?t??
0?(Harry Potter) and ?\?|??(Great Na-
tional Party in South Korea) are all translated
correctly by Wikipedia.
We observe that the most difficult cases in
Korean-Chinese person name translation, espe-
cially Japanese and non-CJK person names, can
286
be successfully translated by the Naver people
search engine. For example, ?T??(William
Cohen, the ex-Secretary of Defense of the U.S.)
and ?tX\?(Ichiro Suzuki, a Japanese base-
ball player). The major advantage of the Naver
people search engine is it can can provide the
original names written in Chinese characters.
According to our evaluation, the extended
dictionaries improve the IR performance of the
baseline system about threefold. It shows that
the extended dictionaries can translate part of
Korean NEs into Chinese. However, there are
still many NEs that the extended dictionaries
cannot cover.
5.2 Effectiveness of Patterns
In our method, we employ automatically learned
patterns to extract translations for the remain-
ing NEs not covered by the offline or online dic-
tionaries. For example, we can extract Chinese
translations for ?$??@?(Okinawa, in Japan)
by using K-C translation patterns. Most non-
CJK NEs can be translated correctly by us-
ing the K-E translation patterns. For exam-
ple, ??| t?D??(Jennifer Capriati),
?? ?(anthrax), and ????(mad cow dis-
ease) can be extracted from Google snippets ef-
fectively by our translation patterns.
Although our method translates some NEs
into English first and then into Chinese in an
indirect manner, it is very effective because the
non-CJK NEs in Korean are mainly from En-
glish. In fact, 16 of the 17 NEs can be suc-
cessfully translated by the two stage translation
method that employs two types of translation
patterns: K-E and E-C.
5.3 Effectiveness Analysis of NET
As shown in Table 2, for topics with NEs, the
rigid MAP of our method is very close to that
of Chinese monolingual IR, while the relax MAP
of our method is even better than that of Chi-
nese monolingual IR. We observe that 26 of the
31 NEs in the topics are successfully translated
into Chinese. These results demonstrate that
our hybrid method comprising the extended dic-
tionaries and translation patterns can deal with
Korean-Chinese NE translation effectively and
Table 2: Results on Topics with NEs
Run MAP RecallRigid Relax Rigid Relax
NET 0.2700 0.3385 0.7565 0.7578
Chinese 0.2746 0.3273 0.7922 0.7846
improve the performance of IR substantially.
Note that, our method can extract more pos-
sible Chinese translations, which is similar to
query expansion. For non-CJK NEs, there may
exist several Chinese transliterations that are ac-
tually used in Chinese, especially for the per-
son names. Take ?Tito?for example; its six
common Chinese transliterations, namely, ??
X?(di-tuo), ??X?(di-tuo), ?X?(di-tuo), ??
X?(ti-tuo), and ??X?(di-tuo) can be extracted.
With our method, the rigid MAP of this topic
achieves 0.8361, which is much better than that
of the same topic in the Chinese monolingual run
(0.4459) because the Chinese topic has only one
transliteration ?X?(di-tuo). This is the rea-
son that our method outperforms the Chinese
monolingual run in topics with NEs.
5.4 Error Analysis
NEs that cannot be translated correctly can
be divided into two categories. The first con-
tains names not selected as NE candidates. The
Japanese person name ?????? (Alberto Fu-
jimori, Peru?s ex-president) is in this category.
For the name ?????? (Fujimori), the first
two characters ???? (hind legs) and the last
two characters ???? (profiting) are all Sino-
Korean words, so it is regarded as a compound
word, not an NE. The other category contains
names with few relevant web pages, like the non-
CJK names ?H??$ ?? (Antonio Toddy).
The other problem is that our method can
translate the Korean NEs into correct Chinese
translations, but not the translation used in the
CIRB 4.0 news collection. For example, ??t?
l? (Kursk) is translated into ??>?K? (Ku-
er-si-ke) correctly, but only the transliteration
???K? (Ke-si-ke) is used in CIRB 4.0. In this
situation, the extracted translation cannot im-
prove the performance of the KCIR.
287
Table 3: Results on Topics with Person Names
Run MAP RecallRigid Relax Rigid Relax
NET 0.2730 0.3274 0.7146 0.7299
Chinese 0.2575 0.3169 0.7513 0.7708
6 Conclusion
In this paper, we have considered the difficul-
ties that arise in translating NEs from Korean
to Chinese for IR. We propose a hybrid method
for K-C NET that exploits an extended dictio-
narie containing Wikipedia and the Naver peo-
ple search engine, combined with the translation
patterns automatically learned from the search
results of the Google search engine. To eval-
uate our method, we use the topics and doc-
ument collection of the NTCIR-5 CLIR task.
Our method?s performance on KCIR is over five
times better than that of the baseline configura-
tion with only an offline dictionary-based trans-
lation module. Moreover, its overall MAP score
is up to 0.2986, and its MAP on the NE topics
is up to 0.3385 which is even better than that
of the Chinese monolingual IR system. The pro-
posed method can translate NEs that originated
in the Chinese, Japanese, Korean, and non-
CJK languages and improve the performance of
KCIR substantially. Our NET method is not
language-specific; therefore, it can be applied to
the other CLIR systems beside K-C IR.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating named entities using monolingual and bilin-
gual resources. Proceedings of the 40th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 400?408.
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding,
and Shih-Cbung Tsai. 1998. Proper name transla-
tion in cross-language information retrieval. Pro-
ceedings of 17th COLING and 36th ACL, pages
232?236.
Kazuaki Kishida, Kuang hua Chen, Sukhoon Lee,
Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen,
and Sung Hyon Myaeng. 2005. Overview of clir
task at the fifth ntcir workshop. Proceedings of the
Fifth NTCIR Workshop.
Juhee Lee. 2003. Loadword phonology revisted: Im-
plications of richness of the base for the analysis
of loanwords input. Explorations in Korean Lan-
guage and Linguistics, pages 361?375.
Mira Oh. 2003. English fricatives in loanword adap-
tion. Explorations in Korean Language and Lin-
guistics, pages 471?487.
S.E. Robertson, S. Walker, MM Beaulieu, M. Gat-
ford, and A. Payne. 1996. Okapi at trec-4. Pro-
ceedings of the Fourth Text Retrieval Conference,
pages 73?97.
Tefko Saracevic, Paul Kantor, Alice Y. Chamis, and
Donna Trivison. 1988. A study of information
seeking and retrieving. Journal of the American
Society for Information Science, 39(3):161?176.
L. Yang, D. Ji, and M. Leong. 2007. Docu-
ment reranking by term distribution and maxi-
mal marginal relevance for chinese information re-
trieval. Information Processing and Management:
an International Journal, 43(2):315?326.
288
 
Text Categorization Using Automatically Acquired Domain Ontology
 
 
Shih-Hung Wu, Tzong-Han Tsai, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
shwu@iis.sinica.edu.tw, thtsai@iis.sinica.edu.tw, hsu@iis.sinica.edu.tw 
 
 
 
Abstract 
 
In this paper, we describe ontology-based 
text categorization in which the domain 
ontologies are automatically acquired 
through morphological rules and statistical 
methods. The ontology-based approach is 
a promising way for general information 
retrieval applications such as knowledge 
management or knowledge discovery. As 
a way to evaluate the quality of domain 
ontologies, we test our method through 
several experiments.  Automatically 
acquired domain ontologies, with or 
without manual editing, have been used 
for text categorization. The results are 
quite satisfactory. Furthermore, we have 
developed an automatic method to 
evaluate the quality of our domain 
ontology.  
1. Introduction 
Domain ontology, consisting of important 
concepts and relationships of the concepts in the 
domain, is useful in a variety of applications 
(Gruber, 1993). However, evaluating the quality of 
domain ontologies is not straightforward. Reusing 
an ontology for several applications can be a 
practical method for evaluating domain ontology. 
Since text categorization is a general tool for 
information retrieval, knowledge management and 
knowledge discovery, we test the ability of 
domain ontology to categorize news clips in this 
paper. 
Traditional IR methods use keyword 
distribution form a training corpus to assign 
testing document. However, using only keywords 
in a training set cannot guarantee satisfactory 
results since authors may use different  keywords. 
We believe that, news clip events are categorized 
by concepts, not just keywords. Previous works 
shows that the latent semantic index (LSI) method 
and the n-gram method give good results for 
Chinese news categorization (Wu et al, 1998). 
However, the indices of LSI and n-grams are less 
meaningful semantically. The implicit rules 
acquired by these methods can be understood by 
computers, not humans. Thus, manual editing for 
exceptions and personalization are not possible 
and it is difficult to further reuse these indices for 
knowledge management. 
With good domain ontology we can identify 
the concept structure of sentences in a document. 
Our idea is to compile the concepts within 
documents in a training set and use these concepts 
to understand documents in a testing set. However, 
building rigorous domain ontology is laborious 
and time-consuming. Previous works suggest that 
ontology acquisition is an iterative process, which 
includes keyword collection and structure 
reorganization. The ontology is revised, refined, 
and accumulated by a human editor at each 
iteration (Noy and McGuinness, 2001). For 
example, in order to find a hyponym of a keyword, 
the human editor must observe sentences 
containing this keyword and its related hyponyms 
(Hearst, 1992). The editor then deduces rules for 
finding more hyponyms of this keyword. At each 
iteration the editor refines the rules to obtain better 
quality pairs of keyword-hyponyms. To speed up 
the above labor-intensive approach, semi-
automatic approaches have been designed in 
which a human editor only has to verify the results 
of the acquisition (Maedche and Staab, 2000).  
A knowledge representation framework, 
Information Map (InfoMap) in our previous work 
(Hsu et al, 2001), has been designed to integrate 
various linguistic, common-sense and domain 
knowledge. InfoMap is designed to perform 
natural language understanding, and applied to 
many application domains, such as question 
answering (QA), knowledge management and 
organization memory (Wu et al, 2002), and shows 
good results. An important characteristic of 
InfoMap is that it extracts events from a sentence 
by capturing the topic words, usually subject-verb 
pairs or hypernym-hyponym pairs, which are 
defined in the domain ontology.  
 
We shall review the InfoMap ontology 
framework in Section 2. The ontology acquisition 
process and extraction rules will be introduced in 
Section 3. We describe ontology-based text 
categorization in Section 4. Experimental results 
are reported in Section 5. We conclude our work 
in Section 6.  
2. Information Map 
InfoMap can serve as domain ontology as well as 
an inference engine. InfoMap is designed for NLP 
applications; its basic function is to identify the 
event structure of a sentence. We shall briefly 
describe InfoMap in this section. Figure 1 gives 
example ontology of the Central News Agency 
(CNA), the target in our experiment.  
2.1 InfoMap Structure Format 
As a domain ontology, InfoMap consists of 
domain concepts and their related sub-concepts 
such as categories, attributes, activities. The 
relationships of a concept and its associated sub-
concepts form a tree-like taxonomy. InfoMap also 
defines references to connect nodes from different 
branches which serves to integrate these 
hierarchical concepts into a network. InfoMap not 
only classifies concepts, but also connects the 
concepts by defining the relationships among them. 
 
Concept A
Category
Attribute
       Concept A'        
(Sub-concept of 
concept A)
   Concept B   
(relavant but not 
belong to concept A)
Action
      Concept  C      
(An activity of 
concept A)
Function node
Concept node
Legend
 
Figure 2. Skeleton of the Ontology Structure of 
InfoMap  
Figure 1. Ontology Structure for CNA News 
In InfoMap, concept nodes represent concepts 
and function nodes represent the relationships 
between concepts. The root node of a domain is 
the name of the domain. Following the root node, 
important topics are stored in a hierarchical order. 
These topics have sub-categories that list related 
sub-topics in a recursive fashion. Figure 1 is a 
partial view of the domain ontology of the CNA. 
Under each domain there are several topics and 
each topic might have sub-concepts and associated 
attributes. In this example, note that, the domain 
ontology is automatically acquired from a domain 
corpus, hence the quality is poor. Figure 2 shows 
the skeleton order of a concept using InfoMap.  
2.2 Event Structure 
Since concepts that are semantically related are 
often clustered together, one can use InfoMap to 
discern the main event structure in a natural 
language sentence. The process of identifying the 
event structure, we call a firing mechanism, which 
matches words in a sentence to both concepts and 
relationships in InfoMap. 
Suppose keywords of concept A and its sub-
concept B (or its hyponyms) appear in a sentence. 
It is likely that the author is describing an event ?B 
of A?. For example, when the words ?tire? and 
?car? appear in a sentence, normally this sentence 
would be about the tire of a car (not tire in the 
sense of fatigue). Therefore, a word-pair with a 
semantic relationship can give more concrete 
information than two words without a semantic 
relationship. Of course, certain syntactic 
constraints also need to be satisfied. This can be 
extended to a noun-verb pair or a combination of 
noun, verb and adjective. We call such words in a 
sentence an event structure. This mechanism 
seems to be especially effective for Chinese 
sentences. 
2.3 Domain Speculation 
With the help of domain ontologies, one can 
categorize a piece of text into a specific domain by 
categorizing each individual sentence within the 
text. There are many different ways to use domain 
ontology to categorize text. It can be used as a 
dictionary, as a keyword lists and as a structure to 
identify NL events. Take a single sentence for 
example. We first use InfoMap as a dictionary to 
do word segmentation (necessary for Chinese 
sentences) in which the ambiguity can be resolved 
by checking the domain topic in the ontology. 
After words are segmented, we can examine the 
distribution of these words in the ontology and 
effectively identify the densest cluster. Thus, we 
can use InfoMap to identify the domains of the 
sentences and their associated keywords. Section 
4.1 will further elaborate on this. 
3. Automatic Ontology Acquisition 
The automatically domain ontology acquisition 
from a domain corpus has three steps: 
1. Identify the domain keywords. 
2. Find the relative concepts. 
3. Merge the correlated activities. 
3.1 Domain Keyword Identification 
The first step of automatic domain ontology 
acquisition is to identify domain keywords. 
Identifying Chinese unknown words is difficult 
since the word boundary is not marked in Chinese 
corpus. According to an inspection of a 5 million 
word Chinese corpus (Chen et al, 1996), 3.51% of 
words are not listed in the CKIP lexicon (a 
Chinese lexicon with more than 80,000 entries). 
We use reoccurrence frequency and fan-out 
numbers to characterize words and their 
boundaries according to PAT-tree (Chien, 1999). 
We then adopt the TF/IDF classifier to choose 
domain keywords. The domain keywords serve as 
the seed topics in the ontology. We then apply 
SOAT to automatically obtain related concepts. 
3.2 SOAT 
To build the domain ontology for a new domain, 
we need to collect domain keywords and concepts 
by finding relationships among keywords. We 
adopt a semi-automatic domain ontology 
acquisition tool (SOAT, Wu et al, 2002), to 
construct a new ontology from a domain corpus. 
With a given domain corpus, SOAT can build a 
prototype of the domain ontology. 
InfoMap uses two major relationships among 
concepts: taxonomic relationships (category and 
synonym) and non-taxonomic relationships 
(attribute and action). SOAT defines rules, which 
consist of patterns of keywords and variables, to 
capture these relationships. The extraction rules in 
SOAT are morphological rules constructed from 
part-of-speech (POS) tagged phrase structure. 
Here we briefly introduce the SOAT process: 
Input: domain corpus with the POS tag 
Output: domain ontology prototype 
Steps: 
1 Select a keyword (usually the name of 
the domain) in the corpus as the seed to 
form a potential root set R 
2 Begin the following recursive process:  
2.1 Pick a keyword A as the root from R 
2.2 Find a new related keyword B of the 
root A by extraction rules and add it 
into the domain ontology according to 
the rules   
2.3 If there is no more related keywords, 
remove A from R 
2.4 Put B into the potential root set 
Repeat step 2 until either R becomes 
empty or the total number of nodes reach 
a threshold 
3.3 Morphological Rules 
To find the relative words of a keyword, we check 
the context in the sentence from which the 
keyword appears. We can then find attributes or 
hyponyms of the keyword. For example, in a 
sentence, we find a noun in front of a keyword 
(say, computer) may form a specific kind of 
concept (say, quantum computer). A noun (say, 
connector) followed by ?of? and a keyword may 
be an attribute of the keyword, (say, connector of 
computer). See (Wu et al, 2002) for details. 
3.4 Ontology Merging 
Ontologies can be created by merging different 
resources.  One NLP resource that we will merge 
into our domain ontology is the noun-verb event 
frame (NVEF) database (Tsai and Hsu, 2002). 
NVEF is a collection of permissible noun-verb 
sense-pairs that appear in general domain corpora. 
The noun will be the subject or object of the verb. 
This noun-verb sense-pair collection is domain 
independent. We can use nouns as domain 
keywords and find their correlated verbs. Adding 
these verbs into the domain ontology makes the 
ontology more suitable for NLP. The correlated 
verbs are added under the action function node. 
4. Ontology-Based Text Categorization 
To incorporate the domain ontology into a text 
categorization, we have to adjust both the training 
process and testing process. Section 4.1 describes 
how to make use of the ontology and the event 
structure during the training process. Section 4.2 
describes how to use ontology to perform domain 
speculation. Section 4.3 describes how to 
categorize news clippings. 
4.1 Feature and Threshold Selection 
With the event structure matched (fired) in the 
domain ontology, we have more features with 
which to index a text. To select useful features and 
a proper threshold, we apply Microsoft Decision 
Tree Algorithm to determine a path?s relevance as 
this algorithm can extract human interpretable 
rules (Soni et al, 2000). 
Features of the event structure include event 
structure score, node score, fired node level, and 
node type. During the training process, we record 
all features of the event structure fired by the news 
clippings in the domain-categorized training 
corpus. The decision tree shows that a threshold of 
0.85 is sufficient to evaluate event structure scores. 
We use event structure score to determine if the 
path is relevant. According to Figure 3, if the 
threshold of true probability is 85%, then the event 
structure score (Pathscore in the figure) should be 
65.75. And the relevance of a path p is true if p 
falls in a node on the decision tree whose ratio of true 
instance is greater than ? .  
 
 4.2 Domain Speculation  
The goal of domain speculation is to categorize a 
sentence S into a domain Dj according to the 
combined score of the keywords and the event 
structure in sentence S. We first calculate the 
similarity score of S and Dj. The keyword score 
and the event structure score are calculated 
independently.  
 
),(_*
),(_),(
SDScoretureEventStruc
SDScoreKeywordSDSimScore
j
jj
?
+=  
We use the TF/IDF classifier (Salton, 1989) to 
calculate the Keyword_Score of a sentence  as 
follows. First, we use a segmentation module to 
split a Chinese sentence into words. The TF/IDF 
classifier represents a domain as a weighted vector, 
Dj =( wj1, wj2,?, wjn), where n is the number of 
words in this domain and wk is the weight of word 
k. wk is defined as nfjk * idfjk, where nfjk is the term 
frequency (i.e., the number of times the word wk 
occurs in the domain j). Let DFk be the number of 
domains in which word k appears and |D| the total 
number of domains. idfk, the inverse document 
frequency, is given by:  
)||log(
k
k DF
Didf = . 
This weighting function assigns high values to 
domain-specific words, i.e. words which appear 
frequently in one domain and infrequently in 
others. Conversely, it will assign low weights to 
words appearing in many domains. The similarity 
between a domain j and a sentence represented by 
a vector Di is measured by the following cosine: 
??
?
==
==
=
n
k ik
n
k jk
n
k ikjk
ij
j
ww
ww
DDSim
SDScoreKeyword
1
2
1
2
1
)()(
),(
),(_  
The event structure score is calculated by 
InfoMap Engine. First, find all the nodes in 
ontology that match the words in the sentence. 
Then determine if there is any concept-attribute 
pair, or hypernym-hyponym pair. Finally, assign a 
score to each fired event structure according to the 
string length of words that match the nodes in the 
ontology. The selected event structure is the one 
with the highest score. 
))((
),(_
max SDkeywordsthStringLeng
SDScoretureEventStruc
j
Event
j
?= ?  
4.3 News Categorization 
Upon receiving a news clipping C, we split it into 
sentences Si. The sentences are scored and 
categorized according to domains.  Thus, every 
sentence has an individual score for each domain 
Score(D, Si). We add up these scores of every 
sentence in the text according to domain, giving us 
total domain scores for the entire text.  The 
domain which has the highest score is the domain 
into which the text is categorized. 
)),(()( maxarg ?
?
=
CS
i
D
SDScoreCDomain  
5. Refining Ontology through the Text 
Categorization Application 
The advantage of ontology compared to other 
implicit knowledge representation mechanism is 
that it can be read, interpreted and edited by 
human. Noise and errors can be detected and 
refined, especially for the automatically acquired 
ontology, in order to obtain a better ontology. 
Another advantage of allowing human editing is 
that the ontology produced can be shared by 
various applications, such as from a QA system to 
a knowledge management system. In contrast, the 
implicit knowledge represented in LSI or other 
representations is difficult to port from one 
application to another. 
Figure 3. Threshold selection using decision 
tree 
In this section, we show how the human 
editing feature improves news categorization. First, 
we can identify a common error type: ambiguity; 
then, depending on the degree of categorization 
ambiguity, the system can report to a human editor 
the possible errors of certain concepts in the 
domain ontology as clues. 
Consider the following common error type: 
event structure ambiguity. Some event structures 
are located in several domains due to the noise of 
training data. We define two formulas to find such 
event structures. The ambiguity of an event 
structure E(Si) is proportional to the number of 
domains in which it appears, and inversely 
proportional to its event score, where Si are the 
sentences that fire event E. 
GlobalCategorizationAmiguityFactor(E(Si) ) 
= number of domains fired by 
Si/average( EventScore(Si) ) 
We also measure the similarity between every 
two event structures by calculating the co-
occurrence multiplied by the global categorization 
ambiguity factor. 
GlobalCategorizationAmbiguityij (E i, E j) 
=Co-occurrence(E i, E j) * 
GlobalCategorizationAmbiguityFactor(E j) 
When the GlobalCategorizationAmbiguity of an 
event structure E i exceeds a threshold, the system 
will suggest that the human editor refine the 
ontology. 
6. Experiments 
To assess the power of domain identification of 
ontology, we test the text categorization ability on 
two different corpora. The ontology of the first 
experiment is edited manually; the ontology of the 
second experiment is automatically acquired. And 
we also conduct an experiment on the effect of 
human editing of the automatically acquired 
ontology. 
6.1 Single Sentence Test 
We test 9,143 sentences, edited manually for a QA 
system. The accuracy is 94%. These sentences are 
questions in the financial domain. Because the 
sentence topics are quite focused, the accuracy is 
very high. See Table 1. 
Table 1. Sentence Categorization Accuracy 
Domain # Sentence # Accuracy 
24 9143 94.01% 
6.2 News Clippings Collection 
The second experiment that we conduct is news 
categorization. We collect daily news from China 
News Agency (CNA) ranging from 1991 to 1999. 
Each news clipping is short with 352 Chinese 
characters (about 150 words) on the average. 
There are more than thirty domains and we choose 
10 major categories for the experiment. 
6.3 10 Categories News Categorization 
Our ten categories are: domestic arts and education 
(DD), foreign affairs (FA), finance report (FX), 
domestic health (HD), Taiwan local news (LD), 
Taiwan sports (LD), domestic military (MD), 
domestic politics (PD), Taiwan stock markets (SD), 
and weather report (WE). From each category, we 
choose the first 100 news clippings as the training 
set and the following 100 news clippings as the 
testing set. After data cleansing, the total training 
set has 979 news clippings, with 27,951 nodes and 
less than 10,000 distinct words. The training set 
for which domain ontologies are automatically 
acquired is shown in Table 2. A partial view of 
this ontology is in Figure 1. 
The result of text categorization based on this 
automatically acquired domain ontology is shown 
in Table 5, which contains the recall and precision 
for each domain. Note that, without the help of the 
event structure, the macro average f-score is 
85.16%. Even the total number of domain key 
concepts is less than 10,000 words (instead of 
100,000 words in standard dictionary), we can still 
obtain a good categorization result. With the help 
of event structure, the macro average f-score is 
85.55%.  
6.4 Human Editing 
To verify the refinement method, we conduct 
an experiment to compare the result of using 
automatically acquired domain ontology and that 
of limited human editing (on only one domain 
ontology). After the training process, we use 
domain ontologies to classify the training data, 
and to calculate the global categorization 
ambiguity factor formula in order to obtain 
ambiguous event structure pairs as candidates for 
human editing. For simplicity, we restrict the 
action of refinement to deletion. It takes a human 
editor one half day to finish the task and delete 
0.62% nodes (172 out of 27,951 nodes). In the 
testing phase, we select 928 new news clippings as 
the testing set. Table 3 shows the results from 
before and after human editing. Due to time 
constraints, we only edit the part of the ontology 
that might affect domain DD. The recall and 
precision of domain DD increase as well as both 
the average recall and average precision. In 
addition, the recall of domains having higher 
correlation with DD, such as PD and FA, 
decreases. Apparently, the event structures that 
mislead the categorization system to theses 
domain have mostly been deleted. The experiment 
result is very consistent with our intuition. 
Table 2. Ten Category training set CNA news 
Training set size 
Domain 
Doc# Char# 
DD 98 41870 
FA 97 38143 
FX 100 30771 
HD 96 39818 
JD 107 35381 
LD 96 36957 
MD 89 32903 
PD 100 43152 
SD 109 33030 
WE 87 30457 
total 979 362,482 
7. Discussions and Conclusions 
Compared to an ordinary n-gram dictionary, our 
ontology dictionary is quite small (roughly 10%) 
but records certain important relations between 
keywords.  
Our goal is to generate rules that are human 
readable via ontology. The experiment result 
shows that event structure enhances text 
categorization, even when the domain ontology is 
automatically acquired without human verification. 
To improve our ontological approach, our future 
work are: 1. human editing in more domains; 2. 
enlarge our dictionary by merging existing 
ontologies, e.g., the names of countries, capitals 
and important persons, which are absent from the 
training corpus; 3. incorporate more sense pairs 
such as N-A (noun-adjective), Adv-V (adverb-
verb); 4. use machine learning model on the 
weighting of the ontological features. 
Previous research shows that some NLP 
techniques can improve information retrieval. 
Ontology-based IR is one of them. However, the 
construction of domain ontology is too costly. 
Thus, automatic acquisition of domain ontology is 
becoming an interesting research topic. Previous 
research shows that implicit rules (such as LSI, N-
gram dictionaries) learned from a training corpus 
give better results than explicit rules generated by 
humans. However, it is hard to use these implicit 
rules or to combine them with other resources for 
further refinement. With the help of domain 
ontology, we can automatically generate rules that 
humans can understand. Since humans and 
machines can maintain ontology independently, 
the ontological approach can be applied more 
easily to other IR applications. Ontologies from 
different sources can be merged into the domain 
ontology. The system should include an editing 
interface that human thoughts can be incorporated 
to complement statistical rules. With semi-
automatically acquired domain ontology, text 
categorization can be adapted to personal 
preferences.  
8. References 
Chen, K.J., C.R. Huang, L.P. Chang & H.L. Hsu, 
SINICA CORPUS: Design Methodology for 
Balanced Corpora, in Proceedings of PACLIC 
11th Conference, pp.167-176, 1996. 
Chien, L.F., PAT-tree-based Adaptive keyphrase 
extraction for Intelligent Chinese Information 
Retrieval, Information Processing and 
Management, Vol. 35, pp. 501-521, 1999. 
Gruber, T.R. (1993), A translation approach to 
portable ontologies. Knowledge Acquisition, 
5(2), pp. 199-220, 1993. 
Hearst, M.A. (1992), Automatic acquisition of 
hyponyms from large text corpora. In 
COLING-92, pp. 539-545. 
Hsu, W.L., Wu, S.H. and Chen, Y.S., Event 
Identification Based On The Information Map - 
INFOMAP, in Natural Language Processing 
and Knowledge Engineering Symposium of the 
IEEE Systems, Man, and Cybernetics 
Conference, Tucson, Arizona, USA, 2001. 
Maedche, A. and Staab, S. (2000), Discovering 
Conceptual Relationships from Text. In: Horn, 
W. (ed.): ECAI 2000. Proceedings of the 14th 
European Conference on Artificial Intelligence, 
IOS Press, Amsterdam. 
Noy, N.F. and McGuinness D.L. (2001), Ontology 
Development 101: A Guide to Creating Your 
First Ontology, SMI technical report SMI-
2001-0880, Stanford Medical Informatics. 
Salton, G., Automatic Text Processing, Addison-
Wesley, Massachusetts, 1989. 
Soni, S, Tang, Z. and Yang, J., ?Microsoft 
Performance Study of Microsoft Data Mining 
Algorithms?, UniSys, 2000/12. 
Tsai, J.L. and Hsu, W.L., ?Applying an NVEF 
Word-Pair Identifier to the Chinese Syllable-to-
Word Conversion Problem,? COLING-02, 
Taipei, ACM press, 2002. 
Wu, S.H. and Hsu, W.L., SOAT: A Semi-
Automatic Domain Ontology Acquisition Tool 
from Chinese Corpus, COLING-02, Taipei, 
ACM press, 2002. 
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L., 
FAQ-centered Organizational Memory, in 
Nada Matta and Rose Dieng-Kuntz (ed.), 
Knowledge Management and Organizational 
Memories, Kluwer Academic Publishers, 
Boston, 2002. 
Wu, S.H., Yang, P.C. and Soo, V.W., An 
Assessment on Character-based Chinese News 
Filtering Using Latent Semantic Indexing, 
Computational Linguistics & Chinese 
Language Processing, Vol. 3, no.2, August 
1998. 
Table 3. Experiment result of CNA news categorization 
# of nodes 
automatically 
acquired 
#of nodes  
deleted in 
human editing 
TF/IDF(baseline)
TF/IDF+Event 
Structure(first 
improvement) 
TF/IDF+Event Structure 
with Human Editing 
(second improvement) 
The different between 
(second improvement) and 
(first improvement) Domain 
Before  After   #  % P% R% F% P% R% F% P% R% F% P+% R+% F+% 
DD 4616 4574 42 0.91 72.90 
82.9
8 
77.6
1 74.04 81.91 77.78 74.29 82.98 78.39 0.25 1.07 0.61
FA 8352 8348 4 0.05 75.83 
94.7
9 
84.2
6 71.32 95.83 81.78 76.67 95.83 85.19 5.35 0.00 3.41
FX 44 44 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
HD 3357 3348 9 0.27 78.79 
88.6
4 
83.4
2 80.21 87.50 83.70 78.79 88.64 83.42 -1.42 1.14 -0.28
JD 1854 1846 8 0.43 88 71.74 
79.0
4 87.18 73.91 80 87.84 70.65 78.31 0.66 -3.26 -1.69
LD 2925 2831 94 3.21 87.64 
80.4
1 
83.8
7 90.36 77.32 83.33 88.51 79.38 83.70 -1.85 2.06 0.37
MD 2010 1999 11 0.55 95.59 
66.3
3 
78.3
1 95.71 68.37 79.76 97.26 72.45 83.04 1.55 4.08 3.28
PD 3199 3195 4 0.13 65.81 
68.7
5 
67.2
5 70.43 72.32 71.37 66.67 69.64 68.12 -3.76 -2.68 -3.25
SD 585 585 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
WE 1009 1009 0 0.00 95.74 100 
97.8
3 95.74 100 97.83 95.74 100 97.83 0.00 0.00 0.00
Total 27951 27779 172 0.62                   
Macro  
Average         
86.0
3 
85.3
6 
85.1
6 86.50 85.72 85.55 86.58 85.96 85.80 0.08 0.24 0.25
 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 233?236, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploiting Full Parsing Information to Label Semantic Roles Using an  
Ensemble of ME and SVM via Integer Linear Programming 
 
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica  
Taipei 115, Taiwan 
{thtsai, cwwu, sbb, hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we propose a method that 
exploits full parsing information by repre-
senting it as features of argument classifi-
cation models and as constraints in integer 
linear learning programs. In addition, to 
take advantage of SVM-based and Maxi-
mum Entropy-based argument classifica-
tion models, we incorporate their scoring 
matrices, and use the combined matrix in 
the above-mentioned integer linear pro-
grams. The experimental results show that 
full parsing information not only in-
creases the F-score of argument classifi-
cation models by 0.7%, but also 
effectively removes all labeling inconsis-
tencies, which increases the F-score by 
0.64%. The ensemble of SVM and ME 
also boosts the F-score by 0.77%. Our 
system achieves an F-score of 76.53% in 
the development set and 76.38% in Test 
WSJ. 
1 Introduction 
The Semantic Role Labeling problem can be for-
mulated as a sentence tagging problem. A sentence 
can be represented as a sequence of words, as 
phrases (chunks), or as a parsing tree. The basic 
units of a sentence are words, phrases, and con-
stituents in these representations, respectively.. 
Pradhan et al (2004) established that Constituent-
by-Constituent (C-by-C) is better than Phrase-by-
Phrase (P-by-P), which is better than Word-by-
Word (W-by-W).  This is probably because the 
boundaries of the constituents coincide with the 
arguments; therefore, C-by-C has the highest ar-
gument identification F-score among the three ap-
proaches.  
In addition, a full parsing tree also provides 
richer syntactic information than a sequence of 
chunks or words. Pradhan et al (2004) compared 
the seven most common features as well as several 
features related to the target constituent?s parent 
and sibling constituents. Their experimental results 
show that using other constituents? information 
increases the F-score by 6%. Punyakanok et al 
(2004) represent full parsing information as con-
straints in integer linear programs. Their experi-
mental results show that using such information 
increases the argument classification accuracy by 
1%. 
In this paper, we not only add more full parsing 
features to argument classification models, but also 
represent full parsing information as constraints in 
integer linear programs (ILP) to resolve label in-
consistencies. We also build an ensemble of two 
argument classification models: Maximum Entropy 
and SVM by combining their argument classifica-
tion results and applying them to the above-
mentioned ILPs. 
2 System Architecture 
Our SRL system is comprised of four stages: prun-
ing, argument classification, classification model 
incorporation, and integer linear programming. 
This section describes how we build these stages, 
including the features used in training the argu-
ment classification models. 
2.1 Pruning 
233
When the full parsing tree of a sentence is avail-
able, only the constituents in the tree are consid-
ered as argument candidates. In CoNLL-2005, full 
parsing trees are provided by two full parsers: the 
Collins parser (Collins, 1999)  and the Charniak 
parser (Charniak, 2000). According to Punyakanok 
et al (2005), the boundary agreement of Charniak 
is higher than that of Collins; therefore, we choose 
the Charniak parser?s results. However, there are 
two million nodes on the full parsing trees in the 
training corpus, which makes the training time of 
machine learning algorithms extremely long. Be-
sides, noisy information from unrelated parts of a 
sentence could also affect the training of machine 
learning models. Therefore, our system exploits the 
heuristic rules introduced by Xue and Palmer 
(2004) to filter out simple constituents that are 
unlikely to be arguments. Applying pruning heuris-
tics to the output of Charniak?s parser effectively 
eliminates 61% of the training data and 61.3% of 
the development data, while still achieves 93% and 
85.5% coverage of the correct arguments in the 
training and development sets, respectively. 
2.2 Argument Classification 
This stage assigns the final labels to the candidates 
derived in Section 2.1. A multi-class classifier is 
trained to classify the types of the arguments sup-
plied by the pruning stage. In addition, to reduce 
the number of excess candidates mistakenly output 
by the previous stage, these candidates can be la-
beled as null (meaning ?not an argument?). The 
features used in this stage are as follows. 
Basic Features 
? Predicate ? The predicate lemma. 
? Path ? The syntactic path through the 
parsing tree from the parse constituent be-
ing classified to the predicate. 
? Constituent Type 
? Position ? Whether the phrase is located 
before or after the predicate. 
? Voice ? passive: if the predicate has a POS 
tag VBN, and its chunk is not a VP, or it is 
preceded by a form of ?to be? or ?to get? 
within its chunk; otherwise, it is active. 
? Head Word ? calculated using the head 
word table described by Collins (1999). 
? Head POS ? The POS of the Head Word. 
? Sub-categorization ? The phrase structure 
rule that expands the predicate?s parent 
node in the parsing tree. 
? First and Last Word/POS 
? Named Entities ? LOC, ORG, PER, and 
MISC. 
? Level ? The level in the parsing tree. 
Combination Features 
? Predicate Distance Combination 
? Predicate Phrase Type Combination 
? Head Word and Predicate Combination 
? Voice Position Combination 
Context Features 
? Context Word/POS ? The two words pre-
ceding and the two words following the 
target phrase, as well as their correspond-
ing POSs.  
? Context Chunk Type ? The two chunks 
preceding and the two chunks following 
the target phrase. 
Full Parsing Features 
We believe that information from related constitu-
ents in the full parsing tree helps in labeling the 
target constituent. Denote the target constituent by 
t. The following features are the most common 
baseline features of t?s parent and sibling constitu-
ents. For example, Parent/ Left Sibling/ Right Sib-
ling Path denotes t?s parents?, left sibling?s, and 
right sibling?s Path features.  
? Parent / Left Sibling / Right Sibling 
Path 
? Parent / Left Sibling / Right Sibling 
Constituent Type 
? Parent / Left Sibling / Right Sibling Po-
sition 
? Parent / Left Sibling / Right Sibling 
Head Word 
? Parent / Left Sibling / Right Sibling 
Head POS 
? Head of PP parent ? If the parent is a PP, 
then the head of this PP is also used as a 
feature. 
Argument Classification Models 
234
We use all the features of the SVM-based and ME-
based argument classification models. All SVM 
classifiers are realized using SVM-Light with a 
polynomial kernel of degree 2. The ME-based 
model is implemented based on Zhang?s MaxEnt 
toolkit1 and L-BFGS (Nocedal and Wright, 1999) 
method to perform parameter estimation. 
2.3 Classification Model Incorporation  
We now explain how we incorporate the SVM-
based and ME-based argument classification mod-
els. After argument classification, we acquire two 
scoring matrices, PME and PSVM, respectively. In-
corporation of these two models is realized by 
weighted summation of PME and PSVM as follows: 
P? = wMEPME + wSVMPSVM
We use P? for the objective coefficients of the 
ILP described in Section 2.4. 
2.4 Integer Linear Programming (ILP) 
To represent full parsing information as features, 
there are still several syntactic constraints on a 
parsing tree in the SRL problem. For example, on a 
path of the parsing tree, there can be only one con-
stituent annotated as a non-null argument. How-
ever, it is difficult to encode this constraint in the 
argument classification models. Therefore, we ap-
ply integer linear programming to resolve inconsis-
tencies produced in the argument classification 
stage.  
According to Punyakanok et al (2004), given a 
set of constituents, S, and a set of semantic role 
labels, A, the SRL problem can be formulated as 
an ILP as follows: 
Let zia be the indicator variable that represents 
whether or not an argument,  a, is assigned to any 
Si ? S; and let pia = score(Si = a). The scoring ma-
trix P composed of all pia is calculated by the ar-
gument classification models. The goal of this ILP 
is to find a set of assignments for all zia that maxi-
mizes the following function: 
??
? ?S AiS a
iaia zp . 
Each Si?  S should have one of these argument 
types, or no type (null). Therefore, we have  
?
?
=
Aa
iaz 1 . 
Next, we show how to transform the constraints in 
                                                          
1 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 
the filter function into linear equalities or inequali-
ties, and use them in this ILP. 
Constraint I: No overlapping or embedding  
For arguments Sj1 , . . . , Sjk  on the same path in a 
full parsing tree, only one argument can be as-
signed to an argument type. Thus, at least k ? 1 
arguments will be null, which is represented by ?  
in the following linear equality: 
?
=
??
k
i
j ki
1
1z ? .                                                     
Constraint II: No duplicate argument classes 
Within the same sentence, A0-A5 cannot appear 
more than once. The inequality for A0 is therefore: 
?
=
?
k
i
iz
1
A0 1. 
Constraint III: R-XXX arguments  
The linear inequalities that represent A0 and its 
reference type R-A0 are: 
?
=
????
k
i
mi zzMm
1
A0RA0:},...,1{ . 
Constraint IV: C-XXX arguments  
The continued argument XXX has to occur before 
C-XXX. The linear inequalities for A0 are: 
??
=
????
1
1
A0CA0:},...,2{
m
i
mj zzMm i . 
Constraint V: Illegal arguments  
For each verb, we look up its allowed roles. This 
constraint is represented by summing all the corre-
sponding indicator variables to 0. 
3 Experiment Results  
3.1 Data and Evaluation Metrics 
The data, which is part of the PropBank corpus, 
consists of sections from the Wall Street Journal 
part of the Penn Treebank. All experiments were 
carried out using Section 2 to Section 21 for train-
ing, Section 24 for development, and Section 23 
for testing. Unlike CoNLL-2004, part of the Brown 
corpus is also included in the test set.  
3.2 Results 
Table 1 shows that our system makes little differ-
ence to the development set and Test WSJ. How-
ever, due to the intrinsic difference between the 
WSJ and Brown corpora, our system performs bet-
ter on Test WSJ than on Test Brown. 
235
Precision Recall F
?=1
Development 81.13% 72.42% 76.53
Test WSJ 82.77% 70.90% 76.38
Test Brown 73.21% 59.49% 65.64
Test WSJ+Brown 81.55% 69.37% 74.97
Test WSJ Precision Recall F
?=1
Overall 82.77% 70.90% 76.38
A0 88.25% 84.93% 86.56
A1 82.21% 72.21% 76.89
A2 74.68% 52.34% 61.55
A3 78.30% 47.98% 59.50
A4 84.29% 57.84% 68.60
A5 100.00% 60.00% 75.00
AM-ADV 64.19% 47.83% 54.81
AM-CAU 70.00% 38.36% 49.56
AM-DIR 38.20% 40.00% 39.08
AM-DIS 83.33% 71.88% 77.18
AM-EXT 86.67% 40.62% 55.32
AM-LOC 63.71% 41.60% 50.33
AM-MNR 63.36% 48.26% 54.79
AM-MOD 98.00% 97.64% 97.82
AM-NEG 99.53% 92.61% 95.95
AM-PNC 44.44% 17.39% 25.00
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.21% 61.09% 70.45
R-A0 91.08% 86.61% 88.79
R-A1 79.49% 79.49% 79.49
R-A2 87.50% 43.75% 58.33
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 72.73% 61.54% 66.67
V 97.32% 97.32% 97.32  
Table 1. Overall results (top) and detailed results 
on the WSJ test (bottom). 
Precision Recall F
?=1
ME w/o parsing 77.28% 70.55% 73.76%
ME 78.19% 71.08% 74.46%
ME with ILP 79.57% 71.11% 75.10%
SVM 79.88% 72.03% 75.76%
Hybrid 81.13% 72.42% 76.53%
 
Table 2. Results of all configurations on the devel-
opment set. 
From Table 2, we can see that the model with 
full parsing features outperforms the model with-
out the features in all three performance matrices. 
After applying ILP, the performance is improved 
further. We also observe that SVM slightly outper-
forms ME. However, the hybrid argument classifi-
cation model achieves the best results in all three 
metrics. 
4 Conclusion  
In this paper, we add more full parsing features to 
argument classification models, and represent full 
parsing information as constraints in ILPs to re-
solve labeling inconsistencies. We also integrate 
two argument classification models, ME and SVM, 
by combining their argument classification results 
and applying them to the above-mentioned ILPs. 
The results show full parsing information increases 
the total F-score by 1.34%. The ensemble of SVM 
and ME also boosts the F-score by 0.77%. Finally, 
our system achieves an F-score of 76.53% in the 
development set and 76.38% in Test WSJ. 
Acknowledgement 
We are indebted to Wen Shong Lin and Prof. Fu 
Chang for their invaluable advice in data pruning, 
which greatly speeds up the training of our ma-
chine learning models. 
References  
X. Carreras and L. M?rquez. 2005. Introduction to the 
CoNLL-2005 Shared Task: Semantic Role Labeling. 
In Proceedings of the CoNLL-2005. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proceedings of the NAACL-2000. 
M. J. Collins. 1999. Head-driven Statistical Models for 
Natural Language Parsing. Ph.D. thesis, University 
of Pennsylvania. 
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion, Springer. 
S. Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J. H. 
Martin, and D. Jurafsky. 2004. Support Vector 
Learning for Semantic Argument Classification. 
Journal of Machine Learning. 
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling. 
In Proceedings of the 19th International Joint Con-
ference on Artificial Intelligence (IJCAI-05). 
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. 
Semantic Role Labeling via Integer Linear Pro-
gramming Inference. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics 
(COLING-04). 
N. Xue and M. Palmer. 2004. Calibrating Features for 
Semantic Role Labeling. In Proceedings of the 
EMNLP 2004. 
236
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Closed Task of Chinese Word Segmentation: An Improved CRF 
Model Coupled with Character Clustering and  
Automatically Generated Template Matching 
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,  
Hong-Jie Dai, and Wen-Lian Hsu 
Intelligent Agent Systems Lab 
Institute of Information Science, Academia Sinica 
No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C. 
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
This paper addresses two major prob-
lems in closed task of Chinese word 
segmentation (CWS): tagging sentences 
interspersed with non-Chinese words, 
and long named entity (NE) identifica-
tion. To resolve the former, we apply K-
means clustering to identify non-Chinese 
characters, and then adopt a two-tagger 
architecture: one for Chinese text and the 
other for non-Chinese text. For the latter 
problem, we apply postprocessing to our 
CWS output using automatically gener-
ated templates. The experiment results 
show that, when non-Chinese characters 
are sparse in the training corpus, our 
two-tagger method significantly im-
proves the segmentation of sentences 
containing non-Chinese words. Identifi-
cation of long NEs and long words is 
also enhanced by template-based post-
processing. In the closed task of 
SIGHAN 2006 CWS, our system 
achieved F-scores of 0.957, 0.972, and 
0.955 on the CKIP, CTU, and MSR cor-
pora respectively.  
1 Introduction 
Unlike Western languages, Chinese does not 
have explicit word delimiters. Therefore, word 
segmentation (CWS) is essential for Chinese 
text processing or indexing. There are two main 
problems in the closed CWS task. The first is to 
identify and segment non-Chinese word se-
quences in Chinese documents, especially in a 
closed task (described later). A good CWS sys-
tem should be able to handle Chinese texts pep-
pered with non-Chinese words or phrases. Since 
non-Chinese language morphologies are quite 
different from that of Chinese, our approach 
must depend on how many non-Chinese words 
appear, whether they are connected with each 
other, and whether they are interleaved with 
Chinese words. If we can distinguish non-
Chinese characters automatically and apply dif-
ferent strategies, the segmentation performance 
can be improved. The second problem in closed 
CWS is to correctly identify longer NEs. Most 
ML-based CWS systems use a five-character 
context window to determine the current charac-
ter?s tag. In the majority of cases, given the con-
straints of computational resources, this com-
promise is acceptable. However, limited by the 
window size, these systems often handle long 
words poorly. 
In this paper, our goal is to construct a general 
CWS system that can deal with the above prob-
lems. We adopt CRF as our ML model. 
2 Chinese Word Segmentation System 
2.1 Conditional Random Fields 
Conditional random fields (CRFs) are undirected 
graphical models trained to maximize a condi-
tional probability (Lafferty et al, 2001). A lin-
ear-chain CRF with parameters ?={?1, ?2, ?} 
defines a conditional probability for a state se-
quence y = y1 ?yT , given that an input sequence 
x = x1 ?xT  is 
???
????
?= ??
=
??
T
t k
ttkk tyyfZ
P
1
1 ),,,(exp
1
)|( xxy
x
?  ,(1)                          
where Zx is the normalization factor that makes 
the probability of all state sequences sum to one; 
fk(yt-1, yt, x, t) is often a binary-valued feature 
function and ?k is its weight. The feature 
134
functions can measure any aspect of a state 
transition, yt-1?yt, and the entire observation 
sequence, x, centered at the current time step, t. 
For example, one feature function might have 
the value 1 when yt-1 is the state B, yt is the state 
I, and tx  is the character ???. 
2.2 Character Clustering 
In many cases, Chinese sentences may be inter-
spersed with non-Chinese words. In a closed 
task, there is no way of knowing how many lan-
guages there are in a given text. Our solution is 
to apply a clustering algorithm to find homoge-
neous characters belonging to the same character 
clusters. One general rule we adopted is that a 
language?s characters tend to appear together in 
tokens. In addition, character clusters exhibit 
certain distinct properties. The first property is 
that the order of characters in some pairs can be 
interchanged. This is referred to as exchange-
ability. The second property is that some charac-
ters, such as lowercase characters, can appear in 
any position of a word; while others, such as 
uppercase characters, cannot. This is referred to 
as location independence. According to the gen-
eral rule, we can calculate the pairing frequency 
of characters in tokens by checking all tokens in 
the corpus. Assuming the alphabet is ?, we first 
need to represent each character as a |?|-
dimensional vector. For each character ci, we use 
vj to represent its j-dimension value, which is 
calculated as follows: 
r
jiijj ffv )],)[min(1( ?? ?+= ?             (2), 
where fij denotes the frequency with which ci and 
cj appear in the same word when ci?s position 
precedes that of cj. We take the minimum value 
of fij and fji because even when ci and cj have a 
high co-occurrence frequency, if either fij or fji is 
low, then one order does not occur often, so vj?s 
value will be low. We use two parameters to 
normalize vj within the range 0 to 1; ? is used to 
enlarge the gap between non-zero and zero fre-
quencies, and ? is used to weaken the influence 
of very high frequencies. 
Next, we apply the K-means algorithm to 
generate candidate cluster sets composed of K 
clusters (Hartigan et al, 1979). Different K?s, 
??s, and ??s are used to generate possible charac-
ter cluster sets. Our K-means algorithm uses the 
cosine distance. 
After obtaining the K clusters, we need to se-
lect the N1 best character clusters among them. 
Assuming the angle between the cluster centroid 
vector and (1, 1, ... , 1) is ?, the cluster with the 
largest cosine ? will be removed. This is because 
characters whose co-occurrence frequencies are 
nearly all zero will be transformed into vectors 
very close to (?, ?, ... , ?); thus, their centroids 
will also be very close to (?, ?, ... , ?), leading to 
unreasonable clustering results. 
After removing these two types of clusters, 
for each character c in a cluster M, we calculate 
the inverse relative distance (IRDist) of c using 
(3): 
??
?
?
?
??
?
?
?
=
?
),cos(
),(cos
log)IRDist(
mc
mc
c
i
i   ,            (3) 
where mi stands for the centroid of cluster Mi, 
and m stands for the centroid of M.  
We then calculate the average inverse dis-
tance for each cluster M. The N1 best clusters are 
selected from the original K clusters.   
The above K-means clustering and character 
cluster selection steps are executed iteratively 
for each cluster set generated from K-means 
clustering with different K?s, ??s, and ??s.  
After selecting the N1 best clusters for each 
cluster set, we pool and rank them according to 
their inner ratios. Each cluster?s inner ratio is 
calculated by the following formula: 
?
?
?
?
= ?
ji
ji
cc
ji
Mcc
ji
cc
cc
M
,
,
),occurence(co
 ),occurence(co
)inner(
 ,   (4) 
where co-occurrence(ci, cj) denotes the fre-
quency with which characters  ci and cj co-occur 
in the same word. 
To ensure that we select a balanced mix of 
clusters, for each character in an incoming clus-
ter M, we use Algorithm 1 to check if the fre-
quency of each character in C?M is greater 
than a threshold ?. 
 
Algorithm 1 Balanced Cluster Selection 
Input: A set of character clusters P={M1 ,  . . .  , MK} 
          Number of selections N2, 
Output: A set of clusters Q={ '1M  ,  . . .  , 
'
2N
M }. 
 
1: C={} 
2: sort the clusters in P by their inner ratios; 
3: while |C|<=N2 do 
4:     pick the cluster M that has highest inner ratio; 
5:     for each character c in M do 
6:          if the frequency of c in C?M is over thresh-
old ? 
7:                 P?P?M; 
8:                 continue; 
9 :        else 
135
10:               C?C?M; 
11:               P?P?M; 
12:        end; 
13:   end; 
14: end 
 
The above algorithm yields the best N1 clus-
ters in terms of exchangeability. Next, we exe-
cute the above procedures again to select the 
best  N2 clusters based on their location inde-
pendence and exchangeability. However, for 
each character ci, we use vj to denote the value of 
its j-th dimension. We calculate vj as follows: 
r
jijiijijj ffffv )]',,',)[min(1(
' ?? ?+= ,      (5) 
where ijf  stands for the frequency with which ci 
and cj appear in the same word when ci is the 
first character; and f?ij stands for the frequency 
with which ci and cj co-occur in the same word 
when ci precedes cj  but not in the first position. 
We choose the minimum value from ijf , f?ij, jif , 
and f?ji  because if ci and cj both appear in the 
first position of a word and their order is ex-
changeable, the four frequency values, including 
the minimum value, will all be large enough. 
Type Cluster Inner (K, ?, ?) 
,.0123456789 0.94 (10, 0.60, 0.16)
EX 
 
-/ABCDEFGHIKLMNOPR 
STUVWabcdefghiklmnoprst 
uvwxy 
0.93 (10, 0.70, 0.16)
??ABCDEFGHIKLMNO 
PRSTUVWabcdefghiklmno 
prstvwxy 
0.84 (10, 0.50, 0.25)EL 
?????????? 0.76 (10, 0.50, 0.26)
Table 1. Clustering Results of the CTU corpus 
Our next goal is to create the best hybrid of 
the above two cluster sets. The set selected for 
exchangeability is referred to as the EX set, 
while the set selected for both exchangeability 
and location independence is referred to as the 
EL set. We create a development set and use the 
best first strategy to build the optimal cluster set 
from EX?EL. The EX and EL for the CTU 
corpus are shown in Table 1. 
2.3 Handling Non-Chinese Words 
Non-Chinese characters suffer from a serious 
data sparseness problem, since their frequencies 
are much lower than those of Chinese characters. 
In bigrams containing at least one non-Chinese 
character (referred as non-Chinese bigrams), the 
problem is more serious. Take the phrase ???  
20?? (about 20 years old) for example. ?2? is 
usually predicted as I, (i.e., ???? is connected 
with ?2?) resulting in incorrect segmentation, 
because the frequency of ?2? in the I class is 
much higher than that of ?2? in the B class, even 
though the feature C-2C-1=???? has a high 
weight for assigning ?2? to the B class. 
Traditional approaches to CWS only use one 
general tagger (referred as the G tagger) for 
segmentation. In our system, we use two CWS 
taggers. One is a general tagger, similar to the 
traditional approaches; the other is a specialized 
tagger designed to deal with non-Chinese words. 
We refer to the composite tagger (the general 
tagger plus the specialized tagger) as the GS 
tagger. 
Here, we refer to all characters in the selected 
clusters as non-Chinese characters. In the devel-
opment stage, the best-first feature selector de-
termines which clusters will be used. Then, we 
convert each sentence in the training data and 
test data into a normalized sentence. Each non-
Chinese character c is replaced by a cluster rep-
resentative symbol ?M, where c is in the cluster 
M. We refer to the string composed of all ?M as 
F. If the length of F is more than that of W, it 
will be shortened to W. The normalized sentence 
is then placed in one file, and the non-Chinese 
character sequence is placed in another. Next, 
we use the normalized training and test file for 
the general tagger, and the non-Chinese se-
quence training and test file for the specialized 
tagger. Finally, the results of these two taggers 
are combined. 
The advantage of this approach is that it re-
solves the data sparseness problem in non-
Chinese bigrams. Consider the previous example 
in which ? stands for the numeral cluster. Since 
there is a phrase ??? 8??  in the training data, 
C-1C0= ?? 8? is still an unknown bigram using 
the G tagger. By using the GS tagger, however, 
??? 20?? and ??? 8?? will be converted 
as ??? ???? and ??? ???, respectively. 
Therefore, the bigram feature C-1C0=?? ?? is no 
longer unknown. Also, since ? in ?? ?? is 
tagged as B, (i.e., ??? and ??? are separated), 
??? and ??? will be separated in  ??? ????. 
2.4 Generating and Applying Templates 
Template Generation 
We first extract all possible word candidates 
from the training set. Given a minimum word 
length L, we extract all words whose length is 
greater than or equal to L, after which we align 
all word pairs. For each pair, if more than fifty 
136
percent of the characters are identical, a template 
will be generated to match both words in the pair. 
Template Filtering 
We have two criteria for filtering the extracted 
templates. First, we test the matching accuracy 
of each template t on the development set. This 
is calculated by the following formula: 
strings matched all of #
separators no with strings matched of #
)( =tA . 
In our system, templates whose accuracy is 
lower than the threshold ?1 are discarded. For the 
remaining templates, we apply two different 
strategies. According to our observations of the 
development set, most templates whose accu-
racy is less than ?2 are ineffective. To refine such 
templates, we employ the character class infor-
mation generated by character clustering to im-
pose a class limitation on certain template slots. 
This regulates the potential input and improves 
the precision. Consider a template with one or 
more wildcard slots. If any string matched with 
these wildcard slots contains characters in dif-
ferent clusters, this template is also discarded.  
Template-Based Post-Processing (TBPP) 
After the generated templates have been filtered, 
they are used to match our CWS output and 
check if the matched tokens can be combined 
into complete words. If a template?s accuracy is 
greater than ?2, then all separators within the 
matched strings will be eliminated; otherwise, 
for a template t with accuracy between ?1 and ?2, 
we eliminate all separators in its matched string 
if no substring matched with t?s wildcard slots 
contains characters in different clusters. Resul-
tant words of less than three characters in length 
are discarded because CRF performs well with 
such words. 
3 Experiment 
3.1 Dataset 
We use the three larger corpora in SIGHAN 
Bakeoff 2006: a Simplified Chinese corpus pro-
vided by Microsoft Research Beijing, and two 
Traditional Chinese corpora provided by Aca-
demia Sinica in Taiwan and the City University 
of Hong Kong respectively. Details of each cor-
pus are listed in Table 2. 
Training Size Test SizeCorpus 
Types Words Types Words
CKIP 141 K 5.45 M 19 K 122 K
City University (CTU) 69 K 1.46 M 9 K 41 K
Microsoft Research (MSR) 88 K 2.37 M 13 K 107 K
Table 2. Corpora Information 
3.2 Results 
Table 3 lists the best combination of n-gram fea-
tures used in the G tagger. 
Uni-gram Bigram  
C-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1
Table 3. Best Combination of N-gram Features 
Table 4 compares the baseline G tagger and the 
enhanced GST tagger. We observe that the GST 
tagger outperforms the G tagger on all three cor-
pora. 
Conf R P F ROOV RIV 
CKIP-g 0.958 0.949 0.954 0.690 0.969 
CKIP-gst 0.961 0.953 0.957 0.658 0.974 
CTU-g 0.966 0.967 0.966 0.786 0.973 
CTU-gst 0.973 0.972 0.972 0.787 0.981 
MSR-g 0.949 0.957 0.953 0.673 0.959 
MSR-gst 0.953 0.956 0.955 0.574 0.966 
Table 4 Performance Comparison of the G Tag-
ger and the GST Tagger  
4 Conclusion  
The contribution of this paper is two fold. First, 
we successfully apply the K-means algorithm to 
character clustering and develop several cluster 
set selection algorithms for our GS tagger. This 
significantly improves the handling of sentences 
containing non-Chinese words as well as the 
overall performance. Second, we develop a post-
processing method that compensates for the 
weakness of ML-based CWS on longer words. 
References 
Hartigan, J. A., & Wong, M. A. (1979). A K-means 
Clustering Algorithm. Applied Statistics, 28, 100-
108. 
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. Pa-
per presented at the ICML-01. 
 
 
137
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 142?145,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Using Ensemble Methods for Chinese Named Entity Recognition  
Chia-Wei Wu Shyh-Yi Jan Richard Tzong-Han 
Tsai 
Wen-Lian Hsu 
 
Institute of Information Science, Academia Sinica, Nankang, Taipei,115, Taiwan 
{cwwu,shihyi,thtsai,Hsu}@iis.sinica.edu.tw 
 
  
Abstract 
In sequence labeling tasks, applying dif-
ferent machine learning models and fea-
ture sets usually leads to different results. 
In this paper, we exploit two ensemble 
methods in order to integrate multiple 
results generated under different condi-
tions. One method is based on majority 
vote, while the other is a memory-based 
approach that integrates maximum en-
tropy and conditional random field clas-
sifiers. Our results indicate that the 
memory-based method can outperform 
the individual classifiers, but the major-
ity vote method cannot. 
1 Introduction 
Sequence labeling and segmentation tasks have 
been studied extensively in the fields of computa-
tional linguistics and information extraction. Sev-
eral tasks, including, word segmentation, and 
semantic role labeling, provide rich information 
for various applications, such as segmentation in 
Chinese information retrieval and named entity 
recognition in biomedical literature mining.  
Probabilistic state automata models, such as the 
Hidden Markov model  (HMM) [6] and condi-
tional random fields (CRF) [5] are some of best, 
and therefore most popular, approaches for se-
quence labeling tasks. Both HMM and CRF con-
sider that the state transition and the state 
prediction are conditional on the observation of 
data. The advantage of the CRF model is that 
richer feature sets can be considered, because, 
unlike HMM, it does not make a dependence as-
sumption. However, the obvious drawback of the 
CRF model is that it needs more computing re-
sources, so we can not apply all the features of 
the model. One possible way to resolve this prob-
lem is to effectively combine the results of vari-
ous individual classifiers trained with different 
feature sets. In this paper, we use two ensemble 
methods to combine the results of the classifiers. 
We also combine the results generated by two 
machine learning models: maximum entropy 
(ME) [1] and CRF. One ensemble method is 
based on the majority vote [3], and the other is 
the memory based learner [7]. Although the en-
semble methods have been applied in some se-
quence labeling tasks [2],[3], similar work in 
Chinese named entity recognition is scarce. 
Our Chinese named entity tagger uses a charac-
ter-based model. For English named entity tasks, 
a character-based NER model proposed by Dan 
Klein [4] proves the usefulness of substrings 
within words. In Chinese NER, the character-
based model is more straightforward, since there 
are no spaces between Chinese words and each 
Chinese character is actually meaningful.  An-
other reason for using a character-based model is 
that it can avoid the errors sometimes made by a 
Chinese word segmentor.  
The remainder of this paper is organized as fol-
lows. In the Section 2, we introduce the machine 
learning models, the features we apply in the ma-
chine learning models, and the ensemble methods. 
In Section 3, we briefly describe the experimental 
data and the experiment results. Then, in Section 
4, we present our conclusions.. 
2 Method 
2.1 Machine Learning Models 
In this section, we introduce ME and CRF.
Maximum Entropy 
ME[1] is a statistical modeling technique used 
for estimating the conditional probability of a 
target label based on given information. The 
technique computes the probability p(y|x), where 
y denotes all possible outcomes of the space, and 
x denotes all possible features of the space. The 
computation of p(y|x) depends on a set of fea-
142
tures in x; the features are helpful for making 
predictions about the outcomes, y. 
Given a set of features and a training set, the ME 
estimation process produces a model, in which 
every feature fi has a weight ?i. The ME model 
can be represented by the following formula: 
( ) ( ) ( )???
?
???
?= ?
i
ii yxfxz
xyp ,exp| ?1
, 
( ) ( )? ? ???
?
???
?=
y i
ii yxfxz ,exp ?
.  
The probability is derived by multiplying the 
weights of the active features (i.e., those fi (y,x) = 
1). 
Conditional Random Field 
A conditional random field (CRF)[5] can be seen 
as an undirected graph model in which the nodes 
corresponding to the label sequence y are condi-
tional on the observed sequence x. The goal of 
CRF is to find the label sequence y that has the 
maximized probability, given an observation se-
quence x. The formula for the CRF model can be 
written as: 
( ) ( ) ( )( )xyxxy ,exp1| jj j FZP ?= ? ,  
where ?j is the parameter of a corresponding fea-
ture Fj , Z(x) is an normalizing factor, and Fj can 
be written as:   
( ) (? = ?= ni iiij iyyfF 0 1 ,,,, xxy ), 
where i means the relative position in the se-
quence, and yi-1 and yi denote the label at position 
i-1 and i respectively. In this paper, we only con-
sider linear chain and first-order Markov assump-
tion CRFs. In NER applications, a feature 
function fj (yi-1, yi, x, i) can be set to check 
whether x is a specific character, and whether yi-1 
is a label (such as Location) and yi is a label (such 
as Others).   
2.2 Chinese Named Entity Recognition 
In this section, we present the features applied in 
our CRF and ME models, namely, characters, 
words, and chuck information. 
Character Features 
The character features we apply in the CRF 
model and the ME model are presented in Tables 
1 and 2 respectively. The numbers listed in the 
feature type column indicate the relative position 
of a character in the sliding window. For example, 
-1 means the previous character of the target 
character. Therefore, the characters in those posi-
tions are applied in the model. The numbers in 
parentheses mean that the feature includes a 
combination of the characters in those positions. 
The unigrams in Tables 1 and 2 indicate that the 
listed features only consider to their own labels, 
whereas the bigram model considers the combi-
nation of the current label and the previous label. 
Since ME does not consider multiple states in a 
single feature, there are only unigrams in Table 2. 
In addition, as ME can handle more features than 
CRF, we apply extra features in the ME model  
 
Table 1 Character features for CRF 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) 
bigram -2 -1 0 +1 +2, (0,1) 
 
Table 2 Character features for ME 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) (-1,1) 
Word Information  
Because of the limitations of the closed task, we 
use the NER corpus to train the segmentors based 
on the CRF model. To simulate noisy word in-
formation in the test corpus, we use a ten-fold 
method for training segmentors to tag the training 
corpus. The word features we apply in our NER 
systems are presented in Tables 3 and 4. 
In addition to the word itself, chuck information, 
i.e., the relative position of a character in a word, 
is also valuable information. Hence, we also add 
chuck information to our models. As the diversity 
of Chinese words is greater than that of Chinese 
characters, the number of features that can be 
used in CRF is much lower than the number that 
can be used in ME.   
Table 3 Word features for CRF 
 Feature Types 
unigram 0 
bigram 0  
 
Table 4 Word features for ME 
 Feature Types 
unigram -1, 0, 1, (-2,-1), (-1,0), (0,1), (1,2) 
2.3 Ensemble Methods 
Majority vote 
We can not put all the features into the CRF 
model because of its limited resources. Therefore, 
we train several CRF classifiers with different 
feature sets so that we can use as many features 
143
as possible. Then, we use the following simple, 
equally weighted linear equation, called majority 
vote, to combine the results of the CRF classifi-
ers.   
( ) ( )? == Ti i xyCxyS 0 ,, , 
where S(y,x) is the score of a label y and a char-
acter x respectively; T denotes the total number 
of CRF models; and the value of Ci(y,x) is 1 if 
the decision of the result of the ith CRF model is 
y, otherwise it is zero. The highest score of y is 
chosen as the label of x. The results are incorpo-
rated into the Viterbi algorithm to search for the 
path with the maximum scores. 
In this paper, the first step in the majority vote 
experiment is to train three CRF classifiers with 
different feature sets. Then, in the second step, 
we use the results obtained in the first step to 
generate the voting scores for the Viterbi algo-
rithm. 
Memory Based learner 
The memory-based learning method memorizes 
all examples in a training corpus. If a word is 
unknown, the memory-based classifier uses the 
k-nearest neighbors to find the most similar ex-
ample as the answer. Instead of using the com-
plete algorithm of the memory-based learner, we 
do not handle unseen data. In our memory- based 
combination method, the learner remembers all 
named entities from the results of the various 
classifiers and then tags the characters that were 
originally tagged as ?Other?. For example, if a 
character x is tagged by one classifier as ?0? 
(?Others? tag) and if the memory-based classifier 
learns from another classifier that this character 
is tagged as PER, then x will be tagged as ?B-
PER? by the memory-based classifier. 
The obvious drawback of this method is that the 
precision rate might decrease as the recall rate 
increases. Therefore, we set the following three 
rules to filter out samples that are likely to have a 
high error rate.  
1. Named entities can not be tagged as differ-
ent named entity tags by different classifiers. 
2. We set an absolute frequency threshold to 
filter out examples that occur less than the 
threshold. 
3. We set a relative frequency threshold to 
filter out examples that occur less than the 
threshold. For example, if a word x appears 
10 times in the corpus, then half of the in-
stances of x have to be tagged as named en-
tities; otherwise, x will be filtered out of the 
memory classifier. 
In our experiment, we used the memory-based 
learner to memorize the named entities from the 
tagging results of an ME classifier and a CRF 
classifier, and then tagged the tagging results of 
the CRF classifier.   
3 Experiments 
3.1 Data 
We selected the corpora of City University of 
Hong Kong (CityU) and Microsoft Research 
(MSRA) corpora to evaluate our methods. CityU 
is a Traditional Chinese corpus, and MSRA is 
Simplified Chinese corpus. 
3.2 Results 
Table 5 shows the results of several methods ap-
plied to the MSRA corpus. The memory-based 
ensemble method, which combines the results of 
a maximum entropy model and those of a CRF 
classifier, achieves the best performance. The 
majority vote combined with the results of three 
CRF models based on different feature sets has 
the worst performance. 
 
Table 5 msra  
 Precision Recall FB1 
Memory based 86.21 78.14 81.98 
Majority Vote 85.83 76.06 80.65 
Only-Character 86.70 75.54 80.74 
CRF 86.23 77.40 81.58 
 
The results obtained on Cityu, presented in Table 
6, show that the single CRF classifier achieved 
the best performance. None of the ensemble 
methods can outperform the non-ensemble meth-
ods. 
 
Table 6 cityu 
 Precision Recall FB1 
Memory based  90.79 86.26 88.47 
Majority Vote 90.52 84.15 87.22 
Only-Character 91.32 84.55 87.80 
CRF 92.01 85.45 88.61 
 
Tables 7 and 8 show the results of the memory-
based ensemble methods under different rules. 
We set the frequency threshold as 2 and the rela-
tive frequency threshold as 0.5. The results show 
that the relative frequencies rule effectively re-
duces the loss of precision caused by more enti-
ties being tagged by the memory-based classifier. 
The memory-based ensemble method works well 
on the MSRA corpus, but not on the CityU cor-
pus. In the MSRA corpus, the memory-based 
144
ensemble method outperforms the individual 
CRF model by approximately 0.4 % in FB1. We 
found that the memory-based classifier can not 
achieve a better performance than the CRF model 
because it misclassifies many organizations? 
names. Therefore, we chose another strategy that 
restricts the memory-based classifier to tagging 
person names only. Under this restriction, the 
performance of the memory-based classifier im-
proves FB1 by approximately 0.2%. 
 
Table 7 msra- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 86.18 78.16 81.97 
Relative Frequency 
Threshold 
86.21 78.14 81.98 
Only Person 86.27 77.58 81.69 
 
Table 8 cityu- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 90.69 86.55 88.57 
Relative Frequency 
Threshold 
90.87 86.29 88.52 
Only Person 92.00 85.66 88.72 
4 Conclusion  
In this paper, we use ME and CRF models to 
train a Chinese named entity tagger. Like previ-
ous researchers, we found that CRF models out-
perform ME models. We also apply two 
ensemble methods, namely, majority vote and 
memory-based approaches, to the closed NER 
shared task. Our results show that integrating 
individual classifiers as the majority vote ap-
proach does not outperform the individual classi-
fiers. Furthermore, a memory-based combination 
only seems to work when we restrict the mem-
ory-based classifier to handling person names. 
Acknowledgement 
We are grateful for the support of National Sci-
ence Council under Grant NSC 95-2752-E-001-
001-PAE. 
References  
1. Berger, A., Pietra, S.A.D. and Pietra, V.J.D. A 
Maximum Entropy Approach to Natural Language 
Processing. Computer Linguistic, 22. 1996 39-71. 
2. Florian, R., Ittycheriah, A., Jing, H. and Zhang, T., 
Named Entity Recognition through Classifier 
Combination. in Proceedings of Conference on 
Computational Natural Language Learning, 2003, 
168-171. 
3. Halteren, H.v., Zavrel, J. and Daelemans, W. Im-
proving accuracy in word class tagging through 
combination of machine learning systems. Compu-
tational Linguistics, 27 (2). 2001 199-230. 
4. Klein, D., Smarr, J., Nguyen, H. and Manning, 
C.D., Named Entity Recognition with Character-
Level Models. in Conference on Computational 
Natural Language Learning, 2003, 180-183. 
5. Lafferty, J., McCallum, A. and Pereira, F. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. International 
Conference on Machine Learning. 2001 282-289. 
6. Rabiner, L. A tutorial on hidden Markov models 
and selected applications in speech recognition. 
Proceedings of the IEEE, 77 (2). 1989 257-286. 
7. Sutton, C., Rohanimanesh, K. and McCallum, A., 
Dynamic Conditional Random Fields: Factorized 
Probabilistic Models for Labeling and Segmenting 
Sequence Data. in Proceedings of the Twenty-First 
International Conference on Machine Learning, 
2004, 99-107. 
8. Zavrel, J. and Daelemans, W. Memory-based learn-
ing: using similarity for smoothing. Proceedings of 
the eighth conference on European chapter of the 
Association for Computational Linguistics. 1997 
436 - 443. 
145
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 5?12,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Semi-Automatic Method for  
Annotating a Biomedical Proposition Bank 
 
Wen-Chi Chou1, Richard Tzong-Han Tsai1,2, Ying-Shan Su1, 
Wei Ku1,3, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica, Taiwan, ROC. 
2Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan, ROC. 
3Institute of Molecular Medicine, National Taiwan University, Taiwan, ROC. 
{jacky957,thtsai,qnn,wilmaku,tsung,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
In this paper, we present a semi-
automatic approach for annotating se-
mantic information in biomedical texts. 
The information is used to construct  
a biomedical proposition bank called 
BioProp. Like PropBank in the newswire 
domain, BioProp contains annotations of 
predicate argument structures and seman-
tic roles in a treebank schema. To con-
struct BioProp, a semantic role labeling 
(SRL) system trained on PropBank is 
used to annotate BioProp. Incorrect tag-
ging results are then corrected by human 
annotators. To suit the needs in the bio-
medical domain, we modify the Prop-
Bank annotation guidelines and charac-
terize semantic roles as components of 
biological events. The method can sub-
stantially reduce annotation efforts, and 
we introduce a measure of an upper 
bound for the saving of annotation efforts. 
Thus far, the method has been applied 
experimentally to a 4,389-sentence tree-
bank corpus for the construction of Bio-
Prop. Inter-annotator agreement meas-
ured by kappa statistic reaches .95 for 
combined decision of role identification 
and classification when all argument la-
bels are considered. In addition, we show 
that, when trained on BioProp, our bio-
medical SRL system called BIOSMILE 
achieves an F-score of 87%. 
1 Introduction 
The volume of biomedical literature available on 
the Web has grown enormously in recent years, a 
trend that will probably continue indefinitely. 
Thus, the ability to process literature automati-
cally would be invaluable for both the design and 
interpretation of large-scale experiments. To this 
end, several information extraction (IE) systems 
using natural language processing techniques 
have been developed for use in the biomedical 
field. Currently, the focus of IE is shifting from 
the extraction of nominal information, such as 
named entities (NEs) to verbal information that 
represents the relations between NEs, e.g., events 
and function (Tateisi et al, 2004; Wattarujeekrit 
et al, 2004). In the IE of relations, the roles of 
NEs participating in a relation must be identified 
along with a verb of interest. This task involves 
identifying main roles, such as agents and objects, 
and adjunct roles (ArgM), such as location, man-
ner, timing, condition, and extent. This identifi-
cation task is called semantic role labeling (SRL). 
The corresponding roles of the verb (predicate) 
are called predicate arguments, and the whole 
proposition is known as a predicate argument 
structure (PAS). 
To develop an automatic SRL system for the 
biomedical domain, it is necessary to train the 
system with an annotated corpus, called proposi-
tion bank (Palmer et al, 2005). This corpus con-
tains annotations of semantic PAS?s superim-
posed on the Penn Treebank (PTB) (Marcus et 
al., 1993; Marcus et al, 1994). However, the 
process of manually annotating the PAS?s to 
construct a proposition bank is quite time-
consuming. In addition, due to the complexity of 
proposition bank annotation, inconsistent annota-
tion may occur frequently and further complicate 
5
the annotation task. In spite of the above difficul-
ties, there are proposition banks in the newswire 
domain that are adequate for training SRL sys-
tems (Xue and Palmer, 2004; Palmer et al, 2005). 
In addition, according to the CoNLL-2005 
shared task (Carreras and M?rquez, 2005), the 
performance of SRL systems in general does not 
decline significantly when tagging out-of-domain 
corpora. For example, when SRL systems trained 
on the Wall Street Journal (WSJ) corpus were 
used to tag the Brown corpus, the performance 
only dropped by 15%, on average. In comparison 
to annotating from scratch, annotation efforts 
based on the results of an available SRL system 
are much reduced. Thus, we plan to use a news-
wire SRL system to tag a biomedical corpus and 
then manually revise the tagging results. This 
semi-automatic procedure could expedite the 
construction of a biomedical proposition bank for 
use in training a biomedical SRL system in the 
future. 
2 The Biomedical Proposition Bank - 
BioProp 
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different 
syntactic realizations of the same verb. The an-
notation captures predicate-argument structures 
based on the sense tags of polysemous verbs 
(called framesets) and semantic role labels for 
each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors ac-
tivate STAT6, STAT3 and STAT5 proteins in 
normal human B cells.? The chosen predicate is 
the word ?activate?; its arguments and their as-
sociated word groups are illustrated in the figure. 
 
IL4 and IL 13 
 receptors 
activate STAT6, STAT3 
and  
STAT5 proteins 
the human 
B cells 
in 
NP 
Arg0 predicate AM-LOC Arg1 
NP 
NP-SBJ VP 
VP PP 
Figure 1. A treebank annotated with semantic 
role labels 
Since proposition banks are annotated on top 
of a Penn-style treebank, we selected a biomedi-
cal corpus that has a Penn-style treebank as our 
corpus. We chose the GENIA corpus (Kim et al, 
2003), a collection of MEDLINE abstracts se-
lected from the search results with the following 
keywords: human, blood cells, and transcription 
factors. In the GENIA corpus, the abstracts are 
encoded in XML format, where each abstract 
also contains a MEDLINE UID, and the title and 
content of the abstract. The text of the title and 
content is segmented into sentences, in which 
biological terms are annotated with their seman-
tic classes. The GENIA corpus is also annotated 
with part-of-speech (POS) tags (Tateisi and Tsu-
jii, 2004), and co-references are added to part of 
the GENIA corpus by the MedCo project at the 
Institute for Infocomm Research, Singapore 
(Yang et al, 2004). 
The Penn-style treebank for GENIA, created 
by Tateisi et al (2005), currently contains 500 
abstracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the 
Penn Treebank II (PTB) scheme (Bies et al, 
1995), is encoded in XML. However, in contrast 
to the WSJ corpus, GENIA lacks a proposition 
bank. We therefore use its 500 abstracts with 
GTB as our corpus. To develop our biomedical 
proposition bank, BioProp, we add the proposi-
tion bank annotation on top of the GTB annota-
tion. 
In the following, we report on the selection of 
biomedical verbs, and explain the difference be-
tween their meaning in PropBank (Palmer et al, 
2005), developed by the University of Pennsyl-
vania, and their meaning in BioProp (a biomedi-
cal proposition bank). We then introduce Bio-
Prop?s annotation scheme, including how we 
modify a verb?s framesets and how we define 
framesets for biomedical verbs not defined in 
VerbNet (Kipper et al, 2000; Kipper et al, 2002). 
2.1 Selection of Biomedical Verbs 
We selected 30 verbs according to their fre-
quency of use or importance in biomedical texts. 
Since our targets in IE are the relations of NEs, 
only sentences containing protein or gene names 
are used to count each verb?s frequency. Verbs 
that have general usage are filtered out in order 
to ensure the focus is on biomedical verbs. Some 
verbs that do not have a high frequency, but play 
important roles in describing biomedical rela-
tions, such as ?phosphorylate? and ?transacti-
vate?, are also selected. The selected verbs are 
listed in Table 1. 
6
 Predicate Frameset Example 
express  
(VerbNet) 
Arg0: agent  
Arg1: theme 
Arg2: recipient or destina-
tion 
[Some legislatorsArg0][expressedpredicate] [concern that a gas-tax 
increase would take too long and possibly damage chances of a 
major gas-tax-increasing ballot initiative that voters will consider 
next JuneArg1 ]. 
translate 
(VerbNet) 
Arg0: causer of transfor-
mation  
Arg1: thing changing   
Arg2: end state 
Arg3: start state 
But some cosmetics-industry executives wonder whether [tech-
niques honed in packaged goodsArg1] [willAM-MOD] [translatepredicate] 
[to the cosmetics businessArg2]. 
express  
(BioProp) 
Arg0: causer of expression 
Arg1: thing expressing 
[B lymphocytes and macrophagesArg0] [expresspredicate] [closely 
related immunoglobulin G ( IgG ) Fc receptors ( Fc gamma RII ) 
that differ only in the structures of their cytoplasmic domainsArg1]. 
Table 2. Framesets and examples of ?express? and ?translate? 
 
Type Verb list 
1 encode, interact, phosphorylate,  transactivate 
2 express, modulate 
3 bind 
4 
activate, affect, alter, associate, block, 
decrease differentiate, encode, enhance, 
increase, induce, inhibit, mediate, mu-
tate, prevent, promote, reduce, regulate, 
repress, signal, stimulate, suppress, 
transform, trigger 
Table 1. Selected biomedical verbs and their 
types 
2.2 Framesets of Biomedical Verbs 
Annotation of BioProp is mainly based on 
Levin?s verb classes, as defined in the VerbNet 
lexicon (Kipper et al, 2000). In VerbNet, the 
arguments of each verb are represented at the 
semantic level, and thus have associated seman-
tic roles. However, since some verbs may have 
different usages in biomedical and newswire 
texts, it is necessary to customize the framesets 
of biomedical verbs. The 30 verbs in Table 1 are 
categorized into four types according to the de-
gree of difference in usage: (1) verbs that do not 
appear in VerbNet due to their low frequency in 
the newswire domain; (2) verbs that do appear in 
VerbNet, but whose biomedical meanings and 
framesets are undefined; (3) verbs that do appear 
in VerbNet, but whose primary newswire and 
biomedical usage differ; (4) verbs that have the 
same usage in both domains. 
Verbs of the first type play important roles in 
biomedical texts, but rarely appear in newswire 
texts and thus are not defined in VerbNet. For 
example, ?phosphorylate? increasingly appears 
in the fast-growing PubMed abstracts that report 
experimental results on phosphorylated events; 
therefore, it is included in our verb list. However, 
since VerbNet does not define the frameset for 
?phosphorylate?, we must define it after analyz-
ing all the sentences in our corpus that contain 
the verb. Other type 1 verbs may correspond to 
verbs in VerbNet; in such cases, we can borrow 
the VerbNet definitions and framesets. For ex-
ample, ?transactivate? is not found in VerbNet, 
but we can adopt the frameset of ?activate? for 
this verb. 
Verbs of the second type appear in VerbNet, 
but have unique biomedical meanings that are 
undefined. Therefore, the framesets correspond-
ing to their biomedical meanings must be added. 
In most cases, we can adopt framesets from 
VerbNet synonyms. For example, ?express? is 
defined as ?say? and ?send very quickly? in 
VerbNet. However, in the biomedical domain, its 
usage is very similar to ?translate?. Thus, we can 
use the frameset of ?translate? for ?express?. Ta-
ble 2 shows the framesets and corresponding ex-
amples of ?express? in the newswire domain and 
biomedical domain, as well as that of ?translate? 
in VerbNet.  
Verbs of the third type also appear in VerbNet. 
Although the newswire and biological senses are 
defined therein, their primary newswire sense is 
not the same as their primary biomedical sense. 
?Bind,? for example, is common in the newswire 
domain, and it usually means ?to tie? or ?restrain 
with bonds.? However, in the biomedical domain, 
its intransitive use- ?attach or stick to?- is far 
more common. For example, a Google search for 
the phrase ?glue binds to? only returned 21 re-
sults, while the same search replacing ?glue? 
with ?protein? yields 197,000 hits. For such 
verbs, we only need select the appropriate alter-
native meanings and corresponding framesets. 
Lastly, for verbs of the fourth type, we can di-
7
rectly adopt the newswire definitions and frame-
sets, since they are identical.  
2.3 Distribution of Selected Verbs 
There is a significant difference between the oc-
currence of the 30 selected verbs in biomedical 
texts and their occurrence in newswire texts. The 
verbs appearing in verb phrases constitute only 
1,297 PAS?s, i.e., 1% of all PAS?s, in PropBank 
(shown in Figure 2), compared to 2,382 PAS?s, 
i.e., 16% of all PAS?s, in BioProp (shown in 
Figure 3). Furthermore, some biomedical verbs 
have very few PAS?s in PropBank, as shown in 
Table 3. The above observations indicate that it 
is necessary to annotate a biomedical proposition 
bank for training a biomedical SRL system. 
 
Figure 2. The percentage of the 30 biomedical 
verbs and other verbs in PropBank 
 
Figure 3. The percentage of the 30 biomedical 
verbs and other verbs in BioProp 
3  Annotation of BioProp 
3.1 Annotation Process 
After choosing 30 verbs as predicates, we 
adopted a semi-automatic method to annotate 
BioProp. The annotation process consists of the 
following steps: (1) identifying predicate candi-
dates; (2) automatically annotating the biomedi-
cal semantic roles with our WSJ SRL system; (3) 
transforming the automatic tagging results into 
WordFreak (Morton and LaCivita, 2003) format; 
and (4) manually correcting the annotation re-
sults with the WordFreak annotation tool. We 
now describe these steps in detail: 
  
Verbs # in BioProp Ratio(%) 
# in 
PropBank Ratio(%) 
induce 290 1.89 16 0.01 
bind 252 1.64 0 0 
activate 235 1.53 2 0 
express 194 1.26 53 0.03 
inhibit 184 1.20 6 0 
increase 166 1.08 396 0.24 
regulate 122 0.79 23 0.01 
mediate 104 0.68 1 0 
stimulate 93 0.61 11 0.01 
associate 82 0.53 51 0.03 
encode 79 0.51 0 0 
affect 60 0.39 119 0.07 
enhance 60 0.39 28 0.02 
block 58 0.38 71 0.04 
reduce 55 0.36 241 0.14 
decrease 54 0.35 16 0.01 
suppress 38 0.25 4 0 
interact 36 0.23 0 0 
alter 27 0.18 17 0.01 
transactivate 24 0.16 0 0 
modulate 22 0.14 1 0 
phosphorylate 21 0.14 0 0 
transform 21 0.14 22 0.01 
differentiate 21 0.14 2 0 
repress 17 0.11 1 0 
prevent 15 0.10 92 0.05 
promote 14 0.09 52 0.03 
trigger 14 0.09 40 0.02 
mutate 14 0.09 1 0 
signal 10 0.07 31 0.02 
Table 3. The number and percentage of PAS?s 
for each verb in BioProp and PropBank 
1. Each word with a VB POS tag in a verb 
phrase that matches any lexical variant of 
the 30 verbs is treated as a predicate candi-
date. The automatically selected targets are 
then double-checked by human annotators. 
As a result, 2,382 predicates were identified 
in BioProp.  
2. Sentences containing the above 2,382 
predicates were extracted and labeled 
automatically by our WSJ SRL system. In 
total, 7,764 arguments were identified. 
3. In this step, sentences with PAS annota-
tions are transformed into WordFreak for-
mat (an XML format), which allows anno-
tators to view a sentence in a tree-like fash-
ion. In addition, users can customize the tag 
set of arguments. Other linguistic informa-
tion can also be integrated and displayed in 
8
WordFreak, which is a convenient annota-
tion tool. 
4. In the last step, annotators check the pre-
dicted semantic roles using WordFreak and 
then correct or add semantic roles if the 
predicted arguments are incorrect or miss-
ing, respectively. Three biologists with suf-
ficient biological knowledge in our labora-
tory performed the annotation task after re-
ceiving computational linguistic training 
for approximately three months.   
Figure 4 illustrates an example of BioProp an-
notation displayed in WordFreak format, using 
the frameset of ?phophorylate? listed in Table 4.  
This annotation process can be used to con-
struct a domain-specific corpus when a general-
purpose tagging system is available.  In our ex-
perience, this semi-automatic annotation scheme 
saves annotation efforts and improves the anno-
tation consistency. 
 
Predicate Frameset 
phosphorylate  
 
Arg0: causer of phosphorylation 
Arg1: thing being phosphorylated 
Arg2: end state  
Arg3: start state 
Table 4. The frameset of ?phosphorylate? 
3.2 Inter-annotation Agreement 
We conducted preliminary consistency tests on 
2,382 instances of biomedical propositions. The 
inter-annotation agreement was measured by the 
kappa statistic (Siegel and Castellan, 1988), the 
definition of which is based on the probability of 
inter-annotation agreement, denoted by P(A), and 
the agreement expected by chance, denoted by 
P(E). The kappa statistics for inter-annotation 
agreement were .94 for semantic role identifica-
tion and .95 for semantic role classification when 
ArgM labels were included for evaluation. When 
ArgM labels were omitted, kappa statistics 
were .94 and .98 for identification and classifica-
tion, respectively. We also calculated the results 
of combined decisions, i.e., identification and 
classification. (See Table 5.) 
3.3 Annotation Efforts 
Since we employ a WSJ SRL system that labels 
semantic roles automatically, human annotators 
can quickly browse and determine correct tag-
ging results; thus, they do not have to examine  
 
Figure 4. An example of BioProp displayed with 
WordFreak 
 
  P(A) P(E) Kappa 
score 
role identification .97 .52 .94 
role classification .96 .18 .95 including ArgM 
combined decision .96 .18 .95 
role identification .97 .26 .94 
role classification .99 .28 .98 excluding ArgM 
combined decision .99 .28 .98 
Table 5. Inter-annotator agreement 
all tags during the annotation process, as in the 
full manual annotation approach. Only incor-
rectly predicted tags need to be modified, and 
missed tags need to be added. Therefore, annota-
tion efforts can be substantially reduced. To 
quantify the reduction in annotation efforts, we 
define the saving of annotation effort, ?, as: 
)1(
nodes missed of# incorrect  of # correct  of #
nodes  labeled correctly  of #
nodes all of#
nodes  labeled correctly  of #
++
<
=?
 
In Equation (1), since the number of nodes 
that need to be examined is usually unknown, we 
9
use an easy approximation to obtain an upper 
bound for ?. This is based on the extremely op-
timistic assumption that annotators should be 
able to recover a missed or incorrect label by 
only checking one node. However, in reality, this 
would be impossible. In our annotation process, 
the upper bound of ? for BioProp is given by: 
%46
40975
18932
15316668218932
18932
==
++
<? , 
which means that, at most, the annotation effort 
could be reduced by 46%. 
A more accurate tagging system is preferred 
because the more accurate the tagging system, 
the higher the upper bound ? will be.  
4 Disambiguation of Argument Annota-
tion 
During the annotation process, we encountered a 
number of problems resulting from different us-
age of vocabulary and writing styles in general 
English and the biomedical domain. In this sec-
tion, we describe three major problems and pro-
pose our solutions. 
4.1 Cue Words for Role Classification 
PropBank annotation guidelines provide a list of 
words that can help annotators decide an argu-
ment?s type. Similarly, we add some rules to our 
BioProp annotation guideline. For example, ?in 
vivo? and ?in vitro? are used frequently in bio-
medical literature; however, they seldom appear 
in general English articles. According to their 
meanings, we classify them as location argument 
(AM-LOC).  
In addition, some words occur frequently in 
both general English and in biomedical domains 
but have different meanings/usages. For instance, 
?development? is often tagged as Arg0 or Arg1 
in general English, as shown by the following 
sentence: 
 
Despite the strong case for stocks, however, most 
pros warn that [individualsArg0] shouldn't try to 
[profitpredicate] [from short-term developmentsArg1].  
 
However, in the biomedical domain, ?devel-
opment? always means the stage of a disease, 
cell, etc. Therefore, we tag it as temporal argu-
ment (AM-TMP), as shown in the following sen-
tence: 
 
[Rhom-2 mRNAArg1] is [expressedpredicate] [in 
early mouse developmentAM-TMP] [in central 
nervous system, lung, kidney, liver, and spleen 
but only very low levels occur in thymusAM-LOC]. 
4.2 Additional Argument Types 
In PropBank, the negative argument (AM-NEG) 
usually contains explicit negative words such as 
?not?. However, in the biomedical domain, re-
searchers usually express negative meaning im-
plicitly by using ?fail?, ?unable?, ?inability?, 
?neither?, ?nor?, ?failure?, etc. Take ?fail? as an 
example. It is tagged as a verb in general English, 
as shown in the following sentence: 
 
But [the new pactArg1] will force huge debt on the 
new firm and [couldAM-MOD] [stillAM-TMP] [failpredi-
cate] [to thwart rival suitor McCaw CellularArg2]. 
 
Negative results are important in the biomedi-
cal domain. Thus, for annotation purposes, we 
create additional negation tag (AM-NEG1) that 
does not exist in PropBank. The following sen-
tence is an example showing the use of AM-
NEG1: 
  
[TheyArg0] [failAM-NEG1] to [inducepredicate] [mRNA 
of TNF-alphaArg1] [after 3 h of culture AM-TMP]. 
  
In this example, if we do not introduce the 
AM-NEG1, ?fail? is considered as a verb like in 
PropBank, not as a negative argument, and it will 
not be included in the proposition for the predi-
cate ?induce?. Thus, BioProp requires the ?AM-
NEG1? tag to precisely express the correspond-
ing proposition. 
4.3 Essentiality of Biomedical Knowledge 
Since PAS?s contain more semantic information, 
proposition bank annotators require more domain 
knowledge than annotators of other corpora. In 
BioProp, many ambiguous expressions require 
biomedical knowledge to correctly annotate them, 
as exemplified by the following sentence in Bio-
Prop: 
 
In the cell types tested, the LS mutations indi-
cated an apparent requirement not only for the 
intact NF-kappa B and SP1-binding sites but also 
for [several regions between -201 and -130Arg1] 
[notAM-NEG] [previouslyAM-MNR] [associatedpredi-
cate][with viral infectivityArg2]. 
 
Annotators without biomedical knowledge 
may consider [between -201 and -130] as extent 
argument (AM-EXT), because the PropBank 
guidelines define numerical adjuncts as AM-
10
EXT. However, it means a segment of DNA. It is 
an appositive of [several regions]; therefore, it 
should be annotated as part of Arg1 in this case. 
5 Effect of Training Corpora on SRL 
Systems 
To examine the possibility that BioProp can im-
prove the training of SRL systems used for 
automatic tagging of biomedical texts, we com-
pare the performance of systems trained on Bio-
Prop and PropBank in different domains. We 
construct a new SRL system (called a BIOmedi-
cal SeMantIc roLe labEler, BIOSMILE) that is 
trained on BioProp and employs all the features 
used in our WSJ SRL system (Tsai et al, 2006).  
As with POS tagging, chunking, and named 
entity recognition, SRL can also be formulated as 
a sentence tagging problem. A sentence can be 
represented by a sequence of words, a sequence 
of phrases, or a parsing tree; the basic units of a 
sentence in these representations are words, 
phrases, and constituents, respectively. Hacioglu 
et al (2004) showed that tagging phrase-by-
phrase (P-by-P) is better than word-by-word (W-
by-W). However, Punyakanok et al (2004) 
showed that constituent-by-constituent (C-by-C) 
tagging is better than P-by-P. Therefore, we use 
C-by-C tagging for SRL in our BIOSMILE. 
SRL can be divided into two steps. First, we 
identify all the predicates. This can be easily ac-
complished by finding all instances of verbs of 
interest and checking their part-of-speech (POS) 
tags. Second, we label all arguments correspond-
ing to each predicate. This is a difficult problem, 
since the number of arguments and their posi-
tions vary according to a verb?s voice (ac-
tive/passive) and sense, along with many other 
factors.  
In BIOSMILE, we employ the maximum en-
tropy (ME) model for argument classification. 
We use Zhang?s MaxEnt toolkit 
(http://www.nlplab.cn/zhangle/maxent_toolkit.ht
ml) and the L-BFGS (Nocedal and Wright, 1999) 
method of parameter estimation for our ME 
model. Table 6 shows the features we employ in 
BIOSMILE and our WSJ SRL system. 
To compare the effects of using biomedical 
training data versus using general English data, 
we train BIOSMILE on 30 randomly selected 
training sets from BioProp (g1,.., g30), and WSJ 
SRL system on 30 from PropBank (w1,.., w30), 
each of which has 1,200 training PAS?s. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree 
from the parse constituent being classified to the 
predicate 
z Constituent type 
z Position ? Whether the phrase is located before or af-
ter the predicate 
z Voice ? passive: If the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form 
of ?to be? or ?to get? within its chunk; otherwise, it is 
active 
z Head word ? Calculated using the head word table 
described by Collins (1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that 
expands the predicate?s parent node in the parsing 
tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, 
constituent types, positions, head words and head 
POS tags 
z Head of PP parent ? If the parent is a PP, then the 
head of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 6. The features used in our argument clas-
sification model 
 We then test both systems on 30 400-PAS test 
sets from BioProp, with g1 and w1 being tested on 
test set 1, g2 and w2 on set 2, and so on. Then we 
generate the scores for g1-g30 and w1-w30, and 
compare their averages. 
Table 7 shows the experimental results. When 
tested on the biomedical corpus, BIOSMILE out-
performs the WSJ SRL system by 22.9%. This 
result is statistically significant as expected. 
 
Training Test Precision Recall F-score 
PropBank BioProp 74.78 56.25 64.20 
BioProp BioProp 88.65 85.61 87.10 
Table 7. Performance comparison of SRL sys-
tems trained on BioProp and PropBank 
11
6 Conclusion & Future Work 
The primary contribution of this study is the an-
notation of a biomedical proposition bank that 
incorporates the following features. First, the 
choice of 30 representative biomedical verbs is 
made according to their frequency and impor-
tance in the biomedical domain. Second, since 
some of the verbs have different usages and oth-
ers do not appear in the WSJ proposition bank, 
we redefine their framesets and add some new 
argument types. Third, the annotation guidelines 
in PropBank are slightly modified to suit the 
needs of the biomedical domain. Fourth, using 
appropriate argument types, framesets and anno-
tation guidelines, we construct a biomedical 
proposition bank, BioProp, on top of the popular 
biomedical GENIA Treebank. Finally, we em-
ploy a semi-automatic annotation approach that 
uses an SRL system trained on the WSJ Prop-
Bank. Incorrect tagging results are then corrected 
by human annotators. This approach reduces an-
notation efforts significantly. For example, in 
BioProp, the annotation efforts can be reduced 
by, at most, 46%. In addition, trained on BioProp, 
BIOSMILE?s F-score increases by 22.9% com-
pared to the SRL system trained on the PropBank. 
In our future work, we will investigate more 
biomedical verbs. Besides, since there are few 
biomedical treebanks, we plan to integrate full 
parsers in order to annotate syntactic and seman-
tic information simultaneously. It will then be 
possible to apply the SRL techniques more ex-
tensively to biomedical relation extraction. 
References 
Ann Bies, Mark Ferguson, Karen Katz, and Robert 
MacIntyre. 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. Technical re-
port, University of Pennsylvania. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL-2005. 
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, 
James H. Martin, and Daniel Jurafsky. 2004. Se-
mantic Role Labeling by Tagging Syntactic 
Chunks. In Proceedings of CoNLL-2004. 
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun'-
ichi Tsujii. 2003. GENIA corpus?a semantically 
annotated corpus for bio-textmining. Bioinformat-
ics, 19(Suppl. 1): i180-i182. 
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexicon. 
In Proceedings of AAAI-2000. 
Karin Kipper, Martha Palmer, and Owen Rambow. 
2002. Extending PropBank with VerbNet semantic 
predicates. In Proceedings of AMTA-2002. 
Mitchell Marcus, Grace Kim, Mary Ann 
Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark 
Ferguson, Karen Katz, and Britta Schasberger. 
1994. The Penn Treebank: Annotating predicate 
argument structure. In Proceedings of ARPA Hu-
man Language Technology Workshop. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2): 313-330. 
Thomas Morton and Jeremy LaCivita. 2003. Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of HLT/NAACL-2003. 
Jorge Nocedal and Stephen J Wright. 1999. Numeri-
cal Optimization, Springer. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1). 
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav 
Zimak. 2004. Semantic Role Labeling via Integer 
Linear Programming Inference. In Proceedings of 
COLING-2004. 
Sidney Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences. 
New York, McGraw-Hill. 
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun 
Lin, Cheng-Lung Sung, Wei Ku, Ying-Shan Su, 
Ting-Yi Sung, and Wen-Lian Hsu. 2006. BIOS-
MILE: Adapting Semantic Role Labeling for Bio-
medical Verbs: An Exponential Model Coupled 
with Automatically Generated Template Features. 
In Proceedings of BioNLP'06. 
Yuka Tateisi, Tomoko Ohta, and Jun-ichi Tsujii. 2004. 
Annotation of Predicate-argument Structure of Mo-
lecular Biology Text. In Proceedings of the 
IJCNLP-04 workshop on Beyond Shallow Analyses. 
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-
Speech Annotation of Biology Research Abstracts. 
In Proceedings of the 4th International Conference 
on Language Resource and Evaluation. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun-ichi Tsujii. 2005. Syntax Annotation for the 
GENIA corpus. In Proceedings of IJCNLP-2005. 
Tuangthong Wattarujeekrit, Parantu K Shah, and 
Nigel Collier1. 2004. PASBio: predicate-argument 
structures for event extraction in molecular biology. 
BMC Bioinformatics, 5(155). 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Proceed-
ings of the EMNLP-2004. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2004. Improving Noun Phrase Corefer-
ence Resolution by Matching Strings. In Proceed-
ings of 1st International Joint Conference on Natu-
ral Language Processing: 226-233. 
  
12
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 57?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
BIOSMILE: Adapting Semantic Role Labeling for Biomedical Verbs: 
An Exponential Model Coupled with 
Automatically Generated Template Features 
 
 
Richard Tzong-Han Tsai1,2, Wen-Chi Chou1, Yu-Chun Lin1,2, Cheng-Lung Sung1,  
Wei Ku1,3, Ying-Shan Su1,4, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Institute of Molecular Medicine, National Taiwan University  
4Dept. of Biochemical Science and Technology, National Taiwan University 
{thtsai,jacky957,sbb,clsung,wilmaku,qnn,tsung,hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we construct a biomedical 
semantic role labeling (SRL) system that 
can be used to facilitate relation extraction. 
First, we construct a proposition bank on 
top of the popular biomedical GENIA 
treebank following the PropBank annota-
tion scheme. We only annotate the predi-
cate-argument structures (PAS?s) of thirty 
frequently used biomedical predicates and 
their corresponding arguments. Second, 
we use our proposition bank to train a 
biomedical SRL system, which uses a 
maximum entropy (ME) model. Thirdly, 
we automatically generate argument-type 
templates which can be used to improve 
classification of biomedical argument 
types. Our experimental results show that 
a newswire SRL system that achieves an 
F-score of 86.29% in the newswire do-
main can maintain an F-score of 64.64% 
when ported to the biomedical domain. 
By using our annotated biomedical corpus, 
we can increase that F-score by 22.9%. 
Adding automatically generated template 
features further increases overall F-score 
by 0.47% and adjunct arguments (AM) F-
score by 1.57%, respectively. 
1 Introduction 
The volume of biomedical literature available has 
experienced unprecedented growth in recent years. 
The ability to automatically process this literature 
would be an invaluable tool for both the design and 
interpretation of large-scale experiments. To this 
end, more and more information extraction (IE) 
systems using natural language processing (NLP) 
have been developed for use in the biomedical 
field. A key IE task in the biomedical field is ex-
traction of relations, such as protein-protein and 
gene-gene interactions. 
Currently, most biomedical relation-extraction 
systems fall under one of the following three ap-
proaches: cooccurence-based (Leroy et al, 2005), 
pattern-based (Huang et al, 2004), and machine-
learning-based. All three, however, share the same 
limitation when extracting relations from complex 
natural language. They only extract the relation 
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that 
describe location, manner, timing, condition, and 
extent. The information in such phrases may be 
important for precise definition and clarification of 
complex biological relations. 
The above problem can be tackled by using se-
mantic role labeling (SRL) because it not only rec-
ognizes main roles, such as agents and objects, but 
also extracts adjunct roles such as location, manner, 
57
timing, condition, and extent. The goal of SRL is 
to group sequences of words together and classify 
them with semantic labels. In the newswire domain, 
Morarescu et al (2005) have demonstrated that 
full-parsing and SRL can improve the performance 
of relation extraction, resulting in an F-score in-
crease of 15% (from 67% to 82%). This significant 
result leads us to surmise that SRL may also have 
potential for relation extraction in the biomedical 
domain. Unfortunately, no SRL system for the 
biomedical domain exists. 
In this paper, we aim to build such a biomedical 
SRL system. To achieve this goal we roughly im-
plement the following three steps as proposed by 
Wattarujeekrit et al, (2004): (1) create semantic 
roles for each biomedical verb; (2) construct a 
biomedical corpus annotated with verbs and their 
corresponding semantic roles (following defini-
tions created in (1) as a reference resource;) (3) 
build an automatic semantic interpretation model 
using the annotated text as a training corpus for 
machine learning. In the first step, we adopt the 
definitions found in PropBank (Palmer et al, 2005), 
defining our own framesets for verbs not in Prop-
Bank, such as ?phosphorylate?. In the second step, 
we first use an SRL system (Tsai et al, 2005) 
trained on the Wall Street Journal (WSJ) to auto-
matically tag our corpus. We then have the results 
double-checked by human annotators. Finally, we 
add automatically-generated template features to 
our SRL system to identify adjunct (modifier) ar-
guments, especially those highly relevant to the 
biomedical domain. 
2 Biomedical Proposition Bank  
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different syn-
tactic realizations of the same verb (Palmer et al, 
2005). The annotation captures predicate-argument 
structures based on the sense tags of polysemous 
verbs (called framesets) and semantic role labels 
for each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors acti-
vate STAT6, STAT3 and STAT5 proteins in the 
human B cells.? The chosen predicate is the word 
?activate?; its arguments and their associated word 
groups are illustrated in the figure. 
 
 
Figure 1. A Treebank Annotated with Semantic 
Role Labels 
Since proposition banks are annotated on top of 
a Penn-style treebank, we selected a biomedical 
corpus that has a Penn-style treebank as our corpus. 
We chose the GENIA corpus (Kim et al, 2003), a 
collection of MEDLINE abstracts selected from 
the search results with the following keywords: 
human, blood cells, and transcription factors. In the 
GENIA corpus, the abstracts are encoded in XML 
format, where each abstract also contains a 
MEDLINE UID, and the title and content of the 
abstract. The text of the title and content is seg-
mented into sentences, in which biological terms 
are annotated with their semantic classes. The 
GENIA corpus is also annotated with part-of-
speech (POS) tags (Tateisi et al, 2004), and co-
references (Yang et al, 2004). 
The Penn-style treebank for GENIA, created by 
Tateisi et al (2005), currently contains 500 ab-
stracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the Penn 
Treebank II (PTB) scheme (Bies et al, 1995), is 
encoded in XML. However, in contrast to the WSJ 
corpus, GENIA lacks a proposition bank. We 
therefore use its 500 abstracts with GTB as our 
corpus. To develop our biomedical proposition 
bank, BioProp, we add the proposition bank anno-
tation on top of the GTB annotation. 
2.1 Important Argument Types 
In the biomedical domain, relations are often de-
pendent upon locative and temporal factors 
(Kholodenko, 2006). Therefore, locative (AM-
LOC) and temporal modifiers (AM-TMP) are par-
ticularly important as they tell us where and when 
biomedical events take place. Additionally, nega-
58
tive modifiers (AM-NEG) are also vital to cor-
rectly extracting relations. Without AM-NEG, we 
may interpret a negative relation as a positive one 
or vice versa. In total, we use thirteen modifiers in 
our biomedical proposition bank. 
2.2 Verb Selection 
We select 30 frequently used verbs from the mo-
lecular biology domain given in Table 1. 
express trigger encode 
associate repress enhance 
interact signal increase 
suppress activate induce 
prevent alter Inhibit 
modulate affect Mediate 
phosphorylate bind Mutated 
transactivate block Reduce 
transform decrease Regulate 
differentiated promote Stimulate 
Table 1. 30 Frequently Biomedical Verbs 
Let us examine a representative verb, ?activate?. 
Its most frequent usage in molecular biology is the 
same as that in newswire. Generally speaking, ?ac-
tivate? means, ?to start a process? or ?to turn on.? 
Many instances of this verb express the action of 
waking genes, proteins, or cells up. The following 
sentence shows a typical usage of the verb ?acti-
vate.?  
[NF-kappaB
 Arg1
] is [not
 AM-NEG
] [activated
predicate
] [upon tetra-
cycline removal
AM-TMP
] [in the NIH3T3 cell line
AM-LOC
]. 
3 Semantic Role Labeling on BioProp 
In this section, we introduce our BIOmedical Se-
MantIc roLe labEler, BIOSMILE. Like POS tag-
ging, chunking, and named entity recognition, SRL 
can be formulated as a sentence tagging problem. 
A sentence can be represented by a sequence of 
words, a sequence of phrases, or a parsing tree; the 
basic units of a sentence are words, phrases, and 
constituents arranged in the above representations, 
respectively. Hacioglu et al (2004) showed that 
tagging phrase by phrase (P-by-P) is better than 
word by word (W-by-W). Punyakanok et al, (2004) 
further showed that constituent-by-constituent (C-
by-C) tagging is better than P-by-P. Therefore, we 
choose C-by-C tagging for SRL. The gold standard 
SRL corpus, PropBank, was designed as an addi-
tional layer of annotation on top of the syntactic 
structures of the Penn Treebank. 
SRL can be broken into two steps. First, we 
must identify all the predicates. This can be easily 
accomplished by finding all instances of verbs of 
interest and checking their POS?s. 
Second, for each predicate, we need to label all 
arguments corresponding to the predicate. It is a 
complicated problem since the number of argu-
ments and their positions vary depending on a 
verb?s voice (active/passive) and sense, along with 
many other factors.  
In this section, we first describe the maximum 
entropy model used for argument classification. 
Then, we illustrate basic features as well as spe-
cialized features such as biomedical named entities 
and argument templates.  
3.1 Maximum Entropy Model 
The maximum entropy model (ME) is a flexible 
statistical model that assigns an outcome for each 
instance based on the instance?s history, which is 
all the conditioning data that enables one to assign 
probabilities to the space of all outcomes. In SRL, 
a history can be viewed as all the information re-
lated to the current token that is derivable from the 
training corpus. ME computes the probability, 
p(o|h), for any o from the space of all possible out-
comes, O, and for every h from the space of all 
possible histories, H. 
The computation of p(o|h) in ME depends on a 
set of binary features, which are helpful in making 
predictions about the outcome. For instance, the 
node in question ends in ?cell?, it is likely to be 
AM-LOC. Formally, we can represent this feature 
as follows: 
??
??
?
=
=
=
otherwise :0
LOC-AMo and    
 true)(s_in_cellde_endcurrent_no if :1
),(
h
ohf
Here, current_node_ends_in_cell(h) is a binary 
function that returns a true value if the current 
node in the history, h, ends in ?cell?. Given a set of 
features and a training corpus, the ME estimation 
process produces a model in which every feature f i 
has a weight ?i. Following Bies et al (1995), we 
can compute the conditional probability as: 
?=
i
ohf
i
i
hZ
hop ),(
)(
1
)|( ?  
??=
o i
ohf
i
ihZ ),()( ?  
59
The probability is calculated by multiplying the 
weights of the active features (i.e., those of f i (h,o) 
= 1).  ?i is estimated by a procedure called Gener-
alized Iterative Scaling (GIS) (Darroch et al, 
1972). The ME estimation technique guarantees 
that, for every feature, f i, the expected value of ?i 
equals the empirical expectation of ?i in the train-
ing corpus. We use Zhang?s MaxEnt toolkit and 
the L-BFGS (Nocedal et al, 1999) method of pa-
rameter estimation for our ME model. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree from 
the parse constituent be-ing classified to the predicate 
z Constituent type 
z Position ? Whether the phrase is located before or after 
the predicate 
z Voice ? passive: if the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form of 
?to be? or ?to get? within its chunk; otherwise, it is ac-
tive 
z Head word ? calculated using the head word table de-
scribed by (Collins, 1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that ex-
pands the predicate?s parent node in the parsing tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, con-
stituent types, positions, head words and head POS 
tags 
z Head of PP parent ? If the parent is a PP, then the head 
of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 2. The Features Used in the Baseline Argu-
ment Classification Model 
3.2 Basic Features 
Table 2 shows the features that are used in our 
baseline argument classification model. Their ef-
fectiveness has been previously shown by (Pradhan 
et al, 2004; Surdeanu et al, 2003; Xue et al, 
2004). Detailed descriptions of these features can 
be found in (Tsai et al, 2005). 
3.3 Named Entity Features 
In the newswire domain, Surdeanu et al (2003) 
used named entity (NE) features that indicate 
whether a constituent contains NEs, such as per-
sonal names, organization names, location names, 
time expressions, and quantities of money. Using 
these NE features, they increased their system?s F-
score by 2.12%. However, because NEs in the 
biomedical domain are quite different from news-
wire NEs, we create bio-specific NE features using 
the five primary NE categories found in the 
GENIA ontology1: protein, nucleotide, other or-
ganic compounds, source and others. Table 3 illus-
trates the definitions of these five categories. When 
a constituent exactly matches an NE, the corre-
sponding NE feature is enabled.  
 NE Definition 
Protein Proteins include protein groups, families, molecules, complexes, and substructures.  
Nucleotide A nucleic acid molecule or the compounds that consist of nucleic acids. 
Other organic 
compounds 
Organic compounds exclude protein and 
nucleotide. 
Source 
Sources are biological locations where 
substances are found and their reactions 
take place.  
Others 
The terms that are not categorized as 
sources or substances may be marked up, 
with 
Table 3. Five GENIA Ontology NE Categories 
3.4 Biomedical Template Features 
Although a few NEs tend to belong almost exclu-
sively to certain argument types (such as ??cell? 
being mainly AM-LOC), this information alone is 
not sufficient for argument-type classification. For 
one, most NEs appear in a variety of argument 
types. For another, many appear in more than one 
constituent (node in a parsing tree) in the same 
sentence. Take the sentence ?IL4 and IL13 recep-
tors activate STAT6, STAT3 and STAT5 proteins 
in the human B cells,? for example. The NE ?the 
human B cells? is found in two constituents (?the 
                                                          
1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
genia-ontology.html  
60
human B cells? and ?in the human B cells?) as 
shown in figure 1. Yet only ?in the human B cells? 
is an AM-LOC because here ?human B cells? is 
preceded by the preposition ?in? and the deter-
miner ?the?. Another way to express this would be 
as a template?<prep> the <cell>.? We believe 
such templates composed of NEs, real words, and 
POS tags may be helpful in identifying constitu-
ents? argument types. In this section, we first de-
scribe our template generation algorithm, and then 
explain how we use the generated templates to im-
prove SRL performance. 
Template Generation (TG) 
Our template generation (TG) algorithm extracts 
general patterns for all argument types using the 
local alignment algorithm. We begin by pairing all 
arguments belonging to the same type according to 
their similarity. Closely matching pairs are then 
aligned word by word and a template that fits both 
is created. Each slot in the template is given con-
straint information in the form of either a word, NE 
type, or POS. The hierarchy of this constraint in-
formation is word > NE type > POS. If the argu-
ments share nothing in common for a given slot, 
the TG algorithm will put a wildcard in that posi-
tion. Figure 2 shows an aligned pair arguments. 
For this pair, the TG algorithm generated the tem-
plate ?AP-1 CC PTN? (PTN: protein name) be-
cause in the first position, both arguments have 
?AP-1;? in the second position, they have the same 
POS ?CC;? and in the third position, they share a 
common NE type, ?PTN.? The complete TG algo-
rithm is described in Algorithm 1. 
AP-1/PTN/NN and/O/CC NF-AT/PTN/NN 
AP-1/PTN/NN or/O/CC NFIL-2A/PTN/NN 
Figure 2. Aligned Argument Pair 
Applying Generated Templates 
The generated templates may match exactly or par-
tially with constituents. According to our observa-
tions, the former is more useful for argument 
classification. For example, constituents that per-
fectly match the template ?IN a * <cell>? are 
overwhelmingly AM-LOCs. Therefore, we only 
accept exact template matches. That is, if a con-
stituent exactly matches a template t, then the fea-
ture corresponding to t will be enabled. 
Algorithm 1 Template Generation 
Input: Sentences set S = {s1, . . . , sn}, 
Output: A set of template T = {t1, . . . , tk}. 
 
1: T = {}; 
2: for each sentence si from s1 to sn-1 do 
3:    for each sentence sj from si to sn do 
4:        perform alignment on si and sj, then 
5:          pair arguments according to similarity; 
6:        generate common template t from argument pairs; 
7:        T?T?t; 
8:    end; 
9: end; 
10: return T; 
4 Experiments 
4.1 Datasets 
In this paper, we extracted all our datasets from 
two corpora, the Wall Street Journal (WSJ) corpus 
and the BioProp, which respectively represent the 
newswire and biomedical domains. The Wall 
Street Journal corpus has 39,892 sentences, and 
950,028 words. It contains full-parsing information, 
first annotated by Marcus et al (1997), and is the 
most famous treebank (WSJ treebank). In addition 
to these syntactic structures, it was also annotated 
with predicate-argument structures (WSJ proposi-
tion bank) by Palmer et al (2005).  
In biomedical domain, there is one available 
treebank for GENIA, created by Yuka Tateshi et al 
(2005), who has so far added full-parsing informa-
tion to 500 abstracts. In contrast to WSJ, however, 
GENIA lacks any proposition bank. 
Since predicate-argument annotation is essential 
for training and evaluating statistical SRL systems, 
to make up for GENIA?s lack of a proposition 
bank, we constructed BioProp. Two biologists with 
masters degrees in our laboratory undertook the 
annotation task after receiving computational lin-
guistic training for approximately three months.  
We adopted a semi-automatic strategy to anno-
tate BioProp. First, we used the PropBank to train 
a statistical SRL system which achieves an F-score 
of over 86% on section 24 of the PropBank. Next, 
we used this SRL system to annotate the GENIA 
treebank automatically. Table 4 shows the amounts 
of all adjunct argument types (AMs) in BioProp. 
The detail description of can be found in (Babko-
Malaya, 2005).  
 
61
Type Description # Type Description # 
NEG negation 
marker 
103 ADV general  
purpose 
307
LOC location 389 PNC purpose 3
TMP time 145 CAU cause 15
MNR manner 489 DIR direction 22
EXT extent 23 DIS discourse 
connectives 
179
   MOD modal verb 121
Table 4. Subtypes of the AM Modifier Tag 
4.2 Experiment Design 
Experiment 1: Portability 
Ideally, an SRL system should be adaptable to the 
task of information extraction in various domains 
with minimal effort. That is, we should be able to 
port it from one domain to another. In this experi-
ment, we evaluate the cross-domain portability of 
our SRL system. We use Sections 2 to 21 of the 
PropBank to train our SRL system. Then, we use 
our system to annotate Section 24 of the PropBank 
(denoted by Exp 1a) and all of BioProp (denoted 
by Exp 1b). 
Experiment 2: The Necessity of BioProp 
To compare the effects of using biomedical train-
ing data vs. using newswire data, we train our SRL 
system on 30 randomly selected training sets from 
BioProp (g1,.., g30) and 30 from PropBank (w1,.., 
w30), each having 1200 training PAS?s. We then 
test our system on 30 400-PAS test sets from Bio-
Prop, with g1 and w1 being tested on test set 1, g2 
and w2 on set 2, and so on. Then we add up the 
scores for w1-w30 and g1-g30, and compare their 
averages. 
Experiment 3: The Effect of Using Biomedical-
Specific Features 
In order to improve SRL performance, we add do-
main specific features. In Experiment 3, we inves-
tigate the effects of adding biomedical NE features 
and argument template features composed of 
words, NEs, and POSs. The dataset selection pro-
cedure is the same as in Experiment 2. 
5 Results and Discussion 
All experimental results are summarized in Table 5. 
For argument classification, we report the preci-
sion (P), recall (R) and F-scores (F). The details 
are illustrated in the following paragraphs. 
Configuration Training Test P R F 
Exp 1a PropBank PropBank 90.47 82.48 86.29
Exp 1b PropBank BioProp 75.28 56.64 64.64
Exp 2a PropBank BioProp 74.78 56.25 64.20
Exp 2b BioProp BioProp 88.65 85.61 87.10
Exp 3a BioProp BioProp 88.67 85.59 87.11
Exp 3b BioProp BioProp 89.13 86.07 87.57
Table 5. Summary of All Experiments 
Exp 1a Exp 1b Role 
P R F P R F 
+/-(%)
Overall 90.47 82.48 86.29 75.28 56.64 64.64 -21.65
ArgX 91.46 86.39 88.85 78.92 67.82 72.95 -15.90
Arg0 86.36 78.01 81.97 85.56 64.41 73.49   -8.48
Arg1 95.52 92.11 93.78 82.56 75.75 79.01 -14.77
Arg2 87.19 84.53 85.84 32.76 31.59 32.16 -53.68
AM 86.76 70.02 77.50 62.70 32.98 43.22 -34.28
-ADV 73.44 52.32 61.11 39.27 26.34 31.53 -29.58
-DIS 81.71 48.18 60.62 67.12 48.18 56.09 -4.53
-LOC 89.19 57.02 69.57 68.54 2.67 5.14 -64.43
-MNR 67.93 57.86 62.49 46.55 22.97 30.76 -31.73
-MOD 99.42 92.5 95.84 99.05 88.01 93.2 -2.64
-NEG 100 91.21 95.40 99.61 80.13 88.81 -6.59
-TMP 88.15 72.83 79.76 70.97 60.36 65.24 -14.52
Table 6. Performance of Exp 1a and Exp 1b 
Experiment 1 
Table 6 shows the results of Experiment 1. The 
SRL system trained on the WSJ corpus obtains an 
F-score of 64.64% when used in the biomedical 
domain. Compared to traditional rule-based or 
template-based approaches, our approach suffers 
acceptable decrease in overall performance when 
recognizing ArgX arguments. However, Table 6 
also shows significant decreases in F-scores from 
other argument types. AM-LOC drops 64.43% and 
AM-MNR falls 31.73%. This may be due to the 
fact that the head words in PropBank are quite dif-
ferent from those in BioProp. Therefore, to achieve 
better performance, we believe it will be necessary 
to annotate biomedical corpora for training bio-
medical SRL systems. 
Experiment 2 
Table 7 shows the results of Experiment 2. When 
tested on BioProp, BIOSMILE (Exp 2b) outper-
forms the newswire SRL system (Exp 2a) by 
22.9% since the two systems are trained on differ-
ent domains. This result is statistically significant. 
Furthermore, Table 7 shows that BIOSMILE 
outperforms the newswire SRL system in most 
62
argument types, especially Arg0, Arg2, AM-ADV, 
AM-LOC, AM-MNR.  
Exp 2a Exp 2b Role 
P R F P R F 
+/-(%)
Overall 74.78 56.25 64.20 88.65 85.61 87.10 22.90
ArgX 78.40 67.32 72.44 91.96 89.73 90.83 18.39
Arg0 85.55 64.40 73.48 92.24 90.59 91.41 17.93
Arg1 81.41 75.11 78.13 92.54 90.49 91.50 13.37
Arg2 34.42 31.56 32.93 86.89 81.35 84.03 51.10
AM 61.96 32.38 42.53 81.27 76.72 78.93 36.40
-ADV 36.00 23.26 28.26 64.02 52.12 57.46 29.20
-DIS 69.55 51.29 59.04 82.71 75.60 79.00 19.96
-LOC 75.51 3.23 6.20 80.05 85.00 82.45 76.25
-MNR 44.67 21.66 29.17 83.44 82.23 82.83 53.66
-MOD 99.38 88.89 93.84 98.00 95.28 96.62 2.78
-NEG 99.80 79.55 88.53 97.82 94.81 96.29 7.76
-TMP 67.95 60.40 63.95 80.96 61.82 70.11 6.16
Table 7. Performance of Exp 2a and Exp 2b 
The performance of Arg0 and Arg2 in our sys-
tem increases considerably because biomedical 
verbs can be successfully identified by BIOSMILE 
but not by the newswire SRL system. For AM-
LOC, the newswire SRL system scored as low as 
76.25% lower than BIOSMILE. This is likely due 
to the reason that in the biomedical domain, many 
biomedical nouns, e.g., organisms and cells, func-
tion as locations, while in the newswire domain, 
they do not. In newswire, the word ?cell? seldom 
appears. However, in biomedical texts, cells repre-
sent the location of many biological reactions, and, 
therefore, if a constituent node on a parsing tree 
contains ?cell?, this node is very likely an AM-
LOC. If we use only newswire texts, the SRL sys-
tem will not learn to recognize this pattern. In the 
biomedical domain, arguments of manner (AM-
MNR) usually describe how to conduct an experi-
ment or how an interaction arises or occurs, while 
in newswire they are extremely broad in scope. 
Without adequate biomedical domain training cor-
pora, systems will easily confuse adverbs of man-
ner (AM-MNR), which are differentiated from 
general adverbials in semantic role labeling, with 
general adverbials (AM-ADV). In addition, the 
performance of the referential arguments of Arg0, 
Arg1, and Arg2 increases significantly. 
Experiment 3 
Table 8 shows the results of Experiment 3. The 
performance does not significantly improve after 
adding NE features. We originally expected that 
NE features would improve recognition of AM 
arguments such as AM-LOC. However, they failed 
to ameliorate the results since in the biomedical 
domain most NEs are just matched parts of a con-
stituent. This results in fewer exact matches. Fur-
thermore, in matched cases, NE information alone 
is insufficient to distinguish argument types. For 
example, even if a constituent exactly matches a 
protein name, we still cannot be sure whether it 
belongs to the subject (Arg0) or object (Arg1). 
Therefore, NE features were not as effective as we 
had expected. 
NE (Exp 3a) Template (Exp 3b) Role 
P R F P R F 
+/-(%)
Overall 88.67 85.59 87.11 89.13 86.07 87.57 0.46
ArgX 91.99 89.70 90.83 91.89 89.73 90.80 -0.03
Arg0 92.41 90.57 91.48 92.19 90.59 91.38 -0.1
Arg1 92.47 90.45 91.45 92.42 90.44 91.42 -0.03
Arg2 86.93 81.3 84.02 87.08 81.66 84.28 0.26
AM 81.30 76.75 78.96 82.96 78.18 80.50 1.54
-ADV 64.11 52.23 57.56 65.66 55.60 60.21 2.65
-DIS 82.51 75.42 78.81 83.00 75.79 79.23 0.42
-LOC 80.07 85.09 82.50 84.24 85.48 84.86 2.36
-MNR 83.50 82.19 82.84 84.56 84.14 84.35 1.51
-MOD 98.14 95.28 96.69 98.00 95.28 96.62 -0.07
-NEG 97.66 94.81 96.21 97.82 94.81 96.29 0.08
-TMP 81.14 62.06 70.33 83.10 63.95 72.28 1.95
Table 8. Performance of Exp 3a and Exp 3b 
6 Conclusions and Future Work 
In Experiment 3b, we used the argument templates 
as features. Since ArgX?s F-score is close to 90%, 
adding the template features does not improve its 
score. However, AM?s F-score increases by 1.54%. 
For AM-ADV, AM-LOC, and AM-TMP, the in-
crease is greater because the automatically gener-
ated templates effectively extract these AMs.  
In Figure 3, we compare the performance of ar-
gument classification models with and without ar-
gument template features. The overall F-score 
improves only slightly. However, the F-scores of 
main adjunct arguments increase significantly. 
The contribution of this paper is threefold. First, 
we construct a biomedical proposition bank, Bio-
Prop, on top of the popular biomedical GENIA 
treebank following the PropBank annotation 
scheme. We employ semi-automatic annotation 
using an SRL system trained on PropBank, thereby 
significantly reducing annotation effort. Second, 
we create BIOSMILE, a biomedical SRL system, 
which uses BioProp as its training corpus. Thirdly, 
we develop a method to automatically generate 
templates that can boost overall performance, es-
63
pecially on location, manner, adverb, and temporal 
arguments. In the future, we will expand BioProp 
to include more verbs and will also integrate an 
automatic parser into BIOSMILE. 
 
Figure 3. Improvement of Template Features 
Overall and on Several Adjunct Types 
Acknowledgement 
We would like to thank Dr. Nianwen Xue for his 
instruction of using the WordFreak annotation tool. 
This research was supported in part by the National 
Science Council under grant NSC94-2752-E-001-
001 and the thematic program of Academia Sinica 
under grant AS94B003. Editing services were pro-
vided by Dorion Berg. 
References  
Babko-Malaya, O. (2005). Propbank Annotation 
Guidelines. 
Bies, A., Ferguson, M., Katz, K., MacIntyre, R., 
Tredinnick, V., Kim, G., et al (1995). Bracketing 
Guidelines for Treebank II Style Penn Treebank 
Project  
Collins, M. J. (1999). Head-driven Statistical Models 
for Natural Language Parsing. Unpublished Ph.D. 
thesis, University of Pennsylvania. 
Darroch, J. N., & Ratcliff, D. (1972). Generalized 
Iterative Scaling for Log-Linear Models. The Annals 
of Mathematical Statistics. 
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & 
Jurafsky, D. (2004). Semantic Role Labeling by 
Tagging Syntactic Chunks. Paper presented at the 
CONLL-04. 
Huang, M., Zhu, X., Hao, Y., Payan, D. G., Qu, K., & 
Li, M. (2004). Discovering patterns to extract 
protein-protein interactions from full texts. 
Bioinformatics, 20(18), 3604-3612. 
Kholodenko, B. N. (2006). Cell-signalling dynamics in 
time and space. Nat Rev Mol Cell Biol, 7(3), 165-176. 
Kim, J. D., Ohta, T., Tateisi, Y., & Tsujii, J. (2003). 
GENIA corpus--semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1, i180-182. 
Leroy, G., Chen, H., & Genescene. (2005). An 
ontology-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5), 457-468. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. 
(1997). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 19. 
Morarescu, P., Bejan, C., & Harabagiu, S. (2005). 
Shallow Semantics for Relation Extraction. Paper 
presented at the IJCAI-05. 
Nocedal, J., & Wright, S. J. (1999). Numerical 
Optimization: Springer. 
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The 
proposition bank: an annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
Pradhan, S., Hacioglu, K., Kruglery, V., Ward, W., 
Martin, J. H., & Jurafsky, D. (2004). Support vector 
learning for semantic argument classification. 
Journal of Machine Learning  
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). 
Semantic Role Labeling via Integer Linear 
Programming Inference. Paper presented at the 
COLING-04. 
Surdeanu, M., Harabagiu, S. M., Williams, J., & 
Aarseth, P. (2003). Using Predicate-Argument 
Structures for Information Extraction. Paper 
presented at the ACL-03. 
Tateisi, Y., & Tsujii, J. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. Paper 
presented at the LREC-04. 
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J. (2005). 
Syntax Annotation for the GENIA corpus. 
Tsai, T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L. 
(2005). Exploiting Full Parsing Information to Label 
Semantic Roles Using an Ensemble of ME and SVM 
via Integer Linear Programming. . Paper presented at 
the CoNLL-05. 
Wattarujeekrit, T., Shah, P. K., & Collier, N. (2004). 
PASBio: predicate-argument structures for event 
extraction in molecular biology. BMC Bioinformatics, 
5, 155. 
Xue, N., & Palmer, M. (2004). Calibrating Features for 
Semantic Role Labeling. Paper presented at the 
EMNLP-04. 
Yang, X., Zhou, G., Su, J., & Tan., C. (2004). 
Improving Noun Phrase Coreference Resolution by 
Matching Strings. Paper presented at the IJCNLP-04. 
64
Coling 2010: Poster Volume, pages 223?231,
Beijing, August 2010
Abstract 
Global ranking, a new information re-
trieval (IR) technology, uses a ranking 
model for cases in which there exist re-
lationships between the objects to be 
ranked. In the ranking task, the ranking 
model is defined as a function of the 
properties of the objects as well as the 
relations between the objects. Existing 
global ranking approaches address the 
problem by ?learning to rank?. In this 
paper, we propose a global ranking 
framework that solves the problem via 
data fusion. The idea is to take each re-
trieved document as a pseudo-IR sys-
tem. Each document generates a pseu-
do-ranked list by a global function. The 
data fusion algorithm is then adapted to 
generate the final ranked list. Taking a 
biomedical information extraction task, 
namely, interactor normalization task 
(INT), as an example, we explain how 
the problem can be formulated as a 
global ranking problem, and demon-
strate how the proposed fusion-based 
framework outperforms baseline me-
thods. By using the proposed frame-
work, we improve the performance of 
the top 1 INT system by 3.2% using 
the official evaluation metric of the 
BioCreAtIvE challenge. In addition, by 
employing the standard ranking quality 
measure, NDCG, we demonstrate that 
                                                 
* Corresponding author 
the proposed framework can be cas-
caded with different local ranking 
models and improve their ranking re-
sults. 
1 Introduction 
Information Retrieval (IR) involves finding 
documents that are relevant to a given query in 
a large corpus. The task is usually formulated 
as a ranking problem. When a user submits a 
query, the IR system retrieves all documents 
that contain at least one query term, calculates 
a ranking score for each of the documents us-
ing a ranking model, and sorts the documents 
according to the ranking scores. The scores 
represent the relevance, importance, and/or 
diversity of the retrieved documents. Thus, the 
quality of a search engine can be determined 
by the accuracy of the ranking results.  
Recently, a machine learning technology 
called learning to rank has been applied exten-
sively to the task. Several state-of-the-art ma-
chine learning-based ranking algorithms have 
been proposed, e.g., RankSVM and RankNet. 
These algorithms differ substantially in terms 
of the ranking models and optimization tech-
niques employed, but most of them can be re-
garded as ?local ranking? approaches in the 
sense that each model is defined on a single 
document without considering the possible 
relations to other documents to be ranked. In 
many applications, this is only a loose approx-
imation as there is always relational informa-
tion among documents. For example, in some 
cases, users may prefer that two similar docu-
ments have similar relevance scores; even 
Global Ranking via Data Fusion 
Hong-Jie Dai1,2 Po-Ting Lai3 Richard Tzong-Han Tsai3* Wen-Lian Hsu1,2* 
1Department of Computer Science, National Tsing-Hua University,  
2Institute of Information Science, Academia Sinica,  
3Department of Computer Science & Engineering, Yuan Ze University 
hongjie@iis.sinica.edu.tw 
s951416@mail.yzu.edu.tw  
thtsai@saturn.yzu.edu.tw 
hsu@iis.sinica.edu.tw 
223
though one of the documents is not as relevant 
to the given query as the other; this problem is 
similar to Pseudo Relevance Feedback (Kwok, 
1984). In other cases, web pages from the 
same site form a sitemap hierarchy in which a 
parent document should be ranked higher than 
its child documents (referred to as Topic Dis-
tillation at TREC (Chowdhury, 2007)). To util-
ize all available information, more advanced 
ranking algorithms define a ranking model as a 
function of all the documents to be ranked, i.e., 
a global ranking model (Qin et al, 2008a; Qin 
et al, 2008b). 
Unlike conventional ranking and learning to 
rank models, such as BM25 and RankSVM, 
whose ranking functions are defined on a 
query and document pair, global ranking mod-
els utilize both content information and rela-
tion information. Qin et al (2008) proposed 
the first supervised learning framework for the 
global ranking problem. They formulated the 
problem as an optimization problem that in-
volves finding an objective function to minim-
ize the trade-off between local consistence and 
global consistence and implemented it on 
SVM. Subsequently, they defined the global 
ranking problem formally in (Qin et al, 2008) 
and solved it by employing continuous condi-
tional random fields (CRF). 
In this paper, we propose a new framework 
for the global ranking problem. The major dif-
ference between our work and that of Qin et al 
(2008a; 2008b) is that we do not compile a 
feature vector of relational information directly 
to construct a new machine-learned ranking 
model for global ranking. Instead, we use the 
ranking results generated by the original rank-
ing model and then employ an algorithm with 
the relational information to transform the 
global ranking problem into a data fusion prob-
lem; that is also known as a rank aggregate 
problem. The proposed framework is flexible 
and can be cascaded with conventional ranking 
models or learning to rank models. 
The remainder of this paper is organized as 
follows. In Section 2, we present a formal de-
finition of global ranking. In Section 3, we de-
scribe the proposed framework and consider 
three fusion algorithms that can be used with 
our framework. We also explain how the algo-
rithms can be adapted to solve the global rank-
ing problem. In Section 4, we introduce a bio-
medical text mining task called the interactor 
normalization task (INT) (Krallinger et al, 
2009) and show why it should be formulated 
as a global ranking problem. In Section 5, we 
report extensive experiments conducted on the 
INT dataset released by BioCreAtIvE 
(Krallinger et al, 2009). Section 6 contains 
some concluding remarks. 
2 Global Ranking Problem 
The global ranking problem was first defined 
formally by Qin et al (2008). In this paper, we 
propose a new global ranking framework 
based on their definition. Although we devel-
oped the framework independently, we adopt 
Qin et al?s terminology. 
Let   denote a query. In addition, let 
        
      
       
    
   
  denote the docu-
ments retrieved by  , and let      
   
      
       
    
   
  denote the ranking scores 
assigned to the documents. Here,      
represents the number of documents retrieved 
by  . Note that the numbers of documents va-
ries according to different queries. We assume 
that      is determined by a ranking model. 
If a ranking model is defined on a single 
document, i.e., in the form of 
  
        
               , 
it is called a ?local ranking? model.  
Let       
      
          
   
 
 be a set of 
real-value functions defined on   
   
,   
   
, and  
     (                ). The functions 
           
represents the relations between documents. 
Equation 2 is defined according to the re-
quirements of different tasks. For example, for 
the Pseudo Relevance Feedback problem, Qin 
et al (2008) defined Equation 2 as the similari-
ties between any two documents in their CRF-
based model. 
If a ranking model takes all the documents 
as its input and exploits both local and global 
information (Equation 2) in the documents, i.e., 
in the form of 
            , 
it is called a ?global ranking? approach. 
(1) 
(2) 
224
 3 Fusion-based Global Ranking 
Framework 
It is usually difficult to develop a global rank-
ing algorithm that can fully utilize all the local 
and global information in documents to pro-
duce a document rank and also consider the 
score ranks. One example of a global ranking 
algorithm that satisfied these criteria is the one 
proposed in (Qin et al, 2008) in which the 
modified CRF algorithm handles context (local) 
features and relational (global) features in the 
documents. Without solving a ranking problem 
directly, however, the modified CRF algorithm 
is more like a regression algorithm since it op-
timizes the CRF parameters in a maximum 
likelihood estimate without considering the 
score ranks. With respect to the ranking feature, 
in this section, we describe our framework 
based on the idea of data fusion for solving the 
global ranking problem. 
3.1 Framework Description 
The flow chart of the proposed framework is 
illustrated in Figure 1. The first step is the 
same as that of the traditional local ranking 
model. Given a query, the local ranking model 
  
   
 defined in Equation 1 is used to calculate 
the ranking score for each document, and re-
turn a document list sorted according to the 
local scores.  
The second step transforms the global rank-
ing problem into a data fusion problem. Our 
idea is to take each retrieved document as a 
pseudo-IR system, and the pseudo-ranking 
model,    
   
, used by each system is the func-
tion defined in Equation 2. For each pseudo-IR 
system,   
   
, the pseudo-ranking model for a 
document   
   
 is defined as follows:  
 
   
        
         
      
           
          . 
There are totally      pseudo-IR systems, 
which generate      pseudo-ranked lists. As a 
result, the global ranking problem is trans-
formed into a data fusion problem, that is to 
aggregate the pseudo-ranked lists. Figure 2 
shows the steps of the transformation algo-
rithm. 
The final step is to adapt fusion algorithms 
to aggregate the pseudo-ranked lists. A canoni-
cal data fusion task is called meta-search 
(Aslam and Montague, 2001; Fox and Shaw, 
1994; Lee, 1997; Nuray and Can, 2006), which 
aggregates Web search query results from sev-
eral engines into a more accurate ranking. The 
origin of research on data fusion can be traced 
back to (Borda, 1781). In recent years, the 
process has been used in many new applica-
tions, including aggregating data from micro-
array experiments to discover cancer-related 
genes (Pihura et al, 2008), integration of re-
sults from multiple mRNA studies (Lin and 
Ding, 2008), and similarity searches across 
datasets and information merging (Adler et al, 
2009; Zhao et al, 2010).  
Liu et al (2007) classified data fusion tech-
nologies into two categories: order-based fu-
sion and score-based fusion. In the first catego-
ry, the orders of the entities in individual rank-
ing lists are used by the fusion algorithm. In 
the second category, the entities in individual 
ranking lists are assigned scores and the fusion 
algorithm uses the scores. In the following 
sub-sections, we adapt three fusion algorithms 
Step 3
Step 2
Step 1
Local Ranking Model
Document Set
Query
Local Ranked 
Document List
Transformation 
Algorithm
Pseudo-IR 
System
1
Pseudo-IR 
System
2
Pseudo-IR 
System
n
(q)
...
Fusion 
Algorithm
Pseudo-
Ranked List
1
Pseudo-
Ranked List
2
Pseudo-
Ranked List
n
(q)
...
Global Ranked 
Document List
Pseudo-
Ranking Model
1
Pseudo-Ranking 
Model
2
Pseudo-Ranking 
Model
n
(q)
 
Figure 1. The Proposed Framework for 
Global Ranking. 
(3) 
225
for the proposed framework. The first is the 
Borda-fuse model (Aslam and Montague, 
2001), an order-based fusion approach based 
on an optimal voting procedure. The second is 
a linear combination (LC) model (Vogt and 
Cottrell, 1999), which is a score-based fusion 
approach. 
3.2 Borda-fuse 
The Borda-fuse model (Aslam and Montague, 
2001) is based on a political election strategy 
called the Borda Count. For our framework, 
the rationale behind the strategy is as follows. 
Each pseudo-IR system   
   
 is an analogy for 
a voter; and each voter ranks a fixed set of      
documents in order of preference (Equation 3). 
For each voter, the top ranked document is 
given      points, the second ranked document 
is given     -  points, and so on. If some doc-
uments left unranked by the voter, the remain-
ing points are divided equally among the un-
ranked documents. The documents are ranked 
in descending order of the total points. 
In our framework, we implement two Bor-
da-fuse-based models. The first is the modified 
Borda-fuse (MBF) model. In MBF, the number 
of points given for a voter's first and subse-
quent preferences is determined by the number 
of documents they have actually ranked, rather 
than the total number of ranked. Because the 
ranking model,    
   
, used by the pseudo-IR 
system may only retrieve  documents where 
  is smaller than     , we penalize systems 
that do not rank a full document set by reduc-
ing the number of points their vote distributes 
among the documents. In other words, if there 
are ten documents, but the pseudo-IR system 
only retrieves five, then the first document will 
only receive 5 points; the second will receive 4 
points, and so on. 
The second is the weighted Borda-fuse 
(WBF) model. The original Borda-fuse model 
reflects a democratic election in which each 
voter has equal weight. However, in many cas-
es, we prefer some voters because they are 
more reliable. We employ a simple weighting 
scheme that multiplies the points assigned to a 
document determined by system   
   
 by a 
weight 
  
   . 
3.3 LC Model 
The LC model has been used by many IR re-
searchers with varying degrees of success 
(Bartell et al, 1994; Knaus et al, 1995; Vogt 
and Cottrell, 1999; Vogt and Cottrell, 1998). In 
our framework, it is defined as follows. Given 
a query  , a document   
   
, the weights 
                     for  
    individual 
pseudo-IR systems, and jth pseudo-IR sys-
tem?s ranking score     
   
, the LC model cal-
culates the ranking score   of   
   
 against all 
pseudo-IR systems as follows: 
      
            
        
    
This score is then used to rank the documents. 
For example, for two pseudo-IR systems, this 
reduces to: 
          
           
          
   
 
Compared with MBF, Equation 4 requires 
both relevance scores and training data to de-
 function transform (    : the documents retrieved 
with query  ) 
{generate pseudo-ranked lists for     } 
 # a dictionary that maps the pseudo-IR systems to 
# their corresponding pseudo-ranked lists 
1. pseudoRankedLists = {} 
2. for   
   
 in     : 
     # a dictionary that maps the relation score (real 
    # value) to a list of documents. 
3.     relation = {} 
     for   
   
 in     : 
4.         relation[    
      
         ].append(  
   
) 
     # relation.keys() returns all keys stored in the  
 # dictionary relation. The key of relation is the 
 # relation score. 
5.     Sort relation.keys() in decreasing order 
     # a dictionary that maps a new rank to a list of 
    # documents. 
6.     pseudoRankedList = {} 
7.     newRank = 0 
     for score in sorted relation.keys(): 
         # relation[score] returns the document list  
         # corresponding to the given score 
         for doc in relation[score]: 
8.             pseudoRankedList[1+newRank] 
                                          .append(doc) 
9.         newRank = newRank + 1 
10.     pseudoRankedLists [  
   
] = pseudoRankedList 
 return pseudoRankedLists 
Figure 2. The Dependent Ranked List Gen-
eration Algorithm (represented using python 
syntax). 
(4) 
226
termine the weight   given to each pseudo-IR 
system. 
4 Case Study 
In this section, we describe the task examined 
in our study. We also explain how we formu-
late the task as a global ranking problem. The 
experiments results are detailed in Section 5. 
4.1 Interactor Normalization Task 
The interactor normalization task (INT) is a 
complicated text mining task that involves the 
following steps: (1) It recognizes gene men-
tions in a full text article. (2) It maps the rec-
ognized gene mentions to corresponding 
unique database identifiers which is similar to 
the word sense disambiguation task in compu-
tational linguistics. (3) It generates a ranked 
list of the identifiers according to their impor-
tance in the article and their probability of 
playing the interactor role in protein-protein 
interactions (PPIs). Such ranked lists are useful 
for human curators and can speed up PPI data-
base curation. 
Dai et al (2010) won first place in the Bio-
CreAtIvE II.5 INT challenge (Mardis et al, 
2009) by using a SVM-based local ranking 
model in which they treat gene mentions? iden-
tifiers in an article as the document set, and the 
query is a constant string ?interactor?. Based 
on their feature sets and evaluation results, we 
can find that their local ranking model tends to 
rank focus genes higher (Dai et al, 2010). 
However, the primary objective of INT is to 
generate a ranked list of interaction gene iden-
tifiers. According to (Jenssen et al, 2001), co-
mentioned genes are usually related in some 
way. For example, if two gene mentions fre-
quently occur alongside each other in the same 
sentence in an article, they probably have an 
association and influence each other?s rank. 
Take a low-ranked interactor that is only men-
tioned twice in an article as an example. If 
both mentions are next to the highest-ranked 
interactor in the article, then the low-ranked 
interactor?s rank should be boosted significant-
ly. Therefore, the ranking task for each article 
can be formulated as a global ranking problem; 
the global ranking algorithm should consider 
both the local information from Dai et al?s 
model and the global information from the as-
sociations among identifiers. 
4.2 Global Ranking in INT 
Let   be a constant ?interactor.? The identifier 
set generated by an INT system for a full-text 
article is analogous to the document set 
        
      
       
    
   
 . Here      denotes 
the number of identifiers. Note that the number 
of identifiers varies for different articles. Let 
        
      
       
    
   
  denote the ranking 
scores assigned to the identifiers given by a 
local ranking model. In this study, we used the 
INT system and SVM-based local ranking 
model released by (Dai et al, 2010) to gener-
ate the identifier set and ranking scores. 
To obtain the global information, we con-
sider the co-occurrence of identifiers and em-
ploy mutual information (MI) to measure the 
association between two identifiers as follows: 
                     
                      . 
In the above formula, the identifier probabili-
ties       and       are estimated by counting 
the number of occurrences in an article norma-
lized by  , i.e., the number of sentences con-
taining identifiers. The joint probability, 
        , is estimated by the number of times 
   co-occurs with    in a window of   words 
normalized by  . Note that, in practice, other 
advanced approaches can be used to calculate 
the association score. 
For the proposed framework, each identifier 
  
   
 is a pseudo-IR system with MI as its 
pseudo-ranking model    
   
. The identifiers 
that co-occur with   
   
 become candidates on 
  
   
?s pseudo-ranked list. 
5 Experiments 
In the following sub-sections, we introduce the 
dataset used in the experiments, describe the 
evaluation methods, report the results of the 
experiments conducted to compare the perfor-
mance of different methods, and discuss the 
efficiency of the proposed global ranking 
framework. 
227
5.1 Dataset 
We used the BioCreAtIvE II.5 Elsevier corpus 
released by BioCreAtIvE II.5 challenge in the 
experiments. The corpus contains 1,190 full-
text journal articles selected mainly from 
FEBS Letters. Following the same format as 
the BioCreAtIvE II.5 INT challenge, we used 
articles published in 2008 (61 articles) as our 
training set and articles published in 2007 or 
earlier (61 articles) as our test set. 
5.2 A Fusion-based Global Ranking 
Framework for INT 
Before applying the proposed framework, we 
preprocess the articles in the dataset to identify 
all gene mentions, and map them to their cor-
responding identifiers. After preprocessing, 
each full-text article is associated with a list of 
identifiers (Step 1 in Figure 1). The transform 
and fusion algorithm is then applied on each 
article (Steps 2 and 3 in Figure 1). 
To apply the WBF and LC models, we need 
to determine the weight assigned to each pseu-
do-IR system. To obtain the weight, we calcu-
late the precision of each rank of the ranked 
lists generated by Dai et al?s INT system. Fig-
ure 3 shows the precision of ranks 1 to 15 cal-
culated by applying three-fold cross validation 
on the INT training set. We observe that the 
precision declines as the rank increases, which 
implies that the higher ranks predicted by their 
SVM-based local ranking model are more reli-
able than the lower ranks. 
5.3 Evaluation Metrics 
Our evaluations focus on two comparisons: the 
first compares the ranking of the proposed 
framework with the original local ranking 
model by using the area under the curve of the 
interpolated precision/recall (iP/R) curve. This 
is the evaluation metric used in the BioCreA-
tIvE II.5 challenge and is a common way to 
depict the degradation of precision as one tra-
verses the retrieved results by plotting interpo-
lated precision numbers against percentage 
recall. The area under the iP/R function     is 
defined as follows: 
                              
 
   
                  
, 
where   is the total number of correct identifi-
ers and    is the highest interpolated precision 
for the correct identifier   at   , the recall for 
that hit. The interpolated precision    is calcu-
lated for each recall   by taking the highest 
precision at   or any     . 
In the second comparison, we use a standard 
quality measure in IR to estimate the ranking 
performance of local ranking models and the 
proposed framework. We adopt Normalized 
Discounted Cumulative Gain (NDCG) to 
measure the performance. The NDCG score of 
a ranking is computed based on DCG (Dis-
counted Cumulative Gain) as follows: 
             
    
       
 
   , 
where   is the rank position, and            
is the relevance grade of the  th identifier in 
the ranked result set. In our experiment, 
       corresponds to an interaction iden-
tifier, and        corresponds to other iden-
tifiers. NDCG is then computed as follows: 
        
      
       
, 
where      denotes the results of a perfect 
ranking. The NDCG values for all articles are 
averaged to obtain the average performance of 
the proposed framework. 
5.4 INT Test Set Performance 
Figure 4 shows the Area_iPR scores of four 
configurations. In the baseline configuration 
(Local/Rank1), the SVM-based local ranking 
model released by Dai et al is employed. In 
the configuration Global+LC, Global+MBF, 
and Global+WBF, the proposed global ranking 
framework is cascaded with the local ranking 
model and with three data fusion models: the 
LC model, the modified Borda-fuse (MBF) 
model, and the weighted Borda-fuse model. 
The figure also shows the Area_iPR scores of 
 
Figure 3. Precision of Different Ranks. 
0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
P
re
c
is
io
n
Rank
228
the top three teams and the average Area_iPR 
score of all BioCreAtIvE II.5 INT participants 
(Average).  
The results show that under the global rank-
ing framework, Area_iPR performance is im-
proved in addition to Global+MBF. The high-
est Area_iPR (Global+LC: 46.7%) is 3.2% 
higher than the Rank 1 score in the BioCreA-
tIvE II.5 INT challenge. According to our 
analysis, before global ranking, identifiers 
whose feature values rarely appear in the train-
ing set are often ranked incorrectly because 
their feature values are under-estimated by the 
ranking model. However, if the identifiers co-
occur with higher-ranked identifiers whose 
feature values appear frequently, the proposed 
framework is very likely to increase their ranks. 
This results in an improved Area_iPR score. 
5.5 Global Ranking Performance 
 
To illustrate the effectiveness of the proposed 
global ranking framework and assess its per-
formance when it is cascaded with other con-
ventional ranking models, we implement a 
simple term frequency-based ranking function, 
which is based on the identifier frequency in 
an article as another local ranking model. If 
two or more identifiers have the same frequen-
cy, two heuristic rules are employed sequen-
tially to rank them: (1) the identifier with the 
highest frequency in the Results section of the 
article, and (2) the identifier mentioned first in 
the article.  
Table 1 shows the NDCG percentage gain 
of different ranking models. It compares the 
ranked list generated by our global ranking 
framework and by the local ranking models. 
We observe that (1) irrespective of whether the 
local ranking model is a conventional model or 
a learning to rank model, Global+LC and 
Global+WBF models achieve NDCG gains 
over the original rankings of the local ranking 
models; (2) the results show that our global 
ranking framework can improve the perfor-
mance by only exploiting MI analysis. Howev-
er, it is expected that employing more ad-
vanced relation extraction methods to deter-
mine the global information (Equation 3) 
would yield more reliable pseudo-ranked lists 
and lead to a further improvement in the final 
ranking; and (3) similar to the results in Sec-
tion 5.4, the performance of Global+MBF does 
not improve. Global+MBF has a negative 
NDCG gain and the Area_iPR decreases by 
2.61%. We believe this is due to MBF gives 
equal weight to each pseudo-IR system. As 
mentioned in Section 4.1, the document set in 
INT is comprised of the identifiers of the gene 
mentions derived by Dai et al?s system. Un-
fortunately, there must be incorrect identifiers 
(the errors may be due to their gene mention 
recognition or identifier mapping processes). 
As in the meta-search, the best performance is 
often achieved by weighting the input systems 
unequally. Reasonable weights allow the algo-
rithm to concentrate on good feedback from 
pseudo-IR systems and ignore poor feedback. 
As shown by the average precision results in 
Figure 3, the identifiers (corresponding to the 
pseudo-IR systems in our framework) in the 
higher ranks are more reliable; however, MBF 
cannot use this information, which leads to a 
negative NDCG gain and a lower Area_iPR 
score. 
6 Conclusion 
We have presented a new global ranking 
framework based on data fusion technology. 
Our approach solves the global ranking prob-
lem in three stages: the first stage ranks the 
document set by the original local ranking 
model; the second stage transforms the prob-
Based on Global Ranking NDCG1 NDCG3 NDCG5 
Local Ranking 
/Rank1 
Global+LC +0.908 +1.323 -0.003 
Global+MBF -3.279 -1.034 -0.020 
Global+WBF -0.016 +3.630 +2.071 
Freq Global+LCf +1.639 +3.152 +2.817 
Global+MBFf -6.860 -4.275 -4.839 
Global+WBFf +2.549 +2.390 +3.043 
Table 1. The NDCG Gain (%) of Different 
Ranking Models. 
 
Figure 4. The Area_iPR Results of Different 
Ranking Models 
0.22 0.27 0.32 0.37 0.42 0.47 0.52
Global+WBF
Global+MBF
Global+LC
Local/Rank1
Hakenberg/Rank 2
S? tre/Rank 3
Average
229
lem into a data fusion task by using global in-
formation, and the final stage adapts fusion 
algorithms to solve the ranking problem. The 
framework is flexible and it can be combined 
with other mature ranking models and fusion 
algorithms. We also show how the BioCreA-
tIvE INT can be formulated as a global ranking 
problem and solved by the proposed frame-
work. Experiments on the INT dataset demon-
strate the effectiveness of the proposed frame-
work and its superior performance over other 
ranking models. 
In our future work, we will address the fol-
lowing issues: (1) the use of advanced data 
fusion algorithms in the proposed framework; 
(2) assessing the performance of the proposed 
framework on other tasks, such as Pseudo Re-
levance Feedback and Topic Distillation; and 
(3) design an advanced supervised learning 
relation extraction algorithm to replace MI in 
INT to evaluate the system performance. 
References 
Adler, P., R. Kolde, M. Kull, A. Tkachenko, H. 
Peterson, J. Reimand and J. Vilo (2009). Mining 
for coexpression across hundreds of datasets 
using novel rank aggregation and visualization 
methods. Genome Biology 10(R139). 
Aslam, J. A. and M. Montague (2001). Models for 
metasearch. Proceedings of the 24th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, New Orleans, Louisiana, United States, 
ACM. 
Bartell, B. T., G. W. Cottrell and R. K. Belew 
(1994). Automatic combination of multiple 
ranked retrieval systems. Proceedings of the 
17th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, Dublin, Ireland Springer-
Verlag New York, Inc. 
Borda, J. (1781). M?moire sur les ?lections au 
scrutin. Histoire del'Acad e?mie Royale des 
Sciences 2: 13. 
Chowdhury, G. (2007). TREC: Experiment and 
Evaluation in Information Retrieval. Online 
Information Review 31(5): 462. 
Dai, H.-J., P.-T. Lai and R. T.-H. Tsai (2010). 
Multi-stage gene normalization and SVM-based 
ranking for protein interactor extraction in full-
text articles. IEEE TRANSACTIONS ON 
COMPUTATIONAL BIOLOGY AND 
BIOINFORMATICS 14 May. 2010. IEEE 
computer Society Digital Library. IEEE 
Computer Society. 
Fox, E. A. and J. A. Shaw (1994). Combination of 
Multiple Searches. 1994, Proceedings of the 
Second Text REtrieval Conference (TREC 2)  
Jenssen, T.-K., A. Lagreid, J. Komorowski and E. 
Hovig (2001). A literature network of human 
genes for high-throughput analysis of gene 
expression. Nature Genetics 28(1): 21-28. 
Knaus, D., E. Mittendorf and P. Sch?uble (1995). 
Improving a basic retrieval method by links and 
passage level evidence. NIST Special 
Publication 500-225: Overview of the Third Text 
REtrieval Conference (TREC-3). 
Krallinger, M., F. Leitner and A. Valencia (2009). 
The BioCreative II.5 challenge overview. 
Proceedings of the BioCreative II.5 Workshop 
2009 on Digital Annotations, Madrid, Spain. 
Kwok, K. L. (1984). A document-document 
similarity measure based on cited titles and 
probability theory, and its application to 
relevance feedback retrieval. SIGIR'84. 
Lee, J. H. (1997). Analyses of multiple evidence 
combination. Proceedings of the 20th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, Philadelphia, Pennsylvania, United 
States, ACM. 
Lin, S. and J. Ding (2008). Integration of Ranked 
Lists via Cross Entropy Monte Carlo with 
Applications to mRNA and microRNA Studies. 
Biometrics 65(1): 9-18. 
Liu, Y.-T., T.-Y. Liu, T. Qin, Z.-M. Ma and H. Li 
(2007). Supervised rank aggregation. 
Proceedings of the 16th international conference 
on World Wide Web, Banff, Alberta, Canada, 
ACM. 
Mardis, S., F. Leitner and L. Hirschman (2009). 
BioCreative II.5: Evaluation and ensemble 
system performance. Proceedings of the 
BioCreative II.5 Workshop 2009 on Digital 
Annotations, Madrid, Spain. 
Nuray, R. and F. Can (2006). Automatic ranking of 
information retrieval systems using data fusion. 
Inf. Process. Manage. 42(3): 595-614. 
Pihura, V., S. Dattaa and S. Datta (2008). Finding 
common genes in multiple cancer types through 
meta?analysis of microarray experiments: A 
230
rank aggregation approach Genomics 92(6): 
400-403  
Qin, T., T.-Y. Liu, X.-D. Zhang, D.-S. Wang and H. 
Li (2008). Global Ranking Using Continuous 
Conditional Random Fields. Proceedings of the 
Twenty-Second Annual Conference on Neural 
Information Processing Systems  (NIPS 2008), 
Vancouver, Canada. 
Qin, T., T. Liu, X. Zhang, D. Wang, W. Xiong and 
H. Li (2008). Learning to rank relational objects 
and its application to web search, ACM. 
Vogt, C. and G. Cottrell (1999). Fusion via a linear 
combination of scores. Information Retrieval 
1(3): 151-173. 
Vogt, C. C. and G. W. Cottrell (1998). Predicting 
the performance of linearly combined IR systems. 
Proceedings of the 21st annual international 
ACM SIGIR conference on Research and 
development in information retrieval, Melbourne, 
Australia ACM. 
Zhao, Z., J. Wang, S. Sharma, N. Agarwal, H. Liu 
and Y. Chang (2010). An Integrative Approach 
to Identifying Biologically Relevant Genes. 
Proceedings of SIAM International Conference 
on Data Mining (SDM). 
 
 
231
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1474?1480,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Leveraging Effective Query Modeling Techniques  
for Speech Recognition and Summarization 
 
 Kuan-Yu Chen*?,  Shih-Hung Liu*, Berlin Chen#, Ea-Ee Jan+,  
Hsin-Min Wang*, Wen-Lian Hsu*, and Hsin-Hsi Chen? 
*Institute of Information Science, Academia Sinica, Taiwan 
?National Taiwan University, Taiwan 
#National Taiwan Normal University, Taiwan 
+IBM Thomas J. Watson Research Center, USA 
{kychen, journey, whm, hsu}@iis.sinica.edu.tw, 
berlin@ntnu.edu.tw, hhchen@csie.ntu.edu.tw, ejan@us.ibm.com 
 
Abstract 
Statistical language modeling (LM) that 
purports to quantify the acceptability of a 
given piece of text has long been an in-
teresting yet challenging research area. In 
particular, language modeling for infor-
mation retrieval (IR) has enjoyed re-
markable empirical success; one emerg-
ing stream of the LM approach for IR is 
to employ the pseudo-relevance feedback 
process to enhance the representation of 
an input query so as to improve retrieval 
effectiveness. This paper presents a con-
tinuation of such a general line of re-
search and the main contribution is three-
fold. First, we propose a principled 
framework which can unify the relation-
ships among several widely-used query 
modeling formulations. Second, on top of 
the successfully developed framework, 
we propose an extended query modeling 
formulation by incorporating critical que-
ry-specific information cues to guide the 
model estimation. Third, we further adopt 
and formalize such a framework to the 
speech recognition and summarization 
tasks. A series of empirical experiments 
reveal the feasibility of such an LM 
framework and the performance merits of 
the deduced models on these two tasks. 
1 Introduction 
Along with the rapidly growing popularity of the 
Internet and the ubiquity of social web commu-
nications, tremendous volumes of multimedia 
contents, such as broadcast radio and television 
programs, digital libraries and so on, are made 
available to the public. Research on multimedia 
content understanding and organization has wit-
nessed a booming interest over the past decade. 
By virtue of the developed techniques, a variety 
of functionalities were created to help distill im-
portant content from multimedia collections, or 
provide locations of important speech segments 
in a video accompanied with their corresponding 
transcripts, for users to listen to or to digest. Sta-
tistical language modeling (LM) (Jelinek, 1999; 
Jurafsky and Martin, 2008; Zhai, 2008), which 
manages to quantify the acceptability of a given 
word sequence in a natural language or capture 
the statistical characteristics of a given piece of 
text, has been proved to offer both efficient and 
effective modeling abilities in many practical 
applications of natural language processing and 
speech recognition (Ponte and Croft, 1998; Jelin-
ek, 1999; Huang, et al., 2001; Zhai and Lafferty, 
2001a; Jurafsky and Martin, 2008; Furui et al., 
2012; Liu and Hakkani-Tur, 2011). 
The LM approach was first introduced for the 
information retrieval (IR) problems in the late 
1990s, indicating very good potential, and was 
subsequently extended in a wide array of follow-
up studies. One typical realization of the LM ap-
proach for IR is to access the degree of relevance 
between a query and a document by computing 
the likelihood of the query generated by the doc-
ument (usually referred to as the query-
likelihood approach) (Zhai, 2008; Baeza-Yates 
and Ribeiro-Neto, 2011). A document is deemed 
to be relevant to a given query if the correspond-
ing document model is more likely to generate 
the query. On the other hand, the Kullback-
Leibler divergence measure (denoted by KLM 
for short hereafter), which quantifies the degree 
of relevance between a document and a query 
from a more rigorous information-theoretic per-
spective, has been proposed (Lafferty and Zhai, 
2001; Zhai and Lafferty, 2001b; Baeza-Yates and 
Ribeiro-Neto, 2011). KLM not only can be 
thought as a natural generalization of the query-
likelihood approach, but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the performance of doc-
ument ranking. For example, a main challenge 
facing such a measure is that since a given query 
usually consists of few words, the true infor-
mation need is hard to be inferred from the sur-
face statistics of a query. As such, one emerging 
stream of thought for KLM is to employ the 
1474
pseudo-relevance feedback process to construct 
an enhanced query model (or representation) so 
as to achieve better retrieval effectiveness (Hi-
emstra et al., 2004; Lv and Zhai, 2009; Carpineto 
and Romano, 2012; Lee and Croft, 2013). 
Following this line of research, the major con-
tribution of this paper is three-fold: 1) we ana-
lyze several widely-used query models and then 
propose a principled framework to unify the rela-
tionships among them; 2) on top of the success-
fully developed query models, we propose an 
extended modeling formulation by incorporating 
additional query-specific information cues to 
guide the model estimation; 3) we explore a nov-
el use of these query models by adapting them to 
the speech recognition and summarization tasks. 
As we will see, a series of experiments indeed 
demonstrate the effectiveness of the proposed 
models on these two tasks. 
2 Language Modeling Framework 
2.1 Kullback-Leibler Divergence Measure 
A promising realization of the LM approach to 
IR is the Kullback-Leibler divergence measure 
(KLM), which determines the degree of rele-
vance between a document and a query from a 
rigorous information-theoretic perspective. Two 
different language models are involved in KLM: 
one for the document and the other for the query. 
The divergence of the document model with re-
spect to the query model is defined by  
.)|( )|(log)|()||(KL ? ?? Vw DwP QPQwPDQ
  (1)  
KLM not only can be thought as a natural gener-
alization of the traditional query-likelihood ap-
proach (Yi and Allan, 2009; Baeza-Yates and 
Ribeiro-Neto, 2011), but also has the additional 
merit of being able to accommodate extra infor-
mation cues to improve the estimation of its 
component models in a systematic way for better 
document ranking (Zhai, 2008).  
Due to that a query usually consists of only a 
few words, the true query model P(w|Q)
 
might 
not be accurately estimated by the simple ML 
estimator (Jelinek, 1991). There are several stud-
ies devoted to estimating a more accurate query 
modeling, saying that it can be approached with 
the pseudo-relevance feedback process (Lavren-
ko and Croft, 2001; Zhai and Lafferty, 2001b). 
However, the success depends largely on the as-
sumption that the set of top-ranked documents, 
DTop={D1,D2,...,Dr,...}, obtained from an initial 
round of retrieval, are relevant and can be used to 
estimate a more accurate query language model. 
2.2 Relevance Modeling  
Under the notion of relevance modeling (RM, 
often referred to as RM-1), each query Q is as-
sumed to be associated with an unknown rele-
vance class RQ, and documents that are relevant 
to the semantic content expressed in query are 
samples drawn from the relevance class RQ. 
Since there is no prior knowledge about RQ, we 
may use the top-ranked documents DTop to ap-
proximate the relevance class RQ. The corre-
sponding relevance model can be estimated using 
the following equation (Lavrenko and Croft, 
2001; Lavrenko, 2004): 
.)|()(
)|()|()(  )|(RM ? ? ??
? ? ??
???
?
?????
??
ToprD
ToprD
Qw rr
Qw rrr
DwPDP
DwPDwPDPQwP
D
D
(2) 
2.3 Simple Mixture Model 
Another perspective of estimating an accurate 
query model with the top-ranked documents is 
the simple mixture model (SMM), which as-
sumes that words in DTop are drawn from a two-
component mixture model: 1) One component is 
the query-specific topic model PSMM(w|Q), and 2) 
the other is a generic background model 
P(w|BG). By doing so, the SMM model 
PSMM(w|Q) can be estimated by maximizing the 
likelihood over all the top-ranked documents 
(Zhai and Lafferty, 2001b; Tao and Zhai, 2006): 
? ? ,)|()1()|( ),(SMM?? ?? ????? Topr rD Vw DwcBGwPQwPL D ??
(3) 
where ?  is a pre-defined weighting parameter 
used to control the degree of reliance between 
PSMM(w|Q) and P(w|BG). This estimation will 
enable more specific words to receive more 
probability mass, thereby leading to a more dis-
criminative query model PSMM(w|Q). 
Although the SMM modeling aims to extract 
extra word usage cues for enhanced query mod-
eling, it may confront two intrinsic problems. 
One is the extraction of word usage cues from 
DTop is not guided by the original query. The oth-
er is that the mixing coefficient ?  is fixed across 
all top-ranked documents albeit that different 
documents would potentially contribute different 
amounts of word usage cues to the enhanced 
query model. To mitigate these two problems, 
the regularized simple mixture model has been 
proposed and can be estimated by maximizing 
the likelihood function (Tao and Zhai, 2006; Dil-
lon and Collins-Thompson, 2010) 
? ? ,)|()1()|(    
)|(
),(
RSMM
)|(
RSMM
?
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
Dwc
DD
Vw
QwP
BGwPQwP
QwPL
D
??
?
(4) 
where   is a weighting factor indicating the con-
fidence on the prior information. 
3 The Proposed Modeling Framework 
3.1 Fundamentals 
It is obvious that the major difference among the 
1475
representative query models mentioned above is 
how to capitalize on the set of top-ranked docu-
ments and the original query. Several subtle rela-
tionships can be deduced through the following 
in-depth analysis. First, a direct inspiration of the 
LM-based query reformulation framework can 
be drawn from the celebrated Rocchio?s formula-
tion, while the former can be viewed as a proba-
bilistic counterpart of the latter (Robertson, 1990; 
Ponte and Croft, 1998; Baeza-Yates and Ribeiro-
Neto, 2011). Second, after some mathematical 
manipulation, the formulation of the RM model 
(c.f. Eq. (2)) can be rewritten as 
.)()|(
)()|()|(  )|(RM ? ? ???? ????? ToprD ToprD rr
rrr DPDQP
DPDQPDwPQwP D D
(5) 
It becomes evident that the RM model is com-
posed by mixing a set of document models 
P(w|Dr). As such, the RM model bears a close 
resemblance to the Rocchio?s formulation. Fur-
thermore, based on Eq. (5), we can recast the 
estimation of the RM model as an optimization 
problem, and the likelihood (or objective) func-
tion is formulated as 
1)|( ..
,)|()|(
),(
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Topr
Topr
D
r
Vw
Qwc
D
rr
QDPts
QDPDwPL
D
D
  (6) 
where the document models P(w|Dr) are known 
in advance; the conditional probability P(Dr|Q) 
of each document Dr is unknown and leave to be 
estimated. Finally, a principled framework can 
be obtained to unify all of these query models, 
including RM (c.f. Eq. (6)), SMM (c.f. Eq. (3)) 
and RSMM (c.f. Eq. (4))), by using a generalized 
objective likelihood function: 
1)( ..
,)()|(
),(
?
???
?
???
?
?
?
?
?
?
?
?
?
?
M
E M
r
i
i
r
M
r
Vw E
Ewc
M
rr
MPts
MPMwPL  (7) 
where E represents a set of observations which 
we want to maximize their likelihood, and M 
denotes a set of mixture components.  
3.2 Query-specific Mixture Modeling 
The SMM model and the RSMM model are in-
tended to extract useful word usage cues from 
DTop, which are not only relevant to the original 
query Q but also external to those already cap-
tured by the generic background model. Howev-
er, we argue in this paper that the ?generic in-
formation? should be carefully crafted for each 
query due mainly to the fact that users? infor-
mation needs may be very diverse from one an-
other. To crystallize the idea, a query-specific 
background model PQ(w|BG) for each query Q 
can be derived from DTop directly. Another con-
sideration is that since the original query model 
P(w|Q) cannot be accurately estimated, it thus 
may not necessarily be the best choice for use in 
defining a conjugate Dirichlet prior for the en-
hanced query model to be estimated. We propose 
to use the RM model as a prior to guide the esti-
mation of the enhanced query model. The en-
hanced query model is termed query-specific 
mixture model (QMM), and its corresponding 
training objective function can be expressed as 
? ??
?
?
?
?
?
?
????
??
Topr
r
rrD Vw
DwcQDD
Vw
QwP
BGwPQwP
QwPL
D
.)|()1()|(    
)|(
),(QMM
)|(QMM RM
??
?
 (8) 
4 Applications 
4.1 Speech Recognition 
Language modeling is a critical and integral 
component in any large vocabulary continuous 
speech recognition (LVCSR) system (Huang et 
al., 2001; Jurafsky and Martin, 2008; Furui et al., 
2012). More concretely, the role of language 
modeling in LVCSR can be interpreted as calcu-
lating the conditional probability P(w|H), in 
which H is a search history, usually expressed as 
a sequence of words H=h1, h2,?, hL, and w is 
one of its possible immediately succeeding 
words. Once the various aforementioned query 
modeling methods are applied to speech recogni-
tion, for a search history H, we can conceptually 
regard it as a query and each of its immediately 
succeeding words w as a (single-word) document. 
Then, we may leverage an IR procedure that 
takes H as a query and poses it to a retrieval sys-
tem to obtain a set of top-ranked documents from 
a contemporaneous (or in-domain) corpus. Final-
ly, the enhanced query model (that is P(w|H) in 
speech recognition) can be estimated by RM, 
SMM, RSMM or QM , and further combined 
with the background n-gram (e.g., trigram) lan-
guage model to form an adaptive language model 
to guide the speech recognition process. 
4.2 Speech Summarization 
On the other hand, extractive speech summariza-
tion aims at producing a concise summary by 
selecting salient sentences or paragraphs from 
the original spoken document according to a pre-
defined target summarization ratio (Carbonell 
and Goldstein, 1998; Mani and Maybury, 1999; 
Nenkova and McKeown, 2011; Liu and 
Hakkani-Tur, 2011). Intuitively, this task could 
be framed as an ad-hoc IR problem, where the 
spoken document is treated as an information 
need and each sentence of the document is re-
garded as a candidate information unit to be re-
trieved according to its relevance to the infor-
mation need. Therefore, KLM can be used to 
quantify how close the document D and one of 
its sentences S are: the closer the sentence model 
P(w|S) to the document model P(w|D), the more 
1476
likely the sentence would be part of the summary. 
Due to that each sentence S of a spoken docu-
ment D to be summarized usually consists of 
only a few words, the corresponding sentence 
model P(w|S) might not be appropriately esti-
mated by the ML estimation. To alleviate the 
deficiency, we can leverage the merit of the 
above query modeling techniques to estimate an 
accurate sentence model for each sentence to 
enhance the summarization performance. 
5 Experimental Setup 
The speech corpus consists of about 196 hours of 
Mandarin broadcast news collected by the Aca-
demia Sinica and the Public Television Service 
Foundation of Taiwan between November 2001 
and April 2003 (Wang et al., 2005), which is 
publicly available and has been segmented into 
separate stories and transcribed manually. Each 
story contains the speech of one studio anchor, as 
well as several field reporters and interviewees. 
A subset of 25-hour speech data compiled during 
November 2001 to December 2002 was used to 
bootstrap the acoustic model training. The vo-
cabulary size is about 72 thousand words. The 
background language model was estimated from 
a background text corpus consisting of 170 mil-
lion Chinese characters collected from the Chi-
nese Gigaword Corpus released by LDC. 
The dataset for use in the speech recognition 
experiments is compiled by a subset of 3-hour 
speech data from the corpus within 2003 (1.5 
hours for development and 1.5 hours for test). 
The contemporaneous (in-domain) text corpus 
used for training the various LM adaptation 
methods was collected between 2001 and 2003 
from the corpus (excluding the test set), which 
consists of one million Chinese characters of the 
orthographic broadcast news transcripts. In this 
paper, all the LM adaptation experiments were 
performed in word graph rescoring. The associ-
ated word graphs of the speech data were built 
beforehand with a typical LVCSR system (Ort-
manns et al., 1997; Young et al., 2006). 
In addition, the summarization task also em-
ploys the same broadcast news corpus as well. A 
subset of 205 broadcast news documents com-
piled between November 2001 and August 2002 
was reserved for the summarization experiments 
(185 for development and 20 for test). A subset 
of about 100,000 text news documents, compiled 
during the same period as the documents to be 
summarized, was employed to estimate the relat-
ed summarization models compared in this paper. 
We adopted three variants of the widely-used 
ROUGE metric (i.e., ROUGE-1, ROGUE-2 and 
ROUGE-L) for the assessment of summarization 
performance (Lin, 2003). The summarization 
ratio, defined as the ratio of the number of words 
in the automatic (or manual) summary to that in 
the reference transcript of a spoken document, 
was set to 10% in this research. 
6 Experimental Results 
In the first part of experiments, we evaluate the 
effectiveness of the various query models applied 
to the speech recognition task. The correspond-
ing results with respect to different numbers of 
top-ranked documents being used for estimating 
their component models are shown in Table 1. 
Also worth mentioning is that the baseline sys-
tem with the background trigram language model, 
which was trained with the SRILM toolkit 
(Stolcke, 2005) and Good-Turing smoothing 
(Jelinek, 1999), results in a Chinese character 
error rate (CER) of 20.08% on the test set. Con-
sulting Table 1 we notice two particularities. One 
is that there is more fluctuation in the CER re-
sults of SMM than in those of RM. The reason 
might be that, for SMM, the extraction of rele-
vance information from the top-ranked docu-
ments is conducted with no involvement of the 
test utterance (i.e., the query; or its correspond-
ing search histories), as elaborated earlier in Sec-
tion 2. When too many feedback documents are 
being used, there would be a concern for SMM 
to be distracted from being able to appropriate 
model the test utterance, which is probably 
caused by some dominant distracting (or irrele-
vant) feedback documents. The other interesting 
observation is that RSMM only achieves a com-
parable (even worse) result when compared to 
SMM. A possible reason is that the prior con-
straint of the RSMM may contain too much 
noisy information so as to bias the model estima-
tion. Furthermore, it is evident that the proposed 
QMM is the best-performing method among all 
the query models compared in the paper. Alt-
hough the improvements made by QMM are not 
as pronounced as expected, we believe that 
QMM has demonstrated its potential to be ap-
plied to other related applications. On the other 
hand, we compare the various query models with 
two well-practiced language models, namely the 
cache model (Cache) (Kuhn and Mori, 1990; 
Jelinek et al., 1991) and the latent Dirichlet allo-
cation (LDA) (Liu and Liu, 2007; Tam and 
Schultz, 2005). The CER results of these two 
models are also shown in Table 1, respectively. 
For the cache model, bigram cache was used 
since it can yield better results than the unigram 
and trigram cache models in our experiments. It 
is worthy to notice that the LDA model was 
trained with the entire set of contemporaneous 
text document collection (c.f. Section 4), while 
all of the query models explored in the paper 
were estimated based on a subset of the corpus 
selected by an initial round of retrieval. The re-
sults reveal that most of these query models can 
achieve superior performance over the two con-
ventional language models. 
1477
In the second part of experiments, we evaluate 
the utilities of the various query models as ap-
plied to the speech summarization task. At the 
outset, we assess the performance level of the 
baseline KLM method by comparison with two 
well-practiced unsupervised methods, viz. the 
vector space model (VSM) (Gong and Liu, 2001), 
and its extension, maximal marginal relevance 
(MMR) (Carbonell and Goldstein, 1998). The 
corresponding results are shown in Table 2 and 
can be aligned with several related literature re-
views. By looking at the results, we find that 
KLM outperforms VSM by a large margin, con-
firming the applicability of the language model-
ing framework for speech summarization. Fur-
thermore, MMR that presents an extension of 
VSM performs on par with KLM for the text 
summarization task (TD) and exhibits superior 
performance over KLM for the speech summari-
zation task (SD). We now turn to evaluate the 
effectiveness of the various query models (viz. 
RM, SMM, RSMM and QMM) in conjunction 
with the pseudo-relevance feedback process for 
enhancing the sentence model involved in the 
KLM method. The corresponding results are also 
shown in Table 2. Two noteworthy observations 
can be drawn from Table 2. One is that all these 
query models can considerably improve the 
summarization performance of the KLM method, 
which corroborates the advantage of using them 
for enhanced sentence representations. The other 
is that QMM is the best-performing one among 
all the formulations studied in this paper for both 
the TD and SD cases.  
Going one step further, we explore to use extra 
prosodic features that are deemed complemen-
tary to the LM cue provided by QMM for speech 
summarization. To this end, a support vector ma-
chine (SVM) based summarization model is 
trained to integrate a set of 28 commonly-used 
prosodic features (Liu and Hakkani-Tur, 2011) 
for representing each spoken sentence, since 
SVM is arguably one of the state-of-the-art su-
pervised methods that can make use of a diversi-
ty of indicative features for text or speech sum-
marization (Xie and Liu, 2010; Chen et al., 
2013). The sentence ranking scores derived by 
QMM and SVM are in turn integrated through a 
simple log-linear combination. The correspond-
ing results are shown in Table 2, demonstrating 
consistent improvements with respect to all the 
three variants of the ROUGE metric as compared 
to that using either QMM or SVM in isolation. 
We also investigate using SVM to additionally 
integrate a richer set of lexical and relevance fea-
tures to complement QMM and further enhance 
the summarization effectiveness. However, due 
to space limitation, we omit the details here. As a 
side note, there is a sizable gap between the TD 
and SD cases, indicating room for further im-
provements. We may seek remedies, such as ro-
bust indexing schemes, to compensate for imper-
fect speech recognition. 
7 Conclusion and Outlook 
In this paper, we have presented a systematic and 
thorough analysis of a few well-practiced query 
models for IR and extended their novel applica-
bility to speech recognition and summarization in 
a principled way. Furthermore, we have pro-
posed an extension of this research line by intro-
ducing query-specific mixture modeling; the util-
ities of the deduced model have been extensively 
compared with several existing query models. As 
to future work, we would like to investigate 
jointly integrating proximity and other different 
kinds of relevance and lexical/semantic infor-
mation cues into the process of feedback docu-
ment selection so as to improve the empirical 
effectiveness of such query modeling.  
Acknowledgements 
This research is supported in part by the ?Aim 
for the Top University Project? of National Tai-
wan Normal University (NTNU), sponsored by 
the Ministry of Education, Taiwan, and by the 
Ministry of Science and Technology, Taiwan, 
under Grants MOST 103-2221-E-003-016-MY2, 
NSC 101-2221-E-003-024-MY3, NSC 102-
2221-E-003-014-, NSC 101-2511-S-003-057-
MY3, NSC 101-2511-S-003-047-MY3 and NSC 
103-2911-I-003-301. 
  
Table 1. The speech recognition results (in CER 
(%)) achieved by various language models along 
with different numbers of latent topics/pseudo-
relevance feedback documents. 
 16 32 64 128 
Baseline 20.08 
Cache 19.86 
LDA 19.29 19.30 19.28 19.15 
RM 19.26 19.26 19.26 19.26 
SMM 19.19 19.00 19.14 19.10 
RSMM 19.18 19.14 19.15 19.19 
QMM 19.05 18.97 19.00 18.99 
Table 2. The summarization results (in F-scores) 
achieved by various language models along with 
text and spoken documents. 
 
Text Documents (TD) Spoken Documents (SD) 
ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L 
VSM 0.347 0.228 0.290 0.342 0.189 0.287 
MMR 0.407 0.294 0.358 0.381 0.226 0.331 
KLM 0.411 0.298 0.361 0.364 0.210 0.307 
RM 0.453 0.335 0.403 0.382 0.239 0.331 
SMM 0.439 0.320 0.388 0.383 0.229 0.327 
RSMM 0.472 0.365 0.423 0.381 0.235 0.329 
QMM 0.486 0.382 0.435 0.395 0.256 0.349 
SVM 0.441 0.334 0.396 0.370 0.222 0.326 
QMM+
SVM 
0.492 0.395 0.448 0.398 0.261 0.358 
 
 
 
 
1478
References 
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 
2011. Modern information retrieval: the con-
cepts and technology behind search, ACM 
Press. 
David M. Blei, Andrew Y. Ng, and Michael I. 
Jordan. 2003. Latent dirichlet allocation. 
Journal of Machine Learning Research, 
pp.993?1022. 
David M. Blei and John Lafferty. 2009. Topic 
models. In A. Srivastava and M. Sahami, 
(eds.), Text Mining: Theory and Applications. 
Taylor and Francis.  
Jaime Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversitybased reranking for 
reordering documents and producing sum-
maries. In Proc. SIGIR, pp. 335?336. 
Claudio Carpineto and Giovanni Romano. 2012. 
A survey of automatic query expansion in in-
formation retrieval. ACM Computing Surveys, 
vol. 44, pp.1?56. 
Stephane Clinchant and Eric Gaussier. 2013. A 
theoretical analysis of pseudo-relevance 
feedback models. In Proc. ICTIR. 
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and 
Stephen Robertson. 2008. Selecting good 
expansion terms for pseudo-relevance feed-
back. In Proc. SIGIR, pp. 243?250. 
Berlin Chen, Shih-Hsiang Lin, Yu-Mei Chang, 
and Jia-Wen Liu. 2013. Extractive speech 
summarization using evaluation metric-
related training criteria. Information Pro-
cessing & Management, 49(1), pp. 1cess 
Arthur P. Dempster, Nan M. Laird, and Donald 
B. Rubin. 1977. Maximum likelihood from 
incomplete data via the EM algorithm. Jour-
nal of Royal Statistical Society B, 39(1), pp. 
1?38. 
Joshua V. Dillon and Kevyn Collins-Thompson. 
2010. A unified optimization framework for 
robust pseudo-relevance feedback algorithms. 
In Proc. CIKM, pp. 1069?1078. 
Sadaoki Furui, Li Deng, Mark Gales, Hermann 
Ney, and Keiichi Tokuda. 2012. Fundamen-
tal technologies in modern speech recogni-
tion. IEEE Signal Processing Magazine, 
29(6), pp. 16?17. 
Yihong Gong and Xin Liu. 2001. Generic text 
summarization using relevance measure and 
latent semantic analysis. In Proc. SIGIR, pp. 
19?25. 
Djoerd Hiemstra, Stephen Robertson, and Hugo 
Zaragoza. 2004. Parsimonious language 
models for information retrieval. In Proc. 
SIGIR, pp. 178?185. 
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. In Proc. SIGIR, pp. 50?57.  
Thomas Hofmann. 2001. Unsupervised learning 
by probabilistic latent semantic analysis. 
Machine Learning, 42, pp. 177?196.  
Xuedong Huang, Alex Acero, and Hsiao-Wuen 
Hon. 2001. Spoken language processing: a 
guide to theory, algorithm, and system de-
velopment. Prentice Hall PTR, Upper Saddle 
River, NJ, USA. 
Frederick Jelinek, Bernard Merialdo, Salim Rou-
kos, and M. Strauss. 1991. A dynamic lan-
guage model for speech recognition. In Proc. 
the DARPA workshop on speech and natural 
language, pp. 293?295. 
Frederick Jelinek. 1999. Statistical methods for 
speech recognition. MIT Press. 
Daniel Jurafsky and James H. Martin. 2008. 
Speech and language processing. Prentice 
Hall PTR, Upper Saddle River, NJ, USA. 
Roland Kuhn and Renato D. Mori. 1990. A 
cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6), pp. 570?583. 
Solomon Kullback and Richard A. Leibler. 1951. 
On information and sufficiency. The Annals 
of Mathematical Statistics, 22(1), pp. 79?86. 
Chin-Yew Lin. 2003. ROUGE: Recall-oriented 
Understudy for Gisting Evaluation. Availa-
ble: http://haydn.isi.edu/ROUGE/. 
Feifan Liu and Yang Liu. 2007. Unsupervised 
language model adaptation incorporating 
named entity information. In Proc. ACL, pp. 
672?769. 
Yang Liu and Dilek Hakkani-Tur. 2011. Speech 
summarization. Chapter 13 in Spoken Lan-
guage Understanding: Systems for Extract-
ing Semantic Information from Speech, G. 
Tur and R. D. Mori (Eds), New York: Wiley. 
John Lafferty and Chengxiang Zhai. 2001. Doc-
ument language models, query models, and 
risk minimization for information retrieval. 
In Proc. SIGIR, pp. 111?119. 
Victor Lavrenko and W. Bruce Croft. 2001. Rel-
evance-based language models. In Proc. 
SIGIR, pp. 120?127. 
Victor Lavrenko. 2004. A Generative Theory of 
Relevance. PhD thesis, University of Massa-
chusetts, Amherst. 
1479
Shasha Xie and Yang Liu. 2010. Improving su-
pervised learning for meeting summarization 
using sampling and regression. Computer 
Speech & Language, 24(3), pp. 495?514. 
Yuanhua Lv and Chengxiang Zhai. 2009. A 
comparative study of methods for estimating 
query language models with pseudo feed-
back. In Proc. CIKM, pp. 1895?1898. 
Yuanhua Lv and Chengxiang Zhai. 2010. Posi-
tional relevance model for pseudo-relevance 
feedback. In Proc. SIGIR, pp. 579?586. 
Kyung Soon Lee, W. Bruce Croft, and James 
Allan. 2008. A cluster-based resampling 
method for pseudo-relevance feedback. In 
Proc. SIGIR, pp. 235?242. 
Kyung Soon Lee and W. Bruce Croft. 2013. A 
deterministic resampling method using over-
lapping document clusters for pseudo-
relevance feedback. Inf. Process. Manage. 
49(4), pp. 792?806. 
Inderjeet Mani and Mark T. Maybury (Eds.). 
1999. Advances in automatic text summari-
zation. Cambridge, MA: MIT Press. 
Ani Nenkova and Kathleen McKeown. 2011. 
Automatic summarization. Foundations and 
Trends in Information Retrieval, 5(2?3), pp. 
103?233. 
Stefan Ortmanns, Hermann Ney, and Xavier Au-
bert. 1997. A word graph algorithm for large 
vocabulary continuous speech recognition. 
Computer Speech and Language, pp. 43?72. 
Jay M. Ponte and W. Bruce Croft. 1998. A lan-
guage modeling approach to information re-
trieval. In Proc. SIGIR, pp. 275?281. 
Stephen E. Robertson. 1990. On term selection 
for query expansion. Journal of Documenta-
tion, 46(4), pp. 359?364. 
Andreas Stolcke. 2005. SRILM - An extensible 
language modeling toolkit. In Proc. INTER-
SPEECH, pp.901?904. 
Tao Tao and Chengxiang Zhai. 2006. Regular-
ized estimation of mixture models for robust 
pseudo-relevance feedback. In Proc. SIGIR, 
pp. 162?169. 
Yik-Cheung Tam and Tanja Schultz. 2005. Dy-
namic language model adaptation using vari-
ational Bayes inference. In Proc. INTER-
SPEECH, pp. 5?8. 
Xuanhui Wang, Hui Fang, and Chengxiang Zhai. 
2008. A study of methods for negative rele-
vance feedback. In Proc. SIGIR, pp. 219?226. 
Hsin-Min Wang, Berlin Chen, Jen-Wei Kuo, and 
Shih-Sian Cheng. 2005. MATBN: A Manda-
rin Chinese broadcast news corpus. Interna-
tional Journal of Computational Linguistics 
& Chinese Language Processing, 10(2), pp. 
219?236. 
Xing Yi and James Allan. 2009. A comparative 
study of utilizing topic models for infor-
mation retrieval. In Proc. ECIR, pp. 29?41. 
Steve Young, Dan Kershaw, Julian Odell, Dave 
Ollason, Valtcho Valtchev, and Phil Wood-
land. 2006. The HTK book version 3.4. 
Cambridge University Press. 
Chengxiang Zhai and John Lafferty. 2001a. A 
study of smoothing methods for language 
models applied to ad hoc information re-
trieval. In Proc. SIGIR, pp. 334?342.  
Chengxiang Zhai and John Lafferty. 2001b. 
Model-based feedback in the language mod-
eling approach to information retrieval. In 
Proc. CIKM, pp. 403?410. 
Chengxiang Zhai. 2008. Statistical language 
models for information retrieval: a critical 
review. Foundations and Trends in Infor-
mation Retrieval, 2 (3), pp. 137?213. 
Yi Zhang, Jamie Callan, and Thomas Minka. 
2002. Novelty and redundancy detection in 
adaptive filtering. In Proc. SIGIR, pp. 81?88.  
1480
Term Contributed Boundary Tagging by Conditional Random 
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff 
Tian-Jian Jiang?? Shih-Hung Liu*? Cheng-Lung Sung*? Wen-Lian Hsu?? 
?Department of 
Computer Science 
National Tsing-Hua University 
*Department of 
Electrical Engineering 
National Taiwan University 
?Institute of 
Information Science 
Academia Sinica 
{tmjiang,journey,clsung,hsu}@iis.sinica.edu.tw 
 
Abstract 
This paper presents a Chinese word 
segmentation system submitted to the 
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses 
a conditional random field model with 
one simple feature called term contri-
buted boundaries (TCB) in addition to 
the ?BI? character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that 
TCB does improve ?BI? tagging domain-
independently about 1% of the F1 meas-
ure score. 
1 Introduction 
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training 
corpora remain unlabeled, and two of the test 
corpora keep domains unknown before releasing, 
therefore it is not easy to apply ordinary machine 
learning approaches, especially for the closed 
training evaluations. 
2 Methodology 
2.1 The ?BI? Character-Based Tagging of 
Conditional Random Field as Baseline 
The character-based ?OBI? tagging of 
Conditional Random Field (Lafferty et al, 2001) 
has been widely used in Chinese word 
segmentation recently (Xue and Shen, 2003; 
Peng and McCallum, 2004; Tseng et al, 2005). 
Under the scheme, each character of a word is 
labeled as ?B? if it is the first character of a 
multiple-character word, or ?I? otherwise. If the 
character is a single-character word itself, ?O? 
will be its label. As Table 1 shows, the lost of 
performance is about 1% by replacing ?O? with 
?B? for character-based CRF tagging on the 
dataset of CIPS-SIGHAN-2010 bakeoff task of 
Chinese word segmentation, thus we choose 
?BI? as our baseline for simplicity, with this 1% 
lost bearing in mind. In tables of this paper, SC 
stands for Simplified Chinese and TC represents 
for Traditional Chinese. Test corpora of SC and 
TC are divided into four domains, where suffix 
A, B, C and D attached, for texts of literature, 
computer, medicine and finance, respectively. 
  R P F OOV 
SC-A OBI 0.906 0.916 0.911 0.539 
 BI 0.896 0.907 0.901 0.508 
SC-B OBI 0.868 0.797 0.831 0.410 
 BI 0.850 0.763 0.805 0.327 
SC-C OBI 0.897 0.897 0.897 0.590 
 BI 0.888 0.886 0.887 0.551 
SC-D OBI 0.900 0.903 0.901 0.472 
 BI 0.888 0.891 0.890 0.419 
TC-A OBI 0.873 0.898 0.886 0.727 
 BI 0.856 0.884 0.870 0.674 
TC-B OBI 0.906 0.932 0.919 0.578 
 BI 0.894 0.920 0.907 0.551 
TC-C OBI 0.902 0.923 0.913 0.722 
 BI 0.891 0.914 0.902 0.674 
TC-D OBI 0.924 0.934 0.929 0.765 
 BI 0.908 0.922 0.915 0.722 
Table 1. OBI vs. BI; where the lost of F > 1%, 
such as SC-B, is caused by incorrect English 
segments that will be discussed in the section 4. 
2.2 Term Contributed Boundary 
The word boundary and the word frequency are 
the standard notions of frequency in corpus-
based natural language processing, but they lack 
the correct information about the actual boun-
dary and frequency of a phrase?s occurrence. 
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus 
when the bigram ?RAIL ENQUIRIES? and tri-
gram ?BRITISH RAIL ENQUIRIES? were ex-
amined and reported by O'Boyle (1993). Both of 
them occur 73 times, which is a large number for 
such a small corpus. ?ENQUIRIES? follows 
?RAIL? with a very high probability when it is 
preceded by ?BRITISH.? However, when 
?RAIL? is preceded by words other than ?BRIT-
ISH,? ?ENQUIRIES? does not occur, but words 
like ?TICKET? or ?JOURNEY? may. Thus, the 
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by ?EN-
QUIRIES? irrespective of what precedes it. This 
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al (2006) were 
proposed. 
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies 
(Sung et al, 2008), called term contributed 
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies 
(TF). For example, in Vodis Corpus, the original 
TF of the term ?RAIL ENQUIRIES? is 73. 
However, the actual TCF of ?RAIL ENQUI-
RIES? is 0, since all of the frequency values are 
contributed by the term ?BRITISH RAIL EN 
QUIRIES?. In this case, we can see that ?BRIT-
ISH RAIL ENQUIRIES? is really a more fre-
quent term in the corpus, where ?RAIL EN-
QUIRIES? is not. Hence the TCB of ?BRITISH 
RAIL ENQUIRIES? is ready for CRF tagging as 
?BRITISH/TB RAIL/TB ENQUIRIES/TI,? for 
example. 
3 Experiments 
Besides submitted results, there are several 
different experiments that we have done. The 
configuration is about the trade-off between data 
sparseness and domain fitness. For the sake of 
OOV issue, TCBs from all the training and test 
corpora are included in the configuration of 
submitted results. For potentially better consis-
tency to different types of text, TCBs from the 
training corpora and/or test corpora are grouped 
by corresponding domains of test corpora. Table 
2 and Table 3 provide the details, where the 
baseline is the character-based ?BI? tagging, and 
others are ?BI? with additional different TCB 
configurations: TCBall stands for the submitted 
results; TCBa, TCBb, TCBta, TCBtb, TCBtc, 
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D, 
respectively. Table 2 indicates that F1 measure 
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the 
major contribution of performance is from TCB 
of each test corpus. 
Table 2. Baseline vs. Submitted Results 
 
 
 
 
 
 
  R P F OOV 
SC-A BI 0.896 0.907 0.901 0.508 
 TCBall 0.917 0.921 0.919 0.699 
SC-B BI 0.850 0.763 0.805 0.327 
 TCBall 0.876 0.799 0.836 0.456 
SC-C BI 0.888 0.886 0.887 0.551 
 TCBall 0.900 0.896 0.898 0.699 
SC-D BI 0.888 0.891 0.890 0.419 
 TCBall 0.910 0.906 0.908 0.562 
TC-A BI 0.856 0.884 0.870 0.674 
 TCBall 0.871 0.891 0.881 0.670 
TC-B BI 0.894 0.920 0.907 0.551 
 TCBall 0.913 0.917 0.915 0.663 
TC-C BI 0.891 0.914 0.902 0.674 
 TCBall 0.900 0.915 0.908 0.668 
TC-D BI 0.908 0.922 0.915 0.722 
 TCBall 0.929 0.922 0.925 0.732 
  F OOV 
SC-A TCBta 0.918 0.690 
 TCBa 0.917 0.679 
 TCBta + TCBa 0.917 0.690 
 TCBall 0.919 0.699 
SC-B TCBtb 0.832 0.465 
 TCBb 0.828 0.453 
 TCBtb + TCBb 0.830 0.459 
 TCBall 0.836 0.456 
SC-C TCBtc 0.897 0.618 
 TCBall 0.898 0.699 
SC-D  TCBtd 0.905 0.557 
 TCBall 0.910 0.562 
Table 3a. Simplified Chinese Domain-specific 
TCB vs. TCBall 
  F OOV 
TC-A TCBta 0.889 0.706 
 TCBa 0.888 0.690 
 TCBta + TCBa 0.889 0.710 
 TCBall 0.881 0.670 
TC-B TCBtb 0.911 0.636 
 TCBb 0.921 0.696 
 TCBtb + TCBb 0.912 0.641 
 TCBall 0.915 0.663 
TC-C TCBtc 0.918 0.705 
 TCBall 0.908 0.668 
TC-D TCBtd 0.927 0.717 
 TCBall 0.925 0.732 
Table 3b. Traditional Chinese Domain-specific 
TCB vs. TCBall 
 
4 Error Analysis 
The most significant type of error in our results 
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix 
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of 
the Simplified Chinese experiment results before 
and after the post-processing. 
 
 
 F1 measure score 
before after 
SC-A OBI 0.911 0.918 
 BI 0.901 0.908 
 TCBta 0.918 0.920 
 TCBta + TCBa 0.917 0.920 
 TCBall 0.919 0.921 
SC-B OBI 0.831 0.920 
 BI 0.805 0.910 
 TCBtb 0.832 0.917 
 TCBtb + TCBb 0.830 0.916 
 TCBall 0.836 0.916 
SC-C OBI 0.897 0.904 
 BI 0.887 0.896 
 TCBtc 0.897 0.901 
 TCBall 0.898 0.902 
SC-D OBI 0.901 0.919 
 BI 0.890 0.908 
 TCBtd 0.905 0.915 
 TCBall 0.908 0.918 
Table 4. F1 measure scores before and after 
English Problem Fixed 
The major difference between gold standards 
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese 
characters. All of the alphanumeric and the 
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments 
in the Traditional Chinese corpora. For example, 
a phrase ??? / simvastatin / ? / statins? / ? / ? /
? / ?? (?/? represents the word boundary) from 
the domain C of the test data cannot be either 
recognized by ?BI? and/or TCB tagging ap-
proaches, or post-processed. This is the reason 
why Table 4 does not come along with Tradi-
tional Chinese experiment results. 
Some errors are due to inconsistencies in the 
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some 
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs 
are connected to parentheses right next to them. 
5 Conclusion 
This paper introduces a simple CRF feature 
called term contributed boundaries (TCB) for 
Chinese word segmentation. The experiment 
result shows that it can improve the basic ?BI? 
tagging scheme about 1% of the F1 measure 
score, domain-independently. 
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters 
to Chinese characters. 
Acknowledgement 
The CRF model used in this paper is developed based 
on CRF++, http://crfpp.sourceforge.net/ 
Term Contributed Boundaries used in this paper are 
extracted by YASA, http://yasa.newzilla.org/ 
References 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591?598. 
Peter O'Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis. 
Queen's University Belfast. 
Fuchun Peng and Andrew McCallum. 2004. Chinese 
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics, 
562?568, Geneva, Switzerland. 
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian 
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design 
and Applications, 325-328, Washington, D.C., 
USA. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A 
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea. 
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional 
random fields for Chinese word segmentation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 193-
196, New York, USA. 
 
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 76?80,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cost-benefit Analysis of Two-Stage Conditional Random Fields based 
English-to-Chinese Machine Transliteration 
 
Chan-Hung Kuoa  Shih-Hung Liuab  MikeTian-Jian Jiangac  
Cheng-Wei Leea  Wen-Lian Hsua 
aInstitute of Information Science, Academia Sinica 
bDepartment of Electrical Engineering, National Taiwan University 
cDepartment of Computer Science, National Tsing Hua University 
{laybow, journey, tmjiang, aska, hsu}@iis.sinica.edu.tw 
 
 
Abstract 
This work presents an English-to-Chinese 
(E2C) machine transliteration system based 
on two-stage conditional random fields 
(CRF) models with accessor variety (AV) 
as an additional feature to approximate 
local context of the source language. 
Experiment results show that two-stage 
CRF method outperforms the one-stage 
opponent since the former costs less to 
encode more features and finer grained 
labels than the latter. 
1 Introduction 
Machine transliteration is the phonetic 
transcription of names across languages and is 
essential in numerous natural language processing 
applications, such as machine translation, cross-
language information retrieval/extraction, and 
automatic lexicon acquisition (Li et al, 2009). It 
can be either phoneme-based, grapheme-based, or 
a hybrid of the above. The phoneme-based 
approach transforms source and target names into 
comparable phonemes for an intuitive phonetic 
similarity measurement between two names 
(Knight and Graehl, 1998; Virga and Khudanpur, 
2003). The grapheme-based approach, which treats 
transliteration as statistical machine translation 
problem under monotonic constraint, aims to 
obtain a direct orthographical mapping (DOM) to 
reduce possible errors introduced in multiple 
conversions (Li et al, 2004). The hybrid approach 
attempts to utilize both phoneme and grapheme 
information (Oh and Choi, 2006). Phoneme-based 
approaches are usually not good enough, because 
name entities have various etymological origins 
and transliterations are not always decided by 
pronunciations (Li et al, 2004). The state-of-the-
art of transliteration approach is bilingual DOMs 
without intermediate phonetic projections (Yang et 
al., 2010). 
Due to the success of CRF on sequential 
labeling problem (Lafferty et al, 2001), numerous 
machine transliteration systems applied it. Some of 
them treat transliteration as a two-stage sequential 
labeling problem: the first stage predicts syllable 
boundaries of source names, and the second stage 
uses those boundaries to get corresponding 
characters of target names (Yang et al, 2010; Qin 
and Chen, 2011). Dramatically de-creasing the cost 
of training with complex features is the major 
advantage of two-stage methods, but their 
downside is, compared to one-stage methods, 
features of target language are not directly applied 
in the first stage. 
Richer context generally gains better results of 
sequential labeling, but squeezed performance 
always comes with a price of computational 
complexity. To balance cost and benefit for 
English-to-Chinese (E2C) transliteration, this work 
compares the one-stage method with the two-stage 
one, using additional features of AV (Feng et al, 
2004) and M2M-aligner as an initial alignment  
(Jiampojamarn et al, 2007), to explore where the 
best investment reward is. 
The remainder of this paper is organized as 
follows. Section 2 briefly introduces related works, 
including two-stage methods and AV. The 
machine transliteration system using M2M-aligner, 
CRF models, and AV features in this work is 
explained in Section 3. Section 4 describes 
76
experiment results along with a discussion in 
Section 5. Finally, Section 6 draws a conclusion. 
2 Related Works 
Reddy and Waxmonsky (2009) presented a phrase-
based transliteration system that groups characters 
into substrings mapping onto target names, to 
demonstrate how a substring representation can be 
incorporated into CRF models with local context 
and phonemic information. Shishtla et al (2009) 
adopted a statistical transliteration technique that 
consists of alignment models of GIZA++ (Och and 
Ney, 2003) and CRF models. Jiang et al (2011) 
used M2M-aligner instead of GIZA++ and applied 
source grapheme?s AV in a CRF-based 
transliteration. 
A two-stage CRF-based transliteration was first 
designed to pipeline two independent processes 
(Yang et al, 2009). To recover from error 
propagations of the pipeline, a joint optimization of 
two-stage CRF method is then proposed to utilize 
n-best candidates of source name segmentations 
(Yang et al 2010). Another approach to resist 
errors from the first stage is split training data into 
pools to lessen computation cost of sophisticated 
CRF models for the second stage (Qin and Chen, 
2011). 
3 System Description  
3.1 EM for Initial Alignments 
M2M-aligner first maximizes the probability of 
observed source-target pairs using EM algorithm 
and subsequently sets alignments via maximum a 
posteriori estimation. To obtain initial alignments 
as good as possible, this work empirically sets the 
parameter ?maxX? of M2M-aligner for the 
maximum size of sub-alignments in the source side 
to 8, and sets the parameter ?maxY? for the 
maximum size of sub-alignments in the target side 
to 1 (denoted as X8Y1 in short), since one of the 
well-known a priori of Chinese is that almost all 
Chinese characters are monosyllabic. 
3.2 Format of Electronic Manuscript 
The two-stage CRF method consists of syllable 
segmentation and Chinese character conversion 
CRF models, namely Stage-1 and Stage-2, 
respectively. Stage-1 CRF model is trained with 
source name segmentations initially aligned by 
M2M-aligner to predict syllable boundaries as 
accurate as possible. According to the 
discriminative power of CRF, some syllable 
boundary errors from preliminary alignments could 
be counterbalanced. Stage-2 CRF model then sees 
predicted syllable boundaries as input to produce 
optimal target names. For CRF modeling, this 
work uses Wapiti (Lavergne et al, 2010). 
Using ?BULLOUGH? as an example, labeling 
schemes below are for Stage-1 training. 
? B/B U/B L/I L/I O/I U/I G/I H/E 
? B/S U/B L/1 L/2 O/3 U/4 G/5 H/E 
The first one is the common three-tag set ?BIE?. 
The last one is the eight-tag set ?B8?, including B, 
1-5, E and S: tag B indicates the beginning 
character of a syllable segment, tag E means the 
ending character, tag I or 1-5 stand for characters 
in-between, and tag S represents a single character 
segment. The expectation of the eight-tag set is the 
finer grained tags we used, the better segmentation 
accuracy we would gain. 
For Stage-2, two labeling schemes are listed in 
the following. 
? B/? ULLOUGH/? 
? B/? U/? L/I L/I O/I U/I G/I H/I 
The former as substring-based labeling scheme are 
commonly used in two-stage CRF-based 
transliteration. Syllable segments in a source word 
are composed from Stage-1 results and then are 
associated with corresponding Chinese characters 
(Yang et al 2009; Yang et al 2010; Qin and Chen, 
2011). The latter is a character-based labeling 
scheme where tags B or S from Stage-1 will be 
labeled with a Chinese character and others will be 
labeled as I. The merit of character-based method 
is to retrench the duration of the training, while 
substring-based method takes too much time to be 
included in this work for NEWS shared task. 
Section 5 will discuss more about pros and cons 
between substring and character based labeling 
schemes. 
This work tests numerous CRF feature 
combinations, for example: 
? C-3, C-2, C-1, C0, C1 , C2, C3 and 
? C-3C-2, C-2C-1, C-1C0, C0C1, C1C2, C2C3, where local context is ranging from -3 to 3, and Ci 
denotes the characters bound individually to the 
prediction label at its current position i. 
77
3.3 CRF with AV  
AV was for unsupervised Chinese word 
segmentation (Feng et al, 2004). Jiang et al, 
(2011) showed that using AV of source grapheme 
as CRF features could improve transliteration. In 
our two-stage system, Source AV is used in Stage-
1 in hope for better syllable segmentations, but not 
in Stage-2 since it may be redundant and surely 
increase training cost of Stage-2. 
4 Experiment Results 
4.1 Results of Standard Runs 
Four standard runs are submitted to NEWS12 E2C 
shared task. Their configurations are listed in Table 
1, where ?U? and ?B? denote observation 
combinations of unigram and bigram, respectively. 
A digit in front of a ?UB?, for example, ?2?, 
indicates local context ranging from -2 to 2. PBIE 
stands for ?BIE? tag set and PB8 is for ?B8? tag set. 
To summarize, the 4th (i.e. the primary) standard 
run exceeds 0.3 in terms of top-1 accuracy (ACC), 
and other ACCs of standard runs are approximate 
to 0.3. The 3rd standard run uses the one-stage CRF 
method to compare with the two-stage CRF 
method. Experiment results show that the two-
stage CRF method can excel the one-stage 
opponent, while AV and richer context also 
improve performance.  
4.2 Results of Inside Tests 
Numerous pilot tests have been conducted by 
training with both the training and development 
sets, and then testing on the development set, as 
?inside? tests. Three of them are shown in Table 2, 
where configurations I and II use the two-stage 
method, and configuration III is in one-stage. 
Table 2 suggests a trend that the one-stage CRF 
method performs better than the two-stage one on 
inside tests, but Table 1 votes the opposite. Since 
the development set includes semi-semantic 
transliterations that are unseen in both the training 
and the test sets (Jiang et al, 2011), models of 
inside tests are probably over-fitted to these noises. 
Table 3 further indicates that the number of 
features in the one-stage CRF method is doubled 
than that in the two-stage one. By putting these 
observations together, the two-stage CRF method 
is believed to be more effective and efficient than 
the one-stage CRF method. 
5 Discussions  
There are at least two major differences of two-
stage CRF-based transliteration between our 
approach and others. One is that we enrich the 
local context as much as possible, such as using 
eight-tag set in Stage-1. The other is using a 
character-based labeling method instead of a 
substring-based one in Stage-2. 
Reasonable alignments can cause CRF models 
troubles when a single source grapheme is mapped 
onto multiple phones. For instance, the alignment 
between ?HAX? and ????? generating by 
M2M-aligner. 
 HA ? ? 
 X ? ?? 
In this case, a single grapheme <X> pronounced as 
/ks/ in English therefore is associated with two 
Chinese characters ????, and won?t be an easy 
case to common character-based linear-chain CRF. 
Although for the sake of efficiency, this work 
adopts character-based CRF models, only a few of 
such single grapheme for consonant blends or 
diphthongs appeared in training and test data, and 
then the decline of accuracy would be moderate. 
One may want to know how high the price is for 
using a substring-based method to solve this 
problem. We explore the number of features 
between substring-based and character-based 
ID Configuration ACC Mean 
F-score
1 Two-stage, 2UB, PBIE 0.295 0.652 2 Two-stage, 2UB, PBIE, AV 0.299 0.659 3 One-stage, 3UB, PBIE, AV 0.291 0.654 4 Two-stage, 3UB, PB8, AV 0.311  0.662 
Table 1. Selected E2C standard runs 
 
ID Configuration ACC Mean F-score
I Two-stage, 2UB, PBIE, AV 0.363 0.707 II Two-stage, 3UB, PB8, AV 0.397 0.727III One-stage, 3UB, PBIE, AV 0.558 0.834 
Table 2. Selected E2C inside tests 
 
ID Number of Features  Numbers of Label 
II Stage-1: 60,496 Stage-1: 8 Stage-2: 2,567,618 Stage-2: 547 
III 4,439,896 548 
Table 3. Cost of selected E2C inside tests 
78
methods in Stage-2 with the same configuration II, 
as shown in Table 4. Features of substring-based 
method are tremendously more than character-
based one. Qin (2011) also reported similar 
observations. 
However, there is another issue in our character-
based method: only the starting position of a 
source syllable segment will be labeled as Chinese 
character, others are labeled as I. Base on this 
labeling strategy, the local context of the target 
graphemes is missing. 
6 Conclusions and Future Works  
This work analyzes cost-benefit trade-offs between 
two-stage and one-stage CRF-based methods for 
E2C transliteration. Experiment results indicate 
that the two-stage method can outperform its one-
stage opponent since the former costs less to 
encode more features and finer grained labels than 
the latter. Recommended future investigations 
would be encoding more features of target 
graphemes and utilizing n-best lattices from the 
outcome of Stage-1. 
Acknowledgments 
This research was supported in part by the National 
Science Council under grant NSC 100-2631-S-
001-001, and the research center for Humanities 
and Social Sciences under grant IIS-50-23. The 
authors would like to thank anonymous reviewers 
for their constructive criticisms. 
References  
Haodi Feng, Kang Chen, Xiaotie Deng, and Wiemin 
Zheng. 2004. Accessor Variety Criteria for Chinese 
Word Extraction. Computational Linguistics, 
30(1):75-93. 
Zellig Sabbetai Harris. 1970. Morpheme boundaries 
within words. Papers in Structural and 
Transformational Linguistics, 68-77. 
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek 
Sherif. 2007. Applying Many-to-Many Alignments 
and Hidden Markov Models to Letter-to-Phoneme 
Conversion. Proceedings of NAACL 2007, 372-379. 
Mike Tian-Jian Jiang, Chan-Hung Kuo and Wen-Lian 
Hsu. 2011. English-to-Chinese Machine 
Transliteration using Accessor Variety Features of 
Source Graphemes. Proceedings of the 2011 Named 
Entities Workshop. 86-90. 
K. Knight and J. Graehl. 1998. Machine Transliteration. 
Computational Linguistics, 24(4):599-612. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of ICML, 591-598. 
Thomas Lavergne, Oliver Capp? and Fran?ois Yvon. 
2010. Practical Very Large Scale CRF. Proceedings 
the 48th ACL, 504-513. 
Haizhou Li, Min Zhang and Jian Su. 2004. A Joint 
Source Channel Model for Machine Transliteration. 
Proceedings of the 42nd ACL, 159-166. 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009. Report of NEWS 2009 
Transliteration Generation Shared Task. Proceedings 
of the 2009 Named Entities Workshop. 1-18. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
J. H. Oh and K. S. Choi. 2006. An Ensemble of 
Transliteration Models for Information Retrieval. 
Information Processing and Management, 42:980-
1002. 
Ying Qin. 2011. Phoneme strings based machine 
transliteration. Proceedings of the 7th IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering. 304-309. 
Ying Qin and Guohua Chen. 2011. Forward-backward 
Machine Transliteration between English and 
Chinese Base on Combined CRF. Proceedings of the 
2011 Named Entities Workshop. 82-85. 
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning 
String Edit Distance. IEEE Transactions on Pattern 
Recognition and Machine Intelligence, 20(5):522-
532. 
Sravana Reddy and Sonjia Waxmonsky. 2009. 
Substring-based transliteration with conditional 
random fields. Proceedings of the 2009 Named 
Entities Workshop, 92-95. 
Praneeth Shishtla, V. Surya Ganesh, Sethuramalingam 
Subramaniam and Vasudeva Varma. 2009. A 
language-independent transliteration schema using 
character aligned models at NEWS 2009. 
Proceedings of the 2009 Named Entities Workshop, 
40-43. 
ID Substring-based Character-Based 
II 106,070,874 2,567,618 
Table 4. Number of features between substring 
and character based method in Stage-2 
79
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-lingual Information Retrieval. 
In the Proceedings of the ACL Workshop on Multi-
lingual Named Entity Recognition. 
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku 
Oonishi, Masanobu Nakamura, Sadaoki Furui. 2009. 
Combining a two-step conditional random field 
model and a joint source channel model for machine 
transliteration. Proceedings of the 2009 Named 
Entities Workshop, 72-75. 
Dong Yang, Paul Dixon and Sadaoki Furui. 2010. 
Jointly optimizing a two-step conditional random 
field model for machine transliteration and its fast 
decoding algorithm. Proceedings of the ACL 2010. 
Conference Short Papers, 275-280 
Hai Zhao and Chunyu Kit. 2008. Unsupervised 
Segmentation Helps Supervised Learning of 
Character Tagging for Word Segmentation and 
Named Entity Recognition. Proceedings of the Sixth 
SIGHAN Workshop on Chinese Language 
Processing. 
 
80
