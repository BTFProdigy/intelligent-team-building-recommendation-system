Mining metalinguistic activity in corpora to create lexical resources using 
Information Extraction techniques: the MOP system 
Carlos Rodr?guez Penagos 
Language Engineering Group, Engineering Institute 
UNAM, Ciudad Universitaria A.P. 70-472  
Coyoac?n 04510  Mexico City, M?xico 
CRodriguezP@iingen.unam.mx 
Abstract 
This paper describes and evaluates MOP, an 
IE system for automatic extraction of 
metalinguistic information from technical and 
scientific documents. We claim that such a 
system can create special databases to boot-
strap compilation and facilitate update of the 
huge and dynamically changing glossaries, 
knowledge bases and ontologies that are vital 
to modern-day research. 
1 Introduction 
Availability of large-scale corpora has made it 
possible to mine specific knowledge from free or 
semi-structured text, resulting in what many con-
sider by now a reasonably mature NLP technolo-
gy. Extensive research in Information Extraction 
(IE) techniques, especially with the series of Mes-
sage Understanding Conferences of the nineties, 
has focused on tasks such as creating and updating 
databases of corporate join ventures or terrorist 
and guerrilla attacks, while the ACQUILEX pro-
ject used similar methods for creating lexical da-
tabases using the highly structured environment of 
machine-readable dictionary entries and other re-
sources. Gathering knowledge from unstructured 
text often requires manually crafting knowledge-
engineering rules both complex and deeply de-
pendent of the domain at hand, although some 
successful experiences using learning algorithms 
have been reported (Fisher et al, 1995; Chieu et 
al., 2003). 
  Although mining specific semantic relations 
and subcategorization information from free-text 
has been successfully carried out in the past 
(Hearst, 1999; Manning, 1993), automatically ex-
tracting lexical resources (including terminologi-
cal definitions) from text in special domains  has 
been a field less explored, but recent experiences 
(Klavans et al, 2001; Rodr?guez, 2001; Cartier, 
1998) show that compiling the extensive resources 
that modern scientific and technical disciplines 
need in order to manage the explosive growth of 
their knowledge, is both feasible and practical. A 
good example of this NLP-based processing need 
is the MedLine abstract database maintained by 
the National Library of Medicine1 (NLM), which 
incorporates around 40,000 Health Sciences pa-
pers each month. Researchers depend on these 
electronic resources to keep abreast of their rapid-
ly changing field. In order to maintain and update 
vital indexing references such as the Unified Me-
dical Language System (UMLS) resources, the 
MeSH and SPECIALIST vocabularies, the NLM 
staff needs to review 400,000 highly-technical 
papers each year. Clearly, neology detection, ter-
minological information update and other tasks 
can benefit from applications that automatically 
search text for information, e.g., when a new term 
is introduced or an existing one is modified due to 
data or theory-driven concerns, or, in general, 
when new information about sublanguage usage is 
being put forward. But the usefulness of robust 
NLP applications for special-domain text goes 
beyond glossary updates. The kind of categoriza-
tion information implicit in many definitions can 
help improve anaphora resolution, semantic ty-
ping or acronym identification in these corpora, as 
well as enhance ?semantic rerendering? of spe-
cial-domain ontologies and thesaurii (Pustejovsky 
et al, 2002).  
In this paper we describe and evaluate the 
MOP2 IE system, implemented to automatically 
create Metalinguistic Information Databases 
(MIDs) from large collections of special-domain 
                                                     
1 http://www.nlm.nih.gov/ 
2 Metalinguistic Operation Processor 
research papers. Section 2 will lay out the theory, 
methodology and the empirical research groun-
ding the application, while Section 3 will describe 
the first phase of the MOP tasks: accurate location 
of good candidate metalinguistic sentences for 
further processing. We experimented both with 
manually coded rules and with learning algo-
rithms for this task. Section 4 focuses on the pro-
blem of identifying and organizing into a useful 
database structure the different linguistic consti-
tuents of the candidate predications, a phase simi-
lar to what are known in the IE literature as 
Named-Entity recognition, Element and Scenario 
template fill-up tasks. Finally, Section 5 discusses 
results and problems of our experiments, as well 
as future lines of research. 
2 Metalanguage and term evolution in scien-
tific disciplines 
2.1 Explicit Metalinguistic Operations 
Preliminary empirical work to explore how re-
searchers modify the terminological framework of 
their highly complex conceptual systems, included 
manual review of a corpus of 19 sociology articles 
(138,183 words) published in various British, 
American and Canadian academic journals with 
strict peer-review policies. We look at how term 
manipulation was done as well as how metalin-
guistic activity was signaled in text, both by lexi-
cal and paralinguistic means. Some of the 
indicators found included verbs and verbal phra-
ses like called, known as, defined as, termed, co-
ined, dubbed, and descriptors such as term and 
word. Other non-lexical markers included quota-
tion marks, apposition and text formatting. 
A collection of potential metalinguistic patterns 
identified in the exploratory Sociology corpus was 
expanded (using other verbal tenses and forms) to 
116 queries sent to the scientific and learned do-
mains of the British National Corpus. The resul-
ting 10,937 sentences were manually classified as 
metalinguistic or otherwise, with 5,407 (49.6% of 
total) found to be truly metalinguistic sentences. 
The presence of three components described be-
low (autonym, informative segment and mar-
kers/operators) was the criteria for classification. 
Reliability of human subjects for this task has not 
been reported in the literature, and was not eva-
luated in our experiments. 
Careful analysis of this extensive corpus presen-
ted some interesting facts about what we have 
termed ?Explicit Metalinguistic Operations? (or 
EMOs) in specialized discourse: 
A) EMOs usually do not follow the genus-
differentia scheme of aristotelian definitions, nor 
conform to the rigid and artificial structure of dic-
tionary entries. More often than not, specific in-
formation about language use and term definition 
is provided by sentences such as: (1) This means 
that they ingest oxygen from the air via fine 
hollow tubes, known as tracheae, in which the 
term trachea is linked to the description fine 
hollow tubes in the context of a globally non-
metalinguistic sentence. Partial and heterogeneous 
information, rather that a complete definition, are 
much more common. 
B) Introduction of metalinguistic information in 
discourse is highly regular, regardless of the spe-
cific domain. This can be credited to the fact that 
the writer needs to mark these sentences for spe-
cial processing by the reader, as they dissect 
across two different semiotic levels: a metalan-
guage and its object language, to use the termino-
logy of logic where these concepts originate.3 Its 
constitutive markedness means that most of the 
times these sentences will have at least two indi-
cators present, for example a verb and a descrip-
tor, or quotation marks, or even have preceding 
sentences that announce them in some way. These 
formal and cognitive properties of EMOs facilitate 
the task of locating them accurately in text.  
C) EMOs can be further analyzed into 3 distinct 
components, each with its own properties and lin-
guistic realizations: 
i) An autonym (see note 3): One or more self-
referential lexical items that are the logical or 
grammatical subject of a predication that needs 
not be a complete grammatical sentence.  
                                                     
3 At a very basic semiotic level natural language has 
to be split (at least methodologically) into two distinct 
systems that share the same rules and elements: a meta-
language, which is a language that is used to talk about 
another one, and an object language, which in turn can 
refer to and describe objects in the mind or in the 
physical world. The two are isomorphic and this ac-
counts for reflexivity, the property of referring to itself, 
as when linguistic items are mentioned instead of being 
used normally in an utterance. Rey-Debove (1978) and 
Carnap (1934) call this condition autonymy.   
ii) An informative segment: a contribution of 
relevant information about the meaning, status, 
coding or interpretation of a linguistic unit. In-
formative segments constitute what we state 
about the autonymical element. 
iii) Markers/Operators: Elements used to mark 
or made prominent whole discourse operation, 
on account of its non-referential, metalinguis-
tic nature. They are usually lexical, typograp-
hic or pragmatic elements that articulate 
autonyms and informative segments into a 
predication. 
Thus, in a sentence such as (2), the [autonym] is 
marked in square brackets, the {informational 
segment} in curly brackets and the <marker-
operators> in angular brackets: 
(2) {The bit sequences representing quanta of 
knowledge} <will be called ?>[Kenes]<?>, {a 
neologism intentionally similar to 'genes'}. 
2.2 Defaults, knowledge and knowledge of 
language 
The 5,400 metalinguistic sentences from our 
BNC-based test corpus (henceforth, the EMO 
corpus) reflect an important aspect of scientific 
sublanguages, and of the scientific enterprise in 
general. Whenever scientists and scholars advance 
the state of the art of a discipline, the language 
they use has to evolve and change, and this build-
up is carried out under metalinguistic control. 
Previous knowledge is transformed into new 
scientific common ground and ontological com-
mitments are introduced and defended when se-
mantic reference is established. That is why when 
we want to structure and acquire new knowledge 
we have to go through a resource-costly cognitive 
process that integrates, within coherent conceptual 
structures, a considerable amount of new and very 
complex lexical items and terms.    
It has to be pointed out that non-specialized 
language is not abundant4 in these kinds of meta-
linguistic exchanges because (unless in the con-
text of language acquisition) we usually rely on a 
lexical competence that, although subsequently 
modified and enhanced, reaches the plateau of a 
generalized lexicon relatively early in our adult 
life. Technical terms can be thought of as seman-
tic anomalies, in the sense that they are ad hoc 
                                                     
4 Our study shows that they represent between 1 and 
6% of all sentences across different domains. 
constructs strongly bounded to a model, a domain 
or a context, and are not, by definition, part of the 
far larger linguistic competence from a first native 
language. The information provided by EMOs is 
not usually inferable from previous one available 
to the speaker?s community or expert group, and 
does not depend on general language competence 
by itself, but nevertheless is judged important and 
relevant enough to warrant the additional proces-
sing effort involved. 
Conventional resources like lexicons and dic-
tionaries compile established meaning definitions. 
They can be seen as repositories of the default, 
core lexical information of words or terms used by 
a community (that is, the information available to 
an average, idealized speaker). A Metalinguistic 
Information Database (MID), on the other hand, 
compiles the real-time data provided by metalan-
guage analysis of leading-edge research papers, 
and can be conceptualized as an anti-dictionary: a 
listing of exceptions, special contexts and specific 
usage, of instances where meaning, value or 
pragmatic conditions have been spotlighted by 
discourse for cognitive reasons. The non-default 
and highly relevant information from MIDs could 
provide the material for new interpretation rules in 
reasoning applications, when inferences won?t 
succeed because the states of the lexico-
conceptual system have changed. When interpre-
ting text, regular lexical information is applied by 
default under normal conditions, but more specific 
pragmatic or discursive information can override 
it if necessary, or if context demands so (Lascari-
des & Copestake, 1995). A neologism or a word 
in an unexpected technical sense could stump a 
NLP system that assumes it will be able to use 
default information from a machine-readable dic-
tionary.  
3 Locating metalinguistic information in 
text: two approaches  
When implementingan IE application to mine 
metalinguistic information from text, the first is-
sue to tackle is how to obtain a reliable set of can-
didate sentences from free text for input into the 
next phases of extraction. From our initial corpus 
analysis we selected 44 patterns that showed the 
best reliability for being EMO indicators. We start 
our processing5  by tokenizing  text, which then is 
                                                     
5 Our implementation is Python-based, using the  
run through a cascade of finite-state devices based 
on identification patterns that extract a candidate 
set for filtering. Our filtering strategies in effect 
distinguish between useful results such as (3) 
from non-metalinguistic instances like (4): 
(3) Since the shame that was elicited by the co-
ding procedure was seldom explicitly mentio-
ned by the patient or the therapist, Lewis 
called it unacknowledged shame. 
(4) It was Lewis (1971;1976) who called attention 
to emotional elements in what until then had 
been construed as a perceptual phenomenon .  
 For this task, we experimented with two strate-
gies: First, we used corpus-based collocations to 
discard non-metalinguistic instances, for example 
the presence of attention in sentence (4) next to 
the marker called. Since immediate co-text seems 
important for this classification task, we also im-
plemented learning algorithms that were trained 
on a subset from our EMO corpus, using as vec-
tors either POS tags or word forms, at 1, 2, and 3 
positions adjacent before and after our markers. 
These approaches are representative of wider pa-
radigmatic approaches to NLP: symbolic and sta-
tistic techniques, each with their own advantages 
and limitations. Our evaluations of the MOP sys-
tem are based on test runs over 3 document sets: 
a) our original exploratory corpus of sociology 
research papers [5581 sentences, 243 EMOs]; b) 
an online histology textbook [5146 sentences, 69 
EMOs] ; and c) a small sample from the MedLine 
abstract database [1403 sentences, 10 EMOs]. 
Using collocational information, our first ap-
proach fared very well, presenting good precision 
numbers, but not so encouraging recall. The so-
ciology corpus, for example, gave 0.94 precision 
(P) and 0.68 recall (R), while the histology one 
presented 0.9 P and 0.5 R. These low recall num-
bers reflect the fact that we only selected a subset 
of the most reliable and common metalinguistic 
patterns, and our list is not exhaustive. Example 
(5) shows one kind of metalinguistic sentence 
(with a copulative structure) attested in corpora, 
                                                                                  
NLTK toolkit (nltk.sf.net) developed by E. Loper and 
S. Byrd  at the University of Pennsylvania, although we 
have replaced stochastic POS taggers with an imple-
mentation of the Brill algorithm by Hugo Liu at MIT. 
Our output files follow XML standards to ensure 
transparency, portability and accessibility 
but that the system does not attempt to extract or 
process: 
(5) ?Intercursive? power , on the other hand , is 
power in Weber's sense of constraint by an ac-
tor or group of actors over others. 
In order to better compare our two strategies, 
we decided to also zoom in on a more limited sub-
set of verb forms for extraction (namely, calls, 
called, call), which presented ratios of metalin-
guistic relevance in our MOP corpus, ranging 
from 100% positives (for the pattern so called + 
quotation marks) to 77% (called, by itself) to 31% 
(call). Restricted to these verbs, our metrics show 
precision and recall rates of around 0.97, and an 
overall F-measure of 0.97.6 Of 5581 sentences (96 
of which were metalinguistic sentences signaled 
by our cluster of verbs), 83 were extracted, with 
13 (or 15.6% of candidates) filtered-out by collo-
cations.  
For our learning experiments (an approach we 
have called contextual feature language models), 
we selected two well-known algorithms that sho-
wed promise for this classification task.7 The nai-
ve Bayes (NB) algorithm estimates the conditional 
probability of a set of features given a label, using 
the product of the probabilities of the individual 
features given that label. The Maximum Entropy 
model establishes a probability distribution that 
favors entropy, or uniformity, subject to the cons-
traints encoded in the feature-label correlation. 
When training our ME classifiers, Generalized 
(GISMax) and Improved Iterative Scaling (IIS-
Max) algorithms are used to estimate the optimal 
maximum entropy of a feature set, given a corpus.  
1,371 training sentences were converted into la-
beled vectors, for example using 3 positions and 
POS tags: ('VB WP NNP', 'calls', 'DT NN NN') 
/'YES'@[102]. The different number of positions 
considered to the left and right of the markers in 
our training corpus, as well as the nature of the 
features selected (there are many more word-types 
than POS tags) ensured that our 3-part vector in-
troduced a wide range of features against our 2 
possible YES-NO labels for processing by our 
algorithms. Although our test runs using only co-
llocations showed initially that structural regulari-
                                                     
6 With a ? factor of 1.0, and within the sociology 
document set 
7 see Ratnaparkhi (1997) and Berger et al (1996) for 
a formal description of these algorithms 
ties would perform well, both with our restricted 
lemma cluster and with our wider set of verbs and 
markers, our intuitions about improvement with 
more features (more positions to the right of left 
of the markers) or a more controlled and gramma-
tically restricted environment (a finite set of su-
rrounding POS tags), turned out to be overly 
optimistic. Nevertheless, stochastic approaches 
that used short range features did perform very 
well, in line with the hand-coded approach.  
The results of the different algorithms, re-
stricted to the lexeme call, are presented in Table 
1, while Figures 1 and 2 present best results in the 
learning experiments for the complete set of pat-
terns used in the collocation approach, over two of 
our evaluation corpora. 
Type Positions Tags/ 
Words 
Features Accuracy Precision Recall 
GISMax 1 W 1254 0.97 0.96 0.98 
IISMax 1 T 136 0.95 0.96 0.94 
IISMax 1 W 1252 0.92 0.97 0.9 
GISMax 1 T 138 0.91 0.9 0.96 
GISMax 2 T 796 0.88 0.93 0.92 
IISMax 2 T 794 0.86 0.95 0.89 
IISMax 3 W 4290 0.87 0.85 0.98 
GISMax 3 W 4292 0.87 0.85 0.98 
IISMax 2 W 3186 0.86 0.87 0.95 
GISMax 2 W 3188 0.86 0.87 0.95 
NB 1 T 136 0.88 0.97 0.84 
NB 2 T 794 0.87 0.96 0.84 
NB 3 W 4290 0.73 0.86 0.77 
Table 1. Best metrics for ?call? lexeme 
 sorted by F-measure and classifier accuracy 
Figure 1. Best metrics for Sociology corpus
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
P
R
F
NB (3/T)
IIS (1/W)
GIS (1/W)
 
Figure 2. Best metrics for Histology corpus
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
P
R
F
NB (3/W)
IIS (3/W)
GIS (1/W)
 
Figures 1 & 2. Best results for  
filtering algorithms.8  
Both Knowledge-Engineering and supervised 
learning approaches can be adequate for extrac-
tion of metalinguistic sentences, although learning 
algorithms can be helpful when procedural rules 
have not been compiled; they also allow easier 
transport of systems to new thematic domains. We 
plan further research into stochastic approaches to 
fine tune them for the task.  
One issue that merits special attention is why 
some of the algorithms and features work well 
with one corpus, but not so well with another. 
This fact is in line with observations in Nigam et 
al. (1999) that naive Bayes and Maximum Entro-
py do not show fundamental baseline superiori-
ties, but are dependent on other factors. A hybrid 
approach that combines hand-crafted collocations 
with classifiers customized to each pattern?s be-
havior and morpho-syntactic contexts in corpora 
might offer better results in future experiments. 
4 Processing EMOs to compile metalinguis-
tic information databases 
Once we have extracted candidate EMOs, the 
MOP system conforms to a general processing 
architecture shown in Figure 3. POS tagging is 
followed by shallow parsing that attempts limited 
PP-attachment. The resulting chunks are then tag-
ged semantically as Autonyms, Agents, Markers, 
Anaphoric elements or simply as Noun Chunks, 
                                                     
8 Legend: P: Precision; R: Recall; F:  F-Measure. NB: na-
?ve Bayes; IIS: Maximum Entropy trained with Improved 
Iterative Scaling; GIS: Maximum Entropy trained with Gen-
eralized Iterative Scaling. (Positions/Feature type) 
using heuristics based on syntactic, pragmatic and 
argument structure observation of the extraction 
patterns.  
Next, a predicate processing phase selects the 
most likely surface realization of informational 
segments, autonyms and makers-operators, and 
proceeds to fill the templates in our databases. 
This was done by following different processing 
routes customized for each pattern using corpus 
analysis as well as FrameNet data from Name 
conferral and Name bearing frames to establish 
relevant arguments and linguistic realizations.  
Figure 3. MOP Architecture 
As mentioned earlier, informational segments 
present many realizations that distance them from 
the clarity, completeness and conciseness of lexi-
cographic entries. In fact, they may show up as 
full-fledged clauses (6), as inter- or intra-
sentential anaphoric elements (7 and 8, the first 
one a relative clause), supply a categorization de-
scriptor (9), or even (10) restrict themselves se-
mantically to what we could call a sententially-
unrealized ?existential variable? (with logical 
form ?x) indicating only that certain discourse 
entity is being introduced. 
(6) In 1965 the term soliton was coined to descri-
be waves with this remarkable behaviour.   
(7) This leap brings cultural citizenship in line 
with what has been called the politics of citi-
zenship . 
(8) They are called ?endothermic compounds.?  
(9) One of the most enduring aspects of all social 
theories are those conceptual entities known 
as structures or groups. 
(10) A ?x so called cell-type-specific TF can be 
used by closely related cells, e.g., in erythro-
cytes and megakaryocytes. 
We have not included an anaphora-resolution 
module in our present system, so that instances 7, 
8 and 10 will only display in the output as unre-
solved surface element or as existential variable 
place-holders,9 but these issues will be explored in 
future versions of the system. Nevertheless, much 
more common occurrences as in (11) and (12) are 
enough to create MIDs quite useful for lexicogra-
phers and for NLP lexical resources.  
(11) The Jovian magnetic field exerts an influ-
ence out to near a surface, called the 
"magnetopause". 
(12) Here we report the discovery of a soluble 
decoy receptor, termed decoy receptor 3 
(DcR3)...  
The correct database entry for example 12 is 
presented in Table 4. 
Reference:  MedLine sample # 6 
Autonym:  decoy receptor 3 (DcR3) 
Information a soluble decoy receptor  
 Markers/ 
Operators:  
termed  
Table 4. Sample entry of MID 
The final processing stage presents metrics 
shown in Figure 4, using a ? factor of 1.0 to esti-
mate F-measures. To better reflect overall perfor-
mance in all template slots, we introduced a 
threshold of similarity of 65% for comparison 
between a golden standard slot entry and the one 
provided by the application. Thus, if the autonym 
or the informational segment is at least 2/3 of the 
correct response, it is counted as a positive, in 
many cases leveling the field for the expected 
errors in the prepositional phrase- or acronym- 
attachment algorithms, but accounting for a (basi-
cally) correct selection of superficial sentence 
segments. 
 
 
                                                     
9 For sentence (8) the system would retrieve a previ-
ous sentence: (?A few have positive enthalpies of for-
mation?). to define ?endothermic compounds?. 
Corpus Tokenization 
Candidate extraction 
MID 
Candidate Filtering 
Collocations  ?  Learning  
POS tagging & 
 Partial parsing 
Semantic labeling 
Database 
template fillup 
5 Results, comparisons and discussion 
The DEFINDER system (Klavans et al 2001) at 
Columbia University is, to my knowledge, the 
only one fully comparable with MOP, both in 
scope and goals, but some basic differences be-
tween them exist. First, DEFINDER examines 
user-oriented documents that are bound to contain 
fully-developed definitions for the layman, as the 
general goal of the PERSIVAL project is to pre-
sent medical information to patients in a less tech-
nical language than the one of reference literature. 
MOP focuses on leading-edge research papers that 
present the less predictable informational templa-
tes of highly technical language. Secondly, by the 
very nature of DEFINDER?s goals their qualitati-
ve evaluation criteria include readability, useful-
ness and completeness as judged by lay subjects, 
criteria which we have not adopted here. Neither 
have we determined coverage against existing on-
line dictionaries, as they have done. Taking into 
account the above-mentioned differences between 
the two systems? methods and goals, MOP com-
pares well with the 0.8 Precision and 0.75 Recall 
of DEFINDER. While the resulting MOP ?defini-
tions? generally do not present high readability or 
completeness, these informational segments are 
not meant to be read by laymen, but used by do-
main lexicographers reviewing existing glossaries 
for neological change, or, for example, in machi-
ne-readable form by applications that attempt au-
tomatic categorization for semantic rerendering of 
an expert ontology, since definitional contexts 
provide sortal information as a natural part of the 
process of precisely situating a term or concept 
against the meaning network of interrelated lexi-
cal items. The Metalinguistic Information Databa-
ses in their present form are not, in full justice, 
lexical knowledge bases comparable with the 
highly-structured and sophisticated resources that 
use inheritance and typed features, like LKB (Co-
pestake et al, 1993). MIDs are semi-structured 
resources (midway between raw corpora and 
structured lexical bases) that can be further pro-
cessed to convert them into usable data sources, 
along the lines suggested by Vossen and Copesta-
ke (1993) for the syntactic kernels of lexicograp-
hic definitions, or by Pustejovsky et al (2002) 
using corpus analytics to increase the semantic 
type coverage of the NLM UMLS ontology. An-
other interesting possibility is to use a dynami-
cally-updated MID to trace the conceptual and 
terminological evolution of a discipline. 
We believe that low recall rates in our tests are 
in part due to the fact that we are dealing with the 
wider realm of metalinguistic information, as op-
posed to structured definitional sentences that 
have been distilled by an expert for consumer-
oriented documents. We have opted in favor of 
exploiting less standardized, non-default metalin-
guistic information that is being put forward in 
text because it can?t be assumed to be part of the 
collective expert-domain competence (Section 
2.1). In doing so, we have exposed our system to 
the less predictable and highly charged lexical 
environment of leading-edge research literature, 
the cauldron where knowledge and terminological 
systems are forged in real time, and where scienti-
Figure 4. Metrics for 3 corpora 
(# of Records/Global F-Measure)
0.6
0.7
0.8
0.9
1
Precision Recall Precision Recall Precision Recall
Global Informational Segments Autonyms
Histology (35/0.71) Sociology (143/0.77) MedLine (10/0.78)
fic meaning and interpretation are constantly de-
bated, modified and agreed. We have not per-
formed major customization of the system (like 
enriching the tagging lexicon with medical terms), 
in order to preserve the ability to use the system 
across different domains. Domain customization 
may improve metrics, but at a cost for portability. 
The implementation we have described here 
undoubtedly shows room for improvement in so-
me areas, including: adding other patterns for bet-
ter overall recall rates, deeper parsing for more 
accurate semantic typing of sentence arguments, 
etc. Also, the issue of which learning algorithms 
can better perform the initial filtering of EMO 
candidates is still very much an open question. 
Applications that can turn MIDs into truly useful 
lexical resources by further processing them need 
to be written. We plan to continue development of 
our proof-of-concept system to explore those ar-
eas. DEFINDER and MOP both show great poten-
tial as robust lexical acquisition systems capable 
of handling the vast electronic resources available 
today to researchers and laymen alike, helping to 
make them more accessible and useful. In doing 
so, they are also fulfilling the promise of NLP 
techniques as mature and practical technologies.   
References 
ACQUILEX projects, final report available at: 
http://www.cl.cam.ac.uk/Research/NL/acquilex/ 
Berger, A., S. Della Pietra  et al, 1996. A Maxi-
mum Entropy Approach to Natural Language 
Processing. Computational Linguistics, vol. 22, 
no. 1. 
Carnap, R. 1934. The Logical Syntax of Lan-
guage. Routledge and Kegan, Londres 1964. 
Cartier, E. 1998. Analyse Automatique des textes: 
l?example des informations d?finitoires. RIFRA 
1998. Sfax, Tunisia. 
Chieu, Hai Leong, Ng, Hwee Tou, & Lee, Yoong 
Keok. 2003. Closing the Gap: Learning-Based 
Information Extraction Rivaling Knowledge-
Engineering Methods. 41st ACL. Sapporo, Ja-
pan. 
Copestake, A., Sanfilippo, A., Briscoe, T. and de 
Pavia, V. 1993. The ACQUILEX LKB: An in-
troduction. In: Inheritance, Defaults and the 
Lexicon. Cambridge University Press. 
Fisher, D., S. Soderland, J. McCarthy, F. Feng, 
and W. Lehnert. 1995. Description of the 
UMass system as used for MUC-6. In Proceed-
ings of MUC-6 
Hearst, M. 1998. Automated discovery of wordnet 
relations. In Christiane Fellbaum, editor, 
WordNet: An Electronic Lexical Database. MIT 
Press, Cambridge, MA 
Klavans, J. and S. Muresan. 2001. Evaluation of 
the DEFINDER System for Fully Automatic 
Glossary Construction, proceedings of the 
American Medical Informatics Association 
Symposium 2001 
Lascarides, A. and Copestake A. 1995. The Prag-
matics of Word Meaning, Proceedings of the 
AAAI Spring Symposium Series: Representa-
tion and Acquisition of Lexical Knowledge: 
Polysemy, Ambiguity and Generativity, Stan-
ford CA. 
Manning, Ch. 1993. Automatic acquisition of a 
large subcategorization dictionary from cor-
pora, In Proceedings of the 31st ACL, Colum-
bus, OH. 
Nigam, K., Lafferty, J., and McCallum, A.  1999. 
Using Maximum Entropy for Text Classifica-
tion, IJCAI-99 Workshop on Machine Learning 
for Information Filtering, pp. 61-67 
Pustejovsky J., A. Rumshisky and J. Casta?o. 
2002. Rerendering Semantic Ontologies: Auto-
matic Extensions to UMLS through Corpus 
Analytics. LREC 2002 Workshop on Ontologies 
and Lexical Knowledge Bases. Las Palmas, Ca-
nary Islands, Spain. 
Ratnaparkhi A. 1997.  A Simple Introduction to 
Maximum Entropy Models for Natural Lan-
guage Processing, TR 97-08, Institute for Re-
search in Cognitive Science, University of 
Pennsylvania 
Rey-Debove, J. 1978. Le M?talangage. Le Robert, 
Paris. 
Rodr?guez, C. 2001. Parsing Metalinguistic 
Knowledge from Texts,  Selected papers from 
CICLING-2000 Collection in Computer Science 
(CCC); National Polytechnic Institute (IPN), 
Mexico. 
Vossen, P. and Copestake, A. 1993. Untangling 
Definition Structure into Knowledge Represen-
tation. In: Inheritance, Defaults and the Lexi-
con. 
Metalinguistic Information Extraction for Terminology 
Carlos Rodr?guez Penagos 
Language Engineering Group, Engineering Institute 
UNAM, Ciudad Universitaria A.P. 70-472  
Coyoac?n 04510  Mexico City, M?xico 
CRodriguezP@iingen.unam.mx 
 
Abstract 
This paper describes and evaluates the 
Metalinguistic Operation Processor (MOP) 
system for automatic compilation of 
metalinguistic information from technical and 
scientific documents. This system is designed 
to extract non-standard terminological 
resources that we have called Metalinguistic 
Information Databases (or MIDs), in order to 
help update changing glossaries, knowledge 
bases and ontologies, as well as to reflect the 
metastable dynamics of special-domain 
knowledge.  
1 Introduction 
Mining terminological information from free or 
semi-structured text in large-scale technical 
corpora is slowly becoming a reasonably mature 
NLP technology, with term extraction systems 
leading the way. Automatically obtaining 
information about terms from free text has been a 
field less explored, but recent experiences have 
shown that compiling the extensive resources that 
modern scientific and technical disciplines need to 
manage the explosive growth of their knowledge 
is both feasible and practical. A good example of 
this NLP-based processing need is the National 
Library of Medicine?s MedLine abstract database, 
which incorporates around 40,000 new Life 
Sciences papers each month. In order to maintain 
and update UMLS knowledge resources1 the 
NLM staff needs to manually review 400,000 
highly-technical papers each year (Powell et al 
2002). Most of these terminological knowledge 
sources have been compiled from existing 
glossaries and vocabularies that might become 
                                                     
1 The MeSH and SPECIALIST vocabularies, a 
Metathesaurus, a Semantic Network, etc. 
dated fairly quickly, and elucidating this 
information from domain experts is not an option. 
Neology detection, terminological information 
update and other tasks can benefit from automatic 
search, in highly technical text, of semantic and 
pragmatic information, e.g. when new information 
about sublanguage usage is being put forward. In 
this paper we describe and evaluate the 
Metalinguistic Operation Processor (MOP) 
system, implemented to automatically create 
Metalinguistic Information Databases (or MIDs) 
from large collections of special-domain research 
and reference documents. Section 2 discusses 
previous work, while Section 3 provides an 
overview of metalinguistic exchanges between 
experts, and their role in the constitution of 
technical knowledge. Section 4 presents 
experiments to localize and disambiguate good 
candidate metalinguistic sentences, using rule-
based and stochastic learning strategies. Section 5 
focuses on the problem of identifying and 
structuring the different linguistic constituents and 
surface segments of metalinguistic predications. 
Finally, Section 6 offers a discussion of results 
and suggestions for possible applications and 
future lines of research. 
2 Previous work 
One of the constraints of recent lines of research 
(Pearson, 1998; Klavans et al, 2001; Pascual & 
Pery-Woodley, 1997) is their focus on definitions, 
a theoretical object that, although undoubtedly 
useful and extensively described, presents by its 
very nature certain limitations when studying 
expert-domain peer-to-peer communication.2 The 
meaning normalization process inherent in 
                                                     
2 In some recent approaches, Meyer (2001) and 
Condamines & Rebeyrolles (2001) exploit wider 
lexico-conceptual relations in free-text that can be 
difficult to model and locate accurately. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 15
compiling definitions may be desirable when 
creating human-readable reference sources, but 
might lead to a loss of valuable information for 
specific contexts where the term appears. 
Pragmatic information (valid usage conditions or 
contextual restriction for the terms), or purely 
evaluative statements (usefulness or validity of a 
certain term for its intended purpose), might not 
be found in classical definitional contexts. 
Metalinguistic information in texts can provide us 
with information not only about what terms mean, 
but also how they are actually used by domain 
experts. A wide spectrum of sentential realizations 
of these kinds of information has been reported by 
Meyer (2001) and Rodr?guez (2001), and 
organizing it to provide useful terminological 
resources is left for manual review by human 
lexicographers. We believe that using the more 
general concept of metalanguage can automate as 
much as possible the extraction of fine-grained 
knowledge about terms, as well as better capture 
the dynamical nature of the evolution of the 
scientific and technical knowledge created 
through the interaction of expert-domain groups. 
3 Metalanguage, terminology and scientific 
knowledge 
3.1 Corpora used in our research 
Preliminary empirical work to explore how 
researchers modify the terminological framework 
of their highly complex conceptual systems 
included an initial manual review of 19 sociology 
articles (138k words) in academic journals. We 
looked at how term introduction and modification 
was done, as well as how metalinguistic activity 
was signalled in text, both by lexical and 
paralinguistic means. Some of the indicators 
found included verbs and verbal phrases like 
called, known as, defined as, termed, coined, 
dubbed, and descriptors such as term and word. 
Non-lexical markers included quotation marks, 
apposition and text layout.3 The metalinguistic 
patterns thus identified were expanded (using 
variations of lexemes, verbal tenses and forms) 
into 116 queries to the scientific and learned 
domains of the British National Corpus. The 
resulting 10,937 sentences (henceforth, the MOP 
                                                     
3 Similar work by Pearson (1998) obtained many of 
the same patterns from the Nature corpus of exact 
science documents. 
corpus) were manually classified as metalinguistic 
or otherwise, with 5,407 (49.6% of total) found to 
be truly metalinguistic sentences, using the 
criteria described in Section 3.2 below.4 Other 
corpora from different domains (described in 
Section 4) was used both in this preliminary 
analysis of metalinguistic exchanges, as well as in 
evaluation and development of the MOP system. 
3.2 Explicit Metalinguistic Operations 
Careful analysis of these corpora, as well of 
examples in other European languages, presented 
some interesting facts about what we have termed 
?Explicit Metalinguistic Operations? (or EMOs):5 
A) EMOs do not usually follow the genus-
differentia scheme of aristotelian definitions, nor 
conform to the rigid and artificial structure of 
lexicographic entries. More often than not, 
specific information about language use and term 
definition is provided by sentences such as (1), in 
which the term trachea is linked to the description 
fine hollow tubes in the context of a globally non-
metalinguistic sentence:  
(1) This means that they ingest oxygen from the 
air via fine hollow tubes, known as tracheae.  
In research papers partial and heterogeneous 
information is much more common than complete 
definitions, although it might otherwise in 
textbooks geared towards learning a discipline. 
B) Introduction of metalinguistic information in 
discourse is highly regular, regardless of the 
domain. This can be credited to the fact that the 
writer needs to mark these sentences for special 
processing by the reader, as they dissect across 
two different semiotic levels: a meta-language and 
its object language, to use the terminology of 
logic where these concepts originated.6 Their 
                                                     
4 Reliability of human subjects for this task has not 
been reported in the literature, and was not evaluated in 
our experiments. 
5 We have used the term to highlight the operational 
nature of such textual instances in technical discourse. 
6 Natural language has to be split (at least 
methodologically) into two distinct systems that share 
the same rules and elements: a metalanguage used to 
refer to an object language, which in turn can refer to 
and describe objects in the mind or in the physical 
world. The fact that the two are isomorphic accounts 
for reflexivity, the property of referring to itself, as 
when linguistic items are mentioned instead of being 
used normally in an utterance. Rey-Debove (1978) 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology16
constitutive markedness means that most of the 
times these sentences will have at least two 
indicators of metalinguistic nature. These formal 
and cognitive properties of EMOs facilitate the 
task of locating them accurately in text.  
C) EMOs can be further analyzed into 3 distinct 
components, each with its own properties and 
linguistic realizations: 
i) An autonym (see note 6): One or more self-
referential lexical items that are the logical or 
grammatical subject of a predication.  
ii) An informational segment: a contribution 
of relevant information about the meaning, 
status, coding or interpretation of a linguistic 
unit. Informational segments constitute what 
we state about the autonymical element. 
iii) Markers/Operators: Elements used to 
make prominent the whole discourse operation 
and its non-referential, metalinguistic nature. 
They are usually lexical, paralinguistic or 
pragmatic devices that articulate autonyms and 
informational segments into a predication. 
In a sentence such as (2) we have marked the 
autonym with italics, the informational segment 
with bold type and the marker-operator items with 
square brackets: 
(2) The bit sequences representing quanta of 
knowledge [ will be called ? ] Kenes [ ? ],  a 
neologism intentionally similar to 'genes' . 
3.3 Knowledge and knowledge of language  
Whenever scientists advance the state of the art 
of a discipline, their language has to evolve and 
change, and this build-up is carried out under 
metalinguistic control. Previous knowledge is 
transformed into new scientific common ground 
and ontological commitments are introduced 
when semantic reference is established. That is 
why when we want to structure and acquire new 
knowledge we have to go through a resource-
costly cognitive process that integrates within 
coherent conceptual structures and theories a 
considerable amount of new and very complex 
lexical items and terms. Technical terms are not, 
by definition, part of the far larger linguistic 
competence of a first native language. Unlike 
everyday words within a specific social group, 
                                                                                  
follows Carnap in calling this condition autonymy.   
terms are conventional, even if they have derived 
from a word that originally belonged to collective 
competence. We could even posit that all 
technical terms owe their existence to a baptismal 
speech act, and that given a big enough sample 
(an impossibly exhaustive corpus of all expert 
language exchanges), an initial metalinguistic 
sentence could be located that constitutes an 
original, foundational source of meaning.  
The information provided by metalinguistic 
exchanges is not usually inferable from previous 
one available to the speaker?s community, and 
does not depend on general language competence 
by itself, but nevertheless is judged important and 
relevant enough to warrant the additional 
processing effort involved. Computing what is 
relevant metalinguistic information has to be done 
dynamically by figuring out which terminological 
items can be assumed to be shared by all, and 
which are new or have to be modified. It?s an 
extended and more complex instance of lexical 
alignment between interlocutors (Pickering & 
Garrod, in press). Observing closely how this 
alignment is achieved can allow us to create 
computer applications that mimic some aspects of 
our impressive human competence as efficient 
readers of technical subjects, as incredibly good 
lexical-data processors that constantly update and 
construct our own special purpose vocabularies.  
4 Filtering out non-metalinguistic sentences: 
two NLP approaches 
The first issue to tackle when mining 
metalanguage is how to obtain a reliable set of 
candidate sentences for input into the next 
extraction phases. We employ a ?discourse-
oriented? approach that differs from Meyer?s 
(2001) ?term-oriented? one. We do not assume we 
have initially identified a terminological unit and 
proceed from there, but rather we first locate a 
metalinguistic discourse operation where a term 
can be retrieved along with information that refers 
to it. Condamines & Rebeyrolles (2001) and 
Meyer (2001) both exploit patterns of 
?knowledge-rich contexts? to obtain semantic and 
conceptual information about terms, either to 
inform terminological definitions or provide 
structure for a terminological system. A key 
problem in such approaches that use lexical-based 
?triggers? is how to control the amount of ?noise?, 
or non-relevant instances. The experiments in this 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 17
section compare two different NLP techniques for 
this task: symbolic and statistic techniques. 
From our initial analysis of various corpora we 
selected 44 patterns that showed the best 
statistical reliability as EMO indicators.7 We 
started out by tokenizing text, which then was run 
through a cascade of finite-state devices that 
extracted a set of candidate sentences before 
filtering out non-metalinguistic instances. Our 
filtering distinguishes between useful results, e.g. 
using the lexical pattern called in (3) from non-
metalinguistic instances in (4): 
(3) Since the shame that was elicited by the 
coding procedure was seldom explicitly 
mentioned by the patient or the therapist, Lewis 
called it unacknowledged shame. 
(4) It was Lewis (1971;1976) who called attention 
to emotional elements in what until then had 
been construed as a perceptual phenomenon .  
 We experimented with two strategies for 
disambiguation: first, we used collocations as 
added restrictions (e.g., verbal vs. nominal 
occurrences of our lexical markers) to discard 
non-metalinguistic instances, for example  
attention in sentence (4) next to the marker called. 
The next table shows a sample of the filtering 
collocations.  
Preceding Subsequent 
for calls 
in, duty, personal, conference, 
local, next, the, their, house, 
anonymous, phone, telephone... 
out, someone, charges, before, 
charge, back, contact, for, 
upon, to, into, off, 911, by... 
for coin 
pound, small, pence, in, toss, 
the, this, a, that, one, gold, 
silver, metal, esophageal ... 
toss 
We also implemented learning algorithms 
trained on a subset from our EMO corpus, using 
as vectors either Part-of Speech tags or word 
strings, at one, two, and three positions adjacent 
before and after our lexical markers. Our 
evaluations are based on 3 document sets: a) our 
original exploratory sociology corpus [5,581 
sentences, 243 EMOs]; b) an online histology 
textbook [5,146 sentences, 69 EMOs]; and c) a 
small sample from the MedLine abstract database 
[1,403 sentences, 10 EMOs]. Our system is coded 
                                                     
7 We excluded dispositional and typographical clues 
from our selectional patterns, involving mainly lexica 
and punctuation. 
in Python, using the NLTK platform (nltk.sf.net) 
and a Brill tagger by Hugo Liu at MIT. 
4.1 The collocation-based approach 
Our first approach fared well, with good 
precision numbers but not so encouraging recall. 
The sociology corpus gave 0.94 Precision (P) and 
0.68 Recall (R), while the histology one presented 
0.9 P and 0.5 R. These low recall numbers reflect 
the fact that we used a non-exhaustive list of 
metalinguistic patterns. Example (5) shows one 
kind of metalinguistic sentence attested in corpora 
that the system does not extract or process: 
(5) ?Intercursive? power, on the other hand, is 
power in Weber's sense of constraint by an actor 
or group of actors over others. 
We also tested extraction against a golden 
standard where sentences that had patterns that 
our list was not designed to retrieve were 
removed, which gave a more realistic picture of 
how the extraction system worked for the actual 
dataset it was designed to consider. For the 
sociology corpus (and a ? factor of 1), P was 0.97 
and R 0.79, with an F-measure of 0.87. In the 
histology one P was measured at 0.94, R at 0.81 
and F-measure at 0.87. In order to better compare 
the two filtering strategies, we decided also to 
zoom in on a more limited subset of verb forms 
(namely, calls, called, call), which presented 
ratios of metalinguistic relevance in our MOP 
corpus ranging from 100% positives (for the 
pattern so called + quotation marks) to 31% (call). 
Restricted to these verbs, our metrics showed 
precision and recall rates around 0.97. One 
problem with this approach is that the hand-coded 
rules are domain-specific, and customization for 
other domains is labour-intensive. In our tests, 
although most of the collocations work language-
wide (phrasal verbs or prepositions), some of 
them are very specific.8 Although collocation-
based filtering will result in a working system, 
customization is error-prone and laborious.  
4.2 Testing learning algorithms 
We selected the co-text of marker/operators as 
relevant features for classifiers based on well-
known naive Bayes and Maximum Entropy 
algorithms that have been reported to work well 
                                                     
8 ?esophageal coins? is quite unusual outside of 
medical documents. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology18
with sparse data.9  We used either as grammatical 
context the POS tags or the word forms 
immediately adjacent in one to three positions 
before and after our triggering markers. Testing 
all possible combinations evaluates empirically 
the ideal mix of algorithm, feature type and 
coverage that insures best accuracy. The naive 
Bayes algorithm estimates the conditional 
probability of a set of features given a label, using 
the product of the probabilities of the individual 
features given that label. It assumes that the 
feature distributions are independent, but it has 
been shown to work well in cases with high 
degree of feature dependencies. The Maximum 
Entropy model establishes a probability 
distribution favouring entropy or uniformity 
subject to the constraints encoded in the feature- 
known label correlation. To train our classifiers, 
Generalized and Improved Iterative Scaling 
algorithms were used to estimate the optimal 
maximum entropy of a feature set, given a 
corpus.10 1,371 training sentences from our MOP 
dataset were converted into YES-NO labelled 
vectors. The following example from the textual 
segment ?... creates what Croft calls a description 
constraint ...?, uses 3 positions and POS tags:  
('VB WP NNP', 'calls', 'DT NN NN')/'YES'@[102]. 
The different number of positions to the left and 
right of our training sentences, as well as the 
nature of the features selected (there are many 
more word-types than POS tags) ensured that our 
3-part vector introduced a wide range of features 
against our 2 possible labels. The best results of 
each algorithms restricted to the lexeme call, are 
presented in the next table. Figures 1 and 2 
present best results in the learning experiments for 
the complete set of patterns used in the collocation 
approach, over two of our evaluation corpora.11 
Type Positions Tags/Words Features Accuracy Precision Recall 
GIS 1 W 1254 0.97 0.96 0.98 
IIS 1 T 136 0.95 0.96 0.94 
NB 1 T 136 0.88 0.97 0.84 
                                                     
9 see Rish, 2001, Ratnaparkhi, 1997 and Berger et al 
1996 for a formal description of these algorithms. 
10 In other words, given known data statistics, 
construct a model that best represents them but is 
otherwise as uniform as possible. 
11 Legend: P: Precision; R: Recall; F:  F-Measure. NB: na?ve 
Bayes; IIS: Maximum Entropy with Improved Iterative Scaling; GIS: 
Maximum Entropy with Generalized Iterative Scaling. 
(Positions/Feature type) 
Figure 1. Best metrics for Sociology corpus
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
P
R
F
NB (3/T)
IIS (1/W)
GIS (1/W)
 
Figure 2. Best metrics for Histology corpus
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
P
R
F
NB (3/W)
IIS (3/W)
GIS (1/W)
 
Although our tests using collocations showed 
that structural regularities would perform well, 
our intuitions about improvement using more 
features (more positions to the right or left of the 
lexical markers) or a more grammatically 
restricted environment (surrounding POS tags), 
turned out to be overly optimistic. Nevertheless, 
stochastic approaches that used short-range 
features did perform in line with the hand-coded 
approach. Both Knowledge-Engineering and 
supervised learning approaches were adequate for 
initial filtering of metalinguistic sentences, 
although learning algorithms might allow easier 
transport of systems into new domains. 
5 From EMOs to metalinguistic databases 
After EMOs were obtained, POS tagging, 
shallow parsing and limited PP-attachment are 
performed. Resulting chunks were tagged as 
Autonyms, Agents, Markers, Anaphoric elements 
or Noun Chunks, using heuristics based on 
syntactic, pragmatic and argument structure of 
lexica in the extraction patterns, as well as on 
FrameNet data in Name conferral and Name 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 19
bearing frames. Next, a predicate processing 
phase selected the most likely surface realization 
for informational segments, autonyms and 
makers-operators, and proceeded to fill out the 
templates of the database. As mentioned earlier, 
informational segments present many realizations 
far from the completeness and conciseness of 
lexicographic entries. In fact, they may show up 
as full-fledged clauses (6), as inter- or intra-
sentential anaphoric elements (7 and 8), as sortal 
information (9), or as an unexpressed ?existential 
variable? (logical form $x) indicating only that 
certain discourse entity is being introduced (10): 
(6) In 1965 the term soliton was coined to 
describe waves with this remarkable 
behaviour.   
(7) This leap brings cultural citizenship in line 
with what has been called the politics of 
citizenship . 
(8) They are called ?endothermic compounds.?  
(9) One of the most enduring aspects of all social 
theories are those conceptual entities known as 
structures or groups. 
(10) A [$x] so called cell-type-specific TF can be 
used by closely related cells?. 
We have not included an anaphora-resolution 
module in our system, so that examples 7, 8 and 
10 only output either unresolved surface elements 
or variable placeholders.12 Nevertheless, more 
common occurrences like example sentence (1) 
                                                     
12 For sentence (8) the system might retrieve useful 
information from a previous one: ?A few have positive 
enthalpies of formation.? 
are enough to create MIDs that constitute useful 
resources for lexicographers. The correct database 
entry for (1) is presented below. 
Reference Histology sample # 6 
Autonym tracheae 
Information fine hollow tubes 
Markers/Operators  known as 
To better reflect overall performance, we 
introduced a threshold of similarity of 65% for 
comparison between a golden standard slot entry 
and the one obtained by the application.13 The 
final processing stage presented metrics shown in 
Figure 4. Our best numbers for informational 
segments ranged around 0.85, while the lowest 
were obtained for the histology corpus, with 
global precision and recall rates around 0.71, but 
with high numbers in the autonym identification 
task (0.91) and midrange ones for the 
informational segments (0.8). We observed that 
even though it is assumed that Bio-Medical 
Sciences have more consolidated vocabularies 
than Social Sciences, results for the MedLine and 
histology corpus occupy the extremes in the 
spectrum, with the sociology one in the middle 
range. The total number of candidate sentences 
was not a good predictor of system performance.  
The DEFINDER system (Klavans et al, 2001) 
is to my knowledge the only one fully comparable 
with MOP, both in scope and goals, but with some 
significant differences.14 Taking into account 
                                                     
13 Thus, if the autonym or the informational segment 
is at least 2/3 of the correct response, it is counted as a 
positive, allowing for expected errors in the PP or 
acronym attachment algorithms. 
14 DEFINDER examines user-oriented documents 
Figure 4. Metrics for 3 corpora 
(# of Records/Global F-Measure)
0.6
0.7
0.8
0.9
1
Precision Recall Precision Recall Precision Recall
Global Informational Segments Autonyms
Histology (35/0.71) Sociology (143/0.77) MedLine (10/0.78)
CompuTerm 2004  -  3rd International Workshop on Computational Terminology20
those differences, MOP compares well with the 
0.8 precision and 0.75 recall of DEFINDER. 
While the resulting MOP ?definitions? generally 
do not present high readability or completeness, 
these informational segments are not meant to be 
read by laymen, but used by domain 
lexicographers updating existing glossaries for 
neological change, or, in machine-readable form, 
by other applications. 
6 Discussion and future work 
We have chosen to exploit metalinguistic 
information that is being put forward in text 
because it can?t be assumed to be part of the 
collective expert-domain competence. In doing so, 
we have exposed our system to the less 
predictable lexical environment of leading-edge 
research literature, the cauldron where knowledge 
and terminological systems are forged in real 
time, and where scientific meaning and 
interpretation are constantly debated, modified 
and agreed upon. We believe that low recall rates 
in our tests are in part due to the fact that we are 
dealing with the wider realm of metalinguistic 
information, as opposed to structured definitional 
sentences that have been distilled by an expert for 
consumer-oriented documents. We have not 
performed major customization of the system (like 
enriching the tagging lexicon with medical terms), 
in order to preserve the ability to use the system 
across different domains. Domain customization 
may improve metrics, but at a real cost for 
portability. 
Conventional resources like lexicons and 
dictionaries compile meaning definitions that are 
considered stable and widely-shared. They can be 
seen as repositories of the default, core lexical 
information for terms used by a research 
community (that is, the information available to 
an average, idealized speaker). An MID, on the 
other hand, might contain the multi-textured real-
time data embedded in research papers, and in this 
sense could be conceptualized as an anti-
                                                                                  
with fully-developed definitions for the layman. MOP 
focuses on leading-edge research papers that present 
less predictable templates. DEFINDER?s qualitative 
evaluation criteria includes readability, usefulness and 
completeness, as judged by lay subjects, criteria which 
we have not adopted here, nor have we determined 
coverage against existing on-line dictionaries. 
dictionary: a listing of exceptions, special 
contexts and specific usage of instances where 
meaning, value or pragmatic conditions have been 
spotlighted by discourse for cognitive reasons. 
Applications that rely on lookup on previously 
compiled resources would miss some of the data 
from EMOs, where the term is put forward for the 
first time, or where important, context-sensitive 
information about the terms is provided. MIDs 
cannot be viewed as end-user products, but as 
semi-structured resources (midway between raw 
corpora and structured lexical bases) that have to 
be further processed to convert them into usable 
data sources. We might better characterize them 
as auxiliary lexical knowledge resources, more 
than core lexical references. Lexicographers and 
terminologist can use them as tools for their own 
labour-intensive work of reviewing and compiling 
special-domain vocabularies. 
MIDs could, in principle, also supply new 
interpretation rules in AI applications when 
inferences won?t succeed because the state of the 
lexico-conceptual system has changed.15 A neo-
logism or a word in an unexpected technical sense 
could stump a NLP system that assumes it will be 
able to use the default information from a 
machine-readable dictionary or TKB. The kind of 
sortal information implicit in many definitions can 
also help improve anaphora resolution, semantic 
typing, acronym identification or bootstrapping of 
ontologies and taxonomies (Hearst, 1992; 
Condamines & Rebeyrolle, 2001; Pustejovsky et 
al., 2002; Malais? et al 2004). Although our 
approach might miss some of the important 
conceptual relations between terms, many of the 
MIDs we have obtained using language-centred 
contexts are rich sources of information. In 
addition, terminological information can be more 
specific than that obtainable by glossary lookup, 
and might be better suited for the interpretation of 
certain texts. The locality of such information can 
be seen as advantageous for specialized 
lexicography. Another area where non-standard 
information could prove useful is the study of the 
evolution of scientific sublanguages and the 
knowledge embodied by them. Changes in the 
                                                     
15 When interpreting text, regular lexical information 
is applied by default under normal conditions, but more 
specific pragmatic or discursive information can 
override it if necessary, or if context demands so 
(Lascarides & Copestake, 1995). 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology 21
conceptual and terminological configuration of a 
discipline might be traced and be better 
understood by the dynamical updates reflected in 
these databases. The next table shows a small 
sample, taken from our corpora, of that 
information?s potential range, for some key 
concepts in the sociology domain.  
Terms Informational segments from EMOs 
Family extend the meaning to include same-sex couples, 
single-parents, nannies, adoptive and step children, 
and so on 
Family two adults of opposite sex, married to each other, 
and living with their common children 
Identity There are two typical contexts 
Identity an emotional attachment and a sense of belonging 
of a semi-sacred kind 
Nationalism used here, deliberately, to describe both aspects of 
the phenomenon 
Nationalism is used for both of these things - world view and 
activism 
6.1 Conclusions  
The implementation we have described here 
undoubtedly shows room for improvement: 
adding more patterns for better overall recall rates, 
deeper parsing for more accurate semantic typing 
of sentence arguments, etc. Also, the question of 
which learning algorithms can better perform the 
initial filtering of EMO candidates is still very 
much an open issue. We believe that the real 
challenge facing work such as this one lies not in 
retrieving EMOs from text to populate a MID, but 
in the successful formalization of heterogeneous 
linguistic information into a robust and 
manageable data structure. An effective and 
efficient computational representation of such 
diverse information is not trivial.  
Nevertheless, we believe that applications 
focused on metalanguage, like the MOP system 
described here, can be very helpful for 
Terminology and lexicography, and that a MID?s 
role would not be to replace, but to enrich and 
complement, Terminological Knowledge Bases. 
References  
Berger, A., S. Della Pietra et al 1996. A Max-
imum Entropy Approach to Natural Language 
Processing. Computational Linguistics, v. 22, 1. 
Condamines, A. & Rebeyrolle, J. 2001. Searching 
for and identifying conceptual relationships via 
a corpus-based approach to a Terminological 
Knowledge Base (CTKB). Recent Advances in 
Computational Terminology. John Benjamins. 
Hearst, M. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora. Proc. 14th 
Int. Conference on Computational Linguistics, 
Nantes, France. 
Klavans, J. and Muresan, S. 2001. Evaluation of 
the DEFINDER System for Fully Automatic 
Glossary Construction, Proc. Am. Med. Inf. 
Ass. Symp. 
Lascarides, A. and Copestake A. 1995. The 
Pragmatics of Word Meaning, in Representation 
and Acquisition of Lexical Knowledge: Poly-
semy, Ambiguity and Generativity. Stanford. 
Malais? V., Zweigenbaum P. & Bachimont, B. 
2004. Rep?rage et exploitation d??nonc?s 
d?finitoires en corpus pour l?aide ? la 
construction d?ontologie. TALN 2004, F?s. 
Meyer, I. 2001. Extracting knowledge-rich cont-
exts for terminography. Recent Advances in 
Computational Terminology. 
Pascual, E. & P?ry-Woodley, M-P 1997. Mod?les 
de texte pour la d?finition. Actes de Journ?es 
Scientifiques et Techniques du R?seau 
Francophone de l?Ing?nierie de la Langue,  
Pearson, J. 1998. Terms in Context. John 
Benjamins 
Pickering, M. and Garrod, S. (in press) Toward a 
mechanistic psychology of dialogue. Behavioral 
and Brain Sciences. Cambridge University 
Press. 
Powell, T., Srinivasan, S., et al (2002) Tracking 
Meaning Over Time in the UMLS 
Metathesaurus. In Biomedical Informatics: One 
Discipline. Proc. Am. Med. Inf. Ass. Symp.  
Pustejovsky J., A. Rumshisky and J. Casta?o. 
2002. Rerendering Semantic Ontologies: Auto-
matic Extensions to UMLS through Corpus 
Analytics. LREC 2002, Spain. 
Ratnaparkhi A., 1997.  A Simple Introduction to 
Maximum Entropy Models for Natural 
Language Processing, TR 97-08, IRCS, 
University of Pennsylvania 
Rish, I., 2001. An empirical study of the naive 
Bayes classifier, in IJCAI-01. 
Rey-Debove, J. 1978. Le M?talangage. Le Robert, 
Paris. 
Rodr?guez, C. 2001. Parsing Metalinguistic 
Knowledge from Texts. Selected papers from 
CICLING-2000 (IPN), Mexico. 
CompuTerm 2004  -  3rd International Workshop on Computational Terminology22
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBM: Combining lexicon-based ML and heuristics
for Social Media Polarities
Carlos Rodr??guez-Penagos, Jordi Atserias, Joan Codina-Filba`,
David Garc??a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur??
Barcelona Media
Av. Diagonal 177, Barcelona 08018
Corresponding author: carlos.rodriguez@barcelonamedia.org
Abstract
This paper describes the system implemented
by Fundacio? Barcelona Media (FBM) for clas-
sifying the polarity of opinion expressions in
tweets and SMSs, and which is supported by
a UIMA pipeline for rich linguistic and sen-
timent annotations. FBM participated in the
SEMEVAL 2013 Task 2 on polarity classifi-
cation. It ranked 5th in Task A (constrained
track) using an ensemble system combining
ML algorithms with dictionary-based heuris-
tics, and 7th (Task B, constrained) using an
SVM classifier with features derived from the
linguistic annotations and some heuristics.
1 Introduction
We introduce the FBM system for classifying the
polarity of short user-generated text (tweets and
SMSs), which participated in the two subtasks of
SEMEVAL 2013 Task 2 on Sentiment Analysis in
Twitter. These are: Task A. Contextual Polarity Dis-
ambiguation, and Task B. Message Polarity Classifi-
cation. The former aimed at classifying the polarity
of already identified opinion expressions (or cues),
whereas the latter consisted in classifying the polar-
ity of the whole text (Wilson et al, 2013).
The literature agrees on two main approaches for
classifying opinion expressions: using supervised
learning methods and applying dictionary/rule-
based knowledge (see (Liu, 2012) for an overview).
Each of them on its own has been used in work-
able systems, and a principled combination of both
of them can yield good results on noisy data, since
generally one (dictionaries/rules) offers good preci-
sion while the other (ML) is able to discover unseen
examples and thus enhances recall.
FBM combined both approaches in order to bene-
fit from their respective strengths and compensating
as much as possible their weaknesses. For Task A
we used linguistic (lexical and syntactic) annotations
to implement both types of approaches. On the one
hand, we built machine learning classifiers based on
Support Vector Machines (SVMs) and Conditional
Random Fields (CRFs). On the other, we imple-
mented a basic classification system mainly based
on polarity dictionaries and negation information, as
well as simple decision tree-like heuristics extracted
from the training data. For task B we trained an
SVM classifier using some of the annotations from
Task A.
The paper first presents the process of data com-
pilation and preprocessing (section 2), and then de-
scribes the systems for Tasks A (section 3) and B
(section 4). Results and conclusions are discussed
in the last section.
2 Data Compilation and Processing
2.1 Making data available
The corpus of SMSs was provided to the partici-
pants by the organizers of the task. As for the corpus
of tweets, legal restrictions on twitter data distribu-
tion required the participants to download the tex-
tual contents of the corpus from a list of tweet ids.
We retrieved the tweet text using the official twit-
ter API instead of script provided by the organizers,
but not all the tweets were available for download
483
due to restrictions of different types (e.g. geograph-
ical), or because the twitter account was temporarily
suspended. In total, we managed to retrieve 10,764
tweets out of 11,777 ids provided by the organizers
(91.4%). It is worth pointing out that the restric-
tions on tweets distribution can become an issue for
future users of the dataset, as the amount of avail-
able tweets will diminish over time. By contrast, the
twitter test corpus was distributed with the full text
to avoid those problems.
2.2 Leveraging the data with rich linguistic
information
We applied the same linguistic processing to both
corpora (SMSs and tweets), even though the SMS
test data presents very different characteristics from
the twitter data, not only because of what can be ap-
preciated as genre differences, but also due to the
fact that is apparently written in Singaporean En-
glish, which differs significantly from American or
British English. No efforts were made to adapt
our linguistic processing modules and dictionaries
to this data.
Tweets and SMSs were processed with a UIMA1-
based pipeline consisting of a set of linguistic and
opinion-oriented modules, which includes:
Basic linguistic processing: Sentence segmen-
tation, tokenization, POS-tagging, lemmatiza-
tion.
Syntax: Dependency parsing.
Lexicon-based annotations:
? Basic polarity, distinguishing among: positive,
negative, and neutral, as encoded in Wilson et
al. (2010).
? Polarity strength, using the score for pos-
itive and negative polarity in SentiWordnet
3.0 (Baccianella et al, 2010). Each Sen-
tiWordNet synset has an associated triplet of
numerical scores (positive, negative,
and objective) expressing the intensity of
positive, negative and objective polarity of the
terms it contains. They range from 0.0 to 1.0,
and their sum is 1.0 for each synset (Esuli and
Sebastiani, 2007). We selected only the synset
1http://uima.apache.org/uima-specification.html
with positive or negative scores higher than 0.5,
containing a total of 16,791 words.
? Subjectiviy clues, from Wilson et al (2010),
which are classified as weak or strong depend-
ing on their degree of subjectivity.
? Sentiment expressions, from the Linguistic In-
quiry and Word Count (LIWC) 2001 Dictio-
nary (Pennebaker et al, 2001).
? In-house compiled lexicons of negation mark-
ers (such as ?no?, ?never?, ?none?) and quanti-
fiers (?all?, ?many?, etc.), the latter further clas-
sified into low, medium and high according to
their quantification degree.
The different classifiers employed by FBM con-
structed their vectors from this output to learn global
and contextual polarities.
3 Task A: Ensemble System
Our system combined Machine Learning and rule-
based approaches. The aim was to combine the
strengths of each individual component while avoid-
ing as much as possible their weaknesses. In what
follows we describe each system component as well
as the way the ensemble system worked out the col-
lective decisions.
3.1 Conditional Random Fields
One of the classifiers uses the Conditional Random
Fields implementation of a biomedical Named En-
tity Recognition system (JNET from JulieLab) 2, ex-
ploiting the classification capabilities of the system
(rather than its span detection) by strongly associat-
ing already defined ?marked instances? with a polar-
ity, and exploring a 5-word window. It uses depen-
dency labels, POS tags, polar words, sentiwordnet
and LWIC sentiment annotations, as well as indica-
tions for quantifiers and negation markers.
3.2 Support Vector Machines
This classifier was implemented using an SVM algo-
rithm with a linear kernel and the C parameter set to
0.2 (determined using a 5 fold cross-validation). The
features set includes those that we used in RepLab
2http://www.julielab.de
484
2012 (Chenlo et al, 2012) (including number of:
characters, words, links, hashtags, positive and neg-
ative emoticons, question-exclamation marks, ad-
jectives, nouns, verbs, adverbs, uppercased words,
words with duplicated vowels), plus a set of new
features at tweet level obtained from the linguistic
annotations: number of high/medium/low polarity
quantifiers, number of positive and negative polar
words, sentiwordnet applied to both the cue and the
whole tweet.
Moreover, the RepLab polarity calculation based
on different dictionaries was modified to take into
account negation (in a 3-word window) potentially
inverting the polarity (negPol). This polarity mea-
sure was applied to the cue and to the whole tweet,
thus generating two additional features.
3.3 Heuristic Approach
In task A, in parallel to the supervised learning sys-
tem, we developed a method (named Heur) based
on polarity dictionary lookup and simple heuristics
(see Figure 1) taking into account opinion words
as well as negation markers and quantifiers. These
heuristics were implemented so as to maximize the
number of correct positive and negative labels in the
training data. To this end, we calculated the aggre-
gate polarity of a cue segment as the sum of word
polarities found in the polarity lexicon. The aggre-
gate values in the training set ranged from -3 to +3,
taking respectively 1, 0 and -1 as the polarity of pos-
itive, neutral and negative words. The label distri-
bution of cue segments with an aggregate polarity
value of -1 is shown in Table 1.
Aggregate polarity -1
Negation no yes
negative 1,032 30
neutral 37 4
positive 178 71
Table 1: Cue segment polarity statistics in training data
for an aggregate polarity value of -1.
In this case, if no negation is present in the cue
segment, a majority (1,032) of examples had the
negative label. In case there was at least a negation, a
majority (71) of examples had a positive label. This
behaviour was observed with all negative aggregate
1: if has polar word(CUE) then
2: polarity= lex(P)-0.5*lex(QP)
3: -lex(N)+0.5*lex(QN)
4: if polarity>0 then
5: if has negation(CUE) then negative
6: else positive
7: end if
8: else if polarity<0 then
9: if has negation(CUE) then positive
10: else negative
11: end if
12: else
13: if has negation(CUE) then positive
14: else negative
15: end if
16: end if
17: else if has negation(CUE) then negative
18: else
19: polarity= tlex(P)-0.5*tlex(QP)
20: -tlex(N)+0.5*tlex(QN)
21: if polarity<0 then negative
22: else if tlex(NEU)>0 then neutral
23: else if polarity>0 then positive
24: else if has negemo(CUE) then negative
25: else if has posemo(CUE) then positive
26: else unknwn
27: end if
28: end if
Figure 1: Heuristics used by the lexicon-based system to
classify the polarity of a segment marked up as opinion
cue (Task A).
polarity values in training data, yielding the rule in
lines 8 to 11 of Figure 1. Similar rules were ex-
tracted for the other aggregate polarity values (lines
4 to 16 of Figure 1).
Figure 1 details the complete classification algo-
rithm. Note (lines 1 to 17) that we first rely on the
basic polarity lexicon annotations (described in sec-
tion 2). The final aggregate polarity formula (lines
2-3) was refined to distinguish sentiment words
which act as quantifiers, such as pretty in pretty mad.
The word pretty is both a positive polar word and a
quantifier. We want its polarity to be positive in case
it occurs in isolation, but less than one so that the
sum with a following negative polar word (such as
mad) be negative. We thus give this kind of words
a polarity of 0.5 by substracting 0.5 for each polar
word which is also a quantifier. In the polarity for-
mula of lines 2-3, lex(X) refers to the number of
words annotated as X, P and N refer respectively
to positive and negative polar words, and QP and
485
QN refer to positive and negative polar words which
are also quantifiers. Quantifiers which are not polar
words are not taken into account because they are
not likely to change the opinion polarity.
In case that no annotations from the basic polar-
ity, quantifiers, and negative markers lexicons are
found (lines 18 to 28), we look up in dictionaries
built from the training data (tlex in lines 19-20).
To build these dictionaries, we counted how many
times each word was labeled positive, negative and
neutral. We considered that a word has a given po-
larity if the number of times it was assigned to this
class is greater than the number of times it was as-
signed to any other class by a given threshold. We
calculated the polarity in the same way as before,
but now with the counts from the lexicon automati-
cally compiled from the training data. To improve
the recall of the dictionary lookup, we performed
some text normalization: lowercasing, deletion of
repeated characters (such as gooood) and deletion of
the hashtag ?#? character. Finally, if no polar word
is found in the automatically compiled lexicon, we
look at the sentiment annotations (extracted from the
LIWC dictionary).
3.4 Ensemble Voting Algorithm
As already mentioned, we combined the results from
the described polarity methods to build a collective
decision. Table 2 shows the performance (in terms
of F1 measure) of the different single methods over
the tweet test data.
SVM Heur Heur+ CRF
Test 80.74 83.47 84.62 62.85
Table 2: Twitter Task A results for different methods
Although the heuristic method outperforms the
ML methods, they are not only different in nature
(ML vs. heuristic) but also use different information
(see Table 5). This suggests that the ensemble solu-
tion will be complementary and capable of obtaining
better results than any of the individual methods by
itself.
The development set was used to calculate the en-
semble response given the individual votes of the
different systems in a way similar to the behavior
knowledge space method (Huang and Suen, 1993).
Table 3 shows an example of how the assemble
voting is built. For each method vote combina-
tion (SVM-Heuristics-CRF) the number of positives
/ negatives / neutral is calculated in the development
data. The ensemble (EV) selects the vote that max-
imizes the number of correct votes in the develop-
ment data (in bold).
SVM Heur CRF EV
# Instances
pos neg neu
? + ? ? 0 6 0
? ? + ? 1 23 2
? ? ? ? 3 125 2
? u + + 1 0 0
+ u n ? 0 1 0
+ ? + + 17 13 2
+ + + + 314 18 17
+ ? n + 3 1 0
Table 3: Oracle building example (EV: Ensemble Vote,
+:positive, ?:negative, n:neutral, u:unknown)
The test data contains some combination of votes
that were not seen in the development data. Thus,
in order to deal with these unseen combinations of
votes in the test set we use the following backup
heuristics based on the preformance figures of the
individual methods: Use the vote of the heuristic
method. If this method does not vote (u), then se-
lect the SVM vote.
Table 4 shows the results of the proposed ensem-
ble method, the well-known majority voting and the
upper bound of this ensemble method (calculated
with the same strategy over the test data), over the
development and test tweet data
Ensemble Majority Upper
Voting Voting Bound
Dev 85.48 81.31 85.48
Test 85.50 82.70 89.37
Table 4: Results for different ensemble strategies
In the development corpus, the upper bound and
ensemble results are the same, given that they ap-
ply the same knowledge. The difference is in the
test dataset, where the ensemble voting is calculated
based on the knowledge obtained from the develop-
ment corpus, while the upper bound uses the knowl-
edge that can be derived from the test corpus.
486
Table 5 illustrates the features used by each com-
ponent.
SVM SVM CRF Heur
(task A) (task B)
word ? ? ?
lemma
pos ? ?
deps ?
pol ? ? ? ?
polW ?
sent ? ? ?
sentiwn ? ? ?
quant ? ? ? ?
neg ? ? ? ?
links ?
hashTags ?
Table 5: Information used (pos: part-of-speech; deps: de-
pendencies; pol: basic polarity classification; polW: basic
polarity word; sent: LIWC sentiments; sentwn: Senti-
Wordnet; quant/neg: quantifiers and negation markers.)
4 Task B: A Support Vector
Machine-based System
The system presented for task B is based on ML us-
ing a SVM model. The feature vector used as input
for the SVM component is composed of the annota-
tions provided by the linguistic annotation pipeline,
extended with a feature obtained by applying nega-
tion to the next polar words (window of size 3).
The features used do not include the words (or
their lemmas) because the number of tweets avail-
able for training is small (104) compared to the num-
ber of different words (4 ? 104). A model based on
bag-of-words would suffer from overfitting and thus
be very domain and time-dependent. If the train and
test sets were randomly selected from a bigger set,
the use of words could increase the model?s accu-
racy, but the model would also be too narrowly ap-
plied to this specific dataset.
From the annotation pipeline we extracted as fea-
tures: the polar words (PolW) and their basic po-
larity (Pol); the sentiment annotations from LIWC
(Sent); the negation markers (Neg) and quantifiers
(Quant). The model was trained using Weka (Hall
et al, 2009).
The model used is SVM with the C parameter set
to 1.0 and applying a 10 fold cross-validation. The
option of doing first a model to discriminate polar
and neutral tweets was discarded because Weka al-
ready does that when training classifiers for more
than two training classes, and the combination of the
two classifiers (a first one between polar and opin-
ionated and a second one between positive and neg-
ative) would produce the same results.
5 Results and Discussion
The results of our system in each subcorpus and task
are presented in Table 5 (average of the F1-measure
over the classes positive and negative, constrained
track), with the ranking achieved in the competition
in parentheses.
Tweet Corpus SMS Corpus
Task A 0.86 (5th) 0.73 (11th)
Task B 0.61 (7th) 0.47 (28th)
Table 6: FBM system performance (F1 average over pos-
itive and negative classes, constrained track) and rankings
Given the differences in style and vocabularies be-
tween the SMS and tweet corpora, and the fact that
we made not effort whatsoever to adapt our system
or models to them, the drop in performance from
one to the other is considerable, but to be expected
since domain customization is an important aspect
of opinion mining.
Task A: The confusion matrix in Table 7 shows
an acceptable performance for the most frequent
classes in the corpus (with an error of 7.75% and
19.5% for postive and negative cues, respectively)
and a very poor job for neutral cues (98.1% of er-
ror), clearly a minority class in the training corpus
(5% of the data).
GOLD: Pos Neg Neu
SYSTEM: Pos 2,522 296 126
Neg 206 1,240 31
Neu 6 5 3
Table 7: Task A confusion matrix
Given the skewed distribution of polarity cate-
gories in the test corpus, however, neutral mistakes
amount to only 23% of our system error, and so we
487
focus our analysis on the problems in positive and
negative cues, respectively amounting to 31.7% and
44.8% of the total error. There are 2 main sources of
error:
? Limitations of the dictionaries employed,
which were short in covering somewhat fre-
quent slang words (e.g., wacky, baddest, shit-
loads), expressions (e.g., ouch, yukk, C?MON),
or phrases (e.g., over the top), some of which
express a particular polarity but contain a word
expressing just the opposite (have a blast, to
want something bad/ly).
? Problems in UGC processing, mainly related to
normalization (e.g., fooooool) and tokenization
(Perfect...not sure), which put at risk the cor-
rect identification of lexical elements that are
crucial for polarity classification.
Task B: The average F-score of positive and neg-
ative classes was 0.62 in the development set (that
was included in the training set) and the averaged F-
score for the test set was 0.61 (so they are very simi-
lar). If focusing on precision and recall, the positive
and negative classes have higher precision but lower
recall in the test set. We think that this low degrada-
tion of perfomance indicates the model?s potential
for generalization.
6 Conclusions
From our results, we can conclude that the use of
ensemble combination of orthogonal methods pro-
vides good performance for Task A. Similar results
could be expected for Task B (judging from mix-
ing dictionaries and ML in similar tasks at RepLab
2012 (Chenlo et al, 2012)). The ML methods that
we applied for Task B are essentially additive, and
hence have difficulties in applying features such as
polarity shifters. To overcome this, one of the fea-
tures includes negation of polar words when a polar-
ity shifter is near.
Overall, the SemEval Tasks have make evident the
usual challenges when mining opinions from Social
Media channels: noisy text, irregular grammar and
orthography, highly specific lingo, etc. Moreover,
temporal dependencies can affect the performance if
the training and test data have been gathered at dif-
ferent times, as is the case with text of such a volatile
nature as tweets and SMSs.
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
45.00%
50.00%
train
dev
test
Figure 2: Distribution of tweets over time
The histogram in Figure 2 shows that this also ap-
plies to the Semeval tweets dataset. It illustrates the
distribution of tweets over time (extrapolated from
the sequential ids) in the 3 subcorpora (train, devel-
opment and test), showing some divergence between
the test corpus on the one hand, and the develop-
ment and training corpora on the other. Neverthe-
less, our system shows little performance degrada-
tion between development and testing results, as at-
tested in Table 4 (ensemble voting column).
Our work here and at other competitions already
cited validate a system that combines stochastic and
symbolic methodologies in a principled, data-driven
approach. Time and domain dependencies of Social
Media data make system and model generalization
highly desirable, and our system hybrid nature also
contribute to this objective.
Acknowledgments
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, the CENIT program project Social Media,
CEN-20101037, and the Marie Curie Reintegration
Grant PIRG04-GA-2008-239414.
488
References
Baccianella, Stefano, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation, Valletta, Malta.
Chenlo, Jose M., Jordi Atserias, Carlos Rodr??guez-
Penagos and Roi Blanco. 2012. FBM-Yahoo!
at RepLab 2012. In: P. Forner, J. Karlgren,
C. Womser-Hacker (eds.) CLEF 2012 Evalua-
tion Labs and Workshop, Online Working Notes.
http://clef2012.org/index.php?page=Pages/procee-
dings.php.
Esuli, Andrea and Fabrizio Sebastiani. 2007. SEN-
TIWORDNET: a high-coverage lexical resource for
opinion mining. Technical Report ISTI-PP-002/2007,
Institute of Information Science and Technologies
(ISTI) of the Italian National Research Council
(CNR).
Hall, Mark, Frank Eibe, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten. 2009.
The WEKA data mining software: an update. In:
ACM SIGKDD Explorations Newsletter, 1: 10?18.
Huang, Y. S. and C. Y. Suen. 1993. Behavior-knowledge
space method for combination of multiple classifiers.
In Proceedings of IEEE Computer Vision and Pattern
Recognition, 347?352.
Liu, Bing. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
(5-1), 1?167.
Pennebaker, James W., Martha E. Francis and Roger
J. Booth. 2001. Linguistic inquiry and word count:
LIWC 2001. Mahway: Lawrence Erlbaum Asso-
ciates.
Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov and Alan. Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2010. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3), 399?433.
489
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46?52,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A hybrid framework for scalable Opinion Mining in Social Media: 
detecting polarities and attitude targets 
Carlos Rodr?guez-Penagos 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
carlos.rodriguez 
@barcelonamedia.org 
Jens Grivolla 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
jens.grivolla 
@barcelonamedia.org 
Joan Codina Fib? 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
joan.codina 
@barcelonamedia.org 
 
 
Abstract 
Text mining of massive Social Media 
postings presents interesting challenges for 
NLP applications due to sparse 
interpretation contexts, grammatical and 
orthographical variability as well as its very 
fragmentary nature. No single 
methodological approach can be expected to 
work across such diverse typologies as 
twitter micro-blogging, customer reviews, 
carefully edited blogs, etc. In this paper we 
present a modular and scalable framework 
to Social Media Opinion Mining that 
combines stochastic and symbolic 
techniques to structure a semantic space to 
exploit and interpret efficiently. We 
describe the use of this framework for the 
discovery and clustering of opinion targets 
and topics in user-generated comments for 
the Telecom and Automotive domains. 
1 Introduction 
Social Media (SM) postings constitute a messy 
and highly heterogeneous media that nonetheless 
represent a highly valuable source of information 
about the attitudes, interests and expectations of 
citizens and consumers everywhere. This fact has 
driven a trove of recent research and 
development efforts aimed at managing and 
interpreting such information for a wide 
spectrum of commercial applications, among 
them: reputation management, branding, 
marketing design, etc. A diverse array of 
techniques representing the state of the art run 
the gamut from knowledge-engineered rule-and 
lexicon-base approaches that (when carefully 
crafted) provide high precision in homogeneous 
contexts, to wide-coverage machine learning 
approaches that (when suitable development data 
is available) tackle noisy text with reasonable 
accuracies in some genres. 
 As SM channels are as different from each 
other as, say, spoken text from essay writing, we 
believe that no single technique, powerful as it 
may be, is capable of interpreting all domains, 
genres and channels in the vast universe of SM 
conversations. Faced with an industrial demand 
for simultaneous monitoring of heterogeneous 
opinion sources, our approach has evolved into 
combining diverse NLP technologies into a 
robust semantic analysis framework to create a 
high-granularity representation of user-generated 
commentaries amenable to machine 
interpretation. 
Analysis of Telecom-related social postings 
has shown how a modular and scalable analysis 
framework can combine a veritable arsenal of 
NLP and data mining techniques into a hybrid 
application that adapts well to the unique 
challenges and demands of different Social 
Media genres.  
Section 2 will present the UIMA-Solr 
framework and components used to process 
opinionated text, as well as discuss the 
representational choices made for analysis. 
Section 3 will frame our approach within the 
State-of-the-Art of Sentiment analysis and 
Opinion mining as we interpret it, while Sections 
4 and 5 describe data and results of the 
application of our proposed approach in the 
context of opinion topic detection and clustering 
of SM postings in the Telecoms and Automobile 
domains respectively, and with different textual 
genres. Finally, Section 6 will focus on the 
conclusions and future work that presents to us at 
this point.   
46
2 A modular toolset for SM processing 
For semantic processing of our data we use a 
UIMA 1  (Ferrucci & Lally, 2004) architecture  
plus Solr-based clustering and indexing 
capabilities. Our choice of UIMA is guided in 
part by our wish to achieve good scalability and 
robustness, and that all components can be 
implemented modularly and in a distributed 
manner using UIMA-AS (Asynchronous Scale 
out). Also, UIMA?s data representation as CAS 
objects allows preserving the documents integrity 
since annotations are added as standoff metadata, 
without modifying the original information. 
Under the UIMA architecture, a hybrid NLP 
analysis framework is possible, combining 
powerful Machine Learning modules like 
Maximum Entropy (ME, OpenNLP) 2  or 
Conditional Random Fields (CRF, JulieLab), 3 
with gazetteer and regular expression matchers 
and rule-based Noun Phrase chunkers. The basic 
linguistic processing has a sentence and token 
identifier, a POS tagger, a lemmatizer, a NP 
chunker and a dependency parser. In addition, we 
employ gazetteers to match products, companies, 
and other entities in text, as well as a hand-
crafted lexicon of polar terms created from 
corpus exploration of Telecom domain text, as 
well as a regular expression module to detect 
emoticons when available. Also, two models for 
Named-Entity recognition were applied using 
CRF: one trained on conventional ENAMEX 
Named Entity Recognition and Classification 
entities, and another trained using data from 
customer reviews from various domains (Cars, 
Banking, and Mobile service providers), in order 
to detect opinion targets and cues. One of the 
objectives of this relatively straightforward 
processing (although by no means the only one), 
was to select candidates for classifiers that could 
identify both the specific subject of each opinion 
expressed in text, as well as capture a more 
general topic of the whole conversation (which 
conceivably could coincide or not with one of the 
specific opinion targets). Targets and topics are 
usually expressed as entity names, concepts or 
attributes, and thus can appear in language as 
noun, adjectival, adverbial or even verbal 
phrases. Opinion cues (or Q-elements) are words, 
emoticons and phrases that convey the actual 
attitude of the speaker towards the topics and 
                                                 
1 Unstructured Information Management Architecture 
2 http://maxent.sourceforge.net 
3 http://www.julielab.de 
targets, and a strength and polarity can be 
attributed to them, both a priori and in context. 
Our modular processing approach allows 
customizing the annotation for each domain or 
genre, since, for example, regular expressions to 
detect emoticons will be useful for twitter micro-
blogging, but less so for more conventional blogs 
where such sentiment-expression devices are less 
frequent; Also pre-compiled lists of known 
entities can provide good target precision while 
customised distributional models will help 
discover unlisted names and concepts in text. 
The output of the semantic and syntactic 
processing pipeline is indexed using the Apache 
Solr framework,4 which is based on the Lucene 
engine. This setup allows the implementation of 
clustering and classification algorithms, allowing 
us to obtain reliable statistical correlations 
between documents and entities.  
We also developed or adapted a number of 
visualization components in order to present the 
data stored in Solr in an interactive page that is 
conducive to data exploration and discovery by 
the system?s corporate users. At the same time, 
Carrot2 is connected to Solr and is used to test 
clustering conditions and algorithms, providing a 
nice visualization interface. Carrot2 is an open 
source search results clustering engine (Osi?ski 
& Weiss, 2005). It can automatically organize 
collections of documents into thematic 
categories. 
3 Previous work 
Two good overviews of general Opinion Mining 
and Sentiment Analysis challenges are Pang & 
Lee (2008) and, focused specifically on customer 
reviews, Bhuiyan, Xu & Josang (2009). 
Detecting the subject or targets of opinions is one 
of the main lines of work within Opinion 
Mining, and considerable effort has been put into 
it, since it has been shown to be a highly-domain 
specific task (consumer reviews will focus on 
specific products and features, tweets have 
hashtags to identify topics, blogs can talk almost 
about anything, etc.).  
Outside of user-generated content, Coursey, 
Mihalcea, & Moen (2009) have suggested using 
indirect semantic resources, such as the 
Wikipedia, to identify document topics. For 
Opinion Mining genres, and extending on Hu & 
Liu (2004), Popescu & Etzioni (2005) use a 
combination of Pointwise Mutual Information, 
                                                 
4 http://lucene.apache.org/solr/ 
47
relaxation labeling and dependency analysis to 
extract possible targets and features in product 
reviews. Kim & Hovy (2006), for example, use 
thematic roles to establish a relation between 
candidate opinion holders and opinion topics, 
while exploiting clustering to improve coverage 
in their role-labeling. Recent approaches have 
included adaptation of NER techniques to noisy 
and irregular text, either by using learning 
algorithms or by doing text normalization (Locke 
& Martin, 2009; Ritter, Clark & Etzioni, 2011). 
4 Exploring the semantic space of 
Telecom-related online postings 
We collected close to 200,000 postings from 
various SM sources in a 4 month timeframe, 
including fairly carefully-written product-
oriented forums, blogs, etc., as well as more 
casually-drafted Facebook and twitter micro-
blogging, that discussed Spanish Telecom?s 
services and products. Of these, we randomly 
sub-selected a representative 190-document 
sample that was manually marked-up (for a test 
involving machine learning of cue-polarity-target 
relationships) by two different human annotators 
with a 20-document overlap, using simplified 
annotation guidelines focused on opinion targets, 
topics, cues and polarities. An interesting 
observation about the interannotator agreement 
(but one we can?t discuss in detail here) is that 
with regard to targets one of the human 
annotators tended more towards complete 
syntactic units (noun phrases), while the other 
chose more conceptual and semantic extensions 
as subjects for the opinions. The 20-document 
overlap was meant to help us evaluate this 
guideline development process, but the 
misalignment of guideline interpretation by the 
two human annotators made it very difficult to 
measure any kind of true interannotator 
agreement. Also, single annotation adjudication 
was made difficult due to the fact that both 
interpretations presented valid aspects, and we 
chose to use each set as an independent 
evaluation set to detect any unnoticed patterns 
that could emerge from using one of the other in 
our training and validation, but those results are 
inconclusive and merit further research. Since no 
adjudicator was incorporated in the process to 
resolve disagreements, the final annotated sets do 
not constitute a true Gold Standard, but each 
human-annotated set was used in turn as a 
benchmark against automatic annotators.  
Content elicitation was combined with activity 
and network mining for an enriched overview of 
the social conversation ecosystems, but the 
second aspect won?t be discussed here for the 
sake of brevity. For the same reason, although 
other aspects of sentiment analysis were 
performed on this data (cue and polarity 
detection, for example), we will also restrict the 
scope of these discussions on the detection and 
clustering of specific targets and general topics 
of the opinions expressed in such SM channels. 
Obviously, a deeper and more textured view of 
opinionated text is needed to be of any real use, 
but the overall features, shortcomings and 
advantages of our chosen approach are 
adequately discussed even if we restrict this 
paper to these very specific tasks. 
The first series of experiments about clustering 
using semantics explored the above-mentioned 
corpus of SM posting that discussed a Spanish 
Telecom, one of the aims being detecting and 
aggregating the topics and targets of online 
opinions. Different processing modules geared 
towards topic and target detection were 
compared against each human annotator?s 
choices, but also against each other and to the 
combined output of each. The main modules 
involved were: (A) generic NERC,  (B) a target 
and topic NERC model (StatTarg), (C) a Noun 
Phrase Chunker, and (D) a Gazetteer matcher 
(Taxonomy). Figures 1 through 4 show, 
respectively, recall (1) and precision (2) with 
regard to human annotated topics, and recall (3) 
and precision (4) with regard to human annotated 
targets. 
The results presented here are the overall 
performance across genres and domains, since 
the 190 documents annotated covered the whole 
range from forums to tweets. 
 
 Figure 1. Topic recall 
  
48
 Figure 2. Topic precision 
 
 Figure 3. Target recall 
 
 Figure 4. Target precision 
For this experiment, and as a guideline for the 
human annotators, targets were roughly defined 
as occurrences in the text of objects of opinion, 
whereas topics where to represent the main focus 
of the document or message. The annotators 
usually marked one topic per document, which 
was almost always also one of the targets. 
The customized taxonomy has a good precision 
with regard to target and topic identification, 
while the NERC and NP Chunk approaches 
improve the recall but suffer a bit on precision. 
Generic NER models have a moderately high 
precision (63%) with regard to manually 
annotated targets but rather low recall (specially 
in genres where capitalization is irregular which 
hinders NER detection), while NP Chunks 
present the opposite case: moderately (56%) high 
recall with low precision. This can be explained 
in part by the ?greediness? of each methodology, 
with the chunker annotating extensively while 
the NERC model being much more selective. 
Another noteworthy result is the strong domain 
bias of target annotators trained on a Ciao 
customer reviews for Banking, Automotive and 
Mobile Service markets. The models 
implemented through training from multi-domain 
review sites were found to have medium 
precision, but very low recall. 
The combination of all modules (AllTargets, a 
combination of NERC, Chunker, Taxonomy and 
StatTarget) had a very high recall of around 90%. 
With regard to topic detection, the combination 
of all modules had a recall of 94% and 83%, 
depending on which gold standard it is compared 
to (the one created by one expert human 
annotator or the other), which is an excellent 
recall level. The precision obtained on topic 
detection is very low. This, however, is expected 
as the evaluation is done using all candidates 
given by the different annotation layers, with no 
selection process. Since most of the topics are 
already identified as targets, the key issue here is 
to identify which of the comment targets is the 
main topic. 
It is important to note that merging the Chunker 
output with that of the rest of the modules 
improves the recall of the system but the 
precision becomes low. The main reason is that 
most targets and topics are noun phrases, but not 
all noun phrases are targets or topics.  
It is important to note that combining the output 
of different annotation layers (except for the NP 
chunker) does not reduce overall precision, while 
greatly increasing recall. 
For the clustering experiments, we chose 
Carrot2?s Lingo, a clustering algorithm based on  
Singular Value Decomposition. We envisioned 
the content-based clustering as an interactive 
exploratory tool, rather that providing a single 
?correct? and definitive set of groupings. Cluster 
analysis as such is not an automatic task, but an 
iterative process of knowledge discovery that 
involves trial and failure. It will often be 
necessary to modify the preprocessing and adjust 
parameters until the result achieves the desired 
properties. 
The  query ?problem?, for example, sent to 
some of the telecom forums in May produced 
groupings suggestive of complaints relating to 
rates, internet access, SIM chips, SMS, as well as 
with regard to specific terminal models and 
companies. Even this limited capability can be 
helpful for some of our user?s market analysis 
purposes. 
49
The visualization of query-based clustering 
with detection of target, cues and topics, and the 
possibility of tracking trends over time, provided 
a very powerful overview of how consumer 
attitudes, expectations and complaints about 
products and services are reflected in dynamic 
interchanges in various SM channels. These 
results are available through an online demo 6 
(Figure 5, shown for Facebook postings). 
5 Visualizing the evolution of customer 
opinion 
In addition to exploring SM data for the Telecom 
domain, we performed some experiments using 
clustering without directly using annotated 
semantics, but instead using the semantics only 
for data interpretation. We crawled more than 
10,000 customer reviews in the automotive 
domain in Spanish, along with some metadata 
that included the numerical ratings added by the 
reviewers themselves. Using our modular 
pipeline, we did shallow document clustering 
followed by linguistic processing that included 
lemmatization, POS tagging and Named Entity 
Recognition, in order to allow for analytical 
exploitation of the community-driven discussion 
on automobiles, product features and 
                                                 
6 http://webmining.barcelonamedia.org/Orange/ 
automakers. The most relevant nouns, adjectives, 
bigrams and named entities from a given query, 
are projected into a polarity versus time dynamic 
map. The clustering was performed by the 
combined use of vector space reduction 
techniques and the K-means classification 
paradigm in a completely unsupervised manner. 
Clusters thus obtained were represented by sets 
of words that best described them to obtain a 
view of the emerging terms, trends and features 
contained in the opinions, with the aim of 
providing a representation of their collective 
content. Since evaluating clustering techniques 
per se was not the objective of these 
experiments, and since a gold standard was not 
available, the purpose of the system was (A) to 
validate the coherence of the groupings 
according to the review?s content, and (B) assess 
if those clusters also aggregate as well along 
declared global polarity. Although inconclusive 
from a quantitative point of view, those 
experiments show the feasibility of leveraging 
existing Social Media resources in order to 
develop applications that can visualize and 
explore the semantic ecosystem of consumer 
opinions and attitudes, in a cost-effective and 
efficient manner. A demo of the functionalities 
of the system described here is also publicly 
Figure 5. Facebook's "Iphone" semantic exploration (screenshot) 
50
available. 7 . One cluster, a very positive one 
(based on the average user rating), is represented 
by the terms land-terreno-todoterreno-rover-
campo-4x4 (off-road, field, ground, land, Rover), 
while another one, aceite-garant?a-servicio-
problemas-a?os (oil-warranty-service-problems-
years), in the lower right side might indicate 
unhappy reviewers. 
6 Conclusion and future work 
The results obtained on the Telecom corpus with 
different automatic annotation layers suggest that 
a possible improvement in the system could 
come from researching which combinations of 
automatic annotators can enhance overall 
performance, as one module?s strength might 
complement another weaknesses and vice versa, 
so that what one is missing another one can 
catch. An additional option to increase overall 
recall is to implement a weighted voting scheme 
among the modules, allowing calculation of 
probabilities from the combinations of various 
annotations that overlap a textual segment. 
The fact that combination of annotation layers 
through simple merging of all annotations has 
such a great impact on recall while not reducing 
precision suggests that the different methods are 
very complementary. We expect to be able to 
trade off some of the gained recall for much 
improved precision by applying more 
sophisticated merging methods. 
Another possibility to be explored is using top 
level dependencies (such as SUBJECT, 
SENTENCE, etc.) to rank and select the main 
topic and target candidates using sentence 
structure configuration. This approach would 
also ensure that once a polarity-laden cue is 
identified, the corresponding target could be 
uniquely identified. This linguistics-heavy 
approach is feasible only in texts whose 
characteristics more closely resemble the data 
used to train the parser. 
Our work has helped us focus more clearly many 
of the challenges faced by any NLP system when 
used in a new user-generated content: scarce 
development data, novel pattern and form 
adaptability, tool robustness, and scalability to 
massive and noisy text.  
One of the lessons learned during these 
experiences is that keeping a modular hybrid 
analysis framework can improve matching by 
either customizing the pipeline to each genre and 
                                                 
7 http://webmining.barcelonamedia.org/cometa/index_dates 
task requirements, or by combining the results of 
different approaches to benefit from each one?s 
strengths while minimizing each one?s 
weaknesses. Extracting opinion centered 
information from highly heterogeneous text and 
from multitudes of authors will never be as 
straightforward as, say, doing IE on newswire or 
financial news, but it should be feasible and 
useful by using the right toolset. We are in the 
process of using crowdsourcing to fully annotate 
vast Spanish and English corpora of opinionated 
text, which will allow us to perform a better and 
more fine-grained quantitative analysis of our 
framework in the near future. 
Another lesson learned is that even if high-
precision opinion classification is not available 
(because not enough development data is 
available, or data is noisy, or for whatever other 
reason) doing even superficial semantic 
annotation of the text and unsupervised 
clustering can help industrial consumer of these 
technologies understand better what is being said 
in the Social Media ecosystems. Valuable 
objectives for a useful opinion mining system do 
not need to include all possible analyses or state-
of-the-art performance. 
Going forward, computational exploitation of 
Social Media and of community-based, data-
driven discussions on diverse topics and products 
is definitely an important facet of future market 
and business intelligence competencies, since 
more and more of our activities as citizens, 
friends and consumers take place in an online 
environment, where everything seems possible 
but where also everything we do leaves a trace 
and has a meaning. Extracting the semantics of 
collective action enables us to access that 
meaning. 
References 
 
Ritter A, Clark S, Mausam, and Etzioni O (2011). 
Named Entity Recognition in Tweets: An 
Experimental Study. Proceedings of the 2011 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2011) 
Bhuiyan, T., Xu, Y., & Josang, A. (2009). State-of-
the-Art Review on Opinion Mining from Online 
Customers? Feedback. Proceedings of the 9th Asia-
Pacific Complex Systems Conference (pp. 385?
390). 
Coursey, K., Mihalcea, R., & Moen, W. (2009). Using 
encyclopedic knowledge for automatic topic 
identification. Proceedings of the Thirteenth 
Conference on Computational Natural Language 
Learning, CoNLL  ?09 (pp. 210?218). Stroudsburg, 
51
PA, USA: Association for Computational 
Linguistics.  
Ferrucci, D., & Lally, A. (2004). UIMA: an 
architectural approach to unstructured information 
processing in the corporate research environment. 
Natural Language Engineering, 10(3-4), 327?348. 
Hu, M., & Liu, B. (2004). Mining and summarizing 
customer reviews. Proceedings of the tenth ACM 
SIGKDD international conference on Knowledge 
discovery and data mining (pp. 168-177). Seattle, 
WA, USA: ACM. doi:10.1145/1014052.1014073 
Kim, S. M., & Hovy, E. (2006). Extracting opinions, 
opinion holders, and topics expressed in online 
news media text. Proceedings of the Workshop on 
Sentiment and Subjectivity in Text (pp. 1?8). 
Locke, B., & Martin, J. (2009). Named entity 
recognition: Adapting to microblogging. University 
of Colorado.  
Osi?ski and D. Weiss (2005), ?Carrot 2: Design of a 
flexible and efficient web information retrieval 
framework,? Advances in Web Intelligence, pp. 
439?444, 2005. 
Pang, B., & Lee, L. (2008). Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
Popescu, A. M., & Etzioni, O. (2005). Extracting 
product features and opinions from reviews. 
Proceedings of HLT/EMNLP (Vol. 5, pp. 339?
346). 
 
 
 
 
52
