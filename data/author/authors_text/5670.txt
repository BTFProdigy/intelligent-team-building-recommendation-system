Proceedings of NAACL HLT 2007, Companion Volume, pages 13?16,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
JOINT VERSUS INDEPENDENT PHONOLOGICAL FEATURE
MODELS WITHIN CRF PHONE RECOGNITION
Ilana Bromberg?, Jeremy Morris?, and Eric Fosler-Lussier??
?Department of Linguistics
?Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
bromberg@ling.ohio-state.edu, {morrijer, fosler}@cse.ohio-state.edu
Abstract
We compare the effect of joint modeling
of phonological features to independent
feature detectors in a Conditional Random
Fields framework. Joint modeling of fea-
tures is achieved by deriving phonological
feature posteriors from the posterior prob-
abilities of the phonemes. We find that
joint modeling provides superior perfor-
mance to the independent models on the
TIMIT phone recognition task. We ex-
plore the effects of varying relationships
between phonological features, and sug-
gest that in an ASR system, phonological
features should be handled as correlated,
rather than independent.
1 Introduction
Phonological features have received attention as a
linguistically-based representation for sub-word in-
formation in automatic speech recognition. These
sub-phonetic features allow for a more refined repre-
sentation of speech by allowing for temporal desyn-
chronization between articulators, and help account
for some phonological changes common in sponta-
neous speech, such as devoicing (Kirchhoff, 1999;
Livescu, 2005). A number of methods have been de-
veloped for detecting acoustic phonological features
and related acoustic landmarks directly from data
using Multi-Layer Perceptrons (Kirchhoff, 1999),
Support Vector Machines (Hasegawa-Johnson et al,
2005; Sharenborg et al, 2006), or Hidden Markov
Models (Li and Lee, 2005). These techniques
typically assume that acoustic phonological feature
events are independent for ease of modeling.
In one study that broke the independence assump-
tion (Chang et al, 2001), the investigators devel-
oped conditional detectors: MLP detectors of acous-
tic phonological features that are hierarchically de-
pendent on a different phonological class. In (Ra-
jamanohar and Fosler-Lussier, 2005) it was shown
that such a conditional training of detectors tended
to have correlated frame errors, and that improve-
ments in detection could be obtained by training
joint detectors. For many features, the best detector
can be obtained by collapsing MLP phone posteriors
into feature classes by marginalizing across phones
within a class. This was shown only for frame-level
classification rather than phone recognition.
Posterior estimates of phonological feature
classes, as in Table 1, particularly those derived
from MLPs, have been used as input to HMMs
(Launay et al, 2002), Dynamic Bayesian Networks
(DBNs) (Frankel et al, 2004; Livescu, 2005),
and Conditional Random Fields (CRFs) (Morris
and Fosler-Lussier, 2006). Here we evaluate
phonological feature detectors created from MLP
phone posterior estimators (joint feature models)
rather than the independently trained MLP feature
detectors used in previous work.
2 Conditional Random Fields
CRFs (Lafferty et al, 2001) are a joint model of
a label sequence conditioned on a set of inputs.
No independence is assumed among the input; the
CRF model discriminates between hypothesized la-
bel sequences according to an exponential function
of weighted feature functions:
P (y|x) ? exp
?
i
(S(x,y, i) + T(x,y, i)) (1)
13
Class Feature Values
SONORITY Vowel, Obstruent, Sonorant, Syllabic, Silence
VOICE Voiced, Unvoiced, N/A
MANNER Fricative, Stop, Stop-Closure, Flap, Nasal, Approximant, Nasalflap, N/A
PLACE Labial, Dental, Alveolar, Palatal, Velar, Glottal, Lateral, Rhotic, N/A
HEIGHT High, Mid, Low, Lowhigh, Midhigh, N/A
FRONT Front, Back, Central, Backfront, N/A
ROUND Round, Nonround, Roundnonround, Nonroundround, N/A
TENSE Tense, Lax N/A
Table 1: Phonetic feature classes and associated values
where P (y|x) is the probability of label sequence
y given an input frame sequence x, i is the frame
index, and S and T are a set of state feature functions
and a set of transition feature functions, defined as:
S(x, y, i) =
?
j
?jsj(y, x, i), and (2)
T (x, y, i) =
?
k
?ktk(yi?1, yi, x, i) (3)
where ? and ? are weights determined by the learn-
ing algorithm. In NLP applications, the component
feature functions sj and tk are typically realized as
binary indicator functions indicating the presence or
absence of a feature, but in ASR applications it is
more typical to utilize real-valued functions, such as
those derived from the sufficient statistics of Gaus-
sians (e.g., (Gunawardana et al, 2005)).
We can use posterior estimates of phone classes or
phonological feature classes from MLPs as feature
functions (inputs) within the CRF model. A more
detailed description of this CRF paradigm can be
found in (Morris and Fosler-Lussier, 2006), which
shows that the results of phone recognition using
CRFs is comparable to that of HMMs or Tandem
systems, with fewer constraints being imposed on
the model. State feature functions in our system are
defined such that
s?,f (yi,x, i) =
{
NNf (xi), ifyi = ?
0, otherwise
(4)
where the MLP output for feature f at time i is
NNf (xi). This allows for an association between
a phone ? and a feature f (even if f is traditionally
not associated with ?).
In this study, we experiment with different meth-
ods of generating these feature functions. In various
experiments, they are generated by training MLP
phone detectors, by evaluating the feature informa-
tion inherent in the MLP phone posteriors, and by
training independent MLPs to detect the various fea-
tures within the classes described. The use of CRFs
allows us to explore the dependencies among feature
classes, as well as the usefulness of phone posteriors
versus feature classes as inputs.
3 Experimental Setup
We use the TIMIT speech corpus for all training and
testing (Garofolo et al, 1993). The acoustic data
is manually labeled at the phonetic level, and we
propagate this phonetic label information to every
frame of data. For the feature analyses, we employ
a lookup table that defines each phone in terms of
8 feature classes, as shown in Table 1. We extract
acoustic features in the form of 12th order PLP fea-
tures plus delta coefficients. We then use these as
inputs to several sets of neural networks using the
ICSI QuickNet MLP neural network software (John-
son, 2004), with the 39 acoustic features as input, a
varying number of phone or feature class posteriors
as output, and 1000 hidden nodes.
4 Joint Phone Posteriors vs. Independent
Feature Posteriors
The first experiment contrasts joint versus indepen-
dent feature modeling within the CRF system. We
compare a set of phonological feature probabilities
derived from the phone posteriors (a joint model)
with MLP phone posteriors and with independently
trained MLP phonological feature posteriors.
The inputs to the first CRF are sets of 61 state fea-
ture functions from the phonemic MLP posteriors,
each function is an estimate of the posterior proba-
14
Input Type. Phn. Accuracy Phn. Correct
Phones 67.27 68.77
Features 65.25 66.65
Phn. ? Feat. 66.45 67.94
Table 2: Results for Exp. 1: Phone and feature pos-
teriors as input to the CRF phone recognition
bility of one phone. The inputs to the second CRF
model are sets of 44 functions corresponding to the
phonological features listed in Table 1. The CRF
models are trained to associate these feature func-
tions with phoneme labels, incorporating the pat-
terns of variation seen in the MLPs.
The results show that phone-based posteri-
ors produce better phone recognition results than
independently-trained phonological features. This
could be due in part to the larger number of param-
eters in the system, but it could also be due to the
joint modeling that occurs in the phone classifier.
In order to equalize the feature spaces, we use the
output of the phoneme classifier to derive phonolog-
ical feature posteriors. In each frame we sum the
MLP phone posteriors of all phones that contain a
given feature. For instance, in the first frame, for
the feature LOW, we sum the posterior estimates at-
tributed to the phones aa, ae and ao. This is repeated
for each feature in each frame. The CRF model is
trained on these data and tested accordingly. The re-
sults are significantly better (p?.001) than the previ-
ous features model, but are significantly worse than
the phone posteriors (p?.005).
The results of Experiment 1 confirm the hypoth-
esis of (Rajamanohar and Fosler-Lussier, 2005) that
joint modeling using several types of feature infor-
mation is superior to individual modeling in phone
recognition, where only phoneme information is
used. The difference between the phone posteriors
and individual feature posteriors seems to be related
both to the larger CRF parameter space with larger
input, and the joint modeling provided by phone
posteriors.
5 Phonological Feature Class Analysis
In the second experiment, we examine the influence
of each feature class on the accuracy of the recog-
nizer. We iteratively remove the set of state fea-
ture functions corresponding to each feature class
Class Removed Feats. Phn. Acc. Phn. Corr.
None 44 65.25 66.65
Sonority 39 65.15 66.58
Voice 41 63.60* 65.03*
Manner 36 58.92* 60.60*
Place 35 53.22* 55.13*
Height 38 62.58* 64.07*
Front 39 64.51* 65.95*
Round 39 65.19 66.64
Tense 41 64.20* 65.65*
* p?.05, different from no features removed
Table 3: Results of Exp. 2: Removing feature
classes from the input
from the input to the CRF. The original functions
are the output of the independently-trained feature
class MLPs. The phone recognition accuracy for the
CRF having removed each class is shown in Table 3.
In Table 4 we show how removing each feature class
affects the labeling of vowels and consonants.
Manner provides an example of the influence of a
single feature class. Both the Accuracy and Correct-
ness scores decrease significantly when features as-
sociated with Manner are removed. Manner features
distinguish consonants but not vowels, so the effect
is concentrated on the recognition of consonants.
The results of Experiment 2 show that certain fea-
ture classes are redundant from the point of view of
phone recognition. In English, Round is correlated
with Front. When we remove Round, the phonemes
remain uniquely identified by the other classes. The
same is true for the Sonority class. The results show
that the inclusion of these redundant features is not
detrimental to the recognition accuracy. Accuracy
and Correctness improve non-significantly when the
redundant features are included.
Clearly, the ?independent? phonological feature
streams are not truly independent. Otherwise, per-
formance would decrease overall as we removed
each feature class, assuming predictiveness.
Removal of Place causes a slight worsening of
recognition of vowels. This is surprising, because
Place does not characterize vowels. An analysis of
the MLP activations showed that the detector for
Place=N/A is a stronger indicator for vowels than
is the Sonority=Vowel detector. This is especially
true for the vowel ax, which is frequent in the data,
15
Class Removed Percent Correct:
Vowels Consonants
None 62.68 68.91
Sonority 62.18 69.08
Voice 62.39 66.53*
Manner 61.84 59.89*
Place 60.77* 51.94*
Height 55.92* 68.69
Frontness 60.80* 68.87
Roundness 62.25 69.13
Tenseness 60.15* 68.76
* p?.05, different from no features removed
Table 4: Effect of removing each feature class on
recognition accuracy of vowels and consonants
thus greatly influences the vowel recognition statis-
tic. Removing the Place detectors leads to a loss in
vowel vs. consonant information. This results in an
increased number of consonant for vowel substitu-
tions (from 560 to 976), thus a decrease in vowel
recognition accuracy.
Besides extending the findings in (Rajamanohar
and Fosler-Lussier, 2005), this provides a cautionary
tale for incorporating redundant phonological fea-
ture estimators into ASR: these systems need to be
able to handle correlated input, either by design (as
in a CRF), through full or semi-tied covariance ma-
trices in HMMs, or by including the appropriate sta-
tistical dependencies in DBNs.
6 Summary
We have shown the effect of using joint model-
ing of phonetic feature information in conjunction
with the use of CRFs as a discriminative classifier.
Phonetic posteriors, as joint models of phonological
features, provide superior phone recognition perfor-
mance over independently-trained phonological fea-
ture models. We also find that redundant features are
often modeled well within the CRF framework.
7 Acknowledgments
The authors thank the International Computer Sci-
ence Institute for providing the neural network soft-
ware. The authors also thank four anonymous re-
viewers. This work was supported by NSF ITR
grant IIS-0427413; the opinions and conclusions ex-
pressed in this work are those of the authors and not
of any funding agency.
References
S. Chang, S. Greenberg, and M. Wester. 2001. An eli-
tist approach to articulatory-acoustic feature classifica-
tion. In Interspeech.
J. Frankel, M. Wester, and S. King. 2004. Articulatory
feature recognition using dynamic bayesian networks.
In ICSLP.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and
N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic
continuous speech corpus. Technical Report NISTIR
4930, National Institute of Standards and Technology,
Gaithersburg, MD, February. Speech Data published
on CD-ROM: NIST Speech Disc 1-1.1, October 1990.
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. In Interspeech.
M. Hasegawa-Johnson et al 2005. Landmark-based
speech recognition: Report of the 2004 Johns Hopkins
Summer Workshop. In ICASSP.
D. Johnson. 2004. ICSI Quicknet software package.
http://www.icsi.berkeley.edu/Speech/qn.html.
K. Kirchhoff. 1999. Robust Speech Recognition Using
Articulatory Information. Ph.D. thesis, University of
Bielefeld.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
B. Launay, O. Siohan, A. C. Surendran, and C.-H. Lee.
2002. Towards knowledge based features for large vo-
cabulary automatic speech recognition. In ICASSP.
J. Li and C.-H. Lee. 2005. On designing and evaluating
speech event detectors. In Interspeech.
K. Livescu. 2005. Feature-Based Pronunciation Model-
ing for Automatic Speech Recognition. Ph.D. thesis,
MIT.
J. Morris and E. Fosler-Lussier. 2006. Combining pho-
netic attributes using conditional random fields. In In-
terspeech.
M. Rajamanohar and E. Fosler-Lussier. 2005. An evalu-
ation of hierarchical articulatory feature detectors. In
IEEE Automatic Speech Recogntion and Understand-
ing Workshop.
O. Sharenborg, V. Wan, and R.K. Moore. 2006. Cap-
turing fine-phonetic variation in speech through auto-
matic classification of articulatory features. In ITRW
on Speech Recognition and Intrinsic Variation.
16
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 725?728,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Investigations into the Crandem Approach to Word Recognition
Rohit Prabhavalkar, Preethi Jyothi, William Hartmann, Jeremy Morris, and Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
{prabhava,jyothi,hartmanw,morrijer,fosler}@cse.ohio-state.edu
Abstract
We suggest improvements to a previously pro-
posed framework for integrating Conditional
Random Fields and Hidden Markov Models,
dubbed a Crandem system (2009). The pre-
vious authors? work suggested that local la-
bel posteriors derived from the CRF were too
low-entropy for use in word-level automatic
speech recognition. As an alternative to the
log posterior representation used in their sys-
tem, we explore frame-level representations
derived from the CRF feature functions. We
also describe a weight normalization transfor-
mation that leads to increased entropy of the
CRF posteriors. We report significant gains
over the previous Crandem system on the Wall
Street Journal word recognition task.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) have recently emerged as a promising
new paradigm in the domain of Automatic Speech
Recognition (ASR). Unlike Hidden Markov Mod-
els (HMMs), CRFs are direct discriminative models:
they predict the probability of a label sequence con-
ditioned on the input. As a result, CRFs can capture
long-range dependencies in the data and avoid the
need for restrictive independence assumptions. Vari-
ants of CRFs have been successfully used in phone
recognition tasks (Gunawardana et al, 2005; Morris
and Fosler-Lussier, 2008; Hifny and Renals, 2009).
While the improvements in the phone recognition
task are encouraging, recent efforts have been di-
rected towards extending the CRF paradigm to the
word recognition level (Zweig and Nguyen, 2009;
Morris and Fosler-Lussier, 2009). The Crandem
system (Morris and Fosler-Lussier, 2009) is one of
the promising approaches in this regard. The Cran-
dem system is directly inspired by the techniques
of the Tandem system (Hermansky et al, 2000),
where phone-label posterior estimates produced by
a Multi-Layer Perceptron (MLP) are transformed
into a suitable acoustic representation for a standard
HMM. In both systems, the frame-based log poste-
rior vector of P (phone|acoustics) over all phones is
decorrelated using the Karhunen-Loeve (KL) trans-
form; unlike MLPs, CRFs take into account the en-
tire label sequence when computing local posteriors.
However, posterior estimates from the CRF tend to
be overconfident compared to MLP posteriors (Mor-
ris and Fosler-Lussier, 2009).
In this paper, we analyze the interplay between
the various steps involved in the Crandem process.
Is the local posterior representation from the CRF
the best representation? Given that the CRF poste-
rior estimates can be overconfident, what transfor-
mations to the posteriors are appropriate?
In Section 2 we briefly describe CRFs and the
Crandem framework. We suggest techniques for im-
proving Crandem word recognition performance in
Section 3. Details of experiments and our results are
discussed in Sections 4 and 5 respectively. We con-
clude with a discussion of future work in Section 6.
2 CRFs and the Crandem System
Conditional random fields (Lafferty et al, 2001) ex-
press the probability of a label sequence Q condi-
tioned on the input data X as a log-linear sum of
725
weighted feature functions,
p(Q|X) =
exp
P
t
P
j ?jsj(qt, X) +
P
j ?jfj(qt?1, qt, X)
Z(X)
(1)
where sj(?) and fj(?) are known as state feature
functions and transition feature functions respec-
tively, and ?j and ?j are the associated weights.
Z(X) is a normalization term that ensures a valid
probability distribution. Given a set of labeled ex-
amples, the CRF is trained to maximize the con-
ditional log-likelihood of the training set. The
log-likelihood is concave over the entire parameter
space, and can be maximized using standard convex
optimization techniques (Lafferty et al, 2001; Sha
and Pereira, 2003). The local posterior probability
of a particular label can be computed via a forward-
backward style algorithm. Mathematically,
p(qt = q|X) =
?t(q|X)?t(q|X)
Z(X)
(2)
where ?t(q|X) and ?t(q|X) accumulate contribu-
tions associated with possible assignments of la-
bels before and after the current time-step t. The
Crandem system utilizes these local posterior val-
ues from the CRF analogously to the way in which
MLP-posteriors are treated in the Tandem frame-
work (Hermansky et al, 2000), by applying a log
transformation to the posteriors. These transformed
outputs are then decorrelated using a KL-transform
and then dimensionality-reduced to be used as a re-
placement for MFCCs in a HMM system. While
the MLP is usually reduced to 39 dimensions, the
standard CRF benefits from a higher dimensionality
reduction (to 19 dimensions). The decorrelated out-
puts are then used as an input representation for a
conventional HMM system.
3 Improving Crandem Recognition
Results
Morris and Fosler-Lussier (2009) indicate that the
local posterior outputs from the CRF model pro-
duces features that are more heavily skewed to the
dominant phone class than the MLP system, leading
to an increase in word recognition errors. In order
to correct for this, we perform a non-linear trans-
formation on the local CRF posterior representa-
tion before applying a KL-transform and subsequent
stages. Specifically, we normalize all of the weights
?j and ?j in Equation 1 by a fixed positive constant
n to obtain normalized weights ??j and ?
?
j . We note
that the probability of a label sequence computed us-
ing the transformed weights, p?(Q|X), is equivalent
to taking the nth-root of the CRF probability com-
puted using the unnormalized weights, with a new
normalization term Z ?(X)
p?(Q|X) =
p(Q|X)1/n
Z ?(X)
(3)
where, p(Q|X) is as defined in Equation 1. Also
observe that the monotonicity of the nth-root func-
tion ensures that if p(Q1|X) > p(Q2|X) then
p?(Q1|X) > p?(Q2|X). In other words, the rank
order of the n-best phone recognition results are not
impacted by this change. The transformation does,
however, increase the entropy between the domi-
nant class from the CRF and its competitors, since
p?(Q|X) < p(Q|X). As we shall discuss in Section
5, this transformation helps improve word recogni-
tion performance in the Crandem framework.
Our second set of experiments are based on the
following observation regarding the CRF posteriors.
As can be seen from Equation 2, the CRF posteri-
ors involve a global normalization over the entire ut-
terance as opposed to the local normalization of the
MLP posteriors in the output softmax layer. This
motivates the use of representations derived from
the CRF that are ?local? in some sense. We there-
fore propose two alternative representations that are
modeled along the lines of the linear outputs from an
MLP. The first uses the sum of the state feature func-
tions, to obtain a vector f state(X, t) for each time
step t and input utterance X of length |Q| dimen-
sions, where Q is the set of possible phone labels
f state(X, t) =
?
?
?
j
?jsj(q,X)
?
?
T
?q ? Q
(4)
where q is a particular phone label. Note that the
lack of an exponential term in this representation en-
sures that the representation is less ?spiky? than the
CRF posteriors. Additionally, the decoupling of the
representation from the transition feature functions
could potentially allow the system to represent rel-
726
ative ambiguity between multiple phones hypothe-
sized for a given frame.
The second ?local? representation that we experi-
mented with incorporates the CRF transition feature
functions as follows. For each utterance X we per-
form a Viterbi decoding of the most likely state se-
quence Qbest = argmaxQ{p(Q|X)} hypothesized
for the utterance X . We then augmented the state
feature representation with the sum of the transition
features corresponding to the phone label hypothe-
sized for the previous frame (qbestt?1) to obtain a vector
f trans(X, t) of length |Q|,
f trans(X, t) =
"
X
j
?jsj(q,X) +
X
j
?jfj(q
best
t?1 , q,X)
#T
(5)
As a final note, following (Morris and Fosler-
Lussier, 2009), our CRF systems are trained using
the linear outputs of MLPs as its state feature func-
tions and transition biases as the transition feature
functions. Hence, f state is a linear transformation of
the MLP linear outputs down to |Q| dimensions.1
Both f state and f trans can thus be viewed as an im-
plicit mapping performed by the CRF of the in-
put feature function dimensions down to |Q| dimen-
sions. Note that the CRF implicitly uses informa-
tion concerning the underlying phone labels unlike
dimensionality reduction using KL-transform.
4 Experimental Setup
To evaluate our proposed techniques, we carried
out word recognition experiments on the speaker-
independent portion of the Wall Street Journal 5K
closed vocabulary task (WSJ0). Since the corpus is
not phonetically transcribed, we first trained a stan-
dard HMM recognition system using PLP features
and produced phonetic transcriptions by force align-
ing the training data. These were used to train an
MLP phone classifier with a softmax output layer,
using a 9-frame window of PLPs with 4000 hidden
layer units to predict one of the 41 phone labels (in-
cluding silence and short pause). The linear outputs
of the MLP were used to train a baseline Tandem
system. We then trained a CRF using the MLP lin-
ear outputs as its state feature functions. We extract
1We note that our system uses an additional state bias feature
that has a fixed value of 1. However, since this is a constant
term, it has no role to play in the derived representation.
System Accuracy (%)
Crandem-baseline 89.4%
Tandem-baseline 91.8%
Crandem-NormMax 91.4%
Crandem-Norm5 92.1%
Crandem-state 91.7%
Crandem-trans 91.0%
Table 1: Word recognition results on the WSJ0 task
local posteriors as well as the two ?local? representa-
tions described in Section 3. These input represen-
tations were then normalized at the utterance level,
before applying a KL-transformation to decorrelate
them and reduce dimensionality to 39 dimensions.
Finally, each of these representations was used to
train a HMM system with intra-word triphones and
16 Gaussians per mixture using the Hidden Markov
Model Toolkit (Young et al, 2002).
5 Results
Results for each of the experiments described in
Section 4 are reported in Table 1 on the 330-
sentence standard 5K non-verbalized test set. The
Crandem-baseline represents the system of (Mor-
ris and Fosler-Lussier, 2009). Normalizing the
CRF weights of the system by either the weight
with largest absolute value (CRF-NormMax) or by
5 (tuned on the development set) leads to signif-
icant improvements (p ? 0.005) over the Cran-
dem baseline. Similarly, using either the state fea-
ture sum (Crandem-state) or the representation aug-
mented with the transition features (Crandem-trans)
leads to significant improvements (p ? 0.005) over
the Crandem baseline. Note that the performance of
these systems is comparable to the Tandem baseline.
To further analyze the results obtained using the
state feature sum representations and the Tandem
baseline, we compute the mean distance for each
phone HMM from every other phone HMM ob-
tained at the end of the GMM-HMM training phase.
The distance between two HMMs is computed as a
uniformly weighted sum of the average distances be-
tween the GMMs of a one-to-one alignment of states
corresponding to the two HMMs. GMM distances
are computed using a 0.5-weighted sum of inter-
dispersions normalized by self-dispersions (Wang et
727
Figure 1: Normalized mean distances for each of the phone models from every other phone model trained using the
Tandem MLP baseline and the state feature sum representation.
al., 2004). Distances between monomodal Gaus-
sian distributions were computed using the Bhat-
tacharyya distance measure. The phone HMM dis-
tances are normalized using the maximum phone
distance for each system. As can be seen in Figure
1, the mean distances obtained from the state feature
sum representation are consistently greater than the
corresponding distances in the Tandem-MLP sys-
tem, indicating larger separability of the phones in
the feature space. Similar trends were seen with the
transition feature sum representation.
6 Conclusions and Future Work
In this paper, we report significant improvements
over the Crandem baseline. The weight normaliza-
tion experiments confirmed the hypothesis that in-
creasing the entropy of the CRF posteriors leads to
better word-level recognition. Our experiments with
directly extracting frame-level representations from
the CRF reinforce this conclusion. Although our ex-
periments with the systems using the state feature
sum and transition feature augmented representation
did not lead to improvements over the Tandem base-
line, the increased separability of the phone models
trained using these representations is encouraging.
In the future, we intend to examine techniques by
which these representations could be used to further
improve word recognition results.
Acknowledgement: The authors gratefully ac-
knowledge support by NSF grants IIS-0643901 and
IIS-0905420 for this work.
References
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. Interspeech.
H. Hermansky, D. Ellis, and S. Sharma. 2000. Tan-
dem connectionist feature stream extraction for con-
ventional hmm systems. ICASSP.
Y. Hifny and S. Renals. 2009. Speech recognition using
augmented conditional random fields. IEEE Trans-
actions on Audio, Speech, and Language Processing,
17(2):354?365.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. ICML.
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3):617?628.
J. Morris and E. Fosler-Lussier. 2009. Crandem: Con-
ditional random fields for word recognition. Inter-
speech.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. NAACL.
Xu Wang, Peng Xuan, and Wang Bingxi. 2004. A gmm-
based telephone channel classification for mandarin
speech recognition. ICSP.
S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2002. The HTK Book. Cambridge Uni-
versity Press.
G. Zweig and P. Nguyen. 2009. A segmental crf ap-
proach to large vocabulary continuous speech recogni-
tion. ASRU.
728
