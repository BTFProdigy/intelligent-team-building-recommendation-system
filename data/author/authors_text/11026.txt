Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 514?522,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
HTM: A Topic Model for Hypertexts
Congkai Sun?
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
martinsck@hotmail.com
Bin Gao
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
bingao@microsoft.com
Zhenfu Cao
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
zfcao@cs.sjtu.edu.cn
Hang Li
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
hangli@microsoft.com
Abstract
Previously topic models such as PLSI (Prob-
abilistic Latent Semantic Indexing) and LDA
(Latent Dirichlet Allocation) were developed
for modeling the contents of plain texts. Re-
cently, topic models for processing hyper-
texts such as web pages were also proposed.
The proposed hypertext models are generative
models giving rise to both words and hyper-
links. This paper points out that to better rep-
resent the contents of hypertexts it is more es-
sential to assume that the hyperlinks are fixed
and to define the topic model as that of gen-
erating words only. The paper then proposes
a new topic model for hypertext processing,
referred to as Hypertext Topic Model (HTM).
HTM defines the distribution of words in a
document (i.e., the content of the document)
as a mixture over latent topics in the document
itself and latent topics in the documents which
the document cites. The topics are further
characterized as distributions of words, as in
the conventional topic models. This paper fur-
ther proposes a method for learning the HTM
model. Experimental results show that HTM
outperforms the baselines on topic discovery
and document classification in three datasets.
1 Introduction
Topic models are probabilistic and generative mod-
els representing contents of documents. Examples
of topic models include PLSI (Hofmann, 1999) and
LDA (Blei et al, 2003). The key idea in topic mod-
eling is to represent topics as distributions of words
* This work was conducted when the first author visited
Microsoft Research Asia as an intern.
and define the distribution of words in document
(i.e., the content of document) as a mixture over hid-
den topics. Topic modeling technologies have been
applied to natural language processing, text min-
ing, and information retrieval, and their effective-
ness have been verified.
In this paper, we study the problem of topic mod-
eling for hypertexts. There is no doubt that this is
an important research issue, given the fact that more
and more documents are available as hypertexts cur-
rently (such as web pages). Traditional work mainly
focused on development of topic models for plain
texts. It is only recently several topic models for pro-
cessing hypertexts were proposed, including Link-
LDA and Link-PLSA-LDA (Cohn and Hofmann,
2001; Erosheva et al, 2004; Nallapati and Cohen,
2008).
We point out that existing models for hypertexts
may not be suitable for characterizing contents of
hypertext documents. This is because all the models
are assumed to generate both words and hyperlinks
(outlinks) of documents. The generation of the latter
type of data, however, may not be necessary for the
tasks related to contents of documents.
In this paper, we propose a new topic model for
hypertexts called HTM (Hypertext Topic Model),
within the Bayesian learning approach (it is simi-
lar to LDA in that sense). In HTM, the hyperlinks
of hypertext documents are supposed to be given.
Each document is associated with one topic distribu-
tion. The word distribution of a document is defined
as a mixture of latent topics of the document itself
and latent topics of documents which the document
cites. The topics are further defined as distributions
514
of words. That means the content (topic distribu-
tions for words) of a hypertext document is not only
determined by the topics of itself but also the top-
ics of documents it cites. It is easy to see that HTM
contains LDA as a special case. Although the idea of
HTM is simple and straightforward, it appears that
this is the first work which studies the model.
We further provide methods for learning and in-
ference of HTM. Our experimental results on three
web datasets show that HTM outperforms the base-
line models of LDA, Link-LDA, and Link-PLSA-
LDA, in the tasks of topic discovery and document
classification.
The rest of the paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed HTM model and its learning and infer-
ence methods. Experimental results are presented in
Section 4. Conclusions are made in the last section.
2 Related Work
There has been much work on topic modeling. Many
models have been proposed including PLSI (Hof-
mann, 1999), LDA (Blei et al, 2003), and their
extensions (Griffiths et al, 2005; Blei and Lafferty,
2006; Chemudugunta et al, 2007). Inference and
learning methods have been developed, such as vari-
ational inference (Jordan et al, 1999; Wainwright
and Jordan, 2003), expectation propagation (Minka
and Lafferty, 2002), and Gibbs sampling (Griffiths
and Steyvers, 2004). Topic models have been uti-
lized in topic discovery (Blei et al, 2003), document
retrieval (Xing Wei and Bruce Croft, 2006), docu-
ment classification (Blei et al, 2003), citation analy-
sis (Dietz et al, 2007), social network analysis (Mei
et al, 2008), and so on. Most of the existing models
are for processing plain texts. There are also models
for processing hypertexts, for example, (Cohn and
Hofmann, 2001; Nallapati and Cohen, 2008; Gru-
ber et al, 2008; Dietz et al, 2007), which are most
relevant to our work.
Cohn and Hofmann (2001) introduced a topic
model for hypertexts within the framework of PLSI.
The model, which is a combination of PLSI and
PHITS (Cohn and Chang, 2000), gives rise to both
the words and hyperlinks (outlinks) of the document
in the generative process. The model is useful when
the goal is to understand the distribution of links
as well as the distribution of words. Erosheva et
al (2004) modified the model by replacing PLSI with
LDA. We refer to the modified mode as Link-LDA
and take it as a baseline in this paper. Note that the
above two models do not directly associate the top-
ics of the citing document with the topics of the cited
documents.
Nallapati and Cohn (2008) proposed an extension
of Link-LDA called Link-PLSA-LDA, which is an-
other baseline in this paper. Assuming that the cit-
ing and cited documents share similar topics, they
explicitly model the information flow from the cit-
ing documents to the cited documents. In Link-
PLSA-LDA, the link graph is converted into a bi-
partite graph in which links are connected from cit-
ing documents to cited documents. If a document
has both inlinks and outlinks, it will be duplicated
on both sides of the bipartite graph. The generative
process for the citing documents is similar to that of
Link-LDA, while the cited documents have a differ-
ent generative process.
Dietz et al(2007) proposed a topic model for ci-
tation analysis. Their goal is to find topical influ-
ence of publications in research communities. They
convert the citation graph (created from the publica-
tions) into a bipartite graph as in Link-PLSA-LDA.
The content of a citing document is assumed to be
generated by a mixture over the topic distribution
of the citing document and the topic distributions of
the cited documents. The differences between the
topic distributions of citing and cited documents are
measured, and the cited documents which have the
strongest influence on the citing document are iden-
tified.
Note that in most existing models described above
the hyperlinks are assumed to be generated and link
prediction is an important task, while in the HTM
model in this paper, the hyperlinks are assumed to
be given in advance, and the key task is topic iden-
tification. In the existing models for hypertexts, the
content of a document (the word distribution of the
document) are not decided by the other documents.
In contrast, in HTM, the content of a document is
determined by itself as well as its cited documents.
Furthermore, HTM is a generative model which can
generate the contents of all the hypertexts in a col-
lection, given the link structure of the collection.
Therefore, if the goal is to accurately learn and pre-
515
Table 1: Notations and explanations.
T Number of topics
D Documents in corpus
D Number of documents
?? , ?? Hyperparameters for ? and ?
? Hyperparameter to control the weight between
the citing document and the cited documents
? Topic distributions for all documents
? Word distribution for topic
b, c, z Hidden variables for generating word
d document (index)
wd Word sequence in document d
Nd Number of words in document d
Ld Number of documents cited by document d
Id Set of cited documents for document d
idl Index of lth cited document of document d
?d Distribution on cited documents of document d
?d Topic distribution associated with document d
bdn Decision on way of generating nth word in doc-
ument d
cdn Cited document that generates nth word in doc-
ument d
zdn Topic of nth word in document d
dict contents of documents, the use of HTM seems
more reasonable.
3 Hypertext Topic Model
3.1 Model
In topic modeling, a probability distribution of
words is employed for a given document. Specifi-
cally, the probability distribution is defined as a mix-
ture over latent topics, while each topic is future
characterized by a distribution of words (Hofmann,
1999; Blei et al, 2003). In this paper, we introduce
an extension of LDA model for hypertexts. Table 1
gives the major notations and their explanations.
The graphic representation of conventional LDA
is given in Figure 1(a). The generative process of
LDA has three steps. Specifically, in each document
a topic distribution is sampled from a prior distribu-
tion defined as Dirichlet distribution. Next, a topic is
sampled from the topic distribution of the document,
which is a multinominal distribution. Finally, a word
is sampled according to the word distribution of the
topic, which also forms a multinormal distribution.
The graphic representation of HTM is given in
Figure 1(b). The generative process of HTM is de-
scribed in Algorithm 1. First, a topic distribution
is sampled for each document according to Dirich-
let distribution. Next, for generating a word in a
document, it is decided whether to use the current
Algorithm 1 Generative Process of HTM
for each document d do
Draw ?d ? Dir(??).
end for
for each word wdn do
if Ld > 0 then
Draw bdn ? Ber(?)
Draw cdn ? Uni(?d)
if bdn = 1 then
Draw zdn ? Multi(?d)
else
Draw zdn ? Multi(?Idcdn )end if
else
Draw a topic zdn ? Multi(?d)
end if
Draw a word wdn ? P (wdn | zdn, ?)
end for
document or documents which the document cites.
(The weight between the citing document and cited
documents is controlled by an adjustable hyper-
parameter ?.) It is also determined which cited doc-
ument to use (if it is to use cited documents). Then, a
topic is sampled from the topic distribution of the se-
lected document. Finally, a word is sampled accord-
ing to the word distribution of the topic. HTM natu-
rally mimics the process of writing a hypertext docu-
ment by humans (repeating the processes of writing
native texts and anchor texts).
The formal definition of HTM is given be-
low. Hypertext document d has Nd words
wd = wd1 ? ? ?wdNd and Ld cited documents Id =
{id1, . . . , idLd}. The topic distribution of d is ?d
and topic distributions of the cited documents are
?i, i ? Id. Given ?, ?, and ?, the conditional proba-
bility distribution of wd is defined as:
p(wd|?, ?, ?) =
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?idcdn )
1?bdnp(wdn|zdn, ?).
Here ?d, bdn, cdn, and zdn are hidden vari-
ables. When generating a word wdn, bdn determines
whether it is from the citing document or the cited
documents. cdn determines which cited document it
516
is when bdn = 0. In this paper, for simplicity we as-
sume that the cited documents are equally likely to
be selected, i.e., ?di = 1Ld .Note that ? represents the topic distributions of
all the documents. For any d, its word distribution
is affected by both ?d and ?i, i ? Id. There is a
propagation of topics from the cited documents to
the citing document through the use of ?i, i ? Id.
For a hypertext document d that does not have
cited documents. The conditional probability dis-
tribution degenerates to LDA:
p(wd|?d, ?) =
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?).
By taking the product of the marginal probabil-
ities of hypertext documents, we obtain the condi-
tional probability of the corpus D given the hyper-
parameters ?, ??, ?,
p(D|?, ??, ?) =
? D?
d=1
p(?d|??)
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?Idcdn )
1?bdn
p(wdn|zdn, ?)d?. (1)
Note that the probability function (1) also covers the
special cases in which documents do not have cited
documents.
In HTM, the content of a document is decided by
the topics of the document as well as the topics of
the documents which the document cites. As a result
contents of documents can be ?propagated? along the
hyperlinks. For example, suppose web page A cites
page B and page B cites page C, then the content of
page A is influenced by that of page B, and the con-
tent of page B is further influenced by the content
of page C. Therefore, HTM is able to more accu-
rately represent the contents of hypertexts, and thus
is more useful for text processing such as topic dis-
covery and document classification.
3.2 Inference and Learning
An exact inference of the posterior probability of
HTM may be intractable, we employ the mean field
variational inference method (Wainwright and Jor-
dan, 2003; Jordan et al, 1999) to conduct approxi-
mation. Let I[?] be an indicator function. We first
define the following factorized variational posterior
distribution q with respect to the corpus:
q =
D?
d=1
q(?d|?d)
Nd?
n=1
(
q(xdn|?dn)(q(cdn|?dn)
)I[Ld>0]
q(zdn|?dn) ,
where ?, ?, ?, and ? denote free variational parame-
ters. Parameter ? is the posterior Dirichlet parameter
corresponding to the representations of documents
in the topic simplex. Parameters ?, ?, and ? cor-
respond to the posterior distributions of their asso-
ciated random variables. We then minimize the KL
divergence between q and the true posterior proba-
bility of the corpus by taking derivatives of the loss
function with respect to variational parameters. The
solution is listed as below.
Let ?iv be p(wvdn = 1|zi = 1) for the word v. If
Ld > 0, we have
E-step:
?di = ??i +
Nd?
n=1
?dn?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{?dnEq [log (?di) |?d]
+ (1? ?dn)
Ld?
l=1
?dnlEq [log (?Idli) |?Idl ]
} .
?dn =
(
1 +
(
exp{
k?
i=1
((?dniEq[log(?di)|?d]
?
Ld?
l=1
?dnl?dniEq[log(?Idli)|?Idl ]
)
+ log ?? log (1? ?)}
)?1)?1
.
517
zw?
???
??
T Nd D
(a) LDA
w
??
?? ??
D T
d
z
D
Nd
?
c
?
b
Id
(b) HTM
z
w?
?
??
??
T Nd
D
z
d? ?
Ld
(c) Link-LDA
z
w
?
?
??
??
T
Nd
z
?
Ld
pi
z
wd?
N M
Cited Documents Citing Documents
d
(d) Link-PLSA-LDA
Figure 1: Graphical model representations
?dnl ? ?dl exp{(1? ?dn)
k?
i=1
?dniEq[log(?Idli)|?Idl ]}.
Otherwise,
?di = ??i +
Nd?
n=1
?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{Eq [log (?di) |?d]
}.
From the first two equations we can see that the
cited documents and the citing document jointly af-
fect the distribution of the words in the citing docu-
ment.
M-step:
?ij ?
D?
d=1
Nd?
n=1
?dniwjdn.
In order to cope with the data sparseness problem
due to large vocabulary, we employ the same tech-
nique as that in (Blei et al, 2003). To be specific,
we treat ? as a K ?V random matrix, with each row
being independently drawn from a Dirichlet distri-
bution ?i ? Dir(??) . Variational inference is
modified appropriately.
4 Experimental Results
We compared the performances of HTM and three
baseline models: LDA, Link-LDA, and Link-PLSA-
LDA in topic discovery and document classification.
Note that LDA does not consider the use of link in-
formation; we included it here for reference.
4.1 Datasets
We made use of three datasets. The documents in the
datasets were processed by using the Lemur Took
kit (http://www.lemurproject.org), and the low fre-
quency words in the datasets were removed.
The first dataset WebKB (available at
http://www.cs.cmu.edu/?webkb) contains six
subjects (categories). There are 3,921 documents
and 7,359 links. The vocabulary size is 5,019.
518
The second dataset Wikipedia (available at
http://www.mpi-inf.mpg.de/?angelova) contains
four subjects (categories): Biology, Physics, Chem-
istry, and Mathematics. There are 2,970 documents
and 45,818 links. The vocabulary size is 3,287.
The third dataset is ODP composed of homepages
of researchers and their first level outlinked pages
(cited documents). We randomly selected five sub-
jects from the ODP archive. They are Cognitive
Science (CogSci), Theory, NeuralNetwork (NN),
Robotics, and Statistics. There are 3,679 pages and
2,872 links. The vocabulary size is 3,529.
WebKB and Wikipedia are public datasets widely
used in topic model studies. ODP was collected by
us in this work.
4.2 Topic Discovery
We created four topic models HTM, LDA, Link-
LDA, and Link-PLSA-LDA using all the data in
each of the three datasets, and evaluated the top-
ics obtained in the models. We heuristically set the
numbers of topics as 10 for ODP, 12 for WebKB,
and 8 for Wikipedia (i.e., two times of the number
of true subjects). We found that overall HTM can
construct more understandable topics than the other
models. Figure 2 shows the topics related to the
subjects created by the four models from the ODP
dataset. HTM model can more accurately extract
the three topics: Theory, Statistic, and NN than the
other models. Both LDA and Link-LDA had mixed
topics, labeled as ?Mixed? in Figure 2. Link-PLSA-
LDA missed the topic of Statistics. Interestingly, all
the four models split Cognitive Science into two top-
ics (showed as CogSci-1 and CogSci-2), probably
because the topic itself is diverse.
4.3 Document Classification
We applied the four models in the three datasets to
document classification. Specifically, we used the
word distributions of documents created by the mod-
els as feature vectors of the documents and used the
subjects in the datasets as categories. We further
randomly divided each dataset into three parts (train-
ing, validation, and test) and conducted 3-fold cross-
validation experiments. In each trial, we trained an
SVM classifier with the training data, chose param-
eters with the validation data, and conducted evalu-
ation on classification with the test data. For HTM,
Table 2: Classification accuracies in 3-fold cross-
validation.
LDA HTM Link-LDA Link-PLSA-LDA
ODP 0.640 0.698 0.535 0.581
WebKB 0.786 0.795 0.775 0.774
Wikipedia 0.845 0.866 0.853 0.855
Table 3: Sign-test results between HTM and the three
baseline models.
LDA Link-LDA Link-PLSA-LDA
ODP 0.0237 2.15e-05 0.000287
WebKB 0.0235 0.0114 0.00903
Wikipedia 1.79e-05 0.00341 0.00424
we chose the best ? value with the validation set in
each trial. Table 2 shows the classification accura-
cies. We can see that HTM performs better than the
other models in all three datasets.
We conducted sign-tests on all the results of the
datasets. In most cases HTM performs statistically
significantly better than LDA, Link-LDA, and Link-
PLSA-LDA (p-value < 0.05). The test results are
shown in Table 3.
4.4 Discussion
We conducted analysis on the results to see why
HTM can work better. Figure 3 shows an example
homepage from the ODP dataset, where superscripts
denote the indexes of outlinked pages. The home-
page contains several topics, including Theory, Neu-
ral network, Statistics, and others, while the cited
pages contain detailed information about the topics.
Table 4 shows the topics identified by the four mod-
els for the homepage. We can see that HTM can
really more accurately identify topics than the other
models.
The major reason for the better performance by
HTM seems to be that it can fully leverage the infor-
Table 4: Comparison of topics identified by the four mod-
els for the example homepage. Only topics with proba-
bilities > 0.1 and related to the subjects are shown.
Model Topics Probabilities
LDA Mixed 0.537
HTM Theory 0.229
NN 0.278
Statistics 0.241
Link-LDA Statistics 0.281
Link-PLSA-LDA Theory 0.527
CogSci-2 0.175
519
(a) LDA
Mixed NN Robot CogSci-1 CogSci-2
statistic learn robot visual conscious
compute conference project model psychology
algorithm system file experiment language
theory neural software change cognitive
complex network code function experience
mathematics model program response brain
model international data process theory
science compute motor data philosophy
computation ieee read move science
problem proceedings start observe online
random process build perception mind
analysis computation comment effect concept
paper machine post figure physical
method science line temporal problem
journal artificial include sensory content
(b) HTM
Theory Statistics NN Robot CogSci-1 CogSci-2
compute model learn robot conscious memory
science statistic system project visual psychology
algorithm data network software experience language
theory experiment neural motor change science
complex sample conference sensor perception cognitive
computation process model code move brain
mathematics method compute program theory human
paper analysis ieee build online neuroscience
problem response international line physical journal
lecture figure proceedings board concept society
random result machine read problem trauma
journal temporal process power philosophy press
bound probable computation type object learn
graph observe artificial comment content abuse
proceedings test intelligence post view associate
(c) Link-LDA
Statistics Mixed Robot CogSci-1 CogSci-2
statistic compute robot visual conscious
model conference project model psychology
data system software experiment cognitive
analysis learn file change language
method network motor function brain
learn computation robotics vision science
sample proceedings informatik process memory
algorithm neural program perception theory
process ieee build move philosophy
bayesian algorithm board response press
application international sensor temporal online
random science power object neuroscience
distribution complex code observe journal
simulate theory format sensory human
mathematics journal control figure mind
(d) Link-PLSA-LDA
Theory NN Robot CogSci-1 CogSci-2
compute conference robot conscious model
algorithm learn code experience process
computation science project language visual
theory international typeof book data
complex system motor change experiment
science compute control make function
mathematics network system problem learn
network artificial serve brain neural
paper ieee power world system
journal intelligence program read perception
proceedings robot software case represent
random technology file than vision
system proceedings build mind response
problem machine pagetracker theory object
lecture neural robotics content abstract
Figure 2: Topics identified by four models
Radford M.Neal
Professor, Dept. of Statistics and Dept. of Computer Science, University of Toronto
I?m currently highlighting the following :
? A new R function for performing univariate slice sampling.1
? A workshop paper on Computing Likelihood Functions for High-Energy Physics
Experiments when Distributions are Defined by Simulators with Nuisance Parameters.2
? Slides from a talk at the Third Workshop on Monte Carlo Methods on
?Short-Cut MCMC: An Alternative to Adaptation?, May 2007: Postscript, PDF.
Courses I?m teaching in Fall 2008 :
? STA 437: Methods for Multivariate Data3
? STA 3000: Advanced Theory of Statistics4
You can also find information on courses I?ve taught in the past.5
You can also get to information on :
? Research interests6 (with pointers to publications)
? Current and former graduate students7
? Current and former postdocs8
? Curriculum Vitae: PostScript, or PDF.
? Full publications list9
? How to contact me10
? Links to various places11
If you know what you want already,you may wish to go directly to :
? Software available on-line12
? Papers available on-line13
? Slides from talks14
? Miscellaneous other stuff15
Information in this hierarchy was last updated 2008-06-20.
Figure 3: An example homepage: http://www.cs.utoronto.ca/? radford/
520
Table 5: Word assignment in the example homepage.
Word bdn cdn Topic Probability
mcmc 0.544 2 Stat 0.949
experiment 0.546 2 Stat 0.956
neal 0.547 8 NN 0.985
likelihood 0.550 2 Stat 0.905
sample 0.557 2 Stat 0.946
statistic 0.559 2 Stat 0.888
parameter 0.563 2 Stat 0.917
perform 0.565 2 Stat 0.908
carlo 0.568 2 Stat 0.813
monte 0.570 2 Stat 0.802
toronto 0.572 8 NN 0.969
distribution 0.578 2 Stat 0.888
slice 0.581 2 Stat 0.957
energy 0.581 13 NN 0.866
adaptation 0.591 7 Stat 0.541
teach 0.999 11 Other 0.612
current 0.999 11 Other 0.646
curriculum 0.999 11 Other 0.698
want 0.999 11 Other 0.706
highlight 0.999 10 Other 0.786
professor 0.999 11 Other 0.764
academic 0.999 11 Other 0.810
student 0.999 11 Other 0.817
contact 0.999 11 Other 0.887
graduate 0.999 11 Other 0.901
Table 6: Most salient topics in cited pages.
URL Topic Probability
2 Stat 0.690
7 Stat 0.467
8 NN 0.786
13 NN 0.776
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5
ODP
Webkb
Wiki
A
c
c
u
r
a
c
y
?
Figure 4: Classification accuracies on three datasets with
different ? values. The cross marks on the curves cor-
respond to the average values of ? in the 3-fold cross-
validation experiments.
mation from the cited documents. We can see that
the content of the example homepage is diverse and
not very rich. It might be hard for the other base-
line models to identify topics accurately. In con-
trast, HTM can accurately learn topics by the help
of the cited documents. Specifically, if the content of
a document is diverse, then words in the document
are likely to be assigned into wrong topics by the
existing approaches. In contrast, in HTM with prop-
agation of topic distributions from cited documents,
the words of a document can be more accurately as-
signed into topics. Table 5 shows the first 15 words
and the last 10 words for the homepage given by
HTM, in ascending order of bdn, which measures
the degree of influence from the cited documents on
the words (the smaller the stronger). The table also
gives the values of cdn, indicating which cited docu-
ments have the strongest influence. Furthermore, the
topics having the largest posterior probabilities for
the words are also shown. We can see that the words
?experiment?, ?sample?, ?parameter?, ?perform?, and
?energy? are accurately classified. Table 6 gives the
most salient topics of cited documents. It also shows
the probabilities of the topics given by HTM. We can
see that there is a large agreement between the most
salient topics in the cited documents and the topics
which are affected the most in the citing document.
Parameter ? is the only parameter in HTM which
needs to be tuned. We found that the performance of
HTM is not very sensitive to the values of ?, which
reflects the degree of influence from the cited doc-
uments to the citing document. HTM can perform
well with different ? values. Figure 4 shows the clas-
sification accuracies of HTM with respect to differ-
ent ? values for the three datasets. We can see that
HTM works better than the other models in most of
the cases (cf., Table 2).
5 Conclusion
In this paper, we have proposed a novel topic
model for hypertexts called HTM. Existing models
for processing hypertexts were developed based on
the assumption that both words and hyperlinks are
stochastically generated by the model. The gener-
ation of latter type of data is actually unnecessary
for representing contents of hypertexts. In the HTM
model, it is assumed that the hyperlinks of hyper-
521
texts are given and only the words of the hypertexts
are stochastically generated. Furthermore, the word
distribution of a document is determined not only
by the topics of the document in question but also
from the topics of the documents which the doc-
ument cites. It can be regarded as ?propagation?
of topics reversely along hyperlinks in hypertexts,
which can lead to more accurate representations than
the existing models. HTM can naturally mimic hu-
man?s process of creating a document (i.e., by con-
sidering using the topics of the document and at the
same time the topics of the documents it cites). We
also developed methods for learning and inferring
an HTM model within the same framework as LDA
(Latent Dirichlet Allocation). Experimental results
show that the proposed HTM model outperforms
the existing models of LDA, Link-LDA, and Link-
PLSA-LDA on three datasets for topic discovery and
document classification.
As future work, we plan to compare the HTM
model with other existing models, to develop learn-
ing and inference methods for handling extremely
large-scale data sets, and to combine the current
method with a keyphrase extraction method for ex-
tracting keyphrases from web pages.
6 Acknowledgement
We thank Eric Xing for his valuable comments on
this work.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press / Addison-
Wesley.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of machine Learning
Research, 3:993?1022.
David Blei and John Lafferty. 2005. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 12.
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd international con-
ference on Machine learning.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
David Cohn and Huan Chang. 2000. Learning to Proba-
bilistically Identify Authoritative Documents. In Pro-
ceedings of the 17rd international conference on Ma-
chine learning.
David Cohn and Thomas Hofmann. 2001. The missing
link - a probabilistic model of document content and
hypertext connectivity. In Neural Information Pro-
cessing Systems 13.
Laura Dietz, Steffen Bickel and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of the 24th international conference on Ma-
chine learning.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. In Proceedings of the National Academy of
Sciences, 101:5220?5227.
Thomas Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. In Proceedings of the National
Academy of Sciences, 101 (suppl. 1) .
Thomas Griffiths, Mark Steyvers, David Blei, and Joshua
Tenenbaum. 2005. Integrating Topics and Syntax. In
Advances in Neural Information Processing Systems,
17.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008.
Latent Topic Models for Hypertext. In Proceedings of
the 24th Conference on Uncertainty in Artificial Intel-
ligence.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Analysis. In Proceedings of the 15th Conference on
Uncertainty in Artificial Intelligence.
Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola,
and Lawrence Saul. 1999. An Introduction to Varia-
tional Methods for Graphical Models. Machine Learn-
ing, 37(2):183?233.
QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008. Topic Modeling with Network Regular-
ization. In Proceeding of the 17th international con-
ference on World Wide Web.
Thomas Minka and John Lafferty. 2002. Expectation-
Propagation for the Generative Aspect Model. In Pro-
ceedings of the 18th Conference in Uncertainty in Ar-
tificial Intelligence.
Ramesh Nallapati and William Cohen. 2008. Link-
PLSA-LDA: A new unsupervised model for topics and
influence of blogs. In International Conference for
Webblogs and Social Media.
Martin Wainwright, and Michael Jordan. 2003. Graph-
ical models, exponential families, and variational in-
ference. In UC Berkeley, Dept. of Statistics, Technical
Report, 2003.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
522
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 118?125,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Dialogue Act Recognition using Reweighted Speaker Adaptation
Congkai Sun
Institute for Creative
Technologies
12015 Waterfront Drive
Playa Vista, CA 90094-2536
csun@ict.usc.edu
Louis-Philippe Morency
Institute for Creative
Technologies
12015 Waterfront Drive
Playa Vista, CA 90094-2536
morency@ict.usc.edu
Abstract
In this work we study the effectiveness of
speaker adaptation for dialogue act recogni-
tion in multiparty meetings. First, we analyze
idiosyncracy in dialogue verbal acts by quali-
tatively studying the differences and conflicts
among speakers and by quantitively compar-
ing speaker-specific models. Based on these
observations, we propose a new approach for
dialogue act recognition based on reweighted
domain adaptation which effectively balance
the influence of speaker specific and other
speakers? data. Our experiments on a real-
world meeting dataset show that with even
only 200 speaker-specific annotated dialogue
acts, the performances on dialogue act recog-
nition are significantly improved when com-
pared to several baseline algorithms. To our
knowledge, this work is the first 1 to tackle this
promising research direction of speaker adap-
tation for dialogue act recogntion.
1 Introduction
By representing a higher level intention of utterances
during human conversation, dialogue act labels are
being used to enrich the information provided by
spoken words (Stolcke et al, 2000). Dialogue act
recognition is a preliminary step towards deep dia-
logue understanding. It plays a key role in the de-
sign of dialogue systems. Besides, Fernandez et al
(2008) find certain dialogue acts are important cues
for detecting decisions in Multi-party dialogue. In
1This paper is an extended version of a poster presented at
SemDial 2011, with new experiments and deeper analysis.
Ranganath et al (2009), dialogue acts are used as
important features for flirt detection.
Automatic dialogue act recognition is still an ac-
tive research topic. The conventional approach is to
train one generic classifier using a large corpus of
annotated utterances. One aspect that makes it so
challenging is that people can express the same idea
(or speech act) using a very different set of spoken
words. Even more, people can mean different things
with the exact same spoken words. These idiosyn-
cratic differences in dialogue acts make the learning
of generic classifiers extremely challenging. Luck-
ily, in many applications such as face-to-face meet-
ings or tele-immersion, we have access to archives
of previous interactions with the same participants.
From these archives, a small subset of spoken utter-
ances can be efficiently annotated. As we will later
show in our experiments, even a small number of an-
notated utterances can make a significant difference.
In this paper, we propose a new approach for
dialogue act recognition based on reweighted do-
main adaptation which effectively balance the influ-
ence of speaker specific and other speakers? data.
By treating each speaker as one domain, we point
out the connection between training speaker spe-
cific dialogue act classifier and supervised domain
adaptation problem. We analyze idiosyncracy in
dialogue verbal acts by qualitatively studying the
differences and conflicts among speakers and by
quantitively comparing speaker-specific models. We
present an extensive set of experiments studying the
effect of speaker adaptation on dialogue act recogn-
tion in multi-party meetings using the ICSI-MRDA
dataset (Shriberg, 2004).
118
The following section presents related work on di-
alogue act recognition and domain adaptation. Sec-
tion 3 describes the ICSI-MRDA (Shriberg, 2004)
dataset which is used in all our experiments. Sec-
tion 4 analyze idiosyncracy in dialogue acts, both
qualitatively and quantitatively. Section 5 ex-
plains our reweighting-based speaker adaptation al-
gorithm. Section 6 contains all experiments to prove
the applicability of speaker adaptation to dialogue
act recognition. Finally, inspired by the promising
results, Section 8 describes some future directions.
2 Previous Work
Automatic dialogue act recognition has been an im-
portant problem in the past decades. Different dia-
logue act labeling standards and datasets have been
provided, including Switchboard-DAMSL (Stolcke
et al, 2000), ICSI-MRDA (Shriberg, 2004) and
AMI (Carletta, 2007). Stolcke et al(2000) is one
of the first work using machine learning technique
(HMM) to automatically segment and recognize di-
alogue acts. Rangarajan et al (2009) demonstrated
well-designed prosodic n-gram features are very
helpful for Dialogue Act recognition in Maximum
Entropy model. And Ang et al(2005) explored
joint segmentation and dialogue act classification for
speech from ICSI.
Domain adaptation is a popular problem in natu-
ral language processing community due to the spar-
sity of labeled data. Jiang (Jiang, 2007) breaks
the analysis of domain adaptation problem into dis-
tributional differences in instances and classifica-
tion functions between source and target data. In
Daume?s work (2007) several domain adaptation al-
gorithms are described. Our speaker adaptation al-
gorithm is inspired by the reweighting-based adap-
tation algorithm introduced in this paper.
Recently, dialogue act adaptation has been getting
a lot of attention. Tur et al (2006) successfully use
Switchboard-DAMSL to help dialogue act recogni-
tion in ICSI-MRDA. Promising results have been
obtained by using a regression model to combine the
model weights obtained by training on Switchboard-
DAMSL and ICSI-MRDA respectively. Following
the work by Tur et al (2006), Guz et al (2009) fur-
ther studied the effectiveness of dialogue act domain
adaptation in cascaded dialogue act segmentation
and recognition system, their results prove adapta-
tion in the intermediate step (segmentation) are also
very helpful for the final output (recognition). Jeong
et al(2009) use semi-supervised boosting algorithm
to leverage labeled data from Switchboard-DAMSL
and ICSI-MRDA to help dialogue act recognition in
email and forums. Margolis et.al (2010) use a struc-
tural correspondence learning technique to adapt di-
alogue act recognition on automatic translated Span-
ish genre with the help of Switchboard-DAMSL and
ICSI-MRDA. Kolar et al (2007) explores the dif-
ference among speakers for dialogue act segmenta-
tion in ICSI-MRDA dataset. Similar to the approach
taken in Tur et al (2006), adaptation is performed
through the combination of generic speaker inde-
pendent Language Model and other speakers? Lan-
guage Model. Significant improvements have been
obtained for most of the selected speakers.
All these previous papers focused on adapting di-
alogue act models between domains and did not
address the person-specific adaptation. The only
exception was Kolar et al (2007) who explored
speaker-specific dialogue act segmentation. To our
knowledge, this paper is the first work to analyze the
effectiveness of speaker adaptation for dialogue act
recognition.
3 ICSI-MRDA Corpus
Different Dialogue Act labeling standards and
datasets have been provided in recent years, in-
cluding Switchboard-DAMSL (Stolcke et al, 2000),
ICSI-MRDA (Shriberg, 2004) and AMI (Carletta,
2007). ICSI-MRDA is the dataset for our exper-
iments because many of its meetings contain the
same speakers, thus making it more suitable for our
speaker adaptation study. The tagset in ICSI-MRDA
is adapted from DAMSL standard (damsl, 1997) by
allowing multiple tags per dialogue act. Each dia-
logue act in ICSI-MRDA has one general tag and
multiple specific tags.
ICSI-MRDA consists of 75 meetings, each
roughly an hour long. There are five categories of
meetings (three of which we are actively using in
our experiments) : Bed is about the discussion of
natural language processing and neural theories of
language, Bmr is for the discussion on ICSI meeting
corpus, Bro is on speech recognition topics and Bns
119
ID Tag Type Nb. Meetings Nb. DAs
1 mn015 Bed 15 6228
2 me010 Bed 11 5309
3 me013 Bmr 25 9753
4 mn017 Bmr 15 4059
5 fe016 Bmr 18 5500
6 me018 Bro 20 4263
7 me013 Bro 22 11928
Table 1: The 7 speakers from ICSI-MRDA dataset used
in our experiments. The table lists: the Speaker ID, orig-
inal speaker tag, the type of meeting selected for this
speaker, the number of meetings this speaker participated
and the total number of dialogue acts by this speaker.
is about network and architecture. The last category
is varies which contains all other topics.
From these 75 meetings, there are 53 unique
speakers in total, and an average of about 6 speakers
per meeting. 7 speakers2 having more than 4, 000
dialogue acts are selected for our adaptation experi-
ments. Table 1 shows the details of our 7 selected
speakers. From the word transcriptions, we cre-
ated an extended list of linguistic features per ut-
terance. From the 7 selected speakers, we com-
puted 14653 unigram features, 158884 bigram fea-
tures and 400025 trigram features.
Following the work of Shriberg et al (2004), we
use the 5 general tags in our experiments:
? Disruption indicates the current Dialogue Act
is interrupted.
? Back Channel are utterances which are not
made directly by a speaker as a response and
do not function in a way that elicits a response
either.
? Floor Mechanism are dialogue acts for grab-
bing or maintaining the floor.
? Question is for eliciting listener feed back.
? And finally, unless an utterance is completely
indecipherable or else can be further described
by a general tag, then its default status is State-
ment.
Our dataset consisted of 47040 dialogue acts. The
distribution of Dialogue Act is shown in Table 2.
2speaker me013 is split into me013-Bmr and me013-Bro to
avoid the difference introduced by meeting types.
Tag proportion
Disruption 14.73%
Back Channel 10.20%
Floor Mechanism 12.40%
Question 7.20%
Statement 55.46%
Table 2: Distribution of dialogue acts in our dataset.
4 Idiosyncrasy in Dialogue Acts
Our goal is to create a dialogue act recognition al-
gorithm that can adapt to specific speakers. Some
important questions must be studied before creat-
ing such algorithm. The first obvious one is: do
speakers really differ in their choice of words and
associated dialogue acts? Do we really see a vari-
ability on how people express their dialogue in-
tent? If the answers are yes, then we will expect
that learning a dialogue act recognizer from speaker-
specific utterances should always outperform a rec-
ognizer learned from someone else data. Section 4.1
presents a comparative experiment addressing these
questions.
To better understand the results from this com-
parative experiment, we also performed a quali-
tative analysis presented in Section 4.2 where we
look more closely at the differences between speak-
ers. These two qualitative and quantitative analysis
are building block for our adaptation algorithm pre-
sented in Section 5.
4.1 Speaker-Specific Recognizers
An important assumption when performing speaker
adaptation (or more generally domain adaptation)
is that data coming from the same speaker should
be similar than data coming from another person.
In other words, a recognizer trained on a speaker
should perform better (when tested on the same per-
son) than a recognizer trained on another speaker.
We designed an experiment to test this hypothesis.
We learned 7 speaker-specific recognizers, one
for each speaker (see Table 1). We then tested all
these recognizers on new utterances from the same
7 speakers. We looked the recognition performance
when (1) the recognizer was trained on the same
person and (2) when the recognizer was trained on
a different person. This experiments quantitatively
120
Figure 1: Effect of same-speaker data on dialogue act
recognition. We compare two approaches: (1) when a
recognizer is trained on the same person and tested on
new utterances from the same person, and (2) when the
recognizer was trained on another speaker (same test set).
We vary the amount of training data to be 200, 500,
1000, 1500 and 2000 dialogue acts. In all cases, using
speaker-specific recognizer outperforms recognizer from
other speakers.
analyze the the difference among speakers. The ex-
perimental methodology used in this experiment is
the same as the other experiments described in this
paper (see Section 6). We use the Maximum En-
tropy model(MaxEnt) for all dialogue act recogniz-
ers (Ratnaparkhi, 1996). Please refer to Section 6.2
for more details about the experimental methodol-
ogy.
Figure 1 compares the average performances
when testing on the same speaker or on some other
speaker. We vary the number of training data for
each speaker to be 200, 500, 1000, 1500 and 2000
dialogue acts. For all five cases, the recognizers
trained on the same speaker outperforms the aver-
age performance when using a recognizer from an
other person. Thus speaker specific dialogue acts
adaptation fits the assumption of domain adaptation
problems.
4.2 Speakers Differences
To better understand the problem, we look more
closely at the differences among speakers and their
use of dialogue acts. We analyze the problem
induced by speaker idiosyncrasy in dialogue acts.
During our qualitative analysis of the ICSI-MRDA
dataset, we identified three major differences ex-
plaining the performances observed in the previous
sections: dialogue act conflicts, word distribution
and dialogue act label distribution. We describe
these three differences with some examples:
Conflicts: These differences happen when two
speakers intended to express different meanings
while speaking the exact same utterance. To exam-
plify these conflicts, we computed mutual informa-
tion between a specific utterance and all dialogue act
labels. We find interesting examples where for ex-
emple the word right is the most important cue for
dialogue act question when spoken by me013-Bmr,
while right is also an important cue for dialogue act
back-channel for speaker me010-Bed. These exam-
ples suggest that conflicts exist among speakers and
simply trying to learn one generic model may not
be able to handle these conflicts. The generic model
will learn what most people mean with this utter-
ance, which may be the wrong prediction for our
specific speaker.
Word distribution: People have their own vocab-
ulary. Although many words are the same, how of-
ten one person use each word will vary. Although we
may not have direct conflict here, the problem can
also be serious. The learning algorithm may mis-
leadingly focus on optimizing the weights for certain
words which are not important(e.g., words that oc-
cur more often in other speakers? dialogue acts than
his/her own) while under-estimating the important
words for this speaker. This observation suggests
that our adaptation should take into account word
distribution.
Label Distribution: Another interesting observa-
tion is to look at the distribution of dialogue act la-
bels for different speakers. Table 2 shows the aver-
age distribution over all 7 speakers. When looking
more closely at each speaker, we find some interest-
ing differences. For example, speaker 1 made state-
ments 61% of the time while speaker 4 made 49% of
the time. While this difference may not look signif-
icant, these changes can definitely affect the recog-
nition performance. So the adaptation model should
also take into account the dialogue act label distri-
bution.
5 Reweighted Speaker Adaptation
Based on the observations described in the previous
sections, we implement a simple reweighting-based
121
domain adaptation algorithm mentioned in (Daume,
2007) based on Maximum Entropy model (MaxEnt)
(Ratnaparkhi, 1996). MaxEnt model is a popular
and efficient discriminative model which can effec-
tively accommodate large numbers of features. All
the unigram, bigram and trigram features are used
as input to the maxEnt model, the output is the di-
alogue act label. MaxEnt model maximizes the log
conditional likelihood of all samples:
Loss =
N?
1
log(p(yn|xn)) (1)
where N is the number of samples for the training
data. xn represents the feature of the nth sample and
yn is the label. The conditional likelihood is defined
as
p(y|x) = exp(
?
i
?ifi(x, y))/Z(x) (2)
where Z(x) is the normalization factor and fi(x, y)
are the n-gram features described in Section 3.
When applied to our problem of speaker adapta-
tion, the reweighting adaptation model can be for-
mally defined as
Loss = w
S?
n=1
log(p(yn|xn))+
O?
m=1
log(p(ym|xm))
(3)
where S is the number of labeled speaker-specific
dialogue acts, O is the number for other speakers?
labeled dialogue acts. For each speaker, we train
one speaker-specific classifier by varying the distri-
bution of training data. We reweight the importance
of speaker specific dialogue acts versus other speak-
ers? labeled dialogue acts in the training data. The
optimal weight parameter w is automatically esti-
mated through validation.
It is worth mentioning a specific instance of the
reweighting adaptation algorithm. When w is set to
1, the reweighting adaptation algorithm is equivalent
to simply training a MaxEnt model by putting the
speaker-specific and generic data samples together
as training data. In our experiments, we will com-
pare the reweighting adaptation approach with this
simpler approach, referred as constant adaptation.
6 Experiments
Our goal is to get one model specifically adapted
for each speaker. We first describes 4 different ap-
proaches to be compared in the experiments, and
section 6.2 explains our experimental methodology.
6.1 4 Approaches
In these experiments, we compare our approach,
called reweighted adaptation, with three more
conventional approaches: speaker-specific only,
Generic and Constant adaptation.
? Speaker Specific Only For this approach, we
train the dialogue act recognizer using training
sentences from the same speaker used during
testing.
? Generic In this case, we train the dialogue act
recognizer using utterances from all speakers
other than the speaker used during testing.
? Constant Adaptation For this approach, we
train the dialogue act recognizer using all
speakers, including the speaker who will later
be used for testing. All utterances have the
same weight in this case.
? Reweighted Adaptation This is our proposed
approach. As described in Section 5, we train
our dialogue act recognizer using all speakers
but reweight the utterances from the speaker
who will later be used for testing.
6.2 Methodology
In all the following experiments we use MaxEnt
models as defined in Section 5. L2 regularization
is used for MaxEnt to avoid overfitting. The optimal
regularization parameter was automatically selected
during validation. The following regularization pa-
rameters were used: 0.01, 0.1, 1, 10 , 100, 1000 and
0 (no regularization). All the unigram, bigram and
trigram features are used in the maxEnt model. The
labels are the five dialogue act tags described in Sec-
tion 3.
All experiments were performed using hold-out
testing and hold-out validation. Both validation
and test sets consisted of 1000 dialogue acts. The
training sets contained only utterances from meet-
ings that were not in the validation set of test set.
122
Train Data 200 500 1000 1500 2000
Speaker-specific
Only 64.07 65.99 68.51 69.99 71.06
Constant
adaptation model 76.81 76.96 77.00 77.23 77.53
Our reweighted
adaptation model 78.17 78.29 78.67 78.74 78.47
Table 3: Average results among all 7 speakers when train
with different combinations of speaker specific data and
other speakers? data. The number of speaker specific data
is varied from 200, 500, 1000, 1500 to 2000.
In many of our experiments, we analyzed the ef-
fect of training set size on the recognition perfor-
mance. The speaker-specific data size varied from
200, 500, 1000, 1500 and 2000 dialogue acts respec-
tively. When training our reweighting adaptation al-
gorithm described in Section 5, we used the follow-
ing weights: 10, 30, 50, 75, and 100. The optimal
weight factor was selected automatically during val-
idation.
7 Results
In this section we present our approaches to study
the importance of speaker adaptation for dialogue
act recognition. All following results are calculated
based on the overall tag accuracies. We designed
three series of experiments for this study:
? Generic Recognizer (Section 7.1)
? Sparsity in speaker-specific data (Section 7.2)
? Effectiveness of Constant Adaptation (Sec-
tion 7.3)
? Performance of the reweighting algorithm
(Section 7.4)
7.1 Generic Recognizer
The first result we get is on average, for each speaker
when we use all other speaker?s data for training,
then test on speaker- specific test data. The perfor-
mance of this generic recognizer is 76.76% is the
baseline we try to improve when adding speaker-
specific data into consideration. 3
3The performance of our generic model is comparable to the
results from Ang et al(2005) when you take into consideration
that we used only 47,040 dialogue acts in our experiments (i.e.,
dialogue acts from our 7 speakers) which is a small fraction
compared with Ang et al(2005) .
7.2 Sparsity of speaker-specific data
A second result is the performance when only us-
ing speaker-specific data. The row Speaker Specific
Only in Table 3 shows the average results among
all speakers when for each speaker, we train us-
ing only data from the same speaker. The number
of speaker-specific training data we tried are 200,
500, 1000, 1500, and 2000 respectively. Even with
2000 speaker-specific dialogue acts for training, the
best accuracy is 71.06% which is lower than 76.76%
when using generic recognizer. Given the challenge
in getting 2000 speaker-specific annotated dialogue
acts, we are looking at a different approach where
we need less speaker-specific data.
7.3 Results of Constant Adaptation
The most straightforward way to combine other
speakers? data is to directly add them with speaker-
specific data as train. We refer to this approach
as constant adaptation. The row Constant Adap-
tation in Table 3 shows the average results among
all speakers when for each speaker, we combine
the speaker-specific data directly with the all other
speaker?s data. In our experiments, we varied the
amount of speaker-specific data included to be 200,
500, 1000, 1500, and 2000 respectively. For all
7 speakers, the performance can always been im-
proved by including speaker-specific data with all
other speakers? data for training. Furthermore, the
more speaker specific data added, the better perfor-
mance we get.
7.4 Results of Reweighting Algorithm
Finally, in this section we describe the results for
a simple adaptation algorithm based on reweight-
ing, as described in Section 5. Following the same
methodology as previous experiments, we vary the
amount of speaker-specific data to be 200, 500,
1000, 1500 and 2000. The best reweighting factor is
selected through validation on speaker-specific val-
idation data described in section 6.2. The results of
all 7 speakers from Reweighting algorithm when we
vary the amount of speaker-specific data are shown
in Figure 3.
We analyze the influence of the weighting factor
on our speaker adaptation by plotting the recogni-
tion performance for different weights. Figure 4 il-
123
Figure 2: The average results among all 7 speakers when
train with different combinations of speaker specific data
and other speakers? data are displayed. In both Constant
adaptation and Reweighted adaptation models the num-
ber of speaker specific data are varied from 200, 500,
1000, 1500 to 2000. In Generic model, only all other
speakers? data are used for training data.
Figure 3: Reweighting algorithm for all 7 Individual
Speakers when varying the amount of training data to be
0, 200, 500, 1000, 1500 and 2000.
lustrates the influence of the weight factor on three
speaker adaptation cases: None, 500 and 2000. In
this case, None represent the Constant Adaptation.
We observe the following trend: with more speaker-
specific data, the optimal reweighting factor is also
lower. This confirms that our reweighting algorithm
finds the right balance between speaker-specific data
and generic data.
Figure 2 and the row Reweighted Adaptation
from Table 3 shows the effectiveness of reweight-
ing algorithm. Results shows that even this sim-
ple algorithm can efficiently balance the influence
of speaker specific data and other speakers? data and
0 20 40 60 80 1000.765
0.77
0.775
0.78
0.785
0.79
 
 None5002000
Figure 4: Average results of Reweighting among all 7
speakers when the amount of speaker specific data is 0,
500, 2000
give significantly improved results. And most sur-
prisingly, even with only 200 speaker specific data
the reweighting algorithm can give very promising
results.
8 Conclusion
In this work we analyze the effectiveness of speaker
adaptation for dialogue act recognition. A simple
reweighting algorithm is shown to give promising
improvement on several baseline algorithms even
with only 200 speaker-specific dialogue acts. This
paper is a first step toward automatic adaptation for
dialogue act recognition. Inspired by the promising
results from the simple reweighting algorithm, we
plan to evaluate other domain adaptation techniques
such as Daume?s feature-based approach (2007). It
will also be interesting to consider the unlabeled
data from each speaker when performing dialogue
act recognition.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1118018 and the U.S. Army Research, Develop-
ment, and Engineering Command (RDECOM). The
content does not necessarily reflect the position or
the policy of the Government, and no official en-
dorsement should be inferred.
124
References
Jeremy Ang, Yang Liu, Elizabeth Shriberg 2005. Au-
tomatic Dialog Act Segmentation and Classification in
Multiparty Meetings. ICASSP.
Jean Carletta. 2007. Unleashing the killer corpus: expe-
riences in creating the multi-everything AMI Meeting
Corpus. Language Resources and Evaluation, 41(2):
181-190
Mark Core and James Allen. 1997. Working Notes:
AAAI Fall Symposium. HLT-NAACL SIGDIAL Work-
shop.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Raquel Fernandez, Matthew Frampton, Patrick Ehlen,
Matthew Purver and Stanley Peters. 2008. Mod-
elling and Detecting Decisions in Multi-Party Dia-
logue. Proceedings of the 9th SIGdial Workshop on
Discourse and Dialogue.
Umit Guz, Gokhan Tur, Dilek Hakkani-Tur, and Se-
bastien Cuendet. 2009. Cascaded model adaptation
for dialog act segmentation and tagging. Computer
Speech & Language, 24(2):289?306.
Minwoo Jeong, Chin-Yew Lin and Gary Lee. 2009.
Semi-supervised speech act recognition in emails and
forums. The 2009 Conference on Empirical Methods
on Natural Language Processing.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics.
Jachym Kolar, Yang Liu, and Elizabeth Shriberg. 2007.
Speaker Adaptation of Language Models for Auto-
matic Dialog Act Segmentation of Meetings. Inter-
speech, 339?373.
Anna Margolis, Karen Livescu, Mari Ostendorf. 2010.
Semi-supervised domain adaptation for automatic di-
alog act tagging. ACL 2010 Workshop on Domain
Adaptation for Natural Language Processing.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s Not You, it?s Me: Detecting Flirting and its
Misperception in Speed-Dates. The 2009 Conference
on Empirical Methods on Natural Language Process-
ing.
Vivek Rangarajan, Srinivas Bangaloreb and Shrikanth
Narayanana. 2009. Combining lexical, syntactic and
prosodic cues for improved online dialog act tagging.
Computer Speech and Language, 23(4): 407-422
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang and Hannah Carvey. 2004. The ICSI Meeting
Recorder Dialog Act (MRDA) Corpus. HLT-NAACL
SIGDIAL Workshop.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol V. Ess-dykema and Marie
Meteer. 2000. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26:339-373.
Gokhan Tur, Umit Guz and Dilek Hakkani-Tur. 2006.
Model Adaptation For Dialogue Act Tagging. Spoken
Language Technology Workshop.
125
