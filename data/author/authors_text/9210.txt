  
Combining Linguistic Features with Weighted Bayesian Classifier 
for Temporal Reference Processing 
 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong 
csghcao@comp.polyu.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong 
kfwong@se.cuhk.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
Abstract 
Temporal reference is an issue of determining 
how events relate to one another. Determining 
temporal relations relies on the combination of 
the information, which is explicit or implicit in 
a language. This paper reports a computational 
model for determining temporal relations in 
Chinese. The model takes into account the ef-
fects of linguistic features, such as tense/aspect, 
temporal connectives, and discourse structures, 
and makes use of the fact that events are repre-
sented in different temporal structures. A ma-
chine learning approach, Weighted Bayesian 
Classifier, is developed to map their combined 
effects to the corresponding relations. An em-
pirical study is conducted to investigate differ-
ent combination methods, including lexical-
based, grammatical-based, and role-based 
methods. When used in combination, the 
weights of the features may not be equal. Incor-
porating with an optimization algorithm, the 
weights are fine tuned and the improvement is 
remarkable. 
 
1 Introduction 
Temporal information describes changes and time 
of the changes. In a language, the time of an event 
may be specified explicitly, for example ????
1997??????????? (They solved the traf-
fic problem of the city in 1997)?; or it may be related 
to the time of another event, for example ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built?. Temporal reference describes how 
events relate to one another, which is essential to 
natural language processing (NLP). Its major appli-
cations cover syntactic structural disambiguation 
(Brent, 1990), information extraction and question 
answering (Li, 2002), language generation and ma-
chine translation (Dorr, 2002). 
Many researchers have attempted to characterize 
the nature of temporal reference in a discourse. Iden-
tifying temporal relations1 between two events de-
                                                 
1 The relations under examined include both intra-sentence and inter-
pends on a combination of information resources. 
This information is provided by explicit tense and 
aspect markers, implicit event classes or discourse 
structures. It has been used to explain semantics of 
temporal expressions (Moens, 1988; Webber, 1988), 
to constrain possible temporal interpretations 
(Hitzeman, 1995; Sing, 1997), or to generate appro-
priate temporally conjoined clauses (Dorr, 2002). 
The purpose of our work is to develop a computa-
tional model, which automatically determines tempo-
ral relations in Chinese. While temporal reference 
interpretation in English has been well studied, Chi-
nese has been rarely discussed. In our study, thirteen 
related features are identified from linguistic per-
spective. How to combine these features and how to 
map their combined effects to the corresponding rela-
tions are the critical issues to be addressed in this 
paper. 
Previous work was limited in that they just con-
structed constraint or preference rules for some rep-
resentative examples. These methods are ineffective 
for computing purpose, especially when a large 
number of the features are involved and the interac-
tion among them is unclear. Therefore, a machine 
learning approach is applied and the empirical stud-
ies are carried out in our work. 
The rest of this paper is organized as follows. Sec-
tion 2 introduces temporal relation representations. 
Section 3 provides linguistic background of temporal 
reference and investigates linguistic features for de-
termining temporal relations in Chinese. Section 4 
explains the methods used to combine linguistic fea-
tures with Bayesian Classifier. It is followed by a 
description of the optimization algorithm which is 
used for estimating feature weights in Section 5. Fi-
nally, Section 6 concludes the paper. 
2 Representing Temporal Relations 
With the growing interests to temporal information 
processing in NLP, a variety of temporal systems 
have been introduced to accommodate the character-
istics of temporal information. In order to process 
temporal reference in a discourse, a formal represen-
                                                                            
sentence relations. 
  
tation of temporal relations is required. Among those 
who worked on representing or explaining temporal 
relations, some have taken the work of Reichenbach 
(Reichenbach, 1947) as a starting point, while others 
based their works on Allen?s (Allen, 1983). 
Reichenbach proposed a point-based temporal the-
ory. Reichenbach?s representation associated English 
tenses and aspects with three time points, namely 
event time (E), speech time (S) and reference time 
(R). The reference of E-R and R-S was either before 
(or after in reverse order) or simultaneous. This the-
ory was later enhanced by Bruce who defined seven 
temporal relations (Bruce, 1972). Given two durative 
events, the interval relations between them were 
modeled by the order between the greatest lower 
bounding point and least upper bounding point of the 
two events. In the other camp, instead of adopting 
time points, Allen took intervals as temporal primi-
tives to facilitate temporal reasoning and introduced 
thirteen basic relations. In this interval-based repre-
sentation, points were relegated to a subsidiary status 
as ?meeting places? of intervals. An extension to 
Allen?s theory, which treated both points and inter-
vals as primitives on an equal footing, was later in-
vestigated by Knight and Ma (Knight, 1994). 
In natural languages, events described can be ei-
ther punctual or durative in nature. A punctual event, 
e.g., ?? (explore), occurs instantaneously. It takes 
time but does not last in a sense that it lacks of a 
process of change. It is adequate to represent a punc-
tual event with a simple point structure. Whilst, a 
durative event, e.g., ?? (built a house), is more 
complex and its accomplishment as a whole involves 
a process spreading in time. Representing a durative 
event requires an interval representation. For this 
reason, Knight and Ma?s model is adopted in our 
work (see Figure 1). Taking the sentence ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built)? as an example, the relation held between 
building the bridge (i.e., an interval) and solving the 
problem (i.e., a point) is BEFORE. 
 
Figure 1 13 relations represented with points and intervals 
3 Linguistic Background of Temporal Refer-
ence in a Discourse 
3.1 Literature Review 
There were a number of theories in the literature 
about how temporal relations between events can be 
determined in English. Most of the researches on 
temporal reference were based on Reichenbach?s 
notion of tense/aspect structure, which was known as 
Basic Tense Structure (BTS). As for relating two 
events adjoined by a temporal/causal connective, 
Hornstein (Hornstein, 1990) proposed a neo-
Reichenbach structure which organized the BTSs 
into a Complex Tense Structure (CTS). It has been 
argued that all sentences containing a matrix and an 
adjunct clause were subject to linguistic constraints 
on tense structure regardless of the lexical words in-
cluded in the sentence. Generally, constraints were 
used to support syntactic disambiguation (Brent, 
1990) or to generate acceptable sentences (Dorr, 
2002). 
In a given CTS, a past perfect clause should pre-
cede the event described by a simple past clause. 
However, the order of two events in CTS does not 
necessarily correspond to the order imposed by the 
interpretation of the connective (Dorr, 2002). Tem-
poral/casual connective, such as ?after?, ?before? or 
?because?, can supply explicit information about the 
temporal ordering of events. Passonneau (Passon-
neau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) 
determined intra-sentential relations by accounting 
for temporal or causal connectives. Dorr and Gaast-
erland (Dorr, 2002), on the other hand, studied how 
to generate the sentences which reflect event tempo-
ral relations by selecting proper connecting words. 
However, temporal connectives can be ambiguous. 
For instance, a ?when? clause permits many possible 
temporal relations. 
Several researchers have developed the models 
that incorporated aspectual types (such as those dis-
tinct from states, processes and events) to interpret 
temporal relations between clauses connected with 
?when?. Moens and Steedmen (Moens, 1988) devel-
oped a tripartite structure of events2, and emphasized 
it was the notion of causation and consequence that 
played a central role in defining temporal relations of 
events. Webber (Webber, 1988) improved upon the 
above work by specifying rules for how events are 
related to one another in a discourse and Sing and 
Sing defined semantic constraints through which 
events can be related (Sing, 1997). The importance 
of aspectual information in retrieving proper aspects 
and connectives for sentence generation was also 
recognized by Dorr and Gaasterland (Dorr, 2002). 
Some literature claimed that discourse structures 
suggested temporal relations. Lascarides and Asher 
(Lascarides, 1991) investigated various contextual 
effects on rhetorical relations (such as narration, 
elaboration, explanation, background and result). 
They corresponded each of the discourse relations to 
a kind of temporal relation. Later, Hitzeman (Hitze-
man, 1995) described a method for analyzing tempo-
ral structure of a discourse by taking into account the 
effects of tense, aspect, temporal adverbials and rhe-
                                                 
2  The structure comprises a culmination, an associated preparatory 
process and a consequence state. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY 
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
  
torical relations. A hierarchy of rhetorical and tempo-
ral relations was adopted so that they could mutually 
constrain each other.  
 To summarize, the interpretation of temporal rela-
tions draws on the combination of various informa-
tion resources, including explicit tense/aspect and 
connectives (temporal or otherwise), temporal 
classes implicit in events, or rhetorical relations hid-
den in a discourse. This conclusion, although drawn 
from the studies of English, provides the common 
understanding on what information is required for 
determining temporal relations across languages. 
3.2 Linguistic Features for Determining Tem-
poral Relations in Chinese 
Thirteen related linguistic features are recognized 
for determining Chinese temporal relations in this 
paper (See Table 1). The selected features are scat-
tered in various grammatical categories due to the 
unique nature of language, but they fall into the fol-
lowing three groups. 
(1) Tense/aspect in English is manifested by verb 
inflections. But such morphological variations are 
inapplicable to Chinese verbs. Instead, they are 
conveyed lexically. In other words, tense and as-
pect in Chinese are expressed using a combination 
of, for example, time words, auxiliaries, temporal 
position words, adverbs and prepositions, and 
particular verbs. They are known as Tense/Aspect 
Markers. 
(2) Temporal Connectives in English primarily in-
volve conjunctions, such as ?after? and ?before?, 
which are the key components in discourse struc-
tures. In Chinese, however, conjunctions, conjunc-
tive adverbs, prepositions and position words, or 
their combinations are required to represent 
connectives. A few verbs that express cause/effect 
imply a temporal relation. They are also regarded 
as a feature relating to discourse structure3. The 
words which contribute to the tense/aspect and 
temporal connective expressions are explicit in a 
sentence and generally known as Temporal Indica-
                                                 
3 The casual conjunctions such as ?because? are included in this 
group. 
tors. 
(3) Event Classes are implicit in a sentence. Events 
can be classified according to their inherent tem-
poral characteristics, such as the degree of telicity 
and atomicity. The four widespread accepted tem-
poral classes are state, process, punctual event and 
developing event (Li, 2002). Based on their 
classes, events interact with the tense/aspect of 
verbs to determine the temporal relations between 
two events. 
Temporal indicators and event classes are both re-
ferred to as Linguistic Features. Table 1 shows the 
association between a temporal indicator and its ef-
fects. Note that the association is not one-to-one. For 
example, adverbs affect tense/aspect (e.g. ?, being) 
as well as discourse structure (e.g. ?, at the same 
time). For another example, tense/aspect can be 
jointly affected by auxiliary words (e.g. ? , 
were/was), trend verbs (??, begin to), and so on. 
Obviously, it is not a simple task to map the com-
bined effects of the thirteen linguistic features to the 
corresponding relations. Therefore, a machine learn-
ing approach is proposed, which investigates how 
these features contribute to the task and how they 
should be combined. 
4 Combining Linguistic Features with Machine 
Learning Approach 
Previous efforts in corpus-based NLP have incor-
porated machine learning methods to coordinate mul-
tiple linguistic features, for example, in accent resto-
ration (Yarowsky, 1994) and event classification 
(Siegel, 1998).  
Temporal relation determination can be modeled 
as a relation classification task. We formulate the 
thirteen temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The classifica-
tion process is to assign an event pair to one class 
according to their linguistic features. There existed 
numerous classification algorithms based upon su-
pervised learning principle. One of the most effective 
classifiers is Bayesian Classifier, introduced by Duda 
and Hart (Duda, 1973) and analyzed in more detail 
by Langley and Thompson (Langley, 1992). Its pre-
dictive performance is competitive with state-of-the-
Linguistic Feature Symbol POS Tag Effect Example 
With/Without punctuations PT Not Applicable Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb objects VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing wish/hope VA TI_va Tense ??, ?, ? 
Verbs related to causality VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, De-
veloping Event, Process 
Table 1 Linguistic features: eleven temporal indicators and one event class 
  
art classifiers, such as C4.5 and SVM (Friedman, 
1997). 
4.1 Bayesian Classifier 
Given the class c , Bayesian Classifier learns from 
training data the conditional probability of each at-
tribute. Classification is performed by applying 
Bayes rule to compute the posterior probability of c  
given a particular instance x , and then predicting the 
class with the highest posterior probability ratio. Let 
],...,,,,[ 2121 nttteex = , Eee ?21 ,  are the two event 
classes and Tttt n ?,...,, 21 are the temporal indicators 
(i.e. the words). E is the set of event classes. T is the 
set of temporal indicators. Then x is classified as: 
???
?
???
?=
),...,,,,|(
),...,,,,|(
logmaxarg
2121
2121*
n
n
c ttteecP
ttteecP
c  (E1)
where c denotes the classes different from c . As-
suming event classes are independent of temporal 
indicators given c , we have: 
???
?
???
?=
???
?
???
?
)()|,...,,,,(
)()|,...,,,,(
log
),...,,,,|(
),...,,,,|(
log
2121
2121
2121
2121
cPcttteeP
cPcttteeP
ttteecP
ttteecP
n
n
n
n
                          (E2)
???
?
???
?+???
?
???
?+???
?
???
?=
)|,...,,(
)|,...,,(
log
)|,(
)|,(
log
)(
)(log
21
21
21
21
ctttP
ctttP
ceeP
ceeP
cP
cP
n
n  
Assuming temporal indicators are independent of 
each other, we have 
?
=
= n
i i
i
n
n
ctP
ctP
ctttP
ctttP
121
21
)|(
)|(
)|,...,,(
)|,...,,( ,    ( ni ,...2,1= ) (E3)
A Na?ve Bayesian Classifier assumes strict inde-
pendence among all attributes. However, this as-
sumption is not satisfactory in the context of tempo-
ral relation determination. For example, if the 
relation between 1e  and 2e  is SAME_AS, 1e  and 2e  
have to be identical. We release the independence 
assumption for 1e and 2e , and decompose the second 
part of (E2) as: 
),|()|(
),|()|(
)|,(
)|,(
121
121
21
21
ceePceP
ceePceP
ceeP
ceeP =  (E4)
Estimation of ),|( 12 ceep is motivated by Absolute 
Discounting N-Gram language model (Goodman, 
2001): 
??
??
?
=
>?=
0),,( if     )|(),(
0),,( if    
),(
),,(
),|(
1221
12
1
12
12
ceeCcePce
ceeC
ceC
DceeC
ceP e
?
 
 
(E5) 
here D is the discount factor and is set to 0.5 experi-
mentally. From the fact that 1),|(
2
12 =?
e
ceeP , we get: 
?
?
>
>
?
?
=
0),,(|
2
0),,(|
12
1
122
122
)|(1
),|(1
),(
ceeCe
ceeCe
ceP
ceeP
ce?  
 
(E6)
)|( ctp i and )|( cep i  are estimated by MLE with 
Dirichlet Smoothing method: 
?
?
+
+=
Tt
i
i
i
i
TuctC
uctC
ctP
||),(
),(
)|(      ( ni ,...2,1= )  
(E7)
?
?
+
+=
Ee
i
i
i
i
EuceC
uceC
ceP
||),(
),(
)|(     ( 2,1=i )  
(E8)
where u (=0.5) is the smoothing factor. Then, 
)|( ctp i , )|( cep i and ),|( 12 ceeP  can be estimated 
with (E5) - (E8) by substituting c  with c . 
4.2 Estimating )|,...,( 21 ctttP n  with Lexical-POS 
Information 
The effects of a temporal indicator are constrained 
by its positions in a sentence. For instance, the con-
junctive word ?? (because) may represent the dif-
ferent relations when it occurs before or after the first 
event. Therefore, in estimating )|,...,( 21 ctttp n , we 
consider an indicator located in three positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Note that 
cases (2) and (3) are ambiguous. The positions of the 
temporal indicators are the same. But it is uncertain 
whether these indicators modify the first or the sec-
ond event if there is no punctuation (such as comma, 
period, exclamation or question mark) separating 
their roles. The ambiguity is resolved by using POS 
information. We assume that an indicator modifies 
the first event if it is an auxiliary word, a trend word 
or a position word; otherwise it modifies the second. 
Thus, we rewrite )|,...,( 21 ctttP n as ,,...,( 1111 nttP  
)|,...,,...,,,...,
432 441331221
ctttttt nnn , where jn is the total 
number of the temporal indicators occurring in the 
position j . 4,3,2,1=j  represents the four positions 
and nn
j
j =?=
4
1
. Assuming jit are independent of each 
other, then ?
=
n
i
i ctP
1
)|( in (E3) is revised as 
??
= =
4
1 1
)|(
j
n
i
ji
j
ctP . Accordingly, (E7) is revised as: 
?
?
+
+=
Tt
ji
ji
ji
ji
TuctC
uctC
ctP
||),(
),(
)|(  
( 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7?)
In addition to taking positions into account, we 
further classify the temporal indicators into two 
groups according to their grammatical categories or 
semantic roles. The rationale of grouping will be 
demonstrated in Section 4.3. 
4.3 Experimental Results 
Several experiments have been designed to evalu-
ate the proposed Bayesian Classifier in combining 
linguistic features for temporal relation determination 
and to reveal the impact of linguistic features on 
learning performance. 700 instances are extracted 
from Ta Kong Pao (a local Hong Kong Chinese 
newspaper) financial version. Among them, 500 are 
used as training data, and 200 as test data, which are 
  
partitioned equally into two sets. One is similar as 
training data in class distribution, while the other is 
quite different. 209 lexical words, gathered from lin-
guistic books and corpus, are used as the temporal 
indicators and manually marked with the tags given 
in Table 1. 
4.3.1 Impact of Individual Features 
From linguistic perspective, the thirteen features 
(see Table 1) are useful for temporal relation deter-
mination. To examine the impact of each individual 
feature, we feed a single linguistic feature to the 
Bayesian Classifier learning algorithm one at a time 
and study the accuracy of the resultant classifier. The 
experimental results are given in Table 2. It shows 
that event classes have greatest accuracy, followed 
by conjunctions in the second place, and adverbs in 
the third in the close test. Since punctuation shows 
no contribution, we only use it as a syntactic feature 
to differentiate cases (2) and (3) mentioned in Sec-
tion 4.2. 
4.3.2 Features in Combination 
We now use Bayesian Classifier introduced in Sec-
tions 4.1 and 4.2 to combine all the related temporal 
indicators and event classes, since none of the fea-
tures can achieve a good result alone. The simplest 
way is to combine the features without distinction. 
The conditional probability )|( ctP ji is estimated by 
(E7?). This model is called Ungrouped Model (UG). 
However, as illustrated in table 1, the temporal in-
dicators play different roles in building temporal ref-
erence. It is not reasonable to treat them equally. We 
claim that the temporal indicators have two functions, 
i.e., representing the connections of the clauses, or 
representing the tense/aspect of the events. We iden-
tify them as connective words or tense/aspect mark-
ers and separate them into two groups. This allows 
features to be compared with those in the same group. 
Let ],[ 21 TTT = , where 1T is the set of connective 
words and 2T is the set of tense/aspect markers. We 
have 1112
1
1 ,..,, Tttt m ? and 222221 ,..,, Tttt l ? , m and l are 
the number of the connective words and the 
tense/aspect markers in a sentence respectively. We 
assume that the occurrences of the two groups are 
independent. By taking both grouping and position 
features into account, we replace ?
=
n
i
i ctP
1
)|(  with 
???
= = =
2
1
4
1 1
)|(
k j
n
i
k
ji
k
j
ctP , 2,1=k  represents the two groups 
and j
k
k
j nn =?=
2
1
. To build the grouping-based Bayes-
ian Classifier, (E7?) is modified as: 
?
?
+
+=
kk
ji Tt
kk
ji
k
jik
ji TuctC
uctC
ctP
||),(
),(
)|(  
( 2,1=k , 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7??)
4.3.3 Grouping Features by Grammatical Cate-
gories or Semantic Roles 
We partition temporal indicators into connective 
words and tense/aspect markers in two ways. One is 
simply based on their grammatical categories (i.e. 
POS information). It separates conjunctions (e.g., ?
?, after; ??, because) and verbs relating to causal-
ity (e.g., ??, cause) from others. They are assumed 
to be connective words (i.e. 1T? ), while others are 
tense/aspect markers (i.e. 2T? ). This model is called 
Grammatical Function based Grouping Model (GFG). 
Unfortunately, such a separation is ineffective. In 
comparison with UG, the performance of GFG de-
creases as shown in figure 2. This reveals the com-
plexity of Chinese in connecting expressions. It 
arises from the fact that some other words, such as 
adverbs (e.g., ???, meanwhile), prepositions (e.g., 
?, at) and position words (e.g., ??, before), can 
also serve such a connecting function (see Table 1). 
Actually, the roles of the words falling into these 
grammatical categories are ambiguous. For instance, 
the adverb? can express an event happened in the 
past, e.g., ????????? (He just finished the 
report)?. It can be also used in a connecting expres-
sion (such as ????), e.g., ??????????
??? (He went to the library after he had finished 
the report)?. 
This finding suggests that temporal indicators 
should be divided into two groups according to their 
semantic roles rather than grammatical categories. 
Therefore we propose the third model, namely 
Semantic Role based Grouping Model (SRG), in 
which the indicators are manually re-marked as 
TI_j_pos or TI_at_pos4. 
Figure 2 shows the accuracies of four models (i.e. 
DM. UG, GFG and SRG) based on the three tests. 
Test 1 is the close test carried out on training data 
and tests 2 and 3 are open tests performed on differ-
ent test data. DM (i.e., Default Model) assigns all 
incoming cases with the most likely class and it is 
used as evaluation baseline. In our case, it is 
SAME_AS, which holds 50.2% in training data. 
SRG model outperforms UG and GFG models. 
These results validate our previous assumption em-
pirically. 
                                                 
4 ?j? and ?at? are the tags representing connecting and tense/aspect 
roles respectively. ?pos? is the POS tag of the temporal indicator TI. 
Accuracy Accuracy  
Feature Close 
test 
Open 
test 1 
Open 
test 2 
 
Feature Close 
test 
Open 
test 1
Open 
test 2
VS 53.4% 48% 30% VA 57% 50% 37%
VC 56.6% 56% 49% C 62.6% 52% 45%
TR 50.2% 46% 28% U 51.8% 50% 32%
P 52.4% 49% 30% T 57.2% 48% 32%
PS 59% 53% 38% D 59.6% 55% 47%
VV 51% 49% 29% EC 72.4% 69% 68%
Table 2 Impact of each individual linguistic feature 
  
20%
30%
40%
50%
60%
70%
80%
90%
Close Test Open Test1 Open Test2
A
cc
ur
ac
y
DM UG GFG SRG
 
Figure 2 Comparing DM, UG, GFG and SRG models 
4.3.4 Impact of Semantic Roles in SRG Model 
When the temporal indicators are classified into 
two groups based on their semantic roles in SRG 
model, there are three types of linguistic features 
used in the Bayesian Classifier, i.e., tense/aspect 
markers, connective words and event classes. A set 
of experiments are conducted to investigate the im-
pacts of each individual feature type and the impacts 
when they are used in combination (shown in Table 
3). We find that the performance of methods 1 and 2 
in the open tests drops dramatically compared with 
those in the close test. But the predictive strength of 
event classes in method 3 is surprisingly high. Two 
conclusions are thus drawn. Firstly, the models using 
tense/aspect markers and connective words are more 
likely to encounter over-fitting problem with insuffi-
cient training data. Secondly, different features have 
varied weights. We then incorporate an optimization 
approach to adjust the weights of the three types of 
features, and propose an algorithm to tackle over-
fitting problem in the next section. 
Method Semantic Groups 
Close 
test 
Open 
test 1 
Open 
test 2
1 Tense/aspect markers 71% 58% 40%
2 Connective words 75% 65% 57%
3 Event classes 66.6% 69% 68%
4 1+2 84.8% 70% 56%
5 1+3 76.6% 72% 66%
6 2+3 82.4% 84% 81%
7 1+2+3 89.8% 84% 80%
8 Default 50.2% 46% 28%
Table 3: Impact of Semantic Role based Groups 
5. Weighted Bayesian Classifier  
Let 1? , 2? , 3? be the weights of event classes, con-
nective words and tense/aspect markers respectively. 
Then the Weighted Bayesian Classifier is: 
???
?
???
?
),...,,,,|(
),...,,,,|(
log
2121
2121
n
n
ttteecP
ttteecP  
???
?
???
?+???
?
???
?=
)|,(
)|,(
log
)(
)(log
21
21
1 ceeP
ceeP
cP
cP ?                             (E9)
???
?
???
?+???
?
???
?+
)|,...,,(
)|,...,,(
log
)|,...,,(
)|,...,,(
log 22
2
2
1
22
2
2
1
311
2
1
1
11
2
1
1
2 ctttP
ctttP
ctttP
ctttP
l
l
m
m ??  
In order to estimate the weights, we need a suit-
able optimization approach to search for the opti-
mal value of ],,[ 321 ???  automatically. 
5.1 Estimating Weights with Simulated Anneal-
ing Algorithm 
Quite a lot optimization approaches are available 
to compute the optimal value of ],,[ 321 ??? . Here, 
Simulated Annealing algorithm is employed to per-
form the task, which is a general and powerful opti-
mization approach with excellent global convergence 
(Kirkpatrick, 1983). Figure 3 shows the procedure of 
searching for an optimal weight vector with the algo-
rithm. 
1. 1=k , )( 1?= kk tTt  
2. Generates a random change from the current weight vec-
tor iv . The updated weight vector is denoted by jv . Then 
computes the increasement of the objective function, i.e. 
)()( ij vfvf ?=? . 
3 Accepts jv as an optimal vector and substitutes iv with the 
following accept rate: 
??
??
?
??
?
? <
>
= 0 if  )exp(
0 if             1
)(
k
ji
t
vvP  
4 If kLk < , lets 1+= kk , goes to step 2. 
5 Else if fk Tt < , goes to step 1. 
6 Else stops looping and outputs the current optimal weight 
vector.  
Figure 3 Simulated Annealing algorithm 
In Figure 3, Markov chain length 20=kL ; tem-
perature update function ttT *9.0)( = ; starting point 
],,[ 03
0
2
0
1
0 ???=v =[1,1,1]; initial temperature 200 =t  
and final temperature 810?=ft . Note that the initial 
temperature is critical for a simulated annealing algo-
rithm (Kirkpatrick, 1983). Its value should assure 
that the initial accept rate is greater than 90%. 
5.2 K-fold Cross-Validation 
The accuracy of the classifier is defined as the ob-
jective function of the Simulated Annealing algo-
rithm illustrated in Figure 3. If it is evaluated with 
the accuracy over all training data, the Weighted 
Bayesian Classifier may trap into over-fitting prob-
lem and lower the performance due to insufficient 
data. To avoid this, we employ K-fold Cross-
Validation technique. It partitions the original set of 
data into K parts. One part is selected arbitrarily as 
evaluating data and the other K-1 parts as training 
data. Then K accuracies on evaluating data are ob-
tained after K iterations and their average is used as 
the objective function. 
5.3 Experimental Results 
Table 4 shows the result of the experiment which 
compares WSRG (Weighted SRG) with SRG. We 
use error reduction to evaluate the benefit from in-
corporating weight parameters into Bayesian Classi-
fier. It is defined as: 
SRG
WSRGSRG
rateerror
rateerrorrateerror
_
__
reductionerror 
?=  
  
The experimental results show that the Weighted 
Bayesian Classifier outperforms the Bayesian Classi-
fier significantly in the two open tests and it tackles 
the over-fitting problem well. To test Simulated An-
nealing algorithm?s global convergence, we ran-
domly choose several initial values and they finally 
converge to a small area [7.2?0.09, 5.8?0.02, 
3.0?0.02]. The empirical result demonstrates that the 
output of a Simulated Annealing algorithm is a 
global optimal weighting vector. 
6 Conclusions 
Temporal reference processing has received grow-
ing attentions in last decades. However this topic has 
not been well studied in Chinese. In this paper, we 
proposed a method to determine temporal relations in 
Chinese by employing linguistic knowledge and ma-
chine learning approaches. Thirteen related linguistic 
features were recognized and temporal indicators 
were further grouped with respect to grammatical 
functions or semantic roles. This allows features to 
be compared with those in the same group. To ac-
commodate the fact that the different types of fea-
tures support varied importance, we extended Na?ve 
Bayesian Classifier to Weighted Bayesian Classifier 
and applied Simulated Annealing algorithm to opti-
mize weight parameters. To avoid over-fitting prob-
lem, K-fold Cross-Validation technique was incorpo-
rated to evaluate the objective function of the optimi-
zation algorithm. Establishing the temporal relations 
between two events could be extended to provide a 
determination of the temporal relations among multi-
ple events in a discourse. With such an extension, 
this temporal analysis approach could be incorpo-
rated into various NLP applications, such as question 
answering and machine translation. 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and CUHK 
Strategic Grant (account number 4410001). 
References 
Allen J., 1983. Maintaining Knowledge about Temporal 
Intervals. Communications of the ACM, 26(11):832-
843. 
Brent M., 1990. A Simplified Theory of Tense Repre-
sentations and Constraints on Their Composition, In 
Proceedings of the 28th Annual Conference of the As-
sociation for Computational Linguistics, pages 119-
126. Pittsburgh. 
Bruce B., 1972. A Model for Temporal References and 
its Application in Question-Answering Program. Arti-
ficial Intelligence, 3(1):1-25. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting Words 
from Temporal Expressions. submitted to Journal of 
Artificial Intelligence Research. 
Duda, R. O. and P. E. Hart, 1973. Pattern Classification 
and Scene Analysis. New York. 
Friedman N., Geiger D. and Goldszmidt M., 1997. 
Bayesian Network Classifiers. Machine Learning 
29:131-163, Kluwer Academic Publisher. 
Goodman J., 2001. A Bit of Progress in Language Mod-
eling. Microsoft Research Technical Report MSR-
TR-2001-72. 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of Dis-
course. In Proceedings of the 7th European Meeting 
of the Association for Computational Linguistics, 
pages 253-260. Dublin, Ireland.  
Hornstein N., 1990. As Time Goes By. MIT Press, Cam-
bridge, MA. 
Kirkpatrick, S., Gelatt C.D., and Vecchi M.P., 1983. 
Optimization by Simulated Annealing. Science, 
220(4598): 671-680. 
Knight B. and Ma J., 1997. Temporal Management Us-
ing Relative Time in Knowledge-based Process Con-
trol, Engineering Applications of Artificial Intelli-
gence, 10(3):269-280.  
Langley, P.W. and Thompson K., 1992. An Analysis of 
Bayesian Classifiers. In Proceedings of the 10th Na-
tional Conference on Artificial Intelligence, pages 
223?228. San Jose, CA. 
Lascarides A. and Asher N., 1991. Discourse Relations 
and Defensible Knowledge. In Proceedings of the 29th 
Meeting of the Association for Computational Lin-
guistics, pages 55-62. Berkeley, USA. 
Li W.J. and Wong K.F., 2002. A Word-based Approach 
for Modeling and Discovering Temporal Relations 
Embedded in Chinese Sentences, ACM Transaction 
on Asian Language Processing, 1(3):173-206. 
Moens M. and Steedmen M., 1988. Temporal Ontology 
and Temporal Reference. Computational Linguistics, 
14(2):15-28. 
Passonneau R., 1988. A Computational Model of the 
Semantics of Tense and Aspect. Computational Lin-
guistics, 14(2):44-60. 
Reichenbach H., 1947. The Elements of Symbolic Logic. 
The Free Press, New York. 
Siegel E.V. and McKeown K.R., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving As-
pectual Classification and Revealing Linguistic In-
sights. Computational Linguistics, 26(4):595-627. 
Singh M. and Singh M., 1997. On the Temporal Struc-
ture of Events. In Proceedings of AAAI-97 Workshop 
on Spatial and Temporal Reasoning, pages 49-54. 
Providence, Rhode Island. 
Webber B., 1988. Tense as Discourse Anaphor. Compu-
tational Linguistics, 14(2):61-73.  
Yarowsky D., 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to the Accent Restora-
tion in Spanish and French. In Proceeding of the 32nd 
Annual Meeting of the Association for Computational 
Linguistics, pages 88-95. San Francisco, CA. 
Error Rate 
Model 
Close Test Open Test1 Open Test2
SRG 10.2% 16% 20% 
WSRG 12.4%% 11% 13% 
Error Reduction -21.57% 31.25% 35% 
Table 4 Compare WSRG with SRG on error rates 
 
Applying Machine Learning to Chinese Temporal Relation Resolution 
 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong
kfwong@se.cuhk.edu.hk 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
csghcao@comp.polyu.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
 
 
Abstract 
Temporal relation resolution involves extraction 
of temporal information explicitly or implicitly 
embedded in a language. This information is of-
ten inferred from a variety of interactive gram-
matical and lexical cues, especially in Chinese. 
For this purpose, inter-clause relations (tempo-
ral or otherwise) in a multiple-clause sentence 
play an important role. In this paper, a computa-
tional model based on machine learning and 
heterogeneous collaborative bootstrapping is 
proposed for analyzing temporal relations in a 
Chinese multiple-clause sentence. The model 
makes use of the fact that events are represented 
in different temporal structures. It takes into ac-
count the effects of linguistic features such as 
tense/aspect, temporal connectives, and dis-
course structures. A set of experiments has been 
conducted to investigate how linguistic features 
could affect temporal relation resolution.  
 
1 Introduction 
In language studies, temporal information de-
scribes changes and time of changes expressed in a 
language. Such information is critical in many typi-
cal natural language processing (NLP) applications, 
e.g. language generation and machine translation, etc. 
Modeling temporal aspects of an event in a written 
text is more complex than capturing time in a physi-
cal time-stamped system. Event time may be speci-
fied explicitly in a sentence, e.g. ???? 1997??
????????? (They solved the traffic prob-
lem of the city in 1997)?; or it may be left implicit, to 
be recovered by readers from context. For example, 
one may know that ???????????????
?????? (after the street bridge had been built, 
they solved the traffic problem of the city)?, yet 
without knowing the exact time when the street 
bridge was built. As reported by Partee (Partee, 
1984), the expression of relative temporal relations 
in which precise times are not stated is common in 
natural language. The objective of relative temporal 
relation resolution is to determine the type of rela-
tive relation embedded in a sentence. 
In English, temporal expressions have been 
widely studied. Lascarides and Asher (Lascarides, 
Asher and Oberlander, 1992) suggested that tempo-
ral relations between two events followed from dis-
course structures. They investigated various 
contextual effects on five discourse relations 
(namely narration, elaboration, explanation, back-
ground and result) and then corresponded each of 
them to a kind of temporal relations. Hitzeman et al 
(Hitzeman, Moens and Grover, 1995) described a 
method for analyzing temporal structure of a dis-
course by taking into account the effects of tense, 
aspect, temporal adverbials and rhetorical relations 
(e.g. causation and elaboration) on temporal order-
ing. They argued that rhetorical relations could be 
further constrained by event temporal classification. 
Later, Dorr and Gaasterland (Dorr and Gaasterland, 
2002) developed a constraint-based approach to 
generate sentences, which reflect temporal relations, 
by making appropriate selections of tense, aspect 
and connecting words (e.g. before, after and when). 
Their works, however, are theoretical in nature and 
have not investigated computational aspects. 
The pioneer work on Chinese temporal relation 
extraction was first reported by Li and Wong (Li and 
Wong, 2002). To discover temporal relations em-
bedded in a sentence, they devised a set of simple 
rules to map the combined effects of temporal indi-
cators, which are gathered from different grammati-
cal categories, to their corresponding relations. 
However, their work did not focus on relative tem-
poral relations. Given a sentence describing two 
temporally related events, Li and Wong only took 
the temporal position words (including before, after 
and when, which serve as temporal connectives) and 
the tense/aspect markers of the second event into 
consideration. The proposed rule-based approach 
was simple; but it suffered from low coverage and 
was particularly ineffective when the interaction be-
tween the linguistic elements was unclear. 
This paper studies how linguistic features in Chi-
nese interact to influence relative relation resolution. 
For this purpose, statistics-based machine learning 
approaches are applied. The remainder of the paper 
is structured as follows: Section 2 summarizes the 
linguistic features, which must be taken into account 
in temporal relation resolution, and introduces how 
these features are expressed in Chinese. In Section 3, 
the proposed machine learning algorithms to identify 
temporal relations are outlined; furthermore, a het-
erogeneous collaborative bootstrapping technique 
for smoothing is presented. Experiments designed 
for studying the impact of different approaches and 
linguistic features are described in Section 4. Finally, 
Section 5 concludes the paper. 
2 Modeling Temporal Relations 
2.1 Temporal Relation Representations 
As the importance of temporal information proc-
essing has become apparent, a variety of temporal 
systems have been introduced, attempting to ac-
commodate the characteristics of relative temporal 
information. Among those who worked on temporal 
relation representations, many took the work of Rei-
chenbach (Reichenbach, 1947) as a starting point, 
while some others based their works on Allen?s (Al-
len, 1981). 
Reichenbach proposed a point-based temporal 
theory. This was later enhanced by Bruce who de-
fined seven relative temporal relations (Bruce. 1972). 
Given two durative events, the interval relations be-
tween them were modeled by the order between the 
greatest lower bounding points and least upper 
bounding points of the two events. In the other camp, 
instead of adopting time points, Allen took intervals 
as temporal primitives and introduced thirteen basic 
binary relations. In this interval-based theory, points 
are relegated to a subsidiary status as ?meeting 
places? of intervals. An extension to Allen?s theory, 
which treated both points and intervals as primitives 
on an equal footing, was later investigated by Ma 
and Knight (Ma and Knight, 1994). 
In natural language, events can either be punctual 
(e.g. ?? (explore)) or durative (e.g. ?? (built a 
house)) in nature. Thus Ma and Knight?s model is 
adopted in our work (see Figure 1). Taking the sen-
tence ????????????????????? 
(after the street bridge had been built, they solved 
the traffic problem of the city)? as an example, the 
relation held between building the bridge (i.e. an 
interval) and solving the problem (i.e. a point) is 
BEFORE. 
Figure 1. Thirteen temporal relations between points and 
intervals 
2.2 Linguistic Features for Determining Relative 
Relations 
Relative relations are generally determined by 
tense/aspect, connecting words (temporal or other-
wise) and event classes.  
Tense/Aspect in English is manifested by verb in-
flections. But such morphological variations are in-
applicable to Chinese verbs; instead, they are 
conveyed lexically (Li and Wong, 2002). In other 
words, tense and aspect in Chinese are expressed 
using a combination of time words, auxiliaries, tem-
poral position words, adverbs and prepositions, and 
particular verbs. 
Temporal Connectives in English primarily in-
volve conjunctions, e.g. after, before and when (Dorr 
and Gaasterland, 2002). They are key components in 
discourse structures. In Chinese, however, conjunc-
tions, conjunctive adverbs, prepositions and position 
words are required to represent connectives. A few 
verbs which express cause and effect also imply a 
forward movement of event time. The words, which 
contribute to the tense/aspect and temporal connec-
tive expressions, are explicit in a sentence and gen-
erally known as Temporal Indicators. 
Event Class is implicit in a sentence. Events can 
be classified according to their inherent temporal 
characteristics, such as the degree of telicity and/or 
atomicity (Li and Wong, 2002). The four widespread 
accepted temporal classes1 are state, process, punc-
tual event and developing event. Based on their 
classes, events interact with the tense/aspect of verbs 
to define the temporal relations between two events. 
Temporal indicators and event classes are together 
referred to as Linguistic Features (see Table 1). For 
example, linguistic features are underlined in the 
sentence ?(??)?????(??)????????
????? after/because the street bridge had been 
built (i.e. a developing event), they solved the traffic 
problem of the city (i.e. a punctual event)?. 
                                                          
1 Temporal classification refers to aspectual classification. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
Table 1 shows the mapping between a temporal 
indicator and its effects. Notice that the mapping is 
not one-to-one. For example, adverbs affect 
tense/aspect as well as discourse structure. For an-
other example, tense/aspect can be affected by auxil-
iary words, trend verbs, etc. This shows that 
classification of temporal indicators based on part-
of-speech (POS) information alone cannot determine 
relative temporal relations. 
3 Machine Learning Approaches for Relative 
Relation Resolution 
Previous efforts in corpus-based natural language 
processing have incorporated machine learning 
methods to coordinate multiple linguistic features 
for example in accent restoration (Yarowsky, 1994) 
and event classification (Siegel and McKeown, 
1998), etc. 
Relative relation resolution can be modeled as a 
relation classification task. We model the thirteen 
relative temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The resolution 
process is to assign an event pair (i.e. the two events 
under concern)2 to one class according to their lin-
guistic features. For this purpose, we train two clas-
sifiers, a Probabilistic Decision Tree Classifier 
(PDT) and a Na?ve Bayesian Classifier (NBC). We 
then combine the results by the Collaborative Boot-
strapping (CB) technique which is used to mediate 
the sparse data problem arose due to the limited 
number of training cases. 
                                                          
2 It is an object in machine learning algorithms. 
3.1 Probabilistic Decision Tree (PDT) 
Due to two domain-specific characteristics, we 
encounter some difficulties in classification. (a) Un-
known values are common, for many events are 
modified by less than three linguistic features. (b) 
Both training and testing data are noisy. For this rea-
son, it is impossible to obtain a tree which can com-
pletely classify all training examples. To overcome 
this predicament, we aim to obtain more adjusted 
probability distributions of event pairs over their 
possible classes. Therefore, a probabilistic decision 
tree approach is preferred over conventional deci-
sion tree approaches (e.g. C4.5, ID3). We adopt a 
non-incremental supervised learning algorithm in 
TDIDT (Top Down Induction of Decision Trees) 
family. It constructs a tree top-down and the process 
is guided by distributional information learned from 
examples (Quinlan, 1993). 
3.1.1 Parameter Estimation 
Based on probabilities, each object in the PDT ap-
proach can belong to a number of classes. These 
probabilities could be estimated from training cases 
with Maximum Likelihood Estimation (MLE). Let l 
be the decision sequence, z the object and c the class. 
The probability of z belonging to c is: 
?? ?=
ll
zlplcpzclpzcp )|()|()|,()|(  (1)
let nBBBl ...21= , by MLE we have: 
)(
),(
)|()|(
n
n
n Bf
Bcf
Bcplcp =?   (2)
),( nBcf  is the count of the items whose leaf nodes 
are Bn and belonging to class c. And 
Linguistic Feature Symbol POS Tag Effect Example 
With/Without 
punctuations 
PT Not Applica-
ble 
Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb 
objects 
VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing 
wish/hope 
VA TI_va Tense ??, ?, ? 
Verbs related to 
causality 
VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, 
Developing Event, 
Process 
Table 1. Linguistic features: eleven temporal indicators and one event class 
),...|(...
),,|(),|()|()|(
11
213121
zBBBp
zBBBpzBBpzBpzlp
nn ?
=
 
 
(3)
where 
)|...(
)|...(
),...|(
121
121
121 zBBBp
zBBBBp
zBBBBp
mm
mmm
mmm
??
??
?? =
)|...(
)|...(
121
121
zBBBf
zBBBBf
mm
mmm
??
??= , ( nm ,...,3,2= ).  
An object might traverse more than one decision 
path if it has unknown attribute values. 
)|...( 121 zBBBBf mmm ??  is the count of the item z, 
which owns the decision paths from B1 to Bm. 
3.1.2 Classification Attributes 
Objects are classified into classes based on their 
attributes. In the context of temporal relation resolu-
tion, how to categorize linguistic features into classi-
fication attributes is a major design issue. We extract 
all temporal indicators surrounding an event. As-
sume m and n are the anterior and posterior window 
size. They represent the numbers of the indicators 
BEFORE and AFTER respectively. Consider the 
most extreme case where an event consists of at 
most 4 temporal indicators before and 2 after. We 
set m and n to 4 and 2 initially. Experiments show 
that learning performance drops when m>4 and n>2 
and there is only very little difference otherwise (i.e. 
when m?4 and n?2).  
In addition to temporal indicators alone, the posi-
tion of the punctuation mark separating the two 
clauses describing the events and the classes of the 
events are also useful classification attributes.  We 
will outline why this is so in Section 4.1. Altogether, 
the following 15 attributes are used to train the PDT 
and NBC classifiers: 
,,),(,,,, 2
1
1
1
1
1
2
1
3
1
4
1 1
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITI  
2
2
1
2
1
2
2
2
3
2
4
2
,),(,,,,, / 2
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITIpuncwowi  
li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before 
and the jth indicator after the event ek (k=1,2). Given 
a sentence, for example, ?/TI_d ?/E0 ?/TI_u ??
/n ?/w ?/TI_d ?/E2 ?/TI_u ??/n ?/w, the at-
tribute vector could be represented as: [0, 0, 0, ?, 
E0, ?, 0, 1, 0, 0, 0, ?, E2, ?, 0]. 
3.1.3 Attribute Selection Function 
Many similar attribute selection functions were 
used to construct a decision tree (Marquez, 2000). 
These included information gain and information 
gain ratio (Quinlan, 1993), 2? Test and Symmetrical 
Tau (Zhou and Dillon, 1991). We adopt the one pro-
posed by Lopez de Mantaraz (Mantaras, 1991) for it 
shows more stable performance than Quinlan?s 
information gain ratio in our experiments. Compared 
with Quinlan?s information gain ratio, Lopez?s dis-
tance-based measurement is unbiased towards the 
attributes with a large number of values and is capa-
ble of generating smaller trees with no loss of accu-
racy (Marquez, Padro and Rodriguez, 2000). This 
characteristic makes it an ideal choice for our work, 
where most attributes have more than 200 values. 
3.2 Na?ve Bayesian Classifier (NBC) 
NBC assumes independence among features. 
Given the class label c, NBC learns from training 
data the conditional probability of each attribute Ai 
(see Section 3.1.2). Classification is then performed 
by applying Bayes rule to compute the probability of 
c given the particular instance of A1,?,An, and then 
predicting the class with the highest posterior prob-
ability ratio. 
),...,,,|(maxarg 321
*
n
c
AAAAcscorec =  (4)
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =  (5)
Apply Bayesian rule to (5), we have: 
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =
)()|,...,,,(
)()|,...,,,(
321
321
cpcAAAAp
cpcAAAAp
n
n=
)()|(
)()|(
1
1
cpcAp
cpcAp
n
i
i
n
i
i
?
?
=
=?  
 
 
 
(6)
)|( cAp i and )|( cAp i  are estimated by MLE from 
training data with Dirichlet Smoothing method: 
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (7)
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (8)
3.3 Collaborative Bootstrapping (CB) 
PDT and NB are both supervised learning ap-
proach. Thus, the training processes require many 
labeled cases. Recent results (Blum and Mitchell, 
1998; Collins, 1999) have suggested that unlabeled 
data could also be used effectively to reduce the 
amount of labeled data by taking advantage of col-
laborative bootstrapping (CB) techniques. In previ-
ous works, CB trained two homogeneous classifiers 
based on different independent feature spaces. How-
ever, this approach is not applicable to our work 
since only a few temporal indicators occur in each 
case. Therefore, we develop an alternative CB algo-
rithm, i.e. to train two different classifiers based on 
the same feature spaces. PDT (a non-linear classifier) 
and NBC (a linear classifier) are under consideration. 
This is inspired by Blum and Mitchell?s theory that 
two collaborative classifiers should be conditionally 
independent so that each classifier can make its own 
contribution (Blum and Mitchell, 1998). The learn-
ing steps are outlined in Figure 2. 
Inputs: A collection of the labeled cases and unla-
beled cases is prepared. The labeled cases 
are separated into three parts, training 
cases, test cases and held-out cases.  
Loop: While the breaking criteria is not satisfied 
1 Build the PDT and NBC classifiers us-
ing training cases 
2 Use PDT and NBC to classify the unla-
beled cases, and exchange with the se-
lected cases which have higher 
Classification Confidence (i.e. the un-
certainty is less than a threshold). 
3 Evaluate the PDT and NBC classifiers 
with the held-out cases. If the error rate 
increases or its reduction is below a 
threshold break the loop; else go to step 
1. 
Output: Use the optimal classifier to label the test 
cases 
Figure 2. Collaborative bootstrapping algorithm 
3.4 Classification Confidence Measurement 
Classification confidence is the metric used to 
measure the correctness of each labeled case auto-
matically (see Step 2 in Figure 2). The desirable 
metric should satisfy two principles:  
? It should be able to measure the uncertainty/ cer-
tainty of the output of the classifiers; and 
? It should be easy to calculate. 
We adopt entropy, i.e. an information theory 
based criterion, for this purpose. Let x be the classi-
fied object, and },...,,,{ 321 nccccC = the set of output. 
x is classified as ci with the probability 
)|( xcp i ni ,..,3,2,1= . The entropy of the output is 
then calculated as: 
?
=
?= n
i
ii xcpxcpxCe
1
)|(log)|()|(  (9)
Once )|( xcp i is known, the entropy can be deter-
mined. These parameters can be easily determined in 
PDT, as each incoming case is classified into each 
class with a probability. However, the incoming 
cases in NBC are grouped into one class which is 
assigned the highest score. We then have to estimate 
)|( xcp i  from those scores. Without loss of general-
ity, the probability is estimated as: 
?
=
= n
j
j
i
i
xcscore
xcscore
xcp
1
)|(
)|(
 )|(   (10)
where )|( xcscore i  is the ranking score of x be-
longing to ci. 
4 Experiment Setup and Evaluation 
Several experiments have been designed to evalu-
ate the proposed learning approaches and to reveal 
the impact of linguistic features on learning per-
formance. 700 sentences are extracted from Ta Kong 
Pao (a local Hong Kong Chinese newspaper) finan-
cial version. 600 cases are labeled manually and 100 
left unlabeled. Among those labeled, 400 are used as 
training data, 100 as test data and the rest as held-out 
data. 
4.1 Use of Linguistic Features As Classification 
Attributes 
The impact of a temporal indicator is determined 
by its position in a sentence. In PDT and NBC, we 
consider an indicator located in four positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Cases (2) 
and (3) are ambiguous. The positions of the temporal 
indicators are the same. But it is uncertain whether 
these indicators modify the first or the second event 
if there is no punctuation separating their roles. We 
introduce two methods, namely NA and SAP to 
check if the ambiguity affects the two learning ap-
proaches. 
N(atural) O(rder): the temporal indicators between 
the two events are extracted and compared accord-
ing to their occurrence in the sentences regardless 
which event they modify.  
S(eparate) A(uxiliary) and P(osition) words: we 
try to resolve the above ambiguity with the gram-
matical features of the indicators. In this method, 
we assume that an indicator modifies the first 
event if it is an auxiliary word (e.g. ?), a trend 
verb (e.g. ??) or a position word (e.g. ?); oth-
erwise it modifies the second event. 
Temporal indicators are either tense/aspect or con-
nectives (see Section 2.2). Intuitively, it seems that 
classification could be better achieved if connective 
features are isolated from tense/ aspect features, 
allowing like to be compared with like. Methods 
SC1 and SC2 are designed based on this assumption. 
Table 2 shows the effect the different classification 
methods. 
SC1 (Separate Connecting words 1): it separates 
conjunctions and verbs relating to causality from 
others. They are assumed to contribute to dis-
course structure (intra- or inter-sentence structure), 
and the others contribute to the tense/aspect ex-
pressions for each individual event. They are built 
into 2 separate attributes, one for each event. 
SC2 (Separate Connecting words 2): it is the same 
as SC1 except that it combines the connecting 
word pairs (i.e. as a single pattern) into one attrib-
ute. 
EC (Event Class): it takes event classes into con-
sideration. 
Accuracy Method PDT NBC 
NO 82.00% 81.00% 
SAP 82.20% 81.50% 
SAP +SC1 80.20% 78.00% 
SAP +SC2 81.70% 79.20% 
SAP +EC 85.70% 82.25% 
Table 2. Effect of encoding linguistic features in the dif-
ferent ways 
4.2 Impact of Individual Features 
From linguistic perspectives, 13 features (see Ta-
ble 1) are useful for relative relation resolution. To 
examine the impact of each individual feature, we 
feed a single linguistic feature to the PDT learning 
algorithm one at a time and study the accuracy of the 
resultant classifier. The experimental results are 
given in Table 3. It shows that event classes have 
greatest accuracy, followed by conjunctions in the 
second place, and adverbs in the third. 
Feature Accuracy Feature Accuracy
PT 50.5% VA 56.5% 
VS 54% C 62% 
VC 54% U 51.5% 
TR 50.5% T 57.2% 
P 52.2 % D 61.7% 
PS 58.7% EC 68.2% 
VS 51.2% None 50.5% 
Table 3. Impact of individual linguistic features 
4.3 Discussions 
Analysis of the results in Tables 2 and 3 reveals 
some linguistic insights: 
1. In a situation where temporal indicators appear 
between two events and there is no punctuation 
mark separating them, POS information help re-
duce the ambiguity. Compared with NO, SAP 
shows a slight improvement from 82% to 82.2%. 
But the improvement seems trivial and is not as 
good as our prediction. This might due to the 
small percent of such cases in the corpus. 
2. Separating conjunctions and verbs relating to 
causality from others is ineffective. This reveals 
the complexity of Chinese in connecting expres-
sions. It is because other words (such as adverbs, 
proposition and position words) also serve such 
a function. Meanwhile, experiments based on 
SC1 and SC2 suggest that the connecting ex-
pressions generally involve more than one word 
or phrase. Although the words in a connecting 
expression are separated in a sentence, the action 
is indeed interactive. It would be more useful to 
regard them as one attribute. 
3. The effect of event classification is striking. 
Taking this feature into account, the accuracies 
of both PDT and NB improved significantly. As 
a matter of fact, different event classes may in-
troduce different relations even if they are con-
strained by the same temporal indicators. 
4.4 Collaborative Bootstrapping 
Table 4 presents the evaluation results of the four 
different classification approaches. DM is the default 
model, which classifies all incoming cases as the 
most likely class. It is used as evaluation baseline. 
Compare with DM, PDT and NBC show improve-
ment in accuracy (i.e. above 60% improvement). 
And CB in turn outperforms PDT and NBC. This 
proves that using unlabeled data to boost the per-
formance of the two classifiers is effective. 
Accuracy Approach Close test Open test 
DM 50.50% 55.00% 
NBC 82.25% 72.00% 
PDT 85.70% 74.00% 
CB 88.70% 78.00% 
Table 4. Evaluation of NBC, PDT and CB approaches 
5 Conclusions 
Relative temporal relation resolution received 
growing attentions in recent years. It is important for 
many natural language processing applications, such 
as information extraction and machine translation. 
This topic, however, has not been well studied, es-
pecially in Chinese. In this paper, we propose a 
model for relative temporal relation resolution in 
Chinese. Our model combines linguistic knowledge 
and machine learning approaches. Two learning ap-
proaches, namely probabilistic decision tree (PDT) 
and naive Bayesian classifier (NBC) and 13 linguis-
tic features are employed. Due to the limited labeled 
cases, we also propose a collaborative bootstrapping 
technique to improve learning performance. The 
experimental results show that our approaches are 
encouraging. To our knowledge, this is the first at-
tempt of collaborative bootstrapping, which involves 
two heterogeneous classifiers, in NLP application. 
This lays down the main contribution of our research. 
In this pilot work, temporal indicators are selected 
based on linguistic knowledge. It is time-consuming 
and could be error-prone. This suggests two direc-
tions for future studies. We will try to automate or at 
least semi-automate feature selection process. An-
other future work worth investigating is temporal 
indicator clustering. There are two methods we 
could investigate, i.e. clustering the recognized indi-
cators which occur in training corpus according to 
co-occurrence information or grouping them into 
two semantic roles, one related to tense/aspect ex-
pressions and the other to connecting expressions 
between two events. 
 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and 
CUHK Strategic Grant (account number 4410001). 
 
References 
Allen J., 1981. An Interval-based Represent Action 
of Temporal Knowledge. In Proceedings of 7th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 221-226. Los Altos, CA. 
Blum, A. and Mitchell T., 1998. Combining Labeled 
and Unlabeled Data with Co-Training. In Proceed-
ings of the Eleventh Annual Conference on Com-
putational Learning Theory, Madison, Wisconsin, 
pages 92-100 
Bruce B., 1972. A Model for Temporal References 
and its Application in Question-Answering Pro-
gram. Artificial Intelligence, 3(1):1-25. 
Collins M. and Singer Y, 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 189-196. Uni-
versity of Maryland. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting 
Words from Temporal Expressions. (submitted to 
JAIR) 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of the 7th European 
Meeting of the Association for Computational 
Linguistics, pages 253-260. Dublin, Ireland.  
Lascarides A., Asher N. and Oberlander J., 1992. 
Inferring Discourse Relations in Context. In 
Proceedings of the 30th Meeting of the 
Association for Computational Linguistics, pages 
1-8, Newark, Del. 
Li W.J. and Wong K.F., 2002. A Word-based Ap-
proach for Modeling and Discovering Temporal 
Relations Embedded in Chinese Sentences, ACM 
Transaction on Asian Language Processing, 
1(3):173-206. 
Ma J. and Knight B., 1994. A General Temporal 
Theory. The Computer Journal, 37(2):114- 123. 
M?ntaras L., 1991. A Distance-based Attribute Se-
lection Measure for Decision Tree Induction. Ma-
chine Learning, 6(1): 81?92. 
M?rquez L., Padr? L. and Rodr?guez H., 2000. A 
Machine Learning Approach to POS Tagging. 
Machine Learning, 39(1):59-91. Kluwer Aca-
demic Publishers. 
Partee, B., 1984. Nominal and Temporal Anaphora. 
Linguistics and Philosophy, 7(3):287-324. 
Quinlan J., 1993. C4.5 Programs for Machine 
Learning. Morgan Kauman Press. 
Reichenbach H., 1947. Elements of Symbolic Logic. 
Berkeley CA, University of California Press.  
Siegel E. and McKeown K., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving 
Aspectual Classification and Revealing Linguistic 
Insights. Computational Linguistics, 26(4): 595-
627. 
Wiebe, J.M., O'Hara, T.P., Ohrstrom-Sandgren, T. 
and McKeever, K.J, 1998. An Empirical Approach 
to Temporal Reference Resolution. Journal of Ar-
tificial Intelligence Research, 9:247-293. 
Wong F., Li W., Yuan C., etc., 2002. Temporal Rep-
resentation and Classification in Chinese. Interna-
tional Journal of Computer Processing of Oriental 
Languages, 15(2):211-230. 
Yarowsky D., 1994. Decision Lists for Lexical Am-
biguity Resolution: Application to the Accent Res-
toration in Spanish and French. In Proceeding of 
the 32rd Annual Meeting of ACL, San Francisco, 
CA. 
Zhou X., Dillon T., 1991. A Statistical-heuristic Fea-
ture Selection Criterion for Decision Tree Induc-
tion. IEEE Transaction on Pattern Analysis and 
Machine Intelligence, 13(8): 834-841. 
 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Extractive Summarization using Inter- and Intra- Event Relevance 
 
Wenjie Li, Mingli Wu and Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University 
{cswjli,csmlwu,csluqin}@comp
.polyu.edu.hk 
Wei Xu and Chunfa Yuan 
Department of Computer Science and 
Technology, Tsinghua University 
{vivian00,cfyuan}@mail.ts
inghua.edu.cn 
 
 
 
Abstract 
Event-based summarization attempts to 
select and organize the sentences in a 
summary with respect to the events or 
the sub-events that the sentences de-
scribe. Each event has its own internal 
structure, and meanwhile often relates to 
other events semantically, temporally, 
spatially, causally or conditionally. In 
this paper, we define an event as one or 
more event terms along with the named 
entities associated, and present a novel 
approach to derive intra- and inter- event 
relevance using the information of inter-
nal association, semantic relatedness, 
distributional similarity and named en-
tity clustering. We then apply PageRank 
ranking algorithm to estimate the sig-
nificance of an event for inclusion in a 
summary from the event relevance de-
rived. Experiments on the DUC 2001 
test data shows that the relevance of the 
named entities involved in events 
achieves better result when their rele-
vance is derived from the event terms 
they associate. It also reveals that the 
topic-specific relevance from documents 
themselves outperforms the semantic 
relevance from a general purpose 
knowledge base like Word-Net. 
 
 
1. Introduction 
Extractive summarization selects sentences 
which contain the most salient concepts in 
documents. Two important issues with it are 
how the concepts are defined and what criteria 
should be used to judge the salience of the con-
cepts. Existing work has typically been based on 
techniques that extract key textual elements, 
such as keywords (also known as significant 
terms) as weighed by their tf*idf score, or con-
cepts (such as events or entities) with linguistic 
and/or statistical analysis. Then, sentences are 
selected according to either the important textual 
units they contain or certain types of inter-
sentence relations they hold.  
Event-based summarization which has e-
merged recently attempts to select and organize 
sentences in a summary with respect to events or 
sub-events that the sentences describe. With re-
gard to the concept of events, people do not 
have the same definition when introducing it in 
different domains. While traditional linguistics 
work on semantic theory of events and the se-
mantic structures of verbs, studies in 
information retrieval (IR) within topic detection 
and tracking framework look at events as 
narrowly defined topics which can be 
categorized or clustered as a set of related 
documents (TDT). IR events are broader (or to 
say complex) events in the sense that they may 
include happenings and their causes, 
consequences or even more extended effects. In 
the information extraction (IE) community, 
events are defined as the pre-specified and struc-
tured templates that relate an action to its 
participants, times, locations and other entities 
involved (MUC-7). IE defines what people call 
atomic events. Regardless of their distinct perspectives, peo-
ple all agree that events are collections of activi-
ties together with associated entities. To apply 
the concept of events in the context of text sum-
marization, we believe it is more appropriate to 
consider events at the sentence level, rather than 
at the document level. To avoid the complexity 
of deep semantic and syntactic processing, we 
complement the advantages of statistical 
techniques from the IR community and struc-
tured information provided by the IE community. 
369
 We propose to extract semi-structured events 
with shallow natural language processing (NLP) 
techniques and estimate their importance for 
inclusion in a summary with IR techniques. 
Though it is most likely that documents nar-
rate more than one similar or related event, most 
event-based summarization techniques reported 
so far explore the importance of the events inde-
pendently. Motivated by this observation, this 
paper addresses the task of event-relevance 
based summarization and explores what sorts of 
relevance make a contribution. To this end, we 
investigate intra-event relevance, that is action-
entity relevance, and inter-event relevance, that 
is event-event relevance. While intra-event rele-
vance is measured with frequencies of the asso-
ciated events and entities directly, inter-event 
relevance is derived indirectly from a general 
WordNet similarity utility, distributional simi-
larity in the documents to be summarized, 
named entity clustering and so on. Pagerank 
ranking algorithm is then applied to estimate the 
event importance for inclusion in a summary 
using the aforesaid relevance.  
The remainder of this paper is organized as 
follows. Section 2 introduces related work. Sec-
tions 3 introduces our proposed event-based 
summarization approaches which make use of 
intra- and inter- event relevance. Section 4 pre-
sents experiments and evaluates different ap-
proaches. Finally, Section 5 concludes the paper. 
2. Related Work 
Event-based summarization has been investi-
gated in recent research. It was first presented in 
(Daniel, Radev and Allison, 2003), who treated 
a news topic in multi-document summarization 
as a series of sub-events according to human 
understanding of the topic. They determined the 
degree of sentence relevance to each sub-event 
through human judgment and evaluated six ex-
tractive approaches. Their paper concluded that 
recognizing the sub-events that comprise a sin-
gle news event is essential for producing better 
summaries. However, it is difficult to automati-
cally break a news topic into sub-events.  
Later, atomic events were defined as the rela-
tionships between the important named entities 
(Filatova and Hatzivassiloglou, 2004), such as 
participants, locations and times (which are 
called relations) through the verbs or action 
nouns labeling the events themselves (which are 
called connectors). They evaluated sentences 
based on co-occurrence statistics of the named 
entity relations and the event connectors in-
volved. The proposed approach claimed to out-
perform conventional tf*idf approach. Appar-
ently, named entities are key elements in their 
model. However, the constraints defining events 
seemed quite stringent.  
The application of dependency parsing, 
anaphora and co-reference resolution in recog-
nizing events were presented involving NLP and 
IE techniques more or less (Yoshioka and Hara-
guchi, 2004), (Vanderwende, Banko and Mene-
zes, 2004) and (Leskovec, Grobelnik and Fral-
ing, 2004). Rather than pre-specifying events, 
these efforts extracted (verb)-(dependent rela-
tion)-(noun) triples as events and took the triples 
to form a graph merged by relations.  
As a matter of fact, events in documents are 
related in some ways. Judging whether the sen-
tences are salient or not and organizing them in 
a coherent summary can take advantage from 
event relevance. Unfortunately, this was ne-
glected in most previous work. Barzilay and La-
pata (2005) exploited the use of the distribu-
tional and referential information of discourse 
entities to improve summary coherence. While 
they captured text relatedness with entity transi-
tion sequences, i.e. entity-based summarization, 
we are particularly interested in relevance be-
tween events in event-based summarization. 
Extractive summarization requires ranking 
sentences with respect to their importance. 
Successfully used in Web-link analysis and 
more recently in text summarization, Google?s 
PageRank (Brin and Page, 1998) is one of the 
most popular ranking algorithms. It is a kind of 
graph-based ranking algorithm deciding on the 
importance of a node within a graph by taking 
into account the global information recursively 
computed from the entire graph, rather than re-
lying on only the local node-specific infor-
mation. A graph can be constructed by adding a 
node for each sentence, phrase or word. Edges 
between nodes are established using inter-
sentence similarity relations as a function of 
content overlap or grammatically relations be-
tween words or phrases.  
The application of PageRank in sentence ex-
traction was first reported in (Erkan and Radev, 
2004). The similarity between two sentence 
nodes according to their term vectors was used 
to generate links and define link strength. The 
same idea was followed and investigated exten-
370
 sively (Mihalcea, 2005). Yoshioka and Haragu-
chi (2004) went one step further toward event-
based summarization. Two sentences were 
linked if they shared similar events. When tested 
on TSC-3, the approach favoured longer sum-
maries. In contrast, the importance of the verbs 
and nouns constructing events was evaluated 
with PageRank as individual nodes aligned by 
their dependence relations (Vanderwende, 2004; 
Leskovec, 2004).  
Although we agree that the fabric of event 
constitutions constructed by their syntactic rela-
tions can help dig out the important events, we 
have two comments. First, not all verbs denote 
event happenings. Second, semantic similarity 
or relatedness between action words should be 
taken into account. 
3. Event-based Summarization 
3.1. Event Definition and Event Map 
Events can be broadly defined as ?Who did 
What to Whom When and Where?. Both lin-
guistic and empirical studies acknowledge that 
event arguments help characterize the effects of 
a verb?s event structure even though verbs or 
other words denoting event determine the se-
mantics of an event. In this paper, we choose 
verbs (such as ?elect?) and action nouns (such as 
?supervision?) as event terms that can character-
ize or partially characterize actions or incident 
occurrences. They roughly relate to ?did What?. 
One or more associated named entities are con-
sidered as what are denoted by linguists as event 
arguments. Four types of named entities are cur-
rently under the consideration. These are <Per-
son>, <Organization>, <Location> and <Date>. 
They convey the information of ?Who?, 
?Whom?, ?When? and ?Where?. A verb or an 
action noun is deemed as an event term only 
when it presents itself at least once between two 
named entities. 
Events are commonly related with one an-
other semantically, temporally, spatially, caus-
ally or conditionally, especially when the docu-
ments to be summarized are about the same or 
very similar topics. Therefore, all event terms 
and named entities involved can be explicitly 
connected or implicitly related and weave a 
document or a set of documents into an event 
fabric, i.e. an event graphical representation (see 
Figure 1). The nodes in the graph are of two 
types. Event terms (ET) are indicated by rectan-
gles and named entities (NE) are indicated by 
ellipses. They represent concepts rather than 
instances. Words in either their original form or 
morphological variations are represented with a 
single node in the graph regardless of how many 
times they appear in documents. We call this 
representation an event map, from which the 
most important concepts can be pick out in the 
summary. 
 
 
 
Figure 1 Sample sentences and their graphical representation 
 
 
The advantage of representing with separated 
action and entity nodes over simply combining 
them into one event or sentence node is to pro-
vide a convenient way for analyzing the rele-
vance among event terms and named entities 
either by their semantic or distributional similar-
ity. More importantly, this favors extraction of 
concepts and brings the conceptual compression 
available. 
We then integrate the strength of the connec-
tions between nodes into this graphical model in 
terms of the relevance defined from different 
perspectives. The relevance is indicated by 
),( ji nodenoder , where inode  and jnode  repre-
sent two nodes, and are either event terms ( iet ) 
or named entities ( jne ). Then, the significance 
of each node, indicated by )( inodew , is calcu-
<Organization> America Online </Organization> was to buy <Organization> 
Netscape </Organization> and forge a partnership with <Organization> Sun 
</Organization>, benefiting all three and giving technological independence 
from <Organization> Microsoft </Organization>. 
371
 lated with PageRank ranking algorithm. Sec-
tions 3.2 and 3.3 address the issues of deriving 
),( ji nodenoder  according to intra- or/and inter- 
event relevance and calculating )( inodew  in de-
tail. 
3.2 Intra- and Inter- Event Relevance 
We consider both intra-event and inter-event 
relevance for summarization. Intra-event rele-
vance measures how an action itself is associ-
ated with its associated arguments. It is indi-
cated as ),( NEETR  and ),( ETNER  in Table 1 
below. This is a kind of direct relevance as the 
connections between actions and arguments are 
established from the text surface directly. No 
inference or background knowledge is required. 
We consider that when the connection between 
an event term iet  and a named entity jne  is 
symmetry, then TNEETRETNER ),(),( = . Events 
are related as explained in Section 2. By means 
of inter-event relevance, we consider how an 
event term (or a named entity involved in an 
event) associate to another event term (or an-
other named entity involved in the same or dif-
ferent events) syntactically, semantically and 
distributionally. It is indicated by ),( ETETR or 
),( NENER in Table 1 and measures an indirect 
connection which is not explicit in the event 
map needing to be derived from the external 
resource or overall event distribution. 
 Event Term 
(ET) 
Named En-
tity (NE) 
Event Term (ET) ),( ETETR  ),( NEETR  
Named Entity (NE) ),( ETNER  ),( NENER
Table 1 Relevance Matrix 
The complete relevance matrix is: 
??
???
?=
),(),(
),(),(
NENERETNER
NEETRETETR
R  
The intra-event relevance ),( NEETR can be 
simply established by counting how many times 
iet  and jne  are associated, i.e.  
),(),( jijiDocument neetfreqneetr =  (E1) 
One way to measure the term relevance is to 
make use of a general language knowledge base, 
such as WordNet (Fellbaum 1998). Word-
Net::Similarity is a freely available software 
package that makes it possible to measure the 
semantic relatedness between a pair of concepts, 
or in our case event terms, based on WordNet 
(Pedersen, Patwardhan and Michelizzi, 2004). It 
supports three measures. The one we choose is 
the function lesk. 
),(),(),( jijijiWordNet etetlesketetsimilarityetetr ==
      (E2) 
Alternatively, term relevance can be meas-
ured according to their distributions in the speci-
fied documents. We believe that if two events 
are concerned with the same participants, occur 
at same location, or at the same time, these two 
events are interrelated with each other in some 
ways. This observation motivates us to try deriv-
ing event term relevance from the number of 
name entities they share. 
|)()(|),( jijiDocument etNEetNEetetr ?=  (E3) 
Where )( ietNE is the set of named entities iet  
associate. | | indicates the number of the ele-
ments in the set. The relevance of named entities 
can be derived in a similar way. 
|)()(|),( jijiDocument neETneETnener ?=  (E4) 
The relevance derived with (E3) and (E4) are 
indirect relevance. In previous work, a cluster-
ing algorithm, shown in Figure 2, has been pro-
posed (Xu et al 2006) to merge the named en-
tity that refer to the same person (such as 
Ranariddh, Prince Norodom Ranariddh and Presi-
dent Prince Norodom Ranariddh). It is used for 
co-reference resolution and aims at joining the 
same concept into a single node in the event 
map. The experimental result suggests that 
merging named entity improves performance in 
some extend but not evidently. When applying 
the same algorithm for clustering all four types 
of name entities in DUC data, we observe that 
the name entities in the same cluster do not al-
ways refer to the same objects, even when they 
are indeed related in some way. For example, 
?Mississippi? is a state in the southeast United 
States, while ?Mississippi River? is the second-
longest rever in the United States and flows 
through ?Mississippi?. 
Step1: Each name entity is represented by 
ikiii wwwne ...21= , where iw  is the ith 
word in it. The cluster it belongs to, in-
dicated by )( ineC , is initialled by 
ikii www ...21 itself.  
Step2: For each name entity  
           ikiii wwwne ...21=  
For each name entity 
372
 jljjj wwwne ...21= , if )( ineC  is a 
sub-string of )( jneC , then 
)()( ji neCneC = . 
Continue Step 2 until no change occurs. 
Figure 2 The algorithm proposed to merge the 
named entities 
Location Person Date Organization
Mississippi 
 
Professor Sir 
Richard 
Southwood 
first six 
months of 
last year 
Long Beach 
City Council 
Sir Richard 
Southwood 
San Jose City 
Council 
Mississippi 
River 
Richard 
Southwood 
last year 
City Council 
Table 2 Some results of the named entity 
merged 
It therefore provides a second way to measure 
named entity relevance based on the clusters 
found. It is actually a kind of measure of lexical 
similarity. 
??
?=
otherwise      ,0
cluster same in the are ,      ,1
),( jijiCluster
nene
nener
     (E5) 
In addition, the relevance of the named enti-
ties can be sometimes revealed by sentence con-
text. Take the following most frequently used 
sentence patterns as examples: 
 
Figure 3 The example patterns  
Considering that two neighbouring name enti-
ties in a sentence are usually relevant, the fol-
lowing window-based relevance is also experi-
mented with. 
??
?=
otherwise      ,0
size  windowspecified-pre a within are ,      1,
),(
ji
jiPattern
nene
nener
     (E6) 
3.3 Significance of Concepts 
The significance score, i.e. the weight 
)( inodew  of each inode , is then estimated recur-
sively with PageRank ranking algorithm which 
assigns the significance score to each node ac-
cording to the number of nodes connecting to it 
as well as the strength of their connections. The 
equation calculating )( inodew using PageRank 
of a certain inode  is shown as follows. 
)
),(
)(
...
),(
)(
...
),(
)(()1()(
1
1
ti
t
ji
j
i
i
nodenoder
nodew
nodenoder
nodew
nodenoder
nodewddnodew
+++
++?=
 (E7) 
In (E7), jnode ( tj ,...2,1= , ij ? ) are the 
nodes linking to inode . d is the factor used to 
avoid the limitation of loop in the map structure. 
It is set to 0.85 experimentally. The significance 
of each sentence to be included in the summary 
is then obtained from the significance of the 
events it contains. The sentences with higher 
significance are picked up into the summary as 
long as they are not exactly the same sentences. 
We are aware of the important roles of informa-
tion fusion and sentence compression in sum-
mary generation. However, the focus of this pa-
per is to evaluate event-based approaches in ex-
tracting the most important sentences. Concep-
tual extraction based on event relevance is our 
future direction. 
4. Experiments and Discussions 
To evaluate the event based summarization ap-
proaches proposed, we conduct a set of experi-
ments on 30 English document sets provide by 
the DUC 2001 multi-document summarization 
task. The documents are pre-processed with 
GATE to recognize the previously mentioned 
four types of name entities. On average, each set 
contains 10.3 documents, 602 sentences, 216 
event terms and 148.5 name entities. 
To evaluate the quality of the generated 
summaries, we choose an automatic summary 
evaluation metric ROUGE, which has been used 
in DUCs. ROUGE is a recall-based metric for 
fixed length summaries. It bases on N-gram co-
occurrence and compares the system generated 
summaries to human judges (Lin and Hovy, 
2003). For each DUC document set, the system 
creates a summary of 200 word length and pre-
sent three of the ROUGE metrics: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on longest common sub-
sequence weighed by the length) in the follow-
ing experiments and evaluations.  
We first evaluate the summaries generated 
based on ),( NEETR  itself. In the pre-evaluation 
experiments, we have observed that some fre-
<Person>, a-position-name of <Organization>, 
does something. 
<Person> and another <Person> do something. 
373
 quently occurring nouns, such as ?doctors? and 
?hospitals?, by themselves are not marked by 
general NE taggers. But they indicate persons, 
organizations or locations. We compare the 
ROUGE scores of adding frequent nouns or not 
to the set of named entities in Table 3. A noun is 
considered as a frequent noun when its fre-
quency is larger than 10. Roughly 5% improve-
ment is achieved when high frequent nouns are 
taken into the consideration. Hereafter, when we 
mention NE in latter experiments, the high fre-
quent nouns are included. 
),( NEETR  NE Without High 
Frequency Nouns 
NE With High 
Frequency Nouns
ROUGE-1 0.33320 0.34859 
ROUGE-2 0.06260 0.07157 
ROUGE-W 0.12965 0.13471 
Table 3 ROUGE scores using ),( NEETR  itself 
Table 4 below then presents the summariza-
tion results by using ),( ETETR  itself. It com-
pares two relevance derivation approaches, 
WordNetR  and DocumentR . The topic-specific rele-
vance derived from the documents to be summa-
rized outperforms the general purpose Word-Net 
relevance by about 4%. This result is reasonable 
as WordNet may introduce the word relatedness 
which is not necessary in the topic-specific 
documents. When we examine the relevance 
matrix from the event term pairs with the high-
est relevant, we find that the pairs, like ?abort? 
and ?confirm?, ?vote? and confirm?, do reflect 
semantics (antonymous) and associated (causal) 
relations to some degree.  
),( ETETR  Semantic Rele-
vance from 
Word-Net 
Topic-Specific 
Relevance from 
Documents 
ROUGE-1 0.32917 0.34178 
ROUGE-2 0.05737 0.06852 
ROUGE-W 0.11959 0.13262 
Table 4 ROUGE scores using ),( ETETR  itself 
Surprisingly, the best individual result is from 
document distributional similarity DocumentR  
),( NENE  in Table 5. Looking more closely, we 
conclude that compared to event terms, named 
entities are more representative of the docu-
ments in which they are included. In other words, 
event terms are more likely to be distributed 
around all the document sets, whereas named 
entities are more topic-specific and therefore 
cluster in a particular document set more. Ex-
amples of high related named entities in rele-
vance matrix are ?Andrew? and ?Florida?, 
?Louisiana? and ?Florida?. Although their rele-
vance is not as explicit as the same of event 
terms (their relevance is more contextual than 
semantic), we can still deduce that some events 
may happen in both Louisiana and Florida, or 
about Andrew in Florida. In addition, it also 
shows that the relevance we would have ex-
pected to be derived from patterns and clustering 
can also be discovered by ),( NENERDocument . 
The window size is set to 5 experimentally in 
window-based practice.  
),( NENER Relevance 
from 
Documents
Relevance 
from 
Clustering 
Relevance 
from Window-
based Context
ROUGE-1 0.35212 0.33561 0.34466 
ROUGE-2 0.07107 0.07286 0.07508 
ROUGE-W 0.13603 0.13109 0.13523 
Table 5 ROUGE scores using ),( NENER  itself 
Next, we evaluate the integration of 
),( NEETR , ),( ETETR  and ),( NENER . As 
DUC 2001 provides 4 different summary sizes 
for evaluation, it satisfies our desire to test the 
sensibility of the proposed event-based summa-
rization techniques to the length of summaries. 
While the previously presented results are 
evaluated on 200 word summaries, now we 
move to check the results in four different sizes, 
i.e. 50, 100, 200 and 400 words. The experi-
ments results show that the event-based ap-
proaches indeed prefer longer summaries. This 
is coincident with what we have hypothesized. 
For this set of experiments, we choose to inte-
grate the best method from each individual 
evaluation presented previously. It appears that 
using the named entities relevance which is de-
rived from the event terms gives the best 
ROUGE scores in almost all the summery sizes. 
Compared with the results provided in (Filatova 
and Hatzivassiloglou, 2004) whose average 
ROUGE-1 score is below 0.3 on the same data 
set, the significant improvement is revealed. Of 
course, we need to test on more data in the fu-
ture. 
),( NENER 50 100 200 400 
ROUGE-1 0.22383 0.28584 0.35212 0.41612
ROUGE-2 0.03376 0.05489 0.07107 0.10275
ROUGE-W 0.10203 0.11610 0.13603 0.13877
),( NEETR 50 100 200 400 
ROUGE-1 0.22224 0.27947 0.34859 0.41644
ROUGE-2 0.03310 0.05073 0.07157 0.10369
ROUGE-W 0.10229 0.11497 0.13471 0.13850
),( ETETR 50 100 200 400 
374
 ROUGE-1 0.20616 0.26923 0.34178 0.41201
ROUGE-2 0.02347 0.04575 0.06852 0.10263
ROUGE-W 0.09212 0.11081 0.13262 0.13742
),( NEETR + 
),( ETETR + 
),( NENER  
 
50 
 
100 
 
200 
 
400 
ROUGE-1 0.21311 0.27939 0.34630 0.41639
ROUGE-2 0.03068 0.05127 0.07057 0.10579
ROUGE-W 0.09532 0.11371 0.13416 0.13913
Table 6 ROUGE scores using complete R matrix 
and with different summary lengths 
As discussed in Section 3.2, the named enti-
ties in the same cluster may often be relevant but 
not always be co-referred. In the following last 
set of experiments, we evaluate the two ways to 
use the clustering results. One is to consider 
them as related as if they are in the same cluster 
and derive the NE-NE relevance with (E5). The 
other is to merge the entities in one cluster as 
one reprehensive named entity and then use it in 
ET-NE with (E1). The rationality of the former 
approach is validated. 
 Clustering is 
used to derive 
NE-NE 
Clustering is used to 
merge entities and 
then to derive ET-NE
ROUGE-1 0.34072 0.33006 
ROUGE-2 0.06727 0.06154 
ROUGE-W 0.13229 0.12845 
Table 7 ROUGE scores with regard to how to 
use the clustering information 
5. Conclusion 
In this paper, we propose to integrate event-
based approaches to extractive summarization. 
Both inter-event and intra-event relevance are 
investigated and PageRank algorithm is used to 
evaluate the significance of each concept (in-
cluding both event terms and named entities). 
The sentences containing more concepts and 
highest significance scores are chosen in the 
summary as long as they are not the same sen-
tences.  
To derive event relevance, we consider the 
associations at the syntactic, semantic and con-
textual levels. An important finding on the DUC 
2001 data set is that making use of named entity 
relevance derived from the event terms they as-
sociate with achieves the best result. The result 
of 0.35212 significantly outperforms the one 
reported in the closely related work whose aver-
age is below 0.3. We are interested in the issue 
of how to improve an event representation in 
order to build a more powerful event-based 
summarization system. This would be one of our 
future directions. We also want to see how con-
cepts rather than sentences are selected into the 
summary in order to develop a more flexible 
compression technique and to know what char-
acteristics of a document set is appropriate for 
applying event-based summarization techniques.  
 
Acknowledgements 
The work presented in this paper is supported 
partially by Research Grants Council on Hong 
Kong (reference number CERG PolyU5181/03E) 
and partially by National Natural Science Foun-
dation of China (reference number: NSFC 
60573186). 
 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL 2003, pp71-78. 
Christiane Fellbaum. 1998, WordNet: An Electronic 
Lexical Database. MIT Press. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive summarization. In Pro-
ceedings of ACL 2004 Workshop on Summariza-
tion, pp104-111.  
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of Docu-
ment Semantic Graphs for Document Summariza-
tion. In LinkKDD 2004.  
Lucy Vanderwende, Michele Banko and Arul Mene-
zes. 2004. Event-Centric Summary Generation. In 
Working Notes of DUC 2004. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information. In Working Notes 
of NTCIR-4, Tokyo. 
MUC-7. http://www-nlpir.nist.gov/related_projects/ 
muc/proceeings/ muc_7_toc.html 
Naomi Daniel, Dragomir Radev and Timothy Allison. 
2003. Sub-event based Multi-document Summari-
zation. In Proceedings of the HLT-NAACL 2003 
Workshop on Text Summarization, pp9-16. 
375
 Page Lawrence, Brin Sergey, Motwani Rajeev and 
Winograd Terry. 1998. The PageRank Citation 
Ranking: Bring Order to the Web. Technical Re-
port, Stanford University. 
Rada Mihalcea. 2005. Language Independent Extrac-
tive Summarization. ACL 2005 poster. 
Regina Barzilay and Michael Elhadad. 2005. Model-
ling Local Coherence: An Entity-based Approach. 
In Proceedings of ACL, pp141-148. 
TDT. http://projects.ldc.upenn.edu/TDT. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measur-
ing the Relatedness of Concepts. In Proceedings of 
AAAI, pp25-29. 
Wei Xu, Wenjie Li, Mingli Wu, Wei Li and Chunfa 
Yuan. 2006. Deriving Event Relevance from the 
Ontology Constructed with Formal Concept 
Analysis, in Proceedings of CiCling?06, pp480-
489. 
 
376
A Clustering Algorithm-for Chinese Adjectives and Nouns 1 
Yang Wen ~, Chunfa Yuan ~, Changning Huang 2
~State Key Laboratory of Intelligent Technology and System 
Deptartment ofComputer Science & Technology, Tsinghua University, 
Beijing 100084, P.R.C. 
2Microsoft Research, China 
Email: ycf@~1000e c~ t~in~hua edu cn 
Key Words: 
? bidirectional hierarchical clustering, 
collocations, minimum description length, 
collocational degree, revisional distance 
Abstract 
This paper proposes a bidirctional 
hierarchical clustering algorithm for 
simultaneously clustering words of different 
parts of speech based on collocations. The 
algorithm is composed of cycles of two 
kinds of alternate clustering processes. We 
construct an objective function based on 
Minimum Description Length. To. partly 
solve the problem caused by sparse data two 
concepts of collocational degree and 
revisional distance are presented. 
1 Introduction 
Recently research on the compositional 
frames (classification and collocafional 
relationship of words) for Chinese words has 
been described in Ji et al (1996)\[1\], Ji 
(1997)\[2\]. The objective of their work is to 
obtain the clusters of words of different parts 
of speech and to derive the coUocational 
relationship between different clusters from 
the collocafional relationship between words 
of different categories. 
There are two ways to construct the 
clusters: One is to get clusters from 
thesaurus classified manually by linguists. 
But the fact is that words with the same 
meanings do not always have the same 
ability of collocating with other words. The 
method isn't fit for the NLP problems under 
our consideration. Another way is to get 
clusters automatically by computing on the 
distribution environments of words based on 
statistical method. The distribution 
environment of a word is the set of words of 
other parts of speech that can be collocated 
with it. We employ the second method in our 
work. 
Previous research usually gets the 
clusters of Words of a certain part of speech 
based on their distribution environments. But 
Supported by National Natural Science Foundation of China (69'7?3031 ) and "973" Project (G1998030507). 
124 
we accept he assumption that the clustering 
processes of words of different parts of 
speech are inherently related. For example, 
having collocations between Chinese 
adjectives and nouns and if we take on nouns 
as entities and adjectives as features of 
nouns' distribution environments, we can 
obtain clusters of nouns and vice versa. The 
key of the relationship of the two clustering 
processes is that they use the same 
collocations. Therefore we consider the 
question of clustering the nouns and 
adjectives imultaneously. Li's work shows 
that they optimize the clustering results 
based on this viewpoint (Li et al, 1997)\[3\]. 
But they don't explain how to get initial 
clusters and their scale of problem is too 
small. 
In this paper, we propose an algorithm 
named bidirectional hierarchical c ustering to 
attempt answering the question. 
2 Concepts 
2.1 Problem Description 
Our problem can be described as follows: 
given the set of adjectives A, the set of nouns 
N and the collocation instances, our system 
will construct a partition P~ over N and a 
partition Pa over A that respectively contain 
sets of nouns and sets of adjectives. And 
both partitions meet he condition that words 
in the same set (called cluster) have similar 
semantic distribution environment. 
2.2 Partitions and Clusters 
Let S be a set, S~ c S(i = 1,2,...,n). If 
Ys ={ S~ } satisfies that 
n 
(1)US, =s 
i=1 
(2) S, NSj = O, Vi, j = 1,2,...n,i ~ j 
Then Ps is a partition over S. 
In this paper, we call A i ~Pa an 
"adjective cluster" and Ni ~P~v a "noun 
cluster". And we want to obtain the 
composition of partitions < PA, PN > as the 
clustering remit. 
2.3 Distance between Clusters 
In order to measure the distance between 
clusters of the same part of speech, we use 
the following equations: 
1 \[~'\["1~/I (1) disa(Ai,Aj) 
and 
lie, U%l (2) 
where O~ is the distribution 
environment of ~ and is make up of nouns 
which can be collocated with 
distribution environment 
composed of adjectives 
collocated with N i . ~ i  
A,. ~ is the 
of N~ and is 
which can be 
andW s follow 
similar definitions. This distance is a kind of 
Euchdean distance. 
125 
2.4 Colloeational Degree 
Since redundant collocations might be 
created during clustering, the concept 
"collocational'degree" is used to measure ~e 
collocational relationship between a cluster 
and its distribution environment. The 
coUocational degree is defined as the ratio of 
the existing collocation instances between 
the cluster and its distribution envffonment 
to all possible collocations generated by 
them. Thus, 
deg~ = I(a? I a 4,? ?,,a? c}l 
1411-,I (3) 
and 
degN~ - I ( -v  I- N,,v  ,nV, sC}l (4) 
IN, till 
where C is the set of all existing instances. 
2.5 Redundant Ratio 
After we get the collocational degree of 
a cluster, redundant ratio (marked as r) is 
calculated to measure the whole performance 
of the clustering result. We define the 
redundant ratio as 1 minus the ratio of all 
existing instances to all possible colloca- 
tions generated by all clusters (including 
nouns and adjectives) and their distribution 
environments. Sor is calculated as 
r : 1- 21cl 
EIA, II*,I+EIN, II'e,I (5) 
i i 
3 A Bidirectional Hierarchical 
Clustering Algorithm 
Usually a hierarchical clustering 
algorithm \[7\] constructs a clustering "tree" 
by combining small clusters into large ones 
or (lividing large clusters into small ones. 
The bidirectional hierarchical clustering 
algorithm proposed by us is composed of 
two kinds of alternate clustering processes. 
The algorithm flow is described as 
follows: 
1)Initially, regard every noun and 
adjective ach as a cluster. Calculate 
the distances between clusters of the 
same part of speech. 
2) Suppose without loss of generality 
that we choose to cluster nouns first. 
Select two noun clusters N, & N s 
of the minimum distance and 
integrate them into a new one N~'. 
J) Calculate the collocational degree of 
the new cluster. Adjust the sequence 
numbers of the original clusters and 
the relational information of adjective 
clusters. 
4) Calculate the distances between the 
new cluster and other clusters. 
5) Repeat from step 2) to 4) until the 
satisfaction of certain condition. For 
example, the number of the clusters 
haas decreased to certain amount. 2 
6) Similarly, we can follow the same 
steps from 2) to 5) for constructing 
adjective clusters, completing one 
cycle, of clustering processes of nouns 
and adjectives. 
7) Repeat from step 2) to 6) until the 
2 In this paper, we set he proportion is 20%. 
126 
objective function 3 reaches the 
minimum value. 
One advantage of this algorithm is that: 
when two clusters of nouns have similar 
distribution environments, "they might be 
classified into one cluster. This information 
can be delivered to the clusters of adjectives 
that respectively collocate with them by the 
clustering process of nouns. Thus these 
clusters of adjectives have great possibility 
to be combined into one cluster, while the 
ordinary hierarchical clustering algorithm 
can not do it. 
4 An Objective Function Based on 
MDL 
The objective function is designed to 
control the processes of clustering words 
based on the Minimum Description Length 
(MDL) principle. According to MDL, the 
best probability model for a given set of data 
is a model that uses the shortest code length 
for encoding the model itself and the given 
data relative to it \[4\] \[5\]. We regard the 
clusters as the model for the collocations of 
adjectives and nouns. The objective function 
is defined as the sum of the code length for 
the model ("model description length") and 
that for the data ("data description length"). 
When the clustering result minimises the 
objective function, the bidirectional 
processes hould be stopped and the result is 
the best probable one. The objective function 
based on MDL trade-offs between the 
simplicity of a model and its accuracy in 
fitting to the data, which are respectively 
quantified by the model description length 
and the data description length. 
3 Described later in section 4. 
The following are the formulas to 
calculate the objective function L: 
L = L,,,od + L,~ t (6) 
Lad is the model description length 
calculated as 
L,~ 
kA1 1 *~'1 1 
= -X-H--log2 -~-- ~.~--  log 2 k~r+l (7) 
i=1 ~A tt'A i=1 ~/q" 
= log2(kAk~) + 1 
Where k A and k N respectively denote 
the number of clusters of adjectives and 
nouns. "+1" means that the algorithm needs 
one bit to indicate whether the collocational 
relationship between the two clusters exists. 
L,~ t is composed of the data description 
length of adjectives and that of nouns, 
namely 
(8) 
And the two types of data description 
length are calculated as follows 
\[?,1 
LeQt (A) - - -  - -~- - - '~ log2 1 
.__. j=. 1411Nkl 
(9) 
Vj,~j ~ ~, and ~ j~N,  
L o,(N) 1 
,__, k k. 
(10) 
~~log  2 1 
Vj, ~ j ~.W, and v i ~ A~ 
5 Our Experiment 
We take the words and collocations 
127 
6 Discussions 
6.1 Rivisional Distance 
When we combine clusters into a new 
cluster, their distribution environments will 
be combined as well. The combination of 
clusters and their distribution environments 
might very likely generate redundant 
collocations that are not listed in the 
thesaurus. With the word clustering 
processes going on, there might be more and 
more redundant collocations. They will 
obviously affect the accuracy of the 
distances between clusters. When calculating 
the distances, the redundant collocations 
must be considered. So the question is how 
to revise the distance quation. Notice that 
the collocational degree defined in the above 
measures the collocational relationship 
gathered in Ni's Thesaurus \[6\] to test our 
algorithm. From Ni's thesaurus, we obtain 
2,569 adjectives, 4,536 nouns and 37,346 
collocations between adjectives and nouns. 
Table 1 shows results of using 5 
different revisional distance formulas 
discussed in the next section. Because the 
length of this paper is limited, we only give 
some examples (10 clusters for each part of 
speech) of clusters in section 8. We can see 
that the redundant ratio obviously decreases 
by using the revisional distance, and the 
result that has the lowest redundant ratio 
corresponds of the minimum value of the 
objective function. By human evaluation, 
most clusters contain the words that have 
similar meanings and distribution 
environments. So our algorithm proves to be 
effective for word clustering based on 
collocations. 
Table 1: Results of different revisional distances 
Revisional distance k~ k u L r 
Not used 409 550 20. 067 99. 01% 
dis' = -deg lnd is  397 610 20. 082 86. 96% 
dis' = d is /deg 383 595 20. 002 78. 78% 
dis' = d is /~-~ 373 586 20. 017 80. 39% 
dis" = -d!s ln  deg 395 557 20. 007 80. 08% 
However, the redundant ratio is still very 
large. The main cause is that existing 
between a cluster and its distribution 
environment. Obviously under the same 
instances are too sparse, covering only 
0.32% of all possible collocations. So 
another advantage of our algorithm is that 
we can acquire many new reasonable 
collocations not gathered in the thesaurus, ff
we add the new collocations into initial 
thesaurus and execute the algorithm on new 
data set, the performance will have great 
potential to improve. It is further work that 
can be carried out in the future. 
distance, clusters having higher coUocational 
degree have more higher similarity between 
each other (because they have more actual 
collocations) than those having lower 
collocational degree. So the collocational 
degree can be used to revise the distance 
equations. 
There are two problems that should-be 
considered when we design the revisional 
distance quations. The first one is to convert 
128 
the collocational degrees of two clusters into 
one collocational degree as the revisional 
factor for distance equations. It is the 
average collocational degree, marked as 
deg, calculated by 
deg A - 
and 
deg I ,I141 + deg A,I ,114 
ua>,l (II) 
deg N, Iv, llN, l + deg N, Iv, IN I 
In fact it is the collocational degree of 
the new cluster into which if we assume 
combining the two original clusters. 
The second problem is that the revjsonal 
distance quations hould keep coherent of 
monotonicity with the original distance. It 
means that under the same average collo- 
cational degree, the revional distance should 
keep the same (or opposite) monotonicity 
with the original distance, and under the 
same original distance, the revional distance 
should keep the same (or opposite) 
monotonicity with the average collocational 
degree. 
In this paper, four simple revisional 
distance equations are presented based on 
consideration of the upper two problems. 
They are: 
a)d is '=-deg lnd is  
dis b) dis' - 
deg 
? dis c) dis' - 
 /deg 
d) dis' = -d i s  In deg 
Where dis'  denotes the revional 
distance and dis denotes the original 
distance. 
From the comparison of the upper 
different results (shown in Table 1), we can 
draw the conclusion that using revisonal 
distance equations can increase the 
clustering accuracy remarkably. 
6.2 Determinant of Objective Function's 
Minimum Value 
The clustering algorithm terminates 
when the objective function is minimized. 
As a result it is very important to find out the 
function's minimum value. After analyzing 
the objective function, we find that it 
normally monotonically declines with 
clustering processes going on until it gets 
minimized. At the beginning, there are a 
large number of clusters with only one 
element in each of them. So the model 
description length is quite large while the 
data description length is quite small. 
Because the clustering process is hierarchical, 
every time when the combination occurs the 
number of clusters will decrease by one with 
the model description length's decreasing as 
well. At the same time the number of a 
certain cluster's elements will increase by 
one with the data description length's 
increment as well. However, the decrement 
is larger than the increment and it is getting 
smaller while the increment is getting larger. 
In this way, the objective function declines 
until the objective function reach its 
129 
Figure 1: Values of the Objective Functions 
addition, the clustering algorithm may help 
to find new collocations that are not in the 
thesaurus. This algodthm can also be 
extended to other collocation models, such 
as verb-noun collocations. 
27 
25 
23 
21 
19 
17 
minimum value. I f  we continue to execute 
the algorithm, we will see that the value of 
the objective function rises very fast like as 
is shown in Figure 1. 
15 
0 1000 2000 3000 4000 5000 6000 7000 8000 
quantity of c lusters 
Therefore we choose a fairly simple 
way to avoid the appearance of the local 
optimum: When there are two consecutive 
increases in the objective function during 
one clustering process, stop the process and 
start another one. When two consecutive 
clustering processes are stopped due to the 
same reason, we assume that we have got the 
minimum value and stop the whole 
clusterilag process. In our future work we 
will try to fred a better way to determine the 
minimum value of the objective function. 
7 Conclusion & Future Work 
In this paper we have presented a 
bidirctional hierarchical clustering algorithm 
of simultaneously clustering Chinese 
adjectives and nouns based on their 
collocations. Our preliminary experiments 
show that it can distinguish different words 
by their distribution environments. In 
Our future work includes: 
1) Because the sparsity of collocations 
is a main factor of affecting the 
word clustering accuracy, we can 
use the clustering results to discover 
new data and enrich the thesaurus. 
2) As there are yet no adjustments o 
the hierarchical clustering results, 
we are considering using some 
iterative algorithm, such as K- 
means algofithm~ to  optimise the 
clustering results. 
8 Attachment (Examples) 
We give 10 clusters of each part of 
speech clustered by our algorithm (using 
revisional distance formula b) as follows: 
8.1 Chinese Adjective dusters (10 of 383) 
130 
A3 ~ ~ ~f~ ~ ~ ~ 
A4 ~ ~ i~ ~ ~ ~ 
A5 ~ ~O ~ -F~ ~ ~ 
A~ ~ '~ ~, ~ ,~ ~ ~ 
~ ~'~ 
A9 ~ ~ ~.  ~ 
A~0 ~,~ E~ ~ ~ ~ ~ ~ 
8. 2 Chinese noun clusters (I0 of 595) 
N3 J~,~ .~_,i,E,)~, ~/~/~ ~IE~:::~T,/~ " f -~:~ 
N6 ~. :~T?:/~ - }t~:t.IL "~i:~ ~..'\]:~ - 1~ ~:~: 
N7 ~ ~,~ ~ ~ ~ ~ 
N8 ~'1~ ~ r~ ~l,~ ~ 'l~r~ 
N9 ~.,k, ~i~:~::. ~:--~: ~:~: 
Communications of COCIPS 6(1): P25- 
P33, 1996 
\[2\] Donghong Ji, Computational Research 
on Issues of Lexical Semantics, Post- 
doctoral Research Report, Tsinghua 
University, P14-P26, 1997 
\[3\] Juanzi Li et al, Two-dimensional 
Clustering Based on Compositional 
Examples, Language Engineering, 
P164-P169, Tsinghua University Press, 
1997 
\[4\] Hang Li & Naoki Abe, Clustering 
Words with the MDL Principle, cmp- 
lg/9605014 v2, 1996 
\[5\] Wei Xu, The Study of Syntax- 
Semantics Integrated Chinese Parsing, 
Thesis for the Degree of Master in 
Computer Science, Tsinghua University, 
1997 
\[6\] Wenjie Ni et al, Modem Chinese 
Thesaurus, China People Press,. 1984 
\[7\] Zhaoqi Bian et al, Pattern gecbgnition, 
Tsinghua University Press, 1997 
References 
\[1\] Donghong Ji & Changning Huang, A 
Semantic Composition Model for 
Chinese Noun and Adjective, 
131 
An Algorithm for Situation Classification 
of Chinese Verbs 
Xiaodan Zhu, Chunfa Yuan 
State Key Laboratory for Intelligent 
Technology and System, DepL of Computer 
Science & Technology, Tsinghua 
University, Beijing 100084, P.R.C. 
K.F.Wong, Wenjie.Li 
Dept. of System Engineenng and Engineering 
Management, Chinese University of 
HongKong 
Abst rac t  
Temporal information analysis is very 
important for Chinese Information Process. 
Comparing with English, Chinese is quite 
different in temporal information 
expression. Based on the feature of Chinese 
a phase-based method is proposed to deal 
with Chinese temporal information. To this 
end, an algorithm is put forward to classify 
verbs into different situation types 
automatically. About 2981 verbs were 
tested. The result has shown that the 
algorithm is effective. 
1.*Introduction 
We are now launching a research 
project on Events Extraction from Chinese 
Financial News, which requires us to extract 
the related temporal information from news. 
Temporal expressions in Chinese form a 
complex system. We cannot fully 
understand the temporal information only by 
extracting the verbs, adverbs, auxiliary 
words and temporal phrases. Instead, more 
profound analysis is needed. In this paper, 
we first introduce the temporal system of 
Chinese, then we put forward a method in 
dealing with Chinese temporal information, 
in which situation types is very important. 
Therefore, an algorithm is rendered to 
classify verbs into several situation types. 
1.1 Temporal System of Chinese 
Commonly, Chinese linguists \[3\]\[4\] 
think that the temporal system of Chinese 
includes three parts: phase, tense and aspect. 
Each of these represents ome profile of 
temporal expression (these definitions are a 
little different from linguistic theory of 
English). 
(1) Phase. A sentence may describe a static 
state or an action; an action may be durative 
or instantaneous; a durative action may 
indicate a terminal or not. All of these are 
the research fields of phase. So, static vs. 
dynamic, durative vs. instantaneous, telic vs. 
non-telic are three pairs of phase features. 
Phase depends fully on meaning. According 
to phase features, we can classify the verbs 
into different situation types. 
(2) Tense. Tense describes the relations 
between an event (E), reference time (R) and 
speaking time (S). First, taking S as the 
origin, we can get three reIations between R 
and S: if R is before S, the sentence 
describes past; if R is the same time as S, it 
describes present; if R is after S, it desci'ibes 
future. This is called primary tense. 
Secondly, we can get three relations 
between E and S: If E is before R, we call it 
anterior; if E is the same time as R, we call it 
simple; otherwise we call it posterior. This is 
called secondary tense. Therefore, there are 
nine tenses including anterior past, anterior 
future, simple future, posterior present, etc. 
* Supported by National Natural Science Foundation 
of China (69975008) and 973 project (G 1998030507) 
140 
(3) Aspect . Aspect reflects the way we 
observe an event. For the same event, there 
are many perspectives. We can take the 
event as atomic and not consider its inner 
structure, and call it perfective. We can 
consider it being in process, and call it 
imperfective. For imperfective, we can 
observe it at a"position before it, at the 
beginning of it, in the middle of it, etc. 
Different perspectives lead to different 
expressions in the language. 
Phase, tense and aspect are not 
independent even though they are different 
conceptions; each of them can influence and 
restrict he others, ultimately building up the 
complex temporal system of Chinese. 
1.2 Phase-based Chinese temporal 
information analysis 
Most languages express temporal 
information through phase, tense and aspect, 
however, for different languages, the 
relative importance of the three parts is 
different. A very important feature of 
English is that tense and aspect are 
expressed by variation of predicates. But for 
Chinese, predicates keep the same form no 
matter how the tense and aspect are 
different. 
Therefore, in English, temporal 
information analysis mainly considers tense 
and aspect, as well as temporal adjective and 
time words and phrases. But in Chinese, 
tense and aspect of a sentence are not very 
clear, verbs do not vary in form with the 
change of tense and aspect. So we suggest 
basing temporal information analysis on 
phase. We mainly perceive the situation type 
of a sentence, then roughly acquire tense 
from adverbs and auxiliary words. After 
considering the temporal phrases, we can 
understand the temporal information of 
single event fully. Finally, according to the 
absolute temporal information of single 
event, we can get the temporal relation 
between two events. Phase-based temporal 
information analysis has been used in our 
research on Event Extraction from Financial 
News, in which the most important and 
fundamental problem is to acquire the 
situation types of a sentence. 
1.3 Situation Classification of Chinese 
Verbs 
In the West, research on situation has a 
long history. The earliest can be traced to 
the times of Aristotle. In resent years, 
Western researchers have published a large 
volume of papers, which present many 
points of view. The most important are 
Vendler(1967), Bache(1982), and Smith 
(1985) They approximately classify the 
situation as four types: 
state, activity, accomplishment, and 
achievement. 
Chinese researchers have also done 
considerable work, among which the most 
typical research were done by Chen\[3\] and 
Ma\[5\]. 
Ma\[5\] stated that the situation of a 
sentence is fully determined by the situation 
of the main verb of the sentence. He use 
three phases: static, durative, telic to classify 
verbs into four situational types 
V1,V2,V3,V4. 
Static Durative Telic 
VI + + + 
V2 + + 
V3 + 
V4 + 
Table 1.1 
Chen\[3\] stated that the situation of a 
sentence not only depends on the main verb 
of the sentence but also on other parts of the 
sentence. That is, although the main verb is 
the most important in determining a 
sentence situation, other parts such as 
adverbs also have effect. Cheri s 
classification is more detailed. 
141 
NO. 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
00) 
Verb types 
Attribute 
Mental state 
Position 
Action and 
Mental Activity 
Verb-object Structure 
Change 
Directional Action 
Instantaneous Change 
Instantaneous Action 
Verb-verb or 
Verb-adjective 
Instances 
:E(be), ~(equal) 
;~:l~'~(believe), ~l~J~(re~'et) 
~.~i(stand), ~(sit), J\]~j 
(lie) 
gf~jump), ,~. (think), 
~i=~q~uess) 
i~t~(read (books)), 
I1~(sing (songs)) 
(become) 
/EgE(run up), ~,_J2(climb on) 
~.(die), ~l ie) ,  IS(snap) ..  
~t~(sit), ~td/(stand) 
~J(push down), 
~,TJ~..(smash (into pieces)) 
Table 1.2 
Static 
Situation types 
State + 
Activity 
Accomplishment 
Simple change 
Complex change 
Dura- Telic verb types 
tive (table above) 
(1) (2) 
(3) 
+ (3) (4) 
(5) 
+ + (3) (4) 
+ (6) (7) 
(S) (9) 
(10) 
Table 1.3 
From the tables above, we can find that 
some words(such as (3) and (4) in table 1.3) 
can belong to more than one category, so 
Chen use modifiers, auxiliary words and 
prepositions to eliminate the ambiguity. 
State Acti- 
vity 
Ell l~ l  I \ [  
~1~ +V 
v~ 
~v 
v+(y) 
+TQP+ 
~act) 
V+(T) + + 
+TQP+ 
~m,e)  
TQP: Time Quantity Phrase, (-) : 
? 
(-1 ? ? 
(-) (-) + 
? 
Accom- Complex Simple 
plishment change change 
+ 
? ? 
? ? 
? ? ? 
in most case, it is 
Table 1.4 
2. Our Classification Algorithm for 
Verbg Situation 
2.1 Guiding Thoughts 
(1) Our algorithm is for information 
processing 
eMa\[5\] uses three pairs of phase features in 
classifying, but from which we can not get 
an automatic classification algorithm for 
computers; the classification can only be 
done manually. 
eln linguistics, telicity is a phase feature 
used in classifying. In table 1.1 the 
difference between category V2 and V3, in 
table 1.3, the difference between "activivJ' 
and "accomplishmenf', are attributed to 
telicity. But in information process, we need 
not distinguish whether an event is telic or 
not. For example, 
Exp. 1 
~)~t\]~'j~, (He is playing the flute) 
'~ .  (He is playing a song '%iangzhu " )
Chen\[3\] thinks that in Exp. 1, the first 
sentence has the features: dynamicity, and 
durativity, and non-telicity; it belongs 
to "activi~' . The second sentence has the 
features dynamicity, durativity, and telicity, 
because in the second sentence, there is a 
default terminal . . . .  when the song 
~angzhd' is over, the action '~la~ is 
over, so the sentence belongs to 
'hccomplishmenf instead of  "activity. 
However we think such discrimination is 
useless for information extraction, because 
telicity is an ambiguous concept itself. What 
we need is to acquire the exact duration of 
the event. So if we knew the event is 
durative or not, and got the temporal 
phrases, we can know terminal time of the 
event. Besides, whether an event is telic or 
not can not be attributed to collocation and 
only can be done manually(as the exp 1 
shows). For these reasons, we consider the 
two verbs in Exp. 1 belonging to the same 
situational type, that is, we do not use 
talicity as a phase feature to classifying 
verbs. 
142 
(2) Separate classification of the verb 
situation from classification of the sentence 
situation. 
Chen\[3\] points that some verbs belong 
to more than one category, and gives a 
method to distinguish these cases. To make 
the ideal more clear, we use two steps to 
complete the seritence situation recognition. 
In this paper, we render an algorithm to 
classify verbs into different categories, 
which is the basis of another research ... .  
recognition of sentence situation, which will 
be discussed in future work. 
'Men(MentalityJ' can follow '~1~ (very). 
Verbs in the "AmlS' category can followed 
by "~-~(preposition-objec0' structures, etc. 
The following is the set of collocational 
features. 
Verb+T 
~ll~+Verb 
Verb+~ 
:i-+Verb 
Verb+~ 
Static verbs Amb Act Ins 
Att Men 
+ ? + 
+ 
(-) + (+) 
(-) (-) + 
? 
Table 2.1 
2.2 Classification Method 
We classify the verbs into five categories , 
Att(Attribute), Men(Mentality), Act 
(Activity) , Ins(Instantaneous) , Amb 
(Ambiguous). 
Att: ~(be), ~'(equal), '~'(include), m~(accord with) 
Men: ~.~J~(like), ~.,(belittle), ~(love), ~ff~ 
(be satisfied with) 
Act: ~(draw), ~l~l(gab), ~(drink), ~( run)  
In;  ~?~(explore), ~l;~(extinguish), I~(snap), 
(discovery) 
Amb: ~.~(sit), ~i(stand), Jig(lie), ~(kneel), ~:(bring), 
~(hang), ~(wear), ~-~(install) 
Amb(Ambiguous) include those words 
which describe different situations in 
different context. For example: 
Exp. 2: 
(they hung the picture on the wall. ) 
(Picture is hanging on the wall.) 
In Exp. 2, the two sentences have the 
same predicate '~  (hang). In the first 
sentence, '~  descnbes an instantaneous 
action, but the second sentence describes a
state. In English, forms of these two 
predicates are different; while in Chinese, 
they are the same. For this reason, we 
consider it ambiguous and indistinguishable 
without context. 
We have pointed out previously that 
phase depends only on meaning. However 
different situational types collocate with 
different words. So the essence of our 
algorithm is replace semantic judgement 
with collocational judgement. For example, 
2.3 Implementation of the algorithm 
According to table 2.1, a classification 
algorithm was designed, and we use two 
resources to implement our algorithm: The 
Contemporary Chinese Cihai \[11\] (which 
we will refer to as the Cihai below) 
dictionary and the Machine Tractable 
Dictionary of Contemporary Chinese 
Predicate Verbs \[12\](which we will refer to 
as the predicate dictionary below). The 
Cihai dictionary includes 12,000 entries and 
700,000 collocation instances, predicate 
dictionary includes about 3000 verbs with 
their semantic information, case relations 
and detailed collocation information. These 
two dictionaries both include some of the 
collocation information that the algorithm 
needs. 
Considenng the features of these two 
dictionaries, we adjust part of our algorithm: 
(1) In predicate dictionary, there is a slot 
named "verb typ? , which includes 
'transitive verlY , 'fntransifive verB' , 
~ttribufive verlS", 'linking vertt' etc. So, at 
the beginning of the algorithm, we judge if 
the verb is a "linking verlS' (
~(be~', ~(equa l~ '  ,etc) or a "possessive 
verlS' ('~-q~J' (have)). If it is, we directly 
classify the verb as "att(attribute~' without 
further processing. 
(2) The predicate dictionary provides the 
case relation of verbs, and their semantic 
ategodes. We restrict the agent of verbs in 
the "Men(mentality~' tobelong to one of: 
143 
"{ .)kI"(people), "{ )l,.,~} "(human), "1 
)l,~} "(multitude) , {~s:} 
(collectivity)'; "{:~:-~?~}(creatures)'; ,,~- ~,-ii,~,~,jtr 
bel ieff ,  "{gJJl~}(animal)" (3) Because 
Cihai includes collocation instances instead 
of collocation relations, we should consider 
synonyms. To be exact, when we determine 
whether a verb belongs to "Men(Mentality~' 
or not, we judge if it can follow 
(very) and synonyms such as 
'trY'P; 
However, some seldom seen instances were 
included. All these cause some errors. 
The final algorithm is as follows: 
if (a verb is labeled as" linking verb" or  '~:~ossessive vertf 
in predicate dictionary ) 
then the verb belongs to "Att(Attribute)" 
else if (the verb can follow ":~\[~"(very) and synonyms "~.~,I~", 
"1-?~"; '~l"~ '~71~2', "~" )  and (its agenf s 
semantic belongs to setl*) 
then the verb belongs to "Men(Mentality)" 
else if (it can follow ";t~E") or (be followed by"~") 
then if (it can be followed by "preposition-object" 
structure) 
then the verb belongs to "Amb(Ambiguous)" 
else the verb belongs to "Act(Activity)" 
else if (it can be followed by"T") 
then the verb belongs to"Ins(Instantaneous)" 
else the verb belongs tff unknown" 
*setl={human, multitude, collectivity, creatures, belief, 
animal } 
3. Resu l ts  and  Ana lys i s  
3.1 Results 
We use the algorithm above to classify 
the 2981 words in predicate dictionary, at 
the same time, we do the classification 
manually, Table 3.1 is the result: 
Att Men Amb Ins Act Un- Totel 
known 
Human 20 112 500 662 1683 4* 2981 
Algo- 20 111 537 691 1519 i 103 2981 
Rithm i 
*this 4 words are not verb. 
Table 3.1 
Table 2.2 shows the details: 
by a lgo -  Classifying b human 
rithm Att Men Amb Ins Act Non- Totel 
verb 
Art 20 0 0 0 0 0 20 
Men 0 99 1 1 10 0 111 
Amb 0 0 473 9 55 0 537 
Ins 0 0 3 637 51 0 691 
Act 0 1 12 2 1504 1519 
Un- 0 12 11 13 63 103 
known 
Totel 20 112 500 662 1683 4 2981 
Table 3.2 
Table 3.3 shows precision and recall: 
Att Men Amb Ins Act Average 
Precision 100.0 89.9 88.1 92.2 99.0 93.8 
Recall 100.0 88.4 94.6 96.2 89.4 93.7 
Table 3.3 
3.2 Analysis 
We mainly analyze the errors: 
(1) Failure of algorithm 
Chinese .is a very complex language. 
Replacing semantic judgment by using 
collocations has limitation itself. For 
example, in most cases, whether a verb is 
durative or not can be decided by whether it 
can be used in such structure "verb-g ~"  (In 
most case, "~ represents an action in 
progress). But some instantaneous verbs 
such as ~.(knocky, can also be used in 
such structure to express a repeated action. 
(2) Errors caused by the resources 
(2.1) Collocation incompleteness in 
Cihai: for example, '~(d isagreeJ '  can 
collocate with '~  (very), but this 
collocation is not included in Cihai. 
(2.2)Errors caused by predicate dictionary: 
It is obvious that a certain proportion of 
dictionary errors is inevitable. For example, 
though '~  (beg) can follow '~i~E to 
represent the action is in progress, it is not 
included in the corresponding slot of 
predicate dictionary. 
(2.3) The inconsistency between the two 
dictionaries: For example, 
NN (admire), (mind), and ( 
belittle) are included in predicate dictionary 
but not in Cihai. Although ~ (regret) is 
included in Cihai, it is taken as an adjective 
instead of a verb. 
144 
4. Conclusion 
In this paper, we advance a phase-based 
method to analyze temporal information in 
Chinese. For this purpose, an algorithm is 
rendered to classify verbs into different 
situation types. Because a verl5 s situation 
depends on the meaning of the verb, the 
essence of our algorithm takes advantage of 
collocations to avoid semantics. The result 
shows the algorithm is successful. We also 
believe that if the errors caused by resources 
were eliminated, the result would be 
improved significantly. 
Although the five categories are defined 
by us, they can describe basic situations of 
Chinese. The classification algorithm itself 
is independent of resources, so it can be 
applied to other resources (dictionaries) if
these resources include sufficient collocation 
information. Furthermore, Discarding 
dictionaries and doing classification directly 
on large-scale real corpus, especially in 
certain domain, deserve the future research. 
Our algorithm is very useful for the 
future analysis of sentence situation for 
Information Extraction system and for 
dictionary construction. 
\[5\]Ma Qingzhu. Time Quantity Phrase and 
Categories of Verbs: China Chinese. 
Vol.2,1981. 
\[6\]Hu Yushu & Fan Xiao. Research on 
Verbs, Henan Univ. Press, 1995 
\[7\]Hwang C.H. & Schubert L. K. 
Interpreting Tense, Aspect and Time 
Adverbials: A Compositional, Unified 
Approach : Proceeding of 1st International 
Conference in Temporal Logic, Bonn, 
Germany, July 1994. 
\[8\]Allen J.F. Towards a General Theory of 
Action and Time: Artificial Intelligence, 
23,123-154. 
\[9\]Allen J.F. & George, F.. Action and 
Events in Interval Temporal Logic: Journal 
of Logic and Computation, Special Issue on 
Actions and Processes, 1994. 
\[10lion Androutsopoulos, Graeme Ritche & 
Peter Thanisch. Time ,Tense and Aspect 
in Natural Language Database Interface, 
CMP-LG Mar 1998. 
\[l l\]Ni Wenjie . Contemporary Chinese 
Cihai, People China Press, 1994. 
\[ 12\]Chen Qunxiu Designing and 
implement of Machine Tractable Dictionary 
of Contemporary Chinese Predicate 
Verbs, Proceedings of ICCC96, Singapore, 
Jun, 1996. 
References 
\[ 1 \]Message Understanding Conference 
Website, http://www.muc.saic.com. 
\[2\]Message Understanding Evaluation and 
Conference: Proceedings of 3rd-6th APRA 
Workshops, Morgan Kaufmann Publishers 
Inc., 1996. 
\[3\]Chen ping. Discussion On Temporal 
System of Contemporary Chinese: China 
Chinese Vol.6,1998. 
\[4\]Gong Qianyan. Phase, Tense and Aspect 
of Chinese, Commercial Press. 
145 
A Model for Processing Temporal References in Chinese
Wenjie Li, Kam-Fai Wong
Department of Systems Engineering
and Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
fwjli, kfwongg@se.cuhk.edu.hk
Chunfa Yuan
Department of Computer Science
and Technology
Tsinghua University,
Beijing, 100084, P.R. China
ycf@s1000e.cs.tsinghua.edu.cn
Abstract
Conventional information systems can-
not cater for temporal information eec-
tively. For this reason, it is useful to cap-
ture and maintain the temporal knowl-
edge (especially the relative knowledge)
associated to each action in an informa-
tion system. In this paper, we propose a
model to mine and organize temporal re-
lations embedded in Chinese sentences.
Three kinds of event expressions are ac-
counted for, i.e. single event, multiple
events and declared event(s). Experi-
ments are conducted to evaluate the min-
ing algorithm using a set of news reports
and the results are signicant. Error
analysis has also been performed open-
ing up new doors for future research.
1 Introduction
Information Extraction (IE) is an upcoming chal-
lenging research area to cope with the increas-
ing volume of unwieldy distributed information re-
sources, such as information over WWW. Among
them, temporal information is regarded as an
equally, if not more, important piece of infor-
mation in domains where the task of extract-
ing and tracking information over time occurs
frequently, such as planning, scheduling and
question-answering. It may be as simple as an ex-
plicit or direct expression in a written language,
such as \the company closed down in May, 1997";
or it may be left implicit, to be recovered by read-
ers from the surrounding texts. For example, one
may know the fact that \the company has closed
down before the earthquake", yet without know-
ing the exact time of the bankruptcy. Relative
temporal knowledge such as this where the pre-
cise time is unavailable is typically determined by
human. An information system which does not
account for this properly is thus rather restrictive.
It is hard to separate temporal information (in
particular refers to temporal relations in this pa-
per) discovery from natural language processing.
In English, tenses and aspects reected by dier-
ent verb forms are important elements in a sen-
tence for expressing temporal reference (Steed-
man, 97) and for transforming situations into
temporal logic operators (Bruce, 72). The pio-
neer work of Reichenbach (Reichenbach, 47) on
tenses forms the basis of many subsequent re-
search eorts in temporal natural language pro-
cessing, e.g. the work of Prior in tense logic (Prior,
67), and of Hwang et alin tense tree (Hwang
92) and temporal adverbial analysis (Hwang 94),
etc. Reichenbach argued that the tense system
provided predication over three underlying times,
namely S (speech time), R (reference time), and
E (event time). Later, a multiple temporal refer-
ences model was introduced by Bruce (Bruce, 72).
He dened the set (S
1
; S
2
; :::; S
n
), which is an el-
ement of a tense. S
1
corresponds to the time of
speech. Each S
i
(i = 2; :::; n  1) is a time of ref-
erence, and S
n
, the time of an event. To facilitate
logic manipulation, Bruce proposed seven rst or-
der logic relations based on time intervals and a
method to map nine English tenses into tempo-
ral rst order logic expressions
1
. His work laid
down the foundation of temporal logic in natu-
ral language. These relations were then gradually
expanded to nine in (Allen, 81) and further to
thirteen in (Allen, 83)
2
.
In contrast, Chinese verbs appear in only one
1
The seven relations are symbolized as R(A;B) for
relation R and time intervals A and B, where R in-
cludes before, after, during, contains, same-time, over-
laps or overlapped-by.
2
meet, met-by, starts, started-by, nishes and
nished-by are added into temporal relations.
form. The lack of regular morphological tense
markers renders Chinese temporal expressions
complicated. For quite a long time, linguists ar-
gued whether tenses existed in Chinese; and if
they did how are they expressed. We believe that
Chinese do have tenses. But they are determined
with the assistance of temporal adverbs and aspect
auxiliary words. For example, ? ...  (being), .
? ... ? (was/been) and  ... (will be) express an
ongoing action, a situation started or nished in
the past, and a situation which will occur in the
future, respectively. Therefore, the conventional
theory to determine temporal information based
on verb axations is inapplicable. Over the past
years, there has been considerable progress in the
areas of information extraction and temporal logic
in English (Antony, 87; Bruce, 72; Kaufmann, 97).
Nevertheless, only a few researchers have investi-
gated these areas in Chinese.
The objective of our research is to design and
develop a temporal information extraction sys-
tem. For practical and cultural reason, the appli-
cation target is on-line nancial news in Chinese.
The nal system, referred to as TICS (Tempo-
ral Information-extraction from Chinese Sources),
will accept a series of Chinese nancial texts as in-
put, analyze each sentence one by one to extract
the desirable temporal information, represent each
piece of information in a concept frame, link all
frames together in chronological order based on
inter- or intra-event relations, and nally apply
this linked knowledge to fulll users' queries.
In this paper, we introduce a fundamental
model of TICS, which is designed to mine and
organize temporal relations embedded in Chinese
sentences. Three kinds of event expressions are
accounted for, i.e. single event, multiple events
and declared event(s). This work involved four
major parts, (1) built temporal model; (2) con-
structed rules sets; (3) developed the algorithm;
and (4) set up the experiments and performed the
evaluation.
2 A Model for Temporal Relation
Discovery
2.1 Temporal Concept Frame
In IE, it is impossible as well as impractical to
extract all the information from an incoming doc-
ument. For this reason, all IE systems are geared
for specic application domains. The domain is
determined by a pre-specied concept dictionary.
Then a certain concept is triggered by several lex-
ical items and activated in the specic linguistic
contexts. Each concept denition contains a set
of slots for the extracted information. In addition,
it contains a set of enabling conditions which are
constraints that must be satised in order for the
concept to be activated. Due to its versatility, a
frame structure is generally used to represent con-
cepts (as shown in Figure 1).
Slots in a temporal concept frame are divided
into two types: activity-related and time-related.
Activity-related slots provide the descriptions of
objects and actions concerning the concept. For
example, company predator, company target and
purchase value are the attributes of the concept
(B?, TAKEOVER). Meanwhile, time-related
slots provide information related to when a con-
cept begins or nishes, how long does it last and
how does it relate to another concept, etc.
referenced concept frame
......
(temporal relations
among activities)
{AND, OR}
Absolute relation
relation 1
relation 
time
time
relation n reference
relative relation
relation 1 reference
Delare frame
speaksman
location
reliability
absolute relation
relative relation
source
absolute relation
publish frame
n
......
......
......
......
Temporal slots
absolute relation
declare
duration
reliability
Activity slots
company
Concept frame
......
concept type
publish
company frame
company name
employees no.
turnover
parent company
company name
employees no.
turnover
parent company
company frame
among entities)(static relations
relative relation
Figure 1: Temporal concept frame construction
2.2 Temporal Relations
The system is designed with two sets of temporal
relations, namely absolute and relative relations.
The role of absolute temporal relations is to posi-
tion situation occurrences on a time axis. These
relations depict the beginning and/or ending time
bounds of an occurrence or its relevance to refer-
ence times, see TR(T ) in Section 2.3. Absolute
relations are organized by Time Line in the sys-
tem, see Figure 2.
r:                                   t:                                d:absolute relation      time parameter        duration
13/6/99
Time_Line
17/6/99 21/6/99 3/7/9929/6/99
New World demands
the payment of 10-
million-dollar debt
from CTI
Nan Hua takes over
Si Hai Travel with
1st Pacific takes over
from Lin Shao Liang
40% Yin Duo Fu?s stock
Jing Kuang sells DC
holdings to Dong Fong
Hong
Ba Ling buys 30%
Lian
stock from Zhong
o o ooo
Event B Time specification Event C Time specificationTime specification
t r r t dtr
25/6/99
23 million dollar
Event A
d d ......
Figure 2: The T ime Line organization for abso-
lute relations in TICS
In many cases, the time when an event takes
place may not be known. But its relevance to
another occurrence time is given. Relative tem-
poral knowledge such as this is manifested by rel-
ative relations. Allen has proposed thirteen re-
lations. The same is adopted in our system, see
TR(E
i
; E
j
) in Section 2.3. The relative relations
are derived either directly from a sentence describ-
ing two situations, or indirectly from the absolute
relations of two individual situations. They are or-
ganized by Relational Chains, as shown in Figure
3.
o                                       o
before
o                                       o
before
R d R d R d
R d R d R d
same_as
o o
o
contains
before
before
Event A
Si Tong parent
stock
Event CEvent A
Hua Ruen places
company reorgnizes
Event B
decreases by 1.3%
The price of Hua Ruen
Event D
Event E Event F
CKI acquires 20% of 
Envestra Limited from Richard Li for $3.8 billion
Event G
Ba Ling buye 30%
stock from Zhong Lian
R d
Event B Event C Event D
Event B Event E Event F
R d R d
Event C Event G
Si Tong parent
company goes public 
Tricom purchases 60% of PCC
R:                                     d:relative relation       duration
Figure 3: The Relational Chain organization for
relative relations in TICS
2.3 Temporal Model
This section describes our temporal model for
discovering relations from Chinese sentences.
Suppose TR indicates a temporal relation, E
indicates an event and T indicates time. The
absolute and relative relations are symbolized as:
OCCUR(E
i
; TR(T ))
3
and TR(E
i
; E
j
), respec-
tively. The sets of TR are:
TR(T ) = fON; BEGIN; END; PAST ,
FUTUER; ONGOING; CONTINUEDg
TR(E
i
; E
j
) = fBEFORE; AFTER; MEETS,
METBY; OV ERLAPS; OV ERLAPPED,
DURING; CONTAINS; STAREDBY ,
STARTS; FINISHES; FINISHEDBY ,
SAME ASg
For an absolute relation of a single event, T is
an indispensable parameter, which includes event
time t
e
, reference time t
r
4
and speech time t
s
:
3
OCCUR is a predicate for the happening of a sin-
gle event. Under the situations where there are no am-
biguity, E
i
can be omitted. The OCCUR(E
i
; TR(T )
is simplied as TR(T ).
4
There maybe exist more than one reference time
in a statement.
T = ft
e
; t
r
; t
s
g
Some Chinese words can function as the tem-
poral indicators. These include time word (TW ),
time position word (F ), temporal adverb (ADV ),
auxiliary word (AUX), preposition word (P ),
auxiliary verb (V A), trend verb (V C) and some
special verbs (V V ). They are all regarded as the
elements of the temporal indicator TI :
TI = fTW; F; ADV; AUX; V A; V C; V V; Pg
Each type of the indicators, e.g. TW , con-
tains a set of words, such as TW = twlist =
ftw
1
; tw
2
; :::tw
n
g, with each word having an tem-
poral attribute, indicated by ATT .
The core of the model is thus a rule set R
which maps the combinational eects of all the
indicators, TI , in a sentence to its corresponding
temporal relation, TR,
R : TI !
An Indexing Method Based on Sentences* 
Li Li 1, Chunfa Yuan1 , K.F. Wong2, and Wenjie Li3 
1State Key Laboratory of Intelligent Technology and System 
1Dept. of Computer Science & Technology, Tsinghua University, Beijing 100084 
 Email: lili97@mails.tsinghua.edu.cn; cfyuan@tsinghua.edu.cn 
2D e p t. o f  S y s te m  E n g in e e r in g  &  E n g in e e r in g  M a n a g e m e n t, T h e  C h in e se  U n iv e rs ity  o f H o n g  K o n g , H o n g  K o n g . 
Email: kfwong@se.cuhk.edu.hk 
3
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong. 
Email: cswjli@comp.polyu.edu.hk  
 
                                                          
*Supported by Natural Science Foundation of China(