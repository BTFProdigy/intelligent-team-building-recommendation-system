Morphological Interfaces to Dictionaries 
Michael Maxwell 
Linguistic Data Consortium 
3600 Market St, Suite 810 
Philadelphia, PA 19104 
Maxwell@ldc.upenn.edu 
William Poser 
Linguistics, University of Pennsylvania 
3600 Market St, Suite 501 
Philadelphia, PA 19104 
billposer@alum.mit.edu  
Abstract 
Languages with complex morphologies present difficulties for dictionaries users. One solution to this 
problem is to use a morphological parser for lookup of morphologically complex words, including fully 
inflected words, without the user needing to explicitly know the morphology. We discuss the sorts of 
morphologies which cause the greatest need for such an interface. 
1 Introduction 
When it comes to dictionaries, not all languages 
are created equal. Quite apart from the fact that 
more effort has been put into lexicography for 
some languages than for others, languages vary in 
how they lend themselves to word look up. 
Generations of English-speaking students have 
been told, when they were uncertain how to spell a 
word, to look it up in the dictionary. How one is 
supposed to look up a word when one does not 
already know how to spell it has been the source of 
much distress for those same students. 
For English, the chief obstacle to dictionary 
lookup is the orthography.
1 
But for other 
languages, the structure of the language itself is the 
problem, and in particular the language s 
morphology. Unless the dictionary user is 
explicitly familiar with that morphology, 
determining the citation form of a given word can 
be quite difficult. 
One solution, at least for electronic dictionaries, 
is to create an interface that uses a morphological 
parser to find the root or stem of the full word 
provided by the user, and then automatically look 
up that form (or its citation form), thereby shifting 
the need to explicitly know the morphology (and 
the choice of citation form) from the user to the 
computer. Such interfaces are described in Breidt 
and Feldweg 1997, Pr?sz?ky and Kis 2002, 
Streiter, Knapp, Voltmer et al 2004, etc. 
But building a morphological parser is a non-
trivial task, and a simpler solution where 
                                                     
 
1 
Orthography presents difficulties for dictionary 
lookup is difficult in languages which are not written 
alphabetically, and for which the lexical entries 
therefore cannot be alphabetized. While we do not 
address that issue in this paper, specialized front ends to 
dictionaries have been used for lookup in such 
languages; see e.g. Bilac, Baldwin and Tanaka 2002. 
possible would be preferable. In this paper, we 
discuss the sort of morphology that makes a parser 
interface especially desirable.  
2 Morphology and Citation Forms 
We need to clarify here that we are concerned 
with how difficult dictionary lookup is for 
average users, that is, for users who may not be 
overtly familiar with the morphology of the 
language. Linguists and (usually) language 
teachers are often familiar enough with the 
morphology that they can compute the citation 
form from any arbitrary inflected form, but many 
other users will not be able to do so. 
Certain kinds of morphology can make it 
difficult for average users to find the citation form 
of an inflected word. Usually this is inflectional 
morphology, simply because forms related by 
derivational morphology are often given separate 
listings. However, some languages have productive 
and regular derivational morphology, so that forms 
related by derivational affixation may not in fact be 
listed. Furthermore, in some languages (such as 
Athabaskan languages, see below) the derivational 
and inflectional morphology interact so as to make 
finding a citation form especially difficult. Finally, 
the boundary between inflectional and derivational 
morphology is not always clear whether to a 
linguist or to the end user. 
We now turn to the specifics of how morphology 
can impede dictionary lookup. For languages with 
any degree of morphology, one form of the 
paradigm for a given part of speech is usually 
chosen as the citation form. Problems may arise for 
words which lack the chosen form (e.g. pluralia 
tantum words, such as scissors). In any case, users 
must generally be told what form to look for. 
Of course, for languages with only a small 
amount of productive morphology, it does not take 
much sophistication to come up with a citation 
form from an inflected form. For English, the 
citation form of an inflected verb is generally 
found by stripping off one of a handful of suffixes 
(and sometimes undoing other spelling rules). 
Irregular verbs present complications, but their 
frequency makes them unlikely candidates for 
lookup, except by language learners. At any rate, 
irregular words can be placed in minor entries, 
separately alphabetized from the major entries, and 
cross-referencing the latter. 
In practice, users may not even need to know 
how to remove the suffixes, since when searching 
for walks or walking they will find walk, and 
generally make the connection. 
If a language is exclusively suffixing, not even a 
large amount of inflectional affixation need stand 
in the way of lookup. If the user cannot figure out 
the citation form of a word, he can simply look up 
the first few letters to find the entry. Thus, even 
languages like Turkish or Quechua often pose little 
problem for lookup. (Nevertheless, for some users, 
it may not be obvious that the citation form thus 
found corresponds to the inflected word, see e.g. 
Corris, Manning, Poetsch et al 2004: 47.) 
More problematic for lookup is prefixation.
2 
Since dictionary words are usually alphabetized 
from the beginning of the word to the end (left to 
right in most writing systems), in theory the user 
would have to strip prefixes before doing lookup. 
An obvious work-around would be to alphabetize 
words in (exclusively) prefixing languages from 
right to left. Alternatively, the dictionary could 
provide an index alphabetized from right to left, 
where the user could find the citation form, then 
look up that form in the main part of the 
dictionary. To our knowledge, this solution has not 
been employed, although this may be due to the 
paucity of exclusively prefixing languages. 
The reverse alphabetization solution would not 
work for languages which employ both prefixing 
and suffixing, such as Tzeltal (Mayan). But even 
here the situation is not too bad if the number of 
prefixes is small, as in fact it is in Tzeltal: the 
common prefixes are h-/k-, a-/aw-, and s-/y-, and 
stripping these probably does not present much of 
a problem to most users of the Vocabulario tzeltal 
de Bachajon (Slocum and Gerdel 1965). 
The real problem for languages having both 
prefixes and suffixes arises when the language has 
a large number of prefixes, or when the language 
productively employs compounding or incorpor-
ation, which can have the same effect for 
dictionary lookup as productive prefixation. 
                                                     
 
2 
If the citation form is prefixed, this may also cause 
problems for alphabetization, since many words may 
fall into the same section of the alphabet. This problem 
is well-known, but is not the focus of our discussion. 
German is a notorious example of the difficulties 
occasioned by compounding, and Nahuatl is an 
example of a language having incorporation. 
In Nahuatl, indefinite direct objects can often be 
incorporated into the verb: chi:lkwa to eat chili is 
composed of the verb stem kwa to eat , preceded 
by the incorporated noun chi:l chili . The na?ve 
user may succeed in finding the incorporated noun 
in a printed dictionary, but may be at a loss to 
decipher the rest of the word, since kwa is not a 
noun suffix in Nahuatl.
3 
A greater difficulty for the average dictionary 
user is nonconcatenative morphology, such as 
infixes, partial reduplication, and templatic 
morphology. In Tagalog, for example, there is an 
affix -um- marking actor focus, which is infixed 
following a word-initial consonant (Schachter and 
Otanes 1972). Furthermore, the Tagalog 
imperfective aspect is indicated by partial re-
duplication. Thus the word bumibili is a form of 
the verb root bili to buy , where the reduplication 
is bi-, and the infix -um- is stuck into the middle of 
this reduplicated syllable.  
In some cases, the user can (or should!) be 
expected to understand this and deal with 
converting bumibili, say, to the appropriate citation 
form. And in fact dictionary writers often assist by 
providing partly inflected forms: in the case of 
Tagalog, for instance, citation forms generally 
include the focus affixes. But as the complexity of 
the morphology increases, relying on the user to 
guess the citation form from an inflected form 
becomes less of an option. At the same time, 
explicitly including multiple inflected forms in the 
dictionary becomes cumbersome, even impossible. 
In the following subsections, we detail 
difficulties occasioned by the particular morpho-
logies of Semitic and Athabaskan languages. 
2.1 Semitic Languages 
Arabic, like most other Semitic languages, 
employs templatic morphology, in which affixes 
composed of vowels can be interdigitated between 
consonants of the root. Affixation can also modify 
the root consonants, frequently by gemination. For 
example, a typical Arabic root ktb can appear in 
inflected forms as diverse as katab, kattab, ktatab, 
ktanbab, and kutib (Spencer 1991). Some of this 
morphology is derivational, and some inflectional, 
but it all poses a problem for users.  
Moreover, Arabic is ordinarily written without 
many of the vowels. While this may ease the 
                                                     
3 
In practice, this problem in Nahuatl is ameliorated 
by the fact that incorporation is not highly productive. 
Therefore the most common cases of incorporation 
should arguably be listed in the dictionary. 
problem caused by the interdigitated vowels, it 
means that the dictionary user may have more 
difficulty distinguishing root consonants from 
affixal consonants, since the vowels are not present 
in the written form to help parse the word.  
Traditional Arabic dictionaries have been root 
based; that is, the head word of a lexical entry is 
the root, with all derivational and inflectional 
morphology removed. Listed derived forms appear 
as subentries under a given root (and inflected 
forms which must be listed are generally included 
as variant forms within the subentry for a given 
derived form). Because of the difficulty undoing 
Arabic derivational and inflectional morphology 
poses for the average user, so-called alphabetic 
dictionaries have become increasingly popular. In 
an alphabetic dictionary, derived forms serve as 
headwords, so that alphabetization is done over the 
entire set of lexemes, whether root or stem.  
Root-based dictionaries and alphabetic diction-
aries each have strengths and weaknesses. A root-
based dictionary gathers the information on related 
forms into one place, rather than scattering it 
throughout the dictionary, as is the case for an 
alphabetic dictionary. On the other hand, a root-
based dictionary requires a much more explicit 
understanding of Arabic morphology than many 
users possess. Even so, finding the citation form of 
an irregular plural or an irregular verb in an 
alphabetic dictionary can be a daunting task. 
In summary, Arabic morphology forces the 
dictionary writer to choose between a root-based 
format and an alphabetic format; both approaches 
have their disadvantages. Similar problems obtain 
for other languages with templatic morphologies. 
Fortunately, these problems can be overcome by 
interposing a morphological parser between the 
user and the electronic dictionary. 
2.2 Athabaskan Languages 
The difficulties that Athabaskan languages pose 
for dictionary lookup have been detailed in Poser 
2002; here we give an outline of the problem for 
one such language, Carrier.  
Like other Athabaskan languages, Carrier is 
predominantly prefixing, with verbs carrying 
numerous prefixes. Each verb can have tens or 
even hundreds of thousands of forms. But the sheer 
number of verb forms is not all that different from 
other agglutinative languages such as Finnish or 
Turkish. The real problem is that Carrier prefixes 
are a mixture of inflectional and derivational 
morphemes, with the derivational affixes often 
appearing outside of inflectional affixes.  
Furthermore, it is not infrequently the case that 
there are prefixes which obligatorily combine with 
a root in a certain meaning. In effect, Athabaskan 
languages have discontinuous verb stems.
4 
For 
instance, the Carrier verb to be red consists of 
the root k'un with the valence prefix l- 
immediately preceding the root and the prefix d- 
several positions to the left, giving forms like: 
dilk'un  you (sg.) are red
 
duzk'un  I am red
hudulk'un  they are red
hudutilk'un they will be red
Note that some subject markers follow the d- 
while others precede it. Also notice that the 
allomorphy sometimes collapses two affixes into a 
single segment (s+l  z in duzk'un).  
For dictionaries, the implication is that there is in 
general no contiguous or invariant portion of the 
verb that can serve as the citation form. The 
morphology is primarily prefixal, but the existence 
of extensive stem variation and some suffixation 
means that the stem is not a good citation form, 
and that ordering forms form right-to-left will not 
keep related forms close together. Worse, the 
phonological material that contributes the basic 
meaning of the word is not, in general, contiguous. 
This means that any citation form will not be easily 
extracted by an unsophisticated user.  Moreover, 
no simple sorting will keep related forms together. 
Worse, many verb roots are highly abstract, so 
that a form can only be given an English 
translation on the basis of the root together with 
one or more prefixes. Examples are found in the 
classificatory verbs. For example, the verb root 
meaning to give takes distinct derivational 
affixes depending on the type of object being 
handled: ball-shaped objects, names, houses, non-
count objects, long rigid objects, contents of open 
containers, liquids, fluffy stuff and these 
classifiers may not be adjacent to the root. 
In light of the difficult of dictionary lookup in 
Athabaskan languages, one approach has been to 
list, for each verb, a single form, as in the major 
dictionary of Navajo (Young and William Morgan 
1987). However, this requires the user to be able to 
analyze a verb form and convert it to the citation 
form. This is a non-trivial task even for fluent 
native speakers; it is difficult or impossible for 
language learners. Indeed, the problem of 
dictionary use for Navajo is so acute that the Din? 
(Navajo) College has instituted a one semester 
course Navajo Grammar and Applied 
Linguistics , which is largely devoted to teaching 
                                                     
4 
These are somewhat analogous to English verb-
particle combinations such as bring a matter up , in 
which the verbal inflection (and often a direct object) 
intervenes between the verb root and the particle. But 
the intervening inflectional morphology in Athabaskan 
is vastly more complex than that of English. 
college-level native speakers of Navajo how to use 
the dictionary of their own language. 
The other major approach to dictionary making 
in Athabaskan languages is to list individual 
morphemes. In order to use such a dictionary, the 
user must be able to analyze the word into root and 
affixes. But the root may have many shapes. For 
example, the root meaning to go around in a boat 
takes forms such as , , , , , and . 
Although there is a pattern to these changes, it is 
complex if not irregular. The resulting difficulty 
for dictionary lookup should be obvious. 
A root-based lexicon has been published for 
Navajo (Young, Morgan and Midgette 1992). It 
has the virtue of being comprehensive, and of 
avoiding duplication. For example, the detailed 
meaning of a verb root can be explained only once, 
in the entry for that root, rather than in each of 
many entries for forms derived from that root. 
The problem with this approach is that it requires 
even more grammatical knowledge on the part of 
the user than traditional Athabaskan dictionaries, 
together with an understanding of an elaborate 
process for analyzing forms, looking up their 
components, and constructing the meaning of the 
form from its components. As a result, while 
analytic dictionaries are useful for linguists, but 
most people, including both language learners and 
native speakers, find them very difficult. 
In sum, the morphological structure of 
Athabaskan languages forces difficult choices on 
the dictionary writer, and results in a steep learning 
curve for the user. Again, this is the sort of 
language structure where a morphological interface 
can make a crucial difference. 
3 Conclusion 
We have outlined ways in which the structure of 
languages can make a morphological parser as a 
front end for dictionary lookup attractive. 
There are more uses to such technology than just 
dictionary lookup. If the morphology engine is a 
transducer, it can be used for generation as well as 
for parsing. Such a bidirectional engine can be 
used to generate the paradigm of any stem. While 
this is of little interest to native speakers, it may be 
of great assistance to language learners.  
Another application would be to provide what 
amounts to a virtual interlinear text with 
morpheme glosses for any text in electronic form. 
To be sure, this text would not be disambiguated, 
unless a knowledgeable user put forth the effort, or 
unless an automatic disambiguator (tagger) was 
provided. Nevertheless, interlinear text, even in an 
ambiguous form, could be a useful for linguists 
and perhaps language learners. 
In sum, a morphological transducer connected to 
an electronic dictionary can provide valuable aid 
for both native speakers and language learners. 
4 Acknowledgements 
Our thanks to Tim Buckwalter and Mohamed 
Maamouri of the Linguistic Data Consortium and 
Jonathan Amith for their comments on earlier 
versions of this paper. 
References  
Bilac, S., T. Baldwin, et al (2002). Bringing the 
Dictionary to the User: the FOKS system. 
COLING-2002. 
Breidt, E. and H. Feldweg (1997). "Accessing 
Foreign Languages with COMPASS." 
Machine Translation Journal, special 
issue on New Tools for Human Translators
 
12: 153-174. 
Corris, M., C. Manning, et al (2004). "How Useful 
and Usable are Dictionaries for Speakers 
of Australian Indigenous Languages?" 
International Journal of Lexicography 17: 
33-68. 
Poser, W. J. (2002). Making Athabaskan 
Dictionaries Usable. Proceedings of the 
Athabaskan Languages Conference. G. 
Holton. Fairbanks, Alaska Native 
Language center, University of Alaska: 
136-147. 
Pr?sz?ky, G. and B. Kis (2002). Context-Sensitive 
Electronic Dictionaries. COLING-2002. 
Schachter, P. and F. T. Otanes (1972). Tagalog 
Reference Grammar. Berkeley, University 
of California Press. 
Slocum, M. and F. Gerdel (1965). Vocabulario 
tzeltal de Bachajon. Mexico, Summer 
Institute of Linguistics. 
Spencer, A. (1991). Morphological theory : an 
introduction to word structure in 
generative grammar. Oxford, UK ; 
Cambridge, Mass., Basil Blackwell. 
Streiter, O., J. Knapp, et al (2004). Bridging the 
Gap between Intentional and Incidental 
Vocabulary Acquisition. ALLC/ ACH 
2004, G?teborg University, Sweden. 
Young, R. W., W. Morgan, et al (1992). 
Analytical Lexicon of Navajo. 
Albuquerque, University of New Mexico 
Press. 
Young, R. W. and S. William Morgan (1987). The 
Navajo Language: a Grammar and 
Colloquial Dictionary. Albuquerque, 
University of New Mexico Press.  
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 1?2,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Building Language Resources: Ways to move forward 
Anne David and Michael Maxwell 
Center for Advanced Study of Language 
University of Maryland, USA 
aeadavid@gmail.com, maxwell@umiacs.umd.edu 
Abstract 
There are perhaps seven thousand languages in the world, ranging from the largest with hundreds of mil-
lions of speakers, to the smallest, with one speaker. On a different axis, languages can be ranked accord-
ing to the quantity and quality of computational resources. Not surprisingly, there are correlations be-
tween these two axes: languages like English and Mandarin have substantial resources, while many of the 
smallest languages are completely undocumented. Nevertheless, the correlation is not perfect; there are 
languages with a million speakers which are more or less unwritten, and there are very large languages ? 
some of the languages of India, for example ? which are relatively resource-poor. 
Unfortunately, what counts as resource-rich (or even resource-adequate) in computational linguistics is 
a moving target. For languages to move in the direction of resource richness, considerable effort (people 
and money) have to be provided over a prolonged period of time. One can sit back and wait for this to 
happen, or give up; alternatively, one can map out a realistic way forward, building on the strengths of 
each language?s situation. 
Among the strengths which may prove useful to building computational resources for languages are the 
following: 
? Long traditions of grammatical and lexical description 
? Traditions of literacy and literature 
? Local expertise in linguistics and computing 
? The world-wide community of linguists and computer experts 
? Resource availability in related languages 
At the same time, there are weaknesses and other problems ? some language specific, some more gen-
eral ? which need to be considered: 
? Lack of consensus on ways of representing the language (scripts, character encoding) 
? Complexities inherent in particular languages (complex scripts, complex morphologies, variant 
orthographies, diglossia, dialectal variation) 
? Economic and educational realities in the countries where the language is spoken 
? Political attitudes towards some languages, particularly minority languages 
? The 'not invented here' syndrome 
? Software obsolescence, and the potential obsolescence of language data 
This talk will look at ways in which the strengths enumerated above might be leveraged, while avoiding 
the potential weaknesses. 
1
 2
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 27?34,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Joint Grammar Development by Linguists and Computer 
Scientists 
Michael Maxwell 
Center for Advanced Study of Language/ 
University of Maryland 
College Park, Maryland, USA 
mmaxwell@casl.umd.edu 
Anne David 
Center for Advanced Study of Language/ 
University of Maryland 
College Park, Maryland, USA 
adavid@casl.umd.edu 
Abstract 
For languages with inflectional morpho-
logy, development of a morphological 
parser can be a bottleneck to further 
development. We focus on two difficulties: 
first, finding people with expertise in both 
computer programming and the linguistics 
of a particular language, and second, the 
short lifetime of software such as parsers. 
We describe a methodology to split parser 
building into two tasks: descriptive 
grammar development, and formal 
grammar development. The two grammars 
are combined into a single document using 
Literate Programming. The formal 
grammar is designed to be independent of a 
particular parsing engine?s programming 
language, so that it can be readily ported to 
a new parsing engine, thus helping solve 
the software lifetime problem. 
1 Problems for Grammar Development 
After several decades of widespread effort in 
computational linguistics, the vast majority of the 
world?s languages lack significant computational 
resources. For many languages, this is attributable 
to the lack of even more basic resources, such as 
standardized writing systems or dictionaries. But 
even for many languages that have been written for 
centuries, computational resources are scarce. 
One resource that is needed for languages with 
significant inflectional morphology is a morpho-
logical parser.1 To the degree that a language has 
complex morphology, parsers are difficult to build. 
                                                 
1 In fact, it is more common to create a morphological 
transducer, that is, a program which functions to both parse 
and generate inflected words. However, because it is more 
familiar, in this paper we will frequently use the term ?parser.? 
While there has been considerable research into 
automatically deriving a morphological parser 
from a corpus (see for example Creutz and Lagus, 
2007; Goldsmith, 2001; Goldsmith and Hu, 2004; 
and the papers in Maxwell, 2002), the results are 
still far from producing reliable, wide-coverage 
parsers. Hence most morphological parsers are still 
built by hand. This paper focuses on practical 
aspects of how such parsers can best be built, and 
presents a model for collaborative development. 
Hand-built parsers suffer from at least two 
drawbacks, which we will call the ?Expertise 
Problem? and the ?Half-Life Problem.? The 
Expertise Problem concerns a difficulty for 
building a parser in the first place: it is hard to find 
one person with the necessary knowledge of both 
the linguistics of the target language and the 
computational technology for building parsers. 
The Half-Life Problem concerns the fact that 
once a parser has been built, its life is limited by 
the life of the software it has been implemented in, 
and this lifetime is often short. 
The following subsections further describe these 
two problems, while the remainder of the paper 
focuses largely on the Expertise Problem. We 
focus specifically on the development of 
morphological grammars. The techniques 
described here may be usable with syntactic 
grammars as well, but we have not investigated 
that problem. We also focus in this paper on the 
development issue; testing and debugging 
grammars is not discussed in this paper. 
1.1 The Expertise Problem 
Writing software requires two kinds of expertise: 
knowledge of the problem to be solved, and 
knowledge of how to program software. For 
parsers, the problem-specific knowledge requires 
understanding the grammar of the target language. 
Since everyone speaks at least one language, it 
27
might seem that finding someone who understands 
the grammar of any particular language should be 
easy. Unfortunately, as generations of field 
linguists have discovered, this is not true. A native 
speaker?s knowledge of a language is notoriously 
implicit; converting that knowledge into explicit 
rules is no simple task. Furthermore, finding a 
speaker of the language who combines explicit 
understanding of the grammar with software 
engineering skills is even more difficult. The 
difficulty is compounded when the number of 
speakers of the language is small. We therefore 
believe that for many languages of the world, for 
the near future, the way to develop computational 
tools in general, and morphological parsers in 
particular, lies in teamwork. 
An example of the team approach was the 
BOAS project (Oflazer et al, 2001). A BOAS 
team consisted of two people?a ?language 
informant? and a programmer?plus a computer 
program which interviewed the informant and 
created the grammar rules. The computer program 
is described as a ?linguist in a box? (Oflazer et al, 
61). The method we describe uses computational 
tools, but purely human teamwork. 
A potential problem with the team approach lies 
in facilitating communication between team 
members. While electronic communication makes 
distributed teams possible, there is still a question 
of how best to enable people with disparate skills 
to actually understand each other. We return to this 
below, when we discuss our collaborative method. 
1.2 The Half-Life Problem 
Another problem with computational tools is their 
lack of longevity. While it would be difficult to 
formally investigate, we estimate the average 
lifetime for computational linguistic tools to be 
five or ten years. In part, this is due to the (lack of) 
longevity of the underlying software.2 Of course, 
some vendors provide backwards compatibility, 
and not all software becomes extinct that 
quickly?but that is the meaning of ?half-life.? 
Software obsolescence can be postponed by the 
judicious choice of programming languages, 
                                                 
                                                2 One of us (Maxwell) was involved in a project in which two 
of the programming languages became defunct before the 
program was complete. In both cases, the cost of porting to 
alternative dialects of the programming language was deemed 
prohibitive.  
avoiding platform- or OS-specific commands, the 
use of open source methods, etc. However, this can 
only prolong the life of a program, not extend it 
indefinitely.3 There are few if any programs that 
were written in 1980 that still run on any but 
computers outside of a museum?and 1980 was 
only twenty-seven years ago. 
In contrast, natural languages change slowly, 
apart from the infusion of new vocabulary. The 
grammar of a language spoken today is unlikely to 
be significantly different from the grammar of that 
same language fifty or a hundred years ago; and 
barring catastrophe, any changes which do happen 
are likely to be incremental.  
One might argue that the short half-life of 
software is unimportant, since twenty years from 
now it may be possible to generate a 
morphological parser automatically from a corpus 
and a dictionary. Perhaps, but this remains to be 
seen. In the meanwhile, the time and effort that go 
into writing such tools mandates that the tools be 
usable for long after the project is completed. 
Another motivation for wanting to build parsing 
tools with a longer half-life is that they constitute a 
description of (part of) the grammar of a language, 
in two senses: first, the grammar that the parser 
uses is in effect a formal description of the 
language?s morphology (or syntax). This formal 
description has the advantage over traditional 
grammar descriptions of being unambiguous.  
A second way in which a parser constitutes 
documentation of a language is that it can be used 
to analyze language texts, and?if it supports a 
generation mode?to produce paradigms. That is, a 
parser is an active description, not a static one. 
However, linguists have drawn attention to the 
issue of longevity for computer-based language 
documentation and description. In their seminal 
paper, Bird and Simons (2003) point out that the 
use of digital technologies brings the potential that 
archive language data can become unusable much 
more quickly than printed grammatical 
descriptions. Indeed, scholars of today can 
understand grammars of South Asian languages 
penned thousands of years ago. 
 
3 Old software can of course be kept on ?life support? by 
running it on old machines running old operating systems. But 
that is a solution for museums, not for software that is 
intended to be actively used. 
28
Since a parser embodies a description of the 
grammar of a language, it should be written to 
provide an explicit, computationally 
implementable description of the language, 
portable to future parsing engines even after the 
language is extinct. As we show below, this is not 
an impossible goal. 
2 A method for Grammar Development 
We have embarked on a project to build 
morphological parsers of languages in a way that 
overcomes the Expertise and Half-Life problems 
described in the previous section. The first parser 
was for the Bengali, or Bangla, language. Our 
choice of Bangla was driven by a number of 
considerations, many of which are not relevant 
here. Most any language with a significant amount 
of inflectional morphology would have worked. 
However, in retrospect the choice was a good one, 
as it forced us to deal with a number of both 
computational and linguistic issues that a more 
highly resourced language such as Spanish would 
not have presented. At the same time, Bangla is 
sufficiently documented by traditional grammars 
that the task was achievable, although not as easy 
as we had anticipated. 
We are writing two kinds of grammars 
simultaneously: the first is a traditional descriptive 
or reference grammar, written in English prose by 
a linguist (Anne David), intended to be read by 
linguists. The other is a formal grammar, written in 
a formal specification language, by a 
computational linguist (Mike Maxwell) and 
intended for conversion into the programming 
language of a parsing engine. (Neither of us is a 
speaker of the Bangla language.) The two 
grammars are intertwined, as described below, so 
that each supports the other in such a way that we 
can combine our differing expertise while also 
avoiding the lack of longevity that plagues 
traditional parser development. 
The following subsections describe the 
methodology we are using, and its advantages. 
2.1 Descriptive Grammar 
The descriptive grammar we have written is not, of 
itself, ground-breaking. Like most reference 
grammars of the morphology of a language, it has 
a chapter on the phonology and writing system of 
Bangla, and chapters for the various parts of 
speech. The latter chapters describe the inflectional 
(and some derivational) affixes each part of speech 
takes, and how the resulting inflected forms define 
the paradigms. The usage of these forms is also 
described, with examples sufficient to illustrate the 
usage; it is not, however, a pedagogical grammar.  
We were surprised to discover that no thorough 
and reliable English-language descriptive grammar 
of modern colloquial Bangla exists, despite its 
having well over 200 million native speakers. 
Instead, we had to glean our description of Bangla 
morphology from half a dozen or so grammars of 
varying quality (some of them pedagogical4), 
several journal articles, and a couple of 
dissertations. Doing so meant comparing and 
reconciling sometimes widely differing 
descriptions and analyses; three major problems 
we encountered were contradictory accounts, lack 
of clarity, and gaps in coverage. Writing a formal 
grammar forced us to both resolve these issues and 
clarify our descriptive grammar. 
For example, we knew from our sources that the 
locative/ instrumental case in Bangla has several 
allomorphs; however, the descriptions of their 
distribution differed, and one of our chief sources 
was, in fact, quite vague on the conditioning 
environments. Moreover, one particular vowel 
alternation that takes place in certain verb forms 
goes unmentioned in nearly all of our sources and 
is inaccurately described in one of the two that do 
mention it. In this instance, a native speaker 
confirmed the correct forms for us. Opinions 
among the written sources on how to classify 
Bangla verbs differed widely as well, with 
anywhere from two to seven classes proposed. We 
ended up choosing the system that defined seven 
stem classes, since it is the only one that enables 
the generation of any verb form, given a stem.  
Resolving such problems was made easier by 
the help of a consultant in the Bangla language. 
Professor Clint Seely, Emeritus of the University 
of Chicago. He corrected our many mistakes and 
helped clear up ambiguities in our sources. 
The difficulties we encountered in 
understanding grammatical descriptions, recon-
ciling different grammatical accounts, and filling 
in gaps in coverage underline the fact that we 
could not have simply picked up a grammar and 
                                                 
4 In fact, the clearest and most reliable sources of information 
were pedagogical grammars. 
29
written a formal grammar from it. For languages 
which have any degree of inflectional 
complexity?and Bengali does, although there are 
languages with still more complicated mor-
phologies?the problems are too great for such a 
simple approach. One might ask why it is so 
difficult to convert a published grammar into a 
morphological parser. One answer is that 
languages are inherently complex. It is common 
for published descriptions to overlook complexity, 
either in the interest of presenting a simple and 
general description, or perhaps because the author 
is unaware of some of the issues.  
Also, as any reader or writer of technical papers 
knows, it is all too easy to talk about complex 
topics unclearly. In our case, writing the formal 
grammar at the same time as the descriptive 
grammar forced a clarity and breadth of coverage 
in our descriptive grammar which we would not 
otherwise have attained. Moreover, by 
incorporating a formal grammar into the 
descriptive grammar, we have gone beyond 
previous work on Bangla, or most other languages. 
The following section describes this. 
2.2 Formal Grammar 
For the formal grammar of Bangla morphology, we 
need a description which is unambiguous and 
capable of being used to build a morphological 
parser. As discussed above, ambiguity is a fact 
about natural language, and one which has long 
plagued software specification efforts (Berry and 
Kamsties, 2003). Building a parser from a 
descriptive grammar is analogous to building 
traditional software from a software specification.  
Since our descriptive grammar is a natural 
language specification, it is not what an 
implementer would want to rely on. We therefore 
needed a formal language for grammar writing.  
One approach would be to use the programming 
language of an existing parsing tool. Amith and 
Maxwell (2005a) propose using the xfst language 
(the language of one of the Xerox finite state tools, 
see Beesley and Karttunen, 2003). While this 
would meet the need for an unambiguous 
representation, it would fail to meet our goal of 
longevity: the Xerox tools will likely not be used 
in ten years, and there is no reason to think that 
whatever morphological parsing engines are 
available then will use the same programming 
language?nor that grammar engineers will 
understand the xfst programming language. 
Our formal grammar needs to be unambiguous, 
iconic, and self-documenting. We have therefore 
chosen to represent our formal grammar in XML, 
and have developed an XML schema for encoding 
linguistic structures, based on a UML model 
developed by SIL researchers.5 The design goals 
of our XML schema are described in more detail in 
Maxwell and David (forthcoming). 
2.3 Combining Descriptive and Formal 
Grammars 
However, as we have argued elsewhere (Amith and 
Maxwell, 2005a; 2005b), neither a descriptive nor 
a formal grammar is adequate to our purposes by 
itself. Descriptive grammars are inherently 
ambiguous and sometimes vague, while formal 
grammars are hard to understand. If a formal 
grammar could be combined with the descriptive 
grammar, we would have an antidote to these 
problems: the combination could be neither 
ambiguous nor vague.  
The question is then whether there is a way to 
combine the two sorts of grammars. Such a method 
would need to support the following:  
(1) Developing the grammars in parallel. 
(2) Combining the grammars so that the 
description of each aspect of the grammar is 
presented to the human reader along with the 
corresponding aspect of the formal grammar. 
(3) Extracting the formal grammar for use by the 
parsing engine. 
In fact, there already is a method that accomplishes 
(2) and (3): Literate Programming, developed by 
Donald Knuth (1984, 1992) as a way of document-
ing computer programs. We use an XML/ 
DocBook implementation of Literate Programming 
(Walsh and Muellner, 1999; Walsh, 2002), since 
XML provides numerous advantages for long-term 
archiving (cf. Bird and Simons, 2002). 
There remains the need for a methodology for 
developing the descriptive and formal grammars in 
parallel, point (1) in the above list. We turn to this 
question in the next section. 
                                                 
5 The SIL model can be downloaded from 
http://fieldworks.sil.org/. 
30
2.4 Collaborative Grammar Development 
We are writing our descriptive grammar of Bangla 
in a commercial program, XMLmind (http://
www.xmlmind.com/xmleditor/). The formal 
grammar is being written in a programmer?s editor, 
although with suitable style sheets, it could be 
written in XMLmind. The formal grammar 
consists of a number of ?fragments,? each paired 
with a section in the descriptive grammar, so that 
the descriptive and formal grammatical 
descriptions are mutually supportive (see the 
appendix for a short excerpt). 
Our working arrangement is one of iterative 
development, with descriptive grammar writing 
leading formal grammar writing. Crucially, this 
iterative development allows frequent exchanges 
for clarification. A typical interchange (one which 
actually took place) is the following. The language 
expert writes a section of the descriptive grammar 
on Bengali noun qualifiers. The computational 
grammar writer reads the description and tries to 
implement it, but a question arises: is the 
diminutive qualifier used in all the environments 
that the three allomorphs of the non-diminutive 
qualifier are used, or only one of those 
environments? The language expert finds examples 
showing the diminutive in all environments, 
enabling the computational grammar writer to 
proceed. Crucially, the descriptive grammar was 
then modified to clarify this issue, and to include 
the new examples.  
Although we are writing our grammars a short 
hallway apart, this interchange was accomplished 
largely by email; we could as well have been a 
continent apart. 
In summary, our division of labor, together with 
the fact that we are simultaneously developing the 
two kinds of grammar using our computational 
tools and incorporating immediate feedback, has 
made possible a much better result than if one of us 
wrote the descriptive grammar, and the other later 
wrote the formal grammar. 
2.5 Conversion to publishable grammar 
As evident from the small portion of our grammar 
in the appendix, the formal grammar is 
understandable in its XML form, but it is not 
?pretty?; nor does it bear any obvious resemblance 
to modern linguistic formalisms.6 At the same 
time, the use of XML means that a variety of tools 
are available for editing the grammar, checking its 
validity against the schema, and converting it into 
the programming language of a parsing engine.  
Fortunately, the flexibility of XML makes it 
possible to display (and eventually publish) the 
formal grammar using linguistic formalisms, such 
as the following: 
__V#   /
k
t
p
k
t
p
       
h
h
h
??
??
?
??
??
?
?
??
??
?
??
??
?
 
The ability to create such display forms of the 
underlying XML data?referred to by Knuth as 
?weaving??is important as we look to publishing 
the combined descriptive and formal grammar. The 
creation of the style sheets necessary for this is 
planned for next year. 
2.6 Conversion to parser 
To build a parser from our grammar, we first 
extract the formal grammar as an XML document 
from the combined descriptive and formal 
grammar. This is a standard process in Literate 
Programming, called ?tangling?; we use a simple 
XSLT (Extensible Stylesheet Language 
Transformation), developed by Norman Walsh 
(http://docbook.sourceforge.net/release/litprog/
current/fo/ldocbook.xsl).  
Second, the extracted XML formal grammar is 
read by a small Python program, then converted 
into the programming language of the target 
morphological parsing engine.  
A computer-readable lexicon must also be 
converted into the programming language of the 
parsing engine, a comparatively simple task. 
Finally, the converted grammar and lexicon are 
read by the parsing engine to produce the parser. 
Currently, the target parsing engine is the Stuttgart 
Finite State Transducer Tools (http://www.ims.uni-
                                                 
6 We have resisted the temptation to make our linguistics too 
modern, since linguistic theories also have a short half-life. 
We model an eclectic but largely 1950s era version of 
linguistics. For example, phonological natural classes are 
defined by listing the phonemes of which they are composed, 
rather than using distinctive features; we use ordered 
phonological rules, rather than Optimality Theory-style 
constraints rankings. While these may be outmoded, they are 
quite understandable. 
31
stuttgart.de/projekte/gramotron/SOFTWARE/
SFST.html). We fully expect that any choice of 
parsing engine we make today will be superseded 
in the future by better and more capable parsing 
engines. Targeting a different parsing engine will 
require rewriting only that part of the conversion 
program that re-writes the program-internal 
representation into the target programming 
language (plus a converter for the lexicon). 
Verifying that the conversion process works 
correctly with a new parsing engine will require 
standard test data. Much of this test data can be 
automatically extracted from the paradigm tables 
and example sentences of the descriptive grammar.  
3 Previous work 
Collaborative work on natural language processing 
programs is not of itself a new idea. It is quite 
common to split up the task of developing a 
grammar among people with skills in linguistics, 
lexicography, and software development. In that 
sense, our work is very traditional.  
Ours is not even the first effort at developing a 
framework for collaborative development of 
computational linguistic tools. Butt et al (1999) 
describe the development of grammars in several 
languages, including English, French and German 
(with other languages added later). However, their 
focus was on enabling collaboration among 
grammar writers working in different languages; 
each author was assumed to be more or less skilled 
in one target language and in computational 
linguistics. Their focus thus differs from ours in its 
scope and in the nature of the collaboration. 
Copestake and Flickinger (2000) devote a 
section to ?Collaborative grammar coding,? but 
conclude that in order to work on a (syntactic) 
parser, a developer needs to combine skills in the 
linguistic theory being implemented, grammar 
debugging, and the grammar of the target 
language. In our work, we are attempting to make 
it possible to split this expertise between different 
people, and to provide them with a collaborative 
tool. 
Significant effort has been directed at enabling 
collaborative annotation of corpora, e.g. 
Cunningham et al 2002, and Ma et al 2002. This 
is similar to our approach in allowing collaboration 
between annotators and experts (annotation 
supervisors); but unlike our project, collaborative 
annotation does not address grammar development. 
Finally, there are linguistic development 
environments such as SIL?s FLEx 
(www.sil.org/computing/fieldworks/flex/), and the 
planned Montage project (Bender et al, 2004), 
which are intended to help linguists write 
computational grammars, incorporating or 
generating descriptive grammars. While these are 
useful tools?we are in fact looking into using 
FLEx to produce interlinear sentences for our 
grammars?they are not intended for the same 
kind of collaborative effort that we describe here. 
4 Conclusion 
What is new about the project we describe is 
therefore the development of a computational 
framework within which computationally 
implemented grammar development can be split 
into distinct tasks: one task for a person (or a team) 
with knowledge of a particular language, and 
another task for a person (or team) with skills in 
computer science. (Lexicography may constitute a 
third task, depending on whether suitable machine-
readable dictionaries are already available.)  
If this division of labor we describe here were 
applicable only to the working relationship 
between the authors, it would be of little general 
interest. However, we believe a similar division of 
skills between language expert and computational 
expert to be quite commonplace, making the same 
division of labor workable in a variety of 
scenarios. This has implications for the develop-
ment of linguistic software in low density 
languages: finding someone who is expert in both a 
language and its grammar, and in computational 
techniques, is likely to be particularly difficult in 
the case of languages which have not been well-
documented, or minority languages, or languages 
spoken in countries where there is not a history of 
work in natural language processing.  
It is easy to imagine other scenarios where this 
division of labor would work. For example, the 
linguistic team might be part of the language or 
linguistics department of a university, while the 
computational team might be part of a computer 
science department.  Grammar development could 
easily be an open source project, with the 
developers never meeting face-to-face. 
32
A question which occurred to us many times 
during this project is, who can best build a 
grammar or parser for a language: people like us, 
who are linguists but do not know the language, or 
native speakers of the language? The answer is not 
at all obvious. We suggest that the answer is 
neither one?alone. None of the language speakers 
or researchers we talked with in the course of this 
project had the expertise to build and test formal 
grammars or morphological parsers. At the same 
time, when the grammars we consulted were not 
clear, or contradicted each other, we needed to 
consult with native speakers or researchers to 
determine the correct answers. 
Hence, we feel strongly that parsers and 
grammars should be built by teams including 
people with a variety of skills. Given modern 
technology, it seems clear that the division of labor 
which our method allows means that there is no 
reason the people involved in the project need even 
be in the same country, or all speak the target 
language. 
In sum, we are developing a methodology to 
build certain kinds of NLP resources in lower 
density languages, and we have demonstrated this 
technology for morphological parsing.  
References 
Amith, Jonathan D., and Maxwell, Michael. 2005a. 
Language Documentation: The Nahuatl Grammar. In 
Alexander Gelbuck (ed.) Computational Linguistics 
and Intelligent Text Processing. Lecture Notes in 
Computer Science. 474-485. Berlin: Springer. 
Amith, Jonathan D., and Maxwell, Michael. 2005b. 
?Language Documentation: Archiving Grammars.? 
Chicago Linguistic Society 41. 
Beesley, Kenneth R., and Karttunen, Lauri. 2003. Finite 
State Morphology: CSLI Studies in Computational 
Linguistics. Chicago: University of Chicago Press. 
Bender, Emily M.; Dan Flickinger; Jeff Good; and Ivan 
A. Sag. 2004. ?Montage: Leveraging Advances in 
Grammar Engineering, Linguistic Ontologies, and 
Mark-up for the Documentation of Underdescribed 
Languages.? Proceedings of the Workshop on First 
Steps for Language Documentation of Minority 
Languages: Computational Linguistic Tools for 
Morphology, Lexicon and Corpus Compilation, 
LREC 2004. 
Berry, Daniel M., and Kamsties, Erik. 2003. Ambiguity 
in Requirements Specification. In Julio Cesar 
Sampaio do Prado Leite and Jorge Horacio Doorn 
(eds.) Perspectives on Software Requirements. The 
Springer International Series in Engineering and 
Computer Science. Vol. 753. Berlin: Springer. 
Bird, Steven, and Simons, Gary. 2002. Seven 
Dimensions of Portability for Language 
Documentation and Description. In Proceedings of 
the Workshop on Portability Issues in Human 
Language Technologies, Third International 
Conference on Language Resources and Evaluation. 
Paris: European Language Resources Association. 
Bird, Steven, and Simons, Gary. 2003. Seven 
dimensions of portability for language documentation 
and description. Language 79:557-582. 
Butt, Myriam, King, Tracy Holloway, Ni?o, Mar?a-
Eugenia, and Segond, Fr?d?rique. 1999. A Grammar 
Writer's Cookbook: CSLI Lecture Notes, 95. 
Stanford, CA: CSLI Publications. 
Copestake, Ann, and Flickinger, Dan. 2000. An open 
source grammar development environment and 
broad-coverage English grammar using HPSG. In 
Proceedings of the Second conference on Language 
Resources and Evaluation (LREC-2000). Athens, 
Greece. 
Creutz, Mathias, and Lagus, Krista. 2007. Unsupervised 
models for morpheme segmentation and morphology 
learning. ACM Transactions on Speech and 
Language Processing 4. 
Cunningham, H., Tablan, V., Bontcheva, K., and 
Dimitrov, M. 2002. Language engineering tools for 
collaborative corpus annotation. 
http://citeseer.ist.psu.edu/734322.html. 
Goldsmith, John. 2001. Unsupervised Learning of the 
Morphology of a Natural Language. Computational 
Linguistics 27:153-198. 
Goldsmith, John , and Hu, Yu. 2004. From Signatures to 
Finite State Automata. Midwest Computational 
Linguistics Colloquium, Bloomington IN. 
Knuth, Donald E. 1984. Literate programming. The 
Computer Journal 27:97-111. 
Knuth, Donald E. 1992. Literate Programming: CSLI 
Lecture Notes. Stanford: Center for the Study of 
Language and Information. 
Ma, Xiaoyi, Lee, Haejoong, Bird, Steven, and Maeda, 
Kazuaki. 2002. Models and Tools for Collaborative 
Annotation. In Proceedings of the Third 
International Conference on Language Resources 
and Evaluation. Paris: European Language 
Resources Association. 
33
Maxwell, Michael B. 2002. Proceedings of the 
Workshop on Morphological and Phonological 
Learning. New Brunswick, NJ: ACL. 
Maxwell, Michael B., and Anne David. Forthcoming. 
?Interoperable Grammars.? Paper to be presented at 
The First International Conference on Global 
Interoperability for Language Resources (ICGL 
2008), Hong Kong.
Nirenburg, Sergei, Biatov, Konstantin, Farwell, David, 
Helmreich, Stephen, McShane, Marjorie, Ponsford, 
Dan, Raskin, Victor, and Sheremetyeva, Svetlana. 
1999. Toward Descriptive Computational 
Linguistics. 
http://crl.nmsu.edu/expedition/publications/boas-
acl99.pdf. 
Oflazer, Kemal, Nirenburg, Sergei, and McShane, 
Marjorie. 2001. Bootstrapping Morphological 
Analyzers by Combining Human Elicitation and 
Machine Learning. Computational Linguistics 27:59-
85. 
Walsh, Norman, and Muellner, Leonard. 1999. 
DocBook: The Definitive Guide. Sebastopol, 
California: O'Reilly & Associates, Inc. 
Walsh, Norman. 2002. Literate Programming in XML. 
XML 2002, Baltimore, MD. 
 
Appendix: Sample Grammar Excerpt 
3.2. Future Tense 
The future tense is used to express: 
? a future state or action 
? propriety or ability [etc.] 
? 
 
Person Suffix (C)VC- (C)aC- (C)V- (C)a- (C)V(i)- Causative 3-
     
/on-a/ 
to hear 
  
/thak-a/  
to stay 
  
/h-oya/ 
to become
 
/kha-oya/ 
to eat 

/ca-oya/ 
to want 
 
/ekha-no/  
to teach 

  
/kama-no/ 
to bite 
1st -  
/-bo/ 

  
/ n bo/

 
/thak bo/
  
/h-bo/ 
  
/kha-bo/ 
 
/cai-bo/ 
  
/ekha-bo/ 

  
/kama bo/
Table 6.2: FutureTense Verb Forms 
[Additional rows omitted to save space] 
The formal grammar's listing of future tense suffixes appears below. 
 
<Mo:InflectionalAffix gloss="-1Fut" id="af1Fut"> 
   <!--The two "allomorphs" are really allographs--> 
   <Mo:Allomorph form=""> 
      <!--Spelled 'bo'; usually (not always) after a C-stem --> 
   </Mo:Allomorph> 
   <Mo:Allomorph form=""> 
      <!--Spelled 'b'; usually (not always) after a vowel stem --> 
   </Mo:Allomorph> 
   <Mo:inflectionFeatures> 
     <Fs:f name="Tense"><Fs:symbol value="Future"/></Fs:f> 
     <Fs:f name="Mood"><Fs:symbol value="Indicative"/></Fs:f> 
     <Fs:f name="Person"><Fs:symbol value="1"/></Fs:f> 
   </Mo:inflectionFeatures> 
/Mo:InflectionalAffix> 
 
<!-- Etc. for the remaining future tense suffixes --> 
34
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 29?37,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Frontiers in Linguistic Annotation for Lower-Density Languages
Mike Maxwell
Center for Advanced Study of Language
University of Maryland
mmaxwell@casl.umd.edu
Baden Hughes
Department of Computer Science
The University of Melbourne
badenh@csse.unimelb.edu.au
Abstract
The languages that are most commonly
subject to linguistic annotation on a large
scale tend to be those with the largest pop-
ulations or with recent histories of lin-
guistic scholarship. In this paper we dis-
cuss the problems associated with lower-
density languages in the context of the de-
velopment of linguistically annotated re-
sources. We frame our work with three
key questions regarding the definition of
lower-density languages; increasing avail-
able resources and reducing data require-
ments. A number of steps forward are
identified for increasing the number lower-
density language corpora with linguistic
annotations.
1 Introduction
The process for selecting a target language for re-
search activity in corpus linguistics, natural lan-
guage processing or computational linguistics is
largely arbitrary. To some extent, the motivation
for a specific choice is based on one or more of a
range of factors: the number of speakers of a given
language; the economic and social dominance of
the speakers; the extent to which computational
and/or lexical resources already exist; the avail-
ability of these resources in a manner conducive to
research activity; the level of geopolitical support
for language-specific activity, or the sensitivity of
the language in the political arena; the degree to
which the researchers are likely to be appreciated
by the speakers of the language simply because
of engagement; and the potential scientific returns
from working on the language in question (includ-
ing the likelihood that the language exhibits inter-
esting or unique phenomena). Notably, these fac-
tors are also significant in determining whether a
language is worked on for documentary and de-
scriptive purposes, although an additional factor
in this particular area is also the degree of endan-
germent (which can perhaps be contrasted with the
likelihood of economic returns for computational
endeavour).
As a result of these influencing factors, it is
clear that languages which exhibit positive effects
in one or more of these areas are likely to be the
target of computational research. If we consider
the availability of computationally tractable lan-
guage resources, we find, unsuprisingly that major
languages such as English, German, French and
Japanese are dominant; and research on computa-
tional approaches to linguistic analysis tends to be
farthest advanced in these languages.
However, renewed interest in the annotation of
lower-density languages has arisen for a number
of reasons, both theoretical and practical. In this
paper we discuss the problems associated with
lower-density languages in the context of the de-
velopment of linguistically annotated resources.
The structure of this paper is as follows. First
we define the lower-density languages and lin-
guistically annotated resources, thus defining the
scope of our interest. We review some related
work in the area of linguistically annotated cor-
pora for lower-density languages. Next we pose
three questions which frame the body of this pa-
per: What is the current status of in terms of lower-
density languages which have linguistically anno-
tated corpora? How can we more efficiently create
this particular type of data for lower-density lan-
guages? Can existing analytical methods methods
perform reliably with less data? A number of steps
are identified for advancing the agenda of linguis-
29
tically annotated resources for lower-density lan-
guages, and finally we draw conclusions.
2 Lower-Density Languages
It should be noted from the outset that in this pa-
per we interpret ?density? to refer to the amount
of computational resources available, rather than
the number of speakers any given language might
have.
The fundamental problem for annotation of
lower-density languages is that they are lower-
density. While on the surface, this is a tautol-
ogy, it in fact is the problem. For a few lan-
guages of the world (such as English, Chinese
and Modern Standard Arabic, and a few West-
ern European languages), resources are abundant;
these are the high-density Languages. For a few
more languages (other European languages, for
the most part), resources are, if not exactly abun-
dant, at least existent, and growing; these may be
considered medium-density languages. Together,
high-density and medium-density languages ac-
count for perhaps 20 or 30 languages, although of
course the boundaries are arbitrary. For all other
languages, resources are scarce and hence they fall
into our specific area of interest.
3 Linguistically Annotated Resources
While the scarcity of language resources for
lower-density languages is apparent for all re-
source types (with the possible exception of mono-
lingual text ), it is particularly true of linguistically
annotated texts. By annotated texts, we include
the following sorts of computational linguistic re-
sources:
? Parallel text aligned with another language at
the sentence level (and/or at finer levels of
parallelism, including morpheme-level gloss-
ing)
? Text annotated for named entities at various
levels of granularity
? Morphologically analyzed text (for non-
isolating languages; at issue here is particu-
larly inflectional morphology, and to a lesser
degree of importance for most computational
purposes, derivational morphology); also a
morphological tag schema appropriate to the
particular language
? Text marked for word boundaries (for those
scripts which, like Thai, do not mark most
word boundaries)
? POS tagged text, and a POS tag schema ap-
propriate to the particular language
? Treebanked (syntactically annotated and
parsed) text
? Semantically tagged text (semantic roles) cf.
Propbank (Palmer et al, 2005), or frames cf.
Framenet1
? Electronic dictionaries and other lexical re-
sources, such as Wordnet2
There are numerous dimensions for linguisti-
cally annotated resources, and a range of research
projects have attempted to identify the core prop-
erties of interest. While concepts such as the Ba-
sic Language Resource Kit (BLARK; (Krauwer,
2003; Mapelli and Choukri, 2003)) have gained
considerable currency in higher-density language
resource creation projects, it is clear that the base-
line requirements of such schemes are signifi-
cantly more advanced than we can hope for for
lower-density languages in the short to medium
term. Notably, the concept of a reduced BLARK
(?BLARKette?) has recently gained some currency
in various forums.
4 Key Questions
Given that the vast majority of the more than seven
thousand languages documented in the Ethno-
logue (Gordon, 2005) fall into the class of lower-
density languages, what should we do? Equally
important, what can we realistically do? We pose
three questions by which to frame the remainder
of this paper.
1. Status Indicators: How do we know where
we are? How do we keep track of what lan-
guages are high-density or medium-density,
and which are lower-density?
2. Increasing Available Resources: How (or
can) we encourage the movement of lan-
guages up the scale from lower-density to
medium-density or high-density?
1http://framenet.icsi.berkeley.edu/
2http://wordnet.princeton.edu
30
3. Reducing Data Requirements: Given that
some languages will always be relatively
lower-density, can language processing ap-
plications be made smarter, so that they don?t
require largely unattainable resources in or-
der to perform adequately?
5 Status Indicators
We have been deliberately vague up to this point
about how many lower-density languages there
are, or the simpler question, how my high and
medium density languages there are. Of course
one reason for this is that the boundary between
low density and medium or high density is inher-
ently vague. Another reason is that the situation
is constantly changing; many Central and East-
ern European languages which were lower-density
languages a decade or so ago are now arguably
medium density, if not high density. (The stan-
dard for high vs. low density changes, too; the bar
is considerably higher now than it was ten years
ago.)
But the primary reason for being vague about
how many ? and which ? languages are low den-
sity today is that no is keeping track of what re-
sources are available for most languages. So we
simply have no idea which languages are low den-
sity, and more importantly (since we can guess that
in the absence of evidence to the contrary, a lan-
guage is likely to be low density), we don?t know
which resource types most languages do or do not
have.
This lack of knowledge is not for lack of trying,
although perhaps we have not been trying hard
enough. The following are a few of the catalogs
of information about languages and their resources
that are available:
? The Ethnologue3: This is the standard list-
ing of the living languages of the world, but
contains little or no information about what
resources exist for each language.
? LDC catalog4 and ELDA catalog5: The
Linguistic Data Consortium (LDC) and the
European Language Resources Distribution
Agency (ELDA) have been among the largest
distributors of annotated language data. Their
catalogs, naturally, cover only those corpora
3http://www.ethnologue.org
4http://www.ldc.upenn.edu/Catalog/
5http://www.elda.org/rubrique6.html
distributed by each organization, and these
include only a small number of languages.
Naturally, the economically important lan-
guages constitute the majority of the holdings
of the LDC and ELDA.
? AILLA (Archive of the Indigenous Lan-
guages of Latin America6), and numerous
other language archiving sites: Such sites
maintain archives of linguistic data for lan-
guages, often with a specialization, such as
indigenous languages of a country or region.
The linguistic data ranges from unannotated
speech recordings to morphologically ana-
lyzed texts glossed at the morpheme level.
? OLAC (Open Archives Language Commu-
nity7): Given that many of the above re-
sources (particularly those of the many lan-
guage archives) are hard to find, OLAC is
an attempt to be a meta-catalog (or aggre-
gator)of such resources. It allows lookup of
data by type, language etc. for all data repos-
itories that ?belong to? OLAC. In fact, all the
above resources are listed in the OLAC union
catalogue.
? Web-based catalogs of additional resources:
There is a huge number of additional web-
sites which catalog information about lan-
guages, ranging from electronic and print
dictionaries (e.g. yourDictionary8), to dis-
cussion groups about particular languages9.
Most such sites do little vetting of the re-
sources, and dead links abound. Neverthe-
less, such sites (or a simple search with an
Internet search engine) can often turn up use-
ful information (such as grammatical descrip-
tions of minority languages). Very few of
these web sites are cataloged in OLAC, al-
though recent efforts (Hughes et al, 2006a)
are slowly addressing the inclusion of web-
based low density language resources in such
indexes.
None of the above catalogs is in any sense com-
plete, and indeed the very notion of completeness
is moot when it comes to cataloging Internet re-
sources. But more to the point of this paper, it
6http://www.ailla.utexas.org
7http://www.language-archives.org
8http://www.yourdictionary.com
9http://dir.groups.yahoo.com/dir/Cultures Community/By Language
31
is difficult, if not impossible, to get a picture of
the state of language resources in general. How
many languages have sufficient bitext (and in what
genre), for example, that one could put together a
statistical machine translation system? What lan-
guages have morphological parsers (and for what
languages is such a parser more or less irrele-
vant, because the language is relatively isolating)?
Where can one find character encoding converters
for the Ge?ez family of fonts for languages written
in Ethiopic script?
The answer to such questions is important for
several reasons:
1. If there were a crisis that involved an arbitrary
language of the world, what resources could
be deployed? An example of such a situa-
tion might be another tsunami near Indone-
sia, which could affect dozens, if not hun-
dreds of minority languages. (The Decem-
ber 26, 2004 tsunami was particularly felt in
the Aceh province of Indonesia, where one of
the main languages is Aceh, spoken by three
million people. Aceh is a lower-density lan-
guage.)
2. Which languages could, with a relatively
small amount of effort, move from lower-
density status to medium-density or high-
density status? For example, where parallel
text is harvestable, a relatively small amount
of work might suffice to produce many appli-
cations, or other resources (e.g. by projecting
syntactic annotation across languages). On
the other hand, where the writing system of
a language is in flux, or the language is po-
litically oppressed, a great deal more effort
might be necessary.
3. For which low density languages might re-
lated languages provide the leverage needed
to build at least first draft resources? For ex-
ample, one might think of using Turkish (ar-
guably at least a medium-density language)
as a sort of pivot language to build lexicons
and morphological parsers for such low den-
sity Turkic languages as Uzbek or Uyghur.
4. For which low density languages are there
extensive communities of speakers living in
other countries, who might be better able to
build language resources than speakers living
in the perhaps less economically developed
home countries? (Expatriate communities
may also be motivated by a desire to main-
tain their language among younger speakers,
born abroad.)
5. Which languages would require more work
(and funding) to build resources, but are still
plausible candidates for short term efforts?
To our knowledge, there is no general, on-going
effort to collect the sort of data that would make
answers to these questions possible. A survey was
done at the Linguistic Data Consortium several
years ago (Strassel et al, 2003) , for text-based re-
sources for the three hundred or so languages hav-
ing at least a million speakers (an arbitrary cutoff,
to be sure, but necessary for the survey to have had
at least some chance of success). It was remark-
ably successful, considering that it was done by
two linguists who did not know the vast majority
of the languages surveyed. The survey was funded
long enough to ?finish? about 150 languages, but
no subsequent update was ever done.
A better model for such a survey might be an
edited book: one or more computational linguists
would serve as ?editors?, responsible for the over-
all framework, and training of other participants.
Section ?editors? would be responsible for a lan-
guage family, or for the languages of a geographic
region or country. Individual language experts
would receive a small amount of training to enable
them to answer the survey questions for their lan-
guage, and then paid to do the initial survey, plus
periodic updates. The model provided by the Eth-
nologue (Gordon, 2005) may serve as a starting
point, although for the level of detail that would
be useful in assessing language resource availabil-
ity will make wholesale adoption unsuitable.
6 Increasing Available Resources
Given that a language significantly lacks compu-
tational linguistic resources (and in the context of
this paper and the associated workshop, annotated
text resources), so that it falls into the class of
lower-density languages (however that might be
defined), what then?
Most large-scale collections of computational
linguistics resources have been funded by govern-
ment agencies, either the US government (typi-
cally the Department of Defense) or by govern-
ments of countries where the languages in ques-
tion are spoken (primarily European, but also a
32
few other financially well-off countries). In some
cases, governments have sponsored collections for
languages which are not indigenous to the coun-
try in question (e.g. the EMILLE project10, see
(McEnery et al, 2000)).
In most such projects, production of resources
for lower-density languages have been the work of
a very small team which oversees the effort, to-
gether with paid annotators and translators. More
specifically, collection and processing of monolin-
gual text can be done by a linguist who need not
know the language (although it helps to have a
speaker of the language who can be called on to
do language identification, etc.). Dictionary col-
lection from on-line dictionaries can also be done
by a linguist; but if it takes much more effort than
that ? for example, if the dictionary needs to be
converted from print format to electronic format ?
it is again preferable to have a language speaker
available.
Annotating text (e.g. for named entities) is dif-
ferent: it can only be done by a speaker of the lan-
guage (more accurately, a reader: for Punjabi, for
instance, it can be difficult to find fluent readers of
the Gurmukhi script). Preferably the annotator is
familiar enough with current events in the country
where the language is spoken that they can inter-
pret cross-references in the text. If two or more an-
notators are available, the work can be done some-
what more quickly. More importantly, there can be
some checking for inter-annotator agreement (and
revision taking into account such differences as are
found).
Earlier work on corpus collection from the web
(e.g. (Resnik and Smith, 2003)) gave some hope
that reasonably large quantities of parallel text
could be found on the web, so that a bitext collec-
tion could be built for interesting language pairs
(with one member of the pair usually being En-
glish) relatively cheaply. Subsequent experience
with lower-density languages has not born that
hope out; parallel text on the web seems rela-
tively rare for most languages. It is unclear why
this should be. Certainly in countries like India,
there are large amounts of news text in English and
many of the target languages (such as Hindi). Nev-
ertheless, very little of that text seems to be gen-
uinely parallel, although recent work (Munteanu
and Marcu, 2005) indicates that true parallelism
may not be required for some tasks, eg machine
10http://bowland-files.lancs.ac.uk/corplang/emille/
translation, in order to gain acceptable results.
Because bitext was so difficult to find for lower-
density languages, corpus creation efforts rely
largely, if not exclusively, on contracting out text
for translation. In most cases, source text is har-
vested from news sites in the target language, and
then translated into English by commercial trans-
lation agencies, at a rate usually in the neighbor-
hood of US$0.25 per word. In theory, one could
reduce this cost by dealing directly with trans-
lators, avoiding the middleman agencies. Since
many translators are in the Third World, this might
result in considerable cost savings. Nevertheless,
quality control issues loom large. The more pro-
fessional agencies do quality control of their trans-
lations; even so, one may need to reject transla-
tions in some cases (and the agencies themselves
may have difficulty in dealing with translators for
languages for which there is comparatively little
demand). Obviously this overall cost is high; it
means that a 100k word quantity of parallel text
will cost in the neighborhood of US$25K.
Other sources of parallel text might include
government archives (but apart from parliamen-
tary proceedings where these are published bilin-
gually, such as the Hansards, these are usually not
open), and the archives of translation companies
(but again, these are seldom if ever open, because
the agencies must guard the privacy of those who
contracted the translations).
Finally, there is the possibility that parallel text
? and indeed, other forms of annotation ? could be
produced in an open source fashion. Wikipedia11
is perhaps the most obvious instance of this, as
there are parallel articles in English and other lan-
guages. Unfortunately, the quantity of such par-
allel text at the Wikipedia is very small for all
but a few languages. At present (May 2006),
there are over 100,000 articles in German, Span-
ish, French, Italian, Japanese, Dutch, Polish, Por-
tuguese and Swedish.12 Languages with over
10,000 articles include Arabic, Bulgarian, Cata-
lan, Czech, Danish, Estonian, Esperanto and Ido
(both constructed languages), Persian, Galician,
Hebrew, Croatian), Bahasa Indonesian, Korean,
Lithuanian, Hungarian, Bahasa Malay, Norwegian
11http://en.wikipedia.org
12Probably some of these articles are non-parallel. Indeed,
a random check of Cebuano articles in Wikipedia revealed
that many were stubs (a term used in the Wikipedia to refer to
?a short article in need of expansion?), or were simply links to
Internet blogs, many of which were monolingual in English.
33
(Bokma?l and Nynorsk), Romanian, Russian, Slo-
vak, Slovenian, Serbian, Finnish, Thai, Turkish,
Ukrainian, and Chinese. The dominance of Euro-
pean languages in these lists is obvious.
During a TIDES exercise in 2003, researchers
at Johns Hopkins University explored an innova-
tive approach to the creation of bitext (parallel En-
glish and Hindi text, aligned at the sentence level):
they elicited translations into English of Hindi sen-
tences they posted on an Internet web page (Oard,
2003; Yarowsky, 2003). Participants were paid for
the best translations in Amazon.com gift certifi-
cates, with the quality of a twenty percent subset
of the translations automatically evaluated using
BLEU scores against highly scored translations of
the same sentences from previous rounds. This
pool of high-quality translations was initialized to
a set of known quality translations. A valuable
side effect of the use of previously translated texts
for evaluation is that this created a pool of multiply
translated texts.
The TIDES translation exercise quickly pro-
duced a large body of translated text: 300K words,
in five days, at a cost of about two cents per word.
This approach to resource creation is similar to
numerous open source projects, in the sense that
the work is being done by the public. It differed
in that the results of this work were not made
publicly available; the use of an explicit qual-
ity control method; and of course the payments
to (some) participants. While the quality control
aspect may be essential to producing useful lan-
guage resources, hiding those resources not cur-
rently being used for evaluation is not essential to
the methodology.
Open source resource creation efforts are of
course common, with the Wikipedia13 being the
best known. Other such projects include Ama-
zon.com?s Mechanical Turk14, LiTgloss15, The
ESP Game16, and the Wiktionary17. Clearly some
forms of annotation will be easier to do using
an open source methodology than others will.
For example, translation and possibly named en-
tity annotation might be fairly straightforward,
while morphological analysis is probably more
difficult, particularly for morphologically complex
languages.
13http://www.wikipedia.org
14http://www.mturk.com/mturk/
15http://litgloss.buffalo.edu/
16http://www.espgame.org/
17http://wiktionary.org/
Other researchers have experimented with the
automatic creation of corpora using web data
(Ghani et al, 2001) Some of these corpora have
grown to reasonable sizes; (Scannell, 2003; Scan-
nell, 2006) has corpora derived from web crawling
which are measured in tens of millions of words
for a variety of lower-density languages. However
it should be noted that in these cases, the type of
linguistic resource created is often not linguisti-
cally annotated, but rather a lexicon or collection
of primary texts in a given language.
Finally, we may mention efforts to create cer-
tain kinds of resources by computer-directed elic-
itation. Examples of projects sharing this focus
include BOAS (Nirenburg and Raskin, 1998), and
the AVENUE project (Probst et al, 2002), (Lavie
et al, 2003).
7 Reducing Data Requirements
Creating more annotated resources is the obvious
way to approach the problem of the lack of re-
sources for lower-density languages. A comple-
mentary approach is to improve the way the infor-
mation in smaller resources is used, for example
by developing machine translation systems that re-
quire less parallel text.
How much reduction in the required amount of
resources might be enough? An interesting ex-
periment, which to our knowledge has never been
tried, would be for a linguist to attempt as a test
case what we hope that computers can do. That
is, a linguist could take a ?small? quantity of paral-
lel text, and extract as much lexical and grammat-
ical information from that as possible. The lin-
guist might then take a previously unseen text in
the target language and translate it into English,
or perform some other useful task on target lan-
guage texts. One might argue over whether this
experiment would constitute an upper bound on
how much information could be extracted, but it
would probably be more information than current
computational approaches extract.
Naturally, this approach partially shifts the
problem from the research community interested
in linguistically annotated corpora to the research
community interested in algorithms. Much ef-
fort has been invested in scaling algorithmic ap-
proaches upwards, that is, leveraging every last
available data point in pursuit of small perfor-
mance improvements. We argue that scaling down
(ie using less training data) poses an equally sig-
34
nificant challenge. The basic question of whether
methods which are data-rich can scale down to im-
poverished data has been the focus of a number of
recent papers in areas such as machine translation
(Somers, 1997; Somers, 1998), language identifi-
cation (Hughes et al, 2006b) etc. However, tasks
which have lower-density language at their core
have yet to become mainstream in shared evalua-
tion tasks which drive much of the algorithmic im-
provements in computational linguistics and natu-
ral language processing.
Another approach to data reduction is to change
the type of data required for a given task. For
many lower-density languages a significant vol-
ume of linguistically annotated data exists, but not
in the form of the curated, standardised corpora
to which language technologists are accustomed.
Neverthless for extremely low density languages,
a degree of standardisation is apparent by virtue of
documentary linguistic practice. Consider for ex-
ample, the number of Shoebox lexicons and cor-
responding interlinear texts which are potentially
available from documentary sources: while not
being the traditional resource types on which sys-
tems are trained, they are reasonably accessible,
and cover a larger number of languages. Bible
translations are another form of parallel text avail-
able in nearly every written language (see (Resnik
et al, 1999)). There are of course issues of quality,
not to mention vocabulary, that arise from using
the Bible as a source of parallel text, but for some
purposes ? such as morphology learning ? Bible
translations might be a very good source of data.
Similarly, a different compromise may be found
in the ratio of the number of words in a corpus
to the richness of linguistic annotation. In many
high-density corpora development projects, an ar-
bitrary (and high) target for the number of words is
often set in advance, and subsequent linguistic an-
notation is layered over this base corpus in a pro-
gressively more granular fashion. It may be that
this corpus development model could be modified
for lower-density language resource development:
we argue that in many cases, the richness of lin-
guistic annotation over a given set of data is more
important than the raw quantity of the data set.
A related issue is different standards for an-
notating linguistic concepts We already see this
in larger languages (consider the difference in
morpho-syntactic tagging between the Penn Tree-
bank and other corpora), but has there is a higher
diversity of standards in lower-density languages.
Solutions may include ontologies for linguistic
concepts e.g. General Ontology for Linguistic
Description18 and the ISO Data Category Reg-
istry (Ide and Romary, 2004), which allow cross-
resource navigation based on common semantics.
Of course, cross-language and cross-cultural se-
mantics is a notoriously difficult subject.
Finally, it may be that development of web
based corpora can act as the middle ground: there
are plenty of documents on the web in lower-
density languages, and efforts such as projects by
Scannell19 and Lewis20 indicate these can be cu-
rated reasonably efficiently, even though the out-
comes may be slightly different to that which we
are accustomed. Is it possible to make use of XML
or HTML markup directly in these cases? Some-
day, the semantic web may help us with this type
of approach.
8 Moving Forward
Having considered the status of linguistically-
annotated resources for lower-density languages,
and two broad strategies for improving this situ-
ation (innovative approaches to data creation, and
scaling down of resource requirements for existing
techniques), we now turn to the question of where
to go from here. We believe that there are a num-
ber of practical steps which can be taken in order
to increase the number of linguistically-annotated
lower-density language resources available to the
research community:
? Encouraging the publication of electronic
corpora of lower-density languages: most
economic incentives for corpus creation only
exhibit return on investment because of the
focus on higher-density languages; new mod-
els of funding and commercializing corpora
for lower-density languages are required.
? Engaging in research on bootstrapping from
higher density language resources to lower-
density surrogates: it seems obvious that
at least for related languages adopting a
derivational approach to the generation of
linguistically annotated corpora for lower-
density languages by using automated an-
notation tools trained on higher-density lan-
18http://www.linguistics-ontology.org
19http://borel.slu.edu/crubadan/stadas.html
20http://www.csufresno.edu/odin
35
guages may at least reduce the human effort
required.
? Scaling down (through data requirement re-
duction) of state of the art algorithms: there
has been little work in downscaling state of
the art algorithms for tasks such as named
entity recognition, POS tagging and syntac-
tic parsing, yet (considerably) reducing the
training data requirement seems like one of
the few ways that existing analysis technolo-
gies can be applied to lower-density lan-
guages.
? Shared evaluation tasks which include lower-
density languages or smaller amounts of data:
most shared evaluation tasks are construed
as exercises in cross-linguistic scalability (eg
CLEF) or data intensivity (eg TREC) or both
(eg NTCIR). Within these constructs there
is certainly room for the inclusion of lower-
density languages as targets, although no-
tably the overhead here is not in the provi-
sion of the language data, but the derivatives
(eg query topics) on which these exercises are
based.
? Promotion of multilingual corpora which in-
clude lower-density languages: as multi-
lingual corpora emerge, there is opportu-
nity to include lower-density languages at
minimal opportunity cost e.g. EuroGOV
(Sigurbjo?nsson et al, 2005) or JRC-Acquis
(Steinberger et al, 2006), which are based on
web data from the EU, includes a number of
lower-density languages by virtue of the cor-
pus creation mechanism not being language-
specific.
? Language specific strategies: collectively we
have done well at developing formal strate-
gies for high density languages e.g. in EU
roadmaps, but not so well at strategies for
medium-density or lower-density languages.
The models for medium to long term strate-
gies of language resource development may
be adopted for lower density languages. Re-
cently this has been evidenced through events
such as the LREC 2006 workshop on African
language resources and the development of a
corresponding roadmap.
? Moving towards interoperability between an-
notation schemes which dominate the higher-
density languages (eg Penn Treebank tag-
ging conventions) and the relatively ad-hoc
schemes often exhibited by lower-density
languages, through means such as markup
ontologies like the General Ontology for Lin-
guistic Description or the ISO Data Category
Registry.
Many of these steps are not about to be realised
in the short term. However, developing a cohesive
strategy for addressing the need for linguistically
annotated corpora is a first step in ensuring com-
mittment from interested researchers to a common
roadmap.
9 Conclusion
It is clear that the number of linguistically-
annotated resources for any language will in-
evitably be less than optimal. Regardless of the
density of the language under consideration, the
cost of producing linguistically annotated corpora
of a substantial size is significant, Inevitably, lan-
guages which do not have a strong political, eco-
nomic or social status will be less well resourced.
Certain avenues of investigation e.g. collect-
ing language specific web content, or building ap-
proximate bitexts web data are being explored, but
other areas (such as rich morphosyntactic annota-
tion) are not particularly evidenced.
However, there is considerable research inter-
est in the development of linguistically annotated
resources for languages of lower density. We are
encouraged by the steady rate at which academic
papers emerge reporting the development of re-
sources for lower-density language targets. We
have proposed a number of steps by which the is-
sue of language resources for lower-density lan-
guages may be more efficiently created and look
forward with anticipation as to how these ideas
motivate future work.
References
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2001.
Mining the web to create minority language corpora.
In Proceedings of 2001 ACM International Con-
ference on Knowledge Management (CIKM2001),
pages 279?286. Association for ComputingMachin-
ery.
Raymond G. Gordon. 2005. Ethnologue: Languages
of the World (15th Edition). SIL International: Dal-
las.
36
Baden Hughes, Timothy Baldwin, and Steven Bird.
2006a. Collecting low-density language data on the
web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). Southern Cross University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006b. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC2006). European Language Resources Asso-
ciation: Paris.
Nancy Ide and Laurent Romary. 2004. A registry of
standard data categories for linguistic annotation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004,
pages 135?139. European Language Resources As-
sociation: Paris.
Steven Krauwer. 2003. The basic language resource
kit (BLARK) as the first milestone for the lan-
guage resources roadmap. In Proceedings of 2nd
International Conference on Speech and Computer
(SPECOM2003).
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,
A. Font Llitjos, R. Reynolds, J. Carbonell, and
R. Cohen. 2003. Experiments with a hindi-to-
english transfer-based mt system under a miserly
data scenario. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 2(2).
Valerie Mapelli and Khalid Choukri. 2003. Report
on a monimal set of language resources to be made
available for as many languages as possible, and a
map of the actual gaps. ENABLER internal project
report (Deliverable 5.1).
Tony McEnery, Paul Baker, and Lou Burnard. 2000.
Corpus resources and minority language engineer-
ing. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation
(LREC2002). European Language Resources Asso-
ciation: Paris.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Sergei Nirenburg and Victor Raskin. 1998. Univer-
sal grammar and lexis for quick ramp-up of mt. In
Proceedings of 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 975?979. Association for Computational Lin-
guistics.
Douglas W. Oard. 2003. The surprise language exer-
cises. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 2(2):79?84.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-
bonell. 2002. Mt for minority languages using
elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4):245?270.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a parallel corpus: Annotating
the ?Book of 2000 Tongues?. Computers and the
Humanities, 33(1-2):129?153.
Kevin Scannell. 2003. Automatic thesaurus genera-
tion for minority languages: an irish example. In
Actes des Traitement Automatique des Langues Mi-
noritaires et des Petites Langues, volume 2, pages
203?212.
Kevin Scannell. 2006. Machine translation for
closely related language pairs. In Proceedings of the
LREC2006 Workshop on Strategies for developing
machine translation for minority languages. Euro-
pean Language Resources Association: Paris.
B. Sigurbjo?nsson, J. Kamps, and M. de Rijke. 2005.
Blueprint of a cross-lingual web collection. Journal
of Digital Information Management, 3(1):9?13.
Harold Somers. 1997. Machine translation and minor-
ity languages. Translating and the Computer, 19:1?
13.
Harold Somers. 1998. Language resources and minor-
ity languages. Language Today, 5:20?24.
R. Steinberger, B. Pouliquen, A. Widger, C. Ignat,
T. Erjavec, D. Tufis, and D. Varga. 2006. The jrc-
acquis: A multilingual aligned parallel corpus with
20+ languages. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2006). European Language Resources
Association: Paris.
Stephanie Strassel, Mike Maxwell, and Christopher
Cieri. 2003. Linguistic resource creation for re-
search and technology development: A recent exper-
iment. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 2(2):101?117.
David Yarowsky. 2003. Scalable elicitation of training
data for machine translation. Team Tides, 4.
10 Acknowledgements
The authors are grateful to Kathryn L. Baker for
her comments on earlier drafts of this paper.
Portions of the research in this paper were sup-
ported by the Australian Research Council Spe-
cial Research Initiative (E-Research) grant num-
ber SR0567353 ?An Intelligent Search Infrastruc-
ture for Language Resources on the Web.?
37
