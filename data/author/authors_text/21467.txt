Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 6?11,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Verbal Valency Frame Detection and Selection in Czech and English
Ond
?
rej Du?ek, Jan Haji
?
c and Zde
?
nka Ure?ov?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m
?
est? 25, 11800 Prague 1, Czech Republic
{odusek,hajic,uresova}@ufal.mff.cuni.cz
Abstract
We present a supervised learning method
for verbal valency frame detection and se-
lection, i.e., a specific kind of word sense
disambiguation for verbs based on subcat-
egorization information, which amounts
to detecting mentions of events in text.
We use the rich dependency annotation
present in the Prague Dependency Tree-
banks for Czech and English, taking ad-
vantage of several analysis tools (taggers,
parsers) developed on these datasets pre-
viously. The frame selection is based on
manually created lexicons accompanying
these treebanks, namely on PDT-Vallex for
Czech and EngVallex for English. The re-
sults show that verbal predicate detection
is easier for Czech, but in the subsequent
frame selection task, better results have
been achieved for English.
1 Introduction
Valency frames are a detailed semantic and syn-
tactic description of individual predicate senses.
1
As such, they represent different event types. We
present a system for automatic detection and se-
lection of verbal valency frames in Czech and En-
glish, which corresponds to detecting and disam-
biguating mentions of events in text. This is an im-
portant step toward event instance identification,
which should help greatly in linking the mentions
of a single event. We took advantage of the fact
that the Prague family of dependency treebanks
contains comparable valency frame annotation for
Czech and English (cf. Section 2). Thus the fea-
ture templates used in frame selection are the same
1
Valency can be observed for verbs, nouns, adjectives and
in certain theories, also for other parts of speech; however,
we focus on verbal valency only, as it is most common and
sufficiently described in theory and annotated in treebanks.
and the features initially considered differ only in
their instantiation (cf. Section 3).
While somewhat similar to the CoNLL 2009
Shared Task (Haji
?
c et al., 2009) in the predicate
detection part, our task differs from the semantic
role labeling task in that the whole frame has to
be detected, not only individual arguments, and is
therefore more difficult not only in terms of scor-
ing, but also in the selection part: several verbal
frames might share the same syntactic features,
making them virtually indistinguishable unless se-
mantics is taken into account, combined with a de-
tailed grammatical and morphological context.
2 Valency in the tectogrammatical
description
The annotation scheme of the Prague Dependency
Treebank (Bej
?
cek et al., 2012, PDT) and the
Prague Czech-English Dependency Treebank (Ha-
ji
?
c et al., 2012, PCEDT) is based on the formal
framework of the Functional Generative Descrip-
tion (Sgall, 1967; Sgall et al., 1986, FGD), de-
veloped within the Prague School of Linguistics.
The FGD is dependency-oriented with a ?strati-
ficational? (layered) approach to a systematic de-
scription of a language. The notion of valency in
the FGD is one of the core concepts operating on
the layer of linguistic meaning (tectogrammatical
layer, t-layer).
2.1 Valency frames
The FGD uses syntactic as well as semantic crite-
ria to identify verbal complements. It is assumed
that all semantic verbs ? and, potentially, nouns,
adjectives, and adverbs ? have subcategorization
requirements, which can be specified in the va-
lency frame.
Verbal valency modifications are specified
along two axes: The first axis concerns the (gen-
eral) opposition between inner participants (argu-
ments) and free modifications (adjuncts). This dis-
6
tinction is based on criteria relating to:
(a) the possibility of the same type of comple-
ment appearing multiple times with the same
verb (arguments cannot), and
(b) the possibility of the occurrence of the given
complements (in principle) with any verb
(typical for adjuncts).
The other axis relates to the distinction between
(semantically) obligatory and optional comple-
ments of the word, which again is based on cer-
tain operational criteria expressed as the dialogue
test (Panevov?, 1974). Five arguments are distin-
guished: Actor (ACT), Patient (PAT), Addressee
(ADDR), Origin (ORIG), and Effect (EFF). The
set of free modifications is much larger than that of
arguments; about 50 types of adjuncts are distin-
guished based on semantic criteria. Their set can
be divided into several subclasses: temporal (e.g.,
TWHEN, TSIN), local (e.g., LOC, DIR3), causal
(such as CAUS, CRIT), and other free modifica-
tions (e.g., MANN for general Manner, ACMP for
Accompaniment, EXT for Extent etc.).
All arguments (obligatory or optional) and
obligatory adjuncts are considered to be part of the
valency frame.
2.2 Tectogrammatical annotation
The PDT is a project for FGD-based manual an-
notation of Czech texts, started in 1996 at the In-
stitute of Formal and Applied Linguistics, Charles
University in Prague. It serves two main purposes:
1. to test and validate the FGD linguistic theory,
2. to apply and test machine learning methods for
part-of-speech and morphological tagging, de-
pendency parsing, semantic role labeling, coref-
erence resolution, discourse annotation, natural
language generation, machine translation and
other natural language processing tasks.
The language data in the PDT are non-abbreviated
articles from Czech newspapers and journals.
The PCEDT contains English sentences from
the Wall Street Journal section of the Penn Tree-
bank (Marcus et al., 1993, PTB-WSJ) and their
Czech translations, all annotated using the same
theoretical framework as the PDT.
The annotation of the PDT and the PCEDT is
very rich in linguistic information. Following the
stratificational approach of the FGD, the texts are
annotated at different but interlinked layers. There
are four such layers, two linear and two structured:
? the word layer (w-layer) ? tokenized but other-
wise unanalyzed original text,
? the morphological layer (m-layer) with parts-
of-speech, morphology and lemmatization,
? analytical layer (a-layer) ? surface dependency
syntax trees,
? tectogrammatical layer (t-layer) ? ?deep syn-
tax? trees according to the FGD theory.
While the PDT has all the layers annotated man-
ually, the PCEDT English annotation on the a-
layer has been created by automatic conversion
from the original Penn Treebank, including the
usual head assignment; morphology and the tec-
togrammatical layer are annotated manually, even
if not as richly as for Czech.
2
Valency is a core ingredient on the t-layer. Since
valency frames guide, i.a., the labeling of argu-
ments, valency lexicons with sense-distinguished
entries for both languages have been created to en-
sure consistent annotation.
2.3 Valency Lexicons for Czech and English
in the FGD Framework
PDT-Vallex (Haji
?
c et al., 2003; Ure?ov?, 2011) is a
valency lexicon of Czech verbs, nouns, and adjec-
tives, created in a bottom-up way during the an-
notation of the PDT. This approach made it pos-
sible to confront the pre-existing valency theory
with the real usage of the language.
Each entry in the lexicon contains a head-
word, according to which the valency frames are
grouped, indexed, and sorted. Each valency frame
includes the frame?s ?valency? (number of argu-
ments, or frame members) and the following in-
formation for each argument:
? its label (see Section 2.1),
? its (semantic) obligatoriness according to Pane-
vov? (1974)?s dialogue test,
? its required surface form (or several alternative
forms) typically using morphological, lexical
and syntactic constraints.
Most valency frames are further accompanied by a
note or an example which explains their meaning
and usage. The version of PDT-Vallex used here
contains 9,191 valency frames for 5,510 verbs.
EngVallex (Cinkov?, 2006) is a valency lex-
icon of English verbs based on the FGD frame-
work, created by an automatic conversion from
2
Attributes such as tense are annotated automatically, and
most advanced information such as topic and focus annota-
tion is not present.
7
PropBank frame files (Palmer et al., 2005) and by
subsequent manual refinement.
3
EngVallex was
used for the tectogrammatical annotation of the
English part of the PCEDT. Currently, it contains
7,699 valency frames for 4,337 verbs.
3 Automatic frame selection
Building on the modules for Czech and English
automatic tectogrammatical annotation used in the
TectoMT translation engine (?abokrtsk? et al.,
2008) and the CzEng 1.0 corpus (Bojar et al.,
2012),
4
we have implemented a system for au-
tomatic valency frame selection within the Treex
NLP Framework (Popel and ?abokrtsk?, 2010).
The frame selection system is based on logistic
regression from the LibLINEAR package (Fan et
al., 2008). We use separate classification models
for each verbal lemma showing multiple valency
frames in the training data. Due to identical anno-
tation schemata in both languages, our models use
nearly the same feature set,
5
consisting of:
? the surface word form of the lexical verb and all
its auxiliaries,
? their morphological attributes, such as part-of-
speech and grammatical categories,
? formemes ? compact symbolic morphosyn-
tactic labels (e.g., v:fin for a finite verb,
v:because+fin for a finite verb governed
by a subordinating conjunction, v:in+ger for
a gerund governed by a preposition),
6
? syntactic labels given by the dependency parser,
? all of the above properties found in the topolog-
ical and syntactic neighborhood of the verbal
node on the t-layer (parent, children, siblings,
nodes adjacent in the word order).
We experimented with various classifier settings
(regularization type and cost C, termination crite-
rion E) and feature selection techniques (these in-
volve adding a subset of features according to a
metric against the target class).
7
3
This process resulted in the interlinkage of both lexicons,
with additional links to VerbNet (Schuler, 2005) where avail-
able. Due to the refinement, the mapping is often not 1:1.
4
Note that annotation used in TectoMT and CzEng does
not contain all attributes found in corpora manually annotated
on the tectogrammatical layer. Valency frame IDs are an ex-
ample of an attribute that is missing from the automatic an-
notation of CzEng 1.0.
5
The only differences are due to the differences of part-
of-speech tagsets used.
6
See (Du?ek et al., 2012; Rosa et al., 2012) for a detailed
description of formemes.
7
The metrics used include the Anova F-score, minimum
4 Experiments
We evaluated the system described in Section 3
on PDT 2.5 for Czech and on the English part
of PCEDT 2.0 for English. From PCEDT 2.0,
whose division follows the PTB-WSJ, we used
Sections 02-21 as training data, Section 24 as
development data, and Section 23 as evaluation
data. Since the system is intended to be used in
a fully automatic annotation scenario, we use au-
tomatically parsed sentences with projected gold-
standard valency frames to train the classifiers.
The results of our system in the best setting
for both languages are given in Table 1.
8
The
unlabeled figures measure the ability of the sys-
tem to detect that a valency frame should be filled
for a given node. The labeled figures show the
overall system performance, including selecting
the correct frame. The frame selection accuracy
value shows only the percentage of frames se-
lected correctly, disregarding misplaced frames.
The accuracy for ambiguous verbs further disre-
gards frames of lemmas where only one frame is
possible. Here we include a comparison of our
trained classifier with a baseline that always se-
lects the most frequent frame seen in the training
data.
9
Our results using the classifier for both lan-
guages have been confirmed by pairwise bootstrap
resampling (Koehn, 2004) to be significantly bet-
ter than the baseline at 99% level.
We can see that the system is more successful
in Czech in determining whether a valency frame
should be filled for a given node. This is most
probably given by the fact that the most Czech
verbs are easily recognizable by their morphologi-
cal endings, whereas English verbs are more prone
to be misrepresented as nouns or adjectives.
The English system is better at selecting the cor-
rect valency frame. This is probably caused by
a more fine-grained word sense resolution in the
Czech valency lexicon, where more figurative uses
and idioms are included. For example, over 16%
Redundancy-Maximum Relevance (mRMR) (Peng et al.,
2005), ReliefF (Kononenko, 1994), mutual information (MI),
symmetric uncertainty (Witten and Frank, 2005, p. 291f.),
and an average of the ranks given by mRMR and MI.
8
The best setting for Czech uses L1-regularization and
10% best features according to Anova, with other parame-
ters tuned on the development set for each lemma. The best
setting for English uses L2-regularization with best feature
subsets tuned on the development set and fixed parameters
C = 0.1, E = 0.01.
9
All other parts of the system, up to the identification of
the frame to be filled in, are identical with the baseline.
8
Czech English
Unlabeled precision 99.09 96.03
Unlabeled recall 94.81 93.07
Unlabeled F-1 96.90 94.53
Labeled precision 78.38 81.58
Labeled recall 74.99 79.06
Labeled F-1 76.65 80.30
Frame selection accuracy 79.10 84.95
Ambiguous verbs
baseline 66.68 68.44
classifier 72.41 80.03
Table 1: Experimental results
of errors in the Czech evaluation data were caused
just by idioms or light verb constructions not be-
ing recognized by our system. In Czech, addi-
tional 15% of errors occurred for verbs where two
or more valency frames share the same number of
arguments and their labels, but these verb senses
are considered different (because they have differ-
ent meaning), compared to only 9% in English.
5 Related Work
As mentioned previously, the task of detecting and
selecting valency frames overlaps with semantic
role labeling (Haji
?
c et al., 2009). However, there
are substantial differences: we have focused only
on verbs (as opposed to all words with some se-
mantic relation marked in the data), and evaluated
on the exact frame assigned to the occurrence of
the verb in the treebank. On the other hand, we
are also evaluating predicate identification as in
Surdeanu et al. (2008), which Haji
?
c et al. (2009)
do not. Tagging and parsing have been automatic,
but not performed jointly with the frame selec-
tion task. This also explains that while the best
results reported for the CoNLL 2009 Shared task
(Bj?rkelund et al., 2009) are 85.41% labeled F-1
for Czech and 85.63% for English, they are not
comparable due to several reasons, the main be-
ing that SRL evaluates each argument separately,
while for a frame to be counted as correct in our
task, the whole frame (by means of the refer-
ence ID) must be selected correctly, which is sub-
stantially harder (if only for verbs). Moreover,
we have used the latest version of the PDT (the
PDT 2.5), and EngVallex-annotated verbs in the
PCEDT, while the English CoNLL 2009 Shared
Task is PropBank-based.
10
10
Please recall that EngVallex is a manually refined Prop-
Bank with different labeling scheme and generally m : n
Selecting valency frames is also very similar to
Word Sense Disambiguation (WSD), see e.g. (Ed-
monds and Cotton, 2001; Chen and Palmer, 2005).
The WSD however does not consider subcatego-
rization/valency information explicitly.
Previous works on the PDT include a rule-based
tool of Honetschl?ger (2003) and experiments by
Semeck? (2007) using machine learning. Both of
them, unlike our work, used gold-standard anno-
tation with just the frame ID removed.
6 Conclusions
We have presented a method of detecting mentions
of events in the form of verbal valency frame se-
lection for Czech and English. This method is
based on logistic regression with morphological
and syntactic features, trained on treebanks with
a comparable annotation scheme. We believe that
these results are first for this task on the granu-
larity of the lexicons (PDT-Vallex for Czech and
EngVallex for English), and they seem to be en-
couraging given that the most frequent verbs like
to be and to have have tens of possible frames,
heavily weighing down the resulting scores.
We plan to extend this work to use additional
features and lexical clustering, as well as to see
if the distinctions in the lexicons are justified, i.e.
if humans can effectively distinguish them in the
first place, similar to the work of Cinkov? et al.
(2012). A natural extension is to combine this
work with argument labeling to match or improve
on the ?perfect proposition? score of Surdeanu et
al. (2008) while still keeping the sense distinctions
on top of it. We could also compare this to other
languages for which similar valency lexicons ex-
ist, such as SALSA for German (Burchardt et al.,
2006) or Chinese PropBank (Xue, 2008).
Acknowledgments
This work was supported by the Grant No.
GPP406/13/03351P of the Grant Agency of the
Czech Republic, the project LH12093 of the Min-
istry of Education, Youth and Sports of the Czech
Republic and the Charles University SVV project
260 104. It has been using language resources
developed, stored, and distributed by the LIN-
DAT/CLARIN project of the Ministry of Edu-
cation, Youth and Sports of the Czech Republic
(project LM2010013).
mapping between PropBank and EngVallex frames.
9
References
E. Bej?cek, J. Panevov?, J. Popelka, P. Stra?n?k,
M. ?ev?c?kov?, J. ?t?ep?nek, and Z. ?abokrtsk?.
2012. Prague Dependency Treebank 2.5 ? a revis-
ited version of PDT 2.0. In Proceedings of COLING
2012: Technical Papers, Mumbai.
A. Bj?rkelund, L. Hafdell, and P. Nugues. 2009. Mul-
tilingual semantic role labeling. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, pages 43?48, Boulder, Colorado, United
States, June.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu??c?kov?,
M. Majli?, D. Mare?cek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012. The joy of paral-
lelism with CzEng 1.0. In LREC, page 3921?3928,
Istanbul.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad?,
and M. Pinkal. 2006. The SALSA corpus: a
German corpus resource for lexical semantics. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC-2006).
J. Chen and M. Palmer. 2005. Towards robust high
performance word sense disambiguation of English
verbs using rich linguistic features. In Natural Lan-
guage Processing?IJCNLP 2005, pages 933?944.
Springer.
S. Cinkov?, M. Holub, and V. Kr??. 2012. Manag-
ing uncertainty in semantic tagging. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 840?850. Association for Computational Lin-
guistics.
S. Cinkov?. 2006. From PropBank to EngValLex:
adapting the PropBank-Lexicon to the valency the-
ory of the functional generative description. In
Proceedings of the fifth International conference on
Language Resources and Evaluation (LREC 2006),
Genova, Italy.
O. Du?ek, Z. ?abokrtsk?, M. Popel, M. Majli?,
M. Nov?k, and D. Mare
?
cek. 2012. Formemes
in English-Czech deep syntactic MT. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, page 267?274.
P. Edmonds and S. Cotton. 2001. Senseval-2:
Overview. In The Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems, SENSEVAL ?01, pages 1?5,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
R. E Fan, K. W Chang, C. J Hsieh, X. R Wang, and
C. J Lin. 2008. LIBLINEAR: a library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871?1874.
J. Haji?c, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart?, L. M?rquez, A. Meyers, J. Nivre,
S. Pad?, J. ?t?ep?nek, P. Stra?n?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009), June 4-5, Boulder, Colorado,
USA.
J. Haji?c, E. Haji?cov?, J. Panevov?, P. Sgall, O. Bo-
jar, S. Cinkov?, E. Fu?c?kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?t?ep?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012.
Announcing Prague Czech-English Dependency
Treebank 2.0. In Proceedings of LREC, pages 3153?
3160, Istanbul.
J. Haji?c, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol??rov?, and P. Pajas. 2003. PDT-VALLEX:
creating a large-coverage valency lexicon for tree-
bank annotation. In Proceedings of The Second
Workshop on Treebanks and Linguistic Theories,
volume 9, page 57?68.
V. Honetschl?ger. 2003. Using a Czech valency lexi-
con for annotation support. In Text, Speech and Di-
alogue, pages 120?125. Springer.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Empirical Methods
in Natural Language Processing, pages 388?395.
I. Kononenko. 1994. Estimating attributes: Analysis
and extensions of RELIEF. In Machine Learning:
ECML-94, page 171?182.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguis-
tics, 19(2):330.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Panevov?. 1974. On verbal frames in functional
generative description. Prague Bulletin of Mathe-
matical Linguistics, 22:3?40.
H. Peng, F. Long, and C. Ding. 2005. Feature selec-
tion based on mutual information: criteria of max-
dependency, max-relevance, and min-redundancy.
IEEE Transactions on pattern analysis and machine
intelligence, page 1226?1238.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modu-
lar NLP framework. Advances in Natural Language
Processing, pages 293?304.
R. Rosa, D. Mare
?
cek, and O. Du?ek. 2012. DEPFIX:
a system for automatic correction of Czech MT out-
puts. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, page 362?368. As-
sociation for Computational Linguistics.
10
K. K. Schuler. 2005. VerbNet: A Broad-Coverage,
Comprehensive Verb Lexicon. Ph.D. thesis, Univ. of
Pennsylvania.
J. Semeck?. 2007. Verb valency frames disambigua-
tion. The Prague Bulletin of Mathematical Linguis-
tics, (88):31?52.
P. Sgall, E. Haji?cov?, and J. Panevov?. 1986. The
meaning of the sentence in its semantic and prag-
matic aspects. D. Reidel, Dordrecht.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Praha.
M. Surdeanu, R. Johansson, A. Meyers, L. M?rquez,
and J. Nivre. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 159?177, Manchester, Eng-
land, August. Coling 2008 Organizing Committee.
Z. Ure?ov?. 2011. Valenc?n? slovn?k Pra?sk?ho z?vis-
lostn?ho korpusu (PDT-Vallex). Studies in Compu-
tational and Theoretical Linguistics. ?stav form?ln?
a aplikovan? lingvistiky, Praha, Czechia, ISBN 978-
80-904571-1-9, 375 pp.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Pub, 2nd edition.
N. Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational linguistics, 34(2):225?
255.
Z. ?abokrtsk?, J. Pt?
?
cek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
page 167?170. Association for Computational Lin-
guistics.
11
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 221?228,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Machine Translation of Medical Texts in the Khresmoi Project
Ond
?
rej Du
?
sek, Jan Haji
?
c, Jaroslava Hlav
?
a
?
cov
?
a, Michal Nov
?
ak,
Pavel Pecina, Rudolf Rosa, Ale
?
s Tamchyna, Zde
?
nka Ure
?
sov
?
a, Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague, Czech Republic
{odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz
Abstract
This paper presents the participation of
the Charles University team in the WMT
2014 Medical Translation Task. Our sys-
tems are developed within the Khresmoi
project, a large integrated project aim-
ing to deliver a multi-lingual multi-modal
search and access system for biomedical
information and documents. Being in-
volved in the organization of the Medi-
cal Translation Task, our primary goal is
to set up a baseline for both its subtasks
(summary translation and query transla-
tion) and for all translation directions.
Our systems are based on the phrase-
based Moses system and standard meth-
ods for domain adaptation. The con-
strained/unconstrained systems differ in
the training data only.
1 Introduction
The WMT 2014 Medical Translation Task poses
an interesting challenge for Machine Translation
(MT). In the ?standard? translation task, the end
application is the translation itself. In the Medi-
cal Translation Task, the MT system is considered
a part of a larger system for Cross-Lingual Infor-
mation Retrieval (CLIR) and is used to solve two
different problems: (i) translation of user search
queries, and (ii) translation of summaries of re-
trieved documents.
In query translation, the end user does not even
necessarily see the MT output as their queries are
translated and search is performed on documents
in the target language. In summary translation, the
sentences to be translated come from document
summaries (snippets) displayed to provide infor-
mation on each of the documents retrieved by the
search. Therefore, translation quality may not be
the most important measure in this task ? the per-
formance of the CLIR system as a whole is the
final criterion. Another fundamental difference
from the standard task is the nature of the trans-
lated texts. While we can consider document sum-
maries to be ordinary texts (despite their higher in-
formation density in terms of terminology from a
narrow domain), search queries in the medical do-
main are an extremely specific type of data, and
traditional techniques for system development and
domain adaptation are truly put to a test here.
This work is a part of the of the large integrated
EU-funded Khresmoi project.
1
Among other
goals, such as joint text and image retrieval of ra-
diodiagnostic records, Khresmoi aims to develop
technology for transparent cross-lingual search of
medical sources for both professionals and laypeo-
ple, with the emphasis primarily on publicly avail-
able web sources.
In this paper, we describe the Khresmoi sys-
tems submitted to the WMT 2014 Medical Trans-
lation Task. We participate in both subtasks (sum-
mary translation and query translation) for all
language pairs (Czech?English, German?English,
and French?English) in both directions (to English
and from English). Our systems are based on the
Moses phrase-based translation toolkit and stan-
dard methods for domain adaptation. We submit
one constrained and one unconstrained system for
each subtask and translation direction. The con-
strained and unconstrained systems differ in train-
ing data only: The former use all allowed training
data, the latter take advantage of additional web-
crawled data.
We first summarize previous works in MT do-
main adaptation in Section 2, then describe the
data we used for our systems in Section 3. Sec-
1
http://www.khresmoi.eu/
221
tion 4 contains an account of the submitted sys-
tems and their performance in translation of search
queries and document summaries. Section 5 con-
cludes the paper.
2 Related work
To put our work in the context of other approaches,
we first describe previous work on domain adap-
tation in Statistical Machine Translation (SMT),
then focus specifically on SMT in the medical do-
main.
2.1 Domain adaptation of Statistical machine
translation
Many works on domain adaptation examine the
usage of available in-domain data to directly im-
prove in-domain performance of SMT. Some au-
thors attempt to combine the predictions of two
separate (in-domain and general-domain) transla-
tion models (Langlais, 2002; Sanchis-Trilles and
Casacuberta, 2010; Bisazza et al., 2011; Nakov,
2008) or language models (Koehn and Schroeder,
2007). Wu and Wang (2004) use in-domain data
to improve word alignment in the training phase.
Carpuat et al. (2012) explore the possibility of us-
ing word sense disambiguation to discriminate be-
tween domains.
Other approaches concentrate on the acquisition
of larger in-domain corpora. Some of them ex-
ploit existing general-domain corpora by select-
ing data that resemble the properties of in-domain
data (e.g., using cross-entropy), thus building a
larger pseudo-in-domain training corpus. This
technique is used to adapt language models (Eck
et al., 2004b; Moore and Lewis, 2010) as well as
translation models (Hildebrand et al., 2005; Axel-
rod et al., 2011) or their combination (Mansour et
al., 2011). Similar approaches to domain adapta-
tion are also applied in other tasks, e.g., automatic
speech recognition (Byrne et al., 2004).
2.2 Statistical machine translation in the
medical domain
Eck et al. (2004a) employ an SMT system for the
translation of dialogues between doctors and pa-
tients and show that according to automatic met-
rics, a dictionary extracted from the Unified Medi-
cal Language System (UMLS) Metathesaurus and
its semantic type classification (U.S. National Li-
brary of Medicine, 2009) significantly improves
translation quality from Spanish to English when
applied to generalize the training data.
Wu et al. (2011) analyze the quality of MT on
PubMed
2
titles and whether it is sufficient for pa-
tients. The conclusions are very positive espe-
cially for languages with large training resources
(English, Spanish, German) ? the average fluency
and content scores (based on human evaluation)
are above four on a five-point scale. In automatic
evaluation, their systems substantially outperform
Google Translate. However, the SMT systems are
specifically trained, tuned, and tested on the do-
main of PubMed titles, and it is not evident how
they would perform on other medical texts.
Costa-juss`a et al. (2012) are less optimistic re-
garding SMT quality in the medical domain. They
analyze and evaluate the quality of public web-
based MT systems (such as Google Translate) and
conclude that in both automatic and manual eval-
uation (on 7 language pairs), the performance of
these systems is still not good enough to be used
in daily routines of medical doctors in hospitals.
Jimeno Yepes et al. (2013) propose a method
for obtaining in-domain parallel corpora from ti-
tles and abstracts of publications in the MED-
LINE
3
database. The acquired corpora contain
from 30,000 to 130,000 sentence pairs (depending
on the language pair) and are reported to improve
translation quality when used for SMT training,
compared to a baseline trained on out-of-domain
data. However, the authors use only one source
of in-domain parallel data to adapt the translation
model, and do not use any in-domain monolingual
data to adapt the language model.
In this work, we investigate methods combining
the different kinds of data ? general-domain, in-
domain, and pseudo-in-domain ? to find the opti-
mal approach to this problem.
3 Data description
This section includes an overview of the parallel
and monolingual data sources used to train our
systems. Following the task specification, they
are split into constrained and unconstrained sec-
tions. The constrained section includes medical-
domain data provided for this task (extracted by
the provided scripts), and general-domain texts
provided as constrained data for the standard task
(?general domain? here is used to denote data
2
http://www.ncbi.nlm.nih.gov/pubmed/
3
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
222
Czech?English German?English French?English
dom set pairs source target pairs source target pairs source target
med con 2,498 18,126 19,964 4,998 123,686 130,598 6,139 202,245 171,928
gen con 15,788 226,711 260,505 4,520 112,818 119,404 40,842 1,470,016 1,211,516
gen unc ? ? ? 9,320 525,782 574,373 13,809 961,991 808,222
Table 1: Number of sentence pairs and tokens (source/target) in parallel training data (in thousands).
dom set English Czech German French
med con 172,991 1,848 63,499 63,022
gen con 6,132,107 627,493 1,728,065 1,837,457
med unc 3,275,272 36,348 361,881 908,911
gen unc 618,084 ? 339,595 204,025
Table 2: Number of tokens in monolingual training data (in thousands).
which comes from a mixture of various different
domains, mostly news, parliament proceedings,
web-crawls, etc.). The unconstrained section con-
tains automatically crawled data from medical and
health websites and non-medical data from patent
collections.
3.1 Parallel data
The parallel data summary is presented in Table 1.
The main sources of the medical-domain data
for all the language pairs include the EMEA cor-
pus (Tiedemann, 2009), the UMLS metathesaurus
of health and biomedical vocabularies and stan-
dards (U.S. National Library of Medicine, 2009),
and bilingual titles of Wikipedia articles belonging
to the categories identified to be medical domain.
Additional medical-domain data comes from the
MAREC patent collection: PatTR (W?aschle and
Riezler, 2012) available for DE?EN and FR?EN,
and COPPA (Pouliquen and Mazenc, 2011) for
FR?EN (only patents from the medical categories
A61, C12N, and C12P are allowed in the con-
strained systems).
The constrained general-domain data include
three parallel corpora for all the language pairs:
CommonCrawl (Smith et al., 2013), Europarl ver-
sion 6 (Koehn, 2005), the News Commentary cor-
pus (Callison-Burch et al., 2012). Further, the con-
strained data include CzEng (Bojar et al., 2012)
for CS?EN and the UN corpus for FR?EN.
For our unconstrained experiments, we also em-
ploy parallel data from the non-medical patents
from the PatTR and COPPA collections (other cat-
egories than A61, C12N, and C12P).
3.2 Monolingual data
The monolingual data is summarized in Table 2.
The main sources of the medical-domain mono-
lingual data for all languages involve Wikipedia
pages, UMLS concept descriptions, and non-
parallel texts extracted from the medical patents
of the PatTR collections. For English, the main
source is the AACT collection of texts from Clin-
icalTrials.gov. Smaller resources include: Drug-
Bank (Knox et al., 2011), GENIA (Kim et al.,
2003), FMA (Rosse and Mejino Jr., 2008), GREC
(Thompson et al., 2009), and PIL (Bouayad-Agha
et al., 2000).
In the unconstrained systems, we use additional
monolingual data from web pages crawled within
the Khresmoi project: a collection of about one
million HON-certified
4
webpages in English re-
leased as the test collection for the CLEF 2013
eHealth Task 3 evaluation campaign,
5
additional
web-crawled HON-certified pages (not publicly
available), and other webcrawled medical-domain
related webpages.
The constrained general-domain resources in-
clude: the News corpus for CS, DE, EN, and FR
collected for the purpose of the WMT 2014 Stan-
dard Task, monolingual parts of the Europarl and
News-Commentary corpora, and the Gigaword for
EN and FR.
For the FR?EN and DE?EN unconstrained sys-
tems, the additional general domain monolingual
data is taken from monolingual texts of non-
medical patents in the PatTR collection.
4
https://www.hon.ch/
5
https://sites.google.com/site/
shareclefehealth/
223
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
Figure 1: Distribution of the domain-specificity
scores in the English?French parallel data sets.
3.3 Data preprocessing
The data consisting of crawled web pages, namely
CLEF, HON, and non-HON, needed to be cleaned
and transformed into a set of sentences. The
Boilerpipe (Kohlsch?utter et al., 2010) and Justext
(Pomik?alek, 2011) tools were used to remove boil-
erplate texts and extract just the main content from
the web pages. The YALI language detection tool
(Majli?s, 2012) trained on both in-domain and gen-
eral domain data then filtered out those cleaned
pages which were not identified as written in one
of the concerned languages.
The rest of the preprocessing procedure was ap-
plied to all the datasets mentioned above, both
parallel and monolingual. The data were tok-
enized and normalized by converting or omit-
ting some (mostly punctuation) characters. A
set of language-dependent heuristics was applied
in an attempt to restore and normalize the open-
ing/closing quotation marks, i.e. convert "quoted"
to ?quoted? (Zeman, 2012). The motivation here
is twofold: First, we hope that paired quota-
tion marks could occasionally work as brackets
and better denote parallel phrases for Moses; sec-
ond, if Moses learns to output directed quotation
marks, the subsequent detokenization will be eas-
ier. For all systems which translate from German,
decompounding is employed to reduce source-side
data sparsity. We used BananaSplit for this task
(M?uller and Gurevych, 2006).
We perform all training and internal evaluation
on lowercased data; we trained recasers to post-
process the final submissions.
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
Figure 2: Distribution of the domain-specificity
scores in the French monolingual data sets.
4 Submitted systems
We first describe our technique of psedo-in-
domain data selection in Section 4.1, then com-
pare two methods of combining the selected data
in Section 4.2. This, along with using constrained
and unconstrained data sets to train the systems
(see Section 3), amounts to a total of four system
variants submitted for each task. A description of
the system settings used is given in Section 4.3.
4.1 Data selection
We follow an approach originally proposed for
selection of monolingual sentences for language
modeling (Moore and Lewis, 2010) and its modi-
fication applied to selection of parallel sentences
(Axelrod et al., 2011). This technique assumes
two language models for sentence scoring, one
trained on (true) in-domain text and one trained
on (any) general-domain text in the same lan-
guage (e.g., English). For both data domains
(general and medical), we score each sentence
by the difference of its cross-perplexity given the
in-domain language model and cross-perplexity
given the general-domain language model (in this
order). We only keep sentences with a negative
score in our data, assuming that these are the
most ?medical-like?. Visualisation of the domain-
specificity scores (cross-perplexity difference) in
the FR?EN parallel data and FR monolingual data
is illustrated in Figures 1 and 2, respectively.
6
The
scores (Y axis) are presented for each sentence in
increasing order from left to right (X axis).
6
For the medical domain, constrained and unconstrained
parallel data are identical.
224
cs?en de?en en?cs en?de en?fr fr?en
con concat 33.64?1.14 32.84?1.24 18.10?0.94 18.29?0.92 33.39?1.11 36.71?1.17
con interpol 32.94?1.11 32.31?1.20 18.96?0.93 18.41?0.93 34.06?1.11 37.42?1.21
unc concat 34.10?1.11 34.52?1.20 21.12?1.03 19.76?0.92 36.23?1.03 38.15?1.16
unc interpol 34.48?1.16 34.92?1.17 22.15?1.06 20.81?0.95 36.26?1.13 37.91?1.13
Table 3: BLEU scores of summary translations.
cs?en de?en en?cs en?de en?fr fr?en
con concat 30.87?4.70 33.21?5.03 23.25?4.85 17.72?4.75 28.64?3.77 35.56?4.94
con interpol 32.46?5.05 33.74?4.97 21.56?4.80 16.90?4.39 29.34?3.73 35.28?5.26
unc concat 34.88?5.04 31.24?5.59 22.61?4.91 19.13?5.66 33.08?3.80 36.73?4.88
unc interpol 33.82?5.16 34.19?5.27 23.93?5.16 15.87?11.31 31.19?3.73 40.25?5.14
Table 4: BLEU scores of query translations.
The two language models for sentence scoring
are trained with a restricted vocabulary extracted
from the in-domain training data as words occur-
ring at least twice (singletons and other words are
treated as out-of-vocabulary). In our experiments,
we apply this technique to select both monolin-
gual data for language models and parallel data
for translation models. Selection of parallel data
is based on the English side only. The in-domain
models are trained on the monolingual data in the
target language (constrained or unconstrained, de-
pending on the setting). The general-domain mod-
els are trained on the WMT News data.
Compared to the approach of Moore and Lewis
(2010) and Axelrod et al. (2011), we prune the
model vocabulary more aggressively ? we discard
not only the singletons, but also all words with
non-Latin characters, which helps clean the mod-
els from noise introduced by the automatic process
of data acquisition by web crawling.
4.2 Data combination
For both parallel and monolingual data, we obtain
two data sets after applying the data selection:
? ?medical-like? data from the medical domain
? ?medical-like? data from the general domain.
For each language pair and for each system
type (constrained/unconstrained), we submitted
two system variants which differ in how the se-
lected data are combined. The first variant uses
a simple concatenation of the two datasets both
for parallel data and for language model data. In
the second variant, we train separate models for
each section and use linear interpolation to com-
bine them into a single model. For language mod-
els, we use the SRILM linear interpolation feature
(Stolcke, 2002). We interpolate phrase tables us-
ing Tmcombine (Sennrich, 2012). In both cases,
the held-out set for minimizing the perplexity is
the system development set.
4.3 System details
We compute word alignment on lowercase 4-cha-
racter stems using fast align (Dyer et al., 2013).
We create phrase tables using the Moses toolkit
(Koehn et al., 2007) with standard settings. We
train 5-gram language models on the target-side
lowercase forms using SRILM. We use MERT
(Och, 2003) to tune model weights in our systems
on the development data provided for the task.
The only difference between the system variants
for query and summary translation is the tuning
set. In both cases, we use the respective sets pro-
vided offcially for the shared task.
4.4 Results
Tables 3 and 4 show case-insensitive BLEU scores
of our systems.
7
As expected, the unconstrained
systems outperform the constrained ones. Linear
interpolation outperforms data concatenation quite
reliably across language pairs for summary trans-
lation. While the picture for query translation is
similar, there is more variance in the results, so
we cannot state that interpolation definitely works
7
As we use the same recasers for both summary and query
translation, our systems are heavily penalized for wrong let-
ter case in query translation. However, letter case is not taken
into account in most CLIR systems. All BLEU scores re-
ported in this paper will be case-insensitive for this reason.
225
better in this case. This is due to the sizes of the
development and test sets and most importantly
due to sentence lengths ? queries are very short,
making BLEU unreliable, MERT unstable, and
bootstrap resampling intervals wide.
If we compare our score to the other competi-
tors, we are clearly worse than the best systems for
summary translation. From this perspective, our
data filtering seems overly eager (i.e., discarding
all sentence pairs with a positive perplexity differ-
ence). An experiment which we leave for future
work is doing one more round of interpolation to
combine a model trained on the data with negative
perplexity with models trained on the remainder.
5 Conclusions
We described the Charles University MT system
used in the Shared Medical Translation Task of
WMT 2014. Our primary goal was to set up a
baseline for both the subtasks and all translation
directions. The systems are based on the Moses
toolkit, pseudo-in-domain data selection based on
perplexity difference and two different methods of
in-domain and out-of-domain data combination:
simple data concatenation and linear model inter-
polation.
We report results of constrained and uncon-
strained systems which differ in the training data
only. In most experiments, using additional data
improved the results compared to the constrained
systems and using linear model interpolation out-
performed data concatenation. While our systems
are on par with best results for case-insensitive
BLEU score in query translation, our overly ea-
ger data selection techniques caused lower scores
in summary translation. In future work, we plan
to include a special out-of-domain model in our
setup to compensate for this problem.
Acknowledgments
This work was supported by the EU FP7 project
Khresmoi (contract no. 257528), the Czech Sci-
ence Foundation (grant no. P103/12/G084), and
SVV project number 260 104. This work has
been using language resources developed, stored,
and distributed by the LINDAT/CLARIN project
of the Ministry of Education, Youth and Sports of
the Czech Republic (project LM2010013).
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 355?
362, Edinburgh, United Kingdom. ACL.
A. Bisazza, N. Ruiz, and M. Federico. 2011. Fill-
up versus interpolation methods for phrase-based
SMT adaptation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 136?143, San Francisco, CA, USA. Interna-
tional Speech Communication Association.
O. Bojar, Z.
?
Zabokrtsk?y, O. Du?sek, P. Galu?s?c?akov?a,
M. Majli?s, D. Mare?cek, J. Mar?s??k, M. Nov?ak,
M. Popel, and A. Tamchyna. 2012. The joy of
parallelism with CzEng 1.0. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation, pages 3921?3928, Istanbul,
Turkey. European Language Resources Association.
N. Bouayad-Agha, D. R. Scott, and R. Power. 2000.
Integrating content and style in documents: A case
study of patient information leaflets. Information
Design Journal, 9(2?3):161?176.
W. Byrne, D. S. Doermann, M. Franz, S. Gustman,
J. Haji?c, D. W. Oard, et al. 2004. Automatic recog-
nition of spontaneous speech for access to multilin-
gual oral history archives. Speech and Audio Pro-
cessing, IEEE Transactions on, 12(4):420?435.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada. ACL.
M. Carpuat, H. Daum?e III, A. Fraser, C. Quirk,
F. Braune, A. Clifton, et al. 2012. Domain adap-
tation in machine translation: Final report. In
2012 Johns Hopkins Summer Workshop Final Re-
port, pages 61?72. Johns Hopkins University.
M. R. Costa-juss`a, M. Farr?us, and J. Serrano Pons.
2012. Machine translation in medicine. A qual-
ity analysis of statistical machine translation in the
medical domain. In Proceedings of the 1st Virtual
International Conference on Advanced Research in
Scientific Areas, pages 1995?1998,
?
Zilina, Slovakia.
?
Zilinsk?a univerzita.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A sim-
ple, fast, and effective reparameterization of IBM
model 2. In Proceedings of NAACL-HLT, pages
644?648.
M. Eck, S. Vogel, and A. Waibel. 2004a. Improv-
ing statistical machine translation in the medical do-
main using the Unified Medical Language System.
In COLING 2004: Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 792?798, Geneva, Switzerland. ACL.
226
M. Eck, S. Vogel, and A. Waibel. 2004b. Language
model adaptation for statistical machine translation
based on information retrieval. In Maria Teresa
Lino, Maria Francisca Xavier, F?atima Ferreira, Rute
Costa, and Raquel Silva, editors, Proceedings of the
International Conference on Language Resources
and Evaluation, pages 327?330, Lisbon, Portugal.
European Language Resources Association.
A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel.
2005. Adaptation of the translation model for statis-
tical machine translation based on information re-
trieval. In Proceedings of the 10th Annual Con-
ference of the European Association for Machine
Translation, pages 133?142, Budapest, Hungary.
European Association for Machine Translation.
A. Jimeno Yepes,
?
E. Prieur-Gaston, and A. N?ev?eol.
2013. Combining MEDLINE and publisher data to
create parallel corpora for the automatic translation
of biomedical text. BMC Bioinformatics, 14(1):1?
10.
J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl 1):i180?
i182.
C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. C. Guo, and D. S. Wishart.
2011. DrugBank 3.0: a comprehensive resource for
?Omics? research on drugs. Nucleic acids research,
39(suppl 1):D1035?D1041.
P. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 224?227, Prague,
Czech Republic. ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Praha, Czechia, June. ACL.
P. Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86, Phuket, Thailand. Asia-Pacific Association
for Machine Translation.
C. Kohlsch?utter, P. Fankhauser, and W. Nejdl. 2010.
Boilerplate detection using shallow text features. In
Proceedings of the Third ACM International Confer-
ence on Web Search and Data Mining, WSDM ?10,
pages 441?450, New York, NY, USA. ACM.
P. Langlais. 2002. Improving a general-purpose statis-
tical translation engine by terminological lexicons.
In COLING-02 on COMPUTERM 2002: second
international workshop on computational terminol-
ogy, volume 14, pages 1?7, Taipei, Taiwan. ACL.
M. Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
46?54, Avignon, France. ACL.
S. Mansour, J. Wuebker, and H. Ney. 2011. Com-
bining translation and language model scoring for
domain-specific data filtering. In International
Workshop on Spoken Language Translation, pages
222?229, San Francisco, CA, USA. ISCA.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Uppsala, Sweden. ACL.
C. M?uller and I. Gurevych. 2006. Exploring the po-
tential of semantic relatedness in information re-
trieval. In LWA 2006 Lernen ? Wissensentdeck-
ung ? Adaptivit?at, 9.-11.10.2006, Hildesheimer In-
formatikberichte, pages 126?131, Hildesheim, Ger-
many. Universit?at Hildesheim.
P. Nakov. 2008. Improving English?Spanish statistical
machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recas-
ing. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 147?150, Colum-
bus, OH, USA. ACL.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167, Morristown,
NJ, USA. ACL.
J. Pomik?alek. 2011. Removing Boilerplate and Du-
plicate Content from Web Corpora. PhD thesis,
Masaryk University, Faculty of Informatics, Brno.
B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR
and TAPTA: three tools to assist in overcoming the
patent barrier at WIPO. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 24?30,
Xiamen, China. Asia-Pacific Association for Ma-
chine Translation.
C. Rosse and Jos?e L. V. Mejino Jr. 2008. The foun-
dational model of anatomy ontology. In A. Burger,
D. Davidson, and R. Baldock, editors, Anatomy On-
tologies for Bioinformatics, volume 6 of Computa-
tional Biology, pages 59?117. Springer London.
G. Sanchis-Trilles and F. Casacuberta. 2010. Log-
linear weight optimisation via Bayesian adaptation
in statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1077?1085, Bei-
jing, China. ACL.
227
R. Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical ma-
chine translation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 539?549. ACL.
J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,
C. Callison-Burch, and A. Lopez. 2013. Dirt cheap
web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1374?1383, Sofia, Bulgaria.
ACL.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
P. Thompson, S. Iqbal, J. McNaught, and Sophia Ana-
niadou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
J. Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248, Borovets,
Bulgaria. John Benjamins.
U.S. National Library of Medicine. 2009. UMLS
reference manual. Metathesaurus. Bethesda, MD,
USA.
K. W?aschle and S. Riezler. 2012. Analyzing paral-
lelism and domain similarities in the MAREC patent
corpus. In M. Salampasis and B. Larsen, edi-
tors, Multidisciplinary Information Retrieval, vol-
ume 7356 of Lecture Notes in Computer Science,
pages 12?27. Springer Berlin Heidelberg.
H. Wu and H. Wang. 2004. Improving domain-specific
word alignment with a general bilingual corpus. In
Robert E. Frederking and Kathryn B. Taylor, editors,
Machine Translation: From Real Users to Research,
volume 3265 of Lecture Notes in Computer Science,
pages 262?271. Springer Berlin Heidelberg.
C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical
machine translation for biomedical text: are we there
yet? AMIA Annual Symposium proceedings, pages
1290?1299.
D. Zeman. 2012. Data issues of the multilingual trans-
lation matrix. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 395?
400, Montr?eal, Canada. ACL.
228
Proceedings of the SIGDIAL 2014 Conference, pages 79?83,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Alex: Bootstrapping a Spoken Dialogue System for a New Domain by Real
Users
?
Ond
?
rej Du?ek, Ond
?
rej Pl?tek, Luk?? ?ilka, and Filip Jur
?
c?
?
cek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m
?
est? 25, CZ-11800 Prague, Czech Republic
{odusek,oplatek,zilka,jurcicek}@ufal.mff.cuni.cz
Abstract
When deploying a spoken dialogue sys-
tem in a new domain, one faces a situation
where little to no data is available to train
domain-specific statistical models. We de-
scribe our experience with bootstrapping
a dialogue system for public transit and
weather information in real-word deploy-
ment under public use. We proceeded in-
crementally, starting from a minimal sys-
tem put on a toll-free telephone number to
collect speech data. We were able to incor-
porate statistical modules trained on col-
lected data ? in-domain speech recogni-
tion language models and spoken language
understanding ? while simultaneously ex-
tending the domain, making use of auto-
matically generated semantic annotation.
Our approach shows that a successful sys-
tem can be built with minimal effort and
no in-domain data at hand.
1 Introduction
The Alex Public Transit Information System is an
experimental Czech spoken dialogue system pro-
viding information about all kinds of public tran-
sit in the Czech Republic, publicly available at a
toll-free 800 telephone number.
1
It was launched
for public use as soon as a first minimal working
version was developed, using no in-domain speech
data. We chose an incremental approach to sys-
tem development in order to collect call data and
use them to bootstrap statistical modules. Nearly
?
This work was funded by the Ministry of Education,
Youth and Sports of the Czech Republic under the grant
agreement LK11221 and core research funding, SVV project
260 104, and grants GAUK 2058214 and 2076214 of Charles
University in Prague. It used language resources stored and
distributed by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech Republic
(project LM2010013).
1
Call 800-899-998 from the Czech Republic.
a year after launch, we have collected over 1,300
calls from the general public, which enabled us
to train and deploy an in-domain language model
for Automatic Speech Recognition (ASR) and a
statistical Spoken Language Understanding (SLU)
module. The domain supported by the system has
extended from transit information in one city to ca.
5,000 towns and cities in the whole country, plus
weather and time information. This shows that a
even a very basic system is useful in collecting in-
domain data and that the incremental approach is
viable.
Spoken dialogue systems have been a topic of
research for the past several decades, and many
experimental systems were developed and tested
with users (Walker et al., 2001; Ga?i
?
c et al., 2013;
Janarthanam et al., 2013). However, few experi-
mental systems became available to general public
use. Let?s Go (Raux et al., 2005; Raux et al., 2006)
is a notable example in the public transportation
domain. Using interaction with users from the
public to bootstrap data-driven methods and im-
prove the system is also not a common practice.
Both Let?s Go and the GOOG-411 business finder
system (Bacchiani et al., 2008) collected speech
data, but applied data-driven methods only to im-
prove statistical ASR. We use the call data for sta-
tistical SLU as well and plan to further introduce
statistical modules for dialogue management and
natural language generation.
Our spoken dialogue system framework is
freely available on GitHub
2
and designed for easy
adaptation to new domains and languages. An En-
glish version of our system is in preparation.
We first present the overall structure of the Alex
SDS framework and then describe the minimal
system that has been put to public use, as well as
our incremental extensions. Finally, we provide
an evaluation of our system based on the recorded
calls.
2
http://github.com/UFAL-DSG/alex
79
2 Overall Alex SDS System Structure
The basic architecture of Alex is modular and con-
sists of the traditional SDS components: automatic
speech recognizer (ASR), spoken language under-
standing (SLU), dialogue manager (DM), natural
language generator (NLG), and a text-to-speech
(TTS) module.
We designed the system to allow for easy re-
placement of the individual components: There is
a defined interface for each of them. As the in-
terfaces are domain-independent, changing the do-
main is facilitated as well by this approach.
3 Baseline Transit Information System
We decided to create a minimal working system
that would not require any in-domain data and
open it to general public to collect call data as soon
as possible. We believe that this is a viable al-
ternative to Wizard-of-Oz experiments (Rieser and
Lemon, 2008), allowing for incremental develop-
ment and producing data that correspond to real
usage scenarios (see Section 4).
3.1 Baseline Implementation of the
Components
Having no in-domain data available, we resorted
to very basic implementations using hand-written
rules or external services:
? ASR used a neural network based voice activity
detector trained on small out-of-domain data.
Recordings classified as speech were fed to the
the web-based Google ASR service.
? SLU was handcrafted for our domain using sim-
ple keyword-spotting rules.
? In DM, the dialogue tracker held only one value
per dialogue slot, and the dialogue policy was
handcrafted for the basic tasks in our domain.
? NLG is a simple template-based module.
? We use a web-based Czech TTS service pro-
vided to us by SpeechTech.
3
3.2 Baseline Domain
At baseline, our domain only consisted of a very
basic public transport information for the city of
Prague. Our ontology contained ca. 2,500 public
transit stops. The system was able to present the
next connection between two stops requested by
the user, repeat the information, or return several
3
http://www.speechtech.cz/
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 120
25
30
35
40
45
50
Google ASR
Kaldi ASR Training set portion
Wo
rde
rror
rate
(%)
Figure 1: ASR word error rate depending on the
size of in-domain language model training data
The full training set amounts to 9,495 utterances (30,126 to-
kens). The test set contains 1,187 utterances (4,392 tokens).
following connections. Connection search was
based on Google Directions API.
4
4 Collecting Data and Extending the
System in Real Usage
We launched our system at a public toll-free 800
number and advertised the service at our univer-
sity, among friends, and via Facebook. We also
cooperate with the Czech Blind United associa-
tion,
5
promoting our system among its members
and receiving comments about its use. We adver-
tised our extensions and improvements using the
same channels.
We record and collect all calls to the system,
including our own testing calls, to obtain training
data and build statistical models into our system.
4.1 Speech Recognition: Building In-Domain
Models
The Google on-line ASR service, while reach-
ing state-of-the-art performance in some tasks
(Morbini et al., 2013), showed very high word er-
ror rate in our specific domain (see Figure 1). We
replaced it with the Kaldi ASR engine (Povey et
al., 2011) trained on general-domain Czech acous-
tic data (Korvas et al., 2014) with an in-domain
class-based language model built using collected
call data and lists of all available cities and stops.
We describe our modifications to Kaldi for on-
line decoding in Pl?tek and Jur
?
c?
?
cek (2014). A
performance comparison of Google ASR with
4
https://developers.google.com/maps/
documentation/directions/
5
http://www.sons.cz
80
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 164
66
68
70
72
74
76
78
80
SLU trained on Google ASR
SLU trained on Kaldi ASR
Training set portion
Dia
logu
eac
tite
ms
F-m
eas
ure
(%)
Figure 2: SLU performance (F-measure on dia-
logue act items) depending on training data size
The same data sets as in Figure 1 are used, with semantic
annotations from handcrafted SLU running on manual tran-
scriptions.
Kaldi trained on our data is shown in Figure 1.
One can see that the in-domain language model
brings a substantial improvement, even with very
small data sizes.
4.2 Spoken Language Understanding
To increase system robustness, we built a statisti-
cal SLU based on a set of logistic regression clas-
sifiers and word n-gram features (Jur
?
c?
?
cek et al.,
2014). We train it on the output of our handcrafted
SLU applied to manual transcriptions. We chose
this approach over obtaining manual semantic an-
notation due to two main reasons:
1. Obtaining semantic annotation for Czech data is
relatively slow and complicated; using crowd-
sourcing is not a possibility due to lack of
speakers of Czech on the platforms.
2. As we intended to gradually extend our domain,
semantic annotation changed over time as well.
This approach still allows the statistical SLU to
improve on a handcrafted one by compensating
for errors made by the ASR. Figure 2 shows that
the performance of the statistical SLU module in-
creases with more training data and with the in-
domain ASR models.
4.3 Dialogue Manager
We have replaced the initial simplistic dialogue
state tracker (see Section 3.1) by the probabilis-
tic discriminative tracker of ?ilka et al. (2013),
which achieves near state-of-the-art performance
while remaining completely parameter-free. This
property allowed us to employ the tracker without
any training data; our gradual domain extensions
also required no further adjustments.
The dialogue policy is handcrafted, though it
takes advantage of uncertainty estimated by the
belief tracker. Its main logic is similar to that of
Jur
?
c?
?
cek et al. (2012). First, it implements a set of
domain-independent actions, such as:
? dialogue opening, closing, and restart,
? implicit confirmation of changed slots with high
probability of the most probable value,
? explicit confirmation for slots with a lower
probability of the most probable value,
? a choice among two similarly probable values.
Second, domain-specific actions are imple-
mented for the domain(s) described in Section 4.4.
4.4 Extending the Domain
We have expanded our public transit information
domain with the following tasks:
? The user may specify departure or arrival time
in absolute or relative terms (?in ten minutes?,
?tomorrow morning?, ?at 6 pm.?, ?at 8:35? etc.).
? The user may request more details about the
connection: number of transfers, journey dura-
tion, departure and arrival time.
? The user may travel not only among public
transport stops within one city, but also among
multiple cities or towns.
The expansion to multiple cities has lead to an
ontology improvement: The system is able to find
the corresponding city in the database based on a
stop name, and can use a default stop for a given
city. We initially supported three Czech major
cities covered by the Google Directions service,
then extended the coverage to the whole country
(ca. 44,000 stops in 5,000 cities and towns) using
Czech national public transport database provided
by CHAPS.
6
We now also include weather information for all
Czech cities in the system. The user may ask for
weather at the given time or on the whole day. We
use OpenWeatherMap as our data source.
7
Furthermore, the user may ask about the current
time at any point in the dialogue.
5 System Evaluation from Recorded
Calls
We have used the recorded call data for an eval-
uation of our system. Figure 3 presents the num-
6
http://www.idos.cz
7
http://openweathermap.org/
81
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
10
20
30
40
50
60
70
80
90
100
110 Total calls (incl.testing)
Public user callsA
B
C
D
E F
G
H
I
Figure 3: Number of calls per week
The dashed line shows all recorded calls, including those
made by the authors. The full line shows calls from the public
only.
Spikes: A ? initial testing, B ? first advertising, C ? system
partially offline due to a bug, D ? testing statistical SLU mod-
ule, E ? larger advertising with Czech Blind United, F ? test-
ing domain enhancements, G ? no advertising and limited
system performance, H ? deploying Kaldi ASR and nation-
wide coverage, I ? no further advertising.
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
% Informed or apologized% Rather positive answer
Figure 4: System success rates by month
Percentage of calls where the system provided information
(or apology for not having one) and percentage of rather pos-
itive responses to the final question, both shown with standard
error bars.
ber of calls to our system per week and reflects
the testing and advertising phases, as well as some
of our extensions and improvements described in
Section 4. A steeper usage increase is visible in
recent weeks after the introduction of Kaldi ASR
engine and nationwide coverage (see Sections 4.1
and 4.4). The number of calls and unique users
(caller phone numbers) grows steadily; so far,
more than 300 users from the public have made
over 1,300 calls to the system (cf. Figure 5 and
Table 1 in the appendix).
8
Figure 4 (and Table 1 in the appendix) give a de-
tailed view of the success of our system. Informa-
8
We only count calls with at least one valid user utterance,
disregarding calls where users hang up immediately.
tion is provided in the vast majority of calls. Upon
manual inspection of call transcripts, we discov-
ered that about half of the cases where no infor-
mation is provided can be attributed to the system
failing to react properly; the rest is off-topic calls
or users hanging up too early.
We have also introduced a ?final question? as
an additional success metric. After the user says
good-bye, the system asks them if they received
the information they were looking for. By looking
at the transcriptions of responses to this question,
we recognize a majority of them as rather positive
(?Yes?, ?Nearly? etc.); the proportion of positive
reactions seems to remain stable. However, the fi-
nal question is not an accurate measure as most
users seem to hang up directly after receiving in-
formation from the system.
6 Conclusions and Further Work
We use an iterative approach to build a complex
dialogue system within the public transit informa-
tion domain. The system is publicly available on a
toll-free phone number. Our extensible dialogue
system framework as well as the system imple-
mentation for our domain can be downloaded from
GitHub under the Apache 2.0 license.
We have shown that even very limited work-
ing version can be used to collect calls from
the public, gathering training data for statistical
system components. Our experiments with the
Kaldi speech recognizer show that already a small
amount of in-domain data for the language model
brings a substantial improvement. Generating au-
tomatic semantic annotation from recording tran-
scripts allows us to maintain a statistical spoken
language understanding unit with changing do-
main and growing data.
The analysis of our call logs shows that our sys-
tem is able to provide information in the vast ma-
jority of cases. Success rating provided by the
users themselves is mostly positive, yet the con-
clusiveness of this metric is limited as users tend
to hang up directly after receiving information.
In future, we plan to add an English version
of the system and further expand the domain, al-
lowing more specific connection options. As we
gather more training data, we plan to introduce sta-
tistical modules into the remaining system compo-
nents.
82
A System Evaluation Data
In the following, we include additional data from
call logs evaluation presented in Section 5.
Jun '13 Jul Aug Sep Oct Nov Dec Jan '14 Feb Mar Apr May0
150
300
450
600
750
900
1050
1200
1350
Total calls
Unique callers
Figure 5: Cumulative number of calls and unique
callers from the public by weeks
The growth rates of the number of unique users and the total
number of calls both correspond to the testing and advertising
periods shown in Figure 3.
Total calls 1,359
Unique users (caller phone numbers) 304
System informed (or apologized) 1,124
System informed about directions 990
System informed about weather 88
System informed about current time 41
Apologized for not having information 223
System asked the final question 229
Final question answered by the user 199
Rather positive user?s answer 146
Rather negative user?s answer 23
Table 1: Detailed call statistics
Total absolute numbers of calls from general public users
over the period of nearly one year are shown.
References
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schus-
ter, and B. Strope. 2008. Deploying GOOG-411:
early lessons in data, measurement, and testing. In
Proceedings of ICASSP, page 5260?5263. IEEE.
M. Ga?i?c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
bayesian spoken dialogue systems via human inter-
action. In Proceedings of ICASSP, page 8367?8371.
IEEE.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas,
A. Dickinson, X. Liu, W. Mackaness, and B. Web-
ber. 2013. Evaluating a city exploration dialogue
system combining question-answering and pedes-
trian navigation. In Proceedings of ACL.
F. Jur?c??cek, B. Thomson, and S. Young. 2012. Rein-
forcement learning for parameter estimation in sta-
tistical spoken dialogue systems. Computer Speech
& Language, 26(3):168?192.
F. Jur?c??cek, O. Du?ek, and O. Pl?tek. 2014. A factored
discriminative spoken language understanding for
spoken dialogue systems. In Proceedings of TSD.
To appear.
M. Korvas, O. Pl?tek, O. Du?ek, L. ?ilka, and F. Ju-
r?c??cek. 2014. Free English and Czech telephone
speech corpus shared under the CC-BY-SA 3.0 li-
cense. In Proceedings of LREC, Reykjav?k.
F. Morbini, K. Audhkhasi, K. Sagae, R. Artstein,
D. Can, P. Georgiou, S. Narayanan, A. Leuski, and
D. Traum. 2013. Which ASR should i choose for
my dialogue system? In Proceedings of SIGDIAL,
page 394?403.
O. Pl?tek and F. Jur?c??cek. 2014. Free on-line speech
recogniser based on kaldi ASR toolkit producing
word posterior lattices. In Proceedings of SIGDIAL.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget,
O. Glembek, N. Goel, M. Hannemann, P. Motlicek,
Y. Qian, P. Schwarz, et al. 2011. The Kaldi speech
recognition toolkit. In Proceedings of ASRU, page
1?4, Hawaii.
A. Raux, B. Langner, D. Bohus, Alan W. Black, and
M. Eskenazi. 2005. Let?s go public! taking a spo-
ken dialog system to the real world. In Proceedings
of Interspeech.
A. Raux, D. Bohus, B. Langner, Alan W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: one year of Let?s Go! ex-
perience. In Proceedings of Interspeech.
V. Rieser and O. Lemon. 2008. Learning effective
multimodal dialogue strategies from Wizard-of-Oz
data: Bootstrapping and evaluation. In Proceedings
of ACL, page 638?646.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001.
Quantitative and qualitative evaluation of DARPA
communicator spoken dialogue systems. In Pro-
ceedings of ACL, page 515?522.
L. ?ilka, D. Marek, M. Korvas, and F. Jur
?
c?
?
cek. 2013.
Comparison of bayesian discriminative and genera-
tive models for dialogue state tracking. In Proceed-
ings of SIGDIAL, page 452?456, Metz, France.
83
