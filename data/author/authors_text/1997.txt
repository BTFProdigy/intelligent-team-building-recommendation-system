Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?36,
New York, June 2006. c?2006 Association for Computational Linguistics
Answering the Question You Wish They Had Asked:
The Impact of Paraphrasing for Question Answering
Pablo Ariel Duboue
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
duboue@us.ibm.com
Jennifer Chu-Carroll
IBM T.J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
jencc@us.ibm.com
Abstract
State-of-the-art Question Answering (QA)
systems are very sensitive to variations
in the phrasing of an information need.
Finding the preferred language for such
a need is a valuable task. We investi-
gate that claim by adopting a simple MT-
based paraphrasing technique and evalu-
ating QA system performance on para-
phrased questions. We found a potential
increase of 35% in MRR with respect to
the original question.
1 Introduction
In a typical Question Answering system, an input
question is analyzed to formulate a query to re-
trieve relevant documents from a target corpus (Chu-
Carroll et al, 2006; Harabagiu et al, 2006; Sun
et al, 2006). This analysis of the input question
affects the subset of documents that will be exam-
ined and ultimately plays a key role in determining
the answers the system chooses to produce. How-
ever, most existing QA systems, whether they adopt
knowledge-based, statistical, or hybrid methods, are
very sensitive to small variations in the question
form, often yielding substantially different answers
for questions that are semantically equivalent. For
example, our system?s answer to ?Who invented the
telephone?? is ?Alexander Graham Bell;? how-
ever, its top answer to a paraphrase of the above
question ?Who is credited with the invention of the
telephone?? is ?Gutenberg,? who is credited with
the invention of the printing press, while ?Alexander
Graham Bell,? who is credited with the invention of
the telephone, appears in rank four.
To demonstrate the ubiquity of this phenomenon,
we asked the aforementioned two questions to sev-
eral QA systems on the web, including LCC?s Pow-
erAnswer system,1 MIT?s START system,2 Answer-
Bus,3 and Ask Jeeves.4 All systems exhibited dif-
ferent behavior for the two phrasings of the ques-
tion, ranging from minor variations in documents
presented to justify an answer, to major differences
such as the presence of correct answers in the answer
list. For some systems, the more complex question
form posed sufficient difficulty that they chose not
to answer it.
In this paper we focus on investigating a high risk
but potentially high payoff approach, that of improv-
ing system performance by replacing the user ques-
tion with a paraphrased version of it. To obtain can-
didate paraphrases, we adopt a simple yet powerful
technique based on machine translation, which we
describe in the next section. Our experimental re-
sults show that we can potentially achieve a 35% rel-
ative improvement in system performance if we have
an oracle that always picks the optimal paraphrase
for each question. Our ultimate goal is to automat-
ically select from the set of candidates a high po-
tential paraphrase using a component trained against
the QA system. In Section 3, we present our ini-
tial approach to paraphrase selection which shows
that, despite the tremendous odds against selecting
performance-improving paraphrases, our conserva-
tive selection algorithm resulted in marginal im-
provement in system performance.
1http://www.languagecomputer.com/demos
2http://start.csail.mit.edu
3http://www.answerbus.com
4http://www.ask.com
33
(A)
What toxins are most
hazardous to expectant
mothers?
en?it Che tossine sono pi? peri-
colose alle donne incinte?
it?en
Which toxins are more
dangerous to the preg-
nant women?
(B)
Find out about India?s
nuclear weapons pro-
gram.
en?es
Descubra sobre el pro-
grama de las armas nu-
cleares de la India.
es?en
Discover on the program
of the nuclear weapons
of India.
Figure 1: Example of lexical and syntactical paraphrases via MT-paraphrasing using Babelfish.
2 MT-Based Automatic Paraphrasing
To measure the impact of paraphrases on QA sys-
tems, we seek to adopt a methodology by which
paraphrases can be automatically generated from a
user question. Inspired by the use of parallel trans-
lations to mine paraphrasing lexicons (Barzilay and
McKeown, 2001) and the use of MT engines for
word sense disambiguation (Diab, 2000), we lever-
age existing machine translation systems to generate
semantically equivalent, albeit lexically and syntac-
tically distinct, questions.
Figure 1 (A) illustrates how MT-based paraphras-
ing captures lexical paraphrasing, ranging from ob-
taining simple synonyms such as hazardous and
dangerous to deriving more complex equivalent
phrases such as expectant mother and pregnant
woman. In addition to lexical paraphrasing, some
two-way translations achieve structural paraphras-
ing, as illustrated by the example in Figure 1 (B).
Using multiple MT engines can help paraphrase
diversity. For example, in Figure 1 (B), if we use the
@promt translator5 for English-to-Spanish transla-
tion and Babelfish6 for Spanish-to-English transla-
tion, we get ?Find out on the nuclear armament
program of India? where both lexical and struc-
tural paraphrasings are observed.
The motivation of generating an array of lexically
and structurally distinct paraphrases is that some of
these paraphrases may better match the processing
capabilities of the underlying QA system than the
original question and are thus more likely to pro-
duce correct answers. Our observation is that while
the paraphrase set contains valuable performance-
improving phrasings, it also includes a large num-
ber of ungrammatical sentences which need to be fil-
5http://www.online-translator.com
6http://babelfish.altavista.com
Q&A
System
Question
Answer List
Paraphrase
Selection
Feature
Extractor
paraphrase
...
paraphrase
paraphrase
paraphrase
...
paraphrase
paraphrase
paraphrase
MT
Paraphraser
Figure 2: System Architecture.
tered out to reduce negative impact on performance.
3 Using Automatic Paraphrasing in
Question Answering
We use a generic architecture (Figure 2) that treats
a QA system as a black box that is invoked after a
paraphrase generation module, a feature extraction
module, and a paraphrase selection module are exe-
cuted. The preprocessing modules identifies a para-
phrase of the original question, which could be the
question itself, to send as input to the QA system.
A key advantage of treating the core QA system as
a black box is that the preprocessing modules can
be easily applied to improve the performance of any
QA system.7
We described the paraphrase generation module
in the previous section and will discuss the remain-
ing two modules below.
Feature Extraction Module. For each possible
paraphrase, we compare it against the original ques-
tion and compute the features shown in Table 1.
These are a subset of the features that we have ex-
perimented with and have found to be meaningful
for the task. All of these features are required in or-
7In our earlier experiments, we adopted an approach that
combines answers to all paraphrases through voting. These ex-
periments proved unsuccessful: in most cases, the answer to the
original question was amplified, both when right and wrong.
34
Feature Description Intuition
Sum
IDF
The sum of the IDF scores for all terms in
the original question and the paraphrase.
Paraphrases with more informative terms for
the corpus at hand should be preferred.
Lengths Number of query terms for each of the para-phrase and the original question.
We expect QA systems to prefer shorter para-
phrases.
Cosine
Distance
The distance between the vectors of both
questions, IDF-weighted.
Certain paraphrases diverge too much from the
original.
Answer
Types
Whether answer types, as predicted by our
question analyzer, are the same or overlap.
Choosing a paraphrase that does not share an
answer type with the original question is risky.
Table 1: Our features, computed for each paraphrase by comparing it against the original question.
der not to lower the performance with respect to the
original question. They are ordered by their relative
contributions to the error rate reduction.
Paraphrase Selection Module. To select a para-
phrase, we used JRip, the Java re-implementation of
ripper (Cohen, 1996), a supervised rule learner in
the Weka toolkit (Witten and Frank, 2000).
We initially formulated paraphrase selection as a
three-way classification problem, with an attempt to
label each paraphrase as being ?worse,? the ?same,?
or ?better? than the original question. Our objective
was to replace the original question with a para-
phrase labeled ?better.? However, the priors for
these classes are roughly 30% for ?worse,? 65% for
?same,? and 5% for ?better?. Our empirical evi-
dence shows that successfully pinpointing a ?better?
paraphrase improves, on average, the reciprocal rank
for a question by 0.5, while erroneously picking a
?worse? paraphrase results in a 0.75 decrease. That
is to say, errors are 1.5 times more costly than suc-
cesses (and five times more likely). This scenario
strongly suggests that a high precision algorithm is
critical for this component to be effective.
To increase precision, we took two steps. First,
we trained a cascade of two binary classifiers. The
first one classifies ?worse? versus ?same or better,?
with a bias for ?worse.? The second classifier has
classes ?worse or same? versus ?better,? now with a
bias towards ?better.? The second step is to constrain
the confidence of the classifier and only accept para-
phrases where the second classifier has a 100% con-
fidence. These steps are necessary to avoid decreas-
ing performance with respect to the original ques-
tion, as we will show in the next section.
4 Experimental Results
We trained the paraphrase selection module us-
ing our QA system, PIQUANT (Chu-Carroll et al,
2006). Our target corpus is the AQUAINT corpus,
employed in the TREC QA track since 2002.
As for MT engines, we employed Babelfish
and Google MT,8 rule-based systems developed by
SYSTRAN and Google, respectively. We adopted
different MT engines based on the hypothesis that
differences in their translation rules will improve the
effectiveness of the paraphrasing module.
To measure performance, we trained and tested by
cross-validation over 712 questions from the TREC
9 and 10 datasets. We paraphrased the questions us-
ing the four possible combinations of MT engines
with up to 11 intermediate languages, obtaining a
total of 15,802 paraphrases. These questions were
then fed to our system and evaluated per TREC an-
swer key. We obtained a baseline MRR (top five
answers) of 0.345 running over the original ques-
tions. An oracle run, in which the best paraphrase
(or the original question) is always picked would
yield a MRR of 0.48. This potential increase is sub-
stantial, taking into account that a 35% improve-
ment separated the tenth participant from the sec-
ond in TREC-9. Our three-fold cross validation us-
ing the features and algorithm described in Section 3
yielded a MRR of 0.347. Over 712 questions, it re-
placed 14, two of which improved performance, the
rest stayed the same. On the other hand, random
selection of paraphrases decreased performance to
0.156, clearly showing the importance of selecting a
good paraphrase.
8http://translate.google.com
35
5 Related Work
Most of the work in QA and paraphrasing focused
on folding paraphrasing knowledge into the question
analyzer or the answer locator (Rinaldi et al, 2003;
Tomuro, 2003). Our work, on the contrary, focuses
on question paraphrasing as an external component,
independent of the QA system architecture.
Some authors (Dumais et al, 2002; Echihabi et
al., 2004) considered the query sent to a search en-
gine as a ?paraphrase? of the original natural lan-
guage question. For instance, Echihabi et al (2004)
presented a large number of ?reformulations? that
transformed the query into assertions that could
match the answers in text. Here we understand a
question paraphrase as a reformulation that is itself
a question, not a search engine query.
Other efforts in using paraphrasing for QA
(Duclaye et al, 2003) focused on using the Web
to obtain different verbalizations for a seed relation
(e.g., Author/Book); however, they have yet to apply
their learned paraphrases to QA.
Recently, there has been work on identifying para-
phrases equivalence classes for log analysis (Hed-
strom, 2005). Hedstrom used a vector model from
Information Retrieval that inspired our cosine mea-
sure feature described in Section 3.
6 Conclusions
The work presented here makes contributions at
three different levels. First, we have shown that po-
tential impact of paraphrasing with respect to QA
performance is significant. Replacing a question
with a more felicitously worded question can poten-
tially result in a 35% performance increase.
Second, we performed our experiments by tap-
ping into a readily available paraphrase resource:
MT engines. Our results speak of the usefulness of
the approach in producing paraphrases. This tech-
nique of obtaining a large, although low quality,
set of paraphrases can be easily employed by other
NLP practitioners wishing to investigate the impact
of paraphrasing on their own problems.
Third, we have shown that the task of selecting a
better phrasing is amenable to learning, though more
work is required to achieve its full potential. In that
respect, the features and architecture discussed in
Section 3 are a necessary first step in that direction.
In future work, we are interested in developing
effective filtering techniques to reduce our candidate
set to a small number of high precision paraphrases,
in experimenting with state-of-the-art paraphrasers,
and in using paraphrasing to improve the stability of
the QA system.
Acknowledgments
The authors would like to thank Nelson Correa and
Annie Ying for helpful discussions and comments.
This work was supported in part by the Disruptive
Technology Office (DTO)?s Advanced Question An-
swering for Intelligence (AQUAINT) Program un-
der contract number H98230-04-C-1577.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a
parallel corpus. In Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL-EACL 2001), Toulouse, France, July.
Jennifer Chu-Carroll, Pablo A. Duboue, John M. Prager, and Krzysztof Czuba.
2006. IBM?s piquant II in TREC 2005. In E. M. Voorhees and Lori P. Buck-
land, editors, Proceedings of the Fourthteen Text REtrieval Conference Pro-
ceedings (TREC 2005), Gaithersburg, MD, USA.
William Cohen. 1996. Learning trees and rules with set-valued features. In
Proceedings of the 14th joint American Association for Artificial Intelligence
and IAAI Conference (AAAI/IAAI-96), pages 709?716. American Association
for Artificial Intelligence.
Mona Diab. 2000. An unsupervised method for word sense tagging using parallel
corpora: A preliminary investigation. In Special Interest Group in Lexical
Semantics (SIGLEX) Workshop, Association for Computational Linguistics,
Hong Kong, China, October.
Florence Duclaye, Francois Yvon, and Olivier Collin. 2003. Learning para-
phrases to improve a question-answering system. In EACL 2003, 11th Con-
ference of the European Chapter of the Association for Computational Lin-
guistics, Workshop in NLP for QA, Budapest, Hungary, April.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering:
is more always better? In Proc. SIGIR ?02, pages 291?298, New York, NY,
USA. ACM Press.
A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran.
2004. Multiple-engine question answering in textmap. In Proc. TREC 2003.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2006.
Employing two question answering systems. In Proc. TREC 2005.
Anna Hedstrom. 2005. Question categorization for a question answering system
using a vector space model. Master?s thesis, Department of Linguistics and
Philology (Language Technology Programme) Uppsala University, Uppsala,
Sweden.
Fabio Rinaldi, James Dowdall, Kaarel Kaljurand, Michael Hess, and Diego Moll?.
2003. Exploiting paraphrases in a question answering system. In Proceedings
of the Second International Workshop on Paraphrasing, pages 25?32, July.
R. Sun, J. Jiang, Y.F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. 2006. Using
syntactic and semantic relation analysis in question answering. In Proc. TREC
2005.
Noriko Tomuro. 2003. Interrogative reformulation patterns and acquisition of
question paraphrases. In Proceedings of the Second International Workshop
on Paraphrasing, pages 33?40, July.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann Pub-
lishers.
36
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073?1080,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving QA Accuracy by Question Inversion  
John Prager  
IBM T.J. Watson Res. Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Pablo Duboue 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
duboue@us.ibm.com 
Jennifer Chu-Carroll 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
 
Abstract 
This paper demonstrates a conceptually simple 
but effective method of increasing the accuracy 
of QA systems on factoid-style questions.  We 
define the notion of an inverted question, and 
show that by requiring that the answers to the 
original and inverted questions be mutually con-
sistent, incorrect answers get demoted in confi-
dence and correct ones promoted.  Additionally, 
we show that lack of validation can be used to 
assert no-answer (nil) conditions.  We demon-
strate increases of performance on TREC and 
other question-sets, and discuss the kinds of fu-
ture activities that can be particularly beneficial 
to approaches such as ours.  
1 Introduction 
Most QA systems nowadays consist of the following 
standard modules:  QUESTION PROCESSING, to de-
termine the bag of words for a query and the desired 
answer type (the type of the entity that will be of-
fered as a candidate answer); SEARCH, which will 
use the query to extract a set of documents or pas-
sages from a corpus; and ANSWER SELECTION, 
which will analyze the returned documents or pas-
sages for instances of the answer type in the most 
favorable contexts. Each of these components im-
plements a set of heuristics or hypotheses, as de-
vised by their authors (cf. Clarke et al 2001, Chu-
Carroll et al 2003). 
 
When we perform failure analysis on questions in-
correctly answered by our system, we find that there 
are broadly speaking two kinds of failure.  There are 
errors (we might call them bugs) on the implementa-
tion of the said heuristics: errors in tagging, parsing, 
named-entity recognition; omissions in synonym 
lists; missing patterns, and just plain programming 
errors.  This class can be characterized by being fix-
able by identifying incorrect code and fixing it, or 
adding more items, either explicitly or through train-
ing.  The other class of errors (what we might call 
unlucky) are at the boundaries of the heuristics; 
situations were the system did not do anything 
?wrong,? in the sense of bug, but circumstances con-
spired against finding the correct answer. 
 
Usually when unlucky errors occur, the system gen-
erates a reasonable query and an appropriate answer 
type, and at least one passage containing the right 
answer is returned.  However, there may be returned 
passages that have a larger number of query terms 
and an incorrect answer of the right type, or the 
query terms might just be physically closer to the 
incorrect answer than to the correct one.  ANSWER 
SELECTION modules typically work either by trying 
to prove the answer is correct (Moldovan & Rus, 
2001) or by giving them a weight produced by 
summing a collection of heuristic features (Radev et 
al., 2000); in the latter case candidates having a lar-
ger number of matching query terms, even if they do 
not exactly match the context in the question, might 
generate a larger score than a correct passage with 
fewer matching terms. 
 
To be sure, unlucky errors are usually bugs when 
considered from the standpoint of a system with a 
more sophisticated heuristic, but any system at any 
point in time will have limits on what it tries to do; 
therefore the distinction is not absolute but is rela-
tive to a heuristic and system. 
 
It has been argued (Prager, 2002) that the success of 
a QA system is proportional to the impedance match 
between the question and the knowledge sources 
available.  We argue here similarly. Moreover, we 
believe that this is true not only in terms of the cor-
rect answer, but the distracters,1 or incorrect answers 
too.  In QA, an unlucky incorrect answer is not usu-
ally predictable in advance; it occurs because of a 
coincidence of terms and syntactic contexts that 
cause it to be preferred over the correct answer.  It 
has no connection with the correct answer and is 
only returned because its enclosing passage so hap-
pens to exist in the same corpus as the correct an-
swer context.  This would lead us to believe that if a 
                                                     
1 We borrow the term from multiple-choice test design. 
1073
different corpus containing the correct answer were 
to be processed, while there would be no guarantee 
that the correct answer would be found, it would be 
unlikely (i.e. very unlucky) if the same incorrect an-
swer as before were returned. 
 
We have demonstrated elsewhere (Prager et al 
2004b) how using multiple corpora can improve QA 
performance, but in this paper we achieve similar 
goals without using additional corpora. We note that 
factoid questions are usually about relations between 
entities, e.g. ?What is the capital of France??, where 
one of the arguments of the relationship is sought 
and the others given.  We can invert the question by 
substituting the candidate answer back into the ques-
tion, while making one of the given entities the so-
called wh-word, thus ?Of what country is Paris the 
capital??  We hypothesize that asking this question 
(and those formed from other candidate answers) 
will locate a largely different set of passages in the 
corpus than the first time around.  As will be ex-
plained in Section 3, this can be used to decrease the 
confidence in the incorrect answers, and also in-
crease it for the correct answer, so that the latter be-
comes the answer the system ultimately proposes. 
 
This work is part of a continuing program of demon-
strating how meta-heuristics, using what might be 
called ?collateral? information, can be used to con-
strain or adjust the results of the primary QA system.   
 
In the next Section we review related work.  In Sec-
tion 3 we describe our algorithm in detail, and in 
Section 4 present evaluation results.  In Section 5 we 
discuss our conclusions and future work. 
2 Related Work 
Logic and inferencing have been a part of Question-
Answering since its earliest days.  The first such 
systems were natural-language interfaces to expert 
systems, e.g., SHRDLU (Winograd, 1972), or to 
databases, e.g., LIFER/LADDER (Hendrix et al 
1977).  CHAT-80 (Warren & Pereira, 1982), for in-
stance, was a DCG-based NL-query system about 
world geography, entirely in Prolog.  In these 
systems, the NL question is transformed into a se-
mantic form, which is then processed further.  Their 
overall architecture and system operation is very 
different from today?s systems, however, primarily 
in that there was no text corpus to process. 
 
Inferencing is a core requirement of systems that 
participate in the current PASCAL Recognizing 
Textual Entailment (RTE) challenge (see 
http://www.pascal-network.org/Challenges/RTE and 
.../RTE2).   It is also used in at least two of the more 
visible end-to-end QA systems of the present day.  
The LCC system (Moldovan & Rus, 2001) uses a 
Logic Prover to establish the connection between a 
candidate answer passage and the question.  Text 
terms are converted to logical forms, and the ques-
tion is treated as a goal which is ?proven?, with real-
world knowledge being provided by Extended 
WordNet.  The IBM system PIQUANT (Chu-
Carroll et al, 2003) used Cyc (Lenat, 1995) in an-
swer verification.  Cyc can in some cases confirm or 
reject candidate answers based on its own store of 
instance information; in other cases, primarily of a 
numerical nature, Cyc can confirm whether candi-
dates are within a reasonable range established for 
their subtype.   
 
At a more abstract level, the use of inversions dis-
cussed in this paper can be viewed as simply an ex-
ample of finding support (or lack of it) for candidate 
answers.  Many current systems (see, e.g. (Clarke et 
al., 2001; Prager et al 2004b)) employ redundancy 
as a significant feature of operation:  if the same an-
swer appears multiple times in an internal top-n list, 
whether from multiple sources or multiple algo-
rithms/agents, it is given a confidence boost, which 
will affect whether and how it gets returned to the 
end-user. 
 
The work here is a continuation of previous work 
described in (Prager et al 2004a,b).  In the former 
we demonstrated that for a certain kind of question, 
if the inverted question were given, we could im-
prove the F-measure of accuracy on a question set 
by 75%.  In this paper, by contrast, we do not manu-
ally provide the inverted question, and in the second 
evaluation presented here we do not restrict the 
question type. 
3 Algorithm 
3.1 System Architecture 
A simplified block-diagram of our PIQUANT sys-
tem is shown in Figure 1.  The outer block on the 
left, QS1, is our basic QA system, in which the 
QUESTION PROCESSING (QP), SEARCH (S) and 
ANSWER SELECTION (AS) subcomponents are indi-
cated.  The outer block on the right, QS2, is another 
QA-System that is used to answer the inverted ques-
tions.  In principle QS2 could be QS1 but parameter-
ized differently, or even an entirely different system, 
but we use another instance of QS1, as-is.  The 
block in the middle is our Constraints Module CM, 
which is the subject of this paper.  
 
1074
The Question Processing component of QS2 is not 
used in this context since CM simulates its output by 
modifying the output of QP in QS1, as described in 
Section 3.3. 
3.2 Inverting Questions 
Our open-domain QA system employs a named-
entity recognizer that identifies about a hundred 
types.  Any of these can be answer types, and there 
are corresponding sets of patterns in the QUESTION 
PROCESSING module to determine the answer type 
sought by any question.  When we wish to invert a 
question, we must find an entity in the question 
whose type we recognize; this entity then becomes 
the sought answer for the inverted question.  We call 
this entity the inverted or pivot term. 
 
Thus for the question: 
(1) ?What was the capital of Germany in 1985?? 
Germany is identified as a term with a known type 
(COUNTRY).  Then, given the candidate answer 
<CANDANS>, the inverted question becomes  
(2) ?Of what country was < CANDANS> the capital 
in 1985?? 
Some questions have more than one invertible term.  
Consider for example:  
(3) ?Who was the 33rd president of the U.S.?? 
This question has 3 inversion points: 
(4) ?What number president of the U.S. was 
<CANDANS>?? 
(5) ?Of what country was <CANDANS> the 33rd 
president?? 
(6) ?<CANDANS> was the 33rd what of the U.S.?? 
 
Having more than one possible inversion is in theory 
a benefit, since it gives more opportunity for enforc-
ing consistency, but in our current implementation 
we just pick one for simplicity.  We observe on 
training data that, in general, the smaller the number 
of unique instances of an answer type, the more 
likely it is that the inverted question will be correctly 
answered.  We generated a set NELIST of the most 
frequently-occurring named-entity types in ques-
tions; this list is sorted in order of estimated cardi-
nality. 
 
It might seem that the question inversion process can 
be quite tricky and can generate possibly unnatural 
phrasings, which in turn can be difficult to reparse.  
However, the examples given above were simply 
English renditions of internal inverted structures ? as 
we shall see the system does not need to use a natu-
ral language representation of the inverted questions. 
 
Some questions are either not invertible, or, like 
?How did X die?? have an inverted form (?Who died 
of cancer??) with so many correct answers that we 
know our algorithm is unlikely to benefit us.  How-
ever, as it is constituted it is unlikely to hurt us ei-
ther, and since it is difficult to automatically identify 
such questions, we don?t attempt to intercept them.  
As reported in (Prager et al 2004a), an estimated 
79% of the questions in TREC question sets can be 
inverted meaningfully.  This places an upper limit 
on the gains to be achieved with our algorithm, but 
is high enough to be worth pursuing. 
Figure 1.  Constraints Architecture.  QS1 and QS2 are (possibly identical) QA systems. 
Answers
Question 
QS1 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
QS2 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
CM 
constraints 
module 
 
1075
3.3 Inversion Algorithm 
As shown in the previous section, not all questions 
have easily generated inverted forms (even by a hu-
man).  However, we do not need to explicate the 
inverted form in natural language in order to process 
the inverted question. 
 
In our system, a question is processed by the 
QUESTION PROCESSING module, which produces a 
structure called a QFrame, which is used by the sub-
sequent SEARCH and ANSWER SELECTION modules.  
The QFrame contains the list of terms and phrases in 
the question, along with their properties, such as 
POS and NE-type (if it exists), and a list of syntactic 
relationship tuples.  When we have a candidate an-
swer in hand, we do not need to produce the inverted 
English question, but merely the QFrame that would 
have been generated from it.  Figure 1 shows that 
the CONSTRAINTS MODULE takes the QFrame as one 
of its inputs, as shown by the link from QP in QS1 
to CM.  This inverted QFrame can be generated by a 
set of simple transformations, substituting the pivot 
term in the bag of words with a candidate answer 
<CANDANS>, the original answer type with the type 
of the pivot term, and in the relationships the pivot 
term with its type and the original answer type with 
<CANDANS>.  When relationships are evaluated, a 
type token will match any instance of that type.  Fig-
ure 2 shows a simplified view of the original 
QFrame for ?What was the capital of Germany in 
1945??, and Figure 3 shows the corresponding In-
verted QFrame.  COUNTRY is determined to be a 
better type to invert than YEAR, so ?Germany? be-
comes the pivot.  In Figure 3, the token 
<CANDANS> might take in turn ?Berlin?, ?Mos-
cow?, ?Prague? etc. 
 
Figure 2. Simplified QFrame 
 
Figure 3. Simplified Inverted QFrame.   
The output of QS2 after processing the inverted 
QFrame is a list of answers to the inverted question, 
which by extension of the nomenclature we call ?in-
verted answers.?  If no term in the question has an 
identifiable type, inversion is not possible. 
3.4 Profiting From Inversions 
Broadly speaking, our goal is to keep or re-rank the 
candidate answer hit-list on account of inversion 
results.  Suppose that a question Q is inverted 
around pivot term T, and for each candidate answer 
Ci, a list of ?inverted? answers {Cij} is generated as 
described in the previous section.  If T is on one of 
the {Cij}, then we say that Ci is validated.  Valida-
tion is not a guarantee of keeping or improving Ci?s 
position or score, but it helps.  Most cases of failure 
to validate are called refutation; similarly, refutation 
of Ci is not a guarantee of lowering its score or posi-
tion.   
 
It is an open question how to adjust the results of the 
initial candidate answer list in light of the results of 
the inversion.  If the scores associated with candi-
date answers (in both directions) were true prob-
abilities, then a Bayesian approach would be easy to 
develop.  However, they are not in our system.  In 
addition, there are quite a few parameters that de-
scribe the inversion scenario. 
 
Suppose Q generates a list of the top-N candidates 
{Ci}, with scores {Si}.  If this inversion method 
were not to be used, the top candidate on this list, 
C1, would be the emitted answer.  The question gen-
erated by inverting about T and substituting Ci is 
QTi.  The system is fixed to find the top 10 passages 
responsive to QTi, and generates an ordered list Cij 
of candidate answers found in this set. 
 
Each inverted question QTi is run through our sys-
tem, generating inverted answers {Cij}, with scores 
{Sij}, and whether and where the pivot term T shows 
up on this list, represented by a list of positions {Pi}, 
where Pi is defined as: 
 
 Pi  =  j    if Cij = T, for some j 
 Pi  =  -1 otherwise 
 
We added to the candidate list the special answer 
nil, representing ?no answer exists in the corpus.? 
 
As described earlier, we had observed from training 
data that failure to validate candidates of certain 
types (such as Person) would not necessarily be a 
real refutation, so we established a set of types 
SOFTREFUTATION which would contain the broadest 
of our types.  At the other end of the spectrum, we 
observed that certain narrow candidate types such as 
UsState would definitely be refuted if validation 
didn?t occur.  These are put in set MUSTCONSTRAIN. 
Our goal was to develop an algorithm for recomput-
ing all the original scores {Si} from some combina-
tion (based on either arithmetic or decision-trees) of 
Keywords: {1945, <CANDANS>, capital} 
AnswerType: COUNTRY 
Relationships: {(COUNTRY, capital), (capital, 
<CANDANS>), (capital, 1945)} 
Keywords: {1945, Germany, capital} 
AnswerType: CAPITAL 
Relationships: {(Germany, capital), (capital, 
CAPITAL), (capital, 1945)} 
1076
{Si} and {Sij} and membership of SOFTREFUTATION 
and MUSTCONSTRAIN.  Reliably learning all those 
weights, along with set membership, was not possi-
ble given only several hundred questions of training 
data.  We therefore focused on a reduced problem. 
 
We observed that when run on TREC question sets, 
the frequency of the rank of our top answer fell off 
rapidly, except with a second mode when the tail 
was accumulated in a single bucket.  Our numbers 
for TRECs 11 and 12 are shown in Table 1. 
 
Top answer rank TREC11 TREC12 
1 170 108 
2 35 32 
3 23 14 
4 7 7
5 14 9
elsewhere 251 244 
% correct 34 26 
Table 1.  Baseline statistics for TREC11-12. 
 
We decided to focus on those questions where we 
got the right answer in second place (for brevity, 
we?ll call these second-place questions).  Given that 
TREC scoring only rewards first-place answers, it 
seemed that with our incremental approach we 
would get most benefit there.  Also, we were keen to 
limit the additional response time incurred by our 
approach.  Since evaluating the top N answers to the 
original question with the Constraints process re-
quires calling the QA system another N times per 
question, we were happy to limit N to 2.  In addition, 
this greatly reduced the number of parameters we 
needed to learn.  
 
For the evaluation, which consisted of determining if 
the resulting top answer was right or wrong, it meant 
ultimately deciding on one of three possible out-
comes:  the original top answer, the original second 
answer, or nil.  We hoped to promote a significant 
number of second-place finishers to top place and 
introduce some nils, with minimal disturbance of 
those already in first place. 
 
We used TREC11 data for training, and established 
a set of thresholds for a decision-tree approach to 
determining the answer, using Weka (Witten & 
Frank, 2005).  We populated sets SOFTREFUTATION 
and MUSTCONSTRAIN by manual inspection.   
 
The result is Algorithm A, where (i ? {1,2}) and 
o The Ci are the original candidate answers 
o The ak are learned parameters (k ? {1..13}) 
o Vi means the ith answer was validated 
o Pi was the rank of the validating answer to ques-
tion QTi 
o Ai was the score of the validating answer to QTi. 
Algorithm A. Answer re-ranking using con-
straints validation data. 
1. If C1 = nil and V2,    return C2 
2. If V1 and A1 > a1,     return C1 
3. If not V1 and not V2 and  
 type(T) ? MUSTCONSTRAIN,  
    return nil 
4. If  not V1 and not V2 and  
 type(T) ?SOFTREFUTATION, 
if S1 > a2,, return C1 else nil 
5. If not V2,    return C1 
6. If not V1 and V2 and  
A2 > a3 and P2 < a4 and  
S1-S2 < a5 and S2 > a6, return C2 
7. If V1 and V2 and  
(A2 - P2/a7) > (A1 - P1/a7) and  
A1 < a8 and P1 > a9 and  
A2 < a10 and P2 > a11 and  
S1-S2 < a12  and (S2 - P2/a7) > a13,  
    return C2 
8. else return C1 
 
4 Evaluation 
Due to the complexity of the learned algorithm, we 
decided to evaluate in stages.  We first performed an 
evaluation with a fixed question type, to verify that 
the purely arithmetic components of the algorithm 
were performing reasonably.  We then evaluated on 
the entire TREC12 factoid question set. 
4.1 Evaluation 1 
We created a fixed question set of 50 questions of 
the form ?What is the capital of X??, for each state 
in the U.S.  The inverted question ?What state is Z 
the capital of?? was correctly generated in each 
case.  We evaluated against two corpora: the 
AQUAINT corpus, of a little over a million news-
wire documents, and the CNS corpus, with about 
37,000 documents from the Center for Nonprolifera-
tion Studies in Monterey, CA.  We expected there to 
be answers to most questions in the former corpus, 
so we hoped there our method would be useful in 
converting 2nd place answers to first place.  The lat-
ter corpus is about WMDs, so we expected there to 
be holes in the state capital coverage2, for which nil 
identification would be useful.3   
                                                     
2 We manually determined that only 23 state capitals were at-
tested to in the CNS corpus, compared with all in AQUAINT. 
3 We added Tbilisi to the answer key for ?What is the capi-
tal of Georgia??, since there was nothing in the question to 
disambiguate Georgia.  
1077
The baseline is our regular search-based QA-System 
without the Constraint process.  In this baseline sys-
tem there was no special processing for nil ques-
tions, other than if the search (which always 
contained some required terms) returned no docu-
ments.  Our results are shown in Table 2. 
 
 AQUAINT 
baseline 
AQUAINT 
w/con-
straints 
CNS 
baseline 
CNS 
w/con-
straints 
Firsts 
(non-nil) 
39/50 43/50 7/23 4/23 
Total 
nils 
0/0 0/0 0/27 16/27 
Total 
firsts 
39/50 43/50 7/50 20/50 
%  
correct 
78 86 14 40 
Table 2.  Evaluation on AQUAINT and CNS 
corpora. 
 
On the AQUAINT corpus, four out of seven 2nd 
place finishers went to first place.  On the CNS cor-
pus 16 out of a possible 26 correct no-answer cases 
were discovered, at a cost of losing three previously 
correct answers.  The percentage correct score in-
creased by a relative 10.3% for AQUAINT and 
186% for CNS.  In both cases, the error rate was 
reduced by about a third. 
4.2 Evaluation 2 
For the second evaluation, we processed the 414 
factoid questions from TREC12.  Of special interest 
here are the questions initially in first and second 
places, and in addition any questions for which nils 
were found. 
 
As seen in Table 1, there were 32 questions which 
originally evaluated in rank 2.  Of these, four ques-
tions were not invertible because they had no terms 
that were annotated with any of our named-entity 
types, e.g. #2285 ?How much does it cost for gas-
tric bypass surgery?? 
 
Of the remaining 28 questions, 12 were promoted to 
first place.  In addition, two new nils were found.  
On the down side, four out of 108 previous first 
place answers were lost.  There was of course 
movement in the ranks two and beyond whenever 
nils were introduced in first place, but these do not 
affect the current TREC-QA factoid correctness 
measure, which is whether the top answer is correct 
or not.  These results are summarized in Table 3.  
 
While the overall percentage improvement was 
small, note that only second?place answers were 
candidates for re-ranking, and 43% of these were 
promoted to first place and hence judged correct.  
Only 3.7% of originally correct questions were 
casualties.  To the extent that these percentages are 
stable across other collections, as long as the size of 
the set of second-place answers is at least about 1/10 
of the set of first-place answers, this form of the 
Constraint process can be applied effectively. 
 
 Baseline Constraints 
Firsts (non-nil) 105 113 
nils 3 5 
Total firsts 108 118 
% correct 26.1 28.5 
 
Table 3.  Evaluation on TREC12 Factoids. 
5 Discussion  
The experiments reported here pointed out many 
areas of our system which previous failure analysis 
of the basic QA system had not pinpointed as being 
too problematic, but for which improvement should 
help the Constraints process.  In particular, this work 
brought to light a matter of major significance, term 
equivalence, which we had not previously focused 
on too much (and neither had the QA community as 
a whole).  We will discuss that in Section 5.4. 
 
Quantitatively, the results are very encouraging, but 
it must be said that the number of questions that we 
evaluated were rather small, as a result of the com-
putational expense of the approach. 
 
From Table 1, we conclude that the most mileage is 
to be achieved by our QA-System as a whole by ad-
dressing those questions which did not generate a 
correct answer in the first one or two positions.  We 
have performed previous analyses of our system?s 
failure modes, and have determined that the pas-
sages that are output from the SEARCH component 
contain the correct answer 70-75% of the time.  The 
ANSWER SELECTION module takes these passages 
and proposes a candidate answer list. Since the CON-
STRAINTS MODULE?s operation can be viewed as a 
re-ranking of the output of ANSWER SELECTION, it 
could in principle boost the system?s accuracy up to 
that 70-75% level.  However, this would either re-
quire a massive training set to establish all the pa-
rameters and weights required for all the possible re-
ranking decisions, or a new model of the answer-list 
distribution.    
5.1 Probability-based Scores 
Our ANSWER SELECTION component assigns scores 
to candidate answers on the basis of the number of 
terms and term-term syntactic relationships from the 
1078
original question found in the answer passage 
(where the candidate answer and wh-word(s) in the 
question are identified terms).  The resulting num-
bers are in the range 0-1, but are not true probabili-
ties (e.g. where answers with a score of 0.7 would be 
correct 70% of the time).  While the generated 
scores work well to rank candidates for a given 
question, inter-question comparisons are not gener-
ally meaningful.  This made the learning of a deci-
sion tree (Algorithm A) quite difficult, and we 
expect that when addressed, will give better per-
formance to the Constraints process (and maybe a 
simpler algorithm).  This in turn will make it more 
feasible to re-rank the top 10 (say) original answers, 
instead of the current 2. 
5.2 Better confidences 
Even if no changes to the ranking are produced by 
the Constraints process, then the mere act of valida-
tion (or not) of existing answers can be used to ad-
just confidence scores.  In TREC2002 (Voorhees, 
2003), there was an evaluation of responses accord-
ing to systems? confidences in their own answers, 
using the Average Precision (AP) metric.  This is an 
important consideration, since it is generally better 
for a system to say ?I don?t know? than to give a 
wrong answer.  On the TREC12 questions set, our 
AP score increased 2.1% with Constraints, using the 
algorithm we presented in (Chu-Carroll et al 2002).    
5.3 More complete NER 
Except in pure pattern-based approaches, e.g. (Brill, 
2002), answer types in QA systems typically corre-
spond to the types identifiable by their named-entity 
recognizer (NER). There is no agreed-upon number 
of classes for an NER system, even approximately.  
It turns out that for best coverage by our 
CONSTRAINTS MODULE, it is advantageous to have a 
relatively large number of types.  It was mentioned 
in Section 4.2 that certain questions were not invert-
ible because no terms in them were of a recogniz-
able type.  Even when questions did have typed 
terms, if the types were very high-level then creating 
a meaningful inverted question was problematic.  
For example, for QA without Constraints it is not 
necessary to know the type of ?MTV? in ?When 
was MTV started??, but if it is only known to be a 
Name then the inverted question ?What <Name> 
was started in 1980?? could be too general to be ef-
fective. 
5.4 Establishing Term Equivalence 
The somewhat surprising condition that emerged 
from this effort was the need for a much more com-
plete ability than had previously been recognized for 
the system to establish the equivalence of two terms.  
Redundancy has always played a large role in QA 
systems ? the more occurrences of a candidate an-
swer in retrieved passages the higher the answer?s 
score is made to be. Consequently, at the very least, 
a string-matching operation is needed for checking 
equivalence, but other techniques are used to vary-
ing degrees. 
 
It has long been known in IR that stemming or lem-
matization is required for successful term matching, 
and in NLP applications such as QA, resources such 
as WordNet (Miller, 1995) are employed for check-
ing synonym and hypernym relationships; Extended 
WordNet (Moldovan & Novischi, 2002) has been 
used to establish lexical chains between terms.  
However, the Constraints work reported here has 
highlighted the need for more extensive equivalence 
testing. 
 
In direct QA, when an ANSWER SELECTION module 
generates two (or more) equivalent correct answers 
to a question (e.g. ?Ferdinand Marcos? vs. ?Presi-
dent Marcos?; ?French? vs. ?France?), and fails to 
combine them, it is observed that as long as either 
one is in first place then the question is correct and 
might not attract more attention from developers.  It 
is only when neither is initially in first place, but 
combining the scores of correct candidates boosts 
one to first place that the failure to merge them is 
relevant.  However, in the context of our system, we 
are comparing the pivot term from the original ques-
tion to the answers to the inverted questions, and 
failure here will directly impact validation and hence 
the usefulness of the entire approach. 
 
As a consequence, we have identified the need for a 
component whose sole purpose is to establish the 
equivalence, or generally the kind of relationship, 
between two terms.  It is clear that the processing 
will be very type-dependent ? for example, if two 
populations are being compared, then a numerical 
difference of 5% (say) might not be considered a 
difference at all; for ?Where? questions, there are 
issues of granularity and physical proximity, and so 
on.  More examples of this problem were given in 
(Prager et al 2004a).  Moriceau (2006) reports a 
system that addresses part of this problem by trying 
to rationalize different but ?similar? answers to the 
user, but does not extend to a general-purpose 
equivalence identifier.   
6 Summary 
We have extended earlier Constraints-based work 
through the method of question inversion.  The ap-
proach uses our QA system recursively, by taking 
candidate answers and attempts to validate them 
through asking the inverted questions. The outcome 
1079
is a re-ranking of the candidate answers, with the 
possible insertion of nil (no answer in corpus) as the 
top answer.   
 
While we believe the approach is general, and can 
work on any question and arbitrary candidate lists, 
due to training limitations we focused on two re-
stricted evaluations.  In the first we used a fixed 
question type, and showed that the error rate was 
reduced by 36% and 30% on two very different cor-
pora.  In the second evaluation we focused on ques-
tions whose direct answers were correct in the 
second position.  43% of these questions were sub-
sequently judged correct, at a cost of only 3.7% of 
originally correct questions.  While in the future we 
would like to extend the Constraints process to the 
entire answer candidate list, we have shown that ap-
plying it only to the top two can be beneficial as 
long as the second-place answers are at least a tenth 
as numerous as first-place answers.  We also showed 
that the application of Constraints can improve the 
system?s confidence in its answers. 
 
We have identified several areas where improve-
ment to our system would make the Constraints 
process more effective, thus getting a double benefit.  
In particular we feel that much more attention 
should be paid to the problem of determining if two 
entities are the same (or ?close enough?). 
7 Acknowledgments 
This work was supported in part by the Disruptive 
Technology Office (DTO)?s Advanced Question 
Answering for Intelligence (AQUAINT) Program 
under contract number H98230-04-C-1577.   We 
would like to thank the anonymous reviewers 
for their helpful comments. 
References 
Brill, E., Dumais, S. and Banko M. ?An analysis of 
the AskMSR question-answering system.? In Pro-
ceedings of EMNLP 2002. 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC, 2003. 
Clarke, C., Cormack, G., Kisman, D. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum 
J.: Developing a Natural Language Interface to 
Complex Data. VLDB 1977: 292  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Miller, G. ?WordNet: A Lexical Database for Eng-
lish?, Communications of the ACM 38(11) pp. 
39-41, 1995. 
Moldovan, D. and Novischi, A, ?Lexical Chains for 
Question Answering?, COLING 2002. 
Moldovan, D. and Rus, V., ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Moriceau, V. ?Numerical Data Integration for Co-
operative Question-Answering?, in EACL Work-
shop on Knowledge and Reasoning for Language 
Processing (KRAQ?06), Trento, Italy, 2006. 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "Ques-
tion Answering using Constraint Satisfaction: 
QA-by-Dossier-with-Constraints", Proc. 42nd 
ACL, pp. 575-582, Barcelona, Spain, 2004(a). 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "A 
Multi-Strategy, Multi-Question Approach to 
Question Answering" in New Directions in Ques-
tion-Answering, Maybury, M. (Ed.), AAAI Press, 
2004(b). 
Prager, J., "A Curriculum-Based Approach to a QA 
Roadmap"' LREC 2002 Workshop on Question 
Answering: Strategy and Resources, Las Palmas, 
May 2002. 
Radev, D., Prager, J. and Samn, V. "Ranking Sus-
pected Answers to Natural Language Questions 
using Predictive Annotation", Proceedings of 
ANLP 2000, pp. 150-157, Seattle, WA. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, Gaithersburg, MD, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Witten, I.H. & Frank, E. Data Mining.  Practical 
Machine Learning Tools and Techniques.  El-
sevier Press, 2005. 
 
1080
Empirically Estimating Order Constraints for
Content Planning in Generation
Pablo A. Duboue and Kathleen R. McKeown
Computer Science Department
Columbia University
10027, New York, NY, USA
{pablo,kathy}@cs.columbia.edu
Abstract
In a language generation system, a
content planner embodies one or more
?plans? that are usually hand?crafted,
sometimes through manual analysis of
target text. In this paper, we present a
system that we developed to automati-
cally learn elements of a plan and the
ordering constraints among them. As
training data, we use semantically an-
notated transcripts of domain experts
performing the task our system is de-
signed to mimic. Given the large degree
of variation in the spoken language of
the transcripts, we developed a novel al-
gorithm to find parallels between tran-
scripts based on techniques used in
computational genomics. Our proposed
methodology was evaluated two?fold:
the learning and generalization capabil-
ities were quantitatively evaluated us-
ing cross validation obtaining a level of
accuracy of 89%. A qualitative evalua-
tion is also provided.
1 Introduction
In a language generation system, a content plan-
ner typically uses one or more ?plans? to rep-
resent the content to be included in the out-
put and the ordering between content elements.
Some researchers rely on generic planners (e.g.,
(Dale, 1988)) for this task, while others use plans
based on Rhetorical Structure Theory (RST) (e.g.,
(Bouayad-Aga et al, 2000; Moore and Paris,
1993; Hovy, 1993)) or schemas (e.g., (McKe-
own, 1985; McKeown et al, 1997)). In all cases,
constraints on application of rules (e.g., plan op-
erators), which determine content and order, are
usually hand-crafted, sometimes through manual
analysis of target text.
In this paper, we present a method for learn-
ing the basic patterns contained within a plan and
the ordering among them. As training data, we
use semantically tagged transcripts of domain ex-
perts performing the task our system is designed
to mimic, an oral briefing of patient status af-
ter undergoing coronary bypass surgery. Given
that our target output is spoken language, there is
some level of variability between individual tran-
scripts. It is difficult for a human to see patterns
in the data and thus supervised learning based on
hand-tagged training sets can not be applied. We
need a learning algorithm that can discover order-
ing patterns in apparently unordered input.
We based our unsupervised learning algorithm
on techniques used in computational genomics
(Durbin et al, 1998), where from large amounts
of seemingly unorganized genetic sequences, pat-
terns representing meaningful biological features
are discovered. In our application, a transcript is
the equivalent of a sequence and we are searching
for patterns that occur repeatedly across multiple
sequences. We can think of these patterns as the
basic elements of a plan, representing small clus-
ters of semantic units that are similar in size, for
example, to the nucleus-satellite pairs of RST.1
By learning ordering constraints over these ele-
1Note, however, that we do not learn or represent inten-
tion.
age, gender, pmh, pmh, pmh, pmh, med-preop,
med-preop, med-preop, drip-preop, med-preop,
ekg-preop, echo-preop, hct-preop, procedure,
. . .
Figure 2: The semantic sequence obtained from
the transcript shown in Figure 1.
ments, we produce a plan that can be expressed
as a constraint-satisfaction problem. In this pa-
per, we focus on learning the plan elements and
the ordering constraints between them. Our sys-
tem uses combinatorial pattern matching (Rigout-
sos and Floratos, 1998) combined with clustering
to learn plan elements. Subsequently, it applies
counting procedures to learn ordering constraints
among these elements.
Our system produced a set of 24 schemata
units, that we call ?plan elements?2 , and 29 order-
ing constraints between these basic plan elements,
which we compared to the elements contained in
the orginal hand-crafted plan that was constructed
based on hand-analysis of transcripts, input from
domain experts, and experimental evaluation of
the system (McKeown et al, 2000).
The remainder of this article is organized as
follows: first the data used in our experiments
is presented and its overall structure and acqui-
sition methodology are analyzed. In Section 3
our techniques are described, together with their
grounding in computational genomics. The quan-
titative and qualitative evaluation are discussed
in Section 4. Related work is presented in Sec-
tion 5. Conclusions and future work are discussed
in Section 6.
2 Our data
Our research is part of MAGIC (Dalal et al, 1996;
McKeown et al, 2000), a system that is designed
to produce a briefing of patient status after un-
dergoing a coronary bypass operation. Currently,
when a patient is brought to the intensive care
unit (ICU) after surgery, one of the residents who
was present in the operating room gives a brief-
ing to the ICU nurses and residents. Several of
these briefings were collected and annotated for
the aforementioned evaluation. The resident was
2These units can be loosely related to the concept of mes-
sages in (Reiter and Dale, 2000).
equipped with a wearable tape recorder to tape
the briefings, which were transcribed to provide
the base of our empirical data. The text was sub-
sequently annotated with semantic tags as shown
in Figure 1. The figure shows that each sentence
is split into several semantically tagged chunks.
The tag-set was developed with the assistance of
a domain expert in order to capture the different
information types that are important for commu-
nication and the tagging process was done by two
non-experts, after measuring acceptable agree-
ment levels with the domain expert (see (McK-
eown et al, 2000)). The tag-set totalled over 200
tags. These 200 tags were then mapped to 29 cat-
egories, which was also done by a domain expert.
These categories are the ones used for our current
research.
From these transcripts, we derive the sequences
of semantic tags for each transcript. These se-
quences constitute the input and working material
of our analysis, they are an average length of 33
tags per transcript (min = 13, max = 66, ? =
11.6). A tag-set distribution analysis showed that
some of the categories dominate the tag counts.
Furthermore, some tags occur fairly regularly to-
wards either the beginning (e.g., date-of-birth) or
the end (e.g., urine-output) of the transcript, while
others (e.g., intraop-problems) are spread more or
less evenly throughout.
Getting these transcripts is a highly expensive
task involving the cooperation and time of nurses
and physicians in the busy ICU. Our corpus con-
tains a total number of 24 transcripts. Therefore,
it is important that we develop techniques that can
detect patterns without requiring large amounts of
data.
3 Methods
During the preliminary analysis for this research,
we looked for techniques to deal with analysis of
regularities in sequences of finite items (semantic
tags, in this case). We were interested in devel-
oping techniques that could scale as well as work
with small amounts of highly varied sequences.
Computational biology is another branch of
computer science that has this problem as one
topic of study. We focused on motif detection
techniques as a way to reduce the complexity of
the overall setting of the problem. In biological
He is 58-year-old
age
male
gender
. History is significant for Hodgkin?s disease
pmh
, treated
with . . . to his neck, back and chest. Hyperspadias
pmh
, BPH
pmh
, hiatal hernia
pmh
and
proliferative lymph edema in his right arm
pmh
. No IV?s or blood pressure down in the left
arm. Medications ? Inderal
med-preop
, Lopid
med-preop
, Pepcid
med-preop
, nitroglycerine
drip-preop
and heparin
med-preop
. EKG has PAC?s
ekg-preop
.
His Echo showed AI, MR of 47 cine amps with hypokinetic basal and anterior apical region.
echo-preop
Hematocrit 1.2
hct-preop
, otherwise his labs are unremarkable. Went to OR for what was felt to be
2 vessel CABG off pump both mammaries
procedure
. . . . . .
Figure 1: An annotated transcription of an ICU briefing (after anonymising).
terms, a motif is a small subsequence, highly con-
served through evolution. From the computer sci-
ence standpoint, a motif is a fixed-order pattern,
simply because it is a subsequence. The problem
of detecting such motifs in large databases has
attracted considerable interest in the last decade
(see (Hudak and McClure, 1999) for a recent sur-
vey). Combinatorial pattern discovery, one tech-
nique developed for this problem, promised to
be a good fit for our task because it can be pa-
rameterized to operate successfully without large
amounts of data and it will be able to iden-
tify domain swapped motifs: for example, given
a?b?c in one sequence and c?b?a in another.
This difference is central to our current research,
given that order constraints are our main focus.
TEIRESIAS (Rigoutsos and Floratos, 1998) and
SPLASH (Califano, 1999) are good representa-
tives of this kind of algorithm. We used an adap-
tation of TEIRESIAS.
The algorithm can be sketched as follows: we
apply combinatorial pattern discovery (see Sec-
tion 3.1) to the semantic sequences. The obtained
patterns are refined through clustering (Section
3.2). Counting procedures are then used to es-
timate order constraints between those clusters
(Section 3.3).
3.1 Pattern detection
In this section, we provide a brief explanation of
our pattern discovery methodology. The explana-
tion builds on the definitions below:
?L,W ? pattern. Given that ? represents the se-
mantic tags alphabet, a pattern is a string of
the form ?(?|?)? ?, where ? represents a
don?t care (wildcard) position. The ?L,W ?
parameters are used to further control the
amount and placement of the don?t cares:
every subsequence of length W, at least L
positions must be filled (i.e., they are non-
wildcards characters). This definition entails
that L ? W and also that a ?L,W ? pattern
is also a ?L,W + 1? pattern, etc.
Support. The support of pattern p given a set of
sequences S is the number of sequences that
contain at least one match of p. It indicates
how useful a pattern is in a certain environ-
ment.
Offset list. The offset list records the matching
locations of a pattern p in a list of sequences.
They are sets of ordered pairs, where the first
position records the sequence number and
the second position records the offset in that
sequence where p matches (see Figure 3).
Specificity. We define a partial order relation on
the pattern space as follows: a pattern p is
said to be more specific than a pattern q
if: (1) p is equal to q in the defined posi-
tions of q but has fewer undefined (i.e., wild-
cards) positions; or (2) q is a substring of p.
Specificity provides a notion of complexity
of a pattern (more specific patterns are more
complex). See Figure 4 for an example.
Using the previous definitions, the algorithm re-
duces to the problem of, given a set of sequences,
L, W , a minimum windowsize, and a support
pattern: AB?D
0 1 2 3 4 5 6 7 8 . . . ? offset
seq?: A B C D F A A B F D . . .
seq? : F C A B D D F F . . . . . .
.
.
.
offset list: {(?, 0); (?, 6); (?, 2); . . .}
Figure 3: A pattern, a set of sequences and an
offset list.
ABC??DF
ABCA?DF ABC??DFG
HHHj

less specific than
Figure 4: The specificity relation among patterns.
threshold, finding maximal ?L,W ?-patterns with
at least a support of support threshold. Our im-
plementation can be sketched as follows:
Scanning. For a given window size n, all the pos-
sible subsequences (i.e., n-grams) occurring
in the training set are identified. This process
is repeated for different window sizes.
Generalizing. For each of the identified subse-
quences, patterns are created by replacing
valid positions (i.e., any place but the first
and last positions) with wildcards. Only
?L,W ? patterns with support greater than
support threshold are kept. Figure 5 shows
an example.
Filtering. The above process is repeated increas-
ing the window size until no patterns with
enough support are found. The list of iden-
tified patterns is then filtered according to
specificity: given two patterns in the list, one
of them more specific than the other, if both
have offset lists of equal size, the less spe-
cific one is pruned3 . This gives us the list
of maximal motifs (i.e. patterns) which are
supported by the training data.
3Since they match in exactly the same positions, we
prune the less specific one, as it adds no new information.
A B C D E F ? subsequence
AB?DEF ABCD?F ? patterns. . .
HHHj

Figure 5: The process of generalizing an existing
subsequence.
3.2 Clustering
After the detection of patterns is finished, the
number of patterns is relatively large. Moreover,
as they have fixed length, they tend to be pretty
similar. In fact, many tend to have their support
from the same subsequences in the corpus. We are
interested in syntactic similarity as well as simi-
larity in context.
A convenient solution was to further cluster the
patterns, according to an approximate matching
distance measure between patterns, defined in an
appendix at the end of the paper.
We use agglomerative clustering with the dis-
tance between clusters defined as the maximum
pairwise distance between elements of the two
clusters. Clustering stops when no inter-cluster
distance falls below a user-defined threshold.
Each of the resulting clusters has a single pat-
tern represented by the centroid of the cluster.
This concept is useful for visualization of the
cluster in qualitative evaluation.
3.3 Constraints inference
The last step of our algorithm measures the fre-
quencies of all possible order constraints among
pairs of clusters, retaining those that occur of-
ten enough to be considered important, accord-
ing to some relevancy measure. We also discard
any constraint that it is violated in any training
sequence. We do this in order to obtain clear-cut
constraints. Using the number of times a given
constraint is violated as a quality measure is a
straight-forward extension of our framework. The
algorithm proceeds as follows: we build a table
of counts that is updated every time a pair of pat-
terns belonging to particular clusters are matched.
To obtain clear-cut constraints, we do not count
overlapping occurrences of patterns.
From the table of counts we need some rele-
vancy measure, as the distribution of the tags is
skewed. We use a simple heuristic to estimate
a relevancy measure over the constraints that are
never contradicted. We are trying to obtain an es-
timate of
Pr (A ?precedes B)
from the counts of
c = A ??preceded B
We normalize with these counts (where x ranges
over all the patterns that match before/after A or
B):
c1 = A ??preceded x
and
c2 = x ??preceded B
The obtained estimates, e1 = c/c1 and e2 = c/c2,
will in general yield different numbers. We use
the arithmetic mean between both, e = (e1+e2)2 ,
as the final estimate for each constraint. It turns
out to be a good estimate, that predicts accuracy
of the generated constraints (see Section 4).
4 Results
We use cross validation to quantitatively evaluate
our results and a comparison against the plan of
our existing system for qualitative evaluation.
4.1 Quantitative evaluation
We evaluated two items: how effective the pat-
terns and constraints learned were in an unseen
test set and how accurate the predicted constraints
were. More precisely:
Pattern Confidence. This figure measures the
percentage of identified patterns that were
able to match a sequence in the test set.
Constraint Confidence. An ordering constraint
between two clusters can only be checkable
on a given sequence if at least one pattern
from each cluster is present. We measure
the percentage of the learned constraints that
are indeed checkable over the set of test se-
quences.
Constraint Accuracy. This is, from our perspec-
tive, the most important judgement. It mea-
sures the percentage of checkable ordering
Table 1: Evaluation results.
Test Result
pattern confidence 84.62%
constraint confidence 66.70%
constraint accuracy 89.45%
constraints that are correct, i.e., the order
constraint was maintained in any pair of
matching patterns from both clusters in all
the test-set sequences.
Using 3-fold cross-validation for computing these
metrics, we obtained the results shown in Ta-
ble 1 (averaged over 100 executions of the exper-
iment). The different parameter settings were de-
fined as follows: for the motif detection algorithm
?L,W ? = ?2, 3? and support threshold of 3. The
algorithm will normally find around 100 maximal
motifs. The clustering algorithm used a relative
distance threshold of 3.5 that translates to an ac-
tual treshold of 120 for an average inter-cluster
distance of 174. The number of produced clusters
was in the order of the 25 clusters or so. Finally, a
threshold in relevancy of 0.1 was used in the con-
straint learning procedure. Given the amount of
data available for these experiments all these pa-
rameters were hand-tunned.
4.2 Qualitative evaluation
The system was executed using all the available
information, with the same parametric settings
used in the quantitative evaluation, yielding a set
of 29 constraints, out of 23 generated clusters.
These constraints were analyzed by hand and
compared to the existing content-planner. We
found that most rules that were learned were val-
idated by our existing plan. Moreover, we gained
placement constraints for two pieces of semantic
information that are currently not represented in
the system?s plan. In addition, we found minor
order variation in relative placement of two differ-
ent pairs of semantic tags. This leads us to believe
that the fixed order on these particular tags can
be relaxed to attain greater degrees of variability
in the generated plans. The process of creation
of the existing content-planner was thorough, in-
formed by multiple domain experts over a three
year period. The fact that the obtained constraints
mostly occur in the existing plan is very encour-
aging.
5 Related work
As explained in (Hudak and McClure, 1999), mo-
tif detection is usually targeted with alignment
techniques (as in (Durbin et al, 1998)) or with
combinatorial pattern discovery techniques such
as the ones we used here. Combinatorial pattern
discovery is more appropriate for our task because
it allows for matching across patterns with permu-
tations, for representation of wild cards and for
use on smaller data sets.
Similar techniques are used in NLP. Align-
ments are widely used in MT, for example
(Melamed, 1997), but the crossing problem is a
phenomenon that occurs repeatedly and at many
levels in our task and thus, this is not a suitable
approach for us.
Pattern discovery techniques are often used for
information extraction (e.g., (Riloff, 1993; Fisher
et al, 1995)), but most work uses data that con-
tains patterns labelled with the semantic slot the
pattern fills. Given the difficulty for humans in
finding patterns systematically in our data, we
needed unsupervised techniques such as those de-
veloped in computational genomics.
Other stochastic approaches to NLG normally
focus on the problem of sentence generation,
including syntactic and lexical realization (e.g.,
(Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Knight and Hatzivassiloglou,
1995)). Concurrent work analyzing constraints on
ordering of sentences in summarization found that
a coherence constraint that ensures that blocks of
sentences on the same topic tend to occur together
(Barzilay et al, 2001). This results in a bottom-
up approach for ordering that opportunistically
groups sentences together based on content fea-
tures. In contrast, our work attempts to automati-
cally learn plans for generation based on semantic
types of the input clause, resulting in a top-down
planner for selecting and ordering content.
6 Conclusions
In this paper we presented a technique for extract-
ing order constraints among plan elements that
performs satisfactorily without the need of large
corpora. Using a conservative set of parameters,
we were able to reconstruct a good portion of a
carefully hand-crafted planner. Moreover, as dis-
cussed in the evaluation, there are several pieces
of information in the transcripts which are not
present in the current system. From our learned
results, we have inferred placement constraints of
the new information in relation to the previous
plan elements without further interviews with ex-
perts.
Furthermore, it seems we have captured order-
sensitive information in the patterns and free-
order information is kept in the don?t care model.
The patterns, and ordering constraints among
them, provide a backbone of relatively fixed struc-
ture, while don?t cares are interspersed among
them. This model, being probabilistic in nature,
means a great deal of variation, but our gener-
ated plans should have variability in the right po-
sitions. This is similar to findings of floating posi-
tioning of information, together with oportunistic
rendering of the data as used in STREAK (Robin
and McKeown, 1996).
6.1 Future work
We are planning to use these techniques to revise
our current content-planner and incorporate infor-
mation that is learned from the transcripts to in-
crease the possible variation in system output.
The final step in producing a full-fledged
content-planner is to add semantic constraints on
the selection of possible orderings. This can be
generated through clustering of semantic input to
the generator.
We also are interested in further evaluating the
technique in an unrestricted domain such as the
Wall Street Journal (WSJ) with shallow seman-
tics such as the WordNet top-category for each
NP-head. This kind of experiment may show
strengths and limitations of the algorithm in large
corpora.
7 Acknowledgments
This research is supported in part by NLM Con-
tract R01 LM06593-01 and the Columbia Uni-
versity Center for Advanced Technology in In-
formation Management (funded by the New York
State Science and Technology Foundation). The
authors would like to thank Regina Barzilay,
intraop-problems intraop-problems
?
?
?
operation 11.11%
drip 33.33%
intraop-problems 33.33%
total-meds-anesthetics 22.22%
?
?
?
drip
intraop-problems
?
?
?
operation 14.29%
drip 14.29%
intraop-problems 42.86%
total-meds-anesthetics 28.58%
?
?
?
drip drip
intraop-problems intraop-problems
?
?
?
operation 20.00%
drip 20.00%
intraop-problems 20.00%
total-meds-anesthetics 40.00%
?
?
?
drip drip
Figure 6: Cluster and patterns example. Each line corresponds to a different pattern. The elements
between braces are don?t care positions (three patterns conform this cluster: intraop-problems intraop-problems ? drip,
intraop-problems ? drip drip and intraop-problems intraop-problems drip drip the don?t care model shown in each brace must sum up to
1 but there is a strong overlap between patterns ?the main reason for clustering)
Noemie Elhadad and Smaranda Muresan for help-
ful suggestions and comments. The aid of two
anonymous reviewers was also highly appreci-
ated.
References
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In COLING, 2000, Saarbrcken, Germany.
Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2001. Sentence ordering in multidoc-
ument summarization. In HLT, 2001, San Diego,
CA.
Nadjet Bouayad-Aga, Richard Power, and Donia
Scott. 2000. Can text structure be incompatible
with rhetorical structure? In Proceedings of the
1st International Conference on Natural Language
Generation (INLG-2000), pages 194?200, Mitzpe
Ramon, Israel.
Andrea Califano. 1999. Splash: Structural pattern lo-
calization analysis by sequential histograms. Bioin-
formatics, 12, February.
Mukesh Dalal, Steven Feiner, , Kathleen McKeown,
ShiMei Pan, Michelle Zhou, Tobias Hollerer, James
Shaw, Yong Feng, and Jeanne Fromer. 1996. Nego-
tiation for automated generation of temporal multi-
media presentations. In Proceedings of ACM Mul-
timedia ?96, Philadelphia.
Robert Dale. 1988. Generating referring expressions
in a domain of objects and processes. Ph.D. thesis,
University of Edinburgh.
Richard Durbin, S. Eddy, A. Krogh, and G. Mitchi-
son. 1998. Biological sequence analysis. Cam-
bridge Univeristy Press.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1995. De-
scription of the umass system as used for muc-
6. In Morgan Kaufman, editor, Proceedings of the
Sixth Message Understanding Conference (MUC-
6), pages 127?140, San Francisco.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence. (Special Issue on Natural Language
Processing).
J. Hudak and Marcela McClure. 1999. A comparative
analysis of computational motif?detection methods.
In R.B. Altman, A. K. Dunker, L. Hunter, T. E.
Klein, and K. Lauderdale, editors, Pacific Sympo-
sium on Biocomputing, ?99, pages 138?149, New
Jersey. World Scientific.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics (ACL?95).
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proceedings
of the Ninth International Natural Language Gen-
eration Workshop (INLG?98).
Kathleen McKeown, ShiMei Pan, James Shaw, Jordan
Desmand, and Barry Allen. 1997. Language gen-
eration for multimedia healthcare briefings. In Pro-
ceedings of the 5th Conference on Applied Natural
Language Processing (ANLP?97), Washington, DC,
April.
Kathleen R. McKeown, Desmond Jordan, Steven
Feiner, James Shaw, Elizabeth Chen, Shabina Ah-
mad, Andre Kushniruk, and Vimla Patel. 2000. A
study of communication in the cardiac surgery in-
tensive care unit and its implications for automated
briefing. In AMIA ?2000.
Kathleen R. McKeown. 1985. Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press.
I. Dan Melamed. 1997. A portable algorithm for
mapping bitext correspondence. In 35th Confer-
ence of the Association for Computational Linguis-
tics (ACL?97), Madrid, Spain.
Johanna D. Moore and Ce?cile L. Paris. 1993. Plan-
ning text for advisory dialogues: Capturing inten-
tional and rhetorical information. Computational
Linguistics, 19(4):651?695.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Isidore Rigoutsos and Aris Floratos. 1998. Combina-
torial pattern discovery in biological sequences: the
teiresias algorithm. Bioinformatics, 14(1):55?67.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction. In AAAI Press
/ MIT Press, editor, Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
811?816.
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision?
based model for summary generation. Artificial In-
telligence, 85(1?2):135?179.
Appendix - Definition of the distance mea-
sure used for clustering.
An approximate matching measure is de-
fined for a given extended pattern. The ex-
tended pattern is represented as a sequence of
sets; defined positions have a singleton set,
while wildcard positions contain the non-zero
probability elements in their don?t care model
(e.g. given intraop-problems, intraop-problems, {drip 10%,intubation
90%}, drip we model this as [{intraop-problems}; {intraop-
problems}; {drip, intubation}; {drip}}]).
Consider p to be such a pattern, o an offset and
S a sequence, the approximate matching is de-
fined by
m?(p, o, S) =
?length(p)
i=0 match(p[i], S[i + o])
length(p)
where the match(P, e) function is defined as 0 if
e ? P , 1 otherwise, and where P is the set at
position i in the extended pattern p and e is the
element of the sequence S at position i + o.
Our measure is normalized to [0, 1]. Using
this function, we define the approximate match-
ing distance measure (one way) between a pattern
p1 and a pattern p2 as the sum (averaged over the
length of the offset list of p1) of all the approxi-
mate matching measures of p2 over the offset list
of p1. This is, again, a real number in [0, 1]. To
ensure symmetry, we define the distance between
p1 and p2 as the average between the one way dis-
tance between p1 and p2 and between p2 and p1.
Statistical Acquisition of Content Selection Rules
for Natural Language Generation
Pablo A. Duboue and Kathleen R. McKeown
Department of Computer Science
Columbia University
 
pablo,kathy  @cs.columbia.edu
Abstract
A Natural Language Generation system
produces text using as input semantic data.
One of its very first tasks is to decide
which pieces of information to convey in
the output. This task, called Content Se-
lection, is quite domain dependent, requir-
ing considerable re-engineering to trans-
port the system from one scenario to an-
other. In this paper, we present a method
to acquire content selection rules automat-
ically from a corpus of text and associated
semantics. Our proposed technique was
evaluated by comparing its output with in-
formation selected by human authors in
unseen texts, where we were able to fil-
ter half the input data set without loss of
recall.
1 Introduction
CONTENT SELECTION is the task of choosing the
right information to communicate in the output of a
Natural Language Generation (NLG) system, given
semantic input and a communicative goal. In gen-
eral, Content Selection is a highly domain dependent
task; new rules must be developed for each new do-
main, and typically this is done manually. Morevoer,
it has been argued (Sripada et al, 2001) that Content
Selection is the most important task from a user?s
standpoint (i.e., users may tolerate errors in wording,
as long as the information being sought is present in
the text).
Designing content selection rules manually is a
tedious task. A realistic knowledge base contains
a large amount of information that could potentially
be included in a text and a designer must examine
a sizable number of texts, produced in different sit-
uations, to determine the specific constraints for the
selection of each piece of information.
Our goal is to develop a system that can auto-
matically acquire constraints for the content selec-
tion task. Our algorithm uses the information we
learned from a corpus of desired outputs for the sys-
tem (i.e., human-produced text) aligned against re-
lated semantic data (i.e., the type of data the sys-
tem will use as input). It produces constraints on
every piece of the input where constraints dictate if
it should appear in the output at all and if so, under
what conditions. This process provides a filter on the
information to be included in a text, identifying all
information that is potentially relevant (previously
termed global focus (McKeown, 1985) or viewpoints
(Acker and Porter, 1994)). The resulting informa-
tion can be later either further filtered, ordered and
augmented by later stages in the generation pipeline
(e.g., see the spreading activation algorithm used in
ILEX (Cox et al, 1999)).
We focus on descriptive texts which realize a sin-
gle, purely informative, communicative goal, as op-
posed to cases where more knowledge about speaker
intentions are needed. In particular, we present ex-
periments on biographical descriptions, where the
planned system will generate short paragraph length
texts summarizing important facts about famous
people. The kind of text that we aim to generate is
shown in Figure 1. The rules that we aim to acquire
will specify the kind of information that is typically
included in any biography. In some cases, whether
Actor, born Thomas Connery on August 25, 1930, in Fountain-
bridge, Edinburgh, Scotland, the son of a truck driver and char-
woman. He has a brother, Neil, born in 1938. Connery dropped
out of school at age fifteen to join the British Navy. Connery is
best known for his portrayal of the suave, sophisticated British
spy, James Bond, in the 1960s. . . .
Figure 1: Sample Target Biography.
the information is included or not may be condi-
tioned on the particular values of known facts (e.g.,
the occupation of the person being described ?we
may need different content selection rules for artists
than politicians). To proceed with the experiments
described here, we acquired a set of semantic infor-
mation and related biographies from the Internet and
used this corpus to learn Content Selection rules.
Our main contribution is to analyze how varia-
tions in the data influence changes in the text. We
perform such analysis by splitting the semantic input
into clusters and then comparing the language mod-
els of the associated clusters induced in the text side
(given the alignment between semantics and text in
the corpus). By doing so, we gain insights on the rel-
ative importance of the different pieces of data and,
thus, find out which data to include in the generated
text.
The rest of this paper is divided as follows: in the
next section, we present the biographical domain we
are working with, together with the corpus we have
gathered to perform the described experiments. Sec-
tion 3 describes our algorithm in detail. The exper-
iments we perform to validate it, together with their
results, are discussed in Section 4. Section 5 sum-
marizes related work in the field. Our final remarks,
together with proposed future work conclude the pa-
per.
2 Domain: Biographical Descriptions
The research described here is done for the auto-
matic construction of the Content Selection mod-
ule of PROGENIE (Duboue and McKeown, 2003a),
a biography generator under construction. Biogra-
phy generation is an exciting field that has attracted
practitioners of NLG in the past (Kim et al, 2002;
Schiffman et al, 2001; Radev and McKeown, 1997;
Teich and Bateman, 1994). It has the advantages
of being a constrained domain amenable to current
generation approaches, while at the same time of-
fering more possibilities than many constrained do-
mains, given the variety of styles that biographies
exhibit, as well as the possibility for ultimately gen-
erating relatively long biographies.
We have gathered a resource of text and asso-
ciated knowledge in the biography domain. More
specifically, our resource is a collection of human-
produced texts together with the knowledge base
a generation system might use as input for gener-
ation. The knowledge base contains many pieces
of information related to the person the biography
talks about (and that the system will use to generate
that type of biography), not all of which necessarily
will appear in the biography. That is, the associated
knowledge base is not the semantics of the target text
but the larger set1 of all things that could possibly be
said about the person in question. The intersection
between the input knowledge base and the semantics
of the target text is what we are interested in captur-
ing by means of our statistical techniques.
To collect the semantic input, we crawled 1,100
HTML pages containing celebrity fact-sheets from
the E! Online website.2 The pages comprised infor-
mation in 14 categories for actors, directors, produc-
ers, screenwriters, etc. We then proceeded to trans-
form the information in the pages to a frame-based
knowledge representation. The final corpus con-
tains 50K frames, with 106K frame-attribute-value
triples, for the 1,100 people mentioned in each fact-
sheet. An example set of frames is shown in Fig-
ure 3.
The text part was mined from two different web-
sites, biography.com, containing typical biogra-
phies, with an average of 450 words each; and
imdb.com, the Internet movie database, 250-word
average length biographies. In each case, we ob-
tained the semantic input from one website and a
separate biography from a second website. We
linked the two resources using techniques from
record linkage in census statistical analysis (Fellegi
and Sunter, 1969). We based our record linkage on
the Last Name, First Name, and Year of Birth at-
tributes.
1The semantics of the text normally contain information not
present in our semantic input, although for the sake of Content
Selection is better to consider it as a ?smaller? set.
2http://www.eonline.com
inputs
texts texts
clusters
semantic
target
(2)
(3)
(4)
(5)
(A) (B) (C)
(1)
matched
baseline content selectionclass?based
rules rulesrules
semantic
MATCHING
counting and rule inductionrule?mixing
thresholding (RIPPER)logic
CLUSTERING STATISTICALSELECTOR
N?GRAM
DISTILLER
EXAMPLE
EXTRACTOR
Figure 2: Our proposed algorithm, see Section 3 for details.
3 Methods
Figure 2 illustrates our two-step approach. In the
first step (shaded region of the figure), we try to
identify and solve the easy cases for Content Selec-
tion. The easy cases in our task are pieces of data
that are copied verbatim from the input to the out-
put. In biography generation, this includes names,
dates of birth and the like. The details of this pro-
cess are discussed in Section 3.1. After these cases
have been addressed, the remaining semantic data is
clustered and the text corresponding to each cluster
post-processed to measure degrees of influence for
different semantic units, presented in Section 3.2.
Further techniques to improve the precision of the
algorithm are discussed in Section 3.3.
Central to our approach is the notion of data
paths in the semantic network (an example is shown
in Figure 3). Given a frame-based representation of
knowledge, we need to identify particular pieces of
knowledge inside the graph. We do so by selecting
a particular frame as the root of the graph (the
person whose biography we are generating, in our
case, doubly circled in the figure) and considering
the paths in the graph as identifiers for the different
pieces of data. We call these data paths. Each
path will identify a class of values, given the
fact that some attributes are list-valued (e.g., the
relative attribute in the figure). We use the notation
 
attribute  attribute  attribute 
to denote data paths.
3.1 Exact Matching
In the first stage (cf. Fig. 2(1)), the objective is to
identify pieces from the input that are copied ver-
batim to the output. These types of verbatim-copied
anchors are easy to identify and they allow us do two
things before further analyzing the input data: re-
move this data from the input as it has already been
selected for inclusion in the text and mark this piece
of text as a part of the input, not as actual text.
The rest of the semantic input is either verbal-
ized (e.g., by means of a verbalization rule of the
form
 
brother age 
	 ?young?) or not
included at all. This situation is much more chal-
lenging and requires the use of our proposed statis-
tical selection technique.
3.2 Statistical Selection
For each class in the semantic input that
was not ruled out in the previous step (e.g.,
 
brother age  ), we proceed to cluster
(cf. Fig. 2(2)) the possible values in the path,
over all people (e.g., ff2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641?645,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
On The Feasibility of Open Domain Referring Expression
Generation Using Large Scale Folksonomies
Fabia?n Pacheco Pablo Ariel Duboue?
Facultad de Matema?tica, Astronom??a y F??sica
Universidad Nacional de Co?rdoba
Co?rdoba, Argentina
Mart??n Ariel Dom??nguez
Abstract
Generating referring expressions has received
considerable attention in Natural Language
Generation. In recent years we start seeing
deployments of referring expression genera-
tors moving away from limited domains with
custom-made ontologies. In this work, we ex-
plore the feasibility of using large scale noisy
ontologies (folksonomies) for open domain
referring expression generation, an important
task for summarization by re-generation. Our
experiments on a fully annotated anaphora
resolution training set and a larger, volunteer-
submitted news corpus show that existing al-
gorithms are efficient enough to deal with
large scale ontologies but need to be extended
to deal with undefined values and some mea-
sure for information salience.
1 Introduction
Given an entity1 (the referent) and a set of com-
peting entities (the set of distractors), the task of
referring expression generation (REG) involves cre-
ating a mention to the referent so that, in the eyes
of the reader, it is clearly distinguishable from any
other entity in the set of distractors. In a traditional
generation pipeline, referring expression generation
happens at the sentence planning level. As a result,
its output is not a textual nugget but a description
employed later on by the surface realizer. In this pa-
per, we consider the output of the REG system to
?To whom correspondence should be addressed. Email:
pablo.duboue@gmail.com.
1Or set of entities, but not in this work.
be Definite Descriptions (DD) consisting of a set of
positive triples and a set of negative triples, enumer-
ating referent-related properties.
Since the seminal work by Dale and Re-
iter (1995), REG has received a lot of attention in the
Natural Language Generation (NLG) community.
However, most of the early work on REG has been
on traditional NLG systems, using custom-tailored
ontologies. In recent years (Belz et al, 2010) there
has been a shift towards what we term ?Open Do-
main Referring Expression Generation,? (OD REG),
that is, a REG task where the properties come from
a folksonomy, a large-scale volunteer-built ontology.
In particular, we are interested in changing
anaphoric references for entities appearing in sen-
tences drafted from different documents, as done
in multi-document summarization (Advaith et al,
2011). For example, consider the following sum-
mary excerpt2 as produced by Newsblaster (McKe-
own et al, 2002):
Thousands of cheering, flag-waving Palestinians gave
Palestinian Authority President Mahmoud Abbas an en-
thusiastic welcome in Ramallah on Sunday, as he told
them triumphantly that a ?Palestinian spring? had been
born following his speech to the United Nations last
week.3 The president pressed Israel, in unusually frank
terms, to reach a final peace agreement with the Pales-
tinians, citing the boundaries in place on the eve of the
June 1967 Arab-Israeli War as the starting point for ne-
2From http://newsblaster.cs.columbia.edu/archives/2011-10-
07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.html.
3After his stint at UN, Abbas is politically stronger than ever
(haaretz.com, 10/07/2011, 763 words).
641
gotiation about borders.4
Here the second sentence refers to U.S. presi-
dent Barack Obama and a referring expression of the
form ?U.S. president? should have been used. Such
expressions depend on the set of distractors present
in the text, a requirement that highlights the dynamic
nature of the problem. Our experiments extracted
thousands of complex cases (such as distinguishing
one musician from a set of five) which we used to
test existing algorithms against a folksonomy, dbPe-
dia5 (Bizer et al, 2009). This folksonomy contains
1.7M triples (for its English version) and has been
curated from Wikipedia.6
We performed two experiments: first we em-
ployed sets of distractors derived from a set of docu-
ments annotated with anaphora resolution informa-
tion (Hasler et al, 2006). We found that roughly
half of the entities annotated in the documents were
present in the folksonomy, which speaks of the feasi-
bility of using a folksonomy for OD REG, given the
fact that Wikipedia has strict notability requirements
for adding information. In the second experiment,
we obtained sets of distractors from Wikinews,7 a
service where volunteers submit news articles inter-
spersed with Wikipedia links. We leveraged said
links to assemble 40k referring expression tasks.
For algorithms, we employed Dale and Re-
iter (1995), Gardent (2002) and Full Brevity (FB)
(Bohnet, 2007). Our results show that the first two
algorithms produce results in a majority of the re-
ferring expression tasks, with the Dale and Reiter
algorithm being the most efficient and resilient of
the three. The results, however, are of mixed quality
and more research is needed to overcome two prob-
lems we have identified in our experiments: dealing
with undefined information in the folksonomy and
the need to incorporate a rough user model in the
form of information salience.
In the next section we briefly summarize the three
algorithms we employed in our experiments. In Sec-
tion 3, we describe the data employed. Section 4
contains the results of our experiments and subse-
quent analysis. We conclude discussing future work.
4Obama prods Mideast allies to embrace reform, make
peace (Washington Post, 10/07/2011, 371 words).
5http://dbpedia.org
6http://wikipedia.org
7http://wikinews.org
2 Referring Expression Generation (REG)
REG literature is vast and spans decades of work.
We picked three algorithms with the following
desiderata: all the algorithms can deal with single
entity referents (a significant amount of recent work
went into multi-entity referents) and we wanted to
showcase a classic algorithm (Dale and Reiter?s), an
algorithm generating negations (Gardent?s) and an
algorithm with a more exhaustive search of the solu-
tions space (Full Brevity). We very briefly describe
each of the algorithms in turn, where R is the refer-
ent, C is the set of distractors and P is a list of prop-
erties, triples in the form (entity, property, value),
describing R:
Dale and Reiter (1995). They assume the prop-
erties in P are ordered according to an established
criteria. Then the algorithm iterates over P , adding
each triple one at a time and removing from C all
entities ruled out by the new triple. Triples that do
not eliminate any new entities from C are ignored.
The algorithm terminates when C is empty.
Gardent (2002). The algorithm uses Constraint
Satisfaction Programming to solve two basic con-
straints: find a set of positive properties P+ and neg-
ative properties P?, such that all properties in P+
are true for the referent and all in P? are false, and
it is the smaller P+ ? P? such that for every c ? C
there exist a property in P+ that does not hold for c
or a property in P? that holds for c.8
Full Brevity (Bohnet, 2007). Starting from a
state E of the form (L,C, P ) with L = ? (selected
properties), it keeps these states into a queue, where
it loops until C = ?. In each loop it generates new
states (added to the end of the queue), as follows:
given a state E = (L,C, P ) for each p ? P , if p re-
moves elements rem from C, it adds (L? {p}, C ?
rem, P ? {p}), otherwise (L,C, P ? {p}).
3 Data
dbPedia. dbPedia (Bizer et al, 2009) is
an ontology curated from Wikipedia infoboxes,
small tables containing structured information at
the top of most Wikipedia pages. The ver-
sion employed in this paper (?Ontology Infobox
Properties?) contains 1,7520,158 triples. Each
8We employed the Choco CSP solver Java library:
http://www.emn.fr/z-info/choco-solver/.
642
Former [[New Mexico]] {{w|Governor of New
Mexico|governor}} {{w|Gary Johnson}} ended
his campaign for the {{w|Republican Party
(United States)|Republican Party}} (GOP)
presidential nomination to seek the backing
of the {{w|Libertarian Party (United
States)|Libertarian Party}} (LP).
Figure 1: Wikinews example, from http://en.wikinews.org
/wiki/U.S. presidential candidate Gary Johnson leaves GOP to vie for
the LP nom
entity is represented by a URI starting with
http://dbpedia.org/resource/ followed by
the name of its associated Wikipedia title. See the
next section for some example triples.
Pilot. While creating unambiguous descriptions
is the NLG task known as referring expression gen-
eration, its NLU counterpart is anaphora resolu-
tion. We took a hand-annotated corpus for training
anaphora resolution algorithms (Hasler et al, 2006)
consisting of 74 documents containing 239 corefer-
ence chains. Each of the chains is an entity that can
be used for our experiments, if the entity is in db-
Pedia and there are other suitable distractors in the
same document. We hand annotated each of those
239 coreference chains by type (person, organiza-
tion and location) and associated them to dbPedia
URIs for the ones we found on Wikipedia. We found
roughly half of the chains in dbPedia (106 out of
239, 44%). This percentage speaks of the coverage
of dbPedia for OD REG. However, only 16 docu-
ments contain multiple entities of the same type and
present in dbPedia, our pilot study criteria. These 16
documents result in the 16 tasks for our pilot. For a
large scale evaluation we turned to Wikinews.
Wikinews. Wikinews is a news service operated
as a wiki. As the news articles are interspersed
with interwiki links, multiple entities can be disam-
biguated as Wikipedia pages (which in turn are db-
Pedia URIs). For example, in Figure 1, both the Lib-
ertarian Party and Republican Party can be consid-
ered potential distractors, as both are organizations.
The Wikimedia Foundation makes a database
dump available for all Wikinews interwiki links (the
links in braces in the above example). If a page con-
tains more than one organization or person, we ex-
tracted the whole set of people (or organizations) as
a referring expression task. To see whether a URI
is a person or an organization we check for a birth
date or creation date, respectively. In this manner,
we obtained 4,230 tasks for people and 12,998 for
organizations. This is dataset is freely available.9
4 Results
Pilot. The 16 tasks were split into 40 runs (a task
spans n runs each, where n is the number of entities
in the task, by rotating through the different alterna-
tive pairs of referent / set of distractors). From these
tasks, Dale and Reiter produced no output 12 times
and FB Brevity was unable to produce a result in 23
times. Gardent produced output for every run. We
consider this an example of the increased expressive
power of negative descriptions (it included a nega-
tion in 25% of the runs). For the other two algo-
rithms, the lack of an unique triple differentiating
one entity from the set of distractors seemed to be
the main issue but there were multiple cases were FB
ran out of memory for its queue of candidate nodes.
With respect to execution timings, Dale and Re-
iter ran into some corner cases and took time com-
parable to Gardent?s algorithm. FB was 16 times
slower (we found this counter-intuitive, as Gardent?s
algorithm is more demanding). Therefore, two of
these algorithms were able to produce results using
large scale ontological information. As FB ran into
problems both in terms of execution time and failure
rates, we omitted it from the large scale experiments.
We adjusted the parameters for the algorithms on
this set to obtain the best possible quality output
given the data and the problem. As such, we do not
report quality assessments on the pilot data.
Wikinews. The tasks obtained from wikinews
contained a large number of entities per task (an av-
erage of 12 people per task) and therefore span a
large number of runs: 17,814 runs for people (from
4,230 tasks) and 44,080 for organizations (from
12,998 tasks).
On these large runs, execution time differences
are in line with our a priori expectations: the greedy
approach of Dale and Reiter is very fast10 with Gar-
dent?s more comprehensive search taking about 40
times more time. Dale and Reiter failure rate was
9
http://www.cs.famaf.unc.edu.ar/?pduboue/data/ also mirrored
at http://duboue.ca/data.
10Dale and Reiter takes less than 3? for the 44,080 runs for
organizations in a 2.3 GHz machine.
643
Referent Dale and Reiter Output Gardent Output
EB { (EB occupation Software Freedom Law Center) } { (EB occupation Software Freedom Law Center) }
LL { (LL birthPlace United States), (LL, occupation Harvard Law School) } { (LL birthPlace Rapid City, South Dakota) }
LT { (LT occupation Software engineer) } { (LT nationality Finnish American) }
Figure 2: Example output for the task: {?Eben Moglen? (EB), ?Lawrence Lessig? (LL), ?Linus Torvalds? (LT) }.
comparable or better than in the pilot (for organiza-
tions that are more mixed, it was slightly lower but
for people it was as low 2.8%). Gardent missed 2%
of the people (and only 54 organizations), employ-
ing negatives 14% of the time for people and 12% of
the time for organizations.
Evaluating referring expressions is hard. Efforts
to automate this task in NLG (Gatt et al, 2007)
have taken an approach similar to machine transla-
tion BLEU scores (Papinini et al, 2001), for exam-
ple, by asking multiple judges to produce referring
expressions for a given scenario. These settings usu-
ally involve images of physical objects and relate to
small ontologies. While such an approach could be
adapted to the Open Domain case, a major problem
is the need for the judges to be acquainted with some
of the less popular entities in the training set. At
this point in our research, we decided to analyze the
quality of a sample of the output ourselves. This
process involved consulting information about each
entity to determine the soundness of the result.
We looked at a random sample of 20 runs and an-
notated it by two authors, measuring a Cohen?s ? of
60% for annotating DD results and 79% for deter-
mining whether the folksonomy had enough infor-
mation to build a satisfactory DD. We then extended
the evaluation to 60 runs and annotated them by one
author. We found that Dale and Reiter produced a
satisfactory DD in 41.6% of the cases and Gardent
in 43.4% of the cases and that the folksonomy con-
tained enough information 81.6% of the time. Fig-
ure 2 shows some example output.
From the evaluation we learned that the default
ordering strategy employed by Dale and Reiter is
not stable across different types of people (compare:
politicians vs. musicians) or organizations. We also
saw that Gardent?s algorithm in many cases selected
a single triple with very little practical value (an ob-
scure fact about the entity) or a negative piece of in-
formation which is actually true for the referent but
it is a missing piece of information.
The first two problems can be solved by either fur-
ther subdividing the taxonomies of entities or (more
interestingly) by incorporating some measure about
the salience of each piece of information, a possibil-
ity which we will discuss next. The last issue can be
addressed by having some form of meaningful de-
fault value.
The negations produced by Gardent?s algorithm
highlighted errors on the folksonomy. For example,
when referring to China with distractors Peru and
Taiwan, it will produce ?the place where they do not
speak Chinese,? as China has the different Chinese
dialects spelled out on the folksonomy (and some
Peruvians do speak Chinese). Given these limita-
tions, we find the current results very encouraging
and we believe folksonomies can help focus on ro-
bust NLG for noisy (ontological) inputs.
5 Discussion
We have shown that by using a folksonomy it should
be possible to deploy traditional NLG referring ex-
pression generation algorithms in Open Domain
tasks. To fulfill this vision, three tasks remain:
Dealing with missing information. Some form of
smart default values are needed, we are considering
using a nearest-neighbor approach to find ontologi-
cal siblings which can provide such defaults.
Estimating salience of each piece of ontological
information. The importance for each triple has to
be obtained in a way consistent with the Open Do-
main nature of the task. For this problem, we believe
search engine salience can be of great help.
Transform the extracted triples into actual text.
This problem has received attention in the past. We
would like to explore traditional surface realizer
with a custom-made grammar.
Acknowledgments
We would like to thank the anonymous reviewers as
well as Annie Ying and Victoria Reggiardo.
644
References
Siddharthan Advaith, Nenkova Ani, and McKeown Kath-
leen. 2011. Information status distinctions and refer-
ring expressions: An empirical study of references to
people in news summaries. Computational Linguis-
tics, 37(4):811?842.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Generating referring expressions in context:
The grec task evaluation challenges. In Emiel Krah-
mer and Marit Theune, editors, Empirical Methods
in Natural Language Generation, volume 5790 of
Lecture Notes in Computer Science, pages 294?327.
Springer.
C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,
R. Cyganiak, and S. Hellmann. 2009. DBpedia-a
crystallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154?165.
B. Bohnet. 2007. is-fbn, is-fbs, is-iac: The adaptation
of two classic algorithms for the generation of refer-
ring expressions in order to produce expressions like
humans do. MT Summit XI, UCNLG+ MT, pages 84?
86.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233?263.
C. Gardent. 2002. Generating minimal definite descrip-
tions. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages 96?
103. Association for Computational Linguistics.
A. Gatt, I. Van Der Sluis, and K. Van Deemter. 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. In Proceedings
of the Eleventh European Workshop on Natural Lan-
guage Generation, pages 49?56. Association for Com-
putational Linguistics.
L. Hasler, C. Orasan, and K. Naumann. 2006. NPs
for events: Experiments in coreference annotation. In
Proceedings of the 5th edition of the International
Conference on Language Resources and Evaluation
(LREC2006), pages 1167?1172.
Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with columbia?s
newsblaster. In Proc. of HLT.
Kishore Papinini, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical report, IBM.
645
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 85?89,
Utica, May 2012. c?2012 Association for Computational Linguistics
Extractive email thread summarization:
Can we do better than He Said She Said?
Pablo Ariel Duboue
Les Laboratoires Foulab
999 du College
Montreal, Quebe?c
pablo.duboue@gmail.com
Abstract
Human-written, good quality extractive sum-
maries pay great attention to the text intermix-
ing the extracts. In this work, we focused on
the lexical choice for verbs introducing quoted
text. We analyzed 4000+ high quality sum-
maries for a high traffic mailing list and manu-
ally assembled 39 quotation-introducing verb
classes that cover the majority of the verb oc-
currences. A significant amount of the data is
covered by on-going work on e-mail ?speech
acts.? However, we found that one third of the
?tail? is composed by ?risky? verbs that most
likely will be beyond the state of the art for
longer time. We used this fact to highlight the
trade-offs of risk taking in NLG, where inter-
esting prose might come at the cost of unset-
tling some of the readers.
1 Introduction
High traffic mailing lists pose a challenge to an ex-
tended audience laterally interested on the subject
matter but unable or unwilling to follow them on
everyday minutiae. In this context, high-level sum-
maries are of great help and in certain cases there are
people or companies that step into the plate to pro-
vide such service. In recent years, there has been
an ever increasing interest (Muresan et al, 2001;
Nenkova and Bagga, 2003; Newman and Blitzer,
2003; Rambow et al, 2004; Wan and McKeown,
2004; McKeown et al, 2007; Ulrich, 2008; Wang et
al., 2009) in automating this task, with many works
focusing on selectively extracting quotes from key
e-mail exchanges.
In this work, we focus on finding appropriate and
varied ways to cite selected quotes from the email
threads. A seemingly simple task, this problem
touches: speech act detection (Searle, 1975) (ques-
tion vs. announcement vs. reply), opinion mining
(Pang and Lee, 2008) (complained vs. thanked) and
citation polarity analysis (Teufel, 1999): (agreed vs.
disagreed vs. added).
At this stage, we will show training data we have
acquired for the task and a set of manually assem-
bled verb clusters that show the richness of the prob-
lem. Moreover, we have used these clusters to high-
light a trade-off of ?risk taking? in NLG, where gen-
erating interesting prose might lead to text that can
upset some readers in the presence of errors.
This paper is structured as follows: in the next
section we discuss the data from where we obtained
the raw verbs and then proceed to describe the man-
ual analysis to cluster and identify ?risky? verbs. We
then present the whole set of clusters and conclude
with a discussion of risk taking in NLG.
2 Data
This work is part of a larger effort to build automatic
tools to replace a key resource that the Linux Ker-
nel development community enjoyed for five years:
the Kernel Traffic summaries of the activities in the
Linux Kernel mailing list (LKML).
The LKML is of extremely high traffic (300 mails
a day on average). For five years (since 1999), Jack
Brown hand-picked the most newsworthy threads
in a week time and published a summary for each
thread. The summaries were made available (under
a Free Software license) in a rich XML-based format
85
<p>Gregory Maxwell r e p l i e d , <quo t e who=?George Maxwell ?>Do you
see t h e ? ( s i c ) ? Tha t u s u a l l y s t a n d s f o r ? S p e l l i n g i s Co r r e c t ? .
</ quo t e></ p>
<p>Ol i v e r Xymoron r e j o i n e d :</ p>
<quo t e who=? O l i v e r Xymoron?>
<p>I t h i n k what we have he r e i s an i r o n i c doub l e typo . The
message i s a c t u a l l y i n d i c a t i n g t h e d r i v e i s no t f e e l i n g ve ry
good :</ p>
<p>+ { 0xb900 , ? P l ay o p e r a t i o n a b o r t e d ( s i c k ) ? } ,</ p>
<p>Hope fu l l y t h i s ve ry impo r t a n t change w i l l make i t i n t o
2 . 2 . 2 .</ p>
</ quo t e>
<p>Brendan Cu l l y k a f l o o g i t a t e d :</ p>
<quo t e who=? Brendan Cu l l y ?>
<p>? s i c ? doesn ? t s t a n d f o r ? s p e l l i n g i s c o r r e c t ? , o r even
? s t a t e d i n c o n t e x t ? ( yech ! ) . < / p>
<p>In f a c t , i t s t a n d s f o r ? yes , I know i t l o ok s funny , bu t
t h a t ? s how I want i t ? . But p eop l e go t t i r e d o f t y p i n g
Y, IKILF , BTHIWI so t h ey a b b r e v i a t e d i t t o SIC .</p>
Figure 1: Kernel Traffic #6, Feb. 18th 1999 (excerpt).
(Figure 1) that included, among many other things,
explicit marking of all quoted text, with attribution.
These summaries were in general followed by a
much larger audience than the mailing list itself due
to a number of factors including the fact that they
make for quite an entertaining read. Mr. Brown?s
prose was high quality and quite consistent in style,1
which highlights its potential as training material
for NLG. As Reiter and Sripada (2002) pointed out,
learning tasks in NLG profit from training data of the
highest possible quality in terms of prose and con-
sistency (as compared with training data for NLU,
where robustness comes from exposing the system
to a variety of malformed texts).
In our journey to approximate Mr. Brown?s work
by automatic means, we decided to start on a rel-
atively unstudied problem: introducing quoted ref-
erences in a rich manner. In the 4,253 hand writ-
ten summaries by Mr. Brown (made available in 344
newsletter issues) 95% contain a quote, with an av-
erage of 3.28 quotes per summary. Moreover, 72%
of the total characters in the summaries are inside
quotes (including markup).
2.1 Processing
We employed a processing pipeline implemented in
the UIMA framework (Ferrucci and Lally, 2004)
to extract the verbs immediately before a quota-
tion. We used annotators from the OpenNLP project
(Apache, 2011) implementing Maximum Entropy
models for NLP (Ratnaparkhi, 1998). For the sen-
tence before a quotation we extracted the word
1A quality of prose that continues with his editorial contri-
butions to Linux Journal and Linux Magazine.
marked with the POS tag ?VBD? closer to the quota-
tion. Processing the 334 issues available for Kernel
Traffic resulted in 11,634 verb occurrences extracted
for 344 verbs (and verb-like errors). These verbs are
the ones we employ for the analysis and inferences
drawn in the next section.
3 Analysis
From the grand-total of 344 verbs (including ty-
pos and POS-tagger errors), we took all the verbs
that appeared at least a hundred times (the top 55
verbs) and expanded them from the larger list (plus
WordNet synsets (Miller, 1995)), grouping them
into classes. The grouping captures synonyms for
the particular task of introducing quoted text in sum-
maries. The resulting 39 classes (Table 1) contain
127 verbs accounting for 96% of the cases (the ta-
ble contains an ?other? class with the remaining 217
verbs that account for 4% of the occurrences). The
verbs included from WordNet do not appear in the
corpus and thus have a count of zero. This large set
of verbs highlights the many possibilities a system
that chooses to go just with ?s/he said? will be miss-
ing. Moreover, such a system can be immediately
enriched with 17 different variations with associated
likelihoods.
We determined whether or not generation errors
for a given verb class would be ?dangerous? using
the following criteria:
If the automatic determination of whether
the original quote fell into a particular
verb class fails, would the original author
take issue with the summary upon reading
the misclassified verb?
That is, if the system decides that Brendan Cully
(from the example in the introduction) has indeed
kafloogitated2 with his reply but such decision was
made in error (and Mr. Cully was just remarking
or explaining), would Mr. Cully take issue with the
summary? As with any automated system, the pos-
sibility of automated mistakes should make its de-
signers err on the side of making more conservative
decisions. Under such desiderata, we think the 10
2That word has been invented by Mr. Brown and was used
only once within the five years of Kernel Traffic.
86
classes highlighted in Table 1 are thus too ?danger-
ous? to be addressed currently by automated means.
Initially, that might not appear such a big loss,
as none of them account for more than 1% of the
total occurrences. However, as with many other
phenomena in NLP, a few cases account for most
occurrences: the clusters for ?said,? ?asked,? and
?replied? account for 2/3 of the total occurrences
and, overall, the top 9 classes account for 93%
of the cases. From the rich tail that encompasses
Mr. Brown prose, the ?dangerous? classes account
for 35% of the cases from position 10 and onward.
It is our opinion that such cases were the reason
Mr. Brown?s summaries were enjoyable to read and
are only a small example of the humor and piquancy
behind his prose. Now, it might be the case such
quality will be beyond the state of the art of NLG
for quite some time.
In that sense, we consider the prevalence of risky
classes as a negative result that highlights a prob-
lem for NLG well beyond the task at hand: we, as
humans, enjoy text that takes a stand, that argues
its points in an opinionated manner.3 Such is the
distinction between dull reports and flourish sum-
maries. Even in the highly technical domain of oper-
ating system kernel discussions, Mr. Brown felt the
need to use words such as ?groused? and ?chastised.?
The problem might as well be cultural, with opin-
ionated prose paradigmatic to the Western world. It
might also be related to our culture as NLG prac-
titioners, where we always thrive for perfect output.
Our data shows that to go beyond ?He Said She Said?
in a truly interesting manner we will have to be ready
to make mistakes which could make some people
unhappy, a trade-off that it would be interesting to
see explored more often in NLG.
4 Related Work
Since the seminal work by Muresan et al (2001),
email summarization and in particular email thread
summarization has spanned full dissertations (Ul-
rich, 2008). Existing resources for email summa-
rization (Ulrich et al, 2008), however, do not em-
phasize explicitly the type of quotes being used.
Understandingly, most of the work has been de-
voted to selecting the particular words, sentences or
3Not unlike this discussion.
paragraphs to extract from the original e-mails. ei-
ther by distilling terms or topics (Muresan et al,
2001; Newman and Blitzer, 2003) or finding a repre-
sentative example (Nenkova and Bagga, 2003; Ram-
bow et al, 2004; Wang et al, 2009).
The issue of choosing how to introduce the ex-
tracted text has only been studied in the context of
speech act detection (Cohen et al, 2004; Wan and
McKeown, 2004) within emails or within threaded
discussions (Feng et al, 2006), which is limited to
questions, replies and the like (a very important case
which covers 2/3 of our available data). The prob-
lem of detecting question / answer pairs in e-mails
is by far the one who has received the most attention
in the field (Bickel and Scheffer, 2004; Shrestha and
McKeown, 2004; McKeown et al, 2007).
The verbs in each of the classes in Table 1 have
a near-synonym relation:4 even though ?recom-
mended? and ?urged? share most of their meaning,
the differences in style, color and subtle meaning
need to be further elucidated for successful lexical
choice. This topic has started to be explored in de-
tail recently (Edmonds and Hirst, 2002).
Our work falls in the larger field of summarization
by using NLG means, a discipline that has received
significant attention of late (Belz et al, 2009).
5 Conclusion
In this paper, we have brought to the attention of
NLG practitioners the rich resource embodied in five
years of Kernel Traffic newsletters. We had also
highlighted the richness of the problem of lexical
choice for verbs introducing quotations in extractive
email summarization.
Moreover, we contributed 39 clusters manually
assembled from naturally occurring verbs extracted
from 4000+ high quality summaries. These clus-
ters can enrich even the most straightforward exist-
ing systems. Finally, we argued that, while useful
summaries might be around the corner, entertaining
summaries will be well beyond the state of the art
until the field is willing to take the risk involved in
standing behind automatically generated prose with
intrinsic value-judgments.
In our ongoing work, we are targeting the creation
4Thanks to an anonymous reviewer for bringing this fact into
our attention.
87
Table 1: Quotation introducing verb classes, with counts. The ?other? class appears in row 7. Lines in bold are
considered ?dangerous.? The last column is the author?s opinion about which type of technology is more relevant for
choosing that class (speech act detection (A), opinion mining (O) or citation link analysis (C)). Verbs missing due to
space restrictions are in the appendix.
# Top Verbs # verbs Total Counts Accum. Type
1 said (2726) remarked (361) posted (163) pointed out (148) 17 3531 (30.35%) 30.35% A
2 replied (3476) responded (21) answered (11) 3 3508 (30.15%) 60.50% A
3 added (1059) included (13) followed (10) 3 1082 (9.30%) 69.80% C
4 announced (902) declared (1) 2 903 (7.76%) 77.56% A
5 asked (509) inquired (0) 2 509 (4.37%) 81.94% A
6 explained (427) 1 427 (3.67%) 85.61% A
7 FELT (21) MADE (21) WANTED (8) BROKE (8) 217 403 (3.46%) 89.07% -
8 reported (254) detailed (1) 2 255 (2.19%) 91.26% A
9 suggested (188) proposed (35) 2 223 (1.91%) 93.18% O
10 objected (90) protested (5) 2 95 (0.81%) 94.00% O
11 concluded (48) ended (5) finished (4) closed (2) 5 59 (0.50%) 94.50% C
12 offered (52) volunteered (6) 2 58 (0.49%) 95.00% O
13 confirmed (44) supported (4) affirmed (3) reasserted (1) 7 52 (0.44%) 95.45% C
14 summed up (21) summarized (18) 2 39 (0.33%) 95.78% A
15 agreed (37) concurred (1) concorded (0) 3 38 (0.32%) 96.11% C
16 described (33) 1 33 (0.28%) 96.39% A
17 took issue (17) disagreed (11) dissented (2) differed (1) 4 31 (0.26%) 96.66% O
18 complained (22) sounded off (2) kicked (1) groused (1) 7 29 (0.24%) 96.91% O
19 argued (28) contended (0) debated (0) 3 28 (0.24%) 97.15% O
20 listed (27) enumerated (0) 2 27 (0.23%) 97.38% A
21 continued (25) kept (1) 2 26 (0.22%) 97.61% A
22 clarified (25) elucidated (0) 2 25 (0.21%) 97.82% C
23 recommended (17) urged (4) advised (2) advocated (1) 4 24 (0.20%) 98.03% C
24 speculated (16) mused (2) guessed (2) supposed (1) 6 22 (0.18%) 98.22% O
25 elaborated (11) expanded (7) expounded (2) 3 20 (0.17%) 98.39% C
26 corrected (18) chastised (1) rectified (0) righted (0) 4 19 (0.16%) 98.55% O
27 exclaimed (6) called out (5) cried out (4) shouted (2) 5 18 (0.15%) 98.71% O
28 quoted (15) cited (2) 2 17 (0.14%) 98.85% C
29 warned (8) cautioned (6) admonished (2) 3 16 (0.13%) 98.99% O
30 interjected (11) sprung (1) interposed (1) 3 13 (0.11%) 99.10% O
31 quipped (10) joked (1) chuckled (1) cracked (1) 4 13 (0.11%) 99.21% O
32 requested (12) 1 12 (0.10%) 99.32% A
33 tried (9) attempted (2) tested (1) 3 12 (0.10%) 99.42% O
34 acknowledged (8) admitted (3) recognized (0) 3 11 (0.09%) 99.51% A
35 countered (10) 1 10 (0.08%) 99.60% C
36 found (7) discovered (2) launched (1) 3 10 (0.08%) 99.69% A
37 reiterated (9) repeated (1) 2 10 (0.08%) 99.77% C
38 started (9) began (1) 2 10 (0.08%) 99.86% A
39 rejoined (6) retorted (2) returned (1) 3 9 (0.07%) 99.93% O
40 chimed (7) 1 7 (0.06%) 100% O
88
of a systemic fragment for the quotation-introducing
verbs, in the style of KPML (Bateman, 1995).
Acknowledgments
The author would like to thank the anonymous re-
viewers as well as Annie Ying for valuable feed-
back and insights. He will also like to thank the
Debian NYC group for bringing the Kernel Traffic
summaries to his attention.
Appendix
The verbs omitted for reasons of space in Table 1 are the fol-
lowing: for the ?said? cluster, mentioned (34), commented (25),
wrote (20), noticed (17), spoke (9), expressed (6), showed (5),
observed (5), stated (5), asserted (4), referred (1), noted (1),
declared (1); for the ?concluded? cluster, resolved (0); for the
?confirmed? cluster, corroborated (0), sustained (0), substanti-
ated (0); for the ?complained? cluster, hollered (1), ranted (1),
kvetched (1); for the ?speculated? cluster, theorized (1), conjec-
tured (0); for the ?exclaimed? cluster, sputtered (1).
References
Apache. 2011. OpenNLP
http : //incubator.apache.org/opennlp.
John A. Bateman. 1995. KPML: The KOMET?Penman
multilingual linguistic resource development environ-
ment. In Proc. of EWNLG, pages 219?222.
Anja Belz, Roger Evans, and Sebastian Varges, editors.
2009. Proc. of the 2009 Workshop on Language
Generation and Summarisation (UCNLG+Sum 2009).
ACL, Suntec, Singapore, August.
Steffen Bickel and Tobias Scheffer. 2004. Learning
from message pairs for automatic email answering. In
ECML, volume 3201 of LNCS, pages 87?98. Springer.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proc. of EMNLP, volume 4.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, 28(2):105?144.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. Learning to detect conversation focus of
threaded discussions. In Proc. HLT-NAACL, pages
208?215.
David Ferrucci and Adam Lally. 2004. UIMA: an ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3-4):327?348.
Kathleen McKeown, Lokesh Shrestha, and Owen Ram-
bow. 2007. Using question-answer pairs in extractive
summarization of email conversations. In CICLing,
volume 4394 of LNCS, pages 542?550. Springer.
G.A. Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
Smaranda Muresan, Evelyn Tzoukermann, and Judith L.
Klavans. 2001. Combining linguistic and machine
learning techniques for email summarization. In Proc.
of the 2001 workshop on Computational Natural Lan-
guage Learning-Volume 7, page 19. ACL.
Ani Nenkova and Amit Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
RANLP, volume 260 of Current Issues in Linguistic
Theory (CILT), pages 287?296.
Paula S. Newman and John C. Blitzer. 2003. Summariz-
ing archived discussions: a beginning. In IUI, pages
273?276. ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Owen Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004: Short Papers, pages 105?108. ACL.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Ehud Reiter and S. Sripada. 2002. Should corpora texts
be gold standards for NLG? In Proceedings of Second
International Conference on Natural Language Gen-
eration INLG-2002, pages 97?104, Arden House, NY.
John R. Searle. 1975. A taxonomy of illocutionary acts.
In Language, Mind and Knowledge, pages 344?369.
University of Minnesota Press.
Lokesh Shrestha and Kathleen McKeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proc. of ACL, page 889. ACL.
Simone Teufel. 1999. Argumentative zoning: Informa-
tion extraction from scientific text. Ph.D. thesis, Uni-
versity of Edinburgh, England.
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for super-
vised email summarization. In AAAI08 EMAIL Work-
shop, Chicago, USA. AAAI.
Jan Ulrich. 2008. Supervised machine learning for email
thread summarization. Master?s thesis, Computer Sci-
ence.
Stephen Wan and Kathleen McKeown. 2004. Generating
overview summaries of ongoing email thread discus-
sions. In Proc. of ACL, page 549. ACL.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiaolong
Wang, and Bo Li. 2009. Adaptive maximum marginal
relevance based multi-email summarization. In AICI,
volume 5855 of LNCS, pages 417?424. Springer.
89
Proceedings of the 14th European Workshop on Natural Language Generation, pages 172?177,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
On the Feasibility of Automatically Describing n-dimensional Objects
Pablo Ariel Duboue
Les Laboratoires Foulab
999 du College
Montreal, Quebe?c
pablo.duboue@gmail.com
Abstract
This paper introduces the problem of gen-
erating descriptions of n-dimensional spa-
tial data by decomposing it via model-
based clustering. I apply the approach
to the error function of supervised clas-
sification algorithms, a practical problem
that uses Natural Language Generation for
understanding the behaviour of a trained
classifier. I demonstrate my system on a
dataset taken from CoNLL shared tasks.
1 Introduction
My focus is the generation of textual descriptions
for n-dimensional data. At this early stage in
this research, I introduce the problem, describe a
potential application and source of interesting n-
dimensional objects and show preliminary work
on a traditional NLG system built on off-the-shelf
text planning and surface realization technology
plus a customized sentence planner.
This work was inspired by a talk by Kathleen
McCoy in which she described a system that pro-
duces Natural Language explanations of maga-
zine infographics for the blind by combining Com-
puter Vision techniques with NLG (Carberry et al,
2013). She mentioned an anecdote in which she
asked a blind user of the system what would the
user would want added to the text description and
the user replied ?I don?t know, I have never seen
an infographic.? I found the comment very inspir-
ing and it led to the realization that n-dimensional
objects (for n > 3) were also something which
we, as humans, have never seen before and which
we will profit from having a computer system to
describe to us.
A type of n-dimensional objects that are of par-
ticular practical interest are the error function for a
machine learning algorithm for particular training
data. That is the case because, for NLP practition-
ers using supervised classification, the task of de-
bugging and improving their classifiers at times in-
volves repeated steps of training with different pa-
rameters. Usually, at each stage the trained model
is kept as an opaque construct of which only ag-
gregate statistics (precision, recall, etc) are inves-
tigated. My technology improves this scenario by
generating Natural Language descriptions for the
error function of trained machine learning models.
My system, Thoughtland,1 (Fig. 1) is a pipeline
with four stages, accessed through a Web-based
interface (Duboue, 2013), further discussed in the
next section.
This early prototype is already able to tackle de-
scriptions of existing, non-trivial data. These re-
sults are very encouraging and the problem merits
attention from other NLG researchers. To further
broad interest in this problem, I am distributing my
prototype under a Free Software license,2 which
should encourage extensions and classroom use. I
have already found the current descriptions useful
for telling apart the output of two different algo-
rithms when run on the same data.
I will now describe the algorithm and then dive
into the NLG details. I conclude with related and
future work discussions.
2 Algorithm
Thoughtland?s architecture is shown in Fig. 1.
While the first stage lies clearly outside the in-
terest of NLG practitioners, the next two stages
(Clustering and Analysis) are related to the mes-
sage generation aspect of content planning (Reiter
and Dale, 2000),3 as they seek to transform the
data into units that can be communicated verbally
(the last stage is the more traditional NLG system
itself).
1http://thoughtland.duboue.net
2https://github.com/DrDub/Thoughtland
3pages 61-63.
172
Training
Data
Figure 1: Thoughtland?s architecture.
2.1 Cross-Validation
The error function is computed as the error for
each point in the input data. For a numeric tar-
get class, that would mean that for every training
instance (~x, y), e =
?
?
?
f( ~x)? y
?
?
?
, where the error
is computed using f trained on the folds that do
not contain (~x, y).4 This stage produces a cloud of
points in n-dimensions, for n = F + 1, where F
is the number of features in the training data (the
extra dimension is the error value).
2.2 Clustering
The cloud of error points obtained in the previous
step is then clustered using a mixture of Dirich-
let models (McCullagh and Yang, 2008) as imple-
mented by Apache Mahout (Owen et al, 2011).5
I choose this clustering approach because each
of the obtained clusters has a geometrical rep-
resentation in the form of n-balls, which are n-
dimensional spheres. These representations are
important later on for the natural language gener-
ation approach.
Some input features present a natural geomet-
ric groupings which will interfere with a clustering
set to elucidate the error function. To make the er-
ror coordinate the most prominent coordinate for
clustering, I re-scale the error coordinate using the
radius of an n-ball that encompasses all the input
features.
2.3 Analysis
In Fig. 1, the Analysis Stage involves determin-
ing the overall size, density, distances to the other
n-balls and extension in each dimension for each
n-ball. These numbers are put into perspective
with respect to the n-ball encompassing the whole
cloud of points. The distance between two n-balls,
for example, is said to be big if in any dimension
4The error is different if the target class is not numeric
(nominal target classes). In that case the error is 1.0 if the
class is different from the target or 0 if it the same.
5See Section 9.4.2, ?Dirichlet clustering.?
it is above half the radius of the large n-ball in
that particular dimension. Each n-ball is also com-
pared to each other in terms of distance.
I have so far determined these thresholds by
working on the mileage data discussed elsewhere
(Duboue, 2013). Objective-function optimization-
based techniques (discussed in the next section)
might prove useful here.
This stage is at its infancy, in future work I
want to analyze the pairs of n-balls in terms of
rotations as they are particularly important to de-
termine how many dimensions are actually being
used by the sets of n-balls.
3 Natural Language Generation
As I go exploring the different aspects of the prob-
lem, I opt for a very traditional generation system
and architecture. Approaches based on learning
(Mairesse et al, 2010; Varges and Mellish, 2010;
Oh and Rudnicky, 2000) are not particularly easy
to apply to this problem as I am producing a text
for which there are no available examples. I do
hope to explore objective-function optimization-
based techniques such as Lemon (2011) or Deth-
lefs and Cuaya?huitl (2011) in the near future.
The NLG system is thus implemented on
top of McKeown?s (1985) Document Structur-
ing Schemata (using the recent implementation
OpenSchema6) and SimpleNLG (Gatt and Reiter,
2009). I use two schemata, in one the n-balls are
presented in order while in the other the attributes
are presented in order. One of the schemata I
am using is shown in Fig. 2. Document structur-
ing schemata are transition networks of rhetorical
predicates that can contain free and bound vari-
ables, with restrictions on each variable. The sys-
tem presents the user the shorter description.
Either strategy should emphasize similarities,
simplifying aggregation (Reape and Mellish,
1999). I employ some basic aggregation rules, that
6http://openschema.sf.net
173
is, for each aggregation segment I assemble all
n-balls with the same property together to make
complex sentences. That works well for size and
density. To verbalize distances, I group the dif-
ferent pairs by distance value and then look for
cliques using the Bron-Kerbosch clique-finding al-
gorithm (Bron and Kerbosch, 1973), as imple-
mented in JGraphT.7 I also determine the most
common distance and verbalize it as a defeasible
rule (Knott et al, 1997), which significantly short-
ens the text.
This pipeline presents a non-trivial NLG appli-
cation that is easy to improve upon and can be used
directly in a classroom setting.
3.1 Case Study
I will now illustrate Thoughtland by virtue of
an example with training data from the CoNLL
Shared Task for the year 2000 (Sang and Buch-
holz, 2000). The task involved splitting a sentence
into syntactically related segments of words:
(NP He) (VP reckons) (NP the current account
deficit) (VP will narrow) (PP to) (NP only # 1.8
billion) (PP in) (NP September) .
The training contains for each word its POS and
its Beginning/Inside/Outside chunk information:
He PRP B-NP
reckons VBZ B-VP
the DT B-NP
current JJ I-NP
account NN I-NP
deficit NN I-NP
will MD B-VP
narrow VB I-VP
I transformed the data into a classification problem
based on the current and previous POS, rendering
it a two dimensional problem. The provided data
consists of 259,104 training instances. Over this
data Na??ve Bayes produces an accuracy of 88.9%
and C4.5, 89.8%. These numbers are very close,
but do the two algorithms produce similar error
function? Looking at Thoughtland?s descriptions
(Fig. 3) we can see that is not the case.
In later runs I add the current and previous
words, to make for a three and fourth dimensional
problem. These are extra dimensions with a nomi-
nal class with 20,000 distinct values (one for each
word). Interestingly, when the classifiers become
good enough, there is no discriminating informa-
tion left to verbalize. A similar situation happens
when the classifiers have poor accuracy.
7http://jgrapht.sourceforge.net/
schema by-attribute(whole: c-full-cloud)
; first sentence, overall numbers
pred-intro(cloud|whole)
aggregation-boundary
star
pred-size()
aggregation-boundary
star
pred-density()
aggregation-boundary
star
pred-distance()
predicate pred-density
variables
req def component : c-n-ball
req attribute : c-density
properties
component == attribute.component
output
pred has-attribute
pred0 component
pred1 attribute
pred2 magnitude
Figure 2: One of the two schemata employed by
Thoughtland. This schema produces descriptions
focusing on the similar attributes of each of the n-
balls. I include one of the predicates for reference.
4 Related Work
The problem of describing n-dimensional objects
is a fascinating topic which Thoughtland just starts
to address. It follows naturally the long term inter-
est in NLG for describing 3D scenes (Blocher et
al., 1992), spatial/GIS data (De Carolis and Lisi,
2002) or just numerical data (Reiter et al, 2008).
In the more general topic of explaining machine
learning decisions, ExOpaque (Guo and Selman,
2007) takes a trained system and uses it to pro-
duce training data for an Inductive Logic Program-
ming (Muggleton and Raedt., 1994) system, pre-
senting the resulting Horn-clauses directly to the
user. Focusing on explaining the impact of specific
attributes in the prediction outcome of a particular
instance, Robnik-Sikonja and Kononenko (2008)
analyze changes to the classification outcome un-
der different input variations, weighted by their
priors, an idea explored early on in agent-based
systems (Johnson, 1994). In general, systems
based on Bayesian networks seem to have a
stronger probabilistic framework that facilitates
explanations (Lacave and Diez, 2000).
By far, most of the attention in understanding
the error function for machine learning algorithms
has come from the graphical visualization commu-
174
THREE DIMENSIONS
Naive Bayes C4.5
Accuracy 88.9% Accuracy 89.8%
There are five components and three dimensions. Component
One is big and components Two, Three and Four are small.
Component Four is dense and components Two and Three are
very dense. Components Three and Five are at a good distance
from each other. The rest are all far from each other.
There are six components and three dimensions. Component
One is big, components Two, Three and Four are small and
component Five is giant. Component Five is sparse and com-
ponents Two, Three and Four are very dense. Components One
and Two are at a good distance from each other. The rest are all
far from each other.
FOUR DIMENSIONS
Accuracy 90.4% Accuracy 91.4%
There are six components and four dimensions. Components
One, Two and Three are big and components Four and Five are
small. Component Three is dense, component One is sparse
and components Four and Five are very dense. Components
Two and Three are at a good distance from each other. The rest
are all far from each other.
There are six components and four dimensions. Components
One, Two and Three are big and components Four and Five are
small. Component One is dense, component Three is sparse and
components Four and Five are very dense. Components Three
and Four are at a good distance from each other. Components
Six and Four are also at a good distance from each other. The
rest are all far from each other.
FIVE DIMENSIONS
Accuracy 91.6% Accuracy 91.6%
There is one component and five dimensions. There is one component and five dimensions.
Figure 3: Example generated descriptions.
nities. However, as stated by Janert (2010):8
As soon as we are dealing with
more than two variables simultaneously,
things become much more complicated ?
in particular, graphical methods quickly
become impractical.
The focus is then in dimensionality reduction9
and projection (Kaski and Peltonen, 2011), usually
as part of an integrated development environment
(Kapoor et al, 2012; Patel et al, 2010). The usual
discussion regarding the complementary role of
text and graphics, as studied for a long time in
NLG (McKeown et al, 1997), applies also here:
there are things like generalizations and excep-
tions that are easier to express in text. We look
forward for NLG-based approaches to be included
in future versions of ML IDEs such as Gestalt.
Finally, Thoughtland uses the error function for
an ML algorithm as applied to training data. A
similarly worded term which should not be con-
fused is error surface (Reed and Marks, 1999),10
which refers to the space of possible ML models.
Error surfaces are particularly important for train-
ing algorithms that explore the said surface, for ex-
ample by gradient descent.
8Chapter 5, page 99.
9A reviewer suggested combining dimensionality reduc-
tion and NLG, an idea most definitely worth exploring.
10Chapter 8.
5 Final Remarks
I have presented Thoughtland, a working proto-
type addressing the problem of describing clouds
of points in n-dimensional space. In this paper I
have identified the problem and shown it to be ap-
proachable with a solution based on model-based
clustering.
For future work, I want to enrich the analysis
with positional information: I want to find planes
on which a majority of the n-balls lie so as to de-
scribe their location relative to them. I am also
considering hierarchical decomposition in up to
five to seven n-balls (to make it cognitively ac-
ceptable (Miller, 1956)) as it will translate well to
textual descriptions.
My preliminary experiments suggest there is
value in generating comparisons for two error
functions. I can therefore employ the existing
body of work in NLG for generating comparisons
(Milosavljevic, 1999).
While the pilot might speak of the feasibility of
the task, Thoughtland still needs to be evaluated.
For this, I want to start with simple cases such as
overfitting or feature leaks and see if the descrip-
tions help humans detect such cases faster.
Acknowledgements
The author would like to thank Annie Ying, Or
Biran, Samira Ebrahimi Kahou and David Racca.
175
References
A. Blocher, E. Stopp, and T. Weis. 1992. ANTLIMA-
1: Ein System zur Generierung von Bildvorstel-
lungen ausgehend von Propositionen. Techni-
cal Report 50, University of Saarbru?cken, Sonder-
forschungsbereich 314, Informatik.
Coenraad Bron and Joep Kerbosch. 1973. Finding
all cliques of an undirected graph (algorithm 457).
Commun. ACM, 16(9):575?576.
Sandra Carberry, Stephanie Elzer Schwartz, Kathleen
Mccoy, Seniz Demir, Peng Wu, Charles Green-
backer, Daniel Chester, Edward Schwartz, David
Oliver, and Priscilla Moraes. 2013. Access to mul-
timodal articles for individuals with sight impair-
ments. ACM Trans. Interact. Intell. Syst., 2(4):21:1?
21:49, January.
Berardina De Carolis and Francesca A Lisi. 2002.
A NLG-based presentation method for supporting
KDD end-users. In Foundations of Intelligent Sys-
tems, pages 535?543. Springer.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hier-
archical reinforcement learning and hidden markov
models for task-oriented natural language genera-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 654?659. Association for Computational
Linguistics.
P.A. Duboue. 2013. Thoughtland: Natural Language
Descriptions for Machine Learning n-dimensional
Error Functions. In Proceedings of ENLG?13.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a
realisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yunsong Guo and Bart Selman. 2007. ExOpaque:
A framework to explain opaque machine learning
models using Inductive Logic Programming. In IC-
TAI (2), pages 226?229. IEEE Computer Society.
Philipp K. Janert. 2010. Data Analysis with Open
Source Tools. O?Reilly.
W Lewis Johnson. 1994. Agents that learn to ex-
plain themselves. In Proceedings of the twelfth
national conference on Artificial intelligence, vol-
ume 2, pages 1257?1263.
Ashish Kapoor, Bongshin Lee, Desney Tan, and Eric
Horvitz. 2012. Performance and preferences: In-
teractive refinement of machine learning procedures.
In Twenty-Sixth AAAI Conference on Artificial Intel-
ligence.
Samuel Kaski and Jaakko Peltonen. 2011. Dimen-
sionality reduction for data visualization [applica-
tions corner]. Signal Processing Magazine, IEEE,
28(2):100?104.
Alistair Knott, Mick O?Donnell, Jon Oberlander, and
Chris Mellish. 1997. Defeasible rules in con-
tent selection and text structuring. In Proceed-
ings of the Sixth European Workshop on Natural
Language Generation, pages 50?60, Duisburg, Ger-
many, March.
Carmen Lacave and Francisco J. Diez. 2000. A re-
view of explanation methods for bayesian networks.
Knowledge Engineering Review, 17:2002.
Oliver Lemon. 2011. Learning what to say and how to
say it: Joint optimisation of spoken dialogue man-
agement and natural language generation. Com-
puter Speech & Language, 25(2):210?221.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation
using graphical models and active learning. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1552?
1561. Association for Computational Linguistics.
Peter McCullagh and Jie Yang. 2008. How many clus-
ters? Bayesian Analysis, 3(1):101?120.
Kathleen McKeown, Shimei Pan, James Shaw, Jordan
Desmond, and Barry Allen. 1997. Language gener-
ation for multimedia healthcare briefings. In Pro-
ceedings of the Fifth Conference on Applied Nat-
ural Language Processing (ANLP-97), Washington
(DC), USA, April.
Kathleen Rose McKeown. 1985. Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press, Cambridge, England.
George Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity for
processing information. The psychological review,
63:81?97.
Maria Milosavljevic. 1999. Maximising the Coher-
ence of Descriptions via Comparison. Ph.D. thesis,
Macquarie University, Sydney, Australia.
S. Muggleton and L. D. Raedt. 1994. Inductive logic
programming: Theory and methods. Journal of
Logic Programming, (19/20):629?679.
Alice Oh and A. Rudnicky. 2000. Stochastic language
generation for spoken dialogue systems. In Pro-
ceedings of the ANLP/NAACL 2000 Workshop on
Conversational Systems, pages 27?32, Seattle, WA,
May.
Sean Owen, Robin Anil, Ted Dunning, and Ellen Fried-
man. 2011. Mahout in Action. Manning Publi-
cations Co., Manning Publications Co. 20 Baldwin
176
Road PO Box 261 Shelter Island, NY 11964, first
edition.
Kayur Patel, Naomi Bancroft, Steven M Drucker,
James Fogarty, Andrew J Ko, and James Landay.
2010. Gestalt: integrated support for implemen-
tation and analysis in machine learning. In Pro-
ceedings of the 23nd annual ACM symposium on
User interface software and technology, pages 37?
46. ACM.
Mike Reape and Chris Mellish. 1999. Just what
is aggregation anyway? In Proceedings of the
European Workshop on Natural Language Genera-
tion (EWNLG?99), pages 20 ? 29, Toulouse, France,
May.
Russell D. Reed and Robert J. Marks. 1999. Neural
Smithing: Supervised Learning in Feedforward Ar-
tificial Neural Networks. MIT Press.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Albert Gatt, Franc?ois Portet and Marian
van der Meulen 2008. The importance of narrative
and other lessons from an evaluation of an NLG sys-
tem that summarises clinical data. In INLG ?08.
Marko Robnik-Sikonja and Igor Kononenko. 2008.
Explaining classifications for individual instances.
IEEE Trans. Knowl. Data Eng., 20(5):589?600.
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: Chunking.
In Proceedings of the 2nd workshop on Learning
language in logic and the 4th conference on Com-
putational natural language learning, September,
pages 13?14.
Sebastian Varges and Chris Mellish. 2010. Instance-
based natural language generation. Natural Lan-
guage Engineering, 16(3):309.
177
Proceedings of the 14th European Workshop on Natural Language Generation, pages 200?201,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Thoughtland: Natural Language Descriptions for
Machine Learning n-dimensional Error Functions
Pablo Ariel Duboue
Les Laboratoires Foulab
999 du College
Montreal, Quebe?c
pablo.duboue@gmail.com
Abstract
This demo showcases Thoughtland, an
end-to-end system that takes training data
and a selected machine learning model,
produces a cloud of points via cross-
validation to approximate its error func-
tion, then uses model-based clustering to
identify interesting components of the er-
ror function and natural language genera-
tion to produce an English text summariz-
ing the error function.
1 Introduction
For Machine Learning practitioners of supervised
classification, the task of debugging and improv-
ing their classifiers involves repeated iterations of
training with different parameters. Usually, at
each stage the trained model is kept as an opaque
construct of which only aggregate statistics (pre-
cision, recall, etc.) are investigated. Thoughtland
(Duboue, 2013) improves this scenario by gener-
ating Natural Language descriptions for the error
function of trained machine learning models. It is
a pipeline with four components:
(1) A cross-validation step that uses a machine
algorithm from a given learning library run over
a given dataset with a given set of parameters.
This component produces a cloud of points in n-
dimensions, where n = F + 1, where F is the
number of features in the training data (the ex-
tra dimension is the error value). (2) A clustering
step that identifies components within the cloud of
points. (3) An analysis step that compares each
of the components among themselves and to the
whole cloud of points. (4) A verbalization step
that describes the error function by means of the
different relations identified in the analysis step.
2 Structure of the Demo
This demo encompasses a number of training
datasets obtained from the UCI Machine Learn-
ing repository (attendees can select different train-
ing parameters and see together the changes in the
text description). It might be possible to work with
some datasets provided by the attendee at demo
time, if they do not take too long to train and they
have it available in the regular Weka ARFF format.
A Web demo where people can submit ARFF
files (of up to a certain size) and get the differ-
ent text descriptions is will also be available at
http://thoughtland.duboue.net (Fig. 1). Moreover, the
project is Free Software1 and people can install it
and share their experiences on the Website and at
the demo booth.
3 An Example
I took a small data set from the UCI Machine
Learning repository, the Auto-Mpg Data2 and
train on it using Weka (Witten and Frank, 2000).
Applying a multi-layer perceptron with two hid-
den layers with three and two units, respectively,
we achieve an accuracy of 65% and the following
description:
There are four components and eight di-
mensions. Components One, Two and
Three are small. Components One, Two
and Three are very dense. Components
Four, Three and One are all far from
each other. The rest are all at a good
distance from each other.
When using a single hidden layer with eight units
we obtain an accuracy 65.7%:
1https://github.com/DrDub/Thoughtland.
2http://archive.ics.uci.edu/ml/machine-
learning-databases/auto-mpg/
200
Figure 1: Web Interface to Thoughtland (composite).
There are four components and eight di-
mensions. Components One, Two and
Three are small. Components One, Two
and Three are very dense. Components
Four and Three are far from each other.
The rest are all at a good distance from
each other.
As both descriptions are very similar (we have
emphasized the difference, which in the first case
is also an example of our clique-based aggregation
system), we can conclude that the two systems are
performing quite similarly. However, if we use a
single layer with only two units, the accuracy low-
ers to 58% and the description becomes:
There are five components and eight di-
mensions. Components One, Two and
Three are small and Component Four is
giant. Components One, Two and Three
are very dense. Components One and
Four are at a good distance from each
other. Components Two and Three are
also at a good distance from each other.
Components Two and Five are also at a
good distance from each other. The rest
are all far from each other.
4 Final Remarks
Thoughtland follows the example of Mathematics,
where understanding high dimensional objects is
an everyday activity, thanks to a mixture of formu-
lae and highly technical language. It?s long term
goal is to mimic these types of descriptions auto-
matically for the error function of trained machine
learning models.
The problem of describing n-dimensional ob-
jects is a fascinating topic which Throughtland just
starts to address. It follows naturally the long term
interest in NLG for describing 3D scenes (Blocher
et al, 1992).
Thoughtland is Free Software, distributed un-
der the terms of the GPLv3+ and it is written in
Scala, which allow for easy extension in both Java
and Scala and direct access to the many machine
learning libraries programmed in Java. It contains
a straightforward, easy to understand and modify
classic NLG pipeline based on well understood
technology like McKeown?s (1985) schemata and
Gatt and Reiter?s (2009) SimpleNLG project. This
pipeline presents a non-trivial NLG application
that is easy to improve upon and can be used di-
rectly in classroom presentations.
References
A. Blocher, E. Stopp, and T. Weis. 1992. ANTLIMA-
1: Ein System zur Generierung von Bildvorstellun-
gen ausgehend von Propositionen. Technical Re-
port 50, University of Saarbru?cken, Informatik.
P.A. Duboue. 2013. On the feasibility of automatically
describing n-dimensional objects. In ENLG?13.
A. Gatt and E. Reiter. 2009. SimpleNLG: A realisation
engine for practical applications. In Proc. ENLG?09.
K.R. McKeown. 1985. Text Generation: Using Dis-
course Strategies and Focus Constraints to Gener-
ate Natural Language Text. Cambridge University
Press.
Ian H. Witten and Eibe Frank. 2000. Data Min-
ing: Practical Machine Learning Tools and Tech-
niques with Java Implementations. Morgan Kauf-
mann Publishers.
201
