Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 608?616, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CoMeT: Integrating different levels of linguistic modeling for
meaning assessment
Niels Ott Ramon Ziai Michael Hahn Detmar Meurers
Sonderforschungsbereich 833
Eberhard Karls Universita?t Tu?bingen
{nott,rziai,mhahn,dm}@sfs.uni-tuebingen.de
Abstract
This paper describes the CoMeT system, our
contribution to the SemEval 2013 Task 7 chal-
lenge, focusing on the task of automatically
assessing student answers to factual questions.
CoMeT is based on a meta-classifier that uses
the outputs of the sub-systems we developed:
CoMiC, CoSeC, and three shallower bag ap-
proaches. We sketch the functionality of all
sub-systems and evaluate their performance
against the official test set of the challenge.
CoMeT obtained the best result (73.1% accu-
racy) for the 3-way unseen answers in Beetle
among all challenge participants. We also dis-
cuss possible improvements and directions for
future research.
1 Introduction
Our contribution to the SemEval 2013 Task 7 chal-
lenge (Dzikovska et al, 2013) presented here is based
on our research in the A4 project1 of the SFB 833,
which is dedicated to the question how meaning can
be computationally compared in realistic situations.
In realistic situations, utterances are not necessarily
well-formed or complete, there may be individual
differences in situative and world knowledge among
the speakers. This can complicate or even preclude
a complete linguistic analysis, leading us to the fol-
lowing research question: Which linguistic repre-
sentations can be used effectively and robustly for
comparing the meaning of sentences and text frag-
ments computationally?
1http://purl.org/dm/projects/sfb833-a4
In order to work on effective and robust processing,
we base our work on reading comprehension exer-
cises for foreign language learners, of which we are
also collecting a large corpus (Ott et al, 2012). Our
first system, CoMiC, is an alignment-based approach
which exists in English and German variants (Meur-
ers et al, 2011a; Meurers et al, 2011b). CoMiC
uses various levels of linguistic abstraction from sur-
face tokens to dependency parses. Further work that
we are starting to tackle includes the utilization of
Information Structure (Krifka, 2007) in the system.
The second approach emerging from the research
project is CoSeC (Hahn and Meurers, 2011; Hahn
and Meurers, 2012), a semantics-based system for
meaning comparison that was developed for German
from the start and was ported to operate on English
for this shared task. As a novel contribution in this
paper, we present CoMeT (Comparing Meaning in
Tu?bingen), a system that employs a meta-classifier
for combining the output of CoMiC and CoSeC and
three shallower bag approaches.
In terms of the general context of our work, short
answer assessment essentially comes in the two fla-
vors of meaning comparison and grading, the first
trying to determine whether or not two utterances
convey the same meaning, the latter aimed at grading
the abilities of students (cf. Ziai et al, 2012). Short
answer assessment is also closely related to the field
of Recognizing Textual Entailment (RTE, Dagan et
al., 2009), which this year is directly reflected by
the fact that SemEval 2013 Task 7 is the Joint Stu-
dent Response Analysis and 8th Recognizing Textual
Entailment Challenge.
608
Turning to the organization of this paper, section 2
introduces the three types of sub-systems and the
meta-classifier. In section 3, we report on the evalu-
ation results of each sub-system both for our devel-
opment set as well as for the official test set of the
shared task. We then discuss possible causes and
implications of the findings we made by participating
in the shared task.
2 Systems
The CoMeT system that we describe in this paper
is a combination of three types of sub-systems in
one meta-classifier. CoSeC and CoMiC are sys-
tems that align linguistic units in the student answer
to those in the reference answer. In contrast, the
bag-based approaches employ a vocabulary of words,
lemmas, and Soundex hashes constructed from all
of the student answers in the training data. In the
meta-classifier, we tried to combine the benefits of the
named sub-systems into one large system that eventu-
ally computed our submission to the SemEval 2013
Task 7 challenge.
2.1 CoMiC
CoMiC (Comparing Meaning in Context) is an
alignment-based system, i.e., it operates on a map-
ping of linguistic units found in a student answer to
those given in a reference answer. CoMiC started off
as a re-implementation of the Content Assessment
Module (CAM) of Bailey and Meurers (2008). It
exists in two flavors: CoMiC-DE for German, de-
scribed in Meurers et al (2011b), and CoMiC-EN for
English, described in Meurers et al (2011a). Both
systems are positioned in the landscape of the short
answer assessment field in Ziai et al (2012). In this
paper, we refer to CoMiC-EN simply as CoMiC.
Sketched briefly, CoMiC operates in three stages:
1. Annotation uses various NLP modules to equip
student answers and reference answers with lin-
guistic abstractions of several types.
2. Alignment creates links between these linguistic
abstractions from the reference answer to the
student answer.
3. Classification uses summary statistics of these
alignment links in machine learning in order to
assign labels to each student answer.
Automatic annotation and alignment are imple-
mented in the Unstructured Information Management
Architecture (UIMA, Ferrucci and Lally, 2004). Our
UIMA modules mainly wrap around standard NLP
tools of which we provide an overview in Table 1.
We used the standard statistical models which are
provided with the NLP tools.
Annotation Task NLP Component
Sentence Detection OpenNLP2
Tokenization OpenNLP
Lemmatization morpha (Minnen et al, 2001)
Spell Checking Edit distance (Levenshtein, 1966),
SCOWL word list3
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking OpenNLP
Synonyms and WordNet (Fellbaum, 1998)
Semantic Types
Similarity Scores PMI-IR (Turney, 2001)
on UkWaC (Baroni et al, 2009)
Dependency Relations MaltParser (Nivre et al, 2007)
Keyword extraction Heads from dependency parse
Table 1: NLP tools used for CoMiC and Bag Approaches
Annotation ranges from very basic linguistic units
such as sentences and tokens with POS and lemmas,
over NP chunks, up to full dependency parses of
the input. For distributional semantic similarity via
PMI-IR (Turney, 2001), a local search engine based
on Lucene (Gospodnetic? and Hatcher, 2005) querying
the UkWaC corpus (Baroni et al, 2009) was used,
since all major search engines meanwhile have shut
down their APIs.
After the annotation of linguistic units has taken
place, candidate alignment links are created within
UIMA. In a simple example case, a candidate align-
ment link is a pair of tokens that is token identical
in the student answer and in the reference answer.
The same token in the student answer may also be
part of a candidate alignment link that maps to an-
other token in the reference answer that, e.g., has the
same lemma, or is a possible synonym, or again is
token identical. Other possible links are based on
spelling-corrected tokens, semantic types, or high
values of the PMI-IR similarity measure.
Words that are present in the reading comprehen-
sion question and that are also found in the student an-
swer are excluded from alignment, resulting in a very
2http://incubator.apache.org/opennlp
3http://wordlist.sourceforge.net
609
basic implementation of an approach to givenness
(cf. Halliday, 1967, p. 204 and many others since).
Subsequently, a globally optimal alignment of lin-
guistic units in the reference answer and student an-
swer is determined using the Traditional Marriage
Algorithm (Gale and Shapley, 1962).
At this point, processing within UIMA comes to
an end with an output module that generates the files
containing the features for machine learning. These
features basically are summary statistics of the types
of alignment links. An overview of these numeric
features used is given in Table 2.
Feature Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned
target/learner tokens
4./5. Chunk Overlap Percent of aligned
target/learner chunks
6./7. Triple Overlap Percent of aligned
target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of
(0-5) token-level alignments
Table 2: Features used in CoMiC?s classification phase
Current versions of CoMiC use the WEKA toolkit
(Hall et al, 2009), allowing us to experiment with
different machine learning strategies. In general, any
type of classification can be trained in this machine
learning phase, a binary correct vs. incorrect de-
cision as in the 2-way task being the simplest case.
The best results with CoMiC on our held-out develop-
ment set were achieved using WEKA?s J48 classifier,
which is an implementation of decision tree based on
Quinlan (1993).
In terms of linguistic abstractions, CoMiC leaves
the choice of representations used to its alignment
step. However, in the final machine learning step, no
concrete information about linguistic units is present
any more. The machine learning component only
sees alignment configurations which are indepen-
dent of concrete words, phrases, or any other lin-
guistic information. This high level of abstraction
suggests that CoMiC should perform better than other
approaches on unseen topics and unseen questions,
since it does not rely on concrete units as, e.g., a
bag-of-words approach does.
2.2 CoSeC
CoSeC (Comparing Semantics in Context) performs
meaning comparison on the basis of an underspec-
ified semantic representation robustly derived from
the learner and the reference answers. The sys-
tem was developed for German (Hahn and Meurers,
2012), on the basis of which we created the English
CoSeC-EN for the SemEval 2013 Task 7 challenge.
Using an explicit semantic formalism in principle
makes it possible to precisely represent meaning dif-
ferences. It also supports a direct representation of
Information Structure as a structuring of semantics
representations (Krifka, 2007).
CoSeC is based on Lexical Resource Semantics
(LRS, Richter and Sailer, 2004). Being an under-
specified semantic formalism, LRS avoids the costly
computation of all readings and provides access to
the building blocks of the semantic representation,
while additional constraints provide the information
about their composition.
As described in Hahn and Meurers (2011), LRS
representations can be derived automatically using
a two-step approach based on part-of-speech tags
assigned by TreeTagger (Schmid, 1994) and depen-
dency parses by MaltParser (Nivre et al, 2007). First,
the dependency structure is transformed into a com-
pletely lexicalized syntax-semantics interface rep-
resentation, which abstracts away from some form
variation at the surface. These representations are
then mapped to LRS representations. The approach
is robust in that it always results in an LRS structure,
even for ill-formed sentences.
CoSeC then aligns the LRS representations of the
reference answer and the student answer to each other
and also to the representation of the question. The
alignment approach takes into account local criteria,
namely the semantic similarity of pairs of elements
that are linked by the alignment, as well as global
criteria measuring the extent to which the alignment
610
preserves structure at the levels of variables and the
subterm structure of the semantic formulas.
Local similarity of semantic expressions is esti-
mated using WordNet (Fellbaum, 1998), FrameNet
(Baker et al, 1998), PMI-IR (Turney, 2001) on the
UkWaC (Baroni et al, 2009) as used in CoMiC, the
Minimum Edit Distance (Levenshtein, 1966), and
special parameters for comparing functional elements
such as quantifiers and grammatical function labels.
Based on the alignments, the system marks ele-
ments which are not linked to elements in the ques-
tion or which are linked to the semantic contribution
of an alternative in an alternative question as ?fo-
cused?. This is intended as a first approximation of
the concept of focus in the sense of Information Struc-
ture (von Heusinger, 1999; Kruijff-Korbayova? and
Steedman, 2003; Krifka, 2007), an active field of re-
search in linguistics addressing the question how the
information in sentences is packaged and integrated
into discourse. Focus elements are expected to be
particularly relevant for determining the correctness
of an answer (Meurers et al, 2011b).
Overall meaning comparison is then done based
on a set of numerical scores computed from the align-
ments and their quality. For each of these scores, a
threshold is empirically determined, over which the
student answer is considered to be correct. Among
the scores discussed by Hahn and Meurers (2011),
weighted-target focus, consistently scored best in the
development set. This score measures the percent-
age of terms in the semantic representation of the
reference answer which are linked to elements of
the student answer in relation to the number of all
elements in the representation of the reference an-
swer. Only terms that were marked as focused in
the preceding step are counted. Functional elements,
i.e., quantifiers, predicates representing grammatical
function labels, or the lambda operator, are weighted
differently from other elements.
This threshold method can only be used to perform
2-way classification. Unlike the machine learning
step in CoMiC, it does not generalize to 3-way or
5-way classification.
The alignment algorithm uses several numerical
parameters, such as weights for the different compo-
nents measuring semantic similarities, weights for
the different overall local and global criteria, and
the weight of the weighted-target focus score. These
parameters are optimized using Powells algorithm
combined with grid-based line optimization (Press et
al., 2002). To avoid overfitting, the parameters and
the threshold are determined on disjoint partitions of
the training set.
In terms of linguistic abstractions, meaning assess-
ment in CoSeC is based entirely on underspecified
semantic representations. Surface forms are indi-
rectly encoded by the structure of the representation
and the predicate names, which are usually derived
from the lemmas. As with CoMiC, parameter opti-
mization and the determination of the thresholds for
the numerical scores do not involve concrete infor-
mation about linguistic objects. Again, the high level
of abstraction suggests that CoSeC should perform
better than other approaches on unseen topics and
unseen questions.
2.3 The Bag Approaches
Inspired by the bag-of-words concept that emerged
from information retrieval (Salton and McGill, 1983),
we designed a system that uses bag representations
of student answers. For each student answer, there
are three bags, each containing one of the following
representations: words, lemmas and Soundex hashes
of that answer. The question ID corresponding to
the answer is added to each bag as a pseudo-word,
allowing the machine learner to adjust to question-
specific properties. Based on the bag representations,
the approach compares a given student answer to a
model trained on all other known student answers.
On the one hand, this method ignores the presence of
reference answers (although they could be added to
the training set as additional correct answers), on the
other hand it makes use of information not taken into
account by alignment-based systems such as CoMiC
or CoSeC.
Concerning pre-processing, the linguistic anal-
yses such as tokenization and lemmatization are
identical to those of CoMiC, since the bag gener-
ator technically is just another output module of the
UIMA-based pipeline used there. No stop-word list
is used. The bags are fed into a support vector-based
machine learner. We used WEKA?s Sequential Min-
imal Optimization (SMO, Platt, 1998) implementa-
tion with the radial basis function (RBF) kernel, since
it yielded good results on our development set and
since it supports output of the estimated probabilities
611
for each class. The optimal gamma parameter and
complexity constant were estimated via 10-fold grid
search.
In terms of abstractions, all bag-based approaches
simply disregard word order and in case of binary
bags even word frequency. Still, a bit of the relation
between words is essentially encoded in their mor-
phology. This piece of information is discarded in
the bags of lemmas, eventually, e.g., putting words
like ?bulb? and ?bulbs? in the same vector slot. Fur-
ther away from the surface are the Soundex hashes,
a phonetic representation of English words patented
by Russell (1918). The well-known algorithm trans-
forms similar-sounding English words into the same
representation of characters and numbers, thereby
ironing out many spelling mistakes and common
confusion cases of homophones such as ?there? vs.
?their?. The MorphAdorner4 implementation we used
returns empty Soundex hashes for input tokens that
do not start with a letter of the alphabet. However,
we found in our experiments, that the presence of
these empty hashes in the bags has a positive impact
on performance. This is most likely due to the fact
that it discriminates answers containing punctuation
(not a letter of the alphabet) from those which do not.
Since the bag approaches use Soundex as pho-
netic equivalence classes, but no semantic equiva-
lence classes, they should perform best on the unseen
answers data in which most lexical material from the
test set is likely to already be present in the training
set.
2.4 CoMeT: A Meta-Classifier
As described in the previous sections, our sub-
systems perform short answer evaluation on differ-
ent representations and at different levels of abstrac-
tion. The bag approaches are very surface-oriented,
whereas CoSeC uses a semantic formalism to com-
pare answers to each other. We expected each system
to show its strengths in different test scenarios, so a
way was needed to combine the predictions of differ-
ent systems into the final result.
CoMeT (Comparing Meaning in Tu?bingen) is a
meta-classifier which builds on the predictions of
our individual systems (feature stacking, see Wolpert,
1992). The rationale is that if systems are comple-
4http://morphadorner.northwestern.edu
mentary, their combination will perform better (or at
least as good) than any individual system on its own.
The design is as follows:
Each system produces predictions on the training
set, using 10-fold cross-validation, and on the test set.
In addition to the predicted class, each system was
also made to output probabilities for each possible
class (cf., e.g., Tetreault et al, 2012a). The class
probabilities were then used as features in the meta
classifier to train a model for the test data. In addition
to the probabilities, we also used the question ID and
module ID in the meta-classifier, in the hope that they
would allow differentiation between scenarios. For
example, an unseen question ID means that we are
not testing on unseen answers and thus predictions
from systems with more abstraction from the surface
may be preferred.
The class probabilities come from different
sources, depending on the system. In the case of
CoMiC, they are extracted directly from the decision
trees. For the bag approaches, we used WEKA?s op-
tion to fit logistic models to the SVM output after
classification in order to estimate probabilities. Fi-
nally, the CoSeC probabilities are derived directly
from its final score. As mentioned in section 2.2,
CoSeC only does binary classification, so those prob-
abilities are used in the meta-classifier for all tasks.
Based on the results on our internal development
set (see section 3.1), we chose different system com-
binations for different scenarios. For unseen topics
and unseen questions, we used only CoMiC in com-
bination with CoSeC, since the inclusion of the bag
approaches had a negative impact on results. For un-
seen answers, we additionally included the bag mod-
els. All meta-classification was done using WEKA?s
Logistic Regression implementation. The results are
discussed in section 3.
3 Evaluation
In this section, we present the results for each of the
sub-systems, both on the custom-made split of the
training data we used in our development, as well as
on the official test data of the SemEval 2013 Task 7
challenge. Subsequently, we discuss possible causes
for issues raised by our evaluation results.
612
3.1 Development Set
In order to be as close as possible to the final test
setting, we replicated the official test scenarios on
the training set, resulting in a train/dev/test split for
each of the corpora. For Beetle, we held out all an-
swers to two random questions for each module to
form the unseen questions scenario, and five random
answers from each remaining question to form the
unseen answers scenario. For SciEntsBank, we held
out module LF for dev and module VB for test to
form the unseen topics scenario, because they have
an average number of questions (11). The LF module
turned out to be far more skewed towards incorrect
answers (76.8%) than the training set on average
(57.5%). While this skewedness needs to be taken
into account for the interpretation of the development
results, it did not have a negative effect on our fi-
nal test results. Furthermore, analogous to Beetle,
we held out all answers to one random question for
each remaining module for unseen-questions, and
two random answers from each remaining question
for unseen answers.
The dev set was used for tuning and design deci-
sions concerning which individual systems to com-
bine in the stacked classifier, while we envisaged
the test set to be used as a final checkpoint before
submission.
The accuracy results for all sub-systems on the
development set are reported in detail in Table 3.
The majority baseline reflects the accuracy a system
would achieve by always labelling any student answer
as ?incorrect?, hence it is equivalent to the percentage
of incorrect answers in the data. The lexical baseline
is the performance of the system provided by the
challenge organizers.
Beetle SciEntsBank
System d-uA d-uQ d-uA d-uQ d-uT
Maj. Baseline 57.14% 59.28% 54.30% 60.70% 76.84%
Lex. Baseline 75.43% 71.10% 63.44% 66.05% 59.54%
CoMiC 76.57% 71.52% 67.20% 70.23% 64.63%
Bag of Words 85.14% 62.03% 80.65% 54.65% 73.79%
? of Lemmas 85.71% 58.02% 80.11% 52.33% 74.55%
? of Soundex 86.86% 60.76% 81.18% 53.95% 72.77%
CoSeC 76.00% 74.89% 64.52% 73.49% 68.96%
CoMeT 88.00% 75.95% 81.18% 66.74% 68.45%
Table 3: Development set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
The systems presented in section 2 performed as
expected: The Bag-of-Soundex system achieved its
best scores on the unseen answers where overlap of
vocabulary was most likely, outperforming CoMiC
and CoSeC with accuracy values as high as 86.86%.
For Beetle unseen answers, the meta-classifier op-
erated as expected and improved the overall results
to 88.86%. For SciEntsBank unseen answers, it re-
mained stable at 81.18%.
As expected, CoMiC and CoSeC with their align-
ment not depending on vocabulary outperformed the
bag approaches in the other scenarios, in which the
question or even the domain were not known during
training. However, both alignment-based systems
failed on SciEntsBank?s unseen topics in comparison
to the rather high majority baseline.
3.2 Official Test Set
For our submission to the SemEval 2013 Task 7 chal-
lenge, we trained our sub-systems on the entire of-
ficial training set. The overall performance of the
CoMeT system on all sub-tasks is shown in Table 4.
Beetle SciEntsBank
uA uQ uA uQ uT
Lexical 2-way 79.7% 74.0% 66.1% 67.4% 67.6%
Overlap 3-way 59.5% 51.2% 55.6% 54.0% 57.7%
Baseline 5-way 51.9% 48.0% 43.7% 41.3% 41.5%
Best 2-way 84.5% 74.1% 77.6% 74.5% 71.1%
System 3-way 73.1% 59.6% 72.0% 66.3% 63.7%
5-way 71.5% 62.1% 64.3% 53.2% 51.2%
CoMeT 2-way 83.8% 70.2% 77.4% 60.3% 67.6%
3-way 73.1% 51.8% 71.3% 54.6% 57.9%
5-way 68.8% 48.8% 60.0% 43.7% 42.1%
Table 4: Official test set: overall accuracy of CoMeT (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
While CoMeT won the Beetle 3-way task in unseen
answers, our main focus is on the 2-way task. The
results for the 2-way task of our sub-systems on the
official test set are shown in Table 5.
The first row of the table reports the results of the
winning system of the challenge; the two baselines
are computed as before. In general, the accuracy val-
ues of CoMeT exhibit a drop of around 5% from
our development set to the official test set. The
meta-classifier was unable to benefit from the dif-
ferent sub-systems except for the unseen answers in
SciEntsBank that slightly outperformed the best bag
approach.
613
Beetle SciEntsBank
System uA uQ uA uQ uT
Best 84.50% 74.10% 77.60% 74.50% 71.10%
Maj. Baseline 59.91% 58.00% 56.85% 58.94% 57.98%
Lex. Baseline 79.70% 74.00% 66.10% 67.40% 67.60%
CoMiC 76.08% 70.57% 67.96% 66.30% 67.97%
Bag of Words 83.14% 67.52% 75.93% 57.84% 59.84%
? of Lemmas 83.60% 67.16% 76.67% 58.25% 58.81%
? of Soundex 84.05% 68.38% 75.93% 57.57% 58.02%
CoSeC 62.19% 63.61% 67.22% 58.94% 62.36%
CoMeT 83.83% 70.21% 77.41% 60.30% 67.62%
CoSeC* 75.40% 70.82% 72.04% 64.94% 70.60%
CoMeT* 84.51% 71.43% 79.26% 65.35% 69.53%
Table 5: Official test set: accuracy for 2-way task (uA:
unseen answers, uQ: unseen questions, uT: unseen topics)
Even though it does not live up to the standards of
the bag approaches in their area of expertise (unseen
answers), the CoMiC systems outperforms the bags
on the unseen question and unseen topic sub-sets as
expected. Note that on unseen topics, CoMiC still
scores 10% above the majority baseline on the official
test set, in contrast to the drop of more than 10%
below the baseline for the corresponding (skewed)
development set.
However, the results for CoSeC are around 10%
lower on the unseen questions, and almost 7% lower
on the unseen topics of the test data than on the de-
velopment set, a drop that the overall meta-classifier
(CoMeT) was unable to catch. Investigating this drop
in comparison to our development set, we checked
the correctness of the training script and discovered a
bug in the CoSeC setup that led to the parameters and
the thresholds being computed on the same partition
of the training set, i.e., the system overfitted to this
partition, while the remainder of the training set was
not used for training. Correcting the bug resulted in
CoSeC accuracy values broadly comparable to those
of CoMiC, as was the case on the development set.
This confirms that the reason for the drop in the sub-
mission was not a flaw in the CoSeC system as such,
but a programming bug in a peripheral component.
With this bug fixed, CoSeC performs 5%?13%
better on the test set, and the meta-classifier would
have been able to benefit from the regularly perform-
ing CoSeC, improving in performance up to 5%.
These two amended systems are listed as CoSeC*
and CoMeT* in Table 5. For the two unseen an-
swers scenarios, CoMeT* would outperform the best
scoring systems of the challenge in the 2-way task.
3.3 Discussion
In this section, we try to identify some general ten-
dencies from studying the results. Firstly, we can
observe that due to the strong performance of the bag
models, unseen answers scores are generally higher
than their counterparts. It seems that if questions
have been seen before, surface-oriented methods out-
perform more abstract approaches. However, the
picture is different for unseen domains and unseen
questions. We are generally puzzled by the fact that
many systems in the shared task scored worse on
unseen questions, where in-domain training data is
available, than on unseen domains, where this is not
the case. The CoMeT classifier suffered especially in
unseen questions of SciEntsBank, scoring lower than
our best system would have on its own (see Table 5);
even after the CoSeC bug was fixed, CoMeT* still
scored worse there than CoMiC on its own.
In general, we likely would have benefited from
domain adaptation, as described in, e.g., Daume III
(2007). Consider that the input for the meta-classifier
always consists of the same set of features produced
via standard cross-validation, regardless of the test
scenario. Instead, the trained model should have dif-
ferent feature weights depending on what the model
will be tested on.
4 Conclusion and Outlook
We presented our approach to Task 7 of SemEval
2013, consisting of a combination of surface-oriented
bag models and the increasingly abstract alignment-
based systems CoMiC and CoSeC. Predictions of
all systems were combined using a meta classifier in
order to produce the final result for CoMeT.
The results presented show that our approach per-
forms competitively, especially in the unseen answers
test scenarios, where we obtained the best result of all
participants in the 3-way task with the Beetle corpus
(73.1% accuracy). As expected, the unseen topics
scenario proved to be more challenging, with results
at 67.6% accuracy in the 2-way task for CoMeT. Sur-
prisingly, CoMeT performed consistently worse in
the unseen questions scenarios, which we attribute
to rather low CoSeC results there and to the way the
meta classifier is trained, which currently does not
take into account the test scenario it is trained for
and instead uses the module and question IDs as fea-
614
tures, which turned out not to be an effective domain
adaptation approach.
In our future research, work on CoMiC will con-
centrate on integrating two aspects of the context:
First, we are planning to develop an automatic ap-
proach to focus identification in order to pinpoint the
essential parts of the student answers. Second, for
data sets where a reading text is available, we will
try to automatically determine the location of the rel-
evant source information given the question, which
can then be used as alternative or additional reference
material for answer evaluation.
The CoMiC system currently also relies on the
Traditional Marriage Algorithm to select the optimal
global alignment between student answer and refer-
ence answer. We plan to replace this algorithm by
a machine learning component that can handle this
selection in a data-driven way.
For CoSeC, we plan to develop an extension that
allows for n-to-m mappings, hence improving the
alignment performance for multi-word units such as,
e.g., phrasal verb constructions.
The bag approaches could be augmented by explor-
ing additional levels of abstractions, e.g., semantic
equivalence classes constructed via WordNet lookup.
In sum, while we will also plan to explore opti-
mizations to the training setup of the meta-classifier
(e.g., domain adaptation along the lines of Daume
III, 2007), the main focus of our further research lies
in improving the individual sub-systems, which then
again are expected to push the overall performance
of the CoMeT meta-classifier system.
Acknowledgements
We are thankful to Sowmya Vajjala and Serhiy Bykh
for their valuable advice on meta-classifiers and other
machine learning techniques. We also thank the re-
viewers for their comments; in consultation with the
SemEval organizers we kept the length at 8 pages
plus references, the page limit for papers describing
multiple systems.
References
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the
3rd Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-3) at ACL?08, pages
107?115, Columbus, Ohio. http://aclweb.org/
anthology/W08-0913.pdf.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings of
the 36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Conference
on Computational Linguistics, volume 1, pages 86?90,
Montreal, Quebec, Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A collec-
tion of very large linguistically processed web-crawled
corpora. Journal of Language Resources and Evalua-
tion, 3(43):209?226.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK Techni-
cal Report ILK 07-03. Induction of Linguistic Knowl-
edge Research Group Department of Communication
and Information Sciences, Tilburg University, Tilburg,
The Netherlands, July 11. Version 6.0.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, evalu-
ation and approaches. Natural Language Engineering,
15(4):i?xvii, 10.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student response
analysis and 8th recognizing textual entailment chal-
lenge. In *SEM 2013: The First Joint Conference on
Lexical and Computational Semantics, Atlanta, Geor-
gia, USA, 13-14 June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natural
Language Engineering, 10(3?4):327?348.
David Gale and Lloyd S. Shapley. 1962. College admis-
sions and the stability of marriage. American Mathe-
matical Monthly, 69:9?15.
Otis Gospodnetic? and Erik Hatcher. 2005. Lucene in
Action. Manning, Greenwich, CT.
Michael Hahn and Detmar Meurers. 2011. On deriv-
ing semantic representations from dependencies: A
615
practical approach for evaluating meaning in learner
corpora. In Proceedings of the Intern. Confer-
ence on Dependency Linguistics (DEPLING 2011),
pages 94?103, Barcelona. http://purl.org/
dm/papers/hahn-meurers-11.html.
Michael Hahn and Detmar Meurers. 2012. Evaluat-
ing the meaning of answers to reading comprehen-
sion questions: A semantics-based approach. In Pro-
ceedings of the 7th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-7) at
NAACL-HLT 2012, pages 94?103, Montreal. http:
//aclweb.org/anthology/W12-2039.pdf.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The WEKA data mining software: An update. In The
SIGKDD Explorations, volume 11, pages 10?18.
Michael Halliday. 1967. Notes on Transitivity and Theme
in English. Part 1 and 2. Journal of Linguistics, 3:37?
81, 199?244.
Manfred Krifka. 2007. Basic notions of information struc-
ture. In Caroline Fery, Gisbert Fanselow, and Manfred
Krifka, editors, The notions of information structure,
volume 6 of Interdisciplinary Studies on Information
Structure (ISIS). Universita?tsverlag Potsdam, Potsdam.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003. Dis-
course and information structure. Journal of Logic,
Language and Information (Introduction to the Special
Issue), 12(3):249?259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011a. Integrating parallel analysis modules to
evaluate the meaning of answers to reading comprehen-
sion questions. IJCEELL. Special Issue on Automatic
Free-text Evaluation, 21(4):355?369. http://purl.
org/dm/papers/meurers-ea-11.html.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011b. Evaluating answers to reading comprehen-
sion questions in context: Results for German and
the role of information structure. In Proceedings of
the TextInfer 2011 Workshop on Textual Entailment,
pages 1?9, Edinburgh, Scotland, UK, July. http:
//aclweb.org/anthology/W11-2401.pdf.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natural
Language Engineering, 7(3):207?233.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012. Cre-
ation and analysis of a reading comprehension exercise
corpus: Towards evaluating meaning in context. In
Thomas Schmidt and Kai Wo?rner, editors, Multilin-
gual Corpora and Multilingual Corpus Analysis, Ham-
burg Studies in Multilingualism (HSM), pages 47?69.
Benjamins, Amsterdam. http://purl.org/dm/
papers/ott-ziai-meurers-12.html.
John C. Platt. 1998. Sequential minimal optimization:
A fast algorithm for training support vector machines.
Technical Report MSR-TR-98-14, Microsoft Research.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge, UK.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers.
Frank Richter and Manfred Sailer. 2004. Basic concepts
of lexical resource semantics. In Arnold Beckmann and
Norbert Preining, editors, European Summer School in
Logic, Language and Information 2003. Course Mate-
rial I, volume 5 of Collegium Logicum, pages 87?143.
Publication Series of the Kurt Go?del Society, Wien.
Robert C. Russell. 1918. US patent number 1.261.167, 4.
Gerard Salton and Michael J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill, New
York.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native language
identification. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING),
pages 2585?2602, Mumbai, India.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502, Freiburg, Germany.
Klaus von Heusinger. 1999. Intonation and Information
Structure. The Representation of Focus in Phonology
and Semantics. Habilitationssschrift, Universita?t Kon-
stanz, Konstanz, Germany.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241?259.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012. Short
answer assessment: Establishing links between re-
search strands. In Joel Tetreault, Jill Burstein, and
Claudial Leacock, editors, Proceedings of the 7th Work-
shop on Innovative Use of NLP for Building Edu-
cational Applications (BEA-7) at NAACL-HLT 2012,
pages 190?200, Montreal, June. http://aclweb.
org/anthology/W12-2022.pdf.
616
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Enhancing Authentic Web Pages for Language Learners
Detmar Meurers1, Ramon Ziai1,
Luiz Amaral2, Adriane Boyd3, Aleksandar Dimitrov1, Vanessa Metcalf3, Niels Ott1
1 Universita?t Tu?bingen
2 University of Massachusetts Amherst
3 The Ohio State University
Abstract
Second language acquisition research since
the 90s has emphasized the importance of
supporting awareness of language categories
and forms, and input enhancement techniques
have been proposed to make target language
features more salient for the learner.
We present an NLP architecture and web-
based implementation providing automatic vi-
sual input enhancement for web pages. Learn-
ers freely choose the web pages they want to
read and the system displays an enhanced ver-
sion of the pages. The current system supports
visual input enhancement for several language
patterns known to be problematic for English
language learners, as well as fill-in-the-blank
and clickable versions of such pages support-
ing some learner interaction.
1 Introduction
A significant body of research into the effectiveness
of meaning-focused communicative approaches to
foreign language teaching has shown that input
alone is not sufficient to acquire a foreign lan-
guage, especially for older learners (cf., e.g., Light-
bown and Spada, 1999). Recognizing the important
role of consciousness in second-language learning
(Schmidt, 1990), learners have been argued to ben-
efit from (Long, 1991) or even require (Lightbown,
1998) a so-called focus on form to overcome incom-
plete or incorrect knowledge of specific forms or
regularities. Focus on form is understood to be ?an
occasional shift of attention to linguistic code fea-
tures? (Long and Robinson, 1998, p. 23).
In an effort to combine communicative and struc-
turalist approaches to second language teaching,
Rutherford and Sharwood Smith (1985) argued for
the use of consciousness raising strategies drawing
the learner?s attention to specific language proper-
ties. Sharwood Smith (1993, p. 176) coined the term
input enhancement to refer to strategies highlighting
the salience of language categories and forms.
Building on this foundational research in second
language acquisition and foreign language teaching,
in this paper we present an NLP architecture and a
system for automatic visual input enhancement of
web pages freely selected by language learners. We
focus on learners of English as a Second Language
(ESL), and the language patterns enhanced by the
system include some of the well-established diffi-
culties: determiners and prepositions, the distinction
between gerunds and to-infinitives, wh-question for-
mation, tense in conditionals, and phrasal verbs.
In our approach, learners can choose any web
page they like, either by using an ordinary search-
engine interface to search for one or by entering the
URL of the page they want to enhance. In contrast to
textbooks and other pre-prepared materials, allow-
ing the learner to choose up-to-date web pages on
any topic they are interested in and enhancing the
page while keeping it intact (with its links, multi-
media, and other components working) clearly has
a positive effect on learner motivation. Input en-
hanced web pages also are attractive for people out-
side a traditional school setting, such as in the vol-
untary, self-motivated pursuit of knowledge often
referred to as lifelong learning. The latter can be
particularly relevant for adult immigrants, who are
10
already functionally living in the second language
environment, but often stagnate in their second lan-
guage acquisition and lack access or motivation to
engage in language classes or other explicit lan-
guage learning activities. Nevertheless, they do use
the web to obtain information that is language-based
and thus can be enhanced to also support language
acquisition while satisfying information needs.
In terms of paper organization, in section 2 we
first present the system architecture and in 2.1 the
language phenomena handled, before considering
the issues involved in evaluating the approach in 2.2.
The context of our work and related approaches are
discussed in section 3, and we conclude and discuss
several avenues for future research in section 4.
2 The Approach
The WERTi system (Working with English Real
Texts interactively) we developed follows a client-
server paradigm where the server is responsible for
fetching the web page and enriching it with annota-
tions, and the client then receives the annotated web
page and transforms it into an enhanced version.
The client here is a standard web browser, so on the
learner?s side no additional software is needed.
The system currently supports three types of input
enhancement: i) color highlighting of the pattern or
selected parts thereof, ii) a version of the page sup-
porting identification of the pattern through clicking
and automatic color feedback, and iii) a version sup-
porting practice, such as a fill-in-the-blank version
of the page with automatic color feedback.
The overall architecture is shown in Figure 1.
Essentially, the automated input enhancement pro-
cess consists of the following steps:
1. Fetch the page.
2. Find the natural language text portions in it.
3. Identify the targeted language pattern.
4. Annotate the web page, marking up the lan-
guage patterns identified in the previous step.
5. Transform the annotated web page into the out-
put by visually enhancing the targeted pattern
or by generating interaction possibilities.
Steps 1?4 take place on the server side, whereas step
5 happens in the learner?s browser.1 As NLP is only
involved in step 3, we here focus on that step.
1As an alternative to the server-based fetching of web pages,
Server
UIMA                     
Browser                                         
URL Fetching
HTML Annotation
Identifying text in HTML page
Tokenization
Sentence Boundary Detection
POS Tagging
Pattern-specific NLP
Colorize Click Practice
Figure 1: Overall WERTi architecture. Grey components
are the same for all patterns and activities, cf. section 2.1.
While the first prototype of the WERTi system2
presented at CALICO (Amaral, Metcalf and Meur-
ers, 2006) and EUROCALL (Metcalf and Meurers,
2006) was implemented in Python, the current sys-
tem is Java-based, with all NLP being integrated in
the UIMA framework (Ferrucci and Lally, 2004).
UIMA is an architecture for the management and
analysis of unstructured information such as text,
which is built on the idea of referential annotation
and can be seen as an NLP analysis counterpart
to current stand-off encoding standards for anno-
tated corpora (cf., e.g., Ide et al 2000). The input
we are developing a Firefox plugin, leaving only the NLP up to
the server. This increases compatibility with web pages using
dynamically generated contents and special session handling.
2http://purl.org/icall/werti-v1
11
can be monotonically enriched while passing from
one NLP component to the next, using a flexible
data repository common to all components (Go?tz
and Suhre, 2004). Such annotation-based processing
is particularly useful in the WERTi context, where
keeping the original text intact is essential for dis-
playing it in enhanced form.
A second benefit of using the UIMA framework is
that it supports a flexible combination of individual
NLP components into larger processing pipelines.
To obtain a flexible approach to input enhancement
in WERTi, we need to be able to identify and an-
alyze phenomena from different levels of linguistic
analysis. For example, lexical classes can be iden-
tified by a POS tagger, whereas other patterns to be
enhanced require at least shallow syntactic chunk-
ing. The more diverse the set of phenomena, the
less feasible it is to handle all of them within a
single processing strategy or formalism. Using the
UIMA framework, we can re-use the same basic
processing (e.g., tokenizing, POS tagging) for all
phenomena and still be able to branch into pattern-
specific NLP in a demand-driven way. Given that
NLP components in UIMA include self-describing
meta-information, the processing pipeline to be run
can dynamically be obtained from the module con-
figuration instead of being hard-wired into the core
system. The resulting extensible, plugin-like archi-
tecture seems particularly well-suited for the task of
visual input enhancement of a wide range of hetero-
geneous language properties.
Complementing the above arguments for the
UIMA-based architecture of the current WERTi sys-
tem, a detailed discussion of the advantages of an
annotation-based, demand-driven NLP architecture
for Intelligent Computer-Assisted Language Learn-
ing can be found in Amaral, Meurers, and Ziai (To
Appear), where it is employed in an Intelligent Lan-
guage Tutoring System.
2.1 Implemented Modules
The modules implemented in the current system
handle a number of phenomena commonly judged
as difficult for second language learners of English.
In the following we briefly characterize each mod-
ule, describing the nature of the language pattern,
the required NLP, and the input enhancement results,
which will be referred to as activities.
Lexical classes
Lexical classes are the most basic kind of linguis-
tic category we use for input enhancement. The in-
ventory of lexical categories to be used and which
ones to focus on should be informed by second
language acquisition research and foreign language
teaching needs. The current system focuses on func-
tional elements such as prepositions and determiners
given that they are considered to be particularly dif-
ficult for learners of English (cf. De Felice, 2008 and
references therein).
We identify these functional elements using the
LingPipe POS tagger (http://alias-i.com/
lingpipe) employing the Brown tagset (Francis
and Kucera, 1979). As we show in section 2.2, the
tagger reliably identifies prepositions and determin-
ers in native English texts such as those expected for
input enhancement.
The input enhancement used for lexical classes is
the default set of activities provided by WERTi. In
the simplest case, Color, all automatically identified
instances in the web page are highlighted by color-
ing them; no learner interaction is required. This is
illustrated by Figure 2, which shows the result of en-
hancing prepositions in a web page from the British
Figure 2: Screenshot of color activity for prepositions, cf.
http://purl.org/icall/werti-color-ex
12
newspaper The Guardian.3
In this and the following screenshots, links al-
ready present in the original web page appear in light
blue (e.g., Vauban in Germany). This raises an im-
portant issue for future research, namely how to de-
termine the best visual input enhancement for a par-
ticular linguistic pattern given a specific web page
with its existing visual design features (e.g., bold-
facing in the text or particular colors used to indicate
links), which includes the option of removing or al-
tering some of those original visual design features.
A more interactive activity type is Click, where
the learner during reading can attempt to identify in-
stances of the targeted language form by clicking on
it. Correctly identified instances are colored green
by the system, incorrect guesses red.
Thirdly, input can be turned into Practice activi-
ties, where in its simplest form, WERTi turns web
pages into fill-in-the-blank activities and provides
immediate color coded feedback for the forms en-
tered by the learner. The system currently accepts
only the form used in the original text as correct.
In principle, alternatives (e.g., other prepositions)
can also be grammatical and appropriate. The ques-
tion for which cases equivalence classes of target an-
swers can automatically be determined is an interest-
ing question for future research.4
Gerunds vs. to-infinitives
Deciding when a verb is required to be realized as
a to-infinitive and when as a gerund -ing form can be
difficult for ESL learners. Current school grammars
teach students to look for certain lexical clues that
reliably indicate which form to choose. Examples
of such clues are prepositions such as after and of,
which can only be followed by a gerund.
In our NLP approach to this language pattern, we
use Constraint Grammar rules (Karlsson et al, 1995)
on top of POS tagging, which allow for straightfor-
ward formulation of local disambiguation rules such
as: ?If an -ing form immediately follows the prepo-
sition by, select the gerund reading.? Standard POS
3Given the nature of the input enhancement using colors, the
highlighting in the figure is only visible in a color printout.
4The issue bears some resemblance to the task of identify-
ing paraphrases (Androutsopoulos and Malakasiotis, 2009) or
classes of learner answers which differ in form but are equiva-
lent in terms of meaning (Bailey and Meurers, 2008).
tagsets for English contain a single tag for all -ing
forms. In order to identify gerunds only, we in-
troduce all possible readings for all -ing forms and
wrote 101 CG rules to locally disambiguate them.
The to-infinitives, on the other hand, are relatively
easy to identify based on the surface form and re-
quire almost no disambiguation.
For the implementation of the Constraint Gram-
mar rules, we used the freely available CG3 system.5
While simple local disambiguation rules are suffi-
cient for the pattern discussed here, through iterative
application of rules, Constraint Grammar can iden-
tify a wide range of phenomena without the need to
provide a full grammatical analysis.
The Color activity resulting from input enhance-
ment is similar to that for lexical classes described
above, but the system here enhances both verb forms
and clue phrases. Figure 3 shows the system high-
lighting gerunds in orange, infinitives in purple, and
clue phrases in blue.
Figure 3: Color activity for gerunds vs. to-infinitives, cf.
http://purl.org/icall/werti-color-ex2
For the Click activity, the web page is shown
with colored gerund and to-infinitival forms and the
learner can click on the corresponding clue phrases.
For the Practice activity, the learner is presented
with a fill-in-the-black version of the web page, as
in the screenshot in Figure 4. For each blank, the
learner needs to enter the gerund or to-infinitival
form of the base form shown in parentheses.
Wh-questions
Question formation in English, with its particu-
lar word order, constitutes a well-known challenge
for second language learners and has received sig-
nificant attention in the second language acquisi-
5http://beta.visl.sdu.dk/cg3.html
13
Figure 4: Practice activity for gerunds vs. to-infinitives,
cf. http://purl.org/icall/werti-cloze-ex
tion literature (cf., e.g., White et al, 1991; Spada
and Lightbown, 1993). Example (1) illustrates the
use of do-support and subject-aux inversion in wh-
questions as two aspects challenging learners.
(1) What do you think it takes to be successful?
In order to identify the wh-question patterns, we
employ a set of 126 hand-written Constraint Gram-
mar rules. The respective wh-word acts as the lex-
ical clue to the question as a whole, and the rules
then identify the subject and verb phrase based on
the POS and lexical information of the local context.
Aside from the Color activity highlighting the rel-
evant parts of a wh-question, we adapted the other
activity types to this more complex language pattern.
The Click activity prompts learners to click on either
the subject or the verb phrase of the question. The
Practice activity presents the words of a wh-question
in random order and requires the learner to rearrange
them into the correct one.
Conditionals
English has five types of conditionals that are used
for discussing hypothetical situations and possible
outcomes. The tenses used in the different condi-
tional types vary with respect to the certainty of the
outcome as expressed by the speaker/writer. For ex-
ample, one class of conditionals expresses high cer-
tainty and uses present tense in the if -clause and fu-
ture in the main clause, as in example (2).
(2) If the rain continues, we will return home.
The recognition of conditionals is approached us-
ing a combination of shallow and deep methods. We
first look for lexical triggers of a conditional, such as
the word if at the beginning of a sentence. This first
pass serves as a filter to the next, more expensive
processing step, full parsing of the candidate sen-
tences using Bikel?s statistical parser (Bikel, 2002).
The parse trees are then traversed to identify and
mark the verb forms and the trigger word.
For the input enhancement, we color all relevant
parts of a conditional, namely the trigger and the
verb forms. The Click activity for conditionals re-
quires the learner to click on exactly these parts. The
Practice activity prompts users to classify the condi-
tional instances into the different classes.
Phrasal verbs
Another challenging pattern for English language
learners are phrasal verbs consisting of a verb and
either a preposition, an adverb or both. The meaning
of a phrasal verb often differs considerably from that
of the underlying verb, as in (3) compared to (4).
(3) He switched the glasses without her noticing.
(4) He switched off the light before he went to bed.
This distinction is difficult for ESL learners, who
often confuse phrasal and non-phrasal uses.
Since this is a lexical phenomenon, we ap-
proached the identification of phrasal verbs via a
database lookup in a large online collection of verbs
known to occur in phrasal form.6 In order to find out
about noun phrases and modifying adverbs possibly
occurring in between the verb and its particles, we
run a chunker and use this information in specifying
a filter for such intervening elements.
The visual input enhancement activities targeting
phrasal verbs are the same as for lexical classes, with
the difference that for the Practice activity, learners
have to fill in only the particle, not the particle and
the main verb, since otherwise the missing contents
may be too difficult to reconstruct. Moreover, we
want the activity to focus on distinguishing phrasal
from non-phrasal uses, not verb meaning in general.
2.2 Evaluation issues
The success of a visual input enhancement approach
such as the one presented in this paper depends on
a number of factors, each of which can in principle
6http://www.usingenglish.com/reference/
phrasal-verbs
14
be evaluated. The fundamental but as far as we are
aware unanswered question in second language ac-
quisition research is for which language categories,
forms, and patterns input enhancement can be effec-
tive. As Lee and Huang (2008) show, the study of
visual input enhancement sorely needs more experi-
mental studies. With the help of the WERTi system,
which systematically produces visual input enhance-
ment for a range of language properties, it becomes
possible to conduct experiments in a real-life foreign
language teaching setting to test learning outcomes7
with and without visual input enhancement under a
wide range of parameters. Relevant parameters in-
clude the linguistic nature of the language property
to be enhanced as well as the nature of the input en-
hancement to be used, be it highlighting through col-
ors or fonts, engagement in different types of activi-
ties such as clicking, entering fill-in-the-blank infor-
mation, reordering language material, etc.
A factor closely related to our focus in this pa-
per is the impact of the quality of the NLP analysis.8
For a quantitative evaluation of the NLP, one signif-
icant problem is the mismatch between the phenom-
ena focused on in second language learning and the
available gold standards where these phenomena are
actually annotated. For example, standard corpora
such as the Penn Treebank contain almost no ques-
tions and thus do not constitute a useful gold stan-
dard for wh-question identification. Another prob-
lem is that some grammatical distinctions taught to
language learners are disputed in the linguistic liter-
ature. For example, Huddleston and Pullum (2002,
p. 1120) eliminate the distinction between gerunds
and present participles, combining them into a class
called ?gerund-participle?. And in corpus annota-
tion practice, gerunds are not identified as a class by
the tagsets used to annotate large corpora, making it
unclear what gold standard our gerund identification
component should be evaluated against.
While the lack of available gold standards means
that a quantitative evaluation of all WERTi mod-
ules is beyond the scope of this paper, the deter-
miner and preposition classes focused on in the lex-
ical classes module can be identified using the stan-
7Naturally, online measures of noticing, such as eye tracking
or Event-Related Potentials (ERP) would also be relevant.
8The processing time for the NLP analysis as other relevant
aspect is negligible for most of the activities presented here.
dard CLAWS-7 or Brown tagsets, for which gold-
standard corpora are available. We thus decided
to evaluate this WERTi module against the BNC
Sampler Corpus (Burnard, 1999), which contains
a variety of genres, making it particularly appro-
priate for evaluating a tool such as WERTi, which
learners are expected to use with a wide range of
web pages as input. The BNC Sampler corpus is
annotated with the fine-grained CLAWS-7 tagset9
where, e.g., prepositions are distinguished from sub-
ordinating conjunctions. By mapping the relevant
POS tags from the CLAWS-7 tagset to the Brown
tagset used by the LingPipe tagger as integrated in
WERTi, it becomes possible to evaluate WERTi?s
performance for the specific lexical classes focused
on for input enhancement, prepositions and deter-
miners. For prepositions, precision was 95.07% and
recall 90.52% while for determiners, precision was
97.06% with a recall of 94.07%.
The performance of the POS tagger on this refer-
ence corpus thus seems to be sufficient as basis for
visual input enhancement, but the crucial question
naturally remains whether identification of the target
patterns is reliable in the web pages that language
learners happen to choose. For a more precise quan-
titative study, it will thus be important to try the sys-
tem out with real-life users in order to identify a set
of web pages which can constitute an adequate test
set. Interestingly, which web pages the users choose
depends on the search engine front-end we provide
for them. As discussed under outlook in section 4,
we are exploring the option to implicitly guide them
towards web pages containing enough instances of
the relevant language patterns in text at the appro-
priate reading difficulty.
3 Context and related work
Contextualizing our work, one can view the auto-
matic visual input enhancement approach presented
here as an enrichment of Data-Driven Learning
(DDL). Where DDL has been characterized as an
?attempt to cut out the middleman [the teacher] as
far as possible and to give the learner direct access
to the data? (Boulton 2009, p. 82, citing Tim Johns),
in visual input enhancement the learner stays in con-
9http://www.natcorp.ox.ac.uk/docs/
c7spec.html
15
trol, but the NLP uses ?teacher knowledge? about rel-
evant and difficult language properties to make those
more prominent and noticeable for the learner.
In the context of Intelligent Computer-Assisted
Language Learning (ICALL), NLP has received
most attention in connection with Intelligent Lan-
guage Tutoring Systems, where NLP is used to ana-
lyze learner data and provide individual feedback on
that basis (cf. Heift and Schulze, 2007). Demands
on such NLP are high given that it needs to be able
to handle learner language and provide high-quality
feedback for any sentence entered by the learner.
In contrast, visual input enhancement makes use
of NLP analysis of authentic, native-speaker text and
thus applies the tools to the native language they
were originally designed and optimized for. Such
NLP use, which we will refer to as Authentic Text
ICALL (ATICALL), also does not need to be able
to correctly identify and manipulate all instances of
a language pattern for which input enhancement is
intended. Success can be incremental in the sense
that any visual input enhancement can be beneficial,
so that one can focus on enhancing those instances
which can be reliably identified in a text. In other
words, for ATICALL, precision of the NLP tools is
more important than recall. It is not necessary to
identify and enhance all instances of a given pattern
as long as the instances we do identify are in fact
correct, i.e., true positives. As the point of our sys-
tem is to enhance the reading experience by raising
language awareness, pattern occurrences we do not
identify are not harmful to the overall goal.10
We next turn to a discussion of some interest-
ing approaches in two closely related fields, exercise
generation and reading support tools.
3.1 Exercise Generation
Exercise generation is widely studied in CALL re-
search and some of the work relates directly to the
input enhancement approach presented in this paper.
For instance, Antoniadis et al (2004) describe the
plans of the MIRTO project to support ?gap-filling?
and ?lexical spotting? exercises in combination with
a corpus database. However, MIRTO seems to fo-
10While identifying all instances of a pattern indeed is not
crucial in this context, representativeness remains relevant to
some degree. Where only a skewed subset of a pattern is high-
lighted, learners may not properly conceptualize the pattern.
cus on a general architecture supporting instructor-
determined activity design. Visual input enhance-
ment or language awareness are not mentioned. The
VISL project (Bick, 2005) offers games and visual
presentations in order to foster knowledge of syntac-
tic forms and rules, and its KillerFiller tool can cre-
ate slot-filler exercises from texts. However, Killer-
Filler uses corpora and databases as the text base and
it presents sentences in isolation in a testing setup.
In contrast to such exercise generation systems, we
aim at enhancing the reader?s second language input
using the described web-based mash-up approach.
3.2 Reading Support Tools
Another branch of related approaches consists of
tools supporting the reading of texts in a foreign lan-
guage. For example, the Glosser-RuG project (Ner-
bonne et al, 1998) supports reading of French texts
for Dutch learners with an online, context-dependent
dictionary, as well as morphological analysis and ex-
amples of word use in corpora. A similar system,
focusing on multi-word lexemes, was developed in
the COMPASS project (Breidt and Feldweg, 1997).
More recently, the ALPHEIOS project11 has pro-
duced a system that can look up words in a lexi-
con and provide aligned translations. While such
lexicon-based tools are certainly useful to learners,
they rely on the learner asking for help instead of
enhancing specific structures from the start and thus
clearly differ from our approach.
Finally, the REAP project12 supports learners in
searching for texts that are well-suited for provid-
ing vocabulary and reading practice (Heilman et al,
2008). While it differs in focus from the visual input
enhancement paradigm underlying our approach, it
shares with it the emphasis on providing the learner
with authentic text in support of language learning.
4 Conclusion and Outlook
In this paper we presented an NLP architecture and
a concrete system for the enhancement of authen-
tic web pages in order to support language aware-
ness in ESL learners. The NLP architecture is flexi-
ble enough to integrate any processing approach that
lends itself to the treatment of the language phe-
11http://alpheios.net
12http://reap.cs.cmu.edu
16
nomenon in question, without confining the devel-
oper to a particular formalism. The WERTi system
illustrates this with five language patterns typically
considered difficult for ESL learners: lexical classes,
gerunds vs. to-infinitives, wh-questions, condition-
als and phrasal verbs.
Looking ahead, we already mentioned the funda-
mental open question where input enhancement can
be effective in section 2.2. A system such as WERTi,
systematically producing visual input enhancement,
can help explore this question under a wide range of
parameters in a real-life language teaching setting.
A more specific future research issue is the auto-
matic computation of equivalence classes of target
forms sketched in section 2.1. Not yet mentioned
but readily apparent is the goal to integrate more
language patterns known to be difficult for language
learners into WERTi (e.g., active/passive, tense and
aspect distinctions, relative clauses), and to explore
the approach for other languages, such as German.
A final important avenue for future research con-
cerns the starting point of the system, the step where
learners search for a web page they are interested
in and select it for presentation with input enhance-
ment. Enhancing of patterns presupposes that the
pages contain instances of the pattern. The less
frequent the pattern, the less likely we are to find
enough instances of it in web pages returned by the
standard web search engines typically used by learn-
ers to find pages of interest to them. The issue is re-
lated to research on providing learners with texts at
the right level of reading difficulty (Petersen, 2007;
Miltsakaki and Troutt, 2008), but the focus for us
is on ensuring that texts which include instances of
the specific language pattern targeted by a given in-
put enhancement are ranked high in the search re-
sults. Ott (2009) presents a search engine prototype
which, in addition to the content-focused document-
term information and traditional readability mea-
sures, supports indexing based on a more general no-
tion of a text model into which the patterns relevant
to input enhancement can be integrated ? an idea we
are exploring further (Ott and Meurers, Submitted).
Acknowledgments
We benefited from the feedback we received at
CALICO 06, EUROCALL 06, and the ICALL
course13 at ESSLLI 09, where we discussed our
work on the Python-based WERTi prototype. We
would like to thank Chris Hill and Kathy Corl
for their enthusiasm and encouragement. We are
grateful to Magdalena Leshtanska, Emma Li, Iliana
Simova, Maria Tchalakova and Tatiana Vodolazova
for their good ideas and WERTi module contribu-
tions in the context of a seminar at the University of
Tu?bingen in Summer 2008. Last but not least, the
paper benefited from two helpful workshop reviews.
References
Luiz Amaral, Vanessa Metcalf, and Detmar Meur-
ers. 2006. Language awareness through re-use
of NLP technology. Presentation at the CALICO
Workshop on NLP in CALL ? Computational and
Linguistic Challenges, May 17, 2006. University
of Hawaii. http://purl.org/dm/handouts/
calico06-amaral-metcalf-meurers.pdf.
Luiz Amaral, Detmar Meurers, and Ramon Ziai.
To Appear. Analyzing learner language: To-
wards a flexible NLP architecture for intelligent
language tutors. Computer-Assisted Language
Learning. http://purl.org/dm/papers/
amaral-meurers-ziai-10.html.
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entailment
methods. Technical report, NLP Group, Informatics
Dept., Athens University of Economics and Business,
Greece. http://arxiv.org/abs/0912.3747.
Georges Antoniadis, Sandra Echinard, Olivier Kraif,
Thomas Lebarbe?, Mathieux Loiseau, and Claude Pon-
ton. 2004. NLP-based scripting for CALL activities.
In Proceedings of the COLING Workshop on eLearn-
ing for CL and CL for eLearning, Geneva.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In (Tetreault et al, 2008), pages
107?115.
Eckhard Bick. 2005. Grammar for fun: IT-based gram-
mar learning with VISL. In P. Juel, editor, CALL for
the Nordic Languages, pages 49?64. Samfundslitter-
atur, Copenhagen.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second Int. Conference on Human
Language Technology Research, San Francisco.
Alex Boulton. 2009. Data-driven learning: Reasonable
fears and rational reassurance. Indian Journal of Ap-
plied Linguistics, 35(1):81?106.
13http://purl.org/dm/09/esslli/
17
Elisabeth Breidt and Helmut Feldweg. 1997. Accessing
foreign languages with COMPASS. Machine Transla-
tion, 12(1?2):153?174.
L. Burnard, 1999. Users Reference Guide for the BNC
Sampler. Available on the BNC Sampler CD.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, St Catherine?s Col-
lege, University of Oxford.
Catherine Doughty and J. Williams, editors. 1998. Fo-
cus on form in classroom second language acquisition.
Cambridge University Press, Cambridge.
David Ferrucci and Adam Lally. 2004. UIMA: an ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3?4):327?348.
W. Nelson Francis and Henry Kucera, 1979. Brown cor-
pus manual. Dept. of Linguistics, Brown University.
Thilo Go?tz and Oliver Suhre. 2004. Design and im-
plementation of the UIMA Common Analysis System.
IBM Systems Journal, 43(3):476?489.
Trude Heift and Mathias Schulze. 2007. Errors and In-
telligence in Computer-Assisted Language Learning:
Parsers and Pedagogues. Routledge.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008. Retrieval of reading materials for vocab-
ulary and reading practice. In (Tetreault et al, 2008),
pages 80?88.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard for
linguistic corpora. In Proceedings of the 2nd Int. Con-
ference on Language Resources and Evaluation.
Tim Johns. 1994. From printout to handout: Grammar
and vocabulary teaching in the context of data-driven
learning. In T. Odlin, editor, Perspectives on Pedagog-
ical Grammar, pages 293?313. CUP, Cambridge.
Fred Karlsson, Atro Voutilainen, Juha Heikkila?, and
Arto Anttila, editors. 1995. Constraint Grammar:
A Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter, Berlin, New York.
Sang-Ki Lee and Hung-Tzu Huang. 2008. Visual in-
put enhancement and grammar learning: A meta-
analytic review. Studies in Second Language Acqui-
sition, 30:307?331.
Patsy M. Lightbown and Nina Spada. 1999. How lan-
guages are learned. Oxford University Press, Oxford.
Patsy M. Lightbown. 1998. The importance of timing
in focus on form. In (Doughty and Williams, 1998),
pages 177?196.
Michael H. Long and Peter Robinson. 1998. Focus on
form: Theory, research, and practice. In (Doughty and
Williams, 1998), pages 15?41.
M. H. Long. 1991. Focus on form: A design feature
in language teaching methodology. In K. De Bot,
C. Kramsch, and R. Ginsberg, editors, Foreign lan-
guage research in cross-cultural perspective, pages
39?52. John Benjamins, Amsterdam.
Vanessa Metcalf and Detmar Meurers. 2006.
Generating web-based English preposition
exercises from real-world texts. Presenta-
tion at EUROCALL, Sept. 7, 2006. Granada,
Spain. http://purl.org/dm/handouts/
eurocall06-metcalf-meurers.pdf.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading diffi-
culty. In (Tetreault et al, 2008), pages 89?97.
John Nerbonne, Duco Dokter, and Petra Smit. 1998.
Morphological processing and computer-assisted lan-
guage learning. Computer Assisted Language Learn-
ing, 11(5):543?559.
Niels Ott and Detmar Meurers. Submitted. Information
retrieval for education: Making search engines lan-
guage aware. http://purl.org/dm/papers/
ott-meurers-10.html.
Niels Ott. 2009. Information retrieval for language learn-
ing: An exploration of text difficulty measures. Mas-
ter?s thesis, International Studies in Computational
Linguistics, University of Tu?bingen.
Sarah E. Petersen. 2007. Natural Language Processing
Tools for Reading Level Assessment and Text Simplifi-
cation for Bilingual Education. Ph.D. thesis, Univer-
sity of Washington.
William E. Rutherford and Michael Sharwood Smith.
1985. Consciousness-raising and universal grammar.
Applied Linguistics, 6(2):274?282.
Richard W. Schmidt. 1990. The role of conscious-
ness in second language learning. Applied Linguistics,
11:206?226.
Michael Sharwood Smith. 1993. Input enhancement in
instructed SLA: Theoretical bases. Studies in Second
Language Acquisition, 15:165?179.
Nina Spada and Patsy M. Lightbown. 1993. Instruction
and the development of questions in l2 classrooms.
Studies in Second Language Acquisition, 15:205?224.
Joel Tetreault, Jill Burstein, and Rachele De Felice, ed-
itors. 2008. Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications. ACL, Columbus, Ohio, June.
Lydia White, Nina Spada, Patsy M. Lightbown, and Leila
Ranta. 1991. Input enhancement and L2 question for-
mation. Applied Linguistics, 12(4):416?432.
18
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 1?9,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Evaluating Answers to Reading Comprehension Questions in Context:
Results for German and the Role of Information Structure
Detmar Meurers Ramon Ziai Niels Ott Janina Kopp
Seminar fu?r Sprachwissenschaft / SFB 833
Universita?t Tu?bingen
Wilhelmstra?e 19 / Nauklerstra?e 35
72074 Tu?bingen, Germany
{dm,rziai,nott,jkopp}@sfs.uni-tuebingen.de
Abstract
Reading comprehension activities are an au-
thentic task including a rich, language-based
context, which makes them an interesting real-
life challenge for research into automatic con-
tent analysis. For textual entailment research,
content assessment of reading comprehension
exercises provides an interesting opportunity
for extrinsic, real-purpose evaluation, which
also supports the integration of context and
task information into the analysis.
In this paper, we discuss the first results for
content assessment of reading comprehension
activities for German and present results which
are competitive with the current state of the
art for English. Diving deeper into the results,
we provide an analysis in terms of the differ-
ent question types and the ways in which the
information asked for is encoded in the text.
We then turn to analyzing the role of the ques-
tion and argue that the surface-based account
of information that is given in the question
should be replaced with a more sophisticated,
linguistically informed analysis of the informa-
tion structuring of the answer in the context of
the question that it is a response to.
1 Introduction
Reading comprehension exercises offer a real-life
challenge for the automatic analysis of meaning.
Given a text and a question, the content assessment
task is to determine whether the answer given to a
reading comprehension question actually answers
the question or not. Such reading comprehension
exercises are a common activity in foreign language
teaching, making it possible to use activities which
are authentic and for which the language teachers
provide the gold standard judgements.
Apart from the availability of authentic exercises
and independently motivated gold standard judge-
ments, there are two further reasons for putting read-
ing comprehension tasks into the spotlight for au-
tomatic meaning analysis. Firstly, such activities
include a text as an explicit context on the basis of
which the questions are asked. Secondly, answers to
reading comprehension questions in foreign language
teaching typically are between a couple of words and
several sentences in length ? too short to rely purely
on the distribution of lexical material (as, e.g., in
LSA, Landauer et al, 1998). The answers also ex-
hibit a significant variation in form, including a high
number of form errors, which makes it necessary to
develop an approach which is robust enough to de-
termine meaning correspondences in the presence of
errors yet flexible enough to support the rich vari-
ation in form which language offers for expressing
related meanings.
There is relatively little research on content assess-
ment for reading comprehension tasks and it so far
has focused exclusively on English, including both
reading comprehension questions answered by na-
tive speakers (Leacock and Chodorow, 2003; Nielsen
et al, 2009) and by language learners (Bailey and
Meurers, 2008). The task is related to the increas-
ingly popular strand of research on Recognizing Tex-
tual Entailment (RTE, Dagan et al, 2009) and the
Answer Validation Exercise (AVE, Rodrigo et al,
2009), which both have also generally targeted En-
glish.
1
The RTE challenge abstracts away from concrete
tasks to emphasize the generic semantic inference
component and it has significantly advanced the field
under this perspective. At the same time, an inves-
tigation of the role of the context under which an
inference holds requires concrete tasks, for which
content assessment of reading comprehension tasks
seems particularly well-suited. Borrowing the ter-
minology Spa?rck Jones (2007) coined in the context
of evaluating automatic summarization systems, one
can say that we pursue an extrinsic, full-purpose eval-
uation of aspects of textual inference. The content
assessment task provides two distinct opportunities
to investigate textual entailment: On the one hand,
one can conceptualize it as a textual inference task
of deciding whether a given text T supports a partic-
ular student answer H . On the other hand, if target
answers are provided by the teachers, the task can be
seen as a special bi-directional case of textual entail-
ment, namely a paraphrase recognition task compar-
ing the student answers to the teacher target answers.
In this paper, we focus on this second approach.
The aim of this paper is twofold. On the one hand,
we want to present the first content assessment ap-
proach for reading comprehension activities focusing
on German. In the discussion of the results, we will
highlight the impact of the question types and the
way in which the information asked for is encoded
in the text. On the other hand, we want to discuss
the importance of the explicit language-based context
and how an analysis of the question and the way a
text encodes the information being asked for can help
advance research on automatic content assessment.
Overall, the paper can be understood as a step in the
long-term agenda of exploring the role and impact
of the task and the context on the automatic analysis
and interpretation of natural language.
2 Data
The experiments described in this paper are based
on the Corpus of Reading comprehension Exercises
in German (CREG), which is being collected in col-
laboration with two large German programs in the
US, at Kansas University (Prof. Nina Vyatkina) and
at The Ohio State University (Prof. Kathryn Corl).
German teachers are using the WEb-based Learner
COrpus MachinE (WELCOME, Meurers et al, 2010)
interface to enter the regular, authentic reading com-
prehension exercises used in class, which are thereby
submitted to a central corpus repository. These exer-
cises consist of texts, questions, target answers, and
corresponding student answers. Each student answer
is transcribed from the hand-written submission by
two independent annotators. These two annotators
then assess the contents of the answers with respect
to meaning: Did the student provide a meaningful
answer to the question? In this binary content as-
sessment one thus distinguishes answers which are
appropriate from those which are inappropriate in
terms of meaning, independent of whether the an-
swers are grammatically well-formed or not.
From the collected data, we selected an even dis-
tribution of unique appropriate and inappropriate stu-
dent answers in order to obtain a 50% random base-
line for our system. Table 1 lists how many questions,
target answers and student answers each of the two
data sets contains. The data used for this paper is
made freely available upon request under a standard
Creative Commons by-nc-sa licence.1
KU data set OSU data set
Target Answers 136 87
Questions 117 60
Student Answers 610 422
# of Students 141 175
avg. Token # 9.71 15.00
Table 1: The reading comprehension data sets used
3 Approach
Our work builds on the English content assessment
approach of Bailey and Meurers (2008), who pro-
pose a Content Assessment Module (CAM) which
automatically compares student answers to target re-
sponses specified by foreign language teachers. As a
first step we reimplemented this approach for English
in a system we called CoMiC (Comparing Mean-
ing in Context) which is discussed in Meurers et al
(2011). This reimplementation was then adapted
for German, resulting in the CoMiC-DE system pre-
sented in this paper.
The comparison of student answers and target an-
swer is based on an alignment of tokens, chunks, and
1http://creativecommons.org/licenses/by-nc-sa/3.0/
2
dependency triples between the student and the target
answer at different levels of abstraction. Figure 1
shows a simple example including token-level and
chunk-level alignments between the target answer
(TA) and the student answer (SA).
Figure 1: Basic example for alignment approach
As the example suggests, it is not sufficient to align
only identical surface forms given that significant lex-
ical and syntactic variation occurs in typical student
answers. Alignment thus is supported at different
levels of abstraction. For example, the token units
are enriched with lemma and synonym information
using standard NLP tools. Table 2 gives an overview
of which NLP tools we use for which task in CoMiC-
DE. In general, the components are very similar to
those used in the English system, with different sta-
tistical models and parameters where necessary.
Annotation Task NLP Component
Sentence Detection OpenNLP
http://incubator.apache.org/opennlp
Tokenization OpenNLP
Lemmatization TreeTagger (Schmid, 1994)
Spell Checking Edit distance (Levenshtein, 1966),
igerman98 word list
http://www.j3e.de/ispell/igerman98
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking OpenNLP
Lexical Relations GermaNet (Hamp and Feldweg, 1997)
Similarity Scores PMI-IR (Turney, 2001)
Dependency Relations MaltParser (Nivre et al, 2007)
Table 2: NLP tools used in the German system
Integrating the multitude of units and their rep-
resentations at different levels of abstraction poses
significant challenges to the system architecture.
Among other requirements, different representations
of the same surface string need to be stored without
interfering with each other, and various NLP tools
need to collaborate in order to produce the final rich
data structures used for answer comparison. To meet
these requirements, we chose to implement our sys-
tem in the Unstructured Information Management
Architecture (UIMA, cf. Ferrucci and Lally, 2004).
UIMA allows automatic analysis modules to access
layers of stand-off annotation, and hence allows for
the coexistence of both independent and interdepen-
dent annotations, unlike traditional pipeline-style ar-
chitectures, where the output of each component re-
places its input. The use of UIMA in recent success-
ful large-scale projects such as DeepQA (Ferrucci
et al, 2010) confirms that UIMA is a good candi-
date for complex language processing tasks where
integration of various representations is required.
In order to determine the global alignment con-
figuration, all local alignment options are computed
for every mappable unit. These local candidates are
then used as input for the Traditional Marriage Al-
gorithm (Gale and Shapley, 1962) which computes a
global alignment solution where each mappable unit
is aligned to at most one unit in the other response,
such as the one we saw in Figure 1.
On the basis of the resulting global alignment con-
figuration, the system performs the binary content
assessment by evaluating whether the meaning of the
learner and the target answer are sufficiently similar.
For this purpose, it extracts features which encode
the numbers and types of alignment and feeds them
to the memory-based classifier TiMBL (Daelemans
et al, 2007). The features used are listed in Table 3.
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned target/learner tokens
4./5. Chunk Overlap Percent of aligned target/learner chunks
6./7. Triple Overlap Percent of aligned target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of token-level
(0-5) alignments
Table 3: Features used for the memory-based classifier
3
4 Content Assessment Experiment
4.1 Setup
We ran our content assessment experiment using
the two data sets introduced in section 2, one from
Kansas University and the other from The Ohio State
University. Both of these contain only records where
both annotators agreed on the binary assessment (ap-
propriate/inappropriate meaning). Each set is bal-
anced, i.e., they contain the same number of appro-
priate and inappropriate student answers.
In training and testing the TiMBL-based classi-
fier, we followed the methodology of Bailey (2008,
p. 240), where seven classifiers are trained using the
different available distance metrics (Overlap, Leven-
shtein, Numeric Overlap, Modified value difference,
Jeffrey divergence, Dot product, Cosine). Training
and testing was performed using the leave-one-out
scheme (Weiss and Kulikowski, 1991) and for each
item the output of the seven classifiers was combined
via majority voting.
4.2 Results
The classification accuracy for both data sets is sum-
marized in Table 4. We report accuracy and the total
number of answers for each data set.
KU data set OSU data set
# of answers 610 422
Accuracy 84.6% 84.6%
Table 4: Classification accuracy for the two data sets
The 84.6% accuracy figure obtained for both data
sets shows that CoMiC-DE is quite successful in
performing content assessment for the German data
collected so far, a result which is competitive with
the one for English obtained by Bailey and Meurers
(2008), who report an accuracy of 78% for the binary
assessment task on a balanced English data set.
A remarkable feature is the identity of the scores
for the two data sets, considering that the data was
collected at different universities from different stu-
dents in different classes run by different teachers.
Moreover, there was no overlap in exercise material
between the two data sets. This indicates that there
is some characteristic uniformity of the learner re-
sponses in authentic reading comprehension tasks,
suggesting that the course setting and task type effec-
tively constrains the degree of syntactic and lexical
variation in the student answers. This includes the
stage of the learners in this foreign language teaching
setting, which limits their exposure to linguistic con-
structions, as well as the presence of explicit reading
texts that the questions are about, which may lead
learners to use the lexical material provided instead
of rephrasing content in other words. We intend to ex-
plore these issues in our future work to obtain a more
explicit picture of the contextual and task properties
involved.
Another aspect which should be kept in mind is
that the scores we obtained are based on a data set
for which the two human annotators had agreed on
their assessment. We expect automatic classification
results to degrade given more controversial data about
which human annotators disagree, especially since
such data will presumably contain more ambiguous
cues, giving rise to multiple interpretations.
4.3 Evaluation by question type
The overall results include many different question
types which pose different kinds of challenges to
our system. To develop an understanding of those
challenges, we performed a more fine-grained evalu-
ation by question types. To distinguish relevant sub-
cases, we applied the question classification scheme
introduced by Day and Park (2005). This scheme is
more suitable here than other common answer-typing
schemata such as the one in Li and Roth (2002),
which tend to focus on questions asking for factual
knowledge.
Day and Park (2005) distinguish five different
question forms: yes/no (question to be answered
with either yes or no), alternative (two or more
yes/no questions connected with or), true or false
(a statement to be classified as true or false),
who/what/when/where/how/why (wh-question con-
taining the respective question word), and multiple
choice (choice between several answers presented
with a question, of any other question type). In addi-
tion, they introduce a second dimension distinguish-
ing the types of comprehension involved, i.e., how
the information asked for by the question can be ob-
tained from the text: literal (questions that can be an-
swered directly and explicitly from the text), reorga-
nization (questions where information from various
4
parts of the text must be combined), inference (ques-
tions where literal information and world knowledge
must be combined), prediction (prediction of how
a story might continue), evaluation (comprehensive
judgement about aspects of the text) and personal
response (personal opinion or feelings about the text
or the subject).
Out of the five different forms of question, our
data contains questions of all forms except for the
multiple choice category and the true or false cate-
gory given that we are explicitly targeting free text
responses. To obtain a more detailed picture of the
wh-question category, we decided to split that cat-
egory into its respective wh-words and added one
more category to it, for which. Also, we added the
type ?several? for questions which contain more than
one question presented to the student at a time. Of the
six comprehension types, our data contained literal,
reorganization and inference questions.
Table 5 reports the accuracy results by question
forms and comprehension types for the combined
OSU and KU data set. The counts encode the num-
ber of student answers for which accuracy is reported
(micro-averages). The numbers in brackets specify
the number of distinct questions and the correspond-
ing accuracy measures are computed by grouping
answers by their question (macro-averages). Com-
paring answer-based (micro-average) accuracy with
question-based (macro-average) accuracy allows us
to see whether the results for questions with a high
number of answers outweigh questions with a small
number of answers. In general the micro- and macro-
averages reported are very similar and the overall
accuracy is the same (84.6%). Overall, the results
thus do not seem to be biased towards a specific, fre-
quently answered question instance. Where larger
differences between micro- and macro-averages do
arise, as for alternative, when, and where questions,
these are cases with few overall instances in the data
set, cautioning us against overinterpreting results for
such small subsets. The 4.2% gap for the relatively
frequent ?several? question type underlines the het-
erogeneous nature of this class, which may warrant
more specific subclasses in the future.
Overall, the accuracy of content assessment for
wh-questions that can be answered with a concrete
piece of information from the text are highest, with
92.6% for ?which? questions, and results in the upper
80s for five other wh-questions. Interestingly, ?who?
questions fare comparatively badly, pointing to a rel-
atively high variability in the expression of subjects,
which would warrant the integration of a dedicated
approach to coreference resolution. Such a direct so-
lution is not available for ?why? questions, which at
79.3% is the worst wh-question type. The high vari-
ability of those answers is rooted in the fact that they
ask for a cause or reason, which can be expressed in
a multitude of ways, especially for comprehension
types involving inferences or reorganization of the
information given in the text.
This drop between comprehension types, from lit-
eral (86.0%) to inference (81.5%) and reorganization
(78.0%), can also be observed throughout and is ex-
pected given that the CoMiC-DE system makes use
of surface-based alignments where it can find them.
For the system to improve on the non-literal com-
prehension types, features encoding a richer set of
abstractions (e.g., to capture distributional similarity
at the chunk level or global linguistic phenomena
such as negation) need to be introduced.
Just as in the discussion of the micro- and macro-
averages above, the ?several? question type again
rears its ugly heads in terms of a low overall accuracy
(77.7%). This supports the conclusion that it requires
a dedicated approach. Based on an analysis of the
nature and sequence of the component questions, in
future work we plan to determine how such combi-
nations constrain the space of variation in acceptable
answers.
Finally, while there are few instances for the ?al-
ternative? question type, the fact that it resulted in
the lowest accuracy (57.1%) warrants some attention.
The analysis indeed revealed a general issue, which
is discussed in the next section.
5 From eliminating repeated elements to
analyzing information structure
Bailey (2008, sec. 5.3.12) observed that answers fre-
quently repeat words given in the question. In her cor-
pus example (1), the first answer repeats ?the moral
question raised by the Clinton incident? from the
question, whereas the second one reformulates this
given material. But both sentences essentially answer
the question in the same way.2
2Independent of the issue discussed here, note the presuppo-
5
Comprehension type
Literal Reorganization Inference Total
Question type Acc. # Acc. # Acc. # Acc. #
Alternative 0 1 (1) ? 0 66.7 (58.3) 6 (3) 57.1 (43.8) 7 (4)
How 85.7 (83.3) 126 (25) 83.3 (77.8) 12 (3) 100 7 (1) 86.2 (83.3) 145 (29)
What 87.0 (87.6) 247 (40) 74.2 (71.7) 31 (4) 83.3 (83.3) 6 (1) 85.6 (86.1) 284 (45)
When 85.7 (93.3) 7 (3) ? 0 ? 0 85.7 (93.3) 7 (3)
Where 88.9 (94.4) 9 (3) ? 0 ? 0 88.9 (94.4) 9 (3)
Which 92.3 (90.7) 183 (29) 100.0 14 (5) 83.3 (83.3) 6 (2) 92.6 (91.6) 203 (36)
Who 73.9 (80.2) 23 (9) 94.4 (88.9) 18 (3) ? 0 82.9 (82.4) 41 (12)
Why 80.5 (83.3) 128 (17) 57.1 (57.9) 14 (3) 84.4 (81.1) 32 (4) 79.3 (79.7) 174 (24)
Yes/No ? 0 100.0 5 (1) ? 0 100.0 5 (1)
Several 82.1 (85.6) 95 (13) 68.4 (75.1) 38 (5) 75 (74.3) 24 (2) 77.7 (81.9) 157 (20)
Total 86.0 (86) 819 (140) 78.0 (80.7) 132 (24) 81.5 (76.8) 81 (13) 84.6 (84.6) 1032 (177)
Table 5: Accuracy by question form and comprehension types following Day and Park (2005). Counts denoting number
of student answers, in brackets: number of questions and macro-average accuracy computed by grouping by questions.
(1) What was the major moral question raised by
the Clinton incident?
a. The moral question raised by the Clinton
incident was whether a politician?s person
life is relevant to their job performance.
b. A basic question for the media is whether
a politician?s personal life is relevant to his
or her performance in the job.
The issue arising from the occurrence of such
given material for a content assessment approach
based on alignment is that all alignments are counted,
yet those for given material do not actually con-
tribute to answering the question, as illustrated by
the (non)answer containing only given material ?The
moral question raised by the Clinton incident was
whatever.? Bailey (2008) concludes that an answer
should not be rewarded (or punished) for repeating
material that is given in the question and her imple-
mentation thus removes all words from the answers
which are given in the question.
While such an approach successfully eliminates
any contribution from these given words, it has the un-
fortunate consequence that any NLP processes requir-
ing well-formed complete sentences (such as, e.g.,
dependency parsers) perform poorly on sentences
from which the given words have been removed. In
our reimplementation of the approach, we therefore
kept the sentences as such intact and instead made
sition failure arising for this authentic reading comprehension
question ? as far as we see, there was no ?major moral question
raised by the Clinton incident?.
use of the UIMA architecture to add a givenness
annotation to those words of the answer which are
repeated from the question. Such given tokens and
any representations derived from them are ignored
when the local alignment possibilities are computed.
While successfully replicating the givenness filter
of Bailey (2008) without the negative consequences
on other NLP analysis, targeting given words in this
way is problematic, which becomes particularly ap-
parent when considering examples for the ?alterna-
tive? question type. In this question type, exemplified
in Figure 2 by an example from the KU data set, the
answer has to select one of the options from an ex-
plicitly given set of alternatives.
Q: Ist die Wohnung in einem Neubau oder einem Altbau?
?Is the flat in a new building or in an old building??
TA: Die
The
Wohnung
flat
ist
is
in
in
einem
a
Neubau
new building
.
SA: Die
The
Wohnung
flat
ist
is
in
in
einem
a
Neubau
new building
Figure 2: ?Alternative? question with answers consisting
entirely of given words, resulting in no alignments.
The question asks whether the apartment is in a
new or in an old building, and both alternatives are
explicitly given in the question. The student picked
the same alternative as the one that was selected in
the target answer. Indeed, the two answers are iden-
tical, but the givenness filter excludes all material
from alignment and hence the content assessment
6
classification fails to identify the student answer as
appropriate. This clearly is incorrect and essentially
constitutes an opportunity to rethink the givenness
filter.
The givenness filter is based on a characterization
of the material we want to ignore, which was moti-
vated by the fact that it is easy to identify the material
that is repeated from the question. On the other hand,
if we analyze the reading comprehension questions
more closely, it becomes possible to connect this
issue to research in formal pragmatics which inves-
tigates the information structure (cf. Krifka, 2007)
imposed on a sentence in a discourse addressing
an explicit (or implicit) question under discussion
(Roberts, 1996). Instead of removing given elements
from an answer, under this perspective we want to
identify which part of an answer constitutes the so-
called focus answering the question.3
The advantage of linking our issue to the more
general investigation of information structure in lin-
guistics is readily apparent if we consider the signif-
icant complexity involved (cf., e.g., Bu?ring, 2007).
The issue of asking what constitutes the focus of a
sentence is distinct from asking what new informa-
tion is included in a sentence. New information can
be contained in the topic of a sentence. On the other
hand, the focus can also contain given information.
In (2a), for example, the focus of the answer is ?a
green apple?, even though apples are explicitly given
in the question and only the fact that a green one will
be bought is new.
(2) You?ve looked at the apples long enough now,
what do you want to buy?
a. I want to buy a green apple.
In some situations the focus can even consist en-
tirely of given information. This is one way of in-
terpreting what goes on in the case of the alternative
questions discussed at the end of the last section.
This question type explicitly mentions all alternatives
as part of the question, so that the focus of the an-
swer selecting one of those alternatives will typically
3The information structure literature naturally also provides
a more sophisticated account of givenness. For example, for
Schwarzschild (1999), givenness also occurs between hypernyms
and coreferent expressions, which would not be detected by the
simple surface-based givenness filter included in the current
CoMiC-DE.
consist entirely of given information.
As a next step we plan to build on the notion of
focus characterized in (a coherent subset of) the infor-
mation structure literature by developing an approach
which identifies the part of an answer which consti-
tutes the focus so that we can limit the alignment
procedure on which content assessment is based to
the focus of each answer.
6 Related Work
There are few systems targeting the short answer eval-
uation tasks. Most prominent among them is C-Rater
(Leacock and Chodorow, 2003), a short answer scor-
ing system for English meant for deployment in Intel-
ligent Tutoring Systems (ITS). The authors highlight
the fact that C-Rater is not simply a string matching
program but instead uses more sophisticated NLP
such as shallow parsing and synonym matching. C-
Rater reportedly achieved an accuracy of 84% in two
different studies, which is remarkably similar to the
scores we report in this paper although clearly the
setting and target language differ from ours.
More recently in the ITS field, Nielsen et al (2009)
developed an approach focusing on recognizing tex-
tual entailment in student answers. To that end, a
corpus of questions and answers was manually an-
notated with word-word relations, so-called ?facets?,
which represent individual semantic propositions in a
particular answer. By learning how to recognize and
classify these facets in student answers, the system
is then able to give a more differentiated rating of
a student answer than ?right? or ?wrong?. We find
that this is a promising move in the fields of answer
scoring and textual entailment since it also breaks
down the complex entailment problem into a set of
sub-problems.
7 Conclusion
We presented CoMiC-DE, the first content assess-
ment system for German. For the data used in evalu-
ation so far, CoMiC-DE performs on a competitive
level when compared to previous work on English,
with accuracy at 84.6%. In addition to these results,
we make our reading comprehension corpus freely
available for research purposes in order to encourage
more work on content assessment and related areas.
In a more detailed evaluation by question and com-
7
prehension type, we gained new insights into how
question types influence the content assessment tasks.
Specifically, our system had more difficulty classify-
ing answers to ?why?-questions than other question
forms, which we attribute to the fact that causal re-
lations exhibit more form variation than other types
of answer material. Also, the comprehension type
?reorganization?, which requires the reader to collect
and combine information from different places in the
text, posed more problems to our system than the
?literal? type.
Related to the properties of questions, we showed
by example that simply marking given material on
a surface level is insufficient and a partitioning into
focused and background material is needed instead.
This is especially relevant for alternative questions,
where the exclusion of all given material renders the
alignment process useless. Future work will therefore
include focus detection in answers and its use in the
alignment process. For example, given a weighting
scheme for individual alignments, focused material
could be weighted more prominently in alignment in
order to reflect its importance in assessing the answer.
Acknowledgements
We would like to thank two anonymous TextInfer
reviewers for their helpful comments.
References
Stacey Bailey, 2008. Content Assessment in Intelli-
gent Computer-Aided Language Learning: Mean-
ing Error Diagnosis for English as a Second Lan-
guage. Ph.D. thesis, The Ohio State University.
http://osu.worldcat.org/oclc/243467551.
Stacey Bailey and Detmar Meurers, 2008. Diagnos-
ing Meaning Errors in Short Answers to Read-
ing Comprehension Questions. In Proceedings
of the 3rd Workshop on Innovative Use of NLP
for Building Educational Applications (BEA-3)
at ACL?08. Columbus, Ohio, pp. 107?115. http:
//aclweb.org/anthology/W08-0913.
Daniel Bu?ring, 2007. Intonation, Semantics and In-
formation Structure. In Gillian Ramchand and
Charles Reiss (eds.), The Oxford Handbook of Lin-
guistic Interfaces, Oxford University Press.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK
Technical Report ILK 07-03. Version 6.0. Tilburg
University.
Ido Dagan, Bill Dolan, Bernardo Magnini and Dan
Roth, 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii.
Richard R. Day and Jeong-Suk Park, 2005. Develop-
ing Reading Comprehension Questions. Reading
in a Foreign Language, 17(1):60?73.
David Ferrucci, Eric Brown et al, 2010. Building
Watson: An Overview of the DeepQA Project. AI
Magazine, 31(3):59?79.
David Ferrucci and Adam Lally, 2004. UIMA: An
Architectural Approach to Unstructured Informa-
tion Processing in the Corporate Research Envi-
ronment. Natural Language Engineering, 10(3?
4):327?348.
David Gale and Lloyd S. Shapley, 1962. College Ad-
missions and the Stability of Marriage. American
Mathematical Monthly, 69:9?15.
Birgit Hamp and Helmut Feldweg, 1997. GermaNet
? a Lexical-Semantic Net for German. In Pro-
ceedings of ACL workshop Automatic Informa-
tion Extraction and Building of Lexical Semantic
Resources for NLP Applications. Madrid. http:
//aclweb.org/anthology/W97-0802.
Manfred Krifka, 2007. Basic Notions of Information
Structure. In Caroline Fery, Gisbert Fanselow and
Manfred Krifka (eds.), The Notions of Information
Structure, Universita?tsverlag Potsdam, Potsdam,
volume 6 of Interdisciplinary Studies on Informa-
tion Structure (ISIS).
Thomas Landauer, Peter Foltz and Darrell Laham,
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, 25:259?284.
Claudia Leacock and Martin Chodorow, 2003. C-
rater: Automated Scoring of Short-Answer Ques-
tions. Computers and the Humanities, 37:389?
405.
Vladimir I. Levenshtein, 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions, and Rever-
sals. Soviet Physics Doklady, 10(8):707?710.
Xin Li and Dan Roth, 2002. Learning Question Clas-
sifiers. In Proceedings of the 19th International
8
Conference on Computational Linguistics (COL-
ING 2002). Taipei, Taiwan, pp. 1?7.
Detmar Meurers, Niels Ott and Ramon Ziai, 2010.
Compiling a Task-Based Corpus for the Analysis
of Learner Language in Context. In Proceedings of
Linguistic Evidence. Tu?bingen, pp. 214?217. http:
//purl.org/dm/papers/meurers-ott-ziai-10.html.
Detmar Meurers, Ramon Ziai, Niels Ott and Stacey
Bailey, 2011. Integrating Parallel Analysis Mod-
ules to Evaluate the Meaning of Answers to
Reading Comprehension Questions. IJCEELL.
Special Issue on Automatic Free-text Evalua-
tion, 21(4):355?369. http://purl.org/dm/papers/
meurers-ziai-ott-bailey-11.html.
Rodney D. Nielsen, Wayne Ward and James H. Mar-
tin, 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov and Erwin Marsi, 2007. MaltParser: A
Language-Independent System for Data-Driven
Dependency Parsing. Natural Language Engineer-
ing, 13(1):1?41.
Craige Roberts, 1996. Information Structure in Dis-
course: Towards an Integrated Formal Theory of
Pragmatics. In Jae-Hak Yoon and Andreas Kathol
(eds.), OSU Working Papers in Linguistics No. 49:
Papers in Semantics, The Ohio State University.
A?lvaro Rodrigo, Anselmo Pen?as and Felisa Verdejo,
2009. Overview of the Answer Validation Exercise
2008. In Carol Peters, Thomas Deselaers, Nicola
Ferro, Julio Gonzalo, Gareth Jones, Mikko Ku-
rimo, Thomas Mandl, Anselmo Pen?as and Vivien
Petras (eds.), Evaluating Systems for Multilin-
gual and Multimodal Information Access, Springer
Berlin / Heidelberg, volume 5706 of Lecture Notes
in Computer Science, pp. 296?313.
Helmut Schmid, 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing. Manchester, UK, pp. 44?49.
Roger Schwarzschild, 1999. GIVENness, AvoidF
and other Constraints on the Placement of Accent.
Natural Language Semantics, 7(2):141?177.
Karen Spa?rck Jones, 2007. Automatic Summarising:
The State of the Art. Information Processing and
Management, 43:1449?1481.
Peter Turney, 2001. Mining the Web for Synonyms:
PMI-IR Versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001). Freiburg, Germany, pp.
491?502.
Sholom M. Weiss and Casimir A. Kulikowski, 1991.
Computer Systems That Learn: Classification and
Prediction Methods from Statistics, Neural Nets,
Machine Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
9
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 190?200,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Short Answer Assessment: Establishing Links Between Research Strands
Ramon Ziai Niels Ott Detmar Meurers
SFB 833 / Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{rziai,nott,dm}@sfs.uni-tuebingen.de
Abstract
A number of different research subfields are
concerned with the automatic assessment of
student answers to comprehension questions,
from language learning contexts to computer
science exams. They share the need to evaluate
free-text answers but differ in task setting and
grading/evaluation criteria, among others.
This paper has the intention of fostering
synergy between the different research strands.
It discusses the different research strands,
details the crucial differences, and explores
under which circumstances systems can be
compared given publicly available data. To that
end, we present results with the CoMiC-EN
Content Assessment system (Meurers et al,
2011a) on the dataset published by Mohler
et al (2011) and outline what was necessary
to perform this comparison. We conclude
with a general discussion on comparability and
evaluation of short answer assessment systems.
1 Introduction
Short answer assessment systems compare students?
responses to questions with manually defined target
responses or answer keys in order to judge the
appropriateness of the responses, or in order to
automatically assign a grade. A number of
approaches have emerged in recent years, each of
them with different aims and different backgrounds.
In this paper, we will draw a map of the short answer
assessment landscape, highlighting the similarities
and differences between approaches and the data used
for evaluation. We will provide an overview of 12
systems and sketch their attributes. Subsequently,
we will zoom into the comparison of two of them,
namely CoMiC-EN (Meurers et al, 2011a) and the
one which we call the Texas system (Mohler et al,
2011) and discuss the issues that arise with this
endeavor. Returning to the bigger picture, we will
explore how such systems could be compared in
general, in the belief that meaningful comparison
of approaches across research strands will be an
important ingredient in advancing this relatively new
research field.
2 The short answer assessment landscape
2.1 General aspects
Researchers from all directions have settled in the
landscape of short answer assessment, each of them
with different backgrounds and different goals. In
this section, we aim at providing an overview of
these research villages, also hoping to construct a
road network that may connect them.
Most approaches to short answer assessment are
situated in an educational context. Some focus on
GCSE1 tests, others aim at university assessment
tests in the medical domain. Another strand
of approaches focuses on language teaching and
learning. All of these approaches share one theme:
they assess short texts written by students. These
may be answers to questions that ask for knowledge
acquired in a course, e.g., in computer science, or to
reading comprehension questions in second language
1The General Certificate of Secondary Education (GCSE) is
an academic qualification in the United Kingdom, usually taken
at the age of 14?16.
190
learning. While thematically related, short answer
assessment is different from essay grading. Short
answers are formulated by students in a much more
controlled setting. Not only are they short, they
usually are supposed to contain only a few facts that
answer only one question.
Another common theme of these approaches is
that they compare the student answers to one or more
previously defined correct answers that are either
given in natural language as target answers or as a list
of concepts in an answer key. The ways of technically
conducting these comparisons vary widely, as we
discuss below in Section 2.2.
There also are conceptual differences between
the approaches. Some systems focus on assessing
whether or not the student has properly answered
the question. They put the spot on comparing the
meaning of target answers and student answers; they
aim at being tolerant of form errors such as spelling
or grammar errors. Others aim at giving a grade as
accurate as possible, therefore not only assessing
meaning but also performing grading similar to
human teachers. This can also include modules that
take into account form errors.
These two views on a similar task are also reflected
in the annotation of the data used in experiments:
Systems performing meaning comparison usually
operate with labels specifying the relations between
target answers and student answers. Grading systems
naturally aim at producing numerical grades. Since
labels are on a nominal scale, and grades are on
an ordinal scale (or even treated as being on an
interval scale), the difference between meaning
comparison and grading results in a whole string
of other differences in methodology.
Researchers also enter the short answer landscape
from different home countries: Some projects are
interested in the strategies and mechanics of meaning
comparison, others aim at reducing the load and costs
of large-scale assessment tests, and yet others aim
at improving intelligent tutoring systems, requiring
additional components that provide useful feedback
to students using these systems.
2.2 Approaches
Table 1 summarizes the features of the short answer
assessment systems discussed hereafter.
One of the earlier systems is WebLAS, presented
by Bachman et al (2002). A human task creator feeds
the system with scores for model answers. Regular
expressions are then created automatically from these
model answers. Since each regular expression is
associated with a score, matching the expression
against a student answer yields a score for that answer.
Bachman et al (2002) do not provide an evaluation
study based on data.
Another earlier system is CarmelTC by Rose? et
al. (2003). It has been designed as a component
in the Why2 tutorial dialogue system (VanLehn et
al., 2002). Even though Rose? et al (2003) position
CarmelTC in the context of essay grading, it may be
considered to deal with short answers: in their data,
the average length of a student response is approx.
48 words. Their system is designed to perform
text classification on single sentences in the student
responses, where each class of text represents one
possible model response, plus an additional class for
?no match?. They combine decision trees operating
on an automatic syntactic analysis, a Naive Bayes
text classifier, and a bag-of-words approach. In a
50-fold cross validation experiment with one physics
question, six classes and 126 student responses,
hand-tagged by two annotators, CarmelTC reaches
an F-measure value of 0.85. They do not report on a
baseline. Concerning the quality of the gold standard,
they report that conflicts in the annotation have been
resolved.
C-Rater (Leacock and Chodorow, 2003) is based
on a paraphrase recognition approach. It employs
correct answer models consisting of essential points
formulated in natural language. C-Rater aims at
automatic scoring and focuses on meaning, thus
tolerating form errors. Leacock and Chodorow
(2003) present two pilot studies, one of them dealing
with reading comprehension. From 16,625 student
answers with an average length of 43 words, they
drew a random sample of 100 answers to each of
the seven questions. This sample was scored by
one human judge using a three-way scoring system
(full credit, partial credit, no credit). Their system
achieved 84% agreement with the gold standard.
Information about the distribution of the scoring
categories is given indirectly: A baseline system that
assigns scores randomly would have achieved 47%
accuracy.
191
System Goal Technique Domain Lang.
WebLAS (Bachman et al, 2002) Assessment of
language ability
Auto-generated regular
expressions
Foreign language
teaching
EN
CarmelTC (Rose? et al, 2003) Automatic grading Text classification Physics EN
C-Rater (Leacock and Chodorow,
2003)
Assessment test Paraphrase recognition Mathematics,
Reading comp.
EN
IAT (Mitchell et al, 2003) Assessment,
Automatic grading
Information extraction
w/ handwritten patterns
Medical EN
Oxford (Pulman and Sukkarieh,
2005)
Assessment,
automatic grading
Information extraction
w/ handwritten patterns
GCSE exams EN
Atenea (Pe?rez et al, 2005) Automatic grading N-gram overlap, Latent
Semantic Analysis
Computer science ES
Logic-based System (Makatchev
and VanLehn, 2007)
Meaning comparison First-order logic,
machine learning
Physics EN
CAM (Bailey and Meurers, 2008),
CoMiC-EN (Meurers et al, 2011a)
Meaning comparison Alignment, machine
learning
Reading comp. in
foreign language
EN
Facets System (Nielsen et al, 2009) Meaning comparison
& tutoring systems
Alignment of facets,
machine learning
Elementary
school science
classes
EN
Texas (Mohler et al, 2011) Automatic grading Graph alignment,
semantic similarity
Computer science EN
CoMiC-DE (Meurers et al, 2011b) Meaning comparison Alignment, machine
learning
Reading comp. in
foreign language
DE
CoSeC-DE (Hahn and Meurers,
2012)
Meaning comparison Alignment via
Lexical-Resource
Semantics
Reading comp. in
foreign language
DE
Table 1: Short Answer Assessment systems and their Features
Information extraction templates form the core of
the Intelligent Assessment Technologies system (IAT,
Mitchell et al 2003). These templates are created
manually in a special-purpose authoring tool by
exploring sample responses. They allow for syntactic
variation, e.g., filling the subject slot in a sentence
with different equivalent concepts. The templates
corresponding to a question are then matched against
the student answer. Unlike other systems, IAT
additionally features templates for explicitly invalid
answers. They tested their approach with a progress
test that has to be taken by medicine students.
Approximately 800 students each plowed through
270 test items. The automatically graded responses
then were moderated: Human judges streamlined the
answers to achieve a more consistent grading. This
step already had been done before with tests graded
by humans. Mitchell et al (2003) state that their
system reaches 99.4% accuracy on the full dataset
after the manual adjustment of the templates via
the moderation process. Summarizing, they report
an error of ?between 5 and 5.5%? in inter-grader
agreement and an error of 5.8% in automatic grading
without the moderation step, though it is not entirely
clear which data these statistics correspond to. No
information on the distribution of grades or a random
baseline is provided.
The Oxford system (Pulman and Sukkarieh, 2005)
is another one to employ an information extraction
approach. Again, templates are constructed manually.
Motivated by the necessary robustness to process
language with grammar mistakes and spelling errors,
they use shallow analyses in their pre-processing.
In order to overcome the hassle of manually con-
structing templates, they also investigated machine
learning techniques. However, the automatically
generated templates were outperformed by the
manually created ones. Furthermore, they state that
manually created templates can be equipped with
messages provided to the student as feedback in a
tutoring system. For evaluating their system, they
used factual science questions and the corresponding
192
student answers from GCSE tests. 200 graded
answers for each of nine questions served as a
training set, while another 60 answers served as a
test set. They report that their system achieves an
accuracy of 84%. With inconsistencies in the human
grading removed, it achieves 93%. However, they do
not report on the level of inter-grader agreement or
on a random baseline.
Pe?rez et al (2005) present the Atenea system,
a combined approach that makes use of Latent
Semantic Analysis (LSA, Landauer et al 1998) and
n-gram overlap. While n-gram overlap supports
comparing target responses and student responses
with differing word order, it does not deal with
synonyms and related terms. Hence, they use LSA to
add a component that deals with semantic relatedness
in the comparison step. As a test corpus, they
collected nine different questions from computer
science exams. A tenth question ?[consists] of a
set of definitions of ?Operating System? obtained
from the Internet.? Altogether, they gathered 924
student responses and 44 target responses written
by teachers. Since their LSA module had been
trained on English but their data were in Spanish,
they chose to use Altavista Babelfish to translate the
data into English. They do not provide information
about the distribution of scores and about inter-grader
agreement. Atenea achieves a Pearson?s correlation
of r = 0.554 with the scores in the gold standard.
The approach by Makatchev and VanLehn (2007),
which we refer to as the Logic-based System,
enters the landscape from the direction of artificial
intelligence. It is related to CarmelTC and its
dataset, but follows a different route: target
responses are manually encoded in first-order
predicate language. Similar logic representations
are constructed automatically for student answers.
They explore various strategies for matching these
two logic representation on the basis of 16 semantic
classes. In an evaluation experiment, they tested the
system on 293 ?natural language utterances? with
ten-fold cross validation. The test data are skewed
towards the ?empty? label that indicates that none
of the 16 semantic labels could be attached. They
do not report on other properties of the dataset such
as number of annotators or number of questions to
which the student answers were given. Their winning
configuration yields a F-measure value of 0.4974.
While Makatchev and VanLehn (2007) position their
approach in the context of the Why2 tutorial dialogue
system, their use of semantic classes seems to make
them more related to meaning comparison than to
grading.
The Content Assessment Module (CAM) pre-
sented in Bailey (2008) and Bailey and Meurers
(2008) utilizes an approach that is different from
the systems discussed so far: Following a three-step
strategy, the system first automatically generates
linguistic annotations for questions, target responses
and student responses. In an alignment phase, these
annotations are then used to map from elements
(words, lemmas, chunks, dependency triples) in
the student responses to elements in the target
responses. Finally, a machine learning classifier
judges on the basis of this alignment, whether
or not the student has answered the question
correctly. The data used for evaluation was made
available as the Corpus of Reading Comprehension
Exercises in English (CREE, Meurers et al 2011a).
This corpus consists of 566 responses produced
by intermediate ESL learners at The Ohio State
University as part of their regular assignments.
Students had access to their textbooks and typically
answered questions in one to three sentences. All
responses were labelled as either appropriate or
inappropriate by two independent annotators, along
with a detailed diagnosis code specifying the nature
of the inappropriateness (missing concept, extra
concept, blend, non-answer). In leave-one-out
evaluation on the development set containing 311
responses to 47 different questions, CAM achieved
87% accuracy on the binary judgment (response
correct/incorrect). For the test set containing 255
responses to 28 questions, the approach achieved
88%. However, the distribution of categories in the
data is heavily skewed with 71% of the responses
marked as correct in the development set and 84% in
the test set. The best result obtained on a balanced
set with leave-one-out-testing is 78%. Meurers et
al. (2011a) present a re-implementation of CAM
called CoMiC-EN (Comparing Meaning in Context
in English), achieving an accuracy of 87.6% on the
CREE development set and 88.4% on the test set.
With their Facets System, Nielsen et al (2009)
establish a connection to the field of Recognizing
Textual Entailment (RTE, Dagan et al 2009). In
193
a number of friendly challenges, RTE research has
spawned numerous systems that try to automatically
answer the following question: Given a text and a
hypothesis, is the hypothesis entailed by the text?
Short answers assessment can be seen as a RTE task
in which the target response corresponds to the text
and the student response to the hypothesis. Nielsen et
al. (2009) base their system on what they call facets.
These facets are meaning representations of parts
of sentences. They are constructed automatically
from dependency and semantic parses of the target
responses. Each facet in the target response is then
looked up in the corresponding student response
and equipped with one of five labels2 ranging from
unaddressed (the student did not mention the fact
in this facet) to expressed (the student named the
fact). This step is taken via machine learning.
From a tutoring system in real-life operation, they
gathered responses from third- to sixth-grade students
answering questions for science classes. Two
annotators worked on these data, producing 142,151
facets. Furthermore, all facets were looked up in
the corresponding student responses and annotated
accordingly, using the mentioned set of labels. The
best result of the Facets System is 75.5% accuracy on
one of the held-out test sets. With ten-fold cross
validation on the training set, it achieves 77.1%
accuracy. The majority label baselines are 51.1% and
54.6% respectively. Providing this more fine-grained
analysis of facets that are searched for in student
responses, Nielsen et al (2009) claim to ?enable
more intelligent dialogue control? in tutoring systems.
From the point of view of grading vs. meaning
comparison, their approach can be counted towards
the latter, since their labels can be conflated to
produce a single yes/no decision.
Another recent approach is described by Mohler et
al. (2011), hereafter referred to as the Texas system.
Student responses and target responses are annotated
using a dependency parser. Thereupon, subgraphs of
the dependency structures are constructed in order to
map one response to the other. These alignments
are generated using machine learning. Dealing
with subgraphs allows for variation in word order
between the two responses that are to be compared.
2In human annotation, they use eight labels, which are
grouped into five broader categories as used by their system.
In order to account for meaning, they combine
lexical semantic similarity with the aforementioned
alignment. They make use of several WordNet-based
measures and two corpus-based measures, namely
Latent Semantic Analysis and Explicit Semantic
Analysis (ESA, Gabrilovich and Markovitch 2007).
For evaluating their system, Mohler et al (2011)
collected student responses from an online learning
environment. 80 questions from ten introductory
computer science assignments spread across two
exams were gathered together with 2,273 student
responses. These responses were graded by two
human judges on a scale from zero to five. The
judges fully agreed in 57% of all cases, their
Pearson correlation computes to r = 0.586. The
gold standard has been created by computing the
arithmetic mean of the two judgments for each
response. The Texas system achieves r = 0.518 and
a Root Mean Square Error of 0.978 as its best result.
Mohler et al (2011) mention that ?[t]he dataset is
biased towards correct answers?. Data are publicly
available. We used these in an evaluation experiment
with the CoMiC-EN system, discussed in Section 3.
While almost all short answer assessment research
has targeted answers written in English, there are
two recent approaches dealing with German answers.
The CoMiC-EN reimplementation of CAM discussed
above was motivated by the need for a modular
architecture supporting a transfer of the system to
German, resulting in its counterpart named CoMiC-
DE (Meurers et al, 2011b). The German system
utilizes the same strategies as the English one,
but with language-dependent processing modules
being replaced. Meurers et al (2011b) evaluated
CoMiC-DE on a subset of the Corpus of Reading
Comprehension Questions in German (CREG, Ott et
al. 2012), collected in collaboration with the German
programs at The Ohio State University and the
University of Kansas. Like in CREE, all responses
are rated by two annotators with both binary and
detailed diagnosis codes.3 The aforementioned
subset contains 1,032 learner responses and 223
target responses to 177 questions. Furthermore, it
features an even distribution of correct and incorrect
answers according to the judgement of two human
3In CREG, correct answers as well as incorrect ones can be
labelled with missing concept, extra concept, or blend.
194
annotators. On that subset, CoMiC-DE achieved an
accuracy of 84.6% in the binary classification task.
CREG is freely available for research purposes under
a Creative Commons by-nc-sa license.
Hahn and Meurers (2012) present the CoSeC-DE
approach based on Lexical Resource Semantics
(LRS, Richter and Sailer 2003). In a first step,
they create LRS representations from POS-tagged
and dependency-parsed data. These underspecified
LRS representations of student responses and target
responses are then aligned. Using A* as heuristic
search algorithm, a best alignment is computed and
equipped with a numeric score representing the
quality of the alignment of the formulae. If this
best alignment scores higher than a threshold, the
system judges student response and target response
to convey the same meaning. The alignment
and comparison mechanism does not utilize any
linguistic representations other than the LRS
semantic formulae. These semantic representations
abstract away from surface features, e.g., by treating
active and passive voice equally. Hahn and Meurers
(2012) claim that that ?[semantic representations]
more clearly expose those distinction which do make
a difference in meaning.? They evaluate the approach
on the above-mentioned subset of CREG containing
1,032 learner responses and report an accuracy of
86.3%.
3 A concrete system comparison
After discussing the broad landscape of Short Answer
Evaluation systems, the main characteristics and
differences, we now turn to a comparison of two
concrete systems, namely CoMiC-EN (Meurers
et al, 2011a) and the Texas system Mohler et
al. (2011), to explore what is involved in such a
concrete comparison of two systems from different
contexts. While CoMiC-EN was developed with
meaning comparison in mind, the purpose of the
Texas system is answer grading. We pick these
two systems because they constitute recent and
interesting instances of their respective fields and
the corresponding data are freely available.
3.1 Data
In evaluating the Texas system, Mohler et al (2011)
used a corpus of ten assignments and two exams from
an introductory computer science class. In total, the
Texas corpus consists of 2,442 responses, which were
collected using an online learning platform. Each
response is rated by two annotators with a numerical
grade on a 0?5 scale. Annotators were not given any
specific instructions besides the scale itself, which
resulted in an exact agreement of 57.7%. In order to
arrive at a gold standard rating, the numerical average
of the two ratings was computed. The data exist in
raw, sentence-segmented and parsed versions and are
freely available for research use. Table 2 presents
a breakdown of the score counts and distribution
statistics of the Texas corpus. A bias towards correct
answers can be observed, which is also mentioned by
Mohler et al (2011).
Score #
0.000 24
0.500 3
1.000 23
1.500 46
1.750 1
2.000 93
2.250 2
2.500 125
3.000 164
Score #
3.250 1
3.500 187
3.625 1
3.750 1
4.000 220
4.125 2
4.500 310
4.750 1
5.000 1238
x = 4.19, s = 1.11
Table 2: Details on the gold standard scores in the Texas
corpus. Non-integer scores result from averaging between
raters and normalization onto the 0?5 scale.
3.2 Approaches
CoMiC-EN uses a three-step approach to meaning
comparison. Annotation uses NLP to enrich the
student and target answers, as well as the question
text, with linguistic information on different levels
(words, chunks, dependency triples) and types of
abstraction (tokens, lemmas, distributional vectors,
etc.). Alignment maps elements of the learner answer
to elements of the target response using annotation.
The global alignment solution is computed using the
Traditional Marriage Algorithm (Gale and Shapley,
1962). Finally, Classification analyzes the possible
alignments and labels the learner response with a
binary or detailed diagnosis code. The features used
in the classification step are shown in Table 3.
For the Texas system, Mohler et al (2011) used a
combination of bag-of-words (BOW) features and
195
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2./3. Token Overlap Percent of aligned
target/learner tokens
4./5. Chunk Overlap Percent of aligned
target/learner chunks
6./7. Triple Overlap Percent of aligned
target/learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of
(0-5) token-level alignments
Table 3: Features used in the CoMiC-EN system
dependency graph alignment in connection with
two different machine learning approaches. Among
the BOW features are WordNet-based similarity
measures such as the one by Lesk (1986) and vector
space measures such as tf ? idf (Salton and McGill,
1983) and the more advanced LSA (Landauer et al,
1998). The dependency graph alignment approach
builds on a node-to-node matching stage which
computes a score for each possible match between
nodes of the student and target response. In the next
stage, the optimal graph alignment is computed based
on the node-to-node scores using the Hungarian
algorithm.
Mohler et al (2011) also employ a technique
they call ?question demoting?, which refers to the
exclusion of words from the alignment process
if they already appeared in the question string.
Incidentally, the technique is also used in the earlier
CAM system (Bailey and Meurers, 2008), but called
?Givenness filter? there, following the long tradition
of research on givenness (Schwarzschild, 1999) as a
notion of information structure investigated in formal
pragmatics.
To produce the final system score, the Texas
system uses two machine learning techniques based
on Support Vector Machines (SVMs), SVMRank and
Support Vector Regression (SVR). Both techniques
are trained with several combinations of the
dependency alignment and BOW features. While
with SVR one trains a function to produce a score on
the 0?5 scale itself, SVMRank produces a ranking of
student answers which does not produce a 0?5 grade.
Therefore, Mohler et al (2011) employ isotonic
regression to map the ranking to the 0?5 scale.
In terms of performance, Mohler et al (2011)
report that the SVMRank system produces a better
correlation measure (r = 0.518) while the SVR
system yields a better RMSE (0.978).
3.3 Evaluation
We now turn to the evaluation of CoMiC-EN on the
Texas corpus as it is a publicly available dataset. As
mentioned before, CoMiC-EN performs meaning
comparison based on a system of categories while
the Texas system is a scoring approach, trying to
predict a grade. While the former is a classification
task, the latter is better characterized as a regression
problem because of the desired numerical outcome.
Of course, one could simply pretend that individual
grades are classes and treat scoring as a classification
task. However, a classification approach has no
knowledge of numerical relationships, i.e., it does
not ?know? that 4 is a higher grade than 3 and a
much higher grade than 1 (assuming a 0?5 scale).
As a result, if an evaluation metric such as Pearson
correlation is used, classification systems are at a
disadvantage because some misclassifications are
punished more than others. We discuss this point
further in Section 4.
For these reasons, to obtain a more interesting
comparison, we modified CoMiC-EN to perform
scoring instead of meaning comparison. This means
that the memory-based learning approach CoMiC-
EN had employed so far was no longer applicable and
had to be replaced with a regression-capable learning
strategy. We chose Support Vector Regression (SVR)
using libSVM4 since that is one of the methods
employed by Mohler et al (2011). However, all other
parts of CoMiC-EN such as the processing pipeline
and the alignment approach and the extracted features
remained the same.
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm
196
The evaluation procedure was carried out as a
12-fold cross-validation due to the 12 assignments
in the Texas corpus. For each fold, one complete
assignment was held out as test set. Parameters for
the SVR were determined using a grid search using
the tools provided with libSVM. As kernel function,
we used a linear kernel as it was also used in the
evaluation of the Texas system and thus constitutes
a vital part of the evaluation setup. In general, we
designed to evaluation procedure to be as close as
possible to the Texas one.
Table 4 presents detailed results on the 12 folds
as well as the overall results and a baseline which
always predicts the median value 5.
Assignment # responses r RMSE
1 203 0.416 0.958
2 210 0.349 1.221
3 217 0.335 0.969
4 210 0.338 1.212
5 112 0.010 1.030
6 182 0.646 0.702
7 182 0.265 0.991
8 189 0.521 0.942
9 189 0.220 0.942
10 168 0.699 0.990
11 (exam) 300 0.436 1.076
12 (exam) 280 0.619 1.165
Median Baseline 2442 ? 1.375
Overall 2442 0.405 1.016
Table 4: Detailed results of CoMiC-EN on Texas corpus
The CoMiC-EN system on the Texas data set does
not quite reach the level achieved by the Texas system
on their data set. We obtained a Pearson correlation
of r = 0.405 and an RMSE of 1.016 over all 12 folds.
However, let us keep in mind the objective of this
experiment as exemplifying the process needed to
directly compare two systems from different research
strands on the same dataset.
4 Comparability of approaches & datasets
It seems clear that for systems to be comparable
and results to be reproducible, datasets must be
publicly available, as is the case with the Texas
corpus. However, data availability alone does not
ensure meaningful comparison. Depending on the
context the corpus was drawn from, datasets will
differ just like the corresponding systems:
? Data source: Reading comprehension task in
language learning setting, language tutoring
context, automated grading of short answer
exams
? Language properties: Native vs. learner
language, domain-specific language (e.g., com-
puter science)
? Assessment scheme: nominal vs. interval scale
Especially the last point deserves some further
discussion. Depending on the kind of assessment
scheme, which in turn is motivated by the task,
different evaluation methods may be chosen. Scoring
systems are often evaluated using a correlation metric
in order to capture the systems? tendency to assign
similar but not necessary equal grades as the human
raters. Conversely, with category-based schemes one
usually reports accuracy, which expresses how many
items were classified correctly.
The question that arises is how a system coming
from one paradigm can be compared to one from
the other paradigm in a meaningful way. One might
argue that the tasks are simply too different: scoring
might take form errors into account while meaning
comparison by definition does not. Moreover,
while classification labels say something explicit and
absolute about a piece of data, grades by definition
are relative to the scale they come from. It thus seems
impossible to somehow unify the two schemes as they
express fundamentally different ideas.
However, the strategies systems use to tackle
scoring or meaning comparison are undoubtedly
similar and should be comparable, as we argue in this
paper. So in order for researchers to learn from other
approaches and also compare their results to those of
other systems which tackle a different task, changes
to systems seem necessary and should be preferred
over changes to the gold standard data. In the case
presented here, a meaning comparison system was
turned into a scoring system by changing the machine
learning component from classification to regression,
which requires a certain level of system modularity.
Having compared the two systems using Pearson
correlation and RMSE, it also makes sense to
consider the relevance of these evaluation metrics.
For example, it is the case that pairwise correlation
assumes a normal distribution whereas datasets like
197
the Texas corpus are heavily skewed towards correct
answers (see Table 2). Mohler et al (2011) also note
that in distributions with zero variance, correlation is
undefined, which is not a problem as such but limits
the use of correlation as evaluation metric. Mohler
et al (2011) propose that RMSE is better suited to
the task since it captures the relative error a system
makes when trying to predict scores. However,
RMSE is scale-dependent and thus RMSE values
across different studies cannot be compared. We
can only suggest that in order to sufficiently describe
a system?s performance, several metrics need to be
reported.
Finally, an important point concerns the quality
of gold standards. Given the relatively low inter-
annotator agreement in the Texas corpus (r =
0.586, RMSE = 0.659) it seems fair to ask whether
answers without perfect agreement should be used in
training and testing systems at all. In the CREE
and CREG corpora, answers with disagreement
among the annotators have either been excluded
from experiments or resolved by an additional judge.
This approach is also supported by recent literature
(cf., e.g., Beigman and Beigman Klebanov 2009;
Beigman Klebanov and Beigman 2009). However,
for the Texas corpus, Mohler et al (2011) have opted
to use the arithmetic mean of the two graders as gold
standard. While mathematically a viable solution,
it seems questionable whether the mean is reliable
with only two graders, especially if they have not
operated on the grounds of explicit guidelines. It
would be interesting to see whether in this case, a
system trained on more, singly annotated data would
perform better than one on less, doubly annotated
data, as argued for by Dligach et al (2010). In any
case, if many disagreements occur, one should ask
the question whether the annotation task is defined
well enough and whether machines should really be
expected to perform it consistently if humans have
trouble doing so.
5 Conclusion
We discussed several issues in the comparison of
short answer evaluation systems. To that end, we
gave an overview of the existing systems and picked
two for a concrete comparison on the same data, the
CoMiC-EN system (Meurers et al, 2011a) and the
Texas system (Mohler et al, 2011). In comparing
the two, it was necessary to turn CoMiC-EN into
a scoring system because the Texas corpus as
the chosen gold standard contains numeric scores
assigned by humans. Taking a step back from
the concrete comparison, we gave a more general
description of what is necessary to compare short
answer evaluation systems. We observed that more
datasets need to be publicly available in order for
performance comparisons to have meaning, a point
also made earlier by Pulman and Sukkarieh (2005).
Moreover, we noted how datasets differ in similar
aspects as systems do, such as task context and
assessment scheme. We then criticized the use of
correlation measures as evaluation metrics for short
answer scoring. Finally, we discussed the importance
of gold standard quality.
We conclude that it is interesting and relevant
to compare short answer evaluation systems even
if the concrete task they tackle, such as grading or
meaning comparison, is not the same. However, the
availability and quality of the datasets will decide
to what extent systems can sensibly be compared.
For progress to be made in this area, more publicly
available datasets and systems are needed. The
upcoming SemEval-2013 task on ?Textual entailment
and paraphrasing for student input assessment?5
will hopefully become one important step into this
direction (see also Dzikovska et al 2012).
Acknowledgements
We are grateful to the three anonymous BEA
reviewers for their detailed and helpful comments.
References
Lyle Bachman, Nathan Carr, Greg Kamei, Mikyung Kim,
Michael Pan, Chris Salvador, and Yasuyo Sawaki.
2002. A reliable approach to automatic assessment of
short answer free responses. In Proceedings of the 19th
International Conference on Computational Linguistics
(COLING 2002), pages 1?4.
Stacey Bailey and Detmar Meurers. 2008. Diagnosing
meaning errors in short answers to reading compre-
hension questions. In Joel Tetreault, Jill Burstein,
and Rachele De Felice, editors, Proceedings of the
5http://www.cs.york.ac.uk/semeval-2013/
task4/
198
3rd Workshop on Innovative Use of NLP for Building
Educational Applications (BEA-3) at ACL?08, pages
107?115, Columbus, Ohio.
Stacey Bailey. 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, volume 1, pages
280?287. Association for Computational Linguistics.
Beata Beigman Klebanov and Eyal Beigman. 2009. From
annotator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Rational,
evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii, 10.
Dmitriy Dligach, Rodney D. Nielsen, and Martha Palmer.
2010. To annotate more accurately or to annotate more.
In Proceedings of the Fourth Linguistic Annotation
Workshop, LAW IV ?10, pages 64?72, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th International Joint Conference on Artificial
Intelligence, pages 6?12.
David Gale and Lloyd S. Shapley. 1962. College
admissions and the stability of marriage. American
Mathematical Monthly, 69:9?15.
Michael Hahn and Detmar Meurers. 2012. Evaluating
the meaning of answers to reading comprehension
questions: A semantics-based approach. In Pro-
ceedings of the 7th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA-7) at
NAACL-HLT 2012, Montreal.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Claudia Leacock and Martin Chodorow. 2003. C-
rater: Automated scoring of short-answer questions.
Computers and the Humanities, 37:389?405.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the 5th annual international conference on Systems
documentation, pages 24?26, Toronto, Ontario,
Canada.
Maxim Makatchev and Kurt VanLehn. 2007. Combining
baysian networks and formal reasoning for semantic
classification of student utterances. In Proceedings
of the International Conference on AI in Education
(AIED), Los Angeles, July.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey
Bailey. 2011a. Integrating parallel analysis modules
to evaluate the meaning of answers to reading
comprehension questions. IJCEELL. Special Issue on
Automatic Free-text Evaluation, 21(4):355?369.
Detmar Meurers, Ramon Ziai, Niels Ott, and Janina Kopp.
2011b. Evaluating answers to reading comprehension
questions in context: Results for german and the
role of information structure. In Proceedings of the
TextInfer 2011 Workshop on Textual Entailment, pages
1?9, Edinburgh, Scotland, UK, July. Association for
Computational Linguistics.
Tom Mitchell, Nicola Aldrige, and Peter Broomhead.
2003. Computerized marking of short-answer
free-text responses. Paper presented at the 29th
annual conference of the International Association for
Educational Assessment (IAEA), Manchester, UK.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
752?762, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. Natural Language Engineering, 15(4):479?
501.
Niels Ott, Ramon Ziai, and Detmar Meurers. 2012.
Creation and analysis of a reading comprehension
exercise corpus: Towards evaluating meaning in
context. In Thomas Schmidt and Kai Wo?rner,
editors, Multilingual Corpora and Multilingual Corpus
Analysis, Hamburg Studies in Multilingualism (HSM).
Benjamins, Amsterdam. to appear.
Diana Pe?rez, Enrique Alfonseca, Pilar Rodr??guez, Alfio
Gliozzo, Carlo Strapparava, and Bernardo Magnini.
2005. About the effects of combining latent semantic
analysis with natural language processing techniques
for free-text assessment. Revista signos, 38(59):325?
343.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005.
Automatic short answer marking. In Jill Burstein and
Claudia Leacock, editors, Proceedings of the Second
199
Workshop on Building Educational Applications Using
NLP, pages 9?16, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Frank Richter and Manfred Sailer. 2003. Basic concepts
of lexical resource semantics. In Arnold Beckmann
and Norbert Preining, editors, ESSLLI 2003 ? Course
Material I, volume 5 of Collegium Logicum, pages
87?143, Wien. Kurt Go?del Society.
Carolyn Penstein Rose?, Antonio Roque, Dumisizwe
Bhembe, and Kurt VanLehn. 2003. A hybrid
approach to content analysis for automatic essay
grading. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: companion volume of the Proceedings
of HLT-NAACL 2003?short papers - Volume 2,
NAACL-Short ?03, pages 88?90, Edmonton, Canada.
Association for Computational Linguistics.
Gerard Salton and Michael J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill, New
York.
Roger Schwarzschild. 1999. GIVENness, AvoidF and
other constraints on the placement of accent. Natural
Language Semantics, 7(2):141?177.
Kurt VanLehn, Pamela W. Jordan, Carolyn Penstein Rose?,
Dumisizwe Bhembe, Michael Boettner, Andy Gaydos,
Maxim Makatchev, Umarani Pappuswamy, Micheal
Ringenberg, Antonio Roque, Stephanie Siler, and
Ramesh Srivastava. 2002. The architecture of why2-
atlas: A coach for qualitative physics essay writing.
In Proceedings of the 6th International Conference
on Intelligent Tutoring Systems, volume 2363, pages
158?167, Biarritz, France and San Sebastian, Spain,
June 2-7. Springer LNCS.
200
