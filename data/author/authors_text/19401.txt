Proceedings of NAACL-HLT 2013, pages 1000?1009,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining Heterogeneous Models for Measuring Relational Similarity
Alisa Zhila?
Instituto Politecnico Nacional
Mexico City, Mexico
alisa.zhila@gmail.com
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Geoffrey Zweig
Microsoft Research
Redmond, WA 98052, USA
gzweig@microsoft.com
Tomas Mikolov?
BRNO University of Technology
BRNO, Czech Republic
tmikolov@gmail.com
Abstract
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman?s rank correlation.
1 Introduction
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al, 2011; Agirre et al, 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
?Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al, 2012).
In order to promote this research direction, Ju-
rgens et al (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman?s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
1000
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially ? having a relative gain of
54.1% in Spearman?s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
2 Related Work
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget?s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al (2012) proposed a
new task of ?Measuring Degrees of Relational Simi-
larity? at SemEval-2012, which includes 79 relation
1001
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is ?Y is a kind/type/instance
of X?. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al, 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
3 Problem Definition & Task Description
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al, 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
1http://www.mturk.com
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al, 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as ?a Y is one item in a
collection/group of X? for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
? all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman?s rank correlation
coefficient (?). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman?s ?
1002
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al, 2012) for the exact procedure).
4 Models for Relational Similarity
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
4.1 Directional Similarity Model
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al, 1998), word clustering (Brown
et al, 1992) and neural-network language model-
ing (Bengio et al, 2003; Mikolov et al, 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al, 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al, 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
?a is to b as c is to d? using the cosine score of
(~vb?~va +~vc, ~vd), where a, b, c, d are the four given
words and ~va, ~vb, ~vc, ~vd are the corresponding vec-
q 
shirt
clothing
furniture
desk
v1
v2'
v2'
Figure 1: Directional vectors ?1 and ?2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of ?.
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ?i = (wi1 , wi2) and
?j = (wj1 , wj2) be the two word pairs being com-
pared. Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-
responding vectors of these words. The directional
vectors of ?i and ?j are defined as ~?i ? ~vi2 ? ~vi1
and ~?j ? ~vj2 ? ~vj1 , respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of ?i and ?j , such as the co-
sine function:
~?i ? ~?j
?~?i??~?j?
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2 . That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman?s ? than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
1003
Word Embedding Spearman?s ? MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman?s ? and MaxDiff accuracy.
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al, 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
4.2 Lexical Pattern Model
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like ?X such as Y?, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
2http://www.fit.vutbr.cz/?imikolov/rnnlm
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al, 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called ?raw pattern?). For instance, ?such as? would
be the context extracted from ?X such as Y? for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
1004
rithm. The performance on the training data is 0.322
in Spearman?s ? and 41.8% in MaxDiff accuracy.
4.3 Word Relation Models
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
4.3.1 Knowledge Bases
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet?s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al, 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman?s ? = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman?s
? = 0.290 and MaxDiff accuracy 32.7%.
4.3.2 Lexical Semantics Measures
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al, 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman?s ? = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
5 Model Combination
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman?s ? of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 ? {0, 0.01, 0.1} and
L2 ? {0, 0.001, 0.01, 1, 10}. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
1005
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.324? 0.235 0.058? -0.010? -0.009? 0.353?
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5? 39.2 33.3? 29.8? 30.7? 45.2?
Table 2: Average Spearman?s ? (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB . ? and ? indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man?s ? and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman?s ? = 0.619 and MaxDiff accuracy
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman?s ? value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10?3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
1006
Spearman?s ? MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238? 0.329 45.2 45.0 44.9? 44.7 39.6? 45.4
Table 3: Average Spearman?s ? and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. ? indicates the difference in the average results is statistically significant with 99% confidence level.
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman?s ?
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman?s ? nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman?s
? drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
6 Conclusions
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman?s ? = 0.353 and MaxDiff accuracy
45.4% ? resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
1007
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
Acknowledgments
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ?07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189?
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356?364, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
1008
2012), pages 497?501, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82?90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilarin?o, David Pinto, and Saul Leo?n. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502?505, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533?585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481?492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
1009
Proceedings of the ACL 2014 Student Research Workshop, pages 78?85,
Baltimore, Maryland USA, June 22-27 2014.
c
?2014 Association for Computational Linguistics
Open Information Extraction for Spanish Language
based on Syntactic Constraints
Alisa Zhila
Centro de Investigaci?on
en Computaci?on,
Instituto Polit?ecnico Nacional,
07738, Mexico City, Mexico
alisa.zhila@gmail.com
Alexander Gelbukh
Centro de Investigaci?on
en Computaci?on,
Instituto Polit?ecnico Nacional,
07738, Mexico City, Mexico
gelbukh@gelbukh.com
Abstract
Open Information Extraction (Open IE)
serves for the analysis of vast amounts of
texts by extraction of assertions, or rela-
tions, in the form of tuples ?argument 1;
relation; argument 2?. Various approaches
to Open IE have been designed to per-
form in a fast, unsupervised manner. All
of them require language specific infor-
mation for their implementation. In this
work, we introduce an approach to Open
IE based on syntactic constraints over POS
tag sequences targeted at Spanish lan-
guage. We describe the rules specific for
Spanish language constructions and their
implementation in EXTRHECH, an Open
IE system for Spanish. We also discuss
language-specific issues of implementa-
tion. We compare EXTRHECH?s perfor-
mance with that of REVERB, a similar
Open IE system for English, on a paral-
lel dataset and show that these systems
perform at a very similar level. We also
compare EXTRHECH?s performance on a
dataset of grammatically correct sentences
against its performance on a dataset of ran-
dom texts extracted from the Web, drasti-
cally different in their quality from the first
dataset. The latter experiment shows ro-
bustness of EXTRHECH on texts from the
Web.
1 Introduction
Open IE is a rapidly developing area in text pro-
cessing, with its own applications and approaches
that are different from traditional IE (Etzioni et
al., 2008; Banko and Etzioni, 2008; Etzioni,
2011). Unlike traditional IE, where systems are
targeted at extraction of instances of particular re-
lations with arguments restricted to certain seman-
tic classes, e.g., to be born in(HUMAN; LOCA-
TION), Open IE serves for extraction of all pos-
sible relations with arbitrary arguments. For ex-
ample, in ?Woman who drove van full of kids is
charged with attempted murder? two relations can
be identified: ?Woman; drove; van full of kids? and
?Woman; is charged with; attempted murder?.
The ability to extract arbitrary relations from
text allows applications of Open IE that are not
possible in the frame of traditional IE. Among
them are fact extraction at sentence level (e.g.,
?Mozart; was born in; Salzburg?), new perspective
on search as question answering (e.g., Where was
Mozart born?) (Etzioni, 2011), or assessment of
the quality of text documents at Web scale (Horn
et al., 2013). Additionally, the output of Open IE
systems can serve for ontology population (Soder-
land et al., 2010) and acquisition of common sense
knowledge (Lin et al., 2010).
Although all Open IE systems are targeted at the
extraction of arbitrary relations, the approaches to
this task vary significantly. The pilot approach
suggested by Banko et al. (2007) is based on
semi-supervised learning of general relation pat-
terns that then serve for extraction of arbitrary
relations. However, the output of such systems
contains many incoherent and inconsistent extrac-
tions, and the training stage is quite computation-
ally complex. Fader et al. (2011) suggested an-
other approach where syntactic and lexical con-
straints were applied over POS-tagged input. This
approach has proven to be robust and fast enough
for relation extraction at Web scale.
Although Open IE is targeted at extraction of
arbitrary relations without any semantic restric-
tions, all approaches have strong language de-
pendent restrictions and require language spe-
cific information to be introduced in the corre-
sponding systems. For Spanish language, the
apporach based on rules over dependency trees
has been implemented both using full parsing
78
(Aguilar-Galicia, 2012) and using shallow depen-
dency parsing (Gamallo et al., 2012). The for-
mer work shows that this approach is too com-
putationally costly and is not always robust even
on grammatically correct texts. The latter work
does not report any results for Spanish language or
discusses any details specific to implementations
for languages other than English. Further, we are
not aware of any existing research on whether the
approach based on syntactic constraints over POS
tags can be generalized to other languages. Ad-
ditionally, although Open IE is claimed to be use-
ful for information extraction from the Web, we
are not aware of any research on its applicability
to texts randomly extracted from the Internet, i.e.,
those that have not been verified for grammatical
correctness by peers or editors.
In this paper we discuss Open IE based on syn-
tactic constraints over POS tag sequences, aimed
at Spanish language. We describe its implemen-
tation and introduce EXTRHECH, an Open IE sys-
tem for Spanish. We also compare its performance
with that of REVERB (Fader et al., 2011) on a
parallel dataset. Additionally, we evaluate perfor-
mance of our system over a dataset of texts ran-
domly extracted from the Internet and discuss the
issues that arise when processing random Internet
texts. We also give a brief analysis of errors.
The paper is organized as follows. Related work
is reviewed in Section 2. Section 3 presents our
approach to Open IE for Spanish and describes
the EXTRHECH system. Section 4 describes the
experiments for a parallel English-Spanish dataset
and for a Spanish dataset of texts randomly ex-
tracted from the Internet. In Section 5, a brief
analysis of errors is presented. Section 6 draws
the conclusions and outlines future work.
2 Related Work
There exist several approaches to Open IE.
Chronologically the first one was introduced
in the pilot works on Open IE by Banko et al.
(2007) and Etzioni et al. (2008). Their approach is
based on semi-supervised machine learning prin-
ciples and includes three main steps: (1) man-
ual labeling of a training corpus for seed relation
phrases and features; (2) further semi-supervised
learning of relations; (3) automatic extractions of
relations and their arguments. This approach is
implemented in TEXTRUNNER (Banko and Et-
zioni, 2008), WOE
pos
, and WOE
parse
, both (Wu
and Weld, 2010). In these systems, the detection
of a relation triple starts from the potential argu-
ments expressed as noun phrases, i.e., before the
connecting relation phrase is detected. Once de-
tected, neither the argument phrases nor the rela-
tion phrase can be backtracked, which makes the
approach prone to incoherent and uninformative
extractions. For example, in ?to make a deal with?,
deal can be erroneously extracted as an argument,
although it is a part of the relation phrase.
The group of rule-based approaches includes
systems based on rules applied over linguisti-
cally annotated texts. FES-2012 system (Aguilar-
Galicia, 2012) applies rules to the fully parsed sen-
tences. However, in the same work the authors
show that this approach is too slow to be scaled to
a Web-sized corpus and that it is not robust. An-
other system implementing rule-based approach is
DEPOE (Gamallo et al., 2012). In this system, the
rules are applied to the output of shallow depen-
dency parsing. In REVERB system (Fader et al.,
2011), syntactic constraints are applied over POS
tags and syntactic chunks. The last two systems
show better results in terms of precision/recall and
speed, and, consequently, scalability to a Web-
sized corpus.
Finally, the approach based on the deep au-
tomatic linguistic analysis is implemented in
OLLIE (Mausam et al., 2012). This system com-
bines various approaches: it uses output of a rule-
based Open IE system to bootstrap learning of the
relation patterns and then additionally applies lex-
ical and semantic patterns to extract relations that
are not expessed through verb phrases. Such a
complex approach leads to high-precision results
with a high yield. However, there is a tradeoff be-
tween accuracy of the output and cost of imple-
mentation and computation and complexity of the
training stage.
All these approaches require language-
dependent information for their implementation.
The third approach directly uses lexical infor-
mation for the context analysis. The other two
approaches employ language-specific morpholog-
ical and syntactic information. Of the described
systems, only two have been implemented for
languages other than English. FES-2012 system
is implemented for Spanish language; however,
its use of the full syntactic parsing does not scale
to a Web-sized corpus. DEPOE system, based on
rules over shallow dependency parsing, is claimed
79
to have its variants for Spanish, Portuguese, and
Galician languages (Gamallo et al., 2012). How-
ever, the authors do not report any experimental
results on languages other than English or any
language-specific details.
The approach based on syntactic constraints
over POS tags has not been applied to languages
other than English, in spite of that this method
can be easily adapted to other languages because it
only requires a reliable POS tagger. The basic al-
gorithm for relation extraction, according to Fader
et al. (2011), is as follows:
? First, search for a verb-containing relation
phrase in a sentence;
? If detected, search for a noun phrase to the
left of the relation phrase;
? If a noun phrase detected, search for another
noun phrase to the right of the relation phrase.
Additionally, the experiments for Open IE sys-
tems have been conducted only on texts that came
from verified sources, i.e., Wikipedia, news, or
textbooks (Banko and Etzioni, 2008; Fader et al.,
2011; Mausam et al., 2012). However, Open IE is
meant to work with Web text data that may come
from any source including those that have not been
edited or verified for grammar errors.
3 System Description
In this section we introduce EXTRHECH,
1
a sys-
tem for Open IE in Spanish. It takes a POS-tagged
text as input, applies syntactic constraints over se-
quences of POS-tags, and returns a list of extracted
relations as triples ?argument 1; relation; argu-
ment 2? that correspond to each sentence.
3.1 Basic Processing
The system takes as input a POS-tagged text. In
our experiemnts, we used a morphological ana-
lyzer from Freeling-2.2 (Padr?o et al., 2010). For
Spanish language, it returns POS tags accoridng to
EAGLES POS tag set (Leech and Wilson, 1999).
Consequently, our system is designed to work with
this POS tag set.
Spanish uses a number of non-ASCII charac-
ters, such as ?a, ?e, ?n, etc. These characters can
come in different encodings. To be able to cor-
rectly analyze text with these characters, Freeling
1
All materials are available on the page
http://www.gelbukh.com/resources/
spanish-open-fact-extraction.
analyzer should receive the input in ISO encod-
ing. Thus, the input text needs an additional pre-
processing stage to be converted into this encod-
ing. Though this might look as a minor technical
issue, guessing the original encoding becomes a
significant problem when working with texts from
arbitrary sources on the Web. We discuss encoding
related issues in Section 4.2.
After the text has been properly POS-tagged,
we feed it into EXTRHECH system, which ap-
plies the fact extraction algorithm described in
Section 2 to each sentence, one sentence at a time.
We use the same basic algorithm as in (Fader et
al., 2011) but with different triple matching rules
as appropriate for Spanish grammar.
The original POS-tag sequences for English
would produce nonsense results on Spanish input
due to substantial difference in grammars: infini-
tives are not preceded by ?to?, adjectives usually
follow nouns, and oblique case pronouns precede
verbs instead of following them, just to name a few
peculiarities of Spanish.
First, the system looks for a verb-containing
phrase in a sentence by matching it against the fol-
lowing expression:
VREL? (V W* P) | (V),
where V stands either for a single verb optionally
preceded by a reflexive pronoun (se realizaron,
?were carried out?), or a participle (calificado,
?qualified?). V W* P matches a verb with depen-
dent words, where W stands for either a noun, an
adjective, an adverb, a pronoun, or an article, and
P stands either for a preposition optionally imme-
diately followed by an infinitive, or for a gerund
(sigue siendo, ?continues to be?). The symbol
* denotes zero or more matches. Here and fur-
ther, the whole match is referred to as verb phrase
(though it is not a verb phrase in linguistic sense).
After detecting a verb phrase, EXTRHECH
looks for a noun phrase to the left from the be-
ginning of the verb phrase. This noun phrase is a
potential first argument of the relation. If a match
is found, then the system looks for another noun
phrase to the right from the end of the verb phrase.
The noun on the right side is treated as the second
argument.
Noun phrases are searched for with the follow-
ing regular expression:
NP? Np (PREP Np)?,
where Np matches a noun optionally preceded by
either an article (la din?amica, ?the dynamics?),
80
an adjective, an ordinal number (los primeros
ganadores, ?the first winners?), a number (3 casas,
?3 houses?), or their combination, and optionally
followed by either a single adjective (un esfuerzo
criminal, ?a criminal effort?), a single participle,
or both (los documentos escritos antiguos, ?the
ancient written documents?). The whole expres-
sion matched by Np can be preceded by an indef-
inite determinant construction, e.g., uno de, ?one
of ?. PREP matches a single preposition. Hence,
an entire noun phrase is either a single noun with
optional modifiers or a noun with optional modi-
fiers followed by a prepositional phrase that is a
preposition and another noun with its correspond-
ing optional modifiers (una larga lista de proble-
mas actuales, ?a long list of current problems?).
The symbol ? denotes 0 or 1 matches.
If noun phrases are matched on both sides of the
verb phrase, all three components are considered
to represent a relation and are extracted in the form
of a triple.
As an output unit, EXTRHECH returns a triple
consisting of ?argument 1; relation; argument 2?,
where argument 1 semantically is, e.g., an agent
or experiencer of the relation and argument 2 is a
general object or circumstance of the relation.
3.2 Additional Processing
Above we described the core rules and the basic
sequence for relation extraction. In addition to
them, we also implemented several optional rules
for processing of certain language constructions
that can be turned on and off with the input pa-
rameters.
First, participle clauses that follow a noun can
be searched for a relational triple if they terminate
with a noun. For example, from a phrase
Precios del caf?e suministrados por la OIC
(?Coffee prices provided by International Coffee
Organization?)
EXTRHECH returns the relation:
?Precios del caf?e; suministrados por; la OIC?.
Second, EXTRHECH also approaches resolu-
tion of coordinating conjunctions between verb
phrases and between noun phrases into corre-
sponding separate relations. Here follows the ex-
ample of a sentence with a coordinating conjunc-
tion between verb phrases:
El cerebro almacena enormes cantidades de informaci?on y
realiza millones de actividades todos los d??as
(?The brain stores vast amounts of information and performs
millions of activities every day?)
. Two facts are detected:
?El cerebro; almacena enormes cantidades de; informaci?on?
and
?El cerebro; realiza millones de; actividades todos los d??as?.
Third, relative clauses introduced by single rel-
ative pronouns (e.g., que (?that?, ?who?), cual
(?which?)) as in las partes que conforman un
trabajo de investigaci?on (?parts that make up a
research work?) are also searched for relations.
However, relative pronoun phrases with preposi-
tions, e.g. en el cual (?in which?) are not taken into
consideration for relation extraction due to their
coreferential complexity.
3.3 Limitations
The implementation of basic processing per-
formed by EXTRHECH system follows the algo-
rithm introduced in (Fader et al., 2011). This
means that extracted facts are limited to the rela-
tions expressed through a verb phrase. This limi-
tation is discussed in the cited paper.
In our apporach to Open IE in Spanish, we do
not allow pronouns to be potential arguments of
a relation. It was mainly done because of a wide
use of a neutral pronoun lo (?this?, ?which?, or no
direct translation) as a head of relative clauses in
Spanish language, e.g., lo que dio valor al poder
judicial (? that gave value to the judiciary?). In-
cluding pronouns for potential argument matches
would return a lot of uninformative relations as
?lo; dio valor a; el poder judicial?. This issue can
be solved only by introducing anaphora resolution
techniques which involves processing on a super-
sentence level. Although seemingly feasible, this
modification will necessarily slow down the ex-
traction speed which is critical while working with
large scale corpora. As mentioned in Section 2,
high speed performance is one of the main advan-
tages of the approach to Open IE based on syntac-
tis constraints compared to the others. Hence, any
modifications that would affect its speed should be
considered with caution.
Another language dependent limitation is re-
lated to the order of the processing. As
earlier described in Section 3.1, an extracted
triple is expected to correspond semantically
to ?agent/experiencer; relation; general ob-
ject/circumstance?. This is expected to be cor-
rect for a direct word order, i.e., Subject ? Verb
? (Indirect) Object, which is a dominant word or-
der for Spanish. Yet the inverted word order, i.e.
81
(Indirect) Object ? Verb ? Subject (e.g., De la
m?edula espinal nacen los nervios perif?ericos, i.e.,
literally *?From the spinal cord arise peripheral
nerves?), also occasionally takes place in gram-
matically correct and stylistically neutral Spanish
texts. However, the occurence of this construction
is less then 10% according to (Clements, 2006).
4 Experiments and Evaluation
In this section we describe the experiments con-
ducted with EXTRHECH system.
4.1 Experiment on parallel news dataset
We compare EXTRHECH?s performance with that
of REVERB, an Open IE system for English based
on the same algorithm (Fader et al., 2011). Since
these systems are designed for different languages,
we ran our experiment on a parallel dataset.
1
We took 300 parallel sentences from the
English-Spanish part of News Commentary Cor-
pus (Callison-Burch et al., 2011). Then, we ran
the extractors over the corresponding languages.
After that, two human annotators labeled each ex-
traction as correct or incorrect. For the Spanish
part of the dataset, the annotators agreed on 80%
of extractions (Cohen?s kappa ? = 0.60), whereas
for the English part they agreed on 85% of extrac-
tions with ? = 0.68. For both datasets their respec-
tive ? coefficients indicate substantial agreement
between the annotators.
Precision was calculated as a fraction of correct
extractions among all returned extractions. We
calculated Recall as a fraction of all returned cor-
rect extractions among all possible (i.e., expected)
correct extractions. By manual revision of the sen-
tences in the datasets, we made a list of all ex-
pected correct extractions. Their number was used
to estimate the recall.
In contrast to REVERB, our system does not
have a confidence score mechnaism at this point.
To make the comparison between the systems ap-
propriate, we ran REVERB extractor with the con-
fidence score level set to 0 that means that the sys-
tem returns all relations that match the rules, i.e.,
in the same way as EXTRHECH does. Hence, the
systems were in equivalent conditions. The results
of the experiment are shown in Table 1.
As we see, on a parallel dataset of texts from
News Commentary Corpus, both systems show a
very similar performance. Based on this observa-
tion, we can conclude that the algorithm suggested
System Precision Recall
Correct Returned
Extractions Extractions
EXTRHECH 0.59 0.48 218 368
REVERB 0.56 0.44 201 358
Table 1: Performance comparison of REVERB and
EXTRHECH systems over a parallel dataset.
in (Fader et al., 2011) can be easily adopted for
other languages with dominating SVO word order
and an available POS-tagger.
4.2 Experiment on Raw Web dataset
One of the most important goals of Open IE sys-
tems is to be able to process large amounts of texts
directly from the Web. This requires high per-
formance speed and robustness on texts that of-
ten lack grammatical and orthographical correct-
ness or coherence. The study showing the ap-
proach?s advantage in speed was already presented
in (Fader et al., 2011). In this work we focused on
robustness. We evaluated the performance of our
system on a dataset of sentences extracted from
the Internet ?as is?. For this dataset, we took
200 random data chunks detected by a sentence
splitter from CommonCrawl 2012 corpus (Kirk-
patrick, 2011), which is a collection of web texts
crawled from over 5 billion web pages. However,
41 from those 200 chunks were not samples of
textual information in human language but rather
pieces of programming codes or numbers. We
took out these chunks because they are not rele-
vant for our research. In a real life scenario they
could be easily detected and eliminated from the
Web data stream. After this, our dataset consisted
of 159 sentences written in human language. We
will refer to this dataset as Raw Web text dataset.
1
Of 159 sentences of the dataset, 36 sentences (22%
of the dataset) were grammatically incorrect or in-
coherent, as evaluated by a professional linguist.
We ran EXTRHECH system over this dataset
and asked two human judges to label extractions
as correct or incorrect. The annotators agreed on
70% of extractions with Cohen?s ? = 0.40, which
indicates the lower bound of moderate agreement
between judges.
Precision and Recall were calculated in the
same manner as described in Section 4.1. We com-
pare these numbers to the results obtained for the
dataset of grammatically correct sentences from
News Commentary Corpus in Table 2.
We can observe that system?s performence has
82
Dataset Precision Recall
News Commentary 0.59 0.48
Raw Web 0.55 0.49
Table 2: Performance of EXTRHECH on the gram-
matically correct dataset and the dataset of noisy
sentences extracted from the Web
not lowered significantly when processing ?noisy?
texts compared to edited newspaper texts. An in-
teresting observation is that texts from the Internet
are poorer in facts than the news texts. The num-
ber of expected extractions was manually evalu-
ated by a human expert for both datasets. The ra-
tio of extractions to sentences for the news dataset
was 1.5:1, while for the Raw Web dataset it was
only 1.03:1.
Now we will briefly discuss the issue arising
due to various encoding standards used for non-
ASCII characters, e.g., of ?a, ?e, ?n, etc. While apply-
ing Freeling morphological analyzer to the dataset,
we encountered an issue that the sentences came
in various encodings. As we mentioned in Sec-
tion 3, Freeling-2.2 analyzer works properly only
with ISO encoded input. Therefore, we had to
convert each sentence from the dataset into ISO
encoding. While most of the sentences were in
UTF-8 encoding and were converted in a single
pass, the encoding of about 3% of the sentences
was initially corrupted, therefore, they were not
processed correctly by the POS-tagger. Although
the issue is manageable at the scale of a small
dataset, it might affect the speed and quality of fact
extraction when working at Web scale.
5 Error Analysis
After running EXTRHECH on the datasets, we an-
alyzied the errors in the output. We followed
the classifications of the types of errors and their
causes suggested in (Zhila and Gelbukh, 2014).
The distribution of the errors in EXTRHECH?s out-
put over the types of errors is shown in Table 3.
The data about error types was gathered over ex-
tractions from Raw Web dataset. When errors are
present both in the arguments and in the relation
phrase, they are likely to have the same cause.
Based on the analysis of the outputs over Raw
Web dataset, the following causes for errors have
been observed:
? Underspecified noun phrase
? Overspecified verb phrase
? Non-contiguous verb phrase
Type of errors Percentage
Incorrect relation phrase 21%
Incorrect argument(s) 45%
of them, with also incorrect relation 19%
Incorrect argument order 6%
Table 3: Distribution of errors in output by
the basic error types in relation extraction for
EXTRHECH system run over Raw Web dataset
? N-ary relation or preposition (e.g., entre, ?be-
tween?)
? Conditional subordinate clause
? Incorrectly resolved relative clause
? Incorrectly resolved conjunction
? Inverse word order
? Incorrect POS-tagging
? Grammatical errors in original sentences
Inverse word order is one of the main causes for
the incorrect order of arguments in extracted rela-
tions. However, as it can be seen in Table 3, this
is the least common type of errors, which is in ac-
cordance to the low frecuency of the inverse word
order (Clements, 2006). A more detailed analysis
of the issues that cause the errors can be found in
(Zhila and Gelbukh, 2014).
6 Conclusions
We have introduced an approach to Open IE based
on syntactic constraints over POS tag sequences
targeted at Spanish language. We described the
rules for relation phrases and their arguments in
Spanish and their implementation in EXTRHECH
system. Further, we presented a series of ex-
periments with EXTRHECH and showed (1) that
the performance of this approach to Open IE
is similar for English and Spanish, and (2) that
EXTRHECH?s performance is robust on texts of
varying quality. We also gave a brief classification
of errors by their types and causes.
Our future plans include implementation of
shallow parsing and syntactic n-grams (Sidorov
et al., 2012; Sidorov et al., 2013; Sidorov et al.,
2014; Sidorov, 2013a; Sidorov, 2013b), as well as
learning techniques, and analysis of their influence
on the system?s performance.
Acknowledgments
The work was partially supported by the Gov-
ernment of Mexico: SIP-IPN 20144534 and
20144274, PIFI-IPN, and SNI. We thank Yahoo!
for travel and conference support for this paper.
83
References
Honorato Aguilar-Galicia. 2012. Extracci?on au-
tom?atica de informaci?on sem?antica basada en estruc-
turas sint?acticas. Master?s thesis, Center for Com-
puting Research, Instituto Polit?ecnico Nacional,
Mexico City, D.F., Mexico.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36. Associ-
ation for Computational Linguistics, June.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, pages 2670?2676.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Joseph Clancy Clements. 2006. Primary and sec-
ondary object marking in Spanish. In J. Clancy
Clements and Jiyoung Yoon, editors, Functional ap-
proaches to Spanish syntax: Lexical semantics, dis-
course, and trasitivity, pages 115?133. London: Pal-
grave MacMillan.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, De-
cember.
Oren Etzioni. 2011. Search Needs a Shake-Up. Na-
ture, 476(7358):25?26, August.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pablo Gamallo, Marcos Garcia, and Santiago
Fern?andez-Lanza. 2012. Dependency-based
open information extraction. In Proceedings of
the Joint Workshop on Unsupervised and Semi-
Supervised Learning in NLP, ROBUS-UNSUP ?12,
pages 10?18, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christopher Horn, Alisa Zhila, Alexander Gelbukh,
Roman Kern, and Elisabeth Lex. 2013. Using fac-
tual density to measure informativeness of web doc-
uments. In Proceedings of the 19th Nordic Confer-
ence on Computational Linguistics, NoDaLiDa.
Marshall Kirkpatrick. 2011. New 5 billion
page web index with page rank now avail-
able for free from common crawl foundation.
http://readwrite.com/2011/11/07/
common_crawl_foundation_announces_
5_billion_page_w, November. [last visited on
25/01/2013].
Geoffrey Leech and Andrew Wilson. 1999. Standards
for tagsets. In Syntactic Wordclass Tagging, pages
55?80. Springer Netherlands.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Iden-
tifying functional relations in web text. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1266?1276.
Association for Computational Linguistics, October.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In EMNLP-CoNLL,
pages 523?534. ACL.
Llu??s Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic services in freeling 2.1:
Wordnet and ukb. In Pushpak Bhattacharyya, Chris-
tiane Fellbaum, and Piek Vossen, editors, Princi-
ples, Construction, and Application of Multilingual
Wordnets, pages 99?105, Mumbai, India, February.
Global Wordnet Conference 2010, Narosa Publish-
ing House.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2012. Syntactic dependency-based n-
grams as classification features. In M. Gonz?alez-
Mendoza and I. Batyrshin, editors, Advances in
Computational Intelligence. Proceedings of MICAI
2012, volume 7630 of Lecture Notes in Artificial In-
telligence, pages 1?11. Springer.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2013. Syntactic dependency-based n-
grams: More evidence of usefulness in classifica-
tion. In Alexander Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing. Proceed-
ings of International Conference on Intelligent Text
Processing and Computational Linguistics, CICLing
2013, volume 7816 of Lecture Notes in Artificial In-
telligence, pages 13?24. Springer.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2014. Syntactic n-grams as machine
learning features for natural language processing.
Expert Systems with Applications, 41(3):853?860.
Grigori Sidorov. 2013a. Non-continuous syntactic n-
grams. Polibits, 48:67?75.
Grigori Sidorov. 2013b. Syntactic dependency based
n-grams in rule based automatic english as second
language grammar correction. International Jour-
nal of Computational Linguistics and Applications,
4(2):169?188.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
84
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 118?127,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alisa Zhila and Alexander Gelbukh. 2014. Automatic
identification of facts in real internet texts in Spanish
using lightweight syntactic constraints: Problems,
their causes, and ways for improvement. Submitted.
85
