Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801?808
Manchester, August 2008
Discourse Level Opinion Interpretation ?
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Abstract
This work proposes opinion frames as a
representation of discourse-level associa-
tions which arise from related opinion top-
ics. We illustrate how opinion frames help
gather more information and also assist
disambiguation. Finally we present the re-
sults of our experiments to detect these as-
sociations.
1 Introduction
Opinions have been investigated at the phrase, sen-
tence, and document levels. However, little work
has been carried out regarding interpreting opin-
ions at the level of the discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television
(the opinion targets ? what the opinions are about
? are shown in italics).
(1) D :: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons.
Speaker D expresses an opinion in favor of a
design that is simple and organic in shape, and
against an alternative design which is not. Several
individual opinions are expressed in this passage.
The first is a negative opinion about the design be-
ing too edgy and box-like, the next is a positive
opinion toward a hand-held design, followed by a
negative opinion toward a computery shape, and
so on. While recognizing individual expressions
?This research was supported in part by the Department of
Homeland Security under grant N000140710152.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of opinions and their properties is important, dis-
course interpretation is needed as well. It is by un-
derstanding the passage as a discourse that we see
edgy, like a box, computery, and many buttons as
descriptions of the type of design D does not pre-
fer, and hand-held, organic shape, and simple de-
signs as descriptions of the type he does. These de-
scriptions are not in general synonyms/antonyms
of one another; for example, there are hand-held
?computery? devices and simple designs that are
edgy. The unison/opposition among the descrip-
tions is due to how they are used in the discourse.
This paper focuses on such relations between
the targets of opinions in discourse. Specifically, in
this work, we propose a scheme of opinion frames,
which consist of two opinions that are related by
virtue of having united or opposed targets. We
argue that recognizing opinion frames will pro-
vide more opinion information for NLP applica-
tions than recognizing individual opinions alone.
Further, if there is uncertainty about any one of the
components, we believe opinion frames are an ef-
fective representation incorporating discourse in-
formation to make an overall coherent interpreta-
tion (Hobbs et al, 1993). Finally, we also report
the first results of experiments in recognizing the
presence of these opinion frames.
We introduce our data in Section 2, present
opinion frames in Section 3 and illustrate their util-
ity in Section 4. Our experiments are in Section 5,
related work is discussed in Section 6, and conclu-
sions are in Section 7.
2 Data
The data used in this work is the AMI meet-
ing corpus (Carletta et al, 2005) which con-
tains multi-modal recordings of group meetings.
Each meeting has rich transcription and seg-
801
ment (turn/utterance) information for each speaker.
Each utterance consists of one or more sentences.
We also use some of the accompanying manual an-
notations (like adjacency pairs) as features in our
machine learning experiments.
3 Opinion Frames
In this section, we lay out definitions relating to
opinion frames, illustrate with examples how these
are manifested in our data, and consider them in
the context of discourse relations.
3.1 Definitions
The components of opinion frames are individual
opinions and the relationships between their tar-
gets. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), we address two types of
opinions, sentiment and arguing.
Sentiment includes positive and negative eval-
uations, emotions, and judgments. Arguing in-
cludes arguing for or against something, and argu-
ing that something should or should not be done.
Opinions have a polarity that can be positive or
negative. 1 The target of an opinion is the entity or
proposition that the opinion is about. We establish
relations between targets, in the process relating
their respective opinions. We address two types of
relations, same and alternative.
The same relation holds between targets that
refer to the same entity, property, or proposi-
tion. Observing the relations marked by an-
notators, we found that same covers not only
identity, but also part-whole, synonymy, gener-
alization, specialization, entity-attribute, instan-
tiation, cause-effect, epithets and implicit back-
ground topic, i.e., relations that have been studied
by many researchers in the context of anaphora and
co-reference (e.g. (Clark, 1975; Vieira and Poe-
sio, 2000; Mueller and Strube, 2001)). Actually,
same relations holding between entities often in-
volve co-reference (where co-reference is broadly
conceived to include relations such as part-whole
listed above). However, there are no morpho-
syntactic constraints on what targets may be. Thus,
same relations may also hold between adjective
phrases, verb phrases, and clauses. An instance of
this is Example 1, where the same target relation
holds between the adjectives edgy and computery.
1Polarity can also be neutral or both (Wilson and Wiebe,
2005), but these values are not significant for our opinion
frames.
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
The alternative relation holds between targets
that are related by virtue of being opposing (mu-
tually exclusive) options in the context of the dis-
course. For example, in the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. Objects appear as alternatives via world and
domain knowledge (for example, shapes of a re-
mote); the context of the discourse (for example,
Hillary Clinton and Barak Obama are alternatives
in discussions of the primaries, but not in discus-
sions of the general election); and the way the ob-
jects are juxtaposed while expressing opinions (for
instance hand-held and computery in Example 1).
While same and alternative are not the only pos-
sible relations between targets, they commonly oc-
cur in task-oriented dialogs such as those in the
data we use.
Now that we have all the ingredients, we can
define opinion frames. An opinion frame is de-
fined as a structure composed of two opinions and
their respective targets connected via their target
relations. With four opinion type/polarity pairs
(SN,SP,AN,AP), for each of two opinion slots, and
two possible target relations, we have 4 * 4 * 2 =
32 types of frame, listed in Table 1.
3.2 Examples
We will now illustrate how the frames are applied
with the following meeting snippets from the AMI
meeting corpus. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
The text span capturing the target of the opinion
(as interpreted in context) is indicated in italics. To
make it easier to understand the opinion frames,
we separately list each opinion, followed by the
major relation between the targets and, in paren-
theses, the relevant subtype of the major relation.
In the passage below, the speaker D expresses
802
his preferences about the material for the TV re-
mote.
(2) D:: ... this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot.
A bit more durable and that 2 can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same (ellipsis)
t3 - t4 same (identity)
t1 - t3 same (identity)
The speaker?s positive sentiment regarding the
rubbery material is apparent from the text spans
bit more bouncy (Sentiment Positive or SP), bit
more durable (SP), ergonomic (SP) and a bit dif-
ferent from all the other remote controls (SP).
As shown, the targets of these opinions (it?s [t1],
that [t3], and it [t4]) are related by the same rela-
tion. The ellipsis occurs with bit more durable.
Target [t2] represents the (implicit) target of that
opinion, and [t2] has a same relation to [t1], the
target of the bit more bouncy opinion. The opin-
ion frames occurring throughout this passage are
all SPSPsame denoting that both the opinion com-
ponents are sentiments with positive polarity with
a same relation between their targets. One frame
occurs between O1 and O2, another between O3
and O4, and so on.
Example 2 illustrates relatively simple same re-
lations between targets. Now let us consider the
more involved passage below, in which a meeting
participant analyzes two leading remotes on the
market.
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same (identity)
t2 - t3 same (t3 subset of t2)
2Note that the ?that? refers to the property of being
durable; however, as our annotation scheme is not hierarchi-
cal, we connect it to the entity the opinion is about ? in this
case the rubbery material.
t3 - t4 same (t4 partof t3)
t5 - t1 same (identity)
Target [t2] is the set of two leading remotes; [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is a part of that remote, namely its
buttons. Thus, opinion O3 is directly about one of
the remotes, and indirectly about the set of both re-
motes. Similarly, O4 is directly about the buttons
of one of the remotes, and indirectly about that re-
mote itself. The assessments at different levels ac-
crue toward the analysis of the main topic under
consideration.
Moving on to alternative (alt) relations, con-
sider the passage below, where the speaker is ar-
guing for the curved shape.
(4) C:: . . . shapes should be curved, so round shapes.
Nothing square-like.
.
.
.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same (specification)
t3 - t4 same (epithet)
Opinion O1 argues for a curved shape, O2 ar-
gues against a square shape, and O3 argues against
square corners. Note that square corners is also
the target of a negative sentiment, O4, expressed
here by too. Opinion O5 argues against the old
box look. In addition, the wording old box look
implies a negative sentiment ? O6 (we list the tar-
get span as ?old box look,? which refers to the look
of having square corners).
There is an alt relation between [t1] and [t2].
Thus, we have an opinion frame of type APANalt
between O1 and O2. From this frame, we are able
to understand that a positive opinion is expressed
toward something and a negative opinion is ex-
pressed toward its alternative.
3.3 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
803
linked by the human annotators. This path can be
traversed to create links between new pairs of tar-
gets, which in turn results in new opinion frame
relations.
Let us illustrate this idea with Example 4. The
frames with direct relations are O1O2 APANalt.
By following the alt link from [t1] to [t2] and the
same link from [t2] to [t3], we have an alt link
between [t1] and [t3], and the additional frames
O1O3 APANalt and O1O4 APSNalt. Repeating
this process would finally link speaker C?s opinion
O1 with B?s opinion O6 via a APSNalt frame.
Simple recipes such as this can be used by ap-
plications such as QA to gather more information
from the discourse.
3.4 Frame Types
In our corpus, we found that the 32 frames of Ta-
ble 1 can be categorized into two functional types:
reinforcing frames and non-reinforcing frames.
The set of frames that occur in scenarios where
the speaker intends to fortify or reinforce his opin-
ion/stance are called reinforcing frames. These
are the ones in the top row of the Table 1. Note that
these frames cover all opinion types, polarities and
target relations. It is the particular combination of
these frame components that bring about the rein-
forcement of the opinion in the discourse.
On the other hand, the frames at the bottom row
of the table are non-reinforcing. In our corpus,
these frames occur when a speaker is ambivalent
or weighing pros and cons.
Example 2 is characterized by opinion frames
in which the opinions reinforce one another ? that
is, individual positive sentiments (SP) occurring
throughout the passage fortify the positive regard
for the rubbery material via the same target rela-
tions and the resulting SPSPsame frames.
Interestingly, interplays among different opin-
ion types may show the same type of reinforce-
ment. For instance, Example 4 is characterized by
mixtures of opinion types, polarities, and target re-
lations. However, the opinions are still unified in
the intention to argue for a particular type of shape.
3.5 Discourse Relations and Opinion Frames
Opinion-frame recognition and discourse interpre-
tation go hand in hand; together, they provide
richer overall interpretations. For example, con-
sider the opinion frames and the Penn Discourse
Treebank relations (Prasad et al, 2007) for Ex-
ample 2. PDTB would see a list or conjunction
relation between the clauses containing opinions
bit more durable (O2) and ergonomic (O3), as
well as between the clauses containing opinions
ergonomic (O3) and a bit different from all the
other remote controls (O4). All of our opinion
frames for this passage are of type SPSPsame, a
reinforcing frame type. This passage illustrates
the case in which discourse relations nicely corre-
spond to opinion frames. The opinion frames flesh
out the discourse relations: we have lists specifi-
cally of positive sentiments toward related objects.
However, opinion-frame and discourse-relation
schemes are not redundant. Consider the following
three passages.
(e1) Non-reinforcing opinion frame (SNSPsame); Con-
trast discourse relation
D:: . . . I draw for you this schema that can be maybe
too technical for you but is very important for me
. . ..
(e2) Reinforcing opinion frame (SNAPalt); Contrast
discourse relation
D:: not too edgy and like a box, more kind of hand-
held
(e3) Reinforcing opinion frame (SPSPsame); no dis-
course relation
. . . they want something that?s easier to use straight
away, more intuitive perhaps.
In both e1 and e2, the discourse relation be-
tween the two opinions is contrast (?too technical?
is contrasted with ?very important?, and ?not too
edgy and like a box? is contrasted with ?more kind
of hand-held?). However, the opinion frame in e1
is SNSPsame, which is a non-reinforcing frame,
while the opinion frame in e2 is SNAPalt, which
is a reinforcing frame. In e3, the opinion frame
holds between targets within a subordinated clause
(easier to use and more intuitive are two desired
targets); most discourse theories don?t predict any
discourse relation in this situation.
Generally speaking, we find that there are not
definitive mappings between opinion frames and
the relations of popular discourse theories. For ex-
ample, Hobbs? (Hobbs et al, 1993) contrast cov-
ers at least four of our frames (SPSPalt, APAPalt,
APANsame, SPSNsame), while, for instance, our
SPSPsame frame can map to both the elaboration
and explanation relations.
4 Benefits of Discourse Opinion Frames
This section argues for two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in
the text, and they may contribute toward arriving
804
Positive Negative
Counting only individual opinions
Accepted Items 120 20
Rejected Items 9 12
individual + opinions via Reinforcing Opinion frames
Accepted Items 252 63
Rejected Items 22 26
Table 2: Opinion Polarity Distribution for Ac-
cepted/Rejected Items
at a coherent interpretation (Hobbs et al, 1993) of
the opinions in the discourse.
4.1 Gathering More Information
Frame relations provide a mechanism to relate
opinions expressed in non-local contexts - the
opinion may occur elsewhere in the discourse, but
will become relevant to a given target due to a re-
lation between its target and the given target. For
instance, in Example 3, there is one direct eval-
uation of the leading remotes (O1) and two eval-
uations via identity (O2, O5). Following frames
constructed via t2-t3 and t3-t4, we get two more
opinions (O3 and O4) for the leading remotes.
Furthermore, opinions regarding something not
lexically or even anaphorically related can be-
come relevant, providing more opinion informa-
tion. This is particularly interesting when alt re-
lations are involved, as opinions towards one alter-
native imply opinions of opposite polarity toward
the competing options. For instance in Example 4,
if we consider only the explicitly stated opinions,
there is only one (positive) opinion, O1, about the
curved shape. However, the speaker expresses sev-
eral other opinions which reinforce his positivity
toward the curved shape. Thus, by using the frame
information, it is possible to gather more opinions
regarding curved shapes for TV remotes.
As a simple proof of concept, we counted the
number of positive and negative opinions towards
the items that were accepted or rejected in the
meetings (information about accepted and rejected
items is obtained from the manual abstractive sum-
maries provided by the AMI corpus). Counts are
obtained, over opinions manually annotated in the
data, for two conditions: with and without frame
information. The items in our meeting data are
mainly options for the new TV remote, which in-
clude attributes and features like different shapes,
materials, designs, and functionalities. We ob-
served that for the accepted items, the number of
positive opinions is higher and, for rejected items,
the number of negative opinions is higher. The
top section of Table 2 shows a contingency ta-
ble of counts of positive/negative opinions for ac-
cepted/rejected items for 5 AMI meetings.
Then we counted the number of reinforc-
ing opinions that were expressed regarding these
items. This meant also counting additional opin-
ions that were related via reinforcing frames. The
bottom section of Table 2 shows the counts when
the reinforcing frames are considered. Compared
to the counts of only individual opinions, we see
that the numbers in each cell have increased, while
maintaining the same pattern of distribution.
Thus, in effect we have procured more instances
of opinions for the items. We believe this added
information would help applications like meeting
summarizers and QA systems to make more in-
formed decisions.
4.2 Interdependent Interpretation
We believe that our opinion frames, anaphoric re-
lations and discourse relations can symbiotically
help disambiguate each other in the discourse. In
particular, suppose that some aspect of an individ-
ual opinion, such as polarity, is unclear. If the dis-
course suggests certain opinion frames, this may in
turn resolve the underlying ambiguity.
Revisiting Example 2 from above, we see that
out of context, the polarities of bouncy and dif-
ferent from other remotes are unclear (bounci-
ness and being different may be negative attributes
for another type of object). However, the polari-
ties of two of the opinions are clear (durable and
ergonomic). There is evidence in this passage of
discourse continuity and same relations such as the
pronouns, the lack of contrastive cue phrases, and
so on. This evidence suggests that the speaker ex-
presses similar opinions throughout the passage,
making the opinion frame SPSPsame more likely
throughout. Recognizing the frames would resolve
the polarity ambiguities of bouncy and different.
In the following example (5), the positive senti-
ment (SP) towards the this and the positive arguing
(AP) for the it are clear. These two individual opin-
ions can be related by a same/alt target relation, be
unrelated, or have some other relation not covered
by our scheme (in which case we would not have
a relation between them). There is evidence in the
discourse that makes one interpretation more likely
than others. The ?so? indicates that the two clauses
are highly likely to be related by a cause discourse
805
relation (PDTB). This information confirms a dis-
course continuity, as well as makes a reinforcing
scenario likely, which makes the reinforcing frame
SPAPsame highly probable. This increase in like-
lihood will in turn help a coreference system to in-
crease its confidence that the ?that? and the ?it?
co-refer.
(5) B :: ... and this will definitely enhance our market
sales, so we should take it into consideration also.
Opinion Span - target Span Rel
O1 definitely enhance our market sales - this [t1] SP
O2 so we should - it [t2] AP
Target - target Rel
t1 -t2 same (identity)
5 Experiments
There has been much work on recognizing indi-
vidual aspects of opinions like extracting individ-
ual opinions from phrases or sentences and recog-
nizing opinion type and polarity. Accordingly, in
our machine learning experiments we assume ora-
cle opinion and polarity information. Our experi-
ments thus focus on the new question: ?Given two
opinion sentences, determine if they participate in
any frame relation.? Here, an opinion sentence is a
sentence containing one or more sentiment or ar-
guing expression. In this work, we consider frame
detection only between sentence pairs belonging to
the same speaker.
5.1 Annotation of Gold Standard
Creating gold-standard opinion-frame data is ac-
complished by annotating frame components and
then building the frames from those underlying an-
notations.
We began with annotations created by Soma-
sundaran et al (2007), namely four meetings
of the AMI meeting corpus annotated for senti-
ment and arguing opinions (text anchor and type).
Following that annotation scheme, we annotated
an additional meeting. This gave us a corpus of
4436 sentences or 2942 segments (utterances). We
added attributes to the existing opinion annota-
tions, namely polarity and target-id. The target-
id attribute links the opinion to its local target
span. Relations between targets were then anno-
tated. When a newly annotated target is similar (or
opposed) to a set of targets already participating in
same relations, then the same (or alt) link is made
only to one of them - the one that seems most natu-
ral. This is often the one that is physically closest.
Content Word overlap between the sentence pair
Focus space overlap between the sentence pair
Anaphoric indicator in the second sentence
Time difference between the sentence pair
Number of intervening sentences
Existence of adjacency pair between the sentence pair
Bag of words for each sentence
Table 3: Features for Opinion Frame detection
Link transitivity is then used to connect targets that
are not explicitly linked by the annotators.
All annotations were performed by two of the
co-authors of this paper by consensus labeling.
The details of our annotation scheme and inter-
annotator agreement studies are presented in (So-
masundaran et al, 2008).
Once the individual frame components are an-
notated, conceptually, a frame exists for a pair of
opinions if their polarities are either positive or
negative and their targets are in a same or alt rela-
tion. For our experiments, if a path exists between
two targets, then their opinions are considered to
be participating in an opinion-frame relation.
The experimental data consists of pairs of opin-
ion sentences and the gold-standard information
whether there exists a frame between them. We
approximate continuous discourse by only pair-
ing sentences that are not more than 10 sentences
apart. We also filter out sentences that are less than
two words in length in order to handle data skew-
ness. This filters out very small sentences (e.g.,
?Cool.?) which rarely participate in frames. The
experiments were performed on a total of 2539
sentence pairs, of which 551 are positive instances.
5.2 Features
The factor that determines if two opinions are
related is primarily the target relations between
them. Instead of first finding the target span for
each opinion sentence and then inferring if they
should be related, we directly try to encode target
relation information in our features. By this ap-
proach, even in the absence of explicit target-span
information, we are able to determine if the opin-
ion sentence pairs are related.
We explored a number of features to incorpo-
rate this. The set that give the best performance
are listed in Table 3. The content word overlap
feature captures the degree of topic overlap be-
tween the sentence pair, and looks for target re-
lations via identity. The focus space overlap fea-
ture is motivated by our observation that partici-
806
Acc. Prec. Recall F-measure
False 78.3% - 0% -
Distribution 66% 21.7% 21.7% 21.4%
Random 50.0% 21.5% 49.4% 29.8 %
True 21.7% 21.6% 100% 35.5 %
System 67.6% 36.8% 64.9% 46%
Table 4: Automatic Detection of Opinion Frames
pants refer to an established discourse topic with-
out explicitly referring to it. Thus, we construct a
focus space for each sentence containing recently
used NP chunks. The feature is the percent over-
lap between the focus spaces of the two opinion
sentences. The anaphoric indicator feature checks
for the presence of pronouns such as it and that
in the second sentence to account for target rela-
tions via anaphora. The time difference between
the sentences and the number of intervening sen-
tences are useful features to capture the idea that
topics shift with time. The existence of an adja-
cency pair 3 between the sentences can clue the
system that the opinions in the sentences are re-
lated too. Finally, standard bag of words features
are included for each sentence.
5.3 Results
We performed 5-fold cross validation experiments,
using the standard SVMperf package (Joachims,
2005), an implementation of SVMs designed
for optimizing multivariate performance measures.
We found that, on our skewed data, optimizing on
F-measure obtains the best results.
Our system is compared to four baselines in Ta-
ble 4. The majority class baseline which always
guesses false (False) has good accuracy but zero
recall. The baseline that always guesses true (True)
has 100% recall and the best f-measure among the
baselines, but poor accuracy. We also constructed
a baseline that guesses true/false over the test set
based on the distribution in the training data (Dis-
tribution). This baseline is smarter than the other
baselines, as it does not indiscriminately guess any
one of the class. The last baseline Random guesses
true 50% of the time.
The bottom row of Table 4 shows the perfor-
mance of our system (System). The skewness of
the data affects the baselines as well as our sys-
tem. Our system beats the best baseline f-measure
by over 10 percentage points, and the best base-
line precision by 14 percentage points. Comparing
3Adjacency Pairs are manual dialog annotations available
in the AMI corpus.
it to the baseline which has comparable accuracy,
namely Distribution, we see that our system im-
proves in f-measure by 24 percentage points.
Our results are encouraging - even using simple
features to capture target relations achieves consid-
erable improvement over the baselines. However,
there is much room for improvement. Using more
detailed target and discourse information promises
to further improve system performance. These are
avenues for future work.
6 Related work
Evidence from the surrounding context has been
used previously to determine if the current sen-
tence should be subjective/objective (Riloff et al,
2003; Pang and Lee, 2004) and adjacency pair in-
formation has been used to predict congressional
votes (Thomas et al, 2006). However, these meth-
ods do not explicitly model the relations between
opinions. An application of the idea of alterna-
tive targets can be seen in Kim and Hovy?s (2007)
work on election prediction. They assume that if
a speaker expresses support for one party, all men-
tions of the competing parties have negative po-
larity, thus creating automatically labeled training
data.
In the field of product review mining, sentiments
and features (aspects) have been mined (Popescu
and Etzioni, 2005), where the aspects correspond
to our definition of targets. However, the aspects
themselves are not related to each other in any
fashion.
Polanyi and Zaenen (2006), in their discussion
on contextual valence shifters, have also observed
the phenomena described in this work - namely
that a central topic may be divided into subtopics
in order to perform evaluations, and that discourse
structure can influence the overall interpretation of
valence. Snyder and Barzilay (2007) combine an
agreement model based on contrastive RST rela-
tions with a local aspect model to make a more
informed overall decision for sentiment classifi-
cation. In our scheme, their aspects would be
related as same and their high contrast relations
would correspond to the non-reinforcing frames
SPSNsame, SNSPsame. Additionally, our frame
relations would link the sentiments across non-
adjacent clauses, and make connections via alt tar-
get relations.
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
807
schemes for various available corpora of conver-
sation (e.g., Carletta et al (2005) for AMI). As
shown by Somasundaran et al (2007), dialog
structure information and opinions are in fact com-
plementary. We believe that, like the discourse
relations, the dialog information will additionally
help in arriving at an overall coherent interpreta-
tion.
7 Conclusions
In this paper, we described the idea of opin-
ion frames as a representation capturing discourse
level relations that arise from related opinion tar-
gets and which are common in task-oriented di-
alogs. We introduced the alternative relations that
hold between targets by virtue of being opposing
in the discourse context. We discussed how our
opinion-frame scheme and discourse relations go
hand in hand to provide a richer overall interpreta-
tion. We also illustrated that such discourse level
opinion associations have useful benefits, namely
they help gather more opinion information and
help interdependent interpretation. Finally, we
showed via our machine learning experiments that
the presence of opinion frames can be automati-
cally detected.
References
Carletta, J., S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of Measuring Behavior Symposium on
?Annotating and measuring Meeting Behavior?.
Clark, H. H. 1975. Bridging. Theoretical issues in
natural language processing . New York: ACM.
Hobbs, J., M. Stickel, D. Appelt, and P. Martin. 1993.
Interpretation as abduction. AI, 63.
Joachims, T. 2005. A support vector method for multi-
variate performance measures. In ICML 2005.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL 2007.
Mueller, C. and M. Strube. 2001. Annotating
anaphoric and bridging relations with mmax. In 2nd
SIGdial Workshop on Discourse and Dialogue.
Pang, B. and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
Polanyi, L. and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect
in Text: Theory and Applications. Springer.
Popescu, A.-M. and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber, 2007. PDTB 2.0 Anno-
tation Manual.
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning
subjective nouns using extraction pattern bootstrap-
ping. In CoNLL 2003.
Snyder, B. and R. Barzilay. 2007. Multiple aspect
ranking using the good grief algorithm. In HLT
2007: NAACL.
Somasundaran, S., J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
Somasundaran, S, J Ruppenhofer, and J Wiebe. 2008.
Discourse level opinion relations: An annotation
study. In SIGdial Workshop on Discourse and Di-
alogue. ACL.
Thomas, M., B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
Vieira, R. and M. Poesio. 2000. An empirically based
system for processing definite descriptions. Comput.
Linguist., 26(4).
Wilson, T. and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
808
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170?179,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Supervised and Unsupervised Methods in Employing Discourse Relations
for Improving Opinion Polarity Classification
Swapna Somasundaran
Univ. of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Galileo Namata
Univ. of Maryland
College Park, MD 20742
namatag@cs.umd.edu
Janyce Wiebe
Univ. of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Lise Getoor
Univ. of Maryland
College Park, MD 20742
getoor@cs.umd.edu
Abstract
This work investigates design choices in
modeling a discourse scheme for im-
proving opinion polarity classification.
For this, two diverse global inference
paradigms are used: a supervised collec-
tive classification framework and an un-
supervised optimization framework. Both
approaches perform substantially better
than baseline approaches, establishing the
efficacy of the methods and the underlying
discourse scheme. We also present quan-
titative and qualitative analyses showing
how the improvements are achieved.
1 Introduction
The importance of discourse in opinion analy-
sis is being increasingly recognized (Polanyi and
Zaenen, 2006). Motivated by the need to en-
able discourse-based opinion analysis, previous
research (Asher et al, 2008; Somasundaran et al,
2008) developed discourse schemes and created
manually annotated corpora. However, it was not
known whether and how well these linguistic ideas
and schemes can be translated into effective com-
putational implementations.
In this paper, we first investigate ways in which
an opinion discourse scheme can be computation-
ally modeled, and then how it can be utilized to
improve polarity classification. Specifically, the
discourse scheme we use is from Somasundaran
et al (2008), which was developed to support a
global, interdependent polarity interpretation. To
achieve discourse-based global inference, we ex-
plore two different frameworks. The first is a
supervised framework that learns interdependent
opinion interpretations from training data. The
second is an unsupervised optimization frame-
work which uses constraints to express the ideas
of coherent opinion interpretation embodied in the
scheme. For the supervised framework, we use It-
erative Collective Classification (ICA), which fa-
cilitates machine learning using relational infor-
mation. The unsupervised optimization is imple-
mented as an Integer Linear Programming (ILP)
problem. Via our implementations, we aim to
empirically test if discourse-based approaches to
opinion analysis are useful.
Our results show that both of our implemen-
tations achieve significantly better accuracies in
polarity classification than classifiers using local
information alone. This confirms the hypothesis
that the discourse-based scheme is useful, and also
shows that both of our design choices are effective.
We also find that there is a difference in the way
ICA and ILP achieve improvements, and a simple
hybrid approach, which incorporates the strengths
of both, is able to achieve significant overall im-
provements over both. Our analyses show that
even when our discourse-based methods bootstrap
from noisy classifications, they can achieve good
improvements.
The rest of this paper is organized as follows:
we discuss related work in Section 2 and the
discourse scheme in Section 3. We present our
discourse-based implementations in Section 4, ex-
periments in Section 5, discussions in Section 6
and conclusions in Section 7.
2 Related Work
Previous work on polarity disambiguation has
used contextual clues and reversal words (Wil-
son et al, 2005; Kennedy and Inkpen, 2006;
Kanayama and Nasukawa, 2006; Devitt and Ah-
mad, 2007; Sadamitsu et al, 2008). However,
these do not capture discourse-level relations.
Researchers, such as (Polanyi and Zaenen,
2006), have discussed how the discourse struc-
ture can influence opinion interpretation; and pre-
vious work, such as (Asher et al, 2008; Soma-
sundaran et al, 2008), have developed annota-
170
tion schemes for interpreting opinions with dis-
course relations. However, they do not empiri-
cally demonstrate how automatic methods can use
their ideas to improve polarity classification. In
this work, we demonstrate concrete ways in which
a discourse-based scheme can be modeled using
global inference paradigms.
Joint models have been previously explored for
other NLP problems (Haghighi et al, 2005; Mos-
chitti et al, 2006; Moschitti, 2009). Our global in-
ference model focuses on opinion polarity recog-
nition task.
The biggest difference between this work and
previous work in opinion analysis that use global
inference methods is in the type of linguistic
relations used to achieve the global inference.
Some of the work is not related to discourse
at all (e.g., lexical similarities (Takamura et al,
2007), morphosyntactic similarities (Popescu and
Etzioni, 2005) and word-based measures like TF-
IDF (Goldberg and Zhu, 2006)). Others use
sentence cohesion (Pang and Lee, 2004), agree-
ment/disagreement between speakers (Thomas et
al., 2006; Bansal et al, 2008), or structural adja-
cency. In contrast, our work focuses on discourse-
based relations for global inference. Another dif-
ference from the above work is that our work is
over multi-party conversations.
Previous work on emotion and subjectivity
detection in multi-party conversations has ex-
plored using prosodic information (Neiberg et al,
2006), combining linguistic and acoustic infor-
mation (Raaijmakers et al, 2008) and combining
lexical and dialog information (Somasundaran et
al., 2007). Our work is focused on harnessing
discourse-based knowledge and on interdependent
inference.
There are several collective classification
frameworks, including (Neville and Jensen, 2000;
Lu and Getoor, 2003; Taskar et al, 2004; Richard-
son and Domingos, 2006; Bilgic et al, 2007). In
this paper, we use an approach by (Lu and Getoor,
2003) which iteratively predicts class values using
local and relational features. ILP has been used
on other NLP tasks, e.g., (Denis and Baldridge,
2007; Choi et al, 2006; Roth and Yih, 2004). In
this work, we employ ILP for modeling discourse
constraints for polarity classification.
3 Discourse Scheme and Data
The scheme in Somasundaran et al (2008) has
been developed and annotated over the AMI meet-
ing corpus (Carletta et al, 2005).
1
This scheme
annotates opinions, their polarities (positive, neg-
ative, neutral) and their targets (a target is what
the opinion is about). The targets of opinions are
related via two types of relations: the same rela-
tion, which relates targets referring to the same
entity or proposition, and the alternative relation,
which relates targets referring to mutually exclu-
sive options in the context of the discourse. Ad-
ditionally, the scheme relates opinions via two
types of frame relations: the reinforcing and non-
reinforcing relations. The frame relations repre-
sent discourse scenarios: reinforcing relations ex-
ist between opinions when they contribute to the
same overall stance, while non-reinforcing rela-
tions exist between opinions that show ambiva-
lence.
The opinion annotations are text-span based,
while in this work, we use Dialog Act (DA) based
segmentation of meetings.
2
As the DAs are our
units of classification, we map opinion annotations
to the DA units as follows. If a DA unit contains
an opinion annotation, the label is transferred up-
wards to the containing DA. When a DA contains
multiple opinion annotations, each with a differ-
ent polarity, one of them is randomly chosen as
the label for the DA. The discourse relations exist-
ing between opinions are also transferred upwards,
between the DAs containing each of these anno-
tations. We recreate an example from Somasun-
daran et al (2008) using DA segmentation in Ex-
ample 1. Here, the speaker has a positive opinion
towards the rubbery material for the TV remote.
(1) DA-1: ... this kind of rubbery material,
DA-2: it?s a bit more bouncy,
DA-3: like you said they get chucked around a lot.
DA-4: A bit more durable and that can also be er-
gonomic and
DA-5: it kind of feels a bit different from all the
other remote controls.
In the example, the individual opinion expressions
(shown in bold) are essentially regarding the same
thing ? the rubbery material. Thus, the explicit
targets (shown in italics), it?s, that, and it, and the
implicit target of a bit more durable are all linked
1
The AMI corpus contains a set of scenario-based meet-
ings where participants have to design a new TV remote pro-
totype.
2
DA segmentation is provided with the AMI corpus.
171
Figure 1: Discourse Relations between DA seg-
ments for Example 1.
with same target relations. Also, notice that the
opinions reinforce a particular stance, i.e., a pro-
rubbery-material stance. Thus, the scheme links
the opinions via reinforcing relations. Figure 1 il-
lustrates the corresponding discourse relations be-
tween the containing DA units.
4 Implementing the Discourse Model
The hypothesis in using discourse information for
polarity classification is that the global discourse
view will improve upon a classification with only
a local view. Thus, we implement a local clas-
sifier to bootstrap the classification process, and
then implement classifiers that use discourse in-
formation from the scheme annotations, over it.
We explore two approaches for implementing our
discourse-based classifier. The first is ICA, where
discourse relations and the neighborhood informa-
tion brought in by these relations are incorporated
as features into the learner. The second approach
is ILP optimization, which tries to maximize the
class distributions predicted by the local classifier,
subject to constraints imposed by discourse rela-
tions. Both classifiers thus accommodate prefer-
ences of the local classifier and for coherence with
discourse neighbors.
4.1 Local Classifier
A supervised local classifier, Local, is used to pro-
vide the classifications to bootstrap the discourse-
based classifiers.
3
It is important to make Local as
reliable as possible; otherwise, the discourse rela-
tions will propagate misclassifications. Thus, we
build Local using a variety of knowledge sources
that have been shown to be useful for opinion anal-
ysis in previous work. Specifically, we construct
features using polarity lexicons (used by (Wilson
et al, 2005)), DA tags (used by (Somasundaran
3
Local is supervised, as previous work has shown that
supervised methods are effective in opinion analysis. Even
though this makes the final end-to-end system with the ILP
implementation semi-supervised, note that the discourse-
based ILP part is itself unsupervised.
et al, 2007)) and unigrams (used by many re-
searchers, e.g., (Pang and Lee, 2004)).
Note that, as our discourse-based classifiers at-
tempt to improve upon the local classifications,
Local is also a baseline for our experiments.
4.2 Iterative Collective Classification
We use a variant of ICA (Lu and Getoor, 2003;
Neville and Jensen, 2000), which is a collective
classification algorithm shown to perform consis-
tently well over a wide variety of relational data.
Algorithm 1 ICA Algorithm
for each instance i do {bootstrapping}
Compute polarity for i using local attributes
end for
repeat {iterative}
Generate ordering I over all instances
for each i in I do
Compute polarity for i using local and re-
lational attributes
end for
until Stopping criterion is met
ICA uses two classifiers: a local classifier and a
relational classifier. The local classifier is trained
to predict the DA labels using only the local fea-
tures. We use Local, described in Section 4.1, for
this purpose. The relational classifier is trained us-
ing the local features, and an additional set of fea-
tures commonly referred to as relational features.
The value of a relational feature, for a given DA,
depends on the polarity of the discourse neighbors
of that DA. Thus, the relational features incorpo-
rate discourse and neighbor information; that is,
they incorporate the information about the frame
and target relations in conjunction with the polar-
ity of the discourse neighbors. Intuitively, our mo-
tivation for this approach can be explained using
Example 1. Here, in interpreting the ambiguous
opinion a bit different as being positive, we use
the knowledge that it participates in a reinforc-
ing discourse, and that all its neighbors (e.g., er-
gonomic, durable) are positive opinions regard-
ing the same thing. On the other hand, if it had
been a non-reinforcing discourse, then the polar-
ity of a bit different, when viewed with respect to
the other opinions, could have been interpreted as
negative.
Table 1 lists the relational features we defined
for our experiments where each row represents a
172
Percent of neighbors with polarity type a related via frame relation f
?
Percent of neighbors with polarity type a related via target relation t
?
Percent of neighbors with polarity type a related via frame relation f and target relation t
Percent of neighbors with polarity type a and same speaker related via frame relation f
?
Percent of neighbors with polarity type a and same speaker related via target relation t
?
Percent of neighbors with polarity type a related via a frame relation or target relation
Percent of neighbors with polarity type a related via a reinforcing frame relation or same target relation
Percent of neighbors with polarity type a related via a non-reinforcing frame relation or alt target relation
Most common polarity type of neighbors related via a same target relation
Most common polarity type of neighbors related via a reinforcing frame relation and same target relation
Table 1: Relational features: a ? {non-neutral (i.e., positive or negative), positive, negative}, t ? {same, alt},
f ? {reinforcing, non-reinforcing}, t
?
? {same or alt, same, alt}, f
?
? {reinforcing or non-reinforcing, reinforcing, non-
reinforcing}
set of features. Features are generated for all com-
binations of a, t, t
?
, f and f
?
for each row. For
example, one of the features in the first row is Per-
cent of neighbors with polarity type positive, that
are related via a reinforcing frame relation. Thus,
each feature for the relational classifier identifies
neighbors for a given instance via a specific rela-
tion (f , t, f
?
or t
?
, obtained from the scheme an-
notations) and factors in their polarity values (a,
obtained from the classifier predictions from the
previous round). This adds a total of 59 relational
features to the already existing local features.
ICA has two main phases: the bootstrapping
and iterative phases. In the bootstrapping phase,
the polarity of each instance is initialized to the
most likely value given only the local classifier
and its features. In the iterative phase, we cre-
ate a random ordering of all the instances and,
in turn, apply the relational classifier to each in-
stance where the relational features, for a given
instance, are computed using the most recent po-
larity assignments of its neighbors. We repeat this
until some stopping criterion is met. For our ex-
periments, we use a fixed number of 30 iterations,
which has been found to be sufficient in most data
sets for ICA to converge to a solution.
The pseudocode for the algorithm is shown in
Algorithm 1.
4.3 Integer Linear Programming
First, we explain the intuition behind viewing dis-
course relations as enforcing constraints on polar-
ity interpretation. Then, we explain how the con-
straints are encoded in the optimization problem.
4.3.1 Discourse Constraints on Polarity
The discourse relations between opinions can pro-
vide coherence constraints on the way their polar-
ity is interpreted. Consider a discourse scenario
in which a speaker expresses multiple opinions
regarding the same thing, and is reinforcing his
stance in the process (as in Example 1). The set
of individual polarity assignments that is most co-
herent with this global scenario is the one where
all the opinions have the same (equal) polarity. On
the other hand, a pair of individual polarity assign-
ments most consistent with a discourse scenario
where a speaker reinforces his stance via opinions
towards alternative options, is one with opinions
having mutually opposite polarity. For instance,
in the utterance ?Shapes should be curved, noth-
ing square-like?, the speaker reinforces his pro-
curved stance via his opinions about the alternative
shapes: curved and square-like. And, we see that
the first opinion is positive and the second is neg-
ative. Table 2 lists the discourse relations (target
and frame relation combinations) found in the cor-
pus, and the likely polarity interpretation for the
related instances.
Target relation + Frame relation Polarity
same+reinforcing equal (e)
same+non-reinforcing opposite (o)
alternative+reinforcing opposite (o)
alternative+non-reinforcing equal (e)
Table 2: Discourse relations and their polarity con-
straints on the related instances.
4.3.2 Optimization Problem
For each DA instance i in a dataset, the local
classifier provides a class distribution [p
i
, q
i
, r
i
],
where p
i
, q
i
and r
i
correspond to the probabilities
that i belongs to positive, negative and neutral cat-
egories, respectively. The optimization problem is
formulated as an ILP minimization of the objec-
tive function in Equation 1.
?1?
?
i
(p
i
x
i
+q
i
y
i
+r
i
z
i
)+
?
i,j

ij
+
?
i,j
?
ij
(1)
173
where the x
i
, y
i
and z
i
are binary class vari-
ables corresponding to positive, negative and neu-
tral classes, respectively. When a class variable
is 1, the corresponding class is chosen. Variables

ij
and ?
ij
are binary slack variables that corre-
spond to the discourse constraints between two
distinct DA instances i and j. When a given slack
variable is 1, the corresponding discourse con-
straint is violated. Note that the objective func-
tion tries to achieve two goals. The first part
(
?
i
p
i
x
i
+ q
i
y
i
+ r
i
z
i
) is a maximization that tries
to choose a classification for the instances that
maximizes the probabilities provided by the local
classifier. The second part (
?
i,j

ij
+
?
i,j
?
ij
) is a
minimization that tries to minimize the number of
slack variables used, that is, minimize the number
of discourse constraints violated.
Constraints in Equations 2 and 3 listed below
impose binary constraints on the variables. The
constraint in Equation 4 ensures that, for each in-
stance i, only one class variable is set to 1.
x
i
? {0, 1}, y
i
? {0, 1}, z
i
? {0, 1} , ?i (2)

ij
? {0, 1}, ?
ij
? {0, 1} , ?i 6= j (3)
x
i
+ y
i
+ z
i
= 1 , ?i (4)
We pair distinct DA instances i and j as ij,
and if there exists a discourse relation between
them, they can be subject to the corresponding po-
larity constraints listed in Table 2. For this, we
define two binary discourse-constraint constants:
the equal-polarity constant, e
ij
and the opposite-
polarity constant, o
ij
. If a given DA pair ij is
related by either a same+reinforcing relation or
an alternative+non-reinforcing relation (rows 1, 4
of Table 2), then e
ij
= 1; otherwise it is zero.
Similarly, if it is related by either a same+non-
reinforcing relation or an alternative+reinforcing
relation (rows 2, 3 of Table 2), then o
ij
= 1. Both
e
ij
and o
ij
are zero if the instance pair is unrelated
in the discourse.
For each DA instance pair ij, equal-polarity
constraints are applied to the polarity variables of i
(x
i
, y
i
) and j (x
j
, y
j
) via the following equations:
|x
i
? x
j
| ? 1? e
ij
+ 
ij
, ?i 6= j (5)
|y
i
? y
j
| ? 1? e
ij
+ 
ij
, ?i 6= j (6)
?(x
i
+ y
i
) ? ?l
i
, ?i (7)
When e
ij
= 1, the Equation 5 constrains x
i
and
x
j
to be of the same value (both zero or both one).
Similarly, Equation 6 constrains y
i
and y
j
to be
of the same value. Via these equations, we ensure
that the instances i and j do not have the oppo-
site polarity when e
ij
= 1. However, notice that,
if we use just Equations 5 and 6, the optimization
can converge to the same, non-polar (neutral) cat-
egory. To guide the convergence to the same polar
(positive or negative) category, we use Equation 7.
Here l
i
= 1 if the instance i participates in one or
more discourse relations. When e
ij
= 0, x
i
and x
j
(and y
i
and y
j
), can take on assignments indepen-
dently of one another. Notice that both constraints
5 and 6 are relaxed when 
ij
= 1; thus, x
i
and x
j
(or y
i
and y
j
) can take on values independently of
one another, even if e
ij
= 1.
Next, the opposite-polarity constraints are ap-
plied via the following equations:
|x
i
+ x
j
? 1| ? 1? o
ij
+ ?
ij
, ?i 6= j (8)
|y
i
+ y
j
? 1| ? 1? o
ij
+ ?
ij
, ?i 6= j (9)
In the above equations, when o
ij
= 1, x
i
and x
j
(and y
i
and y
j
) take on opposite values; for exam-
ple, if x
i
= 1 then x
j
= 0 and vice versa. When
o
ij
= 0, the variable assignments are independent
of one another. This set of constraints is relaxed
when ?
ij
= 1.
In general, in our ILP formulation, notice that
if an instance does not have a discourse relation to
any other instance in the data, its classification is
unaffected by the optimization. Also, as the un-
derlying discourse scheme poses constraints only
on the interpretation of the polarity of the related
instances, discourse constraints are applied only to
the polarity variables x and y, and not to the neu-
tral class variable, z. Finally, even though slack
variables are used, we discourage the ILP system
from indiscriminately setting the slack variables to
1 by making them a part of the objective function
that is minimized.
5 Experiments
In this work, we are particularly interested in
improvements due to discourse-based methods.
Thus, we report performance under three con-
ditions: over only those instances that are re-
lated via discourse relations (Connected), over in-
stances not related via discourse relations (Single-
tons), and over all instances (All).
The annotated data consists of 7 scenario-based,
multi-party meetings from the AMI meeting cor-
pus. We filter out very small DAs (DAs with fewer
than 3 tokens, punctuation included). This gives
174
us a total of 4606 DA instances, of which 1935
(42%) have opinion annotations. For our exper-
iments, the DAs with no opinion annotations as
well as those with neutral opinions are considered
as neutral. Table 3 shows the class distributions in
the data for the three conditions.
Pos Neg Neutral Total
Connected 643 343 81 1067
Singleton 553 233 2753 3539
All 1196 576 2834 4606
Table 3: Class distribution over connected, single
and all instances.
5.1 Classifiers
Our first baseline, Base, is a simple distribution-
based classifier that classifies the test data based
on the overall distribution of the classes in the
training data. However, in Table 3, the class distri-
bution is different for the Connected and Single-
ton conditions. We incorporate this in a smarter
baseline, Base-2, which constructs separate dis-
tributions for connected instances and singletons.
Thus, given a test instance, depending on whether
it is connected, Base-2 uses the corresponding dis-
tribution to make its prediction.
The third baseline is the supervised classifier,
Local, described in Section 4.1. It is imple-
mented using the SVM classifiers from the Weka
toolkit (Witten and Frank, 2002).
4
Our super-
vised discourse-based classifier, ICA from Sec-
tion 4.2, also uses a similar SVM implemen-
tation for its relational classifier. We imple-
ment our ILP approach from Section 4.3 us-
ing the optimization toolbox from Mathworks
(http://www.mathworks.com) and GNU Linear
Programming Kit.
We observed that the ILP system performs bet-
ter than the ICA system on instances that are con-
nected, while ICA performs better on singletons.
Thus, we also implemented a simple hybrid clas-
sifier (HYB), which selects the ICA prediction for
classification of singletons and the ILP prediction
for classification of connected instances.
5.2 Results
We performed 7-fold cross validation experi-
ments, where six meetings are used for training
4
We use the SMO implementation, which, when used
with the logistic regression, has an output that can be viewed
as a posterior probability distribution.
and the seventh is used for testing the supervised
classifiers (Base, Base-2, Local and ICA). In the
case of ILP, the optimization is applied to the out-
put of Local for each test fold. Table 4 reports the
accuracies of the classifiers, averaged over 7 folds.
First, we observe that Base performs poorly
over connected instances, but performs consider-
ably better over singletons. This is expected as the
overall majority class is neutral and the singletons
are more likely to be neutral. Base-2, which incor-
porates the differentiated distributions, performs
substantially better than Base. Local achieves an
overall performance improvement over Base and
Base-2 by 23 percentage points and 9 percent-
age points, respectively. In general, Local outper-
forms Base for all three conditions (p < 0.001),
and Base-2 for the Singleton and All conditions
(p < 0.001). This overall improvement in Local?s
accuracy corroborates the utility of the lexical, un-
igram and DA based features for polarity detection
in this corpus.
Turning to the discourse-based classifiers, ICA,
ILP and HYB, all of these perform better than
Base and Base-2 for all conditions. ICA improves
over Local by 9 percentage points for Connected,
3 points for Singleton and 4 points for All. ILP?s
improvement over Local for Connected and All is
even more substantial: 28 percentage points and
6 points, respectively. Notice that ILP has the
same performance as Local for Singletons, as the
discourse constraints are not applied over uncon-
nected instances. Finally, HYB significantly out-
performs Local under all conditions. The signif-
icance levels of the improvements over Local are
highlighted in Table 4. These improvements also
signify that the underlying discourse scheme is
effective, and adaptable to different implementa-
tions.
Interestingly, ICA and ILP improve over Local
in different ways. While ILP sharply improves the
performance over the connected instances, ICA
shows relatively modest improvements over both
connected and singletons. ICA?s improvement
over singletons is interesting because it indicates
that, even though the features in Table 1 are fo-
cused on discourse relations, ICA utilizes them to
learn the classification of singletons too.
Comparing our discourse-based approaches,
ILP does significantly better than ICA over con-
nected instances (p < 0.001), while ICA does
significantly better than ILP over singletons (p <
175
Base Base-2 Local ICA ILP HYB
Connected 24.4 47.56 46.66 55.64 75.07 75.07
Singleton 51.72 63.23 75.73 78.72 75.73 78.72
All 45.34 59.46 68.72 73.31 75.35 77.72
Table 4: Accuracies of the classifiers measured over Connected, Singleton and All instances. Perfor-
mance significantly better than Local are indicated in bold for p < 0.001 and underline for p < 0.01.
0.01). However, there is no significant difference
between ICA and ILP for the All condition. The
HYB classifier outperforms ILP for the Singleton
condition (p < 0.01) and ICA for the Connected
condition (p < 0.001). Interestingly, over all in-
stances (the All condition), HYB also performs
significantly better than both ICA (p < 0.001) and
ILP (p < 0.01).
5.3 Analysis
Amongst our two approaches, ILP performs bet-
ter, and hence we further analyze its behavior to
understand how the improvements are achieved.
Table 5 reports the performance of ILP and Local
for the precision, recall and f-measure metrics (av-
eraged over 7 test folds), measured separately for
each of the opinion categories. The most promi-
nent improvement by ILP is observed for the re-
call of the polar categories under the Connected
condition: 40 percentage points for the positive
class, and 29 percentage points for the negative
class. The gain in recall is not accompanied by
a significant loss in precision. This results in an
improvement in f-measure for the polar categories
(24 points for positive and 16 points for negative).
Also note that, by virtue of the constraint in Equa-
tion 7, ILP does not classify any connected in-
stance as neutral; thus the precision is NaN, recall
is 0 and the f-meaure is NaN. This is indicated as
* in the Table.
The improvement of ILP for the All condition,
for the polar classes, follows a similar trend for re-
call (18 to 21 point improvement) and f-measure
(9 to 13 point improvement). In addition to this,
the ILP has an overall improvement in precision
over Local. This may seem counterintuitive, as
in Table 5, ILP?s precision for connected nodes is
similar to, or lower than, that of Local. This is
explained by the fact that, while going from con-
nected to overall conditions, Local?s polar predic-
tions increase by threefold (565 to 1482), but its
correct polar predictions increase by only twofold
(430 to 801). Thus, the ratio of change in the total
Gold Local
Pos Neg Neut Total
Pos 551 113 532 1196
Neg 121 250 205 576
Neut 312 135 2387 2834
Total 984 498 3124 4606
Gold ILP
Pos Neg Neut Total
Pos 817 157 222 1196
Neg 147 358 71 576
Neut 358 147 2329 2834
Total 1322 662 2622 4606
Table 6: Contingency table over all instances.
polar predictions to the correct polar predictions is
3 : 2. On the other hand, while polar predictions
by ILP increase by only twofold (1067 to 1984),
its correct polar predictions increase by 1.5 times
(804 to 1175). Here, the ratio of change in the total
polar predictions to the correct polar predictions is
4 : 3, a smaller ratio.
The contingency table (Table 6) shows how Lo-
cal and ILP compare against the gold standard
annotations. Notice here, that even though ILP
makes more polar guesses as compared to Local, a
greater proportion of the ILP guesses are correct.
The number of non-diagonal elements are much
smaller for ILP, resulting in the accuracy improve-
ments seen in Table 4.
6 Examples and Discussion
The results in Table 4 show that Local, which pro-
vides the classifications for bootstrapping ICA and
ILP, predicts an incorrect class for more than 50%
of the connected instances. Methods starting with
noisy starting points are in danger of propagating
the errors and hence worsening the performance.
Interestingly, in spite of starting with so many bad
classifications, ILP is able to achieve a large per-
formance improvement. We discovered that, given
a set of connected instances, even when Local has
only one correct guess, ILP is able to use this to
rectify the related instances. We illustrate this situ-
ation in Figure 2, which reproduces the connected
DAs for Example 1. It shows the classifications
176
Positive Negative Neutral
Local ILP Local ILP Local ILP
Connected-Prec 78.1 78.2 71.9 69.8 12.1
Connected-Recall 45.3 86.3 44.1 73.4 62.8 *
Connected-F1 56.8 81.5 54.0 70.7 18.5
All-Prec 56.2 61.3 52.3 54.6 76.3 88.3
All-Recall 46.6 67.7 44.3 62.5 83.9 81.5
All-F1 50.4 64.0 46.0 57.1 79.6 84.6
Table 5: Precision, Recall, Fmeasure for each Polarity category. Performance significantly better than
Local are indicated in bold (p < 0.001), underline (p < 0.01) and italics (p < 0.05). The * denotes that
ILP does not retrieve any connected node as neutral.
Figure 2: Discourse Relations and Classifications
for Example 1.
for each DA from the gold standard (G), the Local
classifier (L) and the ILP classifier (ILP). Observe
that Local predicts the correct positive class (+) for
only DA-4 (the DA containing bit more durable
and ergonomic). Notice that these are clear cases
of positive evaluation. It incorrectly predicts the
polarity of DA-2 (containing bit more bouncy)
as neutral (*), and DA-5 (containing a bit dif-
ferent from all the other remote controls) as
negative (-). DA-2 and DA-5 exemplify the fact
that polarity classification is a complex and diffi-
cult problem: being bouncy is a positive evalua-
tion in this particular discourse context, and may
not be so elsewhere. Thus, naturally, lexicons and
unigram-based learning would fail to capture this
positive evaluation. Similarly, ?being different?
could be deemed negative in other discourse con-
texts. However, ILP is able to arrive at the correct
predictions for all the instances. As the DA-4 is
connected to both DA-2 and DA-5 via a discourse
relation that enforces an equal-polarity constraint
(same+reinforcing relation of row 1, Table 2), both
of the misclassifications are rectified. Presumably,
the incorrect predictions made by Local are low
confidence estimates, while the predictions of the
correct cases have high confidence, which makes
it possible for ILP to make the corrections.
We also observed the propagation of the correct
classification for other types of discourse relations,
for more complex types of connectivity, and also
for conditions where an instance is not directly
connected to the correctly predicted instance. The
meeting snippet below (Example 2) and its cor-
responding DA relations (Figure 3) illustrate this.
This example is a reinforcing discourse where the
speaker is arguing for the number keypad, which is
an alternative to the scrolling option. Thus, he ar-
gues against the scrolling, and argues for entering
the number (which is a capability of the number
keypad).
(2) D-1: I reckon you?re gonna have to have a num-
ber keypad anyway for the amount of channels these
days,
D-2: You wouldn?t want to just have to scroll
through all the channels to get to the one you want
D-3: You wanna enter just the number of it , if you
know it
D-4: I reckon we?re gonna have to have a number
keypad anyway
In Figure 3, we see that, DA-2 is connected via an
alternative+reinforcing discourse relation to each
of its neighbors DA-1 and DA-3, which encour-
ages the optimization to choose a class for it that
is opposite to DA-1 and DA-3. Notice also, that
even though Local predicts only DA-4 correctly,
this correct classification finally influences the cor-
rect choice for all the instances, including the re-
motely connected DA-2.
7 Conclusions and Future Work
This work focuses on the first step to ascertain
whether discourse relations are useful for improv-
ing opinion polarity classification, whether they
can be modeled and what modeling choices can
be used. To this end, we explored two distinct
paradigms: the supervised ICA and the unsuper-
vised ILP. We showed that both of our approaches
are effective in exploiting discourse relations to
177
Figure 3: Discourse Relations and Classifications for Example 2.
significantly improve polarity classification. We
found that there is a difference in how ICA and
ILP achieve improvements, and that combining
the two in a hybrid approach can lead to further
overall improvement. Quantitatively, we showed
that our approach is able to achieve a large in-
crease in recall of the polar categories without
harming the precision, which results in the perfor-
mance improvements. Qualitatively, we illustrated
how, even if the bootstrapping process is noisy,
the optimization and discourse constraints effec-
tively rectify the misclassifications. The improve-
ments of our diverse global inference approaches
indicate that discourse information can be adapted
in different ways to augment and improve existing
opinion analysis techniques.
The automation of the discourse-relation recog-
nition is the next step in this research. The be-
havior of ICA and ILP can change, depending on
the automation of discourse level recognition. The
implementation and comparison of the two meth-
ods under full automation is the focus of our future
work.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152 and NSF Grant No. 0746930.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
N. Asher, F. Benamara, and Y. Mathieu. 2008. Dis-
tilling opinion in discourse: A preliminary study.
COLING-2008.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. In COLING-
2008.
M. Bilgic, G. M. Namata, and L. Getoor. 2007. Com-
bining collective classification and link prediction.
In Workshop on Mining Graphs and Complex Struc-
tures at the IEEE International Conference on Data
Mining.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The ami meetings corpus. In Pro-
ceedings of the Measuring Behavior Symposium on
?Annotating and measuring Meeting Behavior?.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint extrac-
tion of entities and relations for opinion recognition.
In EMNLP 2006.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL 2007.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based
approach. In ACL 2007.
A. B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
A. Haghighi, K. Toutanova, and C. Manning. 2005. A
joint model for semantic role labeling. In CoNLL.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP-2006, pages 355?363,
Sydney, Australia.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Q. Lu and L. Getoor. 2003. Link-based classification.
In Proceedings of the International Conference on
Machine Learning (ICML).
A. Moschitti, D. Pighin, and R. Basili. 2006. Seman-
tic role labeling via tree kernel joint inference. In
CoNLL.
A. Moschitti. 2009. Syntactic and semantic kernels for
short text pair categorization. In EACL.
178
D. Neiberg, K. Elenius, and K. Laskowski. 2006.
Emotion recognition in spontaneous speech using
gmms. In INTERSPEECH 2006 ICSLP.
J. Neville and D. Jensen. 2000. Iterative classifica-
tion in relational data. In In Proc. AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters. Computing Attitude and Affect in Text:
Theory and Applications.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In EMNLP.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Mach. Learn., 62(1-2):107?136.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL-2004, pages 1?8.
Boston, MA, USA.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008.
Sentiment analysis based on probabilistic models us-
ing inter-sentence information. In LREC?08.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Coling
2008.
H. Takamura, T. Inui, and M. Okumura. 2007. Extract-
ing semantic orientations of phrases from dictionary.
In HLT-NAACL 2007.
B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004.
Link prediction in relational data. In Neural Infor-
mation Processing Systems.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP 2005.
I. H. Witten and E. Frank. 2002. Data mining: practi-
cal machine learning tools and techniques with java
implementations. SIGMOD Rec., 31(1):76?77.
179
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 226?234,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Recognizing Stances in Online Debates
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper presents an unsupervised opin-
ion analysis method for debate-side clas-
sification, i.e., recognizing which stance a
person is taking in an online debate. In
order to handle the complexities of this
genre, we mine the web to learn associa-
tions that are indicative of opinion stances
in debates. We combine this knowledge
with discourse information, and formu-
late the debate side classification task as
an Integer Linear Programming problem.
Our results show that our method is sub-
stantially better than challenging baseline
methods.
1 Introduction
This paper presents a method for debate-side clas-
sification, i.e., recognizing which stance a per-
son is taking in an online debate posting. In on-
line debate forums, people debate issues, express
their preferences, and argue why their viewpoint is
right. In addition to expressing positive sentiments
about one?s preference, a key strategy is also to
express negative sentiments about the other side.
For example, in the debate ?which mobile phone is
better: iPhone or Blackberry,? a participant on the
iPhone side may explicitly assert and rationalize
why the iPhone is better, and, alternatively, also ar-
gue why the Blackberry is worse. Thus, to recog-
nize stances, we need to consider not only which
opinions are positive and negative, but also what
the opinions are about (their targets).
Participants directly express their opinions,
such as ?The iPhone is cool,? but, more often, they
mention associated aspects. Some aspects are par-
ticular to one topic (e.g., Active-X is part of IE
but not Firefox), and so distinguish between them.
But even an aspect the topics share may distin-
guish between them, because people who are pos-
itive toward one topic may value that aspect more.
For example, both the iPhone and Blackberry have
keyboards, but we observed in our corpus that pos-
itive opinions about the keyboard are associated
with the pro Blackberry stance. Thus, we need to
find distinguishing aspects, which the topics may
or may not share.
Complicating the picture further, participants
may concede positive aspects of the opposing is-
sue or topic, without coming out in favor of it,
and they may concede negative aspects of the is-
sue or topic they support. For example, in the fol-
lowing sentence, the speaker says positive things
about the iPhone, even though he does not pre-
fer it: ?Yes, the iPhone may be cool to take it out
and play with and show off, but past that, it offers
nothing.? Thus, we need to consider discourse re-
lations to sort out which sentiments in fact reveal
the writer?s stance, and which are merely conces-
sions.
Many opinion mining approaches find negative
and positive words in a document, and aggregate
their counts to determine the final document po-
larity, ignoring the targets of the opinions. Some
work in product review mining finds aspects of a
central topic, and summarizes opinions with re-
spect to these aspects. However, they do not find
distinguishing factors associated with a preference
for a stance. Finally, while other opinion anal-
ysis systems have considered discourse informa-
tion, they have not distinguished between conces-
sionary and non-concessionary opinions when de-
termining the overall stance of a document.
This work proposes an unsupervised opinion
analysis method to address the challenges de-
scribed above. First, for each debate side, we mine
the web for opinion-target pairs that are associated
with a preference for that side. This information
is employed, in conjunction with discourse infor-
mation, in an Integer Linear Programming (ILP)
framework. This framework combines the individ-
ual pieces of information to arrive at debate-side
226
classifications of posts in online debates.
The remainder of this paper is organized as fol-
lows. We introduce our debate genre in Section 2
and describe our method in Section 3. We present
the experiments in Section 4 and analyze the re-
sults in Section 5. Related work is in Section 6,
and the conclusions are in Section 7.
2 The Debate Genre
In this section, we describe our debate data,
and elaborate on characteristic ways of express-
ing opinions in this genre. For our current
work, we use the online debates from the website
http://www.convinceme.net.1
In this work, we deal only with dual-sided,
dual-topic debates about named entities, for ex-
ample iPhone vs. Blackberry, where topic1 =
iPhone, topic2 =Blackberry, side1 = pro-iPhone,
and side2=pro-Blackberry.
Our test data consists of posts of 4 debates:
Windows vs. Mac, Firefox vs. Internet Explorer,
Firefox vs. Opera, and Sony Ps3 vs. Nintendo
Wii. The iPhone vs. Blackberry debate and two
other debates, were used as development data.
Given below are examples of debate posts. Post
1 is taken from the iPhone vs. Blackberry debate,
Post 2 is from the Firefox vs. Internet Explorer
debate, and Post 3 is from the Windows vs. Mac
debate:
(1) While the iPhone may appeal to younger
generations and the BB to older, there is no
way it is geared towards a less rich popula-
tion. In fact it?s exactly the opposite. It?s a
gimmick. The initial purchase may be half
the price, but when all is said and done you
pay at least $200 more for the 3g.
(2) In-line spell check...helps me with big
words like onomatopoeia
(3) Apples are nice computers with an excep-
tional interface. Vista will close the gap on
the interface some but Apple still has the
prettiest, most pleasing interface and most
likely will for the next several years.
2.1 Observations
As described in Section 1, the debate genre poses
significant challenges to opinion analysis. This
1http://www.forandagainst.com and
http://www.createdebate.com are other similar debating
websites.
subsection elaborates upon some of the complexi-
ties.
Multiple polarities to argue for a side. Debate
participants, in advocating their choice, switch
back and forth between their opinions towards the
sides. This makes it difficult for approaches that
use only positive and negative word counts to de-
cide which side the post is on. Posts 1 and 3 illus-
trate this phenomenon.
Sentiments towards both sides (topics) within a
single post. The above phenomenon gives rise
to an additional problem: often, conflicting sides
(and topics) are addressed within the same post,
sometimes within the same sentence. The second
sentence of Post 3 illustrates this, as it has opinions
about both Windows and Mac.
Differentiating aspects and personal prefer-
ences. People seldom repeatedly mention the
topic/side; they show their evaluations indirectly,
by evaluating aspects of each topic/side. Differen-
tiating aspects determine the debate-post?s side.
Some aspects are unique to one side/topic or the
other, e.g., ?3g? in Example 1 and ?inline spell
check? in Example 2. However, the debates are
about topics that belong to the same domain and
which therefore share many aspects. Hence, a
purely ontological approach of finding ?has-a? and
?is-a? relations, or an approach looking only for
product specifications, would not be sufficient for
finding differentiating features.
When the two topics do share an aspect (e.g., a
keyboard in the iPhone vs. Blackberry debate), the
writer may perceive it to be more positive for one
than the other. And, if the writer values that as-
pect, it will influence his or her overall stance. For
example, many people prefer the Blackberry key-
board over the iPhone keyboard; people to whom
phone keyboards are important are more likely to
prefer the Blackberry.
Concessions. While debating, participants often
refer to and acknowledge the viewpoints of the op-
posing side. However, they do not endorse this ri-
val opinion. Uniform treatment of all opinions in
a post would obviously cause errors in such cases.
The first sentence of Example 1 is an instance of
this phenomenon. The participant concedes that
the iPhone appeals to young consumers, but this
positive opinion is opposite to his overall stance.
227
DIRECT OBJECT Rule: dobj(opinion, target)
In words: The target is the direct object of the opinion
Example: I loveopinion1 Firefoxtarget1 and defendedopinion2 ittarget2
NOMINAL SUBJECT Rule: nsubj(opinion, target)
In words: The target is the subject of the opinion
Example: IEtarget breaksopinion with everything.
ADJECTIVAL MODIFIER Rule: amod(target, opinion)
In words: The opinion is an adjectival modifier of the target
Example: The annoyingopinion popuptarget
PREPOSITIONAL OBJECT Rule: if prep(target1,IN) ? pobj(IN, target2)
In words: The prepositional object of a known target is also a target of the same opinion
Example: The annoyingopinion popuptarget1 in IEtarget2 (?popup? and ?IE? are targets of ?annoying?)
RECURSIVE MODIFIERS Rule: if conj(adj2, opinionadj1) ? amod(target, adj2)
In words: If the opinion is an adjective (adj1) and it is conjoined with another adjective (adj2),
then the opinion is tied to what adj2 modifies
Example: It is a powerfulopinion(adj1) and easyopinion(adj2) applicationtarget
(?powerful? is attached to the target ?application? via the adjective ?easy?)
Table 1: Examples of syntactic rules for finding targets of opinions
3 Method
We propose an unsupervised approach to classify-
ing the stance of a post in a dual-topic debate. For
this, we first use a web corpus to learn preferences
that are likely to be associated with a side. These
learned preferences are then employed in conjunc-
tion with discourse constraints to identify the side
for a given post.
3.1 Finding Opinions and Pairing them with
Targets
We need to find opinions and pair them with tar-
gets, both to mine the web for general preferences
and to classify the stance of a debate post. We use
straightforward methods, as these tasks are not the
focus of this paper.
To find opinions, we look up words in a sub-
jectivity lexicon: all instances of those words are
treated as opinions. An opinion is assigned the
prior polarity that is listed for that word in the lex-
icon, except that, if the prior polarity is positive or
negative, and the instance is modified by a nega-
tion word (e.g., ?not?), then the polarity of that in-
stance is reversed. We use the subjectivity lexicon
of (Wilson et al, 2005),2 which contains approxi-
mately 8000 words which may be used to express
opinions. Each entry consists of a subjective word,
its prior polarity (positive (+), negative (?), neu-
tral (?)), morphological information, and part of
speech information.
To pair opinions with targets, we built a rule-
based system based on dependency parse informa-
tion. The dependency parses are obtained using
2Available at http://www.cs.pitt.edu/mpqa.
the Stanford parser.3 We developed the syntactic
rules on separate data that is not used elsewhere in
this paper. Table 1 illustrates some of these rules.
Note that the rules are constructed (and explained
in Table 1) with respect to the grammatical relation
notations of the Stanford parser. As illustrated in
the table, it is possible for an opinion to have more
than one target. In such cases, the single opin-
ion results in multiple opinion-target pairs, one for
each target.
Once these opinion-target pairs are created, we
mask the identity of the opinion word, replacing
the word with its polarity. Thus, the opinion-
target pair is converted to a polarity-target pair.
For instance, ?pleasing-interface? is converted to
interface+. This abstraction is essential for han-
dling the sparseness of the data.
3.2 Learning aspects and preferences from
the web
We observed in our development data that people
highlight the aspects of topics that are the bases
for their stances, both positive opinions toward as-
pects of the preferred topic, and negative opinions
toward aspects of the dispreferred one. Thus, we
decided to mine the web for aspects associated
with a side in the debate, and then use that infor-
mation to recognize the stances expressed in indi-
vidual posts.
Previous work mined web data for aspects as-
sociated with topics (Hu and Liu, 2004; Popescu
et al, 2005). In our work, we search for aspects
associated with a topic, but particularized to po-
larity. Not all aspects associated with a topic are
3http://nlp.stanford.edu/software/lex-parser.shtml.
228
side1 (pro-iPhone) side2 (pro-blackberry)
termp P (iPhone+|termp) P (blackberry?|termp) P (iPhone?|termp) P (blackberry+|termp)
storm+ 0.227 0.068 0.022 0.613
storm? 0.062 0.843 0.06 0.03
phone+ 0.333 0.176 0.137 0.313
e-mail+ 0 0.333 0.166 0.5
ipod+ 0.5 0 0.33 0
battery? 0 0 0.666 0.333
network? 0.333 0 0.666 0
keyboard+ 0.09 0.12 0 0.718
keyboard? 0.25 0.25 0.125 0.375
Table 2: Probabilities learned from the web corpus (iPhone vs. blackberry debate)
discriminative with respect to stance; we hypoth-
esized that, by including polarity, we would be
more likely to find useful associations. An aspect
may be associated with both of the debate top-
ics, but not, by itself, be discriminative between
stances toward the topics. However, opinions to-
ward that aspect might discriminate between them.
Thus, the basic unit in our web mining process is
a polarity-target pair. Polarity-target pairs which
explicitly mention one of the topics are used to an-
chor the mining process. Opinions about relevant
aspects are gathered from the surrounding context.
For each debate, we downloaded weblogs and
forums that talk about the main topics (corre-
sponding to the sides) of that debate. For ex-
ample, for the iPhone vs. Blackberry debate,
we search the web for pages containing ?iPhone?
and ?Blackberry.? We used the Yahoo search API
and imposed the search restriction that the pages
should contain both topics in the http URL. This
ensured that we downloaded relevant pages. An
average of 3000 documents were downloaded per
debate.
We apply the method described in Section
3.1 to the downloaded web pages. That is,
we find all instances of words in the lexicon,
extract their targets, and mask the words with
their polarities, yielding polarity-target pairs. For
example, suppose the sentence ?The interface
is pleasing? is in the corpus. The system
extracts the pair ?pleasing-interface,? which is
masked to ?positive-interface,? which we notate
as interface+. If the target in a polarity-target
pair happens to be one of the topics, we select the
polarity-target pairs in its vicinity for further pro-
cessing (the rest are discarded). The intuition be-
hind this is that, if someone expresses an opinion
about a topic, he or she is likely to follow it up
with reasons for that opinion. The sentiments in
the surrounding context thus reveal factors that in-
fluence the preference or dislike towards the topic.
We define the vicinity as the same sentence plus
the following 5 sentences.
Each unique target word targeti in the web cor-
pus, i.e., each word used as the target of an opin-
ion one or more times, is processed to generate the
following conditional probabilities.
P (topicqj |target
p
i ) =
#(topicqj , target
p
i )
#targetpi
(1)
where p = {+,? ,? } and q = {+,? ,? } denote the
polarities of the target and the topic, respectively;
j = {1, 2}; and i = {1...M}, where M is the
number of unique targets in the corpus. For exam-
ple, P (Mac+|interface+) is the probability that
?interface? is the target of a positive opinion that is
in the vicinity of a positive opinion toward ?Mac.?
Table 2 lists some of the probabilities learned
by this approach. (Note that the neutral cases are
not shown.)
3.2.1 Interpreting the learned probabilities
Table 2 contains examples of the learned proba-
bilities. These probabilities align with what we
qualitatively found in our development data. For
example, the opinions towards ?Storm? essen-
tially follow the opinions towards ?Blackberry;?
that is, positive opinions toward ?Storm? are usu-
ally found in the vicinity of positive opinions to-
ward ?Blackberry,? and negative opinions toward
?Storm? are usually found in the vicinity of neg-
ative opinions toward ?Blackberry? (for example,
in the row for storm+, P (blackberry+|storm+)
is much higher than the other probabilities). Thus,
an opinion expressed about ?Storm? is usually the
opinion one has toward ?Blackberry.? This is ex-
pected, as Storm is a type of Blackberry. A similar
example is ipod+, which follows the opinion to-
ward the iPhone. This is interesting because an
229
iPod is not a phone; the association is due to pref-
erence for the brand. In contrast, the probability
distribution for ?phone? does not show a prefer-
ence for any one side, even though both iPhone
and Blackberry are phones. This indicates that
opinions towards phones in general will not be
able to distinguish between the debate sides.
Another interesting case is illustrated by the
probabilities for ?e-mail.? People who like e-mail
capability are more likely to praise the Blackberry,
or even criticize the iPhone ? they would thus be-
long to the pro-Blackberry camp.
While we noted earlier that positive evaluations
of keyboards are associated with positive evalua-
tions of the Blackberry (by far the highest prob-
ability in that row), negative evaluations of key-
boards, are, however, not a strong discriminating
factor.
For the other entries in the table, we see that
criticisms of batteries and the phone network are
more associated with negative sentiments towards
the iPhones.
The possibility of these various cases motivates
our approach, in which opinions and their polar-
ities are considered when searching for associa-
tions between debate topics and their aspects.
3.3 Debate-side classification
Once we have the probabilities collected from the
web, we can build our classifier to classify the de-
bate posts.
Here again, we use the process described in Sec-
tion 3.1 to extract polarity-target pairs for each
opinion expressed in the post. Let N be the num-
ber of instances of polarity-target pairs in the post.
For each instance Ij (j = {1...N}), we look up
the learned probabilities of Section 3.2 to create
two scores, wj and uj :
wj = P (topic+1 |target
p
i ) + P (topic?2 |target
p
i ) (2)
uj = P (topic?1 |target
p
i ) + P (topic+2 |target
p
i ) (3)
where targetpi is the polarity-target type of which
Ij is an instance.
Score wj corresponds to side1 and uj corre-
sponds to side2. A point to note is that, if a tar-
get word is repeated, and it occurs in different
polarity-target instances, it is counted as a sepa-
rate instance each time ? that is, here we account
for tokens, not types. Via Equations 2 and 3, we
interpret the observed polarity-target instance Ij in
terms of debate sides.
We formulate the problem of finding the over-
all side of the post as an Integer Linear Program-
ming (ILP) problem. The side that maximizes the
overall side-score for the post, given all the N in-
stances Ij , is chosen by maximizing the objective
function
N
?
j=1
(wjxj + ujyj) (4)
subject to the following constraints
xj ? {0, 1}, ?j (5)
yj ? {0, 1}, ?j (6)
xj + yj = 1, ?j (7)
xj ? xj?1 = 0, j ? {2..N} (8)
yj ? yj?1 = 0, j ? {2..N} (9)
Equations 5 and 6 implement binary constraints.
Equation 7 enforces the constraint that each Ij can
belong to exactly one side. Finally, Equations 8
and 9 ensure that a single side is chosen for the
entire post.
3.4 Accounting for concession
As described in Section 2, debate participants of-
ten acknowledge the opinions held by the oppos-
ing side. We recognize such discourse constructs
using the Penn Discourse Treebank (Prasad et al,
2007) list of discourse connectives. In particu-
lar, we use the list of connectives from the Con-
cession and Contra-expectation category. Exam-
ples of connectives in these categories are ?while,?
?nonetheless,? ?however,? and ?even if.? We use
approximations to finding the arguments to the
discourse connectives (ARG1 and ARG2 in Penn
Discourse Treebank terms). If the connective is
mid-sentence, the part of the sentence prior to
the connective is considered conceded, and the
part that follows the connective is considered non-
conceded. An example is the second sentence of
Example 3. If, on the other hand, the connective
is sentence-initial, the sentence is split at the first
comma that occurs mid sentence. The first part is
considered conceded, and the second part is con-
sidered non-conceded. An example is the first sen-
tence of Example 1.
The opinions occurring in the conceded part are
interpreted in reverse. That is, the weights corre-
sponding to the sides wj and uj are interchanged
in equation 4. Thus, conceded opinions are effec-
tively made to count towards the opposing side.
230
4 Experiments
On http://www.convinceme.net, the html page for
each debate contains side information for each
post (side1 is blue in color and side2 is green).
This gives us automatically labeled data for our
evaluations. For each of the 4 debates in our test
set, we use posts with at least 5 sentences for eval-
uation.
4.1 Baselines
We implemented two baselines: the OpTopic sys-
tem that uses topic information only, and the
OpPMI system that uses topic as well as related
word (noun) information. All systems use the
same lexicon, as well as exactly the same pro-
cesses for opinion finding and opinion-target pair-
ing.
The OpTopic system This system considers
only explicit mentions of the topic for the opin-
ion analysis. Thus, for this system, the step
of opinion-target pairing only finds all topic+1 ,
topic?1 , topic+2 , topic?2 instances in the post
(where, for example, an instance of topic+1 is a
positive opinion whose target is explicitly topic1).
The polarity-topic pairs are counted for each de-
bate side according to the following equations.
score(side1) = #topic+1 + #topic?2 (10)
score(side2) = #topic?1 + #topic+2 (11)
The post is assigned the side with the higher score.
The OpPMI system This system finds opinion-
target pairs for not only the topics, but also for the
words in the debate that are significantly related to
either of the topics.
We find semantic relatedness of each noun in
the post with the two main topics of the debate
by calculating the Pointwise Mutual Information
(PMI) between the term and each topic over the
entire web corpus. We use the API provided by the
Measures of Semantic Relatedness (MSR)4 engine
for this purpose. The MSR engine issues Google
queries to retrieve documents and finds the PMI
between any two given words. Table 3 lists PMIs
between the topics and the words from Table 2.
Each noun k is assigned to the topic with the
higher PMI score. That is, if
PMI(topic1,k) > PMI(topic2,k) ?k= topic1
and if
4http://cwl-projects.cogsci.rpi.edu/msr/
PMI(topic2,k) > PMI(topic1,k) ?k= topic2
Next, the polarity-target pairs are found for the
post, as before, and Equations 10 and 11 are used
to assign a side to the post as in the OpTopic
system, except that here, related nouns are also
counted as instances of their associated topics.
word iPhone blackberry
storm 0.923 0.941
phone 0.908 0.885
e-mail 0.522 0.623
ipod 0.909 0.976
battery 0.974 0.927
network 0.658 0.961
keyboard 0.961 0.983
Table 3: PMI of words with the topics
4.2 Results
Performance is measured using the follow-
ing metrics: Accuracy ( #Correct#Total posts ), Precision
(#Correct#guessed), Recall ( #Correct#relevant ) and F-measure
( 2?Precision?Recall(Precision+Recall)).
In our task, it is desirable to make a pre-
diction for all the posts; hence #relevant =
#Total posts. This results in Recall and Accu-
racy being the same. However, all of the systems
do not classify a post if the post does not con-
tain the information it needs. Thus, #guessed ?
#Total posts, and Precision is not the same as
Accuracy.
Table 4 reports the performance of four systems
on the test data: the two baselines, our method
using the preferences learned from the web cor-
pus (OpPr) and the method additionally using dis-
course information to reverse conceded opinions.
The OpTopic has low recall. This is expected,
because it relies only on opinions explicitly toward
the topics.
The OpPMI has better recall than OpTopic;
however, the precision drops for some debates. We
believe this is due to the addition of noise. This re-
sult suggests that not all terms that are relevant to
a topic are useful for determining the debate side.
Finally, both of the OpPr systems are better than
both baselines in Accuracy as well as F-measure
for all four debates.
The accuracy of the full OpPr system improves,
on average, by 35 percentage points over the Op-
Topic system, and by 20 percentage points over the
231
OpPMI system. The F-measure improves, on aver-
age, by 25 percentage points over the OpTopic sys-
tem, and by 17 percentage points over the OpPMI
system. Note that in 3 out of 4 of the debates, the
full system is able to make a guess for all of the
posts (hence, the metrics all have the same values).
In three of the four debates, the system us-
ing concession handling described in Section 3.4
outperforms the system without it, providing evi-
dence that our treatment of concessions is effec-
tive. On average, there is a 3 percentage point
improvement in Accuracy, 5 percentage point im-
provement in Precision and 5 percentage point im-
provement in F-measure due to the added conces-
sion information.
OpTopic OpPMI OpPr OpPr
+ Disc
Firefox Vs Internet explorer (62 posts)
Acc 33.87 53.23 64.52 66.13
Prec 67.74 60.0 64.52 66.13
Rec 33.87 53.23 64.52 66.13
F1 45.16 56.41 64.52 66.13
Windows vs. Mac (15 posts)
Acc 13.33 46.67 66.67 66.67
Prec 40.0 53.85 66.67 66.67
Rec 13.33 46.67 66.67 66.67
F1 20.0 50.00 66.67 66.67
SonyPs3 vs. Wii (36 posts)
Acc 33.33 33.33 56.25 61.11
Prec 80.0 46.15 56.25 68.75
Rec 33.33 33.33 50.0 61.11
F1 47.06 38.71 52.94 64.71
Opera vs. Firefox (4 posts)
Acc 25.0 50.0 75.0 100.0
Prec 33.33 100 75.0 100.0
Rec 25.0 50 75.0 100.0
F1 28.57 66.67 75.0 100.0
Table 4: Performance of the systems on the test
data
5 Discussion
In this section, we discuss the results from the pre-
vious section and describe the sources of errors.
As reported in the previous section, the OpPr
system outperforms both the OpTopic and the
OpPMI systems. In order to analyze why OpPr
outperforms OpPMI, we need to compare Tables
2 and 3. Table 2 reports the conditional proba-
bilities learned from the web corpus for polarity-
target pairs used in OpPr, and Table 3 reports the
PMI of these same targets with the debate topics
used in OpPMI. First, we observe that the PMI
numbers are intuitive, in that all the words, ex-
cept for ?e-mail,? show a high PMI relatedness to
both topics. All of them are indeed semantically
related to the domain. Additionally, we see that
some conclusions of the OpPMI system are simi-
lar to those of the OpPr system, for example, that
?Storm? is more closely related to the Blackberry
than the iPhone.
However, notice two cases: the PMI values
for ?phone? and ?e-mail? are intuitive, but they
may cause errors in debate analysis. Because the
iPhone and the Blackberry are both phones, the
word ?phone? does not have any distinguishing
power in debates. On the other hand, the PMI
measure of ?e-mail? suggests that it is not closely
related to the debate topics, though it is, in fact, a
desirable feature for smart phone users, even more
so with Blackberry users. The PMI measure does
not reflect this.
The ?network? aspect shows a comparatively
greater relatedness to the blackberry than to the
iPhone. Thus, OpPMI uses it as a proxy for
the Blackberry. This may be erroneous, how-
ever, because negative opinions towards ?net-
work? are more indicative of negative opinions to-
wards iPhones, a fact revealed by Table 2.
In general, even if the OpPMI system knows
what topic the given word is more related to, it
still does not know what the opinion towards that
word means in the debate scenario. The OpPr sys-
tem, on the other hand, is able to map it to a debate
side.
5.1 Errors
False lexicon hits. The lexicon is word based,
but, as shown by (Wiebe and Mihalcea, 2006; Su
and Markert, 2008), many subjective words have
both objective and subjective senses. Thus, one
major source of errors is a false hit of a word in
the lexicon.
Opinion-target pairing. The syntactic rule-
based opinion-target pairing system is a large
source of errors in the OpPr as well as the base-
line systems. Product review mining work has ex-
plored finding opinions with respect to, or in con-
junction with, aspects (Hu and Liu, 2004; Popescu
et al, 2005); however, in our work, we need to find
232
information in the other direction ? that is, given
the opinion, what is the opinion about. Stoyanov
and Cardie (2008) work on opinion co-reference;
however, we need to identify the specific target.
Pragmatic opinions. Some of the errors are due
to the fact that the opinions expressed in the post
are pragmatic. This becomes a problem especially
when the debate post is small, and we have few
other lexical clues in the post. The following post
is an example:
(4) The blackberry is something like $150 and
the iPhone is $500. I don?t think it?s worth
it. You could buy a iPod separate and have
a boatload of extra money left over.
In this example, the participant mentions the
difference in the prices in the first sentence. This
sentence implies a negative opinion towards the
iPhone. However, recognizing this would require
a system to have extensive world knowledge. In
the second sentence, the lexicon does hit the word
?worth,? and, using syntactic rules, we can deter-
mine it is negated. However, the opinion-target
pairing system only tells us that the opinion is tied
to the ?it.? A co-reference system would be needed
to tie the ?it? to ?iPhone? in the first sentence.
6 Related Work
Several researchers have worked on similar tasks.
Kim and Hovy (2007) predict the results of an
election by analyzing forums discussing the elec-
tions. Theirs is a supervised bag-of-words sys-
tem using unigrams, bigrams, and trigrams as fea-
tures. In contrast, our approach is unsupervised,
and exploits different types of information. Bansal
et al (2008) predict the vote from congressional
floor debates using agreement/disagreement fea-
tures. We do not model inter-personal exchanges;
instead, we model factors that influence stance
taking. Lin at al (2006) identify opposing perspec-
tives. Though apparently related at the task level,
perspectives as they define them are not the same
as opinions. Their approach does not involve any
opinion analysis. Fujii and Ishikawa (2006) also
work with arguments. However, their focus is on
argument visualization rather than on recognizing
stances.
Other researchers have also mined data to learn
associations among products and features. In
their work on mining opinions in comparative sen-
tences, Ganapathibhotla and Liu (2008) look for
user preferences for one product?s features over
another?s. We do not exploit comparative con-
structs, but rather probabilistic associations. Thus,
our approach and theirs are complementary. A
number of works in product review mining (Hu
and Liu, 2004; Popescu et al, 2005; Kobayashi et
al., 2005; Bloom et al, 2007) automatically find
features of the reviewed products. However, our
approach is novel in that it learns and exploits as-
sociations among opinion/polarity, topics, and as-
pects.
Several researchers have recognized the im-
portant role discourse plays in opinion analysis
(Polanyi and Zaenen, 2005; Snyder and Barzilay,
2007; Somasundaran et al, 2008; Asher et al,
2008; Sadamitsu et al, 2008). However, previous
work did not account for concessions in determin-
ing whether an opinion supports one side or the
other.
More sophisticated approaches to identifying
opinions and recognizing their contextual polar-
ity have been published (e.g., (Wilson et al, 2005;
Ikeda et al, 2008; Sadamitsu et al, 2008)). Those
components are not the focus of our work.
7 Conclusions
This paper addresses challenges faced by opinion
analysis in the debate genre. In our method, fac-
tors that influence the choice of a debate side are
learned by mining a web corpus for opinions. This
knowledge is exploited in an unsupervised method
for classifying the side taken by a post, which also
accounts for concessionary opinions.
Our results corroborate our hypothesis that find-
ing relations between aspects associated with a
topic, but particularized to polarity, is more effec-
tive than finding relations between topics and as-
pects alone. The system that implements this in-
formation, mined from the web, outperforms the
web PMI-based baseline. Our hypothesis that ad-
dressing concessionary opinions is useful is also
corroborated by improved performance.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152. We would also like to thank
Vladislav D. Veksler for help with the MSR en-
gine, and the anonymous reviewers for their help-
ful comments.
233
References
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse:
A preliminary study. In Coling 2008: Companion
volume: Posters and Demonstrations, pages 5?8,
Manchester, UK, August.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of COLING: Companion vol-
ume: Posters.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Atsushi Fujii and Tetsuya Ishikawa. 2006. A sys-
tem for summarizing and visualizing arguments in
subjective documents: Toward supporting decision
making. In Proceedings of the Workshop on Senti-
ment and Subjectivity in Text, pages 15?22, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 241?248,
Manchester, UK, August.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In AAAI-2004.
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov,
and Manabu Okumura. 2008. Learning to shift
the polarity of words for sentiment classification. In
Proceedings of the Third International Joint Confer-
ence on Natural Language Processing (IJCNLP).
Soo-Min Kim and Eduard Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1056?1064.
Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2005. Opinion extraction using a
learning-based anaphora resolution technique. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing (IJCNLP-05),
poster, pages 175?180.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of the 10th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2006), pages 109?116, New York, New
York.
Livia Polanyi and Annie Zaenen. 2005. Contextual
valence shifters. In Computing Attitude and Affect
in Text. Springer.
Ana-Maria Popescu, Bao Nguyen, and Oren Et-
zioni. 2005. OPINE: Extracting product fea-
tures and opinions from reviews. In Proceedings
of HLT/EMNLP 2005 Interactive Demonstrations,
pages 32?33, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber, 2007. PDTB 2.0 Anno-
tation Manual.
Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-
mamoto. 2008. Sentiment analysis based on
probabilistic models using inter-sentence informa-
tion. In European Language Resources Associa-
tion (ELRA), editor, Proceedings of the Sixth In-
ternational Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, May.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
Proceedings of NAACL-2007.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
817?824, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Fangzhong Su and Katja Markert. 2008. From
word to sense: a case study of subjectivity recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
2008), Manchester, UK, August.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of COLING-ACL
2006.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT-EMNLP, pages
347?354, Vancouver, Canada.
234
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 54?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Manual Annotation of Opinion Categories in Meetings  
 
 
Swapna Somasundaran1,    Janyce Wiebe1,    Paul Hoffmann2,    Diane Litman1 
            1Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260 
                          2Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260 
{swapna,wiebe,hoffmanp,litman}@cs.pitt.edu 
 
 
  
 
Abstract 
This paper applies the categories from an 
opinion annotation scheme developed for 
monologue text to the genre of multiparty 
meetings. We describe modifications to 
the coding guidelines that were required 
to extend the categories to the new type 
of data, and present the results of an in-
ter-annotator agreement study. As re-
searchers have found with other types of 
annotations in speech data, inter-
annotator agreement is higher when the 
annotators both read and listen to the data 
than when they only read the transcripts.   
Previous work exploited prosodic clues 
to perform automatic detection of speaker 
emotion (Liscombe et al 2003). Our 
findings suggest that doing so to recog-
nize opinion categories would be a prom-
ising line of work. 
1 Introduction 
Subjectivity refers to aspects of language that 
express opinions, beliefs, evaluations and specu-
lations (Wiebe et al 2005).  Many natural lan-
guage processing applications could benefit from 
being able to distinguish between facts and opin-
ions of various types, including speech-oriented 
applications such as meeting browsers, meeting 
summarizers, and speech-oriented question an-
swering (QA) systems. Meeting browsers could 
find instances in meetings where opinions about 
key topics are expressed. Summarizers could in-
clude strong arguments for and against issues, to 
make the final outcome of the meeting more un-
derstandable.  A preliminary user survey 
(Lisowska 2003) showed that users would like to 
be able to query meeting records with subjective 
questions like ?Show me the conflicts of opin-
ions between X and Y? , ?Who made the highest 
number of positive/negative comments? and 
?Give me all the contributions of participant X in 
favor of alternative A regarding the issue I.?  A 
QA system with a component to recognize opin-
ions would be able to help find answers to such 
questions. 
Consider the following example from a meet-
ing about an investment firm choosing which car 
to buy1. (In the examples, the words and phrases 
describing or expressing the opinion are under-
lined): 
(1)2 OCK: Revenues of less 
than a million and losses of 
like five million you know 
that's pathetic 
Here, the speaker, OCK, shows his strong nega-
tive evaluation by using the expression ?That?s 
pathetic.? 
(2) OCK: No it might just be 
a piece of junk cheap piece 
of junk that's not a good 
investment 
In (2), the speaker uses the term ?just a piece of 
junk? to express his negative evaluation and uses 
this to argue for his belief that it is ?not a good 
investment.? 
(3) OCK: Yeah I think that's 
the wrong image for an in-
vestment bank he wants sta-
bility and s safety and you 
don't want flashy like zip-
                                                 
1 Throughout this paper we take examples from a meeting 
where a group of people are deciding on a new car for an 
investment bank. The management wants to attract younger 
investors with a sporty car.  
2 We have presented the examples the way they were ut-
tered by the speaker. Hence they may show many false 
starts and repetitions. Capitalization was added to improve 
readability. 
54
ping around the corner kind 
of thing you know 
The example above shows that the speaker has a 
negative judgment towards the suggestion of a 
sports car (that was made in the previous turn) 
which is indicated by the words ?wrong image.? 
The speaker then goes on to positively argue for 
what he wants. He further argues against the cur-
rent suggestion by using more negative terms 
like ?flashy? and ?zipping around the corner.? 
The speaker believes that ?zipping around the 
corner? is bad as it would give a wrong impres-
sion of the bank to the customers. In the absence 
of such analyses, the decision making process 
and rationale behind the outcomes of meetings, 
which form an important part of the organiza-
tion?s memory, might remain unavailable. 
In this paper, we perform annotation of a 
meeting corpus to lay the foundation for research 
on opinion detection in speech. We show how 
categories from an opinion (subjectivity) annota-
tion scheme, which was developed for news arti-
cles, can be applied to the genre of multi-party 
meetings. The new genre poses challenges as it is 
significantly different from the text domain, 
where opinion analysis has traditionally been 
applied. Specifically, differences arise because:  
1) There are many participants interacting with 
one another, each expressing his or her own 
opinion, and eliciting reactions in the process. 
2) Social interactions may constrain how openly 
people express their opinions; i.e., they are often 
indirect in their negative evaluations. 
We also  explore the influence of speech on hu-
man perception of opinions.  
Specifically, we annotated some meeting data 
with the opinion categories Sentiment and Argu-
ing as defined in Wilson and Wiebe (2005). In 
our annotation we first distinguish whether a 
Sentiment or Arguing is being expressed. If one 
is, we then mark the polarity (i.e., positive or 
negative) and the intensity (i.e., how strong the 
opinion is). Annotating the individual opinion 
expressions is useful in this genre, because we 
see many utterances that have more than one 
type of opinion (e.g. (3) above). To investigate 
how opinions are expressed in speech, we divide 
our annotation into two tasks, one in which the 
annotator only reads the raw text, and the other 
in which the annotator reads the raw text and also 
listens to the speech. We measure inter-annotator 
agreement for both tasks.  
We found that the opinion categories apply 
well to the multi-party meeting data, although 
there is some room for improvement: the Kappa 
values range from 0.32 to 0.69.  As has been 
found for other types of annotations in speech, 
agreement is higher when the annotators both 
read and listen to the data than when they only 
read the transcripts. Interestingly, the advantages 
are more dramatic for some categories than oth-
ers.  And, in both conditions, agreement is higher 
for the positive than for the negative categories.  
We discuss possible reasons for these disparities. 
Prosodic clues have been exploited to perform 
automatic detection of speaker emotion (Lis-
combe et al 2003).  Our findings suggest that 
doing so to recognize opinion categories is a 
promising line of work.  
The rest of the paper is organized as follows: 
In Section 2 we discuss the data and the annota-
tion scheme and present examples. We then pre-
sent our inter-annotator agreement results in Sec-
tion 3, and in Section 4 we discuss issues and 
observations. Related work is described in Sec-
tion 5. Conclusions and Future Work are pre-
sented in Section 6. 
2 Annotation  
2.1 Data 
The data is from the ISL meeting corpus (Bur-
ger et al 2002).  We chose task oriented meet-
ings from the games/scenario and discussion 
genres, as we felt they would be closest to the 
applications for which the opinion analysis will 
be useful. The ISL speech is accompanied by 
rich transcriptions, which are tagged according to 
VERBMOBIL conventions. However, since real-
time applications only have access to ASR out-
put, we gave the annotators raw text, from which 
all VERBMOBIL tags, punctuation, and capitali-
zations were removed.  
In order to see how annotations would be af-
fected by the presence or absence of speech, we 
divided each raw text document into 2 segments. 
One part was annotated while reading the raw 
text only. For the annotation of the other part, 
speech as well as the raw text was provided.   
2.2 Opinion Category Definitions  
We base our annotation definitions on the 
scheme developed by Wiebe et al (2005) for 
news articles. That scheme centers on the notion 
of subjectivity, the linguistic expression of pri-
vate states. Private states are internal mental 
states that cannot be objectively observed or veri-
fied (Quirk et al 1985) and include opinions, 
beliefs, judgments, evaluations, thoughts, and 
feelings. Amongst these many forms of subjec-
55
tivity, we focus on the Sentiment and Arguing 
categories proposed by Wilson and Wiebe 
(2005). The categories are broken down by po-
larity and defined as follows:  
Positive Sentiments: positive emotions, 
evaluations, judgments and stances. 
(4) TBC: Well ca How about 
one of the the newer Cadil-
lac the Lexus is good 
In (4), taken from the discussion of which car to 
buy, the speaker uses the term ?good? to express 
his positive evaluation of the Lexus . 
Negative Sentiments: negative emotions, 
evaluations, judgments and stances. 
(5) OCK: I think these are 
all really bad choices 
In (5), the speaker expresses his negative evalua-
tion of the choices for the company car. Note that 
?really? makes the evaluation more intense.  
Positive Arguing:  arguing for something, ar-
guing that something is true or is so, arguing that 
something did happen or will happen, etc. 
(6) ZDN: Yeah definitely 
moon roof  
In (6), the speaker is arguing that whatever car 
they get should have a moon roof. 
Negative Arguing: arguing against some-
thing, arguing that something is not true or is not 
so, arguing that something did not happen or will 
not happen, etc. 
(7) OCK: Like a Lexus or 
perhaps a Stretch Lexus 
something like that but that 
might be too a little too 
luxurious 
In the above example, the speaker is using the 
term ?a little too luxurious? to argue against a 
Lexus for the car choice.  
In an initial tagging experiment, we applied 
the above definitions, without modification, to 
some sample meeting data. The definitions cov-
ered much of the arguing and sentiment we ob-
served. However, we felt that some cases of Ar-
guing that are more prevalent in meeting than in 
news data needed to be highlighted more, namely 
Arguing opinions that are implicit or that under-
lie what is explicitly said. Thus we add the fol-
lowing to the arguing definitions. 
Positive Arguing: expressing support for or 
backing the acceptance of an object, viewpoint, 
idea or stance by providing reasoning, justifica-
tions, judgment, evaluations or beliefs. This sup-
port or backing may be explicit or implicit. 
(8) MHJ: That's That's why I 
wanna What about the the 
child safety locks I think I 
think that would be a good 
thing because if our custom-
ers happen to have children  
Example (8) is marked as both Positive Arguing 
and Positive Sentiment. The more explicit one is 
the Positive Sentiment that the locks are good. 
The underlying Argument is that the company 
car they choose should have child safety locks. 
Negative Arguing: expressing lack of support 
for or attacking the acceptance of an object, 
viewpoint, idea or stance by providing reasoning, 
justifications, judgment, evaluations or beliefs. 
This may be explicit or implicit. 
(9) OCK: Town Car But it's a 
little a It's a little like 
your grandf Yeah your grand-
father would drive that 
Example (9) is explicitly stating who would drive 
a Town Car, while implicitly arguing against 
choosing the Town Car (as they want younger 
investors). 
2.3 Annotation Guidelines 
Due to genre differences, we also needed to 
modify the annotation guidelines. For each Argu-
ing or Sentiment the annotator perceives, he or 
she identifies the words or phrases used to ex-
press it (the text span), and then creates an anno-
tation consisting of the following. 
? Opinion Category and Polarity 
? Opinion Intensity 
? Annotator Certainty 
Opinion Category and Polarity: These are 
defined in the previous sub-section. Note that the 
target of an opinion is what the opinion is about. 
For example, the target of ?John loves baseball? 
is baseball.   An opinion may or may not have a 
separate target.  For example, ?want stability? in 
?We want stability? denotes a Positive Senti-
ment, and there is no separate target.  In contrast, 
?good? in ?The Lexus is good? expresses a Posi-
tive Sentiment and there is a separate target, 
namely the Lexus. 
In addition to Sentiments toward a topic of 
discussion, we also mark Sentiments toward 
other team members (e.g. ?Man you guys 
are so limited?). We do not mark 
agreements or disagreements as Sentiments, as 
these are different dialog acts (though they some-
times co-occur with Sentiments and Arguing).  
Intensity: We use a slightly modified version 
of Craggs and Wood's (2004) emotion intensity 
56
annotation scheme. According to that scheme, 
there are 5 levels of intensity. Level ?0? denotes 
a lack of the emotion (Sentiment or Arguing in 
our case), ?1? denotes traces of emotion, ?2? de-
notes a low level of emotion, ?3? denotes a clear 
expression while ?4? denotes a strong expres-
sion. Our intensity levels mean the same, but we 
do not mark intensity level 0 as this level implies 
the absence of opinion. 
If a turn has multiple, separate expressions 
marked with the same opinion tag (category and 
polarity), and all expressions refer to the same 
target, then the annotators merge all the expres-
sions into a larger text span, including the sepa-
rating text in between the  expressions. This re-
sulting text span has the same opinion tag as its 
constituents, and it has an intensity that is greater 
than or equal to the highest intensity of the con-
stituent expressions that were merged. 
Annotator Certainty: The annotators use this 
tag if they are not sure that a given opinion is 
present, or if, given the context, there are multi-
ple possible interpretations of the utterance and 
the annotator is not sure which interpretation is 
correct. This attribute is distinct from the Inten-
sity attribute, because the Intensity attribute indi-
cates the strength of the opinion, while the Anno-
tator Certainty attribute indicates whether the 
annotator is sure about a given tag (whatever the 
intensity is). 
2.4 Examples 
We conclude this section with some examples 
of annotations from our corpus.  
(10) OCK: So Lexun had reve-
nues of a hundred and fifty 
million last year and prof-
its of like six million.  
That's pretty good 
Annotation: Text span=That's 
pretty good Cate-
gory=Positive Sentiment In-
tensity=3 Annotator Cer-
tainty=Certain  
The annotator marked the text span ?That?s 
pretty good? as Positive Sentiment because this 
this expression is used by OCK to show his fa-
vorable judgment towards the company reve-
nues. The intensity is 3, as it is a clear expression 
of Sentiment.  
(11) OCK: No it might just 
be a piece of junk Cheap 
piece of junk that?s not a 
good investment 
Annotation1: Text span=it 
might just be a piece of 
junk Cheap piece of junk 
that?s not a good investment 
Category=Negative Sentiment 
Intensity=4 Annotator Cer-
tainty=Certain 
Annotation2: Text span=Cheap 
piece of junk that?s not a 
good investment Category 
=Negative Arguing Inten-
sity=3 Annotator Certainty 
=Certain  
In the above example, there are multiple expres-
sions of opinions. In Annotation1, the expres-
sions ?it might just be a piece of junk?, ?cheap 
piece of junk? and ?not a good investment? ex-
press negative evaluations towards the car choice 
(suggested by another participant in a previous 
turn). Each of these expressions is a clear case of 
Negative Sentiment (Intensity=3). As they are all 
of the same category and polarity and towards 
the same target, they have been merged by the 
annotator into one long expression of Inten-
sity=4. In Annotation2, the sub-expression 
?cheap piece of junk that is not a good invest-
ment? is also used by the speaker OCK to argue 
against the car choice. Hence the annotator has 
marked this as Negative Arguing.  
3 Guideline Development and Inter-
Annotator Agreement 
3.1 Annotator Training 
Two annotators (both co-authors) underwent 
three rounds of tagging. After each round, dis-
crepancies were discussed, and the guidelines 
were modified to reflect the resolved ambiguities. 
A total of 1266 utterances belonging to sections 
of four meetings (two of the discussion genre and 
two of the game genre) were used in this phase. 
3.2 Agreement  
The unit for which agreement was calculated 
was the turn. The ISL transcript provides demar-
cation of speaker turns along with the speaker ID. 
If an expression is marked in a turn, the turn is 
assigned the label of that expression. If there are 
multiple expressions marked within a turn with 
different category tags, the turn is assigned all 
those categories. This does not pose a problem 
for our evaluation, as we evaluate each category 
separately. 
A previously unseen section of a meeting con-
taining 639 utterances was selected and divided 
57
into 2 segments. One part of 319 utterances was 
annotated using raw text as the only signal, and 
the remaining 320 utterances were annotated us-
ing text and speech. Cohen?s Kappa (1960) was 
used to calculate inter-annotator agreement. We 
calculated inter-annotator agreement for both 
conditions: raw-text-only and raw-text+speech. 
This was done for each of the categories: Posi-
tive Sentiment, Positive Arguing, Negative Sen-
timent, and Negative Arguing. To evaluate a 
category, we did the following:  
? For each turn, if both annotators tagged 
the turn with the given category, or both 
did not tag the turn with the category, then 
it is a match.  
? Otherwise it is a mismatch 
Table 1 shows the inter-annotator Kappa val-
ues on the test set. 
 
Agreement (Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.54 0.60 
Negative Arguing 0.32 0.65 
Positive Sentiment 0.57 0.69 
Negative Sentiment 0.41 0.61 
Table 1 Inter-annotator agreement on different 
categories. 
 
With raw-text-only annotation, the Kappa 
value is in the moderate range according to 
Landis and Koch (1977), except for Negative 
Arguing for which it is 0.32. Positive Arguing 
and Positive Sentiment were more reliably de-
tected than Negative Arguing and Negative Sen-
timent. We believe this is because participants 
were more comfortable with directly expressing 
their positive sentiments in front of other partici-
pants.  Given only the raw text data, inter-
annotator reliability measures for Negative Argu-
ing and Negative Sentiment are the lowest. We 
believe this might be due to the fact that partici-
pants in social interactions are not very forthright 
with their Negative Sentiments and Arguing. 
Negative Sentiments and Arguing towards some-
thing may be expressed by saying that something 
else is better. For example, consider the follow-
ing response of one participant to another par-
ticipant?s suggestion of aluminum wheels for the 
company car 
(12) ZDN: Yeah see what kind 
of wheels you know they have 
to look dignified to go with 
the car 
The above example was marked as Negative Ar-
guing by one annotator (i.e., they should not get 
aluminum wheels) while the other annotator did 
not mark it at all. The implied Negative Arguing 
toward getting aluminum wheels can be inferred 
from the statement that the wheels should look 
dignified. However the annotators were not sure, 
as the participant chose to focus on what is desir-
able (i.e., dignified wheels). This utterance is 
actually both a general statement of what is de-
sirable, and an implication that aluminum wheels 
are not dignified. But this may be difficult to as-
certain with the raw text signal only.  
When the annotators had speech to guide their 
judgments, the Kappa values go up significantly 
for each category. All the agreement numbers for 
raw text+speech are in the substantial range ac-
cording to Landis and Koch (1977). We observe 
that with speech, Kappa for Negative Arguing 
has doubled over the Kappa obtained without 
speech. The Kappa for Negative Sentiment 
(text+speech) shows a 1.5 times improvement 
over the one with only raw text. Both these ob-
servations indicate that speech is able to help the 
annotators tag negativity more reliably. It is quite 
likely that a seemingly neutral sentence could 
sound negative, depending on the way words are 
stressed or pauses are inserted. Comparing the 
agreement on Positive Sentiment, we get a 1.2 
times improvement by using speech. Similarly, 
agreement improves by 1.1 times for Positive 
Arguing when speech is used. The improvement 
with speech for the Positive categories is not as 
high as compared to negative categories, which 
conforms to our belief that people are more 
forthcoming about their positive judgments, 
evaluations, and beliefs.  
In order to test if the turns where annotators 
were uncertain were the places that caused mis-
match, we calculated the Kappa with the annota-
tor-uncertain cases removed. The corresponding 
Kappa values are shown in Table 2 
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.52 0.63 
Negative Arguing 0.36 0.63 
Positive Sentiment 0.60 0.73 
Negative Sentiment 0.50 0.61 
Table-2 Inter-annotator agreement on different 
categories, Annotator Uncertain cases removed. 
 
The trends observed in Table 1 are seen in Ta-
ble 2 as well, namely annotation reliability im-
proving with speech. Comparing Tables 1 and 2, 
58
we see that for the raw text, the inter-annotator 
agreement goes up by 0.04 points for Negative 
Arguing and goes up by 0.09 points for Negative 
Sentiment. However, the agreement for Negative 
Arguing and Negative Sentiment on raw-text+ 
speech between Tables 1 and 2 remains almost 
the same. We believe this is  because we had 
20% fewer Annotator Uncertainty tags in the 
raw-text+speech annotation as compared to raw-
text-only, thus indicating that some types of un-
certainties seen in raw-text-only were resolved in 
the raw-text+speech due to the speech input. The 
remaining cases of Annotator Uncertainty could 
have been due to other factors, as discussed in 
the next section 
Table 3 shows Kappa with the low intensity 
tags removed. The hypothesis was that low in-
tensity might be borderline cases, and that re-
moving these might increase inter-annotator reli-
ability.  
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.53 0.66 
Negative Arguing 0.26 0.65 
Positive Sentiment 0.65 0.74 
Negative Sentiment 0.45 0.59 
Table-3 Inter-annotator agreement on different 
categories, Intensity 1, 2 removed. 
 
Comparing Tables 1 and 3 (the raw-text col-
umns), we see that there is an improvement in 
the agreement on sentiment (both positive and 
negative) if the low intensity cases are removed.  
The agreement for Negative Sentiment (raw-text) 
goes up marginally by 0.04 points.  Surprisingly, 
the agreement for Negative Arguing (raw-text) 
goes down by 0.06 points. Similarly in raw-
text+speech results, removal of low intensity 
cases does not improve the agreement for Nega-
tive Arguing while hurting Negative Sentiment 
category (by 0.02 points). One possible explana-
tion is that it may be equally difficult to detect 
Negative categories at both low and high intensi-
ties. Recall that in (12) it was difficult to detect if 
there is  Negative Arguing at all. If the annotator 
decided that it is indeed a Negative Arguing, it is 
put at intensity level=3 (i.e., a clear case). 
4 Discussion 
There were a number of interesting subjectiv-
ity related phenomena in meetings that we ob-
served during our annotation. These are issues 
that will need to be addressed for improving in-
ter-annotator reliability. 
Global and local context for arguing: In the 
context of a meeting, participants argue for (posi-
tively) or against (negatively) a topic. This may 
become ambiguous when the participant uses an 
explicit local Positive Arguing and an implicit 
global Negative Arguing. Consider the following 
speaker turn, at a point in the meeting when one 
participant has suggested that the company car 
should have a moon roof and another participant 
has opposed it, by saying that a moon roof would 
compromise the headroom. 
(13) OCK: We wanna make sure 
there's adequate headroom 
for all those six foot six 
investors 
In the above example, the speaker OCK, in the 
local context of the turn, is arguing positively 
that headroom is important. However, in the 
global context of the meeting, he is arguing 
against the idea of a moon roof that was sug-
gested by a participant. Such cases occur when 
one object (or opinion) is endorsed which auto-
matically precludes another, mutually exclusive 
object (or opinion).  
Sarcasm/Humor: The meetings we analyzed 
had a large amount of sarcasm and humor. Issues 
arose with sarcasm due to our approach of mark-
ing opinions towards the content of the meeting 
(which forms the target of the opinion). Sarcasm 
is difficult to annotate because sarcasm can be 
1) On topic: Here the target is the topic of dis-
cussion and hence sarcasm is used as a Negative 
Sentiment. 
2) Off topic: Here the target is not a topic un-
der discussion, and the aim is to purely elicit 
laughter. 
3) Allied topic: In this case, the target is re-
lated to the topic in some way, and it?s difficult 
to determine if the aim of the sarcasm/humor was 
to elicit laughter or to imply something negative 
towards the topic.  
Multiple modalities: In addition to text and 
speech, gestures and visual diagrams play an im-
portant role in some types of meetings. In one 
meeting that we analyzed, participants were 
working together to figure out how to protect an 
egg when it is dropped from a long distance, 
given the materials they have. It was evident they 
were using some gestures to describe their ideas 
(?we can put tape like this?) and that they drew 
diagrams to get points across. In the absence of 
visual input, annotators would need to guess 
59
what was happening. This might further hurt the 
inter-annotator reliability. 
5 Related Work  
Our opinion categories are from the subjectiv-
ity schemes described in Wiebe et al (2005) and 
Wilson and Wiebe (2005). Wiebe et al (2005) 
perform expression level annotation of opinions 
and subjectivity in text. They define their annota-
tions as an experiencer having some type of atti-
tude (such as Sentiment or Arguing), of a certain 
intensity, towards a target. Wilson and Wiebe 
(2005) extend this basic annotation scheme to 
include different types of subjectivity, including 
Positive Sentiment, Negative Sentiment, Positive 
Arguing, and Negative Arguing. 
Speech was found to improve inter-annotator 
agreement in discourse segmentation of mono-
logs (Hirschberg and Nakatani 1996). Acoustic 
clues have been successfully employed for the 
reliable detection of the speaker?s emotions, in-
cluding frustration, annoyance, anger, happiness, 
sadness, and boredom (Liscombe et al 2003).  
Devillers et al (2003) performed perceptual tests 
with and without speech in detecting the 
speaker?s fear, anger, satisfaction and embar-
rassment.  Though related, our work is not con-
cerned with the speaker?s emotions, but rather 
opinions toward the issues and topics addressed 
in the meeting. 
Most annotation work in multiparty conversa-
tion has focused on exchange structures and dis-
course functional units like common grounding 
(Nakatani and Traum, 1998). In common ground-
ing research, the focus is on whether the partici-
pants of the discourse are able to understand each 
other, and not their opinions towards the content 
of the discourse. Other tagging schemes like the 
one proposed by Flammia and Zue (1997) focus 
on information seeking and question answering 
exchanges where one participant is purely seek-
ing information, while the other is providing it. 
The SWBD DAMSL (Jurafsky et al, 1997) an-
notation scheme over the Switchboard telephonic 
conversation corpus labels shallow discourse 
structures. The SWBD-DAMSL had a label ?sv? 
for opinions. However, due to poor inter-
annotator agreement, the authors discarded these 
annotations. The ICSI MRDA annotation scheme 
(Rajdip et al, 2003) adopts the SWBD DAMSL 
scheme, but does not distinguish between the 
opinionated and objective statements. The ISL 
meeting corpus (Burger and Sloane, 2004) is an-
notated with dialog acts and discourse moves like 
initiation and response, which in turn consist of 
dialog tags such as query, align, and statement. 
Their statement dialog category would not only 
include Sentiment and Arguing tags discussed in 
this paper, but it would also include objective 
statements and other types of subjectivity. 
?Hot spots? in meetings closely relate to our 
work because they find sections in the meeting 
where participants are involved in debates or 
high arousal activity (Wrede and Shriberg 2003). 
While that work distinguishes between high 
arousal and low arousal, it does not distinguish 
between  opinion or non-opinion or the different 
types of opinion. However, Janin et al (2004) 
suggest that there is a relationship between dia-
log acts and involvement, and that involved ut-
terances contain significantly more evaluative 
and subjective statements as well as extremely 
positive or negative answers. Thus we believe it 
may be beneficial for such works to make these 
distinctions. 
Another closely related work that finds par-
ticipants? positions regarding issues is argument 
diagramming (Rienks et al 2005). This ap-
proach, based on the IBIS system (Kunz and Rit-
tel 1970), divides a discourse into issues, and 
finds lines of deliberated arguments. However 
they do not distinguish between subjective and 
objective contributions towards the meeting. 
6 Conclusions and Future Work 
In this paper we performed an annotation 
study of opinions in meetings, and investigated 
the effects of speech. We have shown that it is 
possible to reliably detect opinions within multi-
party conversations. Our consistently better 
agreement results with text+speech input over 
text-only input suggest that speech is a reliable 
indicator of opinions. We have also found that 
Annotator Uncertainty decreased with speech 
input. Our results also show that speech is a more 
informative indicator for negative versus positive 
categories. We hypothesize that this is due to the 
fact the people express their positive attitudes 
more explicitly. The speech signal is thus even 
more important for discerning negative opinions. 
This experience has also helped us gain insights 
to the ambiguities that arise due to sarcasm and 
humor. 
Our promising results open many new avenues 
for research. It will be interesting to see how our 
categories relate to other discourse structures, 
both at the shallow level (agree-
ment/disagreement) as well as at the deeper level 
60
(intentions/goals). It will also be interesting to 
investigate how other forms of subjectivity like 
speculation and intention are expressed in multi-
party discourse. Finding prosodic correlates of 
speech as well as lexical clues that help in opin-
ion detection would be useful in building subjec-
tivity detection applications for multiparty meet-
ings.  
References 
Susanne Burger and Zachary A Sloane. 2004. The ISL 
Meeting Corpus: Categorical Features of Commu-
nicative Group Interactions. NIST Meeting Recog-
nition Workshop 2004, NIST 2004, Montreal, Can-
ada, 2004-05-17 
Susanne Burger, Victoria MacLaren and Hua Yu. 
2002. The ISL Meeting Corpus: The Impact of 
Meeting Type on Speech Style. ICSLP-2002. Den-
ver, CO: ISCA, 9 2002. 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Meas., 20:37?46. 
Richard Craggs and Mary McGee Wood. 2004. A 
categorical annotation scheme for emotion in the 
linguistic content of dialogue. Affective Dialogue 
Systems. 2004. 
Laurence Devillers, Lori Lamel and Ioana Vasilescu. 
2003. Emotion detection in task-oriented spoken 
dialogs. IEEE International Conference on Multi-
media and Expo (ICME). 
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey and 
Elizabeth Shriberg. 2003. ?Meeting Recorder Pro-
ject: Dialog Act Labeling Guide,? ICSI Technical 
Report TR-04-002, Version 3, October 2003 
Giovanni Flammia and Victor Zue. 1997. Learning 
The Structure of Mixed Initiative Dialogues Using 
A Corpus of Annotated Conversations. Eurospeech 
1997, Rhodes, Greece 1997, p1871?1874 
Julia Hirschberg and Christine Nakatani. 1996. A Pro-
sodic Analysis of Discourse Segments in Direction-
Giving Monologues Annual Meeting- Association 
For Computational Linguistics 1996, VOL 34, 
pages 286-293 
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhil-
lon, Jane Edwards, Javier Mac??as-Guarasa, Nelson 
Morgan, Barbara Peskin, Elizabeth Shriberg, An-
dreas Stolcke, Chuck Wooters and Britta Wrede. 
2004. ?The ICSI Meeting Project: Resources and 
Research,?  ICASSP-2004 Meeting Recognition 
Workshop. Montreal; Canada: NIST, 5 2004 
Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca, 
1997. Switchboard-DAMSL Labeling Project 
Coder?s Manual. 
http://stripe.colorado.edu/?jurafsky/manual.august1 
Werner Kunz and Horst W. J. Rittel. 1970. Issues as 
elements of information systems. Working Paper 
WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen der 
Planung, 1970 
Richard Landis and Gary Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data 
Biometrics, Vol. 33, No. 1 (Mar., 1977) , pp. 159-
174 
Agnes Lisowska. 2003. Multimodal interface design 
for the multimodal meeting domain: Preliminary 
indications from a query analysis study. Technical 
Report IM2.  Technical report, ISSCO/TIM/ETI. 
Universit de Genve, Switserland, November 2003. 
Jackson Liscombe, Jennifer Venditti and Julia 
Hirschberg. 2003. Classifying Subject Ratings of 
Emotional Speech Using Acoustic Features. Eu-
rospeech 2003. 
Christine Nakatani and David Traum. 1998. Draft: 
Discourse Structure Coding Manual version 
2/27/98 
Randolph Quirk, Sidney Greenbaum, Geoffry Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New 
York.s 
Rutger Rienks, Dirk Heylen and Erik van der Wei-
jden. 2005. Argument diagramming of meeting 
conversations. In Vinciarelli, A. and Odobez, J., 
editors, Multimodal Multiparty Meeting Process-
ing, Workshop at the 7th International Conference 
on Multimodal Interfaces, pages 85?92, Trento, It-
aly 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and 
Evaluation (formerly Computers and the Humani-
ties), volume 39, issue 2-3, pp. 165-210.  
Theresa Wilson and Janyce Wiebe. 2005. Annotating 
attributions and private states. ACL Workshop on 
Frontiers in Corpus Annotation II: Pie in the Sky.  
Britta Wrede and Elizabeth Shriberg. 2003. Spotting 
"Hotspots" in Meetings: Human Judgments and 
Prosodic Cues.  Eurospeech 2003, Geneva 
 
61
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Discourse Level Opinion Relations: An Annotation Study
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work proposes opinion frames as a repre-
sentation of discourse-level associations that
arise from related opinion targets and which
are common in task-oriented meeting dialogs.
We define the opinion frames and explain their
interpretation. Additionally we present an
annotation scheme that realizes the opinion
frames and via human annotation studies, we
show that these can be reliably identified.
1 Introduction
There has been a great deal of research in recent
years on opinions and subjectivity. Opinions have
been investigated at the phrase, sentence, and docu-
ment levels. However, little work has been carried
out at the level of discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television (the
opinion targets - what the opinions are about - are
shown in italics).
(1) D:: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons . . .
Speaker D expresses an opinion in favor of a de-
sign that is simple and organic in shape, and against
an alternative design which is not. Several individ-
ual opinions are expressed in this passage. The first
is a negative opinion about the design being too edgy
and box-like, the next is a positive opinion toward
a hand-held design, followed by a negative opin-
ion toward a computery shape, and so on. While
we believe that recognizing individual expressions
of opinions, their properties, and components is im-
portant, we believe that discourse interpretation is
needed as well. It is by understanding the passage
as a discourse that we see edgy, like a box, com-
putery, and many buttons as descriptions of the type
of design D does not prefer, and hand-held, organic
shape, and simple designs as descriptions of the type
he does. These descriptions are not in general syn-
onyms/antonyms of one another; for example, there
are hand-held ?computery? devices and simple de-
signs that are edgy. The unison/opposition among
the descriptions is due to how they are used in the
discourse.
This paper focuses on such relations between the
targets of opinions in discourse. Specifically, we
propose opinion frames, which consist of two opin-
ions which are related by virtue of having united
or opposed targets. We believe that recognizing
opinion frames will provide more information for
NLP applications than recognizing their individual
components alone. Further, if there is uncertainty
about any one of the components, we believe opin-
ion frames are an effective representation incorpo-
rating discourse information to make an overall co-
herent interpretation (Hobbs, 1979; Hobbs, 1983).
To our knowledge, this is the first work to ex-
tend a manual annotation scheme to relate opinions
in the discourse. In this paper, we present opin-
ion frames, and motivate their usefulness through
examples. Then we provide an annotation scheme
for capturing these opinion frames. Finally we per-
form fine-grained annotation studies to measure the
human reliability in recognizing of these opinion
frames.
129
Opinion frames are presented in Section 2, our an-
notation scheme is described in Section 3, the inter-
annotator agreement studies are presented in Section
4, related work is discussed in Section 5, and conclu-
sions are in Section 6.
2 Opinion Frames
2.1 Introduction
The components of opinion frames are individual
opinions and the relationships between their targets.
We address two types of opinions, sentiment and
arguing. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), sentiment includes posi-
tive and negative evaluations, emotions, and judg-
ments, while arguing includes arguing for or against
something, and arguing that something should or
should not be done. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
In addition, the text span capturing the target of the
opinion (again, as interpreted in context) is indicated
in italics.
(2) D:: . . . this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot. A
bit more durable and that can also be ergonomic and it
kind of feels a bit different from all the other remote
controls.
Speaker D expresses his preference for the rub-
bery material for the remote. He reiterates his opin-
ion with a number of positive evaluations like bit
more bouncy, bit more durable, ergonomic and
so on.
All opinions in this example are related to the oth-
ers via opinion frames by virtue of having the same
targets, i.e., the opinions are essentially about the
same things (the rubbery material for the remote).
For example, the opinions ergonomic and a bit dif-
ferent from all the other remote controls are re-
lated in a frame of type SPSPsame, meaning the first
opinion is a S(entiment) with polarity P(ositive); the
second also is a S(entiment) with polarity P(ositive);
and the targets of the opinions are in a same (target)
relation.
The specific target relations addressed in this pa-
per are the relations of either being the same or being
alternatives to one another. While these are not the
only possible relations, they are not infrequent, and
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
they commonly occur in task-oriented dialogs such
as those in our data.
With four opinion type - polarity pairs (SN, SP,
AN, AP), for each of two opinion slots, and two pos-
sible target relations, we have 4 * 4 * 2 = 32 types
of frame, listed in Table 1.
In the remainder of this section, we elaborate fur-
ther on the same target relation (in 2.2) the alter-
native target relation (in 2.3) and explain a method
by which these relationships can be propagated (in
2.4). Finally, we illustrate the usefulness of opinion
frames in discourse interpretation (in 2.5).
2.2 Same Targets
Our notion of sameness for targets includes cases
of anaphora and ellipses, lexically similar items, as
well as less direct relations such as part-whole, sub-
set, inferable, and instance-class.
Looking at the opinion frames for Example 2 in
more detail, we separately list the opinions, followed
by the relations between targets.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same
t1 - t3 same
t3 - t4 same
Ellipsis occurs with bit more durable. [t2] rep-
resents the (implicit) target of that opinion, and [t2]
has a same relation to [t1], the target of the bit more
bouncy opinion. (Note that the interpretation of the
first target, [t1], would require anaphora resolution
of its target span with a previous noun phrase, rub-
bery material.)
Let us now consider the following passage, in
which a meeting participant analyzes two leading re-
130
motes on the market.1
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same
t2 - t3 same
t3 - t4 same
t5 - t1 same
Target [t2] is the set of two leading remotes, and [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is an aspect of that remote, namely
its buttons. Thus, opinion O3 is directly about one
of the remotes, and indirectly about the set of both
remotes. Similarly, opinion O4 is directly about the
buttons of one of the remotes, and indirectly about
that remote itself.
2.3 Alternative Targets
The alt(ernative) target relation arises when multiple
choices are available, and only one can be selected.
For example, in the domain of TV remote controls,
the set of all shapes are alternatives to one another,
since a remote control may have only one shape at a
time. In such scenarios, a positive opinion regarding
one choice may imply a negative opinion toward the
rest of the choices, and vice versa.
As an example, let us now consider the follow-
ing passage (some intervening utterances have been
removed for clarity).
(4) C:: . . . shapes should be curved, so round shapes2
Nothing square-like.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
1In the other examples in this paper, the source (holder) of
the opinions is the speaker. The leading opinion in this example
is an exception: its source is implicit; it is a consensus opinion
that is not necessarily shared by the speaker (i.e., it is a nested
source (Wiebe et al, 2005)).
2In the context of the dialogs, the annotators read the ?so
round shapes? as a summary statement. Had the ?so? been inter-
preted as Arguing, the round shapes would have been annotated
as a target (and linked to curved).
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same
t3 - t4 same
There is an alt relation between, for example,
[t1] and [t2]. Thus, we have an opinion frame be-
tween O1 and O2, whose type is APANalt. From
this frame, we understand that a positive opinion is
expressed toward something and a negative opinion
is expressed toward its alternative.
2.4 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
linked by the human annotators. This path may be
traversed to create links between new pairs of tar-
gets - which in turn results in new opinion frame re-
lations. For instance, in Example 4, the frame with
direct relation is O1O2 APANalt. By following the
alt link from [t1] to [t2] and the same link from [t2]
to [t3], we have an alt link between [t1] and [t3],
and the additional frames O1O3 APANalt and O1O4
APSNalt. Repeating this process would finally link
speaker C?s opinion O1 with B?s opinion O6, yield-
ing a APSNalt frame.
2.5 Interpretation
This section illustrates two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in the
text, and they may contribute toward arriving at a
coherent interpretation (Hobbs, 1979; Hobbs, 1983)
of the opinions in the discourse.
Through opinion frames, opinions regarding
something not explicitly mentioned in the local con-
text and not even lexically related can become rel-
evant, providing more information about someone?s
opinions. This is particularly interesting when alt
relations are involved, as opinions towards one al-
ternative imply opinions of opposite polarity toward
the remaining options. For instance in Example 4
131
above, if we consider only the explicitly stated opin-
ions, there is only one (positive) opinion about the
curved shape, namely O1. However, the speaker ex-
presses several other opinions which reinforce his
positivity toward the curved shape. These are in
fact opinion frames in which the other opinion has
the opposite polarity as O1 and the target relation is
alt (for example frames such as O1O3 APANalt and
O1O4 APSNalt).
In the dialog, notice that speaker B agrees with
C and exhibits his own reinforcing opinions. These
would be similarly linked via targets resulting in
frames like O1O6 APSNalt.
Turning to our second point, arriving at a coher-
ent interpretation obviously involves disambigua-
tion. Suppose that some aspect of an individual
opinion, such as polarity, is unclear. If the discourse
suggests certain opinion frames, this may in turn re-
solve the underlying ambiguity. For instance in Ex-
ample 2, we see that out of context, the polarities of
bouncy and different from other remotes are un-
clear (bounciness and being different may be neg-
ative attributes for another type of object). How-
ever, the polarities of two of the opinions are clear
(durable and ergonomic). There is evidence in this
passage of discourse continuity and same relations,
such as the pronouns, the lack of contrastive cue
phrases, and so on. This evidence suggests that the
speaker expresses similar opinions throughout the
passage, making the opinion frame SPSPsame more
likely throughout. Recognizing the frames would re-
solve the polarity ambiguities of bouncy and differ-
ent.
Example 2 is characterized by opinion frames in
which the opinions reinforce one other. Interest-
ingly, interplays among different opinion types may
show the same type of reinforcement. As we an-
alyzed above, Example 4 is characterized by mix-
tures of opinion types, polarities, and target rela-
tions. However, the opinions are still unified in
the intention to argue for a particular type of shape.
There is evidence in this passage suggesting rein-
forcing frames: the negations are applied to targets
that are alternative to the desired option, and the pas-
sage is without contrastive discourse cues. If we
are able to recognize the best overall set of opinion
frames for the passage, the polarity ambiguities will
be resolved.
On the other hand, evidence for non-reinforcing
opinions would suggest other frames, potentially re-
sulting in different interpretations of polarity and re-
lations among targets. Such non-reinforcing associ-
ations between opinions and often occur when the
speaker is ambivalent or weighing pros and cons.
Table 1 lists the frames that occur in reinforcing sce-
narios in the top row, and the frames that occur in
non-reinforcing scenarios in the bottom row.
3 Annotation Scheme
Our annotation scheme began with the definition
and basics of the opinion annotation from previ-
ous work (Wilson and Wiebe, 2005; Somasundaran
et al, 2007). We then add to it the attributes and
components that are necessary to make an Opinion
Frame.
First, the text span that reveals the opinion expres-
sion is identified. Then, the text spans corresponding
to the targets are marked, if there exist any (we also
allow span-less targets). Then, the type and polar-
ity of the opinion in the context of the discourse is
marked. Finally the targets that are related (again
in the context of the discourse) are linked. Specif-
ically, the components that form the Annotation of
the frame are as follows:
Opinion Span: This is a span of text that reveals
the opinion.
Type: This attribute specifies the opinion type as ei-
ther Arguing or Sentiment.
Polarity: This attribute identifies the valence of an
opinion and can be one of: positive, negative,
neutral, both, unknown.
Target Span: This is a span of text that captures
what an opinion is about. This can be a propo-
sition or an entity.
Target Link: This is an attribute of a target and
records all the targets in the discourse that the
target is related to.
Link Type: The link between two targets is speci-
fied by this attribute as either same or alterna-
tive.
132
In addition to these definitions, our annotation man-
ual has guidelines detailing how to deal with gram-
matical issues, disfluencies, etc. Appendix A illus-
trates how this annotation scheme is applied to the
utterances of Example 4.
Links between targets can be followed in either
direction to construct chains. In this work, we
consider target relations to be commutative, i.e.,
Link(t1,t2) => Link(t2,t1). When a newly anno-
tated target is similar (or opposed) to a set of tar-
gets already participating in same relations, then the
same (or alt) link is made only to one of them - the
one that looks most natural. This is often the one
that is closest.
4 Annotation Studies
Construction of an opinion frame is a stepwise pro-
cess where first the text spans revealing the opinions
and their targets are selected, the opinion text spans
are classified by type and polarity and finally the
targets are linked via one of the possible relations.
We split our annotation process into these 3 intuitive
stages and use an evaluation that is most applicable
for the task at that stage.
Two annotators (both co-authors on the paper) un-
derwent training at each stage, and the annotation
manual was revised after each round of training. In
order to prevent errors incurred at earlier stages from
affecting the evaluation of later stages, the anno-
tators produced a consensus version at the end of
each stage, and used that consensus annotation as
the starting point for the next annotation stage. In
producing these consensus files, one annotator first
annotated a document, and the other annotator re-
viewed the annotations, making changes if needed.
This prevented any discussion between the annota-
tors from influencing the tagging task of the next
stage.
In the following subsections, we first introduce
the data and then present our results for annotation
studies for each stage, ending with discussion.
4.1 Data
The data used in this work is the AMI meeting cor-
pus (Carletta et al, 2005) which contains multi-
modal recordings of group meetings. We annotated
meetings from the scenario based meetings, where
Gold Exact Lenient Subset
ANN-1 53 89 87
ANN-2 44 76 74
Table 2: Inter-Annotator agreement on Opinion Spans
four participants collaborate to design a new TV
remote control in a series of four meetings. The
meetings represent different project phases, namely
project kick-off, functional design, conceptual de-
sign, and detailed design. Each meeting has rich
transcription and segment (turn/utterance) informa-
tion for each speaker. Each utterance consists of
one or more sentences. At each agreement stage we
used approximately 250 utterances from a meeting
for evaluation. The annotators also used the audio
and video recordings in the annotation of meetings.
4.2 Opinion Spans and Target Spans
In this step, the annotators selected text spans and
labeled them as opinion or target We calculated our
agreement for text span retrieval similar to Wiebe et
al. (2005). This agreement metric corresponds to
the Precision metric in information retrieval, where
annotations from one annotator are considered the
gold standard, and the other annotator?s annotations
are evaluated against it.
Table 2 shows the inter-annotator agreement (in
percentages). For the first row, the annotations pro-
duced by Annotator-1 (ANN-1) are taken as the gold
standard and, for the second row, the annotations
from annotator-2 form the gold standard. The ?Ex-
act? column reports the agreement when two text
spans have to match exactly to be considered cor-
rect. The ?Lenient? column shows the results if
an overlap relation between the two annotators? re-
trieved spans is also considered to be a hit. Wiebe
et al (2005) use this approach to measure agree-
ment for a (somewhat) similar task of subjectivity
span retrieval in the news corpus. Our agreement
numbers for this column is comparable to theirs. Fi-
nally, the third column, ?Subset?, shows the agree-
ment for a more strict constraint, namely, that one
of the spans must be a subset of the other to be con-
sidered a match. Two opinion spans that satisfy this
relation are ensured to share all the opinion words of
the smaller span.
The numbers indicate that, while the annotators
133
Gold Exact Lenient Subset
ANN-1 54 73 71
ANN-2 54 75 74
Table 3: Inter-Annotator agreement on Target Spans
Gold Exact Lenient Subset
ANN-1 74 87 87
ANN-2 76 90 90
Table 4: Inter-Annotator agreement on Targets with Per-
fect Opinion spans
do not often retrieve the exact same span, they
reliably retrieve approximate spans. Interestingly,
the agreement numbers between Lenient and Sub-
set columns are close. This implies that, in the cases
of inexact matches, the spans retrieved by the two
annotators are still close. They agree on the opinion
words and differ mostly on the inclusion of func-
tion words (e.g. articles) and observation of syntac-
tic boundaries.
In similar fashion, Table 3 gives the inter-
annotator agreement for target span retrieval. Ad-
ditionally, Table 4 shows the inter-annotator agree-
ment for target span retrieval when opinions that do
not have an exact match are filtered out. That is, Ta-
ble 4 shows results only for targets of the opinions
on which the annotators perfectly agree. As targets
are annotated with respect to the opinions, this sec-
ond evaluation removes any effects of disagreements
in the opinion detection task. As seen in Table 4, this
improves the inter-coder agreement.
4.3 Opinion Type and Polarity
In this step, the annotators began with the consensus
opinion span and target span annotations. We hy-
pothesized that given the opinion expression, deter-
mining whether it is Arguing or Sentiment would not
be difficult. Similarly, we hypothesized that target
information would make the polarity labeling task
clearer.
As every opinion instance is tagged with a type
Type Tagging Polarity Tagging
Accuracy 97.8% 98.5%
? 0.95 0.952
Table 5: Inter-Annotator agreement on Opinion Types
and Polarity
and polarity, we use Accuracy and Cohen?s Kappa
(?) metric (Cohen, 1960). The ? metric measures
the inter-annotator agreement above chance agree-
ment. The results, in Table 5, show that ? both for
type and polarity tagging is very high. This con-
firms our hypothesis that Sentiment and Arguing can
be reliably distinguished once the opinion spans are
known. Our polarity detection task shows an im-
provement in ? over a similar polarity assignment
task by Wilson et al (2005) for the news corpus (?
of 0.72). We believe this improvement can partly be
attributed to the target information available to our
annotators.
4.4 Target Linking
As an intuitive first step in evaluating target link-
ing, we treat target links in the discourse similarly to
anaphoric chains and apply methods developed for
co-reference resolution (Passonneau, 2004) for our
evaluation. Passonneau?s method is based on Krip-
pendorf?s ? metric (Krippendorff, 2004) and allows
for partial matches between anaphoric chains. In ad-
dition to this, we evaluate links identified by both
annotators for the type (same / alternative) labeling
task with the help of the ? metric.
Passonneau (2004) reports that in her co-reference
task on spoken monologs, ? varies with the diffi-
culty of the corpus (from 0.46 to 0.74). This is true
in our case too. Table 6 shows our agreement for
the four types of meetings in the AMI corpus: the
kickoff meeting (a), the functional design (b), the
conceptual design (c) and the detailed design (d).
Of the meetings, the kickoff meeting (a) we use
has relatively clear discussions. The conceptual de-
sign meeting (c) is the toughest, as as participants
are expressing opinions about a hypothetical (desir-
able) remote. In our detailed design meeting (d),
there are two final designs being evaluated. On an-
alyzing the chains from the two annotators, we dis-
covered that one annotator had maintained two sepa-
rate chains for the two remotes as there is no explicit
linguistic indication (within the 250 utterances) that
these two are alternatives. The second annotator, on
the other hand, used the knowledge that the goal
of the meeting is to design a single TV remote to
link them as alternatives. Thus by changing just
two links in the second annotator?s file to account
for this, our ? for this meeting went up from 0.52
134
Meeting: a b c d
Target linking (?) 0.79 0.74 0.59 0.52
Relation Labeling (?) 1 1 0.91 1
Table 6: Inter-Annotator agreement on Target relation
identification
to 0.70. We plan to further explore other evalua-
tion methodologies that account for severity of dif-
ferences in linking and are more relevant for our
task. Nonetheless, the resulting numbers indicate
that there is sufficient information in the discourse
to provide for reliable linking of targets.
The high ? for the relation type identification
shows that once the presence of a link is detected,
it is not difficult to determine if the targets are simi-
lar or alternatives to each other.
4.5 Discussion
Our agreement studies help to identify the aspects of
opinion frames that are straightforward, and those
that need complex reasoning. Our results indicate
that while the labeling tasks such as opinion type,
opinion polarity and target relation type are rel-
atively reliable for humans, retrieval of opinions
spans, target spans and target links is more difficult.
A common cause of annotation disagreement is
different interpretation of the utterance, particularly
in the presence of disfluencies and restarts. For ex-
ample consider the following utterance where a par-
ticipant is evaluating the drawing of another partici-
pant on the white board.
(5) It?s a baby shark , it looks to me, . . .
One annotator interpreted this ?it looks to me? as
an arguing for the belief that it was indeed a draw-
ing of a baby shark (positive Arguing). The sec-
ond annotator on the other hand looked at it as a
neutral viewpoint/evaluation (Sentiment) being ex-
pressed regarding the drawing. Thus even though
both annotators felt an opinion is being expressed,
they differed on its type and polarity.
There are some opinions that are inherently on the
borderline of Sentiment and Arguing. For example,
consider the following utterance where there is an
appeal to importance:
(6) Also important for you all is um the production cost
must be maximal twelve Euro and fifty cents.
Here, ?also important? might be taken as an assess-
ment of the high value of adhering to the budget (rel-
ative to other constraints), or simply as an argument
for adhering to the budget.
One potential source of problems to the target-
linking process consists of cases where the same
item becomes involved in more than one opposition.
For instance, in the example below, speaker D ini-
tially sets up an alternative between speech recog-
nition and buttons as a possible interface for navi-
gation. But later, speaker A re-frames the choice as
between having speech recognition only and having
both options. Connecting up all references to speech
recognition as a target respects the co-reference but
it also results in incorrect conclusions: the speech
recognition is an alternative to having both speech
recognition and buttons.
(7) A:: One thing is interesting is talking about speech
recognition in a remote control...
D:: ... So that we don?t need any button on the remote
control it would be all based on speech.
A:: ... I think that would not work so well. You wanna
have both options.
5 Related Work
Evidence from the surrounding context has been
used previously to determine if the current sentence
should be subjective/objective (Riloff et al, 2003;
Pang and Lee, 2004)) and adjacency pair informa-
tion has been used to predict congressional votes
(Thomas et al, 2006). However, these methods do
not explicitly model the relations between opinions.
Additionally, in our scheme opinions that are not
in the immediate context may be allowed to influ-
ence the interpretation of a given opinion via target
chains.
Polanyi and Zaenen (2006), in their discussion on
contextual valence shifters, have also observed the
phenomena described in this work - namely that a
central topic may be divided into subtopics in order
to perform evaluations, and that discourse structure
can influence the overall interpretation of valence.
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations with
a local aspect (or target) model to make a more in-
formed overall decision for sentiment classification.
The contrastive cue indicates a change in the senti-
ment polarity. In our scheme, their aspects would
be related as same and their high contrast relations
would result in frames such as SPSNsame, SNSP-
same. Additionally, our frame relations would link
sentiments across non-adjacent clauses, and make
connections via alt target relations.
135
Considering the discourse relation annotations in
the PDTB (Prasad et al, 2006), there can be align-
ment between discourse relations (like contrast) and
our opinion frames when the frames represent dom-
inant relations between two clauses. However, when
the relation between opinions is not the most promi-
nent one between two clauses, the discourse relation
may not align with the opinion frames. And when an
opinion frame is between two opinions in the same
clause, there would be no discourse relation counter-
part at all. Further, opinion frames assume particular
intentions that are not necessary for the establish-
ment of ostensibly similar discourse relations. For
example, we may not impose an opinion frame even
if there are contrastive cues. (Please refer to Ap-
pendix B for examples)
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
schemes for various available corpora of conversa-
tion (Dhillon et al (2003) for ICSI MRDA; Car-
letta et al (2005) for AMI ) As shown by Soma-
sundaran et al (2007), dialog structure information
and opinions are in fact complementary. We believe
that, like discourse relations, dialog information will
additionally help in arriving at an overall coherent
interpretation.
6 Conclusion and Future work
This is the first work that extends an opinion annota-
tion scheme to relate opinions via target relations.
We first introduced the idea of opinion frames as
a representation capturing discourse level relations
that arise from related opinion targets and which are
common in task-oriented dialogs such as our data.
We built an annotation scheme that would capture
these relationships. Finally, we performed extensive
inter-annotator agreement studies in order to find the
reliability of human judgment in recognizing frame
components. Our results and analysis provide in-
sights into the complexities involved in recognizing
discourse level relations between opinions.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152.
References
J. Carletta, S. Ashby, and et al 2005. The AMI Meetings
Corpus. In Proceedings of Measuring Behavior Sym-
posium on ?Annotating and measuring Meeting Be-
havior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg. 2003.
Meeting recorder project: Dialog act labeling guide.
Technical report, ICSI Tech Report TR-04-002.
J. Hobbs. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
J. Hobbs, 1983. Why is Discourse Coherent?, pages 29?
70. Buske Verlag.
K. Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology, 2nd Edition. Sage Publica-
tions, Thousand Oaks, California.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
R. J. Passonneau. 2004. Computing reliability for coref-
erence annotation. In LREC.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect in
Text: Theory and Applications. Springer.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, and B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. In Workshop on Sentiment and Subjectivity
in Text. ACL.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In CoNLL 2003.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In SIG-
dial Workshop on Discourse and Dialogue 2007.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In EMNLP 2006.
J. Wiebe, T. Wilson, and C Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, pages 164?210.
T. Wilson and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT-EMNLP 2005.
136
A Annotation Example
C:: . . . shapes should be curved, so round shapes. Nothing
square-like.
C:: . . . So we shouldn?t have too square corners and that kind
of thing.
B:: Yeah okay. Not the old box look.
Span Attributes
O1 should be type=Arguing; Polarity=pos; target=t1
t1 curved Link,type=(t2,alt)
O2 Nothing type=Arguing; Polarity=neg; target=t2
t2 square-like Link,type=(t1,alt),(t3,same)
O3 shouldn?t have type=Arguing; Polarity=neg; target=t3
O4 too type=Sentiment; Polarity=neg; target=t3
t3 square corners Link,type=(t2,same),(t4,same)
O5 Not type=Arguing; Polarity=neg; target=t4
t4 the old box look Link,type=(t3,same)
O6 the old box look type=Sentiment; Polarity=neg; target=t4
B Comparison between Opinion Frames
and Discourse Relations
Opinion frames can align with discourse relations
between clauses only when the frames represent the
dominant relation between two clauses (1); but not
when the opinions occur in the same clause (2); or
when the relation between opinions is not the most
prominent (3); or when two distinct targets are nei-
ther same nor alternatives (4).
(1) Non-reinforcing opinion frame (SNSP-
same); Contrast discourse relation
D :: And so what I have found and after a lot
of work actually I draw for you this schema
that can be maybe too technical for you but
is very important for me you know.
(2) Reinforcing opinion frame (SPSPsame); no
discourse relation
Thirty four percent said it takes too long
to learn to use a remote control, they want
something that?s easier to use straight away,
more intuitive perhaps.
(3) Reinforcing opinion frame (SPSPsame);
Reason discourse relation
She even likes my manga, actually the quote
is: ?I like it, because you like it, honey.?
(source: web)
(4) Unrelated opinions; Contrast discourse re-
lation
A :: Yeah, what I have to say about means.
The smart board is okay. Digital pen is hor-
rible. I dunno if you use it. But if you want
to download it to your computer, it?s doesn?t
work. No.
137
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 66?74,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Opinion Graphs for Polarity and Discourse Classification
?
Swapna Somasundaran
Univ. of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Galileo Namata
Univ. of Maryland
College Park, MD 20742
namatag@cs.umd.edu
Lise Getoor
Univ. of Maryland
College Park, MD 20742
getoor@cs.umd.edu
Janyce Wiebe
Univ. of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work shows how to construct
discourse-level opinion graphs to perform
a joint interpretation of opinions and dis-
course relations. Specifically, our opinion
graphs enable us to factor in discourse in-
formation for polarity classification, and
polarity information for discourse-link
classification. This inter-dependent frame-
work can be used to augment and im-
prove the performance of local polarity
and discourse-link classifiers.
1 Introduction
Much research in opinion analysis has focused on
information from words, phrases and semantic ori-
entation lexicons to perform sentiment classifica-
tion. While these are vital for opinion analysis,
they do not capture discourse-level associations
that arise from relations between opinions. To cap-
ture this information, we propose discourse-level
opinion graphs for classifying opinion polarity.
In order to build our computational model, we
combine a linguistic scheme opinion frames (So-
masundaran et al, 2008) with a collective classifi-
cation framework (Bilgic et al, 2007). According
to this scheme, two opinions are related in the dis-
course when their targets (what they are about) are
related. Further, these pair-wise discourse-level
relations between opinions are either reinforcing
or non-reinforcing frames. Reinforcing frames
capture reinforcing discourse scenarios where the
individual opinions reinforce one another, con-
tributing to the same opinion polarity or stance.
Non-reinforcing frames, on the other hand, cap-
ture discourse scenarios where the individual opin-
ions do not support the same stance. The indi-
vidual opinion polarities and the type of relation
?
This research was supported in part by the Department
of Homeland Security under grant N000140710152.
between their targets determine whether the dis-
course frame is reinforcing or non-reinforcing.
Our polarity classifier begins with information
from opinion lexicons to perform polarity classifi-
cation locally at each node. It then uses discourse-
level links, provided by the opinion frames, to
transmit the polarity information between nodes.
Thus the opinion classification of a node is not
just dependent on its local features, but also on the
class labels of related opinions and the nature of
these links. We design two discourse-level link
classifiers: the target-link classifier, which deter-
mines if a given node pair has unrelated targets (no
link), or if their targets have a same or alternative
relation, and the frame-link classifier, which deter-
mines if a given node pair has no link, reinforcing
or non-reinforcing link relation. Both these classi-
fiers too first start with local classifiers that use lo-
cal information. The opinion graph then provides
a means to factor in the related opinion informa-
tion into the link classifiers. Our approach enables
using the information in the nodes (and links) to
establish or remove links in the graph. Thus in-
formation flows to and fro between all the opinion
nodes and discourse-level links to achieve a joint
inference.
The paper is organized as follows: We first de-
scribe opinion graphs, a structure that can capture
discourse-level opinion relationships in Section 2,
and then describe our joint interpretation approach
to opinion analysis in Section 3. Next, we describe
our algorithm for joint interpretation in Section 4.
Our experimental results are reported in Section 5.
We discuss related work in Section 6 and conclude
in Section 7.
2 Discourse-Level Opinion Graphs
The pairwise relationships that compose opinion
frames can be used to construct a graph over opin-
ion expressions in a discourse, which we refer
to as the discourse-level opinion graph (DLOG).
66
Figure 1 Opinion Frame Annotations.
In this section, we describe these graphs and il-
lustrate their applicability to goal-oriented multi-
party conversations.
The nodes in the DLOG represent opinions, and
there are two kinds of links: target links and frame
links. Each opinion node has a polarity (positive,
negative or neutral) and type (sentiment or argu-
ing). Sentiment opinions are evaluations, feelings
or judgments about the target. Arguing opinions
argue for or against something. Target links are
labeled as either same or alternatives. Same links
hold between targets that refer to the same en-
tity or proposition, while alternative links hold be-
tween targets that are related by virtue of being op-
posing (mutually exclusive) options in the context
of the discourse. The frame links correspond to
the opinion frame relation between opinions.
We illustrate the construction of the opinion
graph with an example (Example 1, from Soma-
sundaran et al (2008)) from a multi-party meet-
ing corpus where participants discuss and design a
new TV remote control. The opinion expressions
are in bold and their targets are in italics. Notice
here that speaker D has a positive sentiment to-
wards the rubbery material for the TV remote.
(1) D:: ... this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot.
A bit more durable and that can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
All the individual opinions in this example are
essentially regarding the same thing ? the rub-
bery material. The speaker?s positive sentiment is
apparent from the text spans bit more bouncy,
bit more durable, ergonomic and a bit different
from all the other remote controls. The explicit
targets of these opinions (it?s, that, and it) and the
implicit target of ?a bit more durable? are thus all
linked with same relations.
Figure 1 illustrates the individual opinion anno-
tations, target annotations (shown in italics) and
the relations between the targets (shown in dotted
lines). Note that the target of a bit more durable
is a zero span ellipsis that refers back to the rub-
bery material. The opinion frames resulting from
the individual annotations make pairwise connec-
tions between opinion instances, as shown in bold
lines in the figure. For example, the two opinions
bit more bouncy and ergonomic, and the same
link between their targets (it?s and that), make up
an opinion frame. An opinion frame type is de-
rived from the details (type and polarity) of the
opinions it relates and the target relation involved.
Even though the different combinations of opin-
ion type (sentiment and arguing), polarity (posi-
tive and negative) and target links (same and al-
ternative) result in many distinct frames types (32
in total), they can be grouped, according to their
discourse-level characteristics, into the two cat-
egories reinforcing and non-reinforcing. In this
work, we only make this category distinction for
opinion frames and the corresponding frame links.
The next example (Example 2, also from So-
masundaran et al (2008)) illustrates an alterna-
tive target relation. In the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. In this passage, speaker C?s positive stance
towards the curved shape is brought out even more
strongly with his negative opinions toward the al-
ternative, square-like, shapes.
(2) C:: . . . shapes should be curved, so round shapes.
Nothing square-like.
.
.
.
C:: . . . So we shouldn?t have too square corners
and that kind of thing.
The reinforcing frames characteristically show
a reinforcement of an opinion or stance in the dis-
course. Both the examples presented above depict
a reinforcing scenario. In the first example, the
opinion towards the rubbery material is reinforced
by repeated positive sentiments towards it, while
in the second example the positive stance towards
the curved shapes is further reinforced by nega-
tive opinions toward the alternative option. Ex-
amples of non-reinforcing scenarios are ambiva-
lence between alternative options (for e.g., ?I like
the rubbery material but the plastic will be much
67
cheaper?) or mixed opinions about the same tar-
get (for e.g., weighing pros and cons ?The rubbery
material is good but it will be just so expensive?).
3 Interdependent Interpretation
Our interdependent interpretation in DLOGs is
motivated by the observation that, when two opin-
ions are related, a clear knowledge of the polarity
of one of them makes interpreting the other much
easier. For instance, suppose an opinion classi-
fier wants to find the polarity of all the opinion
expressions in Example 1. As a first step, it can
look up opinion lexicons to infer that words like
?bouncy?, ?durable? and ? ergonomic? are pos-
itive. However, ?a bit different ? cannot be re-
solved via this method, as its polarity can be dif-
ferent in different scenarios.
Suppose now we relate the targets of opinions.
There are clues in the passage that the targets are
related via the same relation; for instance they
are all third person pronouns occurring in adja-
cent clauses and sentences. Once we relate the
targets, the opinions of the passage are related via
target links in the discourse opinion graph. We
are also able to establish frames using the opinion
information and target link information wherever
they are available, i.e., a reinforcing link between
bit more bouncy and ergonomic. For the places
where all the information is not available (between
ergonomic and a bit different) there are multiple
possibilities. Depending on the polarity, either a
reinforcing frame (if a bit different has positive
polarity) or a non-reinforcing frame (if a bit dif-
ferent has negative polarity) can exist. There are
clues in the discourse that this passage represents
a reinforcing scenario. For instance there are rein-
forcing frames between the first few opinions, the
repeated use of ?and? indicates a list, conjunction
or expansion relation between clauses (according
to the Penn Discourse TreeBank (PDTB) (Prasad
et al, 2008)), and there is a lack of contrastive
clues that would indicate a change in the opin-
ion. Thus the reinforcing frame link emerges as
being the most likely candidate. This in turn dis-
ambiguates the polarity of a bit different. Thus,
by establishing target links and frame links be-
tween the opinion instances, we are able to per-
form a joint interpretation of the opinions.
The interdependent framework of this example
is iterative and dynamic ? the information in the
nodes can be used to change the structure (i.e.,
establish new links), and the structure provides a
framework to change node polarity. We build our
classification framework and feature sets with re-
spect to this general framework, where the node
labels as well as the structure of the graph are pre-
dicted in a joint manner.
Thus our interdependent interpretation frame-
work has three main units: an instance polarity
classifier (IPC), a target-link classifier (TLC), and
a frame-link classifier (FLC). IPC classifies each
node (instance), which may be a sentence, utter-
ance or an other text span, as positive, negative
or neutral. The TLC determines if a given node
pair has related targets and whether they are linked
by a same or alternative relation. The FLC deter-
mines if a given node pair is related via frames,
and whether it is a reinforcing or non-reinforcing
link. As we saw in the example, there are local
clues available for each unit to arrive at its classi-
fication. The discourse augments this information
to aid in further disambiguation.
4 Collective Classification Framework
For our collective classification framework, we
use a variant of the iterative classification al-
gorithm (ICA) proposed by Bilgic et al(2007).
It combines several common prediction tasks in
graphs: object classification (predicting the label
of an object) and link prediction (predicting the
existence and class of a link between objects).
For our tasks, object classification directly corre-
sponds to predicting opinion polarity and the link
prediction corresponds to predicting the existence
of a same or alternative target link or a reinforc-
ing or non-reinforcing frame link between opin-
ions. We note that given the nature of our problem
formulation and approach, we use the terms link
prediction and link classification interchangeably.
In the collective classification framework, there
are two sets of features to use. The first are local
features which can be generated for each object or
link, independent of the links in which they par-
ticipate, or the objects they connect. For example,
the opinion instance may contain words that oc-
cur in sentiment lexicons. The local features are
described in Section 4.2. The second set of fea-
tures, the relational features, reflect neighborhood
information in the graph. For frame link classifi-
cation, for example, there is a feature indicating
whether the connected nodes are predicted to have
the same polarity. The relational features are de-
68
scribed in Section 4.3.
4.1 DLOG-ICA Algorithm
Our variant of the ICA algorithm begins by pre-
dicting the opinion polarity, and link type using
only the local features. We then randomly order
the set of all opinions and links and, in turn, pre-
dict the polarity or class using the local features
and the values of the currently predicted relational
features based on previous predictions. We repeat
this until some stopping criterion is met. For our
experiments, we use a fixed number of 30 itera-
tions which was sufficient, in most of our datasets,
for ICA to converge to a solution. The pseudocode
for the algorithm is shown in Algorithm 4.1.
Algorithm 1 DLOG-ICA Algorithm
for each opinion o do {bootstrapping}
Compute polarity for o using local attributes
end for
for each target link t do {bootstrapping}
Compute label for t using local attributes
end for
for each frame link f do {bootstrapping}
Compute label for f using local attributes
end for
repeat {iterative classification}
Generate ordering I over all nodes and links
for each i in I do
if i is an opinion instance then
Compute polarity for i using local and
relational attributes
else if i is a target link then
Compute class for i using local and re-
lational attributes
else if i is a frame link then
Compute class for i using local and re-
lational attributes
end if
end for
until Stopping criterion is met
The algorithm is one very simple way of making
classifications that are interdependent. Once the
local and relational features are defined, a variety
of classifiers can be used. For our experiments, we
use SVMs. Additional details are provided in the
experiments section.
4.2 Local Features
For the local polarity classifier, we employ opin-
ion lexicons, dialog information, and unigram fea-
Feature Task
Time difference between the node pair TLC, FLC
Number of intervening instances TLC, FLC
Content word overlap between the node pair TLC,FLC
Focus space overlap between the node pair TLC, FLC
Bigram overlap between the node pair * TLC, FLC
Are both nodes from same speaker * TLC, FLC
Bag of words for each node TLC, FLC
Anaphoric indicator in the second node TLC
Adjacency pair between the node pair FLC
Discourse relation between node pair * FLC
Table 1: Features and the classification task it is used for;
TLC = target-link classification, FLC = Frame-link classifi-
cation
tures. We use lexicons that have been success-
fully used in previous work (the polarity lexicon
from (Wilson et al, 2005) and the arguing lexi-
con (Somasundaran et al, 2007)). Previous work
used features based on parse trees, e.g., (Wilson et
al., 2005; Kanayama and Nasukawa, 2006), but
our data has very different characteristics from
monologic texts ? the utterances and sentences are
much shorter, and there are frequent disfluencies,
restarts, hedging and repetitions. Because of this,
we cannot rely on parsing features. On the other
hand, in this data, we have dialog act information
1
(Dialog Acts), which we can exploit. Note that the
IPC uses only the Dialog Act tags (instance level
tags like Inform, Suggest) and not the dialog struc-
ture information.
Opinion frame detection between sentences has
been previously attempted (Somasundaran et al,
2008) by using features that capture discourse
and dialog continuity. Even though our link
classification tasks are not directly comparable
(the previous work performs binary classifica-
tion of frame-present/frame-absent between opin-
ion bearing sentences, while this work performs
three-way classification: no-link/reinforcing/non-
reinforcing between DA pairs), we adapt the fea-
tures for the link classification tasks addressed
here. These features depend on properties of the
nodes that the link connects. We also create some
new features that capture discourse relations and
lexical overlap.
Table 1 lists the link classification features.
New features are indicated with a ?*?. Continu-
ous discourse indicators, like time difference be-
tween the node pair and number of intervening
instances are useful for determining if the two
nodes can be related. The content word over-
1
Manual annotations for Dialog act tags and adjacency
pairs are available for the AMI corpus.
69
lap, and focus space overlap features (the focus
space for an instance is a list of the most recently
used NP chunks; i.e., NP chunks in that instance
and a few previous instances) capture the overlap
in topicality within the node pair; while the bi-
gram overlap feature captures the alignment be-
tween instances in terms of function words as well
as content words. The entity-level relations are
captured by the anaphoric indicator feature that
checks for the presence of pronouns such as it and
that in the second node in the node pair. The adja-
cency pair and discourse relation are actually fea-
ture sets that indicate specific dialog-structure and
discourse-level relations. We group the list of dis-
course relations from the PDTB into the following
sets: expansion, contingency, alternative, tempo-
ral, comparison. Each discourse relation in PDTB
is associated with a list of discourse connective
words.
2
Given a node pair, if the first word of the
later instance (or the last word first instance) is a
discourse connective word, then we assume that
this node is connecting back (or forward) in the
discourse and the feature set to which the connec-
tive belongs is set to true (e.g., if a latter instance
is ?because we should ...?, it starts with the con-
nective ?because?, and connects backwards via a
contingency relation). The adjacency pair feature
indicates the presence of a particular dialog struc-
ture (e.g., support, positive-assessment) between
the nodes.
4.3 Relational Features
In addition to the local features, we introduce re-
lational features (Table 2) that incorporate related
class information as well as transfer label informa-
tion between classifiers. As we saw in our example
in Figure 1, we need to know not only the polar-
ity of the related opinions, but also the type of the
relation between them. For example, if the frame
relation between ergonomic and a bit different is
non-reinforcing, then the polarity of a bit differ-
ent is likely to be negative. Thus link labels play
an important role in disambiguating the polarity.
Accordingly, our relational features transfer infor-
mation of class labels from other instances of the
same classifier as well as between different clas-
sifiers. Table 2 lists our relational features. Each
row represents a set of features. Features are gen-
erated for all combinations of x, y and z for each
2
The PDTB provides a list of discourse connectives and
the list of discourse relations each connective signifies.
row. For example, one of the features in the first
row is Number of neighbors with polarity type pos-
itive, that are related via a reinforcing frame link.
Thus each feature for the polarity classifier iden-
tifies neighbors for a given node via a specific re-
lation (z or y) and factors in their polarity values.
Similarly, both link classifiers use polarity infor-
mation of the node pair, and other link relations
involving the nodes of the pair.
5 Evaluation
We experimentally test our hypothesis that
discourse-level information is useful and non-
redundant with local information. We also wanted
to test how the DLOG performs for varying
amounts of available annotations: from full neigh-
borhood information to absolutely no neighbor-
hood information.
Accordingly, for polarity classification, we im-
plemented three scenarios: ICA-LinkNeigh, ICA-
LinkOnly and ICA-noInfo. The ICA-LinkNeigh
scenario measures the performance of the DLOG
under ideal conditions (full neighborhood infor-
mation) ? the structure of the graph (link infor-
mation) as well as the neighbors? class are pro-
vided (by an oracle). Here we do not need the
TLC, or the FLC to predict links and the Instance
Polarity Classifier (IPC) is not dependent on its
predictions from the previous iteration. On the
other hand, the ICA-noInfo scenario is the other
extreme, and has absolutely no neighborhood in-
formation. Each node does not know which nodes
in the network it is connected to apriori, and also
has no information about the polarity of any other
node in the network. Here, the structure of the
graph, as well as the node classes, have to be in-
ferred via the collective classification framework
described in Sections 3 and 4. The ICA-LinkOnly
is an intermediate condition, and is representative
of scenarios where the discourse relationships be-
tween nodes is known. Here we start with the link
information (from an oracle) and the IPC uses the
collective classification framework to infer neigh-
bor polarity information.
Similarly, we vary the amounts of neighbor-
hood information for the TLC and FLC classifiers.
In the ICA-LinkNeigh condition, TLC and FLC
have full neighborhood information. In the ICA-
noInfo condition, TLC and FLC are fully depen-
dent on the classifications of the previous rounds.
In the ICA-Partial condition, the TLC classifier
70
Feature
Opinion Polarity Classification
Number of neighbors with polarity type x linked via frame link z
Number of neighbors with polarity type x linked via target link y
Number of neighbors with polarity type x and same speaker linked via frame link z
Number of neighbors with polarity type x and same speaker linked via target link y
Target Link Classification
Polarity of the DA nodes
Number of other target links y involving the given DA nodes
Number of other target links y involving the given DA nodes and other same-speaker nodes
Presence of a frame link z between the nodes
Frame Link Classification
Polarity of the DA nodes
Number of other frame links z involving the given DA nodes
Number of other frame links z involving the given DA nodes and other same-speaker nodes
Presence of a target link y between the nodes
Table 2: Relational features: x ? {non-neutral (i.e., positive or negative), positive, negative}, y ? {same, alt}, z ?
{reinforcing, non-reinforcing}
uses true frame-links and polarity information,
and previous-stage classifications for information
about neighborhood target links; the FLC classi-
fier uses true target-links and polarity information,
and previous-stage classifications for information
about neighborhood frame-links.
5.1 Data
For our experiments, we use the opinion frame
annotations from previous work (Somasundaran
et al, 2008). These annotations consist of the
opinion spans that reveal opinions, their targets,
the polarity information for opinions, the labeled
links between the targets and the frame links be-
tween the opinions. The annotated data consists
of 7 scenario-based, multi-party meetings from the
AMI meeting corpus (Carletta et al, 2005). The
manual Dialog Act (DA) annotations, provided by
AMI, segment the meeting transcription into sep-
arate dialog acts. We use these DAs as nodes or
instances in our opinion graph.
A DA is assigned the opinion orientation of the
words it contains (for example, if a DA contains a
positive opinion expression, then the DA assigned
the positive opinion category). We filter out very
small DAs (DAs with fewer than 3 tokens, punctu-
ation included) in order to alleviate data skewness
problem in the link classifiers. This gives us a to-
tal of 4606 DA instances, of which 1935 (42%)
have opinions. Out of these 1935, 61.7% are posi-
tive, 30% are negative and the rest are neutral. The
DAs that do not have opinions are considered neu-
tral, and have no links in the DLOG. We create
DA pairs by first ordering the DAs by their start
time, and then pairing a DA with five DAs before
it, and five DAs after it. The classes for target-
link classification are no-link, same, alt. The gold
standard target-link class is decided for a DA pair
based on the target link between the targets of the
opinions contained in that pair. Similarly, the la-
bels for the frame-link labeling task are no-link,
reinforcing, non-reinforcing. The gold standard
frame link class is decided for a DA pair based on
the frame between opinions contained by that pair.
In our data, of the 4606 DAs, 1118 (24.27%) par-
ticipate in target links with other DAs, and 1056
(22.9%) form frame links. The gold standard data
for links, which has pair-wise information, has a
total of 22,925 DA pairs, of which 1371 (6%) pairs
have target links and 1264 (5.5%) pairs have frame
links.
We perform 7-fold cross-validation experi-
ments, using the 7 meetings. In each fold, 6 meet-
ings are used for training and one meeting is used
for testing.
5.2 Classifiers
Our baseline (Base) classifies the test data based
on the distribution of the classes in the training
data. Note that due to the heavily skewed nature of
our link data, this classifier performs very poorly
for minority class prediction, even though it may
achieve good overall accuracy.
For our local classifiers, we used the classifiers
from the Weka toolkit (Witten and Frank, 2002).
For opinion polarity, we used the Weka?s SVM
implementation. For the target link and frame link
classes, the huge class skew caused SVM to learn a
trivial model and always predict the majority class.
To address this, we used a cost sensitive classifier
in Weka where we set the cost of misclassifying a
less frequent class, A, to a more frequent class, B,
71
Base Local ICA
LinkNeigh LinkOnly noInfo
Acc 45.9 68.7 78.8 72.9 68.4
Class: neutral (majority class)
Prec 61.2 76.3 83.9 78.2 73.5
Rec 61.5 83.9 89.6 89.1 86.6
F1 61.1 79.6 86.6 83.2 79.3
Class: positive polarity
Prec 26.3 56.2 70.9 63.3 57.6
Rec 26.1 46.6 62.0 47.0 42.8
F1 25.8 50.4 65.9 53.5 48.5
Class: negative polarity
Prec 12.4 52.3 64.6 56.3 55.2
Rec 12.2 44.3 60.2 48.2 38.2
F1 12.2 46.0 61.9 51.2 43.9
Table 3: Performance of Polarity Classifiers
as |B|/|A| where |class| is the size of the class in
the training set. All other misclassification costs
are set to 1.
For our collective classification, we use the
above classifiers for local features (l) and use sim-
ilar, separate classifiers for relational features (r).
For example, we learned an SVM for predicting
opinion polarity using only the local features and
learned another SVM using only relational fea-
tures. For the ICA-noInfo condition, where we
use TLC and FLC classifiers, we combine the
predictions using a weighted combination where
P (class|l, r) = ? ? P (class|l) + (1 ? ?) ?
P (class|r). This allows us to vary the influence
each feature set has to the overall prediction. The
results for ICA-noInfo are reported on the best per-
forming ? (0.7).
5.3 Results
Our polarity classification results are presented
in Table 3, specifically accuracy (Acc), precision
(Prec), recall (Rec) and F-measure (F1). As we
can see, the results are mixed. First, we no-
tice that the Local classifier shows substantial im-
provement over the baseline classifier. This shows
that the lexical and dialog features we use are in-
formative of opinion polarity in multi-party meet-
ings.
Next, notice that the ICA-LinkNeigh classifier
performs substantially better than the Local clas-
sifier for all metrics and all classes. The accuracy
improves by 10 percentage points, while the F-
measure improves by about 15 percentage points
for the minority (positive and negative) classes.
This result confirms that our discourse-level opin-
ion graphs are useful and discourse-level informa-
tion is non-redundant with lexical and dialog-act
Base Local ICA
LinkNeigh Partial noInfo
TLC
Acc 88.5 85.8 98.1 98.2 86.3
P-M 33.3 35.9 76.1 76.1 36.3
R-M 33.3 38.1 78.1 78.1 38.1
F1-M 33.1 36.0 74.6 74.6 36.5
FLC
Acc 89.3 86.2 98.9 98.9 87.6
P-M 33.3 36.9 81.3 82.8 38.0
R-M 33.4 41.2 82.2 84.4 41.7
F1-M 33.1 37.2 80.7 82.3 38.1
Table 4: Performance of Link Classifiers
information.
The results for ICA-LinkOnly follow the same
trend as for ICA-LinkNeigh, with a 3 to 5 percent-
age point improvement. These results show that
even when the neighbors? classes are not known
a priori, joint inference using discourse-level rela-
tions helps reduce errors from local classification.
However, the performance of the ICA-noInfo
system, which is given absolutely no starting in-
formation, is comparable to the Local classifier for
the overall accuracy and F-measure metrics for the
neutral class. There is slight improvement in pre-
cision for both the positive and negative classes,
but there is a drop in their recall. The reason this
classifier does no better than the Local classifier is
because the link classifiers TLC and FLC predict
?none? predominantly due to the heavy class skew.
The performance of the link classifiers are re-
ported in Table 4, specifically the accuracy (Acc)
and macro averages over all classes for preci-
sion (P-M), recall (R-M) and F-measure (F1-M).
Due to the heavy skew in the data, accuracy
of all classifiers is high; however, the macro F-
measure, which depends on the F1 of the minor-
ity classes, is poor for the ICA-noInfo. Note,
however, that when we provide some (Partial) or
full (LinkNeigh) neighborhood information for the
Link classifiers, the performance of these classi-
fiers improve considerably. This overall observed
trend is similar to that observed with the polarity
classifiers.
6 Related Work
Previous work on polarity disambiguation has
used contextual clues and reversal words (Wil-
son et al, 2005; Kennedy and Inkpen, 2006;
Kanayama and Nasukawa, 2006; Devitt and Ah-
mad, 2007; Sadamitsu et al, 2008). However,
these do not capture discourse-level relations.
72
Polanyi and Zaenen (2006) observe that a cen-
tral topic may be divided into subtopics in or-
der to perform evaluations. Similar to Somasun-
daran et al (2008), Asher et al (2008) advo-
cate a discourse-level analysis in order to get a
deeper understanding of contextual polarity and
the strength of opinions. However, these works do
not provide an implementation for their insights.
In this work we demonstrate a concrete way that
discourse-level interpretation can improve recog-
nition of individual opinions and their polarities.
Graph-based approaches for joint inference in
sentiment analysis have been explored previously
by many researchers. The biggest difference be-
tween this work and theirs is in what the links
represent linguistically. Some of these are not
related to discourse at all (e.g., lexical similari-
ties (Takamura et al, 2007), morphosyntactic sim-
ilarities (Popescu and Etzioni, 2005) and word
based measures like TF-IDF (Goldberg and Zhu,
2006)). Some of these work on sentence cohesion
(Pang and Lee, 2004) or agreement/disagreement
between speakers (Thomas et al, 2006; Bansal
et al, 2008). Our model is not based on sen-
tence cohesion or structural adjacency. The re-
lations due to the opinion frames are based on
relationships between targets and discourse-level
functions of opinions being mutually reinforcing
or non-reinforcing. Adjacent instances need not be
related via opinion frames, while long distant rela-
tions can be present if opinion targets are same or
alternatives. Also, previous efforts in graph-based
joint inference in opinion analysis has been text-
based, while our work is over multi-party conver-
sations.
McDonald et al (2007) propose a joint model
for sentiment classification based on relations de-
fined by granularity (sentence and document).
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations
with a local aspect (topic) model. Their aspects
would be related as same and their high contrast
relations would correspond to (a subset of) the
non-reinforcing frames.
In the field of product review mining, senti-
ments and features (aspects or targets) have been
mined (for example, Yi et al (2003), Popescu and
Etzioni (2005), and Hu and Liu (2006)). More re-
cently there has been work on creating joint mod-
els of topic and sentiments (Mei et al, 2007; Titov
and McDonald, 2008) to improve topic-sentiment
summaries. We do not model topics; instead we
directly model the relations between targets. The
focus of our work is to jointly model opinion po-
larities via target relations. The task of finding co-
referent opinion topics by (Stoyanov and Cardie,
2008) is similar to our target link classification
task, and we use somewhat similar features. Even
though their genre is different, we plan to experi-
ment with their full feature set for improving our
TLC system.
Turning to collective classification, there have
been various collective classification frameworks
proposed (for example, Neville and Jensen (2000),
Lu and Getoor (2003), Taskar et al (2004),
Richardson and Domingos (2006)). In this pa-
per, we use an approach proposed by (Bilgic et
al., 2007) which iteratively predicts class and link
existence using local classifiers. Other joint mod-
els used in sentiment classification include the spin
model (Takamura et al, 2007), relaxation labeling
(Popescu and Etzioni, 2005), and label propaga-
tion (Goldberg and Zhu, 2006).
7 Conclusion
This work uses an opinion graph framework,
DLOG, to create an interdependent classifica-
tion of polarity and discourse relations. We em-
ployed this graph to augment lexicon-based meth-
ods to improve polarity classification. We found
that polarity classification in multi-party conver-
sations benefits from opinion lexicons, unigram
and dialog-act information. We found that the
DLOGs are valuable for further improving polar-
ity classification, even with partial neighborhood
information. Our experiments showed three to
five percentage points improvement in F-measure
with link information, and 15 percentage point
improvement with full neighborhood information.
These results show that lexical and discourse in-
formation are non-redundant for polarity classi-
fication, and our DLOG, that employs both, im-
proves performance.
We discovered that link classification is a dif-
ficult problem. Here again, we found that by us-
ing the DLOG framework, and using even partial
neighborhood information, improvements can be
achieved.
References
N. Asher, F. Benamara, and Y. Mathieu. 2008. Dis-
tilling opinion in discourse: A preliminary study.
73
COLING-2008.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. In COLING-
2008.
M. Bilgic, G. M. Namata, and L. Getoor. 2007. Com-
bining collective classification and link prediction.
In Workshop on Mining Graphs and Complex Struc-
tures at the IEEE International Conference on Data
Mining.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of the Measuring Behavior Symposium
on ?Annotating and measuring Meeting Behavior?.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based
approach. In ACL 2007.
A. B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
M. Hu and B. Liu. 2006. Opinion extraction and sum-
marization on the Web. In 21st National Conference
on Artificial Intelligence (AAAI-2006).
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP-2006, pages 355?363,
Sydney, Australia.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Q. Lu and L. Getoor. 2003. Link-based classification.
In Proceedings of the International Conference on
Machine Learning (ICML).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In ACL 2007.
Q. Mei, X. Ling, M. Wondra, H. Su, and C Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In WWW ?07. ACM.
J. Neville and D. Jensen. 2000. Iterative classifica-
tion in relational data. In In Proc. AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters. Computing Attitude and Affect in Text:
Theory and Applications.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
R. Prasad, A. Lee, N. Dinesh, E. Miltsakaki, G. Cam-
pion, A. Joshi, and B. Webber. 2008. Penn dis-
course treebank version 2.0. Linguistic Data Con-
sortium.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Mach. Learn., 62(1-2):107?136.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008.
Sentiment analysis based on probabilistic models us-
ing inter-sentence information. In LREC?08.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Coling
2008.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Coling 2008.
H. Takamura, T. Inui, and M. Okumura. 2007. Extract-
ing semantic orientations of phrases from dictionary.
In HLT-NAACL 2007.
B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004.
Link prediction in relational data. In Neural Infor-
mation Processing Systems.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
ACL 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP 2005.
I. H. Witten and E. Frank. 2002. Data mining: practi-
cal machine learning tools and techniques with java
implementations. SIGMOD Rec., 31(1):76?77.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In ICDM-2003.
74
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 950?961, Dublin, Ireland, August 23-29 2014.
Lexical Chaining for Measuring Discourse Coherence Quality in
Test-taker Essays
Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
ssomasundaran@ets.org
Jill Burstein
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
jburstein@ets.org
Martin Chodorow
Hunter College, CUNY
695 Park Avenue
New York, NY 10065
martin.chodorow@hunter.cuny.edu
Abstract
This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring
discourse coherence quality in test-taker essays. We hypothesize that attributes of lexical chains,
as well as interactions between lexical chains and explicit discourse elements, can be harnessed
for representing coherence. Our experiments reveal that performance achieved by our new lexical
chain features is better than that of previous discourse features used for this task, and that the best
system performance is achieved when combining lexical chaining features with complementary
discourse features, such as those provided by a discourse parser based on rhetorical structure
theory, and features that reflect errors in grammar, word usage, and mechanics.
1 Introduction
Coherence, the reader?s ability to construct meaning from a document, is greatly influenced by the pres-
ence and organization of cohesive elements in the text (Halliday and Hasan, 1976; Moe, 1979). The
lexical chain (Morris and Hirst, 1991) is one such element. It consists of a sequence of related words that
contribute to the continuity of meaning based on word repetition, synonymy and similarity. In this paper
we explore how lexical chains can be employed to measure coherence in essays. Specifically, our goal
is to investigate how attributes of lexical chains can encode discourse coherence quality, such as adher-
ence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and
ideas. To do this, we build lexical chains and extract linguistically-motivated features from them. The
number of chains and their properties, such as length, density and link strength, can potentially reveal
discourse qualities related to focus and elaboration. In addition, features that capture the interactions
between chains and explicit discourse cues, such as transition words, can show if the cohesive elements
in text have been organized in a coherent fashion.
The main contributions of this paper are as follows: We use lexical chaining features to train a dis-
course coherence classifier on annotated essays from six different essay-writing tasks which differ in
essay genre and/or test-taker population. We then perform experiments to measure the effect of the fea-
tures when they are used alone and when they are combined with state-of-the-art features to classify the
coherence quality of essays. Our results indicate that lexical chaining features yield better results than
discourse features previously explored for this task and that the best performing feature combinations
contain lexical chaining features. We also show that lexical chaining features can improve system per-
formance across multiple genres of writing and populations. Our efforts result in the creation of a higher
performing state-of-the-art feature set for measuring coherence in test-taker writing.
The rest of the paper is organized as follows: In Section 2, we describe our intuitions about lexical
chains and how they can be used for measuring discourse coherence quality in essays. Section 3 describes
our data, and Section 4 describes our experiments in predicting discourse coherence quality. We discuss
related work in Section 5 and conclude in Section 6.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
950
2 Lexical Chains and Discourse Coherence Quality
According to Morris and Hirst (1991), lexical cohesion is the result of chains of related words that con-
tribute to the continuity of lexical meaning. These sequences are characterized by the relations between
the words, as well as by their distance and density within a given span. Lexical chains do not stop at
sentence boundaries ? they can connect a pair of adjacent words or range over an entire text. Morris
and Hirst also observe that lexical chains tend to delineate portions of text that have a strong unity of
meaning. In this paper, we use this underlying principle of cohesion to detect the quality of coherence
in a discourse. Specifically, we employ lexical chains to quantify and represent expectations for coher-
ent discourse in test-taker essays. Presumably, violations of these expectations would indicate lack of
(or poor) coherence. We believe lexical chains have the potential to reveal the following characteristics
about discourse coherence in essays:
Text unity: Textual continuity is vital for the reader?s ability to construct meaning from the text (Halli-
day and Hasan, 1976). Coherent essays generally maintain focus over the main theme, so lexical chains
constructed over such essays will have chains representing the central topic running through most of the
length of the essay. These types of chains would presumably represent the main claim or position in
persuasive texts, the main object or person in descriptive texts, and the main story-line in narrative texts.
On the other hand, incoherent texts that jump from one topic to another, or do not adhere to a central
idea, will exhibit no chains or chains with very few member words.
Elaboration and Detailing: A function of elaboration in discourse is to overcome misunderstanding or
lack of understanding, and to enrich the understanding of the reader by expressing the same thought from
a different perspective (Hobbs, 1979). Good writers usually initiate topics, ideas or claims and provide
clear elaborations or reasons. That is, a sequence of many related words and phrases will be evoked to
explain an idea or provide an account of the writer?s reasoning. This development and detailing will be
exhibited by lexical chains with a good number of member words.
Variety: While cohesiveness is vital for coherence, too much repetition of the same word can, in fact,
harm the discourse quality (Witte and Faigley, 1981). Using a variety of words to express an idea or
elaborate on a topic is generally a characteristic of skillful writing. Lexical chains corresponding to such
writing will have a variety of similar words within the same chain.
Organization: In addition to cohesion (as represented by lexical chains in our case), one other factor
must be present for text to have coherence ? organization (Moe, 1979; Perfetti and Lesgold, 1977). Thus,
it is important to organize ideas using clear discourse transitions. Transitions from one topic to another,
or from a topic to its subtopic, should be clearly cued in order to assist the reader?s understanding of
the discourse. Consequently, in coherent writing, we would expect lexical chain patterns to synchronize
with discourse cues. For example, we would expect some chains to start after a new (sub) topic initiation
cue, such as ?Secondly? or ?Finally?, and at least some chains (corresponding to the previous topic) to
end immediately before the cue. Similarly, we would expect at least some chains to cross over discourse
cues indicating elaboration or reason (e.g. ?because?) due to topic continuity.
2.1 Features for Measuring Discourse Coherence
In order to measure discourse coherence quality, we create features based on attributes of lexical chains
extracted from essays. These features are then used to train a machine learning model, using essays
manually labeled for overall discourse coherence quality.
2.1.1 Lexical Chain Construction
Lexical chains in a text are composed of words and terms that are related. Based on Hirst and St-
Onge (1995), these relations can be exact repetitions, called extra-strong relations, close synonyms,
called strong relations, or words with weaker semantic relations, called medium-strong relations. We
implement the lexical chaining program described in Hirst and St. Onge (1995), where if a word or
phrase is potentially chainable, it is considered a candidate node for existing chains. First, an extra-
strong relation is sought throughout all existing chains, and if one is found, the word is added to it. If
not, strong relations are sought, but for these, the search scope is limited to the words in any chain that is
951
no farther away than the previous six sentences in the text; the search ends as soon as a strong relation is
found. Finally, if no relationship has yet been found, medium-strong relations are sought with the search
scope limited to words in chains that are no farther away than the previous three sentences. If the node
cannot be added to any existing chains, it forms its own single-node chain.
In this work, nouns are the focus of the lexical chains. Nouns, adjective-noun and noun-noun structures
are identified as potential chain participants. Lin?s thesaurus (Lin, 1998) is used to measure similarity
between words and phrases. Candidate pairs receiving similarity scores greater than 0.8 are considered
to have an extra-strong relationship (word repetition receives a similarity score of 1), pairs with similarity
greater than 0.172 are considered to have a strong relation, and pairs with similarity scores greater than
0.099 are considered to have a medium-strong relation. These thresholds were chosen after qualitative
inspection of a separate development data set of essays, and are also based on a previous finding (Burstein
et al., 2012) that 0.172 is the mean similarity value across different parts of speech in the Lin thesaurus.
We created two feature sets to capture the intuitions described above. The first set, LEX-1, encodes
the characteristics of text unity, elaboration and variety, while the second, LEX-2, encodes organization.
2.1.2 LEX-1 feature set
In order to capture text unity and detailing, we create features such as: total number of chains in the
essay, average chain size, number (and percentage) of large chains (chains having more than four nodes
are considered to be large chains
1
). As discussed previously, essays that show ample chaining might do
so because they adhere to themes and their development, while the presence of large, dense chains might
be an indicator that a topic is being discussed in detail. To represent variety, we employ features such
as number (and percentage) of chains that have a variety of words (chains containing more than one
word/phrase type are considered to have variety), as well as number (and percentage) of large chains
with a variety of words. To encode the characteristics of cohesive relationships, we look at the nature
of the links. Examples of these features are: number and percentage of each link type, number (and
percentage) of links of each type in large chains as well as in small chains. Corresponding to each
feature that uses counts (e.g. total number of chains) we also created normalized versions of the numbers
to account for the essay length. LEX-1 has a total of 38 features.
2.1.3 LEX-2 feature set
LEX-2 features capture the interactions between discourse transitions, indicated by explicit discourse
cues, and lexical chaining patterns. For this, we use a discourse cue tagger described in Burstein et al.
(1998) that was specifically developed for tagging discourse cues in the essay genre. Using patterns and
syntactic rules, the tagger automatically identifies words and phrases used as discourse cues, and assigns
them a discourse tag. Each tag has a primary component, indicating whether an argument (or topic) is
being initialized (arg-init) or developed (arg-dev), and a secondary component indicating the specific
type of discourse initialization (e.g. CLAIM, SUMMARY), or development (e.g. CLAIM, CONTRAST).
Examples of the discourse tags and their cues are: arg-init:SUMMARY (e.g. all in all, in conclusion,
in summary, overall), arg-init:TRANSITION (e.g. let us), arg-init:PARALLEL (e.g. firstly, similarly,
finally), arg-dev:CONTRAST (e.g. nonetheless, however, on the contrary, rather than, even if ), arg-
dev:EVIDENCE (e.g. because of, since), arg-dev:INFERENCE (e.g. as a result, consequently, there-
fore), arg-dev:DETAIL (e.g. as well as, in this case, in addition, such as), arg-dev:REFORMULATION
(e.g. in other words, that is).
For each discourse cue tagged in the text, we replace the cue with its tag and measure the number of
chains that (1) start after it, (2) end before it, and (3) continue over it (chains having nodes before and
after the tag). We create such features for the tags in the original form (e.g. arg-dev:DETAIL), as well
as for the primary component alone (e.g. arg-dev) and the secondary component alone (e.g. DETAIL).
This alleviates the data sparseness that we see with certain tags, and results in a total of 138 tags for the
LEX-2 feature set.
1
This number was chosen after inspecting chains in a separate development data set.
952
3 Data
We use essays from different essay-writing tasks, representing different genres, writing proficiency and
populations. Specifically, our essays consist of the following six subsets:
1. PE-G-N: Persuasive/Expository essays written by graduate school applicants who are a mix of
native and non-native speakers. (e.g. ?As people rely more and more on technology to solve prob-
lems, the ability of humans to think for themselves will surely deteriorate. Discuss the extent to
which you agree or disagree ... ? ) [n= 145 essays]
2. AC-G-N: Argumentation critique essays written by graduate school applicants who are a mix of
native and non-native speakers. (?Examine the stated and/or unstated assumptions of the argument.
Be sure to explain how the argument depends on the assumptions and what the implications are if
the assumptions prove unwarranted ...?) [n= 138 essays]
3. PE-UG-NN: Persuasive/Expository essays written by undergraduate and graduate school appli-
cants, who are non-native speakers. [n= 146 essays]
4. CS-UG-NN: Contrastive summary essays written by undergraduate and graduate school applicants
who are non-native speakers. Here, the prompt focuses on a specific type of summarization, where
ideas from an audio lecture are to be contrasted with ideas from a written passage. [n= 147 essays]
5. S-G-N: Subject matter essays written by graduates in a professional licensure exam who are a mix
of native and non-native speakers. [n= 150 essays]
6. M-K12-N: A Mix of expository, persuasive, descriptive and narrative essays written by K-12 school
students who are a mix of native and non-native speakers. [n= 150 essays]
Of the total of 876 essays, 40 essays were used for system development, and the rest were used for
cross-validation experiments. Each essay in the data set was manually annotated for overall discourse
coherence quality by annotators not involved in this research. The discourse coherence score was as-
signed using a 4-point scale (with score point 4 for excellent discourse coherence). Twenty percent of
the essays were double annotated and the rest were annotated by one of the annotators. Inter-annotator
agreement over the doubly annotated essays, calculated using quadratic weighted kappa (QWK), was
0.61 (substantial agreement). The data distribution for each score point was: 1% for score 1, 9% for
score 2, 27% for score 3, 63% for score 4.
4 Experiments
4.1 Baseline Features
A review by Burstein et al. (2013a) describes the several systems that measure discourse coherence
quality across various text genres including test-taker essays. Features used to evaluate the discourse
coherence quality systems in this study include those previously discussed in Burstein et al. (described
below). In addition to comparing our features with previously explored features, our goal is to see if the
state-of-the-art feature set can be extended with the use of lexical chaining features.
Entity-grid transition probabilities (entity). Entity-grid transition probabilities (Barzilay and Lap-
ata, 2008) are intended to address unity, progression and coherence by tracking nouns and pronouns in
text. An entity grid is constructed in which all entities (nouns and pronouns) are represented by their
syntactic roles in a sentence (i.e., Subject, Object, Other). Entity grid transitions track how the same
word appears in a syntactic role across adjacent sentences.
Type/Token Ratios for Entities (type/token). These are modified entity-grid transition probabilities.
While the entity grid only captures, for example ?Subject-Subject? transitions, type/token ratios capture
the proportion of unique words that make such transitions. Higher ratios indicate that more concepts are
being introduced in a given syntactic role, and lower ratios indicate fewer concepts.
953
RST-derived features (RST). Rhetorical relations (Mann and Thompson, 1988) derived from an RST
parser (Marcu, 2000) are used to evaluate if and how certain rhetorical relations, combinations of rhetor-
ical relations, or rhetorical relation tree structures contribute to discourse coherence quality. These in-
clude: (a) relative frequencies of n-gram rhetorical relations in the context of the RST parse tree structure
(unigrams, or occurrences of a single relation (e.g., ThemeShift); bigrams, (e.g., ?ThemeShift -> Elab-
oration?); and trigrams, (e.g., ?ThemeShift -> Elaboration -> Circumstance?)); (b) relative proportions
of leaf-parent relation types in the tree structure; and (c) counts of root node relation types in the trees.
Maximum LSA Value for Distant Sentence Pairs (maxLSA). This feature set is the maximum
Latent Semantic Analysis (LSA) similarity score found between pairs of sentences that are separated
by at least 5 intervening sentences in the essay. It captures reintroduction of topics later in an essay,
consistent with a backward inference strategy (Van den Broek et al., 1993; Van den Broek, 2012). LSA
has also been employed to measure semantic relatedness between texts for discourse coherence (Foltz et
al., 1998).
Grammatical error features (gramErr). These features address errors in grammar that could inter-
fere with a reader?s ability to construct meaning and have been used in previous studies (e.g. (Attali and
Burstein, 2006; Burstein et al., 2013b)). Specifically, they are based on more than 30 kinds of errors in
grammar, such as subject-verb agreement errors, in word usage, such as missing article errors, and in
spelling. We use e-rater
r
, an essay scoring engine developed by Educational Testing Service (ETS), to
detect the grammar errors. Aggregate counts of these errors are used as features for predicting discourse
coherence.
Program Features (program). This is a single feature for identifying the data type listed in Section
3. Genre and population play an important role with respect to discourse coherence ? essays written
by more advanced writers, such as those at the graduate level, are typically more coherent than essays
written by populations where English is a second language, or by K-12 school students. Note that the
program feature is not linguistically motivated ? it does not capture the writing construct or a writing
skill. However, it is a strong feature as it can reliably bias the system to change its expectations about the
discourse quality based on population and task.
4.2 Principal Components Analysis
To reduce the number of lexical chain features, a Principal Components Analysis (PCA) was calculated
on an independent set of 6000 essays randomly sampled from the six task types. For 38 LEX-1 features,
a 4-component solution accounted for about 0.70 of the feature variance. An 8-component solution
explained about 0.30 of the feature variance for the 138 LEX-2 features. (While the variance was lower
for this PCA solution, the components were fairly clean.) The component scores were then computed
for the 876 essays in our annotated data set. The 4-component scores were used as LEX-1 features, and
the 8-component scores were used as LEX-2 features. PCA was used for lexical chaining features in
order to reduce the number of features used to build the models rather than using a much larger number
of correlated features. PCA was not applied to features from previous work, as we wanted to reproduce
their performance.
4.3 Results
A 10-fold cross-validation was run with an unscaled, gradient boosting regressor
2
tuned using quadratic
weighted kappa
3
. Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn
toolkit
4
(Pedregosa et al., 2011). The learner was trained to assign 4-point coherence quality scores
using different combinations of the feature sets described in sections 2.1 and 4.1. In addition to each of
the individual features in Section 4.1, we tested two baseline feature combinations: Baseline-1, a system
using all discourse-based features from Section 4.1, and Baseline-2, a system using all features described
in Section 4.1.
2
We experimented with SVMs and Random Forest learners too, but the best results were obtained with the regressor.
3
The software for the regressor can be found at https://github.com/EducationalTestingService/skll/
4
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble
954
Performance was calculated using Quadratic Weighted Kappa (QWK) (Cohen, 1968), which measures
the agreement between the system score and the human-annotated coherence score. QWK corrects for
chance agreement, and it penalizes large disagreements more than small disagreements. The formula for
QWK is as follows:
? = 1?
k
?
i=1
k
?
j=1
w
ij
o
ij
k
?
i=1
k
?
j=1
w
ij
e
ij
where k is the total number of categories (4 in our case), o
ij
is the observed value in cell i, j of the
confusion matrix between system predictions and human scores, e
ij
is the expected value for cell i, j,
and w
ij
is the weight given to the discrepancy between category
i
and category
j
. The expected value
e
ij
is calculated as:
e
ij
=
k
?
j=1
o
ij
k
?
i=1
o
ij
k
?
i=1
k
?
j=1
o
ij
For quadratic weighted kappa, w
ij
is calculated as:
w
ij
= 1?
(i? j)
2
(k ? 1)
2
where i and j are categories, and k is the total number of categories. We use QWK as it is the standard
evaluation metric used in automated essay scoring, and it also helps us to compare our results with
previous work.
Table 1 reports the results for our proposed features and for each individual feature set investigated in
previous work. Here, feature sets explicitly targeting discourse phenomena are grouped under Discourse-
based Features. The features grouped under Non-Discourse Features also capture coherence quality;
however they are based on grammatical errors or data type information. The best performing system in
each group is shown in bold. We see that the full set of lexical chaining features (LEX-1 + LEX-2) is the
best performing discourse-based feature set. It performs better than each of the other discourse-based
features used alone, and also better than Baseline-1, which uses a combination of all discourse-based
features from previous work. Notice that the performance of each discourse-based system is below the
performance of both gramErr and program, indicating that they can play an important role in predicting
text coherence.
While grammar (gramErr) and data type (program) are powerful features, it is also important to incor-
porate capabilities for detecting and evaluating discourse-specific phenomena to ensure construct rele-
vance, as the grading guidelines for essays specify the need for proper organization of ideas (e.g.?sustains
a well-focused, well-organized analysis, connecting ideas logically?). Lack of construct relevance has
been a major criticism of automated scoring methods (Deane, 2013; Shermis and Burstein, 2013). Ad-
ditionally, discourse-relevant features will allow for interpretable, useful, explicit feedback to students
regarding discourse coherence and its breakdown.
In Table 1 we also see that no individual discourse-based system outperforms Baseline-2, compris-
ing all features from the state-of-the-art (Section 4.1). In fact, the human-system agreement obtained
by Baseline-2 surpasses the human-human agreement (QWK of 0.61) reported in Section 3. This phe-
nomenon is not uncommon in essay scoring. For example, Bridgeman et al. (2012) performed detailed
analyses and found that across all test populations, human-automated system score correlations surpassed
human-human score correlations.
Because the gramErr and program features contain information that is complementary to discourse-
based features, we combined the discourse features, first with gramErr features, and then with
gramErr+program features. Table 2 reports the results from these experiments. The best performing sys-
tem for each column is in bold, and all features with QWK higher than Baseline-2 are in italics. Here,
955
Feature set QWK
Discourse-based features
LEX-1+ LEX-2 0.316
LEX-1 0.176
LEX-2 0.246
entity 0.249
type/token 0.178
RST 0.295
maxLSA 0.171
Baseline-1 0.302
Non-Discourse Features
gramErr 0.592
program 0.387
Baseline-2 0.631
Table 1: Performance of individual feature sets.
Feature set +gramErr +gramErr
+program
LEX-1+ LEX-2 0.608 0.646
LEX-1 0.611 0.650
LEX-2 0.577 0.654
entity 0.621 0.609
type/token 0.600 0.623
RST 0.612 0.649
maxLSA 0.592 0.650
gramErr+program 0.644
Table 2: Performance (QWK), of individ-
ual discourse-based features when gramErr is
added (column 2) and gramErr and program
are added (column 3)
we see that, when combined with gramErr+program, the full set of lexical chaining features (LEX-
1+LEX-2), as well as LEX-1 and LEX-2 individually, perform above Baseline-2. Surprisingly, we find
that when some individual discourse features from previous work are combined with gramErr+program,
they achieve better performance than Baseline-2 indicating that using the full combination of discourse
features may not result in the best system. In the last row, we see that the combination of gramErr
and program features alone (gramErr+program) is more competitive that Baseline-2, underscoring their
usefulness for detecting coherence quality.
Finally, we performed full ablation studies to see which feature set combination produces the best
system for identifying discourse coherence quality. Different combinations of the 8 feature sets resulted
in 255 different systems, which we ranked based on their performance. Table 3 lists some of the systems,
with their respective ranks and QWK values.
First, we observe that the best performing system contains the full set of lexical chaining features
and achieves a QWK of 0.673. In fact, all of the top-5 performing systems contain either LEX-1 or
LEX-2. The best performance produced by a system not containing any lexical chaining features ranks
eighth (gramErr+ maxLSA+ program+ RST). Notice that gramErr+program is at rank 31, Baseline-2
is at rank 61, and Baseline-1 is at rank 235. Interestingly, RST features are also seen in all of the top-5
systems, indicating that RST features and lexical chaining features capture complementary information
about discourse quality. Surprisingly, maxLSA features, which have the same underlying principle of
cohesion in text as lexical chains, are in some of the top-performing feature combinations (at ranks 4
and 5), indicating that, in addition to how ideas and themes are presented throughout the essay, the
re-introduction of topics is also important.
We tested the statistical significance of the performance differences between our best system (gramErr
+ LEX-2+ LEX-1+ maxLSA+ program+ RST, at rank 1 in Table 3) and three other systems (Baseline-1,
Baseline-2 and gramErr+program) by drawing 10,000 bootstrap samples (Berg-Kirkpatrick et al., 2012)
from our manually scored essays. For each sample, QWKs were calcuated between the human scores and
the predictions of our best system, and between the human scores and each of the other three systems?
predictions. For each sample, the differences in QWKs were recorded, and the distributions of differences
were used for significance testing. Results show that our best performing system is significantly better
than Baseline-1 (p < 0.001) and Baseline-2 (p < 0.01), and it marginally outperformed the system with
gramErr+program features (p < 0.06).
These results show that lexical chaining information is a reliable indicator of discourse quality, and
that it can be combined synergistically with other complementary features to extend the state-of-the-art
for measuring discourse coherence quality.
956
Feature set QWK Rank
gramErr + LEX-2+ LEX-1+ maxLSA+ program+ RST 0.673 1
gramErr+ LEX-1+ program+ RST 0.661 2
gramErr+ LEX-2+ program+ RST 0.661 3
gramErr+ LEX-2+ maxLSA+ program+ RST 0.660 4
gramErr+ LEX-1+ maxLSA+ program+ RST 0.659 5
gramErr+ maxLSA+ program+ RST 0.656 8
gramErr+ program 0.644 31
Baseline-2: entity+ gramErr+ RST+ maxLSA+ program+ type/token 0.631 61
Baseline-1: entity+ RST+ maxLSA+ type/token 0.302 235
Table 3: Performance (QWK), and ranks of systems using different feature combinations
4.4 Analysis by Data Type
In the previous section we saw that features based on lexical chaining are able to successfully encode and
predict the quality of discourse coherence. We now examine if this impact is uniform across all essay
genres and populations of writers. Table 4 shows the performance of gramErr +program (in column
2), the best performing features and their respective performance (Best system, columns 3 and 4), and
the best feature set when lexical chaining features are removed, with their respective performance (Best
Minus LEX-1 and LEX-2, columns 5 and 6). Here we use gramErr +program as an additional baseline,
as it was found to be more competitive than both Baseline-1 and Baseline-2.
Program gramErr Best system Best Minus LEX-1 and LEX-2
+prog Features QWK Features QWK
CS-UG-NN 0.418 gramErr+ maxLSA+ RST 0.523 gramErr+ maxLSA+ RST 0.523
PE-UG-NN 0.406 gramErr + LEX-2 + maxLSA + RST 0.468 gramErr 0.406
PE-G-N 0.614 gramErr + LEX-1 + maxLSA 0.676 gramErr + maxLSA 0.650
AC-G-N 0.744 gramErr + LEX-2 + maxLSA 0.839 gramErr + maxLSA + type/token 0.766
S-G-N 0.414 entity + gramErr+ LEX-1+ RST+
type/token
0.532 gramErr+ RST+ type/token 0.487
M-K12-N 0.635 gramErr + LEX-2 + maxLSA 0.656 gramErr + maxLSA + RST +
type/token
0.649
Table 4: Performance of feature sets by data type. Best performance is shown in bold.
In general, for all data types, addition of discourse features produces improvement over just using a
combination of gramErr and program features. Also, the addition of lexical chaining features produces
performance improvement for most data types. Specifically, there is substantial improvement in perfor-
mance for persuasive writing (PE-UG-NN and PE-G-N), expository subject writing (S-G-N) and writing
involving critical argumentation (AC-G-N). M-K12-N, which is composed of a mix of genres and writing
proficiency, shows a minor improvement. Interestingly, for contrastive summarization (CS-UG-NN), the
best system for predicting discourse coherence does not employ any lexical chaining features. For this
type of writing, the best feature set using lexical chaining features achieved a QWK of 0.465, which im-
proves over gramErr+program but is lower than the best performing feature set. This is perhaps because
the discourse phenomena targeted by our lexical chaining features (topical detailing, variety and organi-
zation) are already provided for the writer in the source document and the audio lecture, i.e., the materials
that are to be referred to in writing this type of essay. Thus, other features play a more prominent role,
such as the RST features that capture local discourse organization which is needed, for example, when
drawing a contrast between two sources of conflicting information.
957
5 Related Work
5.1 Discourse coherence quality
A number of models for measuring the quality of discourse coherence have been based on Centering
Theory (Grosz et al., 1995). For example, Barzilay and Lapata (2008) construct entity grids based
on syntactic subjects and objects. Their algorithm keeps track of the distribution of entity transitions
between adjacent sentences and computes a value for all transition types based on their proportion of
occurrence in a text. The algorithm has been evaluated with three tasks using well-formed newspaper
corpora: text ordering, summary coherence evaluation, and readability assessment. Along similar lines,
Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Milt-
sakaki and Kukich (2000) use manually marked centering information and find that higher numbers of
Rough Shifts within paragraphs are indicative of a lack of coherence. Using well-formed texts, Pitler
and Nenkova (2008) show that a text coherence detection system yields the best performance when it
includes features using the Barzilay and Lapata (2008) entity grids, syntactic features, discourse rela-
tions from the Penn Discourse Treebank (Prasad et al., 2008), and vocabulary and length features. Wang,
Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki
and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach
and the approaches based on Centering Theory is that we do not use syntactically prominent items or try
to establish a center. Instead, multiple concurrent thematic chains can ?flow? through the paragraph, and
their length, density, and interaction with discourse markers are used to model coherence.
In other related work, Lin et al. (2011) use discourse relations from Discourse Lexicalized Tree
Adjoining Grammar (D-LTAG) and compile sub-sequences of discourse role transitions to see how the
discourse role of a term varies through the progression of the text. Our work, in contrast, traces how
chains or thematic threads are organized with respect to the discourse. Our approach also differs from
models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains
can run though the length of the entire text, and hence the features derived from them are able to capture
aggregate thematic properties of the entire text such as number, distribution and elaboration of topics.
Discourse coherence models have been previously employed for the task of information-ordering in
well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)).
In our tasks, discourse coherence quality is influenced by many factors including, but not limited to,
ordering of information, such as text unity, detailing and organization.
Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality
in essays. Their approach, however, was reliant on organizational structures particular to expository and
persuasive essays, such as thesis statement and conclusion.
5.2 Lexical Chaining and Cohesion
Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003),
question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), de-
tection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000),
topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007).
In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain
features are employed to indicate the number of entities/concepts that a reader must keep in mind while
reading a document, and two of their features (number of chains in the document and average length of
chains) overlap with our LEX-1 features. Our work also differs from systems using cohesion to measure
writing quality (e.g., (Witte and Faigley, 1981; Flor et al., 2013)), in that we focus on predicting the
quality of discourse coherence.
6 Conclusion
In this paper, we investigated the use of lexical chaining for measuring discourse coherence quality.
Based on intuitions about what makes a text coherent, we extracted two sets of features from lexical
chains, one encoding how topical themes and cohesive elements are addressed in the text, and another
958
encoding how the topical themes interact with explicit discourse organizational cues. We performed
detailed experiments which showed that lexical chaining features are useful for predicting discourse
coherence quality. Specifically, when compared to other previously explored discourse-based features,
we found that our lexical chaining features are best performers when used alone. We then experimented
with various feature combinations and showed that top performing systems contain lexical chaining
features. Our analyses also indicated that lexical chaining features can improve performance on various
genres of writing by different populations of writers. Our future work on measuring discourse coherence
quality involves extending chains by using verb information and by exploring finer distinctions within
the chains themselves (e.g., topical and sub-topical chains).
References
Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v. 2.0. Journal of Technology, Learning,
and Assessment, 4:3.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
ACL workshop on intelligent scalable text summarization, volume 17, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical signif-
icance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 995?1005. Association for Computational
Linguistics.
Brent Bridgeman, Catherine Trapani, and Yigal Attali. 2012. Comparison of human and machine scoring of
essays: Differences by gender, ethnicity, and country. Applied Measurement in Education, 25(1):27?40.
Jill Burstein, Karen Kukich, Susanne Wolff, Ji Lu, and Martin Chodorow. 1998. Enriching automated essay
scoring using discourse marking. In Workshop on Discourse Relations and Discourse Marking. ERIC Clear-
inghouse.
Jill Burstein, Jane Shore, John Sabatini, Brad Moulder, Steven Holtzman, and Ted Pedersen. 2012. The language
musesm system: Linguistically focused instructional authoring. Technical report, Educational Testing Services
(ETS).
Jill Burstein, Joel Tetreault, and Martin Chodorow. 2013a. Holistic discourse coherence annotation for noisy essay
writing. Dialogue and Discourse, 4(2):34?52.
Jill Burstein, Joel Tetreault, and Nitin Madnani, 2013b. Handbook of Automated Essay Evaluation: Current
Applications and New Directions, chapter The E-rater Automated Essay Scoring System. Routledge.
Joseph Carthy and Michael Sherwood-Smith. 2002. Lexical chains for topic tracking. In 2002 IEEE International
Conference on Systems, Man and Cybernetics, volume 7. IEEE.
Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.
Psychological Bulletin, 70(4):213.
Paul Deane. 2013. On the relation between automated essay scoring and modern views of the writing construct.
Assessing Writing, 18(1):7 ? 24. Automated Assessment of Writing.
Micha Elsner and Eugene Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of the 46th
Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short
Papers, pages 41?44. Association for Computational Linguistics.
Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical chains for keyword extraction. Information Processing &
Management, 43(6):1705?1714.
Lijun Feng, No?emie Elhadad, and Matt Huenerfauth. 2009. Cognitively motivated features for readability assess-
ment. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, pages 229?237. Association for Computational Linguistics.
959
Michael Flor, Beata Beigman Klebanov, and Kathleen M. Sheehan. 2013. Lexical tightness and text complexity.
In Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility, pages
29?38, Atlanta, Georgia, June. Association for Computational Linguistics.
Peter W Foltz, Walter Kintsch, and Thomas K Landauer. 1998. The measurement of textual coherence with latent
semantic analysis. Discourse processes, 25(2-3):285?307.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995. Centering: A framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Michael AK Halliday and Ruqaiya Hasan. 1976. Cohesion in english. English Language Series. Longman Group
Ltd.
Paula Hatch, Nicola Stokes, and Joe Carthy. 2000. Topic detection, a new application for lexical chaining. In the
proceedings of BCS-IRSG, pages 94?103.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. 2004. Evaluating multiple aspects of coherence
in student essays. In HLT-NAACL, pages 185?192.
Graeme Hirst and David St-Onge. 1995. Lexical chains as representations of context for the detection and correc-
tion of malapropisms. WordNet: An electronic lexical database, 305:305?332.
Jerry R Hobbs. 1979. Coherence and coreference. Cognitive Science, 3(1):67?90.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 997?1006. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press.
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In Proceedings
of LREC 2000.
Alden J Moe. 1979. Cohesion, coherence, and the comprehension of text. Journal of Reading, 23(1):16?20.
Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of the 19th
international conference on Computational linguistics-Volume 1, pages 1?7. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the
structure of text. Computational linguistics, 17(1):21?48.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Charles A Perfetti and Alan M Lesgold. 1977. Discourse Comprehension and Sources of Individual Differences.
ERIC.
Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.
Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber.
2008. The Penn Discourse TreeBank 2.0. In LREC.
Vasile Rus and Nobal Niraula. 2012. Automated detection of local coherence in short argumentative essays based
on centering theory. In Computational Linguistics and Intelligent Text Processing, pages 450?461. Springer.
Mark D Shermis and Jill Burstein. 2013. Handbook of automated essay evaluation: Current applications and new
directions. Routledge.
960
Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceed-
ings of the COLING/ACL on Main conference poster sessions, pages 803?810. Association for Computational
Linguistics.
Nicola Stokes. 2003. Spoken and written news story segmentation using lexical chains. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: Proceedings of the HLT-NAACL 2003 student research workshop-Volume 3, pages 49?
54. Association for Computational Linguistics.
Paul Van den Broek, Charles R Fletcher, and Kirsten Risden. 1993. Investigations of inferential processes in
reading: A theoretical and methodological integration. Taylor & Francis.
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing
cognitive processes and outcomes. Measuring up: Advances in how we assess reading ability, page 39.
Y Wang, M Harrington, and P White. 2012. Detecting breakdowns in local coherence in the writing of Chinese
English learners. Journal of Computer Assisted Learning, 28(4):396?410.
Stephen P Witte and Lester Faigley. 1981. Coherence, cohesion, and writing quality. College composition and
communication, pages 189?204.
961
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247?252,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Content Importance Models for Scoring Writing From Sources
Beata Beigman Klebanov Nitin Madnani Jill Burstein Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,nmadnani,jburstein,ssomasundaran}@ets.org
Abstract
Selection of information from external
sources is an important skill assessed in
educational measurement. We address an
integrative summarization task used in an
assessment of English proficiency for non-
native speakers applying to higher educa-
tion institutions in the USA. We evaluate a
variety of content importance models that
help predict which parts of the source ma-
terial should be selected by the test-taker
in order to succeed on this task.
1 Introduction
Selection and integration of information from ex-
ternal sources is an important academic and life
skill, mentioned as a critical competency in the
Common Core State Standards for English Lan-
guage Arts/Literacy: College-ready students will
be able to ?gather relevant information from mul-
tiple print and digital sources, assess the credibi-
lity and accuracy of each source, and integrate the
information while avoiding plagiarism.?
1
Accordingly, large-scale assessments of writing
incorporate tasks that test this skill. One such test
requires test-takers to read a passage, then to lis-
ten to a lecture discussing the same topic from
a different point of view, and to summarize the
points made in the lecture, explaining how they
cast doubt on points made in the reading. The qua-
lity of the information selected from the lecture is
emphasized in excerpts from the scoring rubric for
this test (below); essays are scored on a 1-5 scale:
Score 5 successfully selects the important infor-
mation from the lecture and coherently and
accurately presents this information in rela-
tion to the relevant information presented in
the reading.
1
http://www.corestandards.org/
ELA-Literacy/CCRA/W.
Score 4 is generally good in selecting the impor-
tant information from the lecture ..., but it
may have a minor omission.
Score 3 contains some important information
from the lecture ..., but it may omit one major
key point.
Score 2 contains some relevant information from
the lecture ... The response significantly
omits or misrepresents important points.
Score 1 provides little or no meaningful or rele-
vant coherent content from the lecture.
The ultimate goal of our project is to improve
automated scoring of such essays by taking into
account the extent to which a response integrates
important information from the lecture. This pa-
per reports on the first step aimed at automatically
assigning importance scores to parts of the lecture.
The next step ? developing an essay scoring sys-
tem using content importance models along with
other features of writing quality, will be addressed
in future work. A simple essay scoring mechanism
will be used for evaluation purposes in this paper,
as described in the next section.
2 Design of Experiment
In evaluations of summarization algorithms, it is
common practice to derive the gold standard con-
tent importance scores from human summaries, as
done, for example, in the pyramid method, where
the importance of a content element corresponds
to the number of reference human summaries that
make use of it (Nenkova and Passonneau, 2004).
Selection of the appropriate content plays a cru-
cial role in attaining a high score for the essays
we consider here, as suggested by the quotes from
the scoring rubric in ?1, as well as by a corpus
study by Plakans and Gebril (2013). We therefore
observe that high-scoring essays can be thought
247
of as high-quality human summaries of the lec-
ture, albeit containing, in addition, references to
the reading material and language that contrasts
the different viewpoints, making them a somewhat
noisy gold standard. On the other hand, since low-
scoring essays contain deficient summaries of the
lecture, our setup allows for a richer evaluation
than typical in studies using gold standard human
data only, in that a good model should not only
agree with the gold standard human summaries
but should also disagree with sub-standard human
summaries. We therefore use correlation with es-
say score to evaluate content importance models.
The evaluation will proceed as follows. Every
essay E is responding to a test prompt that con-
tains a lecture L and a reading R. We identify the
essay?s overlap with the lecture:
O(E,L) = {x|x ? L, x ? E} (1)
where the exact definition of x, that is, what is
taken to be a single unit of information, will be
one of the parameters to be studied. The essay is
then assigned the following score by the content
importance model M :
S
M
(E) =
?
x?O(E,L)
w
M
(x)? C(x,E)
n
E
(2)
where w
M
(x) is the importance weight as-
signed by model M to item x in the lecture,
C(x,E) is the count of tokens in E that realize
the information unit x, and n
E
is the number of
tokens in the essay. In this paper, the distinction
between x and C is that between type and token
count of instances of that type.
2
This simple sco-
ring mechanism quantifies the rate of usage of im-
portant information per token in the essay. Finally,
we calculate the correlation of scores assigned to
essays by model M with scores assigned to the
same essays by human graders.
This design ensures that once x is fixed, all the
content importance models are evaluated within
the same scoring scheme, so any differences in the
correlations can be attributed to the differences in
the weights assigned by the importance models.
2
In the future, we intend to explore more complex rea-
lization functions, allowing paraphrase, skip n-grams (as in
ROUGE (Lin, 2004)), and other approximate matches, such
as misspellings and inflectional variants.
3 Content Importance Models
Our setting can be thought of as a special kind
of summarization task. Test-takers are required
to summarize the lecture while referencing the
reading, making this a hybrid of single- and multi-
document summarization, where one source is
treated as primary and the other as secondary.
We therefore consider models of content impor-
tance that had been found useful in the summariza-
tion literature, as well as additional models that
utilize a special feature of our scenario: We have
hundreds of essays of varying quality responding
to any given prompt, as opposed to a typical news
summarization scenario where a small number of
high quality human summaries are available for a
given article. A sample of these essays can be used
when developing a content importance model.
We define the following importance models.
For all definitions, x is a unit of information
in the lecture; C(x, t) is the number of tokens in
text t that realize x; n
L
and n
R
are the number of
tokens in the lecture and the reading, respectively.
3
Na??ve: w(x) = 1. This is a simple overlap model.
Prob: w(x) =
C(x,L)
n
L
, an MLE estimate of
the probability that x appears in the lecture.
Those x that appear more are more important.
Position: w(x) =
FP (x)
n
L
, where FP (x) is the
offset of the first occurrence of x in the lec-
ture. The offset corresponds to the token?s
serial number in the text, 1 through n
L
.
LectVsRead: w(x) =
C(x,L)
n
L
?
C(x,R)
n
R
, that is, the
difference in the probabilities of occurrence
of x in the lecture and in the reading passage
that accompanies the lecture. This model at-
tempts to capture the contrastive aspect of
importance ? the content that is unique to
the lecture is more important than the content
that is shared by the lecture and the reading.
The following two models capitalize on evi-
dence of use of information in better and worse es-
says. For estimating these models, we sample, for
each prompt, a development set of 750 essays re-
sponding to the prompt (that is, addressing a given
pair of lecture and reading stimuli). Out of these,
we take, for each prompt, all essays at score points
3
Prob, Position, and LectVsRead models normalize by
n
R
and n
L
to enable comparison of essays responding to dif-
ferent lecture + reading stimuli (prompts).
248
4 and 5 (EGood) and all essays at score points 1
and 2 (EBad). These data do not overlap with the
experimental data described in section 4. In both
definitions below, e is an essay.
Good: w(x) =
|{e?EGood|x?e}|
|EGood|
. An x is more im-
portant if more good essays use it. Hong and
Nenkova (2014) showed that a variant of this
measure used on pairs of articles and their ab-
stracts from the New York Times effectively
identified words that typically go into sum-
maries, across topics. In contrast, our mea-
surements are prompt-specific.
GoodVsBad: w(x) =
|{e?EGood|x?e}|
|EGood|
?
|{e?EBad|x?e}|
|EBad|
. An x is more important if
good essays use it more than bad essays.
To our knowledge, this measure has not
been used in the summarization literature,
probably because a large sample of human
summaries of varying quality is typically not
available.
4 Data
We use 116 prompts drawn from an assessment of
English proficiency for non-native speakers. Each
prompt contains a lecture and a reading passage.
For each prompt, we sample about 750 essays.
Each essay has an operational score provided by
a human grader. Table 1 shows the distribution of
essay scores; mean score is 3. Text transcripts of
the lectures were used.
Score 1 2 3 4 5
Proportion 0.13 0.18 0.35 0.25 0.09
Table 1: Distribution of essay scores.
5 Results
Independently from the content importance
models, we address the effect of the granularity of
the unit of information. Intuitively, since all the
materials for a given prompt deal with the same
topic, we expect large unigram overlaps between
lecture and reading, and between good and bad
essays, whereas n-grams with larger n can be
more distinctive. On the other hand, larger n lead
to misses, where an information unit would fail
to be identified in an essay due to a paraphrase,
thus impairing the ability of the scoring function
to use the content importance model effectively.
We therefore evaluate each content importance
model for different granularities of the content
unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows
the correlations with essay scores.
Content Pearson?s r
Importance
Model n=1 n=2 n=3 n=4
Na??ve 0.24 0.27* 0.24 0.20
Prob 0.04 0.14 0.17 0.14
Position 0.22 0.30* 0.26* 0.20
LectVsRead 0.09 0.25* 0.31* 0.26*
Good 0.07 0.15 0.10 0.07
GoodVsBad 0.54* 0.42* 0.32* 0.21
Table 2: Correlations with essay scores attained by
content models, for various definitions of informa-
tion unit (n-grams with n = 1, 2, 3, 4). Five top
scores are boldfaced. The baseline performance
is shown in underlined italics. Correlations that
are significantly better (p < 0.05) than the na??ve
n = 1 model are marked with an asterisk. We
use McNemar (1955, p. 148) test for significance
of difference between same-sample correlations.
N = 85, 252 for all correlations.
6 Discussion
The Na??ve model with n = 1 can be considered a
baseline, corresponding to unweighted word over-
lap between the lecture and the essay. This model
attains a significant positive correlation with essay
score (r = 0.24), suggesting that, in general, bet-
ter writers use more material from the lecture.
Our next observation is that the Prob and Good
models do not improve over the baseline, that is,
their weighting schemes generally assign higher
weights to the wrong units. We believe the rea-
son for this is that the most highly used n-grams,
in the lecture and in the essays, correspond to ge-
neral topical and functional elements. The impor-
tance of these elements is discounted in the more
effective Position, LectVsRead, and GoodVsBad
models, highlighting subtler aspects of the lecture.
Next, let us consider the granularity of the units
of information. We observe that 4-grams are in-
ferior to trigrams for all models, suggesting that
data sparsity is becoming a problem for matching
4-word sequences. For models that assign weight
based on one or two sources (lecture, or lecture
and reading) ? Na??ve, Position, LectVsRead ? un-
igram models are generally ineffective, while bi-
249
gram and trigram models significantly outperform
the baseline. We interpret this as suggesting that
it is certain particular, detailed aspects of the top-
ical concepts that constitute the important nuggets
in the lecture; these are usually realized by multi-
word sequences.
The GoodVsBad models show a different pat-
tern, obtaining the best performance with a uni-
gram version. These models are sensitive to data
sparsity not only when matching essays to the
lecture (this problem is common to all models)
but also during model building. Recall that the
weights in a GoodVsBad model are estimated
based on differential use in samples of good and
bad essays. The estimation of use-in-a-corpus is
more accurate for smaller n, because longer n-
grams are more susceptible to paraphrasing, which
leads to under-estimation of use. Assuming that
paraphrasing behavior of good and bad writers is
not the same ? in fact, there is corpus evidence
that better writers paraphrase more (Burstein et
al., 2012) ? the resulting inaccuracies might im-
pact the estimation of differential use in a sys-
tematic manner, making the n > 1 models less
effective than the unigrams. Given that (a) the
GoodVsBad bigram model is the second best over-
all in spite of the shortcomings of the estimation
process, and (b) that the bigram models worked
better than unigram models for all the other con-
tent importance models, the GoodVsBad bigram
model could probably be improved significantly
by using a more flexible information realization
mechanism.
To illustrate the information assigned high im-
portance by different models, consider a lec-
ture discussing advantages of fish farming. The
top-scoring Good bigrams are topical expressions
(fish farming), functional bigrams around fish and
farming,
4
aspects of content dealt with at length
in the lecture (wild fish, commercial fishing), bi-
grams referencing some of the claims ? fish con-
taining less fat and being used for fish meal. In
addition, this model picks out some sequences of
function words and punctuation (of the, are not,
?, and?, ?, the?) that suggest that better essays
tend to give more detail (hence have more com-
plex noun phrases and coordinated constructions)
and to draw contrast.
For the bigram GoodVsBad model, the topi-
cal bigram fish farming is not in the top 20 bi-
4
such as that fish, of fish, farming is, ?, fish?
grams. Although some bigrams are shared with
the Good model, the GoodVsBad model selects
additional details about the claims, such as the
contrast between inedible fish and edible fish that
is eaten by humans, as well as reference to chemi-
cals used in farming and to the claim that wild fish
are already endangered by other practices.
The most important bigrams according to the
LectVsRead model include functional bigrams
around fish and farming, functional sequences
(that the, is a), as well as commercial fishing and
edible fish. Also selected are functional bigrams
around consumption and species, hinting, indi-
rectly, at the edibility differences between species.
Finally, this model selects almost all bigrams in
the reading passage makes, the reading makes
claims that and the reading says. While distin-
guishing the lecture from the reading, these do not
capture topic-relevant content of the lecture.
The GoodVsBad unigram model selects poul-
try, endangered, edible, chemicals among its top 6
unigrams,
5
effectively touching upon the connec-
tion with other farm-raised foods (poultry, chemi-
cals), with wild fish (endangered) and with human
benefit (edible) that are made in the lecture.
7 Related work
Modern essay scoring systems are complex and
cover various aspects of the writing construct,
such as grammar, organization, vocabulary (Sher-
mis and Burstein, 2013). The quality of content
is often addressed by features that quantify the
similarity between the vocabulary used in an es-
say and reference essays from given score points
(Attali and Burstein, 2006; Foltz et al, 2013; At-
tali, 2011). For example, Attali (2011) proposed a
measure of differential use of words in higher and
lower scoring essays defined similarly to Good-
VsBad, without, however, considering the source
text at all. Such features can be thought of as con-
tent quality features, as they implicitly assume that
writers of better essays use better content. How-
ever, there are various kinds of better content, only
one of them being selection of important informa-
tion from the source; other elements of content
originate with the writer, such as examples, dis-
course markers, evaluations, introduction and con-
clusion, etc. Our approach allows focusing on a
particular aspect of content quality, namely, selec-
tion of appropriate materials from the source.
5
the other two being fishing and used.
250
Our results are related to the findings of Gure-
vich and Deane (2007) who studied the difference
between the reading and the lecture in their im-
pact on essay scores for this test. Using data from
a single prompt, they showed that the difference
between the essay?s average cosine similarity to
the reading and its average cosine similarity to the
lecture is predictive of the score for non-native
speakers of English, thus using a model similar
to LectVsRead, although they took all lecture,
reading, and essay words into account, in contrast
to our model that looks only at n-grams that ap-
pear in the lecture. Our study shows that the ef-
fectiveness of lecture-reading contrast models for
essay scoring generalizes to a large set of prompts.
Similarly, Evanini et al (2013) found that over-
lap with material that is unique to the lecture (not
shared with the reading) was predictive of scores
in a spoken source-based question answering task.
In the vast literature on summarization, our
work is closest to Hong and Nenkova (2014) who
studied models of word importance for multi-
document summarization of news. The Prob, Po-
sition, and Good models are inspired by their
findings of the effectiveness of similar models in
their setting. We found that, in our setting, Prob
and Good models performed worse than assigning
a uniform weight to all words. We note, however,
that models from Hong and Nenkova (2014) are
not strictly comparable, since their word proba-
bility models were calculated after stopword ex-
clusion, and their model that inspired our Good
model was defined somewhat differently and val-
idated using content words only. The defini-
tion of our Position model and its use in the es-
say scoring function S (equation 2) correspond to
Hong and Nenkova (2014) average first location
model for scoring summaries. Differently from
their findings, this model is not effective for sin-
gle words in our setting. Position models over n-
grams with n > 1 are effective, but their predic-
tion is in the opposite direction of that found for
the news data ? the more important materials tend
to appear later in the lecture, as indicated by the
positive r between average first position and essay
score. These findings underscore the importance
of paying attention to the genre of the source ma-
terial when developing summarization systems.
Our summarization task incorporates elements
of contrastive opinion summarization (Paul et al,
2010; Kim and Zhai, 2009), since the lecture and
the reading sometimes interpret the same facts in
a positive or negative light (for example, the fact
that chemicals are used in fish farms is negative
if compared to wild fish, but not so if compared
to other farm-raised foods like poultry). Relation-
ships between aspect and sentiment (Brody and
Elhadad, 2010; Lazaridou et al, 2013) are also
relevant, since aspects of the same fact are em-
phasized with different evaluations (the quantity
vs the variety of species that go into fish meal for
farmed fish). We hypothesize that units participat-
ing in sentiment and aspect contrasts are of higher
importance; this is a direction for future work.
8 Conclusion
In this paper, we addressed the task of automati-
cally assigning importance scores to parts of a lec-
ture that is to be summarized as part of an English
language proficiency test. We investigated the op-
timal units of information to which importance
should be assigned, as well as a variety of impor-
tance scoring models, drawing on the news sum-
marization and essay scoring literature.
We found that bigrams and trigrams were ge-
nerally more effective than unigrams and 4-grams
across importance models, with some exceptions.
We also found that the most effective impor-
tance models are those that equate importance
of an n-gram with its preferential use in higher-
scoring essays than in lower-scoring ones, above
and beyond merely looking at the n-grams used in
good essays. This demonstrates the utility of using
not only gold, high-quality human summaries, but
also sub-standard ones when developing content
importance models.
Additional importance criteria that are intrinsic
to the lecture, as well as those that capture contrast
with a different source discussing the same topic,
were also found to be reasonably effective. Since
different importance models often select different
items as most important, we intend to investigate
complementarity of the different models.
Finally, our results highlight that the effective-
ness of an importance model depends on the genre
of the source text. Thus, while a first sentence
baseline is very competitive in news summariza-
tion, we found that important information tends
not to be located in the opening sentences in our
data (these tend to provide general, introductory
information), but appears later on, when more de-
tailed, specific claims are put forward.
251
References
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater
R
?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali. 2011. A Differential Word Use Measure
for Content Analysis in Automated Essay Scoring.
ETS Research Report, RR-11-36.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 804?812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jill Burstein, Michael Flor, Joel Tetreault, Nitin Mad-
nani, and Steven Holtzman. 2012. Examining Lin-
guistic Characteristics of Paraphrase in Test-Taker
Summaries. ETS Research Report, RR-12-18.
Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based content scoring for automated spoken
language assessment. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 157?162, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Peter Foltz, Lynn Streeter, Karen Lochbaum, and
Thomas Landauer. 2013. Implementation and Ap-
plication of the Intelligent Essay Assessor. In Mark
Shermis and Jill Burstein, editors, Handbook of au-
tomated essay evaluation: Current applications and
new directions, pages 68?88. New York: Routh-
ledge.
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
49?52, Rochester, New York, April. Association for
Computational Linguistics.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In The Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Gottenberg, Sweden, April. As-
sociation for Computational Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of the 18th ACM Confer-
ence on Information and Knowledge Management,
CIKM ?09, pages 385?394, New York, NY, USA.
ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
ACL workshop: Text summarization branches out,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Human Language Technologies
2004: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 145?152, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguis-
tics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lia Plakans and Atta Gebril. 2013. Using multiple
texts in an integrated writing assessment: Source
text use as a predictor of score. Journal of Second
Language Writing, 22:217?230.
Mark Shermis and Jill Burstein, editors. 2013. Hand-
book of Automated Essay Evaluation: Current Ap-
plications and Future Directions. New York: Rout-
ledge.
252
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116?124,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Recognizing Stances in Ideological On-Line Debates
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science and
The Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work explores the utility of sentiment and
arguing opinions for classifying stances in ide-
ological debates. In order to capture arguing
opinions in ideological stance taking, we con-
struct an arguing lexicon automatically from
a manually annotated corpus. We build su-
pervised systems employing sentiment and ar-
guing opinions and their targets as features.
Our systems perform substantially better than
a distribution-based baseline. Additionally,
by employing both types of opinion features,
we are able to perform better than a unigram-
based system.
1 Introduction
In this work, we explore if and how ideologi-
cal stances can be recognized using opinion analy-
sis. Following (Somasundaran and Wiebe, 2009),
stance, as used in this work, refers to an overall po-
sition held by a person toward an object, idea or
proposition. For example, in a debate ?Do you be-
lieve in the existence of God?,? a person may take a
for-existence of God stance or an against existence
of God stance. Similarly, being pro-choice, believ-
ing in creationism, and supporting universal health-
care are all examples of ideological stances.
Online web forums discussing ideological and po-
litical hot-topics are popular.1 In this work, we are
1http://www.opposingviews.com,
http://wiki.idebate.org, http://www.createdebate.com and
http://www.forandagainst.com are examples of such debating
websites.
interested in dual-sided debates (there are two pos-
sible polarizing sides that the participants can take).
For example, in a healthcare debate, participants can
take a for-healthcare stance or an against-healthcare
stance. Participants generally pick a side (the web-
sites provide a way for users to tag their stance)
and post an argument/justification supporting their
stance.
Personal opinions are clearly important in ideo-
logical stance taking, and debate posts provide out-
lets for expressing them. For instance, let us con-
sider the following snippet from a universal health-
care debate. Here the writer is expressing a nega-
tive sentiment2 regarding the government (the opin-
ion spans are highlighted in bold and their targets,
what the opinions are about, are highlighted in ital-
ics).
(1) Government is a disease pretending to be its
own cure. [side: against healthcare]
The writer?s negative sentiment is directed toward
the government, the initiator of universal healthcare.
This negative opinion reveals his against-healthcare
stance.
We observed that arguing, a less well explored
type of subjectivity, is prominently manifested in
ideological debates. As used in this work, arguing is
a type of linguistic subjectivity, where a person is ar-
guing for or against something or expressing a belief
about what is true, should be true or should be done
2As used in this work, sentiment is a type of linguistic sub-
jectivity, specifically positive and negative expressions of emo-
tions, judgments, and evaluations (Wilson and Wiebe, 2005;
Wilson, 2007; Somasundaran et al, 2008).
116
in his or her view of the world (Wilson and Wiebe,
2005; Wilson, 2007; Somasundaran et al, 2008).
For instance, let us consider the following snippet
from a post supporting an against-existence of God
stance.
(2) Obviously that hasn?t happened, and to be
completely objective (as all scientists should
be) we must lean on the side of greatest evi-
dence which at the present time is for evolu-
tion. [side: against the existence of God]
In supporting their side, people not only express
their sentiments, but they also argue about what is
true (e.g., this is prominent in the existence of God
debate) and about what should or should not be done
(e.g., this is prominent in the healthcare debate).
In this work, we investigate whether sentiment
and arguing expressions of opinion are useful for
ideological stance classification. For this, we ex-
plore ways to capture relevant opinion information
as machine learning features into a supervised stance
classifier. While there is a large body of resources
for sentiment analysis (e.g., the sentiment lexicon
from (Wilson et al, 2005)), arguing analysis does
not seem to have a well established lexical resource.
In order to remedy this, using a simple automatic ap-
proach and a manually annotated corpus,3 we con-
struct an arguing lexicon. We create features called
opinion-target pairs, which encode not just the opin-
ion information, but also what the opinion is about,
its target. Systems employing sentiment-based and
arguing-based features alone, or both in combina-
tion, are analyzed. We also take a qualitative look
at features used by the learners to get insights about
the information captured by them.
We perform experiments on four different ideo-
logical domains. Our results show that systems us-
ing both sentiment and arguing features can perform
substantially better than a distribution-based base-
line and marginally better than a unigram-based sys-
tem. Our qualitative analysis suggests that opinion
features capture more insightful information than
using words alone.
The rest of this paper is organized as follows: We
first describe our ideological debate data in Section
2. We explain the construction of our arguing lexi-
con in Section 3 and our different systems in Section
3MPQA corpus available at http://www.cs.pitt.edu/mpqa.
4. Experiments, results and analyses are presented in
Section 5. Related work is in Section 6 and conclu-
sions are in Section 7.
2 Ideological Debates
Political and ideological debates on hot issues are
popular on the web. In this work, we analyze the fol-
lowing domains: Existence of God, Healthcare, Gun
Rights, Gay Rights, Abortion and Creationism. Of
these, we use the first two for development and the
remaining four for experiments and analyses. Each
domain is a political/ideological issue and has two
polarizing stances: for and against.
Table 2 lists the domains, examples of debate top-
ics within each domain, the specific sides for each
debate topic, and the domain-level stances that cor-
respond to these sides. For example, consider the
Existence of God domain in Table 2. The two
stances in this domain are for-existence of God and
against-existence of God. ?Do you believe in God?,
a specific debate topic within this domain, has two
sides: ?Yes!!? and ?No!!?. The former corresponds
to the for-existence of God stance and the latter maps
to the against-existence of God stance. The situa-
tion is different for the debate ?God Does Not Ex-
ist?. Here, side ?against? corresponds to the for-
existence of God stance, and side ?for? corresponds
to the against-existence of God stance.
In general, we see in Table 2 that, while specific
debate topics may vary, in each case the two sides
for the topic correspond to the domain-level stances.
We download several debates for each domain and
manually map debate-level stances to the stances
for the domain. Table 2 also reports the number
of debates, and the total number of posts for each
domain. For instance, we collect 16 different de-
bates in the healthcare domain which gives us a total
of 336 posts. All debate posts have user-reported
debate-level stance tags.
2.1 Observations
Preliminary inspection of development data gave us
insights which shaped our approach. We discuss
some of our observations in this section.
Arguing Opinion
We found that arguing opinions are prominent
when people defend their ideological stances. We
117
Domain/Topics stance1 stance2
Healthcare (16 debates, 336 posts) for against
Should the US have universal health-
care
Yes No
Debate: Public insurance option in
US health care
Pro Con
Existence of God (7 debates, 486
posts)
for against
Do you believe in God Yes!! No!!
God Does Not Exist against for
Gun Rights (18 debates, 566 posts) for against
Should Guns Be Illegal against for
Debate: Right to bear arms in the US Yes No
Gay Rights (15 debates, 1186 posts) for against
Are people born gay Yes No
Is homosexuality a sin No Yes
Abortion (13 debates, 618 posts) for against
Should abortion be legal Yes No
Should south Dakota pass the abor-
tion ban
No Yes
Creationism (15 debates, 729 posts) for against
Evolution Is A False Idea for against
Has evolution been scientifically
proved
It has
not
It has
Table 1: Examples of debate topics and their stances
saw an instance of this in Example 2, where the par-
ticipant argues against the existence of God. He ar-
gues for what (he believes) is right (should be), and
is imperative (we must). He employs ?Obviously?
to draw emphasis and then uses a superlative con-
struct (greatest) to argue for evolution.
Example 3 below illustrates arguing in a health-
care debate. The spans most certainly believe and
has or must do reveal arguing (ESSENTIAL, IM-
PORTANT are sentiments).
(3) ... I most certainly believe that there are
some ESSENTIAL, IMPORTANT things
that the government has or must do [side: for
healthcare]
Observe that the text spans revealing arguing can
be a single word or multiple words. This is differ-
ent from sentiment expressions that are more often
single words.
Opinion Targets
As mentioned previously, a target is what an
opinion is about. Targets are vital for determining
stances. Opinions by themselves may not be as in-
formative as the combination of opinions and tar-
gets. For instance, in Example 1 the writer supports
an against-healthcare stance using a negative senti-
ment. There is a negative sentiment in the example
below (Example 4) too. However, in this case the
writer supports a for-healthcare stance. It is by un-
derstanding what the opinion is about, that we can
recognize the stance.
(4) Oh, the answer is GREEDY insurance com-
panies that buy your Rep & Senator. [side: for
healthcare]
We also observed that targets, or in general items
that participants from either side choose to speak
about, by themselves may not be as informative as
opinions in conjunction with the targets. For in-
stance, Examples 1 and 3 both speak about the gov-
ernment but belong to opposing sides. Understand-
ing that the former example is negative toward the
government and the latter has a positive arguing
about the government helps us to understand the cor-
responding stances.
Examples 1, 3 and 4 also illustrate that there
are a variety of ways in which people support
their stances. The writers express opinions about
government, the initiator of healthcare and insur-
ance companies, and the parties hurt by government
run healthcare. Participants group government and
healthcare as essentially the same concept, while
they consider healthcare and insurance companies
as alternative concepts. By expressing opinions re-
garding a variety of items that are same or alternative
to main topic (healthcare, in these examples), they
are, in effect, revealing their stance (Somasundaran
et al, 2008).
3 Constructing an Arguing Lexicon
Arguing is a relatively less explored category in sub-
jectivity. Due to this, there are no available lexicons
with arguing terms (clues). However, the MPQA
corpus (Version 2) is annotated with arguing sub-
jectivity (Wilson and Wiebe, 2005; Wilson, 2007).
There are two arguing categories: positive arguing
and negative arguing. We use this corpus to gener-
ate a ngram (up to trigram) arguing lexicon.
The examples below illustrate MPQA arguing an-
notations. Examples 5 and 7 illustrate positive argu-
118
ing annotations and Example 6 illustrates negative
arguing.
(5) Iran insists its nuclear program is purely
for peaceful purposes.
(6) Officials in Panama denied that Mr. Chavez
or any of his family members had asked for
asylum.
(7) Putin remarked that the events in Chechnia
?could be interpreted only in the context
of the struggle against international terror-
ism.?
Inspection of these text spans reveal that arguing an-
notations can be considered to be comprised of two
pieces of information. The first piece of information
is what we call the arguing trigger expression. The
trigger is an indicator that an arguing is taking place,
and is the primary component that anchors the argu-
ing annotation. The second component is the ex-
pression that reveals more about the argument, and
can be considered to be secondary for the purposes
of detecting arguing. In Example 5, ?insists?, by it-
self, conveys enough information to indicate that the
speaker is arguing. It is quite likely that a sentence
of the form ?X insists Y? is going to be an arguing
sentence. Thus, ?insists? is an arguing trigger.
Similarly, in Example 6, we see two arguing trig-
gers: ?denied? and ?denied that?. Each of these can
independently act as arguing triggers (For example,
in the constructs ?X denied that Y? and ?X denied
Y?). Finally, in Example 7, the arguing annotation
has the following independent trigger expressions
?could be * only?, ?could be? and ?could?. The wild
card in the first trigger expression indicates that there
could be zero or more words in its place.
Note that MPQA annotations do not provide this
primary/secondary distinction. We make this dis-
tinction to create general arguing clues such as ?in-
sist?. Table 3 lists examples of arguing annotations
from the MPQA corpus and what we consider as
their arguing trigger expressions.
Notice that trigger words are generally at the be-
ginning of the annotations. Most of these are uni-
grams, bigrams or trigrams (though it is possible for
these to be longer, as seen in Example 7). Thus, we
can create a lexicon of arguing trigger expressions
Positive arguing annotations Trigger Expr.
actually reflects Israel?s determination ... actually
am convinced that improving ... am convinced
bear witness that Mohamed is his ... bear witness
can only rise to meet it by making ... can only
has always seen usama bin ladin?s ... has always
Negative Arguing Annotations Trigger Expr.
certainly not a foregone conclusion certainly not
has never been any clearer has never
not too cool for kids not too
rather than issuing a letter of ... rather than
there is no explanation for there is no
Table 2: Arguing annotations from the MPQA corpus and
their corresponding trigger expressions
by extracting the starting n-grams from the MPQA
annotations. The process of creating the lexicon is
as follows:
1. Generate a candidate Set from the annotations
in the corpus. Three candidates are extracted
from the stemmed version of each annotation:
the first word, the bigram starting at the first
word, and the trigram starting at the first word.
For example, if the annotation is ?can only rise
to meet it by making some radical changes?,
the following candidates are extracted from it:
?can?, ?can only? and ?can only rise?.
2. Remove the candidates that are present in the
sentiment lexicon from (Wilson et al, 2005) (as
these are already accounted for in previous re-
search). For example, ?actually?, which is a
trigger word in Table 3, is a neutral subjectivity
clue in the lexicon.
3. For each candidate in the candidate Set,
find the likelihood that it is a reliable indi-
cator of positive or negative arguing in the
MPQA corpus. These are likelihoods of the
form: P (positive arguing|candidate) =
#candidate is in a positive arguing span
#candidate is in the corpus
and P (negative arguing|candidate) =
#candidate is in a negative arguing span
#candidate is in the corpus
4. Make a lexicon entry for each candidate con-
sisting of the stemmed text and the two proba-
bilities described above.
This process results in an arguing lexicon
with 3762 entries, where 3094 entries have
119
P (positive arguing|candidate) > 0; and 668
entries have P (negative arguing|candidate) > 0.
Table 3 lists select interesting expressions from the
arguing lexicon.
Entries indicative of Positive Arguing
be important to, would be better, would need to, be just the, be
the true, my opinion, the contrast, show the, prove to be, only
if, on the verge, ought to, be most, youve get to, render, man-
ifestation, ironically, once and for, no surprise, overwhelming
evidence, its clear, its clear that, it be evident, it be extremely,
it be quite, it would therefore
Entries indicative of Negative Arguing
be not simply, simply a, but have not, can not imagine, we dont
need, we can not do, threat against, ought not, nor will, never
again, far from be, would never, not completely, nothing will,
inaccurate and, inaccurate and, find no, no time, deny that
Table 3: Examples of positive argu-
ing (P (positive arguing|candidate) >
P (negative arguing|candidate)) and negative
arguing (P (negative arguing|candidate) >
P (positive arguing|candidate))from the arguing
lexicon
4 Features for Stance Classification
We construct opinion target pair features, which are
units that capture the combined information about
opinions and targets. These are encoded as binary
features into a standard machine learning algorithm.
4.1 Arguing-based Features
We create arguing features primarily from our ar-
guing lexicon. We construct additional arguing fea-
tures using modal verbs and syntactic rules. The lat-
ter are motivated by the fact that modal verbs such
as ?must?, ?should? and ?ought? are clear cases of
arguing, and are often involved in simple syntactic
patterns with clear targets.
4.1.1 Arguing-lexicon Features
The process for creating features for a post using
the arguing lexicon is simple. For each sentence in
the post, we first determine if it contains a positive or
negative arguing expression by looking for trigram,
bigram and unigram matches (in that order) with the
arguing lexicon. We prevent the same text span from
matching twice ? once a trigram match is found, a
substring bigram (or unigram) match with the same
text span is avoided. If there are multiple arguing ex-
pression matches found within a sentence, we deter-
mine the most prominent arguing polarity by adding
up the positive arguing probabilities and negative ar-
guing probabilities (provided in the lexicon) of all
the individual expressions.
Once the prominent arguing polarity is deter-
mined for a sentence, the prefix ap (arguing positive)
or an (arguing negative) is attached to all the content
words in that sentence to construct opinion-target
features. In essence, all content words (nouns, verbs,
adjectives and adverbs) in the sentence are assumed
to be the target. Arguing features are denoted as ap-
target (positive arguing toward target) and an-target
(negative arguing toward target).
4.1.2 Modal Verb Features for Arguing
Modals words such as ?must? and ?should? are
usually good indicators of arguing. This is a small
closed set. Also, the target (what the arguing is
about) is syntactically associated with the modal
word, which means it can be relatively accurately
extracted by using a small set of syntactic rules.
For every modal detected, three features are cre-
ated by combining the modal word with its subject
and object. Note that all the different modals are
replaced by ?should? while creating features. This
helps to create more general features. For exam-
ple, given a sentence ?They must be available to
all people?, the method creates three features ?they
should?, ?should available? and ?they should avail-
able?. These patterns are created independently of
the arguing lexicon matches, and added to the fea-
ture set for the post.
4.2 Sentiment-based Features
Sentiment-based features are created independent of
arguing features. In order to detect sentiment opin-
ions, we use a sentiment lexicon (Wilson et al,
2005). In addition to positive (+) and negative (?)
words, this lexicon also contains subjective words
that are themselves neutral (=) with respect to po-
larity. Examples of neutral entries are ?absolutely?,
?amplify?, ?believe?, and ?think?.
We find the sentiment polarity of the entire sen-
tence and assign this polarity to each content word in
the sentence (denoted, for example, as target+). In
order to detect the sentence polarity, we use the Vote
120
and Flip algorithm from Choi and Cardie (2009).
This algorithm essentially counts the number of pos-
itive, negative and neutral lexicon hits in a given ex-
pression and accounts for negator words. The algo-
rithm is used as is, except for the default polarity
assignment (as we do not know the most prominent
polarity in the corpus). Note that the Vote and Flip
algorithm has been developed for expressions but we
employ it on sentences. Once the polarity of a sen-
tence is determined, we create sentiment features for
the sentence. This is done for all sentences in the
post.
5 Experiments
Experiments are carried out on debate posts from the
following four domains: Gun Rights, Gay Rights,
Abortion, and Creationism. For each domain, a cor-
pus with equal class distribution is created as fol-
lows: we merge all debates and sample instances
(posts) from the majority class to obtain equal num-
bers of instances for each stance. This gives us a
total of 2232 posts in the corpus: 306 posts for the
Gun Rights domain, 846 posts for the Gay Rights
domain, 550 posts for the Abortion domain and 530
posts for the Creationism domain.
Our first baseline is a distribution-based baseline,
which has an accuracy of 50%. We also construct
Unigram, a system based on unigram content infor-
mation, but no explicit opinion information. Un-
igrams are reliable for stance classification in po-
litical domains (as seen in (Lin et al, 2006; Kim
and Hovy, 2007)). Intuitively, evoking a particular
topic can be indicative of a stance. For example,
a participant who chooses to speak about ?child?
and ?life? in an abortion debate is more likely from
an against-abortion side, while someone speaking
about ?woman?, ?rape? and ?choice? is more likely
from a for-abortion stance.
We construct three systems that use opinion in-
formation: The Sentiment system that uses only the
sentiment features described in Section 4.2, the Ar-
guing system that uses only arguing features con-
structed in Section 4.1, and the Arg+Sent system
that uses both sentiment and arguing features.
All systems are implemented using a standard im-
plementation of SVM in the Weka toolkit (Hall et
al., 2009). We measure performance using the accu-
racy metric.
5.1 Results
Table 4 shows the accuracy averaged over 10 fold
cross-validation experiments for each domain. The
first row (Overall) reports the accuracy calculated
over all 2232 posts in the data.
Overall, we notice that all the supervised systems
perform better than the distribution-based baseline.
Observe that Unigram has a better performance than
Sentiment. The good performance of Unigram indi-
cates that what participants choose to speak about is
a good indicator of ideological stance taking. This
result confirms previous researchers? intuition that,
in general, political orientation is a function of ?au-
thors? attitudes over multiple issues rather than pos-
itive or negative sentiment with respect to a sin-
gle issue? (Pang and Lee, 2008). Nevertheless, the
Arg+Sent system that uses both arguing and senti-
ment features outperforms Unigram.
We performed McNemar?s test to measure the dif-
ference in system behaviors. The test was performed
on all pairs of supervised systems using all 2232
posts. The results show that there is a significant dif-
ference between the classification behavior of Uni-
gram and Arg+Sent systems (p < 0.05). The dif-
ference between classifications of Unigram and Ar-
guing approaches significance (p < 0.1). There is
no significant difference in the behaviors of all other
system pairs.
Moving on to detailed performance in each do-
main, we see that Unigram outperforms Sentiment
for all domains. Arguing and Arg+Sent outperform
Unigram for three domains (Guns, Gay Rights and
Abortion), while the situation is reversed for one do-
main (Creationism). We carried out separate t-tests
for each domain, using the results from each test fold
as a data point. Our results indicate that the perfor-
mance of Sentiment is significantly different from
all other systems for all domains. However there is
no significant difference between the performance of
the remaining systems.
5.2 Analysis
On manual inspection of the top features used by
the classifiers for discriminating the stances, we
found that there is an overlap between the content
words used by Unigram, Arg+Sent and Arguing. For
121
Domain (#posts) Distribution Unigram Sentiment Arguing Arg+Sent
Overall (2232) 50 62.50 55.02 62.59 63.93
Guns Rights (306) 50 66.67 58.82 69.28 70.59
Gay Rights (846) 50 61.70 52.84 62.05 63.71
Abortion (550) 50 59.1 54.73 59.46 60.55
Creationism (530) 50 64.91 56.60 62.83 63.96
Table 4: Accuracy of the different systems
example, in the Gay Rights domain, ?understand?
and ?equal? are amongst the top features in Uni-
gram, while ?ap-understand? (positive arguing for
?understand?) and ?ap-equal? are top features for
Arg+Sent.
However, we believe that Arg+Sent makes finer
and more insightful distinctions based on polarity of
opinions toward the same set of words. Table 5 lists
some interesting features in the Gay Rights domain
for Unigram and Arg+Sent. Depending on whether
positive or negative attribute weights were assigned
by the SVM learner, the features are either indicative
of for-gay rights or against-gay rights. Even though
the features for Unigram are intuitive, it is not ev-
ident if a word is evoked as, for example, a pitch,
concern, or denial. Also, we do not see a clear sep-
aration of the terms (for e.g., ?bible? is an indicator
for against-gay rights while ?christianity? is an indi-
cator for for-gay rights)
The arguing features from Arg+Sent seem to
be relatively more informative ? positive arguing
about ?christianity?, ?corinthians?, ?mormonism?
and ?bible? are all indicative of against-gay rights
stance. These are indeed beliefs and concerns that
shape an against-gay rights stance. On the other
hand, negative arguings with these same words de-
note a for-gay rights stance. Presumably, these oc-
cur in refutations of the concerns influencing the op-
posite side. Likewise, the appeal for equal rights
for gays is captured positive arguing about ?liberty?,
?independence?, ?pursuit? and ?suffrage?.
Interestingly, we found that our features also cap-
ture the ideas of opinion variety and same and alter-
native targets as defined in previous research (So-
masundaran et al, 2008) ? in Table 5, items that
are similar (e.g., ?christianity? and ?corinthians?)
have similar opinions toward them for a given stance
(for e.g., ap-christianity and ap-corinthians belong
to against-gay rights stance while an-christianity and
an-corinthians belong to for-gay rights stance). Ad-
ditionally, items that are alternatives (e.g. ?gay? and
?heterosexuality?) have opposite polarities associ-
ated with them for a given stance, that is, positive
arguing for ?heterosexuality? and negative arguing
for ?gay? reveal the the same stance.
In general, unigram features associate the choice
of topics with the stances, while the arguing features
can capture the concerns, defenses, appeals or de-
nials that signify each side (though we do not ex-
plicitly encode these fine-grained distinctions in this
work). Interestingly, we found that sentiment fea-
tures in Arg+Sent are not as informative as the argu-
ing features discussed above.
6 Related Work
Generally, research in identifying political view-
points has employed information from words in the
document (Malouf and Mullen, 2008; Mullen and
Malouf, 2006; Grefenstette et al, 2004; Laver et al,
2003; Martin and Vanberg, 2008; Lin et al, 2006;
Lin, 2006). Specifically, Lin et al observe that peo-
ple from opposing perspectives seem to use words
in differing frequencies. On similar lines, Kim and
Hovy (2007) use unigrams, bigrams and trigrams for
election prediction from forum posts. In contrast,
our work specifically employs sentiment-based and
arguing-based features to perform stance classifica-
tion in political debates. Our experiments are fo-
cused on determining how different opinion expres-
sions reinforce an overall political stance. Our re-
sults indicate that while unigram information is re-
liable, further improvements can be achieved in cer-
tain domains using our opinion-based approach. Our
work is also complementary to that by Greene and
Resnik (2009), which focuses on syntactic packag-
ing for recognizing perspectives.
122
For Gay Rights Against Gay Rights
Unigram Features
constitution, fundamental, rights, suffrage, pursuit, discrimina-
tion, government, happiness, shame, wed, gay, heterosexual-
ity, chromosome, evolution, genetic, christianity, mormonism,
corinthians, procreate, adopt
pervert, hormone, liberty, fidelity, naval, retarded, orientation, pri-
vate, partner, kingdom, bible, sin, bigot
Arguing Features from Arg+Sent
ap-constitution, ap-fundamental, ap-rights, ap-hormone,
ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap-
discrimination, an-government, ap-fidelity, ap-happiness,
an-pervert, an-naval, an-retarded, an-orientation, an-shame,
ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,
ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an-
christianity, an-mormonism, an-corinthians, an-bible, an-sin,
an-bigot, an-procreate, ap-adopt,
an-constitution, an-fundamental, an-rights, an-hormone,
an-liberty, an-independence, an-suffrage, an-pursuit, an-
discrimination, ap-government, an-fidelity, an-happiness,
ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame,
an-private, an-wed, an-gay, ap-heterosexuality, an-partner,
an-chromosome, an-evolution, an-genetic, ap-kingdom, ap-
christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin,
ap-bigot, ap-procreate, an-adopt
Table 5: Examples of features associated with the stances in Gay Rights domain
Discourse-level participant relation, that is,
whether participants agree/disagree has been found
useful for determining political side-taking (Thomas
et al, 2006; Bansal et al, 2008; Agrawal et
al., 2003; Malouf and Mullen, 2008). Agree-
ment/disagreement relations are not the main focus
of our work. Other work in the area of polarizing po-
litical discourse analyze co-citations (Efron, 2004)
and linking patterns (Adamic and Glance, 2005). In
contrast, our focus is on document content and opin-
ion expressions.
Somasundaran et al (2007b) have noted the use-
fulness of the arguing category for opinion QA. Our
tasks are different; they use arguing to retrieve rele-
vant answers, but not distinguish stances. Our work
is also different from related work in the domain of
product debates (Somasundaran and Wiebe, 2009)
in terms of the methodology.
Wilson (2007) manually adds positive/negative
arguing information to entries in a sentiment lexi-
con from (Wilson et al, 2005) and uses these as ar-
guing features. Our arguing trigger expressions are
separate from the sentiment lexicon entries and are
derived from a corpus. Our n-gram trigger expres-
sions are also different from manually created regu-
lar expression-based arguing lexicon for speech data
(Somasundaran et al, 2007a).
7 Conclusions
In this paper, we explore recognizing stances in ide-
ological on-line debates. We created an arguing lex-
icon from the MPQA annotations in order to recog-
nize arguing, a prominent type of linguistic subjec-
tivity in ideological stance taking. We observed that
opinions or targets in isolation are not as informative
as their combination. Thus, we constructed opinion
target pair features to capture this information.
We performed supervised learning experiments
on four different domains. Our results show that
both unigram-based and opinion-based systems per-
form better than baseline methods. We found that,
even though our sentiment-based system is able to
perform better than the distribution-based baseline,
it does not perform at par with the unigram system.
However, overall, our arguing-based system does as
well as the unigram-based system, and our system
that uses both arguing and sentiment features obtains
further improvement. Our feature analysis suggests
that arguing features are more insightful than uni-
gram features, as they make finer distinctions that
reveal the underlying ideologies.
References
Lada A. Adamic and Natalie Glance. 2005. The political
blogosphere and the 2004 u.s. election: Divided they
blog. In LinkKDD.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
123
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008).
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 590?598, Singapore,
August. Association for Computational Linguistics.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cocitation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 503?511, Boulder, Colorado, June. Association
for Computational Linguistics.
Gregory Grefenstette, Yan Qu, James G. Shanahan, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04, Avignon, FR.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
SIGKDD Explorations, Volume 11, Issue 1.
Soo-Min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1056?1064.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and sen-
tence levels. In Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL-
2006), pages 109?116, New York, New York.
Wei-Hao Lin. 2006. Identifying perspectives at the doc-
ument and sentence levels using statistical models. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Doctoral
Consortium, pages 227?230, New York City, USA,
June. Association for Computational Linguistics.
Robert Malouf and Tony Mullen. 2008. Taking sides:
Graph-based user classification for informal online po-
litical discourse. Internet Research, 18(2).
Lanny W. Martin and Georg Vanberg. 2008. A ro-
bust transformation procedure for interpreting political
text. Political Analysis, 16(1):93?100.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal po-
litical discourse. In AAAI 2006 Spring Symposium
on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, Vol. 2(1-2):pp. 1?135.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234, Suntec, Singapore, August. Association for
Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007a. Detecting arguing and sentiment in
meetings. In SIGdial Workshop on Discourse and Di-
alogue, Antwerp, Belgium, September.
Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,
and Veselin Stoyanov. 2007b. Qa with attitude: Ex-
ploiting opinion type analysis for improving question
answering in on-line discussions and the news. In In-
ternational Conference on Weblogs and Social Media,
Boulder, CO.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801?808, Manchester, UK, August.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 327?335, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of ACL
Workshop on Frontiers in Corpus Annotation II: Pie in
the Sky.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In hltemnlp2005, pages 347?354,
Vancouver, Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
124
Proceedings of the TextGraphs-6 Workshop, pages 1?9,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
A Combination of Topic Models with Max-margin Learning for Relation
Detection
Dingcheng Li
University of Minnesota
Twin Cities, MN 55455
lixxx345@umn.edu
Swapna Somasundaran
Siemens Corporate Research
Princeton, NJ 08540
swapna.somasundaran@siemens.com
Amit Chakraborty
Siemens Corporate Research
Princeton, NJ 08540
amit.chakraborty@siemens.com
Abstract
This paper proposes a novel application of
a supervised topic model to do entity rela-
tion detection (ERD). We adapt Maximum En-
tropy Discriminant Latent Dirichlet Alloca-
tion (MEDLDA) with mixed membership for
relation detection. The ERD task is refor-
mulated to fit into the topic modeling frame-
work. Our approach combines the benefits of
both, maximum-likelihood estimation (MLE)
and max-margin estimation (MME), and the
mixed membership formulation enables the
system to incorporate heterogeneous features.
We incorporate different features into the sys-
tem and perform experiments on the ACE
2005 corpus. Our approach achieves better
overall performance for precision, recall and
Fmeasure metrics as compared to SVM-based
and LLDA-based models.
1 Introduction
Entity relation detection (ERD) aims at finding rela-
tions between pairs of Named Entities (NEs) in text.
Availability of annotated corpora (NIST, 2003; Dod-
dington et al, 2004) and introduction of shared tasks
(e.g. (Farkas et al, 2010; Carreras and Ma`rquez,
2005)) has spurred a large amount of research in this
field in recent times. Researchers have used super-
vised and semi-supervised approaches (Hasegawa et
al., 2004; Mintz et al, 2009; Jiang, 2009), and ex-
plored rich features (Kambhatla, 2004), kernel de-
sign (Culotta and Sorensen, 2004; Zhou et al, 2005;
Bunescu and Mooney, 2005; Qian et al, 2008) and
inference algorithms (Chan and Roth, 2011), to de-
tect predefined relations between NEs.
In this work, we explore if and how the latent se-
mantics of the text can help in detecting entity rela-
tions. For this, we adapt the Latent Dirichlet Alloca-
tion (LDA) approach to solve the ERD task. Specif-
ically, we present a ERD system based on Maxi-
mum Entropy Discriminant Latent Dirichlet Alloca-
tion (MEDLDA). MEDLDA (Zhu et al, 2009), is
an extension of Latent Dirichlet Allocation (LDA)
that combines capability of capturing latent seman-
tics with the discriminative capabilities of SVM.
There are a number of challenges in employing
the LDA framework for ERD. Latent Dirichlet Allo-
cation and its supervised extensions such as Labeled
LDA (LLDA) (Ramage et al, 2009) and supervised
LDA (sLDA) (Blei and McAuliffe, 2008) are pow-
erful generative models that capture the underlying
semantics of texts. However, they have trouble dis-
covering marginal classes and easily employing rich
feature sets, both of which are important for ERD.
We overcome the first drawback by employing a
MEDLDA framework, which integrates maximum
likelihood estimation (MLE) and maximum margin
estimation (MME). Specifically, it is a combination
of sLDA and support vector machines (SVMs). Fur-
ther, in order to employ rich and heterogeneous fea-
tures we introduce a separate exponential family dis-
tribution for each feature, similar to (Shan et al,
2009), into our MEDLDA model.
We formulate the relation detection task within
the topic model framework as follows. Pairs of NE
mentions1 and the text between them is considered
1Adopting the terminology used in the Automatic Context
Extraction (ACE) program (NIST, 2003), specific NE instances
are called mentions.
1
as mini-document. Each mini-document has a re-
lation type (analogous to the response variable in
the supervised topic model). The topic model in-
fers the topic (relation type) distribution of the mini-
documents. The supervised topic model discovers
a latent topic representation of the mini-documents
and a response parameter distribution. The topic
representation is discovered with observed response
variables during training. During testing, the topic
distribution of each mini-document can form a pre-
diction of the relation types.
We carry out experiments to measure the effec-
tiveness of our approach and compare it to SVM-
based and LLDA-based models, as well as to a pre-
vious work using the same corpora. We also mea-
sure and analyze the effectiveness of incorporating
different features in our model relative to other mod-
els. Our approach exhibits better overall precision,
recall and Fmeasure than baseline systems. We also
find that the MEDLDA-based approach shows con-
sistent capability for incorporation and improvement
due to a variety of heterogeneous features.
The rest of the paper is organized as follows. We
describe the proposed model in Section 2 and the
features that we explore in this work in Section 3.
Section 4 describes the data, experiments, results
and analyses. We discuss the related work in Sec-
tion 5 before concluding in Section 6.
2 MEDLDA for Relation Detection
MEDLDA is an extension of LDA proposed by Zhu,
Ahmed and Xing (2009). LDA is itself unsuper-
vised and the results are often hard to interpret.
However, with the addition of supervised informa-
tion (such as response variables), the resulting topic
models have much better predictive power for classi-
fication and regression. In our work, we use relation
annotations from the ACE (ACE, 2000 2005) corpus
to provide the supervision. NE pairs within a sen-
tence, and the text between them are considered as
a mini-document. Each mini-document is assumed
to be composed of a set of topics. The topic model
trained with these mini-documents given their rela-
tion type label can generate topics biased toward re-
lation types. Thus, the trained topic model will have
good predictive power on relation types.
We first describe the MEDLDA model from (Zhu
et al, 2009) and then describe how we adapt it for
relation detection using mixed membership exten-
sions.
2.1 MEDLDA
Figure 1: MEDLDA
The MEDLDA model described in (Zhu et al,
2009) is illustrated in Figure 12.
Here, ? is a k-dimensional parameter of a Dirich-
let distribution, ?1:k are the parameters for k compo-
nent distribution over the words. Each component
refers to a topic. In a collection of documents D,
each document w1:N is generated from a sequence
of topics z1:N . ? is a k-dimensional topic distribu-
tion variable, which is sampled from a Dirichlet dis-
tribution Dir(?). Like common LDAs, MEDLDA
uses independence assumption for a finite set of ran-
dom variables z1, ..., zn which are independent and
identically distributed, conditioned on the parame-
ter ?. Like sLDA, MEDLDA is a supervised model.
A response variable Y connected to each document
is added for incorporating supervised side informa-
tion. The supervised side information is expected
to make MEDLDA topic discoveries more inter-
pretable. Zhu, Ahmed and Xing?s (2009) MEDLDA
model can be used in both regression and classifi-
cation. Concretely, Y is drawn from ?1:c, a c k-
dimensional vector which can be derived from suit-
able statistical model. In our work, c is the num-
ber of relation types. Note that the plate diagram
for MEDLDA is quite similar to sLDA (Blei and
McAuliffe, 2008). But there is a difference ? sLDA
focuses on building regression models, and thus the
response variable Y in sLDA is generated by a nor-
mal distribution.
Based on the plate diagram, the joint distribution
of latent and observable variables for our MEDLDA-
2(Zhu et al, 2009) do not have this plate digram in their
paper; rather, we create this illustration from the description of
their model.
2
based relation detection is given by
p(?, z,w,y|?, ?1:k, ?1:c)
=
D?
d=1
p(?d|?)?
(
N?
n=1
p(zdn|?d)p(wdn|zdn, ?1:k)
)
? p(yd|zd1:dN , ?1:c) (1)
Another important difference from sLDA lies in
the fact that MEDLDA does joint learning with both
MME and MLE. The joint learning is done in two
stages, unsupervised topic discovery and multi-class
classification (we refer the reader to (Zhu et al,
2009) for details). During training, EM algorithms
are utilized to infer the posterior distribution of the
hidden variables ?, z and ?. In testing, the trained
models are used to predict relation types y.
2.2 Mixed Membership MEDLDA
Although the MEDLDA model described above can
be applied to the relation detection and classification
task, a few modifications are necessary before it can
be effective in predicting relation types. Mainly, a
Figure 2: Mixed Membership MEDLDA
limitation of LDA or other existing topic models is
the difficulty in incorporating rich features. This is
because LDA is designed to handle data points with
homogeneous features such as words. But for rela-
tion detection, like many other NLP tasks, it is im-
portant to have the flexibility of incorporating part-
of-speech tags, named entities, grammatical depen-
dencies and other linguistic features. We overcome
this limitation by introducing a separate exponential
family distribution for each feature similar to (Shan
et al, 2009). Thus, our MEDLDA-based relation
detection model is really a mixed-member Bayesian
network. Figure 2 illustrates our model with this ex-
tension.
Figure 2 is very similar to Figure 1; the only dif-
ference is that the topic component number k is now
kN . The generative process for each document this
model is as follows:
1. Sample a component proportion ?d ?
Dirichlet(?),
2. For each feature like word, part-of-speech,
named entity in the document,
(a) For n ? {1, ..., N}, sample zdn = i ?
Discrete(?d)
(b) For n ? {1,...,N}, sample wdn ?
P (wdn|?dni)
3. Sample the relation type label
from a softmax(z?,?) where yd ?
softmax(
exp(?Th z?)?c?1
h=1 exp(?
T
h z?)
)
In the sampling, index i is the number of the topic
component which ranges from 1 : k. P (wdn|?dni) in
2(b) is an exponential family distribution where i is
from 1...k. Note that now we have ?dni rather than
only ?di since we have drawn separate distributions
for each word (or feature) n.
Now, our MEDLDA-based relation-detection
model can integrate diverse features of different
types or the same features with different parameters.
Following the generative process, parameter es-
timation and inferences can be made with either
Gibbs sampling or variational methods. We use vari-
ational methods since we adapt MEDLDA package3
to mixed-membership MEDLDA and train relation
detection models.
2.3 Relation Detection
With the generative process, inference and parame-
ter estimation in place, we are ready to perform rela-
tion detection. The first step is to perform variational
inference given the testing instances.
In classification, we estimate the probability of
the relation type given topics and the response pa-
rameters, i.e. p(yd|zd1:dN , ?1:c?1). With variational
approximation, we can derive the prediction rule as
F (y, z1:N , ?) = ?T f(y, z?) where f(y, z?) is a fea-
ture vector. Now, SVM can be used to derive the
3this package is downloaded from
http://www.cs.cmu.edu/j?unzhu/medlda.htm
3
prediction rule. The final prediction can be general-
ized exactly the same as Zhu, Ahmed and Xing (Zhu
et al, 2009):
y? = argmaxyE[?
T f(y, Z?)|?, ?] (2)
3 Features
We explore the effectiveness of incorporating fea-
tures into our systems as well as the baselines. For
this, we construct feature sets similar to Jiang and
Zhai (2007) and Zhou (2005). Three kinds of fea-
tures are employed:
1. BOW The Bag of Words (BOW) feature cap-
tures all the words in our mini-document. It
comprises of the words of the two NE mentions
and the words between them.
2. SYN The SYN features are constructed to cap-
ture syntactic, semantic and structural infor-
mation of the mini-document. They include
features such as HM1 (the head word of the
first mention), HM2 (the head word of the sec-
ond mention), ET1, ET2, M1 and M2 (Entity
types and mention types of the two mentions
involved), #MB (number of other mentions in
between the two mentions), #WB (number of
words in between the two mentions).
3. COMP The COMP features are composite fea-
tures that are similar to SYN, but they addition-
ally capture language order and dependencies
between the features mentioned above. These
include features such as HM1HM2 (combining
head word of mention 1 and head word of men-
tion 2) , ET12 (combinations of mention entity
type), ML12 (combination of mention levels),
M1InM2 or M2InM1 (flag indicating whether
M2/M1 is included in M1/M2).
The main intuitions behind employing composite
features, COMP, are as follows. First, they capture
the ordering information. The ordering of words are
not captured by BOW. That is, BOW features as-
sume exchangeability. This works for models based
on random or seeded sampling (e.g. LDA) ? as long
as words sampled are associated with a topic, the
hidden topics of the documents can be discovered.
In the case of ERD, this assumption might work
with symmetric relations. However, when the rela-
tions are asymmetric, ordering information is impor-
tant. Composite features such as HM1HM2 encodes
what mention head word precedes the other. Second,
features such as M1InM2 or M2InM1 capture token
dependencies. Besides exchangeability, LDA-based
models also assume that words are conditionally in-
dependent. Consequently, the system cannot capture
the knowledge that some mentions may be included
in other mentions. By constructing features such as
M1InM2 or M2InM1, we encode the dependency in-
formation explicitly.
4 Experiments
As MEDLDA is a combination of maximum mar-
gin principle with maximum likelihood estimation
for topic modes, we compare it with two baseline
systems. The first, SVM, uses only the maximum
margin principle, while the second, LLDA, uses only
maximum likelihood estimation for topic modeling.
4.1 Data
We use the ACE corpus (Phase 2, 2005) for eval-
uation. The ACE corpus has annotations for both
entities and relations. The corpus has six major re-
lations types, 23 subtypes and 7 entity types. In this
work, we focus only on the six high-level relation
types listed in Table 1. In addition to the the 6 ma-
jor types, we have an additional category, no relation
(NO-REL), that exists between entities that are not
related.
The data for our experiments consists of pairs of
NEs from a sentence, and the gold standard annota-
tion of their relation type (or NO-REL). All relations
in the ACE corpus are intra-sentential and hence we
do not create NE pairs that cross sentence bound-
aries. Also, almost all positive instances are within
two mentions of each other. Hence, we create NE
pairs for only those NEs that have at most 2 interven-
ing NEs in between. This gives us a total of 38,342
relation instances of which 32,640 are negative in-
stances (NO-REL) and 5702 are positive relation in-
stances belonging to one of the 6 categories.
4.2 Experimental Setup
We use 80% of the instances for training and 20%
for testing. The topic numbers and the penalty pa-
rameter of the cost function C are first determined
4
Major Type Definition Example
ART artifact
User, owner, inventor or the makers of the Kursk
manufacturer
GEN-AFF
citizen, resident, religion, U.S. Companies
ethnicity and organization-location
ORG-AFF
employment, founder, ownership, The CEO of Siemens
(Org-affiliation) sports-affiliation, investor-shareholder
student-alumni and membership
PART-WHOLE geographical, subsidiary and so on a branch of U.S bank
PER-SOC
business, family and a spokesman for the senator
(person-social) lasting personal relationship
PHYS (physical) located or near a military base in Germany
Table 1: Relation types for ACE 05 corpus
for each of the models (wherever applicable) using
the training data. Best parameters are determined
for the three conditions: 1) BOW features alone
BOW, 2) BOW plus SYN features (PlusSYN) and 3)
BOW plus SYN and COMP features (PlusCOMP).
All systems achieved their overall best performance
with PlusCOMP features (see Section 4.4 for a de-
tailed analysis).
4.2.1 MEDLDA
The number of topics are determined using the
equation 2K0 + K1 following Zhu, Ahmed and
Xing (2009) and K1 = 2K0. K0 is the number
of topics per class and K1 is the number of topics
shared by all relation types. The choice of topics is
based on the intuition that the shared component K1
should use all class labels to model common latent
structure while non-overlapping components should
model specific characteristics data from each class.
The ratio of topics is based on the understanding that
shared topics may be more than topics of each class.
The specific numbers do not produce much variation
in the final results. We experimented with the fol-
lowing number of topics: 20, 40, 70, 80, 90, 100,
110. BOW, PlusSYN, and PlusCOMP configura-
tions obtain the best performance for 90 topics, 80
topics, and 70 topics respectively.
Since SVMs are employed in the MEDLDA im-
plementation, we need to determine the penalty pa-
rameter of the cost function, C. We used 5 fold cross-
validation to locate the parameter C. The best values
for C are 25, 28, 30 respectively for BOW, PlusSYN
and PlusCOMP configurations. We used a linear
kernel as it is the most commonly used kernel for
text classification tasks. Since MEDLDA is run by
sampling, the result may be different each time. We
ran it 5 times for each setting and took the average
as the final results.
4.2.2 LLDA and SVM
The setting of topics for LLDA is similar to
MEDLDA. As LLDA is also run by sampling, we
ran it 5 times for each setting and took the average
as the final results. In SVMlight, a grid search tool
is provided to locate the the best value for parame-
ter C. The best C for all three conditions was found
to be 1. All other settings for the two models are
similar to those of MEDLDA.
4.3 Results
Prec% Rec% F%
SVM 53.2 35.2 40.3
LLDA 28.3 51.6 36.6
MEDLDA 57.8 53.2 55.4
Table 2: Overall performance of the 3 systems
We present the results of the three systems built
using PlusCOMP, as all systems achieved their best
overall performance using these features. Table 2 re-
ports the precision, recall and Fmeasure of the three
systems averaged across all 7 categories (the best
numbers for each metric are highlighted in bold).
Here we see that MEDLDA outperforms LLDA and
5
Labels
SVM LLDA MEDLDA
Pre% Rec% F% Pre% Rec% F% Pre% Rec% F%
ART 30 8 14 1.5 33 3 49 36 41
GEN-AFF 53 48 50 3 32 6 40 39 40
ORG-AFF 55 35 43 59 58 59 53 59 56
PART-WHOLE 39 08 14 31 82 45 44 52 48
PER-SOC 50 17 25 7 92 13 73 76 75
PHYS 55 35 43 26 47 33 56 19 29
NO-REL 90 95 93 70 17 27 89 91 90
Table 3: Multi-class Classification Results with PlusCOMP for SVM, LLDA and MEDLDA for the six ACE 05
categories and NO-REL
SVM across all metrics. Specifically, there is a 15
percentage point improvement in Fmeasure over the
best performing baseline. This result indicates that
our approach of combining topic model with max-
margin learning is effective for relation detection.
Now, looking at the results for each individual
relationship category (see Table 3; the best num-
bers for each category and metric are highlighted
in bold) we see that the Fmeasure for MEDLDA is
better than that for SVM for 4 out of the 6 ACE re-
lation types; and better than the Fmeasure obtained
by LLDA for all relation types except ORG-AFF.
Specifically, comparing with the best performing
baseline, MEDLDA produces a Fmeasure improve-
ment 27 percentage points for ART, 3 percentage
points for PART-WHOLE and 50 percentage points
for PER-SOC. Also, for four of the six ACE rela-
tion types, MEDLDA achieves the best precision.
Even in the cases where MEDLDA is not the best
performer for a relation category, its performance is
not very poor (unlike, for example, SVM for PART-
WHOLE and LLDA for ART, respectively).
Interestingly, the NO-REL category reveals a
sharp contrast in the performance of SVM and
LLDA. NO-REL is a difficult, catch-all category
that is a mixture of data with diverse distributions.
This is a category where maximum-margin learning
is more effective than maximum-likelihood estima-
tion. Notice that MEDLDA achieves performance
close to SVM for this category. This is because,
even though both LLDA and MEDLDA model hid-
den topics and then employ discovered hidden topics
to predict relation types, MEDLDA does joint infer-
ence of MLE and MME. This joint inference helps
to improve the detection of NO-REL.
Finally, we also compare our system?s results (us-
ing PlusCOMP features) with the results of previ-
ous research on the same corpus (Khayyamian et al,
2009). They use similar experimental settings: ev-
ery pair of entities within a sentence is regarded to
involve a negative relation instance unless it is anno-
tated as positive in the corpus. A similar filter (they
use a distance filter) is used to sift out unrelated neg-
ative instances. Their train/test ratio of data split is
also the same as ours.
Khayyamian, Mirroshandel and Abolhas-
sani (2009) employ state-of-art kernel methods
developed by Collins and Duffy (2002) and only
report Fmeasures over the six ACE relation types.
For clarity, we reproduce their results in Table 4
and repeat MEDLDA Fmeasures from Table 3 in
the last column. The last row (Overall) reports the
macro-averages computed over all relation types for
each system. Here we see that overall, MEDLDA
outperforms all kernels. MEDLDA also performs
better than the best kernel for four of the six relation
types.
4.4 Analysis
As mentioned previously, all three systems achieved
their overall best performance with PlusCOMP fea-
tures. Here, we analyze if informative features are
consistently useful and if the systems can harness
the informative features consistently across all re-
lation types. Figures 3, 4 and 5 illustrate the F-
measures for SVM, LLDA and MEDLDA respec-
tively for the three conditions: BOW, PlusSYN and
PlusCOMP.
6
Labels CD?01 AAP AAPD TSAAPD-0 TSAAPD-01 MEDLDA
ART% 51 49 50 48 47 41
GEN-AFF % 9 10 12 11 11 40
ORG-AFF % 43 43 43 43 45 56
PART-WHOLE % 30 28 29 30 28 48
PER-SOC % 62 58 70 63 73 75
PHYS % 32 36 29 33 33 29
Overall (Avg) 38 37 39 38 40 48
Table 4: F-measures for every kernel in (Khayyamian et al, 2009) and MEDLDA
Figure 3: SVM Fmeausres for 3 feature conditions
Figure 4: LLDA Fmeausres for 3 feature conditions
Figure 5: MEDLDA Fmeausres for 3 feature conditions
Let us first look at the best systems (based on
Fmeasure) for each of the six ACE relation types
in Table 3, and look at what feature set pro-
duces the best result for that system and relation.
MEDLDA is the best performer for ART, PART-
WHOLE and PER-SOC in Table 3. Figure 5 re-
veals that MEDLDA?s best performance for these re-
lation types are obtained using PlusCOMP features.
Similarly SVM obtains the best Fmeasure for GEN-
AFF and PHYS relations and Figure 3 shows that
SVM achieves its best performance for these cate-
gories using PlusCOMP. We also see a similar trend
with LLDA and the ORG-AFF relation type. These
results corroborate intuition from previous research
that informative features are important for relation
type recognition. The only exception to this is the
performance of SVM for NO-REL. This is not sur-
prising, as the features we use are focused on deter-
mining true relation types and NO-REL is a mixture
of all cases (and features) where relations do not ex-
ist.
Further analysis of the figures reveal that even
though there is a general trend towards better per-
formance with addition of more informative fea-
tures, not all systems show consistent improvements
across all relation types with the addition of com-
posite features. That is, some systems get degraded
performance due to feature addition. For example,
in Figure 3, we see that the SVM with PlusCOMP
features is outperformed by SVM with PlusSYN for
ART and SVM with BOW for NO-REL. The gains
from features are also inconsistent in the case of
LLDA (Figure 4). While the LLDA system with
PlusSYN features always improves over the one us-
ing BOW, the performance drops considerably when
using PlusCOMP features for ART and GEN-AFF.
On the other hand, MEDLDA (see Figure 5) shows
more consistent improvement for all relation types
with the addition of more complex features. Also,
7
the gains are more substantial. This is encouraging
and opens up avenues for further exploration.
5 Related Work
Previous research has explored various methods and
features for relationship detection and mining. Ker-
nel methods have been popularly used for rela-
tion detection. Some examples are are dependency
tree kernels (Culotta and Sorensen, 2004), short-
est dependency path kernels (Bunescu and Mooney,
2005), and more recently, convolution tree kernels
(Zhao and Grishman, 2005; Zhang et al, 2006)
context-sensitive convolution tree kernels (Zhou et
al., 2007) and dynamic syntax tree kernels (Qian et
al., 2008). Kernel methods for relation extraction
focus on representing and capturing the structured
information of the text between the entities. In our
MEDLDA model, instead of computing distances
between subtrees, we sample topics based on their
distributions. The sampling is not only on the (mini)
document level, but also on the word level or on the
syntactic or semantic level. Our model focuses on
addressing the underlying semantics more directly
than typical kernel-based methods.
Chan and Roth (2011) employ constraints us-
ing an integer linear programming (ILP) framework.
Using this, they apply rich linguistic and knowledge-
based constraints based on coreference annotations,
a hierarchy of relations, syntacto-semantic structure,
and knowledge from Wikipedia. In our work, we
focus on capturing the latent semantics of the text
between the NEs.
A variety of features have been explored for ERD
in previous research (Zhou et al, 2005; Zhou et al,
2008; Jiang and Zhai, 2007; Miller et al, 2000).
Syntactic features such as POS tags and dependency
path between entities; semantic features such as
Word-Net relations, semantic parse trees and types
of NEs; and structural features such as which entity
came first in the sentence have been found useful for
ERD. We too observe the utility of informative fea-
tures for this task. However, exploration of the fea-
ture space is not the main focus of this work. Rather,
our focus is on whether the models are capable of
incorporating rich features. A fuller exploration of
rich heterogeneous features is the focus of our fu-
ture work.
A closely related task is that of relation min-
ing and discovery, where unsupervised, semi-
supervised approaches have been effectively em-
ployed (Hasegawa et al, 2004; Mintz et al, 2009;
Jiang, 2009). For example, Hasegawa et al (2004)
use clustering and entity type information, while
Mintz et al (2009) employ distant supervision. Our
ERD task is different from these as we focus on
classifying the relation types into predefined relation
types in the ACE05 corpus.
Topic models have been applied previously for a
number of NLP tasks (e.g. (Lin et al, 2006; Titov
and McDonald, 2008). LDAs have also been em-
ployed to reduce feature dimensions in relation de-
tection systems (Hachey, 2006). However, to the
best of our knowledge, this is the first work to make
use of topic models to perform relation detection.
6 Conclusion and Future Work
In this work, we presented a system for en-
tity relation detection based on mixed-membership
MEDLDA. Our approach was motivated by the idea
that combination of max margin and maximum like-
lihood can help to improve relation detection task.
For this, we adapted the existing work on MEDLDA
and mixed membership models and formulated ERD
as a topic detection task. To the best of our knowl-
edge, this is the first work to make full use of topic
models for relation detection.
Our experiments show that the proposed approach
achieves better overall performance than SVM-
based and LLDA-based approaches across all met-
rics. We also experimented with different features
and the effectiveness of the different models for har-
nessing these features. Our analysis show that our
MEDLDA-based approach is able to effectively and
consistently incorporate informative features.
As a model that incorporates maximum-
likelihood, maximum-margin and mixed mem-
bership learning, MEDLDA has the potential of
incorporating rich kernel functions or conditional
topic random fields (CTRF) (Zhu and Xing, 2010).
These are some of the promising directions for our
future exploration.
8
References
ACE. 2000-2005. Automatic Content Extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
D.M. Blei and J. McAuliffe. 2008. Supervised topic
models. Advances in Neural Information Processing
Systems, 20:121?128.
R.C. Bunescu and R.J. Mooney. 2005. A shortest path
dependency kernel for relation extraction. In HLT &
EMNLP.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
CONLL, pages 152?164. ACL.
Y. Chan and D. Roth. 2011. Exploiting syntactico-
semantic structures for relation extraction. In ACL.
M. Collins and N. Duffy. 2002. Convolution kernels for
natural language. Advances in neural information pro-
cessing systems, 1:625?632.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 423. ACL.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proceedings of LREC, volume 4,
pages 837?840.
R. Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.
2010. The CoNLL-2010 Shared Task: Learning to De-
tect Hedges and their Scope in Natural Language Text.
In CoNLL-2010, pages 1?12.
B. Hachey. 2006. Comparison of similarity models for
the relation discovery task. In COLING & ACL 2006,
page 25.
T Hasegawa, S Sekine, and Ralph Grishman. 2004. Dis-
covering relations among named entities from large
corpora. In 42nd ACL.
J. Jiang and C.X. Zhai. 2007. A systematic explo-
ration of the feature space for relation extraction. In
NAACL/HLT, pages 113?120.
J. Jiang. 2009. Multi-task transfer learning for weakly-
supervised relation extraction. In 47th ACL & 4th
AFNLP, pages 1012?1020. ACL.
N. Kambhatla. 2004. Combining lexical, syntactic, and
semantic features with maximum entropy models for
extracting relations. In ACL 2004 Interactive poster
and demonstration sessions.
M. Khayyamian, S.A. Mirroshandel, and H. Abolhassani.
2009. Syntactic tree-based relation extraction using a
generalization of Collins and Duffy convolution tree
kernel. In HLT/NAACL,: Student Research Workshop.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you on?
Identifying perspectives at the document and sentence
levels. In CoNLL-2006.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000.
A novel use of statistical parsing to extract information
from text. In NAACL.
M Mintz, S Bills, R Snow, and D Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In 47th ACL & 4th AFNLP.
US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-
tional Institute for Standards and Technology (NIST),
pages 2003?08.
L. Qian, G. Zhou, F. Kong, Q. Zhu, and P. Qian. 2008.
Exploiting constituent dependencies for tree kernel-
based semantic relation extraction. In 22nd ACL.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009. Labeled LDA: A supervised topic model for
credit attribution in multi-labeled corpora. In EMNLP.
H. Shan, A. Banerjee, and N.C. Oza. 2009. Discrim-
inative Mixed-membership Models. In ICDM, pages
466?475. IEEE.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In ACL-08: HLT.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities with
both flat and structured features. In 21st ICCL & 44th
ACL.
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods. In
43rd ACL.
G Zhou, S. Jian, Z. Jie, and Z. Min. 2005. Exploring
various knowledge in relation extraction. In In 43rd
ACL.
G Zhou, M. Zhang, D.H. Ji, and Q Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In EMNLP/CoNLL-
2007, pages 728?736.
G.D. Zhou, M. Zhang, D.H. Ji, and Q.M. Zhu. 2008.
Hierarchical learning strategy in semantic relation ex-
traction. Information Processing & Management,
44(3):1008?1021.
J. Zhu and E.P. Xing. 2010. Conditional Topic Random
Fields. In ICML. ACM.
J. Zhu, A. Ahmed, and E.P. Xing. 2009. MedLDA: max-
imum margin supervised topic models for regression
and classification. In ICML, pages 1257?1264. ACM.
9
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 1?11,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Measures of Specific Vocabulary Knowledge from Constructed
Responses (?Use These Words to Write a Sentence Based on this Picture?)
Swapna Somasundaran
Educational Testing Services
660 Rosedale Road,
Princeton, NJ 08541, USA
ssomasundaran@ets.org
Martin Chodorow
Hunter College and the Graduate Center
City University of New York,
New York, NY 10065, USA
martin.chodorow@hunter.cuny.edu
Abstract
We describe a system for automatically
scoring a vocabulary item type that asks
test-takers to use two specific words in
writing a sentence based on a picture. The
system consists of a rule-based component
and a machine learned statistical model
which uses a variety of construct-relevant
features. Specifically, in constructing the
statistical model, we investigate if gram-
mar, usage, and mechanics features devel-
oped for scoring essays can be applied to
short answers, as in our task. We also ex-
plore new features reflecting the quality of
the collocations in the response, as well as
features measuring the consistency of the
response to the picture. System accuracy
in scoring is 15 percentage points greater
than the majority class baseline and 10
percentage points less than human perfor-
mance.
1 Introduction
It is often said that the best way to see if a per-
son knows the meaning of a word is to have that
person use the word in a sentence. Despite this
widespread view, most vocabulary testing contin-
ues to rely on multiple choice items (e.g. (Law-
less et al., 2012; Lawrence et al., 2012)). In
fact, few assessments use constructed sentence re-
sponses to measure vocabulary knowledge, in part
because of the considerable time and cost required
to score such responses manually. While much
progress has been made in automatically scor-
ing writing quality in essays (Attali and Burstein,
2006; Leacock et al., 2014; Dale et al., 2012),
the essay scoring engines do not measure profi-
ciency in the use of specific words, except perhaps
for some frequently confused homophones (e.g.,
its/it?s, there/their/they?re, affect/effect).
In this paper we present a system for automated
scoring of targeted vocabulary knowledge based
on short constructed responses in a picture de-
scription task. Specifically, we develop a system
for scoring a vocabulary item type that is in op-
erational use in English proficiency tests for non-
native speakers. Each task prompt in this item type
consists of two target key words, for which the vo-
cabulary proficiency is tested, and a picture that
provides the context for the sentence construction.
The task is to generate a single sentence, incorpo-
rating both key words, consistent with the picture.
Presumably, a test-taker with competent knowl-
edge of the key words will be able to use them in a
well-formed grammatical sentence in the context
of the picture.
Picture description tasks have been employed in
a number of areas of study ranging from second
language acquisition to Alzheimer?s disease (El-
lis, 2000; Forbes-McKay and Venneri, 2005). Pic-
tures and picture-based story narration have also
been used to study referring expressions (Lee et
al., 2012) and to analyze child narratives in order
to predict language impairment (Hassanali et al.,
2013). Evanini et al. (2014) employ a series of
pictures and elicit (oral) story narration to test En-
glish language proficiency. In our task, the picture
is used as a constraining factor to limit the type
and content of sentences that can be generated us-
ing the given key words.
In the course of developing our system, we ex-
amined existing features that have been developed
for essay scoring, such as detectors of errors in
grammar, usage and mechanics, as well as col-
location features, to see if they can be re-used
for scoring short responses. We also developed
new features for assessing the quality of sentence
construction using Pointwise Mutual Information
(PMI). As our task requires responses to describe
the prompt pictures, we manually constructed de-
tailed textual descriptions of the pictures, and de-
1
veloped features that measure the overlap between
the content of the responses and the textual de-
scription. Our automated scoring system is partly
based on deterministic scoring criteria and partly
statistical. Overall, it achieves an accuracy of
76%, which is a 15 percentage point improvement
over a simple majority class baseline.
The organization of this paper is as follows:
Section 2 describes the picture description task
and the scoring guide that is used to manually
score the picture description responses opera-
tionally. It also considers which aspects of scor-
ing may be handled best by deterministic proce-
dures and which are more amenable to statistical
modeling. Section 3 details the construction of a
reference corpus of text describing each picture,
and Section 4 presents the features used in scor-
ing. Section 5 describes our system architecture
and presents our experiments and results. Detailed
analysis is presented in Section 6, followed by re-
lated work in Section 7 and a summary with direc-
tions for future research in Section 8.
2 Task Description and Data
The picture description task is an item type that is
in actual operational use as part of a test of En-
glish. It consists of a picture, along with two key
words, one or both of which may be in an inflected
form. Test-takers are required to use the two words
in one sentence to describe the picture. They may
change the inflections of the words as appropriate
to the context of their sentence, but they must use
some form of both words in one sentence. Requir-
ing them to produce a response based on the pic-
ture constrains the variety of sentences and words
that they are likely to generate.
Trained human scorers evaluate the responses
based on appropriate use of grammar and the rel-
evance of the sentence to the picture. The opera-
tional scoring guide is as follows:
score = 3 The response consists of ONE sen-
tence that: (a) has no grammatical errors, (b)
contains forms of both key words used appro-
priately, AND (c) is consistent with the pic-
ture.
score = 2 The response consists of one or
more sentences that: (a) have one or more
grammatical errors that do not obscure the
meaning, (b) contain BOTH key words, (but
they may not be in the same sentence and
the form of the word(s) may not be accurate),
AND (c) are consistent with the picture.
score = 1 The response: (a) has errors that in-
terfere with meaning, (b) omits one or both
key words, OR (c) is not consistent with the
picture.
score = 0 The response is blank, written in
a foreign language, or consists of keystroke
characters.
Our decisions about scoring system design are
based on the scoring guide and its criteria. Some
aspects of the scoring can be handled by simple
pattern matching or lookup, while others require
machine learning. For example, score 0 is as-
signed to responses that are blank or are not in
English. This can be detected and scored in a
straightforward way. On the other hand, the de-
termination of grammaticality for the score points
3, 2 and 1 depends on the presence and severity
of grammatical errors. A wide variety of such er-
rors appear in responses, including errors of punc-
tuation, subject-verb agreement, preposition usage
and article usage. The severity of an error depends
on how problematic the error is, and the system
will have to learn this from the behavior of the
trained human scorer(s), making this aspect of the
scoring more amenable to statistical modeling.
Similarly, statistical modeling is more suitable
for determining the consistency of the response
with respect to the picture. According to the scor-
ing guide, a response gets a score of 0 or 1 if it is
not consistent with the picture, and gets a score of
2 or 3 if it is consistent. Thus, this aspect cannot
solely determine the score of a response ? it influ-
ences the score in conjunction with other language
proficiency factors. Further, measures of how rel-
evant a response is to a picture are likely to fall on
a continuous scale, making a statistical modeling
approach appropriate.
Finally, although there are some aspects of the
scoring guide, such as the number of sentences
and the presence of the key words, that can be
measured trivially, they do not act as sole deter-
minants of the score. For example, having more
than one sentence can result in the response re-
ceiving a score of 2 or 1. The number of sentences
works in conjunction with other factors such as
severity of grammar errors and relevance to the
picture. Hence its contribution to the final score
is best modeled statistically.
2
As a result of the heterogeneous nature of the
problem, our system is made up of a statistical
learning component as well as a non-statistical
component.
2.1 Data
The data set consists of about 58K responses to
434 picture prompts. The mean response length
was 11.26 words with a standard deviation of 5.10.
The data was split into 2 development sets (con-
sisting of a total of about 2K responses) and a fi-
nal train-test set (consisting of the remaining 56K
responses) used for evaluation. All 58K responses
were human scored using the scoring rubric dis-
cussed in Section 2. About 17K responses were
double annotated. The inter-annotator agreement,
using quadratic weighted kappa (QWK), was 0.83.
Score point 3, the most frequent class, was as-
signed to 61% of the responses, followed by score
point 2 (31%), score point 1 (7.6%) and score
point 0 (0.4%).
3 Reference Picture Descriptions
The pictures in our task vary in their complexity.
A typical prompt picture might be a photograph
of an outdoor marketplace, the inside of an airport
terminal, a grocery store, a restaurant or a store
room. Because consistency with respect to the pic-
ture is a crucial component in our task, we needed
a reliable and exhaustive textual representation of
each picture. Therefore, we manually constructed
a reference text corpus for each of our 434 pic-
ture prompts. We chose to use manual creation of
the reference corpus instead of trying automated
image recognition because automated methods of
image recognition are error prone and would result
in a noisy reference corpus. Additionally, auto-
mated approaches would, at best, give us a (noisy)
list of items that are present in the picture, but not
the overall scene or event depicted.
Two annotators employed by a company that
specializes in annotation created the reference cor-
pora of picture descriptions. The protocol used for
creating the reference corpus is shown below:
Part-1: List the items, setting, and events
in the picture.
List, one by one, all the items and events you
see in the picture. These may be animate ob-
jects (e.g. man), inanimate objects (e.g. table)
or events (e.g. dinner). Try to capture both the
overall setting (restaurant), as well as the ob-
jects that make up the picture (e.g. man, table,
food). These are generally (but not necessar-
ily) nouns and noun phrases. Some pictures
can have many items, while some have only a
few. The goal is to list 10-15 items and to cap-
ture as many items as possible, *starting with
the most obvious ones*.
If the picture is too sparse, and you are not
able to list at least 10 items, please indicate
this as a comment.
Part:2 Describe the picture
Describe the scene unfolding in the picture.
The scene in the picture may be greater than
the sum of its parts (many of which you will
list in part-1). For example, the objects in a
picture could be ?shoe? ?man? ?chair?, but the
scene in the picture could be that of a shoe
purchase. The description tries to recreate the
scene (or parts of the scene) depicted in the
picture.
Generate a paragraph of 5-7 sentences de-
scribing the picture. Some of these sentences
will address what is going on, while some may
address relations between items. The propor-
tions of these will differ, based on the picture.
Make sure that you generate at least one sen-
tence containing the two key words.
If the picture is too simple, and you are not
able to generate at least 5 sentences, please
indicate this as a comment.
The human annotator was given the picture and
the two key words. The protocol for creating each
reference corpus asked the annotator to first ex-
haustively list all the items (animate and inani-
mate) in the picture. Then, the annotator was
asked to describe the scene in the picture. We used
this two step process in order to capture, as much
as possible, all objects, relationships between ob-
jects, settings and events depicted in the pictures.
The size of the reference corpus for each prompt
is much larger than the single sentence test-taker
response. This is intentional as the goal is to make
the reference corpus as exhaustive as possible. We
used a single annotator for each prompt. Double
annotation using a secondary annotator was done
in cases where we felt that the coverage of the cor-
pus created by the primary annotator was insuffi-
3
cient
1
.
In order to test coverage, we used a small devel-
opment set of essays from each prompt and com-
pared the coverage of the generated reference cor-
pus over the development essays. If the cover-
age (proportion of content words in the responses
that were found in the reference corpus) was less
than 50% (this was the case for about 20% of
the prompts), we asked the secondary annotator to
create a new reference corpus for the prompt. The
two reference corpora for the prompt were then
simply combined to form a single reference cor-
pus.
4 Features for automated scoring
Because the score points in the scoring guide con-
flate, to some degree, syntactic, semantic, and
other weaknesses in the response, we carried out
a scoring study on a second small development
set (comprising of a total of 80 responses from 4
prompts, picked randomly) to gather insight into
the general problems in English language profi-
ciency exhibited in the responses. For the study,
it was necessary to have test-taker responses re-
scored by an annotator using an analytic scheme
which makes the types and locations of prob-
lems explicit. This exercise revealed that, in ad-
dition to the factors stated explicitly in the scor-
ing guide, there is another factor that results in
low comprehension (readability) of the sentence
and that reflects lower English proficiency. Specif-
ically, the annotator tagged many sentences as be-
ing ?awkward?. This awkwardness was due to
poor choice of words or to poor construction of the
sentence. For example, in the sentence ?The man
is putting some food in bags while he is record-
ing for the payment?, ?recording for the payment?
was marked as an awkward phrase. Based on our
annotation of the scores and on the descriptions in
the scoring guide, we selected features designed to
capture grammar, picture relevance and awkward
usage. We discuss each of our feature sets in the
following subsections.
4.1 Features for Grammatical Error
Detection
Essay scoring engines such as e-rater
R
?
(Attali
and Burstein, 2006) typically use a number of
1
We do not conduct inter-annotator agreement studies as
the goal of the double annotation was to create a diverse de-
scription.
grammar, usage and mechanics features that de-
tect and quantify different types of English usage
errors in essays. Examples of some of these error
types are: Run-on Sentences, Subject Verb Agree-
ment Errors, Pronoun Errors, Missing Posses-
sive Errors, Wrong Article Errors, Missing Arti-
cle Errors, Preposition Errors, Non-standard Verb
or Word Form Errors, Double Negative Errors,
Fragment or Missing Comma Errors, Ill-formed
Verb Errors, Wrong Form of Word Errors, Spelling
Errors, Wrong Part of Speech Errors, and Missing
Punctuation Errors .
In addition to these, essay scoring engines of-
ten also use as features the Number of Sentences
that are Short, the Number of Sentences that are
Long, the Number of Passive Sentences, and other
features that are relevant only for longer texts such
as essays. Accordingly, we selected, from e-rater
113 grammar, word usage, mechanics and lexical
complexity features that could be applied to our
short response task. This forms our grammar fea-
ture set.
4.2 Features for Measuring Content
Relevance
We generated a set of features that measure the
content overlap between a given response and the
corresponding reference corpus for the prompt.
For this, first the keywords and the stop words
were removed from the response and the reference
corpus, and then the proportion of overlap was cal-
culated between the lemmatized content words of
the response and the lemmatized version of the
corresponding reference corpus, as follows:
|Response ? Corpus|
|Response|
It is not always necessary for the test-taker to
use exactly the same words found in the reference
corpus. For example, the annotator might have
referred to a person in the picture as a ?lady?,
while a response may refer to the same person
as a ?woman? or ?girl? or even just ?person?.
Thus, we needed to go beyond simple lexical
match. In order to account for synonyms, we ex-
panded the content words in the reference corpus
by adding their synonyms, as provided in Lin?s
thesaurus (Lin, 1998) and then compared the ex-
panded reference to each response. Along the
same lines, we also used expansions from Word-
Net synonyms, WordNet hypernyms and WordNet
hyponyms. The following is the list of our content
4
relevance features. Each measures the proportion
of overlap as described by the equation above be-
tween the lemmatized response and
1. lemmas: the lemmatized reference corpus.
2. cov-lin: the reference corpus expanded using
Lin?s thesaurus.
3. cov-wn-syns: the reference corpus expanded
using WordNet Synonyms.
4. cov-wn-hyper: the reference corpus ex-
panded using WordNet Hypernyms.
5. cov-wn-hypo: the reference corpus ex-
panded using WordNet Hyponyms.
6. cov-all: the reference corpus expanded using
all of the above methods.
Mean proportions of overlap ranged from 0.65
for lemmas to 0.97 for cov-all.
The 6 features listed above, along with the
prompt id give a total of 7 features that form our
relevance feature set. We use prompt id as a fea-
ture because the extent of overlap can depend on
the prompt. Some pictures are very sparse, so,
the description of the picture in the response will
be short, and will not vary much from the refer-
ence corpus. For these, a good amount of overlap
between the response and reference corpus is ex-
pected. Other pictures are very dense with a large
number of objects and items shown. In this case,
any single response may describe just a small sub-
set of the items and satisfy the consistency criteria,
and consequently, even a small overlap between
the response and the reference corpus may be suf-
ficient.
4.3 Features for Awkward Word Usage
In order to measure awkward word usage, we ex-
plored PMI-based features, and also investigated
whether some features developed for essay scor-
ing can be used effectively for this purpose.
4.3.1 PMI-based ngram features
Non-native writing is often characterized by in-
appropriate combinations of words, indicating the
writer?s lack of knowledge of collocations. For ex-
ample, ?recording for the payment? might be bet-
ter expressed as ?entering the price in the cash reg-
ister?. As ?recording for the payment? is an inap-
propriate construction, it is not likely to be com-
mon, for example, in a large web corpus. We use
this intuition in constructing our PMI-based fea-
tures.
We find the PMI of all adjacent word pairs
(bigrams), as well as all adjacent word triples
(trigrams) in the Google 1T web corpus (Brants
and Franz, 2006) using the TrendStream database
(Flor, 2013).
PMI between word pairs (bigram AB) is defined
as:
log
2
p(AB)
p(A).p(B)
and between word triples (trigram ABC) as
log
2
p(ABC)
p(A).p(B).p(C)
The higher the value of the PMI, the more com-
mon is the collocation for the word pair/triple in
well formed texts. On the other hand, negative
values of PMI indicate that the given word pair (or
triple) is less likely than chance to occur together.
We hypothesized that this would be a good indica-
tor of awkward usage, as suggested in (Chodorow
and Leacock, 2000).
The PMI values for adjacent words obtained
over the entire response are then assigned to bins,
with 8 bins for word pairs and another 8 for word
triples. Each bin represents a range for PMI p tak-
ing real values R as follows:
bin
1
= {p ? R | p > 20}
bin
2
= {p ? R | 10 < p ? 20}
bin
3
= {p ? R | 1 < p ? 10}
bin
4
= {p ? R | 0 < p ? 1}
bin
5
= {p ? R | ? 1 < p ? 0}
bin
6
= {p ? R | ? 10 < p ? ?1}
bin
7
= {p ? R | ? 20 < p ? ?10}
bin
8
= {p ? R | p ? ?20}
Once the PMI values for the adjacent word pairs
in the response are generated, we generate two sets
of features. The first set is based on the counts
of word pairs falling into each bin (for example,
Number of pairs falling into bin
1
, Number of pairs
falling into bin
2
and so on). The second set of fea-
tures are based on percentages (for example Per-
centage of pairs falling into bin
1
, Percentage of
pairs falling into bin
2
etc.). These two sets result
in a total of 16 features. We similarly generate
16 more features for adjacent word triples. We
5
use percentages in addition to raw counts to ac-
count for the length of the response. For example,
it is possible for a long sentence to have phrases
that are awkward as well as well formed, giving
the same counts of phrases in the high-PMI value
bins as that of a short sentence that is entirely well
formed.
In addition to binning, we also encode as fea-
tures the maximum, minimum and median PMI
value obtained over all word pairs. The first two
features capture the best and the worst word col-
locations in a response. The median PMI value
captures the overall general quality of the response
in a single number. For example, if this is a low
number, then the response generally has many bad
phrasal collocations. Finally a null-PMI feature is
used to count the number of pairs that had zero
entries in the database. This feature is an indica-
tor that the given words or word collocations were
not found even once in the database. Given the
size of the underlying database, this usually hap-
pens in cases when words are misspelled, or when
the words never occur together.
All features created for bigrams are also created
for trigrams. We thus have a total of 40 features,
called the pmi feature set.
4.3.2 Features from essay scoring
A number of measures of collocation quality have
been proposed and implemented (e.g. (Futagi et
al., 2008; Dahlmeier and Ng, 2011)). We use e-
rater?s measure of the density of ?good? colloca-
tions found in the response. Another source of
difficulty for non-native writers is the selection of
appropriate prepositions. We use the mean proba-
bility assigned by e-rater to the prepositions in the
response. These two measures, one for the qual-
ity of collocations and the other for the quality of
prepositions, are combined in our colprep feature
set.
4.4 Scoring Rubric-based Features
As seen in Section 2, some of the criteria for scor-
ing are quite straightforward (e.g. ?omits one or
both key words?). While these are not sole deter-
minants of a score, they are certainly strong influ-
ences. Thus, we encode four criteria from the scor-
ing guide. These form our final feature set, rubric,
and are binary values, answering the questions: Is
the first key word from the prompt present in the
response? Is the second key word from the prompt
present in the response? Are both key words from
the prompt present in the response? Is there more
than one sentence in the response?
Table 1 provides a list of feature types and the
corresponding number of features of each type.
Feature set type Number of Features
grammar 113
relevance 7
pmi 40
colprep 2
rubric 4
Table 1: Feature sets and the counts of features in
each set
5 System and Evaluation
Figure 1: System Architecture
As noted earlier, the system is partly rule-based
and partly statistical. Figure 1 illustrates the sys-
tem architecture. The rule-based part captures
the straightforward deterministic scoring criteria
while the machine learning component encodes
features described in Section 4 and learns how to
weight the features for scoring based on human-
scored responses.
As described in Section 2, detection of condi-
tions that result in a score of zero are straight-
forward. Our rule-based scorer (shown as ?For-
eign Language Detector? in Figure 1) assigns a
zero score to a response if it is blank or non-
English. The system determines if the response is
non-English based on the average of PMI bigram
scores over the response. If the average score is
less than a threshold value, the system tags it as
6
a non-English sentence. The threshold was deter-
mined by manually inspecting the PMI values ob-
tained for sentences belonging to English and non-
English news texts. Responses given zero scores
by this module are filtered out and do not go to the
next stage.
Responses that pass the rule-based scorer are
then sent to the statistical scorer. Here, we encode
the features discussed in Section 4. Spell checking
and correction are carried out before features for
content relevance and PMI-based awkward word
usage are computed. This is done in order to pre-
vent misspellings from affecting the reference cor-
pus match or database search. The original text
is sent to the Grammar feature generator as it cre-
ates features based on misspellings and other word
form errors. Finally, we use all the features to train
a Logistic Regression model using sklearn. Note
that the statistical system predicts all 4 scores (0
through 3). This is because the rule-based system
is not perfect; that is, it might miss some responses
that should receive zero scores, and pass them over
to the next stage.
5.1 Metrics
We report our results using overall accuracy,
quadratic weighted kappa (QWK) and score-level
precision, recall and f-measure. The precision P
of the system is calculated for each score point i
as
P
i
=
|S
i
?H
i
|
|S
i
|
where |S
i
| is the number of responses given a
score of i by the system, and |S
i
?H
i
| is the num-
ber of responses given a score of i by the system
as well as the human rater.
Similarly, recall, R is calculated for each score
point i as
R
i
=
|S
i
?H
i
|
|H
i
|
F-measure F
i
is calculated as the harmonic
mean of the precision P
i
and recall R
i
at each
score point i. Accuracy is the ratio of the num-
ber of responses correctly classified over the total
number of responses.
5.2 Results
All of the responses in the train-test set were
passed through the rule-based zero-scorer. A total
of 210 responses had been scored as zero by the
human scorer. The rule-based system scored 222
responses as zeros, of which 184 were correct.
The precision P
rule
of the rule-based system is
calculated as
P
rule
0
=
184
222
= 82.9%
Similarly, Recall is calculated as
R
rule
0
=
184
210
= 87.6%
The corresponding F-measure is 85.2%
The remaining responses pass to the next stage
where machine learning is employed. We per-
formed 10 fold cross-validation experiments us-
ing Logistic Regression as well as Random Forest
learners. As the results are comparable, we only
report those from logistic regression.
Accuracy in % Agreement (QWK)
Baseline 61.00 -
System 76.23 0.63
Human 86.00 0.83
Table 2: Overall system and human accuracy
(in percentage) and agreement (using Quadratic
Weighted Kappa)
Table 2 reports the results. The system achieves
an accuracy of 76.23%, which is more than a 15
percentage point improvement over the majority
class baseline of 61%. The majority class base-
line always predicts a score of 3. Compared to hu-
man performance, system performance is 10 per-
centage points lower (human-human agreement
is 86%). Quadratic weighted kappa for system-
human agreement is also lower (0.63) than for
human-human agreement (0.83).
Table 3 reports the precision, recall and F-
measure of the system for each of the score points.
Score point Precision Recall F-measure
0 84.2 68.3 72.9
1 78.4 67.5 72.6
2 70.6 50.4 58.8
3 77.8 90.5 83.6
Table 3: Overall system performance at each score
point using all features
6 Analysis
In order to understand the usefulness of each fea-
ture set in scoring the responses, we constructed
7
systems using first the individual features alone,
and then using feature combinations. Table 4 re-
ports the accuracy of the learner using individual
features alone. We see that, individually, each fea-
ture set performs much below the performance of
the full system (that has an accuracy of 76.23%),
which is expected, as each feature set represents
a particular aspect of the construct. However, in
general, each of the feature-sets (except colprep)
shows improvement over baseline, indicating that
they contribute towards performance improvement
in the automated system.
Grammar features are the best of the individ-
ual feature sets at 70% accuracy, indicating that
grammatical error features developed for longer
texts can be applied to single sentences. The PMI-
based feature set is the second best performer, in-
dicating its effectiveness in capturing word usage
issues. While colprep and pmi both capture awk-
ward usage, pmi alone shows better performance
(67.44%) than colprep alone (61.26%). Also,
when rubric is used alone, the resulting system
produces a four percentage point improvement
over the baseline, with 65% accuracy, indicating
the presence of responses where the test-takers are
not able to incorporate one or both words in a sin-
gle sentence. The relevance feature set by itself
does not show substantial improvement over the
baseline. This is not surprising, as according to
the scoring guide, a response gets a score of 0 or 1
if it does not describe the picture, and gets a score
of 2 or 3 if it is relevant to the picture. Hence, this
feature cannot solely and accurately determine the
score.
Feature Set Accuracy in %
grammar 70.30
pmi 67.44
rubric 65.00
relevance 62.50
colprep 61.26
Table 4: System performance for individual fea-
tures
Table 5 reports accuracies of systems built us-
ing feature set combinations. The first feature set
combination, grammar + colprep, is a set of all
features obtained from essay scoring. Here we see
that addition of colprep does not improve the per-
formance over that obtained by grammar features
alone. Further, when colprep is combined with
pmi (colprep+pmi, row 2), there is a slight drop
in performance as compared to using pmi-based
features alone. These results indicate that colprep,
while being useful for larger texts, does not trans-
fer well to the simple single sentence responses in
our task.
Further, in Table 5 we see that the system using
a combination of the pmi feature set and the rele-
vance feature set (pmi+relevance) achieves an ac-
curacy of 69%. Thus, this feature combination is
able to improve performance over that using either
feature set alone, indicating that while content rel-
evance features by themselves do not create an im-
pact, they can improve performance when added
to other features. Finally, the feature combination
of all new features developed for this task (pmi +
relevance+ rubric) yields 73% accuracy, which is
again better than each individual feature set?s per-
formance, indicating that they can be synergisti-
cally combined to improve system performance.
Feature Set Accuracy in %
(i) grammar + colprep 70.31
(ii) colprep + pmi 67.42
(iii) pmi + relevance 69.05
(iv) pmi + relevance + rubric 73.21
Table 5: System performance for feature combi-
nations (i) typically used in essay scoring, (ii) that
measure awkwardness, (iii) newly proposed here,
(iv) newly proposed plus rubric-specific criteria
7 Related Work
Most work in automated scoring and learner lan-
guage analysis has focused on detecting grammar
and usage errors (Leacock et al., 2014; Dale et al.,
2012; Dale and Narroway, 2012; Gamon, 2010;
Chodorow et al., 2007; Lu, 2010). This is done
either by means of handcrafted rules or with sta-
tistical classifiers using a variety of information.
In the case of the latter, the emphasis has been on
representing the contexts of function words, such
as articles and prepositions. This work is rele-
vant inasmuch as errors in using content words,
such as nouns and verbs, are often reflected in the
functional elements which accompany them, for
example, articles that indicate the definiteness or
countability of nouns, and prepositions that mark
the cases of the arguments of verbs.
Previous work (Bergsma et al., 2009; Bergsma
et al., 2010; Xu et al., 2011) has shown that mod-
8
els which rely on large web-scale n-gram counts
can be effective for the task of context-sensitive
spelling correction. Measures of ngram associa-
tion such as PMI, log likelihood, chi-square, and
t have a long history of use for detecting colloca-
tions and measuring their quality (see (Manning
and Sch?utze, 1999) and (Leacock et al., 2014)
for reviews). Our application of a large n-gram
database and PMI is to detect inappropriate word
usage.
Our task also differs from work focusing on
evaluating content (e.g. (Meurers et al., 2011;
Sukkarieh and Blackmore, 2009; Leacock and
Chodorow, 2003)) in that, although we are look-
ing for usage of certain content words, we focus
primarily on measuring knowledge of vocabulary.
Recent work on assessment measures of depth
of vocabulary knowledge (Lawless et al., 2012;
Lawrence et al., 2012), has argued that knowl-
edge of specific words can range from superficial
(idiomatic associations built up through word co-
occurrence) to topical (meaning-related associa-
tions between words) to deep (definitional knowl-
edge). Some of our features (e.g. awkward word
usage) capture some of this information (e.g., id-
iomatic associations between words), but assign-
ing the depth of knowledge of the key words is not
the focus of our task.
Work that is closely related to ours is that of
King and Dickinson (2013). They parse picture
descriptions from interactive learner sentences,
classify sentences into syntactic types and extract
the logical subject, verb and object in order to re-
cover simple semantic representations of the de-
scriptions. We do not explicitly model the seman-
tic representations of the pictures, but rather our
goal in this work is to ascertain if a response is
relevant to the picture and to measure other fac-
tors that reflect vocabulary proficiency.
We employ human annotators and use word
similarity measures to obtain alternative forms of
description because the proprietary nature of our
data prevents us from releasing our pictures to
the public. However, crowd sourcing has been
used by other researchers to collect human labels
for images and videos. For example, Rashtchian
et al. (2010) use Amazon Mechanical Turk and
Von Ahn and Dabbish (2004) create games to en-
tice players to correctly label images. Chen and
Dolan (2011) use crowd sourcing to collect multi-
ple paraphrased descriptions of videos to create a
paraphrasing corpus.
In a vast body of related work, automated
methods have been explored for the generation
of descriptions of images (Kulkarni et al., 2013;
Kuznetsova et al., 2012; Li et al., 2011; Yao et
al., 2010; Feng and Lapata, 2010a; Feng and La-
pata, 2010b; Leong et al., 2010; Mitchell et al.,
2012). There is also work in the opposite di-
rection, of finding or generating pictures for a
given narration. Joshi et al. (2006) found the
best set of images from an image database to
match the keywords in a story. Coyne and Sproat
(2001) developed a natural language understand-
ing system which converts English text into three-
dimensional scenes that represent the text. For a
high-stakes assessment, it would be highly unde-
sirable to have any noise in the gold-standard ref-
erence picture descriptions. Hence we chose to use
manual description for creating our reference cor-
pus.
8 Summary and Future Directions
We investigated different types of features for au-
tomatically scoring a vocabulary item type which
requires the test-taker to use two words in writ-
ing a sentence based on a picture. We generated a
corpus of picture descriptions for measuring the
relevance of responses, and as a foundation for
feature development, we performed preliminary
fine-grained annotations of responses. The fea-
tures used in the resulting automated scoring sys-
tem include newly developed statistical measures
of word usage and response relevance, as well as
features that are currently found in essay scoring
engines. System performance shows an overall
accuracy in scoring that is 15 percentage points
above the majority class baseline and 10 percent-
age points below human performance.
There are a number of avenues open for future
exploration. The automated scoring system might
be improved by extending the relevance feature
to include overlap with previously collected high-
scoring responses. The reference corpus could
also be expanded and diversified by using a large
number of annotators, at least some of whom are
speakers of the languages that are most promi-
nently represented in the population of test-takers.
Finally, one particular avenue we would like to ex-
plore is the use of our features to provide feedback
in low stakes practice environments.
9
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v. 2.0. Journal of Technology,
Learning, and Assessment, 4:3.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale
n-gram data. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 865?874. Association for Computa-
tional Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. In Linguistic Data Consortium,
Philadelphia.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190?200.
Association for Computational Linguistics.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics (NAACL), pages 140?147.
Martin Chodorow, Joel R Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proceedings of the fourth ACL-
SIGSEM workshop on prepositions, pages 25?30.
Association for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. Wordseye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques, pages 487?496.
ACM.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with L1 induced
paraphrases. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 107?117, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In LREC, pages
3015?3018.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 54?62. Asso-
ciation for Computational Linguistics.
Rod Ellis. 2000. Task-based research and language
pedagogy. Language teaching research, 4(3):193?
220.
Keelan Evanini, Michael Heilman, Xinhao Wang, and
Daniel Blanchard. 2014. Automated scoring for
TOEFL Junior comprehensive writing and speaking.
Technical report, ETS, Princeton, NJ.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic
models for image annotation and text illustration.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 831?839, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michael Flor. 2013. A fast and flexible architecture for
very large word n-gram datasets. Natural Language
Engineering, 19(1):61?93.
KE Forbes-McKay and Annalena Venneri. 2005. De-
tecting subtle spontaneous language decline in early
Alzheimers disease with a picture description task.
Neurological sciences, 26(4):243?254.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21(4):353?367.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 163?171. Association for Computa-
tional Linguistics.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2013. Using Latent Dirichlet Allocation
for child narrative analysis. ACL 2013, page 111.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The
story picturing engine?a system for automatic text
illustration. ACM Trans. Multimedia Comput. Com-
mun. Appl., 2(1):68?89, February.
Levi King and Markus Dickinson. 2013. Shallow se-
mantic analysis of interactive learner sentences. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 11?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Vicente Ordonez,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. Babytalk: Under-
standing and generating simple image descriptions.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 99(PrePrints):1.
10
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 359?368. Association for
Computational Linguistics.
Ren?e Lawless, John Sabatini, and Paul Deane. 2012.
Approaches to assessing partial vocabulary knowl-
edge and supporting word learning: Assessing vo-
cabulary depth. In Annual Meeting of the Ameri-
can Educational Research Association, April 13-17,
2012, Vancouver, CA.
Joshua Lawrence, Elizabeth Pare-Blagoev, Ren?e Law-
less, and Chen Deane, Paul andLi. 2012. Gen-
eral vocabulary, academic vocabulary, and vocabu-
lary depth: Examiningpredictors of adolescent read-
ing comprehension. In Annual Meeting of the Amer-
ican Educational Research Association.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool.
Choonkyu Lee, Smaranda Muresan, and Karin
Stromswold. 2012. Computational analysis of re-
ferring expressions in narratives of picture books.
NAACL-HLT 2012, page 1.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 647?655.
Association for Computational Linguistics.
Siming Li, Girish Kulkarni, Tamara L Berg, Alexan-
der C Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768?774. Association for Compu-
tational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. Interna-
tional Journal of Corpus Linguistics, 15(4).
Christopher D. Manning and Hinrich Sch?utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M
Bailey. 2011. Integrating parallel analysis mod-
ules to evaluate the meaning of answers to reading
comprehension questions. International Journal of
Continuing Engineering Education and Life Long
Learning, 21(4):355?369.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 747?
756. Association for Computational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147. Association for Computa-
tional Linguistics.
Jana Zuheir Sukkarieh and John Blackmore. 2009.
C-rater: Automatic content scoring for short con-
structed responses. In FLAIRS Conference.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
Wei Xu, Joel Tetreault, Martin Chodorow, Ralph Gr-
ishman, and Le Zhao. 2011. Exploiting syntactic
and distributional information for spelling correc-
tion with web-scale n-gram models. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1291?1300. Asso-
ciation for Computational Linguistics.
Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proceedings of the IEEE,
98(8):1485?1508.
11
LAW VIII - The 8th Linguistic Annotation Workshop, pages 48?53,
Dublin, Ireland, August 23-24 2014.
Finding your ?inner-annotator?: An experiment in annotator 
independence for rating discourse coherence quality in essays  
 
  Jill Burstein              Swapna Somasundaran Martin Chodorow 
Educational Testing Service Educational Testing Service Hunter College, CUNY 
666 Rosedale Road  666 Rosedale Road  695 Park Avenue 
Princeton, NJ 08541  Princeton, NJ 08541  New York, NY  
 
jburstein@ets.org   ssomasundaran@ets.org   martin.chodorow@hunter.cuny.edu 
 
Abstract 
An experimental annotation method is described, showing promise for a subjective labeling task ? 
discourse coherence quality of essays.   Annotators developed personal protocols, reducing front-end 
resources: protocol development and annotator training.  Substantial inter-annotator agreement was 
achieved for a 4-point scale.  Correlational analyses revealed how unique linguistic phenomena were 
considered in annotation. Systems trained with the annotator data demonstrated utility of the data. 
 
1 Introduction1 
  
Systems designed to evaluate discourse coherence quality often use supervised methods, relying on 
human annotation that requires significant front-end resources (time and cost) for protocol 
development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon 
Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional 
means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis & Nenkova, 
2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., 
the need for automated writing evaluation  systems to score proprietary test-taker data is likely to 
increase when Common Core2 assessments are administered to school-age students beginning in 2015 
(Shermis, in press), increasing the need for data annotation. This paper describes an experimental 
method for capturing discourse coherence quality judgments for test-taker essays. Annotators 
developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard 
front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 
3), system evaluations (Section 4), and conclusions (Section 5). 
 
2 Related Work 
 
Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein & 
Wolska, 2003; Reidsma & op den Akker, 2008; Burstein et al., 2013).  Front-end annotation activities 
may require significant resources (protocol development and annotator training) (Miltsakaki and 
Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013).  Burstein et al (2013) 
reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading 
research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text 
coherence is highly personal , relying on a variety of features, including adherence to standard writing 
conventions (e.g., grammar),  and patterns of rhetorical structure and vocabulary usage.  They describe 
an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) 
applied by 2 annotators to label 1,500 test-taker essays from 6 task types (Table 1).  Protocol 
development took several weeks, and offered extensive descriptions of the 3-point scale, including 
illustrative test-taker responses; rigorous annotator training was also conducted. Burstein et al, 2013 
collapsing the 3-point scale to a 2-point scale (i.e., high (3), low (1,2)). Results for a binary discourse 
coherence quality system (high and low coherence) for essays achieved only borderline modest 
                                                          
1 This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings 
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
2
 See http://www.corestandards.org/. 
48
Essay-Writing Item Type Test-Taker Population 
1. K-12 expository  Students3, ages 11-16 
2. Expository  NNES-Univ 
3. Source-based, integrated (reading and listening)  NNES-Univ 
4. Expository  Graduate school applicants 
5. Critical argument  Graduate school applicants 
6. Professional licensing, content/expository  Certification for a business-related profession 
 
Table 1. Six item types & populations in the experimental annotation task. NNES-Univ = non-native 
English speakers, university applicants 
 
performance (?=0.41)4.  Outcomes reported in Burstein et al are consistent with discussions that text 
coherence is a complex and individual process (Graesser et al, 2004; Van den Broek, 2012), motivating 
our experimental method.  In contrast to training annotators to follow an annotation scheme pre-
determined by others, annotators devised their own scoring protocols, capturing their independent 
impressions ? finding their ?inner-annotator.?  The practical outcomes of success of the method would 
be reduced front-end resources in terms of time required to (a) develop the annotation protocol and (b) 
train annotators. As a practical end-goal, another success criterion would be to achieve inter-annotator 
agreement such that classifiers could be trained, yielding substantial annotator-system agreement. 
 
3 Experimental Annotation Study 
 
Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described. 
 
3.1 Human Annotators 
 
Two high school English teachers (employed by a company specializing in annotation) performed the 
annotation.  Annotators never met each other, did not know about each other?s activities, and only 
communicated about the annotation with a facilitator from the company. 
 
3.2 Data 
 
A random sample of 250 essays for 6 different item types (n=1500) and test-taker populations (Table 1) 
was selected. The sample was selected across 20 different prompts (test questions) for each item type in 
order to ensure topic generalizability in the resulting systems. Forty essays were randomly selected for a 
small pilot study; the remaining data (1460 essays) were used for the full annotation study. For the full 
study, 20% of the essays (n=292) had been randomly selected for double annotation to measure inter-
annotator agreement; the remaining 1168 essays were evenly divided, and each annotator labeled half 
(n=584 per annotator). Each annotator labeled a total of 876 essays across the 6 task types. 
 
3.3 Experimental Method Description 
 
A one-week pilot study was conducted. To provide some initial grounding, annotators received a 1-page 
task description that offered a high-level explanation of ?coherence? describing the end-points of a 
potential protocol.  (This description was written in about an hour.) It indicated that high coherence is 
associated with an essay that can be easily understood, and low coherence is associated with an 
incomprehensible essay.  Each annotator developed her own protocol: for each score point she wrote 
descriptive text illustrating a set of defining characteristics for each score point of coherence quality 
(e.g., ?The writer?s point is difficult to understand.?). Annotator 1 (A1) developed a 4-point scale; 
 
                                                          
3 Note that this task type was administered in an instructional setting; all other tasks were completed in high-stakes 
assessment settings. 
4
 Kappa was not reported in the paper, but was accessed through personal communication. 
49
Feature Type A1 (r) A2 (r) 
Grammar errors (e.g., subject verb agreement) 0.42 0.35 
Word usage errors (e.g., determiner errors) 0.46 0.44 
Mechanics errors (e.g.,  spelling, punctuation) 0.58 0.52 
EGT -- best 3 features (out of 112 features): F1, F2, F3 F1. -0.30 
F2. -0.28 
F3.  0.27 
F1. -0.14 
F2. -0.15 
F3.  0.11 
RST features--   best 3 features (out of 100 features): F1, F2, F3 F1. -0.27 
F2.  0.15 
F3.  0.19 
F1. -0.19 
F2.  0.08 
F3.  0.06 
LDSP 0.19 0.06 
Table 2. Pearson r between annotator discourse coherence scores and features. All correlations are 
significant at p < .0001, except for A2?s long-distance sentence-pair similarity at p < .05. 
 
Annotator 2 (A2) developed a 5-point scale. Because the two scales were different, ? could not be used 
to measure agreement, so a Spearman rank-order correlation (rS) was used, yielding a promising value 
(rS=0.82). Annotator protocols were completed at the end of the pilot study. 
 A full experiment was conducted. Each annotator used her protocol to assign a coherence quality 
score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from 
the protocol). Comments provided a score supplement that could be used to support analyses beyond 
quantitative measures (Reidsma & Carletta, 2008).  The data were annotated in 12 batches (by task) 
composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was 
computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which 
was infrequent), one of the authors reviewed the annotator scores and comments to look for 
inconsistencies.  Agreement was re-computed when annotator revisions were completed  to ensure inter-
rater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate 
annotator schedules.  While a time log was not strictly maintained, we estimate the total time for 
communication to resolve inconsistency issues was about 4-6 hours. One author communicated score-
comment inconsistencies (e.g., high score with critical comments) to the company?s facilitator (through a 
brief e-mail); the facilitator then relayed the inconsistency information to the annotator(s).  The author?s 
data review and communication e-mail took no longer than 45 minutes for the few rounds where 
agreement fell below 0.70. Communication between the facilitator and the annotator(s) involved a brief 
discussion, essentially reviewing the points made in the e-mail.  
 
3.4 Results: Inter-annotator agreement 
 
Using the Spearman rank-order correlation, inter-rater agreement on the double-annotated data was 
rS=0.71. In order to calculate Kappa statistic, A2?s 5-point scale assignments were then mapped to a 4-
point scale by collapsing the two lowest  categories (1,2) into one (1), since there were very few cases of 
1?s; this is consistent with low frequencies of very low-scoring essays. Using quadratic weighted kappa 
(QWK), post-mapping indicated substantial agreement between the two annotators (?=0.61).  
 
3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider? 
 
A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation 
drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.). 
Both annotators included descriptions such as ?word patterns,? ?logical sequencing,? and ?clarity of 
ideas?; however, A2 appeared to have more comments related to grammar and spelling.   Burstein et al., 
(2013)  describe the following features in their binary classification system: (1) grammar, word usage, 
and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid 
transition probabilities to capture local ?topic distribution? (Barzilay & Lapata, 2008) (EGT), and (4) a 
long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture 
?long distance, topical distribution. (LDSP).  Annotated data from this study were processed with the 
Burstein et al (2013) system to extract the features above in (1) ? (4).  To quantify the observed 
50
differences in the annotators? comments and potential effects for system score assignment (Section 4), 
we computed Pearson (r) correlations between the system features (on our annotated data set), and the 
discourse coherence scores of A1 and A2 (using the 4-point scale mapping for A2). There are 112 entity-
transition probability features and 100 Rhetorical Structure Theory (RST) features. In Table 2, the 
correlations of the three best predictors from the EGT and RST sets, and the GUM features and the 
LDSP feature are shown. Correlations in Table 2 are significantly correlated between the feature sets and 
annotator coherence scores. However, we observed that the EGT, RST, and LDSP feature correlation 
values for A2 are notably smaller than A1?s. This suggests that A2 may have had a strong reliance on 
GUM features, or that the system feature set did not capture all linguistic phenomena that A2 considered. 
 
4 System Evaluation5 
 
To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with 
all system features (Sys_All), and a second with the GUM features (Sys_GUM). Using 10-fold cross-
validation with a gradient boosting regression learner, four classifiers were trained to predict coherence 
quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1 
and A2 Sys_GUM systems. 
  
4.1 Results 
 
Sys-All trained with A1 data consistently outperformed Sys-All trained with A2 data. Results are 
reported for averages across the 10-folds, and  showed substantial system-human agreement for A1 (? = 
0.68) and modest system-human agreement for A2 (? = 0.55). When Sys_GUM was trained with A1 
data, system-human agreement dropped to a modest  range (? = 0.60); when Sys_GUM was trained with 
A2 data, however, human agreement was essentially unchanged, staying in the modest  agreement range 
(? = 0.50).  Consistent with the correlational analysis, this finding suggests that A2 has strong reliance 
on GUM features, or the system may have been less successful in capturing A2 features beyond GUM. 
 
5  Discussion and Conclusions 
Our experimental annotation method significantly reduced front-end resources for protocol development 
and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time 
was required from the authors or the facilitator (about two hours) for protocol development; the 
annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this 
process was report to take about one month. Approximately 4-6 hours of additional discussion from one 
author and the facilitator was required during the task; Burstein et al (2013) required two researchers and 
two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time 
across the 4 participants (personal communication). In addition to its efficiency, the experimental 
method was successful per criteria in Section 2. The method captures annotators? subjective judgments 
about coherence quality, yielding substantial inter-annotator agreement (?=0.61) across a 4-point scale.  
Second, classifiers trained with annotator data showed that the systems showed substantial and modest 
agreement (A1 and A2, respectively) ? demonstrating annotation utility, especially for A1. Correlational 
analyses were used to analyze effects of features that annotators may have considered in making their 
decisions. Comment patterns and results from the correlation analysis suggested that A2?s decisions 
were either based on narrower considerations (GUM errors), or not captured by our feature set.  
  The experimental task facilitated the successful collection of subjective coherence judgments with 
substantial inter-annotator agreement on test-taker essays. Consistent with conclusions from Reidsma & 
Carletta (2008), outcomes show that quantitative measures of inter-annotator agreement should not be 
used exclusively.  Descriptive comments were useful for monitoring during annotation, interpreting 
annotator considerations and system evaluations during and after annotation, and informing system 
development. In the future, we would explore strategies to evaluate intra-annotator reliability (Beigman-
Klebanov, Beigman, & Diermeier, 2008) which may have contributed to  lower system performance 
with A2 data. 
                                                          
5
 Many thanks to Binod Gywali for engineering support. 
51
References 
 
Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013.  Using Pivot-Based Paraphrasing and 
Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data, Transactions of the Association for 
Computational Linguistics, Vol.1: 99-110. 
 
Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In 
Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7. 
 
Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality 
in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges 
of annotating pragmatic and discourse phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber. 
Discourse & Dialogue 42, 34-52. 
 
Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use. 
In Proceedings of the 11th Conference of the European Chapter of the Association for Computational 
Linguistics. Budapest, Hungary. 
 
Jacob Cohen.  1960. "A coefficient of agreement for nominal scales". Educational and Psychological 
Measurement 20 1: 37?46.  
 
Joseph Fleiss and Jacob Cohen 1973. "The equivalence of weighted kappa and the intraclass correlation 
coefficient as measures of reliability" in Educational and Psychological Measurement, Vol. 33:613?619. 
 
Peter Foltz, Walter Kintsch, & Thomas Landuaer.  1998. Textual coherence using latent semantic analysis. 
Discourse Processes, 252&3: 285?307.  
 
Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang  Cai, Z. 2004. Coh-metrix: Analysis of text 
on cohesion and language. Behavior Research Methods, Instruments, & Computers, 36(2), 193-202. 
 
Derrick Higgins, Jill Burstein,  Daniel Marcu &. Claudia Gentile. 2004. Evaluating Multiple Aspects of 
Coherence in Student Essays. In Proceedings of 4th Annual Meeting of the Human Language Technology and 
North American Association for Computation Linguistics:185?192, Boston, MA 
 
 J. Richard Landis,.  & G. Koch. 1977. "The measurement of observer agreement for categorical 
data". Biometrics 33 1: 159?174. 
 
Annie Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Special Issue 
of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse 
phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber, 42: 87-117. 
 
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In 
Proceedings of the Language Resources and Evaluation Conference, Athens, Greece. 
 
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The 
MIT Press. 
 
Dennis Reidsma,  and  Rieks op den Akker. 2008. Exploiting `Subjective' Annotations.  In Proceedings of the 
Workshop on Human Judgments in Computational Linguistics, Coling 2008, 23 August 2008, Manchester, 
UK. 
 
Dennis Reidsma,  and  Jean Carletta. 2008. Reliability measurements without limits. Computational Linguistics, 
343: 319-336. 
 
Mark Shermis. to appear. State-of-the-art automated essay scoring: Competition, results, and future directions 
from a United States demonstration.  Assessing Writing. 
 
Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing  of 
Chinese English Speakers. The Journal of Computer Assisted Learning. 28: 396?410. 
 
52
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing 
cognitive processes and outcomes. In: Sabatini, J.P., Albro, E.R., O'Reilly, T. (Eds.), Measuring up: 
Advances in how we assess reading ability., pp. 39-58. Lanham: Rowman & Littlefield Education. 
 
 
 
53
