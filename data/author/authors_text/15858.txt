Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 550?560,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Subcat-LMF: Fleshing out a standardized format
for subcategorization frame interoperability
Judith Eckle-Kohler? and Iryna Gurevych??
? Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
? Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
This paper describes Subcat-LMF, an ISO-
LMF compliant lexicon representation for-
mat featuring a uniform representation
of subcategorization frames (SCFs) for
the two languages English and German.
Subcat-LMF is able to represent SCFs at a
very fine-grained level. We utilized Subcat-
LMF to standardize lexicons with large-
scale SCF information: the English Verb-
Net and two German lexicons, i.e., a subset
of IMSlex and GermaNet verbs. To evalu-
ate our LMF-model, we performed a cross-
lingual comparison of SCF coverage and
overlap for the standardized versions of the
English and German lexicons. The Subcat-
LMF DTD, the conversion tools and the
standardized versions of VerbNet and IMS-
lex subset are publicly available.1
1 Introduction
Computational lexicons providing accurate
lexical-syntactic information, such as subcatego-
rization frames (SCFs) are vital for many NLP
applications involving parsing and word sense
disambiguation. In parsing, SCFs have been
successfully used to improve the output of sta-
tistical parsers (Klenner (2007), Deoskar (2008),
Sigogne et al(2011)) which is particularly
significant in high-precision domain-independent
parsing. In word sense disambiguation, SCFs
have been identified as important features for
verb sense disambiguation (Brown et al 2011),
which is due to the correlation of verb senses and
SCFs (Andrew et al 2004).
SCFs specify syntactic arguments of verbs and
other predicate-like lexemes, e.g. the verb say
1http://www.ukp.tu-darmstadt.de/data/uby
takes two arguments that can be realized, for in-
stance, as noun phrase and that-clause as in He
says that the window is open.
Although a number of freely available, large-
scale and accurate SCF lexicons exist, e.g. COM-
LEX (Grishman et al 1994), VerbNet (Kipper
et al 2008) for English, availability and limita-
tions in size and coverage remain an inherent is-
sue. This applies even more to languages other
than English.
One particular approach to address this issue is
the combination and integration of existing man-
ually built SCF lexicons. Lexicon integration
has widely been adopted for increasing the cover-
age of lexicons regarding lexical-semantic infor-
mation types, such as semantic roles, selectional
restrictions, and word senses (e.g., Shi and Mi-
halcea (2005), the Semlink project2, Navigli and
Ponzetto (2010), Niemann and Gurevych (2011),
Meyer and Gurevych (2011)).
Currently, SCFs are represented idiosyncrati-
cally in existing SCF lexicons. However, inte-
gration of SCFs requires a common, interopera-
ble representation format. Monolingual SCF in-
tegration based on a common representation for-
mat has already been addressed by King and
Crouch (2005) and just recently by Necsulescu et
al. (2011) and Padro? et al(2011). However, nei-
ther King and Crouch (2005) nor Necsulescu et
al. (2011) or Padro? et al(2011) make use of ex-
isting standards in order to create a uniform SCF
representation for lexicon merging. The defini-
tion of an interoperable representation format ac-
cording to an existing standard, such as the ISO
standard Lexical Markup Framework (LMF, ISO
24613:2008, see Francopoulo et al(2006)), is the
2http://verbs.colorado.edu/semlink/
550
prerequisite for re-using this format in different
contexts, thus contributing to the standardization
and interoperability of language resources.
While LMF models exist that cover the rep-
resentation of SCFs (see Quochi et al(2008),
Buitelaar et al(2009)), their suitability for repre-
senting SCFs at a large scale remains unclear: nei-
ther of these LMF-models has been used for stan-
dardizing lexicons with a large number of SCFs,
such as VerbNet. Furthermore, the question of
their applicability to different languages has not
been investigated yet, a situation that is compli-
cated by the fact that SCFs are highly language-
specific.
The goal of this paper is to address these gaps
for the two languages English and German by pre-
senting a uniform LMF representation of SCFs
for English and German which is utilized for the
standardization of large-scale English and Ger-
man SCF lexicons. The contributions of this
paper are threefold: (1) We present the LMF
model Subcat-LMF, an LMF-compliant lexicon
representation format featuring a uniform and
very fine-grained representation of SCFs for En-
glish and German. Subcat-LMF is a subset of
Uby-LMF (Eckle-Kohler et al 2012), the LMF
model of the large integrated lexical resource Uby
(Gurevych et al 2012). (2) We convert lexicons
with large-scale SCF information to Subcat-LMF:
the English VerbNet and two German lexicons,
i.e., GermaNet (Kunze and Lemnitzer, 2002) and
a subset of IMSlex3 (Eckle-Kohler, 1999). (3) We
perform a comparison of these three lexicons re-
garding SCF coverage and SCF overlap, based on
the standardized representation.
The remainder of this paper is structured as fol-
lows: Section 2 gives a detailed description of
Subcat-LMF and section 3 demonstrates its use-
fulness for representing and cross-lingually com-
paring large-scale English and German lexicons.
Section 4 provides a discussion including related
work and section 5 concludes.
2 Subcat-LMF
2.1 ISO-LMF: a meta-model
LMF defines a meta-model of lexical resources,
covering NLP lexicons and Machine Readable
Dictionaries. This meta-model is based on the
Unified Modeling Language (UML) and speci-
3http://www.ims.uni-stuttgart.de/projekte/IMSLex/
fies a core package and a number of extensions
for modeling different types of lexicons, includ-
ing subcategorization lexicons.
The development of an LMF-compliant lexi-
con model requires two steps: in the first step,
the structure of the lexicon model has to be de-
fined by choosing a combination of the LMF core
package and zero to many extensions (i.e. UML
packages). While the LMF core package models
a lexicon in terms of lexical entries, each of which
is defined as the pairing of one to many forms and
zero to many senses, the LMF extensions provide
UML classes for different types of lexicon orga-
nization, e.g., covering the synset-based organiza-
tion of WordNet and the class-based organization
of VerbNet. The first step results in a set of UML
classes that are associated according to the UML
diagrams given in ISO LMF.
In the second step, these UML classes may be
enriched by attributes. While neither attributes
nor their values are given by the standard, the
standard states that both are to be linked to Data
Categories (DCs) defined in a Data Category Reg-
istry (DCR) such as ISOCat.4 DCs that are not
available in ISOCat may be defined and submit-
ted for standardization. The second step results in
a so-called Data Category Selection (DCS).
DCs specify the linguistic vocabulary used in
an LMF model. Consider as an example the
linguistic term direct object that often occurs in
SCFs of verbs taking an accusative NP as argu-
ment. In ISOCat, there are two different specifi-
cations of this term, one explicitly referring to the
capability of becoming the clause subject in pas-
sivization5, the other not mentioning passivization
at all.6 Consequently, the use of a DCR plays a
major role regarding the semantic interoperability
of lexicons (Ide and Pustejovsky, 2010). Different
resources that share a common definition of their
linguistic vocabulary are said to be semantically
interoperable.
2.2 Fleshing out ISO-LMF
Approach: We started our development of
Subcat-LMF with a thorough inspection of large-
scale English and German resources providing
SCFs for verbs, nouns, and adjectives. For
4http://www.isocat.org/, the implementation of the ISO
12620 DCR (Broeder et al 2010).
5http://www.isocat.org/datcat/DC-1274
6http://www.isocat.org/datcat/DC-2263
551
English, our analysis included VerbNet7 and
FrameNet syntactically annotated example sen-
tences from Ruppenhofer et al(2010). For Ger-
man, we inspected GermaNet, SALSA annota-
tion guidelines (Burchardt et al 2006) and IM-
Slex documentation (Eckle-Kohler, 1999). In ad-
dition, the EAGLES synopsis on morphosyntactic
phenomena8 (Calzolari and Monachini, 1996), as
well as the EAGLES recommendations on subcat-
egorization9 have been used to identify DCs rele-
vant for SCFs.
We specified Subcat-LMF by a DTD yielding
an XML serialization of ISO-LMF. Thus, existing
lexicons can be standardized, i.e. converted into
Subcat-LMF format, based on the DTD.10
Lexicon structure: Next, we defined the
lexicon structure of Subcat-LMF. In addition
to the core package, Subcat-LMF primarily
makes use of the LMF Syntax and Seman-
tics extension. Figure 1 shows the most
important classes of Subcat-LMF including
SynsemCorrespondence where the linking of
syntactic and semantic arguments is encoded. It
might by worth noting that both synsets from Ger-
maNet and verb classes from VerbNet can be rep-
resented in Subcat-LMF by using the Synset and
SubcategorizationFrameSet class.
Diverging linguistic properties of SCFs in
English and German: For verbs (and also for
predicate-like nouns and adjectives), SCFs spec-
ify the syntactic and morphosyntactic properties
of their arguments that have to be present in con-
crete realizations of these arguments within a sen-
tence. While some properties of syntactic argu-
ments in English and German correspond (both
English and German are Germanic languages and
hence closely related), there are other properties,
mainly morphosyntactic ones that diverge. By
way of examples, we illustrate some of these di-
vergences in the following (we contrast English
examples with their German equivalents):
? overt case marking in German:
He helps him. vs. Er hilft ihm. (dative)
? specific verb form in verb phrase arguments:
He suggested cleaning the house. (ing-form)
7SCFs in VerbNet al cover SCFs in VALEX, a lexicon
automatically extracted from corpora.
8http://www.ilc.cnr.it/EAGLES96/morphsyn/
9http://www.ilc.cnr.it/EAGLES96/synlex/
10Available at http://www.ukp.tu-darmstadt.de/data/uby
vs.
Er schlug vor, das Haus zu putzen. (to-
infinitive)
? morphosyntactic marking of verb phrase ar-
guments in the main clause: He managed to
win. (no marking) vs.
Er hat es geschafft zu gewinnen. (obligatory
es)
? morphosyntactic marking of clausal argu-
ments in the main clause: That depends on
who did it. (preposition) vs.
Das ha?ngt davon ab, wer es getan hat.
(pronominal adverb)
Uniform Data Categories for English and Ger-
man: Thus, the main challenge in developing
Subcat-LMF has been the specification of DCs
(attributes and attribute values) in such a way,
that a uniform specification of SCFs in the two
languages English and German can be achieved.
The specification of DCs for Subcat-LMF in-
volved fleshing out ISO-LMF, because it is a
meta-standard in the sense that it provides only
few linguistic terms, i.e. DCs, and these DCs
are not linked to any DCR: in the Syntax Exten-
sion, the standard only provides 7 class names,
see Figure 1), complemented by 17 example at-
tributes given in an informative, non-binding An-
nex F. These are by far not sufficient to repre-
sent the fine-grained SCFs available in such large-
scale lexicons as VerbNet.
In contrast, the Syntax part of Subcat-LMF
comprises 58 DCs that are properly linked to
ISOCat DCs; a number of DCs were missing in
ISOCat, so we entered them ourselves.11 The
majority of the attributes in Subcat-LMF are at-
tached to the SyntacticArgument class. The
corresponding DCs can be divided into two main
groups:
Cross-lingually valid DCs for the spec-
ification of grammatical functions (e.g.
subject, prepositionalComplement)
and syntactic categories (e.g. nounPhrase,
prepositionalPhrase), see Table 1.
Partly language-specific morphosyntactic
DCs that further specify the syntactic arguments
(e.g. attribute case, attribute verbForm and
11The Subcat-LMF DCS is publicly available on the ISO-
Cat website.
552
Figure 1: Selected classes of Subcat-LMF.
Values of grammaticalFunction Example
subject They arrived in time.
subjectComplement He becomes a teacher.
directObject He saw a rainbow.
objectComplement They elected him governor.
complement He told him a story.
prepositionalComplement It depends on several factors.
adverbialComplement They moved far away.
Values of syntacticCategory Example
nounPhrase The train stopped.
reflexive He drank himself sick.
expletive It is raining.
prepositionalPhrase It depends on several factors.
adverbPhrase They moved far away.
adjectivePhrase The light turned red.
verbPhrase She tried to exercise.
declarativeClause He says he agrees.
subordinateClause He believes that it works.
Table 1: Cross-lingually valid (English-German) attributes and values of the SyntacticArgument class.
values toInfinitive, bareInfinitive,
ingForm, participle), see Table 2.
In the class LexemeProperty, we introduced
an attribute syntacticProperty to encode
control and raising properties of verbs taking in-
finitival verb phrase arguments.12
In Subcat-LMF, syntactic arguments can be
specified by a selection of appropriate attribute-
value pairs. While all syntactic arguments are uni-
formly specified by a grammatical function and a
syntactic category, the use of the morphosyntactic
attributes depends on the particular type of syn-
tactic argument. Different phrase types are spec-
12Control or raising specify the co-reference between the
implicit subject of the infinitival argument and syntactic ar-
guments in the main clause, either the subject (subject con-
trol or raising) or direct object (object control or raising).
ified by different subsets of morphosyntactic at-
tributes, see Table 2. The following examples il-
lustrate some of these attributes:
? number: the number of a noun phrase argu-
ment can be lexically governed by the verb
as in These types of fish mix well together.
? verbForm: the verb form of a clausal com-
plement can be required to be a bare infini-
tive as in They demanded that he be there.
? tense: not only the verb form, but also the
tense of a verb phrase complement can be
lexically governed, e.g., to be a participle in
the past tense as in They had it removed.
553
Morphosyntactic attributes and values NP PP VP C
case: nominative, genitive, dative, accusative x x
determiner: possessive, indefinite x x
number: singular, plural x
verbForm: toInfinitive, bareInfinitive, ingForm(!), Participle x x
tense: present, past x
complementizer: thatType, whType, yesNoType x
prepositionType: external ontological type, e.g. locative x x x
preposition: (string) (!) x x x
lexeme: (string) (!) x x
Table 2: Morphosyntactic attributes of SyntacticArgument and phrase types for which the attributes are
appropriate (NP: noun phrase, PP: prepositional phrase, VP: verb phrase, C: clause). Language-specific attributes
are marked by (!).
3 Utilizing Subcat-LMF
3.1 Standardizing large-scale lexicons
Lexicon Data: We converted VerbNet (VN) and
two German lexicons, i.e., GermaNet (GN) and
a subset of IMSlex (ILS) to Subcat-LMF format.
ILS has been developed independently from GN
and the lexicon data were published in Eckle-
Kohler (1999).
VN is organized in verb classes based on Levin-
style syntactic alternations (Levin, 1993): verbs
with common SCFs and syntactic alternation be-
havior that also share common semantic roles are
grouped into classes. VN (version 3.1) lists 568
frames that are encoded as phrase structure rules
(XML element SYNTAX), specifying phrase types
and semantic roles of the arguments, as well as se-
lectional, syntactic and morphosyntactic restric-
tions on the arguments. Additionally, a descrip-
tive specification of each frame is given (XML
element DESCRIPTION). The verb learn, for in-
stance, has the following VN frame:
DESCRIPTION (primary): NP V NP
SYNTAX: Agent V Topic
We extracted both the descriptive specifications
and the phrase structure rules, using the API
available for VN13, resulting in 682 unique VN
frames.14
GN provides detailed SCFs for verbs, in
contrast to the Princeton WordNet: GN version
6.0 from April 2011 accessed by the GN API15
lists 202 frames. GN SCFs are represented as a
13http://verbs.colorado.edu/verb-index/inspector/
14The VN API was used with the view options wrexyzsq
for verb frame pairs and ctuqw for verb class information.
15GermaNet Java API 2.0.2
dot-separated sequence of letter pairs. Each letter
pair specifies a syntactic argument: the first letter
encodes the grammatical function and the second
letter the syntactic category.16 For instance, the
following shows the GN code for transitive verbs:
NN.AN.
ILS is represented in delimiter-separated
values format and contains 784 verbs in total.
Of these 784 verbs, 740 of them are also present
in GN, and 44 are listed in ILS only. Although
ILS contains only verbs that take clausal ar-
guments and verb phrase arguments, a total
number of 220 SCFs is present in ILS, also
including SCFs without clausal and verb phrase
arguments. ILS lists for each verb lemma a
number of SCFs, thus specifying coarse-grained
verb senses given by a lemma-SCF pair.17 The
SCFs are represented as parenthesized lists. For
instance, the ILS SCF for transitive verbs is:
(subj(NPnom),obj(NPacc)).
Automatic Conversion: We implemented Java
tools for the conversion of VN, GN and ILS to
Subcat-LMF. These tools convert the source lexi-
cons based on a manual mapping of lexicon units
and terms (e.g., VN verb class, GN synset) to
Subcat-LMF. For the majority of SCFs, this map-
ping is defined on argument level. Lexical data
is extracted from the source lexicons by using the
native APIs (VN, GN) and additional Perl scripts.
16See http://www.sfs.uni-tuebingen.de/GermaNet/-
verb frames.shtml
17In addition, ILS provides a semantic class label for each
verb; however, these semantic labels are attached at lemma
level, i.e. they need to be disambiguated.
554
# LexicalEntry # Sense # Subcat.Frame # SemanticPred.
LMF-VN 3962 31891 284 617
orig. VN (3962 verbs) (31891 groups of verb,
frame, sem.pred.)
(568 frames) (572 sem. Pred.)
LMF-GN 8626 12981 147 84
orig. GN (8626 verbs) (12981 verb-synset pairs) (202 GN frames) (no sem. Pred.)
LMF-ILS 784 3675 217 10
orig. ILS (784 verbs) (3675 verb-frame pairs) (220 SCFs) (no sem. Pred.)
Table 3: Evaluation of the automatic conversion. Numbers of Subcat-LMF instances in the converted lexicons
compared to numbers of corresponding units in original lexicons.
Evaluation of Automatic Conversion: Table 3
shows the mapping of the major source lexicon
units (such as verb-synset pairs) to Subcat-LMF
and lists the corresponding numbers of units.
For VN, groups of VN verb, frame and se-
mantic predicate have been mapped to LMF
senses. VN classes have been mapped to
SubcategorizationFrameSet. Thus, the
original VN-sense, a pairing of verb lemma and
class, can be recovered by grouping LMF senses
that share the same verb class. There is a signif-
icant difference between the original VN frames
and their Subcat-LMF representation: the seman-
tic information present in VN frames (seman-
tic roles and selectional restrictions) is mapped
to semantic arguments in Subcat-LMF, i.e. the
mapping splits VN frames into a purely syntac-
tic and a purely semantic part. Consequently,
the number of unique SCFs in the Subcat-LMF
version of VN is much smaller than the num-
ber of frames in the original VN. The conversion
tool creates for each sense (specifying a unique
verb, frame, semantic predicate combination) a
SynSemCorrespondence.
On the other hand, the Subcat-LMF version of VN
contains more semantic predicates than VN. This
is due to selectional restrictions for semantic ar-
guments that are specified in Subcat-LMF within
semantic predicates, in contrast to VN.
For GN, verb-synset pairs (i.e., GN lexical
units), have been mapped to LMF senses. Few
GN frame codes also specify semantic role in-
formation, e.g. manner, location. These were
mapped to the semantics part of Subcat-LMF re-
sulting in 84 semantic predicates that encode the
semantic role information in their semantic argu-
ments.
ILS specifies similar semantic role information
as GN; these few cases were mapped in the same
way as for GN. Therefore, the LMF version of
ILS, too, specifies less SCFs, but additional se-
mantic predicates not present in the original.
Discussion: Grammatical functions of argu-
ments are specified distinctly in the three lexicons.
While both GN and ILS specify grammatical
functions, they are not explicitly encoded in VN.
They have to be inferred on the basis of the phrase
structure rules given in the SYNTAX element. We
assigned subject to the noun phrase which di-
rectly precedes the verb and directObject to
the noun phrase directly following the verb and
having the semantic role Patient. The semantic
role information has to be considered at this point,
because not all noun phrase arguments are able
to become the subject in a corresponding passive
sentence. An example is the verb learn which
has the VN frame NP(Agent) V NP(Topic);
here, the Topic-NP is not able to become the sub-
ject of a corresponding passive sentence. We as-
signed the grammatical function complement to
all other phrase types.
Argument order constraints in SCFs are repre-
sented in LMF by a list implementation of syntac-
tic arguments. Most SCFs from VN require the
subject to be the first argument, reflecting the ba-
sic word order in English sentences. VN lists one
exception to this rule for the verb appear, illus-
trated by the example On the horizon appears a
ship.
Argument optionality in VN is expressed at the
semantic level and at the syntactic level in paral-
lel: it is explicitly specified at the semantic level
and implicitly specified at the syntactic level. At
the syntactic level, two SCF versions exist in VN,
one with the optional argument, the other without
it. In addition, the semantic predicate attached to
555
these SCFs marks optional (semantic) arguments
by a ?-sign. GN, on the other hand, expresses
argument optionality at the level of syntactic ar-
guments, i.e., within the frame code. In Subcat-
LMF, optionality is represented at the syntactic
level by an (optional) attribute optional for syn-
tactic arguments, thus reflecting the explicit repre-
sentation used in GN and the implicit representa-
tion present in VN.18
GN frames specify syntactic alternations of ar-
gument realizations, e.g. adverbial complements
that can alternatively be realized as adverb phrase,
prepositional phrase or noun phrase. We encoded
this generalization in Subcat-LMF by introducing
attribute values for these aggregated syntactic cat-
egories.
3.2 Cross-lingual comparison of lexicons
Lexicons that are standardized according to
Subcat-LMF can be quantitatively compared re-
garding SCFs. For two lexicons, such a com-
parison gives answers to questions, such as: how
many SCFs are present in both lexicons (overlap-
ping SCFs), how many SCFs are only listed in one
of the lexicons (complementary SCFs). Answers
to these questions are important, for instance, for
assessing the potential gain in SCF coverage that
can be achieved by lexicon merging.
In order to validate our claim that Subcat-LMF
yields a cross-lingually uniform SCF represen-
tation, we contrast the monolingual comparison
of GN and ILS with the cross-lingual compari-
son of VN, GN and VN and ILS. Assuming that
our claim is valid, the cross-lingual comparisons
can be expected to yield similar results regard-
ing overlapping and complementary SCFs as the
monolingual comparison.
Comparison: The comparison of SCFs from
two lexicons that are in Subcat-LMF format can
be performed on the basis of the uniform DCs.
As Subcat-LMF is implemented in XML, we
compared string representations of SCFs. SCFs
from VN, GN and ILS were converted to strings
by concatenating attribute values of syntactic ar-
guments and lexemeProperty. We created
string representations of different granularities:
First, fine-grained, language-specific string SCFs
have been generated by concatenating all at-
18As a consequence, all semantic arguments specified in
the Subcat-LMF version of VN have a corresponding syn-
tactic argument.
tribute values apart from the attribute optional
which is specific to GN (resulting in a consid-
erably smaller number of SCFs in GN). Sec-
ond, fine-grained, but cross-lingual string SCFs
were considered; these omit the attributes case,
lexeme, preposition and the attribute value
ingForm. Finally, coarse-grained cross-lingual
string SCFs were compared. These only con-
tain the values of the attributes syntactic
category, complementizer and verbForm
(without the attribute value ingForm). For in-
stance, a coarse cross-lingual string SCF for tran-
sitive verbs is nounPhrasenounPhrase.
Table 4 lists the results of our quantitative com-
parison. For each lexicon pair, the number of
overlapping SCFs and the numbers of comple-
mentary SCFs are given. Regarding VN and the
German lexicons, the overlap at the language-
specific level is (close to) zero, which is due to the
specification of case, e.g. dative, for German ar-
guments. However, the numbers for cross-lingual
SCFs clearly validate our claim: the numbers of
overlapping SCFs for the German lexicon pair and
for the two German-English pairs are comparable,
ranging from 12 to 18 for the fine-grained SCFs
and from 20 to 21 for the coarse SCFs.
Based on the sets of cross-lingually overlap-
ping SCFs, we made an estimation on how many
high frequent verbs actually have SCFs that are
in the cross-lingual SCF overlap of an English-
German lexicon pair. For this, we used the lemma
frequency lists of the English and German WaCky
corpora (Baroni et al 2009) and extracted verbs
from VN, GN and ILS that are on 100 top ranked
positions of these lists, starting from rank 100.19
Table 5 shows the results for the cross-lingual
SCF overlap between VN ? GN and between VN
? ILS. While only around 40% of the high fre-
quent verbs have an SCF in the fine-grained SCF
overlap, more than 70% are in the coarse overlap
between VN ? GN, and even more than 80% in
the coarse overlap between VN ? ILS.
Analysis of results: The small numbers of
overlapping cross-lingual SCFs (relative to the to-
tal number of SCFs), at both levels of granularity,
indicate that the three lexicons each encode sub-
stantially different lexical-syntactic properties of
19Since the WaCky frequency lists do not contain POS in-
formation, our lists of extracted verbs contain some noise,
which we tolerated, because we aimed at an approximate es-
timate.
556
language-specific cross-lingual cross-lingual
(fine-grained) (fine-grained) (coarse)
GN vs. ILS 72 GN 21 both, 196 ILS 61 GN, 23 both, 69 ILS 40 GN, 24 both, 23 ILS
VN vs. GN 284 VN, 0 both, 93 GN 96 VN, 15 both, 69 GN 29 VN, 24 both, 40 GN
VN vs. ILS 283 VN, 1 both, 216 ILS 93 VN, 18 both, 74 ILS 31 VN, 22 both, 25 ILS
Table 4: Comparison of lexicon pairs regarding SCF overlap and complementary SCFs.
VN-GN overlap VN-GN overlap VN-ILS overlap VN-ILS overlap
fine-grained (15 SCFs) coarse (24 SCFs) fine-grained (18 SCFs) coarse (22 SCFs)
43% VN verbs 85% VN verbs 41% VN verbs 84% VN verbs
41% GN verbs 71% GN verbs 43% ILS verbs 87% ILS verbs
Table 5: Percentage of 100 high frequent verbs from VN, GN, ILS with a SCF in the cross-lingual SCF overlap
(fine-grained vs. coarse) between VN ? GN and VN ? ILS.
verbs. This can at least partly be explained by the
historic development of these lexicons in differ-
ent contexts, e.g., Levin?s work on verb classes
(VN), Lexical Functional Grammar (ILS), as well
as their use for different purposes and applica-
tions.
Another reason of the small SCF overlap is
the comparison of strings derived from the XML
format. A more sophisticated representation for-
mat, notably one that provides semantic typing
and type hierarchies, e.g., OWL, could be em-
ployed to define hierarchies of grammatical func-
tions (e.g. direct object would be a sub-type of
complement) and other attributes. These would
presumably support the identification of further
overlapping SCFs.
During a subsequent qualitative analysis of the
overlapping and complementary SCFs, we col-
lected some enlightening background informa-
tion. Overlapping SCFs in the cross-lingual com-
parison (both fine-grained and coarse) include
prominent SCFs corresponding to transitive and
intransitive verbs, as well as verbs with that-
clause and verbs with to-infinitive.
GN and ILS are highly complementary regard-
ing SCFs: for instance, while many SCFs with ad-
verbial arguments are unique in GN, only ILS pro-
vides a fine-grained specification of prepositional
complements including the preposition, as well
as the case the preposition requires.20 VN, too,
contains a large number of SCFs with a detailed
specification of possible prepositions, partly spec-
20In German, prepositions govern the case of their noun
phrase.
ified as language-independent preposition types.
A large number of complementary SCFs in VN
vs. GN and GN vs. ILS are due to a diverging lin-
guistic analysis of extraposed subject clauses with
an es (it) in the main clause (e.g., It annoys him
that the train is late.). In GN, such clauses are not
specified as subject, whereas in VN and ILS they
are.
Regarding VN and ILS, only VN lists subject
control for verbs, while both VN and ILS list ob-
ject control and subject raising. GN, on the other
hand, does not specify control or raising at all.
4 Discussion
4.1 Previous Work
Merging SCFs: Previous work on merging SCF
lexicons has only been performed in a mono-
lingual setting and lacks the use of standards.
King and Crouch (2005) describe the process of
unifying several large-scale verb lexicons for En-
glish, including VN and WordNet. They perform
a conversion of these lexicons into a uniform, but
non-standard representation format, resulting in a
lexicon which is integrated at the level of verb
senses, SCFs and lexical-semantics. Thus, the re-
sult of their work is not applicable to cross-lingual
settings.
Necsulescu et al(2011) and Padro? et al(2011)
report on approaches to automatic merging of
two Spanish SCF lexicons. As these lexicons
lack sense information apart from the SCFs, their
merging approach only works on a very coarse-
grained sense level given by lemma-SCF pairs.
The fully automatic merging approach described
557
in (Padro? et al 2011) assumes that one of the lex-
icons to be integrated is already represented in the
target representation format, i.e. given two lexi-
cons, they map one lexicon to the format of the
other. Moreover, their approach requires a signif-
icant overlap of SCFs and verbs in any two lex-
icons to be merged. The authors state that it is
presently unclear, how much overlap is required
to obtain sufficiently precise merging results.
Standardizing SCFs: Much previous work on
standardizing NLP lexicons in LMF has focused
on WordNet-like resources. Soria et al(2009) de-
scribe WordNet-LMF, an LMF model for repre-
senting wordnets which has been used in the KY-
OTO project.21 Later, WordNet-LMF has been
adapted by Henrich and Hinrichs (2010) to Ger-
maNet and by Toral et al(2010) to the Ital-
ian WordNet. WordNet-LMF does not provide
the possibility to represent subcategorization at
all. The adaption of WordNet-LMF to GN (Hen-
rich and Hinrichs, 2010) allows SCFs to be re-
spresented as string values. However, this ex-
tension is not sufficient, because it provides no
means to model the syntax-semantics interface,
which specifies correspondences between syntac-
tic and semantic arguments of verbs and other
predicates. Quochi et al(2008) report on an LMF
model that covers the syntax-semantics mapping
just mentioned; it has been used for standardizing
an Italian domain-specific lexicon. Buitelaar et al
(2009) describe LexInfo, an LMF-model that is
used for lexicalizing ontologies. LexInfo is imple-
mented in OWL and specifies a linking of syntac-
tic and semantic arguments. For SCFs and argu-
ments, a type hierarchy is defined. In their paper,
Buitelaar et al(2009) show only few SCFs and
do not indicate what kinds of SCFs can be repre-
sented with LexInfo in principle. On the LexInfo
website22, the current LexInfo version 2.0 can be
viewed, but no further documentation is given.
We inspected LexInfo version 2.0 and found that
it specifies a large number of fine-grained SCFs.
However, LexInfo has not been evaluated so far
on large-scale SCF lexicons, such as VerbNet.
4.2 Subcat-LMF
Subcat-LMF enables the uniform representation
of fine-grained SCFs across the two languages
English and German. By mapping large-scale
21http://www.kyoto-project.eu/
22See http://lexinfo.net/
SCF lexicons to Subcat-LMF, we have demon-
strated its usability for uniformly representing a
wide range of SCFs and other lexical-syntactic in-
formation types in English and German.
As our cross-lingual comparison of lexicons
has revealed many complementary SCFs in VN,
GN and ILS, mono- and cross-lingual alignments
of these lexicons at sense level would lead to a
major increase in SCF coverage. Moreover, the
cross-lingually uniform representation of SCFs
can be exploited for an additional alignment of
the lexicons at the level of SCF arguments. Such
a fine-grained alignment of SCFs can be used, for
instance, to project VN semantic roles to GN, thus
yielding a German resource for semantic role la-
beling (see Gildea and Jurafsky (2002), Swier and
Stevenson (2005)).
Subcat-LMF could be used for standardizing
further English and German lexicons. The auto-
matic conversion of lexicons to Subcat-LMF re-
quires the manual definition of a mapping, at least
for syntactic arguments. Furthermore, the auto-
matic merging approach by Padro? et al(2011)
could be tested for English: given our standard-
ized version of VN, other English SCF lexicons
could be merged fully automatically with the
Subcat-LMF version of VN.
5 Conclusion
Subcat-LMF contributes to fostering the standard-
ization of language resources and their interop-
erability at the lexical-syntactic level across En-
glish and German. The Subcat-LMF DTD in-
cluding links to ISOCat, all conversion tools,
and the standardized versions of VN and
ILS23 are publicly available at http://www.ukp.tu-
darmstadt.de/data/uby.
Acknowledgments
This work has been supported by the Volks-
wagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We thank the anonymous reviewers for their valu-
able comments. We also thank Dr. Jungi Kim
and Christian M. Meyer for their contributions to
this paper, and Yevgen Chebotar and Zijad Mak-
suti for their contributions to the conversion soft-
ware.
23The converted version of GN can not be made available
due to licensing.
558
References
Galen Andrew, Trond Grenager, and Christopher D.
Manning. 2004. Verb sense and subcategoriza-
tion: using joint inference to improve performance
on complementary tasks. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 150?157,
Barcelona, Spain.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
a collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Daan Broeder, Marc Kemps-Snijders, Dieter Van Uyt-
vanck, Menzo Windhouwer, Peter Withers, Peter
Wittenburg, and Claus Zinn. 2010. A Data Cat-
egory Registry- and Component-based Metadata
Framework. In Proceedings of the Seventh Inter-
national Conference on Language Resources and
Evaluation (LREC), pages 43?47, Valletta, Malta.
Susan Windisch Brown, Dmitriy Dligach, and Martha
Palmer. 2011. VerbNet Class Assignment as a
WSD Task. In Proceedings of the 9th International
Conference on Computational Semantics (IWCS),
pages 85?94, Oxford, UK.
Paul Buitelaar, Philipp Cimiano, Peter Haase, and
Michael Sintek. 2009. Towards Linguistically
Grounded Ontologies. In Lora Aroyo, Paolo
Traverso, Fabio Ciravegna, Philipp Cimiano, Tom
Heath, Eero Hyvo?nen, Riichiro Mizoguchi, Eyal
Oren, Marta Sabou, and Elena Simperl, editors, The
Semantic Web: Research and Applications, pages
111?125, Berlin Heidelberg. Springer-Verlag.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal.
2006. The SALSA Corpus: a German Corpus Re-
source for Lexical Semantics. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC), pages 969?974,
Genoa, Italy.
Nicoletta Calzolari and Monica Monachini. 1996.
EAGLES Proposal for Morphosyntactic Stan-
dards: in view of a ready-to-use package. In
G. Perissinotto, editor, Research in Humanities
Computing, volume 5, pages 48?64. Oxford Uni-
versity Press, Oxford, UK.
Tejaswini Deoskar. 2008. Re-estimation of lexi-
cal parameters for treebank PCFGs. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING), pages 193?200,
Manchester, United Kingdom.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-
mann, Michael Matuschek, and Christian M.
Meyer. 2012. UBY-LMF ? A Uniform Format
for Standardizing Heterogeneous Lexical-Semantic
Resources in ISO-LMF. In Proceedings of the 8th
International Conference on Language Resources
and Evaluation (LREC 2012), page (to appear), Is-
tanbul, Turkey.
Judith Eckle-Kohler. 1999. Linguistisches Wissen zur
automatischen Lexikon-Akquisition aus deutschen
Textcorpora. Logos-Verlag, Berlin, Germany.
PhDThesis.
Gil Francopoulo, Nuria Bel, Monte George, Nico-
letta Calzolari, Monica Monachini, Mandy Pet, and
Claudia Soria. 2006. Lexical Markup Framework
(LMF). In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 233?236, Genoa, Italy.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288, September.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. 1994. Comlex Syntax: Building a Computa-
tional Lexicon. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING), pages 268?272, Kyoto, Japan.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer,
and Christian Wirth. 2012. Uby - A Large-Scale
Unified Lexical-Semantic Resource. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2012), page (to appear), Avignon, France.
Verena Henrich and Erhard Hinrichs. 2010. Standard-
izing wordnets in the ISO standard LMF: Wordnet-
LMF for GermaNet. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING), pages 456?464, Beijing, China.
Nancy Ide and James Pustejovsky. 2010. What Does
Interoperability Mean, anyway? Toward an Op-
erational Definition of Interoperability. In Pro-
ceedings of the Second International Conference
on Global Interoperability for Language Resources,
Hong Kong.
Tracy Holloway King and Dick Crouch. 2005. Uni-
fying lexical resources. In Proceedings of the In-
terdisciplinary Workshop on the Identification and
Representation of Verb Features and Verb Classes,
Saarbruecken, Germany.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. Language Resources and Evalu-
ation, 42:21?40.
Manfred Klenner. 2007. Shallow dependency la-
beling. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), Companion Volume Proceedings of the
Demo and Poster Sessions, pages 201?204, Prague,
Czech Republic.
Claudia Kunze and Lothar Lemnitzer. 2002. Ger-
maNet ? representation, visualization, applica-
tion. In Proceedings of the Third International
Conference on Language Resources and Evaluation
559
(LREC), pages 1485?1491, Las Palmas, Canary Is-
lands, Spain.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago,
USA.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Align-
ing Wiktionary and WordNet for Increased Domain
Coverage. In Proceedings of the 5th International
Joint Conference on Natural Language Processing
(IJCNLP), pages 883?892, Chiang Mai, Thailand.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 216?225, Uppsala, Sweden.
Silvia Necsulescu, Nu?ria Bel, Munsta Padro?, Montser-
rat Marimon, and Eva Revilla. 2011. Towards
the Automatic Merging of Language Resources. In
Proceedings of the 2011 ESSLI Workshop on Lexi-
cal Resources (WoLeR 2011), Ljubljana, Slovenia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
People?s Web meets Linguistic Knowledge: Auto-
matic Sense Alignment of Wikipedia and WordNet.
In Proceedings of the 9th International Conference
on Computational Semantics (IWCS), pages 205?
214, Oxford, UK.
Muntsa Padro?, Nu?ria Bel, and Silvia Necsulescu.
2011. Towards the Automatic Merging of Lexical
Resources: Automatic Mapping. In Proceedings of
the International Conference on Recent Advances
in Natural Language Processing, pages 296?301,
Hissar, Bulgaria.
Valeria Quochi, Monica Monachini, Riccardo Del
Gratta, and Nicoletta Calzolari. 2008. A lexicon
for biology and bioinformatics: the bootstrep expe-
rience. In Proceedings of the Sixth International
Conference on Language Resources and Evalua-
tion (LREC?08), pages 2285?2292, Marrakech, Mo-
rocco, may.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice, September.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings
of the Sixth International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing), pages 100?111, Mexico City, Mexico.
Anthony Sigogne, Matthieu Constant, and E?ric La-
porte. 2011. Integration of data from a syntac-
tic lexicon into generative and discriminative proba-
bilistic parsers. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing, pages 363?370, Hissar, Bulgaria.
Claudia Soria, Monica Monachini, and Piek Vossen.
2009. Wordnet-LMF: fleshing out a standardized
format for Wordnet interoperability. In Proceedings
of the 2009 International Workshop on Intercultural
Collaboration, pages 139?146, Palo Alto, Califor-
nia, USA.
Robert S. Swier and Suzanne Stevenson. 2005. Ex-
ploiting a verb lexicon in automatic semantic role
labelling. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing (HLT?05), pages
883?890, Vancouver, British Columbia, Canada.
Antonio Toral, Stefania Bracale, Monica Monachini,
and Claudia Soria. 2010. Rejuvenating the Italian
WordNet: upgrading, standarising, extending. In
Proceedings of the 5th Global WordNet Conference,
Bombay, India.
560
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580?590,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
UBY ? A Large-Scale Unified Lexical-Semantic Resource
Based on LMF
Iryna Gurevych??, Judith Eckle-Kohler?, Silvana Hartmann?, Michael Matuschek?,
Christian M. Meyer? and Christian Wirth?
? Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
? Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
We present UBY, a large-scale lexical-
semantic resource combining a wide range
of information from expert-constructed
and collaboratively constructed resources
for English and German. It currently
contains nine resources in two lan-
guages: English WordNet, Wiktionary,
Wikipedia, FrameNet and VerbNet,
German Wikipedia, Wiktionary and
GermaNet, and multilingual OmegaWiki
modeled according to the LMF standard.
For FrameNet, VerbNet and all collabora-
tively constructed resources, this is done
for the first time. Our LMF model captures
lexical information at a fine-grained level
by employing a large number of Data
Categories from ISOCat and is designed
to be directly extensible by new languages
and resources. All resources in UBY can
be accessed with an easy to use publicly
available API.
1 Introduction
Lexical-semantic resources (LSRs) are the foun-
dation of many NLP tasks such as word sense
disambiguation, semantic role labeling, question
answering and information extraction. They are
needed on a large scale in different languages.
The growing demand for resources is met nei-
ther by the largest single expert-constructed re-
sources (ECRs), such as WordNet and FrameNet,
whose coverage is limited, nor by collaboratively
constructed resources (CCRs), such as Wikipedia
and Wiktionary, which encode lexical-semantic
knowledge in a less systematic form than ECRs,
because they are lacking expert supervision.
Previously, there have been several indepen-
dent efforts of combining existing LSRs to en-
hance their coverage w.r.t. their breadth and depth,
i.e. (i) the number of lexical items, and (ii) the
types of lexical-semantic information contained
(Shi and Mihalcea, 2005; Johansson and Nugues,
2007; Navigli and Ponzetto, 2010b; Meyer and
Gurevych, 2011). As these efforts often targeted
particular applications, they focused on aligning
selected, specialized information types. To our
knowledge, no single work focused on modeling
a wide range of ECRs and CCRs in multiple lan-
guages and a large variety of information types in
a standardized format. Frequently, the presented
model is not easily scalable to accommodate an
open set of LSRs in multiple languages and the in-
formation mined automatically from corpora. The
previous work also lacked the aspects of lexicon
format standardization and API access. We be-
lieve that easy access to information in LSRs is
crucial in terms of their acceptance and broad ap-
plicability in NLP.
In this paper, we propose a solution to this. We
define a standardized format for modeling LSRs.
This is a prerequisite for resource interoperabil-
ity and the smooth integration of resources. We
employ the ISO standard Lexical Markup Frame-
work (LMF: ISO 24613:2008), a metamodel for
LSRs (Francopoulo et al 2006), and Data Cate-
gories (DCs) selected from ISOCat.1 One of the
main challenges of our work is to develop a model
that is standard-compliant, yet able to express the
information contained in diverse LSRs, and that in
the long term supports the integration of the vari-
ous resources.
The main contributions of this paper can be
1http://www.isocat.org/
580
summarized as follows: (1) We present an LMF-
based model for large-scale multilingual LSRs
called UBY-LMF. We model the lexical-semantic
information down to a fine-grained level of in-
formation (e.g. syntactic frames) and employ
standardized definitions of linguistic information
types from ISOCat. (2) We present UBY, a large-
scale LSR implementing the UBY-LMF model.
UBY currently contains nine resources in two
languages: English WordNet (WN, Fellbaum
(1998), Wiktionary2 (WKT-en), Wikipedia3 (WP-
en), FrameNet (FN, Baker et al(1998)), and
VerbNet (VN, Kipper et al(2008)); German Wik-
tionary (WKT-de), Wikipedia (WP-de), and Ger-
maNet (GN, Kunze and Lemnitzer (2002)), and
the English and German entries of OmegaWiki4
(OW), referred to as OW-en and OW-de. OW,
a novel CCR, is inherently multilingual ? its ba-
sic structure are multilingual synsets, which are a
valuable addition to our multilingual UBY. Essen-
tial to UBY are the nine pairwise sense alignments
between resources, which we provide to enable
resource interoperability on the sense level, e.g.
by providing access to the often complementary
information for a sense in different resources. (3)
We present a Java-API which offers unified access
to the information contained in UBY.
We will make the UBY-LMF model, the re-
source UBY and the API freely available to the
research community.5 This will make it easy for
the NLP community to utilize UBY in a variety of
tasks in the future.
2 Related Work
The work presented in this paper concerns
standardization of LSRs, large-scale integration
thereof at the representational level, and the uni-
fied access to lexical-semantic information in the
integrated resources.
Standardization of resources. Previous work
includes models for representing lexical informa-
tion relative to ontologies (Buitelaar et al 2009;
McCrae et al 2011), and standardized single
wordnets (English, German and Italian wordnets)
in the ISO standard LMF (Soria et al 2009; Hen-
rich and Hinrichs, 2010; Toral et al 2010).
2http://www.wiktionary.org/
3http://www.wikipedia.org/
4http://www.omegawiki.org/
5http://www.ukp.tu-darmstadt.de/data/uby
McCrae et al(2011) propose LEMON, a con-
ceptual model for lexicalizing ontologies as an
extension of the LexInfo model (Buitelaar et al
2009). LEMON provides an LMF-implementation
in the Web Ontology Language (OWL), which
is similar to UBY-LMF, as it also uses DCs
from ISOCat, but diverges further from the stan-
dard (e.g. by removing structural elements such
as the predicative representation class). While
we focus on modeling lexical-semantic informa-
tion comprehensively and at a fine-grained level,
the goal of LEMON is to support the linking be-
tween ontologies and lexicons. This goal entails
a task-targeted application: domain-specific lex-
icons are extracted from ontology specifications
and merged with existing LSRs on demand. As a
consequence, there is no available large-scale in-
stance of the LEMON model.
Soria et al(2009) define WordNet-LMF, an
LMF model for representing wordnets used in
the KYOTO project, and Henrich and Hinrichs
(2010) do this for GN, the German wordnet.
These models are similar, but they still present
different implementations of the LMF meta-
model, which hampers interoperability between
the resources. We build upon this work, but ex-
tend it significantly: UBY goes beyond model-
ing a single ECR and represents a large number
of both ECRs and CCRs with very heterogeneous
content in the same format. Also, UBY-LMF
features deeper modeling of lexical-semantic in-
formation. Henrich and Hinrichs (2010), for
instance, do not explicitly model the argument
structure of subcategorization frames, since each
frame is represented as a string. In UBY-LMF,
we represent them at a fine-grained level neces-
sary for the transparent modeling of the syntax-
semantics interface.
Large-scale integration of resources. Most
previous research efforts on the integration of re-
sources targeted at world knowledge rather than
lexical-semantic knowledge. Well known exam-
ples are YAGO (Suchanek et al 2007), or DBPe-
dia (Bizer et al 2009).
Atserias et al(2004) present the Meaning Mul-
tilingual Central Repository (MCR). MCR inte-
grates five local wordnets based on the Interlin-
gual Index of EuroWordNet (Vossen, 1998). The
overall goal of the work is to improve word sense
disambiguation. This work is similar to ours, as it
581
aims at a large-scale multilingual resource and in-
cludes several resources. It is however restricted
to a single type of resource (wordnets) and fea-
tures a single type of lexical information (seman-
tic relations) specified upon synsets. Similarly,
de Melo and Weikum (2009) create a multilin-
gual wordnet by integrating wordnets, bilingual
dictionaries and information from parallel cor-
pora. None of these resources integrate lexical-
semantic information, such as syntactic subcate-
gorization or semantic roles.
McFate and Forbus (2011) present NULEX,
a syntactic lexicon automatically compiled from
WN, WKT-en and VN. As their goal is to cre-
ate an open-license resource to enhance syntactic
parsing, they enrich verbs and nouns in WN with
inflection information from WKT-en and syntac-
tic frames from VN. Thus, they only use a small
part of the lexical information present in WKT-en.
Padro? et al(2011) present their work on lex-
icon merging within the Panacea Project. One
goal of Panacea is to create a lexical resource de-
velopment platform that supports large-scale lex-
ical acquisition and can be used to combine exist-
ing lexicons with automatically acquired ones. To
this end, Padro? et al(2011) explore the automatic
integration of subcategorization lexicons. Their
current work only covers Spanish, and though
they mention the LMF standard as a potential data
model, they do not make use of it.
Shi and Mihalcea (2005) integrate FN, VN and
WN, and Palmer (2009) presents a combination of
Propbank, VN and FN in a resource called SEM-
LINK in order to enhance semantic role labeling.
Similar to our work, multiple resources are in-
tegrated, but their work is restricted to a single
language and does not cover CCRs, whose pop-
ularity and importance has grown tremendously
over the past years. In fact, with the excep-
tion of NULEX, CCRs have only been consid-
ered in the sense alignment of individual resource
pairs (Navigli and Ponzetto, 2010a; Meyer and
Gurevych, 2011).
API access for resources. An important factor
to the success of a large, integrated resource is a
single public API, which facilitates the access to
the information contained in the resource. The
most important LSRs so far can be accessed us-
ing various APIs, for instance the Java WordNet
API,6 or the Java-based Wikipedia API.7
With a stronger focus of the NLP community
on sharing data and reproducing experimental re-
sults these tools are becoming important as never
before. Therefore, a major design objective of
UBY is a single API. This is similar in spirit to the
motivation of Pradhan et al(2007), who present
integrated access to corpus annotations as a main
goal of their work on standardizing and integrat-
ing corpus annotations in the OntoNotes project.
To summarize, related work focuses either on
the standardization of single resources (or a single
type of resource), which leads to several slightly
different formats constrained to these resources,
or on the integration of several resources in an
idiosyncratic format. CCRs have not been con-
sidered at all in previous work on resource stan-
dardization, and the level of detail of the model-
ing is insufficient to fully accommodate different
types of lexical-semantic information. API ac-
cess is rarely provided. This makes it hard for
the community to exploit their results on a large
scale. Thus, it diminishes the impact that these
projects might achieve upon NLP beyond their
original specific purpose, if their results were rep-
resented in a unified resource and could easily be
accessed by the community through a single pub-
lic API.
3 UBY ? Data model
LMF defines a metamodel of LSRs in the Uni-
fied Modeling Language (UML). It provides a
number of UML packages and classes for model-
ing many different types of resources, e.g. word-
nets and multilingual lexicons. The design of
a standard-compliant lexicon model in LMF in-
volves two steps: in the first step, the structure
of the lexicon model has to be defined by choos-
ing a combination of the LMF core package and
zero to many extensions (i.e. UML packages). In
the second step, these UML classes are enriched
by attributes. To contribute to semantic interop-
erability, it is essential for the lexicon model that
the attributes and their values refer to Data Cat-
egories (DCs) taken from a reference repository.
DCs are standardized specifications of the terms
that are used for attributes and their values, or in
other words, the linguistic vocabulary occurring
6http://sourceforge.net/projects/jwordnet/
7http://code.google.com/p/jwpl/
582
in a lexicon model. Consider, for instance, the
term lexeme that is defined differently in WN and
FN: in FN, a lexeme refers to a word form, not
including the sense aspect. In WN, on the con-
trary, a lexeme is an abstract pairing of mean-
ing and form. According to LMF, the DCs are
to be selected from ISOCat, the implementation
of the ISO 12620 Data Category Registry (DCR,
Broeder et al(2010)), resulting in a Data Cate-
gory Selection (DCS).
Design of UBY-LMF. We have designed UBY-
LMF8 as a model of the union of various hetero-
geneous resources, namely WN, GN, FN, and VN
on the one hand and CCRs on the other hand.
Two design principles guided our development
of UBY-LMF: first, to preserve the information
available in the original resources and to uni-
formly represent it in UBY-LMF. Second, to be
able to extend UBY in the future by further lan-
guages, resources, and types of linguistic infor-
mation, in particular, alignments between differ-
ent LSRs.
Wordnets, FN and VN are largely complemen-
tary regarding the information types they provide,
see, e.g. Baker and Fellbaum (2009). Accord-
ingly, they use different organizational units to
represent this information. Wordnets, such as
WN and GN, primarily contain information on
lexical-semantic relations, such as synonymy, and
use synsets (groups of lexemes that are synony-
mous) as organizational units. FN focuses on
groups of lexemes that evoke the same prototypi-
cal situation (so-called semantic frames, Fillmore
(1982)) involving semantic roles (so-called frame
elements). VN, a large-scale verb lexicon, is or-
ganized in Levin-style verb classes (Levin, 1993)
(groups of verbs that share the same syntactic al-
ternations and semantic roles) and provides rich
subcategorization frames including semantic roles
and a specification of semantic predicates.
UBY-LMF employs several direct subclasses
of Lexicon in order to account for the various or-
ganization types found in the different LSRs con-
sidered. While the LexicalEntry class reflects
the traditional headword-based lexicon organiza-
tion, Synset represents synsets from wordnets,
SemanticPredicate models FN semantic
frames, and SubcategorizationFrameSet
corresponds to VN alternation classes.
8See www.ukp.tu-darmstadt.de/data/uby
SubcategorizationFrame is com-
posed of syntactic arguments, while
SemanticPredicate is composed of se-
mantic arguments. The linking between syntactic
and semantic arguments is represented by the
SynSemCorrespondence class.
The SenseAxis class is very important in
UBY-LMF, as it connects the different source
LSRs. Its role is twofold: first, it links the cor-
responding word senses from different languages,
e.g. English and German. Second, it represents
monolingual sense alignments, i.e. sense align-
ments between different lexicons in the same lan-
guage. The latter is a novel interpretation of
SenseAxis introduced by UBY-LMF.
The organization of lexical-semantic knowl-
edge found in WP, WKT, and OW can be mod-
eled with the classes in UBY-LMF as well. WP
primarily provides encyclopedic information on
nouns. It mainly consists of article pages which
are modeled as Senses in UBY-LMF.
WKT is in many ways similar to tradi-
tional dictionaries, because it enumerates senses
under a given headword on an entry page.
Thus, WKT entry pages can be represented by
LexicalEntries and WKT senses by Senses.
OW is different from WKT and WP, as it is or-
ganized in multilingual synsets. To model OW
in UBY-LMF, we split the synsets per language
and included them as monolingual Synsets in
the corresponding Lexicon (e.g., OW-en or OW-
de). The original multilingual information is pre-
served by adding a SenseAxis between corre-
sponding synsets in OW-en and OW-de.
The LMF standard itself contains only few lin-
guistic terms and does neither specify attributes
nor their values. Therefore, an important task in
developing UBY-LMF has been the specification
of attributes and their values along with the proper
attachment of attributes to LMF classes. In partic-
ular, this task involved selecting DCs from ISO-
Cat and, if necessary, adding new DCs to ISOCat.
Extensions in UBY-LMF. Although UBY-
LMF is largely compliant with LMF, the task of
building a homogeneous lexicon model for many
highly heterogeneous LSRs led us to extend LMF
in several ways: we added two new classes and
several new relationships between classes.
First, we were facing a huge variety of lexical-
semantic labels for many different dimensions of
583
semantic classification. Examples of such dimen-
sions include ontological type (e.g. selectional re-
strictions in VN and FN), domain (e.g. Biology in
WN), style and register (e.g. labels in WKT, OW),
or sentiment (e.g. sentiment of lexical units in
FN). Since we aim at an extensible LMF-model,
capable of representing further dimensions of se-
mantic classification, we did not squeeze the in-
formation on semantic classes present in the con-
sidered LSRs into existing LMF classes. Instead,
we addressed this issue by introducing a more
general class, SemanticLabel, which is an op-
tional subclass of Sense, SemanticPredicate,
and SemanticArgument. This new class has
three attributes, encoding the name of the label,
its type (e.g. ontological, register, sentiment), and
a numeric quantification (e.g. sentiment strength).
Second, we attached the subclass Frequency
to most of the classes in UBY-LMF, in order to
encode frequency information. This is of partic-
ular importance when using the resource in ma-
chine learning applications. This extension of the
standard has already been made in WordNet-LMF
(Soria et al 2009). Currently, the Frequency
class is used to keep corpus frequencies for lex-
ical units in FN, but we plan to use it for en-
riching many other classes with frequency in-
formation in future work, such as Senses or
SubcategorizationFrames.
Third, the representation of FN in LMF re-
quired adding two new relationships between
LMF classes: we added a relationship between
SemanticArgument and Definition, in or-
der to represent the definitions available for frame
elements in FN. In addition, we added a re-
lationship between the Context class and the
MonoLingualExternalRef, to represent the
links to annotated corpus sentences in FN.
Finally, WKT turned out to be hard to tackle,
because it contains a special kind of ambiguity in
the semantic relations and translation links listed
for senses: the targets of both relations and trans-
lation links are ambiguous, as they refer to lem-
mas (word forms), rather than to senses (Meyer
and Gurevych, 2010). These ambiguous rela-
tion targets could not directly be represented in
LMF, since sense and translation relations are
defined between senses. To resolve this, we
added a relationship between SenseRelation
and FormRepresentation, in order to encode
the ambiguous WKT relation target as a word
form. Disambiguating the WKT relation targets
to infer the target sense is left to future work.
A related issue occurred, when we mapped WN
to LMF. WN encodes morphologically related
forms as sense relations. UBY-LMF represents
these related forms not only as sense relations (as
in WordNet-LMF), but also at the morphologi-
cal level using the RelatedForm class from the
LMF Morphology extension. In LMF, however,
the RelatedForm class for morphologically re-
lated lexemes is not associated with the corre-
sponding sense in any way. Discarding the WN
information on the senses involved in a particular
morphological relation would lead to information
loss in some cases. Consider as an example the
WN verb buy (purchase) which is derivationally
related to the noun buy, while on the other hand
buy (accept as true, e.g. I can?t buy this story) is
not derivationally related to the noun buy. We ad-
dressed this issue by adding a sense attribute to
the RelatedForm class. Thus, in extension of
LMF, UBY-LMF allows sense relations to refer to
a form relation target and morphological relations
to refer to a sense relation target.
Data Categories in UBY-LMF. We encoun-
tered large differences in the availability of DCs
in ISOCat for the morpho-syntactic, lexical-
syntactic, and lexical-semantic parts of UBY-
LMF. Many DCs were missing in ISOCat and we
had to enter them ourselves. While this was feasi-
ble at the morpho-syntactic and lexical-syntactic
level, due to a large body of standardization re-
sults available, it was much harder at the lexical-
semantic level where standardization is still on-
going. At the lexical-semantic level, UBY-LMF
currently allows string values for a number of at-
tribute values, e.g. for semantic roles. We can eas-
ily integrate the results of the ongoing standard-
ization efforts into UBY-LMF in the future.
4 UBY ? Population with information
4.1 Representing LSRs in UBY-LMF
UBY-LMF is represented by a DTD (as suggested
by the standard) which can be used to automat-
ically convert any given resource into the corre-
sponding XML format.9 This conversion requires
a detailed analysis of the resource to be converted,
followed by the definition of a mapping of the
9Therefore, UBY-LMF can be considered as a serializa-
tion of LMF.
584
concepts and terms used in the original resource
to the UBY-LMF model. There are two major
tasks involved in the development of an automatic
conversion routine: first, the basic organizational
unit in the source LSR has to be identified and
mapped, e.g. synset in WN or semantic frame in
FN, and second, it has to be determined, how a
(LMF) sense is defined in the source LSR.
A notable aspect of converting resources into
UBY-LMF is the harmonization of linguistic ter-
minology used in the LSRs. For instance, a
WN Word and a GN Lexical Unit are mapped to
Sense in UBY-LMF.
We developed reusable conversion routines for
the future import of updated versions of the source
LSRs into UBY, provided the structure of the
source LSR remains stable. These conversion
routines extract lexical data from the source LSRs
by calling their native APIs (rather than process-
ing the underlying XML data). Thus, all lexical
information which can be accessed via the APIs
is converted into UBY-LMF.
Converting the LSRs introduced in the previ-
ous section yielded an instantiation of UBY-LMF
named UBY. The LexicalResource instance
UBY currently comprises 10 Lexicon instances,
one each for OW-de and OW-en, and one lexicon
each for the remaining eight LSRs.
4.2 Adding Sense Alignments
Besides the uniform and standardized representa-
tion of the single LSRs, one major asset of UBY
is the semantic interoperability of resources at the
sense level. In the following, we (i) describe how
we converted already existing sense alignments of
resources into LMF, and (ii) present a framework
to infer alignments automatically for any pair of
resources.
Existing Alignments. Previous work on sense
alignment yielded several alignments, such as
WN?WP-en (Niemann and Gurevych, 2011),
WN?WKT-en (Meyer and Gurevych, 2011) and
VN?FN (Palmer, 2009).
We converted these alignments into UBY-LMF
by creating a SenseAxis instance for each pair of
aligned senses. This involved mapping the sense
IDs from the proprietary alignment files to the
corresponding sense IDs in UBY.
In addition, we integrated the sense alignments
already present in OW and WP. Some OW en-
tries provide links to the corresponding WP page.
Also, the German and English language editions
of WP and OW are connected by inter-language
links between articles (Senses in UBY). We can
expect that these links have high quality, as they
were entered manually by users and are subject
to community control. Therefore, we straightfor-
wardly imported them into UBY.
Alignment Framework. Automatically creat-
ing new alignments is difficult because of word
ambiguities, different granularities of senses,
or language specific conceptualizations (Navigli,
2006). To support this task for a large number
of resources across languages, we have designed
a flexible alignment framework based on the
state-of-the-art method of Niemann and Gurevych
(2011). The framework is generic in order to al-
low alignments between different kinds of entities
as found in different resources, e.g. WN synsets,
FN frames or WP articles. The only requirement
is that the individual entities are distinguishable
by a unique identifier in each resource.
The alignment consists of the following steps:
First, we extract the alignment candidates for a
given resource pair, e.g. WN sense candidates for
a WKT-en entry. Second, we create a gold stan-
dard by manually annotating a subset of candi-
date pairs as ?valid? or ?non-valid?. Then, we
extract the sense representations (e.g. lemmatized
bag-of-words based on glosses) to compute the
similarity of word senses (e.g. by cosine similar-
ity). The gold standard with corresponding sim-
ilarity values is fed into Weka (Hall et al 2009)
to train a machine learning classifier, and in the
final step this classifier is used to automatically
classify the candidate sense pairs as (non-)valid
alignment. Our framework also allows us to train
on a combination of different similarity measures.
Using our framework, we were able to re-
produce the results reported by Niemann and
Gurevych (2011) and Meyer and Gurevych
(2011) based on the publicly available evaluation
datasets10 and the configuration details reported
in the corresponding papers.
Cross-Lingual Alignment. In order to align
word senses across languages, we extended the
monolingual sense alignment described above to
the cross-lingual setting. Our approach utilizes
10http://www.ukp.tu-darmstadt.de/data/sense-alignment/
585
Moses,11 trained on the Europarl corpus. The
lemma of one of the two senses to be aligned
as well as its representations (e.g. the gloss) is
translated into the language of the other resource,
yielding a monolingual setting. E.g., the WN
synset {vessel, watercraft} with its gloss ?a craft
designed for water transportation? is translated
into {Schiff, Wasserfahrzeug} and ?Ein Fahrzeug
fu?r Wassertransport?, and then the candidate ex-
traction and all downstream steps can take place
in German. An inherent problem with this ap-
proach is that incorrect translations also lead to
invalid alignment candidates. However, these are
most probably filtered out by the machine learn-
ing classifier as the calculated similarity between
the sense representations (e.g. glosses) should be
low if the candidates do not match.
We evaluated our approach by creating a cross-
lingual alignment between WN and OW-de, i.e.
the concepts in OW with a German lexicaliza-
tion.12 To our knowledge, this is the first study on
aligning OW with another LSR. OW is especially
interesting for this task due to its multilingual con-
cepts, as described by Matuschek and Gurevych
(2011). The created gold standard could, for in-
stance, be re-used to evaluate alignments for other
languages in OW.
To compute the similarity of word senses, we
followed the approach by Niemann and Gurevych
(2011) while covering both translation directions.
We used the cosine similarity for comparing the
German OW glosses with the German translations
of WN glosses and cosine and personalized page
rank (PPR) similarity for comparison of the Ger-
man OW glosses translated into English with the
original English WN glosses. Note that PPR sim-
ilarity is not available for German as it is based
on WN. Thereby, we filtered out the OW con-
cepts without a German gloss which left us with
11,806 unique candidate pairs. We randomly se-
lected 500 WN synsets for analysis yielding 703
candidate pairs. These were manually annotated
as being (non-)alignments. For the subsequent
machine learning task we used a simple threshold-
based classifier and ten-fold cross validation.
Table 1 summarizes the results of different sys-
tem configurations. We observe that translation
11http://www.statmt.org/moses/
12OmegaWiki consists of interlinked language-
independent concepts to which lexicalizations in several
languages are attached.
Translation Similarity
direction measure P R F1
EN > DE Cosine (Cos) 0.666 0.575 0.594
DE > EN Cos 0.674 0.658 0.665
DE > EN PPR 0.721 0.712 0.716
DE > EN PPR + Cos 0.723 0.712 0.717
Table 1: Cross-lingual alignment results
into English works significantly better than into
German. Also, the more elaborate similarity mea-
sure PPR yields better results than cosine similar-
ity, while the best result is achieved by a combina-
tion of both. Niemann and Gurevych (2011) make
a similar observation for the monolingual setting.
Our F-measure of 0.717 in the best configuration
lies between the results of Meyer and Gurevych
(2011) (0.66) and Niemann and Gurevych (2011)
(0.78), and thus verifies the validity of the ma-
chine translation approach. Therefore, the best
alignment was subsequently integrated into UBY.
5 Evaluating UBY
We performed an intrinsic evaluation of UBY by
computing a number of resource statistics. Our
evaluation covers two aspects: first, it addresses
the question if our automatic conversion routines
work correctly. Second, it provides indicators for
assessing UBY in terms of the gain in coverage
compared to the single LSRs.
Correctness of conversion. Since we aim to
preserve the maximal amount of information from
the original LSRs, we should be able to replace
any of the original LSRs and APIs by UBY and
the UBY-API without losing information. As
the conversion is largely performed automatically,
systematic errors and information loss could be
introduced by a faulty conversion routine. In or-
der to detect such errors and to prove the correct-
ness of the automatic conversion and the result-
ing representation, we have compared the orig-
inal resource statistics of the classes and infor-
mation types in the source LSRs to the cor-
responding classes in their UBY counterparts.
For instance, the number of lexical relations in
WordNet has been compared to the number of
SenseRelations in the UBY WordNet lexi-
con.13
13For detailed analysis results see the UBY website.
586
Lexical Sense
Lexicon Entry Sense Relation
FN 9,704 11,942 ?
GN 83,091 93,407 329,213
OW-de 30,967 34,691 60,054
OW-en 51,715 57,921 85,952
WP-de 790,430 838,428 571,286
WP-en 2,712,117 2,921,455 3,364,083
WKT-de 85,575 72,752 434,358
WKT-en 335,749 421,848 716,595
WN 156,584 206,978 8,559
VN 3,962 31,891 ?
UBY 4,259,894 4,691,313 5,300,941
Table 2: UBY resource statistics (selected classes).
Lexicon pair Languages SenseAxis
WN?WP-en EN?EN 50,351
WN?WKT-en EN?EN 99,662
WN?VN EN?EN 40,716
FN?VN EN?EN 17,529
WP-en?OW-en EN?EN 3,960
WP-de?OW-de DE?DE 1,097
WN?OW-de EN?DE 23,024
WP-en?WP-de EN?DE 463,311
OW-en?OW-de EN?DE 58,785
UBY All 758,435
Table 3: UBY alignment statistics.
Gain in coverage. UBY offers an increased
coverage compared to the single LSRs as reflected
in the resource statistics. Tables 2 and 3 show the
statistics on central classes in UBY. As UBY is
organized in several Lexicons, the number of
UBY lexical entries is the sum of the lexical en-
tries in all 10 Lexicons. Thus, UBY contains
more than 4.2 million lexical entries, 4.6 million
senses, 5.3 million semantic relations between
senses and more than 750,000 alignments. These
statistics represent the total numbers of lexical en-
tries, senses and sense relations in UBY without
filtering of identical (i.e. corresponding) lexical
entries, senses and relations. Listing the num-
ber of unique senses would require a full align-
ment between all integrated resources, which is
currently not available.
We can, however, show that UBY contains over
3.08 million unique lemma-POS combinations for
English and over 860,000 for German, over 3.94
million in total, see Table 4. Therefore, we as-
sessed the coverage on lemma level. Table 4 also
shows the number of lemmas with entries in one
or more than one lexicon, additionally split by
POS and language. Lemmas occurring only once
in UBY increase the coverage at lemma level. For
lemmas with parallel entries in several UBY lex-
icons, new information becomes available in the
form of additional sense definitions and comple-
mentary information types attached to lemmas.
Finally, the increase in coverage at sense level
can be estimated for senses that are aligned across
at least two UBY-lexicons. We gain access to
all available, partly complementary information
types attached to these aligned senses, e.g. seman-
tic relations, subcategorization frames, encyclo-
pedic or multilingual information. The number
of pairwise sense alignments provided by UBY is
given in Table 3. In addition, we computed how
many senses simultaneously take part in at least
two pairwise sense alignments. For English, this
applies to 31,786 senses, for which information
from 3 UBY lexicons is available.
EN Lexicons noun verb adjective
5 1 699 -
4 1,630 1,888 430
3 8,439 1,948 2,271
2 53,856 4,727 12,290
1 2,900,652 50,209 41,731
? (unique EN) 3,080,771
DE Lexicons noun verb adjective
4 1,546 - -
3 10,374 372 342
2 26,813 3,174 2,643
1 803,770 6,108 7,737
? (unique DE) 862,879
Table 4: Number of lemmas (split by POS and lan-
guage) with entries in i UBY lexicons, i = 1, . . . , 5.
6 Using UBY
UBY API. For convenient access to UBY, we
implemented a Java-API which is built around
the Hibernate14 framework. Hibernate allows to
easily store the XML data which results from
converting resources into Uby-LMF into a corre-
sponding SQL database.
Our main design principle was to keep the ac-
cess to the resource as simple as possible, despite
the rich and complex structure of UBY. Another
14http://www.hibernate.org/
587
important design aspect was to ensure that the
functionality of the individual, resource-specific
APIs or user interfaces is mirrored in the UBY
API. This enables porting legacy applications to
our new resource. To facilitate the transition to
UBY, we plan to provide reference tables which
list the corresponding UBY-API operations for the
most important operations in the WN API, some
of which are shown in Table 5.
WN function UBY function
Dictionary UBY
getIndexWord(pos,
lemma)
getLexicalEntries(
pos, lemma)
IndexWord LexicalEntry
getLemma() getLemmaForm()
Synset Synset
getGloss() getDefinitionText()
getWords() getSenses()
Pointer SynsetRelation
getType() getRelName()
Word Sense
getPointers() getSenseRelations()
Table 5: Some equivalent operations in WN API and
UBY API.
While it is possible to limit access to single re-
sources by a parameter and thus mimic the behav-
ior of the legacy APIs (e.g. only retrieve Synsets
and their relations from WN), the true power of
UBY API becomes visible when no such con-
straints are applied. In this case, all imported re-
sources are queried to get one combined result,
while retaining the source of the respective in-
formation. On top of this, the information about
existing sense alignments across resources can be
accessed via SenseAxis relations, so that the re-
turned combined result covers not only the lexi-
cal, but also the sense level.
Community issues. One of the most important
reasons for UBY is creating an easy-to-use pow-
erful LSR to advance NLP research and develop-
ment. Therefore, community building around the
resource is one of our major concerns. To this end,
we will offer free downloads of the lexical data
and software presented in this paper under open li-
censes, namely: The UBY-LMF DTD, mappings
and conversion tools for existing resources and
sense alignments, the Java API, and, as far as li-
censing allows,15 already converted resources. If
resources cannot be made available for download,
the conversion tools will still allow users with ac-
cess to these resources to import them into UBY
easily. In this way, it will be possible for users to
build their ?custom UBY? containing selected re-
sources. As the underlying resources are subject
to continuous change, updates of the correspond-
ing components will be made available on a regu-
lar basis.
7 Conclusions
We presented UBY, a large-scale, standardized
LSR containing nine widely used resources in two
languages: English WN, WKT-en, WP-en, FN
and VN, German WP-de, WKT-de, and GN, and
OW in English and German. As all resources
are modeled in UBY-LMF, UBY enables struc-
tural interoperability across resources and lan-
guages down to a fine-grained level of informa-
tion. For FN, VN and all of the CCRs in En-
glish and German, this is done for the first time.
Besides, by integrating sense alignments we also
enable the lexical-semantic interoperability of re-
sources. We presented a unified framework for
aligning any LSRs pairwise and reported on ex-
periments which align OW-de and WN. We will
release the UBY-LMF model, the resource and the
UBY-API at the time of publication.16 Due to the
added value and the large scale of UBY, as well as
its ease of use, we believe UBY will boost the per-
formance of NLP making use of lexical-semantic
knowledge.
Acknowledgments
This work has been supported by the Emmy
Noether Program of the German Research Foun-
dation (DFG) under grant No. GU 798/3-1 and
by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant
No. I/82806. We thank Richard Eckart de
Castilho, Yevgen Chebotar, Zijad Maksuti and Tri
Duc Nghiem for their contributions to this project.
References
Jordi Atserias, Lu??s Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
15Only GermaNet is subject to a restricted license and can-
not be redistributed in UBY format.
16http://www.ukp.tu-darmstadt.de/data/uby
588
Vossen. 2004. The Meaning Multilingual Central
Repository. In Proceedings of the second interna-
tional WordNet Conference (GWC 2004), pages 23?
30, Brno, Czech Republic.
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguis-
tic Annotation Workshop, ACL-IJCNLP ?09, pages
125?129, Suntec, Singapore.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th
International Conference on Computational Lin-
guistics (COLING-ACL?98, pages 86?90, Montreal,
Canada.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia A Crys-
tallization Point for the Web of Data. Journal of
Web Semantics: Science, Services and Agents on the
World Wide Web, (7):154?165.
Daan Broeder, Marc Kemps-Snijders, Dieter Van Uyt-
vanck, Menzo Windhouwer, Peter Withers, Peter
Wittenburg, and Claus Zinn. 2010. A Data Cat-
egory Registry- and Component-based Metadata
Framework. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), pages 43?47, Valletta, Malta.
Paul Buitelaar, Philipp Cimiano, Peter Haase, and
Michael Sintek. 2009. Towards Linguistically
Grounded Ontologies. In Lora Aroyo, Paolo
Traverso, Fabio Ciravegna, Philipp Cimiano, Tom
Heath, Eero Hyvo?nen, Riichiro Mizoguchi, Eyal
Oren, Marta Sabou, and Elena Simperl, editors, The
Semantic Web: Research and Applications, pages
111?125, Berlin/Heidelberg, Germany. Springer.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined ev-
idence. In Proceedings of the 18th ACM conference
on Information and knowledge management (CIKM
?09), CIKM ?09, pages 513?522, New York, NY,
USA. ACM.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Charles J. Fillmore. 1982. Frame Semantics. In The
Linguistic Society of Korea, editor, Linguistics in
the Morning Calm, pages 111?137. Hanshin Pub-
lishing Company, Seoul, Korea.
Gil Francopoulo, Nuria Bel, Monte George, Nico-
letta Calzolari, Monica Monachini, Mandy Pet, and
Claudia Soria. 2006. Lexical Markup Framework
(LMF). In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC), pages 233?236, Genoa, Italy.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Verena Henrich and Erhard Hinrichs. 2010. Standard-
izing wordnets in the ISO standard LMF: Wordnet-
LMF for GermaNet. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING), pages 456?464, Beijing, China.
Richard Johansson and Pierre Nugues. 2007. Us-
ing WordNet to extend FrameNet coverage. In
Proceedings of the Workshop on Building Frame-
semantic Resources for Scandinavian and Baltic
Languages, at NODALIDA, pages 27?30, Tartu, Es-
tonia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. Language Resources and Evalu-
ation, 42:21?40.
Claudia Kunze and Lothar Lemnitzer. 2002. Ger-
maNet ? representation, visualization, application.
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC), pages 1485?1491, Las Palmas, Canary Is-
lands, Spain.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago,
IL, USA.
Michael Matuschek and Iryna Gurevych. 2011.
Where the journey is headed: Collaboratively con-
structed multilingual Wiki-based resources. In
SFB 538: Mehrsprachigkeit, editor, Hamburger Ar-
beiten zur Mehrsprachigkeit, Hamburg, Germany.
John McCrae, Dennis Spohr, and Philipp Cimiano.
2011. Linking Lexical Resources and Ontologies
on the Semantic Web with Lemon. In The Seman-
tic Web: Research and Applications, volume 6643
of Lecture Notes in Computer Science, pages 245?
259. Springer, Berlin/Heidelberg, Germany.
Clifton J. McFate and Kenneth D. Forbus. 2011.
NULEX: an open-license broad coverage lexicon.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 363?367, Portland, OR, USA.
Christian M. Meyer and Iryna Gurevych. 2010. Worth
its Weight in Gold or Yet Another Resource ?
A Comparative Study of Wiktionary, OpenThe-
saurus and GermaNet. In Alexander Gelbukh, ed-
itor, Computational Linguistics and Intelligent Text
Processing: 11th International Conference, volume
6008 of Lecture Notes in Computer Science, pages
38?49. Berlin/Heidelberg: Springer, Ias?i, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Align-
ing Wiktionary and WordNet for Increased Domain
589
Coverage. In Proceedings of the 5th International
Joint Conference on Natural Language Processing
(IJCNLP), pages 883?892, Chiang Mai, Thailand.
Roberto Navigli and Simone Paolo Ponzetto. 2010a.
BabelNet: Building a Very Large Multilingual Se-
mantic Network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 216?225, Uppsala, Sweden, July.
Roberto Navigli and Simone Paolo Ponzetto. 2010b.
Knowledge-rich Word Sense Disambiguation Ri-
valing Supervised Systems. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1522?1531, Uppsala,
Sweden.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation
Performance. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages
105?112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
People?s Web meets Linguistic Knowledge: Auto-
matic Sense Alignment of Wikipedia and WordNet.
In Proceedings of the 9th International Conference
on Computational Semantics (IWCS), pages 205?
214, Oxford, UK.
Muntsa Padro?, Nu?ria Bel, and Silvia Necsulescu.
2011. Towards the Automatic Merging of Lexical
Resources: Automatic Mapping. In Proceedings of
the International Conference on Recent Advances
in Natural Language Processing (RANLP), pages
296?301, Hissar, Bulgaria.
Martha Palmer. 2009. Semlink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Gen-
erative Lexicon Conference (GenLex-09), pages 9?
15, Pisa, Italy.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 517?526, Washington, DC, USA.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings
of the Sixth International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing), pages 100?111, Mexico City, Mexico.
Claudia Soria, Monica Monachini, and Piek Vossen.
2009. Wordnet-LMF: fleshing out a standardized
format for Wordnet interoperability. In Proceed-
ings of the 2009 International Workshop on Inter-
cultural Collaboration, pages 139?146, Palo Alto,
CA, USA.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, pages 697?706, Banff,
Canada.
Antonio Toral, Stefania Bracale, Monica Monachini,
and Claudia Soria. 2010. Rejuvenating the Italian
WordNet: Upgrading, Standarising, Extending. In
Proceedings of the 5th Global WordNet Conference
(GWC), Bombay, India.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht, Nether-
lands.
590
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 68?77,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Automated Verb Sense Labelling Based on Linked Lexical Resources
Kostadin Cholakov
1
Judith Eckle-Kohler
2,3
Iryna Gurevych
2,3
1
Humboldt-Universit?at zu Berlin, kostadin.cholakov@anglistik.hu-berlin.de
2
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
3
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
We present a novel approach for creat-
ing sense annotated corpora automatically.
Our approach employs shallow syntactico-
semantic patterns derived from linked lex-
ical resources to automatically identify in-
stances of word senses in text corpora. We
evaluate our labelling method intrinsically
on SemCor and extrinsically by using au-
tomatically labelled corpus text to train a
classifier for verb sense disambiguation.
Testing this classifier on verbs from the
English MASC corpus and on verbs from
the Senseval-3 all-words disambiguation
task shows that it matches the performance
of a classifier which has been trained on
manually annotated data.
1 Introduction
Sense annotated corpora are important resources
in NLP as they can be used as training data (e.g.,
for word sense disambiguation (WSD) or semantic
role labelling) or as sources for the acquisition of
lexical information (e.g., selectional preference in-
formation). Typically, a particular sense inventory
from a lexical resource is used to annotate some or
all words with word senses from this sense inven-
tory. For instance, various sense-annotated cor-
pora based on WordNet (WN; (Fellbaum, 1998))
exist, such as the data from the Senseval competi-
tions,
1
or the SemCor corpus.
2
Such corpora are
usually created manually which is expensive and
time consuming. Furthermore, the corpora are of-
ten domain specific (e.g. newspaper texts) which
makes statistical systems trained on them strongly
biased.
We present a novel approach for creating sense
annotated corpora automatically. Our approach
1
http://www.senseval.org
2
http://www.cse.unt.edu/
?
rada/
downloads.html#semcor
employs shallow syntactico-semantic patterns de-
rived from linked lexical resources (LLRs) to auto-
matically identify instances of word senses in text
corpora. We significantly extend previous work on
this task by making two important contributions:
(i) we employ a large-scale LLR for automatically
creating sense annotated data and (ii) we perform
meaningful intrinsic and application-based eval-
uations of our method on large sense annotated
datasets.
LLRs are the result of integrating several
lexical-semantic resources by linking them at the
word sense level. Examples of large LLRs are
the multilingual BabelNet (Navigli and Ponzetto,
2012), an integration of wordnets and Wikipedia
3
,
or UBY, (Gurevych et al., 2012), the resource we
employ in our work here. UBY is an integration of
multiple resources, such as wordnets, Wikipedia,
Wiktionary (WKT)
4
, FrameNet (FN; (Baker et al.,
1998)) and VerbNet (VN; (Kipper et al., 2008)) for
English and German.
A distinguishing feature of LLRs is the enriched
sense representation for word senses that are in-
terlinked since different resources provide differ-
ent, often complementary information. Annotat-
ing corpora with such enriched sense representa-
tions turns them into versatile training data for sta-
tistical systems.
Our first contribution (i) also addresses a con-
siderable gap in recent research regarding auto-
mated sense labelling of verbs. Most previous
work is done on nouns. However, verbs pose a
bigger challenge due to their high polysemy and
the fact that, unlike nouns, syntax is of crucial im-
portance because it often reflects particular aspects
of verb meaning. That is why, here we focus on
verbs and present results and evaluations for this
previously neglected part-of-speech (POS). Our
method, however, can be applied to other parts-of
3
http://www.wikipedia.org
4
http://www.wiktionary.org
68
speech as well.
Regarding (ii), we are the first to perform mean-
ingful intrinsic and extrinsic evaluations of auto-
matically labelled data on a larger scale. The in-
trinsic evaluation measures the performance of our
method on the manually annotated SemCor cor-
pus. The extrinsic evaluation compares the perfor-
mance of a classifier for verb sense disambigua-
tion (VSD) which has been trained (a) on auto-
matically sense labelled data and (b) on manually
annotated data. Both settings achieve very simi-
lar results which means that competitive VSD can
be performed without the need of costly manually
created training data. This could be beneficial in
languages (e.g., German, Spanish) for which elab-
orate lexical-semantic resources exist but large,
high-quality sense annotated corpora are unavail-
able. Moreover, we experiment with various link-
ings between lexical resources in order to inves-
tigate how different resource combinations affect
the performance of automated sense labelling. We
show that combining all available resources might
not be the best option.
The remainder of the paper is organised as fol-
lows. Section 2 presents our method. Section 3 de-
scribes the data used in the experiments. Section
4 presents the results of the evaluations. Section
5 analyses in detail the differences between our
method and previous work. Section 6 concludes
the paper.
2 Automated Labelling of Verb Senses
This section describes our novel approach for au-
tomated sense labelling of verbs in a corpus, which
exploits the added value of LLRs.
2.1 Approach
Our approach to automatically label corpus in-
stances of verb senses with sense identifiers from
an LLR is based on a pattern-based representation
of verb senses. Such patterns constitute a common
format for the representation of verb senses avail-
able in LLRs and verb instances found in corpora.
The common format we developed resembles a
syntactico-semantic clause pattern which we call
a sense pattern (SP). Based on a comparison of the
derived SPs by means of a similarity metric, verb
instances in a corpus can automatically be labelled
with sense identifiers from an LLR.
SPs can be derived from corpus instances and
from information given in LLRs, in particular,
sense examples and more abstract predicate argu-
ment structure information.
2.2 Step 1: Creation of SPs from LLRs
For the creation of SPs, we employ the large-scale
LLR UBY which combines 10 lexical resources
for English and German to make use of the en-
riched verb sense representations provided by the
sense links between various resources available in
UBY. Although our method can work with any
LLR, we choose UBY because the various re-
sources are represented in a standardised format
(Eckle-Kohler et al., 2012) and sense links be-
tween them can uniformly and conveniently be ac-
cessed via the freely available UBY-API.
5
Since we evaluate our method on data annotated
with WN senses, we create SPs for enriched WN
senses (see example given in Table 1). We enrich
WN senses by aggregating lexical information that
can be accessed through links given in UBY to
corresponding verb senses in other resources.
In this setting, enrichment means that we make
use of sense examples from WN, from FN via
the WN?FN linking, and from WKT via the
WN?WKT linking. In addition, we use ab-
stract predicate-argument structure information
from VN via the WN?VN linking (see Table 1).
6
For phrasal verb senses (e.g., write up) and
other verbal multiword expressions (e.g., know
what?s going on) listed in WN, UBY rarely pro-
vides links to other resources. Therefore, we in-
duced sense links by following the one sense per
collocation assumption.
7
Based on this assump-
tion, we linked each sense of a verbal multiword
verb lemma in WN with each sense of the same
multiword lemma in FN and WKT.
From sense examples, we derive two different
kinds of SPs. Based on a fragment of a sense ex-
ample given by a window w around the target verb
lemma we create: (i) lemma SPs (LSPs) consisting
only of lemmas (including the target verb) and (ii)
abstract SPs (ASPs) consisting of the target verb
lemma and items from a fixed, linguistically mo-
tivated vocabulary. This is based on the intuition
that LSPs are important to identify relatively fixed
5
http://code.google.com/p/uby/
6
Although VN is linked to sense examples given in the
PropBank corpus, the rationale behind using just abstract
predicate-argument structure information was to explore,
which effect this type of information has on the performance
of an automated labelling algorithm.
7
It assumes that nearby words provide strong and consis-
tent clues to the sense of a target word, see Yarowsky (1995).
69
WN sense tell%2:32:00:: (let something be known) Corresponding sense patterns (SPs)
WN Tell them that you will be late LSP ? tell them that you will be
ASP ? tell PP that PP be JJ
WN?FN But an insider told TODAY : ? There was no animosity.? LSP ? but an insider tell Today : ? there be
ASP ? person tell location be feeling
WN?WKT Please tell me the time. LSP ? Please tell me the time
ASP ? tell PP event
WN?VN Agent[+animate| + organization] V ASP ? PP tell group about communication
Recipient[+animate| + organization]
about Topic[+communication]
Table 1: Examples of SPs derived from an enriched WN sense in UBY. PP, JJ, and VV are POS tags
from the Penn Treebank tagset, standing for personal pronoun, adjective and full verb.
verbal multiword expressions in a corpus, whereas
ASPs are necessary to identify productively used
verb senses that are constrained in their use only
by their syntactic behaviour and particular seman-
tic properties, such as selectional preferences on
their arguments.
The fixed vocabulary used for the creation of
ASPs consists of (i) the target verb lemma, (ii) se-
lected POS tags from the Penn Treebank Tagset
(Marcus et al., 1993), (iii) a list of particular func-
tion words that play an important role in fine-
grained subcategorisation frames of verbs (Eckle-
Kohler and Gurevych, 2012) and (iv) semantic cat-
egories of nouns given by WN semantic fields. We
selected POS tags that play an important role in
syntactic realisations of verbs, e.g. POS tags for
personal pronouns which are potential verb argu-
ments. In our experiments, we tried different sets
of function words and POS tags. For instance,
we found that some function words (e.g., reflex-
ive pronouns) and some POS tags (e.g., those for
past participles and comparative adjectives) intro-
duced too much noise in the data and therefore we
did not select them for the final vocabulary.
8
In order to create SPs from sense examples,
we apply POS tagging and lemmatisation using
the TreeTagger (Schmid, 1994) and named entity
tagging using the Stanford Named Entity Recog-
niser (Klein et al., 2003). The named entity
tags attached by the Named Entity Recogniser are
mapped to WN semantic fields.
For the generation of ASPs from sense exam-
ples, we used a window size of w = 7, while
the generation of LSPs has been performed with
w = 5 in order to put a focus on the closely neigh-
bouring lexemes in multiword verb lemmas. The
8
The vocabulary used for the creation of ASPs is available
at http://www.ukp.tu-darmstadt.de/data/.
window size was set empirically using the English
Lexical Sample task of the Senseval-2 dataset as
a development set. The same set was also used
for the development of the linguistically motivated
vocabulary for ASPs.
9
From the abstract predicate-argument struc-
ture information given in VN, we derived only
ASPs. For this, we employed the subcategori-
sation frames, as well as the semantic role and
selectional preference information from VN, and
created ASPs based on manually created map-
pings between these information types and the
controlled vocabulary used for ASPs.
2.3 Step 2: Automated Labelling
For the automated labelling of verbs in a corpus,
we first derive SPs from each corpus sentence con-
taining a target verb. SPs are derived from corpus
sentences by applying the same procedure as de-
scribed in Step 1 for the creation of SPs from sense
examples, the window size used is w = 7.
To compare two SPs, we propose a similarity
metric based on Dice?s coefficient which calcu-
lates the sum of the weighted number of their com-
mon bi-grams, tri-grams, and four-grams. For-
mally, the similarity score sim
w
? [0..1] of two
SPs p
1
, p
2
is defined as:
(1) sim
w
(p
1
, p
2
) =
4
?
n=2
|G
n
(p
1
)?G
n
(p
2
)|?n
norm
w
where w >= 1 is the size of the window around
the target verb, G
n
(p
i
), i ? {1, 2} is the set of n-
9
However, the Senseval-2 data are annotated with sense
keys of the WN pre-release version 1.7 and therefore, we had
to employ an automated mapping of WN 1.7 pre-release to
WN 3.0 sense keys provided by Rada Mihalcea. Since this
mapping turned out to be rather noisy, we did not use the
Senseval-2 data in our evaluations.
70
Automated labelling of corpus instances
for each sentence s
i
with verb v
derive LSP
i
and ASP
i
forall j = sizeOf(UBY-LSP(v))
compare LSP
i
with LSP
j
in UBY-LSP(v):
maxSim(LSP
i
) = argmax
j
score(LSP
i
, LSP
j
)
add sense(argmax
j
) to MostSimilarSenses(LSP
i
)
forall k = sizeOf(UBY-ASP(v))
compare ASP
i
with ASP
k
in UBY-ASP(v):
maxSim(ASP
i
) = argmax
k
score(ASP
i
, ASP
k
)
add sense(argmax
k
) to MostSimilarSenses(ASP
i
)
if maxSim
i,j
>= threshold t and
maxSim
i,j
>= maxSim
i,k
label(s
i
) = random(MostSimilarSenses(LSP
i
))
else if maxSim
i,k
>= threshold t
label(s
i
) = random(MostSimilarSenses(ASP
i
))
end if
end for
Table 2: Algorithm for labelling corpus instances
with WordNet senses.
grams occurring in SP p
i
, and norm
w
is the nor-
malisation factor defined by the sum of the max-
imum number of common bigrams, trigrams and
fourgrams in the window w. Similarity metrics
based on Dice?s coefficient have often been used
in Lesk-based WSD (Lesk, 1986) to calculate the
overlap of two sets (e.g., Baldwin et al. (2010)). In
our case, however, the elements of the two sets are
bigrams, trigrams and fourgrams, while in Lesk-
based algorithms typically sets of unigrams are
compared, thus not accounting for word order.
Table 2 shows the algorithm used for automated
labelling of corpus instances in pseudo-code. The
algorithm assumes that for each verb v, the corre-
sponding set of SPs derived from UBY sense ex-
amples (UBY-LSP(v) and UBY-ASP(v) in Table
2) has already been computed.
For each corpus sentence containing a target
verb v, the corresponding SPs for verb v derived
from UBY are scored by the similarity metric in
(1). The SPs with the maximum score that is above
a threshold t form the set of most similar senses.
From this set, the algorithm picks one sense ran-
domly as a label. How often this happens, depends
on the value of t: the percentage of randomly se-
lected senses ranges from about 33% for t = 0.14
to about 50% for t = 0.04.
3 Data
Web corpora. For the automated labelling of cor-
pus data with WN senses, we use two very large
web corpora: the English ukWaC corpus (Ba-
roni et al., 2009) and the article pages extracted
from the English Wikipedia using the Java-based
Wikipedia API JWPL (Zesch et al., 2008). Fur-
ther, for the evaluation of our method, we use three
manually sense annotated data sets.
SemCor. We use the SemCor 3.0 corpus which
is annotated with WN 3.0 senses.
MASC. MASC is a balanced subset of 500K
words of written texts and transcribed speech
drawn primarily from the Open American Na-
tional Corpus (OANC).
10
The texts come from 19
different genres which allows us to test our method
on real-life data from multiple sources. The cor-
pus is annotated with various types of linguistic
information, including WN 3.0 sense annotations
for instances of selected words. Therefore, MASC
is a lexical sample corpus.
We extracted instances of 16 MASC verbs
(11,997 instances) which have been sense anno-
tated. Most instances are annotated by multiple
annotators and, to create a gold standard, we took
the sense preferred by the majority of annotators
and ignored instances where there were ties.
Senseval-3. In the test corpus of the Senseval-
3 all-words disambiguation task sense annotations
are provided for each content word in a chunk
of the WSJ corpus (5,000 words of running text).
The third annotated data set for our experiment is
formed by extracting all verb instances from this
test corpus. Note that the gold standard annota-
tions in Senseval-3 were made using WN 1.7.1.
In our experiments, we use Rada Mihalcea?s con-
version of the corpus to WN 3.0.
11
However, we
found out that some verb instances were converted
to sense labels that do not exist in WN 3.0. Af-
ter removing those instances, there were 305 verbs
with 592 instances left.
4 Experiments and Evaluation
Next, we present the intrinsic and the application-
based evaluations of our method.
4.1 Intrinsic Evaluation
We intrinsically evaluate the performance of the
automated labelling algorithm for the Senseval-3
verbs which occur in the SemCor corpus. Occur-
rences of these 152 verbs in SemCor are processed
10
http://www.americannationalcorpus.
org/
11
http://www.cse.unt.edu/
?
rada/
downloads.html#sensevalsemcor
71
WN?FN?WKT WN?FN?WKT?VN
t Cov Cov Acc Cov Cov Acc
(Inst.) (Sense) (Inst.) (Sense)
0.04 0.55 0.27 0.32 0.48 0.25 0.35
0.07 0.15 0.17 0.36 0.13 0.15 0.42
0.1 0.11 0.14 0.35 0.10 0.13 0.42
0.14 0.02 0.07 0.41 0.02 0.05 0.47
Table 3: Performance of the automated labelling
algorithm evaluated for occurrences of Senseval-3
verbs in SemCor.
by the labelling algorithm with a window size
w = 7 and the automatically annotated WN 3.0
senses are compared with the gold senses available
in SemCor 3.0.
Quantitative Evaluation. We calculated the
accuracy as the percentage of correctly labelled in-
stances and the instance coverage as the percent-
age of labelled instances. The sense coverage is
calculated as the percentage of all predicted (not
annotated) senses relative to all gold verb senses
given in SemCor.
A random sense baseline yields 15% accuracy.
Note that a MFS baseline based on WN would
not be meaningful, because the WordNet MFS is
based on the frequency distribution of annotated
senses in SemCor.
Table 3 shows accuracy and coverage results
of the automated labelling algorithm for different
values of the threshold t and two combinations of
sense links from UBY. Depending on the threshold
t, 2% to 55% of the verb instances in SemCor can
automatically be labelled, and the instance cov-
erage goes largely in parallel to the coverage of
predicted WN senses. Accuracy ranges between
32% and 47% and exceeds the random sense base-
line by a large margin. Lowering the threshold in-
creases the coverage of the labelling method, but
it also leads to a decrease in accuracy of 9 percent-
age points (12 for the configuration with VN).
Adding more patterns from VN via the WN?
VN alignment, leads to a decrease in both instance
and sense coverage combined with an increase in
accuracy. Since SemCor is a rather small corpus,
the increase in instance coverage is not as clear
as for large Web corpora such as the ukWaC cor-
pus. Labelling a 1GB subset of the ukWaC cor-
pus based on patterns derived from the WN?FN?
WKT alignments resulted in 15MB of labelled
data, whereas 25MB labelled data could be created
from the same subset with the additional patterns
from the WN?VN alignment.
Qualitative Analysis. In Table 4, we show ex-
amples of the highest ranking patterns and the cor-
responding labelled SemCor instances for senses
that were correctly and falsely annotated. The ex-
amples in Table 4 show that the similarity metric
assigns the highest values to instances where func-
tion words (e.g., in, to, who) or POS tags (e.g., PP,
VV) from the ASP vocabulary occur in the im-
mediate neighbourhood of the target verb. Since
such functions words play an important role in the
ASPs derived from VN, the VN ASPs possibly
tend to dominate over the SPs derived from sense
examples, which explains the observed decrease in
coverage (see Table 3).
The falsely labelled instances turn out to be ex-
amples of WN senses where the gold sense is very
similar to the automatically attached sense as evi-
dent from the synset definition given in the right-
most column.
4.2 Extrinsic Evaluation
We extrinsically evaluate our method for auto-
mated verb sense labelling by using it for learning
a classifier for VSD in a train-test setting. We use
features which have been widely used in super-
vised WSD systems, in particular features based
on dependency parsing. While this might seem
to be in contrast to our labelling algorithm which
is based on shallow linguistic preprocessing, it is
fully justified by the purpose of our extrinsic eval-
uation: The main purpose of the extrinsic evalua-
tion is not to outperform state-of-the-art VSD sys-
tems, but to show that, when operating with rea-
sonable features, a classifier trained on the data
automatically labelled with our method performs
equally well as when this classifier is trained on
manually annotated data.
4.2.1 Features
The training and test data are parsed with the Stan-
ford parser (Klein and Manning, 2003) which pro-
vides Stanford Dependencies output (De Marneffe
et al., 2006) as well as phrase structure trees. We
employ the Stanford Named Entity Recogniser to
identify named entities. We then extract lexical,
syntactic, and semantic features from the parse re-
sults for classification.
Lexical features include the lemmas and POS
tags of the two words before and after the tar-
get verb. To extract syntactic features we select
all dependency relations from the parser output in
72
SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets)
Some of the New York Philharmonic
musicians who live in the suburbs spent
yesterday morning digging themselves
free from snow.
of group person who live
in location VVD time time
VVG
0.29 live%2:42:08:: (live%2:42:08::)
These societies can expect to face diffi-
cult times.
group expect to VV JJ
event
0.22 expect%2:31:01:: (expect%2:31:01::)
As autumn starts its annual sweep , few
Americans and Canadians realize how
fortunate they are in having the world ?s
finest fall coloring.
JJ attribute JJ person real-
ize how JJ PP be in
0.22 realize%2:31:00:: ? perceive (an idea or
situation) mentally (realize%2:31:01::
? be fully aware or cognizant of)
Dan Morgan told himself he would for-
get Ann Turner.
person person VVD PP PP
forget person location
0.16 forget%2:31:00:: ? be unable to re-
member (forget%2:31:01:: ? dismiss
from the mind; stop remembering)
Table 4: Examples of SemCor instances with high similarity scores (upper half shows correctly labelled
instances, lower half incorrectly labelled instances.
which the target verb is related to a noun, a pro-
noun, or a named entity. For each selected word,
the lemma of the word (or the named entity tag in
case of proper nouns) is combined with the type
of the dependency relation which exists between
it and the verb to form a separate feature. In a
similar feature, the lemma of the selected word is
replaced by its POS tag. The semantic features
include all synsets found in WN for nominal argu-
ments of the verb. Personal pronouns are mapped
to ?person? and the three synsets found in WN 3.0
for this word are taken as features.
4.2.2 Train and Test Data
Using exactly the same method as intrinsically
evaluated in section 4.1, we automatically labelled
occurrences of the 16 MASC verbs and the 305
Senseval-3 verbs in both web corpora with WN
senses. Only occurrences with similarity score
above 0.1 are labelled ? all other occurrences are
discarded. We refer to the resulting data as au-
tomatically labelled corpus (ALC) and use it as
training data for statistical VSD.
Instances of the test verbs found in SemCor are
also used as training data in order to compare the
performance of the classifier in a fully supervised
setting.
MASC. There are 22 senses with instances in
MASC which are not found in SemCor. For the
ALC this number is 34. However, in the latter
there are 27 senses, instances of which are un-
seen in MASC. 20 of those represent phrasal verbs
which we attribute to the special treatment of such
verbs in our method.
The classifier cannot correctly classify senses
which are not seen in the training data. The cov-
erage of the ALC is 88.05% and that of SemCor
? 94.8%. The SemCor data can mainly cover
more test instances of 3 verbs ? launch, rule, and
transfer ? the WN senses of which lack sense
examples or links to other senses in UBY. Un-
like the hand-labelled SemCor data, our automated
sense labelling method is limited to the informa-
tion found in the LLR used. However, there are
also 330 MASC instances covered by the ALC
only. Those are mostly instances of phrasal verbs,
such as rip off and show up. Note that the defini-
tion of coverage we use here makes its values the
upper bounds for the performance of the classifier.
Senseval-3. We also generated training data au-
tomatically for the 305 Senseval verbs. However,
only 152 of those verbs (442 instances) are found
in SemCor. This means we cannot train the classi-
fier for the remaining Senseval verbs. The cover-
age of the SemCor training data for the 152 verbs
which can be classified is 96.15% and that of the
ALC ? 95.25%. For all 592 Senseval test in-
stances, the coverage of the ALC is 90.38%.
4.2.3 Results and Analysis
We trained a separate logistic regression classi-
fier for each test verb in the two datasets us-
ing the WEKA data mining software (Hall et al.,
2009) with default parameters. The classifiers
were trained with features extracted from (i) the
SemCor hand-labelled data and (ii) the ALC.
MASC. The classifier achieves 50.23% accu-
racy when SemCor is used and 49% when the
ALC is employed. The difference in the results is
not statistically significant at p < 0.05. The MFS
73
baseline scores at 41.72%.
Senseval-3. The classifier achieves 43.24%
with the ALC. We assigned the MFS to each of
the 143 test verbs not found in SemCor since we
cannot train the classifier for those. The achieved
accuracy is 45.2%. We also measured accuracy
in a setup where no MFS back-off strategy was
employed for SemCor (152 test verbs with 442
instances). When trained on SemCor data, the
classifier achieves 48.64% accuracy compared to
47.51% for the ALC. All differences in the results
are not statistically significant at p < 0.05. Fi-
nally, the MFS baseline accuracy is significantly
lower at 25.34% for all 305 test verbs.
For both test datasets, the overall performance
of the classifier when trained on automatically la-
belled data is very close to the setting in which
manually created training data is employed. We
thus conclude that the quality of the data produced
by our sense labelling method is sufficient and
these data can be directly used for training a statis-
tical VSD classifier. As a reference, the state-of-
the-art supervised VSD system described in Chen
and Palmer (2009) achieves 64.8% accuracy on the
Senseval-2 fine-grained data. However, we cannot
compare to this result due to the different sense in-
ventory which the Senseval-2 data were annotated
with.
4.2.4 Sense Links
In order to investigate the effect of LLRs, we
performed experiments in which sense examples
found in WN only were used. We also experi-
mented with various combinations of the resources
available in UBY to determine the contribution of
each of those to our method. Table 5 shows the re-
sults. The setting which includes only WN has the
worst performance, thus clearly showing the ben-
efits of using LLRs. Next, the inclusion of WKT
improves both coverage and accuracy. We con-
clude that WKT plays an important role in discov-
ering additional verb senses. Finally, similarly to
the results of the intrinsic evaluation, adding VN
to the mix increases slightly the coverage but de-
creases accuracy.
5 Related Work and Discussion
Our work is related to previous research on
(i) using a combination of lexical resources for
knowledge-based WSD, (ii) using lexical re-
sources for distant supervision, and (iii) the auto-
mated acquisition of sense-annotated data.
MASC Senseval
Cov Acc Cov Acc
WN 0.6573 0.3498 0.6372 0.3209
WN?FN 0.8562 0.4810 0.8812 0.4172
WN?FN?WKT 0.8805 0.4900 0.9038 0.4324
WN?FN?WKT?VN 0.8822 0.4688 0.9139 0.4054
Table 5: Performance of the various combinations
of lexical resources.
Knowledge-based WSD. While the combina-
tion of sense-annotated data and wordnets has
been described for knowledge-based WSD before
(e.g., Navigli and Velardi (2005; Agirre and Soroa
(2009) who use graph algorithms), only recently
Ponzetto and Navigli (2010) have investigated the
impact of the combination of different lexical re-
sources on the performance of WSD. They aligned
WN senses with Wikipedia articles and employed
two simple knowledge-based algorithms, i.e., a
Lesk-based algorithm and a graph-based algo-
rithm, to evaluate the resulting LLR for WSD.
While their evaluation demonstrates that the use
of an LLR boosts the performance of knowledege-
based WSD, it is restricted to nouns only since
Wikipedia provides very few verb senses. More-
over, lexical resources that are rich in lexical-
syntactic information such as VN have not been
involved.
Miller et al. (2012) employ a Lesk-based algo-
rithm which makes use of a combination of WN
and an automatically acquired distributional the-
saurus. Lesk-based algorithms play a central role
in knowledge-based WSD. Based on the overlap
of the context of the target word and sense defi-
nitions in a given sense inventory, they assign the
sense with the highest overlap as disambiguation
result. We were kindly provided with the system
described in Miller et al. (2012) and we were able
to test its performance on our test sets. The sys-
tem achieved only 33.86% and 30.16% accuracy
for the MASC and the Senseval-3 verbs, respec-
tively, which is far below the results we presented.
This low performance is due to the fact that Lesk-
based algorithms do not account for word order.
Such information is important especially for verb
senses, as the syntactic behaviour of a verb reflects
aspects of its meaning.
Distant supervision. Distant supervision is
a learning paradigm similar to semi-supervised
learning. Unlike semi-supervised methods which
typically employ a supervised classifier and a
74
small number of seed instances to do bootstrap
learning (Yarowsky, 1995; Mihalcea, 2004; Fujita
and Fujino, 2011), in distant supervision training
data are created in a single run from scratch by
aligning corpus instances with entries in a knowl-
edge base. Distant supervision methods that have
used LLRs as knowledge bases have been previ-
ously applied in relation extraction, e.g. Freebase
(Mintz et al., 2009; Surdeanu et al., 2012) and Ba-
belNet (Krause et al., 2012; Moro et al., 2013).
However, as far as we are aware, we are the first to
apply distant supervision to the task of verb sense
disambiguation.
Acquisition of sense-annotated data. Most
previous work on using lexical resources for au-
tomatically acquiring sense-annotated data either
was mostly restricted to noun senses or, unlike
us, did not present a meaningful evaluation. Lea-
cock et al. (1998) describe the automated creation
of training data for supervised WSD on the ba-
sis of WN as a lexical resource combined with
corpus statistics, but they evaluate their approach
just on one noun, verb, and adjective, and thus
it is unclear whether their results can be gener-
alized. Cuadros and Rigau (2008) used the ap-
proach of Leacock et al. (1998) to automatically
build a large KnowNet from the Web, but they
evaluated this resource only for WSD of nouns.
However, the system based on KnowNet yields re-
sults below the SemCor-MFS baseline. Mihalcea
and Moldovan (1999) use WordNet glosses to ex-
tract sense examples from the Web via a search en-
gine and use this approach in a subsequent paper
(Mihalcea, 2002) to generate a sense tagged cor-
pus. For five randomly selected nouns, they per-
formed a comparative evaluation of a WSD classi-
fier trained on an automatically tagged corpus on
the one hand, and on the manually annotated data
from the Senseval-2 English lexical sample task
on the other hand. The results obtained for these
five nouns seem to be similar but the dataset used
is too small to draw meaningful conclusions and
moreover, it does not cover verbs. Mostow and
Duan (2011) presented a system that extracts ex-
ample contexts for nouns and apply these contexts
in (Duan and Yates, 2010) for WSD by using them
to label text and train a statistical classifier. An
evaluation of this classifier yielded results similar
to those obtained by a supervised WSD system.
K?ubler and Zhekova (2009) extract example
sentences from several English dictionaries and
various types of corpora, including web corpora.
They employ a Lesk-based algorithm to automati-
cally annotate the target word instances in the ex-
tracted example sentences with WN senses and
use them in one of their experiments as train-
ing data for a WSD classifier. However, the per-
formance of the system decreased significantly
achieving the lowest accuracy among all system
configurations. The authors provide only the over-
all accuracy score, so we do not know how disam-
biguation of verbs was affected.
Summary. We consider the ability to estab-
lish a link between the rich knowledge available in
LLRs and corpora of any kind to be the main ad-
vantage of our automated labelling method. How-
ever, to automatically label a suffcient amount
of data for supervised learning, very large cor-
pora are required. Our method can be extended
to other POS (using sense examples and possibly
other types of lexical information), as well as to
other languages where (linked) lexical resources
are available.
6 Conclusion
In this paper, we presented a novel method for cre-
ating sense labelled corpora automatically. We ex-
ploit LLRs and perform large-scale intrinsic and
application-based evaluations. The results of those
evaluations show that the quality of the sense la-
belled corpora created with our method matches
that of manually annotated corpora.
In future research, we plan to use PropBank
(Palmer et al., 2005) in order to extract sense
examples for VN as well. This might improve
the performance of lexical resource combinations
which include VN. We will also apply our method
to languages (e.g., German) for which lexical re-
sources are available but no or little sense anno-
tated corpora exist.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg- Professor-
ship Program under grant No. I/82806 and by the
German Research Foundation under grant No. GU
798/9-1. We would like to thank the anonymous
reviewers for their valuable feedback.
75
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2009), pages 33?41, Athens, Greece.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics-Volume 1, pages
86?90, Montreal, Canada.
Timothy Baldwin, Sunam Kim, Francis Bond, Sanae
Fujita, David Martinez, and Takaaki Tanaka. 2010.
A Reexamination of MRD-Based Word Sense Dis-
ambiguation. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 9(1):4:1?
4:21.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Jinying Chen and Martha Palmer. 2009. Improv-
ing English verb sense disambiguation performance
with linguistically motivated features and clear sense
distinction boundaries. Language Resources and
Evaluation, 43:181?208.
Montse Cuadros and German Rigau. 2008. Knownet:
Building a large net of knowledge from the web.
In 22nd International Conference on Computational
Linguistics (COLING), pages 161?168, Manchester,
UK.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC 2006), pages 449?454, Genoa,
Italy.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
627?635, Los Angeles, USA.
Judith Eckle-Kohler and Iryna Gurevych. 2012.
Subcat-LMF: Fleshing out a standardized format for
subcategorization frame interoperability. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 550?560, Avignon,
France.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-
mann, Michael Matuschek, and Christian M. Meyer.
2012. UBY-LMF ? A uniform format for standard-
izing heterogeneous lexical-semantic resources in
ISO-LMF. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 275?282, Istanbul, Turkey.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Sanae Fujita and Akinori Fujino. 2011. Word sense
disambiguation by combining labeled data expan-
sion and semi-supervised learning method. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 676?685,
Chiang Mai, Thailand.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale uni-
fied lexical-semantic resource based on LMF. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2012), pages 580?590.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42:21?40.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423?430, Sapporo,
Japan. Association for Computational Linguistics.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 180?183, Edmonton,
Canada.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International Se-
mantic Web Conference, pages 263?278, Boston,
Masachusetts, USA, 11. Springer.
Sandra K?ubler and Desislava Zhekova. 2009. Semi-
Supervised Learning for Word Sense Disambigua-
tion: Quality vs. Quantity. In Proceedings of the
International Conference RANLP-2009, pages 197?
202, Borovets, Bulgaria.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and word-
net relations for sense identification. Computational
Linguistics, 24(1):147?165.
76
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference on
Systems Documentation, pages 24?26, Toronto, On-
tario, Canada.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Rada Mihalcea and Dan Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of the American Association for Ar-
tificial Intelligence (AAAI 1999), Orlando, Florida,
USA.
Rada Mihalcea. 2002. Bootstrapping large sense
tagged corpora. In Proceedings of the Third Interna-
tional Conference of Language Resources and Eval-
uation (LREC 2002), pages 1407?1411, Las Palmas,
Canary Islands, Spain.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings
of the Conference on Computational Natural Lan-
guage Learning (CoNLL-2004), Boston, MA, USA.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of the 24th
International Conference on Computational Lin-
guistics (COLING 2012), pages 1781?1796, Mum-
bai, India.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore.
Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu,
Roberto Navigli, and Hans Uszkoreit. 2013. Se-
mantic rule filtering for web-scale relation extrac-
tion. In Proceedings of the 12th International
Semantic Web Conference, Sydney, Australia, 10.
Springer.
Jack Mostow and Weisi Duan. 2011. Generating ex-
ample contexts to illustrate a target word sense. In
Proceedings of the Sixth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 105?110, Portland, Oregon, June. Association
for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: A knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1086.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1):71?
105.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1522?1531, Uppsala,
Sweden.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465, Jeju Island, Korea.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd annual meeting on Associa-
tion for Computational Linguistics, pages 189?196,
Cambridge, Massachusetts, USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the 6th
International Conference on Language Resources
and Evaluation (LREC 2008), volume 8, pages
1646?1652, Marrakech, Morocco.
77
