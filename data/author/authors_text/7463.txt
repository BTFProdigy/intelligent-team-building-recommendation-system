The LinGO Redwoods Treebank
Motivation and Preliminary Applications
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten Brants
{oe |kristina |manning |dan}@csli.stanford.edu,
shieber@deas.harvard.edu, brants@parc.xerox.com
Abstract
The LinGO Redwoods initiative is a seed activity in the de-
sign and development of a new type of treebank. While sev-
eral medium- to large-scale treebanks exist for English (and
for other major languages), pre-existing publicly available re-
sources exhibit the following limitations: (i) annotation is
mono-stratal, either encoding topological (phrase structure) or
tectogrammatical (dependency) information, (ii) the depth of
linguistic information recorded is comparatively shallow, (iii)
the design and format of linguistic representation in the tree-
bank hard-wires a small, predefined range of ways in which
information can be extracted from the treebank, and (iv) rep-
resentations in existing treebanks are static and over the (often
year- or decade-long) evolution of a large-scale treebank tend
to fall behind the development of the field. LinGO Redwoods
aims at the development of a novel treebanking methodology,
rich in nature and dynamic both in the ways linguistic data can
be retrieved from the treebank in varying granularity and in the
constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foun-
dations for this new type of treebank, to develop a basic set of
tools for treebank construction and maintenance, and to con-
struct an initial set of 10,000 annotated trees to be distributed
together with the tools under an open-source license.
1 Why Another (Type of) Treebank?
For the past decade or more, symbolic, linguistically ori-
ented methods and statistical or machine learning ap-
proaches to NLP have often been perceived as incompat-
ible or even competing paradigms. While shallow and
probabilistic processing techniques have produced use-
ful results in many classes of applications, they have not
met the full range of needs for NLP, particularly where
precise interpretation is important, or where the variety
of linguistic expression is large relative to the amount
of training data available. On the other hand, deep
approaches to NLP have only recently achieved broad
enough grammatical coverage and sufficient processing
efficiency to allow the use of precise linguistic grammars
in certain types of real-world applications.
In particular, applications of broad-coverage analyti-
cal grammars for parsing or generation require the use of
sophisticated statistical techniques for resolving ambigu-
ities; the transfer of Head-Driven Phrase Structure Gram-
mar (HPSG) systems into industry, for example, has am-
plified the need for general parse ranking, disambigua-
tion, and robust recovery techniques. We observe general
consensus on the necessity for bridging activities, com-
bining symbolic and stochastic approaches to NLP. But
although we find promising research in stochastic pars-
ing in a number of frameworks, there is a lack of appro-
priately rich and dynamic language corpora for HPSG.
Likewise, stochastic parsing has so far been focussed on
information-extraction-type applications and lacks any
depth of semantic interpretation. The Redwoods initia-
tive is designed to fill in this gap.
In the next section, we present some of the motivation
for the LinGO Redwoods project as a treebank develop-
ment process. Although construction of the treebank is
in its early stages, we present in Section 3 some prelim-
inary results of using the treebank data already acquired
on concrete applications. We show, for instance, that
even simple statistical models of parse ranking trained
on the Redwoods corpus built so far can disambiguate
parses with close to 80% accuracy.
2 A Rich and Dynamic Treebank
The Redwoods treebank is based on open-source HPSG
resources developed by a broad consortium of re-
search groups including researchers at Stanford (USA),
Saarbru?cken (Germany), Cambridge, Edinburgh, and
Sussex (UK), and Tokyo (Japan). Their wide distribution
and common acceptance make the HPSG framework and
resources an excellent anchor point for the Redwoods
treebanking initiative.
The key innovative aspect of the Redwoods ap-
proach to treebanking is the anchoring of all linguis-
tic data captured in the treebank to the HPSG frame-
work and a generally-available broad-coverage gram-
mar of English, the LinGO English Resource Grammar
(Flickinger, 2000) as implemented with the LKB gram-
mar development environment (Copestake, 2002). Un-
like existing treebanks, there is no need to define a (new)
form of grammatical representation specific to the tree-
bank. Instead, the treebank records complete syntacto-
semantic analyses as defined by the LinGO ERG and pro-
vide tools to extract different types of linguistic informa-
tion at varying granularity.
The treebanking environment, building on the [incr
tsdb()] profiling environment (Oepen & Callmeier,
2000), presents annotators, one sentence at a time, with
the full set of analyses produced by the grammar. Using
a pre-existing tree comparison tool in the LKB (similar
in kind to the SRI Cambridge TreeBanker; Carter, 1997),
annotators can quickly navigate through the parse for-
est and identify the correct or preferred analysis in the
current context (or, in rare cases, reject all analyses pro-
posed by the grammar). The tree selection tool presents
users, who need little expert knowledge of the underly-
ing grammar, with a range of basic properties that distin-
guish competing analyses and that are relatively easy to
judge. All disambiguating decisions made by annotators
are recorded in the [incr tsdb()] database and thus become
available for (i) later dynamic extraction from the anno-
tated profile or (ii) dynamic propagation into a more re-
cent profile obtained from re-running a newer version of
the grammar on the same corpus.
Important innovative research aspects in this approach
to treebanking are (i) enabling users of the treebank to
extract information of the type they need and to trans-
form the available representation into a form suited to
their needs and (ii) the ability to update the treebank with
an enhanced version of the grammar in an automated
fashion, viz. by re-applying the disambiguating decisions
on the corpus with an updated version of the grammar.
Depth of Representation and Transformation of In-
formation Internally, the [incr tsdb()] database records
analyses in three different formats, viz. (i) as a deriva-
tion tree composed of identifiers of lexical items and con-
structions used to build the analysis, (ii) as a traditional
phrase structure tree labeled with an inventory of some
fifty atomic labels (of the type ?S?, ?NP?, ?VP? et al), and
(iii) as an underspecified MRS (Copestake, Lascarides,
& Flickinger, 2001) meaning representation. While rep-
resentation (ii) will in many cases be similar to the rep-
resentation found in the Penn Treebank, representation
(iii) subsumes the functor ? argument (or tectogrammati-
cal) structure advocated in the Prague Dependency Tree-
bank or the German TiGer corpus. Most importantly,
however, representation (i) provides all the information
required to replay the full HPSG analysis (using the orig-
inal grammar and one of the open-source HPSG process-
ing environments, e.g., the LKB or PET, which already
have been interfaced to [incr tsdb()]). Using the latter ap-
proach, users of the treebank are enabled to extract infor-
mation in whatever representation they require, simply
by reconstructing full analyses and adapting the exist-
ing mappings (e.g., the inventory of node labels used for
phrase structure trees) to their needs. Likewise, the ex-
isting [incr tsdb()] facilities for comparing across compe-
tence and performance profiles can be deployed to evalu-
ate results of a (stochastic) parse disambiguation system,
essentially using the preferences recorded in the treebank
as a ?gold standard? target for comparison.
Automating Treebank Construction Although a pre-
cise HPSG grammar like the LinGO ERG will typically
assign a small number of analyses to a given sentence,
choosing among a few or sometimes a few dozen read-
ings is time-consuming and error-prone. The project is
exploring two approaches to automating the disambigua-
tion task, (i) seeding lexical selection from a part-of-
speech (POS) tagger and (ii) automated inter-annotator
comparison and assisted resolution of conflicts.
Treebank Maintenance and Evolution One of the
challenging research aspects of the Redwoods initiative
is about developing a methodology for automated up-
dates of the treebank to reflect the continuous evolution
of the underlying linguistic framework and of the LinGO
grammar. Again building on the notion of elementary
linguistic discriminators, we expect to explore the semi-
automatic propagation of recorded disambiguating deci-
sions into newer versions of the parsed corpus. While
it can be assumed that the basic phrase structure inven-
tory and granularity of lexical distinctions have stabilized
to a certain degree, it is not guaranteed that one set of
discriminators will always fully disambiguate a more re-
cent set of analyses for the same utterance (as the gram-
mar may introduce new ambiguity), nor that re-playing
a history of disambiguating decisions will necessarily
identify the correct, preferred analysis for all sentences.
A better understanding of the nature of discriminators
and relations holding among them is expected to provide
the foundations for an update procedure that, ultimately,
should be mostly automated, with minimal manual in-
spection, and which can become part of the regular re-
gression test cycle for the grammar.
Scope and Current State of Seeding Initiative The
first 10,000 trees to be hand-annotated as part of the
kick-off initiative are taken from a domain for which the
English Resource Grammar is known to exhibit broad
and accurate coverage, viz. transcribed face-to-face dia-
logues in an appointment scheduling and travel arrange-
ment domain.1 For the follow-up phase of the project, it
is expected to move into a second domain and text genre,
presumably more formal, edited text taken from newspa-
per text or another widely available on-line source. As
of June 2002, the seeding initiative is well underway.
The integrated treebanking environment, combining [incr
tsdb()] and the LKB tree selection tool, has been estab-
lished and has been deployed in a first iteration of anno-
tating the VerbMobil utterances. The approach to parse
selection through minimal discriminators turned out to
be not hard to learn for a second-year Stanford under-
graduate in linguistics, and allowed completion of the
first iteration in less than ten weeks. Table 1 summarizes
the current Redwoods status.
1Corpora of some 50,000 such utterances are readily available from
the VerbMobil project (Wahlster, 2000) and have already been studied
extensively among researchers world-wide.
2Of the four data sets only VM32 has been double-checked by
an expert grammarian and (almost) completely disambiguated to date;
therefore it exhibits an interestingly higher degree of phrasal ambiguity
in the ?active = 1? subset.
total active = 0 active = 1 active > 1 unannotated
corpus ] ?  ? ] ?  ? ] ?  ? ] ?  ? ] ?  ?
VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5
VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1
VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5
VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0
Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have
been annotated. The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO
grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (?? and ???, respectively), followed
by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active
trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was
not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that
have yet to be annotated.2
3 Early Experimental Results
Development of the treebank has just started. Nonethe-
less, we have performed some preliminary experiments
on concrete applications to motivate the utility of the re-
source being developed. In this section, we describe ex-
periments using the Redwoods treebank to build and test
systems for parse disambiguation. As a component, we
build a tagger for the HPSG lexical tags in the treebank,
and report results on this application as well.
Any linguistic system that allows multiple parses
of strings must address the problem of selecting from
among the admitted parses the preferred one. A variety
of approaches for building statistical models of parse se-
lection are possible. At the simplest end, we might look
only at the lexical type sequence assigned to the words
by each parse and rank the parse based on the likelihood
of that sequence. These lexical types ? the preterminals
in the derivation ? are essentially part-of-speech tags, but
encode considerably finer-grained information about the
words. Well-understood statistical part-of-speech tag-
ging technology is sufficient for this approach.
In order to use more information about the parse,
we might examine the entire derivation of the string.
Most probabilistic parsing research ? including, for ex-
ample, work by by Collins (1997), and Charniak (1997)
? is based on branching process models (Harris, 1963).
The HPSG derivations that the treebank makes available
can be viewed as just such a branching process, and
a stochastic model of the trees can be built as a prob-
abilistic context-free grammar (PCFG) model. Abney
(1997) notes important problems with the soundness of
the approach when a unification-based grammar is ac-
tually determining the derivations, motivating the use
of log-linear models (Agresti, 1990) for parse ranking
that Johnson and colleagues further developed (Johnson,
Geman, Canon, Chi, & Riezler, 1999). These models
can deal with the many interacting dependencies and
the structural complexity found in constraint-based or
unification-based theories of syntax.
Nevertheless, the naive PCFG approach has the advan-
tage of simplicity, so we pursue it and the tagging ap-
proach to parse ranking in these proof-of-concept exper-
iments (more recently, we have begun work on building
log-linear models over HPSG signs (Toutanova & Man-
ning, 2002)). The learned models were used to rank
possible parses of unseen test sentences according to the
probabilities they assign to them. We report parse se-
lection performance as percentage of test sentences for
which the correct parse was highest ranked by the model.
(We restrict attention in the test corpus to sentences that
are ambiguous according to the grammar, that is, for
which the parse selection task is nontrivial.) We examine
four models: an HMM tagging model, a simple PCFG, a
PCFG with grandparent annotation, and a hybrid model
that combines predictions from the PCFG and the tagger.
These models will be described in more detail presently.
The tagger that we have implemented is a standard tri-
gram HMM tagger, defining a joint probability distribu-
tion over the preterminal sequences and yields of these
trees. Trigram probabilities are smoothed by linear in-
terpolation with lower-order models. For comparison,
we present the performance of a unigram tagger and an
upper-bound oracle tagger that knows the true tag se-
quence and scores highest the parses that have the correct
preterminal sequence.
The PCFG models define probability distributions
over the trees of derivational types corresponding to the
HPSG analyses of sentences. A PCFG model has parame-
ters ?i, j for each rule Ai ? ? j in the corresponding con-
text free grammar.3 In our application, the nonterminals
in the PCFG Ai are rules of the HPSG grammar used to
build the parses (such as HEAD-COMPL or HEAD-ADJ).
We set the parameters to maximize the likelihood of the
set of derivation trees for the preferred parses of the sen-
tences in a training set. As noted above, estimating prob-
abilities from local tree counts in the treebank does not
provide a maximum likelihood estimate of the observed
data, as the grammar rules further constrain the possible
derivations. Essentially, we are making an assumption of
context-freeness of rule application that does not hold in
the case of the HPSG grammar. Nonetheless, we can still
build the model and use it to rank parses.
3For an introduction to PCFG grammars see, for example, Manning
& Schu?tze (1999).
As previously noted by other researchers (Charniak &
Caroll, 1994), extending a PCFG with grandparent an-
notation improves the accuracy of the model. We imple-
mented an extended PCFG that conditions each node?s
expansion on its parent in the phrase structure tree. The
extended PCFG (henceforth PCFG-GP) has parameters
P(Ak Ai ? ? j |Ak, Ai) . The resulting grammar can be
viewed as a PCFG whose nonterminals are pairs of the
nonterminals of the original PCFG.
The combined model scores possible parses using
probabilities from the PCFG-GP model together with the
probability of the preterminal sequence of the parse tree
according to a trigram tag sequence model. More specif-
ically, for a tree T ,
Score(t) = log(PPCFG-GP(T )) + ? log(PTRIG(tags(T ))
where PTRIG(tags(T )) is the probability of the sequence
of preterminals t1 ? ? ? tn in T according to a trigram tag
model:
PTRIG(t1 ? ? ? tn) =
?n
i=1
P(ti |ti?1, ti?2)
with appropriate treatment of boundaries. The trigram
probabilities are smoothed as for the HMM tagger. The
combined model is relatively insensitive to the relative
weights of the two component models, as specified by ?;
in any case, exact optimization of this parameter was not
performed. We refer to this model as Combined. The
Combined model is not a sound probabilistic model as it
does not define a probability distribution over parse trees.
It does however provide a crude way to combine ancestor
and left context information.
The second column in Table 2 shows the accuracy
of parse selection using the models described above.
For comparison, a baseline showing the expected perfor-
mance of choosing parses randomly according to a uni-
form distribution is included as the first row. The accu-
racy results are averaged over a ten-fold cross-validation
on the data set summarized in Table 1. The data we used
for this experiment was the set of disambiguated sen-
tences that have exactly one preferred parse (comprising
a total of 5312 sentences). Often the stochastic models
we are considering give the same score to several differ-
ent parses. When a model ranks a set of m parses highest
with equal scores and one of those parses is the preferred
parse in the treebank, we compute the accuracy on this
sentence as 1/m.
Since our approach of defining the probability of anal-
yses using derivation trees is different from the tradi-
tional approach of learning PCFG grammars from phrase
structure trees, a comparison of the two is probably in
order. We tested the model PCFG-GP defined over the
corresponding phrase structure trees and its average ac-
curacy was 65.65% which is much lower than the accu-
racy of the same model over derivation trees (71.73%).
This result suggests that the information about grammar
constructions is very helpful for parse disambiguation.
Method Task
tag sel. parse sel.
Random 90.13% 25.81%
Tagger unigram 96.75% 44.15%
trigram 97.87% 47.74%
oracle 100.00% 54.59%
PCFG simple 97.40% 66.26%
grandparent 97.43% 71.73%
combined 98.08% 74.03%
Table 2: Performance of the HMM and PCFG models for the
tag and parse selection tasks (accuracy).
The results in Table 2 indicate that high disambigua-
tion accuracy can be achieved using very simple statisti-
cal models. The performance of the perfect tagger shows
that, informally speaking, roughly half of the information
necessary to disambiguate parses is available in the lexi-
cal types alone. About half of the remaining information
is recovered by our best method, Combined.
An alternative (more primitive) task is the tagging task
itself. It is interesting to know how much the tagging
task can be improved by perfecting parse disambigua-
tion. With the availability of a parser, we can examine the
accuracy of the tag sequence of the highest scoring parse,
rather than trying to tag the word sequence directly. We
refer to this problem as the tag selection problem, by
analogy with the relation between the parsing problem
and the parse selection problem. The first column of Ta-
ble 2 presents the performance of the models on the tag
selection problem. The results are averaged accuracies
over 10 cross-validation splits of the same corpus as the
previous experiment, and show that parse disambigua-
tion using information beyond the lexical type sequence
slightly improves tag selection performance. Note that
in these experiments, the models are used to rank the tag
sequences of the possible parses and not to find the most
probable tag sequence. Therefore tagging accuracy re-
sults are higher than they would be in the latter case.
Since our corpus has relatively short sentences and low
ambiguity it is interesting to see how much the perfor-
mance degrades as we move to longer and more highly
ambiguous sentences. For this purpose, we report in Ta-
ble 3 the parse ranking accuracy of the Combined model
as a function of the number of possible analyses for sen-
tences. Each row corresponds to a set of sentences with
number of possible analyses greater or equal to the bound
shown in the first column. For example, the first row con-
tains information for the sentences with ambiguity ? 2,
which is all ambiguous sentences. The columns show the
total number of sentences in the set, the expected accu-
racy of guessing at random, and the accuracy of the Com-
bined model. We can see that the parse ranking accuracy
is decreasing quickly and more powerful models will be
needed to achieve good accuracy for highly ambiguous
sentences.
Despite several differences in corpus size and compo-
Analyses Sentences Random Combined
? 2 3824 25.81% 74.03%
? 5 1789 9.66% 59.64%
? 10 1027 5.33% 51.61%
? 20 525 3.03% 45.33%
Table 3: Parse ranking accuracy by number of possible parses.
sition, it is perhaps nevertheless useful to compare this
work with other work on parse selection for unification-
based grammars. Johnson et al (1999) estimate a
Stochastic Unification Based Grammar (SUBG) using a
log-linear model. The features they include in the model
are not limited to production rule features but also ad-
junct and argument and other linguistically motivated
features. On a dataset of 540 sentences (total training
and test set) from a Verbmobil corpus they report parse
disambiguation accuracy of 58.7% given a baseline accu-
racy for choosing at random of 9.7%. The random base-
line is much lower than ours for the full data set, but it is
comparable for the random baseline for sentences with
more than 5 analyses. The accuracy of our Combined
model for these sentences is 59.64%, so the accuracies
of the two models seem fairly similar.
4 Related Work
To the best of our knowledge, no prior research has
been conducted exploring the linguistic depth, flexibil-
ity in available information, and dynamic nature of tree-
banks that we have proposed. Earlier work on building
corpora of hand-selected analyses relative to an exist-
ing broad-coverage grammar was carried out at Xerox
PARC, SRI Cambridge, and Microsoft Research. As all
these resources are tuned to proprietary grammars and
analysis engines, the resulting treebanks are not publicly
available, nor have reported research results been repro-
ducible. Yet, especially in light of the successful LinGO
open-source repository, it seems vital that both the tree-
bank and associated processing schemes and stochastic
models be available to the general (academic) public. An
on-going initiative at Rijksuniversiteit Groningen (NL) is
developing a treebank of dependency structures (Mullen,
Malouf, & Noord, 2001), derived from an HPSG-like
grammar of Dutch (Bouma, Noord, & Malouf, 2001).
The general approach resembles the Redwoods initiative
(specifically the discriminator-based method of tree se-
lection; the LKB tree comparison tool was originally de-
veloped by Malouf, after all), but it provides only a sin-
gle stratum of representation, and has no provision for
evolving analyses in tandem with the grammar. Dipper
(2000) presents the application of a broad-coverage LFG
grammar for German to constructing tectogrammatical
structures for the TiGer corpus. The approach is similar
to the Groningen framework, and shares its limitations.
References
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23, 597 ? 618.
Agresti, A. (1990). Categorical data analysis. John Wiley &
Sons.
Bouma, G., Noord, G. van, & Malouf, R. (2001).
Alpino. Wide-coverage computational analysis of Dutch. In
W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.),
Computational linguistics in the Netherlands (pp. 45 ? 59).
Amsterdam, The Netherlands: Rodopi.
Carter, D. (1997). The TreeBanker. A tool for supervised
training of parsed corpora. In Proceedings of the Workshop
on Computational Environments for Grammar Development
and Linguistic Engineering. Madrid, Spain.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the Four-
teenth National Conference on Artificial Intelligence (pp.
598 ? 603). Providence, RI.
Charniak, E., & Caroll, G. (1994). Context-sensitive statistics
for improved grammatical language models. In Proceedings
of the Twelth National Conference on Artificial Intelligence
(pp. 742 ? 747). Seattle, WA.
Collins, M. J. (1997). Three generative, lexicalised models for
statistical parsing. In Proceedings of the 35th Meeting of
the Association for Computational Linguistics and the 7th
Conference of the European Chapter of the ACL (pp. 16 ?
23). Madrid, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Copestake, A., Lascarides, A., & Flickinger, D. (2001). An
algebra for semantic construction in constraint-based gram-
mars. In Proceedings of the 39th Meeting of the Association
for Computational Linguistics. Toulouse, France.
Dipper, S. (2000). Grammar-based corpus annotation. In
Workshop on linguistically interpreted corpora LINC-2000
(pp. 56 ? 64). Luxembourg.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG), 15 ? 28.
Harris, T. E. (1963). The theory of branching processes.
Berlin, Germany: Springer.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Associa-
tion for Computational Linguistics (pp. 535 ? 541). College
Park, MD.
Manning, C. D., & Schu?tze, H. (1999). Foundations of statis-
tical Natural Language Processing. Cambridge, MA: MIT
Press.
Mullen, T., Malouf, R., & Noord, G. van. (2001). Statistical
parsing of Dutch using Maximum Entropy models with fea-
ture merging. In Proceedings of the Natural Language Pro-
cessing Pacific Rim Symposium. Tokyo, Japan.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased compo-
nent comparability and exchange. In Proceedings of the 6th
International Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Toutanova, K., & Manning, C. D. (2002). Feature selection
for a rich HPSG grammar using decision trees. In Proceed-
ings of the sixth conference on natural language learning
(CoNLL-2002). Taipei.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of speech-
to-speech translation. Berlin, Germany: Springer.
Towards Robust Context-Sensitive Sentence Alignment for Monolingual
Corpora
Rani Nelken and Stuart M. Shieber
Division of Engineering and Applied Sciences
Harvard University
33 Oxford St.
Cambridge, MA 02138
 
nelken,shieber  @deas.harvard.edu
Abstract
Aligning sentences belonging to compa-
rable monolingual corpora has been sug-
gested as a first step towards training
text rewriting algorithms, for tasks such
as summarization or paraphrasing. We
present here a new monolingual sen-
tence alignment algorithm, combining a
sentence-based TF*IDF score, turned into
a probability distribution using logistic re-
gression, with a global alignment dynamic
programming algorithm. Our approach
provides a simpler and more robust solu-
tion achieving a substantial improvement
in accuracy over existing systems.
1 Introduction
Sentence-aligned bilingual corpora are a crucial
resource for training statistical machine trans-
lation systems. Several authors have sug-
gested that large-scale aligned monolingual cor-
pora could be similarly used to advance the perfor-
mance of monolingual text-to-text rewriting sys-
tems, for tasks including summarization (Knight
and Marcu, 2000; Jing, 2002) and paraphras-
ing (Barzilay and Elhadad, 2003; Quirk et al,
2004). Unlike bilingual corpora, such as the Cana-
dian Hansard corpus, which are relatively rare, it is
now fairly easy to amass corpora of related mono-
lingual documents. For instance, with the ad-
vent of news aggregator services such as ?Google
News?, one can readily collect multiple news sto-
ries covering the same news item (Dolan et al,
2004). Utilizing such a resource requires align-
ing related documents at a finer level of resolu-
tion, identifying which sentences from one docu-
ment align with which sentences from the other.
Previous work has shown that aligning related
monolingual documents is quite different from
the well-studied multi-lingual alignment task.
Whereas documents in a bilingual corpus are typ-
ically very closely aligned, monolingual corpora
exhibit a much looser level of alignment, with
similar content expressed using widely divergent
wording, grammatical form, and sentence order.
Consequently, many of the simple surface-based
methods that have proven to be so successful in
bilingual sentence alignment, such as correlation
of sentence length, linearity of alignment, and a
predominance of one-to-one sentence mapping,
are much less likely to be effective for monolin-
gual sentence alignment.
Barzilay and Elhadad (2003) suggested that
these disadvantages could be at least partially off-
set by the recurrence of the same lexical items in
document pairs. Indeed, they showed that a sim-
ple cosine word-overlap score is a good baseline
for the task, outperforming much more sophisti-
cated methods. They also observed that context is
a powerful factor in determining alignment. They
illustrated this on a corpus of Encyclopedia Bri-
tannica entries describing world cities, where each
entry comes in two flavors, the comprehensive en-
cyclopedia entry, and a shorter and simpler ele-
mentary version. Barzilay and Elhadad used con-
text in two different forms. First, using inter-
document context, they took advantage of com-
monalities in the topical structure of the encyclo-
pedia entries to identify paragraphs that are likely
to be about the same topic. They then took ad-
vantage of intra-document context by using dy-
namic programming to locally align sequences of
sentences belonging to paragraphs about the same
topic, yielding improved accuracy on the corpus.
While powerful, such commonalities in document
structure appear to be a special feature of the
Britannica corpus, and therefore cannot be relied
upon for other corpora.
In this paper we present a novel algorithm for
sentence alignment in monolingual corpora. At
the core of the algorithm is a classical similar-
161
ity score based on differentially weighting words
according to their Term Frequency-Inverse Doc-
ument Frequency (TF*IDF) (Spa?rck-Jones, 1972;
Salton and Buckley, 1988). We treat sentences as
documents, and the collection of sentences in the
two documents being compared as the document
collection, and use this score to estimate the prob-
ability that two sentences are aligned using logis-
tic regression. Surprisingly, this approach by it-
self yields competitive accuracy, yielding the same
level of accuracy as Barzilay and Elhadad?s algo-
rithm, and higher than all previous approaches on
the Britannica corpus. Such matching, however,
is still noisy. We further improve accuracy by us-
ing a global alignment dynamic programming al-
gorithm, which prunes many spurious matches.
Our approach validates Barzilay and Elhadad?s
observation regarding the utility of incorporating
context. In fact, we are able to extract more infor-
mation out of the intra-document context. First, by
using TF*IDF at the level of sentences, we weigh
words in a sentence with respect to other sentences
of the document. Second, global alignment takes
advantage of (noisy) linear order of sentences. We
make no use of inter-document context, and in par-
ticular make no assumptions about common topi-
cal structure that are unique to the Britannica cor-
pus, thus ensuring the scalability of the approach.
Indeed, we successfully apply our algorithm to
a very different corpus, the three Synoptic gospels
of the New Testament: Matthew, Mark, and Luke.
Putting aside any religious or theological signifi-
cance of these texts, they offer an excellent data
source for studying alignment, since they contain
many parallels, which have been conveniently an-
notated by bible scholars (Aland, 1985). Our algo-
rithm achieves a significant improvement over the
baseline for this corpus as well, demonstrating the
general applicability of our approach.
2 Related work
Several authors have tackled the monolingual sen-
tence correspondence problem. SimFinder (Hatzi-
vassiloglou et al, 1999; Hatzivassiloglou et al,
2001) examined 43 different features that could
potentially help determine the similarity of two
short text units (sentences or paragraphs). Of
these, they automatically selected 11 features, in-
cluding word overlap, synonymy as determined
by WordNet (Fellbaum, 1998), matching proper
nouns and noun phrases, and sharing semantic
classes of verbs (Levin, 1993).
The Decomposition method (Jing, 2002) re-
lies on the observation that document summaries
are often constructed by extracting sentence frag-
ments from the document. It attempts to identify
such extracts, using a Hidden Markov Model of
the process of extracting words. The HMM uses
features of word identity and document position,
in which transition probabilities are based on lo-
cality assumptions. For instance, after a word is
extracted, an adjacent word or one that belongs to
a nearby sentence is more likely to be extracted
than one that is further away.
Barzilay and Elhadad (2003) apply a 4-step al-
gorithm:
1. Cluster the paragraphs of the training docu-
ments into topic-specific clusters, based on
word overlap. For instance, paragraphs in
the Britannica city entries describing climate
might cluster together.
2. Learn mapping rules between paragraphs of
the full and elementary versions, taking the
word-overlap and the clusters as features.
3. Given a new pair of texts, identify sentence
pairs with high overlap, and take these to be
aligned. Then, classify paragraphs accord-
ing to the clusters learned in Step 1, and use
the mapping rules of Step 2 to match pairs of
paragraphs between the documents.
4. Finally, take advantage of the paragraph clus-
tering and mapping, by locally aligning only
sentences belonging to mapped paragraph
pairs.
Dolan et al (2004) used Web-aggregated news
stories to learn both sentence-level and word-level
alignments. Having collected a large corpus of
clusters of related news stories from Google and
MSN news aggregator services, they first seek re-
lated sentences, using two methods. First, using
a high Levenshtein distance score they identify
139K sentence pairs of which about 16.7% are es-
timated to be unrelated (using human evaluation of
a sample). Second, assuming that the first two sen-
tences of related news stories should be matched,
provided they have a high enough word-overlap,
yields 214K sentence pairs of which about 40%
are estimated to be unrelated. No recall estimates
162
are provided; however, with the release of the an-
notated Microsoft Research Paraphrase Corpus,1
it is apparent that Dolan et al are seeking much
more tightly related pairs of sentences than Barzi-
lay and Elhadad, ones that are virtually semanti-
cally equivalent. In subsequent work, the same au-
thors (Quirk et al, 2004) used such matched sen-
tence pairs to train Giza++ (Och and Ney, 2003)
on word-level alignment.
The recent PASCAL ?Recognizing Textual En-
tailment? (RTE) challenge (Dagan et al, 2005) fo-
cused on the problem of determining whether one
sentence entails another. Beyond the difference in
the definition of the required relation between sen-
tences, the RTE challenge focuses on isolated sen-
tence pairs, as opposed to sentences within a doc-
ument context. The task was judged to be quite
difficult, with many of the systems achieving rela-
tively low accuracy.
3 Data
The Britannica corpus, collected and annotated
by Barzilay and Elhadad (2003), consists of 103
pairs of comprehensive and elementary encyclope-
dia entries describing major world cities. Twenty
of these document pairs were annotated by human
judges, who were asked to mark sentence pairs
that contain at least one clause expressing the same
information, and further split into a training and
testing set.
As a rough indication of the diversity of the
dataset and the difference of the task from bilin-
gual alignment, we define the alignment diver-
sity measure (ADM) for two texts, T1   T2, to be:
2  matches

T1  T2 

T1
 
T2
 , where matches is the number of
matching sentence pairs. Intuitively, for closely
aligned document pairs, as prevalent in bilingual
alignment, one would expect an ADM value close
to 1. The average ADM value for the training doc-
ument pairs of the Britannica corpus is 0  26.
For the gospels, we use the King James ver-
sion, available electronically from the Sacred Text
Archive.2 The gospels? lengths span from 678
verses (Mark) to 1151 verses (Luke), where we
treat verses as sentences. For training and eval-
uation purposes, we use the list of parallels given
by Aland (1985).3 We use the pair Matthew-Mark
1http://research.microsoft.com/
research/downloads/
2http://www.sacred-texts.com
3The parallels are available online from http://www.
bible-researcher.com/parallels.html.
for training and the two pairs: Matthew-Luke and
Mark-Luke for testing. Whereas for the Britannica
corpus parallels were marked at the resolution of
sentences, Aland?s annotation presents parallels as
matched sequences of verses, known as pericopes.
For instance, Matthew:4.1-11 matches Mark:1.12-
13. We write v 	 p to indicate that verse v belongs
to pericope p.4
4 Algorithm
We now describe the algorithm, starting with the
TF*IDF similarity score, followed by our use of
logistic regression, and the global alignment.
4.1 From word overlap to TF*IDF
Barzilay and Elhadad (2003) use a cosine mea-
sure of word-overlap as a baseline for the task.
As can be expected, word overlap is a relatively
effective indicator of sentence similarity and re-
latedness (Marcu, 1999). Unfortunately, plain
word-overlap assigns all words equal importance,
not even distinguishing between function and con-
tent words. Thus, once the overlap threshold is
decreased to improve recall, precision degrades
rapidly. For instance, if a pair of sentences has
one or two words in common, this is inconclusive
evidence of their similarity or difference.
One way to address this problem is to differ-
entially weight words using the TF*IDF scoring
scheme, which has become standard in Informa-
tion Retrieval (Salton and Buckley, 1988). IDF
was also used for the similar task of directional en-
tailment by Monz and de Rijke (2001). To apply
this scheme for the task at hand we diverge from
the standard IDF definition by viewing each sen-
tence as a document, and the pair of documents as
a combined collection of N single-sentence docu-
ments. For a term t in sentence s, we define TFs 
 t 
to be a binary indicator of whether t occurs in s,5
and DF


t  to be the number of sentences in which
t occurs. The TF*IDF weight is:
ws 
 t  de f TFs 
 t  log

N
DF


t 
.
4The annotation of matched pericopes induces a partial
segmentation of each gospel into paragraph-like segments.
Since this segmentation is part of the gold annotation, we do
not use it in our algorithm.
5Using a binary indicator rather than the more typical
number of occurrences yielded better accuracy on the Bri-
tannica training set. This is probably due to the ?documents?
being only of sentence length.
163
 1
 0.8
 0.6
 0.5
 0.4
 0.276
 0.2
 0
 1 0.8 0.6 0.4 0.2 0
pr
ob
ab
ilit
y
similarity
pr
ob
ab
ilit
y
pr
ob
ab
ilit
y
Figure 1: Logistic Regression for Britannica train-
ing data
We use these scores as the basis of a standard
cosine similarity measure,
sim


s1   s2 
s1   s2

s1
 
s2


?t ws1

t
 
ws2

t


?t w2s1

t

?t w2s2

t

.
We normalize terms by using Porter stem-
ming (Porter, 1980). For the Britannica corpus, we
also normalized British/American spelling differ-
ences using a small manually-constructed lexicon.
4.2 Logistic regression
TF*IDF scores provide a numeric measure of sen-
tence similarity. To use them for choosing sen-
tence pairs, we proceeded to learn a probability of
two sentences being matched, given their TF*IDF
similarity score, pr


match  1  sim  . We expect
this probability to follow a sigmoid-shaped curve.
While it is always monotonically increasing, the
rate of ascent changes; for very low or very high
values it is not as steep as for middle values. This
reflects the intuition that while we always prefer a
higher scoring pair over a lower scoring pair, this
preference is more pronounced in the middle range
than in the extremities.
Indeed, Figure 1 shows a graph of this distri-
bution on the training part of the Britannica cor-
pus, where point


x
 
y  represents the fraction y of
correctly matched sentences of similarity x. Over-
layed on top of the points is a logistic regression
model of this distribution, defined as the function
p 
ea

bx
1  ea

bx ,
where a and b are parameters. We used
Weka (Witten and Frank, 1999) to automatically
learn the parameters of the distribution on the
training data. These are set to a  7  89 and
b  27  56 for the Britannica corpus.
1 2 3 4
a b c
pg2
pg1
Figure 2: Reciprocal best hit example. Arrows in-
dicate the best hit for each verse. The pairs con-
sidered correct are  2
 
b  and  4
 
c  .
Logistic regression scales the similarity scores
monotonically but non-linearly. In particular, it
changes the density of points at different score
levels. In addition, we can use this distribution
to choose a threshold, th, for when a similarity
score is indicative of a match. Optimizing the
F-measure on the training data using Weka, we
choose a threshold value of th  0  276. Note
that since the logistic regression transformation is
monotonic, the existence of a threshold on proba-
bilities implies the existence of a threshold on the
original sim scores. Moreover, such a threshold
might be obtained by means other than logistic re-
gression. The scaling, however, will become cru-
cial once we do additional calculations with these
probabilities in Section 4.4.
Applying logistic regression to the gospels is
complicated by the fact that we only have a cor-
rect alignment at the resolution of pericopes, and
not individual verses. Verse pairs that do not be-
long to a matched pericope pair can be safely con-
sidered unaligned, but for a matched pericope pair,
pg1   pg2 , we do not know which verse is matched
with which. We solve this by searching for the
reciprocal best hit, a method often used to find
orthologous genes in related species (Mushegian
and Koonin, 1996). For each verse in each peri-
cope, we find the top matching verse in the other
pericope. We take as correct all and only pairs
of verses x
 
y, such that x is y?s best match and y
is x?s best match. An example is shown in Fig-
ure 2. Taking these pairs as matched yields an
ADM value of 0  34 for the training pair of doc-
uments.
We used the reciprocally best-matched pairs of
the training portion of the gospels to find logistic
regression parameters


a 	 9  60
 
b  25  00  , and
164
a threshold,


th  0  250  . Note that we rely on this
matching only for training, but not for evaluation
(see Section 5.2).
4.3 Method 1: TF*IDF
As a simple method for choosing sentence pairs,
we just select all sentence pairs with pr


match  
th. We use the following additional heuristics:
 We unconditionally match the first sentence
of one document with the first sentence of
the other document. As noted by Quirk et al
(2004), these are very likely to be matched,
as verified on our training set as well.
 We allow many-to-one matching of sen-
tences, but limit them to at most 2-to-1 sen-
tences in both directions (by allowing only
the top two matches per sentence to be cho-
sen), since such multiple matchings often
arise due to splitting a sentence into two, or
conversely, merging two sentences into one.
4.4 Method 2: TF*IDF + Global alignment
Matching sentence pairs according to TF*IDF ig-
nores sentence ordering completely. For bilingual
texts, Gale and Church (1991) demonstrated the
extraordinary effectiveness of a global alignment
dynamic programming algorithm, where the basic
similarity score was based on the difference in sen-
tence lengths, measured in characters. Such meth-
ods fail to work in the monolingual case. Gale
and Church?s algorithm (using the implementation
of Danielsson and Ridings (1997)) yields 2% pre-
cision at 2.85% recall on the Britannica corpus.
Moore?s algorithm (2002), which augments sen-
tence length alignment with IBM Model 1 align-
ment, reports zero matching sentence pairs (re-
gardless of threshold).
Nevertheless, we expect sentence ordering can
provide important clues for monolingual align-
ment, bearing in mind two main differences from
the bilingual case. First, as can be expected by the
ADM value, there are many gaps in the alignment.
Second, there can be large segments that diverge
from the linear order predicted by a global align-
ment, as illustrated by the oval in Figure 3 (Figure
2, (Barzilay and Elhadad, 2003)).
To model these features of the data, we use a
variant of Needleman-Wunsch alignment (1970).
We compute the optimal alignment between sen-
tences 1   i of the comprehensive text and sentences
1   j of the elementary version by
 0
 50
 100
 150
 200
 250
 0  5  10  15  20  25  30
Se
nt
en
ce
s 
in
 c
om
pr
eh
en
siv
e 
ve
rs
io
n
Sentences in elementary version
Manual alignment
Se
nt
en
ce
s 
in
 c
om
pr
eh
en
siv
e 
ve
rs
io
n
Figure 3: Gold alignment for a text from the Bri-
tannica corpus.
s


i
 
j   max


s


i  1
 
j  1   pr


match


i
 
j  
s


i  1
 
j   pr


match


i
 
j  
s


i
 
j  1   pr


match


i
 
j  
Note that the dynamic programming sums match
probabilities, rather than the original sim scores,
making crucial use of the calibration induced by
the logistic regression. Starting from the first pair
of sentences, we find the best path through the ma-
trix indexed by i and j, using dynamic program-
ming. Unlike the standard algorithm, we assign no
penalty to off-diagonal matches, allowing many-
to-one matches as illustrated schematically in Fig-
ure 4. This is because for the loose alignment ex-
hibited by the data, being off-diagonal is not in-
dicative of a bad match. Instead, we prune the
complete path generated by the dynamic program-
ming using two methods. First, as in Section 4.3,
we limit many-to-one matches to 2-to-1, by al-
lowing just the two best matches per sentence to
be included. Second, we eliminate sentence pairs
with very low match probabilities


pr


match 
0  005  , a value learned on the training data. Fi-
nally, to deal with the divergences from the lin-
ear order, we add the top n pairs with very high
match probability, above a higher threshold, th 	 .
Optimizing on the training data, we set n  5 and
th 	  0  65 for both corpora.
Note that although Barzilay and Elhadad also
used an alignment algorithm, they restricted it
only to sentences judged to belong to topically re-
lated paragraphs. As noted above, this restriction
relies on a special feature of the corpus, the fact
that encyclopedia entries follow a relatively regu-
lar structure of paragraphs. By not relying on such
165



Figure 4: Global alignment
corpus-specific features, our approach gains in ro-
bustness.
5 Evaluation
5.1 Britannica corpus
Precision/recall curves for both methods, aggre-
gated over all the documents of the testing por-
tion of the Britannica corpus are given in Fig-
ure 5. To obtain different precision/recall points,
we vary the threshold above which a sentence pair
is deemed matched. Of course, when practically
applying the algorithm, we have to pick a partic-
ular threshold, as we have done by choosing th.
Precision/recall values at this threshold are also in-
dicated in the figure.6
 1
 0.9
 0.8
 0.7
 0.6
 0.7 0.6 0.558 0.5 0.4 0.3
Pr
ec
is
io
n
Recall
TF*IDF + Align
Pr
ec
is
io
n
TF*IDF
Pr
ec
is
io
n
Precision @ 55.8 Recall
Pr
ec
is
io
n
Pr
ec
is
io
n
Precision/Recall @ th
Figure 5: Precision/Recall curves for the Britan-
nica corpus
Comparative results with previous algorithms
are given in Table 1, in which the results for Barzi-
lay and Elhadad?s algorithm and previous ones are
taken from Barzilay and Elhadad (2003). The pa-
per reports the precision at 55.8% recall, since
the Decomposition method (Jing, 2002) only pro-
duced results at this level of recall, as some of the
method?s parameters were hard-coded.
Interestingly, the TF*IDF method is highly
competitive in determining sentence similarity.
6Decreasing the threshold to 0.0 does not yield all pairs,
since we only consider pairs with similarity strictly greater
than 0.0, and restrict many-to-one matches to 2-to-1.
Algorithm Precision
SimFinder 24%
Word Overlap 57.9%
Decomposition 64.3%
Barzilay & Elhadad 76.9%
TF*IDF 77.0%
TF*IDF + Align 83.1%
Table 1: Precision at 55.8% Recall
Despite its simplicity, it achieves the same perfor-
mance as Barzilay and Elhadad?s algorithm,7 and
is better than all previous ones. Significant im-
provement is achieved by adding the global align-
ment.
Clearly, the method is inherently limited in that
it can only match sentences with some lexical
overlap. For instance, the following sentence pair
that should have been matched was missed:
 Population soared, reaching 756,000 by
1903, and urban services underwent exten-
sive modification.
 At the beginning of the 20th century, Warsaw
had about 700,000 residents.
Matching ?1903? with ?the beginning of the
20th century? goes beyond the scope of any
method relying predominantly on word identity.
The hope is, however, that such mappings could
be learned by amassing a large corpus of accu-
rately sentence-aligned documents, and then ap-
plying a word-alignment algorithm, as proposed
by Quirk et al (2004). Incidentally, examining
sentence pairs with high TF*IDF similarity scores,
there are some striking cases that appear to have
been missed by the human judges. Of course, we
faithfully and conservatively relied on the human
annotation in the evaluation, ignoring such cases.
5.2 Gospels
For evaluating our algorithm?s accuracy on the
gospels, we again have to contend with the fact
that the correct alignments are given at the resolu-
tion of pericopes, not verses. We cannot rely on
the reciprocal best hit method we used for train-
ing, since it relies on the TF*IDF similarity scores,
which we are attempting to evaluate. We therefore
devise an alternative evaluation criterion, counting
7We discount the minor difference as insignifi cant.
166
a pair of verses as correctly aligned if they belong
to a matched pericope in the gold annotation.
Let Gold


g1   g2  be the set of matched pericope
pairs for gospels g1   g2, according to Aland (1985).
For each pair of matched verses, vg1   vg2 , we count
the pair as a true positive if and only if there is
a pericope pair  pg1   pg2  	 Gold 
 g1   g2  such that
vgi 	 pgi   i  1   2. Otherwise, it is a false positive.
Precision is defined as usual (P  t p  


t p  f p  ).
For recall, we note that not all the verses of a
matched pericope should be matched, especially
when one pericope has substantially more verses
than the other. In general, we may expect the num-
ber of verses to be matched to be the minimum of
 pg1  and  pg2  . We thus define recall as:
R  t p  

?

pg1  pg2  Gold

g1  g2 
min


 pg1     pg2  	 .
The results are given in Figure 6, including the
word-overlap baseline, TF*IDF ranking with lo-
gistic regression, and the added global alignment.
Once again, TF*IDF yields a substantial improve-
ment over the baseline, and results are further im-
proved by adding the global alignment.
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.6 0.5 0.4 0.3 0.2 0.1 0
Pr
ec
is
io
n
Recall
TF*IDF + Align
Pr
ec
is
io
n
TF*IDF
Pr
ec
is
io
n
Overlap
Figure 6: Precision/Recall curves for the gospels
6 Conclusions and future work
For monolingual alignment to achieve its full po-
tential for text rewriting, huge amounts of text
would need to be accurately aligned. Since mono-
lingual corpora are so noisy, simple but effective
methods as described in this paper will be required
to ensure scalability.
We have presented a novel algorithm for align-
ing the sentences of monolingual corpora of com-
parable documents. Our algorithm not only yields
substantially improved accuracy, but is also sim-
pler and more robust than previous approaches.
The efficacy of TF*IDF ranking is remarkable in
the face of previous results. In particular, TF*IDF
was not chosen by the feature selection algorithm
of Hatzivassiloglou et al (2001), who directly ex-
perimented and rejected TF*IDF measures as be-
ing less effective in determining similarity. We be-
lieve this striking difference can be attributed to
the source of the weights. Recall that our TF*IDF
weights treat each sentence as a separate docu-
ment for the purpose of weighting. TF*IDF scores
used in previous work are likely to have been ob-
tained either by aggregation over the full docu-
ment corpus, or by comparison with an external
general collection, which is bound to yield lower
discriminative power. To illustrate this, consider
two words, such as the name of a city, and the
name of a building in that city. Viewed globally,
both words are likely to belong to the long tail
of the Zipf distribution, having almost indistin-
guishable logarithmic IDF. However, in the ency-
clopedia entry describing the city, the city?s name
is likely to appear in many sentences, while the
building name may appear only in the single sen-
tence that refers to it, and thus the latter should
be scored higher. Conversely, a word that is rela-
tively frequent in general usage, e.g., ?river? might
be highly discriminative between sentences.
We further improve on the TF*IDF results by
using a global alignment algorithm. We expect
that more sophisticated sequence alignment tech-
niques, as studied for biological sequence anal-
ysis, might yield improved results, in particular
for comparing loosely matched document pairs in-
volving non-linear text transformations such as in-
versions and translocations. Such methods could
still modularly rely on the TF*IDF scoring.
We reiterate Barzilay and Elhadad?s conclusion
about the effectiveness of using the document con-
text for the alignment of text. In fact, we are
able to take better advantage of the intra-document
context, while not relying on any assumptions
about inter-document context that might be spe-
cific to one particular corpus. Identifying scalable
principles for the use of inter-document context
poses a challenging topic for future research.
We have restricted our attention here to pre-
annotated corpora, allowing better comparison
with previous work, and sidestepping the labor-
intensive task of human annotation. Having es-
167
tablished a simple and robust document alignment
method, we leave its application to much larger-
scale document sets for future work.
Acknowledgments
We thank Regina Barzilay and Noemie Elhadad
for providing access to the annotated Britannica
corpus, and for discussion. This work was sup-
ported in part by National Science Foundation
grant BCS-0236592.
References
Kurt Aland, editor. 1985. Synopsis Quattuor Evange-
liorum. American Bible Society, 13th edition, De-
cember.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entail-
ment challenge. In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment, pages 1?8, April.
Pernilla Danielsson and Daniel Ridings. 1997. Prac-
tical presentation of a vanilla aligner. Research re-
ports from the Department of Swedish, Goeteborg
University GU-ISS-97-2, Sprakdata, February.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING-
2004), Geneva, Switzerland, August.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Meeting of the Association for Computational
Linguistics, pages 177?184.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
1999 Joint SIGDAT conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 203?212, College Park, Maryland.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
SIMFINDER: A flexible clustering tool for sum-
marization. In Proceedings of the Workshop on
Automatic Summarization, pages 41?49. Associa-
tion for Computational Linguistics, 2001.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization ? step one: Sentence compres-
sion. In Proceedings of the American Association
for Artificial Intelligence conference (AAAI).
Beth Levin. 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, August 15-19,
1999, Berkeley, CA, USA, pages 137?144. ACM.
Christof Monz and Maarten de Rijke. 2001. Light-
weight subsumption checking for computational
semantics. In Patrick Blackburn and Michael
Kohlhase, editors, Proceedings of the 3rd Workshop
on Inference in Computational Semantics (ICoS-3),
pages 59?72.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, AMTA, volume 2499 of Lec-
ture Notes in Computer Science, pages 135?144.
Springer.
Arcady R. Mushegian and Eugene V. Koonin. 1996.
A minimal gene set for cellular life derived by com-
parison of complete bacterial genomes. Proceedings
of the National Academies of Science, 93:10268?
10273, September.
S.B. Needleman and C.D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. J. Mol.
Biol., 48:443?453.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Martin F. Porter. 1980. An algorithm for suffi x strip-
ping. Program, 14(3):130?137.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 142?149, Barcelona Spain, July.
Gerard Salton and Chris Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
Karen Spa?rck-Jones. 1972. Exhaustivity and speci-
fi city. Journal of Documentation, 28(1):11?21.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
168
Unifying Synchronous Tree-Adjoining Grammars and
Tree Transducers via Bimorphisms
Stuart M. Shieber
Division of Engineering and Applied Sciences
Harvard University
Cambridge, MA, USA
shieber@deas.harvard.edu
Abstract
We place synchronous tree-adjoining
grammars and tree transducers in the
single overarching framework of bimor-
phisms, continuing the unification of
synchronous grammars and tree transduc-
ers initiated by Shieber (2004). Along the
way, we present a new definition of the
tree-adjoining grammar derivation relation
based on a novel direct inter-reduction of
TAG and monadic macro tree transducers.
Tree transformation systems such as tree trans-
ducers and synchronous grammars have seen re-
newed interest, based on a perceived relevance
to new applications, such as importing syntactic
structure into statistical machine translation mod-
els or founding a formalism for speech command
and control.
The exact relationship among a variety of for-
malisms has been unclear, with a large number
of seemingly unrelated formalisms being inde-
pendently proposed or characterized. An initial
step toward unifying the formalisms was taken
(Shieber, 2004) in making use of the formal-
language-theoretic device of bimorphisms, previ-
ously used to characterize the tree relations defin-
able by tree transducers. In particular, the tree re-
lations definable by synchronous tree-substitution
grammars (STSG) were shown to be just those de-
finable by linear complete bimorphisms, thereby
providing for the first time a clear relationship be-
tween synchronous grammars and tree transduc-
ers.
In this work, we show how the bimorphism
framework can be used to capture a more powerful
formalism, synchronous tree-adjoining grammars,
providing a further uniting of the various and dis-
parate formalisms.
After some preliminaries (Section 1), we be-
gin by recalling the definition of tree-adjoining
grammars and synchronous tree-adjoining gram-
mars (Section 2). We turn then to a set of known
results relating context-free languages, tree homo-
morphisms, tree automata, and tree transducers
to extend them for the tree-adjoining languages
(Section 3), presenting these in terms of restricted
kinds of functional programs over trees, using a
simple grammatical notation for describing the
programs. This allows us to easily express gener-
alizations of the notions: monadic macro tree ho-
momorphisms, automata, and transducers, which
bear (at least some of) the same interrelationships
that their traditional simpler counterparts do (Sec-
tion 4). Finally, we use this characterization to
place the synchronous TAG formalism in the bi-
morphism framework (Section 5), further unify-
ing tree transducers and other synchronous gram-
mar formalisms. We also, in passing, provide a
new characterization of the relation between TAG
derivation and derived trees, and a new simpler
and more direct proof of the equivalence of TALs
and the output languages of monadic macro tree
transducers.
1 Preliminaries
Wewill notate sequences with angle brackets, e.g.,
?a,b,c?, or where no confusion results, simply as
abc, with the empty string written ? .
Trees will have nodes labeled with elements of
a RANKED ALPHABET, a set of symbols F, each
with a non-negative integer RANK or ARITY as-
signed to it, determining the number of children
for nodes so labeled. To emphasize the arity of
a symbol, we will write it as a parenthesized su-
perscript, for instance f (n) for a symbol f of ar-
ity n. Analogously, we write F(n) for the set of
symbols in F with arity n. Symbols with arity
zero (F(0)) are called NULLARY symbols or CON-
377
STANTS. The set of nonconstants is written F(?1).
To express incomplete trees, trees with ?holes?
waiting to be filled, we will allow leaves to be la-
beled with variables, in addition to nullary sym-
bols. The set of TREES OVER A RANKED AL-
PHABET F AND VARIABLES X, notated T(F,X),
is the smallest set such that (i) f ? T(F,X) for
all f ? F(0); (ii) x ? T(F,X) for all x ? X; and
(iii) f (t1, . . . , tn) ? T(F,X) for all f ? F(?1), and
t1, . . . , tn ? T(F,X). We abbreviate T(F, /0), where
the set of variables is empty, as T(F), the set
of GROUND TREES over F. We will also make
use of the set of n numerically ordered variables
Xn = {x1, . . . ,xn}, and write x, y, z as synonyms
for x1, x2, x3, respectively.
Trees can also be viewed as mappings from
TREE ADDRESSES, sequences of integers, to the
labels of nodes at those addresses. The address ?
is the address of the root, 1 the address of the first
child, 12 the address of the second child of the first
child, and so forth. We will use the notation t/p to
pick out the subtree of the node at address p in the
tree t. Replacing the subtree of t at address p by
a tree t ?, written t[p 7? t ?] is defined as (using ? for
the insertion of an element on a list)
t[? 7? t ?] = t ?
f (t1, . . . , tn)[(i ? p) 7? t ?] =
f (t1, . . . , ti[p 7? t ?], . . . , tn) for 1 ? i ? n .
The HEIGHT of a tree t, notated height(t), is de-
fined as follows: height(x) = 0 for all x ? X and
height( f (t1, . . . , tn)) = 1+maxni=1 height(ti) for all
f ? F .
We can use trees with variables as CONTEXTS
in which to place other trees. A tree in T(F,Xn)
will be called a context, typically denoted with the
symbol C. For a context C ? T(F,Xn) and a se-
quence of n trees t1, . . . , tn ? T(F), the SUBSTITU-
TION OF t1, . . . , tn INTO C, notated C[t1, . . . , tn], is
defined inductively as follows:
( f (u1, . . . ,um))[t1, . . . , tn]
= f (u1[t1, . . . , tn], . . . ,um[t1, . . . , tn])
xi[t1, . . . , tn] = ti .
A tree t ? T(F,X) is LINEAR if and only if no
variable in X occurs more than once in t.
We will use a notation akin to BNF to specify
equations defining functional programs of various
sorts. As an introduction to the notation we will
use, here is a grammar defining trees over a ranked
alphabet and variables (essentially identically to
the definition given above):
f (n) ? F(n)
x ? X ::= x0 | x1 | x2 | ? ? ?
t ? T(F,X) ::= f (m)(t1, . . . , tm)
| x
The notation allows definition of classes of ex-
pressions (e.g., F(n)) and specifies metavariables
over them ( f (n)). These classes can be primitive
(F(n)) or defined (X), even inductively in terms
of other classes or themselves (T(F,X)). We use
the metavariables and subscripted variants on the
right-hand side to represent an arbitrary element
of the corresponding class. Thus, the elements
t1, . . . , tm stand for arbitrary trees in T(F,X), and
x an arbitrary variable in X. Because numerically
subscripted versions of x appear explicitly on the
right hand side of the rule defining variables, nu-
merically subscripted variables (e.g., x1) on the
right-hand side of all rules are taken to refer to
the specific elements of x, whereas otherwise sub-
scripted elements (e.g., xi) are taken generically.
2 Tree-Adjoining Grammars
Tree adjoining grammar (TAG) is a tree gram-
mar formalism distinguished by its use of a tree
adjunction operation. Traditional presentations
of TAG, which we will assume familiarity with,
take the symbols in elementary and derived trees
to be unranked; nodes labeled with a given non-
terminal symbol may have differing numbers of
children. (Joshi and Schabes (1997) present a
good overview.) For example, foot nodes of aux-
iliary trees and substitution nodes have no chil-
dren, whereas the similarly labeled root nodes
must have at least one. Similarly, two nodes with
the same label but differing numbers of children
may match for the purpose of allowing an ad-
junction (as the root nodes of ?1 and ?1 in Fig-
ure 1). In order to integrate TAG with tree trans-
ducers, however, we move to a ranked alphabet,
which presents some problems and opportunities.
(In some ways, the ranked alphabet definition of
TAGs is slightly more elegant than the traditional
one.) Although the bulk of the later discussion
integrating TAGs and transducers assumes (with-
out loss of expressivity (Joshi and Schabes, 1997,
fn. 6)) a limited form of TAG that includes adjunc-
tion but not substitution, we define the more com-
plete form here.
We will thus take the nodes of TAG trees to be
labeled with symbols from a ranked alphabet F;
a given symbol then has a fixed arity and a fixed
378
T?
S T
c
S?
a S
a S?
b S
b
?1 : ?2 : ?2 :?1 : S /0 S /0
Figure 1: Sample TAG for the copy language
{wcw | w ? {a,b}? }.
number of children. However, in order to main-
tain information about which symbols may match
for the purpose of adjunction and substitution, we
take the elements of F to be explicitly formed as
pairs of an unranked label e and an arity n. (For
notational consistency, we will use e for unranked
and f for ranked symbols.) We will notate these
elements, abusing notation, as e(n), and make use
of a function |?| to unrank symbols in F, so that
|e(n)| = e.
To handle foot nodes, for each non-nullary sym-
bol e(i) ? F(?1), we will associate a new nullary
symbol e?, which one can take to be the pair of e
and ?; the set of such symbols will be notated F?.
Similarly, for substitution nodes, F? will be the set
of nullary symbols e? for all e(i) ? F(?1). These
additional symbols, since they are nullary, will
necessarily appear only at the frontier of trees. Fi-
nally, to allow null adjoining constraints, for each
f ? F(i), we introduce a symbol f /0 also of arity i,
and take F /0 to be the set of all such symbols. We
will extend the function |?| to provide the unranked
symbol associated with these symbols as well, so
|e?| = |e?| = |e(i) /0| = e.
A TAG is then a quadruple ?F,S, I,A?, where F
is a ranked alphabet; S?F is a distinguished initial
symbol; I is the set of initial trees, a finite subset of
T(F?F /0 ?F?); and A is the set of auxiliary trees,
a finite subset of T(F?F /0?F??F?). An auxiliary
tree ? whose root is labeled f must have exactly
one node labeled with | f |? ?F? and no other nodes
labeled in F?; this node is its foot node, its address
notated foot(? ). In Figure 1, ?1 and ?2 are initial
trees; ?1 and ?2 are auxiliary trees.
In order to allow reference to a particular tree in
the set P, we associate with each tree in P a unique
index, conventionally notated with a subscripted
? or ? for initial and auxiliary trees respectively.
This further allows us to have multiple instances
of a tree in I or A, distinguished by their index.
(We will abuse notation by using the index and the
tree that it names interchangably.)
The trees are combined by two operations, sub-
stitution and adjunction. Under substitution, a
?S : S?S
T
c
?1 : 1
S?
a S
a
?1 :
S?
b S
b
?2 :
1 1
S /0 S /0
Figure 2: Sample core-restricted TAG for the copy
language {wcw | w ? {a,b}? }.
node labeled e? (at address p) in a tree ? can
be replaced by an initial tree ? ? with the corre-
sponding label f at the root when | f | = e. The
resulting tree, the substitution of ? ? at p in ? , is
?[p 7? ? ?]. Under adjunction, an internal node of
? at p labeled f ? F is split apart, replaced by
an auxiliary tree ? rooted in f ? when | f | = | f ?|.
The resulting tree, the adjunction of ? at p in ? ,
is ?[p 7? ? [foot(? ) 7? ?/p]]. This definition (by
requiring f to be in F, not F? or F?) maintains
the standard convention, without loss of expres-
sivity, that adjunction is disallowed at foot nodes
and substitution nodes.
The TAG in Figure 1 generates a tree set
whose yield is the non-context-free copy language
{wcw | w ? {a,b}? }. The arities of the nodes are
suppressed, as they are clear from context.
A derivation tree D records the operations over
the elementary trees used to derive a given derived
tree. Each node in the derivation tree specifies
an elementary tree ? , the node?s child subtrees Di
recording the derivations for trees that are adjoined
or substituted into that tree. A method is required
to record at which node in ? the tree specified
by child subtree Di operates. For trees recording
derivations in context-free grammars, there are ex-
actly as many substitution operations as nontermi-
nals on the right-hand side of the rule used. Thus,
child order in the derivation tree can be used to
record the identity of the substitution node. But for
TAG trees, operations occur throughout the tree,
and some, namely adjunctions, can be optional, so
a simple convention using child order is not pos-
sible. Traditionally, the branches in the derivation
tree have been notated with the address of the node
in the parent tree at which the child node oper-
ates. Figure 4 presents a derivation tree (a) us-
ing this notation, along with the corresponding de-
rived tree (b) for the string abcab.
For simplicity below, we use a stripped down
TAG formalism, one that loses no expressivity in
weak generative capacity but is easier for analysis
purposes.
First, we make all adjunction obligatory, in the
379
AB
A?B?a b
2
3
1
B /0
Figure 3: Sample TAG tree marked with diacritics
to show the permutation of operable nodes.
sense that if a node in a tree allows adjunction, an
adjunction must occur there. To get the effect of
optional adjunction, for instance at a node labeled
B, we add a vestigial tree of a single node ?B = B?,
which has no adjunction sites and does not itself
modify any tree that it adjoins into. It thus founds
the recursive structure of derivations.
Second, now that it is determinate whether an
operation must occur at a node, the number of
children of a node in a derivation tree is deter-
mined by the elementary tree at that node; it is just
the number of adjunction or substitution nodes in
the tree, the OPERABLE NODES. All that is left
to determine is the mapping between child order
in the derivation tree and node in the elementary
tree labeling the parent, that is, a permutation pi
on the operable nodes (or equivalently, their ad-
dresses), so that the i-th child of a node labeled ?
in a derivation tree is taken to specify the tree that
operates at the node pii in ? . This permutation can
be thought of as specified as part of the elemen-
tary tree itself. For example, the tree in Figure 3,
which requires operations at the nodes at addresses
? , 12, and 2, may be associated with the permuta-
tion ?12,2,??. This permutation can be marked on
the tree itself with numeric diacritics i , as shown
in the figure.
Finally, as mentioned before, we eliminate sub-
stitution (Joshi and Schabes, 1997, fn. 6). With
these changes, the sample TAG grammar and
derivation tree of Figures 1 and 4(a) might be ex-
pressed with the core TAG grammar and deriva-
tion tree of Figures 2 and 4(c).
3 Tree Transducers, Homomorphisms,
and Automata
3.1 Tree Transducers
Informally, a TREE TRANSDUCER is a function
from T(F) to T(G) defined such that the symbol
at the root ofthe input tree and a current state de-
termines an output context in which the recursive
images of the subtrees are placed. Formally, we
can define a transducer as a kind of functional pro-
gram, that is, a set of equations characterized by
the following grammar for equations Eqn. (The
set of states is conventionally notated Q, with
members notated q. One of the states is distin-
guished as the INITIAL STATE of the transducer.)1
q ? Q
f (n) ? F(n)
g(n) ? G(n)
xi ? X ::= x0 | x1 | x2 | ? ? ?
Eqn ::= q( f (n)(x1, . . . ,xn)) = ?(n)
?(n) ? R(n) ::= g(m)(?(n)1 , . . . ,?
(n)
m )
| q j(xi) where 1 ? i ? n
Intuitively speaking, the expressions in R(n) are
right-hand-side terms using variables limited to
the first n.
For example, the grammar allows definition of
the following set of equations defining a tree trans-
ducer:2
q( f (x)) = g(q?(x),q(x))
q(a) = a
q?( f (x)) = f (q?(x))
q?(a) = a
This transducer allows for the following deriva-
tion:
q( f ( f (a)) = g(q?( f (a),q( f (a))))
= g( f (q?(a)),g(q?(a),q(a)))
= g( f (a),g(a,a))
The relation defined by a tree transducer with
initial state q is {?t,u? | q(t) = u}. By virtue of
nondeterminism in the equations, multiple equa-
tions for a given state q and symbol f , tree trans-
ducers define true relations rather than merely
functions.
TREE HOMOMORPHISMS are a subtype of tree
transducers, those with only a single state, hence
essentially stateless. Other subtypes of tree trans-
ducers can be defined by restricting the trees ?
1Strictly speaking, what we define here are nondetermin-
istic top-down tree transducers.
2Full definitions of tree transducers typically describe a
transducer in terms of a set of states, an input and output
ranked alphabet, and an initial state, in addition to the set of
transitions, that is, defining equations. We will leave off these
details, in the expectation that the sets of states and symbols
can be inferred from the equations, and the initial state de-
termined under a convention that it is the state defined in the
textually first equation.
Note also that we avail ourselves of consistent renaming
of the variables x1, x2, and so forth, where convenient for
readability.
380
that form the right-hand sides of equations, the
elements of R(n) used. A transducer is LINEAR
if all such ? are linear; is COMPLETE if ? con-
tains every variable in Xn; is ? -FREE if ? 6? Xn; is
SYMBOL-TO-SYMBOL if height(?) = 1; and is a
DELABELING if ? is complete, linear, and symbol-
to-symbol.
Another subcase is TREE AUTOMATA, tree
transducers that compute a partial identity func-
tion; these are delabeling tree transducers that pre-
serve the label and the order of arguments. Be-
cause they compute only the identity function, tree
automata are of interest for their domains, not the
mappings they compute. Their domains define
tree languages, in particular, the so-called REGU-
LAR TREE LANGUAGES.
3.2 The Bimorphism Characterization of
Tree Transducers
Tree transducers can be characterized directly in
terms of equations defining a simple kind of func-
tional program, as above. There is an elegant alter-
native characterization of tree transducers in terms
of a constellation of elements of the various sub-
types of transducers ? homomorphisms and au-
tomata ? we have introduced, called a bimor-
phism.
A bimorphism is a triple ?L,hi,ho?, consisting
of a regular tree language L (or, equivalently, a
tree automaton) and two tree homomorphisms hi
and ho. The tree relation defined by a bimor-
phism is the set of tree pairs that are generable
from elements of the tree language by the homo-
morphisms, that is,
L(?L,hi,ho?) = {?hi(t),ho(t)? | t ? L} .
We can limit attention to bimorphisms in which
the input or output homomorphisms are restricted
to a certain type, linear (L), complete (C), epsilon-
free (F), symbol-to-symbol (S), delabeling (D), or
unrestricted (M). We will write B(I,O) where I
and O characterize a subclass of homomorphisms
for the set of bimorphisms for which the input ho-
momorphism is in the subclass indicated by I and
the output homomorphism is in the subclass indi-
cated by O. Thus, B(D,M) is the set of bimor-
phisms for which the input homomorphism is a
delabeling but the output homomorphism can be
arbitrary.
The tree relations definable by tree transducers
turn out to be exactly this class B(D,M) (Comon
et al, 1997). The bimorphism notion thus allows
us to characterize the tree transductions purely in
terms of tree automata and tree homomorphisms.
We have shown (Shieber, 2004) that the tree
relations defined by synchronous tree-substitution
grammars were exactly the relations B(LC,LC).
Intuitively speaking, the tree language in such a
bimorphism represents the set of derivation trees
for the synchronous grammar, and each homomor-
phism represents the relation between the deriva-
tion tree and the derived tree for one of the pro-
jected tree-substitution grammars. The homomor-
phisms are linear and complete because the tree re-
lation between a tree-substitution grammar deriva-
tion tree and its associated derived tree is exactly
a linear complete tree homomorphism. To charac-
terize the tree relations defined by a synchronous
tree-adjoining grammar, it similary suffices to find
a simple homomorphism-like characterization of
the tree relation between TAG derivation trees and
derived trees. In Section 5 below, we show that
linear complete embedded tree homomorphisms,
which we introduce next, serve this purpose.
4 Embedded Tree Transducers
Embedded tree transducers are a generalization
of tree transducers in which states are allowed
to take a single additional argument in a re-
stricted manner. They correspond to a restric-
tive subcase of macro tree transducers with one
recursion variable. We use the term ?embed-
ded tree transducer? rather than the more cumber-
some ?monadic macro tree transducer? for brevity
and by analogy with embedded pushdown au-
tomata (Schabes and Vijay-Shanker, 1990), an-
other automata-theoretic characterization of the
tree-adjoining languages.
Wemodify the grammar of transducer equations
to add an extra argument to each occurrence of a
state q. To highlight the special nature of the extra
argument, it is written in angle brackets before the
input tree argument. We uniformly use the other-
wise unused variable x0 for this argument in the
left-hand side, and add x0 as a possible right-hand
side itself. Finally, right-hand-side occurrences
of states may be passed an arbitrary further right-
hand-side tree in this argument.
q ? Q
f (n) ? F(n)
xi ? X ::= x0 | x1 | x2 | ? ? ?
Eqn ::= q?[x0]?( f (n)(x1, . . . ,xn)) = ?(n)
?(n) ? R(n) ::= f (m)(?(n)1 , . . . ,?
(n)
m )
| x0
| q j??(n)j ?(xi) where 1 ? i ? n
381
Embedded transducers are strictly more expres-
sive than traditional transducers, because the extra
argument allows unbounded communication be-
tween positions unboundedly distant in depth in
the output tree. For example, a simple embedded
transducer can compute the reversal of a string,
e.g., 1(2(2(nil))) reverses to 2(2(1(nil))). (This
is not computable by a traditional tree transducer.)
It is given by the following equations:
r??(x) = r??nil?(x)
r??x0?(nil) = x0
r??x0?(1(x)) = r??1(x0)?(x)
r??x0?(2(x)) = r??2(x0)?(x)
(1)
This is, of course, just the normal accumulating
reverse functional program, expressed as an em-
bedded transducer. The additional power of em-
bedded transducers is, we will show in this sec-
tion, exactly what is needed to characterize the ad-
ditional power that TAGs represent over CFGs in
describing tree languages. In particular, we show
that the relation between a TAG derivation tree
and derived tree is characterized by a determinis-
tic linear complete embedded tree transducer (DL-
CETT).
The relation between tree-adjoining languages
and embedded tree transducers may be implicit in
a series of previous results in the formal-language
theory literature.3 For instance, Fujiyoshi and
Kasai (2000) show that linear, complete monadic
context-free tree grammars generate exactly the
tree-adjoining languages via a normal form for
spine grammars. Separately, the relation between
context-free tree grammars and macro tree trans-
ducers has been described, where the relation-
ship between the monadic variants of each is im-
plicit. Thus, taken together, an equivalence be-
tween the tree-adjoining languages and the im-
age languages of monadic macro tree transducers
might be pieced together. In the present work,
we define the relation between tree-adjoining lan-
guages and linear complete monadic tree trans-
ducers directly, simply, and transparently, by giv-
ing explicit constructions in both directions, care-
fully handling the distinction between the un-
ranked trees of tree-adjoining grammars and the
ranked trees of macro tree transducers and other
important issues of detail in the constructions.
The proof requires reductions in both directions.
First, we show that for any TAG we can construct
a DLCETT that specifies the tree relation between
the derivation trees for the TAG and the derived
3We are indebted to Uwe Mo?nnich for this observation.
trees. Then, we show that for any DLCETT we
can construct a TAG such that the tree relation be-
tween the derivation trees and derived trees is re-
lated through a simple homomorphism to the DL-
CETT tree relation.
4.1 From TAG to Transducer
Given an elementary tree ? with the label A at its
root, let the sequence pi = ?pi1, . . . ,pin? be a per-
mutation on the nodes in ? at which adjunction
occurs. (We use this ordering by means of the dia-
critic representation below.) Then, if ? is an aux-
iliary tree, construct the equation
qA?x0?(?(x1, . . . ,xn)) = b?c
and if ? is an initial tree, construct the equation
qA??(?(x1, . . . ,xn)) = b?c
where the right-hand-side transformation b?c is de-
fined by4
bA /0(t1, . . . , tn)c = A(bt1c, . . . ,btnc)
b k A(t1, . . . , tn)c = qA?bA /0(t1, . . . , tn)c?(xk)
bA?c = x0
bac = a
(2)
Note that the equations are linear and complete,
because each variable xi is generated once as the
tree ? is traversed, namely at position pii in the
traversal (marked with i ), and the variable x0 is
generated at the foot node only. Thus, the gener-
ated embedded tree transducer is linear and com-
plete. Because only one equation is generated per
tree, the transducer is trivially deterministic.
By way of example, we consider the core TAG
grammar given by the following trees:
? : 1 A(e)
?A : A /0( 1 B(a), 2 C( 3 D(A?)))
?B : 1 B(b,B?)
?B : B?
?C : C?
?D : D?
4It may seem like trickery to use the diacritics in this way,
as they are not really components of the tree being traversed,
but merely reflexes of an extrinsic ordering. But their use is
benign. The same transformation can be defined, a bit more
cumbersomely, keeping the permutation pi separate, by track-
ing the permutation and the current address p in a revised
transformation b?cpi,p defined as follows:
bA /0(t1, . . . , tn)cpi,p = A(bt1cpi,p?1, . . . ,btncpi,p?n)
bA(t1, . . . , tn)cpi,p = qA?bA /0(t1, . . . , tn)cpi,p?(xpi?1(p))
bA?cpi,p = x0
bacpi,p = a
We then use b?cpi,? for the transformation of the tree ? .
382
?1
?2
?1
?2
1 ?
2
a
b
S
aS
T
c
b
S
S
S
?S
?1
?2
?1
(a) (b) (c)
Figure 4: Derivation and derived trees for the sam-
ple grammars: (a) derivation tree for the gram-
mar of Figure 1; (b) corresponding derived tree;
(c) corresponding derivation tree for the core TAG
version of the grammar in Figure 2.
Starting with the auxiliary tree ?A =
A /0( 1 B(a), 2 C( 3 D(A?))), the adjunction sites,
corresponding to the nodes labeled B, C, and D at
addresses 1, 2, and 21, have been arbitrarily given
a preorder permutation. We therefore construct
the equation as follows:
qA?x0?(?A(x1,x2,x3))
= bA /0( 1 B(a), 2 C( 3 D(A?)))c
= A(b 1 B(a)c,b 2 C( 3 D(A?))c)
= A(qB?bB /0(a)c?(x1),b 2 C( 3 D(A?))c)
= A(qB?B(bac)?(x1),b 2 C( 3 D(A?))c)
= ? ? ?
= A(qB?B(a)?(x1),qC?C(qD?D(x0)?(x3))?(x2))
Similar derivations for the remaining trees yield
the (deterministic linear complete) embedded tree
transducer defined by the following set of equa-
tions:
qA??(?(x1)) = qA?A(e)?(x1)
qA?x0?(?A(x1,x2,x3)) =
A(qB?B(a)?(x1),qC?C(qD?D(x0)?(x3))?(x2))
qB?x0?(?B(x1)) = qB?B(b,x0)?(x1)
qB?x0?(?B()) = x0
qC?x0?(?C()) = x0
qD?x0?(?D()) = x0
We can use this transducer to compute the derived
tree for the derivation tree ?(?A(?B(?B),?C,?D)).
qA??(?(?A(?B(?B),?C,?D)))
= qA?A(e)?(?A(?B(?B),?C,?D))
= A( qB?B(a)?(?B(?B)),
qC?C(qD?D(A(e))?(?D))?(?C))
= A(qB?B(b,B(a))?(?B),C(qD?D(A(e))?(?D)))
= A(B(b,B(a)),C(D(A(e))))
As a final step, useful later for the bimor-
phism characterization of synchronous TAG, it is
straightforward to show that the transducer so con-
structed is the composition of a regular tree lan-
guage and a linear complete embedded tree homo-
morphism.
4.2 From Transducer to TAG
Given a linear complete embedded tree transducer,
we construct a corresponding TAG as follows: For
each rule of the form
qi?[x0]?( f (m)(x1, . . . ,xm)) = ?
we build a tree named ?qi, f ,??. Where this tree
appears is determined solely by the state qi, so
we take the root node of the tree to be the state.
Any foot node in the tree will also need to be
marked with the same label, so we pass this infor-
mation down as the tree is built inductively. The
tree is therefore of the form qi /0(d?ei) where the
right-hand-side transformation d?ei constructs the
remainder of the tree by the inductive walk of ? ,
with the subscript noting that the root is labeled
qi.
d f (t1, . . . , tm)ei = f /0(dt1ei, . . . ,dtmei)
dq j???(xk)ei = k q j(d?ei)
dx0ei = qi?
daei = a
Note that at x0, a foot node is generated of the
proper label. (Because the equation is linear, only
one foot node is generated, and it is labeled ap-
propriately by construction.) Where recursive pro-
cessing of the input tree occurs (q j???(xl)), we
generate a tree that admits adjunctions at q j. The
role of the diacritic k is merely to specify the per-
mutation of operable nodes for interpreting deriva-
tion trees; it says that the k-th child in a derivation
tree rooted in the current elementary tree is taken
to specify adjunctions at this node.
The trees generated by this TAG are intended
to correspond to the outputs of the corresponding
tree transducer. Because of the more severe con-
straints on TAG, in particular that all combinato-
rial limitations on putting subtrees together must
be manifest in the labels in the trees themselves,
the outputs actually contain more structure than
the corresponding transducer output. In particu-
lar, the state-labeled nodes are merely for book-
keeping. A homomorphism removing these nodes
gives the desired transducer output. Most impor-
tantly, then, the weak generative capacity of TAGs
and LCETTs are identical.
383
Some examples may clarify the construction.
Recall the reversal embedded transducer in (1)
above. The construction above generates a TAG
containing the following trees. We have given
them indicative names rather than the cumbersome
ones of the form ?qi, f ,??.
? : r /0(1 : r?(nil))
?nil : r? /0(r??)
?1 : r? /0(1 : r?(1 /0(r??)))
?2 : r? /0(1 : r?(2 /0(r??)))
It is simple to verify that the derivation tree
?(?1(?2(?2(?nil))))
derives the tree
r(r?6(2(r?(2(r?(1(r?(nil))))))))
Simple homomorphisms that extract the input
function symbols on the input and drop the book-
keeping states on the output reduce these trees to
1(2(2(nil))) and 2(2(1(nil))) respectively, just as
for the corresponding tree transducer.
5 Synchronous TAGs as Bimorphisms
The major advantage of characterizing TAG
derivation in terms of tree transducers (via the
compilation (2)) is the integration of synchronous
TAGs into the bimorphism framework. A syn-
chronous TAG (Shieber, 1994) is composed of a
set of triples ?tL, tR,_? where the two trees tL and
tR are elementary trees and _ is a set of links spec-
ifying pairs of linked operable nodes from tL and
tR. Without loss of generality, we can stipulate that
each operable node in each tree is impinged upon
by exactly one link in _. (If a node is unlinked,
the triple can never be used; if overlinked, a set
of replacement triples can be ?multiplied out?.) In
this case, a projection of the triples on first or sec-
ond component, with a permutation defined by the
corresponding projections on the links, is exactly a
TAG as defined above. Thus, derivations proceed
just as in a single TAG except that nodes linked by
some link in _ are simultaneously operated on by
paired trees derived by the grammar.
In order to model a synchronous grammar for-
malism as a bimorphism, the well-formed deriva-
tions of the synchronous formalism must be char-
acterizable as a regular tree language and the rela-
tion between such derivation trees and each of the
paired derived trees as a homomorphism of some
sort. For synchronous tree-substitution grammars,
derivation trees are regular tree languages, and the
map from derivation to each of the paired derived
trees is a linear complete tree homomorphism.
Thus, synchronous tree-substitution grammars fall
in the class of bimorphisms B(LC,LC). The other
direction can be shown as well; all bimorphisms
in B(LC,LC) define tree relations expressible by
an STSG.
A similar result follows immediately for STAG.
Crucially relying on the result above that the
derivation relation is a DLCETT, we can use
the method of Shieber (2004) directly to char-
acterize the synchronous TAG tree relations as
just B(ELC,ELC). We have thus integrated syn-
chronous TAG with the other transducer and syn-
chronous grammar formalisms falling under the
bimorphism umbrella.
Acknowledgements
We wish to thank Mark Dras, Uwe Mo?nnich, Re-
becca Nesson, James Rogers, and Ken Shan for
helpful discussions on the topic of this paper. This
work was supported in part by grant IIS-0329089
from the National Science Foundation.
References
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard,
D. Lugiez, S. Tison, and M. Tommasi. 1997.
Tree automata techniques and applications. Avail-
able at: http://www.grappa.univ-lille3.fr/
tata. Release of October 1, 2002.
A. Fujiyoshi and T. Kasai. 2000. Spinal-formed
context-free tree grammars. Theory of Computing
Systems, 33:59?83.
Aravind Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer, Berlin.
Yves Schabes and K. Vijay-Shanker. 1990. Determin-
istic left to right parsing of tree adjoining languages.
In Proceedings of the 28th Annual Meeting of the As-
sociation for Computational Linguistics, pages 276?
283, Pittsburgh, Pennsylvania, 6?9 June.
Stuart M. Shieber. 1994. Restricting the weak-
generative capacity of synchronous tree-adjoining
grammars. Computational Intelligence, 10(4):371?
385, November. Also available as cmp-lg/9404003.
Stuart M. Shieber. 2004. Synchronous grammars
as tree transducers. In Proceedings of the Seventh
International Workshop on Tree Adjoining Gram-
mar and Related Formalisms (TAG+7), pages 88?
95, Vancouver, Canada, May 20-22.
384
Comma Restoration Using Constituency Information
Stuart M. Shieber
Harvard University
shieber@deas.harvard.edu
Xiaopeng Tao
Harvard University
xptao@deas.harvard.edu
Abstract
Automatic restoration of punctuation from un-
punctuated text has application in improving
the fluency and applicability of speech recog-
nition systems. We explore the possibility that
syntactic information can be used to improve
the performance of an HMM-based system for
restoring punctuation (specifically, commas) in
text. Our best methods reduce sentence error
rate substantially ? by some 20%, with an ad-
ditional 8% reduction possible given improve-
ments in extraction of the requisite syntactic in-
formation.
1 Motivation
The move from isolated word to connected speech recog-
nition engendered a qualitative improvement in the nat-
uralness of users? interactions with speech transcription
systems, sufficient even to make up in user satisfaction
for some modest increase in error rate. Nonetheless, such
systems still retain an important source of unnaturalness
in dictation, the requirement to utter all punctuation ex-
plicitly. In order to free the user from this burden, a tran-
scription system would have to reconstruct the punctua-
tion from the word sequence. For certain applications ?
for instance, transcription of naturally occurring speech
not originally targeted to a speech recognizer (as broad-
cast audio) ? there is no alternative to performing recon-
struction of punctuation.
Reconstruction of different punctuation marks is likely
to respond to different techniques. Reconstruction of pe-
riods, question marks, and exclamation marks, for in-
stance, is in large part the problem of sentence bound-
ary detection. In this paper, we address the problem of
comma restoration. The published literature on intrasen-
tence punctuation restoration is quite limited, the state of
the art represented by Beeferman, Berger, and Lafferty?s
CYBERPUNC system, which we review in Section 2, and
reimplement as a baseline for our own experiments. (See
Section 5 for discussion of related work.)
The CYBERPUNC system uses a simple HMM with tri-
gram probabilities to model the comma restoration prob-
lem. It is trained on fully punctuated text, and then
tested for precision and recall in reconstructing commas
in text that has had them removed. Our replication of the
trigram-based method yields a sentence accuracy of 47%.
However, the role of the comma in text is closely
related to syntactic constituency. Nunberg (1990) de-
scribes two main classes of comma: the delimiter comma,
which is used to mark off a constituent, and the sepa-
rator comma, which is inserted between conjoined ele-
ments with or without a conjunction. In both cases, one
expects to see commas at the beginning or end of con-
stituents, rather than in the middle. But this type of cor-
relation is difficult to model with a flat model such as an
HMM. For this reason, we explore here the use of syn-
tactic constituency information for the purpose of comma
restoration. We show that even very rarefied amounts of
syntactic information can dramatically improve comma
restoration performance; our best method accurately re-
stores 58% of sentences. Furthermore, even approximate
syntactic information provides significant improvement.
There is, of course, great variation in appropriate punc-
tuation of a single word stream.1 For this reason, inde-
pendent human annotators consider only about 86% of
the sentences in the test set to be correct with respect
to comma placement (Beeferman et al, 1998). Thus, a
move from 47% to 58% is a quite substantial improve-
ment, essentially a reduction in sentence error rate of
some 30%.
1In an old unattributed joke, an English professor asks some
students to punctuate the word sequence ?Woman without her
man is nothing?. The male students preferred ?Woman, without
her man, is nothing.? whereas the female proposed ?Woman!
Without her, man is nothing.? No, it?s not funny, but it does
make the point.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 142-148
                                                         Proceedings of HLT-NAACL 2003
Digression: What?s Statistical Parsing Good For?
There has been a tremendous amount of research since
the early 1990?s on the problem of parsing using statisti-
cal models and evaluated by statistical measures such as
crossing brackets rate. Statistical parsing, like language
modeling, is presumably of interest not in and of itself but
rather by virtue of its contribution to some end-user appli-
cation. In the case of language modeling, speech recog-
nition is the leading exemplar among a large set of end-
user natural-language-processing applications that bene-
fit from the technology. Further, the statistical figures of
merit are appropriate just insofar as they vary more or less
continuously and monotonically with the performance of
the end-user application. Again, in the case of language
modeling, speech recognition error rate is generally ac-
knowledged to improve in direct relation to reduction in
cross-entropy of the language model employed.
For statistical parsing, it is much more difficult to say
what applications actually benefit from this component
technology in the sense that incremental improvements
to the technology as measured by the statistical figures
of merit provide incremental benefit to the application.
The leading argument for parsing a sentence is that this
establishes the structure upon which semantic interpreta-
tion can be performed. But it is hard to imagine in what
sense an 85% correct parse is better than an 80% correct
parse, as the semantic interpretation generated off of each
is likely to be wrong. Barring a sensible notion of ?par-
tially correct interpretation? and an end-user application
in which a partially correct interpretation is partially as
good as a fully correct one, we would not expect statisti-
cal parsers to be useful for end user applications based on
sentence interpretation.2 In fact, to the authors? knowl-
edge, the comma restoration results presented here are the
first instance of an end-user application that bears on its
face this crucial property, that incremental improvement
on statistical parsing provides incremental improvement
in the application.
2 The Trigram Baseline
As a baseline, we replicated the three-state HMM method
of Beeferman et al (1998). In this section, we describe
that method, which we use as the basis for our extensions.
The input to comma restoration is a sentence x =
x1 . . . xn of words and punctuation but no commas. We
would like to generate a restored string y = y1 . . . yn+c,
which is the string x with c commas inserted. The se-
lected y should maximize conformance with a simple tri-
2We might expect nonstatistical parsers also not to be use-
ful, but for a different reason, their fragility. Rather than de-
livering partially correct results, they partially deliver correct
results. But that is a different issue.
w?w,
w,w_
w_w_
xi ,
xi ,
xi ,xi
xi
xi
p(xi | xi-2 xi-1)
p(xi | , xi-1)
p(xi | xi-2 xi-1) p(, | xi-1 xi)
p(xi | xi-1 ,) p(, | , xi)
p(xi | , xi-1) p(, | xi-1 xi)
p(xi | xi-1 ,)
Figure 1: Three-state HMM for decoding a comma-
reduced string x1 ? ? ?xn to its comma-restored form.
Transitions are labeled with their position-dependent
probabilities.
gram model:
y? = argmaxy
n+c?
i=1
p(yi | yi?2yi?1)
We take the string x to be the observed output of an
HMM with three states and transition probabilities de-
pendent on output; the states encode the position of com-
mas in a reconstructed string. Figure 1 depicts the au-
tomaton. The start state (1) corresponds to having seen a
word with no prior or following comma, state (2) a word
with a following comma, and state (3) a word with a prior
but no following comma. It is easy to see that a path
through the automaton traverses a string y with probabil-
ity
?n+c
i=1 p(yi | yi?2yi?1). The decoded string y? can
therefore be computed by Viterbi decoding.3
This method requires a trigram language model p().
We train this language model on sections 02?22 of the
Penn Treebank Wall Street Journal data (WSJ)4, com-
prising about 36,000 sentences. The CMU Statistical
Language Modeling Toolkit (Clarkson and Rosenfeld,
1997) was used to generate the model. Katz smoothing
was used to incorporate lower-order models. The model
3As it turns out, the same computation can be done using a
two-state model. This automaton does not, however, lend itself
as easily to extensions.
4For consistency, we use the version of the Wall Street Jour-
nal data that was used by Beeferman et al (1998) for their CY-
BERPUNC experiments. This comprises sections 02?23 of the
Wall Street Journal (the last of these being used as test data)
with minor variations from the Treebank version, for instance,
a small number of missing sentences and some variation in the
tags. Runs of the experiments below using the Treebank ver-
sions of the data yield essentially identical results.
was then tested on the approximately 2300 sentences of
WSJ Section 23. Precision of the comma restoration
was 71.1% and recall 55.2%. F-measure, calculated as
2PR/(P + R), where P is precision and R recall, is
62.2%. Overall 96.3% of all comma placement decisions
were made correctly, a metric we refer to as token accu-
racy. Sentence accuracy, the percentage of sentences cor-
rectly restored, was 47.0%. (These results are presented
as model 1 in Table 1.) This is the baseline against which
we evaluate our alternative comma restoration models.
Beeferman et al present an alternative trigram model,
which computes the following:
y? = argmaxy
n+c?
i=1
p(yi | yi?2yi?1)
(1 ? p(, | yi?2yi?1))?(yi)
where
?(yi) =
{
0 yi =,
1 otherwise
That is, an additional penalty is assessed for not placing
a comma at a given position. By penalizing omission of
a comma between two words, the model implicitly re-
wards commas; we would therefore expect higher recall
and correspondingly lower precision.5 In fact, the method
with the omission penalty (model 2 in Table 1), does have
higher recall and lower precision, essentially identical F-
measure, but lower sentence accuracy. Henceforth, the
models described here do not use an omission penalty.
3 Commas and Constituency
Insofar as commas are used as separators or delimiters,
we should see correlation of comma position with con-
stituent structure of sentences. A simple test reveals that
this is so. We define the start count sci of a string posi-
tion i as the number of constituents whose left boundary
is at i. The end count eci is defined analogously. For ex-
ample, in Figure 2, sc0 is 4, as the constituents labeled
JJ, NPB, S, and S start there; ec0 is 0. We compute the
end count for positions that have a comma by first drop-
ping the comma from the tree. Thus, at position 5, sc5
is 2 (constituents DT, NPB) and ec5 is 4 (constituents JJ,
ADJP, VP, S).
We expect to find that the distributions of sc and ec
for positions in which a comma is inserted should differ
from those in which no comma appears. Figure 3 reveals
that this intuition is correct. The charts show the per-
centage of string positions with each possible value of
sc (resp. ec) for those positions with commas and those
5Counterintuitively, Beeferman et al (1998) come to the
opposite expectation, and their reported results bear out their
intuition. We have no explanation for this disparity with our
results.
S
S
NPB VP
JJ NN NNS VBP JJ PUNC, DT NN VBD PUNC.
Further   staff   cuts     are   likely       ,       the   spokesman   indicated     .
0              1        2         3      4                5            6                   7                8      9
ADJP NPB
VP
Figure 2: Sample tree, showing computation of sc and
ec values. The four constituents leading to ec5 = 4 are
shown circled, and the two leading to sc5 = 2 are shown
circled and shaded.
without. We draw the data again from sections 02?22 of
the Wall Street Journal, using as the specification for the
constituency of sentences the parses for these sentences
from the Penn Treebank. The distributions are quite dif-
ferent, hinting at an opportunity for improved comma
restoration.
The ec distribution is especially well differentiated,
with a cross-over point at about 2 constituents. We can
add this kind of information, a single bit specifying an ec
value of k or greater (call it e?ci), to the language model,
as follows. We replace p(yi | yi?2yi?1) with the proba-
bility p(yi | yi?2yi?1e?ci). We smooth the model using
lower order models p(yi | yi?1e?ci), p(yi | e?ci), p(yi).6
These distributions can be estimated from the training
data directly, and smoothed appropriately.
Adding just this one bit of information provides signifi-
cant improvement to comma restoration performance. As
it turns out, a k value of 3 turns out to maximize perfor-
mance.7 Compared to the baseline, F-measure increases
to 63.2% and sentence accuracy to 52.3%. This exper-
iment shows that constituency information, even in rar-
efied form, can provide significant performance improve-
ment in comma restoration. (Figure 1 lists performance
figures as model 3.)
Of course, this result does not provide a practical al-
gorithm for comma restoration, as it is based on a prob-
abilistic model that requires data from a manually con-
structed parse for the sentence to be restored. To make the
method practical, we might replace the Treebank parse
with a statistically generated parse. In the sequel, we use
Collins?s statistical parser (Collins, 1997) as our canon-
ical automated approximation of the Treebank. We can
train a similar model, but using ec values extracted from
6Alternative backoff paths, for instance backing off first to
p(yi | yi?2yi?1), exhibit inferior performance.
7With k = 2 (model 4), precision drops precipitously to
60.4%, recall stays roughly the same at 66.4%.
In
fo
so
u
rc
es
Tr
a
in
in
g
Te
st
in
g
trigram
insertionpenalty
wordclass
ec,threshold=2
ec,threshold=3
ec,nothreshold
stemmer
Treebank
Collinsparse,commas
Collinsparse,nocommas
Treebank
Collinsparse,commas
Collinsparse,nocommas
Modelnumber
precision
recall
F-measure
tokenaccuracy
sentenceaccuracy
reductioninsentence?error?
?
1
.
71
1
.
55
2
.
62
1
.
96
3
.
47
0
.
00
0
?
?
2
.
68
4
.
57
6
.
62
5
.
96
2
.
45
7
-
.
03
3
?
?
?
?
.
83
4
.
51
1
.
63
4
.
96
7
.
51
4
.
11
3
?
?
?
?
.
70
9
.
55
9
.
62
5
.
96
3
.
46
4
-
.
01
4
?
?
?
?
3
.
75
2
.
62
7
.
68
3
.
96
8
.
52
3
.
13
5
?
?
?
?
4
.
60
4
.
66
4
.
63
3
.
95
8
.
43
5
-
.
09
1
?
?
?
?
?
.
86
3
.
56
3
.
68
1
.
97
1
.
55
5
.
21
7
?
?
?
?
8
.
67
1
.
78
0
.
72
1
.
96
7
.
50
8
.
09
7
?
?
?
?
?
10
.
79
6
.
70
4
.
74
8
.
97
4
.
57
9
.
28
0
?
?
?
?
?
?
11
.
79
1
.
71
1
.
74
9
.
97
4
.
57
6
.
27
1
?
?
?
?
5
.
71
4
.
58
8
.
64
5
.
96
4
.
48
9
.
04
9
?
?
?
?
.
73
3
.
55
7
.
63
3
.
96
4
.
48
9
.
04
8
?
?
?
?
?
.
85
1
.
53
2
.
65
5
.
96
9
.
53
4
.
16
4
?
?
?
?
9
.
65
7
.
67
4
.
66
6
.
96
3
.
47
6
.
01
5
?
?
?
?
?
12
.
79
7
.
62
6
.
70
1
.
97
0
.
54
9
.
20
4
?
?
?
?
?
?
.
79
1
.
63
1
.
70
2
.
97
0
.
54
4
.
19
0
?
?
?
?
6
.
71
4
.
58
1
.
64
1
.
96
4
.
48
6
.
04
1
?
?
?
?
7
.
73
8
.
60
9
.
66
8
.
96
7
.
50
7
.
09
5
?
?
?
?
.
74
6
.
60
2
.
66
6
.
96
7
.
50
3
.
08
5
?
?
?
?
?
.
85
9
.
55
6
.
67
5
.
97
0
.
55
0
.
20
5
?
?
?
?
.
68
1
.
72
8
.
70
4
.
96
6
.
50
1
.
08
0
?
?
?
?
?
.
81
1
.
67
2
.
73
5
.
97
3
.
57
1
.
26
0
?
?
?
?
?
?
.
80
5
.
67
7
.
73
5
.
97
3
.
57
0
.
25
6
Table 1: Performance of the various comma restoration models described in this paper.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 1 2 3 4 5 6 7 8
without comma
with comma
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 1 2 3 4 5 6 7 8 9 10 11 12 13
without comma
with comma
(b)
Figure 3: Differential pattern of constituent starts and
ends for string positions with and without commas. Chart
(a) shows the percentage of constituents with various val-
ues of sc (number of constituents starting at the posi-
tion) for string positions with commas (square points)
and without (diamond points). Chart (b) shows the corre-
sponding pattern for values of ec (number of constituents
ending).
Collins parses of the training data, and use the model to
restore commas on a test sentence again using ec values
from the Collins parse of the test datum. This model,
listed as model 5 in Table 1, has an F-measure of 64.5%,
better than the pure trigram model (62.2%), but not as
good as the oracular Treebank-trained model (68.4%).
The other metrics show similar relative orderings.
In this model, since the test sentence has no commas
initially, we want to train the model on the parses of sen-
tences that have had commas removed, so that the model
is being applied to data as similar as possible to that on
which it was trained. We would expect, and experiments
verify (model 6), that training on the parses with com-
mas retained yields inferior performance (in particular,
F-measure of 64.1% and sentence accuracy of 48.6%).
Again consistent with expectations, if we could clairvoy-
antly know the value of e?ci based on a Collins parse of
the test sentence with the commas that we are trying to
restore (model 7), performance is improved over model
5; F-measure rises to 66.8%.
The steady rise in performance from models 6 to 5 to
7 to 3 exactly tracks the improved nature of the syntac-
tic information available to the system. As the quality
of the syntactic information better approximates ground
truth, our ability to restore commas gradually and mono-
tonically improves.
4 Using More Detailed Syntactic
Information
4.1 Using full end count information
The examples above show that even a tiny amount of syn-
tactic information can have a substantive advantage for
comma restoration. In order to use more information,
we might imagine using values of ec directly, rather than
thresholding. However, this quickly leads to data spar-
sity problems. To remedy this, we assume independence
between the bigram in the conditioning context and the
syntactic information, that is, we take
p(yi | yi?2yi?1eci) ?
p(yi | yi?2yi?1)p(yi | yi?1eci)
p(yi)
This model8 (model 8) has an F-measure of 72.1% due
to a substantial increase in recall, demonstrating that the
increased articulation in the syntactic information avail-
able provides a concomitant benefit. Although the sen-
tence accuracy is slightly less than that with thresholded
ec, we will show in a later section that this model com-
bines well with other modifications to generate further
8We back off the first term in the approximation as before,
and the second to p(yi | yi?1).
improvements.9
4.2 Using part of speech
Additional information from the parse can be useful in
predicting comma location. In this section, we incorpo-
rate part of speech information into the model, generating
model 10. We estimate the joint probability of each word
xi and its part of speech Xi as follows:
p(xi, Xi | xi?2, Xi?2, xi?1, Xi?1, ec) ?
p(xi | xi?2xi?1ec)p(Xi | Xi?2Xi?1)
The first term is computed as in model 8, the second back-
ing off to bigram and unigram models. Adding a part of
speech model in this way provides a further improvement
in performance. F-measure improves to 74.8%, sentence
accuracy to 57.9%, a 28% improvement over the base-
line.
These models (8 and 10), like model 3, assumed
availability of the Treebank parse and part of speech
tags. Using the Collins-parse-generated parses still shows
improvement over the corresponding model 5: an F-
measure of 70.1% and sentence accuracy of 54.9%, twice
the improvement over the baseline as exhibited by model
5.
5 Related Work
We compare our comma restoration methods to those of
Beeferman et al (1998), as their results use only textual
information to predict punctuation. Several researchers
have shown prosodic information to be useful in predict-
ing punctuation (Christensen et al, 2001; Kim and Wood-
land, 2001) (along with related phenomena such as dis-
fluencies and overlapping speech (Shriberg et al, 2001)).
These studies, typically based on augmenting a Marko-
vian language model with duration or other prosodic cues
as conditioning features, show that prosody information
is orthogonal to language model information; combined
models outperform models based on each type of infor-
mation separately. We would expect therefore, that our
techniques would similarly benefit from the addition of
prosodic information.
In the introduction, we mentioned the problem of sen-
tence boundary detection, which is related to the punc-
tuation reconstruction problem especially with regard to
predicting sentence boundary punctuation such as peri-
ods, question marks, and exclamation marks. (This prob-
lem is distinct from the problem of sentence boundary
disambiguation, where punctuation is provided, but the
categorization of the punctuation as to whether or not
9An alternative method of resolving the data sparsity issues
is to back off the model p(yi | yi?2yi?1eci), for instance to
p(yi | yi?2yi?1) or to p(yi | yi?1eci). Both of these perform
less well than the approximation in model 8.
it marks a sentence boundary is at issue (Palmer and
Hearst, 1994; Reynar and Ratnaparkhi, 1997).) Stolcke
and Shriberg (1996) used HMMs for the related problem
of linguistic segmentation of text, where the segments
corresponded to sentences and other self-contained units
such as disfluencies and interjections. They argue that a
linguistic segmentation is useful for improving the per-
formance and utility of language models and speech rec-
ognizers. Like the present work, they segment clean text
rather than automatically transcribed speech. Stevenson
and Gaizauskas (Stevenson and Gaizauskas, 2000) and
Goto and Renals (Gotoh and Renals, 2000) address the
sentence boundary detection problem directly, again us-
ing lexical and, in the latter, prosodic cues.
6 Future Work and Conclusion
The experiments reported here ? like much of the previ-
ous work in comma restoration (Beeferman et al, 1998)
and sentence boundary disambiguation and restoration
(Stolcke and Shriberg, 1996; Shriberg et al, 2001; Go-
toh and Renals, 2000; Stevenson and Gaizauskas, 2000)
(though not all (Christensen et al, 2001; Stolcke et al,
1998; Kim and Woodland, 2001)) ? assume an ideal ref-
erence transcription of the text. The performance of the
method on automatically transcribed speech with its con-
comitant error remains to be determined. A hopeful sign
is the work of Kim and Woodland (Kim and Woodland,
2001) on punctuation reconstruction using prosodic in-
formation. The performance of their system drops from
an F-measure of 78% on reference transcriptions to 44%
on automatically transcribed speech at a word error rate
of some 20%. Nonetheless, prosodic features were still
useful in improving the reconstructed punctuation even
in the automatically transcribed case.
The simple HMM model that we inherit from earlier
work dramatically limits the features of the parse that we
can easily appeal to in predicting comma locations. Many
alternatives suggest themselves to expand the options,
including maximum entropy models, which have been
previously successfully applied to, inter alia, sentence
boundary detection (Reynar and Ratnaparkhi, 1997), and
transformation-based learning, as used in part-of-speech
tagging and statistical parsing applications (Brill, 1995).
In addition, all of the methods above are essentially
nonhierarchical, based as they are on HMMs. An alter-
native approach would use the statistical parsing model
itself as a model of comma placement, that is, to select
the comma placement for a string such that the resulting
reconstructed string has maximum likelihood under the
statistical parsing model. This approach has the benefit
that the ramifications of comma placement on all aspects
of the syntactic structure are explored, but the disadvan-
tage that the longer distance lexical relationships found
in a trigram model are eliminated.
Nonetheless, even under these severe constraints and
using quite simple features distilled from the parse, we
can reduce sentence error by 20%, with the potential of
another 8% if statistical parsers were to approach Tree-
bank quality. As such, comma restoration may stand as
the first end-user application that benefits from statisti-
cal parsing technology smoothly and incrementally. Fi-
nally, our methods use features that are orthogonal to the
prosodic features that other researchers have explored.
They therefore have the potential to combine well with
prosodic methods to achieve further improvements.
Acknowledgments
Partial support for the work reported in this paper was
provided by the National Science Foundation under grant
number IRI 9712068.
We are indebted to Douglas Beeferman for making
available his expertise and large portions of the code and
data for replicating the CYBERPUNC experiments.
The first author would like to express his appreciation
to Ivan Sag and the Center for the Study of Language and
Information, Stanford, California and to Oliviero Stock
and the Centro per la Ricerca Scientifica e Tecnologica,
Trento, Italy, for space and support for this work during
spring and summer of 2002.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1998. CYBERPUNC: A lightweight punctuation an-
notation system for speech. In Proceeding as of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, pages 689?692, Seattle, WA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.
2001. Punctuation annotation using statistical prosody
models. In Proceedings of the 2001 ISCA Tutorial and
Research Workshop on Prosody in Speech Recognition
and Understanding, Red Bank, NJ, October 22?24. In-
ternational Speech Communication Association.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In Proceedings of Eurospeech ?97, pages 2707?2710,
Rhodes, Greece, 22?25 September.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics and Eighth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain, 7?11 July.
Yoshihiko Gotoh and Steve Renals. 2000. Sentence
boundary detection in broadcast speech transcripts.
In Proceedings of the ISCA Workshop on Automatic
Speech Recognition: Challenges for the New Millen-
nium (ASR-2000), Paris, France, 18?20 September. In-
ternational Speech Communication Association.
Ji-Hwan Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation gener-
ation and speech recognition. In Proceedings of Eu-
rospeech ?01, pages 2757?2760, Aalborg, Denmark,
September 3?7.
Geoffrey Nunberg. 1990. The Linguistics of Punctua-
tion. CSLI Publications, Stanford, CA.
David D. Palmer and Marti A. Hearst. 1994. Adaptive
sentence boundary disambiguation. In Proceedings of
the Fourth ACL Conference on Applied Natural Lan-
guage Processing, pages 78?83, Stuttgart, Germany,
13?15 October. Morgan Kaufmann.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 16?19,
Washington, DC, 31 March?3 April.
Elizabeth Shriberg, Andreas Stolcke, and Don Baron.
2001. Can prosody aid the automatic processing of
multi-party meetings? Evidence from predicting punc-
tuation, disfluencies, and overlapping speech. In Pro-
ceedings of the 2001 ISCA Tutorial and Research
Workshop on Prosody in Speech Recognition and Un-
derstanding, Red Bank, NJ, October 22?24. Interna-
tional Speech Communication Association.
Mark Stevenson and Robert Gaizauskas. 2000. Ex-
periments on sentence boundary detection. In Pro-
ceedings of the Sixth Conferernce on Applied Natu-
ral Language Processing and the First Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 24?30, Seattle, WA,
April.
Andreas Stolcke and Elizabeth Shriberg. 1996. Au-
tomatic linguistic segmentation of conversational
speech. In H. T. Bunnell and W. Idsardi, editors, Pro-
ceedings of the International Conference on Spoken
Language Processing, volume 2, pages 1005?1008,
Philadelphia, PA, 3?6 October.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Mari Ostendorf, Dilek Hakkani, Madelaine Plauche,
Go?khan Tu?r, and Yu Lu. 1998. Automatic detection of
sentence boundaries and disfluencies based on recog-
nized words. In Proceedings of the International Con-
ference on Spoken Language Processing, volume 5,
pages 2247?2250, Sydney, Australia, 30 November?4
December.
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 92?100,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficiently Parsable Extensions to Tree-Local Multicomponent TAG
Rebecca Nesson
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA
nesson@seas.harvard.edu
Stuart M. Shieber
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA
shieber@seas.harvard.edu
Abstract
Recent applications of Tree-Adjoining Gram-
mar (TAG) to the domain of semantics as well
as new attention to syntactic phenomena have
given rise to increased interested in more ex-
pressive and complex multicomponent TAG
formalisms (MCTAG). Although many con-
structions can be modeled using tree-local
MCTAG (TL-MCTAG), certain applications
require even more flexibility. In this pa-
per we suggest a shift in focus from con-
straining locality and complexity through tree-
and set-locality to constraining locality and
complexity through restrictions on the deriva-
tional distance between trees in the same tree
set in a valid derivation. We examine three
formalisms, restricted NS-MCTAG, restricted
Vector-TAG and delayed TL-MCTAG, that
use notions of derivational distance to con-
strain locality and demonstrate how they
permit additional expressivity beyond TL-
MCTAG without increasing complexity to the
level of set local MCTAG.
1 Introduction
Tree-Adjoining Grammar (TAG) has long been pop-
ular for natural language applications because of its
ability to naturally capture syntactic relationships
while also remaining efficient to process. More re-
cent applications of TAG to the domain of seman-
tics as well as new attention to syntactic phenomena
such as scrambling have given rise to increased in-
terested in multicomponent TAG formalisms (MC-
TAG), which extend the flexibility, and in some
cases generative capacity of the formalism but also
have substantial costs in terms of efficient process-
ing. Much work in TAG semantics makes use of
tree-local MCTAG (TL-MCTAG) to model phenom-
ena such as quantifier scoping, Wh-question forma-
tion, and many other constructions (Kallmeyer and
Romero, 2004; Romero et al, 2004). Certain ap-
plications, however, appear to require even more
flexibility than is provided by TL-MCTAG. Scram-
bling is one well-known example (Rambow, 1994).
In addition, in the semantics domain, the use of a
new TAG operation, flexible composition, is used to
perform certain semantic operations that seemingly
cannot be modeled with TL-MCTAG alone (Chiang
and Scheffler, 2008) and in work in synchronous
TAG semantics, constructions such as nested quanti-
fiers require a set-local MCTAG (SL-MCTAG) anal-
ysis (Nesson and Shieber, 2006).
In this paper we suggest a shift in focus from
constraining locality and complexity through restric-
tions that all trees in a tree set must adjoin within
a single tree or tree set to constraining locality and
complexity through restrictions on the derivational
distance between trees in the same tree set in a
valid derivation. We examine three formalisms, two
of them introduced in this work for the first time,
that use derivational distance to constrain locality
and demonstrate by construction of parsers their re-
lationship to TL-MCTAG in both expressivity and
complexity. In Section 2 we give a very brief in-
troduction to TAG. In Section 3 we elaborate fur-
ther the distinction between these two types of lo-
cality restrictions using TAG derivation trees. Sec-
tion 4 briefly addresses the simultaneity requirement
present in MCTAG formalisms but not in Vector-
92
SX
a
X
?
Y
b
S
X
a
Y
b
S
X
a
Y
b
Y
?
c
Y
Z
S
X
a Y
b c
Y
Z
? ?
Figure 1: An example of the TAG operations substitu-
tion and adjunction.
TAG formalisms and argues for dropping the re-
quirement. In Sections 5 and 6 we introduce two
novel formalisms, restricted non-simultaneous MC-
TAG and restricted Vector-TAG, respectively, and
define CKY-style parsers for them. In Section 7
we recall the delayed TL-MCTAG formalism intro-
duced by Chiang and Scheffler (2008) and define a
CKY-style parser for it as well. In Section 8 we
explore the complexity of all three parsers and the
relationship between the formalisms. In Section 9
we discuss the linguistic applications of these for-
malisms and show that they permit analyses of some
of the hard cases that have led researchers to look
beyond TL-MCTAG.
2 Background
A tree-adjoining grammar consists of a set of el-
ementary tree structures of arbitrary depth, which
are combined by operations of adjunction and sub-
stitution. Auxiliary trees are elementary trees in
which the root and a frontier node, called the foot
node and distinguished by the diacritic ?, are labeled
with the same nonterminalA. The adjunction opera-
tion entails splicing an auxiliary tree in at an internal
node in an elementary tree also labeled with nonter-
minal A. Trees without a foot node, which serve as
a base for derivations and may combine with other
trees by substitution, are called initial trees. Exam-
ples of the adjunction and substitution operations are
given in Figure 1. For further background, refer to
the survey by (Joshi and Schabes, 1997).
Shieber et al (1995) and Vijay-Shanker (1987)
apply the Cocke-Kasami-Younger (CKY) algorithm
first introduced for use with context-free grammars
in Chomsky normal form (Kasami, 1965; Younger,
1967) to the TAG parsing problem to generate
parsers with a time complexity of O(n6|G|2). In
order to clarify the presentation of our extended TL-
MCTAG parsers below, we briefly review the algo-
rithm of Shieber et al (1995) using the inference
rule notation from that paper. As shown in Figure 2,
items in CKY-style TAG parsing consist of a node
in an elementary tree and the indices that mark the
edges of the span dominated by that node. Nodes,
notated ?@a?, are specified by three pieces of infor-
mation: the identifier ? of the elementary tree the
node is in, the Gorn address a of the node in that
tree1, and a diacritic, ?, indicating that an adjunc-
tion or substitution is still available at that node or ?,
indicating that one has already taken place.
Each item has four indices, indicating the left and
right edges of the span covered by the node as well
as any gap in the node that may be the result of a
foot node dominated by the node. Nodes that do
not dominate a foot node will have no gap in them,
which we indicate by the use of underscores in place
of the indices for the gap. To limit the number of in-
ference rules needed, we define the following func-
tion i ? j for combining indices:
i ? j =
?
????
????
i j =
j i =
i i = j
undefined otherwise
The side conditions Init(?) and Aux(?) hold if ?
is an initial tree or an auxiliary tree, respectively.
Label(?@a) specifies the label of the node in tree
? at address a. Ft(?) specifies the address of the
foot node of tree ?. Adj(?@a, ?) holds if tree ?
may adjoin into tree ? at address a. Subst(?@a, ?)
holds if tree ? may substitute into tree ? at address
a. These conditions fail if the adjunction or substitu-
tion is prevented by constraints such as mismatched
node labels.
Multi-component TAG (MCTAG) generalizes
TAG by allowing the elementary items to be sets
of trees rather than single trees (Joshi and Schabes,
1997). The basic operations are the same but all
trees in a set must adjoin (or substitute) into another
tree or tree set in a single step in the derivation.
An MCTAG is tree-local if tree sets are required
to adjoin within a single elementary tree (Weir,
1A Gorn address uniquely identifies a node within a tree.
The Gorn address of the root node is ?. The jth child of the
node with address i has address i ? j.
93
Goal Item: ??@??, 0, , , n? Init(?)
Label(?@?) = S
Terminal Axiom: ??@a?, i? 1, , , i? Label(?@a) = wi
Empty Axiom: ??@a?, i, , , i? Label(?@a) = ?
Foot Axiom: ??@Ft(?)?, p, p, q, q? Aux(?)
Unary Complete: ??@(a ? 1)?, i, j, k, l? ?@(a ? 2) undefined
??@a?, i, j, k, l?
Binary Complete: ??@(a ? 1)?, i, j, k, l?, ??@(a ? 2)?, l, j?, k?,m?
??@a?, i, j ? j?, k ? k?,m?
Adjoin: ??@??, i, p, q, l?, ??@a?, p, j, k, q? Adj(?@a, ?)
??@a?, i, j, k, l?
No Adjoin: ??@a?, i, j, k, l?
??@a?, i, j, k, l?
Substitute: ??@??, i, , , l? Subst(?@a, ?)
??@a?, i, , , l?
Figure 2: The CKY algorithm for TAG
1988). Although tree-local MCTAG (TL-MCTAG)
has the same generative capacity as TAG (Weir,
1988), the conversion to TAG is exponential and
the TL-MCTAG formalism is NP-hard to recognize
(S?gaard et al, 2007). An MCTAG is set-local
if tree sets required to adjoin within a single ele-
mentary tree set (Weir, 1988). Set-local MCTAG
(SL-MCTAG) has equivalent expressivity to linear
context-free rewriting systems and recognition is
provably PSPACE complete (Nesson et al, 2008).
3 Domains of Locality and Derivation
Trees
The domains of locality of TL-MCTAG and SL-
MCTAG (and trivially, TAG) can be thought of as
lexically defined. That is, all locations at which the
adjunction of one tree set into another may occur
must be present within a single lexical item. How-
ever, we can also think of locality derivationally. In
a derivationally local system the constraint is on the
relationships allowed between members of the same
tree set in the derivation tree.
TAG derivation trees provide the information
about how the elementary structures of the grammar
combine that is necessary to construct the derived
tree. Nodes in a TAG derivation tree are labeled with
identifiers of elementary structures. One elementary
structure is the child of another in the derivation tree
if it adjoins or substitutes into it in the derivation.
Arcs in the derivation tree are labeled with the ad-
dress in the target elementary structure at which the
operation takes place.
In MCTAG the derivation trees are often drawn
with identifiers of entire tree sets as the nodes of
the tree because the lexical locality constraints re-
quire that each elementary tree set be the deriva-
tional child of only one other tree set. However, if
we elaborate the derivation tree to include a node for
each tree in the grammar rather than only for each
tree set we can see a stark contrast in the derivational
94
SA B
? ?
A
?
B
?
a
b
{ }
A
a
B
A
?
B
?
A B
b{ }
1:
2:
3:
1
2a 2b
3a 3b
3a 3b
2a 2b
???
???
Figure 3: An example SL-MCTAG grammar that gener-
ates the language ww and associated derivation tree that
demonstrating an arbitrarily long derivational distance
between the trees of a given tree set and their nearest com-
mon ancestor. Note that if this grammar is interpreted as
a TL-MCTAG grammar only two derivations are possible
(for the strings aa and bb).
locality of these two formalisms. In TL-MCTAG
all trees in a set must adjoin to the same tree. This
means that they must all be siblings in the derivation
tree. In SL-MCTAG, on the other hand, it is possi-
ble to generate derivations with arbitrarily long dis-
tances before the nearest common ancestor of two
trees from the same elementary tree set is reached.
An example SL-MCTAG grammar that can produce
an arbitrarily long derivational distance to the near-
est common ancestor of the trees in a given tree set
is given in Figure 3.
Chiang and Scheffler (2008) recently introduced
one variant of MCTAG, delayed Tree-Local MC-
TAG (delayed TL-MCTAG) that uses a derivational
notion of locality. In this paper we introduce two ad-
ditional derivationally local TAG-based formalisms,
restricted non-simultaneous MCTAG (restricted NS-
MCTAG) and restricted Vector TAG (restricted V-
TAG) and demonstrate by construction of parsers
how each gives rise to a hierarchy of derivation-
ally local formalisms with a well-defined efficiency
penalty for each step of derivational distance permit-
ted.
4 The Simultaneity Requirement
In addition to lexical locality constraints the defini-
tion of MCTAG requires that all trees from a set ad-
join simultaneously. In terms of well-formed deriva-
tion trees, this amounts to disallowing derivations
in which a tree from a given set is the ancestor of
a tree from the same tree set. For most linguistic
applications of TAG, this requirement seems natu-
ral and is strictly obeyed. There are a few appli-
cations, including flexible composition and scram-
bling in free-word order languages that benefit from
TAG-based grammars that drop the simultaneity re-
quirement (Chiang and Scheffler, 2008; Rambow,
1994). From a complexity perspective, however,
checking the simultaneity requirement is expensive
(Kallmeyer, 2007). As a result, it can be advan-
tageous to select a base formalism that does not
require simultaneity even if the grammars imple-
mented with it do not make use of that additional
freedom.
5 Restricted Non-simultaneous MCTAG
The simplest version of a derivationally local TAG-
based formalism is most similar to non-local MC-
TAG. There is no lexical locality requirement at all.
In addition, we drop the simultaneity requirement.
Thus the only constraint on elementary tree sets is
the limit, d, on the derivational distance between
the trees in a given set and their nearest common
ancestor. We call this formalism restricted non-
simultaneous MCTAG. Note that if we constrain d to
be one, this happens to enforce both the derivational
delay limit and the lexical locality requirement of
TL-MCTAG.
A CKY-style parser for restricted NS-MCTAG
with a restriction of d is given in Figure 4. The items
of this parser contain d lists, ?1, . . . ,?d, called his-
tories that record the identities of the trees that have
already adjoined in the derivation in order to enforce
the locality constraints. The identities of the trees in
a tree set that have adjoined in a given derivation are
maintained in the histories until all the trees from
that set have adjoined. Once the locality constraint
is checked for a tree set, the Filter side condition
expunges those trees from the histories. A tree is
recorded in this history list with superscript i, where
i is the derivational distance between the location
where the recorded tree adjoined and the location of
the current item. The locality constraint is enforced
at the point of adjunction or substitution where the
95
Goal Item Init(?1)
??0@??, 0, , , n, ?, . . . , ?? Label(?0@?) = S
|?| = 1
Terminal Axiom
??x@a?, i? 1, , , i, ?, . . . , ?? Label(?x@a) = wi
Empty Axiom
??x@a?, i, , , i, ?, . . . , ?? Label(?x@a) = ?
Foot Axiom
??x@Ft(?x)?, p, p, q, q, ?, . . . , ?? Aux(?x)
Unary Complete
??x@(a ? 1)?, i, j, k, l,?1, . . . ,?d? ?x@(a ? 2) undefined
??x@a?, i, j, k, l,?1, . . . ,?d?
Binary Complete Filter(?11 ? ?12, . . . ,
??x@(a ? 1)?, i, j, k, l,?11, . . . ,?d1???x@(a ? 2)?, l, j?, k?,m,?12, . . . ,?d2? ?d1 ? ?d2) =
??x@a?, i, j ? j?, k ? k?,m,?1, . . . ,?d? ?1, . . . ,?d
Adjoin: Adj(?x@a, ?y)
??y@??, i, p, q, l,?11, . . . ,?d?11 , ????x@a?, p, j, k, q,?12, . . . ,?d2? Filter(?12 ? {?y},?22 ? ?11,
??x@a?, i, j, k, l,?1, . . . ,?d? . . . ,?d2 ? ?d?11 ) =
?1, . . . ,?d
Substitute:
??y@??, i, , , l,?11, . . . ,?d?11 , ?? Subst(?x@a, ?y)
??x@a?, i, , , l,?1, . . . ,?d? Filter({?y},?11, . . . ,?d?11 )
= ?1, . . . ,?d
No Adjoin:
??x@a?, i, j, k, l,?1, . . . ,?d?
??x@a?, i, j, k, l,?1, . . . ,?d?
Figure 4: Axioms and inference rules for the CKY algorithm for restricted NS-MCTAG with a restriction of d.
history at the limit of the permissible delay must be
empty for the operation to succeed.
6 Restricted V-TAG
A Vector-TAG (V-TAG) (Rambow, 1994) is similar
to an MCTAG in that the elementary structures are
sets (or vectors) of TAG trees. A derivation in a V-
TAG is defined as in TAG. There is no locality re-
quirement or other restriction on adjunction except
that if one tree from a vector is used in a derivation,
all trees from that vector must be used in the deriva-
tion. The trees in a vector may be connected by
dominance links between the foot nodes of auxiliary
trees and any node in other trees in the vector. All
adjunctions must respect the dominance relations in
that a node ?1 that dominates a node ?2 must appear
on the path from ?2 to the root of the derived tree.
The definition of V-TAG is very similar to that of
non-local MCTAG as defined by Weir (1988) except
that in non-local MCTAG all trees from a tree set are
required to adjoin simultaneously.
Restricted V-TAG constrains V-TAG in several
ways. First, the dominance chain in each elementary
tree vector is required to define a total order over
the trees in the vector. This means there is a sin-
gle base tree in each vector. Note also that all trees
other than the base tree must be auxiliary trees in or-
der to dominate other trees in the vector. The base
tree may be either an initial tree or an auxiliary tree.
Second, a restricted V-TAG has a restriction level,
d, that determines the largest derivational distance
that may exists between the base tree and the high-
est tree in a tree vector in a derivation. Restricted
V-TAG differs from restricted NS-MCTAG in one
important respect: the dominance requirements of
restricted V-TAG require that trees from the same
96
set must appear along a single path in the derived
tree, whereas in restricted NS-MCTAG trees from
the same set need not adhere to any dominance rela-
tionship in the derived tree.
A CKY-style parser for restricted V-TAG with re-
striction level d is given in Figure 5. Parsing is sim-
ilar to delayed TL-MCTAG in that we have a set
of histories for each restriction level. However, be-
cause of the total order over trees in a vector, the
parser only needs to maintain the identity of the
highest tree from a vector that has been used in the
derivation along with its distance from the base tree
from that vector. The Filter side condition accord-
ingly expunges trees that are the top tree in the dom-
inance chain of their tree vector. The side conditions
for the Adjoin non-base rule enforce that the domi-
nance constraints are satisfied and that the deriva-
tional distance from the base of a tree vector to its
currently highest adjoined tree is maintained accu-
rately. We note that in order to allow a non-total or-
dering of the trees in a vector we would simply have
to record all trees in a tree vector in the histories as
is done in the delayed TL-MCTAG parser.
7 Delayed TL-MCTAG
Chiang and Scheffler (2008) introduce the de-
layed TL-MCTAG formalism which makes use of
a derivational distance restriction in a somewhat dif-
ferent way. Rather than restricting the absolute dis-
tance between the trees of a set and their nearest
common ancestor, given a node ? in a derivation
tree, delayed TL-MCTAG restricts the number of
tree sets that are not fully dominated by ?. Bor-
rowing directly from Chiang and Scheffler (2008),
Figure 7 gives two examples.
Parsing for delayed TL-MCTAG is not discussed
by Chiang and Scheffler (2008) but can be accom-
plished using a similar CKY-style strategy as in the
two parsers above. We present a parser in Fig-
ure 6. Rather than keeping histories that record
derivational distance, we keep an active delay list
for each item that records the delays that are active
(by recording the identities of the trees that have ad-
joined) for the tree of which the current node is a
part. At the root of each tree the active delay list is
filtered using the Filter side condition to remove all
tree sets that are fully dominated and the resulting
Figure 7: Examples of 1-delay (top) and 2-delay (bottom)
taken from Chiang and Scheffler (2008). The delays are
marked with dashed boxes on the derivation trees.
list is checked using the Size to ensure that it con-
tains no more than d distinct tree sets where d is the
specified delay for the grammar. The active delays
for a given tree are passed to its derivational parent
when it adjoins or substitutes.
Delayed TL-MCTAG differs from both of the pre-
vious formalisms in that it places no constraint on
the length of a delay. On the other hand while
the previous formalisms allow unlimited short de-
lays to be pending at the same time, in delayed TL-
MCTAG, only a restricted number of delays may be
active at once. Similar to restricted V-TAG, there
is no simultaneity requirement, so a tree may have
another tree from the same set as an ancestor.
8 Complexity
The complexity of the restricted NS-MCTAG and
restricted V-TAG parsers presented above depends
on the number of possible histories that may appear
in an item. For each step of derivational distance
permitted between trees of the same set, the corre-
sponding history permits many more entries. His-
tory ?1 may contain trees that have adjoined into
the same tree as the node of the current item. The
number of entries is therefore limited by the num-
ber of adjunction sites in that tree, which is in turn
limited by the number of nodes in that tree. We will
call the maximum number of nodes in a tree in the
grammar t. Theoretically, any tree in the grammar
could adjoin at any of these adjunction sites, mean-
ing that the number of possible values for each entry
in the history is bounded by the size of the grammar
|G|. Thus the size of ?1 is O(|G|t). For ?2 the en-
97
Unary Complete
??x@(a ? 1)?, i, j, k, l,?1, . . . ,?d? ?x@(a ? 2) undefined
??x@a?, i, j, k, l,?1, . . . ,?d?
Binary Complete
??x@(a ? 1)?, i, j, k, l,?11, . . . ,?d1???x@(a ? 2)?, l, j?, k?,m,?12, . . . ,?d2?
??x@a?, i, j ? j?, k ? k?,m,?11 ? ?12, . . . ,?d1 ? ?d2?
Adjoin base: Adj(?x@a, ?1)
??1@??, i, p, q, l,?11, . . . ,?d?11 , ????x@a?, p, j, k, q,?12, . . . ,?d2? Filter(?12 ? {?1},?22 ? ?11,
??x@a?, i, j, k, l,?1, . . . ,?d? . . . ,?d2 ? ?d?11 ) =
?1, . . . ,?d
Adjoin non-base:
??y@??, i, p, q, l,?11, . . . ,?d?11 , ????x@a?, p, j, k, q,?12, . . . ,?d2? Adj(?x@a, ?y)
??x@a?, i, j, k, l,?1, . . . ,?d? Filter(?12? ,?22? ? ?11, . . . ,
for unique ?i2 s.t. ?y?1 ? ?i2,?i2? = (?i2 ? ?i?11 ? {?y})? {?y?1} ?d2? ? ?d?11 ) =
for ?i2 s.t. ?y?1 /? ?i2,?i2? = ?i2 ? ?i?11 ?1, . . . ,?d
Substitute:
??1@??, i, , , l,?11, . . . ,?d?11 , ?? Subst(?x@a, ?1)
??x@a?, i, , , l,?1, . . . ,?d? Filter({?1},?11, . . . ,?d?11 )
= ?1, . . . ,?d
No Adjoin:
??x@a?, i, j, k, l,?1, . . . ,?d?
??x@a?, i, j, k, l,?1, . . . ,?d?
Figure 5: Inference rules for the CKY algorithm for restricted V-TAG with a restriction of d. Item form, goal item and
axioms are omitted because they are identical to those in restricted NS-MCTAG parser.
tries correspond to tree that have adjoined into a tree
that has adjoined into the tree of the current item.
Thus, for each of the t trees that may have adjoined
at a derivational distance of one, there are t more
trees that may have adjoined at a derivational dis-
tance of two. The size of ?2 is therefore |G|t2 . The
combined size of the histories for a grammar with a
delay or restriction of d is therefore O(|G|
?d
i=1 t
d).
Replacing the sum with its closed form solution, we
have O(|G| t
d+1?1
t?1 ?1) histories.
Using the reasoning about the size of the histories
given above, the restricted NS-MCTAG parser pre-
sented here has a complexity of O(n6 |G|1+ t
d+1?1
t?1 ),
where t is as defined above and d is the limit on de-
lay of adjunction. For a tree-local MCTAG, the com-
plexity reduces to O(n6 |G|2+t). For the linguis-
tic applications that motivate this chapter no delay
greater than two is needed, resulting in a complexity
of O(n6 |G|2+t+t2).
The same complexity analysis applies for re-
stricted V-TAG. However, we can provide a some-
what tighter bound by noting that the rank, r, of
the grammar?how many tree sets adjoin in a sin-
gle tree?and the fan out, f of the grammar?how
many trees may be in a single tree set?are limited
by t. That is, a complete derivation containing |D|
tree sets can contain no more than t |D| individual
trees and also no more than rf |D| individual trees.
In the restricted V-TAG algorithm we maintain only
one tree from a tree set in the history at a time, so
rather than maintaining O(t) entries in each history,
we only need to maintain the smaller O(r) entries.
The complexity of the delayed TL-MCTAG
parser depends on the number of possible active de-
lay lists. As above, each delay list may have a maxi-
mum of t entries for trees that adjoin directly into it.
The restriction on the number of active delays means
that the active delay lists passed up from these child
nodes at the point of adjunction or substitution can
have size no more than d. This results in an addi-
tional td(f ? 1) possible entries in the active de-
98
Goal Item: Init(?1)
??0@??, 0, , , n, ?, . . . , ?? Label(?0@?) = S
|?| = 1
Terminal Axiom
??x@a?, i? 1, , , i, ?, . . . , {?x}? Label(?x@a) = wi
Empty Axiom
??x@a?, i, , , i, ?, . . . , {?x}? Label(?x@a) = ?
Foot Axiom
??x@Ft(?x)?, p, p, q, q, ?, . . . , {?x}? Aux(?x)
Unary Complete
??x@(a ? 1)?, i, j, k, l,?? ?x@(a ? 2) undefined
??x@a?, i, j, k, l,??
Binary Complete
??x@(a ? 1)?, i, j, k, l,?1???x@(a ? 2)?, l, j?, k?,m,?2?
??x@a?, i, j ? j?, k ? k?,m,?1 ? ?2?
Adjoin:
??y@??, i, p, q, l,?????x@a?, p, j, k, q,??? Adj(?x@a, ?y)
??x@a?, i, j, k, l,??? ? ??? Filter(??,???)
Size(???) ? d
Substitute:
??y@??, i, , , l,??? Subst(?x@a, ?y)
??x@a?, i, , , l,??? ? {?x}? Filter(??,???)
Size(???) ? d
No Adjoin:
??x@a?, i, j, k, l,??
??x@a?, i, j, k, l,??
Figure 6: Axioms and inference rules for the CKY algorithm for delayed TL-MCTAG with a delay of d.
lay list, giving a total number of active delay lists
of O(|G|t(1+d(f?1))). Thus the complexity of the
parser is O(n6 |G|2+t(1+d(f?1))).
9 Conclusion
Each of the formalisms presented above extends the
flexibility of MCTAG beyond that of TL-MCTAG
while maintaining, as we have shown herein, com-
plexity much less than that of SL-MCTAG. All three
formalisms permit modeling of flexible composi-
tion (because they permit one member of a tree set
to be a derivational ancestor of another tree in the
same set), at least restricted NS-MCTAG and re-
stricted V-TAG permit analyses of scrambling, and
all three permit analyses of the various challeng-
ing semantic constructions mentioned in the intro-
duction. We conclude that extending locality by
constraining derivational distance may be an effec-
tive way to add flexibility to MCTAG without losing
computational tractability.
Acknowledgments
This material is based upon work supported by the
National Science Foundation under Grant No. BCS-
0827979.
References
David Chiang and Tatjana Scheffler. 2008. Flexible com-
position and delayed tree-locality. In The Ninth Inter-
national Workshop on Tree Adjoining Grammars and
Related Formalisms (TAG+9).
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, pages
69?124. Springer.
99
Laura Kallmeyer and Maribel Romero. 2004. LTAG
semantics with semantic unification. In Proceedings
of the 7th International Workshop on Tree-Adjoining
Grammars and Related Formalisms (TAG+7), pages
155?162, Vancouver, May.
Laura Kallmeyer. 2007. A declarative characterization
of different types of multicomponent tree adjoining
grammars. In Andreas Witt Georg Rehm and Lothar
Lemnitzer, editors, Datenstrukturen fu?r linguistische
Ressourcen und ihre Anwendungen, pages 111?120.
T. Kasami. 1965. An efficient recognition and syntax
algorithm for context-free languages. Technical Re-
port AF-CRL-65-758, Air Force Cambridge Research
Laboratory, Bedford, MA.
Rebecca Nesson and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Complexity, parsing, and factorization of tree-
local multi-component tree-adjoining grammar. Tech-
nical report, Harvard University.
Owen Rambow. 1994. Formal and computational as-
pects of natural language syntax. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
Maribel Romero, Laura Kallmeyer, and Olga Babko-
Malaya. 2004. LTAG semantics for questions. In
Proceedings of the 7th International Workshop on
Tree-Adjoining Grammars and Related Formalisms
(TAG+7), pages 186?193, Vancouver, May.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1?2):3?36, July?August. Also available as cmp-
lg/9404008.
Anders S?gaard, Timm Lichte, and Wolfgang Maier.
2007. On the complexity of linguistically motivated
extensions of tree-adjoining grammar. In Recent Ad-
vances in Natural Language Processing 2007.
K. Vijay-Shanker. 1987. A study of tree-adjoining gram-
mars. PhD Thesis, Department of Computer and In-
formation Science, University of Pennsylvania.
David Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. PhD Thesis, Depart-
ment of Computer and Information Science, Univer-
sity of Pennsylvania.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
100
Proceedings of NAACL HLT 2009: Short Papers, pages 105?108,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Importance of Sub-Utterance Prosody in Predicting Level of Certainty
Heather Pon-Barry
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
ponbarry@eecs.harvard.edu
Stuart Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
shieber@seas.harvard.edu
Abstract
We present an experiment aimed at under-
standing how to optimally use acoustic and
prosodic information to predict a speaker?s
level of certainty. With a corpus of utterances
where we can isolate a single word or phrase
that is responsible for the speaker?s level of
certainty we use different sets of sub-utterance
prosodic features to train models for predict-
ing an utterance?s perceived level of certainty.
Our results suggest that using prosodic fea-
tures of the word or phrase responsible for the
level of certainty and of its surrounding con-
text improves the prediction accuracy without
increasing the total number of features when
compared to using only features taken from
the utterance as a whole.
1 Introduction
Prosody is a fundamental part of human-to-human
spoken communication; it can affect the syntac-
tic and semantic interpretation of an utterance
(Hirschberg, 2003) and it can be used by speakers
to convey their emotional state. In recent years, re-
searchers have found prosodic features to be useful
in automatically detecting emotions such as annoy-
ance and frustration (Ang et al, 2002) and in dis-
tinguishing positive from negative emotional states
(Lee and Narayanan, 2005).
In this paper, we address the problem of predict-
ing the perceived level of certainty of a spoken ut-
terance. Specifically, we have a corpus of utter-
ances where it is possible to isolate a single word
or phrase responsible for the speaker?s level of cer-
tainty. With this corpus we investigate whether us-
ing prosodic features of the word or phrase causing
uncertainty and of its surrounding context improves
the prediction accuracy when compared to using fea-
tures taken only from the utterance as a whole.
This work goes beyond existing research by look-
ing at the predictive power of prosodic features ex-
tracted from salient sub-utterance segments. Pre-
vious work on uncertainty has examined the pre-
dictive power of utterance- and intonational phrase-
level prosodic features (Liscombe et al, 2005) as
well as the relative strengths of correlations between
level of certainty and sub-utterance prosodic fea-
tures (Pon-Barry, 2008). Our results suggest that
we can do a better job at predicting an utterance?s
perceived level of certainty by using prosodic fea-
tures extracted from the whole utterance plus ones
extracted from salient pieces of the utterance, with-
out increasing the total number of features, than by
using only features from the whole utterance.
This work is relevant to spoken language applica-
tions in which the system knows specific words or
phrases that are likely to cause uncertainty. For ex-
ample, this would occur in a tutorial dialogue system
when the speaker answers a direct question (Pon-
Barry et al, 2006; Forbes-Riley et al, 2008), or in
language (foreign or ESL) learning systems and lit-
eracy systems (Alwan et al, 2007) when new vocab-
ulary is being introduced.
2 Previous Work
Researchers have examined certainty in spoken lan-
guage using data from tutorial dialogue systems
(Liscombe et al, 2005) and data from an uncertainty
corpus (Pon-Barry, 2008).
Liscombe et al (2005) trained a decision tree
105
classifier on utterance-level and intonational phrase-
level prosodic features to distinguish between cer-
tain, uncertain, and neutral utterances. They
achieved 76% accuracy, compared to a 66% accu-
racy baseline (choosing the most common class).
We have collected a corpus of utterances spoken
under varying levels of certainty (Pon-Barry, 2008).
The utterances were elicited by giving adult native
English speakers a written sentence containing one
or more gaps, then displaying multiple options for
filling in the gaps and telling the speakers to read
the sentence aloud with the gaps filled in according
to domain-specific criteria. We elicited utterances
in two domains: (1) using public transportation in
Boston, and (2) choosing vocabulary words to com-
plete a sentence. An example is shown below.
Q: How can I get from Harvard to the Silver Line?
A: Take the red line to
a. South Station
b. Downtown Crossing
The term ?context? refers to the fixed part of the re-
sponse (?Take the red line to ?, in this exam-
ple) and the term ?target word? refers to the word or
phrase chosen to fill in the gap.
The corpus contains 600 utterances from 20
speakers. Each utterance was annotated for level
of certainty, on a 5-point scale, by five human
judges who listened to the utterances out of context.
The average inter-annotator agreement (Kappa) was
0.45. We refer to the average of the five ratings as
the ?perceived level of certainty? (the quantity we at-
tempt to predict in this paper).
We computed correlations between perceived
level of certainty and prosodic features extracted
from the whole utterance, the context, and the tar-
get word. Pauses preceding the target word were
considered part of the target word; all segmenta-
tion was done manually. Because the speakers had
unlimited time to read over the context before see-
ing the target words, the target word is considered
to be the source of the speaker?s confidence or un-
certainty; it corresponds to the decision that the
speaker had to make. Our correlation results sug-
gest that while some prosodic cues to level of cer-
tainty were strongest in the whole utterance, others
were strongest in the context or the target word. In
this paper, we extend this past work by testing the
prediction accuracy of models trained on different
subsets of these prosodic features.
3 Prediction Experiments
In our experiments we used 480 of the 600 utter-
ances in the corpus, those which contained exactly
one gap. (Some had two or three gaps.) We ex-
tracted the following 20 prosodic feature-types from
each whole utterance, context, and target word (a to-
tal of 60 features) using WaveSurfer1 and Praat2.
Pitch: minf0, maxf0, meanf0, stdevf0, rangef0, rel-
ative position minf0, relative position maxf0,
absolute slope (Hz), absolute slope (semitones)
Intensity: minRMS, maxRMS, meanRMS, stdev-
RMS, relative position minRMS, relative posi-
tion maxRMS
Temporal: total silence, percent silence, total dura-
tion, speaking duration, speaking rate
These features are comparable to those used in Lis-
combe et al?s (2005) prediction experiments. The
pitch and intensity features were represented as
z-scores normalized by speaker; the temporal fea-
tures were not normalized.
Next, we created a ?combination? set of 20 fea-
tures based on our correlation results. Figure 1 il-
lustrates how the combination set was created: for
each prosodic feature-type (each row in the table) we
chose either the whole utterance feature, the context
feature, or the target word feature, whichever one
had the strongest correlation with perceived level of
certainty. The selected features (highlighted in Fig-
ure 1) are listed below.
Whole Utterance: total silence, total duration,
speaking duration, relative position maxf0, rel-
ative position maxRMS, absolute slope (Hz),
absolute slope (semitones)
Context: minf0, maxf0, meanf0, stdevf0, rangef0,
minRMS, maxRMS, meanRMS, relative posi-
tion minRMS
Target Word: percent silence, speaking rate, rela-
tive position minf0, stdevRMS
1http://www.speech.kth.se/wavesurfer/
2http://www.fon.hum.uva.nl/praat/
106
Feature-type Whole Utterance Context Target Word
min f0 0.107 0.119 0.041
max f0 ?0.073 ?0.153 ?0.045
mean f0 0.033 0.070 ?0.004
stdev f0 ?0.035 ?0.047 ?0.043
range f0 ?0.128 ?0.211 ?0.075
rel. position min f0 0.042 0.022 0.046
rel. position max f0 0.015 0.008 0.001
abs. slope f0 (Hz) 0.275 0.180 0.191
abs. slope f0 (Semi) 0.160 0.147 0.002
min RMS 0.101 0.172 0.027
max RMS ?0.091 ?0.110 ?0.034
mean RMS ?0.012 0.039 ?0.031
stdev RMS ?0.002 ?0.003 ?0.019
rel. position min RMS 0.101 0.172 0.027
rel. position max RMS ?0.039 ?0.028 ?0.007
total silence ?0.643 ?0.507 ?0.495
percent silence ?0.455 ?0.225 ?0.532
total duration ?0.592 ?0.502 ?0.590
speaking duration ?0.430 ?0.390 ?0.386
speaking rate 0.090 0.014 0.136
1
Figure 1: The Combination feature set (highlighted in ta-
ble) was produced by selecting either the whole utterance
feature, the context feature, or the target word feature
for each prosodic feature-type, whichever one was most
strongly correlated with perceived level of certainty.
To compare the prediction accuracies of different
subsets of features, we fit five linear regression mod-
els to the feature sets. The five subsets are: (A)
whole utterance features only, (B) target word fea-
tures only, (C) context features only, (D) all fea-
tures, and (E) the combination feature set. We di-
vided the data into 20 folds (one fold per speaker)
and performed a 20-fold cross-validation for each
set of features. Each experiment fits a model us-
ing data from 19 speakers and tests on the remain-
ing speaker. Thus, when we test our models, we are
testing the ability to classify utterances of an unseen
speaker.
Table 1 shows the accuracies of the models
trained on the five subsets of features. The num-
bers reported are averages of the 20 cross-validation
accuracies. We report results for two cases: 5 pre-
diction classes and 3 prediction classes. We first
computed the prediction accuracy over five classes
(the regression output was rounded to the nearest
integer). Next, in order to compare our results to
those of Liscombe et al (2005), we recoded the
5-class results into 3-class results, following Pon-
Barry (2008), in the way that maximized inter-
annotator agreement. The naive baseline numbers
are the accuracies that would be achieved by always
choosing the most common class.
4 Discussion
Assuming that the target word is responsible for the
speaker?s level of certainty, it is not surprising that
the target word feature set (B) yields higher accura-
cies than the context feature set (C). It is also not sur-
prising that the set of all features (D) yields higher
accuracies than sets (A), (B), and (C).
The key comparison to notice is that the combi-
nation feature set (E), with only 20 features, yields
higher average accuracies than the utterance fea-
ture set (A): a difference of 6.42% for 5 classes
and 5.83% for 3 classes. This suggests that using a
combination of features from the context and target
word in addition to features from the whole utter-
ance leads to better prediction of the perceived level
of certainty than using features from only the whole
utterance.
One might argue that these differences are just
due to noise. To address this issue, we compared
the prediction accuracies of sets (A) and (E) per fold.
This is illustrated in Figure 2. Each fold in our cross-
validation corresponds to a different speaker, so the
folds are not identically distributed and we do not
expect each fold to yield the same prediction accu-
racy. That means that we should compare predic-
tions of the two feature sets within folds rather than
between folds. Figure 2 shows the correlations be-
tween the predicted and perceived levels of certainty
for the models trained on sets (A) and (E). The com-
bination set (E) predictions were more strongly cor-
related than whole utterance set (A) predictions in
16 out of 20 folds. This result supports our claim
that using a combination of features from the con-
text and target word in addition to features from the
whole utterance leads to better prediction of level of
certainty.
Our best prediction accuracy for the 3 class case,
74.79%, was slightly lower than the accuracy re-
ported by Liscombe et al (2005), 76.42%. However,
our difference from the naive baseline was 18.54%
where Liscombe et al?s was 10.42%. Liscombe et
al. randomly divided their data into training and test
sets, so it is unclear whether they tested on seen or
unseen speakers. Further, they ran one experiment
rather than a cross-validation, so their reported ac-
curacy may not be indicative of the entire data set.
We also trained support vector models on these
subsets of features. The main result was the same:
107
Table 1: Average prediction accuracies for the linear regression models trained on five subsets of prosodic features.
The models trained on the Combination feature set and the All feature set perform better than the other three models
in both the 3- and 5-class settings.
Feature Set Num Features Accuracy (5 classes) Accuracy (3 classes)
Naive Baseline N/A 31.46% 56.25%
(A) Utterance 20 39.00% 68.96%
(B) Target Word 20 43.13% 68.96%
(C) Context 20 37.71% 67.50%
(D) All 60 48.54% 74.58%
(E) Combination 20 45.42% 74.79%
Fold UTT COMBI13 0.60742805 0.74174205 1 0.13431410 0.71345083 0.84506209 1 0.131611272 0.65645441 0.7745844 1 0.1181299920 0.59862684 0.69875998 1 0.1001331419 0.61302941 0.70460363 1 0.091574223 0.67823016 0.75606366 1 0.07783355 0.54426476 0.61711862 1 0.072853864 0.74102672 0.81252066 1 0.0714939417 0.71910042 0.77176522 1 0.05266486 0.78220835 0.82806993 1 0.0458615818 0.66737245 0.71009756 1 0.0427251115 0.66996149 0.70962379 1 0.039662319 0.63477603 0.66365739 1 0.028881371 0.7359401 0.7631083 1 0.027168217 0.71645922 0.73071498 1 0.0142557512 0.78824649 0.79491313 1 0.0066666411 0.39557157 0.39381644 0 -0.0017551316 0.62168851 0.61984036 0 -0.0018481514 0.6971148 0.67762751 0 -0.019487298 0.82685033 0.80103581 0 -0.0258145216
00.20.4
0.60.81
0 2 4 6 8 10 12 14 16 18 20FoldC
orrelation C
oeff (R) CombinationUtterance
Figure 2: Correlations with perceived level of certainty
per fold for the Combination (O) and the Utterance (X)
feature set predictions, sorted by the size of the difference.
In 16 of the 20 experiments, the correlation coefficients
for the Combination feature set are greater than those of
the Utterance feature set.
the set of all features (D) and the combination set
(E) had better prediction accuracies than the utter-
ance feature set (A). In addition, the combination set
(E) had the best prediction accuracies (of all models)
in both the 3- and 5-class settings. The raw accura-
cies were approximately 5% lower than those of the
linear regression models.
5 Conclusion and Future Work
The results of our experiments suggest a better pre-
dictive model of level of certainty for systems where
words or phrases likely to cause uncertainty are
known ahead of time. Without increasing the total
number of features, combining select prosodic fea-
tures from the target word, the surrounding context
and the whole utterance leads to better prediction of
level of certainty than using features from the whole
utterance only. In the near future, we plan to exper-
iment with prediction models of the speaker?s self-
reported level of certainty.
Acknowledgments
This work was supported by a National Defense Sci-
ence and Engineering Graduate Fellowship.
References
Abeer Alwan, Yijian Bai, Matthew Black, et al 2007. A
system for technology based assessment of language
and literacy in young children: the role of multiple in-
formation sources. Proc. of IEEE International Work-
shop on Multimedia Signal Processing, pp. 26?30,
Chania, Greece.
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, et al
2002. Prosody-based automatic detection of annoy-
ance and frustration in human-computer dialog. Proc.
of ICSLP 2002, pp. 2037?2040, Denver, CO.
Kate Forbes-Riley, Diane Litman, and Mihai Rotaru.
2008. Responding to student uncertainty during com-
puter tutoring: a preliminary evaluation. Proc. of the
9th International Conference on Intelligent Tutoring
Systems, Montreal, Canada.
Julia Hirschberg. 2003. Intonation and pragmatics. In
L. Horn and G. Ward (ed.), Handbook of Pragmatics,
Blackwell.
Chul Min Lee and Shrikanth Narayanan. 2005. Towards
detecting emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, 13(2):293?303.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. Proceedings of Eurospeech 2005, Lisbon,
Portugal.
Heather Pon-Barry, Karl Schultz, Elizabeth Bratt, Brady
Clark, and Stanley Peters. 2006. Responding to stu-
dent uncertainty in spoken tutorial dialogue systems.
International Journal of Artificial Intelligence in Edu-
cation 16:171-194.
Heather Pon-Barry. 2008. Prosodic manifestations of
confidence and uncertainty in spoken language. Proc.
of Interspeech 2008, pp. 74?77, Brisbane, Australia.
108
Proceedings of ACL-08: HLT, pages 604?612,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Optimal k-arization of Synchronous Tree-Adjoining Grammar
Rebecca Nesson
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA 02138
nesson@seas.harvard.edu
Giorgio Satta
Department of
Information Engineering
University of Padua
I-35131 Padova, Italy
satta@dei.unipd.it
Stuart M. Shieber
School of Engineering
and Applied Sciences
Harvard University
Cambridge, MA 02138
shieber@seas.harvard.edu
Abstract
Synchronous Tree-Adjoining Grammar
(STAG) is a promising formalism for syntax-
aware machine translation and simultaneous
computation of natural-language syntax and
semantics. Current research in both of these
areas is actively pursuing its incorporation.
However, STAG parsing is known to be
NP-hard due to the potential for intertwined
correspondences between the linked nonter-
minal symbols in the elementary structures.
Given a particular grammar, the polynomial
degree of efficient STAG parsing algorithms
depends directly on the rank of the grammar:
the maximum number of correspondences that
appear within a single elementary structure.
In this paper we present a compile-time
algorithm for transforming a STAG into a
strongly-equivalent STAG that optimally
minimizes the rank, k, across the grammar.
The algorithm performs inO(|G|+ |Y | ? L3
G
)
time where L
G
is the maximum number of
links in any single synchronous tree pair in
the grammar and Y is the set of synchronous
tree pairs of G.
1 Introduction
Tree-adjoining grammar is a widely used formal-
ism in natural-language processing due to its mildly-
context-sensitive expressivity, its ability to naturally
capture natural-language argument substitution (via
its substitution operation) and optional modifica-
tion (via its adjunction operation), and the existence
of efficient algorithms for processing it. Recently,
the desire to incorporate syntax-awareness into ma-
chine translation systems has generated interest in
the application of synchronous tree-adjoining gram-
mar (STAG) to this problem (Nesson, Shieber, and
Rush, 2006; Chiang and Rambow, 2006). In a par-
allel development, interest in incorporating seman-
tic computation into the TAG framework has led
to the use of STAG for this purpose (Nesson and
Shieber, 2007; Han, 2006b; Han, 2006a; Nesson
and Shieber, 2006). Although STAG does not in-
crease the expressivity of the underlying formalisms
(Shieber, 1994), STAG parsing is known to be NP-
hard due to the potential for intertwined correspon-
dences between the linked nonterminal symbols in
the elementary structures (Satta, 1992; Weir, 1988).
Without efficient algorithms for processing it, its po-
tential for use in machine translation and TAG se-
mantics systems is limited.
Given a particular grammar, the polynomial de-
gree of efficient STAG parsing algorithms depends
directly on the rank of the grammar: the maximum
number of correspondences that appear within a sin-
gle elementary structure. This is illustrated by the
tree pairs given in Figure 1 in which no two num-
bered links may be isolated. (By ?isolated?, we
mean that the links can be contained in a fragment
of the tree that contains no other links and domi-
nates only one branch not contained in the fragment.
A precise definition is given in section 3.)
An analogous problem has long been known
to exist for synchronous context-free grammars
(SCFG) (Aho and Ullman, 1969). The task of
producing efficient parsers for SCFG has recently
been addressed by binarization or k-arization of
SCFG grammars that produce equivalent grammars
in which the rank, k, has been minimized (Zhang
604
AB
C
D
w
A
B
C
DE F G
1
2
3
4
A
B C
D E F G
A
B C
D
2
3
1
4
1 2 3 4 2 4 31
w ?w w ?x x ? y ?y z z ?
A
B C
D 1
w
3 4
E 2
x
5 A
B C
D
1
3 4E
2
5
w ? x ?
?1 : ?2 : ?3 :
Figure 1: Example of intertwined links that cannot be binarized. No two links can be isolated in both trees in a tree
pair. Note that in tree pair ?
1
, any set of three links may be isolated while in tree pair ?
2
, no group of fewer than four
links may be isolated. In ?
3
no group of links smaller than four may be isolated.
S
V P
V
likes
red candies
aime
les b o n b o n srouges
Det
N P?
S
V P
V N P?
N P
N
N P
NN ?
N
A d j N ?
N
A d j
S
N P V P
J o h n V
likes
J ean
aime
S
N P V P
V
les
Det
N PN P
red
N
A d j
candies
N
b o n b o n s
N
rouges
N
A d j
2
1
2
1 J ean
N PN P
J o h n
N P? 1 N P? 1
likes
J o h n candies
red
1 2
1
( a ) ( b ) ( c )
Figure 2: An example STAG derivation of the English/French sentence pair ?John likes red candies?/?Jean aime les
bonbons rouges?. The figure is divided as follows: (a) the STAG grammar, (b) the derivation tree for the sentence
pair, and (c) the derived tree pair for the sentences.
and Gildea, 2007; Zhang et al, 2006; Gildea, Satta,
and Zhang, 2006). The methods for k-arization
of SCFG cannot be directly applied to STAG be-
cause of the additional complexity introduced by
the expressivity-increasing adjunction operation of
TAG. In SCFG, where substitution is the only avail-
able operation and the depth of elementary struc-
tures is limited to one, the k-arization problem re-
duces to analysis of permutations of strings of non-
terminal symbols. In STAG, however, the arbitrary
depth of the elementary structures and the lack of
restriction to contiguous strings of nonterminals in-
troduced by adjunction substantially complicate the
task.
In this paper we offer the first algorithm address-
ing this problem for the STAG case. We present
a compile-time algorithm for transforming a STAG
into a strongly-equivalent STAG that optimally min-
imizes k across the grammar. This is a critical mini-
mization because k is the feature of the grammar that
appears in the exponent of the complexity of parsing
algorithms for STAG. Following the method of Seki
et al (1991), an STAG parser can be implemented
with complexity O(n4?(k+1) ? |G|). By minimizing
k, the worst-case complexity of a parser instanti-
ated for a particular grammar is optimized. The k-
arization algorithm performs in O(|G|+ |Y | ? L3G)
time where LG is the maximum number of links in
any single synchronous tree pair in the grammar and
Y is the set of synchronous tree pairs of G. By com-
parison, a baseline algorithm performing exhaustive
search requires O(|G|+ |Y | ? L6G) time.
1
The remainder of the paper proceeds as follows.
In section 2 we provide a brief introduction to the
STAG formalism. We present the k-arization algo-
rithm in section 3 and an analysis of its complexity
in section 4. We prove the correctness of the algo-
rithm in section 5.
1In a synchronous tree pair with L links, there are O(L4)
pairs of valid fragments. It takes O(L) time to check if the two
components in a pair have the same set of links. Once the syn-
chronous fragment with the smallest number of links is excised,
this process iterates at most L times, resulting in time O(L6G).
605
DE F
A
B
C
1
2
3 4
y z
5
H
I J2 3
1
NM 4
w ? x ?
5
L
y ?
K
? :
x
G
z ?
n 1 :
n 2 :
n 3 :
n 4 :
n 5 :
Figure 3: A synchronous tree pair containing frag-
ments ?
L
= ?
L
(n
1
, n
2
) and ?
R
= ?
R
(n
3
). Since
links(n
1
, n
2
) = links(n
3
) = { 2 , 4 , 5}, we can de-
fine synchronous fragment ? = ??
L
, ?
R
?. Note also
that node n
3
is a maximal node and node n
5
is not.
?(n
1
) = 2 5 5 3 3 2 4 4 ; ?(n
3
) = 2 5 5 4 4 2 .
2 Synchronous Tree-Adjoining Grammar
A tree-adjoining grammar (TAG) consists of a set of
elementary tree structures of arbitrary depth, which
are combined by substitution, familiar from context-
free grammars, or an operation of adjunction that is
particular to the TAG formalism. Auxiliary trees
are elementary trees in which the root and a frontier
node, called the foot node and distinguished by the
diacritic ?, are labeled with the same nonterminalA.
The adjunction operation involves splicing an auxil-
iary tree in at an internal node in an elementary tree
also labeled with nonterminal A. Trees without a
foot node, which serve as a base for derivations, are
called initial trees. For further background, refer to
the survey by Joshi and Schabes (1997).
We depart from the traditional definition in nota-
tion only by specifying adjunction and substitution
sites explicitly with numbered links. Each link may
be used only once in a derivation. Operations may
only occur at nodes marked with a link. For sim-
plicity of presentation we provisionally assume that
only one link is permitted at a node. We later drop
this assumption.
In a synchronous TAG (STAG) the elementary
structures are ordered pairs of TAG trees, with a
linking relation specified over pairs of nonterminal
nodes. Each link has two locations, one in the left
tree in a pair and the other in the right tree. An ex-
ample of an STAG derivation including both substi-
tution and adjunction is given in Figure 2. For fur-
ther background, refer to the work of Shieber and
Schabes (1990) and Shieber (1994).
3 k-arization Algorithm
For a synchronous tree pair ? = ??L, ?R?, a frag-
ment of ?L (or ?R) is a complete subtree rooted at
some node n of ?L, written ?L(n), or else a subtree
rooted at n with a gap at node n?, written ?L(n, n?);
see Figure 3 for an example. We write links(n) and
links(n, n?) to denote the set of links of ?L(n) and
?L(n, n
?
), respectively. When we do not know the
root or gap nodes of some fragment ?L, we also
write links(?L).
We say that a set of links ? from ? can be iso-
lated if there exist fragments ?L and ?R of ?L
and ?R, respectively, both with links ?. If this is
the case, we can construct a synchronous fragment
? = ??L, ?R?. The goal of our algorithm is to de-
compose ? into synchronous fragments such that the
maximum number of links of a synchronous frag-
ment is kept to a minimum, and ? can be obtained
from the synchronous fragments by means of the
usual substitution and adjunction operations. In or-
der to simplify the presentation of our algorithm we
assume, without any loss of generality, that all ele-
mentary trees of the source STAG have nodes with
at most two children.
3.1 Maximal Nodes
A node n of ?L (or ?R) is called maximal if
(i) links(n) 6= ?, and (ii) it is either the root node
of ?L or, for its parent node n?, we have links(n?) 6=
links(n). Note that for every node n? of ?L such
that links(n?) 6= ? there is always a unique maxi-
mal node n such that links(n?) = links(n). Thus,
for the purpose of our algorithm, we need only look
at maximal nodes as places for excising tree frag-
ments. We can show that the number of maxi-
mal nodes Mn in a subtree ?L(n) always satisfies
|links(n)| ?Mn ? 2? |links(n)| ? 1.
Let n be some node of ?L, and let l(n) be the
(unique) link impinging on n if such a link exists,
and l(n) = ? otherwise. We associate n with a
string ?(n), defined by a pre- and post-order traver-
sal of fragment ?L(n). The symbols of ?(n) are the
links in links(n), viewed as atomic symbols. Given
a node n with p children n1, . . . , np, 0 ? p ? 2,
we define ?(n) = l(n)?(n1) ? ? ??(np) l(n). See
again Figure 3 for an example. Note that |?(n)| =
2? |links(n)|.
606
31
1
1
1
2
2
2
2
X X X X
R
R
R
R
R
R
G
G
G
G
G
G
X
?
X
?
X
?
?
X
?
X
?
excise adjoin transform
?
L
:
n
1
:
n
2
:
Figure 4: A diagram of the tree transformation performed
when fragment ?
L
(n
1
, n
2
) is removed. In this and the
diagrams that follow, patterned or shaded triangles rep-
resent segments of the tree that contain multiple nodes
and at least one link. Where the pattern or shading corre-
sponds across trees in a tree pair, the set of links contained
within those triangles are equivalent.
3.2 Excision of Synchronous Fragments
Although it would be possible to excise synchronous
fragments without creating new nonterminal nodes,
for clarity we present a simple tree transforma-
tion when a fragment is excised that leaves exist-
ing nodes intact. A schematic depiction is given in
Figure 4. In the figure, we demonstrate the exci-
sion process on one half of a synchronous fragment:
?L(n1, n2) is excised to form two new trees. The
excised tree is not processed further. In the exci-
sion process the root and gap nodes of the original
tree are not altered. The material between them is
replaced with a single new node with a fresh non-
terminal symbol and a fresh link number. This non-
terminal node and link form the adjunction or sub-
stitution site for the excised tree. Note that any link
impinging on the root node of the excised fragment
is by our convention included in the fragment and
any link impinging on the gap node is not.
To regenerate the original tree, the excised frag-
ment can be adjoined or substituted back into the
tree from which it was excised. The new nodes that
were generated in the excision may be removed and
the original root and gap nodes may be merged back
together retaining any impinging links, respectively.
Note that if there was a link on either the root or gap
node in the original tree, it is not lost or duplicated
1 1 0 0 0 0 0 0 0 0 1
2 0 1 0 0 0 0 1 0 1 0
5 0 0 1 1 0 0 0 0 0 0
5 0 0 1 1 0 0 0 0 0 0
3 0 0 0 0 0 0 0 1 1 0
3 0 0 0 0 0 0 0 1 1 0
2 0 1 0 0 0 0 1 0 0 0
4 0 0 0 0 1 1 0 0 0 0
4 0 0 0 0 1 1 0 0 0 0
1 1 0 0 0 0 0 0 0 0 1
1 2 5 5 4 4 2 3 3 1
0
Figure 5: Table pi with synchronous fragment
??
L
(n
1
, n
2
), ?
R
(n
3
)? from Figure 3 highlighted.
in the process.
3.3 Method
Let nL and nR be the root nodes of trees ?L and ?R,
respectively. We know that links(nL) = links(nR),
and |?(nL)| = |?(nR)|, the second string being a
rearrangement of the occurrences of symbols in the
first one. The main data structure of our algorithm is
a Boolean matrix pi of size |?(nL)|?|?(nL)|, whose
rows are addressed by the occurrences of symbols in
?(nL), in the given order, and whose columns are
similarly addressed by ?(nR). For occurrences of
links x1 , x2 , the element of pi at a row addressed by
x1 and a column addressed by x2 is 1 if x1 = x2,
and 0 otherwise. Thus, each row and column of pi
has exactly two non-zero entries. See Figure 5 for
an example.
For a maximal node n1 of ?L, we let pi(n1) de-
note the stripe of adjacent rows of pi addressed by
substring ?(n1) of ?(nL). If n1 dominates n2 in ?L,
we let pi(n1, n2) denote the rows of pi addressed by
?(n1) but not by ?(n2). This forms a pair of hori-
zontal stripes in pi. For nodes n3, n4 of ?R, we sim-
ilarly define pi(n3) and pi(n3, n4) as vertical stripes
of adjacent columns. See again Figure 5.
Our algorithm is reported in Figure 6. For each
synchronous tree pair ? = ??L, ?R? from the in-
put grammar, we maintain an agenda B with all
candidate fragments ?L from ?L having at least
two links. These fragments are processed greed-
ily in order of increasing number of links. The
function ISOLATE(), described in more detail be-
607
1: Function KARIZE(G) {G a binary STAG}
2: G? ? STAG with empty set of synch trees;
3: for all ? = ??L, ?R? in G do
4: init pi and B;
5: while B 6= ? do
6: ?L ? next fragment from B;
7: ?R ? ISOLATE(?L, pi, ?R);
8: if ?R 6= null then
9: add ??L, ?R? to G?;
10: ? ? excise ??L, ?R? from ?;
11: update pi and B;
12: add ? to G?;
13: return G?
Figure 6: Main algorithm.
low, looks for a right fragment ?R with the same
links as ?L. Upon success, the synchronous frag-
ment ? = ??L, ?R? is added to the output grammar.
Furthermore, we excise ? from ? and update data
structures pi and B. The above process is iterated
until B becomes empty. We show in section 5 that
this greedy strategy is sound and complete.
The function ISOLATE() is specified in Figure 7.
We take as input a left fragment ?L, which is asso-
ciated with one or two horizontal stripes in pi, de-
pending on whether ?L has a gap node or not. The
left boundary of ?L in pi is the index x1 of the col-
umn containing the leftmost occurrence of a 1 in the
horizontal stripes associated with ?L. Similarly, the
right boundary of ?L in pi is the index x2 of the col-
umn containing the rightmost occurrence of a 1 in
these stripes. We retrieve the shortest substring ?(n)
of ?(nR) that spans over indices x1 and x2. This
means that n is the lowest node from ?R such that
the links of ?L are a subset of the links of ?R(n).
If the condition at line 3 is satisfied, all of the ma-
trix entries of value 1 that are found from column
x1 to column x2 fall within the horizontal stripes
associated with ?L. In this case we can report the
right fragment ?R = ?R(n). Otherwise, we check
whether the entries of value 1 that fall outside of
the two horizontal stripes in between columns x1
and x2 occur within adjacent columns, say from col-
umn x3 ? x1 to column x4 ? x2. In this case,
we check whether there exists some node n? such
that the substring of ?(n) from position x3 to x4 is
1: Function ISOLATE(?L, pi, ?R)
2: select n ? ?R such that ?(n) is the shortest
string within ?(nR) including left/right bound-
aries of ?L in pi;
3: if |?(n)| = 2? |links(?L)| then
4: return ?R(n);
5: select n? ? ?R such that ?(n?) is the gap string
within ?(n) for which links(n) ? links(n?) =
links(?L);
6: if n? is not defined then
7: return null; {more than one gap}
8: return ?R(n, n?);
Figure 7: Find synchronous fragment.
an occurrence of string ?(n?). This means that n?
is the gap node, and we report the right fragment
?L = ?R(n, n
?
). See again Figure 5.
We now drop the assumption that only one link
may impinge on a node. When multiple links im-
pinge on a single node n, l(n) is an arbitrary order
over those links. In the execution of the algorithm,
any stripe that contains one link in l(n) it must in-
clude every link in l(n). This prevents the excision
of a proper subset of the links at any node. This pre-
serves correctness because excising any proper sub-
set would impose an order over the links at n that
is not enforced in the input grammar. Because the
links at a node are treated as a unit, the complexity
of the algorithm is not affected.
4 Complexity
We discuss here an implementation of the algo-
rithm of section 3 resulting in time complexity
O(|G|+ |Y | ? L3G), where Y is the set of syn-
chronous tree pairs of G and LG is the maximum
number of links in a synchronous tree pair in Y .
Consider a synchronous tree pair ? = ??L, ?R?
with L links. If M is the number of maximal nodes
in ?L or ?R, we have M = ?(L) (Section 3.1). We
implement the sparse table pi inO(L) space, record-
ing for each row and column the indices of its two
non-zero entries. We also assume that we can go
back and forth between maximal nodes n and strings
?(n) in constant time. Here each ?(n) is represented
by its boundary positions within ?(nL) or ?(nR),
nL and nR the root nodes of ?L and ?R, respectively.
608
At line 2 of the function ISOLATE() (Figure 7) we
retrieve the left and right boundaries by scanning the
rows of pi associated with input fragment ?L. We
then retrieve node n by visiting all maximal nodes
of ?L spanning these boundaries. Under the above
assumptions, this can be done in time O(L). In a
similar way we can implement line 5, resulting in
overall run time O(L) for function ISOLATE().
In the function KARIZE() (Figure 6) we use buck-
ets Bi, 1 ? i ? L, where each Bi stores the candi-
date fragments ?L with |links(?L)| = i. To populate
these buckets, we first process fragments ?L(n) by
visiting bottom up the maximal nodes of ?L. The
quantity |links(n)| is computed from the quantities
|links(ni)|, where ni are the highest maximal nodes
dominated by n. (There are at most two such nodes.)
Fragments ?L(n, n?) can then be processed using
the relation |links(n, n?)| = |links(n)| ? |links(n?)|.
In this way each fragment is processed in constant
time, and population of all the buckets takes O(L2)
time.
We now consider the while loop at lines 5 to 11 in
function KARIZE(). For a synchronous tree pair ?,
the loop iterates once for each candidate fragment
?L in some bucket. We have a total of O(L2) it-
erations, since the initial number of candidates in
the buckets is O(L2), and the possible updating of
the buckets after a synchronous fragment is removed
does not increase the total size of all the buckets. If
the links in ?L cannot be isolated, one iteration takes
time O(L) (the call to function ISOLATE()). If the
links in ?L can be isolated, then we need to restruc-
ture pi and to repopulate the buckets. The former
can be done in time O(L) and the latter takes time
O(L2), as already discussed. Crucially, the updat-
ing of pi and the buckets takes place no more than
L ? 1 times. This is because each time we excise
a synchronous fragment, the number of links in ? is
reduced by at least one.
We conclude that function KARIZE() takes time
O(L3) for each synchronous tree ?, and the total
running time is O(|G|+ |Y | ? L3G), where Y is the
set of synchronous tree pairs of G. The term |G| ac-
counts for the reading of the input, and dominates
the complexity of the algorithm only in case there
are very few links in each synchronous tree pair.
A
B C
D 1
w
3 4
E 2
x
5
B
D 1
w
3
6
n 1 :
n 2 :
n 3 :
n 4 :
? : ?
?
:
A?
A
Figure 8: In ? links 3 and 5 cannot be isolated because
the fragment would have to contain two gaps. However,
after the removal of fragment ?(n
1
, n
2
), an analogous
fragment ??(n
3
, n
4
) may be removed.
5 Proof of Correctness
The algorithm presented in the previous sections
produces an optimal k-arization for the input gram-
mar. In this section we sketch a proof of correctness
of the strategy employed by the algorithm.2
The k-arization strategy presented above is
greedy in that it always chooses the excisable frag-
ment with the smallest number of links at each step
and does not perform any backtracking. We must
therefore show that this process cannot result in a
non-optimal solution. If fragments could not overlap
each other, this would be trivial to show because the
excision process would be confluent. If all overlap-
ping fragments were cases of complete containment
of one fragment within another, the proof would also
be trivial because the smallest-to-largest excision or-
der would guarantee optimality. However, it is pos-
sible for fragments to partially overlap each other,
meaning that the intersection of the set of links con-
tained in the two fragments is non-empty and the dif-
ference between the set of links in one fragment and
the other is also non-empty. Overlapping fragment
configurations are given in Figure 9 and discussed in
detail below.
The existence of partially overlapping fragments
complicates the proof of optimality for two reasons.
First, the excision of a fragment ? that is partially
overlapped with another fragment ? necessarily pre-
cludes the excision of ? at a later stage in the ex-
2Note that the soundness of the algorithm can be easily veri-
fied from the fact that the removal of fragments can be reversed
by performing standard STAG adjunction and substitution oper-
ations until a single STAG tree pair is produced. This tree pair
is trivially homomorphic to the original tree pair and can easily
be mapped to the original tree pair.
609
(1, 1?)
[ [
A
B
C
D
n 1 :
n 2 :
n 3 :
n 4 :
A
B C
n 5 :
n 6 : n 7 :
A
B
C D
n 8 :
n 9 :
n 10 : n 11 :
(2) (3 )
Figure 9: The four possible configurations of overlapped
fragments within a single tree. For type 1, let ? =
?(n
1
, n
3
) and ? = ?(n
2
, n
4
). The roots and gaps of the
fragments are interleaved. For type 1?, let ? = ?(n
1
, n
3
)
and ? = ?(n
2
). The root of ? dominates the gap of ?.
For type 2, let ? = ?(n
5
, n
6
) and ? = ?(n
5
, n
7
). The
fragments share a root and have gap nodes that do not
dominate each other. For type 3 let ? = ?(n
8
, n
10
) and
? = ?(n
9
, n
11
). The root of ? dominates the root of ?,
both roots dominate both gaps, but neither gap dominates
the other.
cision process. Second, the removal of a fragment
may cause a previously non-isolatable set of links to
become isolatable, effectively creating a new frag-
ment that may be advantageous to remove. This is
demonstrated in Figure 8. These possibilities raise
the question of whether the choice between remov-
ing fragments ? and ? may have consequences at a
later stage in the excision process. We demonstrate
that this choice cannot affect the k found for a given
grammar.
We begin by sketching the proof of a lemma that
shows that removal of a fragment ? that partially
overlaps another fragment ? always leaves an anal-
ogous fragment that may be removed.
5.1 Validity Preservation
Consider a STAG tree pair ? containing the set of
links ? and two synchronous fragments ? and ?
with ? containing links links(?) and ? containing
links(?) (links(?), links(?) ( ?).
If ? and ? do not overlap, the removal of ? is
defined as validity preserving with respect to ?.
If ? and ? overlap, removal of ? from ? is valid-
ity preserving with respect to ? if after the removal
there exists a valid synchronous fragment (contain-
ing at most one gap on each side) that contains all
and only the links (links(?)? links(?))?{x}where
x is the new link added to ?.
remove ?
remove ?
A
B
C
D
E
F G
n 1 :
n 2 :
n 3 :
n 4 :
n 5 :
n 6 : n 7 :
An 1 : En 5 :
Cn 3 :
x x
Dn 4 :
Fn 6 :
H I
An 1 :
Bn 2 :
J x
Dn 4 :
En 5 :
K x
Dn 4 :
Figure 10: Removal from a tree pair ? containing type 1?
type 2 fragment overlap. The fragment ? is represented
by the horizonal-lined pieces of the tree pair. The frag-
ment ? is represented by the vertical-lined pieces of the
tree pair. Cross-hatching indicates the overlapping por-
tion of the two fragments.
We prove a lemma that removal of any syn-
chronous fragment from an STAG tree pair is va-
lidity preserving with respect to all of the other syn-
chronous fragments in the tree pair.
It suffices to show that for two arbitrary syn-
chronous fragments ? and ?, the removal of ? is
validity preserving with respect to ?. We show this
by examination of the possible configurations of ?
and ?.
Consider the case in which ? is fully contained
within ?. In this case links(?) ( links(?). The re-
moval of ? leaves the root and gap of ? intact in both
trees in the pair, so it remains a valid fragment. The
new link is added at the new node inserted where
? was removed. Since ? is fully contained within
?, this node is below the root of ? but not below
its gap. Thus, the removal process leaves ? with the
links (links(?)?links(?))?{x}, where x is the link
added in the removal process; the removal is validity
preserving.
Synchronous fragments may partially overlap in
several different ways. There are four possible con-
figurations for an overlapped fragment within a sin-
gle tree, depicted in Figure 9. These different single-
tree overlap types can be combined in any way to
form valid synchronous fragments. Due to space
constraints, we consider two illustrative cases and
leave the remainder as an exercise to the reader.
An example of removing fragments from
a tree set containing type 1?type 2 over-
lapped fragments is given in Figure 10.
Let ? = ??L(n1, n3), ?R(n5, n6)?. Let
610
? = ??L(n2, n4), ?R(n5, n7)?. If ? is re-
moved, the validity preserving fragment for ? is
???L(n1, n4), ?
?
R(n5)?. It contains the links in the
vertical-lined part of the tree and the new link x .
This forms a valid fragment because both sides con-
tain at most one gap and both contain the same set
of links. In addition, it is validity preserving for ?
because it contains exactly the set of links that were
in links(?) and not in links(?) plus the new link
x . If we instead choose to remove ?, the validity
preserving fragment for ? is ???L(n1, n4), ?
?
R(n5)?.
The links in each side of this fragment are the same,
each side contains at most one gap, and the set of
links is exactly the set left over from links(?) once
links(?) is removed plus the newly generated link x .
An example of removing fragments from a tree
set containing type 1??type 3 (reversed) overlapped
fragments is given in Figure 11. If ? is re-
moved, the validity preserving fragment for ? is
???L(n1), ?
?
R(n4)?. If ? is removed, the validity pre-
serving fragment for ? is ???L(n1, n8), ?
?
R(n4)?.
Similar reasoning follows for all remaining types
of overlapped fragments.
5.2 Proof Sketch
We show that smallest-first removal of fragments is
optimal. Consider a decision point at which a choice
is made about which fragment to remove. Call the
size of the smallest fragments at this pointm, and let
the set of fragments of size m be X with ?, ? ? X .
There are two cases to consider. First, consider
two partially overlapped fragments ? ? X and
? /? X . Note that |links(?)| < |links(?)|. Valid-
ity preservation of ? with respect to ? guarantees
that ? or its validity preserving analog will still be
available for excision after ? is removed. Excising
? increases k more than excising ? or any fragment
that removal of ? will lead to before ? is considered.
Thus, removal of ? cannot result in a smaller value
for k if it is removed before ? rather than after ?.
Second, consider two partially overlapped frag-
ments ?, ? ? X . Due to the validity preservation
lemma, we may choose arbitrarily between the frag-
ments in X without jeopardizing our ability to later
remove other fragments (or their validity preserving
analogs) in that set. Removal of fragment ? cannot
increase the size of any remaining fragment.
Removal of ? or ? may generate new fragments
remove ?
remove ?
A
B
C
n 1 :
n 2 :
n 3 :
E
F G
n 5 :
n 6 : n 7 :
Dn 4 : An 1 :
Cn 3 :
xH En 5 :
x
Fn 6 :
I
Dn 4 : An 1 :
Bn 2 :
xJ ?
Dn 4 :
K x
Gn 7 :
n 8 :
Figure 11: Removal from a tree pair ? containing a type
1
??type 3 (reversed) fragment overlap. The fragment ? is
represented by the horizontal lined pieces of the tree pair.
The fragment ? is represented by the vertical-lined pieces
of the tree pair. Cross-hatching indicates the overlapping
portion of the two fragments.
that were not previously valid and may reduce the
size of existing fragments that it overlaps. In addi-
tion, removal of ?may lead to availability of smaller
fragments at the next removal step than removal of ?
(and vice versa). However, since removal of either ?
or ? produces a k of size at leastm, the later removal
of fragments of size less than m cannot affect the k
found by the algorithm. Due to validity preservation,
removal of any of these smaller fragments will still
permit removal of all currently existing fragments or
their analogs at a later step in the removal process.
If the removal of ? generates a new fragment ? of
size larger thanm all remaining fragments inX (and
all others smaller than ?) will be removed before ?
is considered. Therefore, if removal of ? generates a
new fragment smaller than ?, the smallest-first strat-
egy will properly guarantee its removal before ?.
6 Conclusion
In order for STAG to be used in machine translation
and other natural-language processing tasks it must
be possible to process it efficiently. The difficulty in
parsing STAG stems directly from the factor k that
indicates the degree to which the correspondences
are intertwined within the elementary structures of
the grammar. The algorithm presented in this pa-
per is the first method available for k-arizing a syn-
chronous TAG grammar into an equivalent grammar
with an optimal value for k. The algorithm operates
offline and requires only O(|G|+ |Y | ? L3G) time.
Both the derivation trees and derived trees produced
are trivially homomorphic to those that are produced
by the original grammar.
611
References
Aho, Alfred V. and Jeffrey D. Ullman. 1969. Syntax di-
rected translations and the pushdown assembler. Jour-
nal of Computer and System Sciences, 3(1):37?56.
Chiang, David and Owen Rambow. 2006. The hid-
den TAG model: synchronous grammars for parsing
resource-poor languages. In Proceedings of the 8th
International Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+ 8), pages 1?8.
Gildea, Daniel, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics and the Association for Computa-
tional Linguistics (COLING/ACL-06), July.
Han, Chung-Hye. 2006a. Pied-piping in relative clauses:
Syntax and compositional semantics based on syn-
chronous tree adjoining grammar. In Proceedings
of the 8th International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+ 8), pages
41?48, Sydney, Australia.
Han, Chung-Hye. 2006b. A tree adjoining grammar
analysis of the syntax and semantics of it-clefts. In
Proceedings of the 8th International Workshop on Tree
Adjoining Grammars and Related Formalisms (TAG+
8), pages 33?40, Sydney, Australia.
Joshi, Aravind K. and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Sa-
lomaa, editors, Handbook of Formal Languages.
Springer, pages 69?124.
Nesson, Rebecca and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July.
Nesson, Rebecca and Stuart M. Shieber. 2007. Extrac-
tion phenomena in synchronous TAG syntax and se-
mantics. In Proceedings of Syntax and Structure in
Statistical Translation (SSST), Rochester, NY, April.
Nesson, Rebecca, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas (AMTA
2006), Boston, Massachusetts, 8-12 August.
Satta, Giorgio. 1992. Recognition of linear context-free
rewriting systems. In Proceedings of the 10th Meet-
ing of the Association for Computational Linguistics
(ACL92), pages 89?95, Newark, Delaware.
Seki, H., T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
Shieber, Stuart M. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385, Novem-
ber.
Shieber, Stuart M. and Yves Schabes. 1990. Syn-
chronous tree adjoining grammars. In Proceedings of
the 13th International Conference on Computational
Linguistics (COLING ?90), Helsinki, August.
Weir, David. 1988. Characterizing mildly context-
sensitive grammar formalisms. PhD Thesis, Depart-
ment of Computer and Information Science, Univer-
sity of Pennsylvania.
Zhang, Hao and Daniel Gildea. 2007. Factorization of
synchronous context-free grammars in linear time. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation (SSST), April.
Zhang, Hao, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference/North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
612
Unifying Annotated Discourse Hierarchies to Create a Gold Standard
Marco Carbone, Ya?akov Gal, Stuart Shieber, and Barbara Grosz
Division of Engineering and Applied Sciences
Harvard University
33 Oxford St.
Cambridge, MA 02138
mcarbone,gal,shieber,grosz@eecs.harvard.edu
Abstract
Human annotation of discourse corpora typi-
cally results in segmentation hierarchies that
vary in their degree of agreement. This paper
presents several techniques for unifying multi-
ple discourse annotations into a single hierar-
chy, deemed a ?gold standard? ? the segmen-
tation that best captures the underlying linguis-
tic structure of the discourse. It proposes and
analyzes methods that consider the level of em-
beddedness of a segmentation as well as meth-
ods that do not. A corpus containing annotated
hierarchical discourses, the Boston Directions
Corpus, was used to evaluate the ?goodness?
of each technique, by comparing the similar-
ity of the segmentation it derives to the original
annotations in the corpus. Several metrics of
similarity between hierarchical segmentations
are computed: precision/recall of matching ut-
terances, pairwise inter-reliability scores (   ),
and non-crossing-brackets. A novel method for
unification that minimizes conflicts among an-
notators outperforms methods that require con-
sensus among a majority for the   and precision
metrics, while capturing much of the structure
of the discourse. When high recall is preferred,
methods requiring a majority are preferable to
those that demand full consensus among anno-
tators.
1 Introduction
The linguistic structure of a discourse is composed of
utterances that exhibit meaningful hierarchical relation-
ships (Grosz and Sidner, 1986). Automatic segmentation
of discourse forms the basis for many applications, from
information retrieval and text summarization to anaphora
resolution (Hearst, 1997). These automatic methods, usu-
ally based on supervised machine learning techniques,
require a manually annotated corpus of data for train-
ing. The creation of these corpora often involves multiple
judges annotating the same discourses, so as to avoid bias
from using a single judge?s annotations as ground truth.
Usually, for a particular discourse, these multiple annota-
tions are unified into a single annotation, either manually
by the annotators? discussions or automatically. How-
ever, annotation unification approaches have not been for-
mally evaluated, and although manual unification might
be the best approach, it can be time-consuming. In-
deed, much of the work on automatic recognition of dis-
course structure has focused on linear, rather than hi-
erarchical segmentation (Hearst, 1997; Hirschberg and
Nakatani, 1996), because of the difficulties of obtain-
ing consistent hierarchical annotations. In addition, those
approaches that do handle hierarchical segmentation do
not address automatic unification methods (Carlson et al,
2001; Marcu, 2000).
There are several reasons for the prevailing emphasis
on linear annotation and the lack of work on automatic
methods for unifying hierarchical discourse annotations.
First, initial attempts to create annotated hierarchical cor-
pora of discourse structure using naive annotators have
met with difficulties. Rotondo (1984) reported that ?hi-
erarchical segmentation is impractical for naive subjects
in discourses longer than 200 words.? Passonneau and
Litman (1993) conducted a pilot study in which subjects
found it ?difficult and time-consuming? to identify hi-
erarchical relations in discourse. Other attempts have
had more success using improved annotation tools and
more precise instructions (Grosz and Hirschberg, 1992;
Hirschberg and Nakatani, 1996). Second, hierarchical
segmentation of discourse is subjective. While agreement
among annotators regarding linear segmentation has been
found to be higher than 80% (Hearst, 1997), with respect
to hierarchical segmentation it has been observed to be
as low as 60% (Flammia and Zue, 1995). Moreover, the
precise definition of ?agreement? with respect to hierar-
chical segmentation is unclear, complicating evaluation.
It is natural to consider two segments in separate annota-
tions to agree if they both span precisely the same utter-
ances and agree on the level of embeddedness. However,
it is less clear how to handle segments that share the same
utterances but differ with respect to the level of embed-
dedness.
In this paper, we show that despite these difficulties it is
possible to automatically combine a set of multi-level dis-
course annotations together into a single gold standard, a
segmentation that best captures the underlying linguis-
tic structure of the discourse. We aspire to create cor-
pora analogous to the Penn Treebank in which a unique
parse tree exists for each sentence that is agreed upon by
all to convey the ?correct? parse of the sentence. How-
ever, whereas the Penn Treebank parses are determined
through a time-consuming negotiation between labelers,
we aim to derive gold standard segmentations automati-
cally.
There are several potential benefits for having a unify-
ing standard for discourse corpora. First, it can constitute
a unique segmentation of the discourse that is deemed the
nearest approximation of the true objective structure, as-
suming one exists. Second, it can be used as a single uni-
fied version with which to train and evaluate algorithms
for automatic discourse segmentation. Third, it can be
used as a preprocessing step for computational tasks that
require discourse structure, such as anaphora resolution
and summarization.
In this work, we describe and evaluate several ap-
proaches for unifying multiple hierarchical discourse seg-
mentations into one gold standard. Some of our ap-
proaches measure the agreement between annotations by
taking into account the level of embeddedness and others
ignore the hierarchy. We also introduce a novel method,
called the Conflict-Free Union, that minimizes the num-
ber of conflicts between annotations. For our experi-
ments, we used the Boston Directions Corpus (BDC).1
Ideally, each technique would be evaluated with re-
spect to a single unified segmentation of the BDC that
was deemed ?true? by annotators who are experts in dis-
course theory, but we did not have the resources to at-
tempt this task. Instead, we measure each technique by
comparing the average similarity between its gold stan-
dard and the original annotations used to create it. Our
similarity metrics measure both hierarchical and linear
segment agreement using precision/recall metrics, inter-
reliability similarities among annotations using the (   )
metric, and percentage of non-crossing-brackets.
We found that there is no single approach that does
1The Boston Directions Corpus was designed and collected
by Barbara Grosz, Julia Hirschberg, and Christine H. Nakatani.
well with respect to all of the similarity metrics. How-
ever, the Conflict-Free Union approach outperforms the
other methods for the   and precision metrics. Also, tech-
niques that include majority agreements of annotators
have better recall than techniques which demanded full
consensus among annotators. We also uncovered some
flaws in each technique; for example, we found that gold
standards that include dense structure are over-penalized
by some of the metrics.
2 Methods for Creating a Gold Standard
It is likely that there is no perfect way to find and evaluate
a gold standard, and in some cases there may be multiple
segmentations that are equally likely to serve as a gold
standard. In the BDC corpus, unlike the Penn Treebank,
there are multiple annotations for each discourse which
were not manually combined into one gold standard an-
notation. In this paper, we explore automatic methods to
create a gold standard for the BDC corpus. These meth-
ods could also be used on other corpora with non-unified
annotations. Next, we present several automatic methods
to combine multiple human-annotated discourse segmen-
tations into one gold standard.
2.1 Flat vs. Hierarchical Approaches
Most previous work that has combined multiple an-
notations has used linear segmentations, i.e. dis-
course segmentations without hierarchies (Hirschberg
and Nakatani, 1996). In general, the hierarchical nature
of discourse structure has not been considered when com-
puting labeler inter-reliability and in evaluations of agree-
ment with automatic methods. Since computational dis-
course theory relies on the hierarchy of its segments, we
will consider it in this paper. For each approach that fol-
lows, we consider both a ?flat? version, which does not
consider level of embeddedness, and a ?full? approach,
which does. We analyze the differences between the flat
and full versions for each approach.
2.2 Segment Definition
A discourse is made up of a sequence of utterances,
  	 
 
. In this paper, we define a segment as
a triple

	
, where  Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 79?86,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
		
ffProceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 9?16,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Extraction Phenomena in Synchronous TAG Syntax and Semantics
Rebecca Nesson and Stuart M. Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138
{nesson,shieber}@deas.harvard.edu
Abstract
We present a proposal for the structure
of noun phrases in Synchronous Tree-
Adjoining Grammar (STAG) syntax and
semantics that permits an elegant and uni-
form analysis of a variety of phenom-
ena, including quantifier scope and ex-
traction phenomena such as wh-questions
with both moved and in-place wh-words,
pied-piping, stranding of prepositions, and
topicalization. The tight coupling be-
tween syntax and semantics enforced by
the STAG helps to illuminate the critical
relationships and filter out analyses that
may be appealing for either syntax or se-
mantics alone but do not allow for a mean-
ingful relationship between them.
1 Introduction
Nesson and Shieber (2006) showed how a now-
standard variant of the tree-adjoining grammar
(TAG) formalism (multi-component, multiple ad-
junction, finite-feature-based TAG), when synchro-
nized, leads to a natural analysis of the syntax-
semantics relation, including handling of syntactic
movement phenomena such as wh questions and rel-
ativization, semantic ?movement? phenomena such
as quantification, quantifier scope ambiguity, and
even their interactions as found in pied-piped rela-
tive clauses.1 phenomena were previously viewed
1This work was supported in part by grant IIS-0329089 from
the National Science Foundation.
as problematic for TAG analyses, leading to the hy-
pothesizing of various extensions to the TAG for-
malism (Kallmeyer and Romero, 2004, and work
cited therein). Independently, Han (2006a) devel-
oped a similar synchronous TAG analysis of pied-
piping, providing evidence for the naturalness of the
analysis.
Here, we update the analyses of noun phrases
found in the previous works in one simple way,
again with no additional formal TAG innovations,
and show that it allows a further coverage of extrac-
tion and quantification phenomena as well as in-situ
wh-phrases and topicalization. We emphasize that
no novel formal devices are postulated to achieve
this increased coverage ? just a simple, natural and
uniform change to the canonical structure of NPs
and their semantics.
A word may be useful on the pertinence of this
work in a workshop on ?syntax and structure in ma-
chine translation?, above and beyond the intrinsic
importance of exploring the ?applications of [syn-
chronous/transduction grammars] to related areas
including. . . formal semantics? underlying the work-
shop. Tree-structured mappings are advocated for
machine translation systems because they allow for
the expression of generalizations about relationships
between languages more accurately and effectively.
Evidence for this benefit ought to be found in the
ability of the formalisms to characterize the primi-
tive linguistic relationships as well, in particular, the
form-meaning relationship for a natural language.
The present work is part of a general program to
explore the suitability of synchronous grammars for
expressing this primary linguistic relationship. Inso-
9
far as it is successful, it lends credence to the use of
these formal tools for a variety of language process-
ing tasks, including MT. Insofar as it reveals insuffi-
ciencies in the formalism, it may lead to insights in
the design or deployment of alternative systems.
We present a proposal for the structure of noun
phrases in Synchronous Tree-Adjoining Grammar
(STAG) syntax and semantics that permits an elegant
and uniform analysis of a variety of phenomena, in-
cluding quantifier scope and extraction phenomena
such as wh-questions with both moved and in-situ
wh-words, pied-piping, stranding of prepositions,
and topicalization. Furthermore, the tight coupling
between syntax and semantics enforced by grammar
synchronization helps to illuminate the critical rela-
tionships and filter out analyses that may be appeal-
ing for either syntax or semantics alone but do not
allow for a meaningful relationship between them.
We begin in Section 2 with a brief review of syn-
chronous TAG and its application to English syntax
and semantics. In Section 3, we present an analysis
of quantifier scope that elucidates the relationship
between the syntactic and semantic structures and
explains an anomaly of previously proposed analy-
ses. We apply the underlying idea from Section 3
to wh-questions in Section 4, showing that an al-
teration of the standard TAG syntax analysis of wh-
questions produces the same derived trees while also
elegantly modeling in-place wh-words. In Section 5
we present a challenging case for STAG syntax and
semantics, the stranding of prepositions. This case
is particularly difficult because the syntactic analy-
ses suggested by previous work in STAG syntax do
not encapsulate the relationships that appear neces-
sary for the semantics. Our proposed analysis falls
out naturally from the revision to the syntax of wh-
words and respects both Frank?s Condition on Ele-
mentary Tree Minimality (CETM) and the seman-
tic relationships in the construction. In Section 6
we give an analysis of topicalization that also fol-
lows from the underlying ideas of the earlier analy-
ses. We summarize the main ideas of the analysis in
Section 7.
2 Introduction to Synchronous TAG
A tree-adjoining grammar (TAG) consists of a
set of elementary tree structures of arbitrary depth,
S
NP? V P
NP?V
likes
NP
John
S
V P
NP?V
likes
NP
John
V P
Adv V P?
S
NP? V P
NP?V
likesapparently
V P
Adv
S
NP?
V P
NP?V
likes
apparently
=?
=?
Figure 1: Example TAG substitution and adjunction.
which are combined with two operations, substitu-
tion and adjunction. Internal nodes in the elementary
trees are labeled with a nonterminal symbol. Fron-
tier nodes may be labeled with either terminal sym-
bols or nonterminal symbols annotated with one of
the diacritics ? or ?. The ? diacritic marks a frontier
nonterminal node as a substitution node, the target
of the substitution operation. The substitution op-
eration occurs when an elementary tree rooted in a
nonterminal symbol A replaces a substitution node
with the same nonterminal symbol.
Auxiliary trees are elementary trees in which the
root and a frontier node, called the foot node and
distinguished by the diacritic ?, are labeled with the
same nonterminal A. The adjunction operation in-
volves splicing an auxiliary tree in at an internal
node in an elementary tree also labeled with non-
terminal A. Trees without a foot node, intended for
substitution rather than adjunction into other trees,
are called initial trees. Examples of the substitu-
tion and adjunction operations on sample elemen-
tary trees are shown in Figure 1. For further infor-
mation, refer to Joshi and Schabes (1997).
Synchronous TAG (Shieber, 1994; Shieber and
Schabes, 1990) extends TAG by taking the elemen-
tary structures to be pairs of TAG trees with links
between particular nodes in those trees. Derivation
proceeds as in TAG except that all operations must
be paired. That is, a tree can only be substituted or
adjoined at a node if its pair is simultaneously sub-
stituted or adjoined at a linked node. We notate the
links by using boxed indices i marking linked nodes.
10
mary
j o h n
apparently
likes
1 2
3
4
1
23
4
NP
NP
e
e
V P
Adv V P?
t
t ?
S
NP? V P
NP?V
t
e?
e?
likes
? t , t ?
apparently
John
M a r y
?e , t?
likes
j o h n
apparently
mary
2
3 4
V P
V
likes
V P
Adv
apparently
NP
NP
John M a r y
S
likes
t
?e , t?apparently
t
? t , t ?
mary
e j o h n
e
( a ) ( b ) ( c )
Figure 2: An English syntax/semantics STAG fragment (a), derived tree pair (b), and derivation tree (c) for
the sentence ?John apparently likes Mary.?
As first described by Shieber and Schabes (1990),
STAG can be used to provide a semantics for a TAG
syntactic analysis by taking the tree pairs to repre-
sent a syntactic analysis synchronized with a seman-
tic analysis.
For example, Figure 2(a) contains a sample En-
glish syntax/semantics grammar fragment that can
be used to analyze the sentence ?John apparently
likes Mary?. The node labels we use in the seman-
tics correspond to the semantic types of the phrases
they dominate.
Figure 2(c) shows the derivation tree for the sen-
tence. Substitutions are notated with a solid line and
adjunctions are notated with a dashed line. Each link
in the derivation tree specifies a link number in the
elementary tree pair, providing the location at which
the operations take place. In this case, the tree pairs
for the noun phrases John and Mary substitute into
the likes tree pair at links 3 and 4 , respectively. The
word apparently adjoins at link 2 . The tree pair so
derived is shown in Figure 2(b). The resulting se-
mantic representation can be read off the right-hand
derived tree by treating the leftmost child of a node
as a functor and its siblings as its arguments. Our
sample sentence thus results in the semantic repre-
sentation apparently(likes( john,mary)).
3 Quantifier Scope
We start by reviewing the prior approach to quan-
tifier semantics in synchronous TAG. Consider the
sentence ?Everyone likes someone.? We would like
to allow both the reading where some takes scope
over every and the reading where every takes scope
over some. We start with the proposal of Shieber and
Schabes (1990), which used multi-component TAG
for the semantic portion of a synchronous TAG.
Each quantified noun phrase has a two-component
tree set as its semantics. One component introduces
the variable quantified over in the scope of the quan-
tifier; the other adjoins over the scope to provide the
quantifier and restriction. Williford (1993) explored
the use of multiple adjunction (Schabes and Shieber,
1993) to achieve scope ambiguity. Since the scope
components of subject and object noun phrases
adjoin at the same location in the semantic tree,
they give rise to a systematic ambiguity as to which
dominates the other in the derived tree, reflecting
the semantic scope ambiguity of the sentence; the
derivation tree itself is therefore a scope neutral
representation. Previous work by Han (2006a;
2006b) and Nesson and Shieber (2006) describe
this approach in detail, showing its applicability to
a range of semantic phenomena.
A range of research has proceeded in an alter-
native line of using complex-feature-based TAG ?
rather than synchronous TAG ? for TAG seman-
tics (Kallmeyer and Romero, 2004, and work cited
therein). Semantic representations are carried in fea-
tures associated with nodes. Nonetheless, multi-
component TAG with separate trees for bound po-
sition and scope is used here too. However, the two
trees are syntactic trees, the quantified NP tree and a
vestigial S tree, respectively. (An example is shown
in Figure 6.) In such analyses, the single-node aux-
iliary S tree is used for the scope part of the syntax
in order to get the desired relationship between the
quantifier and the quantified expression in features
threaded through the derivation tree and hence in the
semantics.
The present analysis marries these two ap-
11
NP
Det N?
tevery x
x
t
t ?
e
x
11
every
?e, t??
NP
Det N?
t
t
t ?
e11 ?e, t??
s o m e
so m e
y
y
y
23
4
S
NP? V P
NP?V
likes
likes
1 2
4
t
e ?
e ??e, t? 3
3 4
personone
?e, t?N
NP
Det
every
S
V P
V
likes
NP
Det N
so m e
N t
person
?e, t?
ys o m e
y
t
ttevery x
x
t
person
?e, t?
one one likes
?e, t? e
xe
y
t
person
?e, t?
t
tt
t
person
?e, t?
likes
?e, t? e
xe
y
every
s o m e
x
y
y
x
likes
every s o m e
personperson
3
1 1
4
( a )
( b )( c )
S ? S ? 31 4
Figure 3: The elementary tree pairs (a), derivation tree (b), and derived trees (c) for the sentence ?Everyone
likes someone?. Note that the derivation tree is a scope neutral representation: depending on whether every
or some adjoins higher, we obtain different semantic derived trees and scope orderings.
proaches. Like the previous STAG work, we pro-
pose a solution in which a multi-component tree set
provides semantics for quantified phrases, with mul-
tiple adjunction providing scope ambiguity. Like
the complex-feature-based approach, we reflect the
multi-component structure in the syntax as well. It
is this single change in the analysis that makes pos-
sible the coverage of the wide range of phenomena
we describe here.
Combining these two approaches, we give both
the syntactic and semantic trees for quantifiers two
parts, as depicted in Figure 3(a). In the semantics,
the top part corresponds to the scope of the quan-
tifier and attaches where the quantifier takes scope.
The bottom part corresponds to the bound variable
of the quantifier. By multiply adjoining the scope
parts of the semantic trees of the quantifiers at the
same location in the likes tree, we generate both
available scope readings of the sentence.2 Corre-
spondingly on the syntax side, an NP tree provides
the content of the noun phrase with a vestigial S tree
available as well. Prior to the analyses given in this
paper, the use of two trees in the quantifier syntax
was an arbitrary stipulation used to make the seman-
tic analysis possible. The pairing of the upper tree
2Nesson and Shieber (2006) provide a more in-depth expla-
nation of the multiple-adjunction-driven approach to scope neu-
trality in STAG.
in the syntax with the scope tree in the semantics
explicitly demonstrates their relationship and leads
naturally to the exploration of non-degenerate upper
trees in the syntax that we explore in this paper.
In order to use these multi-component quantifiers,
we change the links in the elementary trees for verbs
to allow a single link to indicate two positions in
the syntax and semantics where a tree pair can ad-
join, as shown in Figure 3(a). We add four-way
links and drop the two-way links used by the un-
quantified noun phrases in the first example. This
choice forces all noun phrase tree pairs to be multi-
component in the syntax and semantics. Essentially,
all noun phrases are ?lifted? a` la Montague. We ex-
plore the consequences of this in Section 6.
We turn now to the ramifications of this new
syntactico-semantic STAG representation, showing
its utility for a range of phenomena.
4 Wh-questions
The structure we propose for quantifiers suggests a
new possibility for the TAG analysis of wh-words.
We propose to simply treat wh-words as regular
noun phrases by making them a multi-component
tree set with an auxiliary tree that adjoins at the root
of the verb tree and contains the lexical content and
an initial tree with an empty frontier that substitutes
at the argument position. This syntactic tree set can
12
S?WH
N P
x
e
t
who t ?
x
S
who
!
S?WH
who N P
!
S
or
which
N P
S
WH N ?
S?
N P
!
S?
S
!
N P
which
N P
WH N ?
t
which x t t ?
e
x
e
x
? e , t??or
Figure 4: Elementary tree pairs for who and which. The left and middle tree sets are the syntactic alternatives
used to model wh-movement and in-situ wh-words. The tree sets on the right provide the semantics.
23
4
S
NP? V P
NP?V
likes
31 4
2
3
4
S
V P
NP?V
likes
31 4
S
W H ?
23
4 S
NP? V P
V
likes
31 4
S
W H ?
NP
! NP
!
Figure 5: Traditional elementary trees for the verb
likes. Using a revised, elementary syntax tree set for
wh-words like who, only the left tree is necessary.
be paired with a multi-component semantic tree set
that has an auxiliary tree containing the scope part
and an initial tree that contains the bound variable.
Wh-questions with the wh-word in place can be ele-
gantly modeled with an alternative syntactic tree set
in which the auxiliary tree has no lexical content and
the wh-word is on the frontier of the initial tree that
substitutes into the argument position. The seman-
tic tree sets for both syntactic variations is the same.
These trees are shown in Figure 4.
Besides the incorporation of a semantics, the ba-
sic analyses for wh-questions familiar from TAG
syntax are otherwise unchanged because the top
piece of the syntax tree set still ends up at the root of
the main verb in sentences such as the following:
(1) Who likes Mary?
who(x, likes(mary,x))
(2) Which person does John like?3
which(x, person(x), likes(x, john))
3The presence of do-support in wh-questions can be handled
independently using a feature on the NP node into which the
bottom part of the wh-word tree pair substitutes that governs
whether and where a do tree adjoins.
(3) Which person does Bill think John likes?
which(x, person(x), thinks(bill, likes(x, john)))
(4) Who does each person like?
who(x,each(y, person(y), likes(x,y)))
each(y, person(y),who(x, likes(x,y)))
Note that in Sentence 3 thinks is not constrained
to appear to the right of who in the syntax, because
thinks and who both adjoin at the same location in
the syntax. However, we can use a feature to force
embedding verbs to adjoin lower than wh-words.
The same situation exists in Sentence 4, though only
in the semantics; the order of words in the syntax
is well-defined but the multiple adjunction of the
scope of who and the scope of each underspecifies
the scope ordering between them. Both scope or-
derings are indeed arguably valid. Again, the pref-
erences for certain orderings can be regulated us-
ing a feature. These issues highlight the many open
questions about how to combine quantification and
wh-terms, but also provides a first step towards their
analysis within a concise STAG construction.
Our approach has several distinct advantages.
First, it allows wh-words to be analyzed in a way that
is uniform with the analysis of other noun phrases
and allows us to simplify the lexical entries for
verbs. In the traditional TAG analysis, wh-words
substitute into specialized lexical trees for verbs that
add an additional frontier node for the wh-word and
abstract over one of the arguments of the verb by
adding an empty terminal node at the frontier. Our
revision to the elementary trees for wh-words allows
us to remove several tree pairs from the elementary
tree sets for verbs such as like. Instead of requir-
ing an elementary tree pair for declarative sentences
and an additional elementary tree for each argument
13
S?? S?
W H
whom
N P
J ohn
?
Sd oe s
V P
V
lik e
S?
Det
a
N ?
N P ?
N
P Ppicture
o f
?
?
?
?
?
?
?
a
b
c
d
e
?
?
?
?
?
?
?
?
?
?
?
?
?
?
a
b
c
d
e
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
a
b
c
d
e
f
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
a
b
c
d
e
f
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
W H ? S
N PN P
N P[ ]
N P
!
Figure 6: Kallmeyer and Scheffler?s syntactic analy-
sis for Sentence 6.
that can be replaced by a fronted wh-word to form a
question (as shown in Figure 5), we can use just the
single declarative sentence elementary tree.
Second, it provides a simple and elegant char-
acterization of the syntax and semantics of wh-
movement and the relationship between fronted and
in-place wh-words. Using the alternative syntax tree
set given in Figure 4 we model in-place use of wh-
words as in Sentence 5 while still maintaining the
usual semantic analysis:
(5) John likes who?
who(x, likes(x, john))
5 Stranded Prepositions
Sentence 6 presents a particularly challenging case
for TAG semantics. The problem arises because who
must contribute its bound variable, x, to the noun
phrase ?a picture of x?. However, in the standard
syntactic analysis who substitutes into the likes tree,
and in any reasonable semantic analysis, who takes
scope at the root of the likes tree.
(6) Who does John like a picture of?
who(x,a(y, and(picture(y),of (x,y)),
likes( john,y)))
Kallmeyer and Scheffler (2004) propose a syntac-
tic analysis in which ?a picture of? adjoins into the
syntactic tree for ?likes?. The syntax for this anal-
ysis is shown for comparison in Figure 6. As-
sociated with the syntactic analysis is a semantic
analysis, which differs from ours in that all of the
semantic computation is accomplished by use of
a flexible set of features that are associated with
nodes in the syntactic trees. This analysis main-
tains Frank?s Constraint on Elementary Tree Min-
imality (CETM) if one analyzes the prepositional
phrase as a complement of picture but it does so at
the expense of a straightforward compositional se-
mantics.4 The source of the problem is that who
contributes its bound variable to likes to form an
intermediate semantics who(x, likes( john,x)), then
a picture of combines non-compositionally to form
the complete semantics given in Sentence 6.
Kroch (1989) describes the intuition eschewing
this analysis: ?The problem is that under such a
derivation, the preposed wh-phrase changes its the-
matic role with each adjunction and the interpreta-
tion of the derived tree is not a simple function of the
interpretations of its component elementary trees.?
When we consider the semantics of the two sen-
tences, the anomaly of this analysis becomes appar-
ent. In the first sentence the entity liked by John is
referred to by the variable contributed by who. In the
second sentence John likes an entirely different en-
tity: the entity referred to by the variable contributed
by a. Kallmeyer and Scheffler obtain the correct se-
mantics by making use of non-local TAG operations
to have the scope part of a adjoin into likes to cap-
ture the semantics of the likes proposition and em-
ploying a feature-based mechanism for swapping the
variables as necessary.
Our revision to the syntax of wh-words provides
an alternative way of maintaining the CETM that of-
fers a much simpler semantic analysis. The details
of the analysis are given in Figure 7. We adjoin who
into the preposition of at link 1 where it contributes
both variable and scope. The tree pair for of at-
taches to a at link 1 , thus allowing the scope parts
of the quantifier a and the wh-word who to end up
taking scope over the main verb as in the analysis of
prepositional phrases given by Nesson and Shieber
(2006). It also places all the bound variables in the
correct propositions without use of non-local opera-
tions or additional manipulation. A diagram of the
derived syntax and semantics is given in Figure 8.
4In addition to suggesting a non-compositional seman-
tics, their syntactic analysis makes use of non-local multi-
component TAG in order to achieve the necessary semantic rela-
tionships. Although their use of non-local TAG may be benign
in terms of complexity, our analysis is set-local. Our proposal
therefore simplifies the syntactic analysis while also bringing it
in line with a straightforward, compositional semantics.
14
NP?
V P
V
likes
t
e ?
?e, t?
?e, ?e, t??
NP? e ?
likes
S
3
4
31 4 1 2 3 4
3
4
2
likes
john a
picture of
w h o
1
12
43
Det
a
yN P
N ?
t
t ?a t
?e, t?? ey
y
1S ? 1
1
1
22
N P
P P
P
of
t ?
?e, t?
?e, t??a n d
e ?of
?e, t?
S ?
NP?
NP?
1 1
11
S ?W H
w h o NP
!
x
e
t
w h o t ?
x
S
john john
N P e
t ?S ?
N ?e, t?
picture picture
Figure 7: The elementary tree pairs and derivation tree for the sentence ?Who does John like a picture of??.
NP?
V P
V
likes
NP?
S
3
4
31 4
2
Det
a
NP
N ?
S ? 1
1
2
NP
P P
P
o f
S ?
NP?
NP?
1
1
john
NP
S ?
N
picture
S ?W H
w h o
NP
!
S
t
e ?
?e, t?
?e, ?e, t??
e ?
likes
1 2 3 4
3
4
y
t
t ?a t
?e, t??
e
y
y
1
1 2
t ?
?e, t?
?e, t??a n d
e ?o f
?e, t?
1
1
x
e
t
w h o t ?
x
john
e
t ?
?e, t?
picture
Figure 8: The derived syntax and semantics for Sentence 6.
6 Topicalization
The insight that allows us to model in-place wh-
words extends to an elegant analysis of topicaliza-
tion as well. The vestigial S? tree that we added
to the tree set for the syntax of every noun phrase
need not always be contentless. Just as we moved
the wh-word who from the top tree in its set to the
bottom tree to model in-situ wh-words, we can move
the lexical content of noun phrases to the top tree in
their sets to model topicalization. For instance, the
alternative tree pair for Mary shown in Figure 9 pro-
vides for an analysis of the sentence
(7) Mary, John likes.
likes(mary,john)
The analysis interacts properly with that for prepo-
sition stranding, so that the sentence
S?
NP e
t ?
S
!
NP
Mary
Mary
S?
NP
S
!
y
t
t ?a t
?e, t?? ey
y
1
1 2D et
a
NP
N ?
1
2
Figure 9: Alternative tree pairs for Mary and a that
model topicalization.
(8) A picture of Mary, John likes.
a(x, and(picture(x), of(mary,x)), likes(x,john))
follows from the tree pair for a in the same figure.
7 Conclusion
In this paper we have proposed a uniform change
to the structure of noun phrases in the STAG
syntactico-semantic grammar. The formal tools we
avail ourselves of comprise synchronous TAG with
15
set-local multicomponent adjunction and multiple
adjunction. Nothing more is required.
All noun phrases now have a uniform multi-
component structure in both the syntax and the
semantics. In the semantics the top part corresponds
to the scope-giving piece provided by the noun
phrase and the bottom part to the bound variable
or simple noun-phrase meaning. In the syntax, the
top part corresponds to the lexical material that
should appear moved to the edge of the sentence or
clause; the bottom part corresponds to the lexical
material that will fill an argument position of some
head. By moving lexical material among the pieces
of the multi-component set in the syntax, we can
simply model phenomena like in-place wh-words
and topicalization.
Making the top parts of wh-word tree sets into
auxiliary trees allows them to adjoin not just to the
main verb but also to heads of modifying clauses,
such as prepositional phrases. This allows us to
handle more complex sentences like Sentence 6
without violating either the CETM or going beyond
simple compositional semantics. In order to allow
the scope-giving part of the wh-word to percolate
up to the root of the semantics of the main verb,
each tree set that it adjoins into on its way must
also have a scope part in the semantics to which
it can adjoin. Scope carriers, such as prepositions,
are therefore also multi-component in the semantics
with a top node to which scope-givers can adjoin.
One nice property of this analysis is that it predicts
the observed facts about disallowed scope orderings
in sentences that have three quantifiers, one of
which is in a modifying clause. The scope part of
the quantifier of the modified clause and the scope
part of the quantifier of the modifying clause form
an indivisible set as the derivation proceeds so that
when they adjoin multiply with the scope part of the
unmodified clause, that quantifier cannot intervene
between them.
Our synchronous grammar treatment of the
syntax-semantic relation with TAG is at least as
simple and arguably more accurate than previous
TAG proposals, offering treatments of such phe-
nomena as in-situ wh-words, stranded prepositions,
and topicalization.
References
Chung-Hye Han. 2006a. Pied-piping in relative clauses:
Syntax and compositional semantics based on syn-
chronous tree adjoining grammar. In Proceedings
of the 8th International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+ 8), pages
41?48, Sydney, Australia.
Chung-Hye Han. 2006b. A tree adjoining grammar anal-
ysis of the syntax and semantics of it-clefts. In Pro-
ceedings of the 8th International Workshop on Tree
Adjoining Grammars and Related Formalisms (TAG+
8), pages 33?40, Sydney, Australia.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, pages
69?124. Springer.
Laura Kallmeyer and Maribel Romero. 2004. LTAG se-
mantics with semantic unification. In Proceedings of
TAG+7, pages 155?162, Vancouver, May.
Laura Kallmeyer and Tatjana Scheffler. 2004. LTAG
analysis for pied-piping and stranding of wh-phrases.
In Proceedings of TAG+7, pages 32?39, Vancouver,
May.
Anthony Kroch. 1989. Asymmetries in long distance
extraction in a tree adjoining grammar. In Mark Baltin
and Anthony Kroch, editors, Alternative Conceptions
of Phrase Structure. University of Chicago Press.
Rebecca Nesson and Stuart M. Shieber. 2006. Sim-
pler TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Grammar,
Malaga, Spain, 29?30 July.
Yves Schabes and Stuart M. Shieber. 1993. An alterna-
tive conception of tree-adjoining derivation. Compu-
tational Linguistics, 20(1):91?124.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics, volume 3, pages 253?258, Helsinki.
Stuart M. Shieber. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385, Novem-
ber.
Sean Williford. 1993. Application of synchronous tree-
adjoining grammar to quantifier scoping phenomena in
English. Undergraduate Thesis, Harvard College.
16
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 88?95,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Probabilistic Synchronous Tree-Adjoining Grammars for Machine
Translation: The Argument from Bilingual Dictionaries
Stuart M. Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138
shieber@seas.harvard.edu
Abstract
We provide a conceptual basis for think-
ing of machine translation in terms of syn-
chronous grammars in general, and proba-
bilistic synchronous tree-adjoining gram-
mars in particular. Evidence for the view
is found in the structure of bilingual dic-
tionaries of the last several millennia.
1 Introduction
In this paper, we provide a conceptual basis for
thinking of machine translation in terms of syn-
chronous grammars in general, and probabilistic
synchronous tree-adjoining grammars in particular.
The basis is conceptual in that the arguments are
based on generalizations about the translation re-
lation at a conceptual level, and not on empirical
results at an engineering level. Nonetheless, the
conceptual idea is consistent with current efforts in
MT, and in fact may be seen as underlying so-called
syntax-aware MT.
We will argue that the nature of the translation re-
lation is such that an appropriate formalism for re-
alizing it should have a set of properties ? expres-
sivity, trainability, efficiency ? that we will charac-
terize more precisely below. There may be multi-
ple formalisms that can achieve these ends, but one,
at least, is probabilistic synchronous tree-adjoining
grammar, and to our knowledge, no other qualita-
tively distinct formalism has been argued to display
all of the requisite properties.
Below, we will discuss the various properties,
with particular attention to an examination of a par-
ticular source of data about the translation relation,
namely bilingual dictionaries. Multilingual lexicog-
raphy has a history of some four millennia or more.
In that time, a great deal of knowledge about par-
ticular translation relations has been explicitly codi-
fied in multilingual dictionaries. More interestingly
for our present purposes, multilingual dictionaries
through their own structuring implicitly express in-
formation about translation relations in general.
In Section 2, we introduce the Construction Prin-
ciple, a property of the translation relation implicit
in the structure of bilingual dictionaries throughout
their four millennium history. Section 3 provides
a review of synchronous tree-adjoining grammars
showing that this formalism directly incorporates the
Construction Principle and allows the formal im-
plementation of bilingual dictionary relations. In
Section 4, we argue that the probabilistic variant
of STAG (PSTAG) inherits the expressivity advan-
tages of STAG while adding the trainability of sta-
tistical MT. Section 5 concerns the practical efficacy
of STAG. We conclude (Section 6) with an overall
proposal for the use of PSTAG in a statistical MT
system. By virtue of its fundamentality to the mod-
eling of the translation relation, PSTAG or its formal
relatives merits empirical examination as a basis for
statistical MT.
2 Expressivity
Of course, a formalism for describing the transla-
tion relation must be able to capture the relations
between words in the two languages: acqua means
water, dormire means sleep, and so forth. Indeed,
the stereotype of a bilingual dictionary is just such
a relation; the HarperCollins Italian College Dictio-
nary (HCICD) (Clari and Love, 1995) contains en-
88
tries ?acqua / water?10 and ?dormire / sleep?191.1
This property doesn?t distinguish among any of the
formal means for capturing these direct lexical re-
lationships. Finite-state string transducers naturally
capture these simple relationships, but so do more
(and less) expressive formalisms.
Simple word-by-word replacement is not a viable
translation method; this was noted even as early
as Weaver?s famous memorandum (Weaver, 1955).
Systems based on word-to-word lexicons, such as
the IBM systems (Brown et al, 1990; Brown et
al., 1993), incorporate further devices that allow re-
ordering of words (a ?distortion model?) and rank-
ing of alternatives (a monolingual language model).
Together, these allow for the possibility that
The Word Principle:
Words translate differently when adjacent to
other words.
This property of the translation relation is patently
true.
Even a word-to-word system with the ability to
reorder words and rank alternatives has obvious lim-
itations, which have motivated the machine transla-
tion research community toward progressively more
expressive formalisms. Again, we see precedent for
the move in bilingual dictionaries, which provide
phrasal translations in addition to simple word trans-
lations: ?by and large / nel complesso?86, ?full moon
/ luna piena?406. The insight at work here is
The Phrase Principle:
Phrases (not words) translate differently when
adjacent to other phrases.
And again, we see this insight informing statisti-
cal machine translation systems, for instance, in the
phrase-based approaches of Och (2003) and Koehn
et al (2003). These two principles, while true, do
not exhaust the insights implicit in the structure of
bilingual dictionaries. A fuller view is accomplished
by moving from words and phrases to constructions.
2.1 The construction principle
The phenomenon that underlies the use of syn-
chronous grammars for MT is simply this:
1Throughout, we notate entries in HCICD with the notation
?entry form / translation form?page, providing the Italian and
English forms, along with the page number of the cited entry.
The Construction Principle:
Words and phrases translate differently in con-
struction with other words.
The notion of in construction with is a structural no-
tion. A word is in construction with another if they
are related by a structural relation of some sort de-
pendent on the identity or role of the word.
For example, the English word take is prototypi-
cally translated with a form of the Italian prendere
?take / prendere?661. But when its object is a bath,
as in the sentence ?I like to take several long bubble
baths every day?, the word is translated with a form
of fare. More accurately, the construction typified
by the phrase take a bath is translated by the corre-
sponding construction typified by the phrase fare un
bagno (?take a bath / fare un bagno?662).
One may think that we are still in the realm of the
Phrase Principle; the phrase take a bath translates as
the phrase fare un bagno. But the generalization is
clearly much more general than that in several ways.
First, the notion of in construction with does not
necessarily lead to contiguous phrases because of
variability within the constructions. Bilingual dic-
tionaries have developed notational conventions for
such cases. When freely variable objects can inter-
vene between the words in construction, a kind of
variable word is used in dictionary entries, such as
SB (somebody), STH (something), QN (qualcuno),
QC (qualcosa). The word take participates in an-
other construction ?take SB by surprise / cogliere
[literally ?catch?] QN di sorpresa?. The phe-
nomenon is widespread. We find entries for light
verb phrases such as take SB by surprise, idiomatic
constructions such as ?pull SB?s leg / prendere in
giro QN?507, and particle constructions such as ?call
SB up / chiamare QN?86. These variable notations
not only stand in for variable textual material and
categorize that material (as specifying an entity (QC)
or human (QN)) but also provide links between the
portions of the two constructions. Whatever lexi-
cal material instantiates a SB variable on the English
side, its translation instantiates the QN in the Ital-
ian. Thus translations require not only structure in
the monolingual representations, but structure bilin-
gually across them.2
2The linking of the subject roles in these constructions is
typically left implicit in these entries, following from an as-
89
Second, even constructions that are in and of
themselves contiguous may become discontiguous
by intervention of other lexical material: modifiers,
appositives, and the like. An example has already
been seen in the example ?I like to take several long
bubble baths every day?. There is no contiguity be-
tween take and bath here. A formalism based purely
on concatenation of contiguous phrases will be un-
able to model such constructions.
These two aspects of variability and interven-
tion within and between constructions preclude sim-
ple concatenative formalisms such as finite-state or
context-free formalisms.
2.2 Prevalence of bilingual constructions
A natural question arises as to the prevalence of such
nontrivial bilingual constructions. Presumably, if
they are sufficiently rare and exotic, it may be ac-
ceptable, and in fact optimal, from an engineering
point of view to ignore them and stay with simpler
formalisms.
We can ask the prevalence question at the level of
types or tokens. At the type level, a simple examina-
tion of a comprehensive modern bilingual dictionary
reveals a quite high frequency of non-word-for-word
translations. Analysis of a small random subsam-
ple of HCICD yielded only 34% of entries of the
?acqua / water?10 sort. In contrast, 52% were con-
tiguous multi-word translations, e.g., ?guarda caso /
strangely enough?100. An additional 11% of entries
had variable content, split about equally between en-
tries with overt marking of variability (?prendere QN
in castagna / to catch SB in the act?100) and im-
plicit variability (?hai fatto caso al suo cappello? /
did you notice his hat??100, in which the ?suo cap-
pello / his hat? pair serves as a placeholder for other
translates. (The remaining 3% is accounted for by
entries providing monolingual equivalences and un-
translated proper names.) The line between implicit
variability and multi-word translations is quite per-
meable, so that many of the 54% of entries classified
as the latter might in fact be better thought of as the
former, and in any case many of the multi-word en-
sumption that subjects are typically linked across these lan-
guages. Where this assumption fails, however, explicit marking
is found in the dictionary, either by using a passive alternation
?piacere a QN / to be liked by SB?424, or implicit linking ?mi
piace / I like it?424.
tries would be subject to noncontiguity through in-
sertion of other lexical material. At the type level,
then, there is plenty of evidence for the Phrase Prin-
ciple and the Construction Principle.
At the token level, the general interest in so-
called syntax-aware statistical MT approaches is it-
self evidence that researchers believe that the to-
kens accounting for the performance gap in current
systems based on the Word and Phrase Principles
transcend those principles in some way, presum-
ably because they manifest the Construction Prin-
ciple.3 Only time will tell if such syntax-aware
systems are able to display performance improve-
ments over their nonstructural alternatives. Success-
ful experiments such as those of Chiang (2005) us-
ing synchronous context-free grammar are a good
first start.4
2.3 Heritage of the construction principle
We have argued that a formalism expressive enough
to model the translation relation implicit in bilin-
gual dictionaries must be based on relations over
constructions, the primitive relations found in such
bilingual dictionaries and founded by the Construc-
tion Principle. The fundamentality of this princi-
ple is evidenced by the fact that it has informed
bilingual dictionaries literally since their inception.
The earliest known bilingual dictionaries are those
incorporated in the so-called lexical texts of an-
cient Mesopotamia from four millennia ago. Even
there, we find evidence of the Construction Princi-
ple in entries that describe translation of words de-
pendent upon words they are in construction with.
Civil (1995) cites an example of the Akkadian word
naka?pu (to gore, to knock down) whose translation
into Sumerian is given differentially dependent on
the nature of ?grammatical constructions with par-
ticular subjects or objects?:
3A reviewer objects that this point is vacuous: ?Is the fact
that researchers aren?t building large-scale statistical semantic
transfer models evidence for the fact that they don?t believe in
semantics?? This is an instance of the logical fallacy of denying
the antecedent. If researchers act on a premise, they believe the
premise. From this it does not follow that if they fail to act on a
premise, they deny the premise.
4It would be more convincing to have empirical token-level
statistics on the prevalence of constructions found in bilingual
dictionaries. Unfortunately, this would require much of the ef-
fort of building an MT system on a construction basis itself.
90
Translation When said of
sag-ta-dug4-ga the head
du7 oxen
ru5 rams
si-tu10 oxen/bulls
kur-ku a flood
ru-gu? a finger
si-ga a garment
3 Synchronous Grammars Reviewed
To summarize, the translation relation in evidence
implicitly in bilingual dictionaries requires a for-
malism expressive enough to directly represent re-
lations between constructions, appropriately linked,
and to do so in a way that allows these constructions
to be realized noncontiguously by virtue of vari-
ability and intervention. As we will show, the for-
mer requirement is exactly the idea underlying syn-
chronous grammars. The latter requirement of non-
contiguity in its two aspects further implicates oper-
ations of substitution and adjunction (respectively)
to combine constructions. The requirements lead
naturally to a consideration of synchronous tree-
adjoining grammar as the direct embodiment of the
bilingual dictionaries of the last four millennia.
A synchronous grammar formalism is built by
synchronizing grammars from some base formal-
ism. A grammar in the base formalism consists
of a set of elementary tree structures along with
one or more combining operations. All of the fa-
miliar monolingual formalisms?finite-state gram-
mars, context-free grammars, tree-substitution and
-adjoining grammars, categorial grammars, inter
alia?can be thought of in this way. A synchronous
grammar consists of a set of pairs of elementary
trees from the base formalism together with a link-
ing relation between nodes in the trees at which
combining operations can perform. Derivation pro-
ceeds as in the base formalism, whatever that is, ex-
cept that a pair of trees operate at a pair of linked
nodes in an elementary tree pair. An operation per-
formed at one end of a link must be matched by a
corresponding operation at the other end of the link.
For example, the tree pair in Figure 1 might be ap-
propriate for use in translating the sentence Eli took
his father by surprise. The links between the NP
nodes play the same role as the linked variables SB
and QN in the bilingual dictionary entry. They allow
for substitution of tree pairs for Eli and its translation
and his father and its. The additional links allow for
further modification, as in Eli recently took his fa-
ther by surprise by preparing dinner, the modifiers
recently and by preparing dinner adjoining at the VP
and S links, respectively.
Expressing this relation in other frameworks in-
volves either limiting its scope (for instance, to par-
ticular objects and intervening material), expanding
its scope (by separating the translations of the con-
tiguous portions of the constructions), or mimicking
the structure of the STAG (as described at the end of
Section 5).
The basic idea of using synchronous TAG for ma-
chine translation dates from the original definition
(Shieber and Schabes, 1990), and has been pur-
sued by several researchers (Abeille et al, 1990;
Dras, 1999; Prigent, 1994; Palmer et al, 1999), but
only recently in its probabilistic form (Nesson et al,
2006). The directness with which the formalism fol-
lows from the structure of bilingual dictionaries has
not to our knowledge been previously noted. It leads
to the possibility of making direct use of bilingual
dictionary material in a statistical machine transla-
tion system.5 But even if the formalism is not used
in that way, there is import to the fact that its expres-
sivity matches that thought by lexicographers of the
last several millennia to be needed for capturing the
translation relation; this fact indicates at least that
STAG?s use as a substrate for MT systems may be a
promising research direction to pursue, should other
necessary properties be satisfiable as well. We turn
next to two of these properties: trainability and effi-
ciency.
4 Trainability
The mere ability to formally represent the contents
of manually developed bilingual dictionaries is not
sufficient for the building of robust machine trans-
lation systems. The last decade and a half of MT
research has demonstrated the importance of train-
ability of the models based on statistical evidence
found in corpora. Without such training, manually
5For construction-based MT, reconstruction of tree align-
ments from data is much more difficult than for phrase-based
MT, and hence extracting them from a dictionary becomes
much more appealing.
91
SNP VP
V NP
took
by surprise
PP
P NP
S
NP VP
V NP
a colto
di sorpresa
PP
P NP
Figure 1: A synchronous tree pair.
developed models are too brittle to be seriously con-
sidered as a basis for machine translation.
It may also be the case that with such training, the
manually generated materials are redundant. Cer-
tainly, it has been difficult to show the utility of man-
ually generated annotations in improving MT per-
formance. But this may be because the means by
which the materials are represented is not yet appro-
priate; it does not articulate well with the statistical
substrate used by the training methodology.
A further property, then, for the formalism is that
it be trainable based on bilingual corpora. Consider
training of the sort that underlies the IBM-style word
models and their phrase-based offshoots, or statisti-
cal parsing based on probabilistic CFGs (Lari and
Young, 1990) or other generative formalisms. Such
methods use an underlying probabilistic formalism,
typically structuring the parameters based on a uni-
versal parametric normal form (as n-gram proba-
bilities are for finite-state grammars and Chomsky-
normal form is for PCFGs), and applying an efficient
training algorithm to set values for the parameters.
A full system based on STAG would use the for-
malism to express both the detailed bilingual con-
structional relationships as found in a bilingual dic-
tionary and a backbone in the form of the uni-
versal normal form. Trained together, the normal
form would serve to smooth the brittle construction-
specific part, while the construction-specific part
would relieve the burden on the universal learned
portion to allocate parameters to rare constructions.
How do synchronous tree-adjoining grammars
fare in this area? Do they admit of the kind of uni-
versal normal-form training that might serve as a
smoothing method for the more highly articulated
but brittle lexicographic relation?
A probabilistic variant of synchronous TAG is
straightforward to specify, given that the formal-
ism itself has a natural generative interpretation
(Shieber, 1994). A universal parametric normal
form has been provided by Nesson et al (2006)
(see Figure 2), who show that, at least on small
training sets, a synchronous TAG in this normal
form performs at a level comparable to standard
word- and phrase-based systems. Synchronous
TAGs thus seem to have the best of both worlds:
They can directly express the types of ramified bilin-
gual constructions as codified in bilingual dictionar-
ies, and they can also express the types of universal
assumption-free normal forms that underlie modern
statistical MT. Importantly, they can do so at one
and the same time, as both types of information are
expressed in the same way, as sets of tree pairs. Both
can therefore be trained together based on bilingual
corpora.
We emphasize that the advantage that we find for
STAGs in displaying well the necessary properties
for statistical machine translation systems implicit in
bilingual dictionaries is not that they are able to code
efficiently all generalizations about the translation
relation. Indeed, STAG is not able to do so (Shieber,
1994), which has motivated more expressive exten-
92
XX
X
w Tw S
!
X
X
X
X
X
X
w Tw S
X
X
X
X
X
X
w S w T
X
X
X
S
X
!
S
X
X? X? X?X? X? X?
!
S
X
!
S
X !
S
X
!
S
X
Figure 2: A normal form for synchronous tree-insertion grammar. (Reproduced from Nesson et al (2006).)
sions of the formalism (Chiang et al, 2000). For
example, STAG might express the construction rela-
tion ?attraversare QC di corsa / run across ST? and
similar relations between Italian verbs of direction
with modifiers of motion and English verbs of mo-
tion with directional modifiers. However, the gener-
alization that directional verbs with motion-manner
adverbials translate as motion-manner verbs with di-
rectional adverbials is not expressed or expressible
by STAG. Each instance of the generalization must
be specified or learned separately.6 Nonetheless, we
are content (in the spirit of statistical MT) to have
lots of such particular cases missing a generaliza-
tion, so long as the parts from which they are con-
structed are pertinent, that is, so long as we do not
need to specify ?attraversare la strada di corsa / run
across the road?51 separately from all of the other
things one might run across.
5 Efficiency
A final set of considerations has to do with the effi-
ciency of the formalism. Is it practical to use STAG
for the purposes we have outlined? It is important
not to preclude a formalism merely based on im-
practicality of its current use (given the constant in-
creases in computer speed), but inherent intractabil-
ity is another matter.7
6Palmer et al (1999) provide an approach to STAG that at-
tempts to address this particular problem as does the extension
of Dras (1999). It is unclear to what extent such extensions are
amenable to trainable probabilistic variants.
7Of course, too much might be made of this question of
computational complexity. The algorithms used for decoding
of statistical MT systems almost universally incorporate heuris-
tics for efficiency reasons, even those that are polynomial. One
reviewer notes that ?the admittedly perplexing reality is that ex-
ponential decoders run much faster than polynomial ones, pre-
Here, the STAG situation is equivocal. Bilingual
parsing of a corpus relative to an STAG is a nec-
essary first step in parameter training. The recog-
nition problem for STAG, like that for synchronous
context-free grammar (SCFG) is NP-hard (Satta and
Peserico, 2005). Under appropriate restrictions of
binarizability, SCFG parsing can be done in O(n6)
time, doubling the exponent of CFG parsing. Simi-
larly, STAG parsing under suitable limitations (Nes-
son et al (2005)) can be done in O(n12) time dou-
bling the exponent of monolingual TAG parsing. On
the positive side, recent work exploring the auto-
matic binarization of synchronous grammars (Zhang
et al, 2006) has indicated that non-binarizable con-
structions seem to be relatively rare in practice.
Nonetheless, such a high-degree polynomial makes
the complete algorithm impractical.
Nesson et al (2006) use synchronous tree-
insertion grammar (STIG) (Schabes and Waters,
1995) rather than STAG for this very reason.
STIG retains the ability to express a universal nor-
mal form, while allowing O(n6) bilingual parsing.
(Again, limitations on the formalism are required to
achieve this complexity.) Even this complexity may
be too high. Methods such as those of Chiang (2005)
have been proposed for further reducing the com-
plexity of SCFG parsing; they may be applicable to
STIG (and STAG) parsing as well.
The STIG formalism can be shown to be expres-
sively equivalent to synchronous tree-substitution
grammar (STSG) and even SCFG. Does this viti-
ate the argument for STIG as a natural formalism
for MT? No. The reductions of STIG to these other
formalisms operate by introducing additional nodes
sumably because they prune more intelligently.?
93
in the elementary trees that extend the size of those
trees and hence the complexity of their parsing, un-
less subtle tricks are used to take advantage of the
special structure of these added nodes. These tricks
essentially amount to treating the formalism as an
STIG, not an SCFG. That is, even if an SCFG were
to be used, its structure would best be built on the
observations found here.
For example, the method of Cowan et al (2006)
synchronizes elementary trees of a prescribed form
to handle translation of clauses (verbs plus their ar-
guments) essentially implementing a kind of STSG.
However, because modifiers can make these trees
discontiguous, they augment the model by allowing
for free insertion of modifiers in certain locations.
One view of this is as an implementation of the prin-
ciple that motivates adjoining, without using adjoin-
ing itself. Thus, systems that are designed to take
account of the principles adduced in this paper are
likely to be implementing aspects of STAG implic-
itly, even if not explicitly.
Similarly, recent research is beginning to unify
synchronous grammar formalisms and tree trans-
ducers (Shieber, 2004; Shieber, 2006). There may
well be equally direct transducer formalisms that el-
egantly express construction-based translation rela-
tions. This would not be a denial of the present the-
sis but a happy acknowledgment of it.
6 Conclusion
We have argued that probabilistic synchronous TAG
or some closely related formalism possesses a con-
stellation of properties?expressivity, trainability,
and efficiency?that make it a good candidate at
a conceptual level for founding a machine transla-
tion system. What would such a system look like?
It would start with a universal normal form sub-
grammar serving as the robust ?backoff? relation to
which additional more articulated bilingual material
could be added in the form of additional tree pairs.
These tree pairs might be manually generated, au-
tomatically reconstructed from repurposed bilingual
dictionaries, or automatically induced from aligned
bilingual treebanks (Groves et al, 2004; Groves and
Way, 2005) or even unannotated bilingual corpora
(Chiang, 2005). In fact, since all of these sources
of data yield interacting tree pairs, more than one of
these techniques might be used. In any case, further
training would automatically determine the interac-
tions of these information sources.
The conclusions of this paper are admittedly pro-
grammatic. But plausible arguments for a program
of research may be just the thing for clarifying a re-
search direction and even promoting its pursual. In
that sense, this paper can be read as a kind of man-
ifesto for the use of probabilistic synchronous TAG
as a substrate for MT research.
Acknowledgments
We thank Rani Nelken, Rebecca Nesson, and
Alexander Rush for helpful discussion and the
anonymous reviewers for their insightful comments.
This work was supported in part by grant IIS-
0329089 from the National Science Foundation.
References
Anne Abeille, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tags for machine translation.
In Proceedings of the 13th International Conference
on Computational Linguistics.
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
David Chiang, William Schuler, and Mark Dras. 2000.
Some remarks on an extension of synchronous TAG.
In Proceedings of the 5th International Workshop on
Tree Adjoining Grammars and Related Formalisms
(TAG+5), Paris, France, 25?27 May.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Miguel Civil. 1995. Ancient Mesopotamian lexicogra-
phy. In Jack M. Sasson, editor, Civilizations of the An-
cient Near East, volume 4, pages 2305?14. Scribners,
New York.
94
Michela Clari and Catherine E. Love, editors. 1995.
HarperCollins Italian College Dictionary. Harper-
Collins Publishers, Inc., New York, NY.
Brooke Cowan, Ivona Kucerov, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of EMNLP 2006.
Mark Dras. 1999. A meta-level grammar: Redefining
synchronous TAG for translation and paraphrase. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 80?87,
Morristown, NJ, USA. Association for Computational
Linguistics.
Declan Groves and Andy Way. 2005. Hybrid example-
based SMT: the best of both worlds? In Workshop on
Building and Using Parallel Texts: Data-Driven Ma-
chine Translation and Beyond, Ann Arbor, MI, June.
ACL ?05.
Declan Groves, Mary Hearne, and Andy Way. 2004. Ro-
bust sub-sentential alignment of phrase-structure trees.
In COLING ?04, Geneva Switzerland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Karim Lari and Steve J. Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
Rebecca Nesson, Alexander Rush, and Stuart M. Shieber.
2005. Induction of probabilistic synchronous tree-
insertion grammars. Technical Report TR-20-05, Di-
vision of Engineering and Applied Sciences, Harvard
University, Cambridge, MA.
Rebecca Nesson, Stuart M. Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Conference of the Association for
Machine Translation in the Americas (AMTA 2006),
Boston, Massachusetts, 8-12 August.
Franz Josef Och. 2003. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, Technical University of Aachen,
Aachen, Germany.
Martha Palmer, Joseph Rosenzweig, and William
Schuler. 1999. Capturing motion verb generalizations
in synchronous tree-adjoining grammar. In Patrick
Saint-Dizier, editor, Predicative Forms in Natural Lan-
guage and in Lexical Knowledge Bases. Kluwer Press.
Gilles Prigent. 1994. Synchronous TAGs and machine
translation. In Proceedings of the Third International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+3), Universite? Paris 7.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of the Conference on
Human Language Technology and Empirical Methods
in Natural Language Processing (HLT/EMNLP 05),
pages 803?810, Morristown, NJ, USA. Association for
Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: A cubic time, parsable formalism
that lexicalizes context-free grammars without chang-
ing the trees produced. Computational Linguistics,
21(3):479?512.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics, volume 3, pages 253?258, Helsinki, Finland.
Stuart M. Shieber. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385, Novem-
ber. Also available as cmp-lg/9404003.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms (TAG+ 7), Vancouver, Canada, May
20-22.
Stuart M. Shieber. 2006. Unifying synchronous tree-
adjoining grammars and tree transducers via bimor-
phisms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL-06), Trento, Italy, 3?7 April.
Warren Weaver. 1955. Translation. In W.N. Locke
and A. D. Booth, editors, Machine Translation of Lan-
guages: Fourteen Essays, pages 15?23. Technology
Press of the Massachusetts Institute of Technology,
Cambridge, Massachusetts.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Conference on Hu-
man Language Technology and Annual Meeting of the
North American Chapter of the Association of Compu-
tational Linguistics (HLT/NAACL 2006), pages 256?
263, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
95
Proceedings of the 10th Conference on Parsing Technologies, page 93,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Synchronous Grammars and Transducers:
Good News and Bad News
Stuart M. Shieber
School of Engineering and Applied Sciences
Harvard University
Cambridge MA 02138
USA
shieber@seas.harvard.edu
Much of the activity in linguistics, especially
computational linguistics, can be thought of as char-
acterizing not languages simpliciter but relations
among languages. Formal systems for characteriz-
ing language relations have a long history with two
primary branches, based respectively on tree trans-
ducers and synchronous grammars. Both have seen
increasing use in recent work, especially in machine
translation. Indeed, evidence from millennia of ex-
perience with bilingual dictionaries argues for syn-
chronous grammars as an appropriate substrate for
statistical machine translation systems.
On the positive side, some new results have
integrated the two branches through the formal-
language-theoretic construct of the bimorphism. I
will present some background on this integration,
and briefly describe two applications of synchronous
grammars: to tree-adjoining grammar semantics and
to syntax-aware statistical machine translation.
On the negative side, algorithms for making use of
these formalisms are computationally complex, per-
haps prohibitively so. I will close with a plea for
novel research by the parsing technology commu-
nity in making the systems practical.
93
Complexity, Parsing, and Factorization
of Tree-Local Multi-Component
Tree-Adjoining Grammar
Rebecca Nesson?
School of Engineering and Applied
Sciences,
Harvard University
Giorgio Satta??
Department of Information Engineering
University of Padua
Stuart M. Shieber?
School of Engineering and Applied
Sciences
Harvard University
Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing formal-
ism for natural language representation because it arguably allows the encapsulation of the
appropriate domain of locality within its elementary structures. Its multicomponent structure
allows modeling of lexical items that may ultimately have elements far apart in a sentence, such
as quantifiers and wh-words. When used as the base formalism for a synchronous grammar, its
flexibility allows it to express both the close relationships and the divergent structure necessary
to capture the links between the syntax and semantics of a single language or the syntax of
two different languages. Its limited expressivity provides constraints on movement and, we
posit, may have generated additional popularity based on a misconception about its parsing
complexity.
Although TL-MCTAG was shown to be equivalent in expressivity to TAG when it was
first introduced, the complexity of TL-MCTAG is still not well understood. This article offers
a thorough examination of the problem of TL-MCTAG recognition, showing that even highly
restricted forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable
difficulty of the recognition problem, we offer several algorithms that can substantially improve
processing efficiency. First, we present a parsing algorithm that improves on the baseline parsing
? School of Engineering and Applied Sciences, Harvard University, 38 Plymouth St., Cambridge, MA
02141. E-mail: nesson@seas.harvard.edu.
?? Department of Information Engineering, University of Padua, via Gradenigo 6/A, 1-35131 Padova, Italy.
E-mail: satta@dei.unipd.it.
? School of Engineering and Applied Sciences, Harvard University, Maxwell Dworkin Laboratory,
33 Oxford Street, Cambridge, MA 02138. E-mail: shieber@seas.harvard.edu.
Submission received: 4 November 2008; revised submission received: 13 November 2009; accepted for
publication: 18 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
method and runs in polynomial time when both the fan-out and rank of the input grammar are
bounded. Second, we offer an optimal, efficient algorithm for factorizing a grammar to produce a
strongly equivalent TL-MCTAG grammar with the rank of the grammar minimized.
1. Introduction
Tree-Local Multi-Component Tree-Adjoining Grammar (TL-MCTAG) is an appealing
formalism for natural language representation because it arguably allows the encapsu-
lation of the appropriate domain of locality within its elementary structures (Kallmeyer
and Romero 2007). Its flexible multicomponent structure allows modeling of lexical
items that may ultimately have elements far apart in a sentence, such as quantifiers and
wh-words. Its limited expressivity provides constraints on movement and, we posit,
may have generated additional popularity based on a misconception about its parsing
complexity.
TL-MCTAG can model highly structurally divergent but closely related elementary
structures, such as the syntax and the semantics of a single word or construction or the
syntax of a single word or construction and its translation into another language, with a
pair of elementary trees. This flexibility permits conceptually simple, highly expressive,
and tightly coupled modeling of the relationship between the syntax and semantics of
a language or the syntax and semantics of two languages. As a result, it has frequently
been put to use in a growing body of research into incorporating semantics into the Tree-
Adjoining Grammar (TAG) framework (Kallmeyer and Joshi 2003; Han 2006; Nesson
and Shieber 2006, 2007). It is also under investigation as a possible base formalism
for use in synchronous-grammar based machine translations systems (Nesson 2009).
Similar pairing of elementary structures of the TAG formalism is too constrained to
capture the inherent divergence in structure between different languages or even be-
tween the syntax and semantics of a language. Pairing of more expressive formalisms is
too flexible to provide appropriate constraints and has unacceptable consequences for
processing efficiency.
Although TL-MCTAG was first introduced by Weir (1988) and shown at that time
to be equivalent in expressivity to TAG, the complexity of TL-MCTAG is still not
well understood. Perhaps because of its equivalence to TAG, questions of processing
efficiency have not been adequately addressed. This article offers a thorough exami-
nation of the problem of TL-MCTAG recognition, showing that even highly restricted
forms of TL-MCTAG are NP-complete to recognize. However, in spite of the provable
difficulty of the recognition problem, we offer several algorithms that can substantially
improve processing efficiency. First, we present a parsing algorithm that improves
on the baseline parsing method and runs in polynomial time when both the fan-out
(the maximum number of trees in a tree set) and rank (the maximum number of
trees that may be substituted or adjoined into a given tree) of the input grammar are
bounded. Second, we offer an optimal, efficient algorithm for factorizing a grammar
to produce a strongly equivalent TL-MCTAG grammar with the rank of the grammar
minimized.
1.1 Summary of Results
TAG is a mildly context-sensitive grammar formalism widely used in natural language
processing. Multicomponent TAG (MCTAG) refers to a group of formalisms that
444
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
generalize TAG by allowing elementary structures to be sets of TAG trees. One member
of the MCTAG formalism group is Tree-Local MCTAG (TL-MCTAG), in which all
trees from a single elementary tree set are constrained to adjoin or substitute into a
single tree in another elementary tree set. Weir (1988) shows that this constraint is
sufficient to guarantee that TL-MCTAG has weak generative capacity equivalent to the
polynomially parsable TAG.
Recent work on the complexity of several TAG variants has demonstrated indirectly
that the universal recognition problem for TL-MCTAG is NP-hard. This result calls
into question the practicality of systems that employ TL-MCTAG as the formalism for
expressing a natural language grammar. In this article we present a more fine-grained
analysis of the processing complexity of TL-MCTAG. We demonstrate (Section 3) that
even under restricted definitions where either the rank or the fan-out of the grammar is
bounded, the universal recognition problem is NP-complete.
We define a novel variant of multi-component TAG formalisms that treats the
elementary structures as vectors of trees rather than as unordered sets (Section 4). We
demonstrate that this variant of the definition of the formalism (the vector definition) is
consistent with the linguistic applications of the formalism presented in the literature.
Universal recognition of the vector definition of TL-MCTAG is NP-complete when
both the rank and fan-out are unbounded. However, when the rank is bounded, the
universal recognition problem is polynomial in both the length of the input string and
the grammar size.
We present a novel parsing algorithm for TL-MCTAG (Section 5) that accommo-
dates both the set and vector definitions of TL-MCTAG. Although no algorithms for
parsing TL-MCTAG have previously been published, the standard method for parsing
linear context-free rewriting systems (LCFRS)?equivalent formalisms can be applied
directly to TL-MCTAG to produce a quite inefficient baseline algorithm in which the
polynomial degree of the length of the input string depends on the input grammar.
We offer an alternative parser for TL-MCTAG in which the polynomial degree of the
length of the input string is constant, though the polynomial degree of the grammar size
depends on the input grammar. This alternative parsing algorithm is more appealing
than the baseline algorithm because it performs universal recognition of TL-MCTAG
(vector definition) with constant polynomial degree in both the length of the input string
and the grammar size when rank is bounded.
It may not be generally desirable to impose an arbitrary rank bound on TL-MCTAGs
to be used for linguistic applications. However, it is possible given a TL-MCTAG to
minimize the rank of the grammar. In the penultimate section of the paper (Section 6)
we offer a novel and efficient algorithm for transforming an arbitrary TL-MCTAG into
a strongly equivalent TL-MCTAG where the rank is minimized.
1.2 Related Work
Our work on TL-MCTAG complexity bears comparison to that of several others.
Kallmeyer (2009) provides a clear and insightful breakdown of the different charac-
teristics of MCTAG variants and the effect of these characteristics on expressivity and
complexity. That work clarifies the definitions of MCTAG variants and the relationship
between them rather than presenting new complexity results. However, it suggests
the possibility of proving results such as ours in its assertion that, after a standard
TAG parse, a check of whether particular trees belong to the same tree set cannot
be performed in polynomial time. Kallmeyer also addresses the problem of parsing
445
Computational Linguistics Volume 36, Number 3
MCTAG, although not specifically for TL-MCTAG. The method proposed differs from
ours in that MCTAGs are parsed first as a standard TAG, with any conditions on tree
or set locality checked on the derivation forest as a second step. No specific algorithm
is presented for performing the check of tree-locality on a TAG derivation forest, so it is
difficult to directly compare the methods. However, that method cannot take advantage
of the gains in efficiency produced by discarding inappropriate partial parses at the
time that they are first considered. Aside from Kallmeyer?s work, little attention has
been paid to the problem of directly parsing TL-MCTAG.
S?gaard, Lichte, and Maier (2007) present several proofs regarding the complexity
of the recognition problem for some linguistically motivated extensions of TAG that are
similar to TL-MCTAG. Their work shows the NP-hardness of the recognition problem
for these variants and, as an indirect result, also demonstrates the NP-hardness of TL-
MCTAG recognition. This work differs from ours in that it does not directly show the
NP-hardness of TL-MCTAG recognition and does not further locate and constrain the
source of the NP-hardness of the problem to the rank of the input grammar, nor does it
provide mitigation through rank reduction of the grammar or by other means.
Our work on TL-MCTAG factorization is thematically though not formally related
to the body of work on induction of TAGs from a treebank exemplified by Chen and
Shanker (2004). The factorization performed in their work is done on the basis of syn-
tactic constraints rather than with the goal of reducing complexity. Working from a
treebank of actual natural language sentences, their work does not have the benefit of
explicitly labeled adjunction sites but rather must attempt to reconstruct a derivation
from complete derived trees.
The factorization problem we address is more closely related to work on factorizing
synchronous context-free grammars (CFGs) (Gildea, Satta, and Zhang 2006; Zhang and
Gildea 2007) and on factorizing synchronous TAGs (Nesson, Satta, and Shieber 2008).
Synchronous grammars are a special case of multicomponent grammars, so the prob-
lems are quite similar to the TL-MCTAG factorization problem. However, synchronous
grammars are fundamentally set-local rather than tree-local formalisms, which in some
cases simplifies their analysis. In the case of CFGs, the problem reduces to one of
identifying problematic permutations of non-terminals (Zhang and Gildea 2007) and
can be done efficiently by using a sorting algorithm to binarize any non-problematic per-
mutations until only the intractable correspondences remain (Gildea, Satta, and Zhang
2006). This method is unavailable in the TAG case because the elementary structures
may have depth greater than one and therefore the concept of adjacency relied upon
in their work is inapplicable. The factorization algorithm of Nesson, Satta, and Shieber
(2008) is the most closely related to this one but is not directly applicable to TL-MCTAG
because each link is presumed to have exactly two locations and all adjunctions occur
in a set-local rather than tree-local manner.
2. Technical Background
A tree-adjoining grammar consists of a set of elementary tree structures of arbitrary
depth, which are combined by the operations of adjunction and substitution. Auxiliary
trees are elementary trees in which the root and a frontier node, called the foot node
and distinguished by the diacritic ?, are labeled with the same nonterminal A. The
adjunction operation entails splicing in an auxiliary tree in an internal node within
an elementary tree also labeled with nonterminal A. Trees without a foot node, which
serve as a base case for derivations and may combine with other trees by substitution,
446
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
are called initial trees. Examples of the adjunction and substitution operations are
given in Figure 1. For further background, we refer the reader to the survey by Joshi
and Schabes (1997).
A TAG derivation can be fully specified by a derivation tree, which records how
the elementary structures are combined using the TAG operations to form the derived
tree. The nodes of the derivation tree are labeled by the names of the elementary trees
and the edges are labeled by the addresses at which the child trees substitute or adjoin.
In contrast to CFGs, the derivation and derived trees are distinct.
We depart from the traditional definition in notation only by specifying adjunc-
tion sites explicitly with numbered links in order to simplify the presentation of the
issues raised by multi-component adjunctions. Each link may be used only once in a
derivation. Adjunctions may only occur at nodes marked with a link. A numbered link
at a single site in a tree specifies that a single adjunction is available at that site. An
obligatory adjunction constraint indicates that at least one link at a given node must
be used (Joshi, Levy, and Takahashi, 1975; Vijay-Shanker and Joshi 1985). We notate
obligatory adjunction constraints by underlining the label of the node to which the
constraint applies. Because we use explicit links, the edges in the derivation tree are
labeled with the number of the link used rather than the traditional label of the address
at which the operation takes place.
Multiple adjunction refers to permitting an unbounded number of adjunctions to
occur at a single adjunction site (Vijay-Shanker 1987; Shieber and Schabes 1994). In
the standard definition of TAG, multiple adjunction is disallowed to ensure that each
derivation tree unambiguously specifies a single derived tree (Vijay-Shanker 1987). Be-
cause each available adjunction is explicitly notated with a numbered link, our notation
implicitly disallows multiple adjunction but permits a third possibility: bounded mul-
tiple adjunction. Bounded multiple adjunction permits the formalism to obtain some
of the potential linguistic advantages of allowing multiple adjunction while preventing
unbounded multiple adjunction. The usual constraint of allowing only one adjunction
at a given adjunction site may be enforced in our link notation by permitting only one
link at a particular link site to be used.
MCTAG generalizes TAG by allowing the elementary items to be sets of trees rather
than single trees (Joshi and Schabes 1997). The basic operations are the same but all trees
in a set must adjoin (or substitute) into another tree set in a single step in the derivation.
To allow for multi-component adjunction, a numbered link may appear on two or more
nodes in a tree, signifying that the adjoining trees must be members of the same tree
set. Any tree in a set may adjoin at any link location if it meets other adjunction or
substitution conditions such as a matching node label. Thus a single multicomponent
Figure 1
An example of TAG operations substitution and adjunction used here to model natural
language syntax.
447
Computational Linguistics Volume 36, Number 3
Figure 2
An example of the way in which two tree sets may produce several different derived trees when
combined under the standard definition of multicomponent TAG.
link may give rise to many distinct derived trees even when the link is always used
by the same multicomponent tree set. An example is given in Figure 2. This standard
definition of multicomponent adjunction we will call the set definition for contrast with
a variation we introduce in Section 4. A derivation tree for a multicomponent TAG is
the same as for TAG except that the nodes are labeled with the names of elementary
tree sets.
An MCTAG is tree-local if tree sets are required to adjoin within a single elementary
tree (Weir 1988). Using the numbered link notation introduced earlier for adjunction
sites, a tree-local MCTAG (TL-MCTAG) is one in which the scope of the link numbers
is a single elementary tree. An example TL-MCTAG operation is given in Figure 3. In
contrast, an MCTAG is set-local if the trees from a single tree set are required to adjoin
within a single elementary tree set and an MCTAG is non-local if the trees from a single
tree set may adjoin to trees that are not within a single tree set. In a set-local MCTAG
the scope of a link is a single elementary tree set, and in a non-local MCTAG the scope
of a link is the entire grammar.
Weir (1988) noted in passing that TL-MCTAG has generative capacity equivalent to
TAG; a combination of well-chosen additional constraints and additions of duplicates
of trees to the grammar can produce a weakly equivalent TAG. Alternatively, a feature-
based TAG where the features enforce the same constraints may be used. Although the
generative capacity of the formalism is not increased, any such conversion from TL-
MCTAG to TAG may require an exponential increase in the size of the grammar as we
prove in Section 3.
3. Complexity
We present several complexity results for TL-MCTAG. S?gaard, Lichte, and Maier (2007)
show indirectly that TL-MCTAG membership is NP-hard. For clarity, we present a direct
Figure 3
An example TL-MCTAG operation demonstrating the use of TL-MCTAG to model wh-question
syntax.
448
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
proof here. We then present several novel results demonstrating that the hardness result
holds under significant restrictions of the formalism.
For a TL-MCTAG G we write |G| to denote the size of G, defined as the total number
of nodes appearing in all elementary trees in the tree sets of the grammar. Fan-out, f ,
measures the number of trees in the largest tree set in the grammar. We show that even
when the fan-out is bounded to a maximum of two, the NP-hardness result still holds.
The rank, r, of a grammar is the maximum number of derivational children possible
for any tree in the grammar, or in other words, the maximum number of links in any
tree in the grammar. We show that when rank is bounded, the NP-hardness result also
holds.
A notable aspect of all of the proofs given here is that they do not make use of
the additional expressive power provided by the adjunction operation of TAG. Put
simply, the trees in the tree sets used in our constructions meet the constraints of Tree
Insertion Grammar (TIG), a known context-free?equivalent formalism (Schabes and
Waters 1995). As a result, we can conclude that the increase in complexity stems from
the multi-component nature of the formalism rather than from the power added by an
unconstrained adjunction operation.
3.1 Universal Recognition of TL-MCTAG is NP-Complete
In this section we prove that universal recognition of TL-MCTAG is NP-complete when
neither the rank nor the fan-out of the grammar is bounded.
Recall the 3SAT decision problem, which is known to be NP-complete. Let V =
{v1, . . . , vp} be a set of variables and C = {c1, . . . , cn} be a set of clauses. Each clause in C
is a disjunction of three literals over the alphabet of all literals LV = {v1, v1, . . . , vp, vp}.
We represent each clause by a set of three literals. The language 3SAT is defined as the
set of all conjunctive formulas over the members of C that are satisfiable.
Theorem 1
The universal recognition problem for TL-MCTAG with unbounded rank and fan-out
is NP-hard.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem.1 We use the derivations of the
grammar to guess the truth assignments for V and use the tree sets to keep track of the
dependencies among different clauses in C. Two tree sets are constructed for each vari-
able, one corresponding to an assignment of true to the variable and one corresponding
to an assignment of false. The links in the single initial tree permit only one of these two
sets to be used. The tree set for a particular truth assignment for a particular variable vi
makes it possible to introduce, by means of another adjunction, terminal symbols taken
from the set {1, . . . , n} that correspond to each clause in C that would be satisfied by the
given assignment to vi. In this way, the string w = 1 ? ? ? n can be generated if and only if
all clauses are satisfied by the truth assignment to some variable they contain.
1 We follow the proof strategy of Satta and Peserico (2005) in this and the proof of Theorem 3.
449
Computational Linguistics Volume 36, Number 3
We define a tree-local MCTAG G containing the following tree sets. The initial tree
set S contains the single tree:
In this tree, the ?rows? correspond to the variables and the ?columns? to the clauses.
Each non-terminal node within a row is labeled with the same link to ensure that a tree
set representing a single variable?s effect on each clause will adjoin at each link.
For every variable vi, 1 ? i ? p, tree set Ti, used when representing an assignment of
the value true to vi, contains n trees, one for each clause cj, 1 ? j ? n, defined as follows:
For every variable vi, 1 ? i ? p, tree set Fi ? used when representing an assignment
of the value false to vi ? contains n trees, one for each clause cj, 1 ? j ? n, defined as
follows:
For every clause cj, 1 ? j ? n, tree set Cj contains a single tree as shown here. This
tree allows the corresponding clause number terminal symbol to be recognized by an
appropriate variable instance.2
2 Note that because adjunction is not obligatory, the tree from Cj need not adjoin into the tree for a
particular variable. In fact, to generate w, exactly one instance of Cj must adjoin for each clause even if
more than one variable satisfies the clause. If w can be generated, however, we can conclude that at least
one variable must have satisfied each clause.
450
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
From the definition of G it directly follows that w ? L(G) implies the existence of a
truth assignment that satisfies C. A satisfying truth assignment can be read directly off
of any derivation tree for w. If Ti (respectively, Fi) is a child of S in the derivation tree,
then vk is true (respectively, false). The converse can be shown by using a satisfying
truth assignment for C to construct a derivation for w ? L(G).
?G, w? can be constructed in deterministic polynomial time because the number of
tree sets in the grammar is 2p + 2n + 1, the total number of trees in the grammar is
bounded by n(2p + 2n + 1), and the length of w is n. All trees in the grammar have
constant size except for the initial tree, which has size np. 
Theorem 2
The universal recognition problem for TL-MCTAG with unbounded rank and fan-out
is in NP.
Proof
We show that given an arbitrary TL-MCTAG grammar G and any input string w, the
determination of w ? L(G) can be performed in non-deterministic polynomial time.
Note that the collection of elementary tree sets of G that can generate the empty
string, E , can be generated in time polynomial in |G| using the standard graph reach-
ability algorithm used for context-free grammars in time polynomial in |G| (Sippu and
Soisalon-Soininen 1988).
We begin by showing that given an arbitrary input string w and derivation tree D
for w ? L(G), there must exist a truncated derivation tree for w that has size no larger
than |G| ? |w|. We define a truncated derivation tree as a derivation tree in which the
children of elementary tree sets in E are optionally removed.
Consider D. Each node in D represents an elementary structure of G: a tuple of
one or more TAG trees. We call a node n of D a non-splitting node if a single one
of its children in the derivation tree, ni, generates the same lexical material from the
input string as n itself.3 We call it a splitting node if more than one of its children
generates a non-empty part of the portion of the input string generated by n itself
or if n itself contributes lexical material. We proceed from the root of D examining
chains of non-splitting nodes. Assume that the root of D is a non-splitting node. This
means that it has a single child node, ni, that generates the lexical material for the entire
input string. Its other children all generate the empty string (and therefore must also be
members of E). We truncate the derivation tree at each child of n other than ni. We now
iterate the process on node ni. If during the examination of a chain of non-splitting
nodes we encounter a node identical to one that we have already seen, we remove
the entire cycle from the derivation tree because it is not essential to the derivation.
Because all cycles are removed, the longest possible chain of non-splitting nodes we
can find before encountering a splitting node or reaching the bottom of the derivation
tree is |G|.
If a splitting node is encountered, we truncate all child nodes that generate the
empty string and then iterate the process of non-splitting node identification on those
3 The child tree tuple ni may generate the same lexical material in several distinct pieces, which are
arranged into the string generated by n when the adjunction occurs. Because the adjunction necessarily
connects all of these pieces into a single string in a single predetermined way, it does not matter for our
proof that the lexical material derived by the child may be in any order before the adjunctions.
451
Computational Linguistics Volume 36, Number 3
children that generate lexical material. In the worst case, the process encounters w ? 1
splitting nodes, each of which may be separated by a chain of non-splitting nodes
of maximum length bounded by |G|. This process, therefore, produces a truncated
derivation tree with size bounded by |G| ? |w|.
The truncation of the tree at each node that generates the empty string is necessary
because the size of the subderivation tree generating the empty string may not be
bounded by a polynomial in the size of the grammar. However, the content of the part
of the derivation tree used to generate the empty string is not necessary for determining
membership of w ? L(G) because we know that each truncated node is a member of E .
To show that TL-MCTAG membership is in NP, we construct a Turing machine
that will non-deterministically guess a truncated derivation tree of size no larger than
|G| ? |w|. It then checks that the guessed derivation successfully derives w. Because the
correctness of the derivation can be checked in linear time, this is sufficient to show that
TL-MCTAG membership is in NP. 
We know from the equivalence of LCFRS and SL-MCTAG (and the rule-to-tree-
tuple conversion method used to prove equivalency) (Weir 1988) and the fact that
LCFRS membership is PSPACE-complete that SL-MCTAG membership is also PSPACE-
complete (Kaji et al 1992, 1994). Until the results shown in Theorems 1 and 2 it was
not known whether TL-MCTAG was in NP. Although the difference in generative
capacity between TL-MCTAG and SL-MCTAG is well known, this proven difference
in complexity (assuming NP = PSPACE) is novel.
To understand the reason underlying the difference, we note that the bound on the
length of non-splitting chains does not hold for set-local MCTAG. In set-local MCTAG
a tree tuple may be non-splitting while also performing a permutation of the order of
the lexical output generated by its children. Permutation is possible because set-locality
allows the tuple of strings generated by a tree tuple to be held separate for an arbitrary
number of steps in a derivation. This directly follows the basis of the reasoning of Kaji
et al (1992) in their proof that LCFRS is PSPACE-complete.
3.2 Universal Recognition of TL-MCTAG with Bounded Fan-Out is NP-Complete
The grammar constructed in the proof of Theorem 1 has fan-out n, the number of
clauses. However, the hardness result proved herein holds even if we restrict tree sets
to have at most two elements (TL-MCTAG(2)).4 The result provided here is as tight
as possible. If tree sets are restricted to a maximum size of one (TL-MCTAG(1)), the
formalism reduces to TAG and the hardness result does not hold.
Theorem 3
The universal recognition problem for TL-MCTAG(2) with fan-out limited to two and
unbounded rank is NP-complete.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem. We define a more complex
string w = w(1)w(2) ? ? ?w(p)wc where wc is a representation of C and w(i) controls the truth
assignment for the variable vi, 1 ? i ? p. The proof strategy is as follows. We construct
4 We use the postfix (2) to indicate the restriction on the fan-out.
452
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
a TL-MCTAG(2) grammar G such that each w(i) can be derived from G in exactly two
ways using the left members of tree sets of size 2 that correspond to the variables (and a
single initial tree set of size 1). We call the part of w comprising w(1)w(2) ? ? ?w(p) the prefix
string. The prefix string enforces the constraint of permitting only two derivations by
requiring a strictly alternating string of terminal symbols that can only be generated
by the grammar when the truth assignment is stable for a particular variable. The
derivation of the prefix string w(1)w(2) ? ? ?w(p) therefore corresponds to a guess of a truth
assignment for V. The right trees from the tree sets derive the components of wc that are
compatible with the guessed truth assignments for v1, . . . , vp. Subsequently we explain
how ?G, w? is constructed given an instance of 3SAT ?V, C?.
For every variable vi, 1 ? i ? p, let Ai = {cj | vi ? cj} and Ai = {cj | vi ? cj} be the
sets of clauses in which vi occurs positively and negatively, respectively; let alo mi =
|Ai| + |Ai| be the number of occurrences of the variable vi. Let ?? = {ai, bi | 1 ? i ? p}
be an alphabet of not already used symbols; let w(i) (again for 1 ? i ? p) denote a
sequence of mi + 1 alternating symbols ai and bi such that if mi is even w(i) = (aibi)mi/2ai
and if mi is odd w(i) = (aibi)(mi+1)/2. We define three functions, ?, ?, and ?, to aid in
the construction. The functions ? and ? are used to produce pieces of the prefix string
and will only produce the correct prefix string for a variable if the truth assignment is
consistent within the derivation. The function ? is used to produce strings representing
the clauses satisfied by a particular truth assignment to a variable. For every variable
vi, 1 ? i ? p, the clauses ?(i, 1), ?(i, 2), . . . , ?(i, |Ai|) are all the clauses in Ai and the
clauses ?(i, |Ai| + 1), . . . , ?(i, mi) are all the clauses in Ai. Further, for every 1 ? i ? p, let
?(i, 1) = aibi and let ?(i, h) = ai if h is even and ?(i, h) = bi if h is odd, for 2 ? h ? mi. For
every 1 ? i ? p, let ?(i, h) = ai if h is odd, and ?(i, h) = bi if h is even for 1 ? h ? mi ? 1
and let ?(i, mi) = aibi if mi is odd and biai if mi is even. The crucial property of ? and
? is that a string w(i) can be parsed either as a sequence of ?(i, ?) or ?(i, ?) strings, not
intermixed elements. The grammar must ?commit? to parsing the string one way or the
other, corresponding to committing to a value for the variable vi.
We define a TL-MCTAG(2) G to consist of the tree sets described herein. We con-
struct: (1) a tree set of length two for each combination of a variable and clause that the
variable can satisfy under some truth assignment, (2) two filler tree sets for each variable
(one for each truth assignment) of length two that only contribute the string indicating
the truth assignment of the variable but no satisfied clause, and (3) a singleton tree
set containing only an initial tree rooted in S. The initial tree has n + 1 branches with
the first branch intended to yield the prefix string w(1) ? ? ?w(p) and the (k + 1)-st branch
intended to yield ck where 1 ? k ? n. Although it is possible to generate strings not of
the form of w using this construction, given a pair ?G, w? where w respects the definition
above, we show that w ? L(G) if and only if C is satisfiable.
The initial tree set S contains the single tree pictured in Figure 4.5 The name of each
link in the initial tree set is composed of three indices that indicate the role of the link.
The first index, i, corresponds to variable vi. The second is an index into the series 1 ? ? ?mi
where mi is defined from vi as described previously. The third index, j, corresponds to a
clause cj. The use of multiple indices to name the links is for clarity only. They may be
renamed freely.
5 Although we permit the presence of multiple links at a single node in the S tree, we follow the usual TAG
convention of disallowing multiple adjunction. If one of the links at a node is used, the other links at that
node are assumed to be unavailable in the derivation.
453
Computational Linguistics Volume 36, Number 3
Figure 4
The start tree for TL-MCTAG(2) grammar G. The multiply-indexed link numbers are for clarity
only and are treated as simple link names.
For every variable vi, 1 ? i ? p, and index h, 1 ? h ? mi:
 if h ? |Ai|, tree set T(h)+i contains the following two trees:
 if h > |Ai|, tree set F(h)+i contains the following two trees:
454
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
 for all h, tree set T(h)?i contains the following two trees:
 for all h, tree set F(h)?i contains the following two trees:
An illustrative example is provided in Figure 5. In this example we demonstrate
derivations of two possible satisfying truth assignments for Boolean formula (x ? y ?
z) ? (x ? y ? z) ? (y ? y ? z). The truth assignments correspond to whether the T or F tree
sets are used in the derivation of the prefix string for a particular variable. As can be seen
from the example, the structure of the prefix string enforces the requirement that either
all T tree sets or all F tree sets are chosen for a particular variable. Each tree set marked
with a + is used to satisfy a single clause. Which clause a tree set satisfies can be read
off the link number at which it adjoins.
Inspection of the grammar and construction of the input string show that |G| and
|w| are polynomially related to p and n. The sum of the mi is maximally 3n. There are no
Figure 5
Example derivations of two satisfying assignments for the boolean formula
(x ? y ? z) ? (x ? y ? z) ? (y ? y ? z).
455
Computational Linguistics Volume 36, Number 3
more than 9pn + 1 tree sets and no more than 18pn + 1 total trees. The size of the initial
tree is bounded by 3pn and all other trees have constant size.
From a derivation of w ? L(G) we can find a truth assignment satisfying C by
examining the derivation. If the tree sets T(h)+i or T
(h)?
i are children of S for some i
and all h where 1 ? i ? p and 1 ? h ? mi, then vi is true. If the tree sets F(h)+i or F
(h)?
i
are children of S for some i and all h where 1 ? i ? p and 1 ? h ? mi, then vi is false.
By the construction, if w is of the form just described, for a given variable vi only
two derivations of w(i) will be possible, one in which all tree sets corresponding to
that variable are T tree sets and one in which all are F tree sets. Starting from a truth
assignment that satisfies C, we can prove that w ? L(G) by induction on |V|.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
3.3 Universal Recognition of TL-MCTAG with Bounded Rank is NP-Complete
We now show that universal recognition of TL-MCTAG is NP-complete even when the
rank is bounded.
We briefly recall here the definition of a decision problem called 3PAR. Let t and si ?
t be positive integers, 1 ? i ? 3m, m ? 1. The language 3PAR is defined as the set of all
tuples ?s1, . . . , s3m, t?, satisfying the following condition: The multiset Q = {s1, . . . , s3m}
can be partitioned into multisets Qi, 1 ? i ? m, such that for every 1 ? i ? m, |Qi| = 3
and
?
s?Qi s = t.
Language 3PAR is strongly NP-complete (Garey and Johnson 1979). This means that
3PAR is NP-complete even in case the integers si are all represented in unary notation.
Theorem 4
The universal recognition problem for TL-MCTAG with rank 1 and unbounded fan-out
is NP-complete.
Proof
We provide a reduction from 3PAR.6 Let ?s1, . . . , s3m, t? be an input instance of the 3PAR
problem, with all of the integers si represented in unary notation. Our target grammar G
is defined as follows. We use a set of nonterminal symbols {S, A}, with S being the start
symbol. We take the set of terminal symbols to be {a, $}. G contains two elementary
tree sets. The first set has a single elementary tree ?, corresponding to a context-free
production of the form S ? (AAA$)m?1AAA:
6 We follow the proof strategy of Barton (1985) in this proof.
456
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Tree ? has a unique link impinging on all of the 3m occurrences of nonterminal
A. The second (multi)set of G contains elementary trees ?i , 1 ? i ? 3m. Each ?i corre-
sponds to a context-free production of the form A ? asi :
We also construct a string w = (at$)m?1at.
If there exists a partition for multiset Q = {s1, . . . , s3m} satisfying the 3PAR require-
ment, we can directly construct a derivation for w in G, by sorting the elementary trees in
the second set accordingly, and by inserting these trees into the link of the elementary
tree ?. Conversely, from any derivation of w in G, we can read off a partition for Q
satisfying the requirement for membership in 3PAR for the input instance of the 3PAR
problem.
Finally, it is easy to see that G and w can be constructed in linear deterministic time
with respect to the size of the input instance of the 3PAR problem.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
3.4 Universal Recognition of TL-MCTAG with Fixed Input String is NP-Complete
We now show the unusual complexity result that universal recognition of TL-MCTAG is
NP-complete even when the input string is fixed. Although it is uncommon to require
this result, we rely on it in Section 5 to demonstrate that our parser has better time
complexity than the baseline parsing method for TL-MCTAG that we generalize from
the standard parsing method for LCFRS-equivalent formalisms.
We reduce from a variant of the 3SAT problem introduced above in which each
variable occurs in at most four clauses with no repeats in a clause. This problem was
shown to be NP-complete by Tovey (1984).
Theorem 5
Universal recognition of TL-MCTAG is NP-complete when the input string is fixed.
Proof
Let ?V, C? be an arbitrary instance of the 3SAT problem where each variable occurs in
no more than four clauses and does not repeat within a single clause. As in the proof of
Theorem 1, we use the derivations of the grammar to guess the truth assignments for
V and use the tree sets to keep track of the dependencies among different clauses in C.
Two tree sets are constructed for each variable, one corresponding to a true assignment
and one corresponding to a false assignment. The prohibition on multiple adjunction
ensures that only one of these two tree sets can be used for each variable. The tree set of
a particular truth assignment for a particular variable vi makes it possible to satisfy the
obligatory adjunction constraints for the nonterminal symbols representing each of the
clauses that vi satisfies in the 3SAT formula.7 Additional adjunction sites for each clause
7 Obligatory adjunction constraints are standard in the definition of TAG and MCTAG (Joshi, Levy, and
Takahashi, 1975; Weir 1988). However, obligatory adjunction may be avoided in this proof by creating a
457
Computational Linguistics Volume 36, Number 3
provide overflow space in the event that more than one variable satisfies a particular
clause. We fix the input string w to be the empty string. None of the trees of the grammar
contain any terminal symbols. However, a successful parse of the empty string can only
be achieved if all of the obligatory adjunction constraints are satisfied and this occurs
if and only if all clauses of the formula are satisfied by the truth assignment to some
variable.
We define a tree-local MCTAG G containing the following tree sets. We notate
obligatory adjunction constraints by underlining the nodes at which they apply. The
initial tree set S contains the single tree:
For every variable vi, 1 ? i ? p, tree set Ti (respectively, Fi) is used when represent-
ing an assignment of the value true (respectively, false) to vi. Ti (respectively, Fi) contains
at most five trees, one for the variable itself and one for each clause cj, 1 ? j ? n, such
that when vi is true (respectively false) cj is satisfied. More formally, tree set Ti contains
trees Vi? and Cj? if and only if vi ? cj, for 1 ? j ? n. Tree set Fi contains trees Vi? and Cj?
if and only if vi ? cj, for 1 ? j ? n.
Note that the diagram of the initial tree does not show the explicitly notated link
locations that we have used throughout the article. We omit the link locations to avoid
cluttering the diagram. However, because each variable occurs at most four times in the
formula, the total number of links is bounded by pn12.
From the definition of G it directly follows that ? ? L(G) implies the existence of a
truth assignment that satisfies C. A satisfying truth assignment can be read directly off
of any derivation tree for w. If Ti (respectively, Fi) is a child of S in the derivation tree,
then vk is true (respectively, false). The converse can be shown by using a satisfying
truth assignment for C to construct a derivation for w ? L(G).
?G, w? can be constructed in deterministic polynomial time because the number of
tree sets in the grammar is 2p + 1, the total number of trees in the grammar is bounded
by n(2p + 1), and the length of w is 0. All trees in the grammar have constant size except
for the initial tree, which has size 3n + p.
That this problem is in NP can be seen from the same reasoning as in the proof of
Theorem 2. 
larger grammar in which a separate tree set is created for each combination of clauses that may be
satisfied by a given variable. Because each variable may appear in no more than four clauses, this
increases the number of tree sets in the grammar by 24. We leave the details of this alternative proof
strategy to the reader.
458
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
4. An Alternative Definition of TL-MCTAG: Tree Vectors
The proof of NP-hardness of TL-MCTAG in the bounded rank case given in Theorem 4
depends crucially on the treatment of the elementary structures of the TL-MCTAG as
unordered sets. In order to produce the satisfying partitions for the 3PAR problem, any
tree from the second tree set must be able to adjoin at any location of link 1 in the first
tree set. This is in accordance with the usual definition of multi-component TAG. An
alternative definition of multi-component TAG in which the elementary structures are
treated as vectors is suggested by the explicit use of numbered links at the available
adjunction sites. Under this definition, each location of a link is also given an index and
only the tree at that index in a given vector may adjoin at that link location. An example
contrasting the two definitions is given in Figure 6.
The dependence of our bounded-rank proof on the set definition of TL-MCTAG
does not in itself show that vector-definition TL-MCTAG is polynomial in the bounded
rank case. We show this constructively in Section 5 by presenting a parser for vector
definition TL-MCTAG for which the polynomial degree of both the length of the in-
put string and the grammar size is constant when the rank of the input grammar is
bounded.
The difference in complexity between the set and vector definitions of TL-MCTAG
makes the vector definition an appealing possibility for research using TL-MCTAG for
natural language applications. Although all uses of TL-MCTAG in the computational
linguistics literature assume the set definition of TL-MCTAG, the linguistic analyses
therein do not require the additional flexibility provided by the set definition (Kallmeyer
and Joshi 2003; Nesson and Shieber 2006, 2007; Kallmeyer and Romero 2007; Nesson
2009). This is not a coincidence. Multicomponent tree sets are generally used to model
syntactic and semantic constructs in which one tree in the set strictly dominates another
and has a different syntactic or semantic type: for instance, a quantifier and its bound
variable. The locations at which the trees in these sets adjoin are not interchangeable
both because of the dominance constraint and because of the difference in type (and,
correspondingly, root node label). As a result, these grammars may be converted to the
Figure 6
An example contrasting the set definition of MCTAG (shown in Figure 2) with the vector
definition.
459
Computational Linguistics Volume 36, Number 3
Figure 7
The deductive rule generated for tree ? using the naive TAG parsing method.
vector definition without any change in the elementary trees, the generated language, or
grammar size but with crucial gains in the worst case bounds on processing efficiency.8
5. Parsing
Although no algorithms for parsing TL-MCTAG have previously been published, the
standard method for parsing LCFRS-equivalent formalisms can be applied directly to
TL-MCTAG to produce an algorithm with complexity O(|G|p|w|q) (Seki et al 1991). We
offer a novel parser for TL-MCTAG for which q is constant. With our algorithm, for
the set definition of TL-MCTAG p depends on both the rank and fan-out of the input
grammar. For the vector definition of TL-MCTAG p depends on the rank of the input
grammar but contains no index of the fan-out.
We begin with a brief introduction to TAG parsing before discussing our novel TL-
MCTAG parsing algorithm.
5.1 CKY-Style TAG Parsing
Following the method of Seki et al (1991), a naive parser for TAG may be constructed
by generating a single inference rule for each tree in the grammar. For a tree containing
r links, the rule will have r antecedents with each antecedent item representing a tree
that can adjoin at one of the links. Each adjoining tree will cover a span of the input
string that can be represented by four indices, indicating the left and right edges of the
span and of the subspan that will ultimately be dominated by its foot node. Because the
location of the links within the consequent tree is known, the indices in the antecedent
items are not entirely independent. An example is given in Figure 7. Observation shows
that there will be a worst case of 2(r + 1) independent indices in a given rule. Because
each adjoining tree is independent, there may be r + 1 different trees represented in a
single rule. This results in a time complexity of O(n2(r+1)|G|r+1) where n is the length
of the input string, |G| is a representation of the grammar size, and r is the rank of the
input grammar.
8 Various sorts of multicomponent TAGs have been proposed for analysis of scrambling (Rambow 1994;
Kallmeyer 2005). Scrambling entails several different trees of the same type adjoining in different orders,
and therefore seems like a candidate for making use of the flexibility provided by the set definition.
However, in these analyses the elementary tree structures are composed of one VP-rooted auxiliary tree
and one VP-rooted initial tree. Because auxiliary trees and initial trees cannot adjoin at the same link
location for structural reasons, these analyses do not ultimately make use of the flexibility in the selection
of an adjunction site that the set definition provides. The different VP-rooted auxiliary trees which could
benefit from interchanging adjunction sites achieve this flexibility because they appear in different tree
sets, not because they are members of a single set using the set definition.
460
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Following Graham, Harrison, and Ruzzo (1980) in their optimization of the Earley
parser (Earley 1970), the identifiers of specific trees need not be represented in the items
of the parser. Rather the tree identifiers may be replaced by the labels of the root nodes
of those trees, effectively bundling items of trees that share a root node label and cover
the same span. This modification to the algorithm reduces the time complexity of the
parser to O(n2(r+1)|G|).
We refer to this method of reducing complexity by removing unnecessary informa-
tion about specific elementary structures from the items of the parser as the GHR opti-
mization. When applied, it reduces the time complexity in the grammar size but does
not alter the basic form of the time complexity expression. There remains a single term
consisting of the product of a polynomial in the input string length and a polynomial in
the grammar size. We will return to this observation when examining the complexity of
TL-MCTAG parsing.
Shieber, Schabes, and Pereira (1995) and Vijay-Shanker (1987) apply the Cocke-
Kasami-Younger (CKY) algorithm, first introduced for use with CFGs in Chomsky
normal form (Kasami 1965; Younger 1967), to the TAG parsing problem to generate
parsers with a time complexity of O(n6|G|2). The speed-up in the parser comes from
traversing elementary trees bottom-up, handling only one link at a time. As a result, no
inference rule needs to maintain information about more than one link at a time. If the
GHR optimization is applied, the time complexity is reduced to O(n6|G|).
In order to clarify the presentation of our TL-MCTAG parser, we briefly review the
algorithm of Shieber, Schabes, and Pereira (1995) with minor modifications, using the
deductive inference rule notation from that paper. As shown in Figure 8, items in CKY-
style TAG parsing consist of a node in an elementary tree and the indices that mark
the edges of the span dominated by that node. Nodes, notated ?@a  , are specified by
three pieces of information: the identifier ? of the elementary tree the node is in, the
Gorn address a of the node in that tree,9 and the link  available at that node if there
is one. When no link is present, it is indicated by an underscore, . The node notation
?@a   may be read as ?node ? at address a with link ?.
Each item has four indices, indicating the left and right edges of the span covered by
the node as well as any gap in the node that may be the result of a foot node dominated
by the node. The indices are constrained to be non-decreasing from left to right in an
item. Nodes that do not dominate a foot node will have no gap in them, which we
indicate by the use of underscores in place of the indices for the gap. To limit the number
of inference rules needed, we define the following function i ? j for combining indices:
i ? j =
?
?
?
?
?
?
?
i j =
j i =
i i = j
undefined otherwise
The Adjoin rule has two indices, p and q, that appear in the antecedent but not in
the consequent. These indices specify the gap in one antecedent item and the edges of
the span in the other antecedent item, indicating that one antecedent item will fill the
gap in the span of the other antecedent item. The Foot Axiom similarly makes use of
unbound indices p and q. In this rule the entire span of the item is the gap that must be
9 A Gorn address uniquely identifies a node within a tree. The Gorn address of the root node is ?. The jth
child of the node with address i has address i ? j.
461
Computational Linguistics Volume 36, Number 3
Figure 8
The CKY algorithm for binary-branching TAG.
filled when the item adjoins to another item. As noted in Shieber, Schabes, and Pereira
(1995), the parser can be made more efficient by only introducing foot items of this sort
once an appropriate tree to adjoin into has been parsed for the span from p to q.
Each item of the form ??@a  , i, , , l? maintains the invariant that the input gram-
mar can derive a subtree rooted at ?@a with no foot node that spans wi+1 . . .wl. Items
of the form ??@a  , i, j, k, l? maintain the invariant that the input grammar can derive
a subtree rooted at ?@a with a foot node such that the fringe of the tree is the string
wi+1 . . . wjLabel(Foot(?))wk+1 . . . wl. The invariants for items of the form ??@a  , i, , , l?
and ??@a  , i, j, k, l? are similar except that no adjunction operation may occur at ?@a.
The side conditions Init(?) and Aux(?) hold if ? is an initial tree or an auxiliary
tree, respectively. Label(?@a) specifies the label of the node in tree ? at address a. Ft(?)
specifies the address of the foot node of tree ?. Link(?@a) specifies the link available at
node ?@a if there is one and null (represented as in the inference rules) otherwise.
Adj(?@a  , ?) holds if  is a link at which tree ? may adjoin into tree ? at address a.
Subst(?@a  , ?) holds if  is a link at which tree ? may substitute into tree ? at address
a. If  is null or the adjunction or substitution is prevented by other constraints such as
mismatched node labels, these conditions fail.
462
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 9
The deductive rule generated for tree ? using the baseline TL-MCTAG parsing method.
Consistent with the usual definition of TAG, only one link is permitted at a given
node. This effectively rules out multiple adjunction. Bounded multiple adjunction may
be permitted without affecting the complexity of the parsing algorithm by allowing a
list of links at a node. Although it first appears that the introduction of multiple links at
a single node could result in an exponential increase in the number of derivations, this
is not the case. The link diacritics themselves carry no information about the trees which
may adjoin at the associated adjunction site. Any restrictions, such as the requirement
of a matching node label, arise from the node itself. As a result, the links are fully
interchangeable and serve only as counters of the number of available adjunctions at a
node.10
5.2 CKY-Style Tree-Local MCTAG Parsing
As shown in Figure 9, the naive algorithm for parsing TAG may also be applied to
TL-MCTAG. The only difference is that each link may have multiple locations within a
given tree. Let r and f represent the rank and fan-out of the input grammar, respectively.
The time complexity of the naive parser will therefore be O(n2(rf+1)|G|r+1). However,
the GHR optimization cannot straightforwardly be applied because the maintenance of
tree locality requires items to carry information about the identities of the specific trees
involved rather than just the labels of the root nodes. Theorem 5 addresses the case in
which the input string length is 0. Therefore, in this case, any factor in the complexity
including the input string length cannot contribute to the overall time complexity. By
showing that the problem is NP-complete when the input string length is 0, Theorem 5
demonstrates that there must be some exponential factor or term in the time complex-
ity expression other than the input string length factor. Due to the earlier observation
that the GHR optimization does not change the form of the time complexity expression,
Theorem 5 therefore shows that the GHR optimization cannot reduce the exponent
of the grammar size term to a constant unless P = NP. This leaves open the possibility
of the existence of an algorithm that is polynomial in the grammar size but has an addi-
tional exponential term in the time complexity expression. However, such an algorithm,
if it exists, cannot be generated by application of the GHR optimization to the baseline
parser.
We can generalize the CKY TAG parsing algorithm presented above to the TL-
MCTAG case. This is an improvement over the standard LCFRS algorithm because it
reduces the q in the |w|q factor of the complexity to a constant. The direct specification
10 Note, however, that the finite length of the lists of links is necessary for multiple adjunction to remain
benign.
463
Computational Linguistics Volume 36, Number 3
of a CKY-style tree-local MCTAG parser is given in Figures 10 and 11. For a tree set or
vector ? from G, we notate the trees in the set or vector using indices that are indicated
as subscripts on the tree set identifier. A tree set or vector ? from G with length two will
therefore contain trees ?1 and ?2. Under the set definition these indices serve only as a
way of differentiating the members of the tree set. Under the vector definition, the index
must match the index of the link location where the tree will adjoin.
In order to directly parse tree-local MCTAG, items must keep track of the trees that
adjoin at each multicomponent link. We handle this by adding a link history to each
item. Under the set definition, a link history is an associative array of links notated with
indices and tree set identifiers notated with indices to identify a unique tree within the
set. Note that because under the set definition a tree may adjoin at any location of a
link, the indices of the link and tree set need not match. The axioms introduce empty
link histories, indicating that no adjunctions have yet occurred. When an adjunction
takes place, the tree identifier of the adjoining tree is associated with the link at which it
adjoins. In order for an adjunction to take place at a multicomponent link, the adjoining
tree?s tree set must be the same as that of any tree identifier already stored for that
link. This is enforced by the Valid(?) condition (Figure 12) defined on link histories. The
Filter(?, ?@a  ) function removes links that are completely used from the argument
link history. An empty link history indicates that tree locality has been enforced for the
subtree specified by the item; thus no additional information need be maintained or
passed on to later stages of the parse.
For the vector definition, the link histories may be simplified because each location
of a link fully specifies which tree from within a vector may adjoin there. As a result, the
link history is an associative array of links (not annotated with indices) and tree vector
identifiers. An example contrasting the link histories for the set and vector definitions
is given in Figure 13.
Figure 10
Modified item form, goal, and axioms for the CKY algorithm for tree-local MCTAG. Inference
rules of the algorithm are given in Figure 11.
464
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 11
Modified inference rules for the CKY algorithm for tree-local MCTAG. Alternative Adjoin,
Substitute, and No Adjoin rules are given for the set and vector definitions of TL-MCTAG. The
item form, goal item, and axioms are given in Figure 10.
The addition of a link history to each item increases the complexity of the algorithm.
The maximum link history length is bounded by the rank of the input grammar, r.
Under the set definition, the number of possible values for each element of a link history
is on the order of the number of tree sets in the grammar multiplied by the power
set of the fan-out: |G| ? 2f . Thus, for the set definition, the complexity of the algorithm
465
Computational Linguistics Volume 36, Number 3
Figure 12
Definition of the Valid condition, which ensures that all locations of a link are used by unique
trees from the same tree set. Under the set definition there is an entry for each link location and
both the identity of the tree set and the uniqueness of the tree from that tree set must be checked.
Under the vector definition only the link name and the tree vector identifier are stored because
the link locations uniquely select trees from within tree vectors.
Figure 13
A sample TL-MCTAG with examples of the possible link histories under the set and vector
definitions when the parser reaches the top of the circled node. Although the tree sets are
notated in set definition, the reader may substitute angle braces to get the corresponding vector
definition items.
is O(n6|G|r+22rf ). Under the vector definition, the number of possible values for each
element of a link history is on the order of the number of tree sets in the grammar. Thus,
for the vector definition, the complexity of the algorithm is O(n6|G|r+2). Note that the
variable representing fan-out, f , is present only in the complexity of the set definition.
This demonstrates the novel result that when rank is bounded, even with unbounded
fan-out, parsing the vector definition of TL-MCTAG is polynomial.
Permitting multiple adjunction may be accomplished by a method similar to the
one described for the TAG algorithm. Rather than associating each node with at most
one link, we permit nodes to be accompanied by a set of links. In contrast to the
TAG case, here we must use a set rather than a list to allow for the expressivity that
multiple adjunction can provide. In the TAG case a list is sufficient because the links
at a node are fully interchangeable. In the TL-MCTAG case, because the links are
defined not just by the node where they appear but by the full set of nodes at which
locations of that link appear, the links at a given node are not interchangeable. It must
be possible to use them in any order.11 Because the links can be used in any order, the
11 For links that share all locations it is still possible to enforce a strict order over them without
compromising expressivity.
466
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
addition of multiple adjunction adds a factor of 2r to the time complexity of the parsing
algorithm.
6. Link Factorization
The parser presented in the previous section has the advantage of running in polyno-
mial time if the elementary structures of the input TL-MCTAG are defined as vectors
and if the rank of the grammar is bounded by some constant. Bounding the rank by
a constant might be too strong a limitation in natural language parsing applications,
however. Thus, in the general case the running time of our algorithm contains a factor
that is an exponential function of the rank of the input grammar. To optimize parsing
time, then, we seek a method to ?factorize? the elementary trees of the grammar in
such a way that the rank is effectively reduced and the set of derived trees is preserved.
Although the precise meaning of factorization should be inferred from the subsequent
definitions, informally, by factorize we mean splitting a single elementary tree into sev-
eral smaller elementary trees without violating the locality constraints of the grammar
formalism. In this section we present a novel and efficient algorithm for factorizing a
TL-MCTAG into a strongly equivalent TL-MCTAG in which rank is minimized across
the grammar. Here, strongly equivalent means that the two grammars generate the
same set of derived trees.12
6.1 Preliminaries
Let ? be some elementary tree. We write |?| to denote the number of nodes of ?. For a
link l, we write |l| to denote the number of nodes of l.
For an elementary tree ?, we call a fragment of ? a complete subtree rooted at some
node n of ?, written ?(n), or else a subtree rooted at n with a gap at node n? in its
yield, written ?(n, n?). See Figure 14 for an example. We also use ? to denote a generic
fragment with or without a gap node in its yield.
Consider some fragment ? of ?. Let N? be the set of all nodes of ? and let N? be the
set of nodes of ? with the exclusion of the gap node, in case ? has such a node. We say
that ? is an isolated fragment iff ? includes at least one link and no link in ? impinges
both on nodes in N? and on nodes in N? ? N?. Figure 14 provides an example.
Intuitively, we can ?excise? an isolated fragment from ? without splitting apart
the links of ? itself, and therefore preserving the tree locality. This operation may also
reduce the number of links in ?, which is our main goal. The factorization algorithm we
present in Section 6.2 is based on the detection and factorization of isolated fragments.
Let n be a node from some elementary tree ?. We write lnodes(n) to denote the set
of all nodes from fragment ?(n) that are part of some link from ?. Node n is maximal if
 lnodes(n) = ?; and
 n is either the root node of ? or, for its parent node n?, we have
lnodes(n?) = lnodes(n).
12 The trees are not actually the same because of the small, reversible transformation that we make to ensure
that the factorized trees obey the TAG constraint that auxiliary trees must have matching root and foot
node labels. This transformation adds additional nodes into the tree structure but does not change the
shape of the trees and can be reversed to produce trees that are actually the same as the derived trees of
the original grammar.
467
Computational Linguistics Volume 36, Number 3
Figure 14
An elementary tree ? demonstrating fragments, isolation, and maximal nodes. Fragment
?1 = ?(n1, n2) contains all locations of links 2 and 3 , because links at the root node of a
fragment are contained within that fragment. It does not contain any locations of link 4 , because
links at the gap node of a fragment are not contained within that fragment. Because links 2 and
3 impinge only on nodes in ?1 and all other links impinge only on nodes not in ?1, ?1 is an
isolated fragment. Fragment ?2 = ?(n4) is not an isolated fragment because it contains only one
of the link locations of 4 . Note also that n4 is a maximal node but n5 is not.
Note that for every node n? of ? such that lnodes(n?) = ? there is always a unique
maximal node n such that lnodes(n?) = lnodes(n) (see Figure 14). Thus, for the purpose
of TL-MCTAG factorization, we can consider only maximal nodes. The first criterion
in the definition of maximal node, stating that a maximal node always dominates
(possibly reflexively) some node involved in a link, will often be implicitly used in the
following.
We need to distinguish the nodes in lnodes(n) depending on their impinging links.
Assume that {l1, l2, . . . , lr} is the set of all links occurring in ?. For 1 ? j ? r, we write
lnodes(n, lj) to denote the set of all nodes from fragment ?(n) with impinging link lj.
Thus,
?r
j=1 lnodes(n, lj) = lnodes(n). We associate with each maximal node n of ? a
signature ?(n), defined as a vector of size r and taking values over the subsets of
lnodes(n). For each j, 1 ? j ? r, we define
?(n)[j] =
?
?
?
lnodes(n, lj), if 0 < |lnodes(n, lj)| < |lj|;
?, if |lnodes(n, lj)| = 0 or
|lnodes(n, lj)| = |lj|.
Observe that, in this definition, ?(n)[j] = ? means that none or all of the nodes of lj are
found within fragment ?(n). The empty signature, written 0, is the signature with all of
its components set to ?.
Consider maximal nodes n1 and n2 such that n1 = n2, ?(n1) = 0, and ?(n2) = 0.
It is not difficult to see that ?(n1) = ?(n2) always implies that one of the two nodes
dominates the other. This observation is implicitly used in several places subsequently.
When visiting nodes of ? in a path from some leaf node to the root node,13 one
may encounter several maximal nodes having the same non-empty signature. In our
factorization algorithm, we need to consider pairs of such nodes that are as close as
possible. Consider two maximal nodes n1 and n2, n1 = n2, such that n1 dominates n2.
13 We view trees as directed graphs with arcs directed from each node to its parent.
468
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
The ordered pair (n1, n2) is called a minimal pair if ?(n1) = ?(n2) = 0 and, for every
maximal node n3 in the path from n2 to n1 with n3 = n1 and n3 = n2, we have ?(n3) =
?(n1). Consider now a sequence ?n1, n2, . . . , nq?, q ? 2, of nodes from ?. Such a sequence
is called a maximal chain if each pair (ni?1, ni) is a minimal pair, 2 ? i ? q, and all nodes
n from ? with ?(n) = ?(n1) are included in the sequence itself.
Notice that two maximal nodes belonging to two different maximal chains must
have different signatures, and thus one maximal node cannot belong to more than one
maximal chain. We now prove some basic properties of the notions just introduced that
will be used later in the development of our factorization algorithm and in the proof of
some of its mathematical properties.
Lemma 1
Let ? be an elementary tree and let n, n? be maximal nodes, with n properly dominating
n? in (ii).
(i) ?(n) = 0 if and only if ?(n) is an isolated fragment;
(ii) ?(n) = ?(n?) if and only if ?(n, n?) is an isolated fragment.
Proof
(i). If ?(n) = 0, then for each link l we have that either all nodes impinged on by l are
dominated (possibly reflexively) by n or none of these nodes is dominated by n. Because
n is maximal, we further conclude that at least some link l is found within ?(n).
Conversely, if ?(n) is an isolated fragment then all or none of the nodes impinged
on by some link l are dominated by n, and thus ?(n) = 0.
(ii). Let ?(n) = ?(n?), with n properly dominating n?. For each link lj, there are two
possible cases. First consider the case where ?(n)[j] = ?(n?)[j] = ?. In order for this to be
true, the link must be in one of three configurations, all of which satisfy the requirement
that the locations of lj must be all inside or all outside of the fragment ?(n1, n2).
 lnodes(n, j) = ?. In this configuration no one of the nodes on which lj
impinges is dominated by n.
 |lnodes(n, j)| = |lj|. We distinguish two possible cases.
? lnodes(n?, j) = ?. In this configuration all the nodes on which lj
impinges are within the fragment ?(n1, n2).
? |lnodes(n?, j)| = |lj|. In this configuration all the nodes on which lj
impinges are ?below? the fragment ?(n, n?).
Now consider the case where ?(n)[j] = ?(n?)[j] = ?. The nodes in lnodes(n?, j) are dom-
inated (possibly reflexively) by n? and therefore fall ?below? ?(n, n?). The remaining
nodes on which lj impinges cannot be dominated (possibly reflexively) by n. We thus
conclude that no nodes impinged on by lj occur within the fragment ?(n, n?).
Assume now that ?(n, n?) can be isolated. We can use exactly the same arguments
in the analysis of sets lnodes(n, j) and lnodes(n?, j), and conclude that ?(n) = ?(n?). 
The next lemma will be useful later in establishing that the factorization found by
our algorithm is optimal, namely, that it achieves the smallest rank under the imposed
conditions.
469
Computational Linguistics Volume 36, Number 3
Lemma 2
Let (n1, n2) be some minimal pair. Then
(i) for any node n3 in the path from n2 to n1, ?(n3) = 0;
(ii) for any minimal pair (n3, n4), neither or both of n3 and n4 are found in the
path from n2 to n1.
Proof
(i). Because ?(n2) = 0, there is some link lj for which ?(n2)[j] = lnodes(n2, j) = ?. Be-
cause n3 dominates n2, n3 dominates the nodes in lnodes(n2, j). Therefore, the only way
?(n3) could equal 0 is if |lnodes(n3, j)| = |lj|. But then ?(n1)[j] = ? because n1 dominates
n3. This is a contradiction.
(ii). Assume that n4 is on the path from n2 to n1. From the definition of minimal
pair, there must exist a link lk such that ?(n4)[k] = ?(n2)[k]. By the same reasoning
as in the proof of statement (i) for any link lj such that ?(n2)[j] = ?, we must have
?(n2)[j] = ?(n4)[j] = ?(n1)[j]. We thus conclude that ?(n2)[k] = ? and ?(n4)[k] = ?. Be-
cause ?(n4)[k] = ?(n3)[k] = ? and ?(n2)[k] = ?(n1)[k] = ?, node n3 must be in the path
from n2 to n1.
By a similar argument, we can argue that if n3 is on the path from n2 to n1, then
node n4 must be in that path as well. 
6.2 Factorization Algorithm
Let G be an input TL-MCTAG grammar. In this subsection we provide a method for the
construction of a TL-MCTAG that produces a grammar that generates the same derived
trees as G and that has minimal rank. We start with the discussion of some preprocessing
of the input.
We annotate each elementary tree ? as follows: We compute sets lnodes(n, lj)
for all nodes n and all links lj of ?. This can easily be done with a bottom up
visit of ?, by observing that if an internal node n has children n1, n2, . . . , nk then
lnodes(n, lj) =
?k
i=1 lnodes(ni, lj) ? Xj, where Xj = ? if lj does not impinge on n and Xj =
{n} if it does. Using sets lnodes(n, lj), we can then mark all nodes n in ? that are maximal,
and compute the associated signatures ?(n).
We also mark all maximal chains within ?. This simple procedure is reported in
Figure 15. We maintain an associative array with node signatures as entries and node
lists as values. We visit all maximal nodes of ? in a top?down fashion, creating a list for
each different signature and appending to such a list all nodes having that signature.
In the following algorithm we excise isolated fragments from each elementary tree
?. We now introduce some conventions for doing this. Although it would be possible to
Figure 15
Construction of maximal chains in the factorization algorithm.
470
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
excise fragments without the introduction of additional tree structure, we adopt instead
two simple tree transformations that preserve auxiliary tree root and foot label matching
and result in some simplification of the notation used by the algorithm, particularly in
case the root node of a fragment is the same as the gap node of a second fragment within
?. A schematic depiction of both transformations is given in Figure 16.
When a fragment ?(n) is excised, we leave a copy of the root node n without its
impinging links that dominates a fresh node n? with a fresh link indicating obligatory
substitution of the excised fragment. The excised fragment consists of ?(n) including
any links impinging on n, but has a fresh root node immediately dominating n with the
same label as n?. This is shown in the top row of Figure 16.
A similar transformation is used to excise a fragment ?(n, n?). Nodes n and n? of
the original tree are not altered, and thus they retain their names. The material between
them is replaced with a single new node with a fresh nonterminal symbol and a fresh
link. This link indicates the obligatory adjunction of the excised fragment. A new root
and gap node are added to ?(n, n?) to form the excised fragment. This is shown in the
bottom row of Figure 16. We remark that any link impinging on the root node of the
excised fragment is by our convention included in the excised fragment, and any link
impinging on the gap node is not.
To regenerate the original tree, the excised fragment ?(n, n?) can be adjoined back
into the tree from which it was excised. The new nodes that have been generated in
the excision may be removed and the original root and gap nodes may be merged back
together retaining any impinging links.
We need to introduce one more convention for tree excision. Consider a maximal
chain c = ?n1, n2, . . . , nq? in ?, q ? 2. In case q = 2, our algorithm processes c by excis-
ing a fragment ?(n1, n2) from ?, exactly as explained above. In case q > 2, a special
processing is required for c. Chain c represents q ? 1 minimal pairs, corresponding to
fragments ?(ni?1, ni), 2 ? i ? q. We do not excise these q ? 1 fragments one by one,
Figure 16
Diagrams of the tree transformations performed when fragments ?(n) and ?(n, n?) are removed.
471
Computational Linguistics Volume 36, Number 3
because this would create q ? 1 > 1 new links within ?. We follow instead a procedure
that ?binarizes? c, as explained here.
Let us recursively define an elementary tree ?c as follows, for |c| = q and q ? 3:
 In case q = 3, ?c is a tree composed of two nodes besides the root and the
gap nodes, n and n?, with n immediately dominating n?. Node n hosts
the (obligatory) adjunction of the fragment ?(n1, n2) and node n? hosts
the (obligatory) adjunction of ?(n2, n3). Both fragments are transformed
as previously discussed.
 In case q > 3, ?c is a tree composed of two nodes besides the root and the
gap nodes specified as above, with n? hosting the (obligatory) adjunction
of the transformed fragment ?(nq?1, nq). Node n hosts the adjunction of
tree ?c? , with c? = ?n1, . . . , nq?1?.
Note that each tree ?c has rank two.
When processing a maximal chain c with q > 2, the whole fragment ?(n1, nq) is
excised, using this convention. This results in a single fresh link added to ?. In this case
the link refers to the adjunction of a newly created elementary tree ?c, defined as above.
An example of the binarization of a maximal chain with q = 4 is reported in Figure 17.
We can now discuss the factorization algorithm, reported in Figure 18. For a maxi-
mal node n in an elementary tree ?, we write links(n) to denote the number of links from
? that are entirely contained in fragment ?(n). We process each tree set I? of the source
grammar and each elementary tree ? in I? as follows.
Figure 17
The binarization procedure applied to a maximal chain c = ?n1, n2, n3, n4?.
472
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
Figure 18
The factorization algorithm for tree-local MCTAG.
In the first phase, we add to an agenda A each maximal node n different from the
root of ? such that ?(n) = 0. We associate this agenda item with the score links(n). At
the same time, each maximal chain ?n1, n2, . . . , nq?, q ? 2, is added to A, with associated
score links(n1) ? links(nq).
In the second phase, we process all items in A, in order of increasing score, ignoring
those items that have a score of one. If the current item is a maximal node n, we excise
the fragment ?(n) from ?, leaving in place a fresh node with a single node link denoting
obligatory substitution. If the current item is a maximal chain of the form ?n1, n2?, we
excise from ? the fragment ?(n1, n2), leaving in place a fresh node with a single node
link denoting obligatory adjunction of the excised fragment. Finally, if the current item
is a maximal chain c = ?n1, . . . , nq? with q > 2, we excise from ? the whole fragment
?(n1, nq), and we apply to the chain the binarization procedure described in this sub-
section. This results in the addition to the output grammar of fragments ?(ni?1, ni), for
2 ? i ? q, and of newly created elementary tree ?c and elementary trees ?c? for each
chain c? that is a proper prefix of c. After the processing of all elementary trees in tree
set I? is completed, the resulting version of set I? is also added to the output grammar.
As a simple example of a run of the factorization algorithm, we discuss the process-
ing of the elementary tree ? depicted in Figure 19. Tree ? has four links, called li,
1 ? i ? 4. Link l1 impinges on nodes n11 and n12, link l2 impinges on nodes n21 and
n22. Links l3 and l4 impinge on a single node each, and the impinging nodes are called
n3 and n4, respectively. In Figure 19 we have outlined the maximal nodes n, n?, and n??
473
Computational Linguistics Volume 36, Number 3
Figure 19
An example tree to be processed by the factorization algorithm.
that are relevant to this example. Node n dominates both n? and n?? but none of n? and n??
dominates the other. Note that within ? there must exist maximal nodes other than n, n?,
and n??. For instance, there must be a maximal node dominating (possibly reflexively)
node n3 but not node n4. However, this node dominates a single link, and will not be
processed by the algorithm because of the requirement at line 12 in Figure 18. We thus
ignore this and other maximal nodes in what follows.
We have
lnodes(n?, l1) = {n11}, lnodes(n, l1) = {n11},
lnodes(n?, li) = ?, 2 ? i ? 4, lnodes(n, l2) = {n21, n22},
lnodes(n??, li) = ?, 1 ? i ? 2, lnodes(n, l3) = {n3},
lnodes(n??, l3) = {n3}, lnodes(n, l4) = {n4},
lnodes(n??, l4) = {n4},
and
?(n?) = [{n11}, ?, ?, ?],
?(n??) = 0,
?(n) = ?(n?).
The algorithm in Figure 15 will then mark the chain ?n, n??. When processing the elemen-
tary tree ?, the algorithm in Figure 18 will add to its agenda an item n?? with a score of
links(n??) = 2, as well as the above chain, with a score of links(n) ? links(n?) = 3 ? 0 = 3.
Node n?? is processed first, and fragment ?(n??) is excised from ? leaving in its place
a fresh link l5. Later on, the algorithm pops the chain ?n, n?? from the agenda, and
fragment ?(n, n?) is excised from ? leaving in its place a fresh link l6. The algorithm
then stops. The resulting factorization consists in fragment ?(n??) with links l3 and l4,
fragment ?(n, n?) with links l2 and l5, and what is left of the elementary tree ?, with
links l1 and l6.
The discussion of the correctness of the algorithm is reported in the next section,
along with some other mathematical properties.
474
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
6.3 Mathematical Properties
We discuss in this section some mathematical properties of our factorization algorithm.
Let G be the input TL-MCTAG and let G? be the output of the algorithm. We start with
the issue of correctness. First, notice that our algorithm stops after a finite number of
steps, because the number of possible excisions for G is finite. Assume now that ? and
?? are two isolated fragments within some elementary tree ?, and ?? is itself a fragment
within ?. It is easy to see that excising ?? from ? results in a new fragment of ? that is
still an isolated fragment. Using this observation together with Lemma 1, we can then
conclude that all fragments that are excised by the algorithm are isolated fragments.
This in turn implies that each fragment excision in our algorithm preserves tree locality,
and G? is still a TL-MCTAG.
Each fragment that is excised from some source tree must obligatorily be adjoined
back into that tree, at the point from which it was removed. Thus, G? generates the same
derived trees as G, modulo our trivial tree transformation for the root and the gap nodes.
This proves the correctness of our factorization algorithm.
One remark is in order here. Note that we always excise fragments that have at
least two links. This can be shown inductively as follows. Consider first the smallest
fragments that are excised from some elementary tree ?, that is, those fragments that
do not contain any other fragment within themselves. These fragments always have
at least two links, because of the requirement stated in line 12 in the algorithm. In the
inductive case, let ? be some fragment of ? from which a second fragment ?? has been
already excised in some iteration of the loop at lines from 11 to 23. Fragment ?? is thus
replaced by some link l?. Because of the definition of maximal node, ? must contain at
least one link l that is not contained in ??. In case l itself is part of some excised fragment
???, there will still be some other fresh link replacing ???. We thus conclude that, when
excised, ? always has at least two links. Because excised fragments always have at least
two links and since we never consider elementary trees as candidate fragments (line 6),
we can conclude that our algorithm always finds a non-trivial factorization of G.
We can now turn to an analysis of the computational complexity of our algorithm.
Consider an elementary tree ? of G with r links and with a maximum of f nodes per
link. In the preprocessing phase of the algorithm, the computation of sets lnodes(n, lj)
can be carried out in time O(|?| ? r ? f ). To see this, notice that there are no more than
|?| ? r such sets. Furthermore, we have |lnodes(n, lj)| ? f for each j, and each node in
lnodes(n, lj) is processed in constant time through the union operator, when constructing
the set lnodes(n?, lj) for the parent node n? of n. Clearly, O(|?| ? r ? f ) is also a time upper
bound for the computation of quantities ?(n) and links(n) for all nodes in ?, and for
extracting a list of the maximal nodes therein as well.14
In what follows, we will need to compare signatures of different nodes for equality.
Despite the fact that each signature has r elements, and each element of a signature is a
set with O( f ) elements, there are at most |?| different signatures. We can therefore use
an atomic symbol to name each signature (perfect hashing). In this way, signatures can
be compared in constant time.
The marking of all maximal chains within ?, as specified by the algorithm in
Figure 15, can be implemented in time O(|?|). This is done by encoding the associative
14 We remark here that a further improvement in efficiency could be achieved by replacing the sets of nodes
in a signature with the single node that is the least common ancestor of the set of nodes. However, using
the set of nodes substantially improves the clarity of the presentation of the algorithm, so we do not
pursue this optimization here.
475
Computational Linguistics Volume 36, Number 3
array L in the algorithm through a one-dimensional array indexed by signature names.
Each element of the array points to a linked list of nodes, representing a maximal chain.
We now analyze the running time of the factorization function in Figure 18. Let us
first consider a single elementary tree ?. We implement the priority queue A through
a heap data structure. The loops at lines 6 and 9 run in time O(|?| ? log(|?|)): This is
the standard result for populating a heap; see for instance Cormen et al (2001). At
each iteration of the while loop at lines 11 to 23, we extract some fragment ?(n) or
?(n1, nq). The processing of each such fragment ? takes an amount of time O(|?|),
where |?| is the number of nodes of ?. In such an iteration, ? needs to be re-edited
into a new elementary tree with the number of nodes |?| ? |?| + c, where c ? 3 is a
constant that depends on the specific transformation in Figure 16 that was applied in
the excision of the fragment tree. Nonetheless, if a suitable representation is maintained
for ?, making use of nodes and pointers, the re-editing of ? can be done in constant
time. Then a single iteration of the while loop takes time O(|?|), where ? is the excised
fragment. We can then conclude that all iterations of the while loop take an amount of
time O(|?| ? log(|?|)).15
Now let ?M be the elementary tree of G with the largest size, and let rG and fG be the
rank and fan-out of G, respectively. Putting everything together, the total running time
of the factorization algorithm is O(|G| ? (rG ? fG + log(|?M|))), where |G|, the size of the
input grammar, is defined as the sum of terms |?| for all elementary trees ? of G. Because
we always have fG ? |?M|, this upper bound can be rewritten as O(|G| ? |?M| ? rG).
A special case is worth discussing here. If the maximum number of links impinging
on a node of our elementary trees is bounded by some constant, we have rG ? fG =
O(|?M|). In this case, the above bound reduces to O(|G| ? |?M|). The constant bound on
the number of links impinging on the nodes of a grammar holds for all of the grammars
we have studied in Section 3.
We now argue that our algorithm provides the factorization G? of G with the
smallest possible rank, under the assumption that G and G? are strongly equivalent,
that is, that they generate the same derived trees.
A factorization f of G is called maximal if no one of its fragments has a smaller
isolated fragment within itself. We start by observing that the factorization of G found
by our algorithm is maximal. To see this, consider the excision by our algorithm of
a maximal chain ?n1, . . . , nq? within an elementary tree ?. This item is added to the
priority heap at line 10, with a score of links(n1) ? links(nq). This score is the number of
links found in fragment ?(n1, nq), with the exclusion of the links at the gap node nq.
The chain is then factorized into fragments ?(ni?1, ni), for each i with 2 ? i ? q. Assume
that some fragment ?(ni?1, ni) contains in turn a maximal chain ?n?1, . . . , n?q?? or else
an isolated fragment of the form ?(n?). In the first case we have links(n?1) ? links(n?q? ) <
links(n1) ? links(nq) and in the second case we have links(n?) < links(n1) ? links(nq).
Thus the smaller chain or fragment is processed earlier than our maximal chain, and
by the time our maximal chain is processed, the smaller chain or fragment has already
been excised. A similar argument shows that the excision by our algorithm of an isolated
fragment of the form ?(n) happens after the excision of any maximal chain or fragment
included within ?(n) itself.
15 We mention here a second possible optimization of the algorithm. The priority queue allows us to excise
tree segments always from the input elementary tree ?, making the algorithm easier to analyze.
However, as one of the reviewers has pointed out to us, we could do away with the use of the priority
queue and process fragment trees in any order. This results in running time O(|?|) for the factorization
function in Figure 18.
476
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
We now show that the maximal factorization of G is unique. Let ? and ?? be two
isolated fragments of some elementary tree ?. We say that ? and ?? partially overlap if
the set of nodes shared by ? and ?? is not empty and is a proper subset of the nodes of
both fragments. It is not difficult to see that if ? and ?? partially overlap, then at least
one among ? and ?? must have the form ?(n1, n2).
Without any loss of generality, we assume that the elementary trees of G are always
factorized at their maximal nodes, as discussed in Section 6.1. Let us assume that f
and f ? are two distinguishable maximal factorizations of G. Because no fragment of one
factorization can be a sub-fragment of some fragment of the other factorization, there
must be some fragment ? of f and some fragment ?? of f ? such that ? and ?? partially
overlap.
Assume that ? has the form ?(n1). Then ?? must have the form ?(n2, n3), and n1
must be in the path from n3 to n2. Because ?? is as small as possible, (n2, n3) must be a
minimal pair. We have then established a violation of Lemma 2(i). Assume now that ?
has the form ?(n1, n2). Again, (n1, n2) must be a minimal pair. If ?? has the form ?(n3),
this argument applies again, resulting in a violation of Lemma 2(i). If ?? has the form
?(n3, n4), then (n3, n4) must be a minimal pair. Furthermore, n1, n2, n3, and n4 must all
be on the same path within ?, with n1, n2 in alternation with n3, n4. This establishes a
violation of Lemma 2(ii). The assumption that f and f ? partially overlap then leads to a
contradiction, and we must conclude that the maximal factorization of G is unique.
We can also use this argument against the existence of overlapping fragments to
show that any factorization f of G other than the unique maximal factorization fM must
be coarser than fM, meaning that each fragment ? of f is also a fragment of fM, or else
? can be represented as a combination of the fragments of fM (through substitution and
adjunction). This means that no factorization of G can have rank smaller than the rank
of the maximal factorization fM. We conclude that our algorithm is optimal.
This discussion on the optimality of the factorization algorithm crucially assumes
strong equivalence with the source TL-MCTAG G. Of course there might be TL-
MCTAGs that are weakly equivalent to G, that is, they generate the same language,
and have rank strictly smaller than the rank of G?. However, finding such structurally
different grammars is a task that seems to require techniques quite different from the
factorization techniques we have developed in this section. Furthermore, the task might
be computationally unfeasible, considering the fact that the weak equivalence problem
for TL-MCTAG is undecidable. (Such a problem is undecidable even for CFGs.)
We remark here that if we are allowed to change G by recasting its elementary trees
in some suitable way, we might be able to further reduce the rank with respect to the
algorithm we have presented in this section. In this case the output grammar would
not preserve the derived trees, that is, we lose the strong equivalence, but still retain
the derivation trees unaltered. Although this is likely not desirable for applications
in which the input grammar consists of linguistically motivated trees, there may be
other applications for which the preservation of the internal structure of the trees is
less important than the processing efficiency that can be gained by more aggressive
factorization. Furthermore, it is well known that the desired derived tree for the source
grammar can be easily reconstructed from the derivation tree.
Consider for instance cases in which the input TL-MCTAG is not in binary form,
that is, some nodes have more than two children. Currently, the definition of fragment
does not allow splitting apart a subset of the children of a given node from the remaining
ones. However, if we allow binarization of the elementary trees of the source grammar,
then we might be able to isolate sets of links that could not be factorized in the source
grammar itself. It is not difficult to construct an elementary tree ? with r links such
477
Computational Linguistics Volume 36, Number 3
that no factorization of ? is possible if we are required to preserve ??s structure, but if
we drop such a requirement then we could binarize ? in such a way that a factorization
can be obtained through the application of the algorithm above, such that any tree in the
factorization has no more than two links. However, the general problem of restructuring
elementary trees in such a way that an optimal factorization is possible is not trivial and
requires further research. We leave this problem for future work.
A second case arises when multiple links impinge on the same node of an elemen-
tary tree. As presented, the factorization algorithm is designed to handle grammars in
which multiple adjunction is permitted. However, if multiple adjunction is disallowed
and the grammar contains trees in which multiple links impinge on the same node,
the use of one link at a node will disqualify any other impinging links from use. This
opens up the possibility of further reducing the rank of the grammar by producing
tree sets that do not contain any nodes on which multiple links impinge. This can
be accomplished by performing a first-pass grammar transformation in which a copy
of each elementary tree set is added to the grammar for each distinct, maximal, non-
conflicting set of links appearing in the tree set. This transformation in itself may result
in a reduction of the rank of the source grammar. The factorization algorithm can then be
applied to the new grammar. However, if the elementary trees in the source grammar
contain clusters of links that are mutually overlapping, the suggested transformation
may blow up the size of the input grammar in a way that is not bounded by any
polynomial function.
7. Conclusion
This paper explores the complexity of TL-MCTAG, showing that recognition is NP-
complete under a range of interesting restrictions. It then provides a parsing algorithm
that performs better than the extrapolation of the standard multiple CFG parsing
method to TL-MCTAG. As shown by our proofs, the difficulty in parsing TL-MCTAG
stems from the rank of the input grammar. We offer a novel and efficient algorithm for
minimizing the rank of the input grammar while preserving its strong generative capac-
ity. It fits into an active line of research into efficient processing of multicomponent and
synchronous formalisms that appear computationally intractable but have desirable
characteristics for meeting the expressive needs of natural language. It presents novel
complexity results and algorithms for TL-MCTAG, a widely known and used formalism
in computational linguistics that may be applied more effectively in natural-language
processing using algorithms that process it as efficiently as possible.
Acknowledgments
This work was supported in part by the
National Science Foundation under award
BCS-0827979. The second author has been
partially supported by MIUR under project
PRIN No. 2007TJNZRE 002.
References
Barton, G. Edward. 1985. On the complexity
of ID/LP parsing. Computational
Linguistics, 11(4):205?218.
Chen, John and Vijay K. Shanker. 2004.
Automated extraction of TAGs from the
Penn treebank. In H. Blunt, J. Carroll, and
G. Satta, editors, New Developments in
Parsing Technology. Kluwer Academic,
Amsterdam, pages 73?89.
Cormen, Thomas H., Charles E. Leiserson,
Ronald L. Rivest, and Clifford Stein. 2001.
Introduction to Algorithms. The MIT Press,
Cambridge, MA.
Earley, J. 1970. An Efficient Context-free
Parsing Algorithm. Ph.D. thesis, University
of California, Berkeley, CA.
Garey, M. R. and D. S. Johnson. 1979.
Computers and Intractability. Freeman and
Co., New York, NY.
Gildea, Daniel, Giorgio Satta, and Hao
Zhang. 2006. Factoring synchronous
478
Nesson, Satta, and Shieber Complexity, Parsing, and Factorization of TL-MCTAG
grammars by sorting. In The International
Conference on Computational Linguistics/
Association for Computational Linguistics
(COLING/ACL-06) Poster Session
pages 279?286, Sydney.
Graham, S. L., M. A. Harrison, and W. L.
Ruzzo. 1980. An improved context-free
recognizer. ACM Transactions on
Programming Languages and Systems,
2:415?462.
Han, Chung-Hye. 2006. Pied-piping in
relative clauses: Syntax and compositional
semantics based on synchronous tree
adjoining grammar. In Proceedings of the 8th
International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+ 8),
pages 41?48, Sydney.
Joshi, A. K., L. S. Levy, and M. Takahashi.
1975. Tree adjunct grammars. Journal of
Computer and System Sciences,
10(1):136?163.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages, volume 3. Springer,
Berlin, pages 69?124.
Kaji, Yuichi, Ryuchi Nakanishi, Hiroyuki
Seki, and Tadao Kasami. 1992. The
universal recognition problems for
multiple context-free grammars and for
linear context-free rewriting systems.
IEICE Transactions on Information and
Systems, E75-D(1):78?88.
Kaji, Yuichi, Ryuchi Nakanishi, Hiroyuki
Seki, and Tadao Kasami. 1994. The
computational complexity of the universal
recognition problem for parallel multiple
context-free grammars. Computational
Intelligence, 10(4):440?452.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?225.
Kallmeyer, Laura. 2009. A declarative
characterization of different types of
multicomponent tree adjoining grammars.
Research on Language and Computation,
7(1):55?99.
Kallmeyer, Laura and Aravind K. Joshi.
2003. Factoring predicate argument and
scope semantics: Underspecified
semantics with LTAG. Research on
Language and Computation, 1:3?58.
Kallmeyer, Laura and Maribel Romero.
2007. Reflexives and reciprocals in
LTAG. In Proceedings of the Seventh
International Workshop on Computational
Semantics ICWS-7, pages 271?282,
Tilburg.
Kasami, T. 1965. An efficient recognition
and syntax algorithm for context-free
languages. Technical Report
AF-CRL-65-758, Air Force Cambridge
Research Laboratory, Bedford, MA.
Nesson, Rebecca. 2009. Synchronous and
Multicomponent Tree-Adjoining Grammars:
Complexity, Algorithms and Linguistic
Applications. Ph.D. thesis, Harvard
University, Cambridge, MA.
Nesson, Rebecca, Giorgio Satta, and Stuart
Shieber. 2008. Optimal k-arization of
synchronous tree-adjoining grammar. In
the Association for Computational Linguistics
(ACL-2008), pages 604?612, Columbus, OH.
Nesson, Rebecca and Stuart M. Shieber.
2006. Simpler TAG semantics through
synchronization. In Proceedings of the
11th Conference on Formal Grammar,
pages 129?142, Malaga.
Nesson, Rebecca and Stuart M. Shieber.
2007. Extraction phenomena in
synchronous TAG syntax and semantics.
In Proceedings of Syntax and Structure in
Statistical Translation (SSST), pages 9?16,
Rochester, NY.
Rambow, Owen. 1994. Formal and
computational aspects of natural language
syntax. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results for
synchronous context-free grammars. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing
(HLT05/EMNLP05), pages 803?810,
Vancouver.
Schabes, Yves and Richard C. Waters. 1995.
Tree insertion grammar: A cubic-time
parsable formalism that lexicalizes
context-free grammar without changing
the trees produced. Computational
Linguistics, 21(4):479?513.
Seki, H., T. Matsumura, M. Fujii, and
T. Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science,
88:191?229.
Shieber, Stuart M. and Yves Schabes. 1994.
An alternative conception of tree-adjoining
derivation. Computational Linguistics,
20(1):91?124.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
Sippu, S. and E. Soisalon-Soininen. 1988.
Parsing Theory: Languages and Parsing.
Springer-Verlag, Berlin.
479
Computational Linguistics Volume 36, Number 3
S?gaard, Anders, Timm Lichte, and
Wolfgang Maier. 2007. On the complexity
of linguistically motivated extensions of
tree-adjoining grammar. In Recent
Advances in Natural Language Processing
2007, Borovets.
Tovey, C. A. 1984. A simplified NP-complete
satisfiability problem. Discrete Applied
Mathematics, 8(1):85?90.
Vijay-Shanker, K. 1987. A study of
tree-adjoining grammars. Ph.D. thesis,
Department of Computer and Information
Science, University of Pennsylvania,
Philadelphia, PA.
Vijay-Shanker, K. and Aravind K. Joshi. 1985.
Some computational properties of
tree-adjoining grammars. In Proceedings of
the 23rd Annual Meeting of the Association
for Computational Linguistics, pages 82?93,
Chicago, IL.
Weir, David. 1988. Characterizing mildly
context-sensitive grammar formalisms.
Ph.D. thesis, Department of Computer and
Information Science, University of
Pennsylvania, Philadelphia, PA.
Younger, D. H. 1967. Recognition and
parsing of context-free languages in time
n3. Information and Control, 10(2):189?208.
Zhang, Hao and Daniel Gildea. 2007.
Factorization of synchronous context-free
grammars in linear time. In NAACL
Workshop on Syntax and Structure in
Statistical Translation (SSST), pages 25?32,
Rochester, NY.
480
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937?947,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bayesian Synchronous Tree-Substitution Grammar Induction
and its Application to Sentence Compression
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
We describe our experiments with training
algorithms for tree-to-tree synchronous
tree-substitution grammar (STSG) for
monolingual translation tasks such as
sentence compression and paraphrasing.
These translation tasks are characterized
by the relative ability to commit to parallel
parse trees and availability of word align-
ments, yet the unavailability of large-scale
data, calling for a Bayesian tree-to-tree
formalism. We formalize nonparametric
Bayesian STSG with epsilon alignment in
full generality, and provide a Gibbs sam-
pling algorithm for posterior inference tai-
lored to the task of extractive sentence
compression. We achieve improvements
against a number of baselines, including
expectation maximization and variational
Bayes training, illustrating the merits of
nonparametric inference over the space of
grammars as opposed to sparse parametric
inference with a fixed grammar.
1 Introduction
Given an aligned corpus of tree pairs, we might
want to learn a mapping between the paired trees.
Such induction of tree mappings has application
in a variety of natural-language-processing tasks
including machine translation, paraphrase, and
sentence compression. The induced tree map-
pings can be expressed by synchronous grammars.
Where the tree pairs are isomorphic, synchronous
context-free grammars (SCFG) may suffice, but in
general, non-isomorphism can make the problem
of rule extraction difficult (Galley and McKeown,
2007). More expressive formalisms such as syn-
chronous tree-substitution (Eisner, 2003) or tree-
adjoining grammars may better capture the pair-
ings.
In this work, we explore techniques for inducing
synchronous tree-substitution grammars (STSG)
using as a testbed application extractive sentence
compression. Learning an STSG from aligned
trees is tantamount to determining a segmentation
of the trees into elementary trees of the grammar
along with an alignment of the elementary trees
(see Figure 1 for an example of such a segmenta-
tion), followed by estimation of the weights for the
extracted tree pairs.1 These elementary tree pairs
serve as the rules of the extracted grammar. For
SCFG, segmentation is trivial ? each parent with
its immediate children is an elementary tree ? but
the formalism then restricts us to deriving isomor-
phic tree pairs. STSG is much more expressive,
especially if we allow some elementary trees on
the source or target side to be unsynchronized, so
that insertions and deletions can be modeled, but
the segmentation and alignment problems become
nontrivial.
Previous approaches to this problem have
treated the two steps ? grammar extraction and
weight estimation ? with a variety of methods.
One approach is to use word alignments (where
these can be reliably estimated, as in our testbed
application) to align subtrees and extract rules
(Och and Ney, 2004; Galley et al, 2004) but
this leaves open the question of finding the right
level of generality of the rules ? how deep the
rules should be and how much lexicalization they
should involve ? necessitating resorting to heuris-
tics such as minimality of rules, and leading to
1Throughout the paper we will use the word STSG to re-
fer to the tree-to-tree version of the formalism, although the
string-to-tree version is also commonly used.
937
large grammars. Once a given set of rules is ex-
tracted, weights can be imputed using a discrimi-
native approach to maximize the (joint or condi-
tional) likelihood or the classification margin in
the training data (taking or not taking into account
the derivational ambiguity). This option leverages
a large amount of manual domain knowledge en-
gineering and is not in general amenable to latent
variable problems.
A simpler alternative to this two step approach
is to use a generative model of synchronous
derivation and simultaneously segment and weight
the elementary tree pairs to maximize the prob-
ability of the training data under that model; the
simplest exemplar of this approach uses expecta-
tion maximization (EM) (Dempster et al, 1977).
This approach has two frailties. First, EM search
over the space of all possible rules is computation-
ally impractical. Second, even if such a search
were practical, the method is degenerate, pushing
the probability mass towards larger rules in order
to better approximate the empirical distribution of
the data (Goldwater et al, 2006; DeNero et al,
2006). Indeed, the optimal grammar would be one
in which each tree pair in the training data is its
own rule. Therefore, proposals for using EM for
this task start with a precomputed subset of rules,
and with EM used just to assign weights within
this grammar. In summary, previous methods suf-
fer from problems of narrowness of search, having
to restrict the space of possible rules, and overfit-
ting in preferring overly specific grammars.
We pursue the use of hierarchical probabilistic
models incorporating sparse priors to simultane-
ously solve both the narrowness and overfitting
problems. Such models have been used as gener-
ative solutions to several other segmentation prob-
lems, ranging from word segmentation (Goldwa-
ter et al, 2006), to parsing (Cohn et al, 2009; Post
and Gildea, 2009) and machine translation (DeN-
ero et al, 2008; Cohn and Blunsom, 2009; Liu
and Gildea, 2009). Segmentation is achieved by
introducing a prior bias towards grammars that are
compact representations of the data, namely by en-
forcing simplicity and sparsity: preferring simple
rules (smaller segments) unless the use of a com-
plex rule is evidenced by the data (through repeti-
tion), and thus mitigating the overfitting problem.
A Dirichlet process (DP) prior is typically used
to achieve this interplay. Interestingly, sampling-
based nonparametric inference further allows the
possibility of searching over the infinite space of
grammars (and, in machine translation, possible
word alignments), thus side-stepping the narrow-
ness problem outlined above as well.
In this work, we use an extension of the afore-
mentioned models of generative segmentation for
STSG induction, and describe an algorithm for
posterior inference under this model that is tai-
lored to the task of extractive sentence compres-
sion. This task is characterized by the availabil-
ity of word alignments, providing a clean testbed
for investigating the effects of grammar extraction.
We achieve substantial improvements against a
number of baselines including EM, support vector
machine (SVM) based discriminative training, and
variational Bayes (VB). By comparing our method
to a range of other methods that are subject dif-
ferentially to the two problems, we can show that
both play an important role in performance limi-
tations, and that our method helps address both as
well. Our results are thus not only encouraging for
grammar estimation using sparse priors but also il-
lustrate the merits of nonparametric inference over
the space of grammars as opposed to sparse para-
metric inference with a fixed grammar.
In the following, we define the task of extrac-
tive sentence compression and the Bayesian STSG
model, and algorithms we used for inference and
prediction. We then describe the experiments in
extractive sentence compression and present our
results in contrast with alternative algorithms. We
conclude by giving examples of compression pat-
terns learned by the Bayesian method.
2 Sentence compression
Sentence compression is the task of summarizing a
sentence while retaining most of the informational
content and remaining grammatical (Jing, 2000).
In extractive sentence compression, which we fo-
cus on in this paper, an order-preserving subset of
the words in the sentence are selected to form the
summary, that is, we summarize by deleting words
(Knight and Marcu, 2002). An example sentence
pair, which we use as a running example, is the
following:
? Like FaceLift, much of ATM?s screen perfor-
mance depends on the underlying applica-
tion.
? ATM?s screen performance depends on the
underlying application.
938
Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression.
where the underlined words were deleted. In su-
pervised sentence compression, the goal is to gen-
eralize from a parallel training corpus of sentences
(source) and their compressions (target) to unseen
sentences in a test set to predict their compres-
sions. An unsupervised setup also exists; meth-
ods for the unsupervised problem typically rely
on language models and linguistic/discourse con-
straints (Clarke and Lapata, 2006a; Turner and
Charniak, 2005). Because these methods rely on
dynamic programming to efficiently consider hy-
potheses over the space of all possible compres-
sions of a sentence, they may be harder to extend
to general paraphrasing.
3 The STSG Model
Synchronous tree-substitution grammar is a for-
malism for synchronously generating a pair of
non-isomorphic source and target trees (Eisner,
2003). Every grammar rule is a pair of elemen-
tary trees aligned at the leaf level at their frontier
nodes, which we will denote using the form
cs/ct ? es/et, ?
(indices s for source, t for target) where cs, ct are
root nonterminals of the elementary trees es, et re-
spectively and ? is a 1-to-1 correspondence be-
tween the frontier nodes in es and et. For example,
the rule
S / S? (S (PP (IN Like) NP[]) NP[1] VP[2]) /
(S NP[1] VP[2])
can be used to delete a subtree rooted at PP. We
use square bracketed indices to represent the align-
ment ? of frontier nodes ? NP[1] aligns with
NP[1], VP[2] aligns with VP[2], NP[] aligns with
the special symbol  denoting a deletion from the
source tree. Symmetrically -aligned target nodes
are used to represent insertions into the target tree.
Similarly, the rule
NP / ? (NP (NN FaceLift)) / 
can be used to continue deriving the deleted sub-
tree. See Figure 1 for an example of how an STSG
with these rules would operate in synchronously
generating our example sentence pair.
STSG is a convenient choice of formalism for
a number of reasons. First, it eliminates the iso-
morphism and strong independence assumptions
of SCFGs. Second, the ability to have rules deeper
than one level provides a principled way of model-
ing lexicalization, whose importance has been em-
phasized (Galley and McKeown, 2007; Yamangil
and Nelken, 2008). Third, we may have our STSG
operate on trees instead of sentences, which allows
for efficient parsing algorithms, as well as provid-
ing syntactic analyses for our predictions, which is
desirable for automatic evaluation purposes.
A straightforward extension of the popular EM
algorithm for probabilistic context free grammars
(PCFG), the inside-outside algorithm (Lari and
Young, 1990), can be used to estimate the rule
weights of a given unweighted STSG based on a
corpus of parallel parse trees t = t1, . . . , tN where
tn = tn,s/tn,t for n = 1, . . . , N . Similarly, an
939
Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a
target node (top row in blue), and split/merge a deletion rule via aligning with  (bottom row in red).
extension of the Viterbi algorithm is available for
finding the maximum probability derivation, use-
ful for predicting the target analysis tN+1,t for a
test instance tN+1,s. (Eisner, 2003) However, as
noted earlier, EM is subject to the narrowness and
overfitting problems.
3.1 The Bayesian generative process
Both of these issues can be addressed by taking
a nonparametric Bayesian approach, namely, as-
suming that the elementary tree pairs are sampled
from an independent collection of Dirichlet pro-
cess (DP) priors. We describe such a process for
sampling a corpus of tree pairs t.
For all pairs of root labels c = cs/ct that we
consider, where up to one of cs or ct can be  (e.g.,
S / S, NP / ), we sample a sparse discrete distribu-
tion Gc over infinitely many elementary tree pairs
e = es/et sharing the common root c from a DP
Gc ? DP(?c, P0(? | c)) (1)
where the DP has the concentration parameter ?c
controlling the sparsity of Gc, and the base dis-
tribution P0(? | c) is a distribution over novel el-
ementary tree pairs that we describe more fully
shortly.
We then sample a sequence of elementary tree
pairs to serve as a derivation for each observed de-
rived tree pair. For each n = 1, . . . , N , we sam-
ple elementary tree pairs en = en,1, . . . , en,dn in
a derivation sequence (where dn is the number of
rules used in the derivation), consulting Gc when-
ever an elementary tree pair with root c is to be
sampled.
e
iid
? Gc, for all e whose root label is c
Given the derivation sequence en, a tree pair tn is
determined, that is,
p(tn | en) =
{
1 en,1, . . . , en,dn derives tn
0 otherwise.
(2)
The hyperparameters ?c can be incorporated
into the generative model as random variables;
however, we opt to fix these at various constants
to investigate different levels of sparsity.
For the base distribution P0(? | c) there are a
variety of choices; we used the following simple
scenario. (We take c = cs/ct.)
Synchronous rules For the case where neither cs
nor ct are the special symbol , the base dis-
tribution first generates es and et indepen-
dently, and then samples an alignment be-
tween the frontier nodes. Given a nontermi-
nal, an elementary tree is generated by first
making a decision to expand the nontermi-
nal (with probability ?c) or to leave it as a
frontier node (1 ? ?c). If the decision to ex-
pand was made, we sample an appropriate
rule from a PCFG which we estimate ahead
940
of time from the training corpus. We expand
the nonterminal using this rule, and then re-
peat the same procedure for every child gen-
erated that is a nonterminal until there are no
generated nonterminal children left. This is
done independently for both es and et. Fi-
nally, we sample an alignment between the
frontier nodes uniformly at random out of all
possible alingments.
Deletion/insertion rules If ct = , that is, we
have a deletion rule, we need to generate
e = es/. (The insertion rule case is symmet-
ric.) The base distribution generates es using
the same process described for synchronous
rules above. Then with probability 1 we align
all frontier nodes in es with . In essence,
this process generates TSG rules, rather than
STSG rules, which are used to cover deleted
(or inserted) subtrees.
This simple base distribution does nothing to
enforce an alignment between the internal nodes
of es and et. One may come up with more sophis-
ticated base distributions. However the main point
of the base distribution is to encode a control-
lable preference towards simpler rules; we there-
fore make the simplest possible assumption.
3.2 Posterior inference via Gibbs sampling
Assuming fixed hyperparameters ? = {?c} and
? = {?c}, our inference problem is to find the
posterior distribution of the derivation sequences
e = e1, . . . , eN given the observations t =
t1, . . . , tN . Applying Bayes? rule, we have
p(e | t) ? p(t | e)p(e) (3)
where p(t | e) is a 0/1 distribution (2) which does
not depend on Gc, and p(e) can be obtained by
collapsing Gc for all c.
Consider repeatedly generating elementary tree
pairs e1, . . . , ei, all with the same root c, iid from
Gc. Integrating over Gc, the ei become depen-
dent. The conditional prior of the i-th elementary
tree pair given previously generated ones e<i =
e1, . . . , ei?1 is given by
p(ei | e<i) =
nei + ?cP0(ei | c)
i? 1 + ?c
(4)
where nei denotes the number of times ei occurs
in e<i. Since the collapsed model is exchangeable
in the ei, this formula forms the backbone of the
inference procedure that we describe next. It also
makes clear DP?s inductive bias to reuse elemen-
tary tree pairs.
We use Gibbs sampling (Geman and Geman,
1984), a Markov chain Monte Carlo (MCMC)
method, to sample from the posterior (3). A
derivation e of the corpus t is completely specified
by an alignment between the source nodes and the
corresponding target nodes (as well as  on either
side), which we take to be the state of the sampler.
We start at a random derivation of the corpus, and
at every iteration resample a derivation by amend-
ing the current one through local changes made
at the node level, in the style of Goldwater et al
(2006).
Our sampling updates are extensions of those
used by Cohn and Blunsom (2009) in MT, but are
tailored to our task of extractive sentence compres-
sion. In our task, no target node can align with
 (which would indicate a subtree insertion), and
barring unary branches no source node i can align
with two different target nodes j and j? at the same
time (indicating a tree expansion). Rather, the
configurations of interest are those in which only
source nodes i can align with , and two source
nodes i and i? can align with the same target node
j. Thus, the alignments of interest are not arbitrary
relations, but (partial) functions from nodes in es
to nodes in et or . We therefore sample in the
direction from source to target. In particular, we
visit every tree pair and each of its source nodes i,
and update its alignment by selecting between and
within two choices: (a) unaligned, (b) aligned with
some target node j or . The number of possibil-
ities j in (b) is significantly limited, firstly by the
word alignment (for instance, a source node dom-
inating a deleted subspan cannot be aligned with
a target node), and secondly by the current align-
ment of other nearby aligned source nodes. (See
Cohn and Blunsom (2009) for details of matching
spans under tree constraints.)2
2One reviewer was concerned that since we explicitly dis-
allow insertion rules in our sampling procedure, our model
that generates such rules wastes probability mass and is there-
fore ?deficient?. However, we regard sampling as a separate
step from the data generation process, in which we can for-
mulate more effective algorithms by using our domain knowl-
edge that our data set was created by annotators who were
instructed to delete words only. Also, disallowing insertion
rules in the base distribution unnecessarily complicates the
definition of the model, whereas it is straightforward to de-
fine the joint distribution of all (potentially useful) rules and
then use domain knowledge to constrain the support of that
distribution during inference, as we do here. In fact, it is pos-
941
More formally, let eM be the elementary tree
pair rooted at the closest aligned ancestor i? of
node i when it is unaligned; and let eA and eB
be the elementary tree pairs rooted at i? and i re-
spectively when i is aligned with some target node
j or . Then, by exchangeability of the elementary
trees sharing the same root label, and using (4), we
have
p(unalign) =
neM + ?cMP0(eM | cM )
ncM + ?cM
(5)
p(align with j) =
neA + ?cAP0(eA | cA)
ncA + ?cA
(6)
?
neB + ?cBP0(eB | cB)
ncB + ?cB
(7)
where the counts ne? , nc? are with respect to the
current derivation of the rest of the corpus; except
for neB , ncB we also make sure to account for hav-
ing generated eA. See Figure 2 for an illustration
of the sampling updates.
It is important to note that the sampler described
can move from any derivation to any other deriva-
tion with positive probability (if only, for example,
by virtue of fully merging and then resegment-
ing), which guarantees convergence to the poste-
rior (3). However some of these transition prob-
abilities can be extremely small due to passing
through low probability states with large elemen-
tary trees; in turn, the sampling procedure is prone
to local modes. In order to counteract this and to
improve mixing we used simulated annealing. The
probability mass function (5-7) was raised to the
power 1/T with T dropping linearly from T = 5
to T = 0. Furthermore, using a final tempera-
ture of zero, we recover a maximum a posteriori
(MAP) estimate which we denote eMAP.
3.3 Prediction
We discuss the problem of predicting a target tree
tN+1,t that corresponds to a source tree tN+1,s
unseen in the observed corpus t. The maximum
probability tree (MPT) can be found by consid-
ering all possible ways to derive it. However a
much simpler alternative is to choose the target
tree implied by the maximum probability deriva-
sible to prove that our approach is equivalent up to a rescaling
of the concentration parameters. Since we fit these parame-
ters to the data, our approach is equivalent.
tion (MPD), which we define as
e? = argmax
e
p(e | ts, t)
= argmax
e
?
e
p(e | ts, e)p(e | t)
where e denotes a derivation for t = ts/tt. (We
suppress the N + 1 subscripts for brevity.) We
approximate this objective first by substituting
?eMAP(e) for p(e | t) and secondly using a finite
STSG model for the infinite p(e | ts, eMAP), which
we obtain simply by normalizing the rule counts in
eMAP. We use dynamic programming for parsing
under this finite model (Eisner, 2003).3
Unfortunately, this approach does not ensure
that the test instances are parsable, since ts may
include unseen structure or novel words. A work-
around is to include all zero-count context free
copy rules such as
NP / NP? (NP NP[1] PP[2]) / (NP NP[1] PP[2])
NP / ? (NP NP[] PP[]) / 
in order to smooth our finite model. We used
Laplace smoothing (adding 1 to all counts) as it
gave us interpretable results.
4 Evaluation
We compared the Gibbs sampling compressor
(GS) against a version of maximum a posteriori
EM (with Dirichlet parameter greater than 1) and
a discriminative STSG based on SVM training
(Cohn and Lapata, 2008) (SVM). EM is a natural
benchmark, while SVM is also appropriate since
it can be taken as the state of the art for our task.4
We used a publicly available extractive sen-
tence compression corpus: the Broadcast News
compressions corpus (BNC) of Clarke and Lap-
ata (2006a). This corpus consists of 1370 sentence
pairs that were manually created from transcribed
Broadcast News stories. We split the pairs into
training, development, and testing sets of 1000,
3We experimented with MPT using Monte Carlo integra-
tion over possible derivations; the results were not signifi-
cantly different from those using MPD.
4The comparison system described by Cohn and Lapata
(2008) attempts to solve a more general problem than ours,
abstractive sentence compression. However, given the nature
of the data that we provided, it can only learn to compress
by deleting words. Since the system is less specialized to the
task, their model requires additional heuristics in decoding
not needed for extractive compression, which might cause a
reduction in performance. Nonetheless, because the compar-
ison system is a generalization of the extractive SVM com-
pressor of Cohn and Lapata (2007), we do not expect that the
results would differ qualitatively.
942
SVM EM GS
Precision 55.60 58.80 58.94
Recall 53.37 56.58 64.59
Relational F1 54.46 57.67 61.64
Compression rate 59.72 64.11 65.52
Table 1: Precision, recall, relational F1 and com-
pression rate (%) for various systems on the 200-
sentence BNC test set. The compression rate for
the gold standard was 65.67%.
SVM EM GS Gold
Grammar 2.75? 2.85? 3.69 4.25
Importance 2.85 2.67? 3.41 3.82
Comp. rate 68.18 64.07 67.97 62.34
Table 2: Average grammar and importance scores
for various systems on the 20-sentence subsam-
ple. Scores marked with ? are significantly dif-
ferent than the corresponding GS score at ? < .05
and with ? at ? < .01 according to post-hoc Tukey
tests. ANOVA was significant at p < .01 both for
grammar and importance.
170, and 200 pairs, respectively. The corpus was
parsed using the Stanford parser (Klein and Man-
ning, 2003).
In our experiments with the publicly available
SVM system we used all except paraphrasal rules
extracted from bilingual corpora (Cohn and Lap-
ata, 2008). The model chosen for testing had pa-
rameter for trade-off between training error and
margin set to C = 0.001, used margin rescaling,
and Hamming distance over bags of tokens with
brevity penalty for loss function. EM used a sub-
set of the rules extracted by SVM, namely all rules
except non-head deleting compression rules, and
was initialized uniformly. Each EM instance was
characterized by two parameters: ?, the smooth-
ing parameter for MAP-EM, and ?, the smooth-
ing parameter for augmenting the learned gram-
mar with rules extracted from unseen data (add-
(? ? 1) smoothing was used), both of which were
fit to the development set using grid-search over
(1, 2]. The model chosen for testing was (?, ?) =
(1.0001, 1.01).
GS was initialized at a random derivation. We
sampled the alignments of the source nodes in ran-
dom order. The sampler was run for 5000 itera-
tions with annealing. All hyperparameters ?c, ?c
were held constant at ?, ? for simplicity and were
fit using grid-search over ? ? [10?6, 106], ? ?
[10?3, 0.5]. The model chosen for testing was
(?, ?) = (100, 0.1).
As an automated metric of quality, we compute
F-score based on grammatical relations (relational
F1, or RelF1) (Riezler et al, 2003), by which the
consistency between the set of predicted grammat-
ical relations and those from the gold standard is
measured, which has been shown by Clarke and
Lapata (2006b) to correlate reliably with human
judgments. We also conducted a small human sub-
jective evaluation of the grammaticality and infor-
mativeness of the compressions generated by the
various methods.
4.1 Automated evaluation
For all three systems we obtained predictions for
the test set and used the Stanford parser to extract
grammatical relations from predicted trees and the
gold standard. We computed precision, recall,
RelF1 (all based on grammatical relations), and
compression rate (percentage of the words that are
retained), which we report in Table 1. The results
for GS are averages over five independent runs.
EM gives a strong baseline since it already uses
rules that are limited in depth and number of fron-
tier nodes by stipulation, helping with the overfit-
ting we have mentioned, surprisingly outperform-
ing its discriminative counterpart in both precision
and recall (and consequently RelF1). GS however
maintains the same level of precision as EM while
improving recall, bringing an overall improvement
in RelF1.
4.2 Human evaluation
We randomly subsampled our 200-sentence test
set for 20 sentences to be evaluated by human
judges through Amazon Mechanical Turk. We
asked 15 self-reported native English speakers for
their judgments of GS, EM, and SVM output sen-
tences and the gold standard in terms of grammat-
icality (how fluent the compression is) and impor-
tance (how much of the meaning of and impor-
tant information from the original sentence is re-
tained) on a scale of 1 (worst) to 5 (best). We re-
port in Table 2 the average scores. EM and SVM
perform at very similar levels, which we attribute
to using the same set of rules, while GS performs
at a level substantially better than both, and much
closer to human performance in both criteria. The
943
Figure 3: RelF1, precision, recall plotted against
compression rate for GS, EM, VB.
human evaluation indicates that the superiority of
the Bayesian nonparametric method is underap-
preciated by the automated evaluation metric.
4.3 Discussion
The fact that GS performs better than EM can be
attributed to two reasons: (1) GS uses a sparse
prior and selects a compact representation of the
data (grammar sizes ranged from 4K-7K for GS
compared to a grammar of about 35K rules for
EM). (2) GS does not commit to a precomputed
grammar and searches over the space of all gram-
mars to find one that bests represents the corpus.
It is possible to introduce DP-like sparsity in EM
using variational Bayes (VB) training. We exper-
iment with this next in order to understand how
dominant the two factors are. The VB algorithm
requires a simple update to the M-step formulas
for EM where the expected rule counts are normal-
ized, such that instead of updating the rule weight
in the t-th iteration as in the following
?t+1c,e =
nc,e + ?? 1
nc,. +K??K
where nc,e represents the expected count of rule
c ? e, and K is the total number of ways
to rewrite c, we now take into account our
DP(?c, P0(? | c)) prior in (1), which, when
truncated to a finite grammar, reduces to a
K-dimensional Dirichlet prior with parameter
?cP0(? | c). Thus in VB we perform a variational
E-step with the subprobabilities given by
?t+1c,e =
exp (?(nc,e + ?cP0(e | c)))
exp (?(nc,. + ?c))
where ? denotes the digamma function. (Liu and
Gildea, 2009) (See MacKay (1997) for details.)
Hyperparameters were handled the same way as
for GS.
Instead of selecting a single model on the devel-
opment set, here we provide the whole spectrum of
models and their performances in order to better
understand their comparative behavior. In Figure
3 we plot RelF1 on the test set versus compres-
sion rate and compare GS, EM, and VB (? = 0.1
fixed, (?, ?) ranging in [10?6, 106]?(1, 2]). Over-
all, we see that GS maintains roughly the same
level of precision as EM (despite its larger com-
pression rates) while achieving an improvement in
recall, consequently performing at a higher RelF1
level. We note that VB somewhat bridges the gap
between GS and EM, without quite reaching GS
performance. We conclude that the mitigation of
the two factors (narrowness and overfitting) both
contribute to the performance gain of GS.5
4.4 Example rules learned
In order to provide some insight into the grammar
extracted by GS, we list in Tables (3) and (4) high
5We have also experimented with VB with parametric in-
dependent symmetric Dirichlet priors. The results were sim-
ilar to EM with the exception of sparse priors resulting in
smaller grammars and slightly improving performance.
944
(ROOT (S CC[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[1] ADVP[] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S ADVP[] (, ,) NP[1] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[] (, ,) NP[1] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[] ,[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[] (VP VBP[] (SBAR (S NP[1] VP[2]))) .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S ADVP[] NP[1] (VP MD[2] VP[3]) .[4])) / (ROOT (S NP[1] (VP MD[2] VP[3]) .[4]))
(ROOT (S (SBAR (IN as) S[]) ,[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[] (, ,) CC[] (S NP[1] VP[2]) .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S PP[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[1] (, ,) CC[] S[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S S[] ,[] NP[1] ADVP[2] VP[3] .[4])) / (ROOT (S NP[1] ADVP[2] VP[3] .[4]))
(ROOT (S (NP (NP NNP[] (POS ?s)) NNP[1] NNP[2]) / (ROOT (S (NP NNP[1] NNP[2])
(VP (VBZ reports)) .[3])) (VP (VBZ reports)) .[3]))
Table 3: High probability ROOT / ROOT compression rules from the final state of the sampler.
(S NP[1] ADVP[] VP[2]) / (S NP[1] VP[2])
(S INTJ[] (, ,) NP[1] VP[2] (. .)) / (S NP[1] VP[2] (. .))
(S (INTJ (UH Well)) ,[] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S PP[] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
(S ADVP[] (, ,) S[1] (, ,) (CC but) S[2] .[3]) / (S S[1] (, ,) (CC but) S[2] .[3])
(S ADVP[] NP[1] VP[2]) / (S NP[1] VP[2])
(S NP[] (VP VBP[] (SBAR (IN that) (S NP[1] VP[2]))) (. .)) / (S NP[1] VP[2] (. .))
(S NP[] (VP VBZ[] ADJP[] SBAR[1])) / S[1]
(S CC[] PP[] (, ,) NP[1] VP[2] (. .)) / (S NP[1] VP[2] (. .))
(S NP[] (, ,) NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S NP[1] (, ,) ADVP[] (, ,) VP[2]) / (S NP[1] VP[2])
(S CC[] (NP PRP[1]) VP[2]) / (S (NP PRP[1]) VP[2])
(S ADVP[] ,[] PP[] ,[] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S ADVP[] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
Table 4: High probability S / S compression rules from the final state of the sampler.
probability subtree-deletion rules expanding cate-
gories ROOT / ROOT and S / S, respectively. Of
especial interest are deep lexicalized rules such as
a pattern of compression used many times in the
BNC in sentence pairs such as ?NPR?s Anne Gar-
rels reports? / ?Anne Garrels reports?. Such an
informative rule with nontrivial collocation (be-
tween the possessive marker and the word ?re-
ports?) would be hard to extract heuristically and
can only be extracted by reasoning across the
training examples.
5 Conclusion
We explored nonparametric Bayesian learning
of non-isomorphic tree mappings using Dirich-
let process priors. We used the task of extrac-
tive sentence compression as a testbed to investi-
gate the effects of sparse priors and nonparamet-
ric inference over the space of grammars. We
showed that, despite its degeneracy, expectation
maximization is a strong baseline when given a
reasonable grammar. However, Gibbs-sampling?
based nonparametric inference achieves improve-
ments against this baseline. Our investigation with
variational Bayes showed that the improvement is
due both to finding sparse grammars (mitigating
overfitting) and to searching over the space of all
grammars (mitigating narrowness). Overall, we
take these results as being encouraging for STSG
induction via Bayesian nonparametrics for mono-
lingual translation tasks. The future for this work
would involve natural extensions such as mixing
over the space of word alignments; this would al-
low application to MT-like tasks where flexible
word reordering is allowed, such as abstractive
sentence compression and paraphrasing.
References
James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the 21st Interna-
945
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 144?151, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 377?384, Sydney, Australia, July.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 352?361, Morristown, NJ,
USA. Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and on Computational Natural Lan-
guage Learning, pages 73?82, Prague. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In COLING
?08: Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 137?144,
Manchester, United Kingdom. Association for Com-
putational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548?556, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39 (Series B):1?38.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In StatMT ?06: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, pages 31?38, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP ?08: Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 314?323, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 205?208,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS, pages 3?10. MIT
Press.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the Inside-
Outside algorithm. Computer Speech and Lan-
guage, 4:35?56.
Ding Liu and Daniel Gildea. 2009. Bayesian learn-
ing of phrasal tree-to-string templates. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1308?1317,
Singapore, August. Association for Computational
Linguistics.
946
David J.C. MacKay. 1997. Ensemble learning for hid-
den markov models. Technical report, Cavendish
Laboratory, Cambridge, UK.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48, Suntec, Singapore, August. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 118?125, Morristown, NJ, USA.
Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 290?297, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Elif Yamangil and Rani Nelken. 2008. Mining
wikipedia revision histories for improving sentence
compression. In Proceedings of ACL-08: HLT,
Short Papers, pages 137?140, Columbus, Ohio,
June. Association for Computational Linguistics.
947
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 110?114,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Estimating Compact Yet Rich Tree Insertion Grammars
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
We present a Bayesian nonparametric model
for estimating tree insertion grammars (TIG),
building upon recent work in Bayesian in-
ference of tree substitution grammars (TSG)
via Dirichlet processes. Under our general
variant of TIG, grammars are estimated via
the Metropolis-Hastings algorithm that uses
a context free grammar transformation as a
proposal, which allows for cubic-time string
parsing as well as tree-wide joint sampling of
derivations in the spirit of Cohn and Blun-
som (2010). We use the Penn treebank for
our experiments and find that our proposal
Bayesian TIG model not only has competitive
parsing performance but also finds compact
yet linguistically rich TIG representations of
the data.
1 Introduction
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity ? to allow accurate modeling of the data
with sparse grammars ? and low complexity ?
making induction of the grammars and parsing of
novel sentences computationally practical. Recent
work that incorporated Dirichlet process (DP) non-
parametric models into TSGs has provided an effi-
cient solution to the problem of segmenting train-
ing data trees into elementary parse tree fragments
to form the grammar (Cohn et al, 2009; Cohn and
Blunsom, 2010; Post and Gildea, 2009). DP infer-
ence tackles this problem by exploring the space of
all possible segmentations of the data, in search for
fragments that are on the one hand large enough so
that they incorporate the useful dependencies, and
on the other small enough so that they recur and have
a chance to be useful in analyzing unseen data.
The elementary trees combined in a TSG are, in-
tuitively, primitives of the language, yet certain lin-
guistic phenomena (notably various forms of modifi-
cation) ?split them up?, preventing their reuse, lead-
ing to less sparse grammars than might be ideal.
For instance, imagine modeling the following set of
structures:
? [NP the [NN [NN [NN president] of the university] who
resigned yesterday]]
? [NP the [NN former [NN [NN president] of the univer-
sity]]]
? [NP the [NN [NN president] who resigned yesterday]]
A natural recurring structure here would be the
structure ?[NP the [NN president]]?, yet it occurs
not at all in the data.
TSGs are a special case of the more flexible gram-
mar formalism of tree adjoining grammar (TAG)
(Joshi et al, 1975). TAG augments TSG with an
adjunction operator and a set of auxiliary trees in
addition to the substitution operator and initial trees
of TSG, allowing for ?splicing in? of syntactic frag-
ments within trees. In the example, by augmenting a
TSG with an operation of adjunction, a grammar that
hypothesizes auxiliary trees corresponding to ad-
joining ?[NN former NN ]?, ?[NN NN of the uni-
versity]?, and ?[NN NN who resigned yesterday]?
would be able to reuse the basic structure ?[NP the
[NN president]]?.
Unfortunately, TAG?s expressivity comes at the
cost of greatly increased complexity. Parsing com-
plexity for unconstrained TAG scales as O(n6), im-
110
NP
DT NN
the president
NP
NP* SBAR
WHNP
S
who
NP
NP* PP
IN
NP
of
NN
JJ
NN*
former
Figure 1: Example TIG derivation of an NP constituent:
One left insertion (at NN) and two simultaneous right in-
sertions (at NP).
practical as compared to CFG and TSG?s O(n3). In
addition, the model selection problem for TAG is
significantly more complicated than for TSG since
one must reason about many more combinatorial op-
tions with two types of derivation operators.1 This
has led researchers to resort to heuristic grammar ex-
traction techniques (Chiang, 2000; Carreras et al,
2008) or using a very small number of grammar cat-
egories (Hwa, 1998).
Hwa (1998) first proposed to use tree-insertion
grammars (TIG), a kind of expressive compromise
between TSG and TAG, as a substrate on which to
build grammatical inference. TIG constrains the ad-
junction operation so that spliced-in material falls
completely to the left or completely to the right of
the splice point. By restricting the form of possible
auxiliary trees to only left or right auxiliary trees in
this way, TIG remains within the realm of context-
free formalisms (with cubic complexity) while still
modeling rich linguistic phenomena (Schabes and
Waters, 1995). Figure 1 depicts some examples of
TIG derivations.
Sharing the same intuitions, Shindo et al (2011)
have provided a previous attempt at combining TIG
and Bayesian nonparametric principles, albeit with
severe limitations. Their TIG variant (which we will
refer to as TIG0) is highly constrained in the follow-
ing ways.
1. The foot node in an auxiliary tree must be the immediate
child of the root node.
2. Only one adjunction can occur at a given node.
1This can be seen by the fact that tree-path languages under
TAG are context free, whereas they are regular for TSG. (Sch-
abes and Waters, 1995)
(a)
(b)
NP
NP
R
NP
L
NP
DT NN
the
NP
L
?
NN
president
NN
L
NN
R
NN
R
?
NP
NP*
PP
IN NP
of
?
NP
SBAR
WHNP
S
who
NP
R
NP
NP* PP
IN NP
of
NP
NP
R
?
NP
R
NP
NP* SBAR
WHNP
S
who
?
NP
R
NN
JJ
NN*
former ?
NN
L
Figure 2: TIG-to-TSG transform: (a) and (b) illus-
trate transformed TSG derivations for two different TIG
derivations of the same parse tree structure. The TIG
nodes where we illustrate the transformation are in bold.
(We suppress the rest of the transformational nodes.)
3. Even modeling multiple adjunction with root adjunction
is disallowed. There is thus no recursion possibility with
adjunction, no stacking of auxiliary trees.
4. As a consequence of the prior two constraints, no adjunc-
tion along the spines of auxiliary trees is allowed.
5. As a consequence of the first constraint, all nonterminals
along the spine of an auxiliary tree are identical.
In this paper we explore a Bayesian nonparamet-
ric model for estimating a far more expressive ver-
sion of TIG, and compare its performance against
TSG and the restricted TIG0 variant. Our more gen-
eral formulation avoids these limitations by support-
ing the following features and thus relaxing four of
the five restrictions of TIG0.
1. Auxiliary trees may have the foot node at depth greater
than one.2
2. Both left and right adjunctions may occur at the same
node.
3. Simultanous adjunction (that is, more than one left or
right adjunction per node) is allowed via root adjunction.
4. Adjunctions may occur along the spines of auxiliary trees.
The increased expressivity of our TIG variant is
motivated both linguistically and practically. From
a linguistic point of view: Deeper auxiliary trees can
help model large patterns of insertion and potential
correlations between lexical items that extend over
multiple levels of tree. Combining left and right
auxiliary trees can help model modifiers of the same
node from left and right (combination of adjectives
2Throughout the paper, we will refer to the depth of an aux-
iliary tree to indicate the length of its spine.
111
and relative clauses for instance). Simultaneous in-
sertion allows us to deal with multiple independent
modifiers for the same constituent (for example, a
series of adjectives). From a practical point of view,
we show that an induced TIG provides modeling
performance superior to TSG and comparable with
TIG0. However we show that the grammars we in-
duce are compact yet rich, in that they succinctly
represent complex linguistic structures.
2 Probabilistic Model
In the basic nonparametric TSG model, there is an
independent DP for every grammar category (such
as c = NP ), each of which uses a base distribution
P0 that generates an initial tree by making stepwise
decisions.
Ginitc ? DP(?
init
c , P
init
0 (? | c))
The canonical P0 uses a probabilistic CFG P? that
is fixed a priori to sample CFG rules top-down and
Bernoulli variables for determining where substitu-
tions should occur (Cohn et al, 2009; Cohn and
Blunsom, 2010).
We extend this model by adding specialized DPs
for left and right auxiliary trees.3
Grightc ? DP(?
right
c , P
right
0 (? | c))
Therefore, we have an exchangeable process for
generating right auxiliary trees
p(aj | a<j) =
naj + ?
right
c P
right
0 (aj | c)
j ? 1 + ?rightc
(1)
as for initial trees in TSG.
We must define three distinct base distributions
for initial trees, left auxiliary trees, and right aux-
iliary trees. P init0 generates an initial tree with root
label c by sampling CFG rules from P? and making
a binary decision at every node generated whether
to leave it as a frontier node or further expand (with
probability ?c) (Cohn et al, 2009). Similarly, our
P right0 generates a right auxiliary tree with root la-
bel c by first making a binary decision whether to
generate an immediate foot or not (with probability
?rightc ), and then sampling an appropriate CFG rule
3We use right insertions for illustration; the symmetric ana-
log applies to left insertions.
(VP (, ,) (VP PP (VP (, ,) VP*)))
(VP (SBAR (WHADVP (WRB (WRB When) ) ) S) (VP (, ,) VP*))
(VP (PP (IN For) (NP NN )) (VP (, ,) VP*))
(VP (CC But) (VP PP (VP (, ,) VP*)))
(VP ADVP (VP (, ,) VP*))
(IN (ADVP (RB (RB particularly) ) ) IN*)
(NP PP (NP (CC and) (NP PP NP*)))
Figure 3: Example left auxiliary trees that occur in the
top derivations for Section 23. Simultaneous insertions
occur most frequently for the labels VP (85 times), NNS
(21 times), NNP (14 times).
from P? . For the right child, we sample an initial tree
from P init0 . For the left child, if decision to gener-
ate an immediate foot was made, we generate a foot
node, and stop. Otherwise we recur into P right0 which
generates a right auxiliary tree that becomes the left
child.
We bring together these three sets of processes
via a set of insertion parameters ?leftc , ?
right
c . In any
derivation, for every initial tree node labelled c (ex-
cept for frontier nodes) we determine whether or
not there are insertions at this node by sampling a
Bernoulli(?leftc ) distributed left insertion variable and
a Bernoulli(?rightc ) distributed right insertion vari-
able. For left auxiliary trees, we treat the nodes that
are not along the spine of the auxiliary tree the same
way we treat initial tree nodes, however for nodes
that are along the spine (including root nodes, ex-
cluding foot nodes) we consider only left insertions
by sampling the left insertion variable (symmetri-
cally for right insertions).
3 Inference
Given this model, our inference task is to explore
optimal derivations underlying the data. Since TIG
derivations are highly structured objects, a basic
sampling strategy based on local node-level moves
such as Gibbs sampling (Geman and Geman, 1984)
would not hold much promise. Following previ-
ous work, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al, 2011). This is achieved by
proposing derivations from an approximating distri-
bution and stochastically correcting via accept/reject
to achieve convergence into the correct posterior
(Johnson et al, 2007).
Since our base distributions factorize over levels
of tree, CFG is the most convenient choice for a
112
CFG rule CFG probability
Base distribution: P init0
NP? NPinit ?initc /(n
init
NP + ?
init
c )
NPinit? NPL NP
init NPR 1.0
NPinit? DT NN P? (NP? DT NN)? (1? ?DT)? (1? ?NN)
NPinit? DT NNinit P? (NP? DT NN)? (1? ?DT)? ?NN
NPinit? DTinit NN P? (NP? DT NN)? ?DT ? (1? ?NN)
NPinit? DTinit NNinit P? (NP? DT NN)? ?DT ? ?NN
Base distribution: P right0
NPR? NP
right ?rightNP ?
(
?rightc /(n
right
NP + ?
right
c )
)
NPR?  1? ?
right
NP
NPright? NPright NPR 1.0
NPright? NP* SBAR
init P? (NP? NP SBAR | NP? NP )
?(1? ?rightNP )? (1? ?SBAR)
NPright? NP* SBAR P? (NP? NP SBAR | NP? NP )
?(1? ?rightNP )? ?SBAR
NPright? NPright SBARinit P? (NP? NP SBAR | NP? NP )
??rightNP ? (1? ?SBAR)
NPright? NPright SBAR P? (NP? NP SBAR | NP? NP )
??rightNP ? ?SBAR
Figure 4: Transformation CFG rules that represent infi-
nite base distributions. P init0 is taken from Cohn and Blun-
som (2010). Underscored labels (such as NPright as op-
posed to NPright) are used to differentiate the pre-insertion
nodes in Figure 2 from the post-insertion ones. P left0 rules
are omitted for brevity and mirror the P right0 rules above.
Model FMeasure # Initial Trees # Auxiliary Trees (# Left)
TSG 77.51 6.2K -
TIG0 78.46 6.0K 251 (137)
TIG 78.62 5.6K 604 (334)
Figure 5: EVALB results after training on Section 2 and
testing on Section 23. Note that TIG finds a compact yet
rich representation. Elementary tree counts are based on
ones with count > 1.
proposal distribution. Fortunately, Schabes and Wa-
ters (1995) provide an (exact) transformation from a
fully general TIG into a TSG that generates the same
string languages. It is then straightforward to repre-
sent this TSG as a CFG using the Goodman trans-
form (Goodman, 2002; Cohn and Blunsom, 2010).
Figure 4 lists the additional CFG productions we
have designed, as well as the rules used that trigger
them.
4 Evaluation Results
We use the standard Penn treebank methodology of
training on sections 2?21 and testing on section 23.
All our data is head-binarized and words occurring
only once are mapped into unknown categories of
the Berkeley parser. As has become standard, we
carried out a small treebank experiment where we
train on Section 2, and a large one where we train
on the full training set. All hyperparameters are re-
sampled under appropriate vague gamma and beta
priors. All reported numbers are averages over three
runs. Parsing results are based on the maximum
probability parse which was obtained by sampling
derivations under the transform CFG.
We compare our system (referred to as TIG) to
our implementation of the TSG system of (Cohn
and Blunsom, 2010) (referred to as TSG) and the
constrained TIG variant of (Shindo et al, 2011) (re-
ferred to as TIG0). The upshot of our experiments is
that, while on the large training set al models have
similar performance (85.6, 85.3, 85.4 for TSG, TIG0
and TIG respectively), on the small dataset inser-
tion helps nonparametric model to find more com-
pact and generalizable representations for the data,
which affects parsing performance (Figure 4). Al-
though TIG0 has performance close to TIG, note that
TIG achieves this performance using a more suc-
cinct representation and extracting a rich set of aux-
iliary trees. As a result, TIG finds many chances to
apply insertions to test sentences, whereas TIG0 de-
pends mostly on TSG rules. If we look at the most
likely derivations for the test data, TIG0 assigns 663
insertions (351 left insertions) in the parsing of en-
tire Section 23, meanwhile TIG assigns 3924 (2100
left insertions). Some of these linguistically sophis-
ticated auxiliary trees that apply to test data are listed
in Figure 3.
5 Conclusion
We described a nonparametric Bayesian inference
scheme for estimating TIG grammars and showed
the power of TIG formalism over TSG for returning
rich, generalizable, yet compact representations of
data. The nonparametric inference scheme presents
a principled way of addressing the difficult model
selection problem with TIG which has been pro-
hibitive in this area of research. TIG still remains
within context free and both our sampling and pars-
ing techniques are highly scalable.
Acknowledgements
The first author was supported in part by a Google
PhD Fellowship in Natural Language Processing.
113
References
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 9?16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
456?463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 225?230, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ?09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 548?556, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721?741.
J. Goodman. 2002. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 17th international conference on Com-
putational linguistics - Volume 1, pages 557?563, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136?163.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 45?48, Suntec, Singapore, August. Association
for Computational Linguistics.
Remko Scha and Rens Bod. 2003. Efficient parsing of
DOP with PCFG-reductions, October.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: a cubic-time parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21:479?513, De-
cember.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitution
grammars. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ?11, pages 206?211, Stroudsburg, PA, USA.
Association for Computational Linguistics.
114
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Context Free TAG Variant
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Elif Yamangil
Harvard University
Cambridge, MA
elif@eecs.harvard.edu
Stuart Shieber
Harvard University
Cambridge, MA
shieber@eecs.harvard.edu
Abstract
We propose a new variant of Tree-
Adjoining Grammar that allows adjunc-
tion of full wrapping trees but still bears
only context-free expressivity. We provide
a transformation to context-free form, and
a further reduction in probabilistic model
size through factorization and pooling of
parameters. This collapsed context-free
form is used to implement efficient gram-
mar estimation and parsing algorithms.
We perform parsing experiments the Penn
Treebank and draw comparisons to Tree-
Substitution Grammars and between dif-
ferent variations in probabilistic model de-
sign. Examination of the most probable
derivations reveals examples of the lin-
guistically relevant structure that our vari-
ant makes possible.
1 Introduction
While it is widely accepted that natural language
is not context-free, practical limitations of ex-
isting algorithms motivate Context-Free Gram-
mars (CFGs) as a good balance between model-
ing power and asymptotic performance (Charniak,
1996). In constituent-based parsing work, the pre-
vailing technique to combat this divide between
efficient models and real world data has been to
selectively strengthen the dependencies in a CFG
by increasing the grammar size through methods
such as symbol refinement (Petrov et al, 2006).
Another approach is to employ a more power-
ful grammatical formalism and devise constraints
and transformations that allow use of essential ef-
ficient algorithms such as the Inside-Outside al-
gorithm (Lari and Young, 1990) and CYK pars-
ing. Tree-Adjoining Grammar (TAG) is a natural
starting point for such methods as it is the canoni-
cal member of the mildly context-sensitive family,
falling just above CFGs in the hierarchy of for-
mal grammars. TAG has a crucial advantage over
CFGs in its ability to represent long distance in-
teractions in the face of the interposing variations
that commonly manifest in natural language (Joshi
and Schabes, 1997). Consider, for example, the
sentences
These pretzels are making me thirsty.
These pretzels are not making me thirsty.
These pretzels that I ate are making me thirsty.
Using a context-free language model with
proper phrase bracketing, the connection between
the words pretzels and thirsty must be recorded
with three separate patterns, which can lead to
poor generalizability and unreliable sparse fre-
quency estimates in probabilistic models. While
these problems can be overcome to some extent
with large amounts of data, redundant representa-
tion of patterns is particularly undesirable for sys-
tems that seek to extract coherent and concise in-
formation from text.
TAG allows a linguistically motivated treatment
of the example sentences above by generating the
last two sentences through modification of the
first, applying operations corresponding to nega-
tion and the use of a subordinate clause. Un-
fortunately, the added expressive power of TAG
comes with O(n6) time complexity for essential
algorithms on sentences of length n, as opposed to
O(n3) for the CFG (Schabes, 1990). This makes
TAG infeasible to analyze real world data in a rea-
sonable time frame.
In this paper, we define OSTAG, a new way to
constrain TAG in a conceptually simple way so
302
SNP VP NP
NP
DT
the
NN
lack
NP
NNS
computers
VP
VBP
do
RB
not
VP
NP
NP PP
NP
PRP
I
PP
IN
of
PRP
them
VP
VB
fear
Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the
sentences from a quote of Isaac Asimov?s - ?I do not fear computers. I fear the lack of them.?
that it can be reduced to a CFG, allowing the use of
traditional cubic-time algorithms. The reduction is
reversible, so that the original TAG derivation can
be recovered exactly from the CFG parse. We pro-
vide this reduction in detail below and highlight
the compression afforded by this TAG variant on
synthetic formal languages.
We evaluate OSTAG on the familiar task of
parsing the Penn Treebank. Using an automati-
cally induced Tree-Substitution Grammar (TSG),
we heuristically extract an OSTAG and estimate
its parameters from data using models with var-
ious reduced probabilistic models of adjunction.
We contrast these models and investigate the use
of adjunction in the most probable derivations of
the test corpus, demonstating the superior model-
ing performance of OSTAG over TSG.
2 TAG and Variants
Here we provide a short history of the relevant
work in related grammar formalisms, leading up
to a definition of OSTAG. We start with context-
free grammars, the components of which are
?N,T,R, S?, where N and T are the sets of non-
terminal and terminal symbols respectively, and S
is a distinguished nonterminal, the start symbol.
The rules R can be thought of as elementary trees
of depth 1, which are combined by substituting a
derived tree rooted at a nonterminalX at some leaf
node in an elementary tree with a frontier node
labeled with that same nonterminal. The derived
trees rooted at the start symbol S are taken to be
the trees generated by the grammar.
2.1 Tree-Substitution Grammar
By generalizing CFG to allow elementary trees in
R to be of depth greater than or equal to 1, we
get the Tree-Substitution Grammar. TSG remains
in the family of context-free grammars, as can be
easily seen by the removal of the internal nodes
in all elementary trees; what is left is a CFG that
generates the same language. As a reversible al-
ternative that preserves the internal structure, an-
notation of each internal node with a unique index
creates a large number of deterministic CFG rules
that record the structure of the original elementary
trees. A more compact CFG representation can be
obtained by marking each node in each elemen-
tary tree with a signature of its subtree. This trans-
form, presented by Goodman (2003), can rein in
the grammar constant G, as the crucial CFG algo-
rithms for a sentence of length n have complexity
O(Gn3).
A simple probabilistic model for a TSG is a set
of multinomials, one for each nonterminal in N
corresponding to its possible substitutions in R. A
more flexible model allows a potentially infinite
number of substitution rules using a Dirichlet Pro-
cess (Cohn et al, 2009; Cohn and Blunsom, 2010).
This model has proven effective for grammar in-
duction via Markov Chain Monte Carlo (MCMC),
in which TSG derivations of the training set are re-
peatedly sampled to find frequently occurring el-
ementary trees. A straightforward technique for
induction of a finite TSG is to perform this non-
parametric induction and select the set of rules that
appear in at least one sampled derivation at one or
several of the final iterations.
2.2 Tree-Adjoining Grammar
Tree-adjoining grammar (TAG) (Joshi, 1985;
Joshi, 1987; Joshi and Schabes, 1997) is an exten-
sion of TSG defined by a tuple ?N,T,R,A, S?,
and differs from TSG only in the addition of a
303
VP
always VP
VP* quickly
+ S
NP VP
runs
? S
NP VP
always VP
VP
runs
quickly
Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)
to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is
denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node
only, as it is along the spine of the auxiliary tree.
set of auxiliary trees A and the adjunction oper-
ation that governs their use. An auxiliary tree ?
is an elementary tree containing a single distin-
guished nonterminal leaf, the foot node, with the
same symbol as the root of ?. An auxiliary tree
with root and foot node X can be adjoined into an
internal node of an elementary tree labeled with
X by splicing the auxiliary tree in at that internal
node, as pictured in Figure 2. We refer to the path
between the root and foot nodes in an auxiliary
tree as the spine of the tree.
As mentioned above, the added power afforded
by adjunction comes at a serious price in time
complexity. As such, probabilistic modeling for
TAG in its original form is uncommon. However,
a large effort in non-probabilistic grammar induc-
tion has been performed through manual annota-
tion with the XTAG project(Doran et al, 1994).
2.3 Tree Insertion Grammar
Tree Insertion Grammars (TIGs) are a longstand-
ing compromise between the intuitive expressivity
of TAG and the algorithmic simplicity of CFGs.
Schabes and Waters (1995) showed that by re-
stricting the form of the auxiliary trees in A and
the set of auxiliary trees that may adjoin at par-
ticular nodes, a TAG generates only context-free
languages. The TIG restriction on auxiliary trees
states that the foot node must occur as either the
leftmost or rightmost leaf node. This introduces
an important distinction between left, right, and
wrapping auxiliary trees, of which only the first
two are allowed in TIG. Furthermore, TIG disal-
lows adjunction of left auxiliary trees on the spines
of right auxiliary trees, and vice versa. This is
to prevent the construction of wrapping auxiliary
trees, whose removal is essential for the simplified
complexity of TIG.
Several probabilistic models have been pro-
posed for TIG. While earlier approaches such as
Hwa (1998) and Chiang (2000) relied on hueristic
induction methods, they were nevertheless sucess-
ful at parsing. Later approaches (Shindo et al,
2011; Yamangil and Shieber, 2012) were able to
extend the non-parametric modeling of TSGs to
TIG, providing methods for both modeling and
grammar induction.
2.4 OSTAG
Our new TAG variant is extremely simple. We al-
low arbitrary initial and auxiliary trees, and place
only one restriction on adjunction: we disallow
adjunction at any node on the spine of an aux-
iliary tree below the root (though we discuss re-
laxing that constraint in Section 4.2). We refer to
this variant as Off Spine TAG (OSTAG) and note
that it allows the use of full wrapping rules, which
are forbidden in TIG. This targeted blocking of
recursion has similar motivations and benefits to
the approximation of CFGs with regular languages
(Mohri and jan Nederhof, 2000).
The following sections discuss in detail the
context-free nature of OSTAG and alternative
probabilistic models for its equivalent CFG form.
We propose a simple but empirically effective
heuristic for grammar induction for our experi-
ments on Penn Treebank data.
3 Transformation to CFG
To demonstrate that OSTAG has only context-
free power, we provide a reduction to context-free
grammar. Given an OSTAG ?N,T,R,A, S?, we
define the set N of nodes of the corresponding
CFG to be pairs of a tree inR orA together with an
304
?: S
T
x
T
y
?: T
a T* a
?: T
b T* b
S ? X Y S ? X Y
X ? x X ? x
Y ? y Y ? y
X ? A
X ? B
Y ? A?
Y ? B?
A ? a X ? a X ? a X a
A? ? a Y ? a Y ? a Y a
X ? ? X
Y ? ? Y
B ? b X ?? b X ? b X b
B? ? b Y ?? b Y ? b Y b
X ?? ? X
Y ?? ? Y
(a) (b) (c)
Figure 3: (a) OSTAG for the language wxwRvyvR where w, v ? {a|b}+ and R reverses a string. (b) A
CFG for the same language, which of necessity must distinguish between nonterminalsX and Y playing
the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish
between X and Y .
address (Gorn number (Gorn, 1965)) in that tree.
We take the nonterminals of the target CFG gram-
mar to be nodes or pairs of nodes, elements of the
setN +N ?N . We notate the pairs of nodes with
a kind of ?applicative? notation. Given two nodes
? and ??, we notate a target nonterminal as ?(??).
Now for each tree ? and each interior node ?
in ? that is not on the spine of ? , with children
?1, . . . , ?k, we add a context-free rule to the gram-
mar
? ? ?1 ? ? ? ?k (1)
and if interior node ? is on the spine of ? with
?s the child node also on the spine of ? (that is,
dominating the foot node of ? ) and ?? is a node (in
any tree) where ? is adjoinable, we add a rule
?(??)? ?1 ? ? ? ?s(??) ? ? ? ?k . (2)
Rules of type (1) handle the expansion of a node
not on the spine of an auxiliary tree and rules of
type (2) a spinal node.
In addition, to initiate adjunction at any node ??
where a tree ? with root ? is adjoinable, we use a
rule
?? ? ?(??) (3)
and for the foot node ?f of ? , we use a rule
?f (?)? ? (4)
The OSTAG constraint follows immediately
from the structure of the rules of type (2). Any
child spine node ?s manifests as a CFG nonter-
minal ?s(??). If child spine nodes themselves al-
lowed adjunction, we would need a type (3) rule
of the form ?s(??) ? ?s(??)(???). This rule itself
would feed adjunction, requiring further stacking
of nodes, and an infinite set of CFG nonterminals
and rules. This echoes exactly the stacking found
in the LIG reduction of TAG .
To handle substitution, any frontier node ? that
allows substitution of a tree rooted with node ??
engenders a rule
? ? ?? (5)
This transformation is reversible, which is to
say that each parse tree derived with this CFG im-
plies exactly one OSTAG derivation, with substi-
tutions and adjunctions coded by rules of type (5)
and (3) respectively. Depending on the definition
of a TAG derivation, however, the converse is not
necessarily true. This arises from the spurious am-
biguity between adjunction at a substitution site
(before applying a type (5) rule) versus the same
adjunction at the root of the substituted initial tree
(after applying a type (5) rule). These choices
lead to different derivations in CFG form, but their
TAG derivations can be considered conceptually
305
identical. To avoid double-counting derivations,
which can adversely effect probabilistic modeling,
type (3) and type (4) rules in which the side with
the unapplied symbol is a nonterminal leaf can be
omitted.
3.1 Example
The grammar of Figure 3(a) can be converted to
a CFG by this method. We indicate for each CFG
rule its type as defined above the production arrow.
All types are used save type (5), as substitution
is not employed in this example. For the initial
tree ?, we have the following generated rules (with
nodes notated by the tree name and a Gorn number
subscript):
? 1?? ?1 ?2 ?1 3?? ?(?1)
?1 1?? x ?1 3?? ?(?1)
?2 1?? y ?2 3?? ?(?2)
?2 3?? ?(?2)
For the auxiliary trees ? and ? we have:
?(?1) 2?? a ?1(?1) a
?(?2) 2?? a ?1(?2) a
?1(?1) 4?? ?1
?1(?2) 4?? ?2
?(?1) 2?? b ?1(?1) b
?(?2) 2?? b ?1(?2) b
?1(?1) 4?? ?1
?1(?2) 4?? ?2
The grammar of Figure 3(b) is simply a renaming
of this grammar.
4 Applications
4.1 Compact grammars
The OSTAG framework provides some leverage in
expressing particular context-free languages more
compactly than a CFG or even a TSG can. As
an example, consider the language of bracketed
palindromes
Pal = aiw aiwR ai
1 ? i ? k
w ? {bj | 1 ? j ? m}?
containing strings like a2 b1b3 a2 b3b1 a2. Any
TSG for this language must include as substrings
some subpalindrome constituents for long enough
strings. Whatever nonterminal covers such a
string, it must be specific to the a index within
it, and must introduce at least one pair of bs as
well. Thus, there are at least m such nontermi-
nals, each introducing at least k rules, requiring at
least km rules overall. The simplest such gram-
mar, expressed as a CFG, is in Figure 4(a). The
ability to use adjunction allows expression of the
same language as an OSTAG with k +m elemen-
tary trees (Figure 4(b)). This example shows that
an OSTAG can be quadratically smaller than the
corresponding TSG or CFG.
4.2 Extensions
The technique in OSTAG can be extended to ex-
pand its expressiveness without increasing gener-
ative capacity.
First, OSTAG allows zero adjunctions on each
node on the spine below the root of an auxiliary
tree, but any non-zero finite bound on the num-
ber of adjunctions allowed on-spine would simi-
larly limit generative capacity. The tradeoff is in
the grammar constant of the effective probabilis-
tic CFG; an extension that allows k levels of on
spine adjunction has a grammar constant that is
O(|N |(k+2)).
Second, the OSTAG form of adjunction is con-
sistent with the TIG form. That is, we can extend
OSTAG by allowing on-spine adjunction of left- or
right-auxiliary trees in keeping with the TIG con-
straints without increasing generative capacity.
4.3 Probabilistic OSTAG
One major motivation for adherence to a context-
free grammar formalism is the ability to employ
algorithms designed for probabilistic CFGs such
as the CYK algorithm for parsing or the Inside-
Outside algorithm for grammar estimation. In this
section we present a probabilistic model for an OS-
TAG grammar in PCFG form that can be used in
such algorithms, and show that many parameters
of this PCFG can be pooled or set equal to one and
ignored. References to rules of types (1-5) below
refer to the CFG transformation rules defined in
Section 3. While in the preceeding discussion we
used Gorn numbers for clarity, our discussion ap-
plies equally well for the Goodman transform dis-
cussed above, in which each node is labeled with a
signature of its subtree. This simply redefines ? in
the CFG reduction described in Section 3 to be a
subtree indicator, and dramatically reduces redun-
dancy in the generated grammar.
306
S ? ai Ti ai
Ti ? bj Ti bj
Ti ? ai
?i | 1 ? i ? k: S
ai T
ai
ai
?j | 1 ? j ? m: T
bj T* bj
(a) (b)
Figure 4: A CFG (a) and more compact OSTAG (b) for the language Pal
The first practical consideration is that CFG
rules of type (2) are deterministic, and as such
we need only record the rule itself and no asso-
ciated parameter. Furthermore, these rules employ
a template in which the stored symbol appears in
the left-hand side and in exactly one symbol on
the right-hand side where the spine of the auxil-
iary tree proceeds. One deterministic rule exists
for this template applied to each ?, and so we may
record only the template. In order to perform CYK
or IO, it is not even necessary to record the index
in the right-hand side where the spine continues;
these algorithms fill a chart bottom up and we can
simply propagate the stored nonterminal up in the
chart.
CFG rules of type (4) are also deterministic and
do not require parameters. In these cases it is not
necessary to record the rules, as they all have ex-
actly the same form. All that is required is a check
that a given symbol is adjoinable, which is true for
all symbols except nonterminal leaves and applied
symbols. Rules of type (5) are necessary to cap-
ture the probability of substitution and so we will
require a parameter for each.
At first glance, it would seem that due to the
identical domain of the left-hand sides of rules of
types (1) and (3) a parameter is required for each
such rule. To avoid this we propose the follow-
ing factorization for the probabilistic expansion of
an off spine node. First, a decision is made as to
whether a type (1) or (3) rule will be used; this cor-
responds to deciding if adjunction will or will not
take place at the node. If adjunction is rejected,
then there is only one type (1) rule available, and
so parameterization of type (1) rules is unneces-
sary. If we decide on adjunction, one of the avail-
able type (3) rules is chosen from a multinomial.
By conditioning the probability of adjunction on
varying amounts of information about the node,
alternative models can easily be defined.
5 Experiments
As a proof of concept, we investigate OSTAG in
the context of the classic Penn Treebank statistical
parsing setup; training on section 2-21 and testing
on section 23. For preprocessing, words that oc-
cur only once in the training data are mapped to
the unknown categories employed in the parser of
Petrov et al (2006). We also applied the annota-
tion from Klein and Manning (2003) that appends
?-U? to each nonterminal node with a single child,
drastically reducing the presence of looping unary
chains. This allows the use of a coarse to fine
parsing strategy (Charniak et al, 2006) in which
a sentence is first parsed with the Maximum Like-
lihood PCFG and only constituents whose prob-
ability exceeds a cutoff of 10?4 are allowed in
the OSTAG chart. Designed to facilitate sister ad-
junction, we define our binarization scheme by ex-
ample in which the added nodes, indicated by @,
record both the parent and head child of the rule.
NP
@NN-NP
@NN-NP
DT @NN-NP
JJ NN
SBAR
A compact TSG can be obtained automatically
using the MCMC grammar induction technique of
Cohn and Blunsom (2010), retaining all TSG rules
that appear in at least one derivation in after 1000
iterations of sampling. We use EM to estimate the
parameters of this grammar on sections 2-21, and
use this as our baseline.
To generate a set of TAG rules, we consider
each rule in our baseline TSG and find all possi-
307
All 40 #Adj #Wrap
TSG 85.00 86.08 ? ?
TSG? 85.12 86.21 ? ?
OSTAG1 85.42 86.43 1336 52
OSTAG2 85.54 86.56 1952 44
OSTAG3 85.86 86.84 3585 41
Figure 5: Parsing F-Score for the models under
comparison for both the full test set and sentences
of length 40 or less. For the OSTAG models, we
list the number of adjunctions in the MPD of the
full test set, as well as the number of wrapping
adjunctions.
ble auxiliary root and foot node pairs it contains.
For each such root/foot pair, we include the TAG
rule implied by removal of the structure above the
root and below the foot. We also include the TSG
rule left behind when the adjunction of this auxil-
iary tree is removed. To be sure that experimental
gains are not due to this increased number of TSG
initial trees, we calculate parameters using EM for
this expanded TSG and use it as a second base-
line (TSG?). With our full set of initial and aux-
iliary trees, we use EM and the PCFG reduction
described above to estimate the parameters of an
OSTAG.
We investigate three models for the probabil-
ity of adjunction at a node. The first uses a con-
servative number of parameters, with a Bernoulli
variable for each symbol (OSTAG1). The second
employs more parameters, conditioning on both
the node?s symbol and the symbol of its leftmost
child (OSTAG2).The third is highly parameterized
but most prone to data sparsity, with a separate
Bernoulli distribution for each Goodman index ?
(OSTAG3). We report results for Most Probable
Derivation (MPD) parses of section 23 in Figure
5.
Our results show that OSTAG outperforms both
baselines. Furthermore, the various parameteri-
zations of adjunction with OSTAG indicate that,
at least in the case of the Penn Treebank, the
finer grained modeling of a full table of adjunction
probabilities for each Goodman index OSTAG3
overcomes the danger of sparse data estimates.
Not only does such a model lead to better parsing
performance, but it uses adjunction more exten-
sively than its more lightly parameterized alterna-
tives. While different representations make direct
comparison inappropriate, the OSTAG results lie
in the same range as previous work with statistical
TIG on this task, such as Chiang (2000) (86.00)
and Shindo et al (2011) (85.03).
The OSTAG constraint can be relaxed as de-
scribed in Section 4.2 to allow any finite number of
on-spine adjunctions without sacrificing context-
free form. However, the increase to the grammar
constant quickly makes parsing with such models
an arduous task. To determine the effect of such a
relaxation, we allow a single level of on-spine ad-
junction using the adjunction model of OSTAG1,
and estimate this model with EM on the training
data. We parse sentences of length 40 or less in
section 23 and observe that on-spine adjunction is
never used in the MPD parses. This suggests that
the OSTAG constraint is reasonable, at least for
the domain of English news text.
We performed further examination of the MPD
using OSTAG for each of the sentences in the test
corpus. As an artifact of the English language, the
majority have their foot node on the left spine and
would also be usable by TIG, and so we discuss
the instances of wrapping auxiliary trees in these
derivations that are uniquely available to OSTAG.
We remove binarization for clarity and denote the
foot node with an asterisk.
A frequent use of wrapping adjunction is to co-
ordinate symbols such as quotes, parentheses, and
dashes on both sides of a noun phrase. One com-
mon wrapping auxiliary tree in our experiments is
NP
? NP* ? PP
This is used frequently in the news text of
the Wall Street Journal for reported speech when
avoiding a full quotation. This sentence is an ex-
ample of the way the rule is employed, using what
Joshi and Schabes (1997) referred to as ?factoring
recursion from linguistic constraints? with TAG.
Note that replacing the quoted noun phrase and
its following prepositional phrase with the noun
phrase itself yields a valid sentence, in line with
the linguistic theory underlying TAG.
Another frequent wrapping rule, shown below,
allows direct coordination between the contents of
an appositive with the rest of the sentence.
308
NP
NP , CC
or
NP* ,
This is a valuable ability, as it is common to
use an appositive to provide context or explanation
for a proper noun. As our information on proper
nouns will most likely be very sparse, the apposi-
tive may be more reliably connected to the rest of
the sentence. An example of this from one of the
sentences in which this rule appears in the MPD is
the phrase ?since the market fell 156.83, or 8 %,
a week after Black Monday?. The wrapping rule
allows us to coordinate the verb ?fell? with the pat-
tern ?X %? instead of 156.83, which is mapped to
an unknown word category.
These rules highlight the linguistic intuitions
that back TAG; if their adjunction were undone,
the remaining derivation would be a valid sen-
tence that simply lacks the modifying structure of
the auxiliary tree. However, the MPD parses re-
veal that not all useful adjunctions conform to this
paradigm, and that left-auxiliary trees that are not
used for sister adjunction are susceptible to this
behavior. The most common such tree is used to
create noun phrases such as
P&G?s share of [the Japanese market]
the House?s repeal of [a law]
Apple?s family of [Macintosh Computers]
Canada?s output of [crude oil]
by adjoining the shared unbracketed syntax onto
the NP dominating the bracketed text. If adjunc-
tion is taken to model modification, this rule dras-
tically changes the semantics of the unmodified
sentence. Furthermore, in some cases removing
the adjunction can leave a grammatically incorrect
sentence, as in the third example where the noun
phrase changes plurality.
While our grammar induction method is a crude
(but effective) heuristic, we can still highlight the
qualities of the more important auxiliary trees
by examining aggregate statistics over the MPD
parses, shown in Figure 6. The use of left-
auxiliary trees for sister adjunction is a clear trend,
as is the predominant use of right-auxiliary trees
for the complementary set of ?regular? adjunc-
tions, which is to be expected in a right branch-
ing language such as English. The statistics also
All Wrap Right Left
Total 3585 (1374) 41 (26) 1698 (518) 1846 (830)
Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)
Lex 2244 (990) 28 (19) 894 (299) 1322 (672)
FLex 1028 (558) 7 (2) 835 (472) 186 (84)
Figure 6: Statistics for MPD auxiliary trees us-
ing OSTAG3. The columns indicate type of aux-
iliary tree and the rows correspond respectively to
the full set found in the MPD, those that perform
sister adjunction, those that are lexicalized, and
those that are fully lexicalized. Each cell shows
the number of tokens followed by the number of
types of auxiliary tree that fit its conditions.
reflect the importance of substitution in right-
auxiliary trees, as they must capture the wide va-
riety of right branching modifiers of the English
language.
6 Conclusion
The OSTAG variant of Tree-Adjoining Grammar
is a simple weakly context-free formalism that
still provides for all types of adjunction and is
a bit more concise than TSG (quadratically so).
OSTAG can be reversibly transformed into CFG
form, allowing the use of a wide range of well
studied techniques in statistical parsing.
OSTAG provides an alternative to TIG as a
context-free TAG variant that offers wrapping ad-
junction in exchange for recursive left/right spine
adjunction. It would be interesting to apply both
OSTAG and TIG to different languages to deter-
mine where the constraints of one or the other are
more or less appropriate. Another possibility is the
combination of OSTAG with TIG, which would
strictly expand the abilities of both approaches.
The most important direction of future work for
OSTAG is the development of a principled gram-
mar induction model, perhaps using the same tech-
niques that have been successfully applied to TSG
and TIG. In order to motivate this and other re-
lated research, we release our implementation of
EM and CYK parsing for OSTAG1. Our system
performs the CFG transform described above and
optionally employs coarse to fine pruning and re-
laxed (finite) limits on the number of spine adjunc-
tions. As a TSG is simply a TAG without adjunc-
tion rules, our parser can easily be used as a TSG
estimator and parser as well.
1bllip.cs.brown.edu/download/bucketparser.tar
309
References
Eugene Charniak, Mark Johnson, Micha Elsner,
Joseph L. Austerweil, David Ellis, Isaac Hax-
ton, Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Eugene Charniak. 1996. Tree-bank grammars. In As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1031?1036.
David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining grammar.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Association for Computational Linguistics.
Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-
lore Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem: a wide coverage grammar for English. pages
922?928. Association for Computational Linguis-
tics.
J. Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Saul Gorn. 1965. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77?115.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 557?563. Association for Computational Lin-
guistics.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer.
Aravind K Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is required to provide
reasonable structural descriptions? University of
Pennsylvania.
Aravind K Joshi. 1987. An introduction to tree ad-
joining grammars. Mathematics of Language, pages
87?115.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. pages 423?430. Associ-
ation for Computational Linguistics.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
pages 35?56.
Mehryar Mohri and Mark jan Nederhof. 2000. Regu-
lar approximation of context-free grammars through
transformation. In Robustness in language and
speech technology.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, (4):479?513.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for bayesian tree substi-
tution grammars. pages 206?211. Association for
Computational Linguistics.
Elif Yamangil and Stuart M. Shieber. 2012. Estimat-
ing compact yet rich tree insertion grammars. pages
110?114. Association for Computational Linguis-
tics.
310
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597?603,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Nonparametric Bayesian Inference and Efficient Parsing for
Tree-adjoining Grammars
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
In the line of research extending statis-
tical parsing to more expressive gram-
mar formalisms, we demonstrate for the
first time the use of tree-adjoining gram-
mars (TAG). We present a Bayesian non-
parametric model for estimating a proba-
bilistic TAG from a parsed corpus, along
with novel block sampling methods and
approximation transformations for TAG
that allow efficient parsing. Our work
shows performance improvements on the
Penn Treebank and finds more compact
yet linguistically rich representations of
the data, but more importantly provides
techniques in grammar transformation and
statistical inference that make practical
the use of these more expressive systems,
thereby enabling further experimentation
along these lines.
1 Introduction
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity ? to allow accurate modeling of the
data with sparse grammars ? and low complexity
? making induction of the grammars (say, from
a treebank) and parsing of novel sentences com-
putationally practical. Tree-substitution grammars
(TSG), by expanding the domain of locality of
context-free grammars (CFG), can achieve better
expressivity, and the ability to model more con-
textual dependencies; the payoff would be better
modeling of the data or smaller (sparser) models
or both. For instance, constructions that go across
levels, like the predicate-argument structure of a
verb and its arguments can be modeled by TSGs
(Goodman, 2003).
Recent work that incorporated Dirichlet pro-
cess (DP) nonparametric models into TSGs has
provided an efficient solution to the daunting
model selection problem of segmenting training
data trees into appropriate elementary fragments
to form the grammar (Cohn et al, 2009; Post and
Gildea, 2009). The elementary trees combined in
a TSG are, intuitively, primitives of the language,
yet certain linguistic phenomena (notably various
forms of modification) ?split them up?, preventing
their reuse, leading to less sparse grammars than
might be ideal (Yamangil and Shieber, 2012; Chi-
ang, 2000; Resnik, 1992).
TSGs are a special case of the more flexible
grammar formalism of tree adjoining grammar
(TAG) (Joshi et al, 1975). TAG augments TSG
with an adjunction operator and a set of auxil-
iary trees in addition to the substitution operator
and initial trees of TSG, allowing for ?splicing in?
of syntactic fragments within trees. This func-
tionality allows for better modeling of linguistic
phenomena such as the distinction between modi-
fiers and arguments (Joshi et al, 1975; XTAG Re-
search Group, 2001). Unfortunately, TAG?s ex-
pressivity comes at the cost of greatly increased
complexity. Parsing complexity for unconstrained
TAG scales as O(n6), impractical as compared to
CFG and TSG?s O(n3). In addition, the model
selection problem for TAG is significantly more
complicated than for TSG since one must reason
about many more combinatorial options with two
types of derivation operators. This has led re-
searchers to resort to manual (Doran et al, 1997)
or heuristic techniques. For example, one can con-
sider ?outsourcing? the auxiliary trees (Shieber,
2007), use template rules and a very small num-
ber of grammar categories (Hwa, 1998), or rely
on head-words and force lexicalization in order to
constrain the problem (Xia et al, 2001; Chiang,
597
2000; Carreras et al, 2008). However a solution
has not been put forward by which a model that
maximizes a principled probabilistic objective is
sought after.
Recent work by Cohn and Blunsom (2010) ar-
gued that under highly expressive grammars such
as TSGs where exponentially many derivations
may be hypothesized of the data, local Gibbs sam-
pling is insufficient for effective inference and
global blocked sampling strategies will be nec-
essary. For TAG, this problem is only more se-
vere due to its mild context-sensitivity and even
richer combinatorial nature. Therefore in previ-
ous work, Shindo et al (2011) and Yamangil and
Shieber (2012) used tree-insertion grammar (TIG)
as a kind of expressive compromise between TSG
and TAG, as a substrate on which to build nonpara-
metric inference. However TIG has the constraint
of disallowing wrapping adjunction (coordination
between material that falls to the left and right
of the point of adjunction, such as parentheticals
and quotations) as well as left adjunction along the
spine of a right auxiliary tree and vice versa.
In this work we formulate a blocked sampling
strategy for TAG that is effective and efficient, and
prove its superiority against the local Gibbs sam-
pling approach. We show via nonparametric in-
ference that TAG, which contains TSG as a sub-
set, is a better model for treebank data than TSG
and leads to improved parsing performance. TAG
achieves this by using more compact grammars
than TSG and by providing the ability to make
finer-grained linguistic distinctions. We explain
how our parameter refinement scheme for TAG
allows for cubic-time CFG parsing, which is just
as efficient as TSG parsing. Our presentation as-
sumes familiarity with prior work on block sam-
pling of TSG and TIG (Cohn and Blunsom, 2010;
Shindo et al, 2011; Yamangil and Shieber, 2012).
2 Probabilistic Model
In the basic nonparametric TSG model, there is
an independent DP for every grammar category
(such as c = NP), each of which uses a base dis-
tribution P0 that generates an initial tree by mak-
ing stepwise decisions and concentration parame-
ter ?c that controls the level of sparsity (size) of
the generated grammars: Gc ? DP(?c, P0(? | c))
We extend this model by adding specialized DPs
for auxiliary trees Gauxc ? DP(?auxc , P aux0 (? | c))
Therefore, we have an exchangeable process for
generating auxiliary tree aj given j ? 1 auxiliary
trees previously generated
p(aj | a<j) =
nc,aj + ?auxc P aux0 (aj | c)
j ? 1 + ?auxc
(1)
as for initial trees in TSG (Cohn et al, 2009).
We must define base distributions for initial
trees and auxiliary trees. P0 generates an initial
tree with root label c by sampling rules from a
CFG P? and making a binary decision at every
node generated whether to leave it as a frontier
node or further expand (with probability ?c) (Cohn
et al, 2009). Similarly, our P aux0 generates an aux-
iliary tree with root label c by sampling a CFG rule
from P? , flipping an unbiased coin to decide the di-
rection of the spine (if more than a unique child
was generated), making a binary decision at the
spine whether to leave it as a foot node or further
expand (with probability ?c), and recurring into P0
or P aux0 appropriately for the off-spine and spinal
children respectively.
We glue these two processes together via a set
of adjunction parameters ?c. In any derivation for
every node labeled c that is not a frontier node
or the root or foot node of an auxiliary tree, we
determine the number (perhaps zero) of simulta-
neous adjunctions (Schabes and Shieber, 1994)
by sampling a Geometric(?c) variable; thus k si-
multaneous adjunctions would have probability
(?c)k(1 ? ?c). Since we already provide simul-
taneous adjunction we disallow adjunction at the
root of auxiliary trees.
3 Inference
Given this model, our inference task is to ex-
plore posterior derivations underlying the data.
Since TAG derivations are highly structured ob-
jects, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al, 2011; Yamangil and
Shieber, 2012). As in previous work, we use a
Goodman-transformed TAG as our proposal dis-
tribution (Goodman, 2003) that incorporates ad-
ditional CFG rules to account for the possibil-
ity of backing off to the infinite base distribution
P aux0 , and use the parsing algorithm described by
Shieber et al (1995) for computing inside proba-
bilities under this TAG model.
The algorithm is illustrated in Table 1 along
with Figure 1. Inside probabilities are computed
in a bottom-up fashion and a TAG derivation is
sampled top-down (Johnson et al, 2007). The
598
N? N
? N
N
Ni
. . .
?
?N0
? N1
? N2
N3
N4
. . .
?
?
Nj
? Nk
N? ?
Nl
? Nm
N? ?
Figure 1: Example used for illustrating blocked
sampling with TAG. On the left hand side we have
a partial training tree where we highlight the par-
ticular nodes (with node labels 0, 1, 2, 3, 4) that the
sampling algorithm traverses in post-order. On the
right hand side is the TAG grammar fragment that
is used to parse these particular nodes: one initial
tree and two wrapping auxiliary trees where one
adjoins into the spine of the other for full general-
ity of our illustration. Grammar nodes are labeled
with their Goodman indices (letters i, j, k, l,m).
Greek letters ?, ?, ?, ? denote entire subtrees. We
assume that a subtree in an auxiliary tree (e.g., ?)
parses the same subtree in a training tree.
sampler visits every node of the tree in post-order
(O(n) operations, n being the number of nodes),
visits every node below it as a potential foot (an-
other O(n) operations), visits every mid-node in
the path between the original node and the poten-
tial foot (if spine-adjunction is allowed) (O(log n)
operations), and forms the appropriate chart items.
The complexity is O(n2 log n) if spine-adjunction
is allowed, O(n2) otherwise.
4 Parameter Refinement
During inference, adjunction probabilities are
treated simplistically to facilitate convergence.
Only two parameters guide adjunction: ?c, the
probability of adjunction; and p(aj | a<j , c) (see
Equation 1), the probability of the particular aux-
iliary tree being adjoined given that there is an
adjunction. In all of this treatment, c, the con-
text of an adjunction, is the grammar category la-
bel such as S or NP, instead of a unique identi-
fier for the node at which the adjunction occurs as
was originally the case in probabilistic TAG liter-
ature. However it is possible to experiment with
further refinement schemes at parsing time. Once
the sampler converges on a grammar, we can re-
estimate its adjunction probabilities. Using the
O(n6) parsing algorithm (Shieber et al, 1995) we
experimented with various refinements schemes
? ranging from full node identifiers, to Goodman
Chart item Why made? Inside probability
Ni[4] By assumption. ?
Nk[3-4] N?[4] and ? (1 ? ?c) ? pi(?)
Nm[2-3] N?[3] and ? (1 ? ?c) ? pi(?)
Nl[1-3] ? and Nm[2-3] (1 ? ?c) ? pi(?)
?pi(Nm[2-3])
Naux[1-3] Nl[1-3] nc,al/(nc + ?auxc )
?pi(Nl[1-3])
Nk[1-4] Naux[1-3] and Nk[3-4] ?c ? pi(Naux[1-3])
?pi(Nk[3-4])
Nj [0-4] ? and Nk[1-4] (1 ? ?c) ? pi(?)
?pi(Nk[1-4])
Naux[0-4] Nj [0-4] nc,aj /(nc + ?auxc )
?pi(Nj [0-4])
Ni[0] Naux[0-4] and Ni[4] ?c ? pi(Naux[0-4])
?pi(Ni[4])
Table 1: Computation of inside probabilities for
TAG sampling. We create two types of chart
items: (1) per-node, e.g., Ni[?] denoting the
probability of starting at an initial subtree that
has Goodman index i and generating the subtree
rooted at node ?, and (2) per-path, e.g., Nj[?-?]
denoting the probability of starting at an auxiliary
subtree that has Goodman index j and generating
the subtree rooted at ? minus the subtree rooted
at ?. Above, c denotes the context of adjunction,
which is the nonterminal label of the node of ad-
junction (here, N), ?c is the probability of adjunc-
tion, nc,a is the count of the auxiliary tree a, and
nc =
?
a nc,a is total number of adjunctions atcontext c. The function pi(?) retrieves the inside
probability corresponding to an item.
index identifiers of the subtree below the adjunc-
tion (Hwa, 1998), to simple grammar category la-
bels ? and find that using Goodman index identi-
fiers as c is the best performing option.
Interestingly, this particular refinement scheme
also allows for fast cubic-time parsing, which we
achieve by approximating the TAG by a TSG with
little loss of coverage (no loss of coverage under
special conditions which we find that are often sat-
isfied) and negligible increase in grammar size, as
discussed in the next section.
5 Cubic-time parsing
MCMC training results in a list of sufficient statis-
tics of the final derivation that the TAG sampler
converges upon after a number of iterations. Basi-
cally, these are the list of initial and auxiliary trees,
their cumulative counts over the training data, and
their adjunction statistics. An adjunction statistic
is listed as follows. If ? is any elementary tree, and
? is an auxiliary tree that adjoins n times at node ?
of ? that is uniquely reachable at path p, we write
? p? ? (n times). We denote ? alternatively as
?[p].
599
*q
!
p
"
n
m
k
# *
p
"
i
i
i
q
!
i k
# *
m
i
"
i
#
i
i
i
#
j
j
j
q
!
i
j
i
j
!
ij
i
(1) (2) (3)
Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end
up with a larger yet adjunction-free TSG.
Now imagine that we end up with a small gram-
mar that consists of one initial tree ? and two aux-
iliary trees ? and ?, and the following adjunctions
occurring between them
? p? ? (n times)
? p? ? (m times)
? q? ? (k times)
as shown in Figure 2. Assume that ? itself occurs
l > n +m times in total so that there is nonzero
probability of no adjunction anywhere within ?.
Also assume that the node uniquely identified by
?[p] has Goodman index i, which we denote as
i = G(?[p]).
The general idea of this TAG-TSG approxima-
tion is that, for any auxiliary tree that adjoins at a
node ? with Goodman index i, we create an ini-
tial tree out of it where the root and foot nodes of
the auxiliary tree are both replaced by i. Further,
we split the subtree rooted at ? from its parent and
rename the substitution site that is newly created
at ? as i as well. (See Figure 2.) We can sep-
arate the foot subtree from the rest of the initial
tree since it is completely remembered by any ad-
joined auxiliary trees due to the nature of our re-
finement scheme. However this method fails for
adjunctions that occur at spinal nodes of auxiliary
trees that have foot nodes below them since we
would not know in which order to do the initial
tree creation. However when the spine-adjunction
relation is amenable to a topological sort (as is the
case in Figure 2), we can apply the method by go-
ing in this order and doing some extra bookkeep-
ing: updating the list of Goodman indices and re-
directing adjunctions as we go along. When there
is no such topological sort, we can approximate
the TAG by heuristically dropping low-frequency
adjunctions that introduce cycles.1
The algorithm is illustrated in Figure 2. In (1)
we see the original TAG grammar and its adjunc-
tions (n,m, k are adjunction counts). Note that
the adjunction relation has a topological sort of
?, ?, ?. We process auxiliary trees in this order
and iteratively remove their adjunctions by creat-
ing specialized initial tree duplicates. In (2) we
first visit ?, which has adjunctions into ? at the
node denoted ?[p] where p is the unique path from
the root to this node. We retrieve the Goodman in-
dex of this node i = G(?[p]), split the subtree
rooted at this node as a new initial tree ?i, relabel
its root as i, and rename the newly-created sub-
stitution site at ?[p] as i. Since ? has only this
adjunction, we replace it with initial tree version
?i where root/foot labels of ? are replaced with
i, and update all adjunctions into ? as being into
?i. In (3) we visit ? which now has adjunctions
into ? and ?i. For the ?[p] adjunction we create ?i
the same way we created ?i but this time we can-
not remove ? as it still has an adjunction into ?i.
We retrieve the Goodman index of the node of ad-
junction j = G(?i[q]), split the subtree rooted at
this node as new initial tree ?ij , relabel its root
as j, and rename the newly-created substitution
site at ?i[q] as j. Since ? now has only this ad-
junction left, we remove it by also creating initial
tree version ?j where root/foot labels of ? are re-
placed with j. At this point we have an adjunction-
free TSG with elementary trees (and counts)
?(l), ?i(l), ?i(n), ?ij(n), ?i(m), ?j(k) where l is
the count of initial tree ?. These counts, when they
are normalized, lead to the appropriate adjunc-
1We found that, on average, about half of our grammars
have a topological sort of their spine-adjunctions. (On aver-
age fewer than 100 spine adjunctions even exist.) When no
such sort exists, only a few low-frequency adjunctions have
to be removed to eliminate cycles.
600
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60
P
a
r
s
i
n
g
 
t
i
m
e
 
(
s
e
c
o
n
d
s
)
Sentence length (#tokens)
Figure 3: Nonparametric TAG (blue) parsing is ef-
ficient and incurs only a small increase in parsing
time compared to nonparametric TSG (red).
tion probability refinement scheme of ?c ? p(aj |
a<j , c) where c is the Goodman index.
Although this algorithm increases grammar
size, the sparsity of the nonparametric solution
ensures that the increase is almost negligible: on
average the final Goodman-transformed CFG has
173.9K rules for TSG, 189.2K for TAG. Figure 3
demonstrates the comparable Viterbi parsing times
for TSG and TAG.
6 Evaluation
We use the standard Penn treebank methodology
of training on sections 2?21 and testing on section
23. All our data is head-binarized, all hyperpa-
rameters are resampled under appropriate vague
gamma and beta priors. Samplers are run 1000
iterations each; all reported numbers are aver-
ages over 5 runs. For simplicity, parsing results
are based on the maximum probability derivation
(Viterbi algorithm).
In Table 4, we compare TAG inference
schemes and TSG. TAGGibbs operates by locally
adding/removing potential adjunctions, similar to
Cohn et al (2009). TAG? is the O(n2) algorithm
that disallows spine adjunction. We see that TAG?
has the best parsing performance, while TAG pro-
vides the most compact representation.
model F measure # initial trees # auxiliary trees
TSG 84.15 69.5K -
TAGGibbs 82.47 69.9K 1.7K
TAG? 84.87 66.4K 1.5K
TAG 84.82 66.4K 1.4K
Figure 4: EVALB results. Note that the Gibbs
sampler for TAG has poor performance and pro-
vides no grammar compaction due to its lack of
convergence.
label #adj ave. #lex. #left #right #wrap
(spine adj) depth trees trees trees trees
VP 4532 (23) 1.06 45 22 65 0
NP 2891 (46) 1.71 68 94 13 1
NN 2160 (3) 1.08 85 16 110 0
NNP 1478 (2) 1.12 90 19 90 0
NNS 1217 (1) 1.10 43 9 60 0
VBN 1121 (1) 1.05 6 18 0 0
VBD 976 (0) 1.0 16 25 0 0
NP 937 (0) 3.0 1 5 0 0
VB 870 (0) 1.02 14 31 4 0
S 823 (11) 1.48 42 36 35 3
total 23320 (118) 1.25 824 743 683 9
Table 2: Grammar analysis for an estimated TAG,
categorized by label. Only the most common top
10 are shown, binarization variables are denoted
with overline. A total number of 98 wrapping
adjunctions (9 unique wrapping trees) and 118
spine adjunctions occur.
ADJP
?
?
ADJP
ADJP* ?
?
NP
-LRB- NP
NP* -RRB-
S
-LRB-
-LRB-
S
S* -RRB-
-RRB-
S
?
?
S
S* ?
?NP
-LRB-
-LRB-
NP
NP* -RRB-
-RRB-
NNP
,
,
NNP
NNP
NNP* CC
&
NNP
NP
?
?
NP
NP* ?
?
NP
NP
NP :
NP
NP* PP
Figure 5: Example wrapping trees from estimated
TAGs.
7 Conclusion
We described a nonparametric Bayesian inference
scheme for estimating TAG grammars and showed
the power of TAG formalism over TSG for return-
ing rich, generalizable, yet compact representa-
tions of data. The nonparametric inference scheme
presents a principled way of addressing the diffi-
cult model selection problem with TAG. Our sam-
pler has near quadratic-time efficiency, and our
parsing approach remains context-free allowing
for fast cubic-time parsing, so that our overall
parsing framework is highly scalable.2
There are a number of extensions of this
work: Experimenting with automatically in-
duced adjunction refinements as well as in-
corporating substitution refinements can benefit
Bayesian TAG (Shindo et al, 2012; Petrov et al,
2006). We are also planning to investigate TAG
for more context-sensitive languages, and syn-
chronous TAG for machine translation.
2An extensive report of our algorithms and experiments
will be provided in the PhD thesis of the first author (Ya-
mangil, 2013). Our code will be made publicly available at
code.seas.harvard.edu/?elif.
601
References
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages 9?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
456?463, Morristown, NJ, USA. Association for
Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 225?230, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548?556, Morristown, NJ, USA. Association
for Computational Linguistics.
Christine Doran, Beth Hockey, Philip Hopely, Joseph
Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia,
Alexis Nasr, and Owen Rambow. 1997. Maintain-
ing the forest and burning out the underbrush in xtag.
In Proceedings of the ENVGRAM Workshop.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Rens Bod, Remko Scha,
and Khalil Sima?an, editors, Data-Oriented Parsing.
CSLI Publications, Stanford, CA.
Rebecca Hwa. 1998. An empirical evaluation of
probabilistic lexicalized tree insertion grammars. In
Proceedings of the 17th international conference on
Computational linguistics - Volume 1, pages 557?
563, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York, April.
Association for Computational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136?163.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48, Suntec, Singapore, August. Association for
Computational Linguistics.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natural lan-
guage processing. In Proceedings of the 14th con-
ference on Computational linguistics - Volume 2,
COLING ?92, pages 418?424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91?124. Also
available as cmp-lg/9404001.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Log. Program., 24(1&2):3?36.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Dekai
Wu and David Chiang, editors, Proceedings of the
Workshop on Syntax and Structure in Statistical
Translation, Rochester, New York, 26 April.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitu-
tion grammars. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 206?211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440?448, Jeju Island, Korea,
July. Association for Computational Linguistics.
Fei Xia, Chung-hye Han, Martha Palmer, and Aravind
Joshi. 2001. Automatically extracting and compar-
ing lexicalized grammars for different languages. In
Proceedings of the 17th international joint confer-
ence on Artificial intelligence - Volume 2, IJCAI?01,
pages 1321?1326, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
602
Elif Yamangil and Stuart Shieber. 2012. Estimating
compact yet rich tree insertion grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 110?114, Jeju Island, Korea, July.
Association for Computational Linguistics.
Elif Yamangil. 2013. Rich Linguistic Structure from
Large-Scale Web Data. Ph.D. thesis, Harvard Uni-
versity. Forthcoming.
603
