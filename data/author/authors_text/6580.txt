Ranking suspected answers to natural language questions using 
predictive annotation 
Dragomir R. Radev" 
School of Information 
University of Michigan 
Ann Arbor, MI 48103 
radev@umich, edu 
John Prager 
TJ  Watson Research Center 
IBM Research Division 
Hawthorne, NY 10532 
jprager@us. ibm.com 
Valerie Samn* 
Teachers College 
Columbia University 
New York, NY 10027 
vs l l5@columbia.edu 
Abstract 
In this paper, we describe a system to rank sus- 
pected answers to natural language questions. 
We process both corpus and query using a new 
technique, predictive annotation, which aug- 
ments phrases in texts with labels anticipating 
their being targets of certain kinds of questions. 
Given a natural anguage question, our IR sys- 
tem returns a set of matching passages, which 
we then rank using a linear function of seven 
predictor variables. We provide an evaluation of 
the techniques based on results from the TREC 
Q&A evaluation in which our system partici- 
pated. 
1 Introduction 
Question Answering is a task that calls for a 
combination of techniques from Information Re- 
trieval and Natural Language Processing. The 
former has the advantage of years of develop- 
ment of efficient techniques for indexing and 
searching large collections of data, but lacks of 
any meaningful treatment of the semantics of 
the query or the texts indexed. NLP tackles 
the semantics, but tends to be computationally 
expensive. 
We have attempted to carve out a middle 
ground, whereby we use a modified IR system 
augmented by shallow NL parsing. Our ap- 
proach was motivated by the following problem 
with traditional IR systems. Suppose the user 
asks "Where did <some event> happen?". If 
the system does no pre-processing of the query, 
then "where" will be included in the bag of 
words submitted to the search engine, but this 
will not be helpful since the target text will 
be unlikely to contain the word "where". If 
the word is stripped out as a stop-word, then 
* The work presented in this paper was performed while 
the first and third authors were at 1BM Research. 
the search engine will have no idea that a lo- 
cation is sought. Our approach, called predic- 
tive annotation, is to augment he query with 
semantic ategory markers (which we call QA- 
Tokens), in this case with the PLACES to- 
ken, and also to label with QA-Tokens all oc- 
currences in text that are recognized entities, 
(for example, places). Then traditional bag-of- 
words matching proceeds uccessfully, and will 
return matching passages. The answer-selection 
process then looks for and ranks in these pas- 
sages occurrences ofphrases containing the par- 
ticular QA-Token(s) from the augmented query. 
This classification of questions is conceptually 
similar to the query expansion in (Voorhees, 
1994) but is expected to achieve much better 
performance since potentially matching phrases 
in text are classified in a similar and synergistic 
way. 
Our system participated in the official TREC 
Q&A evaluation. For 200 questions in the eval- 
uation set, we were asked to provide a list of 
50-byte and 250-byte xtracts from a 2-GB cor- 
pus. The results are shown in Section 7. 
Some techniques used by other participants in
the TREC evaluation are paragraph indexing, 
followed by abductive inference (Harabagiu and 
Maiorano, 1999) and knowledge-representation 
combined with information retrieval (Breck et 
al., 1999). Some earlier systems related to our 
work are FaqFinder (Kulyukin et al, 1998), 
MURAX (Kupiec, 1993), which uses an encyclo- 
pedia as a knowledge base from which to extract 
answers, and PROFILE (Radev and McKeown, 
1997) which identifies named entities and noun 
phrases that describe them in text. 
2 System description 
Our system (Figure 1) consists of two pieces: 
an IR component (GuruQA) that which returns 
matching texts, and an answer selection compo- 
150 
neat (AnSel/Werlect) that extracts and ranks 
potential answers from these texts. 
This paper focuses on the process of rank- 
ing potential answers selected by the IR engine, 
which is itself described in (Prager et al, 1999). 
~ lndexer 
~ Searc~x'~ GuruQA 
\ 
\ 
Rankcd ~ AnSel/ ~ Hit List 
H tL st \[ \[ Werlect I 
Answer selection 
Figure 1: System Architecture. 
2.1 The Information Retrieval 
component 
In the context of fact-seeking questions, we 
made the following observations: 
? In documents that contain the answers, the 
query terms tend to occur in close proxim- 
ity to each other. 
? The answers to fact-seeking questions are 
usually phrases: "President Clinton", "in 
the Rocky Mountains", and "today"). 
? These phrases can be categorized by a set of 
a dozen or so labels (Figure 2) correspond- 
ing to question types. 
? The phrases can be identified in text by 
pattern matching techniques (without full 
NLP). 
As a result, we defined a set of about 20 cat- 
egories, each labeled with its own QA-Token, 
and built an IR system which deviates from the 
traditional model in three important aspects. 
? We process the query against a set of ap- 
proximately 200 question templates which, 
may replace some of the query words 
with a set of QA-Tokens, called a SYN-  
class. Thus "Where" gets mapped 
to "PLACES", but "How long " goes 
to "@SYN(LENGTH$, DURATIONS)". 
Some templates do not cause complete re- 
placement of the matched string. For ex- 
ample, the pattern "What is the popula- 
tion" gets replaced by "NUMBERS popu- 
lation'. 
? Before indexing the text, we process it 
with Textract (Byrd and Ravin, 1998; 
Wacholder et al, 1997), which performs 
lemmatization, and discovers proper names 
and technical terms. We added a new 
module (Resporator) which annotates text 
segments with QA-Tokens using pattern 
matching. Thus the text "for 5 centuries" 
matches the DURATIONS pattern "for 
:CARDINAL _timeperiod", where :CAR- 
DINAL is the label for cardinal numbers, 
and _timeperiod marks a time expression. 
? GuruQA scores text passages instead of 
documents. We use a simple document- 
and collection-independent weighting 
scheme: QA-Tokens get a weight of 400, 
proper nouns get 200 and any other word 
- 100 (stop words are removed in query 
processing after the pattern template 
matching operation). The density of 
matching query tokens within a passage is 
contributes a score of 1 to 99 (the highest 
scores occur when all matched terms are 
consecutive). 
Predictive Annotation works best for Where, 
When, What, Which and How+adjective ques- 
tions than for How+verb and Why questions, 
since the latter are typically not answered by 
phrases. However, we observed that "by" + 
the present participle would usually indicate 
the description of a procedure, so we instan- 
tiate a METHODS QA-Token for such occur- 
rences. We have no such QA-Token for Why 
questions, but we do replace the word "why" 
with "@SYN(result, cause, because)", since the 
occurrence of any of these words usually beto- 
kens an explanation. 
3 Answer  select ion 
So far, we have described how we retrieve rel- 
evant passages that may contain the answer to 
a query. The output of GuruQA is a list of 
10 short passages containing altogether a large 
151 
QA-Token Question type Example 
PLACES 
COUNTRY$ 
STATES 
PERSONS 
ROLES 
NAMES 
ORG$ 
DURATIONS 
AGES 
YEARS 
TIMES 
DATES 
VOLUMES 
AREAS 
LENGTHS 
WEIGHTS 
NUMBERS 
METHODS 
RATES 
MONEYS 
Where 
Where/What country 
Where/What state 
Who 
Who 
Who/What/Which 
Who/What 
How long 
How old 
When/What year 
When 
When/What date 
How big 
How big 
How big/long/high 
How big/heavy 
How many 
How 
How much 
How much 
In the Rocky Mountains 
United Kingdom 
Massachusetts 
Albert Einstein 
Doctor 
The Shakespeare F stival 
The US Post Office 
For 5 centuries 
30 years old 
1999 
In the afternoon 
July 4th, 1776 
3 gallons 
4 square inches 
3 miles 
25 tons 
1,234.5 
By rubbing 
50 per cent 
4 million dollars 
Figure 2: Sample QA-Tokens. 
number (often more than 30 or 40) of potential 
answers in the form of phrases annotated with 
QA-Tokens. 
3.1 Answer  rank ing  
We now describe two algorithms, AnSel and 
Werlect, which rank the spans returned by Gu- 
ruQA. AnSel and Werlect 1 use different ap- 
proaches, which we describe, evaluate and com- 
pare and contrast. The output of either system 
consists of five text extracts per question that 
contain the likeliest answers to the questions. 
3.2 Sample  Input to AnSel /Wer lect  
The role of answer selection is to decide which 
among the spans extracted by GuruQA are 
most likely to contain the precise answer to the 
questions. Figure 3 contains an example of the 
data structure passed from GuruQA to our an- 
swer selection module. 
The input consists of four items: 
? a query (marked with <QUERY> tokens 
in the example), 
? a list of 10 passages (one of which is shown 
above), 
? a list of annotated text spans within the 
passages, annotated with QA-Tokens, and 
1 from ANswer SELect and ansWER seLECT, respec- 
tively 
? the SYN-class corresponding to the type of 
question (e.g., "PERSONS NAMES"). 
The text in Figure 3 contains five spans (po- 
tential answers), of which three ("Biography of 
Margaret Thatcher", "Hugo Young", and "Mar- 
garet Thatcher") are of types included in the 
SYN-class for the question (PERSON NAME). 
The full output of GuruQA for this question in- 
cludes a total of 14 potential spans (5 PERSONs 
and 9 NAMEs). 
3.3 Sample  Output  o f  AnSel /Wer lect  
The answer selection module has two outputs: 
internal (phrase) and external (text passage). 
Internal output:  The internal output is a 
ranked list of spans as shown in Table 1. It 
represents a ranked list of the spans (potential 
answers) sent by GuruQA. 
Externa l  output :  The external output is 
a ranked list of 50-byte and 250-byte xtracts. 
These extracts are selected in a way to cover 
the highest-ranked spans in the list of potential 
answers. Examples are given later in the paper. 
The external output was required for the 
TREC evaluation while system's internal out- 
put can be used in a variety of applications, e.g., 
to highlight the actual span that we believe is 
the answer to the question within the context 
of the passage in which it appears. 
152 
<p> <NUMBER> 1 </NUMBER> </p> 
<p><QUERY>Who is the author of the book, "The Iron Lady: A Biography of Margaret Thatcher"? 
</QUERY></p> 
<p> <PROCESSED_QUERY> @excwin(*dynamic* @weight (200 * Iron_Lady) @weight (200 
Biography_of_Margaret_Thatcher) @weight(200 Margaret) @weight(100 author) 
@weight(100 book) @weight(100 iron) @weight(100 lady) @weight(100 :) @weight(100 biography) 
@weight(100 thatcher) @weight(400 @syn(PERSON$ NAME$)))</PROCESSED_QUERY></p> 
<p> <DOC>LA090290-0118</DOC> </p> <p> <SCORE> 1020.8114</SCORE> d/p> 
<TEXT><p>THE IRON LADY; A <span class="NAME">Biography of Margaret Thatcher </span> 
by <span class--"PERSON">Hugo Y ung</span> (<span class='ORG">Farrar , Straus 
& Giroux</span>) The central riddle revealed here is why, as a woman <span class--'PLACEDEF'>in a 
man</span>'s world, <span class--'PERSON'>Margaret Thatcher</span> evinces uch an exclusionary 
attitude toward women.</p></TEXT> 
Figure 3: Input sent from GuruQA to AnSel/Werlect. 
Score Span 
5.06 
-8.14 
-13.60 
-18.00 
-19.38 
-26.06 
-31.75 
-32.38 
-36.78 
-42.68 
-198.34 
-217.80 
-234.55 
Hugo Young 
Biography of Margaret Thatcher 
David Williams 
Williams 
Sir Ronald Millar 
Santiago 
Oxford 
Maggie 
Seriously Rich 
FT 
Margaret Thatcher 
Thatcher 
Iron Lady 
Questlon/Answer (TR38) 
Q: Who was Johnny Mathis' high school 
track coach? 
A: Lou Vasquez 
Q: What year was the Magna Carta signed? 
A: 1215 
Q: What two companies produce bovine 
somatotropin? 
A: Monsanto and Eli Lilly 
Figure 4: Sample questions from TR38. 
Table 1: Ranked potential answers to Quest. 1. 
4 Ana lys i s  o f  corpus  and quest ion 
sets 
In this section we describe the corpora used for 
training and evaluation as well as the questions 
contained in the training and evaluation ques- 
tion sets. 
4.1 Corpus  analysis 
For both training and evaluation, we used the 
TREC corpus, consisting of approximately 2
GB of articles from four news agencies. 
4.2 Training set TR38 
To train our system, we used 38 questions (see 
Figure 4) for which the answers were provided 
by NIST. 
4.3 Test set T200 
The majority of the 200 questions (see Figure 5) 
in the evaluation set (T200) were not substan- 
tially different from these in TR38, although the 
introduction of "why" and "how" questions as 
well as the wording of questions in the format 
"Name X" made the task slightly harder. 
Questlon/Answer (T200) 
Q: Why did David Koresh ask the FBI for a 
word processor? 
A: to record his revelations. 
Q: How tall is the Matterhorn? 
A: 14,776 feet 9 inches 
Q: How tall is the replica of the Matterhorn 
at Disneyland? 
A: 147-foot 
Figure 5: Sample questions from T200. 
Some examples of problematic questions are 
shown in Figure 6. 
153 
Q: Why did David Koresh ask the FBI for 
a word processor? 
Q: Name the first private citizen to fly in 
space. 
Q: What is considered the costliest disaster 
the insurance industry has ever faced? 
Q: What did John Hinckley do to impress 
Jodie Foster? 
Q: How did Socrates die? 
Figure 6: Sample harder questions from T200. 
5 AnSe l  
AnSel uses an optimization algorithm with 7 
predictive variables to describe how likely a 
given span is to be the correct answer to a 
question. The variables are illustrated with ex- 
amples related to the sample question number 
10001 from TR38 "Who was Johnny Mathis' 
high school track coach?". The potential an- 
swers (extracted by GuruQA) are shown in Ta- 
ble 2. 
5.1 Feature selection 
The seven span features described below were 
found to correlate with the correct answers. 
Number :  position of the span among M1 spans 
returned from the hit-list. 
Rspanno:  position of the span among all spans 
returned within the current passage. 
Count:  number of spans of any span class re- 
trieved within the current passage. 
Notinq: the number of words in the span that 
do not appear in the query. 
Type: the position of the span type in the list 
of potential span types. Example: Type 
("Lou Vasquez") = 1, because the span 
type of "Lou Vasquez", namely "PER- 
SON" appears first in the SYN-class "PER- 
SON ORG NAME ROLE". 
Avgdst:  the average distance in words between 
the beginning of the span and query words 
that also appear in the passage. Example: 
given the passage "Tim O'Donohue, Wood- 
bridge High School's varsity baseball coach, 
resigned Monday and will be replaced by 
assistant Johnny Ceballos, Athletic Direc- 
tor Dave Cowen said." and the span "Tim 
O'Donohue", the value of avgdst is equal 
to 8. 
Sscore: passage relevance as computed by Gu- 
ruQA. 
Number :  the position of the span among all 
retrieved spans. 
5.2 AnSel  algorithm 
The TOTAL score for a given potential answer 
is computed as a linear combination of the fea- 
tures described in the previous ubsection: 
TOTAL  = ~ w~ , fi 
i 
The Mgorithm that the training component 
of AnSel uses to learn the weights used in the 
formula is shown in Figure 7. 
For each <question,span> tuple in training 
set : 
i. Compute features for each span 
2. Compute TOTAL score for each span 
using current set of weights 
Kepeat 
3. Compute performance on training 
set 
4. Adjust weights wi through 
logistic regression 
Until performance > threshold 
Figure 7: Training algorithm used by AnSel. 
Training discovered the following weights: 
Wnurnbe r -~ --0.3; Wrspann o -~ --0.5; Wcount : 
3 .0 ;  Wnot inq  = 2 .0 ;  Wtypes  = 15 .0 ;  Wavgdst  ---- 
-1.0; W~score = 1.5 
At runtime, the weights are used to rank po- 
tential answers. Each span is assigned a TO- 
TAL score and the top 5 distinct extracts of 
50 (or 250) bytes centered around the span are 
output. The 50-byte xtracts for question 10001 
are shown in Figure 8. For lack of space, we are 
omitting the 250-byte xtracts. 
6 Wer lec t  
The Werlect algorithm used many of the same 
features of phrases used by AnSel, but employed 
a different ranking scheme. 
6.1 Approach 
Unlike AnSel, Werlect is based on a two-step, 
rule-based process approximating a function 
with interaction between variables. In the first 
stage of this algorithm, we assign a rank to 
154 
Spat ,  
Ollie Matson 
Lou Vasquez 
Tim O'Donohue 
Athlet ic Director Dave Cowen 
Johnny Ceballos 
Civic Center Director Mart in Durham 
Johnny Hodges 
Derric Evans 
NEWSWIRE Johnny Majors 
Woodbr idge High School 
Evan 
Gary Edwards 
O. J .  Simpson 
South Lake Tahoe 
Washington High 
Morgan 
Tennessee football  
El l ington 
assistant 
the Volunteers 
J ohnny  Mathis 
Mathis 
coach 
Type  Nun lber  
PERSON 3 
PERSON 1 
PERSON 17 
PERSON 23 
PERSON 22 
PERSON 13 
PERSON 25 
PERSON 33 
PERSON 3O 
ORG 18 
PERSON 37 
PERSON 38 
NAME 2 
NAME 7 
NAME 10 
NAME 26 
NAME 31 
NAME 24 
ROLE 21 
ROLE 34 
PERSON 4 
NAME 14 
ROLE 19 
Rspanno Count  Not lnq  Type  Avgdst  Sscore  
3 6 2 I 12 0 .02507 
1 6 2 1 16 0 .02507 
1 4 2 I 8 0 .02257 
6 4 4 1 I I  0 .02257 
5 4 I I 9 0 .02257 
I 2 5 1 16 0 .02505 
2 4 I I 15 0 .02256 
4 4 2 l 14 0 .02256 
1 4 2 1 17 0 .02256 
2 4 1 2 6 0 .02257 
6 4 1 1 14 0 .02256 
7 4 2 1 17 0 .02256 
2 6 2 3 12 0 .02507 
5 6 3 3 14 0 .02507 
6 6 1 3 18 0 .02507 
3 4 1 3 12 0 .02256 
2 4 1 3 15 0 .02256 
1 4 1 3 20 0 .02256 
4 4 1 4 8 0 .02257 
5 4 2 4 14 0 .02256 
4 6 - I00  I I I  0 .02507 
2 2 -100 3 I0  0 .02505 
3 4 -100 4 4 0 .02257 
Table 2: Feature set and span rankings for training question 10001. 
Document  ID Score Extract 
LA053189-0069 892.5 
LA053189-0069 890.1 
LA060889-0181 887.4 
LA060889-0181 884.1 
LA060889-0181 880.9 
of O.J. Simpson , Ollie Matson and Johnny Mathis 
Lou Vasquez , track coach of O.J. Simpson , Ollie 
Tim O'Donohue, Woodbridge High School's varsity 
nny Ceballos , Athletic Director Dave Cowen said. 
aced by assistant Johnny Ceballos , Athletic Direc 
Figure 8: Fifty-byte extracts. 
TOTAL  
-7.53 
-9.93 
-12.57 
-15.87 
-19.07 
-19.36 
-25.22 
-25.37 
-25.47 
-28.37 
-29.57 
-30,87 
-37.40 
-40.06 
-49.80 
-52 .52  
-56.27 
-59.42 
-62.77 
-71.17 
-211.33 
-254.16 
-259.67 
every relevant phrase within each sentence ac- 
cording to how likely it is to be the target an- 
swer. Next, we generate and rank each N-byte 
fragment based on the sentence score given by 
GuruQA, measures of the fragment's relevance, 
and the ranks of its component phrases. Unlike 
AnSel, Werlect was optimized through manual 
trial-and-error using the TR38 questions. 
6.2 Step  One: Feature  Se lect ion  
The features considered in Werlect that were 
also used by AnSel, were Type, Avgdst and Ss- 
core. Two additional features were also taken 
into account: 
Not inqW:  a modified version of Notinq. As 
in AnSel, spans that are contained in the 
query are given a rank of 0. However, par- 
tial matches are weighted favorably in some 
cases. For example, if the question asks, 
"Who was Lincoln's Secretary of State?" 
a noun phrase that contains "Secretary of 
State" is more likely to be the answer than 
one that does not. In this example, the 
phrase, "Secretary of State William Se- 
ward" is the most likely candidate. This 
criterion also seems to play a role in the 
event that Resporator fails to identify rel- 
evant phrase types. For example, in the 
training question, "What shape is a por- 
poise's tooth?" the phrase "spade-shaped" 
is correctly selected from among all nouns 
and adjectives of the sentences returned by 
Guru-QA. 
F requency :  how often the span occurs across 
different passages. For example, the test 
question, "How many lives were lost in the 
Pan Am crash in Lockerbie, Scotland?" re- 
sulted in four potential answers in the first 
two sentences returned by Guru-QA. Ta- 
ble 3 shows the frequencies of each term, 
and their eventual influence on the span 
rank. The repeated occurrence of "270", 
helps promote it to first place. 
6.3 Step  two:  rank ing the  sentence  
spans  
After each relevant span is assigned a rank, we 
rank all possible text segments of 50 (or 250) 
bytes from the hit list based on the sum of the 
phrase ranks plus additional points for other 
words in the segment that match the query. 
The algorithm used by Werlect is shown in 
Figure 9. 
155 
I n i t ia l  Sentence  Rank  Phrase  F requency  Span Rank  
1 Two 5 2 
1 365 million 1 3 
1 11 1 4 
2 270 7 1 (ranked highest) 
Table 3: Influence of frequency on span rank. 
i. Let cand idate_set  = all potential 
answers, ranked and sorted. 
2. For each hit-l ist passage, extract 
ali spans of 50 (or 250) bytes, on 
word boundaries. 
3. Rank and sort all segments based 
on phrase ranks, matching terms, 
and sentence ranks. 
4. For each candidate in sorted 
candidate_set 
- Let highest_ranked_span 
= highest-ranked span 
containing candidate 
- Let answer_set\[i++\] = 
h ighest_ rankedspan 
- Remove every candidate from 
candidate_set that is found in 
h ighest_ rankedspan 
- Exit if i > 5 
5. Output answer_set 
noted that on the 14 questions we were unable 
to classify with a QA-Token, Werlect (runs W50 
and W250) achieved an MRAR of 3.5 to Ansel's 
2.0. 
The cumulative RAR of A50 on T200 (Ta- 
ble 4) is 63.22 (i.e., we got 49 questions among 
the 198 right from our first try and 39 others 
within the first five answers). 
The performance of A250 on T200 is shown 
in Table 5. We were able to answer 71 questions 
with our first answer and 38 others within our 
first five answers (cumulative RAR = 85.17). 
To better characterize the performance of our 
system, we split the 198 questions into 20 groups 
of 10 questions. Our performance on groups 
of questions ranged from 0.87 to 5.50 MRAR 
for A50 and from 1.98 to 7.5 MRAR for A250 
(Table 6). 
Figure 9: Algorithm used by Werlect. 
7 Eva luat ion  
In this section, we describe the performance of
our system using results from our four official 
runs. 
7.1 Evaluat ion scheme 
For each question, the performance is computed 
as the reciprocal value of the rank (RAR) of 
the highest-ranked correct answer given by the 
system. For example, if the system has given 
the correct answer in three positions: second, 
third, and fifth, RAR for that question is ! 2" 
The Mean Reciprocal Answer Rank (MRAR) 
is used to compute the overall performance of
systems participating in the TREC evaluation: 
RAR - rank i  MRAR = . rank i  ) 
$ 
7.2 Per fo rmance  on the official 
evaluat ion data 
Overall, Ansel (runs A50 and A25) performed 
marginally better than Werlect. However, we 
50 bytes 250 bytes  
n 
Avg 
Min 
Max 
Std Dev 
20 
3.19 
0.87 
5.50 
1.17 
20 
4.30 
1.98 
7.50 
1.27 
Table 6: Performance on groups of ten questions 
Finally, Table 7 shows how our official runs 
compare to the rest of the 25 official submis- 
sions. Our performance using AnSel and 50- 
byte output was 0.430. The performance of 
Werlect was 0.395. On 250 bytes, AnSel scored 
0.319 and Werlect - 0.280. 
8 Conc lus ion  
We presented a new technique, predict ive an- 
notat ion,  for finding answers to natural an- 
guage questions in text corpora. We showed 
that a system based on predictive annotation 
can deliver very good results compared to other 
competing systems. 
We described a set of features that correlate 
with the plausibility of a given text span be- 
ing a good answer to a question. We experi- 
156 
nb of cases 
Points 
nb of cases 
Points 
First I Second Third Fourth Fifth TOTAL 
49 I 15 ll 9 4 88 
49.00 7.50 3.67 2.25 0.80 63.22 
Table 4: Performance of A50 on T200 
First Second Third Fourth Fifth TOTAL 
71 16 11 6 5 109 
71.00 8.00 3.67 1.50 1.00 85.17 
Run 
Table 
Median Average 
W50 0.12 
A50 0.12 
W250 0.29 
A250 0.29 
5: Performance of A250 on 
Our Average 
0.280 
0.319 
0.395 
0.430 
T200 
Nb Times Nb Times 
> Median -=- Median 
56 126 
72 112 
60 106 
66 110 
Nb Times 
< Median 
16 
14 
32 
22 
Table 7: Comparison of our system with the other participants 
mented with two algorithms for ranking poten- 
tial answers based on these features. We discov- 
ered that a linear combination of these features 
performs better overall, while a non-linear algo- 
rithm performs better on unclassified questions. 
9 Acknowledgments  
We would like to thank Eric Brown, Anni Co- 
den, and Wlodek Zadrozny from IBM Research 
for useful comments and collaboration. We 
would also like to thank the organizers of the 
TREC Q~zA evaluation for initiating such a 
wonderful research initiative. 
References  
Eric Breck, John Burger, David House, Marc 
Light, and Inderjeet Mani. 1999. Ques- 
tion answering from large document collec- 
tions. In Proceedings of AAAI Fall Sympo- 
sium on Question Answering Systems, North 
Falmouth, Massachusetts. 
Roy Byrd and Yael Ravin. 1998. Identifying 
and extracting relations in text. In Proceed- 
ings of NLDB, Klagenfurt, Austria. 
Sanda Harabagiu and Steven J. Maiorano. 
1999. Finding answers in large collections of 
texts : Paragraph indexing + abductive in- 
ference. In Proceedings ofAAAI Fall Sympo- 
sium on Question Answering Systems, North 
Falmouth, Massachusetts. 
Vladimir Kulyukin, Kristian Hammond, and 
Robin Burke. 1998. Answering questions 
for an organization online. In Proceedings of
AAAI, Madison, Wisconsin. 
Julian M. Kupiec. 1993. MURAX: A robust 
linguistic approach for question answering us- 
ing an ondine encyclopedia. In Proceedings, 
16th Annual International ACM SIGIR Con- 
ference on Research and Development in In- 
formation Retrieval. 
John Prager, Dragomir R. Radev, Eric Brown, 
Anni Coden, and Valerie Samn. 1999. The 
use of predictive annotation for question an- 
swering in TREC8. In Proceedings o/TREC- 
8, Gaithersburg, Maryland. 
Dragomir R. Radev and Kathleen R. McKe- 
own. 1997. Building a generation knowledge 
source using internet-accessible newswire. In 
Proceedings ofthe 5th Conference on Applied 
Natural Language Processing, pages 221-228, 
Washington, DC, April. 
Ellen Voorhees. 1994. Query expansion using 
lexical-semantic relations. In Proceedings of
A CM SIGIR, Dublin, Ireland. 
Nina Wacholder, Yael Ravin, and Misook Choi. 
1997. Disambiguation of proper names in 
text. In Proceedings ofthe Fifth Applied Nat- 
ural Language Processing Conference, Wash- 
ington, D.C. Association for Computational 
Linguistics. 
157 
Answering What-Is Questions by Virtual Annotation 
 
John Prager  
IBM T.J. Watson Research Center 
Yorktown Heights, N.Y. 10598 
(914) 784-6809 
jprager@us.ibm.com 
Dragomir Radev 
University of Michigan 
Ann Arbor, MI 48109 
(734) 615-5225 
radev@umich.edu 
Krzysztof Czuba  
Carnegie-Mellon University 
Pittsburgh, PA 15213 
(412) 268 6521 
kczuba@cs.cmu.edu 
 
 
ABSTRACT 
We present the technique of Virtual Annotation as a specialization 
of Predictive Annotation for answering definitional What is 
questions.  These questions generally have the property that the 
type of the answer is not given away by the question, which poses 
problems for a system which has to select answer strings from 
suggested passages.  Virtual Annotation uses a combination of 
knowledge-based techniques using an ontology, and statistical 
techniques using a large corpus to achieve high precision. 
 
Keywords 
Question-Answering, Information Retrieval, Ontologies 
 
1. INTRODUCTION 
Question Answering is gaining increased attention in both the 
commercial and academic arenas.  While algorithms for general 
question answering have already been proposed, we find that such 
algorithms fail to capture certain subtleties of particular types of 
questions.  We propose an approach in which different types of 
questions are processed using different algorithms.  We introduce a 
technique named Virtual Annotation (VA) for answering one such 
type of question, namely the What is question. 
 
We have previously presented the technique of Predictive 
Annotation (PA) [Prager, 2000], which has proven to be an 
effective approach to the problem of Question Answering.  The 
essence of PA is to index the semantic types of all entities in the 
corpus, identify the desired answer type from the question, search 
for passages that contain entities with the desired answer type as 
well as the other query terms, and to extract the answer term or 
phrase.  One of the weaknesses of PA, though, has been in dealing 
with questions for which the system cannot determine the correct 
answer type required.  We introduce here an extension to PA 
which we call Virtual Annotation and show it to be effective for 
those ?What is/are (a/an) X? questions that are seeking hypernyms 
of X.  These are a type of definition question, which other QA 
systems attempt to answer by searching in the document collection 
for textual clues similar to those proposed by [Hearst, 1998], that 
are characteristic of definitions.  Such an approach does not use 
the strengths of PA and is not successful in the cases in which a 
deeper understanding of the text is needed in order to identify 
the defining term in question. 
 
We first give a brief description of PA.  We look at a certain 
class of What is questions and describe our basic algorithm.  
Using this algorithm we develop the Virtual Annotation 
technique, and evaluate its performance with respect to both the 
standard TREC and our own benchmark.  We demonstrate on 
two question sets that the precision improves from .15 and .33 to 
.78 and .83 with the addition of VA. 
2.  BACKGROUND 
For our purposes, a question-answering (QA) system is one 
which takes a well-formed user question and returns an 
appropriate answer phrase found in a body of text.  This 
generally excludes How and Why questions from consideration, 
except in the relatively rare cases when they can be answered by 
simple phrases, such as ?by fermenting grapes? or ?because of 
the scattering of light?.  In general, the response of a QA system 
will be a named entity such as a person, place, time, numerical 
measure or a noun phrase, optionally within the context of a 
sentence or short paragraph.  
 
The core of most QA systems participating in TREC [TREC8, 
2000 & TREC9, 2001] is the identification of the answer type 
desired by analyzing the question.  For example, Who questions 
seek people or organizations, Where questions seek places, 
When questions seek times, and so on.  The goal, then, is to find 
an entity of the right type in the text corpus in a context that 
justifies it as the answer to the question.  To achieve this goal, 
we have been using the technique of PA to annotate the text 
corpus with semantic categories (QA-Tokens) prior to indexing. 
 
Each QA-Token is identified by a set of terms, patterns, or 
finite-state machines defining matching text sequences.  Thus 
?Shakespeare? is annotated with ?PERSON$?, and the text 
string ?PERSON$? is indexed at the same text location as 
?Shakespeare?.  Similarly, ?$123.45? is annotated with 
?MONEY$?.  When a question is processed, the desired QA-
Token is identified and it replaces the Wh-words and their 
auxiliaries.  Thus, ?Who? is replaced by ?PERSON$?, and 
?How much? + ?cost? are replaced by ?MONEY$?.  The 
resulting query is then input to the search engine as a bag of 
words.  The expectation here is that if the initial question were 
?Who wrote Hamlet?, for example, then the modified query of 
?PERSON$ write Hamlet? (after lemmatization) would be a 
 
 
 
 perfect match to text that states ?Shakespeare wrote Hamlet? or 
?Hamlet was written by Shakespeare?. 
 
The modified query is matched by the search engine against 
passages of 1-2 sentences, rather than documents.  The top 10 
passages returned are processed by our Answer Selection module 
which re-annotates the text, identifies all potential answer phrases, 
ranks them using a learned evaluation function and selects the top 
5 answers (see [Radev et al, 2000]). 
 
The problem with ?What is/are (a/an) X? questions is that the 
question usually does not betray the desired answer type.  All the 
system can deduce is that it must find a noun phrase (the QA-
Token THING$).  The trouble with THING$ is that it is too 
general and labels a large percentage of the nouns in the corpus, 
and so does not help much in narrowing down the possibilities.  A 
second problem is that for many such questions the desired answer 
type is not one of the approximately 50 high-level classes (i.e. QA-
Tokens) that we can anticipate at indexing; this phenomenon is 
seen in TREC9, whose 24 definitional What is questions are listed 
in the Appendix.  These all appear to be calling out for a 
hypernym.  To handle such questions we developed the technique 
of Virtual Annotation which is like PA and shares much of the 
same machinery, but does not rely on the appropriate class being 
known at indexing time.  We will illustrate with examples from the 
animal kingdom, including a few from TREC9. 
 
3.  VIRTUAL ANNOTATION 
If we look up a word in a thesaurus such as WordNet [Miller et al, 
1993]), we can discover its hypernym tree, but there is no 
indication which hypernym is the most appropriate to answer a 
What is question.  For example, the hypernym hierarchy for 
?nematode? is shown in Table 1.  The level numbering counts 
levels up from the starting term.  The numbers in parentheses will 
be explained later. 
 
Table 1.  Parentage of ?nematode? according to WordNet. 
 
Level Synset 
0 {nematode, roundworm} 
1 {worm(13)} 
2 {invertebrate} 
3 {animal(2), animate being, beast, brute, creature, 
fauna} 
4 {life form(2), organism(3), being, living thing} 
5 {entity, something} 
 
 
At first sight, the desirability of the hypernyms seems to decrease 
with increasing level number.  However, if we examine ?meerkat? 
we find the hierarchy in Table 2. 
 
We are leaving much unsaid here about the context of the question 
and what is known of the questioner, but it is not unreasonable to 
assert that the ?best? answer to ?What is a meerkat? is either ?a 
mammal? (level 4) or ?an animal? (level 7).  How do we get an 
automatic system to pick the right candidate? 
 
 
 
Table 2.  Parentage of ?meerkat? according to WordNet 
 
Level Synset 
0 {meerkat, mierkat} 
1 {viverrine, viverrine mammal} 
2 {carnivore} 
3 {placental, placental mammal, eutherian, eutherian 
mammal} 
4 {mammal} 
5 {vertebrate, craniate} 
6 {chordate} 
7 {animal(2), animate being, beast, brute, creature, 
fauna} 
8 {life form, organism, being, living thing} 
9 {entity, something} 
 
 
It seems very much that what we would choose intuitively as the 
best answer corresponds to Rosch et al?s basic categories 
[Rosch et al, 1976].  According to psychological testing, these 
are categorization levels of intermediate specificity that people 
tend to use in unconstrained settings.  If that is indeed true, then 
we can use online text as a source of evidence for this tendency.  
For example, we might find sentences such as ??  meerkats and 
other Y ? ?, where Y is one of its hypernyms, indicating that Y 
is in some sense the preferred descriptor. 
 
We count the co-occurrences of the target search term (e.g. 
?meerkat? or ?nematode?) with each of its hypernyms (e.g. 
?animal?) in 2-sentence passages, in the TREC9 corpus.  These 
counts are the parenthetical numbers in Tables 1 and 2.  The 
absence of a numerical label there indicates zero co-occurrences.  
Intuitively, the larger the count, the better the corresponding 
term is as a descriptor. 
 
3.1  Hypernym Scoring and Selection  
Since our ultimate goal is to find passages describing the target 
term, discovering zero co-occurrences allows elimination of 
useless candidates.  Of those remaining, we are drawn to those 
with the highest counts, but we would like to bias our system 
away from the higher levels.  Calling a nematode a life-form is 
correct, but hardly helpful.   
 
The top levels of WordNet (or any ontology) are by definition 
very general, and therefore are unlikely to be of much use for 
purposes of definition.  However, if none of the immediate 
parents of a term we are looking up co-occur in our text corpus, 
we clearly will be forced to use a more general term that does.  
We want to go further, though, in those cases where the 
immediate parents do occur, but in small numbers, and the very 
general parents occur with such high frequencies that our 
algorithm would select them.  In those cases we introduce a 
tentative level ceiling to prevent higher-level terms from being 
chosen if there are suitable lower-level alternatives.   
 
We would like to use a weighting function that decreases 
monotonically with level distance.  Mihalcea and  Moldovan 
[1999], in an analogous context, use the logarithm of the number 
of terms in a given term?s subtree to calculate weights, and they 
claim to have shown that this function is optimal.  Since it is 
approximately true that the level population increases 
 exponentially in an ontology, this suggests that a linear function of 
level number will perform just as well. 
 
Our first step is to generate a level-adapted count (LAC) by 
dividing the co-occurrence counts by the level number (we are 
only interested in levels 1 and greater).  We then select the best 
hypernym(s) by using a fuzzy maximum calculation.  We locate 
the one or more hypernyms with greatest LAC, and then also select 
any others with a LAC within a predefined threshold of it; in our 
experimentation we have found that a threshold value of 20% 
works well.  Thus if, for example, a term has one hypernym at 
level 1 with a count of 30, and another at level 2 with a count of 
50, and all other entries have much smaller counts, then since the 
LAC 25 is within 20% of the LAC 30, both of these hypernyms 
will be proposed.   
 
To prevent the highest levels from being selected if there is any 
alternative, we tentatively exclude them from consideration 
according to the following scheme: 
 
If the top of the tree is at level N, where N <= 3, we set a tentative 
ceiling at N-1, otherwise if N<=5, we set the ceiling at N-2, 
otherwise we set the ceiling at N-3.  If no co-occurrences are found 
at or below this ceiling, then it is raised until a positive value is 
found, and the corresponding term is selected.  
 
If no hypernym at all co-occurs with the target term, then this 
approach is abandoned:  the ?What? in the question is replaced by 
?THING$? and normal procedures of Predictive Annotation are 
followed. 
 
When successful, the algorithm described above discovers one or 
more candidate hypernyms that are known to co-occur with the 
target term.  There is a question, though, of what to do when the 
question term has more than one sense, and hence more than one 
ancestral line in WordNet.  We face a choice of either selecting the 
hypernym(s) with the highest overall score as calculated by the 
algorithm described above, or collecting together the best 
hypernyms in each parental branch.  After some experimentation 
we made the latter choice.  One of the questions that benefitted 
from this was ?What is sake?.  WordNet has three senses for sake: 
good (in the sense of welfare), wine (the Japanese drink) and 
aim/end, with computed scores of 122, 29 and 87/99 respectively.  
It seems likely (from the phrasing of the question) that the ?wine? 
sense is the desired one, but this would be missed entirely if only 
the top-scoring hypernyms were chosen. 
 
We now describe how we arrange for our Predictive Annotation 
system to find these answers.  We do this by using these 
descriptors as virtual QA-Tokens; they are not part of the search 
engine index, but are tagged in the passages that the search engine 
returns at run time. 
 
3.2 Integration 
Let us use H to represent either the single hypernym or a 
disjunction of the several hypernyms found through the WordNet 
analysis.  The original question Q =  
?What is (a/an) X? 
is converted to Q? =  
?DEFINE$ X H? 
where DEFINE$ is a virtual QA-Token that was never seen at 
indexing time, does not annotate any text and does not occur in the 
index.  The processed query Q? then will find passages that 
contain occurrences of both X and H; the token DEFINE$ will 
be ignored by the search engine.  The top passages returned by 
the search engine are then passed to Answer Selection, which re-
annotates the text.  However, this time the virtual QA-Token 
DEFINE$ is introduced and the patterns it matches are defined 
to be the disjuncts in H.  In this way, all occurrences of the 
proposed hypernyms of X in the search engine passages are 
found, and are scored and ranked in the regular fashion.  The 
end result is that the top passages contain the target term and one 
of its most frequently co-occurring hypernyms in close 
proximity, and these hypernyms are selected as answers. 
 
When we use this technique of Virtual Annotation on the 
aforementioned questions, we get answer passages such as 
  
?Such genes have been found in nematode worms 
but not yet in higher animals.? 
and 
?South African golfer Butch Kruger had a good 
round going in the central Orange Free State trials, 
until a mongoose-like animal grabbed his ball with 
its mouth and dropped down its hole. Kruger wrote 
on his card: "Meerkat."? 
 
4 RESULTS 
4.1 Evaluation 
We evaluated Virtual Annotation on two sets of questions ? the 
definitional questions from TREC9 and similar kinds of 
questions from the Excite query log (see 
http://www.excite.com).  In both cases we were looking for 
definitional text in the TREC corpus.  The TREC questions had 
been previously verified (by NIST) to have answers there; the 
Excite questions had no such guarantee.   We started with 174 
Excite questions of the form ?What is X?, where X was a 1- or 
2-word phrase.  We removed those questions that we felt would 
not have been acceptable as TREC9 questions.  These were 
questions where: 
o The query terms did not appear in the TREC corpus, 
and some may not even have been real words (e.g. 
?What is a gigapop?).1  37 questions. 
o The query terms were in the corpus, but there was no 
definition present (e.g ?What is a computer 
monitor?).2  18 questions. 
o The question was not asking about the class of the 
term but how to distinguish it from other members of 
the class (e.g. ?What is a star fruit?).  17 questions. 
o The question was about computer technology that 
emerged after the articles in the TREC corpus were 
written (e.g. ?What is a pci slot?).  19 questions. 
o The question was very likely seeking an example, not 
a definition (e.g. ?What is a powerful adhesive?).  1 
question plus maybe some others ? see the Discussion 
                                               
1 That is, after automatic spelling correction was attempted.   
2 The TREC10 evaluation in August 2001 is expected to contain 
questions for which there is no answer in the corpus 
(deliberately).   While it is important for a system to be able to 
make this distinction, we kept within the TREC9 framework for 
this evaluation. 
 section later.  How to automatically distinguish these 
cases is a matter for further research. 
 
Of the remaining 82 Excite questions, 13 did not have entries in 
WordNet.  We did not disqualify those questions. 
 
For both the TREC and Excite question sets we report two 
evaluation measures.  In the TREC QA track, 5 answers are 
submitted per question, and the score for the question is the 
reciprocal of the rank of the first correct answer in these 5 
candidates, or 0 if the correct answer is not present at all.  A 
submission?s overall score is the mean reciprocal rank (MRR) over 
all questions.  We calculate MRR as well as mean binary score 
(MBS) over the top 5 candidates; the binary score for a question is 
1 if a correct answer was present in the top 5 candidates, 0 
otherwise.  The first sets of MBS and MRR figures are for our base 
system, the second set the system with VA. 
 
Table 3.  Comparison of base system and system with VA on 
both TREC9 and Excite definitional questions. 
 
Source No. of 
Questions 
MBS 
w/o 
VA 
MRR 
w/o 
VA 
MBS 
with 
VA 
MRR 
with 
VA 
TREC9  
(in WN) 
20 .3 .2 .9 .9 
TREC9  
(not in WN) 
4 .5 .375 .5 .5 
TREC9 
Overall 
24 .333 .229 .833 .833 
Excite 
(in WN) 
69 .101 .085 .855 
 
.824 
Excite  
(not in WN) 
13 .384 .295 .384 .295 
Excite 
Overall 
82 .146 .118 .780 .740 
 
 
We see that for the 24 TREC9 definitional questions, our MRR 
score with VA was the same as the MBS score.  This was because 
for each of the 20 questions where the system found a correct 
answer, it was in the top position. 
 
By comparison, our base system achieved an overall MRR score of 
.315 across the 693 questions of TREC9.  Thus we see that with 
VA, the average score of definitional questions improves from 
below our TREC average to considerably higher.  While the 
percentage of definitional questions in TREC9 was quite small, we 
shall explain in a later section how we plan to extend our 
techniques to other question types. 
 
4.2  Errors 
The VA process is not flawless, for a variety of reasons.  One is 
that the hierarchy in WordNet does not always exactly correspond 
to the way people classify the world.  For example, in WordNet a 
dog is not a pet, so ?pet? will never even be a candidate answer to 
?What is a dog?. 
 
When the question term is in WordNet, VA succeeds most of the 
time.  One of the error sources is due to the lack of uniformity of 
the semantic distance between levels.  For example, the parents 
of ?architect? are ?creator? and ?human?, the latter being what 
our system answers to ?What is an architect?.  This is 
technically correct, but not very useful.   
 
Another error source is polysemy.  This does not seem to cause 
problems with VA very often ? indeed the co-occurrence 
calculations that we perform are similar to those done by 
[Mihalcea and Moldovan, 1999] to perform word sense 
disambiguation ? but it can give rise to amusing results.  For 
example, when asked ?What is an ass? the system responded 
with ?Congress?.  Ass has four senses, the last of which in 
WordNet is a slang term for sex.  The parent synset contains the 
archaic synonym congress (uncapitalized!).  In the TREC corpus 
there are several passages containing the words ass and 
Congress, which lead to congress being the hypernym with the 
greatest score.  Clearly this particular problem can be avoided 
by using orthography to indicate word-sense, but the general 
problem remains.  
 
5  DISCUSSION AND FURTHER WORK 
5.1  Discussion 
While we chose not to use Hearst?s approach of key-phrase 
identification as the primary mechanism for answering What is 
questions, we don?t reject the utility of the approach.  Indeed, a 
combination of VA as described here with a key-phrase analysis 
to further filter candidate answer passages might well reduce the 
incidence of errors such as the one with ass mentioned in the 
previous section.  Such an investigation remains to be done. 
 
We have seen that VA gives very high performance scores at 
answering What is questions ? and we suggest it can be 
extended to other types ? but we have not fully addressed the 
issue of automatically selecting the questions to which to apply 
it.  We have used the heuristic of only looking at questions of 
the form ?What is (a/an) X? where X is a phrase of one or two 
words.  By inspection of the Excite questions, almost all of those 
that pass this test are looking for definitions, but some - such as 
?What is a powerful adhesive? - very probably do not.  There 
are also a few questions that are inherently ambiguous 
(understanding that the questioners are not all perfect 
grammarians):  is ?What is an antacid? asking for a definition or 
a brand name?  Even if it is known or assumed that a definition 
is required, there remains the ambiguity of the state of 
knowledge of the questioner.  If the person has no clue what the 
term means, then a parent class, which is what VA finds, is the 
right answer.  If the person knows the class but needs to know 
how to distinguish the object from others in the class, for 
example ?What is a star fruit?, then a very different approach is 
required.  If the question seems very specific, but uses common 
words entirely, such as the Excite question ?What is a yellow 
spotted lizard?, then the only reasonable interpretation seems to 
be a request for a subclass of the head noun that has the given 
property.  Finally, questions such as ?What is a nanometer? and 
?What is rubella? are looking for a value or more common 
synonym.   
 
 5.2 Other Question Types 
The preceding discussion has centered upon What is questions and 
the use of WordNet, but the same principles can be applied to other 
question types and other ontologies.  Consider the question ?Where 
is Chicago?, from the training set NIST supplied for TREC8.  Let 
us assume we can use statistical arguments to decide that, in a 
vanilla context, the question is about the city as opposed to the 
rock group, any of the city?s sports teams or the University.  There 
is still considerable ambiguity regarding the granularity of the 
desired answer.  Is it:  Cook County?  Illinois?  The Mid-West?  
The United States?  North America?  The Western Hemisphere? ?  
 
There are a number of geographical databases available, which 
either alone or with some data massaging can be viewed as 
ontologies with ?located within? as the primary relationship.  Then 
by applying Virtual Annotation to Where questions we can find 
the enclosing region that is most commonly referred to in the 
context of the question term.  By manually applying our algorithm 
to ?Chicago? and the list of geographic regions in the previous 
paragraph we find that ?Illinois? wins, as expected, just beating out 
?The United States?.  However, it should be mentioned that a more 
extensive investigation might find a different weighting scheme 
more appropriate for geographic hierarchies. 
 
The aforementioned answer of ?Illinois? to the question ?Where is 
Chicago?? might be the best answer for an American user, but for 
anyone else, an answer providing the country might be preferred.  
How can we expect Virtual Annotation to take this into account?  
The ?hidden variable? in the operation of VA is the corpus.  It is 
assumed that the user belongs to the intended readership of the 
articles in the corpus, and to the extent that this is true, the results 
of VA will be useful to the user.    
 
Virtual Annotation can also be used to answer questions that are 
seeking examples or instances of a class.  We can use WordNet 
again, but this time look to hyponyms.  These questions are more 
varied in syntax than the What is kind;  they include, for example 
from TREC9 again: 
?Name a flying mammal.? 
?What flower did Vincent Van Gogh paint?? 
and 
?What type of bridge is the Golden Gate Bridge?? 
 
6.  SUMMARY 
We presented Virtual Annotation, a technique to extend the 
capabilities of PA to a class of definition questions in which the 
answer type is not easily identifiable.  Moreover, VA can find text 
snippets that do not contain the regular textual clues for presence 
of definitions.  We have shown that VA can considerably improve 
the performance of answering What is questions, and we indicate 
how other kinds of questions can be tackled by similar techniques. 
 
7.  REFERENCES 
[1] Hearst, M.A. ?Automated Discovery of WordNet Relations? 
in WordNet: an Electronic Lexical Database, Christiane 
Fellbaum Ed, MIT Press, Cambridge MA, 1998. 
[2] Mihalcea, R. and Moldovan, D. ?A Method for Word Sense 
Disambiguation of Unrestricted Text?.  Proceedings of the 
37th Annual Meeting of the Association for Computational 
Linguistics (ACL-99), pp. 152-158, College Park, MD, 1999. 
[3] Miller, G. ?WordNet: A Lexical Database for English?, 
Communications of the ACM 38(11) pp. 39-41, 1995. 
[4] Moldovan, D.I. and Mihalcea, R. ?Using WordNet and 
Lexical Operators to Improve Internet Searches?, IEEE 
Internet Computing, pp. 34-43, Jan-Feb 2000.  
[5] Prager, J.M., Radev, D.R., Brown, E.W. and Coden, A.R. 
?The Use of Predictive Annotation for Question-Answering 
in TREC8?, Proceedings of TREC8, Gaithersburg, MD, 
2000. 
[6] Prager, J.M., Brown, E.W., Coden, A.R., and Radev, D.R. 
"Question-Answering by Predictive Annotation", 
Proceedings of SIGIR 2000, pp. 184-191, Athens, Greece, 
2000. 
[7] Radev, D.R., Prager, J.M. and Samn, V. ?Ranking 
Suspected Answers to Natural Language Questions using 
Predictive Annotation?, Proceedings of ANLP?00, Seattle, 
WA, 2000. 
[8] Rosch, E. et al ?Basic Objects in Natural Categories?, 
Cognitive Psychology 8, pp. 382-439, 1976. 
[9] TREC8 - ?The Eighth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
2000. 
[10] TREC9 - ?The Ninth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
to appear. 
 
APPENDIX 
What-is questions from TREC9 
617: What are chloroplasts?  (X) 
528: What are geckos? 
544: What are pomegranates?   
241: What is a caldera?  (X) 
358: What is a meerkat? 
434: What is a nanometer?  (X) 
354: What is a nematode? 
463: What is a stratocaster? 
447: What is anise? 
386: What is anorexia nervosa? 
635: What is cribbage? 
300: What is leukemia? 
305: What is molybdenum? 
644: What is ouzo? 
420: What is pandoro?  (X) 
228: What is platinum? 
374: What is porphyria? 
483: What is sake? 
395: What is saltpeter? 
421: What is thalassemia? 
438: What is titanium? 
600: What is typhoid fever? 
468: What is tyvek? 
539: What is witch hazel? 
 
Our system did not correctly answer the questions marked with 
an ?X?.  For all of the others the correct answer was the first of 
the 5 attempts returned. 
 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 113?120, Vancouver, October 2005. c?2005 Association for Computational Linguistics
113
114
115
116
117
118
119
120
In Question Answering, Two Heads Are Better Than One
Jennifer Chu-Carroll Krzysztof Czuba John Prager Abraham Ittycheriah
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
jencc,kczuba,jprager,abei@us.ibm.com
Abstract
Motivated by the success of ensemble methods
in machine learning and other areas of natu-
ral language processing, we developed a multi-
strategy and multi-source approach to question
answering which is based on combining the re-
sults from different answering agents searching
for answers in multiple corpora. The answer-
ing agents adopt fundamentally different strate-
gies, one utilizing primarily knowledge-based
mechanisms and the other adopting statistical
techniques. We present our multi-level answer
resolution algorithm that combines results from
the answering agents at the question, passage,
and/or answer levels. Experiments evaluating
the effectiveness of our answer resolution algo-
rithm show a 35.0% relative improvement over
our baseline system in the number of questions
correctly answered, and a 32.8% improvement
according to the average precision metric.
1 Introduction
Traditional question answering (QA) systems typically
employ a pipeline approach, consisting roughly of ques-
tion analysis, document/passage retrieval, and answer se-
lection (see e.g., (Prager et al, 2000; Moldovan et al,
2000; Hovy et al, 2001; Clarke et al, 2001)). Although a
typical QA system classifies questions based on expected
answer types, it adopts the same strategy for locating po-
tential answers from the same corpus regardless of the
question classification. In our own earlier work, we de-
veloped a specialized mechanism called Virtual Annota-
tion for handling definition questions (e.g., ?Who was
Galileo?? and ?What are antibiotics??) that consults,
in addition to the standard reference corpus, a structured
knowledge source (WordNet) for answering such ques-
tions (Prager et al, 2001). We have shown that better
performance is achieved by applying Virtual Annotation
and our general purpose QA strategy in parallel. In this
paper, we investigate the impact of adopting such a multi-
strategy and multi-source approach to QA in a more gen-
eral fashion.
Our approach to question answering is additionally
motivated by the success of ensemble methods in ma-
chine learning, where multiple classifiers are employed
and their results are combined to produce the final output
of the ensemble (for an overview, see (Dietterich, 1997)).
Such ensemble methods have recently been adopted in
question answering (Chu-Carroll et al, 2003b; Burger
et al, 2003). In our question answering system, PI-
QUANT, we utilize in parallel multiple answering agents
that adopt different processing strategies and consult dif-
ferent knowledge sources in identifying answers to given
questions, and we employ resolution mechanisms to com-
bine the results produced by the individual answering
agents.
We call our approach multi-strategy since we com-
bine the results from a number of independent agents im-
plementing different answer finding strategies. We also
call it multi-source since the different agents can search
for answers in multiple knowledge sources. In this pa-
per, we focus on two answering agents that adopt fun-
damentally different strategies: one agent uses predomi-
nantly knowledge-based mechanisms, whereas the other
agent is based on statistical methods. Our multi-level
resolution algorithm enables combination of results from
each answering agent at the question, passage, and/or an-
swer levels. Our experiments show that in most cases
our multi-level resolution algorithm outperforms its com-
ponents, supporting a tightly-coupled design for multi-
agent QA systems. Experimental results show signifi-
cant performance improvement over our single-strategy,
single-source baselines, with the best performing multi-
level resolution algorithm achieving a 35.0% relative im-
provement in the number of correct answers and a 32.8%
improvement in average precision, on a previously un-
seen test set.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 24-31
                                                         Proceedings of HLT-NAACL 2003
Answering Agents
KSP
SemanticSearch
KeywordSearch
Question
WordNet
Answer
Cyc
QFrame
QuestionAnalysis QGoals
Knowledge-BasedAnswering Agent
StatisticalAnswering Agent
Aquaintcorpus
TRECcorpus
EB
AnswerResolution
Definition QAnswering Agent
KSP-BasedAnswering Agent
Knowledge Sources
Figure 1: PIQUANT?s Architecture
2 A Multi-Agent QA Architecture
In order to enable a multi-source and multi-strategy ap-
proach to question answering, we developed a modu-
lar and extensible QA architecture as shown in Figure 1
(Chu-Carroll et al, 2003a; Chu-Carroll et al, 2003b).
With a consistent interface defined for each component,
this architecture allows for easy plug-and-play of individ-
ual components for experimental purposes.
In our architecture, a question is first processed by the
question analysis component. The analysis results are
represented as a QFrame, which minimally includes a set
of question features that help activate one or more an-
swering agents. Each answering agent takes the QFrame
and generates its own set of requests to a variety of
knowledge sources. This may include performing search
against a text corpus and extracting answers from the re-
sulting passages, or performing a query against a struc-
tured knowledge source, such as WordNet (Miller, 1995)
or Cyc (Lenat, 1995). The (intermediate) results from
the individual answering agents are then passed on to the
answer resolution component, which combines and re-
solves the set of results, and either produces the system?s
final answers or feeds the intermediate results back to the
answering agents for further processing.
We have developed multiple answering agents, some
general purpose and others tailored for specific ques-
tion types. Figure 1 shows the answering agents cur-
rently available in PIQUANT. The knowledge-based and
statistical answering agents are general-purpose agents
that adopt different processing strategies and consult a
number of different text resources. The definition-Q
agent targets definition questions (e.g., ?What is peni-
cillin?? and ?Who is Picasso??) with a technique called
Virtual Annotation using the external knowledge source
WordNet (Prager et al, 2001). The KSP-based answer-
ing agent focuses on a subset of factoid questions with
specific logical forms, such as capital(?COUNTRY) and
state tree(?STATE). The answering agent sends requests
to the KSP (Knowledge Sources Portal), which returns, if
possible, an answer from a structured knowledge source
(Chu-Carroll et al, 2003a).
In the rest of this paper, we briefly describe our two
general-purpose answering agents. We then focus on a
multi-level answer resolution algorithm, applicable at dif-
ferent points in the QA process of these two answering
agents. Finally, we discuss experiments conducted to dis-
cover effective methods for combining results from mul-
tiple answering agents.
3 Component Answering Agents
We focus on two end-to-end answering agents designed
to answer short, fact-seeking questions from a collection
of text documents, as motivated by the requirements of
the TREC QA track (Voorhees, 2003). Both answer-
ing agents adopt the classic pipeline architecture, con-
sisting roughly of question analysis, passage retrieval,
and answer selection components. Although the answer-
ing agents adopt fundamentally different strategies in
their individual components, they have performed quite
comparably in past TREC QA tracks (Voorhees, 2001;
Voorhees, 2002).
3.1 Knowledge-Based Answering Agent
Our first answering agent utilizes a primarily knowledge-
driven approach, based on Predictive Annotation (Prager
et al, 2000). A key characteristic of this approach is that
potential answers, such as person names, locations, and
dates, in the corpus are predictively annotated. In other
words, the corpus is indexed not only with keywords, as
is typical for most search engines, but also with the se-
mantic classes of these pre-identified potential answers.
During the question analysis phase, a rule-based mech-
anism is employed to select one or more expected an-
swer types, from a set of about 80 classes used in the
predictive annotation process, along with a set of ques-
tion keywords. A weighted search engine query is then
constructed from the keywords, their morphological vari-
ations, synonyms, and the answer type(s). The search en-
gine returns a hit list of typically 10 passages, each con-
sisting of 1-3 sentences. The candidate answers in these
passages are identified and ranked based on three criteria:
1) match in semantic type between candidate answer and
expected answer, 2) match in weighted grammatical rela-
tionships between question and answer passages, and 3)
frequency of answer in candidate passages (redundancy).
The answering agent returns the top n ranked candidate
answers along with a confidence score for each answer.
3.2 Statistical Answering Agent
The second answering agent takes a statistical approach
to question answering (Ittycheriah, 2001; Ittycheriah et
al., 2001). It models the distribution p(c|q, a), which
measures the ?correctness? (c) of an answer (a) to a ques-
tion (q), by introducing a hidden variable representing the
answer type (e) as follows:
p(c|q, a) =
?
e p(c, e|q, a)
=
?
e p(c|e, q, a)p(e|q, a)
p(e|q, a) is the answer type model which predicts, from
the question and a proposed answer, the answer type they
both satisfy. p(c|e, q, a) is the answer selection model.
Given a question, an answer, and the predicted answer
type, it seeks to model the correctness of this configura-
tion. These distributions are modeled using a maximum
entropy formulation (Berger et al, 1996), using training
data which consists of human judgments of question an-
swer pairs. For the answer type model, 13K questions
were annotated with 31 categories. For the answer selec-
tion model, 892 questions from the TREC 8 and TREC 9
QA tracks were used, along with 4K trivia questions.
During runtime, the question is first analyzed by the
answer type model, which selects one out of a set of 31
types for use by the answer selection model. Simultane-
ously, the question is expanded using local context anal-
ysis (Xu and Croft, 1996) with an encyclopedia, and the
top 1000 documents are retrieved by the search engine.
From these documents, the top 100 passages are chosen
that 1) maximize the question word match, 2) have the
desired answer type, 3) minimize the dispersion of ques-
tion words, and 4) have similar syntactic structures as the
question. From these passages, candidate answers are ex-
tracted and ranked using the answer selection model. The
top n candidate answers are then returned, each with an
associated confidence score.
4 Answer Resolution
Given two answering agents with the same pipeline archi-
tecture, there are multiple points in the process at which
(intermediate) results can be combined, as illustrated in
Figure 2. More specifically, it is possible for one answer-
ing agent to provide input to the other after the question
analysis, passage retrieval, and answer selection phases.
In PIQUANT, the knowledge based agent may accept in-
put from the statistical agent after each of these three
phases.1 The contributions from the statistical agent are
taken into consideration by the knowledge based answer-
ing agent in a phase-dependent fashion. The rest of this
section details our combination strategies for each phase.
4.1 Question-Level Combination
One of the key tasks of the question analysis component
is to determine the expected answer type, such as PERSON
for ?Who discovered America?? and DATE for ?When
did World War II end?? This information is taken into ac-
count by most existing QA systems when ranking candi-
date answers, and can also be used in the passage retrieval
process to increase the precision of candidate passages.
We seek to improve the knowledge-based agent?s
performance in passage retrieval and answer selection
through better answer type identification by consulting
the statistical agent?s expected answer type. This task,
however, is complicated by the fact that QA systems em-
ploy different sets of answer types, often with different
granularities and/or with overlapping types. For instance,
while one system may generate ROYALTY for the ques-
tion ?Who was the King of France in 1702??, another
system may produce PERSON as the most specific an-
swer type in its repertoire. This is quite a serious problem
for us as the knowledge based agent uses over 80 answer
types while the statistical agent adopts only 31 categories.
In order to distinguish actual answer type discrepan-
cies from those due to granularity differences, we first
manually created a mapping between the two sets of an-
swer types. This mapping specifies, for each answer type
used by the statistical agent, a set of possible correspond-
ing types used by the knowledge-based agent. For exam-
ple, the GEOLOGICALOBJ class is mapped to a set of finer
grained classes: RIVER, MOUNTAIN, LAKE, and OCEAN.
At processing time, the statistical agent?s answer type
is mapped to the knowledge-based agent?s classes (SA-
1Although it is possible for the statistical agent to receive
input from the knowledge based agent as well, we have not pur-
sued that option because of implementation issues.
Question
Analysis 1
Passage
Retrieval 1
Answer
Selection 1
passages answers
Question
Analysis 2
Passage
Retrieval 2
Answer
Selection 2
Agent 1 (Knowledge-Based)
Agent 2 (Statistical)
question
typeQFrame
Figure 2: Answer Resolution Strategies
types), which are then merged with the answer type(s) se-
lected by the knowledge-based agent itself (KBA-types)
as follows:
1. If the intersection of KBA-types and SA-types is
non-null, i.e., the two agents produced consistent an-
swer types, then the merged set is KBA-types.
2. Otherwise, the two sets of answer types are truly
in disagreement, and the merged set is the union of
KBA-types and SA-types.
The merged answer types are then used by the
knowledge-based agent in further processing.
4.2 Passage-Level Combination
The passage retrieval component selects, from a large text
corpus, a small number of short passages from which an-
swers are identified. Oftentimes, multiple passages that
answer a question are retrieved. Some of these passages
may be better suited than others for the answer selection
algorithm employed downstream. For example, consider
?When was Benjamin Disraeli prime minister??, whose
answer can be found in both passages below:
1. Benjamin Disraeli, who had become prime minister
in 1868, was born into Judaism but was baptized a
Christian at the age of 12.
2. France had a Jewish prime minister in 1936, Eng-
land in 1868, and Spain, of all countries, in 1835,
but none of them, Leon Blum, Benjamin Disraeli or
Juan Alvarez Mendizabel, were devoutly observant,
as Lieberman is.
Although the correct answer, 1868, is present in both
passages, it is substantially easier to identify the answer
from the first passage, where it is directly stated, than
from the second passage, where recognition of parallel
constructs is needed to identify the correct answer.
Because of strategic differences in question analysis
and passage retrieval, our two answering agents often re-
trieve different passages for the same question. Thus, we
perform passage-level combination to make a wider va-
riety of passages available to the answer selection com-
ponent, as shown in Figure 2. The potential advantages
are threefold. First, passages from agent 2 may contain
answers absent in passages retrieved by agent 1. Sec-
ond, agent 2 may have retrieved passages better suited for
the downstream answer selection algorithm than those re-
trieved by agent 1. Third, passages from agent 2 may con-
tain additional occurrences of the correct answer, which
boosts the system?s confidence in the answer through the
redundancy measure.2
Our passage-level combination algorithm adds to the
passages extracted by the knowledge-based agent the top-
ranked passages from the statistical agent that contain
candidate answers of the right type. More specifically,
the statistical agent?s passages are semantically annotated
and the top 10 passages containing at least one candidate
of the expected answer type(s) are selected.3
4.3 Answer-Level Combination
The answer selection component identifies, from a set
of passages, the top n answers for the given question,
with their associated confidence scores. An answer-level
combination algorithm takes the top answer(s) from the
individual answering agents and determines the overall
best answer(s). Of our three combination algorithms, this
most closely resembles traditional ensemble methods, as
voting takes place among the end results of individual an-
2On the other hand, such redundancy may result in error
compounding, as discussed in Section 5.3.
3We selected the top 10 passages so that the same number
of passages are considered from both answering agents.
swering agents to determine the final output of the ensem-
ble.
We developed two answer-level combination algo-
rithms, both utilizing a simple confidence-based voting
mechanism, based on the premise that answers selected
by both agents with high confidence are more likely to
be correct than those identified by only one agent.4 In
both algorithms, named entity normalization is first per-
formed on all candidate answers considered. In the first
algorithm, only the top answer from each agent is taken
into account. If the two top answers are equivalent, the
answer is selected with the combined confidence from
both agents; otherwise, the more confident answer is se-
lected.5 In the second algorithm, the top 5 answers from
each agent are allowed to participate in the voting pro-
cess. Each instance of an answer votes with a weight
equal to its confidence value and the weights of equiv-
alent answers are again summed. The answer with the
highest weight, or confidence value, is selected as the
system?s final answer. Since in our evaluation, the second
algorithm uniformly outperforms the first, it is adopted as
our answer-level combination algorithm in the rest of the
paper.
5 Performance Evaluation
5.1 Experimental Setup
To assess the effectiveness of our multi-level answer res-
olution algorithm, we devised experiments to evaluate the
impact of the question, passage, and answer-level combi-
nation algorithms described in the previous section.
The baseline systems are the knowledge-based and sta-
tistical agents performing individually against a single
reference corpus. In addition, our earlier experiments
showed that when employing a single answer finding
strategy, consulting multiple text corpora yielded better
performance than using a single corpus. We thus con-
figured a version of our knowledge-based agent to make
use of three available text corpora,6 the AQUAINT cor-
pus (news articles from 1998-2000), the TREC corpus
(news articles from 1988-1994),7 and a subset of the En-
cyclopedia Britannica. This multi-source version of the
knowledge-based agent will be used in all answer resolu-
tion experiments in conjunction with the statistical agent.
We configured multiple versions of PIQUANT to eval-
uate our question, passage, and answer-level combination
4In future work we will be investigating weighted voting
schemes based on question features.
5The confidence values from both answering agents are nor-
malized to be between 0 and 1.
6The statistical agent is currently unable to consult multiple
corpora.
7Both the AQUAINT and TREC corpora are available from
the Linguistics Data Consortium, http://www.ldc.org.
algorithms individually and cumulatively. For cumula-
tive effects, we 1) combined the algorithms pair-wise,
and 2) employed all three algorithms together. The two
test sets were selected from the TREC 10 and 11 QA
track questions (Voorhees, 2002; Voorhees, 2003). For
both test sets, we eliminated those questions that did not
have known answers in the reference corpus. Further-
more, from the TREC 10 test set, we discarded all defini-
tion questions,8 since the knowledge-based agent adopts
a specialized strategy for handling definition questions
which greatly reduces potential contributions from other
answering agents. This results in a TREC 10 test set of
313 questions and a TREC 11 test set of 453 questions.
5.2 Experimental Results
We ran each of the baseline and combined systems on the
two test sets. For each run, the system outputs its top
answer and its confidence score for each question. All
answers for a run are then sorted in descending order of
the confidence scores. Two established TREC QA eval-
uation metrics are adopted to assess the results for each
run as follows:
1. % Correct: Percentage of correct answers.
2. Average Precision: A confidence-weighted score
that rewards systems with high confidence in cor-
rect answers as follows, where N is the number of
questions:
1
N
N?
i=1
# correct up to question i/i
Table 1 shows our experimental results. The top sec-
tion shows the comparable baseline results from the sta-
tistical agent (SA-SS) and the single-source knowledge-
based agent (KBA-SS). It also includes results for the
multi-source knowledge-based agent (KBA-MS), which
improve upon those for its single-source counterpart
(KBA-SS).
The middle section of the table shows the answer
resolution results, including applying the question, pas-
sage, and answer-level combination algorithms individu-
ally (Q, P, and A, respectively), applying them pair-wise
(Q+P, P+A, and Q+A), and employing all three algo-
rithms (Q+P+A). Finally, the last row of the table shows
the relative improvement by comparing the best perform-
ing system configuration (highlighted in boldface) with
the better performing single-source, single-strategy base-
line system (SA-SS or KBA-SS, in italics).
Overall, PIQUANT?s multi-strategy and multi-source
approach achieved a 35.0% relative improvement in the
8Definition questions were intentionally excluded by the
track coordinator in the TREC 11 test set.
TREC 10 (313) TREC 11 (453)
% Corr Avg Prec % Corr Avg Prec
SA-SS 36.7% 0.569 32.9% 0.534
KBA-SS 39.6% 0.595 32.5% 0.531
KBA-MS 43.8% 0.641 38.2% 0.622
Q 44.7% 0.647 38.9% 0.632
P 49.5% 0.661 40.0% 0.627
A 49.5% 0.712 43.5% 0.704
Q+P 48.9% 0.656 41.1% 0.640
P+A 51.1% 0.711 44.2% 0.686
Q+A 49.8% 0.716 43.9% 0.709
Q+P+A 50.8% 0.706 44.4% 0.690
rel. improv. 29.0% 20.3% 35.0% 32.8%
Table 1: Experimental Results
number of correct answers and a 32.8% improvement in
average precision on the TREC 11 data set. Of the com-
bined improvement, approximately half was achieved by
the multi-source aspect of PIQUANT, while the other half
was obtained by PIQUANT?s multi-strategy feature. Al-
though the absolute average precision values are com-
parable on both test sets and the absolute percentage of
correct answers is lower on the TREC 11 data, the im-
provement is greater on TREC 11 in both cases. This
is because the TREC 10 questions were taken into ac-
count for manual rule refinement in the knowledge-based
agent, resulting in higher baselines on the TREC 10 test
set. We believe that the larger improvement on the previ-
ously unseen TREC 11 data is a more reliable estimate of
PIQUANT?s performance on future test sets.
We applied an earlier version of our combination algo-
rithms, which performed between our current P and P+A
algorithms, in our submission to the TREC 11 QA track.
Using the average precision metric, that version of PI-
QUANT was among the top 5 best performing systems
out of 67 runs submitted by 34 groups.
5.3 Discussion and Analysis
A cursory examination of the results in Table 1 allows
us to draw two general conclusions about PIQUANT?s
performance. First, all three combination algorithms ap-
plied individually improved upon the baseline using both
evaluation metrics on both test sets. In addition, overall
performance is generally better the later in the process
the combination occurs, i.e., the answer-level combina-
tion algorithm outperformed the passage-level combina-
tion algorithm, which in turn outperformed the question-
level combination algorithm. Second, the cumulative im-
provement from multiple combination algorithms is in
general greater than that from the components. For in-
stance, the Q+A algorithm uniformly outperformed the Q
and A algorithms alone. Note, however, that the Q+P+A
algorithm achieved the highest performance only on the
TREC 11 test set using the % correct metric. We believe
KBA
TREC 10 (313) TREC 11 (453)
+ - + -
SA + 185 43 254 58
- 24 61 41 100
Table 2: Passage Retrieval Analysis
that this is because of compounding errors that occurred
during the multiple combination process.
In ensemble methods, the individual components must
make different mistakes in order for the combined sys-
tem to potentially perform better than the component sys-
tems (Dietterich, 1997). We examined the differences
in results between the two answering agents from their
question analysis, passage retrieval, and answer selection
components. We focused our analysis on the potential
gain/loss from incorporating contributions from the sta-
tistical agent, and how the potential was realized as actual
performance gain/loss in our end-to-end system.
At the question level, we examined those questions
for which the two agents proposed incompatible answer
types. On the TREC 10 test set, the statistical agent in-
troduced correct answer types in 6 cases and incorrect
answer types in 9 cases. As a result, in some cases the
question-level combination algorithm improved system
performance (comparing A and Q+A) and in others it
degraded performance (comparing P and Q+P). On the
other hand, on the TREC 11 test set, the statistical agent
introduced correct and incorrect answer types in 15 and
6 cases, respectively. As a result, in most cases perfor-
mance improved when the question-level combination al-
gorithm was invoked. The difference in question analysis
performance again reflects the fact that TREC 10 ques-
tions were used in question analysis rule refinement in
the knowledge-based agent.
At the passage level, we examined, for each ques-
tion, whether the candidate passages contained the cor-
rect answer. Table 2 shows the distribution of ques-
tions for which correct answers were (+) and were not
(-) present in the passages for both agents. The bold-
faced cells represent questions for which the statistical
agent retrieved passages with correct answers while the
knowledge-based agent did not. There were 43 and 58
such questions in the TREC 10 and TREC 11 test sets, re-
spectively, and employing the passage-level combination
algorithm resulted only in an additional 18 and 8 correct
answers on each test set. This is because the statistical
agent?s proposes in its 10 passages, on average, 29 candi-
date answers, most of which are incorrect, of the proper
semantic type per question. As the downstream answer
selection component takes redundancy into account in an-
swer ranking, incorrect answers may reinforce one an-
other and become top ranked answers. This suggests that
KBA
TREC 10 (313) TREC 11 (453)
1st 2-5th none 1st 2-5th none
SA 1st 66 22 26 93 21 35
2-5th 26 9 13 29 19 22
none 45 14 92 51 21 162
Table 3: Answer Voting Analysis
the relative contributions of our answer selection features
may not be optimally tuned for our multi-agent approach
to QA. We plan to investigate this issue in future work.
At the answer level, we analyzed each agent?s top 5
answers, used in the combination algorithm?s voting pro-
cess. Table 3 shows the distribution of questions for
which an answer was found in 1st place, in 2nd-5th place,
and not found in top 5. Since we employ a linear vot-
ing strategy based on confidence scores, we classify the
cells in Table 3 as follows based on the perceived likeli-
hood that the correct answers for questions in each cell
wins in the voting process. The boldfaced and underlined
cells contain highly likely candidates, since a correct an-
swer was found in 1st place by both agents.9 The bold-
faced cells consist of likely candidates, since a 1st place
correct answer was supported by a 2nd-5th place answer.
The italicized and underlined cells contain possible can-
didates, while the rest of the cells cannot produce correct
1st place answers using our current voting algorithm. On
TREC 10 data, 194 questions fall into the highly likely,
likely, and possible categories, out of which the voting al-
gorithm successfully selected 155 correct answers in 1st
place. On TREC 11 data, 197 correct answers were se-
lected out of 248 questions that fall into these categories.
These results represent success rates of 79.9% and 79.4%
for our answer-level combination algorithm on the two
test sets.
6 Related Work
There has been much work in employing ensemble meth-
ods to increase system performance in machine learning.
In NLP, such methods have been applied to tasks such
as POS tagging (Brill and Wu, 1998), word sense dis-
ambiguation (Pedersen, 2000), parsing (Henderson and
Brill, 1999), and machine translation (Frederking and
Nirenburg, 1994).
In question answering, a number of researchers have
investigated federated systems for identifying answers to
questions. For example, (Clarke et al, 2003) and (Lin et
al., 2003) employ techniques for utilizing both unstruc-
9These cells are not marked as definite because in a small
number of cases, the two answers are not equivalent. For exam-
ple, for the TREC 9 question, ?Who is the emperor of Japan??,
Hirohito, Akihito, and Taisho are all considered correct answers
based on the reference corpus.
tured text and structured databases for question answer-
ing. However, the approaches taken by both these sys-
tems differ from ours in that they enforce an order be-
tween the two strategies by attempting to locate answers
in structured databases first for select question types and
falling back to unstructured text when the former fails,
while we explore both options in parallel and combine
the results from multiple answering agents.
The multi-agent approach to question answering most
similar to ours is that by Burger et al (2003). They
applied ensemble methods to combine the 67 runs sub-
mitted to the TREC 11 QA track, using an unweighted
centroid method for selecting among the 67 proposed an-
swers for each question. However, their combined sys-
tem did not outperform the top scoring system(s). Fur-
thermore, their approach differs from ours in that they fo-
cused on combining the end results of a large number of
systems, while we investigated a tightly-coupled design
for combining two answering agents.
7 Conclusions
In this paper, we introduced a multi-strategy and multi-
source approach to question answering that enables com-
bination of answering agents adopting different strategies
and consulting multiple knowledge sources. In partic-
ular, we focused on two answering agents, one adopt-
ing a knowledge-based approach and one using statistical
methods. We discussed our answer resolution component
which employs a multi-level combination algorithm that
allows for resolution at the question, passage, and answer
levels. Best performance using the % correct metric was
achieved by the three-level algorithm that combines af-
ter each stage, while highest average precision was ob-
tained by a two-level algorithm merging at the question
and answer levels, supporting a tightly-coupled design
for multi-agent question answering. Our experiments
showed that our best performing algorithms achieved a
35.0% relative improvement in the number of correct an-
swers and a 32.8% improvement in average precision on
a previously unseen test set.
Acknowledgments
We would like to thank Dave Ferrucci, Chris Welty, and
Salim Roukos for helpful discussions, Diane Litman and
the anonymous reviewers for their comments on an ear-
lier draft of this paper. This work was supported in
part by the Advanced Research and Development Ac-
tivity (ARDA)?s Advanced Question Answering for In-
telligence (AQUAINT) Program under contract number
MDA904-01-C-0988.
References
Adam L. Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics, pages 191?195.
John D. Burger, Lisa Ferro, Warren Greiff, John Hender-
son, Marc Light, and Scott Mardis. 2003. MITRE?s
Qanda at TREC-11. In Proceedings of the Eleventh
Text Retrieval Conference. To appear.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003a. Hybridization in ques-
tion answering systems. In Working Notes of the AAAI
Spring Symposium on New Directions in Question An-
swering, pages 116?121.
Jennifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003b. A
multi-strategy and multi-source approach to question
answering. In Proceedings of the Eleventh Text Re-
trieval Conference. To appear.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
C.L.A. Clarke, G.V. Cormack, G. Kemkes, M. Laszlo,
T.R. Lynam, E.L. Terra, and P.L. Tilker. 2003. Statis-
tical selection of exact answers. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4):97?136.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Fourth
Conference on Applied Natural Language Processing.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the 4th Conference on Em-
pirical Methods in Natural Language Processing.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2001. Question answering
in Webclopedia. In Proceedings of the Ninth Text RE-
trieval Conference, pages 655?664.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. Question answering using
maximum entropy components. In Proceedings of the
2nd Conference of the North American Chapter of the
Association for Computational Linguistics, pages 33?
39.
Abraham Ittycheriah. 2001. Trainable Question Answer-
ing Systems. Ph.D. thesis, Rutgers - The State Univer-
sity of New Jersey.
Douglas B. Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11).
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory Mar-
ton, and Stefanie Tellex. 2003. Extracting an-
swers from the web using knowledge annotation and
knowledge mining techniques. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
George Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 563?570.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 63?69.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive anno-
tation. In Proceedings of the 23rd SIGIR Conference,
pages 184?191.
John Prager, Dragomir Radev, and Krzysztof Czuba.
2001. Answering what-is questions by virtual anno-
tation. In Proceedings of Human Language Technolo-
gies Conference, pages 26?30.
Ellen M. Voorhees. 2001. Overview of the TREC-9
question answering track. In Proceedings of the 9th
Text Retrieval Conference, pages 71?80.
Ellen M. Voorhees. 2002. Overview of the TREC 2001
question answering track. In Proceedings of the 10th
Text Retrieval Conference, pages 42?51.
Ellen M. Voorhees. 2003. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text Retrieval Conference. To appear.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of the 19th SIGIR Conference, pages 4?11.
Question Answering using Constraint Satisfaction:                                       
QA-by-Dossier-with-Constraints  
John Prager  
T.J. Watson Research Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Jennifer Chu-Carroll 
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
Krzysztof Czuba  
T.J. Watson Research Ctr. 
Yorktown Heights  
N.Y. 10598 
kczuba@us.ibm.com 
 
 
 
Abstract 
QA-by-Dossier-with-Constraints is a new ap-
proach to Question Answering whereby candi-
date answers? confidences are adjusted by 
asking auxiliary questions whose answers con-
strain the original answers.  These constraints 
emerge naturally from the domain of interest, 
and enable application of real-world knowledge 
to QA.  We show that our approach signifi-
cantly improves system performance (75% rela-
tive improvement in F-measure on select 
question types) and can create a ?dossier? of in-
formation about the subject matter in the origi-
nal question. 
1 Introduction 
Traditionally, Question Answering (QA) has 
drawn on the fields of Information Retrieval, Natural 
Language Processing (NLP), Ontologies, Data Bases 
and Logical Inference, although it is at heart a prob-
lem of NLP.  These fields have been used to supply 
the technology with which QA components have 
been built.  We present here a new methodology 
which attempts to use QA holistically, along with 
constraint satisfaction, to better answer questions, 
without requiring any advances in the underlying 
fields. 
Because NLP is still very much an error-prone 
process, QA systems make many mistakes; accord-
ingly, a variety of methods have been developed to 
boost the accuracy of their answers.  Such methods 
include redundancy (getting the same answer from 
multiple documents, sources, or algorithms), deep 
parsing of questions and texts (hence improving the 
accuracy of confidence measures), inferencing 
(proving the answer from information in texts plus 
background knowledge) and sanity-checking (veri-
fying that answers are consistent with known facts).  
To our knowledge, however, no QA system deliber-
ately asks additional questions in order to derive 
constraints on the answers to the original questions.  
We have found empirically that when our own 
QA system?s (Prager et al, 2000; Chu-Carroll et al, 
2003) top answer is wrong, the correct answer is 
often present later in the ranked answer list.  In other 
words, the correct answer is in the passages re-
trieved by the search engine, but the system was un-
able to sufficiently promote the correct answer 
and/or deprecate the incorrect ones.  Our new ap-
proach of QA-by-Dossier-with-Constraints (QDC) 
uses the answers to additional questions to provide 
more information that can be used in ranking candi-
date answers to the original question.  These auxil-
iary questions are selected such that natural 
constraints exist among the set of correct answers.  
After issuing both the original question and auxiliary 
questions, the system evaluates all possible combi-
nations of the candidate answers and scores them by 
a simple function of both the answers? intrinsic con-
fidences, and how well the combination satisfies the 
aforementioned constraints.  Thus we hope to im-
prove the accuracy of an essentially NLP task by 
making an end-run around some of the more diffi-
cult problems in the field. 
We describe QDC and experiments to evaluate its 
effectiveness. Our results show that on our test set, 
substantial improvement is achieved by using con-
straints, compared with our baseline system, using 
standard evaluation metrics. 
2 Related Work 
Logic and inferencing have been a part of Ques-
tion-Answering since its earliest days.  The first 
such systems employed natural-language interfaces 
to expert systems, e.g.  SHRDLU (Winograd, 1972), 
or to databases e.g. LUNAR (Woods, 1973) and 
LIFER/LADDER (Hendrix et al 1977).  CHAT-80 
(Warren & Pereira, 1982) was a DCG-based NL-
query system about world geography, entirely in 
Prolog.  In these systems, the NL question is trans-
formed into a semantic form, which is then proc-
essed further; the overall architecture and system 
operation is very different from today?s systems, 
however, primarily in that there is no text corpus to 
process. 
Inferencing is used in at least two of the more 
visible systems of the present day.  The LCC system 
(Moldovan & Rus, 2001) uses a Logic Prover to 
establish the connection between a candidate answer 
passage and the question.  Text terms are converted 
to logical forms, and the question is treated as a goal 
which is ?proven?, with real-world knowledge being 
provided by Extended WordNet.  The IBM system 
PIQUANT (Chu-Carroll et al, 2003) uses Cyc (Le-
nat, 1995) in answer verification.  Cyc can in some 
cases confirm or reject candidate answers based on 
its own store of instance information; in other cases, 
primarily of a numerical nature, Cyc can confirm 
whether candidates are within a reasonable range 
established for their subtype.   
At a more abstract level, the use of constraints 
discussed in this paper can be viewed as simply an 
example of finding support (or lack of it) for candi-
date answers.  Many current systems (see, e.g. 
(Clarke et al, 2001), (Prager et al, 2004)) employ 
redundancy as a significant feature of operation:  if 
the same answer appears multiple times in an inter-
nal top-n list, whether from multiple sources or mul-
tiple algorithms/agents, it is given a confidence 
boost, which will affect whether and how it gets re-
turned to the end-user. 
 Finally, our approach is somewhat reminiscent of 
the scripts introduced by Schank (Schank et al, 
1975, and see also Lehnert, 1978). In order to gener-
ate meaningful auxiliary questions and constraints, 
we need a model (?script?) of the situation the ques-
tion is about.  Among others, we have identified one 
such script modeling the human life cycle that seems 
common to different question types regarding peo-
ple.   
3 Introducing QDC 
QA-by-Dossier-with-Constraints is an extension 
of on-going work of ours called QA-by-Dossier 
(QbD) (Prager et al, 2004).  In the latter, defini-
tional questions of the form ?Who/What is X? are 
answered by asking a set of specific factoid ques-
tions about properties of X.  So if X is a person, for 
example, these auxiliary questions may be about 
important dates and events in the person?s life-cycle, 
as well as his/her achievement.  Likewise, question 
sets can be developed for other entities such as or-
ganizations, places and things.  
QbD employs the notion of follow-on questions.  
Given an answer to a first-round question, the sys-
tem can ask more specific questions based on that 
knowledge.  For example, on discovering a person?s 
profession, it can ask occupation-specific follow-on 
questions: if it finds that people are musicians, it can 
ask what they have composed, if it finds they are 
explorers, then what they have discovered, and so 
on. 
QA-by-Dossier-with-Constraints extends this ap-
proach by capitalizing on the fact that a set of an-
swers about a subject must be mutually consistent, 
with respect to constraints such as time and geogra-
phy.   The essence of the QDC approach is to ini-
tially return instead of the best answer to 
appropriately selected factoid questions, the top n 
answers (we use n=5), and to choose out of this top 
set the highest confidence answer combination that 
satisfies consistency constraints. 
We illustrate this idea by way of the example, 
?When did Leonardo da Vinci paint the Mona 
Lisa??.  Table 1 shows our system?s top answers to 
this question, with associated scores in the range    
0-1. 
 
 Score Painting Date 
1 .64 2000 
2 .43 1988 
3 .34 1911 
4 .31 1503 
5 .30 1490 
 
Table 1.  Answers for ?When did Leonardo da 
Vinci paint the Mona Lisa?? 
 
The correct answer is ?1503?, which is in 4th 
place, with a low confidence score.  Using QA-by-
Dossier, we ask two related questions ?When was 
Leonardo da Vinci born?? and ?When did Leonardo 
da Vinci die??  The answers to these auxiliary ques-
tions are shown in Table 2.   
Given common knowledge about a person?s life 
expectancy and that a painting must be produced 
while its author is alive, we observe that the best 
dates proposed in Table 2 consistent with one an-
other are that Leonardo da Vinci was born in 1452, 
died in 1519, and painted the Mona Lisa in 1503.  
[The painting date of 1490 also satisfies the con-
straints, but with a lower confidence.]  We will ex-
amine the exact constraints used a little later.  This 
example illustrates how the use of auxiliary ques-
tions helps constrain answers to the original ques-
tion, and promotes correct answers with initial low 
confidence scores.  As a side-effect, a short dossier 
is produced. 
 
 
 Score Born  Score Died 
1 .66 1452  .99 1519 
2 .12 1519  .98 1989 
3 .04 1920  .96 1452 
4 .04 1987  .60 1988 
5 .04 1501  .60 1990 
Table 2.  Answers for auxiliary questions ?When 
was Leonardo da Vinci born?? and ?When did Leo-
nardo da Vinci die??. 
3.1 Reciprocal Questions 
QDC also employs the notion of reciprocal ques-
tions.  These are a type of follow-on question used 
solely to provide constraints, and do not add to the 
dossier.  The idea is simply to double-check the an-
swer to a question by inverting it, substituting the 
first-round answer and hoping to get the original 
subject back.  For example, to double-check ?Sac-
ramento? as the answer to ?What is the capital of 
California?? we would ask ?Of what state is Sacra-
mento the capital??.  The reciprocal question would 
be asked of all of the candidate answers, and the 
confidences of the answers to the reciprocal ques-
tions would contribute to the selection of the opti-
mum answer.  We will discuss later how this 
reciprocation may be done automatically.  In a sepa-
rate study of reciprocal questions (Prager et al, 
2004), we demonstrated an increase in precision 
from .43 to .95, with only a 30% drop in recall. 
Although the reciprocal questions seem to be 
symmetrical and thus redundant, their power stems 
from the differences in the search for answers inher-
ent in our system. The search is primarily based on 
the expected answer type (STATE vs. CAPITAL in 
the above example). This results in different docu-
ment sets being passed to the answer selection mod-
ule. Subsequently, the answer selection module 
works with a different set of syntactic and semantic 
relationships, and the process of asking a reciprocal 
question ends up looking more like the process of 
asking an independent one. The only difference be-
tween this and the ?regular? QDC case is in the type 
of constraint applied to resolve the resulting answer 
set. 
3.2 Applying QDC 
In order to automatically apply QDC during ques-
tion answering, several problems need to be ad-
dressed.  First, criteria must be developed to 
determine when this process should be invoked.  
Second, we must identify the set of question types 
that would potentially benefit from such an ap-
proach, and, for each question type, develop a set of 
auxiliary questions and appropriate constraints 
among the answers.  Third, for each question type, 
we must determine how the results of applying con-
straints should be utilized.  
3.2.1 When to apply QDC 
To address these questions we must distinguish 
between ?planned? and ?ad-hoc? uses of QDC.  For 
answering definitional questions (?Who/what is 
X??) of the sort used in TREC2003, in which collec-
tions of facts can be gathered by QA-by-Dossier, we 
can assume that QDC is always appropriate.  By 
defining broad enough classes of entities for which 
these questions might be asked (e.g. people, places, 
organizations and things, or major subclasses of 
these), we can for each of these classes manually 
establish once and for all a set of auxiliary questions 
for QbD and constraints for QDC.  This is the ap-
proach we have taken in the experiments reported 
here.  We are currently working on automatically 
learning effective auxiliary questions for some of 
these classes. 
In a more ad-hoc situation, we might imagine that 
a simple variety of QDC will be invoked using 
solely reciprocal questions whenever the difference 
between the scores of the first and second answer is 
below a certain threshold.    
3.2.2 How to apply QDC 
We will posit three methods of generating auxil-
iary question sets: 
o By hand 
o Through a structured repository, such as a 
knowledge-base of real-world information 
o Through statistical techniques tied to a machine-
learning algorithm, and a text corpus. 
We think that all three methods are appropriate, 
but we initially concentrate on the first for practical 
reasons.  Most TREC-style factoid questions are 
about people, places, organizations, and things, and 
we can generate generic auxiliary question sets for 
each of these classes.  Moreover, the purpose of this 
paper is to explain the QDC methodology and to 
investigate its value.   
3.2.3 Constraint Networks 
The constraints that apply to a given situation can 
be naturally represented in a network, and we find it 
useful for visualization purposes to depict the con-
straints graphically.  In such a graph the entities and 
values are represented as nodes, and the constraints 
and questions as edges.   
It is not clear how possible, or desirable, it is to 
automatically develop such constraint networks 
(other than the simple one for reciprocal questions), 
since so much real-world knowledge seems to be 
required.  To illustrate, let us look at the constraints 
required for the earlier example.  A more complex 
constraint system is used in our experiments de-
scribed later.  For our Leonardo da Vinci example, 
the set of constraints applied can be expressed as 
follows1: 
 
Date(Died) <= Date(Born) + 100 
Date(Painting) >=  Date(Born) + 7 
Date(Painting) <=  Date(Died) 
 
The corresponding graphical representation is in 
Figure 1.  Although the numerical constants in these 
constraints betray a certain arbitrariness, we found it 
a useful practice to find a middle ground between 
absolute minima or maxima that the values can 
achieve and their likely values.  Furthermore, al-
though these constraints are manually derived for 
our prototype system, they are fairly general for the 
human life-cycle and can be easily reused for other, 
similar questions, or for more complex dossiers, as 
described below. 
 
 
 
Figure 1.  Constraint Network for Leonardo ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We also note that even though a constraint net-
work might have been inspired by and centered 
around a particular question, once the network is 
established, any question employed in it could be the 
end-user question that triggers it. 
There exists the (general) problem of when more 
than one set of answers satisfies our constraints.  
Our approach is to combine the first-round scores of 
the individual answers to provide a score for the 
dossier as a whole.  There are several ways to do 
this, and we found experimentally that it does not 
appear critical exactly how this is done.  In the ex-
ample in the evaluation we mention one particular 
combination algorithm. 
3.2.4 Kinds of constraint network 
There are an unlimited number of possible con-
straint networks that can be constructed.  We have 
experimented with the following: 
Timelines.  People and even artifacts have life-
cycles.  The examples in this paper exploit these. 
                                                        
1 Painting is only an example of an activity in these constraints. 
Any other achievement that is usually associated with adulthood 
can be used. 
Geographic (?Where is X?).  Neighboring entities 
are in the same part of the world.   
Kinship (?Who is married to X?).  Most kinship 
relationships have named reciprocals e.g. husband-
wife, parent-child, and cousin-cousin.  Even though 
these are not in practice one-one relationships, we 
can take advantage of sufficiency even if necessity is 
not entailed. 
Definitional (?What is X??, ?What does XYZ stand 
for??)   For good definitions, a term and its defini-
tion are interchangeable. 
Part-whole.  Sizes of parts are no bigger than sizes 
of wholes.  This fact can be used for populations, 
areas, etc. 
3.2.5 QDC potential 
We performed a manual examination of the 500 
TREC2002 questions2 to see for how many of these 
questions the QDC framework would apply.  Being 
a manual process, these numbers provide an upper 
bound on how well we might expect a future auto-
matic process to work.  
We noted that for 92 questions (18%) a non-
trivial constraint network of the above kinds would 
apply.  For a total of 454 questions (91%), a simple 
reciprocal constraint could be generated.  However, 
for 61 of those, the reciprocal question was suffi-
ciently non-specific that the sought reciprocal an-
swer was unlikely to be found in a reasonably-sized 
hit-list.  For example, the reciprocal question to 
?How did Mickey Mantle die?? would be ?Who died 
of cancer??  However, we can imagine using other 
facts in the dossier to craft the question, giving us 
?What famous baseball player (or Yankees player) 
died of cancer??, giving us a much better chance of 
success.  For the simple reciprocation, though, sub-
tracting these doubtful instances leaves 79% of the 
questions appearing to be good candidates for QDC. 
4 Experimental Setup 
4.1 Test set generation 
To evaluate QDC, we had our system develop 
dossiers of people in the creative arts, unseen in pre-
vious TREC questions.  However, we wanted to use 
the personalities in past TREC questions as inde-
pendent indicators of appropriate subject matter.  
Therefore we collected all of the ?creative? people 
in the TREC9 question set, and divided them up into 
classes by profession, so we had, for example, male 
singers Bob Marley, Ray Charles, Billy Joel and 
Alice Cooper; poets William Wordsworth and 
Langston Hughes; painters Picasso, Jackson Pollock 
                                                        
2 This set did not contain definition questions, which, by our 
inspection, lend themselves readily to reciprocation. 
Birthdate 
Deathdate 
Leonardo Painting 
and Vincent Van Gogh, etc. ? twelve such groupings 
in all.  For each set, we entered the individuals in the 
?Google Sets? interface 
(http://labs.google.com/sets), which finds ?similar? 
entities to the ones entered.  For example, from our 
set of male singers it found: Elton John, Sting, Garth 
Brooks, James Taylor, Phil Collins, Melissa 
Etheridge, Alanis Morissette, Annie Lennox, Jack-
son Browne, Bryan Adams, Frank Sinatra and Whit-
ney Houston. 
Altogether, we gathered 276 names of creative 
individuals this way, after removing duplicates, 
items that were not names of individuals, and names 
that did not occur in our test corpus (the AQUAINT 
corpus).  We then used our system manually to help 
us develop ?ground truth? for a randomly selected 
subset of 109 names.  This ground truth served both 
as training material and as an evaluation key.  We 
split the 109 names randomly into a set of 52 for 
training and 57 for testing.  The training process 
used a hill-climbing method to find optimal values 
for three internal rejection thresholds.  In developing 
the ground truth we might have missed some in-
stances of assertions we were looking for, so the 
reported recall (and hence F-measure) figures should 
be considered to be upper bounds, but we believe the 
calculated figures are not far from the truth. 
4.2 QDC Operation 
The system first asked three questions for each 
subject X: 
 
 In what year was X born? 
 In what year did X die? 
 What compositions did X have? 
 
The third of these triggers our named-entity type 
COMPOSITION that is used for all kinds of titled 
works ? books, films, poems, music, plays and so 
on, and also quotations.  Our named-entity recog-
nizer has rules to detect works of art by phrases that 
are in apposition to ?the film ? ? or the ?the book 
? ? etc., and also captures any short phrase in quotes 
beginning with a capital letter.  The particular ques-
tion phrasing we used does not commit us to any 
specific creative verb.  This is of particular impor-
tance since it very frequently happens in text that 
titled works are associated with their creators by 
means of a possessive or parenthetical construction, 
rather than subject-verb-object. 
The top five answers, with confidences, are re-
turned for the born and died questions (subject to 
also passing a confidence threshold test).  The com-
positions question is treated as a list question, mean-
ing that all answers that pass a certain threshold are 
returned.  For each such returned work Wi, two addi-
tional questions are asked: 
 What year did X have Wi? 
 Who had Wi? 
 
The top 5 answers to each of these are returned, 
again as long as they pass a confidence threshold.  
We added a sixth answer ?NIL? to each of the date 
sets, with a confidence equal to the rejection thresh-
old.  (NIL is the code used in TREC ever since 
TREC10 to indicate the assertion that there is no 
answer in the corpus.)  We used a two stage con-
straint-satisfaction process: 
Stage 1:  For each work Wi for subject X, we 
added together its original confidence to the confi-
dence of the answer X in the answer set of the recip-
rocal question (if it existed ? otherwise we added 
zero).  If the total did not exceed a learned threshold 
(.50) the work was rejected. 
Stage 2.  For each subject, with the remaining 
candidate works we generated all possible combina-
tions of the date answers.  We rejected any combina-
tion that did not satisfy the following constraints: 
 
 DIED >= BORN + 7 
 DIED <= BORN + 100 
 WORK >= BORN + 7 
 WORK <= BORN + 100 
 WORK <= DIED 
 DIED <= WORK + 100 
 
The apparent redundancy here is because of the 
potential NIL answers for some of the date slots.  
We also rejected combinations of works whose 
years spanned more than 100 years (in case there 
were no BORN or DIED dates).  In performing these 
constraint calculations, NIL satisfied every test by 
fiat.  The constraint network we used is depicted in 
Figure 2. 
 
 
 
Figure 2.  Constraint Network for evaluation ex-
ample.  Dashed lines represent question-answer 
pairs, solid lines constraints between the answers. 
We used as a test corpus the AQUAINT corpus 
used in TREC-QA since 2002.  Since this was not 
the same corpus from which the test questions were 
generated (the Web), we acknowledged that there 
might be some difference in the most common spell-
ing of certain names, but we made no attempt to cor-
rect for this.  Neither did we attempt to normalize, 
translate or aggregate names of the titled works that 
were returned, so that, for example, ?Well-
Birthdate of X 
Deathdate of X 
Work Wi 
Author X Date of Wi 
Xi = Author of Wi 
Tempered Klavier? and ?Well-Tempered Clavier? 
were treated as different.  Since only individuals 
were used in the question set, we did not have in-
stances of problems we saw in training, such as 
where an ensemble (such as The Beatles) created a 
certain piece, which in turn via the reciprocal ques-
tion was found to have been written by a single per-
son (Paul McCartney).  The reverse situation was 
still possible, but we did not handle it.  We foresee a 
future version of our system having knowledge of 
ensembles and their composition, thus removing this 
restriction.  In general, a variety of ontological rela-
tionships could occur between the original individ-
ual and the discovered performer(s) of the work. 
We generated answer keys by reading the pas-
sages that the system had retrieved and from which 
the answers were generated, to determine ?truth?.  In 
cases of absent information in these passages, we 
did our own corpus searches.  This of course made 
the issue of evaluation of recall only relative, since 
we were not able to guarantee we had found all ex-
isting instances. 
We encountered some grey areas, e.g., if a paint-
ing appeared in an exhibition or if a celebrity en-
dorsed a product, then should the exhibition?s or 
product?s name be considered an appropriate ?work? 
of the artist?  The general perspective adopted was 
that we were not establishing or validating the nature 
of the relationship between an individual and a crea-
tive work, but rather its existence.  We answered 
?yes? if we subjectively felt the association to be 
both very strong and with the individual?s participa-
tion ? for example, Pamela Anderson and Playboy.  
However, books/plays about a person or dates of 
performances of one?s work were considered incor-
rect.  As we shall see, these decisions would not 
have a big impact on the outcome.   
4.3 Effect of Constraints 
The answers collected from these two rounds of 
questions can be regarded as assertions about the 
subject X.  By applying constraints, two possible 
effects can occur to these assertions: 
1. Some works can get thrown out. 
2. An asserted date (which was the top candidate 
from its associated question) can get replaced by 
a candidate date originally in positions 2-6 
(where sixth place is NIL) 
Effect #1 is expected to increase precision at the 
risk of worsening recall; effect #2 can go either way.  
We note that NIL, which is only used for dates, can 
be the correct answer if the desired date assertion is 
absent from the corpus; NIL is considered a ?value? 
in this evaluation. 
By inspection, performances and other indirect 
works (discussed in the previous section) were usu-
ally associated with the correct artist, so our decision 
to remove them from consideration resulted in a de-
crease in both the numerator and denominator of the 
precision and recall calculations, resulting in a 
minimal effect. 
The results of applying QDC to the 57 test indi-
viduals are summarized in Table 3.  The baseline 
assertions for individual X were: 
o Top-ranking birthdate/NIL   
o Top-ranking deathdate/NIL   
o Set of works Wi that passed threshold 
o Top-ranking date for Wi /NIL 
 
The sets of baseline assertions (by individual) are 
in effect the results of QA-by-Dossier WITHOUT 
Constraints (QbD). 
 
  Assertions Micro-Average Macro-Average 
  Total Cor-
rect 
Tru-
th 
Prec Rec F Prec Rec F 
Base-
line 
1671 517 933 .309 .554 .396 .331 .520 .386 
QDC 1417 813 933 .573 .871 .691 .603 .865 .690 
 
Table 3.  Results of Performance Evaluation.  
Two calculations of P/R/F are made, depending on 
whether the averaging is done over the whole set, or 
first by individual; the results are very similar.   
The QDC assertions were the same as those for 
QbD, but reflecting the following effects: 
o Some {Wi, date} pairs were thrown out (3 out of 
14 on average) 
o Some dates in positions 2-6 moved up (applica-
ble to birth, death and work dates) 
The results show improvement in both precision 
and recall, in turn determining a 75-80% relative 
increase in F-measure. 
5 Discussion 
This exposition of QA-by-Dossier-with-
Constraints is very short and undoubtedly leaves 
may questions unanswered.  We have not presented 
a precise method for computing the QDC scores. 
One way to formalize this process would be to treat 
it as evidence gathering and interpret the results in a 
Bayesian-like fashion. The original system confi-
dences would represent prior probabilities reflecting 
the system?s belief that the answers are correct.  As 
more evidence is found, the confidences would be 
updated to reflect the changed likelihood that an an-
swer is correct.  
We do not know a priori how much ?slop? should 
be allowed in enforcing the constraints, since auxil-
iary questions are as likely to be answered incor-
rectly as the original ones.  A further problem is to 
determine the best metric for evaluating such ap-
proaches, which is a question for QA in general.   
The task of generating auxiliary questions and 
constraint sets is a matter of active research.  Even 
for simple questions like the ones considered here, 
the auxiliary questions and constraints we looked at 
were different and manually chosen. Hand-crafting a 
large number of such sets might not be feasible, but 
it is certainly possible to build a few for common 
situations, such as a person?s life-cycle. More gener-
ally, QDC could be applied to situations in which a 
certain structure is induced by natural temporal (our 
Leonardo example) and/or spatial constraints, or by 
properties of the relation mentioned in the question 
(evaluation example). Temporal and spatial con-
straints appear general to all relevant question types, 
and include relations of precedence, inclusion, etc. 
For certain relationships, there are naturally-
occurring reciprocals (if X is married to Y, then Y is 
married to X; if X is a child of Y then Y is a parent 
of X; compound-term to acronym and vice versa).  
Transitive relationships (e.g. greater-than, located-
in, etc.) offer the immediate possibility of con-
straints, but this avenue has not yet been explored. 
5.1 Automatic Generation of Reciprocal Ques-
tions 
While not done in the work reported here, we are 
looking at generating reciprocal questions automati-
cally.  Consider the following transformations: 
 
?What is the capital of California?? -> ?Of what 
state is <candidate> the capital?? 
 
?What is Frank Sinatra?s nickname?? -> 
?Whose (or what person?s) nickname is <can-
didate>?? 
 
?How deep is Crater Lake?? -> ?What (or what 
lake) is <candidate> deep?? 
 
?Who won the Oscar for best actor in 1970??  
-> ?In what year did <candidate> win the 
Oscar for best actor?? (and/or ?What award 
did <candidate> win in 1970??) 
 
These are precisely the transformations necessary 
to generate the auxiliary reciprocal questions from 
the given original questions and candidate answers 
to them.  Such a process requires identifying an en-
tity in the question that belongs to a known class, 
and substituting the class name for the entity.  This 
entity is made the subject of the question, the previ-
ous subject (or trace) being replaced by the candi-
date answer.  We are looking at parse-tree rather 
than string transformations to achieve this.  This 
work will be reported in a future paper.  
5.2 Final Thoughts 
Despite these open questions, initial trials with 
QA-by-Dossier-with-Constraints have been very 
encouraging, whether it is by correctly answering 
previously missed questions, or by improving confi-
dences of correct answers.  An interesting question 
is when it is appropriate to apply QDC.  Clearly, if 
the base QA system is too poor, then the answers to 
the auxiliary questions will be useless; if the base 
system is highly accurate, the increase in accuracy 
will be negligible.  Thus our approach seems most 
beneficial to middle-performance levels, which, by 
inspection of TREC results for the last 5 years, is 
where the leading systems currently lie. 
We had initially thought that use of constraints 
would obviate the need for much of the complexity 
inherent in NLP.  As mentioned earlier, with the 
case of ?The Beatles? being the reciprocal answer to 
the auxiliary composition question to ?Who is Paul 
McCartney??, we see that structured, ontological 
information would benefit QDC.  Identifying alter-
nate spellings and representations of the same name 
(e.g. Clavier/Klavier, but also taking care of varia-
tions in punctuation and completeness) is also nec-
essary.  When we asked ?Who is Ian Anderson??, 
having in mind the singer-flautist for the Jethro Tull 
rock band, we found that he is not only that, but also 
the community investment manager of the English 
conglomerate Whitbread, the executive director of 
the U.S. Figure Skating Association, a writer for 
New Scientist, an Australian medical advisor to the 
WHO, and the general sales manager of Houseman, 
a supplier of water treatment systems.  Thus the 
problem of word sense disambiguation has returned 
in a particularly nasty form.  To be fully effective, 
QDC must be configured not just to find a consistent 
set of properties, but a number of independent sets 
that together cover the highest-confidence returned 
answers3.  Altogether, we see that some of the very 
problems we aimed to skirt are still present and need 
to be addressed.  However, we have shown that even 
disregarding these issues, QDC was able to provide 
substantial improvement in accuracy. 
6 Summary 
We have presented a method to improve the accu-
racy of a QA system by asking auxiliary questions 
for which natural constraints exist.  Using these con-
straints, sets of mutually consistent answers can be 
generated.  We have explored questions in the bio-
graphical areas, and identified other areas of appli-
cability.  We have found that our methodology 
exhibits a double advantage:  not only can it im-
                                                        
3 Possibly the smallest number of sets that provide such cover-
age.   
prove QA accuracy, but it can return a set of mutu-
ally-supporting assertions about the topic of the 
original question.  We have identified many open 
questions and areas of future work, but despite these 
gaps, we have shown an example scenario where 
QA-by-Dossier-with-Constraints can improve the F-
measure by over 75%. 
7 Acknowledgements 
We wish to thank Dave Ferrucci, Elena Filatova 
and Sasha Blair-Goldensohn for helpful discussions.  
This work was supported in part by the Advanced 
Research and Development Activity (ARDA)'s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
MDA904-01-C-0988. 
References 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC,  2003. 
Clarke, C., Cormack, G., Kisman, D.. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., E. Sacerdoti, D. Sagalowicz, J. Slocum: 
Developing a Natural Language Interface to Com-
plex Data. VLDB 1977: 292  
Lehnert, W.  The Process of Question Answering. A 
Computer Simulation of Cognition. Lawrence 
Erlbaum Associates, Publishers, 1978.  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Moldovan, D. and V. Rus, ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Prager, J., E. Brown, A. Coden, and D. Radev. 2000. 
"Question-Answering by Predictive Annotation?.  
In Proceedings of SIGIR 2000, pp. 184-191.  
Prager, J., J. Chu-Carroll and K. Czuba, "A Multi-
Agent Approach to using Redundancy and Rein-
forcement in Question Answering" in New Direc-
tions in Question-Answering, Maybury, M. (Ed.),  
to appear in 2004. 
Schank, R. and R. Abelson. ?Scripts, Plans and 
Knowledge?, Proceedings of IJCAI?75. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Woods, W. Progress in natural language understand-
ing --- an application in lunar geology. Proceed-
ings of the 1973 National Computer Conference, 
AFIPS Conference Proceedings, Vol. 42, 441--
450, 1973. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073?1080,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving QA Accuracy by Question Inversion  
John Prager  
IBM T.J. Watson Res. Ctr. 
Yorktown Heights 
N.Y. 10598 
jprager@us.ibm.com 
Pablo Duboue 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
duboue@us.ibm.com 
Jennifer Chu-Carroll 
IBM T.J. Watson Res. Ctr. 
Yorktown Heights  
N.Y. 10598 
jencc@us.ibm.com 
 
Abstract 
This paper demonstrates a conceptually simple 
but effective method of increasing the accuracy 
of QA systems on factoid-style questions.  We 
define the notion of an inverted question, and 
show that by requiring that the answers to the 
original and inverted questions be mutually con-
sistent, incorrect answers get demoted in confi-
dence and correct ones promoted.  Additionally, 
we show that lack of validation can be used to 
assert no-answer (nil) conditions.  We demon-
strate increases of performance on TREC and 
other question-sets, and discuss the kinds of fu-
ture activities that can be particularly beneficial 
to approaches such as ours.  
1 Introduction 
Most QA systems nowadays consist of the following 
standard modules:  QUESTION PROCESSING, to de-
termine the bag of words for a query and the desired 
answer type (the type of the entity that will be of-
fered as a candidate answer); SEARCH, which will 
use the query to extract a set of documents or pas-
sages from a corpus; and ANSWER SELECTION, 
which will analyze the returned documents or pas-
sages for instances of the answer type in the most 
favorable contexts. Each of these components im-
plements a set of heuristics or hypotheses, as de-
vised by their authors (cf. Clarke et al 2001, Chu-
Carroll et al 2003). 
 
When we perform failure analysis on questions in-
correctly answered by our system, we find that there 
are broadly speaking two kinds of failure.  There are 
errors (we might call them bugs) on the implementa-
tion of the said heuristics: errors in tagging, parsing, 
named-entity recognition; omissions in synonym 
lists; missing patterns, and just plain programming 
errors.  This class can be characterized by being fix-
able by identifying incorrect code and fixing it, or 
adding more items, either explicitly or through train-
ing.  The other class of errors (what we might call 
unlucky) are at the boundaries of the heuristics; 
situations were the system did not do anything 
?wrong,? in the sense of bug, but circumstances con-
spired against finding the correct answer. 
 
Usually when unlucky errors occur, the system gen-
erates a reasonable query and an appropriate answer 
type, and at least one passage containing the right 
answer is returned.  However, there may be returned 
passages that have a larger number of query terms 
and an incorrect answer of the right type, or the 
query terms might just be physically closer to the 
incorrect answer than to the correct one.  ANSWER 
SELECTION modules typically work either by trying 
to prove the answer is correct (Moldovan & Rus, 
2001) or by giving them a weight produced by 
summing a collection of heuristic features (Radev et 
al., 2000); in the latter case candidates having a lar-
ger number of matching query terms, even if they do 
not exactly match the context in the question, might 
generate a larger score than a correct passage with 
fewer matching terms. 
 
To be sure, unlucky errors are usually bugs when 
considered from the standpoint of a system with a 
more sophisticated heuristic, but any system at any 
point in time will have limits on what it tries to do; 
therefore the distinction is not absolute but is rela-
tive to a heuristic and system. 
 
It has been argued (Prager, 2002) that the success of 
a QA system is proportional to the impedance match 
between the question and the knowledge sources 
available.  We argue here similarly. Moreover, we 
believe that this is true not only in terms of the cor-
rect answer, but the distracters,1 or incorrect answers 
too.  In QA, an unlucky incorrect answer is not usu-
ally predictable in advance; it occurs because of a 
coincidence of terms and syntactic contexts that 
cause it to be preferred over the correct answer.  It 
has no connection with the correct answer and is 
only returned because its enclosing passage so hap-
pens to exist in the same corpus as the correct an-
swer context.  This would lead us to believe that if a 
                                                     
1 We borrow the term from multiple-choice test design. 
1073
different corpus containing the correct answer were 
to be processed, while there would be no guarantee 
that the correct answer would be found, it would be 
unlikely (i.e. very unlucky) if the same incorrect an-
swer as before were returned. 
 
We have demonstrated elsewhere (Prager et al 
2004b) how using multiple corpora can improve QA 
performance, but in this paper we achieve similar 
goals without using additional corpora. We note that 
factoid questions are usually about relations between 
entities, e.g. ?What is the capital of France??, where 
one of the arguments of the relationship is sought 
and the others given.  We can invert the question by 
substituting the candidate answer back into the ques-
tion, while making one of the given entities the so-
called wh-word, thus ?Of what country is Paris the 
capital??  We hypothesize that asking this question 
(and those formed from other candidate answers) 
will locate a largely different set of passages in the 
corpus than the first time around.  As will be ex-
plained in Section 3, this can be used to decrease the 
confidence in the incorrect answers, and also in-
crease it for the correct answer, so that the latter be-
comes the answer the system ultimately proposes. 
 
This work is part of a continuing program of demon-
strating how meta-heuristics, using what might be 
called ?collateral? information, can be used to con-
strain or adjust the results of the primary QA system.   
 
In the next Section we review related work.  In Sec-
tion 3 we describe our algorithm in detail, and in 
Section 4 present evaluation results.  In Section 5 we 
discuss our conclusions and future work. 
2 Related Work 
Logic and inferencing have been a part of Question-
Answering since its earliest days.  The first such 
systems were natural-language interfaces to expert 
systems, e.g., SHRDLU (Winograd, 1972), or to 
databases, e.g., LIFER/LADDER (Hendrix et al 
1977).  CHAT-80 (Warren & Pereira, 1982), for in-
stance, was a DCG-based NL-query system about 
world geography, entirely in Prolog.  In these 
systems, the NL question is transformed into a se-
mantic form, which is then processed further.  Their 
overall architecture and system operation is very 
different from today?s systems, however, primarily 
in that there was no text corpus to process. 
 
Inferencing is a core requirement of systems that 
participate in the current PASCAL Recognizing 
Textual Entailment (RTE) challenge (see 
http://www.pascal-network.org/Challenges/RTE and 
.../RTE2).   It is also used in at least two of the more 
visible end-to-end QA systems of the present day.  
The LCC system (Moldovan & Rus, 2001) uses a 
Logic Prover to establish the connection between a 
candidate answer passage and the question.  Text 
terms are converted to logical forms, and the ques-
tion is treated as a goal which is ?proven?, with real-
world knowledge being provided by Extended 
WordNet.  The IBM system PIQUANT (Chu-
Carroll et al, 2003) used Cyc (Lenat, 1995) in an-
swer verification.  Cyc can in some cases confirm or 
reject candidate answers based on its own store of 
instance information; in other cases, primarily of a 
numerical nature, Cyc can confirm whether candi-
dates are within a reasonable range established for 
their subtype.   
 
At a more abstract level, the use of inversions dis-
cussed in this paper can be viewed as simply an ex-
ample of finding support (or lack of it) for candidate 
answers.  Many current systems (see, e.g. (Clarke et 
al., 2001; Prager et al 2004b)) employ redundancy 
as a significant feature of operation:  if the same an-
swer appears multiple times in an internal top-n list, 
whether from multiple sources or multiple algo-
rithms/agents, it is given a confidence boost, which 
will affect whether and how it gets returned to the 
end-user. 
 
The work here is a continuation of previous work 
described in (Prager et al 2004a,b).  In the former 
we demonstrated that for a certain kind of question, 
if the inverted question were given, we could im-
prove the F-measure of accuracy on a question set 
by 75%.  In this paper, by contrast, we do not manu-
ally provide the inverted question, and in the second 
evaluation presented here we do not restrict the 
question type. 
3 Algorithm 
3.1 System Architecture 
A simplified block-diagram of our PIQUANT sys-
tem is shown in Figure 1.  The outer block on the 
left, QS1, is our basic QA system, in which the 
QUESTION PROCESSING (QP), SEARCH (S) and 
ANSWER SELECTION (AS) subcomponents are indi-
cated.  The outer block on the right, QS2, is another 
QA-System that is used to answer the inverted ques-
tions.  In principle QS2 could be QS1 but parameter-
ized differently, or even an entirely different system, 
but we use another instance of QS1, as-is.  The 
block in the middle is our Constraints Module CM, 
which is the subject of this paper.  
 
1074
The Question Processing component of QS2 is not 
used in this context since CM simulates its output by 
modifying the output of QP in QS1, as described in 
Section 3.3. 
3.2 Inverting Questions 
Our open-domain QA system employs a named-
entity recognizer that identifies about a hundred 
types.  Any of these can be answer types, and there 
are corresponding sets of patterns in the QUESTION 
PROCESSING module to determine the answer type 
sought by any question.  When we wish to invert a 
question, we must find an entity in the question 
whose type we recognize; this entity then becomes 
the sought answer for the inverted question.  We call 
this entity the inverted or pivot term. 
 
Thus for the question: 
(1) ?What was the capital of Germany in 1985?? 
Germany is identified as a term with a known type 
(COUNTRY).  Then, given the candidate answer 
<CANDANS>, the inverted question becomes  
(2) ?Of what country was < CANDANS> the capital 
in 1985?? 
Some questions have more than one invertible term.  
Consider for example:  
(3) ?Who was the 33rd president of the U.S.?? 
This question has 3 inversion points: 
(4) ?What number president of the U.S. was 
<CANDANS>?? 
(5) ?Of what country was <CANDANS> the 33rd 
president?? 
(6) ?<CANDANS> was the 33rd what of the U.S.?? 
 
Having more than one possible inversion is in theory 
a benefit, since it gives more opportunity for enforc-
ing consistency, but in our current implementation 
we just pick one for simplicity.  We observe on 
training data that, in general, the smaller the number 
of unique instances of an answer type, the more 
likely it is that the inverted question will be correctly 
answered.  We generated a set NELIST of the most 
frequently-occurring named-entity types in ques-
tions; this list is sorted in order of estimated cardi-
nality. 
 
It might seem that the question inversion process can 
be quite tricky and can generate possibly unnatural 
phrasings, which in turn can be difficult to reparse.  
However, the examples given above were simply 
English renditions of internal inverted structures ? as 
we shall see the system does not need to use a natu-
ral language representation of the inverted questions. 
 
Some questions are either not invertible, or, like 
?How did X die?? have an inverted form (?Who died 
of cancer??) with so many correct answers that we 
know our algorithm is unlikely to benefit us.  How-
ever, as it is constituted it is unlikely to hurt us ei-
ther, and since it is difficult to automatically identify 
such questions, we don?t attempt to intercept them.  
As reported in (Prager et al 2004a), an estimated 
79% of the questions in TREC question sets can be 
inverted meaningfully.  This places an upper limit 
on the gains to be achieved with our algorithm, but 
is high enough to be worth pursuing. 
Figure 1.  Constraints Architecture.  QS1 and QS2 are (possibly identical) QA systems. 
Answers
Question 
QS1 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
QS2 
QA system 
QP 
 question proc. 
S 
 search 
AS 
answer selection 
CM 
constraints 
module 
 
1075
3.3 Inversion Algorithm 
As shown in the previous section, not all questions 
have easily generated inverted forms (even by a hu-
man).  However, we do not need to explicate the 
inverted form in natural language in order to process 
the inverted question. 
 
In our system, a question is processed by the 
QUESTION PROCESSING module, which produces a 
structure called a QFrame, which is used by the sub-
sequent SEARCH and ANSWER SELECTION modules.  
The QFrame contains the list of terms and phrases in 
the question, along with their properties, such as 
POS and NE-type (if it exists), and a list of syntactic 
relationship tuples.  When we have a candidate an-
swer in hand, we do not need to produce the inverted 
English question, but merely the QFrame that would 
have been generated from it.  Figure 1 shows that 
the CONSTRAINTS MODULE takes the QFrame as one 
of its inputs, as shown by the link from QP in QS1 
to CM.  This inverted QFrame can be generated by a 
set of simple transformations, substituting the pivot 
term in the bag of words with a candidate answer 
<CANDANS>, the original answer type with the type 
of the pivot term, and in the relationships the pivot 
term with its type and the original answer type with 
<CANDANS>.  When relationships are evaluated, a 
type token will match any instance of that type.  Fig-
ure 2 shows a simplified view of the original 
QFrame for ?What was the capital of Germany in 
1945??, and Figure 3 shows the corresponding In-
verted QFrame.  COUNTRY is determined to be a 
better type to invert than YEAR, so ?Germany? be-
comes the pivot.  In Figure 3, the token 
<CANDANS> might take in turn ?Berlin?, ?Mos-
cow?, ?Prague? etc. 
 
Figure 2. Simplified QFrame 
 
Figure 3. Simplified Inverted QFrame.   
The output of QS2 after processing the inverted 
QFrame is a list of answers to the inverted question, 
which by extension of the nomenclature we call ?in-
verted answers.?  If no term in the question has an 
identifiable type, inversion is not possible. 
3.4 Profiting From Inversions 
Broadly speaking, our goal is to keep or re-rank the 
candidate answer hit-list on account of inversion 
results.  Suppose that a question Q is inverted 
around pivot term T, and for each candidate answer 
Ci, a list of ?inverted? answers {Cij} is generated as 
described in the previous section.  If T is on one of 
the {Cij}, then we say that Ci is validated.  Valida-
tion is not a guarantee of keeping or improving Ci?s 
position or score, but it helps.  Most cases of failure 
to validate are called refutation; similarly, refutation 
of Ci is not a guarantee of lowering its score or posi-
tion.   
 
It is an open question how to adjust the results of the 
initial candidate answer list in light of the results of 
the inversion.  If the scores associated with candi-
date answers (in both directions) were true prob-
abilities, then a Bayesian approach would be easy to 
develop.  However, they are not in our system.  In 
addition, there are quite a few parameters that de-
scribe the inversion scenario. 
 
Suppose Q generates a list of the top-N candidates 
{Ci}, with scores {Si}.  If this inversion method 
were not to be used, the top candidate on this list, 
C1, would be the emitted answer.  The question gen-
erated by inverting about T and substituting Ci is 
QTi.  The system is fixed to find the top 10 passages 
responsive to QTi, and generates an ordered list Cij 
of candidate answers found in this set. 
 
Each inverted question QTi is run through our sys-
tem, generating inverted answers {Cij}, with scores 
{Sij}, and whether and where the pivot term T shows 
up on this list, represented by a list of positions {Pi}, 
where Pi is defined as: 
 
 Pi  =  j    if Cij = T, for some j 
 Pi  =  -1 otherwise 
 
We added to the candidate list the special answer 
nil, representing ?no answer exists in the corpus.? 
 
As described earlier, we had observed from training 
data that failure to validate candidates of certain 
types (such as Person) would not necessarily be a 
real refutation, so we established a set of types 
SOFTREFUTATION which would contain the broadest 
of our types.  At the other end of the spectrum, we 
observed that certain narrow candidate types such as 
UsState would definitely be refuted if validation 
didn?t occur.  These are put in set MUSTCONSTRAIN. 
Our goal was to develop an algorithm for recomput-
ing all the original scores {Si} from some combina-
tion (based on either arithmetic or decision-trees) of 
Keywords: {1945, <CANDANS>, capital} 
AnswerType: COUNTRY 
Relationships: {(COUNTRY, capital), (capital, 
<CANDANS>), (capital, 1945)} 
Keywords: {1945, Germany, capital} 
AnswerType: CAPITAL 
Relationships: {(Germany, capital), (capital, 
CAPITAL), (capital, 1945)} 
1076
{Si} and {Sij} and membership of SOFTREFUTATION 
and MUSTCONSTRAIN.  Reliably learning all those 
weights, along with set membership, was not possi-
ble given only several hundred questions of training 
data.  We therefore focused on a reduced problem. 
 
We observed that when run on TREC question sets, 
the frequency of the rank of our top answer fell off 
rapidly, except with a second mode when the tail 
was accumulated in a single bucket.  Our numbers 
for TRECs 11 and 12 are shown in Table 1. 
 
Top answer rank TREC11 TREC12 
1 170 108 
2 35 32 
3 23 14 
4 7 7
5 14 9
elsewhere 251 244 
% correct 34 26 
Table 1.  Baseline statistics for TREC11-12. 
 
We decided to focus on those questions where we 
got the right answer in second place (for brevity, 
we?ll call these second-place questions).  Given that 
TREC scoring only rewards first-place answers, it 
seemed that with our incremental approach we 
would get most benefit there.  Also, we were keen to 
limit the additional response time incurred by our 
approach.  Since evaluating the top N answers to the 
original question with the Constraints process re-
quires calling the QA system another N times per 
question, we were happy to limit N to 2.  In addition, 
this greatly reduced the number of parameters we 
needed to learn.  
 
For the evaluation, which consisted of determining if 
the resulting top answer was right or wrong, it meant 
ultimately deciding on one of three possible out-
comes:  the original top answer, the original second 
answer, or nil.  We hoped to promote a significant 
number of second-place finishers to top place and 
introduce some nils, with minimal disturbance of 
those already in first place. 
 
We used TREC11 data for training, and established 
a set of thresholds for a decision-tree approach to 
determining the answer, using Weka (Witten & 
Frank, 2005).  We populated sets SOFTREFUTATION 
and MUSTCONSTRAIN by manual inspection.   
 
The result is Algorithm A, where (i ? {1,2}) and 
o The Ci are the original candidate answers 
o The ak are learned parameters (k ? {1..13}) 
o Vi means the ith answer was validated 
o Pi was the rank of the validating answer to ques-
tion QTi 
o Ai was the score of the validating answer to QTi. 
Algorithm A. Answer re-ranking using con-
straints validation data. 
1. If C1 = nil and V2,    return C2 
2. If V1 and A1 > a1,     return C1 
3. If not V1 and not V2 and  
 type(T) ? MUSTCONSTRAIN,  
    return nil 
4. If  not V1 and not V2 and  
 type(T) ?SOFTREFUTATION, 
if S1 > a2,, return C1 else nil 
5. If not V2,    return C1 
6. If not V1 and V2 and  
A2 > a3 and P2 < a4 and  
S1-S2 < a5 and S2 > a6, return C2 
7. If V1 and V2 and  
(A2 - P2/a7) > (A1 - P1/a7) and  
A1 < a8 and P1 > a9 and  
A2 < a10 and P2 > a11 and  
S1-S2 < a12  and (S2 - P2/a7) > a13,  
    return C2 
8. else return C1 
 
4 Evaluation 
Due to the complexity of the learned algorithm, we 
decided to evaluate in stages.  We first performed an 
evaluation with a fixed question type, to verify that 
the purely arithmetic components of the algorithm 
were performing reasonably.  We then evaluated on 
the entire TREC12 factoid question set. 
4.1 Evaluation 1 
We created a fixed question set of 50 questions of 
the form ?What is the capital of X??, for each state 
in the U.S.  The inverted question ?What state is Z 
the capital of?? was correctly generated in each 
case.  We evaluated against two corpora: the 
AQUAINT corpus, of a little over a million news-
wire documents, and the CNS corpus, with about 
37,000 documents from the Center for Nonprolifera-
tion Studies in Monterey, CA.  We expected there to 
be answers to most questions in the former corpus, 
so we hoped there our method would be useful in 
converting 2nd place answers to first place.  The lat-
ter corpus is about WMDs, so we expected there to 
be holes in the state capital coverage2, for which nil 
identification would be useful.3   
                                                     
2 We manually determined that only 23 state capitals were at-
tested to in the CNS corpus, compared with all in AQUAINT. 
3 We added Tbilisi to the answer key for ?What is the capi-
tal of Georgia??, since there was nothing in the question to 
disambiguate Georgia.  
1077
The baseline is our regular search-based QA-System 
without the Constraint process.  In this baseline sys-
tem there was no special processing for nil ques-
tions, other than if the search (which always 
contained some required terms) returned no docu-
ments.  Our results are shown in Table 2. 
 
 AQUAINT 
baseline 
AQUAINT 
w/con-
straints 
CNS 
baseline 
CNS 
w/con-
straints 
Firsts 
(non-nil) 
39/50 43/50 7/23 4/23 
Total 
nils 
0/0 0/0 0/27 16/27 
Total 
firsts 
39/50 43/50 7/50 20/50 
%  
correct 
78 86 14 40 
Table 2.  Evaluation on AQUAINT and CNS 
corpora. 
 
On the AQUAINT corpus, four out of seven 2nd 
place finishers went to first place.  On the CNS cor-
pus 16 out of a possible 26 correct no-answer cases 
were discovered, at a cost of losing three previously 
correct answers.  The percentage correct score in-
creased by a relative 10.3% for AQUAINT and 
186% for CNS.  In both cases, the error rate was 
reduced by about a third. 
4.2 Evaluation 2 
For the second evaluation, we processed the 414 
factoid questions from TREC12.  Of special interest 
here are the questions initially in first and second 
places, and in addition any questions for which nils 
were found. 
 
As seen in Table 1, there were 32 questions which 
originally evaluated in rank 2.  Of these, four ques-
tions were not invertible because they had no terms 
that were annotated with any of our named-entity 
types, e.g. #2285 ?How much does it cost for gas-
tric bypass surgery?? 
 
Of the remaining 28 questions, 12 were promoted to 
first place.  In addition, two new nils were found.  
On the down side, four out of 108 previous first 
place answers were lost.  There was of course 
movement in the ranks two and beyond whenever 
nils were introduced in first place, but these do not 
affect the current TREC-QA factoid correctness 
measure, which is whether the top answer is correct 
or not.  These results are summarized in Table 3.  
 
While the overall percentage improvement was 
small, note that only second?place answers were 
candidates for re-ranking, and 43% of these were 
promoted to first place and hence judged correct.  
Only 3.7% of originally correct questions were 
casualties.  To the extent that these percentages are 
stable across other collections, as long as the size of 
the set of second-place answers is at least about 1/10 
of the set of first-place answers, this form of the 
Constraint process can be applied effectively. 
 
 Baseline Constraints 
Firsts (non-nil) 105 113 
nils 3 5 
Total firsts 108 118 
% correct 26.1 28.5 
 
Table 3.  Evaluation on TREC12 Factoids. 
5 Discussion  
The experiments reported here pointed out many 
areas of our system which previous failure analysis 
of the basic QA system had not pinpointed as being 
too problematic, but for which improvement should 
help the Constraints process.  In particular, this work 
brought to light a matter of major significance, term 
equivalence, which we had not previously focused 
on too much (and neither had the QA community as 
a whole).  We will discuss that in Section 5.4. 
 
Quantitatively, the results are very encouraging, but 
it must be said that the number of questions that we 
evaluated were rather small, as a result of the com-
putational expense of the approach. 
 
From Table 1, we conclude that the most mileage is 
to be achieved by our QA-System as a whole by ad-
dressing those questions which did not generate a 
correct answer in the first one or two positions.  We 
have performed previous analyses of our system?s 
failure modes, and have determined that the pas-
sages that are output from the SEARCH component 
contain the correct answer 70-75% of the time.  The 
ANSWER SELECTION module takes these passages 
and proposes a candidate answer list. Since the CON-
STRAINTS MODULE?s operation can be viewed as a 
re-ranking of the output of ANSWER SELECTION, it 
could in principle boost the system?s accuracy up to 
that 70-75% level.  However, this would either re-
quire a massive training set to establish all the pa-
rameters and weights required for all the possible re-
ranking decisions, or a new model of the answer-list 
distribution.    
5.1 Probability-based Scores 
Our ANSWER SELECTION component assigns scores 
to candidate answers on the basis of the number of 
terms and term-term syntactic relationships from the 
1078
original question found in the answer passage 
(where the candidate answer and wh-word(s) in the 
question are identified terms).  The resulting num-
bers are in the range 0-1, but are not true probabili-
ties (e.g. where answers with a score of 0.7 would be 
correct 70% of the time).  While the generated 
scores work well to rank candidates for a given 
question, inter-question comparisons are not gener-
ally meaningful.  This made the learning of a deci-
sion tree (Algorithm A) quite difficult, and we 
expect that when addressed, will give better per-
formance to the Constraints process (and maybe a 
simpler algorithm).  This in turn will make it more 
feasible to re-rank the top 10 (say) original answers, 
instead of the current 2. 
5.2 Better confidences 
Even if no changes to the ranking are produced by 
the Constraints process, then the mere act of valida-
tion (or not) of existing answers can be used to ad-
just confidence scores.  In TREC2002 (Voorhees, 
2003), there was an evaluation of responses accord-
ing to systems? confidences in their own answers, 
using the Average Precision (AP) metric.  This is an 
important consideration, since it is generally better 
for a system to say ?I don?t know? than to give a 
wrong answer.  On the TREC12 questions set, our 
AP score increased 2.1% with Constraints, using the 
algorithm we presented in (Chu-Carroll et al 2002).    
5.3 More complete NER 
Except in pure pattern-based approaches, e.g. (Brill, 
2002), answer types in QA systems typically corre-
spond to the types identifiable by their named-entity 
recognizer (NER). There is no agreed-upon number 
of classes for an NER system, even approximately.  
It turns out that for best coverage by our 
CONSTRAINTS MODULE, it is advantageous to have a 
relatively large number of types.  It was mentioned 
in Section 4.2 that certain questions were not invert-
ible because no terms in them were of a recogniz-
able type.  Even when questions did have typed 
terms, if the types were very high-level then creating 
a meaningful inverted question was problematic.  
For example, for QA without Constraints it is not 
necessary to know the type of ?MTV? in ?When 
was MTV started??, but if it is only known to be a 
Name then the inverted question ?What <Name> 
was started in 1980?? could be too general to be ef-
fective. 
5.4 Establishing Term Equivalence 
The somewhat surprising condition that emerged 
from this effort was the need for a much more com-
plete ability than had previously been recognized for 
the system to establish the equivalence of two terms.  
Redundancy has always played a large role in QA 
systems ? the more occurrences of a candidate an-
swer in retrieved passages the higher the answer?s 
score is made to be. Consequently, at the very least, 
a string-matching operation is needed for checking 
equivalence, but other techniques are used to vary-
ing degrees. 
 
It has long been known in IR that stemming or lem-
matization is required for successful term matching, 
and in NLP applications such as QA, resources such 
as WordNet (Miller, 1995) are employed for check-
ing synonym and hypernym relationships; Extended 
WordNet (Moldovan & Novischi, 2002) has been 
used to establish lexical chains between terms.  
However, the Constraints work reported here has 
highlighted the need for more extensive equivalence 
testing. 
 
In direct QA, when an ANSWER SELECTION module 
generates two (or more) equivalent correct answers 
to a question (e.g. ?Ferdinand Marcos? vs. ?Presi-
dent Marcos?; ?French? vs. ?France?), and fails to 
combine them, it is observed that as long as either 
one is in first place then the question is correct and 
might not attract more attention from developers.  It 
is only when neither is initially in first place, but 
combining the scores of correct candidates boosts 
one to first place that the failure to merge them is 
relevant.  However, in the context of our system, we 
are comparing the pivot term from the original ques-
tion to the answers to the inverted questions, and 
failure here will directly impact validation and hence 
the usefulness of the entire approach. 
 
As a consequence, we have identified the need for a 
component whose sole purpose is to establish the 
equivalence, or generally the kind of relationship, 
between two terms.  It is clear that the processing 
will be very type-dependent ? for example, if two 
populations are being compared, then a numerical 
difference of 5% (say) might not be considered a 
difference at all; for ?Where? questions, there are 
issues of granularity and physical proximity, and so 
on.  More examples of this problem were given in 
(Prager et al 2004a).  Moriceau (2006) reports a 
system that addresses part of this problem by trying 
to rationalize different but ?similar? answers to the 
user, but does not extend to a general-purpose 
equivalence identifier.   
6 Summary 
We have extended earlier Constraints-based work 
through the method of question inversion.  The ap-
proach uses our QA system recursively, by taking 
candidate answers and attempts to validate them 
through asking the inverted questions. The outcome 
1079
is a re-ranking of the candidate answers, with the 
possible insertion of nil (no answer in corpus) as the 
top answer.   
 
While we believe the approach is general, and can 
work on any question and arbitrary candidate lists, 
due to training limitations we focused on two re-
stricted evaluations.  In the first we used a fixed 
question type, and showed that the error rate was 
reduced by 36% and 30% on two very different cor-
pora.  In the second evaluation we focused on ques-
tions whose direct answers were correct in the 
second position.  43% of these questions were sub-
sequently judged correct, at a cost of only 3.7% of 
originally correct questions.  While in the future we 
would like to extend the Constraints process to the 
entire answer candidate list, we have shown that ap-
plying it only to the top two can be beneficial as 
long as the second-place answers are at least a tenth 
as numerous as first-place answers.  We also showed 
that the application of Constraints can improve the 
system?s confidence in its answers. 
 
We have identified several areas where improve-
ment to our system would make the Constraints 
process more effective, thus getting a double benefit.  
In particular we feel that much more attention 
should be paid to the problem of determining if two 
entities are the same (or ?close enough?). 
7 Acknowledgments 
This work was supported in part by the Disruptive 
Technology Office (DTO)?s Advanced Question 
Answering for Intelligence (AQUAINT) Program 
under contract number H98230-04-C-1577.   We 
would like to thank the anonymous reviewers 
for their helpful comments. 
References 
Brill, E., Dumais, S. and Banko M. ?An analysis of 
the AskMSR question-answering system.? In Pro-
ceedings of EMNLP 2002. 
Chu-Carroll, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci.  ?A Multi-Strategy and Multi-Source 
Approach to Question Answering?, Proceedings 
of the 11th TREC, 2003. 
Clarke, C., Cormack, G., Kisman, D. and Lynam, T.  
?Question answering by passage selection 
(Multitext experiments for TREC-9)? in Proceed-
ings of the 9th TREC, pp. 673-683, 2001. 
Hendrix, G., Sacerdoti, E., Sagalowicz, D., Slocum 
J.: Developing a Natural Language Interface to 
Complex Data. VLDB 1977: 292  
Lenat, D. 1995.  "Cyc: A Large-Scale Investment in 
Knowledge Infrastructure." Communications of 
the ACM 38, no. 11. 
Miller, G. ?WordNet: A Lexical Database for Eng-
lish?, Communications of the ACM 38(11) pp. 
39-41, 1995. 
Moldovan, D. and Novischi, A, ?Lexical Chains for 
Question Answering?, COLING 2002. 
Moldovan, D. and Rus, V., ?Logic Form Transfor-
mation of WordNet and its Applicability to Ques-
tion Answering?, Proceedings of the ACL, 2001. 
Moriceau, V. ?Numerical Data Integration for Co-
operative Question-Answering?, in EACL Work-
shop on Knowledge and Reasoning for Language 
Processing (KRAQ?06), Trento, Italy, 2006. 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "Ques-
tion Answering using Constraint Satisfaction: 
QA-by-Dossier-with-Constraints", Proc. 42nd 
ACL, pp. 575-582, Barcelona, Spain, 2004(a). 
Prager, J.M., Chu-Carroll, J. and Czuba, K. "A 
Multi-Strategy, Multi-Question Approach to 
Question Answering" in New Directions in Ques-
tion-Answering, Maybury, M. (Ed.), AAAI Press, 
2004(b). 
Prager, J., "A Curriculum-Based Approach to a QA 
Roadmap"' LREC 2002 Workshop on Question 
Answering: Strategy and Resources, Las Palmas, 
May 2002. 
Radev, D., Prager, J. and Samn, V. "Ranking Sus-
pected Answers to Natural Language Questions 
using Predictive Annotation", Proceedings of 
ANLP 2000, pp. 150-157, Seattle, WA. 
Voorhees, E. ?Overview of the TREC 2002 Ques-
tion Answering Track?, Proceedings of the 11th 
TREC, Gaithersburg, MD, 2003. 
Warren, D., and F. Pereira "An efficient easily 
adaptable system for interpreting natural language 
queries," Computational Linguistics, 8:3-4, 110-
122, 1982.  
Winograd, T. Procedures as a representation for data 
in a computer program for under-standing natural 
language. Cognitive Psychology, 3(1), 1972. 
Witten, I.H. & Frank, E. Data Mining.  Practical 
Machine Learning Tools and Techniques.  El-
sevier Press, 2005. 
 
1080
A Hybrid Approach to Natural Language Web Search
Jennifer Chu-Carroll, John Prager, Yael Ravin and Christian Cesar
IBM T.J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598, U.S.A.
{jencc,jprager,ravin,cesar}@us.ibm.com
Abstract
We describe a hybrid approach to improv-
ing search performance by providing a
natural language front end to a traditional
keyword-based search engine. The key
component of the system is iterative query
formulation and retrieval, in which one or
more queries are automatically formulated
from the user?s question, issued to the
search engine, and the results accumulated
to form the hit list. New queries are gener-
ated by relaxing previously-issued queries
using transformation rules, applied in an
order obtained by reinforcement learning.
This statistical component is augmented
by a knowledge-driven hub-page identi-
fier that retrieves a hub-page for the most
salient noun phrase in the question, if
possible. Evaluation on an unseen test
set over the www.ibm.com public web-
site with 1.3 million webpages shows that
both components make substantial contri-
bution to improving search performance,
achieving a combined 137% relative im-
provement in the number of questions cor-
rectly answered, compared to a baseline
of keyword queries consisting of two noun
phrases.
1 Introduction
Keyword-based search engines have been one of the
most highly utilized internet tools in recent years.
Nevertheless, search performance remains unsatis-
factory at most e-commerce sites (Hagen et al,
2000). Librarians and search professionals have tra-
ditionally favored Boolean keyword search systems,
which, when successful, return a small set of rele-
vant hits. However, the success of these systems crit-
ically depends on the choice of the right keywords
and the appropriate Boolean operators. As the pop-
ulation of search engine users has grown beyond a
small dedicated search professional community and
as these new users are less familiar with the contents
they are searching, it has become harder for them to
formulate successful keyword queries. To improve
search performance, one can improve search engine
accuracy with respect to fixed keyword queries, or
provide the search engine with better queries, those
more likely to retrieve good results. While there is
much on-going work in the IR community on the
former topic, we have taken the latter approach by
providing a natural language search interface and
automatically generating keyword queries that uti-
lize advanced search features typically unused by
end users. We believe that natural language ques-
tions are easier for users to construct than keyword
queries, thus shifting the burden of optimal query
formulation from the user to the system. Such ques-
tions also eliminate much of the ambiguity of key-
word queries that often leads to poor results. Fur-
thermore, the methodology we describe may be ap-
plied to different search engines with only minor
modification.
To transform natural language input into a search
query, the system must identify information perti-
nent for search and utilize it to formulate keyword
queries likely to retrieve relevant answers. We de-
scribe and evaluate a hybrid system, RISQUE, that
adopts an iterative approach to query formulation
and retrieval for search on the www.ibm.com pub-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 180-187.
                         Proceedings of the Conference on Empirical Methods in Natural
lic website with 1.3 million webpages. RISQUE
may issue multiple queries per question, where a
new query is generated by relaxing a previously is-
sued query via transformation rule application, in
an order obtained by reinforcement learning. In ad-
dition, RISQUE identifies a hub-page for the most
salient noun phrase in the question, if possible,
utilizing traditional knowledge-driven mechanisms.
Evaluation on an unseen test set showed that both
the machine-learned and knowledge-driven compo-
nents made substantial contribution to improving
RISQUE?s performance, resulting in a combined
137% relative improvement in the number of ques-
tions correctly answered, compared to a baseline ob-
tained by queries consisting of two noun phrases
(2NP baseline).
2 Related Work
The popularity of natural language search is evi-
denced by the growing number of search engines,
such as AskJeeves, Electric Knowledge, and North-
ern Light,1 that offer such functionality. For most
sites, we were only able to perform a cursory ex-
amination of their proprietary techniques. Adopt-
ing a similar approach as FAQFinder (Hammond
et al, 1995), AskJeeves maintains a database of
questions and webpages that provide answers to
them. User questions are compared against those in
the database, and links to webpages for the closest
matches are returned. Similar to our approach, Elec-
tric Knowledge transforms a natural language ques-
tion into a series of increasingly more general key-
word queries (Bierner, 2001). However, their query
formulation process utilizes hard-crafted regular ex-
pressions, while we adopt a more general machine
learning approach for transformation rule applica-
tion.
Our work is also closely related to question an-
swering in the question analysis component (e.g.,
(Harabagui et al, 2001; Prager et al, 2000; Clarke
et al, 2001; Ittycheriah et al, 2001)). In partic-
ular, Harabagui et al(2001) also iteratively refor-
mulate queries based partly on the search results.
However, their mechanism for query reformulation
is heuristic-based. We utilized machine learning to
1www.askjeeves.com, www.electricknowledge.com, and
www.northernlight.com, respectively.
optimize the query formulation process.
3 Data Analysis
To generate optimal keyword queries from natural
language questions, we first analyzed a set of 502
questions related to the purchasing and support of
ThinkPads (notebook computers) and their acces-
sories, such as ?How do I set up hibernation for my
ThinkPad?? and ?Show me all p3 laptops.? Our
analysis focused on three tasks. First, we attempted
to identify an exhaustive set of correct webpages for
each question, where a correct webpage is one that
contains either an answer to the question or a hyper-
link to such a page. Second, we manually formu-
lated successful keyword queries from the question,
i.e., queries which retrieved at least one correct web-
page. Third, we attempted to discover general pat-
terns in how the natural language questions may be
transformed into successful keyword queries.
Our analysis eliminated 110 questions for which
no correct webpage was found. Of the remaining
392 questions, we identified, on average, 4.37 cor-
rect webpages and 1.58 successful queries per ques-
tion. We found that the characteristics of success-
ful queries varied greatly. In the simplest case, a
successful query may contain all the content bear-
ing NPs in the question, such as thinkpad AND
?answering machine? for ?Can I use my ThinkPad
as an answering machine??2 In the vast majority
of cases, however, more complex transformations
were applied to the question to result in a successful
query. For instance, a successful query for ?How do
I hook an external mouse to my laptop?? is (mouse
OR mice) AND thinkpad AND +url:support. In
this case, the head noun mouse was inflected,3 the
premodifier external was dropped, hook was deleted,
laptop was replaced by thinkpad, and a URL con-
straint was applied.
We observed that in our corpus, most success-
ful queries can be derived by applying one or
more transformation rules to the NPs and verbs
in the questions. Table 1 shows the manually in-
2Our search engine (www.alltheweb.com) accepts a con-
junction of terms (a word, quoted phrase, or disjunction of
words/phrases), and inclusion/exclusion of text strings in the
URL, such as +url:support.
3Many commercial search engines purposely do not inflect
search words to avoid overgeneralization of queries.
Rule Function
ConstrainURL Apply URL constraints
RelaxNP Relax phrase to conjunction of words
DropNP Remove least salient NP
DropModifier Remove premodifiers of nouns
DropVerb Remove verb
ApplySynonyms Add synonyms of NPs
Inflect Inflect head nouns and verb
Table 1: Query Transformation Rules
duced commonly-used transformation rules based
on our corpus analysis. Though the rules were quite
straightforward to identify, the order in which they
should be applied to yield optimal overall perfor-
mance was non-intuitive. In fact, the best order
we manually derived did not yield sufficient per-
formance improvement over our baseline (see Sec-
tion 7). We further hypothesize that the optimal rule
application sequence may be dependent on ques-
tion characteristics. For example, DropVerb may
be a higher priority rule for buy questions than for
support questions, since the verbs indicative of buy
questions (typically ?buy? or ?sell?) are often ab-
sent in the target product pages. Therefore, we in-
vestigated a machine learning approach to automat-
ically obtain the optimal rule application sequence.
4 A Reinforcement Learning Approach to
Query Formulation
Our problem consists of obtaining an optimal strat-
egy for choosing transformation rules to generate
successful queries. A key feature of this problem is
that feedback during training is often delayed, i.e.,
the positive effect of applying a rule may not be ap-
parent until a successful query is constructed after
the application of other rules. Thus, we adopt a rein-
forcement learning approach to obtain this optimal
strategy.
4.1 Q Learning
We adopted the Q learning paradigm (Watkins,
1989; Mitchell, 1997) to model our problem as a set
of possible states, S, and a set of actions, A, which
can be performed to alter the current state. While
in state s ? S and performing action a ? A, the
learner receives a reward r(s, a), and advances to
state s? = ?(s, a).
To learn an optimal control strategy that maxi-
mizes the cumulative reward over time, an evalua-
tion function Q(s, a) is defined as follows:
Q(s, a) ? r(s, a) + ? maxa?Q(?(s, a), a?) (1)
In other words, Q(s, a) is the immediate reward,
r(s, a), plus the discounted maximum future reward
starting from the new state ?(s, a).
The Q learning algorithm iteratively selects an ac-
tion and updates Q?, an estimate of Q, as follows:
Q?n(s, a) ? (1? ?n)Q?n?1(s, a) + (2)
?n(r(s, a) + maxa?Q?n?1(s?, a?))
where s? = ?(s, a) and ?n is inversely proportional
to the number of times a state/action pair <s,a> has
been visited up to the nth iteration of the algorithm.4
Once the system learns Q?, it can select from the
possible actions in state s based on Q?(s, ai).
4.2 Query Formulation Using Q Learning
To formulate our problem in the Q learning
paradigm, we represent a state as a 6-tuple,
<qtype, url constraint, np phrase, num nps,
num modifiers, num verbs>, where:
? qtype is buy or support depending on question
classification.
? url constraint is true or false, and determines
if manually predefined URL restrictions will be
applied in the query.
? np phrase is true or false, and determines
whether each NP will be searched for as a
phrase or a conjunction of words.
? num nps is an integer between 1 and 3, and
determines how many NPs will be included in
the query.
? num modifiers is an integer between 0 and 2,
and indicates the maximum number of premod-
ifiers in each NP.
? num verbs is 0 or 1, and determines if the verb
will be included in the query.
4Equation (2) modifies (1) by taking a decaying weighted
average of the current Q? value and the new value to guarantee
convergence of Q? in non-deterministic environments. We ex-
plain in the next section why our query formulation problem in
the Q learning framework is non-deterministic.
This representation is chosen based on the rules
identified in Section 3. The actions, A, include the
first 5 actions in Table 1, and the ?undo? counterpart
for each action.5 Except for qtype, which remains
static for a question, each remaining element in the
tuple can be altered by one of the 5 pairs of actions
in a straightforward manner. The state, s, and the
question, q, generate a unique keyword query which
results in a hit list, h(s, q). The hit list is evaluated
for correctness, whose result is used to define the
reward function as follows:
r(s, a) =
?
??????
??????
1 if h(s?, q) contains at least one
correct webpage
0 if h(s?, q) has no correct page &
|h(s?, q)| < 10
?1 otherwise
where s? = ?(s, a). Note that our system operates in
a non-deterministic environment because the reward
is dependent not only on s and a, but also on q.6
Having defined S, A, ?, and r, Q? is determined by
applying the Q learning algorithm, using the update
function in (2), to our corpus of 392 questions. For
each question, an initial state is randomly selected
within the bounds of the question. The system then
iteratively selects and applies actions, and updates
Q? until a successful query is generated or the maxi-
mum number of iterations is reached (in our imple-
mentation, 15). The training algorithm iterates over
all questions in the training set and terminates when
Q? converges.
5 RISQUE: A Hybrid System for Natural
Language Search
5.1 System Overview
In addition to motivating machine learning based
query transformation as our central approach to nat-
ural language search, our analysis revealed the need
for several other key system components. As shown
in Figure 1, RISQUE adopts a hybrid architecture
5Morphological and synonym expansions are applied at the
outset, which was shown to result in better performance than
optional application of those rules.
6It is theoretically possible to encode pertinent information
in q in the state representation, thus making the environment
deterministic. However, data sparseness problems associated
with such a representation makes it impractical.
Hit List Accumulation
Ontology
Hub-Page Identifier
Natural Language Question
Question Pre-Processing
Top n Hits for Question
Question Understanding
NP Sequencing
and Retrieval
Query Formulation
Figure 1: RISQUE Architecture
that combines the utility of traditional knowledge-
based methods and statistical approaches. Given
a question, RISQUE first performs question analy-
sis by extracting pertinent information to be used in
query formulation, such as the NPs, VPs, and ques-
tion type, and then orders the NPs in terms of their
relative salience. This information is then used for
hit list construction by two modules. The first com-
ponent is the hub-page identifier, which retrieves, if
possible, a hub page for the most salient NP in the
question. The second component is the Q learning
based query formulation and retrieval module that
iteratively generates queries via transformation rule
application and issues them to the search engine.
The results from both processes are combined and
accumulated until n distinct hits are retrieved.
In addition to the above components, RISQUE
employs an ontology for the ThinkPad domain,
which consists of 1) a hierarchy of about 500 do-
main objects, 2) nearly 400 instances of relation-
ships, such as isa and accessory-of, between objects,
and 3) a synonym dictionary containing about 1000
synsets. The ontology was manually constructed
and took approximately 2 person-months for cov-
erage in the ThinkPad domain. It provides perti-
nent information to the question pre-processing and
query formulation modules, which we will describe
in the next sections.
5.2 Question Pre-Processing
5.2.1 Question Understanding
RISQUE?s question understanding component is
based primarily on a rule-driven parser in the slot
grammar framework (McCord, 1989). The result-
ing parse tree is first analyzed for NP/VP extrac-
tion. Each NP includes the head noun and up to
two premodifiers, which covers most NPs in our do-
main. The NPs are further processed by a named-
entity recognizer (Prager et al, 2000; Wacholder et
al., 1997), with reference to domain-specific proper
names in our ontology. Recognized compound
terms, such as ?hard drive?, are treated as single en-
tities, rather than as head nouns (?drive?) with pre-
modifiers (?hard?). This prevents part of the com-
pound term from being dropped when the DropMod-
ifier transformation rule is applied.
The parse tree is also used to classify the question
as buy or support. The classifier utilizes a set of rules
based on lexical and part-of-speech information. For
example, ?how? tagged as a adverb (as in ?How do
I ...?) suggests a support question, while ?buy/sell?
used as a verb indicates a buy question. These rules
were manually derived based on our training data.
5.2.2 NP Sequencing
Our analysis showed that when a successful query
is to contain fewer NPs than in the question, it is
not straightforward to determine which NPs to elim-
inate, as it requires both domain and content knowl-
edge. However, we observed that less salient NPs
are often removed first, where salience indicates the
importance of the term in the search process. The
relative salience of NPs in this context can, for the
most part, be determined based on the ontological
relationship between the NPs and knowledge about
the website organization. For instance, if A is an
accessory-of B, then A is more salient than B since,
on our website, accessories typically have their own
webpages with significantly more information than
pages about, for instance, the ThinkPads with which
they are compatible.
Our NP sequencer utilizes a rule-based reasoning
mechanism to rank a set of NPs based on their rel-
ative salience, as determined by their relationship
in the ontology.7 Objects not present in the ontol-
7We are aware that factors involving deeper question under-
ogy are considered less important than those present.
This process produces a list of NPs ranked in de-
creasing order of salience.
5.3 Hub-Page Identifier
As with most websites, the ThinkPad pages on
www.ibm.com are organized hierarchically, with a
dozen or so hub-pages that serve as good starting
points for specific sub-topics, such as mobile acces-
sories and personal printers. However, since these
hub-pages are typically not content-rich, they often
do not receive high scores from the search engine
(over which we have no control). Thus, we devel-
oped a mechanism to explicitly retrieve these hub-
pages when appropriate, and to combine its results
with the outcome of the actual search process.
The hub-page identifier consists of a mapping
from a subset of the named entities in the ontology to
their corresponding hub-pages.8 For each question,
the hub-page identifier retrieves the hub-page for the
most salient NP, if possible, which is presented as
the first entry in the hit list.
5.4 Reinforcement Learning Based Query
Formulation
This main component of RISQUE iteratively formu-
lates queries, issues them to the search engine, and
accumulates the results to construct the hit list. The
query formulation process starts with the most con-
strained query, and each new query is a relaxation of
a previously issued query, obtained by applying one
or more transformation rules to the current query.
The transformation rules are applied in the order ob-
tained by the Q learning algorithm as described in
Section 4.2.
The initial state of the query formulation process
is as follows: url constraint and np phrase are set
to true, while the other attributes are set to their re-
spective maximum values based on the outcome of
the question understanding process. This initial state
represents the most constrained query possible for
the given question, and allows for subsequent relax-
ation via the application of transformation rules.
standing come into play in determining relative salience. We
leave investigation of such features as future work.
8For reasons of robustness, we actually map a named entity
to manually selected keywords which, when issued to the search
engine, retrieves the desired hub-page as the first hit.
When a state s, is visited, a query is generated
based on s and the question. The query terms
are instantiated based on the values of np phrase,
num nps, num modifiers, and num verbs in s and
the question itself, while URL constraints may be
applied based on url constraint and qtype. Finally,
synonyms expansion is applied based on the syn-
onym dictionary in the ontology, while morphologi-
cal expansion is performed on all NPs using a rule-
based inflection procedure.
After a query is issued, the search results are
incorporated into the hit list, and duplicate hits
are removed. A transformation rule amax =
argmaxaQ?(s, a) is applied to yield the new state.
Q?(s, amax) is then decreased to remove it from fur-
ther consideration. This iterative process continues
until the hit list contains 10 or more elements.
6 Example
To illustrate RISQUE?s end-to-end operation, con-
sider the question ?Do you sell a USB hub for a
ThinkPad??
The question is classified as a buy question, given
presence of the verb sell. In addition, two NPs are
identified:
NP1: head = USB hub
NP2: head = ThinkPad
Note that ?USB hub? is identified as a compound
noun by our named-entity recognizer. The NP se-
quencer determines that USB hub is more salient
than ThinkPad since the former is an accessory of
the latter.
The hub-page identifier finds the networking de-
vices hub-page for USB hub, presented as the first
entry in the hit list in Figure 2, where correct web-
pages are boldfaced.
Next, RISQUE invokes its iterative query for-
mulation process to populate the remaining hit
list entries. The initial state is <qtype = buy,
url constraint = true, np phrase = true, num nps
= 2, num modifiers = 0, num verbs = 0>. This
state generates the query shown as ?Query 2? in Fig-
ure 2, which results in 6 hits, of which 4 are correct.
RISQUE selects the optimal transformation rule
for the current state, which is ReinstateModifier.
Since neither NP has any modifier, a second rule,
RelaxNP is attempted, which resulted in a new query
that did not retrieve any previously unseen hits.
Next, RISQUE selects ConstrainNP and RelaxURL,
resulting in the query shown as ?Query 3? in Fig-
ure 2.9 Note that relaxing the URL constraint results
in retrieval of USB hub support pages.
7 Performance Evaluation and Analysis
We evaluated RISQUE?s performance on 102 ques-
tions in the ThinkPad domain previously unseen
to both RISQUE?s knowledge-based and statistical
components. The top 10 hits returned by RISQUE
for each question were manually evaluated for cor-
rectness as in Section 3. A 2NP baseline was ob-
tained by extracting up to two most salient NPs in
each question, searching for the conjunction of all
words in the NPs, and manually evaluating the 10
top hits returned.
We selected the 2NP baseline based on statistics
of keyword query logs on our website, which show
that 98.2% of all queries contain 4 keywords or less.
Furthermore, most three and four-word queries con-
tain two distinct noun phrases, such as ?visualage for
java? and ?application framework for e-business?.
Thus, we use the 2NP baseline as an approximation
of user keyword search performance for our natural
language questions.10
We compared RISQUE?s performance to the
baseline using three metrics:11
1. Total correct: number of questions for which
at least one correct webpage is retrieved.
2. Average correct: average number of correct
webpages retrieved per question.
3. Average rank: average rank of the first correct
webpage in the hit list.
The evaluation results are summarized in Table 2,
where the first and last rows show the 2NP base-
line and RISQUE?s performance, respectively. The
9A set of negative URL constraints is applied at all times to
best exclude parts of the website unrelated to ThinkPads.
10This is likely too high an estimate for current keyword
search performance, since the majority of user queries employ
only one noun phrase.
11We chose not to evaluate our results using the traditional
IR recall measure because for our task, it is often sufficient to
return one page that answers the question instead of attempting
to retrieve all relevant pages.
Question: Do you have a USB hub for a ThinkPad?
Query 1: hub-page identifier
1 Communications, Networking, Input/Output Devices ...
Query 2: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub? ?usb hubs?)
+url: (commerce accessories proven products thinkpad)
-url: research press rs6000 eserver ...
2 Mobile Accessories ...
3 4-Port Ultra-Mini USB Hub
4 ThinkPad TransNote Port Extender
5 Belkin ExpressBus 7-Port USB Hub
6 Portable Drive Bay 2000
7 Belkin BusStation 7 Port Modular USB Hub
Query 3: (thinkpad thinkpads laptop laptops notebook notebooks) (?usb hub? ?usb hubs?)
-url: research press rs6000 eserver ...
8 Java technology for the universal serial bus
9 Multiport USB Hub - Printer compatibility list
10 Multi-Port USB Hub - Overview
Figure 2: RISQUE Results for Sample Question
Total Average Average
Correct Correct Rank
2NPs 30 0.63 4.0
Fixed Order 45 1.24 2.71
RISQUE w/o
hub identifier 56 1.67 2.25
RISQUE 71 1.87 2.11
Table 2: RISQUE Evaluation Results
results show that RISQUE correctly answered 71
questions, a 137% relative improvement over the
baseline. Furthermore, the average number of cor-
rect answers found nearly tripled, while, on average,
the rank of the first correct answer improved from
4.0 to 2.11.
Table 2 further shows performance figures that
evaluate the individual contribution of RISQUE?s
two main components, the hub-page identifier and
the iterative query formulation module. Comparison
between the last two rows in Table 2 shows the effec-
tiveness of the hub-page identifier, which substan-
tially increased the number of questions correctly
answered, but resulted in only minor gain using the
other two performance metrics. To assess the effec-
tiveness of the query formulation module, we used
the best manually-derived rule application sequence
obtained in Section 3. We compared these fixed or-
der performance figures to those for RISQUE w/o
hub identifier which shows that applying Q learning
to derive an optimal state-dependent rule application
order resulted in fairly substantial improvement us-
Maxq 10 5 3 2 1
RISQUE 5.07 4.47 2.89 1.98 1
RISQUE w/o hpi 4.26 3.86 2.80 1.93 1
Table 3: Average Queries Issued for Select Maxqs
20
30
40
50
60
70
0 1 2 3 4 5 6
Nu
mb
er o
f Qu
esti
ons
 An
swe
red
Average Number of Queries Issued
RISQUERISQUE w/o hpi2NP baseline
Figure 3: # Queries Issued vs. System Performance
ing all three metrics.
One of RISQUE?s parameters, maxq, specifies the
maximum number of distinct queries it can issue to
the search engine for each question. Table 3 shows
the average number of queries actually issued for
select values of maxq.12 Figure 3 shows how per-
formance degrades when fewer queries are issued
as a result of lowering maxq for both RISQUE and
RISQUE without the hub-page identifier. It shows
that, with the exception of RISQUE?s performance
12Maxq is 10 for the results in Table 2.
when only one query is issued,13 the number of
questions answered have a near-linear relationship
with the number of queries issued for both sys-
tems. Notice that without the hub-page identifier,
RISQUE?s performance when issuing an average of
1.93 queries per question is nearly the same as that
of the 2NP baseline, while it performs worse than
the baseline when issuing only one query per ques-
tion. This is because our iterative query formula-
tion process intentionally begins with the most con-
strained query, resulting in an empty hit list in many
cases.
8 Conclusions and Future Work
In this paper, we described and evaluated RISQUE,
a hybrid system for performing natural language
search on a large company public website. RISQUE
utilizes a two-pronged approach to generate hit lists
for answering natural language questions. On the
one hand, RISQUE employs a hub-page identi-
fier to retrieve, when possible, a hub-page for the
most salient NP in the question. On the other
hand, RISQUE adopts a statistical iterative query
formulation and retrieval mechanism that generates
new queries by applying transformation rules to
previously-issued queries. By employing these two
components in parallel, RISQUE takes advantages
of both knowledge-driven and machine learning ap-
proaches, and achieves an overall 137% relative im-
provement in the number of questions correctly an-
swered on an unseen test set, compared to a baseline
of 2NP keyword queries.
In our current work, we are focusing on expand-
ing system coverage to other domains. In particu-
lar, we plan to investigate semi-automatic methods
for extracting ontological knowledge from existing
webpages and databases.
Acknowledgments
We would like to thank Dave Ferrucci and Nanda
Kambhatla for helpful discussions, Ruchi Kalra and
Jerry Cwiklik for data preparation, Eric Brown and
the anonymous reviewers for helpful comments on
an earlier draft of this paper, as well as Mike Moran
13In most cases, this query is issued by the hub-page identi-
fier, which has a higher success rate than the queries generated
by the query formulation module.
and Alex Holt for providing technical assistance
with ibm.com search.
References
G. Bierner. 2001. Alternative phrases and natural lan-
guage information retrieval. In Proc. 39th ACL, pages
58?65.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploit-
ing redundancy in question answering. In Proc. 24th
SIGIR, pages 358?365.
P. Hagen, H. Manning, and Y. Paul. 2000. Must search
stink? The Forrester Report, June.
K. Hammond, R. Burke, C. Martin, and S. Lytinen. 1995.
Faq finder: A case-based approach to knowledge nav-
igation. In AAAI Spring Symposium on Information
Gathering in Heterogeneous Environments, pages 69?
73.
S. Harabagui, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and
P. Morarescu. 2001. The role of lexico-semantic feed-
back in open-domain textual question-answering. In
Proc. 39th ACL, pages 274?281.
A. Ittycheriah, M. Franz, W-J. Zhu, and A. Ratnaparkhi.
2001. Question answering using maximum entropy
components. In Proc. 2nd NAACL, pages 33?39.
M. McCord. 1989. Slot grammar: A system for simpler
construction of practical natural language grammars.
In Natural Language and Logic, pages 118?145.
T. Mitchell. 1997. Machine Learning. McGraw Hill.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question-answering by predictive annotation. In Proc.
23rd SIGIR, pages 184?191.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disam-
biguation of proper names in text. In Proc. 5th ANLP.
C. Watkins. 1989. Learning from Delayed Rewards.
Ph.D. thesis, King?s College.
   
		
	 ff