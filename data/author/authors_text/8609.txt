Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 299?306, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Question Series To Evaluate Question Answering System Effectiveness
Ellen M. Voorhees
National Institute of Standards and Technology
Gaithersburg, MD 20899
Abstract
The original motivation for using ques-
tion series in the TREC 2004 question an-
swering track was the desire to model as-
pects of dialogue processing in an evalu-
ation task that included different question
types. The structure introduced by the se-
ries also proved to have an important ad-
ditional benefit: the series is at an appro-
priate level of granularity for aggregating
scores for an effective evaluation. The
series is small enough to be meaningful
at the task level since it represents a sin-
gle user interaction, yet it is large enough
to avoid the highly skewed score distribu-
tions exhibited by single questions. An
analysis of the reliability of the per-series
evaluation shows the evaluation is stable
for differences in scores seen in the track.
The development of question answering technol-
ogy in recent years has been driven by tasks de-
fined in community-wide evaluations such as TREC,
NTCIR, and CLEF. The TREC question answering
(QA) track started in 1999, with the first several edi-
tions of the track focused on factoid questions. A
factoid question is a fact-based, short answer ques-
tion such as How many calories are there in a Big
Mac?. The track has evolved by increasing the type
and difficulty of questions that are included in the
test set. The task in the TREC 2003 QA track was
a combined task that contained list and definition
questions in addition to factoid questions (Voorhees,
2004). A list question asks for different instances of
a particular kind of information to be returned, such
as List the names of chewing gums. Answering such
questions requires a system to assemble an answer
from information located in multiple documents. A
definition question asks for interesting information
about a particular person or thing such as Who is
Vlad the Impaler? or What is a golden parachute?.
Definition questions also require systems to locate
information in multiple documents, but in this case
the information of interest is much less crisply de-
lineated.
Like the NTCIR4 QACIAD challenge (Kato et
al., 2004), the TREC 2004 QA track grouped ques-
tions into series, using the series as abstractions of
information-seeking dialogues. In addition to mod-
eling a real user task, the series are a step toward in-
corporating context-processing into QA evaluation
since earlier questions in a series provide some con-
text for the current question. In the case of the TREC
series, each series contained factoid and list ques-
tions and had the target of a definition associated
with it. Each question in a series asked for some
information about the target. In addition, the final
question in each series was an explicit ?other? ques-
tion, which was to be interpreted as ?Tell me other
interesting things about this target I don?t know
enough to ask directly?. This last question was
roughly equivalent to the definition questions in the
TREC 2003 task.
This paper examines the efficacy of series-based
QA evaluation, and demonstrates that aggregating
scores over individual series provides a more mean-
ingful evaluation than averages of individual ques-
299
tion scores. The next section describes the question
series that formed the basis of the TREC 2004 eval-
uation. Since TREC uses different evaluation proto-
cols for different question types, the following sec-
tion describes the way in which individual question
types were evaluated. Section 3 contrasts the scores
obtained by aggregating individual question scores
by question type or by series, and shows the use of
series leads to a reliable evaluation at differences in
scores that are observed in practice.
1 Question Series
A question series as used in the TREC 2004 QA
track consisted of several factoid questions, zero to
two list questions, and exactly one Other question.
Associated with each series was a definition target.
The series a question belonged to, the order of the
question in the series, and the type of each question
(factoid, list, or Other) were all explicitly encoded in
the XML format used to describe the test set. Exam-
ple series (minus the XML tags) are shown in fig-
ure 1. A target was a person, an organization, or
thing that was a plausible match for the scenario as-
sumed for the task: that the questioner was an ?aver-
age? adult reader of US newspapers who was look-
ing for more information about a term encountered
while reading the paper.
The TREC 2004 test set contains 65 series. Of
the 65 targets, 23 are PERSONs, 25 are ORGANI-
ZATIONs, and 17 are THINGs. The series contain
a total of 230 factoid questions, 56 list questions,
and 65 (one per target) Other questions. Each se-
ries contains at least four questions, counting the
Other question, with most series containing five or
six questions. The maximum number of questions
in a series is ten.
Question series were also the fundamental struc-
ture used in the QACIAD challenge (Question An-
swering Challenge for Information Access Dia-
logue) of NTCIR4. However, there are some impor-
tant differences between the QACIAD and TREC
series. The QACIAD series model a more natu-
ral flow of questions in an information-seeking di-
alogue. Given other evaluation requirements (most
questions need to have an answer in the source doc-
uments, answers to earlier questions should not be
given in later questions, etc.), the series in the TREC
test set are heavily edited versions of the series col-
lected from the original information seekers. The
resulting edited series appear as a stilted conversa-
tional style when viewed from the perspective of true
dialogue, and the series do not reflect the full range
of information requested in the original series. (For
example, TREC requires list question answers to be
concrete entities such as cities or book titles while
the information seekers often asked for fuzzier in-
formation such as lists of descriptive qualities.) The
QACIAD challenge contained two types of series,
gathering series and browsing series. In a gather-
ing series, all of the questions are related to a single
target (that was not explicitly given in QACIAD),
while questions in a browsing series can refer to un-
related targets. The TREC series are all gathering
type series with the target explicitly given. Finally,
the QACIAD series consist of list questions only,
since factoid questions are treated as list questions
with a single answer.
Systems participating in the TREC evaluation
were required to process series independently from
one another, and were required to process an individ-
ual series in question order. That is, systems were
allowed to use questions and answers from earlier
questions in a series to answer later questions in that
same series, but could not ?look ahead? and use later
questions to help answer earlier questions. The se-
ries was the unit used to structure the test set, but
there was no requirement for systems to process a
series as a unit. Some systems appended the target
to each of the questions in its series and then pro-
cessed all resulting question strings independently
as in earlier TREC evaluations. Per-series evaluation
is still valid since the task to be evaluated is defined
in terms of the series and is independent of how sys-
tems choose to process the questions.
Sixty-three runs from 28 participants were sub-
mitted to the TREC 2004 QA track.
2 Scoring Question Series
The evaluation protocol for individual questions de-
pends on the type of the question. This section
summarizes the protocols for the individual question
types and for a series as a whole.
300
3 Hale Bopp comet
3.1 FACTOID When was the comet discovered?
3.2 FACTOID How often does it approach the earth?
3.3 LIST In what countries was the comet visible on its last return?
3.4 OTHER
21 Club Med
21.1 FACTOID How many Club Med vacation spots are there worldwide?
21.2 LIST List the spots in the United States.
21.3 FACTOID Where is an adults-only Club Med?
21.4 OTHER
22 Franz Kafka
22.1 FACTOID Where was Franz Kafka born?
22.2 FACTOID When was he born?
22.3 FACTOID What is his ethnic background?
22.4 LIST What books did he author?
22.5 OTHER
Figure 1: Sample question series from the test set. Series 3 has a THING as a target, series 21 has an
ORGANIZATION as a target, and series 22 has a PERSON as a target.
2.1 Factoid questions
The system response for a factoid question is either
exactly one [doc-id, answer-string] pair or the literal
string ?NIL?. NIL is returned by a system when it be-
lieves there is no answer to the question in the docu-
ment collection. Otherwise, answer-string is a string
containing precisely an answer to the question, and
doc-id is the id of a document in the collection that
supports answer-string as an answer.
Each response was assigned exactly one of the
following four judgments:
incorrect: the answer string does not contain a right
answer or the answer is not responsive;
not supported: the answer string contains a right
answer but the document returned does not sup-
port that answer;
not exact: the answer string contains a right answer
and the document supports that answer, but the
string contains more than just the answer or is
missing bits of the answer;
correct: the answer string consists of exactly the
right answer and that answer is supported by
the document returned.
To be responsive, an answer string is required to
contain appropriate units and to refer to the correct
?famous? entity (e.g., the Taj Mahal casino is not re-
sponsive when the question asks about ?the Taj Ma-
hal?). NIL responses are correct only if there is no
known answer to the question in the collection and
are incorrect otherwise. NIL is correct for 22 of the
230 factoid questions in the test set
An individual factoid question has a binary score,
1 if the response is judged correct and 0 otherwise.
The score for a set of factoid questions is accuracy,
the fraction of questions in the set judged correct.
2.2 List questions
A list question can be thought of as a shorthand for
asking the same factoid question multiple times. The
set of all correct, distinct answers in the document
collection that satisfy the factoid question is the cor-
rect answer for a list question.
A system?s response for a list question is an un-
ordered set of [doc-id, answer-string] pairs such
that each answer-string is considered an instance of
the requested type. Judgments of incorrect, unsup-
ported, not exact, and correct are made for individual
response pairs as in the factoid judging. The asses-
sor is given one run?s entire list at a time, and while
judging for correctness also marks a set of responses
as distinct. The assessor chooses an arbitrary mem-
ber of the equivalent responses to be marked distinct,
301
and the remainder are not marked as distinct. Only
correct responses may be marked as distinct.
The final correct answer set for a list question is
compiled from the union of the correct responses
across all runs plus the instances the assessor found
during question development. For the 55 list ques-
tions used in the evaluation (one list question was
dropped because the assessor decided there were no
correct answers during judging), the average number
of answers per question is 8.8, with 2 as the smallest
number of answers, and 41 as the maximum num-
ber of answers. A system?s response to a list ques-
tion was scored using instance precision (IP) and
instance recall (IR) based on the list of known in-
stances. Let S be the number of known instances,
D be the number of correct, distinct responses re-
turned by the system, and N be the total number of
responses returned by the system. Then IP = D/N
and IR = D/S. Precision and recall were then
combined using the F measure with equal weight
given to recall and precision:
F = 2? IP ? IRIP + IR
The score for a set of list questions is the mean of
the individual questions? F scores.
2.3 Other questions
The Other questions were evaluated using the same
methodology as the TREC 2003 definition ques-
tions (Voorhees, 2003). A system?s response for
an Other question is an unordered set of [doc-id,
answer-string] pairs as for list questions. Each string
is presumed to be a facet in the definition of the
series? target that had not yet been covered by ear-
lier questions in the series. The requirement to not
repeat information already covered by earlier ques-
tions in the series made answering Other questions
somewhat more difficult than answering TREC 2003
definition questions.
Judging the quality of the systems? responses is
done in two steps. In the first step, all of the answer
strings from all of the systems? responses are pre-
sented to the assessor in a single list. Using these
responses and the searches done during question de-
velopment, the assessor creates a list of information
nuggets about the target. An information nugget is
an atomic piece of information about the target that
is interesting (in the assessor?s opinion) and is not
part of an earlier question in the series or an answer
to an earlier question in the series. An information
nugget is atomic if the assessor can make a binary
decision as to whether the nugget appears in a re-
sponse. Once the nugget list is created for a target,
the assessor marks some nuggets as vital, meaning
that this information must be returned for a response
to be good. Non-vital nuggets act as don?t care con-
ditions in that the assessor believes the information
in the nugget to be interesting enough that returning
the information is acceptable in, but not necessary
for, a good response.
In the second step of judging the responses, an
assessor goes through each system?s response in turn
and marks which nuggets appear in the response. A
response contains a nugget if there is a conceptual
match between the response and the nugget; that is,
the match is independent of the particular wording
used in either the nugget or the response. A nugget
match is marked at most once per response?if the
response contains more than one match for a nugget,
an arbitrary match is marked and the remainder are
left unmarked. A single [doc-id, answer-string] pair
in a system response may match 0, 1, or multiple
nuggets.
Given the nugget list and the set of nuggets
matched in a system?s response, the nugget recall
of a response is the ratio of the number of matched
nuggets to the total number of vital nuggets in the
list. Nugget precision is much more difficult to com-
pute since there is no effective way of enumerat-
ing all the concepts in a response. Instead, a mea-
sure based on length (in non-white space charac-
ters) is used as an approximation to nugget preci-
sion. The length-based measure starts with an initial
allowance of 100 characters for each (vital or non-
vital) nugget matched. If the total system response
is less than this number of characters, the value of
the measure is 1.0. Otherwise, the measure?s value
decreases as the length increases using the function
1 ? length?allowancelength . The final score for an Other
question is computed as the F measure with nugget
recall three times as important as nugget precision:
F (? = 3) = 10? precision ? recall9? precision + recall .
Note that the Other question for series S7 was
302
mistakenly left unjudged, so the series was was re-
moved from the TREC 2004 evaluation. This means
final scores for runs were computed over 64 rather
than 65 question series.
2.4 Per-series scores
In the TREC 2003 evaluation, the final score for a
run was computed as a weighted average of the mean
scores for different question types:
FinalScore = .5FactoidAccuracy + .25ListAveF
+.25DefinitionAveF.
Since each of the component scores ranges between
0 and 1, the final score is also in that range. The
weights for the different components reflect the de-
sire to emphasize factoid scores, since factoid tech-
nology is the most mature, while still allowing other
components to affect the final score. The specific
weights used match this general objective, but are
otherwise arbitrary. No experiments have been run
examining the effect of different weights on the sta-
bility of the final scores, but small perturbations in
the weights should have little effect on the results.
An individual question series also contains a mix-
ture of different question types, so the weighted av-
erage can be computed for an individual series rather
than the test set as a whole. The mean of the per-
series scores is then used as the final score for a run.
We use the same weighted average as above to
compute the score for an individual series that con-
tains all three question types, using only the scores
for questions belonging to that series in the compu-
tation and using the Other question?s score in place
of the average of definition questions scores. For
those series that did not contain any list questions,
the score was computed as .67FactoidAccuracy +
.33OtherF. Figure 2 shows the average per-series
score for the best run for each of the top 10 groups
that participated in TREC 2004.
3 Analysis of Per-series Evaluation
The main purpose of evaluations such as TREC is to
provide system builders with the information needed
to improve their systems. An informative evaluation
must be reliable (i.e., the results must be trustwor-
thy) as well as capture salient aspects of the real
user task. This section first examines the user task
0.0
0.2
0.4
0.6
A
ve
ra
ge
 p
er
-s
er
ie
s s
co
re
Figure 2: Average per-series scores for top ten QA
track runs.
abstracted by the per-series evaluation, and then de-
rives an empirical estimate of the reliability of the
evaluation.
3.1 Modeling a User Task
The set of questions used to aggregate individual
questions? scores determines the emphasis of a QA
evaluation. In the TREC 2003 combined task there
were no series but there were different question
types, so question scores were first averaged by
question type and then those averages were com-
bined. This strategy emphasizes question-type anal-
ysis in that it is easy to compare different systems?
abilities for the different question types. The QA-
CIAD challenge contained only a single question
type but introduced a series structure into the test
set (Kato et al, 2004). In QACIAD, the scores
were aggregated over the series and the series scores
averaged. The QACIAD series were specifically
constructed to be an abstraction of an information
seeker?s dialogue, and the aggregation of scores over
series supports comparing different series types. For
example, QACIAD results show browsing series to
be more difficult than gathering series.
The TREC 2004 QA track contained both series
structure and different question types, so individual
question scores could be aggregated either by series
or by question type. In general, the two methods
of aggregation lead to different final scores. Ag-
gregating by question type gives equal weight to
303
S1 S4 S8 S11 S15 S19 S23 S27 S31 S35 S39 S43 S47 S51 S55 S59 S63
0.
0
0.
2
0.
4
0.
6
0.
8
Figure 3: Box and whiskers plot of per-series scores across all TREC 2004 runs. The x-axis shows the series
number and the y-axis the score.
each of the questions of the same type, while aggre-
gating by series gives equal weight to each series.
This is the same difference as between micro- and
macro-averaging of document retrieval scores. For
the set of runs submitted to TREC 2004, the abso-
lute value of the final scores when aggregated by se-
ries were generally somewhat greater than the final
scores when aggregated by question type, though it
is possible for the question-type-aggregated score to
be the greater of the two. The relative scores for dif-
ferent runs (i.e., whether one run was better than an-
other) were usually, but not always, the same regard-
less of which aggregation method was used. The
Kendall ? (Stuart, 1983) measure of correlation be-
tween the system rankings produced by sorting the
runs by final score for each of the two aggregation
methods was 0.971, where identical rankings would
have a correlation of 1.0.
Despite the relatively minor differences in runs?
final scores when aggregating by series or by ques-
tion type, there is a strong reason to prefer the series
aggregation. An individual series is small enough to
be meaningful at the task level (it represents a sin-
gle user?s interaction) yet large enough for a series
score to be meaningful. Figure 3 shows a box-and-
whiskers plot of the per-series scores across all runs
for each series. A box in the plot shows the extent
of the middle half of the scores for that series, with
the median score indicated by the line through the
box. The dotted lines (the ?whiskers?) extend to
a point that is 1.5 times the interquartile distance,
or the most extreme score, whichever is less. Ex-
treme scores that are greater than the 1.5 times the
interquartile distance are plotted as circles. The plot
shows that only a few series (S21, S25, S37, S39)
have median scores of 0.0. This is in sharp con-
trast to the median scores of individual questions.
For the TREC 2004 test set, 212 of the 230 factoid
questions (92.2%) have a zero median, 39 of 55 list
questions (70.9%) have a zero median, and 41 of 64
Other questions (64.1%) have a zero median.
Having a unit of evaluation that is at the appro-
priate level of granularity is necessary for meaning-
ful results from the methodology used to assess the
reliability of an evaluation. This methodology, de-
scribed below, was originally created for document
retrieval evaluations (Voorhees and Buckley, 2002)
where the topic is the unit of evaluation. The distri-
304
bution of scores across runs for an individual topic
is much the same as the distribution of scores for
the individual series as in figure 3. Score distribu-
tions that are heavily skewed toward zero make the
evaluation look far more reliable than is likely to be
the case since the reliability methodology computes
a measure of the variability in scores.
3.2 Reliability
TREC uses comparative evaluations: one system is
considered to be more effective than another if the
evaluation score computed for the output of the first
system is greater than the evaluation score computed
for the output of the second system. Since all mea-
surements have some (unknown) amount of error as-
sociated with them, there is always a chance that
such a comparison can lead to the wrong result. An
analysis of the reliability of an evaluation establishes
bounds for how likely it is for a single comparison
to be in error.
The reliability analysis uses the runs submitted to
the track to empirically determine the relationship
among the number of series in a test set, the ob-
served difference in scores (?) between two runs,
and the likelihood that a single comparison of two
runs leads to the correct conclusion. Once estab-
lished, the relationship is used to derive the mini-
mum difference in scores required for a certain level
of confidence in the results given that there are 64
series in the test set.
The core of the procedure for establishing the re-
lationship is comparing the effectiveness of a pair
runs on two disjoint, equal-sized sets of series to see
if the two sets disagree as to which of the runs is
better. We define the error rate as the percentage of
comparisons that have such a disagreement. Since
the TREC 2004 track had 64 series, we can directly
compute the error rate for test sizes up to 32 series.
The smallest test set used is five series since fewer
than five series in a test set is too noisy to be infor-
mative. By fitting curves to the values observed for
test set sizes between 5 and 32, we can extrapolate
the error rates to test sets up to 64 series.
When calculating the error rate, the difference be-
tween two runs? scores is categorized into a set of
bins based on the size of the difference. The first bin
contains runs with a difference of less than 0.01 (in-
cluding no difference at all). The next bin contains
runs whose difference is at least 0.01 but less than
0.02. The limits for the remaining bins increase by
increments of 0.01.
Each test set size from 5 to 32 is treated as a sep-
arate experiment. Within an experiment, we ran-
domly select two disjoint sets of series of the re-
quired size. We compute the average series score
over both sets for all runs, then count the number of
times we see a disagreement as to which run is bet-
ter for all pairs of runs using the bins to segregate the
counts by size of the difference in scores. The entire
procedure is repeated 50 times (i.e., we perform 50
trials), with the counts of the number of disagree-
ments kept as running totals over all trials. The ratio
of the number of disagreements observed in a bin to
the total number of cases that land in that bin is the
error rate for the bin.
Figure 4 shows the error rate curves for five sep-
arate bins. In the figure the test set size is plot-
ted on the x-axis and the error rate is plotted on
the y-axis. The individual points in the graphs are
the data points actually computed by the procedure
above, while the lines are the best-fit exponential
curve for the data points in the current bin and ex-
trapolated to size 64. The top curve is for the bin
with 0.01 ? ? < 0.02 and the bottom curve for the
bin with 0.05 ? ? < 0.06; the intervening curves
are for the intervening bins, in order with smaller
??s having larger error rates. An error rate no greater
than 5%, requires a difference in scores of at least
0.05, which can be obtained with a test set of 47 se-
ries. Score differences of between 0.04 and 0.05 (the
fourth curve) have an error rate slightly greater than
5% when there are 64 series in the test set.
Having established the minimum size of the dif-
ference in scores needed to be confident that two
runs are actually different, it is also important to
know whether differences of the required size actu-
ally occur in practice. If it is rare to observe a dif-
ference in scores as large as the minimum, then the
evaluation will be reliable but insensitive. With 64
runs submitted to the TREC 2004 QA track, there
are 1953 run pairs; 70% of the pairs have a dif-
ference in average per-series score that is at least
0.05. Many of the pairs in the remaining 30% are
truly equivalent?for example, runs submitted by
the same group that had very small differences in
their processing. In figure 2, the difference in scores
305
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0 5 10 15 20 25 30 35 40 45 50 55 60 65
Test Set Size (number of series)
Er
ro
r 
Ra
te
0.01   <0.02
0.02   < 0.03
0.03   < 0.04
0.04   < 0.05
0.05   < 0.06
Figure 4: Extrapolated error rates for per-series scores for different test set sizes.
between each of the first three runs and its next clos-
est run is greater than 0.05, while the next five runs
are all within 0.05 of one another.
4 Conclusion
Question series have been introduced into recent
question answering evaluations as a means of mod-
eling dialogues between questioners and systems.
The abstraction allows researchers to investigate
methods for answering contextualized questions and
for tracking (some forms of) the way objects are re-
ferred to in natural dialogues. The series have an
important evaluation benefit as well. The individual
series is at the correct level of granularity for aggre-
gating scores for a meaningful evaluation. Unlike
individual questions that have heavily skewed score
distributions across runs, per-series score distribu-
tions resemble the distributions of per-topic scores
in document retrieval evaluations. This allows the
methodology developed for assessing the quality of
a document retrieval evaluation to be meaningfully
applied to the per-series evaluation. Such an analy-
sis of the TREC 2004 QA track per-series evaluation
shows the evaluation results to be reliable for differ-
ences in scores that are often observed in practice.
References
Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui, and
Noriko Kando. 2004. Handling information access di-
alogue through QA technologies?A novel challenge
for open-domain question answering. In Proceedings
of the HLT-NAACL 2004 Workshop on Pragmatics of
Question Answering, pages 70?77, May.
Alan Stuart. 1983. Kendall?s tau. In Samuel Kotz and
Norman L. Johnson, editors, Encyclopedia of Statisti-
cal Sciences, volume 4, pages 367?369. John Wiley &
Sons.
Ellen M. Voorhees and Chris Buckley. 2002. The effect
of topic set size on retrieval experiment error. In Pro-
ceedings of the 25th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 316?323.
Ellen M. Voorhees. 2003. Evaluating answers to defi-
nition questions. In Proceedings of the 2003 Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2003), Volume 2, pages 109?
111, May.
Ellen M. Voorhees. 2004. Overview of the TREC
2003 question answering track. In Proceedings of
the Twelfth Text REtrieval Conference (TREC 2003),
pages 54?68.
306
Evaluating the Evaluation:
A Case Study Using the TREC 2002 Question Answering Track
Ellen M. Voorhees
National Institute of Standards and Technology
Gaithersburg, MD 20899
ellen.voorhees@nist.gov
Abstract
Evaluating competing technologies on a com-
mon problem set is a powerful way to improve
the state of the art and hasten technology trans-
fer. Yet poorly designed evaluations can waste
research effort or even mislead researchers with
faulty conclusions. Thus it is important to ex-
amine the quality of a new evaluation task to es-
tablish its reliability. This paper provides an ex-
ample of one such assessment by analyzing the
task within the TREC 2002 question answer-
ing track. The analysis demonstrates that com-
parative results from the new task are stable,
and empirically estimates the size of the dif-
ference required between scores to confidently
conclude that two runs are different.
Metric-based evaluations of human language technol-
ogy such as MUC and TREC and DUC continue to pro-
liferate (Sparck Jones, 2001). This proliferation is not
difficult to understand: evaluations can forge communi-
ties, accelerate technology transfer, and advance the state
of the art. Yet evaluations are not without their costs. In
addition to the financial resources required to support the
evaluation, there are also the costs of researcher time and
focus. Since a poorly defined evaluation task wastes re-
search effort, it is important to examine the validity of an
evaluation task. In this paper, we assess the quality of
the new question answering task that was the focus of the
TREC 2002 question answering track.
TREC is a workshop series designed to encourage re-
search on text retrieval for realistic applications by pro-
viding large test collections, uniform scoring procedures,
and a forum for organizations interested in comparing
results. The conference has focused primarily on the
traditional information retrieval problem of retrieving a
ranked list of documents in response to a statement of
information need, but also includes other tasks, called
tracks, that focus on new areas or particularly difficult
aspects of information retrieval. A question answering
(QA) track was started in TREC in 1999 (TREC-8) to ad-
dress the problem of returning answers, rather than doc-
ument lists, in response to a question.
The task for each of the first three years of the QA track
was essentially the same. Participants received a large
corpus of newswire documents and a set of factoid ques-
tions such as How many calories are in a Big Mac? and
Who invented the paper clip?. Systems were required to
return a ranked list of up to five [document-id, answer-
string] pairs per question such that each answer string
was believed to contain an answer to the question. Hu-
man assessors read each string and decided whether the
string actually did contain an answer to the question. An
individual question received a score equal to the recip-
rocal of the rank at which the first correct response was
returned, or zero if none of the five responses contained
a correct answer. The score for a submission was then
the mean of the individual questions? reciprocal ranks.
Analysis of the TREC-8 track confirmed the reliability of
this evaluation task (Voorhees and Tice, 2000): the asses-
sors understood and could do their assessing job; relative
scores between systems were stable despite differences
of opinion by assessors; and intuitively better systems re-
ceived better scores.
The task for the TREC 2002 QA track changed sig-
nificantly from the previous years? task, and thus a new
assessment of the track is needed. This paper provides
that assessment by examining both the ability of the hu-
man assessors to make the required judgments and the
effect that differences in assessor opinions have on com-
parative results, plus empirically establishing confidence
intervals for the reliability of a comparison as a function
of the difference in effectiveness scores. The first section
defines the 2002 QA task and provides a brief summary
of the system results. The following three sections look
at each of the evaluation issues in turn. The final sec-
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 181-188
                                                         Proceedings of HLT-NAACL 2003
tion summarizes the findings, and outlines shortcomings
of the evaluation that remain to be addressed.
1 The TREC 2002 QA Track
The goal of the question answering track is to foster re-
search on systems that retrieve answers rather than docu-
ments, with particular emphasis on systems that function
in unrestricted domains. To date the track has consid-
ered only a very restricted version of the general ques-
tion answering problem, finding answers to closed-class
questions in a large corpus of newspaper articles. Kupiec
defined a closed-class question as ?a question stated in
natural language, which assumes some definite answer
typified by a noun phrase rather than a procedural an-
swer? (Kupiec, 1993). The TREC 2002 track continued
to use closed-class questions, but made two major de-
partures from the task as defined in earlier years. The
first difference was that systems were to return exact an-
swers rather than the text snippets containing an answer
that were accepted previously. The second difference was
that systems were required to return exactly one response
per question and the questions were to be ranked by the
system?s confidence in the answer it had found.
The change to exact answers was motivated by the be-
lief that a system?s ability to recognize the precise extent
of the answer is crucial to improving question answering
technology. The problems with using text snippets as re-
sponses were illustrated in the TREC 2001 track. Each of
the answer strings shown in Figure 1 was judged correct
for the question What river in the US is known as the Big
Muddy?, yet earlier responses are clearly better than later
ones. Accepting only exact answers as correct forces sys-
tems to demonstrate that they know precisely where the
answer lies in the snippets.
The second change, ranking questions by confidence
in the answer, tested a system?s ability to recognize when
it has found a correct answer. Systems must be able to
recognize when they do not know the answer to avoid
returning incorrect responses. In many applications re-
turning a wrong answer is much worse than returning a
?Don?t know? response.
1.1 Task Definition
Incorporating these two changes into the previous QA
task resulted in the following task definition. Participants
were given a large corpus of newswire articles and a set
of 500 closed-class questions. Some of the questions did
not have answers in the document collection. A run con-
sisted of exactly one response for each question. A re-
sponse was either a [document-id, answer-string] pair or
the string ?NIL?, which was used to indicate that the sys-
tem believed there was no correct answer in the collec-
tion. Within a run, questions were ordered from most
confident response to least confident response. All runs
were required to be produced completely automatically?
no manual intervention of any kind was permitted.
The document collection used as the source of answers
was the the AQUAINT Corpus of English News Text
(LDC catalog number LDC2002T31). The collection
is comprised of documents from three different sources:
the AP newswire from 1998?2000, the New York Times
newswire from 1998?2000, and the (English portion of
the) Xinhua News Agency from 1996?2000. There are
approximately 1,033,000 documents and 3 gigabytes of
text in the collection.
The test set of questions were drawn from MSNSearch
and AskJeeves logs. NIST assessors searched the docu-
ment collection for answers to candidate questions from
the logs. NIST staff selected the final test set from among
the candidates that had answers, keeping some questions
for which the assessors found no answer. NIST corrected
the spelling, punctuation, and grammar of the questions
in the logs1, but left the content as it was. NIST did
not include any definition questions (Who is Duke Elling-
ton? What are polymers?) in the test set, but otherwise
made no attempt to control the relative number of differ-
ent types of questions in the test set.
A system response consisting of an [document-id,
answer-string] pair was assigned exactly one judgment
by a human assessor as follows:
wrong: the answer string does not contain a correct an-
swer or the answer is not responsive;
not supported: the answer string contains a correct an-
swer but the document returned does not support
that answer;
not exact: the answer string contains a correct answer
and the document supports that answer, but the
string contains more than just the answer (or is miss-
ing bits of the answer);
right: the answer string consists of exactly a correct an-
swer and that answer is supported by the document
returned.
Only responses judged right were counted as correct in
the final scoring. A NIL response was counted as correct
if there is no known answer in the document collection
for that question (i.e., the assessors did not find an an-
swer during the candidate selection phase and no system
returned a right response for it). Forty-six questions have
no known answer in the collection.
The scoring metric used, called the confidence-
weighted score, was chosen to emphasize the system?s
ability to correctly rank its responses. The metric is
1Unfortunately, some errors remain in the test questions.
Scores were nevertheless computed over all 500 questions as
released by NIST.
the Mississippi
Known as Big Muddy, the Mississippi is the longest
as Big Muddy , the Mississippi is the longest
messed with . Known as Big Muddy , the Mississip
Mississippi is the longest river in the US
the Mississippi is the longest river in the US,
the Mississippi is the longest river(Mississippi)
has brought the Mississippi to its lowest
ipes.In Life on the Mississippi, Mark Twain wrote t
Southeast;Mississippi;Mark Twain;officials began
Known; Mississippi; US,; Minnesota;Gulf Mexico
Mud Island,;Mississippi;"The;-- history,;Memphis
Figure 1: Correct text snippets for What river in the US is known as the Big Muddy?
an analog of document retrieval?s uninterpolated average
precision in that it rewards a system for a correct answer
early in the ranking more than it rewards for a correct an-
swer later in the ranking. More formally, if there are
 
questions in the test set, the confidence-weighted score is
defined to be

 

 

number correct in first  ranks
 	
1.2 Track Results
Table 1 gives evaluation results for a subset of the runs
submitted to the TREC 2002 QA track. The table in-
cludes one run each from the ten groups who submitted
the top-scoring runs. The run shown in the table is the
run with the best confidence-weighted score (?Score?).
Also given in the table are the percentage of questions
answered correctly, and the precision and recall for rec-
ognizing when there is no correct answer in the document
collection (?NIL Accuracy?). Precision of recognizing
no answer is the ratio of the number of times NIL was re-
turned and correct to the number of times it was returned;
recall is the ratio of the number of times NIL was returned
and correct to the number of times it was correct (46).
QA systems have become increasingly complex over
the four years of the TREC track such that there is now lit-
tle in common across all systems. Generally a system will
classify an incoming question according to an ontology of
question types (which varies from small sets of broad cat-
egories to highly-detailed hierarchical schemes) and then
perform type-specific processing. Many TREC 2002 sys-
tems used specific data sources such as name lists and
gazetteers, which were searched when the system deter-
mined the question to be of an appropriate type. The web
was used as a data source by most systems, though it was
used in different ways. For some systems the web was the
primary source of an answer that the system then mapped
to a document in the corpus to return as a response. Other
% NIL Accuracy
Run Tag Score Correct Prec Recall
LCCmain2002 

	  	

 

	 


	 


exactanswer 

	 



	


	


	 

pris2002 

	 



	

 

	




	 

IRST02D1 

	 	
 

	

Evaluating Answers to Definition Questions
Ellen M. Voorhees
National Institute of Standards and Technology
Gaithersburg, MD 20899
ellen.voorhees@nist.gov
Abstract
This paper describes an initial evaluation of
systems that answer questions seeking defini-
tions. The results suggest that humans agree
sufficiently as to what the basic concepts that
should be included in the definition of a par-
ticular subject are to permit the computation of
concept recall. Computing concept precision is
more problematic, however. Using the length
in characters of a definition is a crude approxi-
mation to concept precision that is nonetheless
sufficient to correlate with humans? subjective
assessment of definition quality.
The TREC question answering track has sponsored
a series of evaluations of systems? abilities to answer
closed class questions in many domains (Voorhees,
2001). Closed class questions are fact-based, short an-
swer questions. The evaluation of QA systems for closed
class questions is relatively simple because a response
to such a question can be meaningfully judged on a bi-
nary scale of right/wrong. Increasing the complexity of
the question type even slightly significantly increases the
difficulty of the evaluation because partial credit for re-
sponses must then be accommodated.
The ARDA AQUAINT1 program is a research initia-
tive sponsored by the U.S. Department of Defense aimed
at increasing the kinds and difficulty of the questions au-
tomatic systems can answer. A series of pilot evaluations
has been planned as part of the research agenda of the
AQUAINT program. The purpose of each pilot is to de-
velop an effective evaluation methodology for systems
that answer a certain kind of question. One of the first
pilots to be implemented was the Definitions Pilot, a pi-
lot to develop an evaluation methodology for questions
such as What is mold? and Who is Colin Powell?.
1See http:///www.ic-arda.org/InfoExploit/
aquaint/index.html.
This paper presents the results of the pilot evaluation.
The pilot demonstrated that human assessors generally
agree on the concepts that should appear in the definition
for a particular subject, and can find those concepts in the
systems? responses. Such judgments support the compu-
tation of concept recall, but do not support concept pre-
cision since it is not feasible to enumerate all concepts
contained within a system response. Instead, the length
of a response is used to approximate concept precision.
An F-measure score combining concept recall and length
is used as the final metric for a response. Systems ranked
by average F score correlate well with assessors? subjec-
tive opinions as to definition quality.
1 The Task
The systems? task in the pilot was as follows. For each
of 25 questions the system retrieved a list of text frag-
ments such that each fragment was a component of the
definition. The list was assumed to be ordered such that
the more important elements in the definition appeared
earlier in the list. There were no limits placed on ei-
ther the length of an individual fragment or on the num-
ber of items in a list, though systems knew they would
be penalized for retrieving extraneous information. Six
AQUAINT contractors submitted eight runs to the pilot.
The eight runs are labeled A?H in the discussion below.
The questions were developed by NIST assessors who
searched a set of news articles for definition targets. The
result of question development was a question phrased as
either ?Who is. . . ? or ?What is. . . ?, plus their own defi-
nition of the target. In general, these definitions consisted
of one or two paragraphs of English prose.
2 Assessing System Responses
Each system response was independently judged by two
different assessors. In what follows, the ?author? asses-
sor is the assessor who originally created the question; the
?other? assessor is the second assessor to judge the ques-
tion. Each assessor performed two rounds of assessing
per question.
In the first round of assessing, the assessor assigned
two scores to the response from a system. One score was
for the content of the response and the other for its or-
ganization, with each score on a scale of 0?10. A high
content score indicated that the response contained most
of the information it should contain and little misleading
information. A high organization score indicated the re-
sponse ranked the more important information before the
less important information, and contained little or no ir-
relevant information. The final score for a question was a
function of the organization and content scores, with the
content score receiving much more emphasis.
The ranking of systems when using the question au-
thor to assign scores was FADEBGCH; the ranking was
FAEGDBHC when using scores assigned by the other as-
sessor. The final scores for the systems varied across as-
sessors largely due to different interpretations of the or-
ganization score. Different assessors used different de-
fault scores when there was only one entry in the sys-
tem response; organization scores also appeared to be
strongly correlated with content scores. Despite these dif-
ferences, the judgments do provide some guidance as to
how a more quantitative scoring metric should rank sys-
tems. The assessors preferred the responses from system
F over those from system A, which in turn was preferred
over the remainder of the systems. Responses from sys-
tems C and H were the least preferred.
The goal of the second round of assessing was to sup-
port a more quantitative evaluation of the system re-
sponses. In this round of assessing, an assessor first cre-
ated a list of ?information nuggets? about the target us-
ing all the system responses and the question author?s
definition. An information nugget was defined as a fact
for which the assessor could make a binary decision as
to whether a response contained the nugget. The asses-
sor then decided which nuggets were vital?nuggets that
must appear in a definition for that definition to be good.
Finally, the assessor went through each of the system re-
sponses and marked where each nugget appeared in the
response. If a system returned a particular nugget more
than once, it was marked only once.
Figure 1 shows an example of how one response was
judged for the question Who is Christopher Reeve?. The
left side of the figure shows the concept list developed by
the assessor, with vital concepts marked with a star. The
right side of the figure shows a system response with the
concepts underlined and tagged with the concept number.
In Figure 1, each list entry has at most one concept
marked. However, that was not generally the case. Many
list entries contained multiple concepts while others con-
tained none. Thus, using the list entry as the unit for eval-
uation is not sensible. Instead, we should calculate mea-
sures in terms of the concepts themselves. Computing
concept recall is straightforward given these judgments;
it is the ratio of the number of correct concepts retrieved
to the number of concepts in the assessor?s list. But the
corresponding measure of concept precision, the ratio of
the number of correct concepts retrieved to the total num-
ber of concepts retrieved, is problematic since the correct
value for the denominator is unknown. A trial evalua-
tion prior to the pilot showed that assessors found enu-
merating all concepts represented in a response to be so
difficult as to be unworkable. For example, how many
concepts are contained in ?stars on Sunday in ABC?s re-
make of ?Rear Window?? Using only concept recall as
the final score is not workable either, since systems would
not be rewarded for being selective: retrieving the entire
document collection would get a perfect score for every
question.
Borrowing from the evaluation of summarization sys-
tems (Harman and Over, 2002), we can use length as
a (crude) approximation to precision. A length-based
measure captures the intuition that users would prefer
the shorter of two definitions that contain the same con-
cepts. The length-based measure used in the pilot gives
a system an allowance of 100 (non-white-space) char-
acters for each correct concept it retrieves. The pre-
cision score is set to one if the response is no longer
than this allowance. If the response is longer than the
allowance, the precision score is downgraded using the
function precision   length  allowancelength .
Remember that the assessors marked some concepts as
vital and the remainder are not vital. The non-vital con-
cepts act as a ?don?t care? condition. That is, systems
should be penalized for not retrieving vital concepts, and
penalized for retrieving items that are not on the asses-
sor?s concept list at all, but should be neither penalized
nor rewarded for retrieving a non-vital concept. To imple-
ment the don?t care condition, concept recall is computed
only over vital concepts, while the character allowance in
the precision computation is based on both vital and non-
vital concepts. The recall for the example in Figure 1 is
thus 2/3, and the character allowance is 300.
The final score for a response was computed using the
F-measure, a function of both recall (R) and precision (P).
The general version of the F-measure is
F  

	
 RP

	
P

R
where

is a parameter signifying the relative importance
of recall and precision. The main evaluation in the pi-
lot used a value of 5, indicating that recall is 5 times as
important as precision. The value of 5 is arbitrary, but re-
flects both the emphasis given to content in the first round
1 * actor
2 * accident
3 * treatment/therapy
4 spinal cord injury activist
5 written an autobiography
6 human embryo research activist
a) list of concepts
  Actor 
  the actor who was paralyzed when he fell off his horse 	
  the name attraction
  stars on Sunday in ABC?s remake of ?Rear Window
  was injured in a show jumping accident and
has become a spokesman for the cause 
b) system response
Figure 1: Assessor annotation of a sample response for Who is Christopher Reeve?
author other
F 0.688 F 0.757
A 0.606 A 0.687
D 0.568 G 0.671
G 0.562 D 0.669
E 0.555 E 0.657
B 0.467 B 0.522
C 0.349 C 0.384
H 0.330 H 0.365
Table 1: Average F scores per system per assessor type.
of assessing and acknowledges the crudeness of the pre-
cision approximation.
Table 1 gives the average F scores for the pilot runs
as evaluated using both assessors? judgments. As can be
seen from the table, the rankings of systems are stable
across different assessors in that the only difference in
the rankings are for two runs whose scores are extremely
similar (D and G). While the absolute value of the scores
is different when using different assessors, the magnitude
of the difference between scores is generally preserved.
For example, there is a large gap between the scores for
systems F and A, and a much smaller gap for systems C
and H. The rankings also obey the ordering constraints
suggested by the first round of assessing.
The different systems in the pilot took different ap-
proaches to producing their definitions. System H always
returned a single text snippet as a definition. System B
returned a set of complete sentences. System G tended to
be relatively terse, while F and A were more verbose. The
average length of a response for each system is A: 1121.2,
B: 1236.5, C: 84.7, D: 281.8, E: 533.9, F: 935.6, G: 164.5,
and H: 33.7. The differences in the systems are reflected
in their relative scores when different

values are used.
For example, when evaluated using

  and the au-
thors? judgments, the system ranking is GFDAECHB; for

   the ranking is GDFAECHB. Thus as expected, as
precision gains in importance, system G rises in the rank-
ings, system B falls quickly, and system F also sinks.
3 Conclusion
The AQUAINT pilot evaluations are designed to explore
the issues surrounding new evaluation methodologies for
question answering systems using a small set of systems.
If a pilot is successful, the evaluation will be transferred
to the much larger TREC QA track. The definition pi-
lot demonstrated that relative F scores based on concept
recall and adjusted response length are stable when com-
puted using different human assessor judgments, and re-
flect intuitive judgments of quality. The main measure
used in the pilot strongly emphasized recall, but varying
the F measure?s

parameter allows different user prefer-
ences to be accommodated as expected. Definition ques-
tions will be included as a part of the TREC 2003 QA
track where they will be evaluated using this methodol-
ogy.
Acknowledgments
This paper was informed by the discussion that took
place on the AQUAINT definition question mailing list.
My thanks to the AQUAINT contractors who submitted
results to the pilot evaluation, and especially to Ralph
Weischedel and Dan Moldovan who coordinated the def-
inition pilot.
References
Donna Harman and Paul Over. 2002. The DUC sum-
marization evaluations. In Proceedings of the Interna-
tional Conference on Human Language Technology.
Ellen M. Voorhees. 2001. The TREC question answer-
ing track. Journal of Natural Language Engineering,
7(4):361?378.
Proceedings of ACL-08: HLT, pages 63?71,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Contradictions and Justifications: Extensions to the Textual Entailment Task
Ellen M. Voorhees
National Institute of Standards and Technology
Gaithersburg, MD 20899-8940, USA
ellen.voorhees@nist.gov
Abstract
The third PASCAL Recognizing Textual En-
tailment Challenge (RTE-3) contained an op-
tional task that extended the main entailment
task by requiring a system to make three-way
entailment decisions (entails, contradicts, nei-
ther) and to justify its response. Contradic-
tion was rare in the RTE-3 test set, occurring
in only about 10% of the cases, and systems
found accurately detecting it difficult. Subse-
quent analysis of the results shows a test set
must contain many more entailment pairs for
the three-way decision task than the traditional
two-way task to have equal confidence in sys-
tem comparisons. Each of six human judges
representing eventual end users rated the qual-
ity of a justification by assigning ?understand-
ability? and ?correctness? scores. Ratings of
the same justification across judges differed
significantly, signaling the need for a better
characterization of the justification task.
1 Introduction
The PASCAL Recognizing Textual Entailment (RTE)
workshop series (see www.pascal-network.
org/Challenges/RTE3/) has been a catalyst
for recent research in developing systems that are
able to detect when the content of one piece of
text necessarily follows from the content of another
piece of text (Dagan et al, 2006; Giampiccolo et al,
2007). This ability is seen as a fundamental com-
ponent in the solutions for a variety of natural lan-
guage problems such as question answering, sum-
marization, and information extraction. In addition
to the main entailment task, the most recent Chal-
lenge, RTE-3, contained a second optional task that
extended the main task in two ways. The first exten-
sion was to require systems to make three-way en-
tailment decisions; the second extension was for sys-
tems to return a justification or explanation of how
its decision was reached.
In the main RTE entailment task, systems report
whether the hypothesis is entailed by the text. The
system responds with YES if the hypothesis is en-
tailed and NO otherwise. But this binary decision
conflates the case when the hypothesis actually con-
tradicts the text?the two could not both be true?
with simple lack of entailment. The three-way en-
tailment decision task requires systems to decide
whether the hypothesis is entailed by the text (YES),
contradicts the text (NO), or is neither entailed by
nor contradicts the text (UNKNOWN).
The second extension required a system to explain
why it reached its conclusion in terms suitable for an
eventual end user (i.e., not system developer). Ex-
planations are one way to build a user?s trust in a
system, but it is not known what kinds of informa-
tion must be conveyed nor how best to present that
information. RTE-3 provided an opportunity to col-
lect a diverse sample of explanations to begin to ex-
plore these questions.
This paper analyzes the extended task results,
with the next section describing the three-way deci-
sion subtask and Section 3 the justification subtask.
Contradiction was rare in the RTE-3 test set, occur-
ring in only about 10% of the cases, and systems
found accurately detecting it difficult. While the
level of agreement among human annotators as to
63
the correct answer for an entailment pair was within
expected bounds, the test set was found to be too
small to reliably distinguish among systems? three-
way accuracy scores. Human judgments of the qual-
ity of a justification varied widely, signaling the need
for a better characterization of the justification task.
Comments from the judges did include some com-
mon themes. Judges prized conciseness, though they
were uncomfortable with mathematical notation un-
less they had a mathematical background. Judges
strongly disliked being shown system internals such
as scores reported by various components.
2 The Three-way Decision Task
The extended task used the RTE-3 main task test set
of entailment pairs as its test set. This test set con-
tains 800 text and hypothesis pairs, roughly evenly
split between pairs for which the text entails the hy-
pothesis (410 pairs) and pairs for which it does not
(390 pairs), as defined by the reference answer key
released by RTE organizers.
RTE uses an ?ordinary understanding? principle
for deciding entailment. The hypothesis is consid-
ered entailed by the text if a human reading the text
would most likely conclude that the hypothesis were
true, even if there could exist unusual circumstances
that would invalidate the hypothesis. It is explicitly
acknowledged that ordinary understanding depends
on a common human understanding of language as
well as common background knowledge. The ex-
tended task also used the ordinary understanding
principle for deciding contradictions. The hypoth-
esis and text were deemed to contradict if a human
would most likely conclude that the text and hypoth-
esis could not both be true.
The answer key for the three-way decision task
was developed at the National Institute of Standards
and Technology (NIST) using annotators who had
experience as TREC and DUC assessors. NIST as-
sessors annotated all 800 entailment pairs in the test
set, with each pair independently annotated by two
different assessors. The three-way answer key was
formed by keeping exactly the same set of YES an-
swers as in the two-way key (regardless of the NIST
annotations) and having NIST staff adjudicate as-
sessor differences on the remainder. This resulted
in a three-way answer key containing 410 (51%)
Reference Systems? Responses
Answer YES UNKN NO Totals
YES 2449 2172 299 4920
UNKN 929 2345 542 3816
NO 348 415 101 864
Totals 3726 4932 942 9600
Table 1: Contingency table of responses over all 800 en-
tailment pairs and all 12 runs.
YES answers, 319 (40%) UNKNOWN answers, and
72 (9%) NO answers.
2.1 System results
Eight different organizations participated in the
three-way decision subtask submitting a total of 12
runs. A run consists of exactly one response of YES,
NO, or UNKNOWN for each of the 800 test pairs.
Runs were evaluated using accuracy, the percentage
of system responses that match the reference answer.
Figure 1 shows both the overall accuracy of each
of the runs (numbers running along the top of the
graph) and the accuracy as conditioned on the ref-
erence answer (bars). The conditioned accuracy for
YES answers, for example, is accuracy computed us-
ing just those test pairs for which YES is the ref-
erence answer. The runs are sorted by decreasing
overall accuracy.
Systems were much more accurate in recognizing
entailment than contradiction (black bars are greater
than white bars). Since conditioned accuracy does
not penalize for overgeneration of a response, the
conditioned accuracy for UNKNOWN is excellent for
those systems that used UNKNOWN as their default
response. Run H never concluded that a pair was a
contradiction, for example.
Table 1 gives another view of the relative diffi-
culty of detecting contradiction. The table is a con-
tingency table of the systems? responses versus the
reference answer summed over all test pairs and all
runs. A reference answer is represented as a row in
the table and a system?s response as a column. Since
there are 800 pairs in the test set and 12 runs, there
is a total of 9600 responses.
As a group the systems returned NO as a response
942 times, approximately 10% of the time. While
10% is a close match to the 9% of the test set for
which NO is the reference answer, the systems de-
tected contradictions for the wrong pairs: the table?s
64
A B C D E F G H I J K L
0.0
0.2
0.4
0.6
0.8
1.0
C
on
di
tio
ne
d 
A
cc
ur
ac
y
YES
UNKNOWN
NO
0.731 0.713 0.591 0.569 0.494 0.471 0.454 0.451 0.436 0.425 0.419 0.365
Figure 1: Overall accuracy (top number) and accuracy conditioned by reference answer for three-way runs.
diagonal entry for NO is the smallest entry in both its
row and its column. The smallest row entry means
that systems were more likely to respond that the hy-
pothesis was entailed than that it contradicted when
it in fact contradicted. The smallest column entry
means than when the systems did respond that the
hypothesis contradicted, it was more often the case
that the hypothesis was actually entailed than that it
contradicted. The 101 correct NO responses repre-
sent 12% of the 864 possible correct NO responses.
In contrast, the systems responded correctly for 50%
(2449/4920) of the cases when YES was the refer-
ence answer and for 61% (2345/3816) of the cases
when UNKNOWN was the reference answer.
2.2 Human agreement
Textual entailment is evaluated assuming that there
is a single correct answer for each test pair. This is a
simplifying assumption used to make the evaluation
tractable, but as with most NLP phenomena it is not
actually true. It is quite possible for two humans to
have legitimate differences of opinions (i.e., to dif-
fer when neither is mistaken) about whether a hy-
pothesis is entailed or contradicts, especially given
annotations are based on ordinary understanding.
Since systems are given credit only when they re-
spond with the reference answer, differences in an-
notators? opinions can clearly affect systems? accu-
racy scores. The RTE main task addressed this issue
by including a candidate entailment pair in the test
set only if multiple annotators agreed on its dispo-
sition (Giampiccolo et al, 2007). The test set alo
Main Task NIST Judge 1
YES UNKN NO
YES 378 27 5
NO 48 242 100
conflated agreement = .90
Main Task NIST Judge 2
YES UNKN NO
YES 383 23 4
NO 46 267 77
conflated agreement = .91
Table 2: Agreement between NIST judges (columns) and
main task reference answers (rows).
contains 800 pairs so an individual test case con-
tributes only 1/800 = 0.00125 to the overall accu-
racy score. To allow the results from the two- and
three-way decision tasks to be comparable (and to
leverage the cost of creating the main task test set),
the extended task used the same test set as the main
task and used simple accuracy as the evaluation mea-
sure. The expectation was that this would be as ef-
fective an evaluation design for the three-way task as
it is for the two-way task. Unfortunately, subsequent
analysis demonstrates that this is not so.
Recall that NIST judges annotated all 800 entail-
ment pairs in the test set, with each pair indepen-
dently annotated twice. For each entailment pair,
one of the NIST judges was arbitrarily assigned as
the first judge for that pair and the other as the sec-
ond judge. The agreement between NIST and RTE
annotators is shown in Table 2. The top half of
65
the table shows the agreement between the two-way
answer key and the annotations of the set of first
judges; the bottom half is the same except using the
annotations of the set of second judges. The NIST
judges? answers are given in the columns and the
two-way reference answers in the rows. Each cell in
the table gives the raw count before adjudication of
the number of test cases that were assigned that com-
bination of annotations. Agreement is then com-
puted as the percentage of matches when a NIST
judge?s NO or UNKNOWN annotation matched a NO
two-way reference answer. Agreement is essentially
identical for both sets of judges at 0.90 and 0.91 re-
spectively.
Because the agreement numbers reflect the raw
counts before adjudication, at least some of the dif-
ferences may be attributable to annotator errors that
were corrected during adjudication. But there do ex-
ist legitimate differences of opinion, even for the ex-
treme cases of entails versus contradicts. Typical
disagreements involve granularity of place names
and amount of background knowledge assumed.
Example disagreements concerned whether Holly-
wood was equivalent to Los Angeles, whether East
Jerusalem was equivalent to Jerusalem, and whether
members of the same political party who were at
odds with one another were ?opponents?.
RTE organizers reported an agreement rate of
about 88% among their annotators for the two-way
task (Giampiccolo et al, 2007). The 90% agree-
ment rate between the NIST judges and the two-
way answer key probably reflects a somewhat larger
amount of disagreement since the test set aleady
had RTE annotators? disagreements removed. But
it is similar enough to support the claim that the
NIST annotators agree with other annotators as of-
ten as can be expected. Table 3 shows the three-
way agreement between the two NIST annotators.
As above, the table gives the raw counts before ad-
judication and agreement is computed as percentage
of matching annotations. Three-way agreement is
0.83?smaller than two-way agreement simply be-
cause there are more ways to disagree.
Just as annotator agreement declines as the set
of possible answers grows, the inherent stability of
the accuracy measure also declines: accuracy and
agreement are both defined as the percentage of ex-
act matches on answers. The increased uncertainty
YES UNKN NO
YES 381
UNKN 82 217
NO 11 43 66
three-way agreement = .83
Table 3: Agreement between NIST judges.
when moving from two-way to three-way decisions
significantly reduces the power of the evaluation.
With the given level of annotator agreement and 800
pairs in the test set, in theory accuracy scores could
change by as much as 136 (the number of test cases
for which annotators disagreed) ?0.00125 = .17 by
using a different choice of annotator. The maximum
difference in accuracy scores actually observed in
the submitted runs was 0.063.
Previous analyses of other evaluation tasks such
as document retrieval and question answering
demonstrated that system rankings are stable de-
spite differences of opinion in the underlying anno-
tations (Voorhees, 2000; Voorhees and Tice, 2000).
The differences in accuracy observed for the three-
way task are large enough to affect system rank-
ings, however. Compared to the system ranking of
ABCDEFGHIJKL induced by the official three-way
answer key, the ranking induced by the first set of
judges? raw annotations is BADCFEGKHLIJ. The
ranking induced by the second set of judges? raw an-
notations is much more similar to the official results,
ABCDEFGHKIJL.
How then to proceed? Since the three-way de-
cision task was motivated by the belief that distin-
guishing contradiction from simple non-entailment
is important, reverting back to a binary decision task
is not an attractive option. Increasing the size of the
test set beyond 800 test cases will result in a more
stable evaluation, though it is not known how big the
test set needs to be. Defining new annotation rules
in hopes of increasing annotator agreement is a satis-
factory option only if those rules capture a character-
istic of entailment that systems should actually em-
body. Reasonable people do disagree about entail-
ment and it is unwise to enforce some arbitrary defi-
nition in the name of consistency. Using UNKNOWN
as the reference answer for all entailment pairs on
which annotators disagree may be a reasonable strat-
egy: the disagreement itself is strong evidence that
66
neither of the other options holds. Creating balanced
test sets using this rule could be difficult, however.
Following this rule, the RTE-3 test set would have
360 (45%) YES answers, 64 (8%) NO answers, and
376 (47%) UNKNOWN answers, and would induce
the ranking ABCDEHIJGKFL. (Runs such as H, I,
and J that return UNKNOWN as a default response
are rewarded using this annotation rule.)
3 Justifications
The second part of the extended task was for systems
to provide explanations of how they reached their
conclusions. The specification of a justification for
the purposes of the task was deliberately vague?
a collection of ASCII strings with no minimum or
maximum size?so as to not preclude good ideas by
arbitrary rules. A justification run contained all of
the information from a three-way decision run plus
the rationale explaining the response for each of the
800 test pairs in the RTE-3 test set. Six of the runs
shown in Figure 1 (A, B, C, D, F, and H) are jus-
tification runs. Run A is a manual justification run,
meaning there was some human tweaking of the jus-
tifications (but not the entailment decisions).
After the runs were submitted, NIST selected a
subset of 100 test pairs to be used in the justification
evaluation. The pairs were selected by NIST staff
after looking at the justifications so as to maximize
the informativeness of the evaluation set. All runs
were evaluated on the same set of 100 pairs.
Figure 2 shows the justification produced by each
run for pair 75 (runs D and F were submitted by
the same organization and contained identical jus-
tifications for many pairs including pair 75). The
text of pair 75 is Muybridge had earlier developed
an invention he called the Zoopraxiscope., and the
hypothesis is The Zoopraxiscope was invented by
Muybridge. The hypothesis is entailed by the text,
and each of the systems correctly replied that it is
entailed. Explanations for why the hypothesis is en-
tailed differ widely, however, with some rationales
of dubious validity.
Each of the six different NIST judges rated all 100
justifications. For a given justification, a judge first
assigned an integer score between 1?5 on how un-
derstandable the justification was (with 1 as unintel-
ligible and 5 as completely understandable). If the
understandability score assigned was 3 or greater,
the judge then assigned a correctness score, also an
integer between 1?5 with 5 the high score. This sec-
ond score was interpreted as how compelling the ar-
gument contained in the justification was rather than
simple correctness because justifications could be
strictly correct but immaterial.
3.1 System results
The motivation for the justification subtask was to
gather data on how systems might best explain them-
selves to eventual end users. Given this goal and the
exploratory nature of the exercise, judges were given
minimal guidance on how to assign scores other than
that it should be from a user?s, not a system devel-
oper?s, point of view. Judges used a system that dis-
played the text, hypothesis, and reference answer,
and then displayed each submission?s justification in
turn. The order in which the runs? justifications were
displayed was randomly selected for each pair; for a
given pair, each judge saw the same order.
Figure 2 includes the scores assigned to each of
the justifications of entailment pair 75. Each pair
of numbers in brackets is a score pair assigned by
one judge. The first number in the pair is the un-
derstandability score and the second the correctness
score. The correctness score is omitted (???) when
the understandability score is 1 or 2 because no cor-
rectness score was assigned in that case. The scores
from the different judges are given in the same order
for each justification.
With 100 entailment pairs evaluated by each of
6 judges assigning 2 separate scores, each run had
a total of 1200 numbers assigned to it. Figure 3
shows two views of these numbers: a histogram of
the number of justifications in the run that were as-
signed a given score value summed over all judges
and all test pairs, and the overall mean score for the
run1. A correctness score that was not assigned be-
cause understandability was too poor is displayed as
a score of 0 in the histogram and treated as a 0 in the
computation of the mean. Understandability scores
are shown toward the left in the figure and correct-
ness scores toward the right.
The mean scores for correctness are fairly low
1Using the mean treats a score as an interval variable rather
than an ordinal variable, a questionable treatment given a 5-
point scale. A mean is a convenient summary, however.
67
Run A:
There is a relationship between Zoopraxiscope and Muybridge in both the text and
hypothesis. The term "invention" is morphologically similar to "invented".
Scores: [4,4] [4,4] [5,4] [4,1] [5,4], [3,2]
Run B:
The text mentions ?Muybridge?. We can infer that Muybridge is inventor. From
the fact that Muybridge is inventor, we can infer that Muybridge invented. We can
conclude that the Zoopraxiscope was invented by Muybridge.
Scores:[4,3] [3,3] [5,4] [5,1] [5,3] [3,2]
Run C:
1: The Hypothesis could be precisely matched with content in the Text, with
allowance for polarity and embedded contexts.
2: Hypothesis words match well with words in the Text.
3: text adjunct "called" of "invention" dropped on aligned hyp word "invented"
Scores: [3,3] [4,4] [4,4] [2,?] [1,?] [2,?]
Runs D and F:
The words in the hypothesis are all found, with the approximately all the exact same
syntactic dependencies, also in the text. Therefore, I concluded that the given
hypothesis:
The Zoopraxiscope was invented by Muybridge.
is entailed by the given text
Muybridge had earlier developed an invention he called the Zoopraxiscope.
Scores: [3,3] [4,3] [4,3] [5,1] [4,3] [2,?]
Run H:
Yes!
I have general knowledge that:
IF Y is developed by X THEN Y is manufactured by X
Here: X = Muybridge, Y = the invention
Thus, here:
We are told in T: the invention is developed by Muybridge
Thus it follows that: the invention is manufactured by Muybridge
In addition, I know:
"manufacture" and "invent" mean roughly the same thing
Hence: The Zoopraxiscope was invented by Muybridge.
Scores: [2,?] [4,1] [3,3] [3,1] [2,?] [1,?]
Figure 2: Justification for entailment pair 75 from each justification run. Brackets contain the pair of scores assigned
to the justification by one of the six human judges; the first number in the pair is the understandability score and the
second is the correctness score.
for all runs. Recall, however, that the ?correctness?
score was actually interpreted as compellingness.
There were many justifications that were strictly cor-
rect but not very informative, and they received low
correctness scores. For example, the low correctness
scores for the justification from run A in Figure 2
were given because those judges did not feel that
the fact that ?invention and inventor are morpholog-
ically similar? was enough of an explanation. Mean
correctness scores were also affected by understand-
ability. Since an unassigned correctness score was
treated as a zero when computing the mean, systems
with low understandability scores must have lower
correctness scores. Nonetheless, it is also true that
systems reached the correct entailment decision by
faulty reasoning uncomfortably often, as illustrated
by the justification from run H in Figure 2.
68
0100
200
300
400 Run A* [4.27 2.75]
0
1
1
2
2
3 3
4
4
5
5
Understandability Correctness
0
100
200
300
400 Run B [4.11 2.00]
0
1
1
2
2
3
3
4
4
5
5
Understandability Correctness
0
100
200
300
400 Run C [2.66 1.23]
0
1
1
2
2
3
3
4
4
5
5
Understandability Correctness
0
100
200
300
400 Run D [3.15 1.54]
0
1
12
2
3
3
4
4
5
5
Understandability Correctness
0
100
200
300
400 Run F [3.11 1.47]
0
1
1
2
2
3
3
4
4
5
5
Understandability Correctness
0
100
200
300
400 Run H [4.09 1.49]
0
1
1
2
23
34
4
5
5
Understandability Correctness
Figure 3: Number of justifications in a run that were assigned a particular score value summed over all judges and all
test pairs. Brackets contain the overall mean understandability and correctness scores for the run. The starred run (A)
is the manual run.
3.2 Human agreement
The most striking feature of the system results in
Figure 3 is the variance in the scores. Not explicit
in that figure, though illustrated in the example in
Figure 2, is that different judges often gave widely
different scores to the same justification. One sys-
tematic difference was immediately detected. The
NIST judges have varying backgrounds with respect
to mathematical training. Those with more train-
ing were more comfortable with, and often pre-
ferred, justifications expressed in mathematical no-
tation; those with little training strongly disliked any
mathematical notation in an explanation. This pref-
erence affected both the understandability and the
correctness scores. Despite being asked to assign
two separate scores, judges found it difficult to sep-
arate understandability and correctness. As a result,
correctness scores were affected by presentation.
The scores assigned by different judges were suf-
ficiently different to affect how runs compared to
one another. This effect was quantified in the follow-
ing way. For each entailment pair in the test set, the
set of six runs was ranked by the scores assigned by
one assessor, with rank one assigned to the best run
and rank six the worst run. If several systems had the
same score, they were each assigned the mean rank
for the tied set. (For example, if two systems had the
same score that would rank them second and third,
they were each assigned rank 2.5.) A run was then
assigned its mean rank over the 100 justifications.
Figure 4 shows how the mean rank of the runs varies
by assessor. The x-axis in the figure shows the judge
assigning the score and the y-axis the mean rank (re-
member that rank one is best). A run is plotted us-
ing its letter name consistent with previous figures,
and lines connect the same system across different
judges. Lines intersect demonstrating that different
judges prefer different justifications.
After rating the 100 justifications, judges were
asked to write a short summary of their impression
of the task and what they looked for in a justification.
These summaries did have some common themes.
Judges prized conciseness and specificity, and ex-
pected (or at least hoped for) explanations in fluent
English. Judges found ?chatty? templates such as
the one used in run H more annoying than engaging.
Verbatim repetition of the text and hypothesis within
69
Judge1 Judge2 Judge3 Judge4 Judge5 Judge6
1
2
3
4
5
M
ea
n 
Ra
nk
  
Understandabilty
B
B
B
B
B
BA A A
A
A A
C
C
C
C C C
D
D D
D
D
D
F
F
F
F
F
F
H
H
H
H
H H
Judge1 Judge2 Judge3 Judge4 Judge5 Judge6
1
2
3
4
5
M
ea
n 
Ra
nk
  
Correctness
B
B
B B
B
BA
A
A
A
A
A
C C C
C C
C
D
D
D
D D
D
F
F
F
F F
F
H
H
H
H
H
H
Figure 4: Relative effectiveness of runs as measured by mean rank.
the justification (as in runs D and F) was criticized
as redundant. Generic phrases such as ?there is a re-
lation between? and ?there is a match? were worse
than useless: judges assigned no expository value to
such assertions and penalized them as clutter.
Judges were also adverse to the use of system in-
ternals and jargon in the explanations. Some sys-
tems reported scores computed from WordNet (Fell-
baum, 1998) or DIRT (Lin and Pantel, 2001). Such
reports were penalized since the judges did not care
what WordNet or DIRT are, and if they had cared,
had no way to calibrate such a score. Similarly, lin-
guistic jargon such as ?polarity? and ?adjunct? and
?hyponym? had little meaning for the judges.
Such qualitative feedback from the judges pro-
vides useful guidance to system builders on ways to
explain system behavior. A broader conclusion from
the justifications subtask is that it is premature for a
quantitative evaluation of system-constructed expla-
nations. The community needs a better understand-
ing of the overall goal of justifications to develop
a workable evaluation task. The relationships cap-
tured by many RTE entailment pairs are so obvious
to humans (e.g., an inventor creates, a niece is a rel-
ative) that it is very unlikely end users would want
explanations that include this level of detail. Having
a true user task as a target would also provide needed
direction as to the characteristics of those users, and
thus allow judges to be more effective surrogates.
4 Conclusion
The RTE-3 extended task provided an opportunity
to examine systems? abilities to detect contradic-
tion and to provide explanations of their reasoning
when making entailment decisions. True contradic-
tion was rare in the test set, accounting for approx-
imately 10% of the test cases, though it is not pos-
sible to say whether this is a representative fraction
for the text sources from which the test was drawn
or simply a chance occurrence. Systems found de-
tecting contradiction difficult, both missing it when
it was present and finding it when it was not. Levels
of human (dis)agreement regarding entailment and
contradiction are such that test sets for a three-way
decision task need to be substantially larger than for
binary decisions for the evaluation to be both reli-
able and sensitive.
The justification task as implemented in RTE-3
is too abstract to make an effective evaluation task.
Textual entailment decisions are at such a basic level
of understanding for humans that human users don?t
want explanations at this level of detail. User back-
grounds have a profound effect on what presentation
styles are acceptable in an explanation. The justifi-
cation task needs to be more firmly situated in the
context of a real user task so the requirements of the
user task can inform the evaluation task.
Acknowledgements
The extended task of RTE-3 was supported by the
Disruptive Technology Office (DTO) AQUAINT
program. Thanks to fellow coordinators of the task,
Chris Manning and Dan Moldovan, and to the par-
ticipants for making the task possible.
70
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190. Springer-Verlag.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9. Association for Computational
Linguistics.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of inference rules from text. In Proceedings of the
ACM Conference on Knowledge Discovery and Data
Mining (KDD-01), pages 323?328.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceedings
of the Twenty-Third Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 200?207, July.
Ellen M. Voorhees. 2000. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
Information Processing and Management, 36:697?
716.
71
