Effects of Adjective Orientation and Gradability 
on Sentence Subjectivity 
Vas i le ios  Hatz ivass i log lou  
Depar tment  o1' Computer  Sc ience  
Co lumbia  Un ivers i l y  
New York,  NY  10027 
vh@cs ,  co lumbia ,  edu  
Janyce  M.  Wiebe  
Depar tment  o f  Computer  Sc ience  
New Mex ico  State Un ivers i ty  
Las  Cruces ,  NM 88003 
w iebe@cs ,  nmsu.  edu  
Abstract 
Subjectivity is a pragmatic, sentence-level feature that 
has important implications for texl processing applica- 
lions such as information exlractiou and information ic- 
lricwd. We study tile elfeels of dymunic adjectives, se- 
mantically oriented adjectives, and gradable ad.ieclivcs 
on a simple subjectivity classiiicr, and establish lhat 
lhcy arc strong predictors of subjectivity. A novel train- 
able mclhod thai statistically combines two indicators of 
gradability is presented and ewlhlalcd, complementing 
exisling automatic Icchniques for assigning orientation 
labels. 
1 I n t roduct ion  
In recent years, computalional tcchniqt,es for the deter- 
mination of &:deal semantic features have been proposed 
and ewdualed. Such features include sense, register, do- 
main spccilicity, pragmatic restrictions on usage, scnlan- 
lic markcdncss, and orientation, as well as automatically 
ictcnlifiecl links between words (e.g., semantic rclalcd- 
hess, syllollynly, antonylny, and tneronymy). Aulomal- 
ically learning features of this type from hugc corpora 
allows the construction or augmentation of lexicons, and 
the assignment of scmanlic htbcls lo words and phrases 
in running text. This information in turn can bc used to 
help dcterlninc addilional features at the It?teal, clause, 
sentence, or document level. 
Tiffs paper explores lira benelits that some lexical fea- 
tures of adjectives offer l'or the prediction of a contexlual 
sentence-level feature, suOjectivity. Subjectivity in nat- 
ural language re\['crs to aspects of language used to ex- 
press opinions and ewfluations. The computatiomtl task 
addressed here is to distinguish sentences used to present 
opinions and other tbrms of subjectivity (suOjective sen- 
tences, e.g., "At several different layers, it's a fascinating 
title") from sentences used to objectively present factual 
information (objective sentences, e.g., "Bell industries 
Inc. increased its quarterly to 10 cents from 7 cents a 
share"). 
Much research in discourse processing has focused 
on task-oriented and insmmtional dialogs. The task ad- 
dressed here comes to the fore in other genres, especially 
news reporting and lnternet lorums, in which opinions 
of various agents are expressed and where subjectivity 
judgements couht help in recognizing inllammatory rues- 
sages ("llanles') and mining online sources for product 
reviews. ()thor (asks for whicll subjectivity recognition 
is potentially very useful include infornmtion extraction 
and information retrieval. Assigning sub.icctivity labels 
to documents or portions of documents is an example of 
non-topical characteri?ation f information. Current in- 
formation extraction and rolricval lechnology focuses al- 
most exclusively on lhe subject matter of the documcnls. 
Yet, additiomtl components of a document inllucncc its 
relevance to imrlicuhu ? users or tasks, including, for ex- 
alnple, the evidential slatus el: lhc material presented, and 
attitudes adopted in fawn" or against a lmrticular person, 
event, or posilion (e.g., articles on a presidenlial cam- 
paign wrillen to promote a specific candidate). In sum- 
marization, subjectivity judgmcnls could be included in 
documcllt proiilcs to augment aulomatically produced 
docunacnt summaries, and to hel l) the user make rele- 
vance judgments when using a search engine. 
()thor work on sub.iectivity (Wicbc et al, 1999; Bruce 
and Wicbc, 2000) has established a positive and statisti- 
cally signilicant correlation with the presence of adiec- 
lives. ?incc the mere presence of one or iDoi'c adjectives 
is useful for prcdicling (hat a scntcrtce is subjective we 
investigate ill this paper (lie cfl'ccts of additional cxical 
scmanlic lcalurcs of adjectives that can be automatically 
learned from corpora. We consider two such l%atures: se- 
mantic orientation, which represents an ewdualivc har- 
acterization of a word's deviation from the norm for its 
semantic group (e.g., beauti/'ul is positively oriented, as 
opposed to ugly); and gradability, which characterizes a 
word's ability to express a property in wlrying degrees. 
In lira remainder of this paper, we \[irst address adjec- 
tive orientation in Section 2, summarizing a previously 
published method for automatically separating oriented 
adjectives into positive and negative classes. Then, Sec- 
tion 3 presents a novel method for learning gradablc ad- 
jectives using a largo corpus and a statistical feature com- 
bination naodel. In Section 4, we review earlier exper- 
iments on testing subjectivity using wu'ious fcatt, res as 
predictors, and then present comparative analyses of the 
effects that orientation and gradability have on our abil- 
ity to In'edict sentence subjectivity from adjectives. Wc 
show that both give us higher-quality features for recog- 
nizing st@icctive sentences, and conclude by discussing 
future extensions to Ibis work. 
299 
Ct' 
Number of Number of Average nnmber Ratio o1' average 
adjectives in links in of links for Accuracy 
test set (IAc~l) test set (IL~I) each adjective group frequencies 
730 2,568 7.04 78.08% 1.8699 
516 2,159 8.37 82.56% 1.9235 
369 1,742 9.44 87.26% 1.3486 
236 1,238 10.49 92.37% 1.4040 
Table 1: Evaluation o1' the adjective orientation classification and labeling methods (from (Hatzivassiloglou and McK- 
eown, 1997)). 
2 Semantic Orientation 
The semantic orientation or polarity of a word indicates 
the direction the word deviates fl'om the norm for its se- 
mantic group or lexicalfield (Lehrer, 1974). It is an eval- 
uative characteristic (Battistella, 1990) of the meaning of 
the word which restricts its usage to appropriate prag- 
matic contexts. Words that encode a desirable state (e.g., 
beautiful, unbiased) have a positive orientation, while 
words that represent undesirable states have a negative 
orientation. Within tile particular syntactic lass o1' ad- 
jectives, orientation can be expressed as the ability of an 
adjective to ascribe in general a positive or negative qual- 
ity to the modified item, making it better or worse than a 
similar unmodilied item. 
Most antonymous adjectives can be contrasted on 
the basis of orientation (e.g., beautil)d-ugly); similarly, 
nearly synonymous terms are often distinguished by dill 
fcrent orientations (e.g., simple-siml)listic). While ori- 
entation applies to many adjectives, there are also those 
that have no orientation, typically as members of groups 
of complementary, qualitative terms (Lyons, 1977) (e.g., 
domestic, medical, or red). Since orientation is inher- 
ently connected with cwduative judgements, it appears 
to be a promising feature for predicting subjectivity. 
Hatzivassiloglou and McKeown (1997) presented a 
method for autonmtically assigning a + or - orientation 
label to adjectives known to have some semantic orien- 
tation. Their method is based on information extracted 
fi'om conjunctions between adjectives in a large corpus I  
because orientation constrains the use of the words in 
specific contexts (e.g., compare corrupt and brutal with 
*corrupt but brutal), observed conjunctions of adjectives 
can be exploited to inl'er whether the conjoined words 
are of the same or different orientation. Using a shallow 
parser on a 21 million word corpus of Wall Street Jour- 
nal articles, Hatzivassiloglou and McKeown developed 
and trained a log-linear statistical model that predicts 
whether any two adjectives have the same orientation 
with 82% accuracy. The predicted links o1' same or dil L 
ferent orientation are automatically assigned a strength 
value (essentially, a confidence stimate) by tile model, 
and induce a graph that can be partitioned with a clus- 
tering algorithm into components so that all words in the 
same component belong to the same orientation class. 
Once the classes have been determined, fl'equency infor- 
mation is used to assign positive or negative labels to 
each class (there are slightly fewer positive terms, but 
with a significantly higher rate of occurrence than nega- 
tive terms). 
Hatzivassiloglou and McKeown applied their method 
to 1,336 (657 positive and 679 negative) adjectives which 
were all the oriented adjectives appearing in the corpus 
20 times or more. Orientation labels were assigned to 
these adjectives by hand. I Subsequent validation of the 
initial selection and label assignment steps with indepen- 
dent human judges showed an agreement of 89% t'or tile 
first step and 97% for the second step, establishing that 
orientation is a fairly objective semantic property. Be- 
cause the accuracy ol' the method depends on the den- 
sity of conjunctions per adjective, Hatzivassiloglou and 
MeKeown tested separately their algorithm for adjectives 
appearing in at least 2, 3, 4, or 5 conjunctions in the co l  
pus; their results are shown in Table I. 
In this paper, we use the model labels assigned by 
hand by Hatzivassiloglou and McKeown, and tile labels 
automatically obtained by their method and reported in 
(Hatzivassiloglou and McKeown, 1997) with the follow- 
ing extension: An adjective that appears in k conjunc- 
tions will receive (possibly different) labels when ana- 
lyzed together with all adjectives appearing in at least 2, 
3 . . . . .  k conjunctions; since performance generally in- 
creases with the number of conjunctions per adjective, 
we select as the orientation label the one assigned by 
the experi,nent t,sing the highest applicable conjunctions 
threshold. Overall, we have labels for 730 adjectives 2,
with a prediction accuracy of 81.51%. 
3 Gradability 
Gradability (or grading) (Sapir, 1944; Lyons, 1977, p. 
27 I) is the semantic property that enables a word to par- 
ticipate in comparative constructs and to accept mod- 
ifying expressions that act as intensitiers or diminish- 
ers. Gradable adjectives express properties in varying 
degrees ot' strength, relative to a norm either explicitly 
ISome adjectives with unclem; mnbiguous, or conlexl,-dependenl 
orientation were excluded. 
2Those appearing in the corpus in two conjunctions or inore, since 
some conjunction data nlust be left out to h'ain the link prediction algo- 
rithm. 
300 
cold 
Unmodilied by 
grading words 
Moditied by 
grading words 
civil 
Unmodilied by 
grading words 
Modified by 
grading words 
Uninllected 392 20 1,296 1 
Inllected for degree 18 0 0 0 
'litble 2: Extracted wdues of gradability indicators, i.e., frequencies of the word with or without he specitied intlection 
or moditication, for two adjectives, one gradable (cold) and one primarily non-gradable (civil). The frequencies were 
compt, ted li'om the 1987 Wall Street Journal corpus. 
mentioned or implicitly supplied by the modilied noun 
(for example, asmall planet is usually much larger thart a 
large house; cf. the distinction between absolute and tel- 
alive adjectives made by Katz (1972, p. 254)). This rel- 
ativism in the interpetation of gradable words indicates 
that gradability is likely to be a good predictor ?71' subjec- 
tivity. 
3.1 Indicators ofgradability 
Most gradable words appear at least several times in a 
large corpt, s either in forms inflected for degree (i.e., 
comparative and superlative), or in tile context of grading 
modilicrs such as veo,. However, non-gradable words 
may also occasionally appear in such contexts or forms 
under exceptional circumstances. For example, ve O, 
dead can be used tk)r emphasis, and re&let am~ re&let 
(as in "her lhce became redder and redder") can be used 
to indicate a progression of coloring, qb distinguish be- 
tween truly gradablc adjectives and non-gradable adjec- 
tives in these exceptional contexts, we have developed 
a trainable log-linear statistical model that lakes into ac- 
count tile number of times an ad.iective has been observed 
in a form or context indicating gradability relative to the 
number of limes it has been seen in non-gradable con- 
texts. 
We use a shallow parser to retrieve from a large corpus 
tagged for part-of-speech with Church's PARTS tagger 
(Church, 1988) all adjectives and their modifiers. Al- 
though the most common use of an adverb modifying 
an adjective is to function as an intensilier or diminisher 
(Quirk et al, 1985, p. 445), adverbs can also add to tile 
semantic ontent of the adjectival phrase instead of pro- 
viding a grading effect (e.g., immediately available, po- 
litically vuhmrable), or function as cmphasizers, adding 
to the force o1' tile base adjective and not lo its degree 
(e.g., virtually impossible; compare *re O, impossible). 
Therefore, we compiled by hand a list of 73 adverbs and 
noun phrases (such as a little, exceedingly, somewhat, 
and veo') that are fi'equently used as grading moditicrs. 
The number of times each adjective appears mod ilied by 
a term form this list becomes a first indicator of gradabil- 
ity. 
To detect inflected forms o1' adjectives (which, in 15> 
glish, always indicate gradability st, bject to the excel> 
tions discussed earlier), we have implemented an auto- 
matic lnorphology analysis component. This program 
recognizes several irregular forms (e.g., good-better- 
best) and strips tile grading suffixes -er and -est Dora 
regularly inllected adjectives, producing a list of candi- 
date base forms that if inflected would yield tilt origi- 
nal adjective (e.g., bigger produces three potential forms, 
big, bigg, and bigge). The frequency of these candi- 
date base words is checked against ile corpus, and tile 
form with signilicantly higher frequency is selected. To 
guard against cases of base adjective forms that end in -er 
or-est (e.g., sih,er), the original word is also included 
alllong tile candidates. The total number of times this 
procedure is successfully applied for each adjective be- 
comes a second indicator of gradability. 
3.2 l )etermlnlng radabil l ty 
The presence or absence of each of the above two indica- 
tors results in a 2 x 2 frequency table IBr each adjective; 
examples for one gradable and one non-gradable adjec- 
tive are given in "lhble 2. "lb convert lhese four numbers 
to a single decision on tile gradability of tile ad.iective, we 
use a log-linear model. Ix)g-linear models (Nantnef and 
l)ufly, 1989) construct a linear combination (weighted 
sum) of the predictor wlriables 1~, 
i=1 
and relate it to the actual response H. (in this case, 0 for 
non-gmdable and 1 for gradable) via the so-called logis- 
tic trcm,sJbrmation, 
1~- 
I -t- e'J 
Maximum likelihood estimates for the coefficients fli 
are obtained from training samples for which the correct 
response H, is known, using the iterative reweighted non- 
linear least squares algorithm (Bates and Watts, 1988). 
This statistical model is particularly suited for model- 
ing variables with a "yes"-"no" (binary) value, because, 
unlike linear models, it captures the dependency of IFs 
variance on its mean (Santner and Dully, 1989). 
We normalize the counts for the two indicators of 
g,'adability, and the cot, at ot'joint occurrences of both in- 
tleetion and modilication by grading moditiers, by divid- 
ing with the total frequency of the adjective in the corpus. 
In this manner, we obtain three real-valued predictors 
301 
Classitied as gradable: 
acceptable accurate afraid aware busy careful 
cautious el~eap creative critical dangerous 
different disappointing equal fair fanfiliar far  
favorable formal free frequent good grand 
inadequate intense interesting legitimate likely 
positive professional reasonable rich 
short-term significant slow solid sophisticated 
sound speculative thin tight tough uucertain 
widespread worth 
Classilied as non-gradable: 
additional alleged alternative annual antitrust 
automatic ertain criminal cumulative daily 
deputy domestic ldcrly false linaneial 
first-quarter full hefty illegal institutional 
internal egislative long-distance military 
min imum monthly moral national official 
one-time other outstanding present prior 
prospective punitive regional scientific 
secondary sexual subsidiary taxable 
three-nmnth three-year total tremendous 
two-year unfifir unsolicited upper vohmtary 
white wholesale world-wide wrong 
Figure 1: Automatically obtained classification of a 
sample of 100 adjectives as gradable or not. Correct 
decisions (according to the COBU1LD-based reference 
model) are indicated in bold. 
14", i = 1 , . . . ,  3 for the log-linear model. We also con- 
sider a modilied model, where any adjective for which 
any occurrence of simultaneous inflection and modilica- 
tion has been detected is automatically labeled gradable; 
the remaining two predictors are used to classify the ad- 
jectives that do not fullill this condition. This modilica- 
tion is motivated by the fact that observing an adjective 
in such a context offers a very high likelihood o1' grad- 
ability. 
3.3 Experimental results 
We extracted from the 1987 Wall Street Journal corpus 
(21 million words) all adjectives with a frequency o1' 300 
or more; this produced a collection of 496 words. Grad- 
ability labels specifying whether each word is gradable 
or not were manually assigned, using tim designations 
of the Collins COBUILD (Collins Birmingham Univer- 
sity International Language Database) dictionary (Sin- 
clair, 1987). COBUILD marks each sense of each adjec- 
tive with one of the labels QUALIT, CLASSIF, or COLOR, 
corresponding to gradable, non-gradable, and color ad- 
jectives. In cases where COBUILD supplies conflicting 
labels for different senses of a word, we either omitted 
that word or, if a sense were predominant, gave it the 
label of that sense. In some cases, the word did not 
appear in COBUILD; these typically were descriptive 
compounds peci\[ic to the domain (e.g., anti-takeover, 
over-the-coullter) and were in most cases marked as non- 
gradable adjectives. Overall, 453 of tile 496 adjeclives 
(91.33%) were assigned gradability labels by hand, while 
the remaining 53 words were discarded because they 
were misclassitied as adjectives by the part-ol:speech 
tagger (e.g., such) or because they coukt not be assigned 
a unique gradability label in accordance with COBUILD. 
Out of these words, 235 (51.88%) were manually classi- 
lied as gradable adjectives, and 218 (48.12%) were clas- 
silied as non-gradablc adjectives. 
Following the methodology of the preceding subsec- 
tion, we recovered the inflection and modilication indica- 
tors for these 453 adjectives, and trained both the unmod- 
ified and modilied log-linear models rcpcatedly, using a 
randomly selected subset ol' 300 adjectives for training 
and 100 adjectives for testing. The entire cycle of se- 
lecting random test and training sets, fitting the model's 
coefficients, making predictions, and evaluating the pre- 
dicted gradability labels is repeated 100 times, to ensure 
that the ewtluation is not affected by a lucky (or unlucky) 
partition of the data between training and test sets. This 
procedure yields over the 453 adjectives gradability clas- 
sifications with an average precision o1' 93.55% and av- 
erage recall o1' 82.24% (in terms of the gradable words 
reported or recovered, respectively). The overall accu- 
racy of the predicted gradability labels is 87.97%. These 
results were obtained with the modified log-linear model, 
which slightly ot, tperformed the model that uses all three 
predictors (in that case, we obtained an average precision 
of 93.86%, average recall ol' 81.70%, and average over- 
all accuracy o1' 87.70%). Figure I lists the gradability 
labels that were automatically assigned to one of the 100 
random test sets ttsing the moditied prediction algorithm. 
We also assigned automatically labels to the entire set of 
453 adjectives, using 4-fold cross-validation (repeatedly 
training on three-fourths of tim 453 adjectives and test- 
ing on the rest). This resulted in precision of 94.15%, 
recall of 82.13%, and accuracy of 88.08% for the entire 
adjective set. 
4 Subjectivity 
The main motivation for the present paper is to examine 
the effect that information about an adjective's semantic 
orientation and gradability has on its probability of oc- 
curring in a subjective sentence (and hence on its quality 
as a subjectivity predictor). We tirst review related work 
on subjectivity recognition and then present our results. 
4.1 Previous work on subjectivity recognition 
In work by Wiebc, Bruce, and O'Hara (Wiebe ct al., 
1999; Bruce and Wicbe, 2000), a corpus of 1,001 sen- 
tences 3 of the Wall Street Journal TreeBank Corpus 
3Conlpoutld sentences were manually segmented into their con- 
juncts, and each conjtmct treated as a scparale sentence. 
302 
(Marcus et al, 1993) was nlanually annotated with sub- 
jeciivity chlssifications. Specifically, each sentence was 
assigned a subjective or objective classitication, accord- 
ing to concensus lags derived by a slalistical analysis of 
lhe chisses assigned by three human judges (see (Wiebe 
et al, 1999) for further infornmtion). The total nulnber 
of subjective sentences in lhe data is 486, and the total 
number of objeclive sentences i 515. 
Bruce and Wiebe (2000) performed a statistical anal- 
ysis of the assigned classitications, linding lhat ac(iec- 
tivcs are statistically signilicantly and positively corre- 
lated with subjective sentences in the corpus on the basis 
(, . The proba- of the log-likelihood ratio test statistic -,2 
bility of a sentence being subjective, simply given din! 
there is at least one adjective in lhe sentellee, is 56%, 
even though there are more objective than subjective sen- 
lences in the corpus. In addition, Bruce and Wicbe iden- 
tiffed a type of adjective that is indicative of subjective 
sentences: those Quirk et al (1985) term dynamic, which 
"denote qualities that a,'e thoughl to be subjecl to con- 
trol by the possessor" (p. 434). IZxamples are "kind" and 
"careful". Bruce and Wiebe nianually applied synlactic 
tests to identify dynamic adjectives in hall' of the corpus 
nlentioned above. We inclutle such adjectives in the anal- 
ysis below, to assess whether additional lexical seinantic 
features associated with subjectivity hel I ) improve pro- 
dictability. 
Wiebe el al. (1999) developed an automatic system to 
perform st, bjectivily lagging. In 10-fold cross valida- 
lion experiments applied to the corpus described above, 
a probabilislic lassilier oblaincd an average accuracy on 
subjectivity lagging of 72.17%, nlorc Ihan 20 perccnlage 
poinls higher than the baseline accuracy obtained by al- 
ways choosing tile nlore frcquent class. A binary feature 
is included for each of lhe lbllowing: lhe presence in lhe 
sentence of a pl'ollotln, an adjective, a cardinal number, 
a modal other fllan will, and an adverb other than #lot. 
They also inchlded a binary feature representing whether 
or not the sentence begins a new lxu'agraph, l:inally, a 
feature was included representing co-occurrence of word 
tokens and punciuation marks with tile sul~jective and ob- 
jective classilicfition. An analysis of the system showed 
that the adjective \['cature was imporlant to realizing the 
inlprovolncnts over lllO baseline accuracy, in this \])apci', 
we use lhe performance of the simple adjcclive fealtu'e as 
a baseline, and identify higher quality adjeclive features 
based on gradability and orienlalion. 
4.2 Or ientat ion and gradabi l i ty  as subjectivity 
predictors: Results 
We measure the precision of a simple prediction method 
for subjectivity: a sonlence is classilicd as subjcclivc il' at 
least one nlonlbor of a set of adjectives N occurs in 1he 
sontonco, alld objeclive otherwise. By wirying 1tlo sot 
(e.g., all adjeclives, only gradable adjectives, only nega- 
tively orienied adjectives, etc.) we call assess the t, seful- 
heSS of ihe additional knowledge for predicting subjec- 
livity. 
For the present study, we use tile set of all adjectives 
automatically identified in tile corpt, s by Wiebc et al 
(1999) (Section 4.1 ); the set of dynamic adjectives Ill,{Inu- 
ally identified by Bruce and Wiebe (2000) (Section 4.1); 
tile set of scnmntic orientation labels assigned by Hatzi- 
vassiloglou and McKeown (1997), both manually and 
automatically with our extension described in Section 2; 
and the set of gradability labels, both manually and att- 
tomatically assigned according to the revised log-linear 
model of Section 3. We calculate restllts (shown in 'hi- 
ble 3) for each of lhese sets of all adjectives, dynamic, 
oriented and gradable adjectives, as well as for unions 
and intersections of lhose sets. Nole fliat these four sets 
have been extracted l'rom comparable but different cor- 
pora (different years of the Wall Street Journal), therefore 
sometimes adjectives in one corpus may not be present 
in the other corpus, reducing the size of intersection sets. 
Also, for gradability, we worked with a sample set of 100 
adjectives rather than all possible adjectives we could 
automatically calcuhtte gladabiliiy vahles for, since our 
goal in the present work is to measure correlations be- 
tween these sets and sul~jeciivity, rather than building a 
system for predicling subjectivity for as many ac\[iectives 
as possible. 
In Table 3, the second cohmm identifies 8, the set 
of ac\[iective types in question. The third cohimn gives 
the number of subjective sentences that contain one or 
more instances of members of S, and the fourth colunul 
gives lhe same ligure for ol~jective sentences. Therefore 
these two cohinuls together specify lhe coverage of tlm 
subjectivity indicator examined. The lifth cohimn gives 
111c onditional probability that a sentence is subjective. 
givell that (tile of iilorc illstatices of ti/enlbcl+S of +5; ap- 
pears. This is a precishm inetrie that assesses feature 
quality: if inslances of <"7 appear, how likely is the son- 
tence to be subjective? The last two colunuls contrast the 
observed conditional probability with the a priori prob- 
ability of subjective sentellees (i.e., chalice; sixth col- 
ulnn) and with the probability assigned by the baseline 
all-adjectives model (i.e., the lirst row in the table; sev- 
enth colunm). 
The nlost striking aspect of these results is lhat all sets 
involviug dynamic adiectives positive or negative po- 
larity, or gradability are better predictors of sul~jective 
sentenccs than the class of adjectives as a whole, lqve 
of the sets are at least 25 points better (LI4, LI6, L21, 
L23, and L24); four others are at least 20 points better 
(L2, L9, L13, and 1,15); and live others are at least 15 
points better (L4, LI I, 1,18, L20, and 1,22). In most of 
these cases, the difference between these predictors and 
all adjectives i  statistically signiticant 4 fit the 5% level or 
less; ahnost all of these predictors offer statistically sig- 
nificantly better than even odds in predicting subjectivity 
correctly. In nlany cases where statistical signilicance 
'Iwe applied achi-square l st Oll the 2 x 2 cross-classificalion able 
(Fleiss, 1981). 
303 
LI. 
L2. 
L3. 
L4. 
L5. 
L6. 
L7. 
L8. 
L9. 
L10. 
L l l .  
Ll2. 
LI3. 
LI4. 
LI5. 
L16. 
LI7. 
LI8. 
LI9. 
L20. 
L21. 
L22. 
L23. 
L24. 
Adjeclive Set S 
# Subj Sents 
with (s G ,5') + 
Dyn Adjs fq S of L5. 
# Obj Sents l'(Subj Sent I Significance 
with (s G ,5') + (~ e S) +) Against maiority Against all adjs 
All Adjectives 403 321 0.56 0.0041 N/A 
Dynamic Adjectives 92 32 0.74 1.1989 ? 1.0 - r  1..6369 - 10 -4 
Pol+, man 138 87 0.61 0.0007 0.1546 
Pol- ,  man 79 37 0.67 0.0001 0.0158 
Pol+ U Pol- ,  man 197 114 0.63 6.91.91 ? 10 -~ 0.0260 
Grad, man 193 115 0.63 1.9633 ? 10 -~ 0.0440 
Not Grad, man 172 147 0.54 0.1084 0.6496 
t'o1+, auto 121 79 0.60 0.0026 0.2537 
Pol- ,  auto 61 21 0.74 1.1635 ? 10 -~ 0.0017 
PoI+ U I'o1--, auto 170 95 0.64 8.5888 - 10 -~ 0.0202 
Grad, auto 30 14 0.68 0.0166 0.1418 
Not Grad, auto 63 51 0.55 0.2079 0.9363 
51 19 0.73 0.0001 0.0081 
8.0397.10 -~ Dyn Adjs 71 S of L6. 39 8 0.83 
l)yn Adjs 71 S of L I0. 50 19 0.72 0.0002 0.0103 
Dyn Adjs 71 S ofLl  I 7 2 0.78 0.1582 0.3220 
Grad 71 Pol+, man 90 58 0.61 0.0070 0.2891 
Grad 71 Pol-,  man 35 I6 0.69 0.0080 0.09711 
Grad 71 (Pol+ U Pol-), man 119 71 0.63 0.0005 0.1000 
Grad fl Pol+, auto 13 6 0.68 0.1376 0.3833 
Grad n Pol-,  auto 2 0 1.00 0.4556 0.5838 
Grad 71 (Pol+ U Pol-), auto 15 6 0.71 0.0636 0.2255 
l)yn Adjs N S o1' L22. 4 0 1.00 0.1203 0.2019 
I)yn Adjs ('1 ,_"; of L19. 24 5 0.83 0.0006 0.0070 
Key: (s G ,5')+: one or more instances of members ofS. Ib/+: positive polarity, l b l - :  negalive polarity. 
GtzM: gradable. Dyn: dynamic. Matt: manually identilied. Auto: automalically identified. 
Table 3: Subjectivity prediction results. 
4.3671.. 10 -4 
could not established this is due to small counts, caused 
by the small size of the set of adjectives automatically 
labeled for gradability. 
It is also important to note that, in most cases, 
tile automatically-classified adjectives are comparable 
or better predictors of subjective sentences than the 
manually-assigned ones. Comparing tile automatically 
generated classes with the manually identilied ones, the 
positive polarity set decreases by 1 percentage point (L3 
and L8), while the negative polarity set increases by 7 
points (L4 and L9), and the gradable sot increases by 5 
percentage points (L6 and LI 1). Among the intersection 
sets, in two cases the results are lower for tile computer- 
generated sets (Ll 3/LI 5 and L 14/L 16), but in tile other 4 
eases, the results are higher (LI 7/L20, L 18/L21, L19/L2, 
L24/L23). 
Finally, the table shows that, in most cases, pro- 
dictability improves or at worst remains essentially tile 
same as additional lexical features are considered. For 
tile set of dynamic adjectives, the predictability is 74% 
(L2), and improves in 4 of the 6 cases in which it is in- 
tersected with other sets (LI4, L l6, L23, and L24). For 
the other two (L 13 and LI 5), predictability is only 1 or 2 
points lower (not statistically significant). For the man- 
ually assigned polarity and gradability sets, in one case 
predictability is lower (L17 < L6), but in the other cases 
it remains the same or improves. The results are even 
better for the automatically assigned polarity and grad- 
ability sets: predictability improves when both features 
are considered in all but one case, when predictability 
remains the same (L20 > L8; L21 > L9; L22 > LI0; 
and LI 1 _< L20, L21, and L22). 
5 Conclusion and Future Work 
This paper presents an analysis of different adjective fea- 
tures for predicting subjectivity, showing that tlmy are 
more precise than those previously used for this task. Wc 
establish that lexical semantic features uch as seman- 
tic orientation and gradability determine in large part the 
subjectivity status of sentences in which they appear. We 
also present an automatic meflmd for extracting radabil- 
ity values reliably, complementing earlier work on se- 
mantic orientation and dynamic adjectives. 
In addition to finding more precise features for auto- 
marie subjectivity recognition, this kind of analysis could 
help efforts to encode subjective features in ontologies 
such as those described in (Knight and Luk, 1994; Ma- 
hesh and Nirenburg, 1995; Hovy, 1998). These on- 
tologies are useful for many NLP tasks, such as ma- 
chine translation, word-sense disambiguation, and gen- 
eration. Some subjective features are included in exist- 
ing ontologies (for example, Mikrokosmos (Mahesh and 
304 
Nirenburg, 1995) includes atlitude slots). Our corpus- 
based methods could help in idenlifying more or exlend- 
ing their coverage. 
To be able to use automatic subjectivily recognition 
in texl-processing applications, good ch,cs o1' sub.iccliv- 
ity mttst be found. The features developed in lhis paper 
are not only good clues of subjectivity, lhey can be Men- 
tilied automatically from corpora (see (Hatzivassiloglou 
and McKeown, 1997), and Section 3 in the present pa- 
per). In fact, the results in "Iable 3 show that the pre- 
dictability of the automatically determined gradability 
and polarity sets is better than or at least comparable to
the predictability of the manually determined sets. Thus, 
tile oriented and gradable adjectives in the particular ap- 
plication genre can be idenlified fo," use in subjectivity 
recognition. 
Ou, efforts in this paper are largely exploratory, aim- 
ing to establish correlations among tim wlrious features 
examined. In related work, we have begun to incorporale 
the features developed herc into systems for recognizing 
flames and mining reviews in lnternel forums, extend- 
ing subjectivity judgments froth the sentence to the doc- 
ument level. In addition, we are seeking ways lo extend 
the orientation and gradability methods o that individual 
word occurrences, rather than word lypes, are character- 
ized as oriented or gradable. We also pla n l{7 incorpo- 
rate the new features presented here in machine learning 
models for tile prediction of subjectivity (e.g., (Wiebe ct 
al., 1999)) and lest lheir interaclions wilh olhcr proposed 
features. 
Acknowledglnents 
This research was SUl~ported in part by the National Sci- 
ence Foundation under grant number IIS-9817434, and 
by |he Of lice of Nawtl Research under grant number 
N00014-95-1-0776. Any opinions, tindings, or recom- 
mendations a,e those of tile authors, and do not neces- 
sarily rellect the views of the above agencies. 
References 
I)ouglas M. Bates and 1)onald G. Watts. 1988. NoMi~> 
ear Regression Analysis and its Applicatiolls. Wiley, 
New York. 
Edwin L. Battistella. 1990. Markedness: 7he Evahiative 
Siq~etwtructure qf'Language. State University of New 
York Press, Albany, New York. 
Rebecca Bruce and ,lanyce Wiebe. 2000. Recognizing 
subjectivity: A case study of rllanual tagging. Natural 
Language E, gineering, 6(2). 
Kenneth W. Church. 1988. A stochastic paris p,'ogranl 
and noun phrase parser for unrestricted text. In Pro- 
ceedings of the Second Co,ference o, Applied Natu- 
ral Language Processing (ANLP-88), pages 136-143, 
Austin, Texas, February. Association for Computa- 
tional Linguistics. 
Joseph 1~. Fleiss. 1981. Statistical Methods for Rates 
and l'mportions. Wiley, New York, 2rid edition. 
Vasileios Hatzivassiloglou and Kathlcen R. McKeown. 
1997. Predicting tile semantic orientation of adjec- 
tives. In Pmeeedi,gs o\[' the 35th Annual Meeting 
q/' the ACL and the 8th Col!/'erence o/' rite Europeall 
Ch(q)ter of the ACL, pages 174-181, Madrid, Spain, 
July. Association re," Computational Linguistics. 
Eduard Hovy. 1998. Combining and slandardizing 
large-scale practical ontologies for machine lransla- 
lion and other uses. In Proceedings of the 1st Interna- 
tional Conference on Language Resources and Evaht- 
alien (LREC), Granada, Spain. 
Jerrold J. Kalz. 1972. Sema,tic Theory. Harper and 
Row, New York. 
Kevin Knight and Steve K. Luk. 1994. Building a large- 
scale knowledge base liw machine h'anslation. Ill Pro- 
ceedi,gs o\[" the 12th Natio,al Co,ference o, Artifi- 
cial l,telli,q,e, ce (AAAI-94), w)lume 1, pages 773-778, 
Sealtlc, Washinglon, July-Augt, st. American Associ- 
ation for Artificial Intelligence. 
Adrienne Lehrer. 1974. Sema, tic l,}'elds and Lexical 
Structztre. North Holland, Amster&tm and New York. 
John I,yons. 1977. Sema,tics, volume 1. Cambridge 
University Press, Cambridge, England. 
K. Mahesh and S. Nirenburg. 1995. A siluated ontol- 
ogy for practical NLP. In Pmceedi,gs of the Work- 
shol~ oil Basic Ontological Issues in Knowledge Shar- 
ing, 14th lntenmtio,al.loi, t Co,ference oil Artificial 
Intelligence (LICAI-95), MontrEal, Canada, Augusl. 
Milchell E /Vlarcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large aunotaled cor- 
pus of Fmglish: the Penn Treebank. Coml;tttatioltal 
Lin,~?uistics, 19(2):313-330, June. 
I~tandolph Quirk, Sidney Grecnbaum, Geoffrey l,eech, 
alld Jall Svartvik. 1985. A Complvhe,sive Grammar 
el'the English l.cmguage. Longman, London and New 
York. 
Thomas .l. Sanlner and Diane E. l)uffy. 1989. The Statis- 
tical Analysis of Discrete Data. Springer-Verlag, New 
York. 
tTAward Sapir. 1944. ()n grading: A study ill semantics. 
l~hilosol;hy qfScie,ce, 2:93-116. Reprinted in (Sapir, 
1949). 
Edwa,d Sapir. 1949. Selected Wiqtings i, Language, 
Culture and Personality. University of California 
Press, Be,'keley, California. Edited by David G. Mat> 
delbat, m. 
John M. Sinclair (editor in chiet). 1987. Collins 
COBU1LD English Language Dictionary. Collins, 
London. 
J. Wiebe, R. Bruce, and T. O'Hara. 1999. Develop- 
ment and use of a gold standard ata set for subjec- 
tivity classilieations. In Proceedings of tile 37th An- 
total Meeting of the Association for Computational 
Li,guistics (ACL-99), pages 246-253, Universily of 
Maryhmd, June. 
305 
Empirical Acquisition of Differentiating Relations from Definitions
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003
tomohara@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper describes a new automatic approach for
extracting conceptual distinctions from dictionary
definitions. A broad-coverage dependency parser is
first used to extract the lexical relations from the def-
initions. Then the relations are disambiguated using
associations learned from tagged corpora. This con-
trasts with earlier approaches using manually devel-
oped rules for disambiguation.
1 Introduction
Large-scale lexicons for computational semantics of-
ten lack sufficient distinguishing information for the
concepts serving to define words. For example,
WordNet (Miller, 1990) recently introduced new re-
lations for domain category and location in Version
2.0, along with 6,000+ instances; however, about
38% of the noun synsets are still not explicitly dis-
tinguished from sibling synsets.
Work on the Extended WordNet project
(Harabagiu et al, 1999) is achieving substan-
tial progress in making the information in WordNet
more explicit. The main goal is to transform the
definitions into a logical form representation suit-
able for drawing inferences; in addition, the content
words in the definitions are being disambiguated. In
the logical form representation, separate predicates
are used for each preposition, as well as for some
other functional words (e.g., conjunctions); thus,
ambiguity in the underlying relations implicit in
the definitions is not being resolved. The work
described here automates the process of relation
disambiguation. This can be used to further the
transformation of WordNet into an explicit lexical
knowledge base.
Earlier approaches to differentia extraction have
predominantly relied upon manually constructed
pattern matching rules for extracting relations from
dictionary definitions (Vanderwende, 1996; Barrie`re,
1997; Rus, 2002). These rules can be very precise,
but achieving broad-coverage can be difficult. Here
a broad coverage dependency parser is first used to
determine the syntactic relations that are present
among the constituents in the sentence. Then the
syntactic relations between sentential constituents
are converted into semantic relations between the
underlying concepts using statistical classification.
Isolating the disambiguation step from the extrac-
tion step in this manner allows for greater flexibil-
ity over earlier approaches. For example, different
parsers can be incorporated without having to re-
work the disambiguation process.
This paper is organized as follows: Section 2 de-
tails the steps in extracting the initial relations from
the definition parse. Section 3 illustrates the disam-
biguation process, the crucial part of this approach.
Section 4 presents an evaluation of the relations that
are extracted from the WordNet definitions. Lastly,
Section 5 compares the approach to previous ap-
proaches that have been tried.
2 Differentia Extraction
The approach to differentia extraction is entirely au-
tomated. This starts with using the Link Grammar
Parser (Sleator and Temperley, 1993), a dependency
parser, to determine the syntactic lexical relations
that occur in the sentence. Dictionary definitions are
often given in the form of sentence fragments with
the headword omitted. For example, the definition
for the beverage sense of ?wine? is ?fermented juice
(of grapes especially).? Therefore, prior to running
the definition analysis, the definitions are converted
into complete sentences, using simple templates for
each part of speech.
After parsing, a series of postprocessing steps is
performed prior to the extraction of the lexical rela-
tions. For the Link Parser, this mainly involves con-
version of the binary dependencies into relational tu-
ples and the realignment of the tuples around func-
tion words. The Link Parser outputs syntactic de-
pendencies among words, punctuation, and sentence
boundary markers. The parser uses quite specialized
syntactic relations, so these are converted into gen-
eral ones prior to the extraction of the relational tu-
ples. For example, the relation A, which is used for
pre-noun adjectives, is converted into modifies. Fig-
ure 1 illustrates the syntactic relations that would
be extracted, along with the original parser output.
The syntactic relationships are first converted
into relational tuples using the format ?source-word,
relation-word , target-word?. This conversion is per-
formed by following the dependencies involving the
content words, ignoring cases involving non-word el-
ements (e.g., punctuation). For example, the first
tuple extracted from the parse would be ?n:wine,
Definition sentence:
Wine is fermented juice (of grapes especially).
Link Grammar parse:
?/////, Wd, 1. n:wine?
?/////, Xp, 10. .?
?1. n:wine, Ss, 2. v:is?
?10. ., RW, 11. /////?
?2. v:is, Ost, 4. n:juice?
?3. v:fermented, A, 4. n:juice?
?4. n:juice, MXs, 6. of?
?5. (, Xd, 6. of?
?6. of, Jp, 7. n:grapes?
?6. of, Xc, 9. )?
Extracted relations:
?1. n:wine, 2. v:is, 4. n:juice?
?3. v:fermented, modifies-3-4, 4. n:juice?
?4. n:juice, 6. of, 7. n:grapes?
Figure 1: Example for relation extraction.
v:is , n:juice?. Certain types of dependencies are
treated specially by converting the syntactic rela-
tionships directly into a relational tuple involving a
special relation-indicating word (e.g., ?modifies?).
The relational tuples extracted from the parse
form the basis for the lexical relations derived from
the definition. Structural ambiguity resolution is not
addressed here, so the first parse returned is used.
The remaining optional step assigns weights to the
relations that are extracted.
When using the relations in applications, it is de-
sirable to have a measure of how relevant the re-
lations are to the associated concepts. One such
measure would be the degree to which the relation
applies to the concept being described as opposed
to sibling concepts. To account for this, cue validi-
ties are used, borrowing from cognitive psychology
(Smith and Medin, 1981). Cue validities can be in-
terpreted as probabilities indicating the degree to
which features apply to a given concept versus sim-
ilar concepts (i.e., P (C|F )).
Cue validities are estimated by calculating the
percentage of times that the feature is associated
with a concept versus the total associations of con-
trasting concepts. This requires a means of deter-
mining the set of contrasting concepts for a given
concept. The simplest way of doing this would be to
just select the set of sibling concepts (e.g., synsets
sharing a common parent in WordNet). However,
due to the idiosyncratic way concepts are special-
ized in knowledge bases, this likely would not include
concepts intuitively considered as contrasting.
To alleviate this problem the most-informative an-
cestor will be used instead of the parent. This is
determined by selecting the ancestor that best bal-
ances frequency of occurrence in a tagged corpus
with specificity. This is similar to Resnik?s (1995)
notion of most-informative subsumer for a pair of
concepts. In his approach, estimated frequencies for
synsets are percolated up the hierarchy, so that the
frequency always increases as one proceeds up the
hierarchy. Therefore the first common ancestor for
a pair is the most-informative subsumer (i.e., has
most information content). Here attested frequen-
cies from SemCor (Miller et al, 1994) are used, so all
ancestors are considered. Specificity is accounted for
by applying a scaling factor to the frequencies that
decreases as one proceeds up the hierarchy. Thus,
?informative? is used more in an intuitive sense rather
than technical.
More details on the extraction process and the
subsequent disambiguation can be found in (O?Hara,
forthcoming).
3 Differentia Disambiguation
After the differentia properties have been extracted
from a definition, the words for the relation source
and object terms are disambiguated to order to re-
duce vagueness in the relationships. In addition, the
relation types are converted from surface-level rela-
tions (e.g., object) or relation-indicating words (e.g.,
prepositions) into the underlying semantic relation.
Since WordNet serves as the knowledge base be-
ing targeted, term disambiguation involves select-
ing the most appropriate synset for both the source
and target terms. The WordNet definitions have re-
cently been sense-tagged as part of the Extended
WordNet (Novischi, 2002), so these annotations are
incorporated. For other dictionaries, use of tradi-
tional word-sense disambiguation algorithms would
be required.
With the emphasis on corpus analysis in computa-
tional linguistics, there has been shift away from re-
lying on explicitly coded knowledge towards the use
of knowledge inferred from naturally occurring text,
in particular text that has been annotated by hu-
mans to indicate phenomena of interest. The Penn
Treebank version II (Marcus et al, 1994) provided
the first large-scale set of case role annotations for
general-purpose text. These are very general roles
akin to Fillmore?s (1968) case roles. The Berkeley
FrameNet (Fillmore et al, 2001) project provides
the most recent large-scale annotation of semantic
roles. These are at a much finer granularity than
those in Treebank, so they should prove quite use-
ful for applications learning detailed semantics from
corpora. O?Hara and Wiebe (2003) explain how
both inventories can be used for preposition disam-
biguation.
The goal of relation disambiguation is to deter-
mine the underlying semantic role indicated by par-
ticular words in a phrase or by word order. For
relations indicated directly by function words, the
disambiguation can be seen as a special case of word-
sense disambiguation (WSD). As an example, refin-
ing the relationship ??dog?, ?with? , ?ears?? into ??dog?,
has-part , ?ears??, is equivalent to disambiguating the
preposition ?with,? given that the senses are the dif-
Local-context features
POS: part of speech of target word
POS?i: part-of-speech of ith word to left
POS+i: part-of-speech of ith word to right
Word: target wordform as is
Word?i: ith word to the left
Word+i: ith word to the right
Collocational features
WordColl
i
: word collocation for sense i
Class-based collocational features
HyperColl
s
: hypernym collocation for sense i
Figure 2: Features for preposition classifier.
ferent relations it can indicate. For relations that are
indicated implicitly (e.g., adjectival modification),
other classification techniques would be required, re-
flecting the more syntactic nature of the task.
A straightforward approach for preposition disam-
biguation would be to use standard WSD features,
such as the parts-of-speech of surrounding words
and, more importantly, collocations (e.g., lexical as-
sociations). Although this can be highly accurate,
it tends to overfit the data and to generalize poorly.
The latter is of particular concern here as the train-
ing data is taken from a different genre (e.g., news-
paper text rather than dictionary definitions). To
overcome these problems, a class-based approach is
used for the collocations, with WordNet high-level
synsets as the source of the word classes. Figure 2
lists the features used for the classifier.
For the application to differentia disambiguation,
the classifiers learned over Treebank and FrameNet
need to be combined. This can be done readily in a
cascaded fashion with the classifier for the most spe-
cific relation inventory (i.e., FrameNet) being used
first and then the other classifiers being applied in
turn whenever the classification is inconclusive. This
has the advantage that new resources can be in-
tegrated into the combined relation classifier with
minimal effort. However, the resulting role inven-
tory will likely be heterogeneous and might be prone
to inconsistent classifications. In addition, the role
inventory could change whenever new annotation re-
sources are incorporated, making the differentia dis-
ambiguation system less predictable.
Alternatively, the annotations can be converted
into a common inventory, and a separate relation
classifier induced over the resulting data. This has
the advantage that the target relation-type inven-
tory remains stable whenever new sources of relation
annotations are introduced. The drawback however
is that annotations from new resources must first be
mapped into the common inventory before incorpo-
ration. The latter approach is employed here. The
common inventory incorporates some of the general
relation types defined by Gildea and Jurafsky (2002)
for their experiments in classifying semantic rela-
tions in FrameNet using a reduced inventory.
Relation Frequency
Theme 0.316
Goal 0.116
Ground 0.080
Category 0.069
Agent 0.069
Cause 0.061
Manner 0.058
Recipient 0.053
Medium 0.039
Characteristic 0.022
Resource 0.021
Means 0.021
Source 0.019
Path 0.017
Experiencer 0.017
Accompaniment 0.011
Area 0.010
Direction 0.001
Table 1: Frequency of relations extracted.
4 Evaluation
The evaluation discussed here assesses the quality of
the information that would be added to the lexicons
with respect to relation disambiguation, which is the
focus of the research. An application-oriented evalu-
ation is discussed in (O?Hara, forthcoming), showing
how using the extracted information improves word-
sense disambiguation.
All the definitions from WordNet 1.7.1 were run
through the differentia-extraction process. This in-
volved 111,223 synsets, of which 10,810 had prepro-
cessing or parse-related errors leading to no relations
being extracted. Table 1 shows the frequency of the
relations in the output from the differentia extrac-
tion process. The most common relation used is
Theme, which occurs four times as much compared
to the annotations. It is usually annotated as the
sense for ?of,? which also occurs with roles Source,
Category, Ground , Agent , Characteristic, and Expe-
riencer . Some of these represent subtle distinctions,
so it is likely that the difference in the text genre is
causing the classifier to use the default more often.
Four human judges were recruited to evaluate ran-
dom samples of the relations that were extracted. To
allow for inter-coder reliability analysis, each evalua-
tor evaluated some samples that were also evaluated
by the others, half as part of a training phase and
half after training. In addition, they also evaluated
a few samples that were manually corrected before-
hand. This provides a baseline against which the
uncorrected results can be measured against. Be-
cause the research only addresses relations indicated
by prepositional phrases, the evaluation is restricted
to these cases. Specifically, the judges rate the as-
signment of relations to the prepositional phrases on
a scale from 1 to 5, with 5 being an exact match.
The evaluation is based on averaging the assess-
Corrected
#Cases 10
#Scores 40
Mean 3.225
stdev 1.625
Score 0.60
Uncorrected
#Cases 15
#Scores 60
Mean 3.033
stdev 1.551
Score 0.58
Table 2: Mean assessment score for all ex-
tracted relationships. 25 relationships were each
evaluated by 4 judges. Mean gives the mean of the
assessment ratings (from 1 to 5). Score gives ratings
relative to scale from 0 to 1.
ment scores over the relationships. Table 2 shows
the results from this evaluation, including the man-
ually corrected as well as the uncorrected subsets
of the relationships. For the corrected output, the
mean assessment value was 3.225, which translates
into an overall score of 0.60. For the uncorrected sys-
tem output, the mean assessment value was 3.033,
which translates into an overall score of 0.58. Al-
though the absolute score is not high, the system?s
output is generally acceptable, as the score for the
uncorrected set of relationships is close to that of the
manually corrected set.
5 Related work
Most of the work addressing differentia extraction
has relied upon manually constructed extraction
rules (Vanderwende, 1996; Barrie`re, 1997; Rus,
2002). Here the emphasis is switched from transfor-
mation patterns for extracting relations into statis-
tical classification for relation disambiguation, given
tagged corpora with examples. This allows for bet-
ter coverage at the expense of precision. Note that
relation disambiguation is not yet addressed in Ex-
tended WordNet (Rus, 2002); for example, preposi-
tions are treated as predicates in the logical form
representation. Their extraction process is also
closely tied into the specifics of the parser, as a trans-
formation rule is developed for each grammar rule.
This work addresses the acquisition of conceptual
distinctions. In principle, it can handle any level
of granularity given sufficient training data; how-
ever, addressing distinctions at the level of near-
synonyms (Edmonds and Hirst, 2002) might require
customized analysis for each cluster of nearly syn-
onymous words. Inkpen and Hirst (2001) discuss
how this can be automated by analyzing specialized
synonymy dictionaries. Decision lists of indicative
keywords are learned for the broad types of prag-
matic distinctions, and these are then manually split
into decision lists for more-specific distinctions.
6 Conclusion
We have presented an empirical methodology for
extracting information from dictionary definitions.
This differs from previous approaches by using data-
driven relation disambiguation, using FrameNet se-
mantic roles annotations mapped into a reduced in-
ventory. All the definitions from WordNet 1.7.1 were
analyzed using this process, and the results evalu-
ated by four human judges. The overall results were
not high, but the evaluation was comparable to re-
lations that were manually corrected before coding.
References
C. Barrie`re. 1997. From Machine Readable Dictio-
naries to a Lexical Knowledge Base of Conceptual
Graphs. Ph.D. thesis, Simon Fraser University.
P. Edmonds and G. Hirst. 2002. Near-synonymy
and lexical choice. Computational Linguistics,
28(2):105?144.
C. Fillmore, C. Wooters, and C. Baker. 2001. Build-
ing a large lexical databank which provides deep
semantics. In Proc. PACLIC-01.
C. Fillmore. 1968. The case for case. In E. Bach
and R. Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart and Winston, New York.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
S. Harabagiu, G. Miller, and D. Moldovan. 1999.
WordNet 2?A morphologically and semantically
enhanced resource. In Proc. SIGLEX Workshop.
D. Inkpen and G. Hirst. 2001. Building a lexical
knowledge-base of near-synonym differences. In
Proc. WordNet and Other Lexical Resources.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, et al 1994. The Penn Treebank: Annotat-
ing predicate argument structure. In Proc. ARPA
Human Language Technology Workshop.
G. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. Thomas. 1994. Using a semantic concordance
for sense identification. In Proc. ARPA Human
Language Technology Workshop.
G. Miller. 1990. Introduction. International Jour-
nal of Lexicography, 3(4).
A. Novischi. 2002. Accurate semantic annotations
via pattern matching. In Proc. FLAIRS 2002.
T. O?Hara and J. Wiebe. 2003. Preposition se-
mantic classification via Penn Treebank and
FrameNet. In Proc. CoNLL-03.
T. O?Hara. forthcoming. Empirical acquisition of
conceptual distinctions via dictionary definitions.
Ph.D. thesis, New Mexico State University.
P. Resnik. 1995. Disambiguating noun groupings
with respect to WordNet senses. In Proc. WVLC.
V. Rus. 2002. Logic Forms for WordNet Glosses.
Ph.D. thesis, Southern Methodist University.
D. Sleator and D. Temperley. 1993. Parsing English
with a link grammar. In Proc. Workshop on Pars-
ing Technologies.
E. Smith and D. Medin. 1981. Categories and Con-
cepts. Harvard University Press, Cambridge, MA.
L. Vanderwende. 1996. Understanding Noun Com-
pounds using Semantic Information Extracted
from On-Line Dictionaries. Ph.D. thesis, George-
town University.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801?808
Manchester, August 2008
Discourse Level Opinion Interpretation ?
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Abstract
This work proposes opinion frames as a
representation of discourse-level associa-
tions which arise from related opinion top-
ics. We illustrate how opinion frames help
gather more information and also assist
disambiguation. Finally we present the re-
sults of our experiments to detect these as-
sociations.
1 Introduction
Opinions have been investigated at the phrase, sen-
tence, and document levels. However, little work
has been carried out regarding interpreting opin-
ions at the level of the discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television
(the opinion targets ? what the opinions are about
? are shown in italics).
(1) D :: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons.
Speaker D expresses an opinion in favor of a
design that is simple and organic in shape, and
against an alternative design which is not. Several
individual opinions are expressed in this passage.
The first is a negative opinion about the design be-
ing too edgy and box-like, the next is a positive
opinion toward a hand-held design, followed by a
negative opinion toward a computery shape, and
so on. While recognizing individual expressions
?This research was supported in part by the Department of
Homeland Security under grant N000140710152.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of opinions and their properties is important, dis-
course interpretation is needed as well. It is by un-
derstanding the passage as a discourse that we see
edgy, like a box, computery, and many buttons as
descriptions of the type of design D does not pre-
fer, and hand-held, organic shape, and simple de-
signs as descriptions of the type he does. These de-
scriptions are not in general synonyms/antonyms
of one another; for example, there are hand-held
?computery? devices and simple designs that are
edgy. The unison/opposition among the descrip-
tions is due to how they are used in the discourse.
This paper focuses on such relations between
the targets of opinions in discourse. Specifically, in
this work, we propose a scheme of opinion frames,
which consist of two opinions that are related by
virtue of having united or opposed targets. We
argue that recognizing opinion frames will pro-
vide more opinion information for NLP applica-
tions than recognizing individual opinions alone.
Further, if there is uncertainty about any one of the
components, we believe opinion frames are an ef-
fective representation incorporating discourse in-
formation to make an overall coherent interpreta-
tion (Hobbs et al, 1993). Finally, we also report
the first results of experiments in recognizing the
presence of these opinion frames.
We introduce our data in Section 2, present
opinion frames in Section 3 and illustrate their util-
ity in Section 4. Our experiments are in Section 5,
related work is discussed in Section 6, and conclu-
sions are in Section 7.
2 Data
The data used in this work is the AMI meet-
ing corpus (Carletta et al, 2005) which con-
tains multi-modal recordings of group meetings.
Each meeting has rich transcription and seg-
801
ment (turn/utterance) information for each speaker.
Each utterance consists of one or more sentences.
We also use some of the accompanying manual an-
notations (like adjacency pairs) as features in our
machine learning experiments.
3 Opinion Frames
In this section, we lay out definitions relating to
opinion frames, illustrate with examples how these
are manifested in our data, and consider them in
the context of discourse relations.
3.1 Definitions
The components of opinion frames are individual
opinions and the relationships between their tar-
gets. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), we address two types of
opinions, sentiment and arguing.
Sentiment includes positive and negative eval-
uations, emotions, and judgments. Arguing in-
cludes arguing for or against something, and argu-
ing that something should or should not be done.
Opinions have a polarity that can be positive or
negative. 1 The target of an opinion is the entity or
proposition that the opinion is about. We establish
relations between targets, in the process relating
their respective opinions. We address two types of
relations, same and alternative.
The same relation holds between targets that
refer to the same entity, property, or proposi-
tion. Observing the relations marked by an-
notators, we found that same covers not only
identity, but also part-whole, synonymy, gener-
alization, specialization, entity-attribute, instan-
tiation, cause-effect, epithets and implicit back-
ground topic, i.e., relations that have been studied
by many researchers in the context of anaphora and
co-reference (e.g. (Clark, 1975; Vieira and Poe-
sio, 2000; Mueller and Strube, 2001)). Actually,
same relations holding between entities often in-
volve co-reference (where co-reference is broadly
conceived to include relations such as part-whole
listed above). However, there are no morpho-
syntactic constraints on what targets may be. Thus,
same relations may also hold between adjective
phrases, verb phrases, and clauses. An instance of
this is Example 1, where the same target relation
holds between the adjectives edgy and computery.
1Polarity can also be neutral or both (Wilson and Wiebe,
2005), but these values are not significant for our opinion
frames.
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
The alternative relation holds between targets
that are related by virtue of being opposing (mu-
tually exclusive) options in the context of the dis-
course. For example, in the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. Objects appear as alternatives via world and
domain knowledge (for example, shapes of a re-
mote); the context of the discourse (for example,
Hillary Clinton and Barak Obama are alternatives
in discussions of the primaries, but not in discus-
sions of the general election); and the way the ob-
jects are juxtaposed while expressing opinions (for
instance hand-held and computery in Example 1).
While same and alternative are not the only pos-
sible relations between targets, they commonly oc-
cur in task-oriented dialogs such as those in the
data we use.
Now that we have all the ingredients, we can
define opinion frames. An opinion frame is de-
fined as a structure composed of two opinions and
their respective targets connected via their target
relations. With four opinion type/polarity pairs
(SN,SP,AN,AP), for each of two opinion slots, and
two possible target relations, we have 4 * 4 * 2 =
32 types of frame, listed in Table 1.
3.2 Examples
We will now illustrate how the frames are applied
with the following meeting snippets from the AMI
meeting corpus. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
The text span capturing the target of the opinion
(as interpreted in context) is indicated in italics. To
make it easier to understand the opinion frames,
we separately list each opinion, followed by the
major relation between the targets and, in paren-
theses, the relevant subtype of the major relation.
In the passage below, the speaker D expresses
802
his preferences about the material for the TV re-
mote.
(2) D:: ... this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot.
A bit more durable and that 2 can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same (ellipsis)
t3 - t4 same (identity)
t1 - t3 same (identity)
The speaker?s positive sentiment regarding the
rubbery material is apparent from the text spans
bit more bouncy (Sentiment Positive or SP), bit
more durable (SP), ergonomic (SP) and a bit dif-
ferent from all the other remote controls (SP).
As shown, the targets of these opinions (it?s [t1],
that [t3], and it [t4]) are related by the same rela-
tion. The ellipsis occurs with bit more durable.
Target [t2] represents the (implicit) target of that
opinion, and [t2] has a same relation to [t1], the
target of the bit more bouncy opinion. The opin-
ion frames occurring throughout this passage are
all SPSPsame denoting that both the opinion com-
ponents are sentiments with positive polarity with
a same relation between their targets. One frame
occurs between O1 and O2, another between O3
and O4, and so on.
Example 2 illustrates relatively simple same re-
lations between targets. Now let us consider the
more involved passage below, in which a meeting
participant analyzes two leading remotes on the
market.
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same (identity)
t2 - t3 same (t3 subset of t2)
2Note that the ?that? refers to the property of being
durable; however, as our annotation scheme is not hierarchi-
cal, we connect it to the entity the opinion is about ? in this
case the rubbery material.
t3 - t4 same (t4 partof t3)
t5 - t1 same (identity)
Target [t2] is the set of two leading remotes; [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is a part of that remote, namely its
buttons. Thus, opinion O3 is directly about one of
the remotes, and indirectly about the set of both re-
motes. Similarly, O4 is directly about the buttons
of one of the remotes, and indirectly about that re-
mote itself. The assessments at different levels ac-
crue toward the analysis of the main topic under
consideration.
Moving on to alternative (alt) relations, con-
sider the passage below, where the speaker is ar-
guing for the curved shape.
(4) C:: . . . shapes should be curved, so round shapes.
Nothing square-like.
.
.
.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same (specification)
t3 - t4 same (epithet)
Opinion O1 argues for a curved shape, O2 ar-
gues against a square shape, and O3 argues against
square corners. Note that square corners is also
the target of a negative sentiment, O4, expressed
here by too. Opinion O5 argues against the old
box look. In addition, the wording old box look
implies a negative sentiment ? O6 (we list the tar-
get span as ?old box look,? which refers to the look
of having square corners).
There is an alt relation between [t1] and [t2].
Thus, we have an opinion frame of type APANalt
between O1 and O2. From this frame, we are able
to understand that a positive opinion is expressed
toward something and a negative opinion is ex-
pressed toward its alternative.
3.3 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
803
linked by the human annotators. This path can be
traversed to create links between new pairs of tar-
gets, which in turn results in new opinion frame
relations.
Let us illustrate this idea with Example 4. The
frames with direct relations are O1O2 APANalt.
By following the alt link from [t1] to [t2] and the
same link from [t2] to [t3], we have an alt link
between [t1] and [t3], and the additional frames
O1O3 APANalt and O1O4 APSNalt. Repeating
this process would finally link speaker C?s opinion
O1 with B?s opinion O6 via a APSNalt frame.
Simple recipes such as this can be used by ap-
plications such as QA to gather more information
from the discourse.
3.4 Frame Types
In our corpus, we found that the 32 frames of Ta-
ble 1 can be categorized into two functional types:
reinforcing frames and non-reinforcing frames.
The set of frames that occur in scenarios where
the speaker intends to fortify or reinforce his opin-
ion/stance are called reinforcing frames. These
are the ones in the top row of the Table 1. Note that
these frames cover all opinion types, polarities and
target relations. It is the particular combination of
these frame components that bring about the rein-
forcement of the opinion in the discourse.
On the other hand, the frames at the bottom row
of the table are non-reinforcing. In our corpus,
these frames occur when a speaker is ambivalent
or weighing pros and cons.
Example 2 is characterized by opinion frames
in which the opinions reinforce one another ? that
is, individual positive sentiments (SP) occurring
throughout the passage fortify the positive regard
for the rubbery material via the same target rela-
tions and the resulting SPSPsame frames.
Interestingly, interplays among different opin-
ion types may show the same type of reinforce-
ment. For instance, Example 4 is characterized by
mixtures of opinion types, polarities, and target re-
lations. However, the opinions are still unified in
the intention to argue for a particular type of shape.
3.5 Discourse Relations and Opinion Frames
Opinion-frame recognition and discourse interpre-
tation go hand in hand; together, they provide
richer overall interpretations. For example, con-
sider the opinion frames and the Penn Discourse
Treebank relations (Prasad et al, 2007) for Ex-
ample 2. PDTB would see a list or conjunction
relation between the clauses containing opinions
bit more durable (O2) and ergonomic (O3), as
well as between the clauses containing opinions
ergonomic (O3) and a bit different from all the
other remote controls (O4). All of our opinion
frames for this passage are of type SPSPsame, a
reinforcing frame type. This passage illustrates
the case in which discourse relations nicely corre-
spond to opinion frames. The opinion frames flesh
out the discourse relations: we have lists specifi-
cally of positive sentiments toward related objects.
However, opinion-frame and discourse-relation
schemes are not redundant. Consider the following
three passages.
(e1) Non-reinforcing opinion frame (SNSPsame); Con-
trast discourse relation
D:: . . . I draw for you this schema that can be maybe
too technical for you but is very important for me
. . ..
(e2) Reinforcing opinion frame (SNAPalt); Contrast
discourse relation
D:: not too edgy and like a box, more kind of hand-
held
(e3) Reinforcing opinion frame (SPSPsame); no dis-
course relation
. . . they want something that?s easier to use straight
away, more intuitive perhaps.
In both e1 and e2, the discourse relation be-
tween the two opinions is contrast (?too technical?
is contrasted with ?very important?, and ?not too
edgy and like a box? is contrasted with ?more kind
of hand-held?). However, the opinion frame in e1
is SNSPsame, which is a non-reinforcing frame,
while the opinion frame in e2 is SNAPalt, which
is a reinforcing frame. In e3, the opinion frame
holds between targets within a subordinated clause
(easier to use and more intuitive are two desired
targets); most discourse theories don?t predict any
discourse relation in this situation.
Generally speaking, we find that there are not
definitive mappings between opinion frames and
the relations of popular discourse theories. For ex-
ample, Hobbs? (Hobbs et al, 1993) contrast cov-
ers at least four of our frames (SPSPalt, APAPalt,
APANsame, SPSNsame), while, for instance, our
SPSPsame frame can map to both the elaboration
and explanation relations.
4 Benefits of Discourse Opinion Frames
This section argues for two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in
the text, and they may contribute toward arriving
804
Positive Negative
Counting only individual opinions
Accepted Items 120 20
Rejected Items 9 12
individual + opinions via Reinforcing Opinion frames
Accepted Items 252 63
Rejected Items 22 26
Table 2: Opinion Polarity Distribution for Ac-
cepted/Rejected Items
at a coherent interpretation (Hobbs et al, 1993) of
the opinions in the discourse.
4.1 Gathering More Information
Frame relations provide a mechanism to relate
opinions expressed in non-local contexts - the
opinion may occur elsewhere in the discourse, but
will become relevant to a given target due to a re-
lation between its target and the given target. For
instance, in Example 3, there is one direct eval-
uation of the leading remotes (O1) and two eval-
uations via identity (O2, O5). Following frames
constructed via t2-t3 and t3-t4, we get two more
opinions (O3 and O4) for the leading remotes.
Furthermore, opinions regarding something not
lexically or even anaphorically related can be-
come relevant, providing more opinion informa-
tion. This is particularly interesting when alt re-
lations are involved, as opinions towards one alter-
native imply opinions of opposite polarity toward
the competing options. For instance in Example 4,
if we consider only the explicitly stated opinions,
there is only one (positive) opinion, O1, about the
curved shape. However, the speaker expresses sev-
eral other opinions which reinforce his positivity
toward the curved shape. Thus, by using the frame
information, it is possible to gather more opinions
regarding curved shapes for TV remotes.
As a simple proof of concept, we counted the
number of positive and negative opinions towards
the items that were accepted or rejected in the
meetings (information about accepted and rejected
items is obtained from the manual abstractive sum-
maries provided by the AMI corpus). Counts are
obtained, over opinions manually annotated in the
data, for two conditions: with and without frame
information. The items in our meeting data are
mainly options for the new TV remote, which in-
clude attributes and features like different shapes,
materials, designs, and functionalities. We ob-
served that for the accepted items, the number of
positive opinions is higher and, for rejected items,
the number of negative opinions is higher. The
top section of Table 2 shows a contingency ta-
ble of counts of positive/negative opinions for ac-
cepted/rejected items for 5 AMI meetings.
Then we counted the number of reinforc-
ing opinions that were expressed regarding these
items. This meant also counting additional opin-
ions that were related via reinforcing frames. The
bottom section of Table 2 shows the counts when
the reinforcing frames are considered. Compared
to the counts of only individual opinions, we see
that the numbers in each cell have increased, while
maintaining the same pattern of distribution.
Thus, in effect we have procured more instances
of opinions for the items. We believe this added
information would help applications like meeting
summarizers and QA systems to make more in-
formed decisions.
4.2 Interdependent Interpretation
We believe that our opinion frames, anaphoric re-
lations and discourse relations can symbiotically
help disambiguate each other in the discourse. In
particular, suppose that some aspect of an individ-
ual opinion, such as polarity, is unclear. If the dis-
course suggests certain opinion frames, this may in
turn resolve the underlying ambiguity.
Revisiting Example 2 from above, we see that
out of context, the polarities of bouncy and dif-
ferent from other remotes are unclear (bounci-
ness and being different may be negative attributes
for another type of object). However, the polari-
ties of two of the opinions are clear (durable and
ergonomic). There is evidence in this passage of
discourse continuity and same relations such as the
pronouns, the lack of contrastive cue phrases, and
so on. This evidence suggests that the speaker ex-
presses similar opinions throughout the passage,
making the opinion frame SPSPsame more likely
throughout. Recognizing the frames would resolve
the polarity ambiguities of bouncy and different.
In the following example (5), the positive senti-
ment (SP) towards the this and the positive arguing
(AP) for the it are clear. These two individual opin-
ions can be related by a same/alt target relation, be
unrelated, or have some other relation not covered
by our scheme (in which case we would not have
a relation between them). There is evidence in the
discourse that makes one interpretation more likely
than others. The ?so? indicates that the two clauses
are highly likely to be related by a cause discourse
805
relation (PDTB). This information confirms a dis-
course continuity, as well as makes a reinforcing
scenario likely, which makes the reinforcing frame
SPAPsame highly probable. This increase in like-
lihood will in turn help a coreference system to in-
crease its confidence that the ?that? and the ?it?
co-refer.
(5) B :: ... and this will definitely enhance our market
sales, so we should take it into consideration also.
Opinion Span - target Span Rel
O1 definitely enhance our market sales - this [t1] SP
O2 so we should - it [t2] AP
Target - target Rel
t1 -t2 same (identity)
5 Experiments
There has been much work on recognizing indi-
vidual aspects of opinions like extracting individ-
ual opinions from phrases or sentences and recog-
nizing opinion type and polarity. Accordingly, in
our machine learning experiments we assume ora-
cle opinion and polarity information. Our experi-
ments thus focus on the new question: ?Given two
opinion sentences, determine if they participate in
any frame relation.? Here, an opinion sentence is a
sentence containing one or more sentiment or ar-
guing expression. In this work, we consider frame
detection only between sentence pairs belonging to
the same speaker.
5.1 Annotation of Gold Standard
Creating gold-standard opinion-frame data is ac-
complished by annotating frame components and
then building the frames from those underlying an-
notations.
We began with annotations created by Soma-
sundaran et al (2007), namely four meetings
of the AMI meeting corpus annotated for senti-
ment and arguing opinions (text anchor and type).
Following that annotation scheme, we annotated
an additional meeting. This gave us a corpus of
4436 sentences or 2942 segments (utterances). We
added attributes to the existing opinion annota-
tions, namely polarity and target-id. The target-
id attribute links the opinion to its local target
span. Relations between targets were then anno-
tated. When a newly annotated target is similar (or
opposed) to a set of targets already participating in
same relations, then the same (or alt) link is made
only to one of them - the one that seems most natu-
ral. This is often the one that is physically closest.
Content Word overlap between the sentence pair
Focus space overlap between the sentence pair
Anaphoric indicator in the second sentence
Time difference between the sentence pair
Number of intervening sentences
Existence of adjacency pair between the sentence pair
Bag of words for each sentence
Table 3: Features for Opinion Frame detection
Link transitivity is then used to connect targets that
are not explicitly linked by the annotators.
All annotations were performed by two of the
co-authors of this paper by consensus labeling.
The details of our annotation scheme and inter-
annotator agreement studies are presented in (So-
masundaran et al, 2008).
Once the individual frame components are an-
notated, conceptually, a frame exists for a pair of
opinions if their polarities are either positive or
negative and their targets are in a same or alt rela-
tion. For our experiments, if a path exists between
two targets, then their opinions are considered to
be participating in an opinion-frame relation.
The experimental data consists of pairs of opin-
ion sentences and the gold-standard information
whether there exists a frame between them. We
approximate continuous discourse by only pair-
ing sentences that are not more than 10 sentences
apart. We also filter out sentences that are less than
two words in length in order to handle data skew-
ness. This filters out very small sentences (e.g.,
?Cool.?) which rarely participate in frames. The
experiments were performed on a total of 2539
sentence pairs, of which 551 are positive instances.
5.2 Features
The factor that determines if two opinions are
related is primarily the target relations between
them. Instead of first finding the target span for
each opinion sentence and then inferring if they
should be related, we directly try to encode target
relation information in our features. By this ap-
proach, even in the absence of explicit target-span
information, we are able to determine if the opin-
ion sentence pairs are related.
We explored a number of features to incorpo-
rate this. The set that give the best performance
are listed in Table 3. The content word overlap
feature captures the degree of topic overlap be-
tween the sentence pair, and looks for target re-
lations via identity. The focus space overlap fea-
ture is motivated by our observation that partici-
806
Acc. Prec. Recall F-measure
False 78.3% - 0% -
Distribution 66% 21.7% 21.7% 21.4%
Random 50.0% 21.5% 49.4% 29.8 %
True 21.7% 21.6% 100% 35.5 %
System 67.6% 36.8% 64.9% 46%
Table 4: Automatic Detection of Opinion Frames
pants refer to an established discourse topic with-
out explicitly referring to it. Thus, we construct a
focus space for each sentence containing recently
used NP chunks. The feature is the percent over-
lap between the focus spaces of the two opinion
sentences. The anaphoric indicator feature checks
for the presence of pronouns such as it and that
in the second sentence to account for target rela-
tions via anaphora. The time difference between
the sentences and the number of intervening sen-
tences are useful features to capture the idea that
topics shift with time. The existence of an adja-
cency pair 3 between the sentences can clue the
system that the opinions in the sentences are re-
lated too. Finally, standard bag of words features
are included for each sentence.
5.3 Results
We performed 5-fold cross validation experiments,
using the standard SVMperf package (Joachims,
2005), an implementation of SVMs designed
for optimizing multivariate performance measures.
We found that, on our skewed data, optimizing on
F-measure obtains the best results.
Our system is compared to four baselines in Ta-
ble 4. The majority class baseline which always
guesses false (False) has good accuracy but zero
recall. The baseline that always guesses true (True)
has 100% recall and the best f-measure among the
baselines, but poor accuracy. We also constructed
a baseline that guesses true/false over the test set
based on the distribution in the training data (Dis-
tribution). This baseline is smarter than the other
baselines, as it does not indiscriminately guess any
one of the class. The last baseline Random guesses
true 50% of the time.
The bottom row of Table 4 shows the perfor-
mance of our system (System). The skewness of
the data affects the baselines as well as our sys-
tem. Our system beats the best baseline f-measure
by over 10 percentage points, and the best base-
line precision by 14 percentage points. Comparing
3Adjacency Pairs are manual dialog annotations available
in the AMI corpus.
it to the baseline which has comparable accuracy,
namely Distribution, we see that our system im-
proves in f-measure by 24 percentage points.
Our results are encouraging - even using simple
features to capture target relations achieves consid-
erable improvement over the baselines. However,
there is much room for improvement. Using more
detailed target and discourse information promises
to further improve system performance. These are
avenues for future work.
6 Related work
Evidence from the surrounding context has been
used previously to determine if the current sen-
tence should be subjective/objective (Riloff et al,
2003; Pang and Lee, 2004) and adjacency pair in-
formation has been used to predict congressional
votes (Thomas et al, 2006). However, these meth-
ods do not explicitly model the relations between
opinions. An application of the idea of alterna-
tive targets can be seen in Kim and Hovy?s (2007)
work on election prediction. They assume that if
a speaker expresses support for one party, all men-
tions of the competing parties have negative po-
larity, thus creating automatically labeled training
data.
In the field of product review mining, sentiments
and features (aspects) have been mined (Popescu
and Etzioni, 2005), where the aspects correspond
to our definition of targets. However, the aspects
themselves are not related to each other in any
fashion.
Polanyi and Zaenen (2006), in their discussion
on contextual valence shifters, have also observed
the phenomena described in this work - namely
that a central topic may be divided into subtopics
in order to perform evaluations, and that discourse
structure can influence the overall interpretation of
valence. Snyder and Barzilay (2007) combine an
agreement model based on contrastive RST rela-
tions with a local aspect model to make a more
informed overall decision for sentiment classifi-
cation. In our scheme, their aspects would be
related as same and their high contrast relations
would correspond to the non-reinforcing frames
SPSNsame, SNSPsame. Additionally, our frame
relations would link the sentiments across non-
adjacent clauses, and make connections via alt tar-
get relations.
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
807
schemes for various available corpora of conver-
sation (e.g., Carletta et al (2005) for AMI). As
shown by Somasundaran et al (2007), dialog
structure information and opinions are in fact com-
plementary. We believe that, like the discourse
relations, the dialog information will additionally
help in arriving at an overall coherent interpreta-
tion.
7 Conclusions
In this paper, we described the idea of opin-
ion frames as a representation capturing discourse
level relations that arise from related opinion tar-
gets and which are common in task-oriented di-
alogs. We introduced the alternative relations that
hold between targets by virtue of being opposing
in the discourse context. We discussed how our
opinion-frame scheme and discourse relations go
hand in hand to provide a richer overall interpreta-
tion. We also illustrated that such discourse level
opinion associations have useful benefits, namely
they help gather more opinion information and
help interdependent interpretation. Finally, we
showed via our machine learning experiments that
the presence of opinion frames can be automati-
cally detected.
References
Carletta, J., S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of Measuring Behavior Symposium on
?Annotating and measuring Meeting Behavior?.
Clark, H. H. 1975. Bridging. Theoretical issues in
natural language processing . New York: ACM.
Hobbs, J., M. Stickel, D. Appelt, and P. Martin. 1993.
Interpretation as abduction. AI, 63.
Joachims, T. 2005. A support vector method for multi-
variate performance measures. In ICML 2005.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL 2007.
Mueller, C. and M. Strube. 2001. Annotating
anaphoric and bridging relations with mmax. In 2nd
SIGdial Workshop on Discourse and Dialogue.
Pang, B. and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
Polanyi, L. and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect
in Text: Theory and Applications. Springer.
Popescu, A.-M. and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber, 2007. PDTB 2.0 Anno-
tation Manual.
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning
subjective nouns using extraction pattern bootstrap-
ping. In CoNLL 2003.
Snyder, B. and R. Barzilay. 2007. Multiple aspect
ranking using the good grief algorithm. In HLT
2007: NAACL.
Somasundaran, S., J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
Somasundaran, S, J Ruppenhofer, and J Wiebe. 2008.
Discourse level opinion relations: An annotation
study. In SIGdial Workshop on Discourse and Di-
alogue. ACL.
Thomas, M., B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
Vieira, R. and M. Poesio. 2000. An empirically based
system for processing definite descriptions. Comput.
Linguist., 26(4).
Wilson, T. and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
808
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127?135,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multilingual Subjectivity Analysis Using Machine Translation
Carmen Banea and Rada Mihalcea
University of North Texas
carmenb@unt.edu, rada@cs.unt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Samer Hassan
University of North Texas
samer@unt.edu
Abstract
Although research in other languages is in-
creasing, much of the work in subjectivity
analysis has been applied to English data,
mainly due to the large body of electronic re-
sources and tools that are available for this lan-
guage. In this paper, we propose and evalu-
ate methods that can be employed to transfer a
repository of subjectivity resources across lan-
guages. Specifically, we attempt to leverage
on the resources available for English and, by
employing machine translation, generate re-
sources for subjectivity analysis in other lan-
guages. Through comparative evaluations on
two different languages (Romanian and Span-
ish), we show that automatic translation is a
viable alternative for the construction of re-
sources and tools for subjectivity analysis in
a new target language.
1 Introduction
We have seen a surge in interest towards the ap-
plication of automatic tools and techniques for the
extraction of opinions, emotions, and sentiments in
text (subjectivity). A large number of text process-
ing applications have already employed techniques
for automatic subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 2005), text semantic analysis (Wiebe and Mihal-
cea, 2006; Esuli and Sebastiani, 2006), tracking sen-
timent timelines in on-line forums and news (Lloyd
et al, 2005; Balog et al, 2006), mining opinions
from product reviews (Hu and Liu, 2004), and ques-
tion answering (Yu and Hatzivassiloglou, 2003).
A significant fraction of the research work to date
in subjectivity analysis has been applied to English,
which led to several resources and tools available for
this language. In this paper, we explore multiple
paths that employ machine translation while lever-
aging on the resources and tools available for En-
glish, to automatically generate resources for sub-
jectivity analysis for a new target language. Through
experiments carried out with automatic translation
and cross-lingual projections of subjectivity annota-
tions, we try to answer the following questions.
First, assuming an English corpus manually an-
notated for subjectivity, can we use machine trans-
lation to generate a subjectivity-annotated corpus in
the target language? Second, assuming the availabil-
ity of a tool for automatic subjectivity analysis in
English, can we generate a corpus annotated for sub-
jectivity in the target language by using automatic
subjectivity annotations of English text and machine
translation? Finally, third, can these automatically
generated resources be used to effectively train tools
for subjectivity analysis in the target language?
Since our methods are particularly useful for lan-
guages with only a few electronic tools and re-
sources, we chose to conduct our initial experiments
on Romanian, a language with limited text process-
ing resources developed to date. Furthermore, to
validate our results, we carried a second set of ex-
periments on Spanish. Note however that our meth-
ods do not make use of any target language specific
knowledge, and thus they are applicable to any other
language as long as a machine translation engine ex-
ists between the selected language and English.
127
2 Related Work
Research in sentiment and subjectivity analysis has
received increasingly growing interest from the nat-
ural language processing community, particularly
motivated by the widespread need for opinion-based
applications, including product and movie reviews,
entity tracking and analysis, opinion summarization,
and others.
Much of the work in subjectivity analysis has
been applied to English data, though work on other
languages is growing: e.g., Japanese data are used
in (Kobayashi et al, 2004; Suzuki et al, 2006;
Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006).
In addition, several participants in the Chinese
and Japanese Opinion Extraction tasks of NTCIR-
6 (Kando and Evans, 2007) performed subjectivity
and sentiment analysis in languages other than En-
glish.
In general, efforts on building subjectivity analy-
sis tools for other languages have been hampered by
the high cost involved in creating corpora and lexical
resources for a new language. To address this gap,
we focus on leveraging resources already developed
for one language to derive subjectivity analysis tools
for a new language. This motivates the direction of
our research, in which we use machine translation
coupled with cross-lingual annotation projections to
generate the resources and tools required to perform
subjectivity classification in the target language.
The work closest to ours is the one reported in
(Mihalcea et al, 2007), where a bilingual lexicon
and a manually translated parallel text are used to
generate the resources required to build a subjectiv-
ity classifier in a new language. In that work, we
found that the projection of annotations across par-
allel texts can be successfully used to build a cor-
pus annotated for subjectivity in the target language.
However, parallel texts are not always available for
a given language pair. Therefore, in this paper we
explore a different approach where, instead of rely-
ing on manually translated parallel corpora, we use
machine translation to produce a corpus in the new
language.
3 Machine Translation for Subjectivity
Analysis
We explore the possibility of using machine transla-
tion to generate the resources required to build sub-
jectivity annotation tools in a given target language.
We focus on two main scenarios. First, assuming a
corpus manually annotated for subjectivity exists in
the source language, we can use machine translation
to create a corpus annotated for subjectivity in the
target language. Second, assuming a tool for auto-
matic subjectivity analysis exists in the source lan-
guage, we can use this tool together with machine
translation to create a corpus annotated for subjec-
tivity in the target language.
In order to perform a comprehensive investiga-
tion, we propose three experiments as described be-
low. The first scenario, based on a corpus manu-
ally annotated for subjectivity, is exemplified by the
first experiment. The second scenario, based on a
corpus automatically annotated with a tool for sub-
jectivity analysis, is subsequently divided into two
experiments depending on the direction of the trans-
lation and on the dataset that is translated.
In all three experiments, we use English as a
source language, given that it has both a corpus man-
ually annotated for subjectivity (MPQA (Wiebe et
al., 2005)) and a tool for subjectivity analysis (Opin-
ionFinder (Wiebe and Riloff, 2005)).
3.1 Experiment One: Machine Translation of
Manually Annotated Corpora
In this experiment, we use a corpus in the source
language manually annotated for subjectivity. The
corpus is automatically translated into the target lan-
guage, followed by a projection of the subjectivity
labels from the source to the target language. The
experiment is illustrated in Figure 1.
We use the MPQA corpus (Wiebe et al, 2005),
which is a collection of 535 English-language news
articles from a variety of news sources manually an-
notated for subjectivity. Although the corpus was
originally annotated at clause and phrase level, we
use the sentence-level annotations associated with
the dataset (Wiebe and Riloff, 2005). From the total
of 9,700 sentences in this corpus, 55% of the sen-
tences are labeled as subjective while the rest are
objective. After the automatic translation of the cor-
128
Figure 1: Experiment one: machine translation of man-
ually annotated training data from source language into
target language
pus and the projection of the annotations, we obtain
a large corpus of 9,700 subjectivity-annotated sen-
tences in the target language, which can be used to
train a subjectivity classifier.
3.2 Experiment Two: Machine Translation of
Source Language Training Data
In the second experiment, we assume that the only
resources available are a tool for subjectivity anno-
tation in the source language and a collection of raw
texts, also in the source language. The source lan-
guage text is automatically annotated for subjectiv-
ity and then translated into the target language. In
this way, we produce a subjectivity annotated cor-
pus that we can use to train a subjectivity annotation
tool for the target language. Figure 2 illustrates this
experiment.
In order to generate automatic subjectivity anno-
tations, we use the OpinionFinder tool developed by
(Wiebe and Riloff, 2005). OpinionFinder includes
two classifiers. The first one is a rule-based high-
precision classifier that labels sentences based on the
presence of subjective clues obtained from a large
lexicon. The second one is a high-coverage classi-
fier that starts with an initial corpus annotated us-
ing the high-precision classifier, followed by several
bootstrapping steps that increase the size of the lex-
icon and the coverage of the classifier. For most of
our experiments we use the high-coverage classifier.
Figure 2: Experiment two: machine translation of raw
training data from source language into target language
Table 1 shows the performance of the two Opinion-
Finder classifiers as measured on the MPQA corpus
(Wiebe and Riloff, 2005).
P R F
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 1: Precision (P), Recall (R) and F-measure (F) for
the two OpinionFinder classifiers, as measured on the
MPQA corpus
As a raw corpus, we use a subset of the SemCor
corpus (Miller et al, 1993), consisting of 107 docu-
ments with roughly 11,000 sentences. This is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others. The reason
for working with this collection is the fact that we
also have a manual translation of the SemCor docu-
ments from English into one of the target languages
used in the experiments (Romanian), which enables
comparative evaluations of different scenarios (see
Section 4).
Note that in this experiment the annotation of sub-
jectivity is carried out on the original source lan-
guage text, and thus expected to be more accurate
than if it were applied on automatically translated
text. However, the training data in the target lan-
guage is produced by automatic translation, and thus
likely to contain errors.
129
3.3 Experiment Three: Machine Translation of
Target Language Training Data
The third experiment is similar to the second one,
except that we reverse the direction of the transla-
tion. We translate raw text that is available in the
target language into the source language, and then
use a subjectivity annotation tool to label the auto-
matically translated source language text. After the
annotation, the labels are projected back into the tar-
get language, and the resulting annotated corpus is
used to train a subjectivity classifier. Figure 3 illus-
trates this experiment.
Figure 3: Experiment three: machine translation of raw
training data from target language into source language
As before, we use the high-coverage classifier
available in OpinionFinder, and the SemCor corpus.
We use a manual translation of this corpus available
in the target language.
In this experiment, the subjectivity annotations
are carried out on automatically generated source
text, and thus expected to be less accurate. How-
ever, since the training data was originally written
in the target language, it is free of translation errors,
and thus training carried out on this data should be
more robust.
3.4 Upper bound: Machine Translation of
Target Language Test Data
For comparison purposes, we also propose an ex-
periment which plays the role of an upper bound on
the methods described so far. This experiment in-
volves the automatic translation of the test data from
the target language into the source language. The
source language text is then annotated for subjectiv-
ity using OpinionFinder, followed by a projection of
the resulting labels back into the target language.
Unlike the previous three experiments, in this
experiment we only generate subjectivity-annotated
resources, and we do not build and evaluate a stan-
dalone subjectivity analysis tool for the target lan-
guage. Further training of a machine learning algo-
rithm, as in experiments two and three, is required in
order to build a subjectivity analysis tool. Thus, this
fourth experiment is an evaluation of the resources
generated in the target language, which represents
an upper bound on the performance of any machine
learning algorithm that would be trained on these re-
sources. Figure 4 illustrates this experiment.
Figure 4: Upper bound: machine translation of test data
from target language into source language
4 Evaluation and Results
Our initial evaluations are carried out on Romanian.
The performance of each of the three methods is
evaluated using a dataset manually annotated for
subjectivity. To evaluate our methods, we generate a
Romanian training corpus annotated for subjectivity
on which we train a subjectivity classifier, which is
then used to label the test data.
We evaluate the results against a gold-standard
corpus consisting of 504 Romanian sentences man-
ually annotated for subjectivity. These sentences
represent the manual translation into Romanian of
a small subset of the SemCor corpus, which was
removed from the training corpora used in experi-
ments two and three. This is the same evaluation
dataset as used in (Mihalcea et al, 2007). Two
Romanian native speakers annotated the sentences
individually, and the differences were adjudicated
130
through discussions. The agreement of the two an-
notators is 0.83% (? = 0.67); when the uncertain an-
notations are removed, the agreement rises to 0.89
(? = 0.77). The two annotators reached consensus
on all sentences for which they disagreed, resulting
in a gold standard dataset with 272 (54%) subjective
sentences and 232 (46%) objective sentences. More
details about this dataset are available in (Mihalcea
et al, 2007).
In order to learn from our annotated data, we ex-
periment with two different classifiers, Na??ve Bayes
and support vector machines (SVM), selected for
their performance and diversity of learning method-
ology. For Na??ve Bayes, we use the multinomial
model (McCallum and Nigam, 1998) with a thresh-
old of 0.3. For SVM (Joachims, 1998), we use the
LibSVM implementation (Fan et al, 2005) with a
linear kernel.
The automatic translation of the MPQA and of
the SemCor corpus was performed using Language
Weaver,1 a commercial statistical machine transla-
tion software. The resulting text was post-processed
by removing diacritics, stopwords and numbers. For
training, we experimented with a series of weight-
ing schemes, yet we only report the results obtained
for binary weighting, as it had the most consistent
behavior.
The results obtained by running the three experi-
ments on Romanian are shown in Table 2. The base-
line on this data set is 54.16%, represented by the
percentage of sentences in the corpus that are sub-
jective, and the upper bound (UB) is 71.83%, which
is the accuracy obtained under the scenario where
the test data is translated into the source language
and then annotated using the high-coverage Opin-
ionFinder tool.
Perhaps not surprisingly, the SVM classifier out-
performs Na??ve Bayes by 2% to 6%, implying that
SVM may be better fitted to lessen the amount of
noise embedded in the dataset and provide more ac-
curate classifications.
The first experiment, involving the automatic
translation of the MPQA corpus enhanced with man-
ual annotations for subjectivity at sentence level,
does not seem to perform well when compared to the
experiments in which automatic subjectivity classi-
1http://www.languageweaver.com/
Romanian
Exp Classifier P R F
E1 Na??ve Bayes 60.91 60.91 60.91
SVM 66.07 66.07 66.07
E2 Na??ve Bayes 63.69 63.69 63.69
SVM 69.44 69.44 69.44
E3 Na??ve Bayes 65.87 65.87 65.87
SVM 67.86 67.86 67.86
UB OpinionFinder 71.83 71.83 71.83
Table 2: Precision (P), Recall (R) and F-measure (F) for
Romanian experiments
fication is used. This could imply that a classifier
cannot be so easily trained on the cues that humans
use to express subjectivity, especially when they are
not overtly expressed in the sentence and thus can
be lost in the translation. Instead, the automatic
annotations produced with a rule-based tool (Opin-
ionFinder), relying on overt mentions of words in
a subjectivity lexicon, seems to be more robust to
translation, further resulting in better classification
results. To exemplify, consider the following sub-
jective sentence from the MPQA corpus, which does
not include overt clues of subjectivity, but was an-
notated as subjective by the human judges because
of the structure of the sentence: It is the Palestini-
ans that are calling for the implementation of the
agreements, understandings, and recommendations
pertaining to the Palestinian-Israeli conflict.
We compare our results with those obtained by
a previously proposed method that was based on
the manual translation of the SemCor subjectivity-
annotated corpus. In (Mihalcea et al, 2007), we
used the manual translation of the SemCor corpus
into Romanian to form an English-Romanian par-
allel data set. The English side was annotated us-
ing the Opinion Finder tool, and the subjectivity la-
bels were projected on the Romanian text. A Na??ve
Bayes classifier was then trained on the subjectivity
annotated Romanian corpus and tested on the same
gold standard as used in our experiments. Table 3
shows the results obtained in those experiments by
using the high-coverage OpinionFinder classifier.
Among our experiments, experiments two and
three are closest to those proposed in (Mihalcea
et al, 2007). By using machine translation, from
131
OpinionFinder classifier P R F
high-coverage 67.85 67.85 67.85
Table 3: Precision (P), Recall (R) and F-measure (F) for
subjectivity analysis in Romanian obtained by using an
English-Romanian parallel corpus
English into Romanian (experiment two) or Roma-
nian into English (experiment three), and annotating
this dataset with the high-coverage OpinionFinder
classifier, we obtain an F-measure of 63.69%, and
65.87% respectively, using Na??ve Bayes (the same
machine learning classifier as used in (Mihalcea et
al., 2007)). This implies that at most 4% in F-
measure can be gained by using a parallel corpus as
compared to an automatically translated corpus, fur-
ther suggesting that machine translation is a viable
alternative to devising subjectivity classification in a
target language leveraged on the tools existent in a
source language.
As English is a language with fewer inflections
when compared to Romanian, which accommodates
for gender and case as a suffix to the base form of a
word, the automatic translation into English is closer
to a human translation (experiment three). Therefore
labeling this data using the OpinionFinder tool and
projecting the labels onto a fully inflected human-
generated Romanian text provides more accurate
classification results, as compared to a setup where
the training is carried out on machine-translated Ro-
manian text (experiment two).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 5: Experiment two: Machine learning F-measure
over an incrementally larger training set
We also wanted to explore the impact that the cor-
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 6: Experiment three: Machine learning F-measure
over an incrementally larger training set
pus size may have on the accuracy of the classifiers.
We re-ran experiments two and three with 20% cor-
pus size increments at a time (Figures 5 and 6). It
is interesting to note that a corpus of approximately
6000 sentences is able to achieve a high enough F-
measure (around 66% for both experiments) to be
considered viable for training a subjectivity classi-
fier. Also, at a corpus size over 10,000 sentences, the
Na??ve Bayes classifier performs worse than SVM,
which displays a directly proportional trend between
the number of sentences in the data set and the ob-
served F-measure. This trend could be explained
by the fact that the SVM classifier is more robust
with regard to noisy data, when compared to Na??ve
Bayes.
5 Portability to Other Languages
To test the validity of the results on other languages,
we ran a portability experiment on Spanish.
To build a test dataset, a native speaker of Span-
ish translated the gold standard of 504 sentences into
Spanish. We maintain the same subjectivity anno-
tations as for the Romanian dataset. To create the
training data required by the first two experiments,
we translate both the MPQA corpus and the Sem-
Cor corpus into Spanish using the Google Transla-
tion service,2 a publicly available machine transla-
tion engine also based on statistical machine transla-
tion. We were therefore able to implement all the ex-
periments but the third, which would have required
2http://www.google.com/translate t
132
a manually translated version of the SemCor corpus.
Although we could have used a Spanish text to carry
out a similar experiment, due to the fact that the
dataset would have been different, the results would
not have been directly comparable.
The results of the two experiments exploring the
portability to Spanish are shown in Table 4. Inter-
estingly, all the figures are higher than those ob-
tained for Romanian. We assume this occurs be-
cause Spanish is one of the six official United Na-
tions languages, and the Google translation engine
is using the United Nations parallel corpus to train
their translation engine, therefore implying that a
better quality translation is achieved as compared to
the one available for Romanian. We can therefore
conclude that the more accurate the translation en-
gine, the more accurately the subjective content is
translated, and therefore the better the results. As it
was the case for Romanian, the SVM classifier pro-
duces the best results, with absolute improvements
over the Na??ve Bayes classifier ranging from 0.2%
to 3.5%.
Since the Spanish automatic translation seems to
be closer to a human-quality translation, we are not
surprised that this time the first experiment is able
to generate a more accurate training corpus as com-
pared to the second experiment. The MPQA corpus,
since it is manually annotated and of better quality,
has a higher chance of generating a more reliable
data set in the target language. As in the experiments
on Romanian, when performing automatic transla-
tion of the test data, we obtain the best results with
an F-measure of 73.41%, which is also the upper
bound on our proposed experiments.
Spanish
Exp Classifier P R F
E1 Na??ve Bayes 65.28 65.28 65.28
SVM 68.85 68.85 68.85
E2 Na??ve Bayes 62.50 62.50 62.50
SVM 62.70 62.70 62.70
UB OpinionFinder 73.41 73.41 73.41
Table 4: Precision (P), Recall (R) and F-measure (F) for
Spanish experiments
6 Discussion
Based on our experiments, we can conclude that ma-
chine translation offers a viable approach to gener-
ating resources for subjectivity annotation in a given
target language. The results suggest that either a
manually annotated dataset or an automatically an-
notated one can provide sufficient leverage towards
building a tool for subjectivity analysis.
Since the use of parallel corpora (Mihalcea et al,
2007) requires a large amount of manual labor, one
of the reasons behind our experiments was to asses
the ability of machine translation to transfer subjec-
tive content into a target language with minimal ef-
fort. As demonstrated by our experiments, machine
translation offers a viable alternative in the construc-
tion of resources and tools for subjectivity classifica-
tion in a new target language, with only a small de-
crease in performance as compared to the case when
a parallel corpus is available and used.
To gain further insights, two additional experi-
ments were performed. First, we tried to isolate the
role played by the quality of the subjectivity anno-
tations in the source-language for the cross-lingual
projections of subjectivity. To this end, we used the
high-precision OpinionFinder classifier to annotate
the English datasets. As shown in Table 1, this clas-
sifier has higher precision but lower recall as com-
pared to the high-coverage classifier we used in our
previous experiments. We re-ran the second exper-
iment, this time trained on the 3,700 sentences that
were classified by the OpinionFinder high-precision
classifier as either subjective or objective. For Ro-
manian, we obtained an F-measure of 69.05%, while
for Spanish we obtained an F-measure of 66.47%.
Second, we tried to isolate the role played by
language-specific clues of subjectivity. To this end,
we decided to set up an experiment which, by com-
parison, can suggest the degree to which the lan-
guages are able to accommodate specific markers for
subjectivity. First, we trained an English classifier
using the SemCor training data automatically anno-
tated for subjectivity with the OpinionFinder high-
coverage tool. The classifier was then applied to the
English version of the manually labeled test data set
(the gold standard described in Section 4). Next, we
ran a similar experiment on Romanian, using a clas-
sifier trained on the Romanian version of the same
133
SemCor training data set, annotated with subjectiv-
ity labels projected from English. The classifier was
tested on the same gold standard data set. Thus, the
two classifiers used the same training data, the same
test data, and the same subjectivity annotations, the
only difference being the language used (English or
Romanian).
The results for these experiments are compiled in
Table 5. Interestingly, the experiment conducted on
Romanian shows an improvement of 3.5% to 9.5%
over the results obtained on English, which indi-
cates that subjective content may be easier to learn
in Romanian versus English. The fact that Roma-
nian verbs are inflected for mood (such as indicative,
conditional, subjunctive, presumptive), enables an
automatic classifier to identify additional subjective
markers in text. Some moods such as conditional
and presumptive entail human judgment, and there-
fore allow for clear subjectivity annotation. More-
over, Romanian is a highly inflected language, ac-
commodating for forms of various words based on
number, gender, case, and offering an explicit lex-
icalization of formality and politeness. All these
features may have a cumulative effect in allowing
for better classification. At the same time, English
entails minimal inflection when compared to other
Indo-European languages, as it lacks both gender
and adjective agreement (with very few notable ex-
ceptions such as beautiful girl and handsome boy).
Verb moods are composed with the aid of modals,
while tenses and expressions are built with the aid
of auxiliary verbs. For this reason, a machine learn-
ing algorithm may not be able to identify the same
amount of information on subjective content in an
English versus a Romanian text. It is also interesting
to note that the labeling of the training set was per-
formed using a subjectivity classifier developed for
English, which takes into account a large, human-
annotated, subjectivity lexicon also developed for
English. One would have presumed that any clas-
sifier trained on this annotated text would therefore
provide the best results in English. Yet, as explained
earlier, this was not the case.
7 Conclusion
In this paper, we explored the use of machine trans-
lation for creating resources and tools for subjec-
Exp Classifier P R F
En Na??ve Bayes 60.32 60.32 60.32
SVM 60.32 60.32 60.32
Ro Na??ve Bayes 67.85 67.85 67.85
SVM 69.84 69.84 69.84
Table 5: Precision (P), Recall (R) and F-measure (F) for
identifying language specific information
tivity analysis in other languages, by leveraging on
the resources available in English. We introduced
and evaluated three different approaches to generate
subjectivity annotated corpora in a given target lan-
guage, and exemplified the technique on Romanian
and Spanish.
The experiments show promising results, as they
are comparable to those obtained using manually
translated corpora. While the quality of the trans-
lation is a factor, machine translation offers an effi-
cient and effective alternative in capturing the sub-
jective semantics of a text, coming within 4% F-
measure as compared to the results obtained using
human translated corpora.
In the future, we plan to explore additional
language-specific clues, and integrate them into the
subjectivity classifiers. As shown by some of our
experiments, Romanian seems to entail more subjec-
tivity markers compared to English, and this factor
motivates us to further pursue the use of language-
specific clues of subjectivity.
Our experiments have generated corpora of about
20,000 sentences annotated for subjectivity in Ro-
manian and Spanish, which are available for down-
load at http://lit.csci.unt.edu/index.php/Downloads,
along with the manually annotated data sets.
Acknowledgments
The authors are grateful to Daniel Marcu and Lan-
guageWeaver for kindly providing access to their
Romanian-English and English-Romanian machine
translation engines. This work was partially sup-
ported by a National Science Foundation grant IIS-
#0840608.
134
References
C. Ovesdotter Alm, D. Roth, and R. Sproat. 2005.
Emotions from text: Machine learning for text-based
emotion prediction. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in blog
mood levels. In Proceedings of the 11th Meeting of
the European Chapter of the Association for Compu-
tational Linguistics (EACL-2006).
A. Esuli and F. Sebastiani. 2006. Determining term sub-
jectivity and term orientation for opinion mining. In
Proceedings the 11th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 193?200, Trento, IT.
R. Fan, P. Chen, and C. Lin. 2005. Working set selection
using the second order information for training svm.
Journal of Machine Learning Research, 6:1889?1918.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2004 (KDD 2004), pages 168?177, Seattle, Wash-
ington.
Y. Hu, J. Duan, X. Chen, B. Pei, and R. Lu. 2005. A new
method for sentiment classification in text retrieval. In
IJCNLP, pages 1?9.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
H. Kanayama and T. Nasukawa. 2006. Fully automatic
lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 355?363, Sydney, Australia.
N. Kando and D. Kirk Evans, editors. 2007. Proceed-
ings of the Sixth NTCIR Workshop Meeting on Evalua-
tion of Information Access Technologies: Information
Retrieval, Question Answering, and Cross-Lingual In-
formation Access, 2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, Japan, May. National Institute of In-
formatics.
S.-M. Kim and E. Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 200?207, New York, New York.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proceedings of the 1st
International Joint Conference on Natural Language
Processing (IJCNLP-04).
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia: A
system for large-scale news analysis. In String Pro-
cessing and Information Retrieval (SPIRE 2005).
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
Y. Suzuki, H. Takamura, and M. Okumura. 2006. Ap-
plication of semi-supervised learning to evaluative ex-
pression classification. In Proceedings of the 7th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2006),
pages 502?513, Mexico City, Mexico.
H. Takamura, T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientations of phrases.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2006), Trento, Italy.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2005) ( invited paper), Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
135
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170?179,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Supervised and Unsupervised Methods in Employing Discourse Relations
for Improving Opinion Polarity Classification
Swapna Somasundaran
Univ. of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Galileo Namata
Univ. of Maryland
College Park, MD 20742
namatag@cs.umd.edu
Janyce Wiebe
Univ. of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Lise Getoor
Univ. of Maryland
College Park, MD 20742
getoor@cs.umd.edu
Abstract
This work investigates design choices in
modeling a discourse scheme for im-
proving opinion polarity classification.
For this, two diverse global inference
paradigms are used: a supervised collec-
tive classification framework and an un-
supervised optimization framework. Both
approaches perform substantially better
than baseline approaches, establishing the
efficacy of the methods and the underlying
discourse scheme. We also present quan-
titative and qualitative analyses showing
how the improvements are achieved.
1 Introduction
The importance of discourse in opinion analy-
sis is being increasingly recognized (Polanyi and
Zaenen, 2006). Motivated by the need to en-
able discourse-based opinion analysis, previous
research (Asher et al, 2008; Somasundaran et al,
2008) developed discourse schemes and created
manually annotated corpora. However, it was not
known whether and how well these linguistic ideas
and schemes can be translated into effective com-
putational implementations.
In this paper, we first investigate ways in which
an opinion discourse scheme can be computation-
ally modeled, and then how it can be utilized to
improve polarity classification. Specifically, the
discourse scheme we use is from Somasundaran
et al (2008), which was developed to support a
global, interdependent polarity interpretation. To
achieve discourse-based global inference, we ex-
plore two different frameworks. The first is a
supervised framework that learns interdependent
opinion interpretations from training data. The
second is an unsupervised optimization frame-
work which uses constraints to express the ideas
of coherent opinion interpretation embodied in the
scheme. For the supervised framework, we use It-
erative Collective Classification (ICA), which fa-
cilitates machine learning using relational infor-
mation. The unsupervised optimization is imple-
mented as an Integer Linear Programming (ILP)
problem. Via our implementations, we aim to
empirically test if discourse-based approaches to
opinion analysis are useful.
Our results show that both of our implemen-
tations achieve significantly better accuracies in
polarity classification than classifiers using local
information alone. This confirms the hypothesis
that the discourse-based scheme is useful, and also
shows that both of our design choices are effective.
We also find that there is a difference in the way
ICA and ILP achieve improvements, and a simple
hybrid approach, which incorporates the strengths
of both, is able to achieve significant overall im-
provements over both. Our analyses show that
even when our discourse-based methods bootstrap
from noisy classifications, they can achieve good
improvements.
The rest of this paper is organized as follows:
we discuss related work in Section 2 and the
discourse scheme in Section 3. We present our
discourse-based implementations in Section 4, ex-
periments in Section 5, discussions in Section 6
and conclusions in Section 7.
2 Related Work
Previous work on polarity disambiguation has
used contextual clues and reversal words (Wil-
son et al, 2005; Kennedy and Inkpen, 2006;
Kanayama and Nasukawa, 2006; Devitt and Ah-
mad, 2007; Sadamitsu et al, 2008). However,
these do not capture discourse-level relations.
Researchers, such as (Polanyi and Zaenen,
2006), have discussed how the discourse struc-
ture can influence opinion interpretation; and pre-
vious work, such as (Asher et al, 2008; Soma-
sundaran et al, 2008), have developed annota-
170
tion schemes for interpreting opinions with dis-
course relations. However, they do not empiri-
cally demonstrate how automatic methods can use
their ideas to improve polarity classification. In
this work, we demonstrate concrete ways in which
a discourse-based scheme can be modeled using
global inference paradigms.
Joint models have been previously explored for
other NLP problems (Haghighi et al, 2005; Mos-
chitti et al, 2006; Moschitti, 2009). Our global in-
ference model focuses on opinion polarity recog-
nition task.
The biggest difference between this work and
previous work in opinion analysis that use global
inference methods is in the type of linguistic
relations used to achieve the global inference.
Some of the work is not related to discourse
at all (e.g., lexical similarities (Takamura et al,
2007), morphosyntactic similarities (Popescu and
Etzioni, 2005) and word-based measures like TF-
IDF (Goldberg and Zhu, 2006)). Others use
sentence cohesion (Pang and Lee, 2004), agree-
ment/disagreement between speakers (Thomas et
al., 2006; Bansal et al, 2008), or structural adja-
cency. In contrast, our work focuses on discourse-
based relations for global inference. Another dif-
ference from the above work is that our work is
over multi-party conversations.
Previous work on emotion and subjectivity
detection in multi-party conversations has ex-
plored using prosodic information (Neiberg et al,
2006), combining linguistic and acoustic infor-
mation (Raaijmakers et al, 2008) and combining
lexical and dialog information (Somasundaran et
al., 2007). Our work is focused on harnessing
discourse-based knowledge and on interdependent
inference.
There are several collective classification
frameworks, including (Neville and Jensen, 2000;
Lu and Getoor, 2003; Taskar et al, 2004; Richard-
son and Domingos, 2006; Bilgic et al, 2007). In
this paper, we use an approach by (Lu and Getoor,
2003) which iteratively predicts class values using
local and relational features. ILP has been used
on other NLP tasks, e.g., (Denis and Baldridge,
2007; Choi et al, 2006; Roth and Yih, 2004). In
this work, we employ ILP for modeling discourse
constraints for polarity classification.
3 Discourse Scheme and Data
The scheme in Somasundaran et al (2008) has
been developed and annotated over the AMI meet-
ing corpus (Carletta et al, 2005).
1
This scheme
annotates opinions, their polarities (positive, neg-
ative, neutral) and their targets (a target is what
the opinion is about). The targets of opinions are
related via two types of relations: the same rela-
tion, which relates targets referring to the same
entity or proposition, and the alternative relation,
which relates targets referring to mutually exclu-
sive options in the context of the discourse. Ad-
ditionally, the scheme relates opinions via two
types of frame relations: the reinforcing and non-
reinforcing relations. The frame relations repre-
sent discourse scenarios: reinforcing relations ex-
ist between opinions when they contribute to the
same overall stance, while non-reinforcing rela-
tions exist between opinions that show ambiva-
lence.
The opinion annotations are text-span based,
while in this work, we use Dialog Act (DA) based
segmentation of meetings.
2
As the DAs are our
units of classification, we map opinion annotations
to the DA units as follows. If a DA unit contains
an opinion annotation, the label is transferred up-
wards to the containing DA. When a DA contains
multiple opinion annotations, each with a differ-
ent polarity, one of them is randomly chosen as
the label for the DA. The discourse relations exist-
ing between opinions are also transferred upwards,
between the DAs containing each of these anno-
tations. We recreate an example from Somasun-
daran et al (2008) using DA segmentation in Ex-
ample 1. Here, the speaker has a positive opinion
towards the rubbery material for the TV remote.
(1) DA-1: ... this kind of rubbery material,
DA-2: it?s a bit more bouncy,
DA-3: like you said they get chucked around a lot.
DA-4: A bit more durable and that can also be er-
gonomic and
DA-5: it kind of feels a bit different from all the
other remote controls.
In the example, the individual opinion expressions
(shown in bold) are essentially regarding the same
thing ? the rubbery material. Thus, the explicit
targets (shown in italics), it?s, that, and it, and the
implicit target of a bit more durable are all linked
1
The AMI corpus contains a set of scenario-based meet-
ings where participants have to design a new TV remote pro-
totype.
2
DA segmentation is provided with the AMI corpus.
171
Figure 1: Discourse Relations between DA seg-
ments for Example 1.
with same target relations. Also, notice that the
opinions reinforce a particular stance, i.e., a pro-
rubbery-material stance. Thus, the scheme links
the opinions via reinforcing relations. Figure 1 il-
lustrates the corresponding discourse relations be-
tween the containing DA units.
4 Implementing the Discourse Model
The hypothesis in using discourse information for
polarity classification is that the global discourse
view will improve upon a classification with only
a local view. Thus, we implement a local clas-
sifier to bootstrap the classification process, and
then implement classifiers that use discourse in-
formation from the scheme annotations, over it.
We explore two approaches for implementing our
discourse-based classifier. The first is ICA, where
discourse relations and the neighborhood informa-
tion brought in by these relations are incorporated
as features into the learner. The second approach
is ILP optimization, which tries to maximize the
class distributions predicted by the local classifier,
subject to constraints imposed by discourse rela-
tions. Both classifiers thus accommodate prefer-
ences of the local classifier and for coherence with
discourse neighbors.
4.1 Local Classifier
A supervised local classifier, Local, is used to pro-
vide the classifications to bootstrap the discourse-
based classifiers.
3
It is important to make Local as
reliable as possible; otherwise, the discourse rela-
tions will propagate misclassifications. Thus, we
build Local using a variety of knowledge sources
that have been shown to be useful for opinion anal-
ysis in previous work. Specifically, we construct
features using polarity lexicons (used by (Wilson
et al, 2005)), DA tags (used by (Somasundaran
3
Local is supervised, as previous work has shown that
supervised methods are effective in opinion analysis. Even
though this makes the final end-to-end system with the ILP
implementation semi-supervised, note that the discourse-
based ILP part is itself unsupervised.
et al, 2007)) and unigrams (used by many re-
searchers, e.g., (Pang and Lee, 2004)).
Note that, as our discourse-based classifiers at-
tempt to improve upon the local classifications,
Local is also a baseline for our experiments.
4.2 Iterative Collective Classification
We use a variant of ICA (Lu and Getoor, 2003;
Neville and Jensen, 2000), which is a collective
classification algorithm shown to perform consis-
tently well over a wide variety of relational data.
Algorithm 1 ICA Algorithm
for each instance i do {bootstrapping}
Compute polarity for i using local attributes
end for
repeat {iterative}
Generate ordering I over all instances
for each i in I do
Compute polarity for i using local and re-
lational attributes
end for
until Stopping criterion is met
ICA uses two classifiers: a local classifier and a
relational classifier. The local classifier is trained
to predict the DA labels using only the local fea-
tures. We use Local, described in Section 4.1, for
this purpose. The relational classifier is trained us-
ing the local features, and an additional set of fea-
tures commonly referred to as relational features.
The value of a relational feature, for a given DA,
depends on the polarity of the discourse neighbors
of that DA. Thus, the relational features incorpo-
rate discourse and neighbor information; that is,
they incorporate the information about the frame
and target relations in conjunction with the polar-
ity of the discourse neighbors. Intuitively, our mo-
tivation for this approach can be explained using
Example 1. Here, in interpreting the ambiguous
opinion a bit different as being positive, we use
the knowledge that it participates in a reinforc-
ing discourse, and that all its neighbors (e.g., er-
gonomic, durable) are positive opinions regard-
ing the same thing. On the other hand, if it had
been a non-reinforcing discourse, then the polar-
ity of a bit different, when viewed with respect to
the other opinions, could have been interpreted as
negative.
Table 1 lists the relational features we defined
for our experiments where each row represents a
172
Percent of neighbors with polarity type a related via frame relation f
?
Percent of neighbors with polarity type a related via target relation t
?
Percent of neighbors with polarity type a related via frame relation f and target relation t
Percent of neighbors with polarity type a and same speaker related via frame relation f
?
Percent of neighbors with polarity type a and same speaker related via target relation t
?
Percent of neighbors with polarity type a related via a frame relation or target relation
Percent of neighbors with polarity type a related via a reinforcing frame relation or same target relation
Percent of neighbors with polarity type a related via a non-reinforcing frame relation or alt target relation
Most common polarity type of neighbors related via a same target relation
Most common polarity type of neighbors related via a reinforcing frame relation and same target relation
Table 1: Relational features: a ? {non-neutral (i.e., positive or negative), positive, negative}, t ? {same, alt},
f ? {reinforcing, non-reinforcing}, t
?
? {same or alt, same, alt}, f
?
? {reinforcing or non-reinforcing, reinforcing, non-
reinforcing}
set of features. Features are generated for all com-
binations of a, t, t
?
, f and f
?
for each row. For
example, one of the features in the first row is Per-
cent of neighbors with polarity type positive, that
are related via a reinforcing frame relation. Thus,
each feature for the relational classifier identifies
neighbors for a given instance via a specific rela-
tion (f , t, f
?
or t
?
, obtained from the scheme an-
notations) and factors in their polarity values (a,
obtained from the classifier predictions from the
previous round). This adds a total of 59 relational
features to the already existing local features.
ICA has two main phases: the bootstrapping
and iterative phases. In the bootstrapping phase,
the polarity of each instance is initialized to the
most likely value given only the local classifier
and its features. In the iterative phase, we cre-
ate a random ordering of all the instances and,
in turn, apply the relational classifier to each in-
stance where the relational features, for a given
instance, are computed using the most recent po-
larity assignments of its neighbors. We repeat this
until some stopping criterion is met. For our ex-
periments, we use a fixed number of 30 iterations,
which has been found to be sufficient in most data
sets for ICA to converge to a solution.
The pseudocode for the algorithm is shown in
Algorithm 1.
4.3 Integer Linear Programming
First, we explain the intuition behind viewing dis-
course relations as enforcing constraints on polar-
ity interpretation. Then, we explain how the con-
straints are encoded in the optimization problem.
4.3.1 Discourse Constraints on Polarity
The discourse relations between opinions can pro-
vide coherence constraints on the way their polar-
ity is interpreted. Consider a discourse scenario
in which a speaker expresses multiple opinions
regarding the same thing, and is reinforcing his
stance in the process (as in Example 1). The set
of individual polarity assignments that is most co-
herent with this global scenario is the one where
all the opinions have the same (equal) polarity. On
the other hand, a pair of individual polarity assign-
ments most consistent with a discourse scenario
where a speaker reinforces his stance via opinions
towards alternative options, is one with opinions
having mutually opposite polarity. For instance,
in the utterance ?Shapes should be curved, noth-
ing square-like?, the speaker reinforces his pro-
curved stance via his opinions about the alternative
shapes: curved and square-like. And, we see that
the first opinion is positive and the second is neg-
ative. Table 2 lists the discourse relations (target
and frame relation combinations) found in the cor-
pus, and the likely polarity interpretation for the
related instances.
Target relation + Frame relation Polarity
same+reinforcing equal (e)
same+non-reinforcing opposite (o)
alternative+reinforcing opposite (o)
alternative+non-reinforcing equal (e)
Table 2: Discourse relations and their polarity con-
straints on the related instances.
4.3.2 Optimization Problem
For each DA instance i in a dataset, the local
classifier provides a class distribution [p
i
, q
i
, r
i
],
where p
i
, q
i
and r
i
correspond to the probabilities
that i belongs to positive, negative and neutral cat-
egories, respectively. The optimization problem is
formulated as an ILP minimization of the objec-
tive function in Equation 1.
?1?
?
i
(p
i
x
i
+q
i
y
i
+r
i
z
i
)+
?
i,j

ij
+
?
i,j
?
ij
(1)
173
where the x
i
, y
i
and z
i
are binary class vari-
ables corresponding to positive, negative and neu-
tral classes, respectively. When a class variable
is 1, the corresponding class is chosen. Variables

ij
and ?
ij
are binary slack variables that corre-
spond to the discourse constraints between two
distinct DA instances i and j. When a given slack
variable is 1, the corresponding discourse con-
straint is violated. Note that the objective func-
tion tries to achieve two goals. The first part
(
?
i
p
i
x
i
+ q
i
y
i
+ r
i
z
i
) is a maximization that tries
to choose a classification for the instances that
maximizes the probabilities provided by the local
classifier. The second part (
?
i,j

ij
+
?
i,j
?
ij
) is a
minimization that tries to minimize the number of
slack variables used, that is, minimize the number
of discourse constraints violated.
Constraints in Equations 2 and 3 listed below
impose binary constraints on the variables. The
constraint in Equation 4 ensures that, for each in-
stance i, only one class variable is set to 1.
x
i
? {0, 1}, y
i
? {0, 1}, z
i
? {0, 1} , ?i (2)

ij
? {0, 1}, ?
ij
? {0, 1} , ?i 6= j (3)
x
i
+ y
i
+ z
i
= 1 , ?i (4)
We pair distinct DA instances i and j as ij,
and if there exists a discourse relation between
them, they can be subject to the corresponding po-
larity constraints listed in Table 2. For this, we
define two binary discourse-constraint constants:
the equal-polarity constant, e
ij
and the opposite-
polarity constant, o
ij
. If a given DA pair ij is
related by either a same+reinforcing relation or
an alternative+non-reinforcing relation (rows 1, 4
of Table 2), then e
ij
= 1; otherwise it is zero.
Similarly, if it is related by either a same+non-
reinforcing relation or an alternative+reinforcing
relation (rows 2, 3 of Table 2), then o
ij
= 1. Both
e
ij
and o
ij
are zero if the instance pair is unrelated
in the discourse.
For each DA instance pair ij, equal-polarity
constraints are applied to the polarity variables of i
(x
i
, y
i
) and j (x
j
, y
j
) via the following equations:
|x
i
? x
j
| ? 1? e
ij
+ 
ij
, ?i 6= j (5)
|y
i
? y
j
| ? 1? e
ij
+ 
ij
, ?i 6= j (6)
?(x
i
+ y
i
) ? ?l
i
, ?i (7)
When e
ij
= 1, the Equation 5 constrains x
i
and
x
j
to be of the same value (both zero or both one).
Similarly, Equation 6 constrains y
i
and y
j
to be
of the same value. Via these equations, we ensure
that the instances i and j do not have the oppo-
site polarity when e
ij
= 1. However, notice that,
if we use just Equations 5 and 6, the optimization
can converge to the same, non-polar (neutral) cat-
egory. To guide the convergence to the same polar
(positive or negative) category, we use Equation 7.
Here l
i
= 1 if the instance i participates in one or
more discourse relations. When e
ij
= 0, x
i
and x
j
(and y
i
and y
j
), can take on assignments indepen-
dently of one another. Notice that both constraints
5 and 6 are relaxed when 
ij
= 1; thus, x
i
and x
j
(or y
i
and y
j
) can take on values independently of
one another, even if e
ij
= 1.
Next, the opposite-polarity constraints are ap-
plied via the following equations:
|x
i
+ x
j
? 1| ? 1? o
ij
+ ?
ij
, ?i 6= j (8)
|y
i
+ y
j
? 1| ? 1? o
ij
+ ?
ij
, ?i 6= j (9)
In the above equations, when o
ij
= 1, x
i
and x
j
(and y
i
and y
j
) take on opposite values; for exam-
ple, if x
i
= 1 then x
j
= 0 and vice versa. When
o
ij
= 0, the variable assignments are independent
of one another. This set of constraints is relaxed
when ?
ij
= 1.
In general, in our ILP formulation, notice that
if an instance does not have a discourse relation to
any other instance in the data, its classification is
unaffected by the optimization. Also, as the un-
derlying discourse scheme poses constraints only
on the interpretation of the polarity of the related
instances, discourse constraints are applied only to
the polarity variables x and y, and not to the neu-
tral class variable, z. Finally, even though slack
variables are used, we discourage the ILP system
from indiscriminately setting the slack variables to
1 by making them a part of the objective function
that is minimized.
5 Experiments
In this work, we are particularly interested in
improvements due to discourse-based methods.
Thus, we report performance under three con-
ditions: over only those instances that are re-
lated via discourse relations (Connected), over in-
stances not related via discourse relations (Single-
tons), and over all instances (All).
The annotated data consists of 7 scenario-based,
multi-party meetings from the AMI meeting cor-
pus. We filter out very small DAs (DAs with fewer
than 3 tokens, punctuation included). This gives
174
us a total of 4606 DA instances, of which 1935
(42%) have opinion annotations. For our exper-
iments, the DAs with no opinion annotations as
well as those with neutral opinions are considered
as neutral. Table 3 shows the class distributions in
the data for the three conditions.
Pos Neg Neutral Total
Connected 643 343 81 1067
Singleton 553 233 2753 3539
All 1196 576 2834 4606
Table 3: Class distribution over connected, single
and all instances.
5.1 Classifiers
Our first baseline, Base, is a simple distribution-
based classifier that classifies the test data based
on the overall distribution of the classes in the
training data. However, in Table 3, the class distri-
bution is different for the Connected and Single-
ton conditions. We incorporate this in a smarter
baseline, Base-2, which constructs separate dis-
tributions for connected instances and singletons.
Thus, given a test instance, depending on whether
it is connected, Base-2 uses the corresponding dis-
tribution to make its prediction.
The third baseline is the supervised classifier,
Local, described in Section 4.1. It is imple-
mented using the SVM classifiers from the Weka
toolkit (Witten and Frank, 2002).
4
Our super-
vised discourse-based classifier, ICA from Sec-
tion 4.2, also uses a similar SVM implemen-
tation for its relational classifier. We imple-
ment our ILP approach from Section 4.3 us-
ing the optimization toolbox from Mathworks
(http://www.mathworks.com) and GNU Linear
Programming Kit.
We observed that the ILP system performs bet-
ter than the ICA system on instances that are con-
nected, while ICA performs better on singletons.
Thus, we also implemented a simple hybrid clas-
sifier (HYB), which selects the ICA prediction for
classification of singletons and the ILP prediction
for classification of connected instances.
5.2 Results
We performed 7-fold cross validation experi-
ments, where six meetings are used for training
4
We use the SMO implementation, which, when used
with the logistic regression, has an output that can be viewed
as a posterior probability distribution.
and the seventh is used for testing the supervised
classifiers (Base, Base-2, Local and ICA). In the
case of ILP, the optimization is applied to the out-
put of Local for each test fold. Table 4 reports the
accuracies of the classifiers, averaged over 7 folds.
First, we observe that Base performs poorly
over connected instances, but performs consider-
ably better over singletons. This is expected as the
overall majority class is neutral and the singletons
are more likely to be neutral. Base-2, which incor-
porates the differentiated distributions, performs
substantially better than Base. Local achieves an
overall performance improvement over Base and
Base-2 by 23 percentage points and 9 percent-
age points, respectively. In general, Local outper-
forms Base for all three conditions (p < 0.001),
and Base-2 for the Singleton and All conditions
(p < 0.001). This overall improvement in Local?s
accuracy corroborates the utility of the lexical, un-
igram and DA based features for polarity detection
in this corpus.
Turning to the discourse-based classifiers, ICA,
ILP and HYB, all of these perform better than
Base and Base-2 for all conditions. ICA improves
over Local by 9 percentage points for Connected,
3 points for Singleton and 4 points for All. ILP?s
improvement over Local for Connected and All is
even more substantial: 28 percentage points and
6 points, respectively. Notice that ILP has the
same performance as Local for Singletons, as the
discourse constraints are not applied over uncon-
nected instances. Finally, HYB significantly out-
performs Local under all conditions. The signif-
icance levels of the improvements over Local are
highlighted in Table 4. These improvements also
signify that the underlying discourse scheme is
effective, and adaptable to different implementa-
tions.
Interestingly, ICA and ILP improve over Local
in different ways. While ILP sharply improves the
performance over the connected instances, ICA
shows relatively modest improvements over both
connected and singletons. ICA?s improvement
over singletons is interesting because it indicates
that, even though the features in Table 1 are fo-
cused on discourse relations, ICA utilizes them to
learn the classification of singletons too.
Comparing our discourse-based approaches,
ILP does significantly better than ICA over con-
nected instances (p < 0.001), while ICA does
significantly better than ILP over singletons (p <
175
Base Base-2 Local ICA ILP HYB
Connected 24.4 47.56 46.66 55.64 75.07 75.07
Singleton 51.72 63.23 75.73 78.72 75.73 78.72
All 45.34 59.46 68.72 73.31 75.35 77.72
Table 4: Accuracies of the classifiers measured over Connected, Singleton and All instances. Perfor-
mance significantly better than Local are indicated in bold for p < 0.001 and underline for p < 0.01.
0.01). However, there is no significant difference
between ICA and ILP for the All condition. The
HYB classifier outperforms ILP for the Singleton
condition (p < 0.01) and ICA for the Connected
condition (p < 0.001). Interestingly, over all in-
stances (the All condition), HYB also performs
significantly better than both ICA (p < 0.001) and
ILP (p < 0.01).
5.3 Analysis
Amongst our two approaches, ILP performs bet-
ter, and hence we further analyze its behavior to
understand how the improvements are achieved.
Table 5 reports the performance of ILP and Local
for the precision, recall and f-measure metrics (av-
eraged over 7 test folds), measured separately for
each of the opinion categories. The most promi-
nent improvement by ILP is observed for the re-
call of the polar categories under the Connected
condition: 40 percentage points for the positive
class, and 29 percentage points for the negative
class. The gain in recall is not accompanied by
a significant loss in precision. This results in an
improvement in f-measure for the polar categories
(24 points for positive and 16 points for negative).
Also note that, by virtue of the constraint in Equa-
tion 7, ILP does not classify any connected in-
stance as neutral; thus the precision is NaN, recall
is 0 and the f-meaure is NaN. This is indicated as
* in the Table.
The improvement of ILP for the All condition,
for the polar classes, follows a similar trend for re-
call (18 to 21 point improvement) and f-measure
(9 to 13 point improvement). In addition to this,
the ILP has an overall improvement in precision
over Local. This may seem counterintuitive, as
in Table 5, ILP?s precision for connected nodes is
similar to, or lower than, that of Local. This is
explained by the fact that, while going from con-
nected to overall conditions, Local?s polar predic-
tions increase by threefold (565 to 1482), but its
correct polar predictions increase by only twofold
(430 to 801). Thus, the ratio of change in the total
Gold Local
Pos Neg Neut Total
Pos 551 113 532 1196
Neg 121 250 205 576
Neut 312 135 2387 2834
Total 984 498 3124 4606
Gold ILP
Pos Neg Neut Total
Pos 817 157 222 1196
Neg 147 358 71 576
Neut 358 147 2329 2834
Total 1322 662 2622 4606
Table 6: Contingency table over all instances.
polar predictions to the correct polar predictions is
3 : 2. On the other hand, while polar predictions
by ILP increase by only twofold (1067 to 1984),
its correct polar predictions increase by 1.5 times
(804 to 1175). Here, the ratio of change in the total
polar predictions to the correct polar predictions is
4 : 3, a smaller ratio.
The contingency table (Table 6) shows how Lo-
cal and ILP compare against the gold standard
annotations. Notice here, that even though ILP
makes more polar guesses as compared to Local, a
greater proportion of the ILP guesses are correct.
The number of non-diagonal elements are much
smaller for ILP, resulting in the accuracy improve-
ments seen in Table 4.
6 Examples and Discussion
The results in Table 4 show that Local, which pro-
vides the classifications for bootstrapping ICA and
ILP, predicts an incorrect class for more than 50%
of the connected instances. Methods starting with
noisy starting points are in danger of propagating
the errors and hence worsening the performance.
Interestingly, in spite of starting with so many bad
classifications, ILP is able to achieve a large per-
formance improvement. We discovered that, given
a set of connected instances, even when Local has
only one correct guess, ILP is able to use this to
rectify the related instances. We illustrate this situ-
ation in Figure 2, which reproduces the connected
DAs for Example 1. It shows the classifications
176
Positive Negative Neutral
Local ILP Local ILP Local ILP
Connected-Prec 78.1 78.2 71.9 69.8 12.1
Connected-Recall 45.3 86.3 44.1 73.4 62.8 *
Connected-F1 56.8 81.5 54.0 70.7 18.5
All-Prec 56.2 61.3 52.3 54.6 76.3 88.3
All-Recall 46.6 67.7 44.3 62.5 83.9 81.5
All-F1 50.4 64.0 46.0 57.1 79.6 84.6
Table 5: Precision, Recall, Fmeasure for each Polarity category. Performance significantly better than
Local are indicated in bold (p < 0.001), underline (p < 0.01) and italics (p < 0.05). The * denotes that
ILP does not retrieve any connected node as neutral.
Figure 2: Discourse Relations and Classifications
for Example 1.
for each DA from the gold standard (G), the Local
classifier (L) and the ILP classifier (ILP). Observe
that Local predicts the correct positive class (+) for
only DA-4 (the DA containing bit more durable
and ergonomic). Notice that these are clear cases
of positive evaluation. It incorrectly predicts the
polarity of DA-2 (containing bit more bouncy)
as neutral (*), and DA-5 (containing a bit dif-
ferent from all the other remote controls) as
negative (-). DA-2 and DA-5 exemplify the fact
that polarity classification is a complex and diffi-
cult problem: being bouncy is a positive evalua-
tion in this particular discourse context, and may
not be so elsewhere. Thus, naturally, lexicons and
unigram-based learning would fail to capture this
positive evaluation. Similarly, ?being different?
could be deemed negative in other discourse con-
texts. However, ILP is able to arrive at the correct
predictions for all the instances. As the DA-4 is
connected to both DA-2 and DA-5 via a discourse
relation that enforces an equal-polarity constraint
(same+reinforcing relation of row 1, Table 2), both
of the misclassifications are rectified. Presumably,
the incorrect predictions made by Local are low
confidence estimates, while the predictions of the
correct cases have high confidence, which makes
it possible for ILP to make the corrections.
We also observed the propagation of the correct
classification for other types of discourse relations,
for more complex types of connectivity, and also
for conditions where an instance is not directly
connected to the correctly predicted instance. The
meeting snippet below (Example 2) and its cor-
responding DA relations (Figure 3) illustrate this.
This example is a reinforcing discourse where the
speaker is arguing for the number keypad, which is
an alternative to the scrolling option. Thus, he ar-
gues against the scrolling, and argues for entering
the number (which is a capability of the number
keypad).
(2) D-1: I reckon you?re gonna have to have a num-
ber keypad anyway for the amount of channels these
days,
D-2: You wouldn?t want to just have to scroll
through all the channels to get to the one you want
D-3: You wanna enter just the number of it , if you
know it
D-4: I reckon we?re gonna have to have a number
keypad anyway
In Figure 3, we see that, DA-2 is connected via an
alternative+reinforcing discourse relation to each
of its neighbors DA-1 and DA-3, which encour-
ages the optimization to choose a class for it that
is opposite to DA-1 and DA-3. Notice also, that
even though Local predicts only DA-4 correctly,
this correct classification finally influences the cor-
rect choice for all the instances, including the re-
motely connected DA-2.
7 Conclusions and Future Work
This work focuses on the first step to ascertain
whether discourse relations are useful for improv-
ing opinion polarity classification, whether they
can be modeled and what modeling choices can
be used. To this end, we explored two distinct
paradigms: the supervised ICA and the unsuper-
vised ILP. We showed that both of our approaches
are effective in exploiting discourse relations to
177
Figure 3: Discourse Relations and Classifications for Example 2.
significantly improve polarity classification. We
found that there is a difference in how ICA and
ILP achieve improvements, and that combining
the two in a hybrid approach can lead to further
overall improvement. Quantitatively, we showed
that our approach is able to achieve a large in-
crease in recall of the polar categories without
harming the precision, which results in the perfor-
mance improvements. Qualitatively, we illustrated
how, even if the bootstrapping process is noisy,
the optimization and discourse constraints effec-
tively rectify the misclassifications. The improve-
ments of our diverse global inference approaches
indicate that discourse information can be adapted
in different ways to augment and improve existing
opinion analysis techniques.
The automation of the discourse-relation recog-
nition is the next step in this research. The be-
havior of ICA and ILP can change, depending on
the automation of discourse level recognition. The
implementation and comparison of the two meth-
ods under full automation is the focus of our future
work.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152 and NSF Grant No. 0746930.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
N. Asher, F. Benamara, and Y. Mathieu. 2008. Dis-
tilling opinion in discourse: A preliminary study.
COLING-2008.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. In COLING-
2008.
M. Bilgic, G. M. Namata, and L. Getoor. 2007. Com-
bining collective classification and link prediction.
In Workshop on Mining Graphs and Complex Struc-
tures at the IEEE International Conference on Data
Mining.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The ami meetings corpus. In Pro-
ceedings of the Measuring Behavior Symposium on
?Annotating and measuring Meeting Behavior?.
Y. Choi, E. Breck, and C. Cardie. 2006. Joint extrac-
tion of entities and relations for opinion recognition.
In EMNLP 2006.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL 2007.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based
approach. In ACL 2007.
A. B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
A. Haghighi, K. Toutanova, and C. Manning. 2005. A
joint model for semantic role labeling. In CoNLL.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP-2006, pages 355?363,
Sydney, Australia.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Q. Lu and L. Getoor. 2003. Link-based classification.
In Proceedings of the International Conference on
Machine Learning (ICML).
A. Moschitti, D. Pighin, and R. Basili. 2006. Seman-
tic role labeling via tree kernel joint inference. In
CoNLL.
A. Moschitti. 2009. Syntactic and semantic kernels for
short text pair categorization. In EACL.
178
D. Neiberg, K. Elenius, and K. Laskowski. 2006.
Emotion recognition in spontaneous speech using
gmms. In INTERSPEECH 2006 ICSLP.
J. Neville and D. Jensen. 2000. Iterative classifica-
tion in relational data. In In Proc. AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters. Computing Attitude and Affect in Text:
Theory and Applications.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In EMNLP.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Mach. Learn., 62(1-2):107?136.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL-2004, pages 1?8.
Boston, MA, USA.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008.
Sentiment analysis based on probabilistic models us-
ing inter-sentence information. In LREC?08.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Coling
2008.
H. Takamura, T. Inui, and M. Okumura. 2007. Extract-
ing semantic orientations of phrases from dictionary.
In HLT-NAACL 2007.
B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004.
Link prediction in relational data. In Neural Infor-
mation Processing Systems.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP 2005.
I. H. Witten and E. Frank. 2002. Data mining: practi-
cal machine learning tools and techniques with java
implementations. SIGMOD Rec., 31(1):76?77.
179
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190?199,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Subjectivity Word Sense Disambiguation
Cem Akkaya and Janyce Wiebe
University of Pittsburgh
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
This paper investigates a new task, subjec-
tivity word sense disambiguation (SWSD),
which is to automatically determine which
word instances in a corpus are being used
with subjective senses, and which are be-
ing used with objective senses. We pro-
vide empirical evidence that SWSD is
more feasible than full word sense dis-
ambiguation, and that it can be exploited
to improve the performance of contextual
subjectivity and sentiment analysis sys-
tems.
1 Introduction
The automatic extraction of opinions, emotions,
and sentiments in text (subjectivity analysis) to
support applications such as product review min-
ing, summarization, question answering, and in-
formation extraction is an active area of research
in NLP.
Many approaches to opinion, sentiment, and
subjectivity analysis rely on lexicons of words that
may be used to express subjectivity. Examples of
such words are the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
Knowing the meaning (and thus subjectivity) of
these words would help a system recognize the
negative sentiments in these sentences.
Most subjectivity lexicons are compiled as lists
of keywords, rather than word meanings (senses).
However, many keywords have both subjective
and objective senses. False hits ? subjectivity
clues used with objective senses ? are a signifi-
cant source of error in subjectivity and sentiment
analysis. For example, even though the follow-
ing sentence contains all of the negative keywords
above, it is nevertheless objective, as they are all
false hits:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
To tackle this source of error, we define a
new task, subjectivity word sense disambigua-
tion (SWSD), which is to automatically determine
which word instances in a corpus are being used
with subjective senses, and which are being used
with objective senses. We hypothesize that SWSD
is more feasible than full word sense disambigua-
tion, because it is more coarse grained ? often, the
exact sense need not be pinpointed. We also hy-
pothesize that SWSD can be exploited to improve
the performance of contextual subjectivity analy-
sis systems via sense-aware classification.
The paper consists of two parts. In the first
part, we build and evaluate a targeted supervised
SWSD system that aims to disambiguate members
of a subjectivity lexicon. It labels clue instances as
having a subjective sense or an objective sense in
context. The system relies on common machine
learning features for word sense disambiguation
(WSD). The performance is substantially above
both baseline and the performance of full WSD
on the same data, suggesting that the task is feasi-
ble, and that subjectivity provides a natural coarse-
grained grouping of senses.
The second part demonstrates the promise of
SWSD for contextual subjectivity analysis. First,
we show that subjectivity sense ambiguity is
highly prevalent in the MPQA opinion-annotated
corpus (Wiebe et al, 2005; Wilson, 2008), thus
establishing the potential benefit of performing
SWSD. Then, we exploit SWSD to improve per-
formance on several subjectivity analysis tasks,
from subjective/objective sentence-level classi-
fication to positive/negative/neutral expression-
level classification. To our knowledge, this is the
190
first attempt to explicitly use sense-level subjec-
tivity tags in contextual subjectivity and sentiment
analysis.
2 Background
We adopt the definitions of subjective and objec-
tive from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2008). Subjective expres-
sions are words and phrases being used to ex-
press mental and emotional states, such as spec-
ulations, evaluations, sentiments, and beliefs. A
general covering term for such states is private
state (Quirk et al, 1985), an internal state that
cannot be directly observed or verified by others.
(Wiebe and Mihalcea, 2006) give the following
examples:
(3) His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is
also important to NLP applications. In review
mining, for example, we want to know whether
an opinion about a product is positive or negative.
Nonetheless, as argued by (Wiebe and Mihalcea,
2006; Su and Markert, 2008), there are also mo-
tivations for a separate subjective/objective (S/O)
classification.
First, expressions may be subjective but not
have any particular polarity. An example given by
(Wilson et al, 2005a) is Jerome says the hospi-
tal feels no different than a hospital in the states.
An NLP application system may want to find a
wide range of private states attributed to a person,
such as their motivations, thoughts, and specula-
tions, in addition to their positive and negative sen-
timents. Second, benefits for sentiment analysis
can be realized by decomposing the problem into
S/O (or neutral versus polar) and polarity classifi-
cation (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Wilson et al, 2005a; Kim and Hovy,
2006). We will see further evidence of this in Sec-
tion 4.2.3 in this paper.
The contextual subjectivity analysis experi-
ments in Section 4 include both S/O and polarity
classifications. The data used in those experiments
is from the MPQA Corpus (Wiebe et al, 2005;
Wilson, 2008),1 which consists of texts from the
world press annotated for subjective expressions.
1Available at http://www.cs.pitt.edu/mpqa
In the MPQA Corpus, subjective expressions of
varying lengths are marked, from single words to
long phrases. In addition, other properties are an-
notated, including polarity.
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjec-
tivity. As noted in (Wiebe and Mihalcea, 2006),
sentences containing objective senses may not be
objective. Thus, objective senses are defined as
follows: Classifying a sense as O means that,
when the sense is used in a text or conversation,
one does not expect it to express subjectivity and,
if the phrase or sentence containing it is subjective,
the subjectivity is due to something else. Finally,
classifying a sense as B means it covers both sub-
jective and objective usages.
The following subjective examples are given in
(Wiebe and Mihalcea, 2006):
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
They give the following objective examples:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals
the occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particu-
lar purpose; ?the device is small enough to wear on your
wrist?; ?a device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was
only 10 fish?)
=> indefinite quantity ? (an estimated quantity)
Wiebe and Mihalcea performed an agreement
study and report that good agreement (?=0.74) can
be achieved between human annotators labeling
the subjectivity of senses. For a similar task, (Su
and Markert, 2008) also report good agreement
(?=0.79).
191
3 Subjectivity Word Sense
Disambiguation
3.1 Task Definition and Method
We now turn to SWSD, and our method for per-
forming it.
Note that SWSD is midway between pure dic-
tionary classification and pure contextual interpre-
tation. For SWSD, the context of the word is con-
sidered in order to perform the task, but the sub-
jectivity is determined solely by the dictionary. In
contrast, full contextual interpretation can deviate
from a sense?s subjectivity label in the dictionary.
As noted above, words used with objective senses
may appear in subjective expressions. For exam-
ple, an SWSD system would label the following
examples of alarm as S, O and O, respectively. On
the other hand, a sentence-level subjectivity clas-
sifier would label the sentences as S, S, and O, re-
spectively.
(4) His alarm grew.
Will someone shut that darn alarm off?
The alarm went off.
We use a supervised approach to SWSD. We
train a different classifier for each lexicon entry
for which we have training data. Thus, our ap-
proach is like targeted WSD (in contrast to all-
words WSD), with two labels: S and O.
We borrow machine learning features which
have been successfully used in WSD. Specifically,
given an ambiguous target word, we use the fol-
lowing features from (Mihalcea, 2002):
CW : the target word itself
CP : POS of the target word
CF : surrounding context of 3 words and their POS
HNP : the head of the noun phrase to which the
target word belongs
NB : the first noun before the target word
VB : the first verb before the target word
NA : the first noun after the target word
VA : the first verb after the target word
SK : at most 10 context words occurring at least 5
times; determined for each sense
3.2 Lexicon and Data
Our target words are members of a subjectivity
lexicon, because, since they are in such a lexicon,
we know they have subjective usages. Specifically,
we use the lexicon of (Wilson et al, 2005b; Wil-
son, 2008).2 The entries have been divided into
2Available at http://www.cs.pitt.edu/mpqa
those that are strongly subjective (strongsubj) and
those that are weakly subjective (weaksubj), re-
flecting their reliability as subjectivity clues. The
sources of the entries in the lexicon are identified
in (Wilson, 2008). In the second part of this pa-
per, we evaluate systems against the MPQA cor-
pus. Wilson also uses this corpus for her eval-
uations. To enable this, entries were added to
the lexicon independently from the MPQA corpus
(that is, none of the entries were derived using the
MPQA corpus).
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, indi-
cating whether they are used with a subjective or
objective sense. Because we do not have data la-
beled with the S/O coarse-grained senses and we
did not want to undertake the annotation effort at
this stage, we created an annotated corpus by com-
bining two types of sense annotations: (1) labels
of senses within a dictionary as S or O (i.e., sub-
jectivity sense labels), and (2) sense tags of word
instances in a corpus (i.e., sense-tagged data). The
subjectivity sense labels are used to collapse the
sense labels in the sense-tagged data into the two
new senses, S and O.
Our sense-tagged data are the lexical sample
corpora (training and test data) from SENSEVAL1
(Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss
and Yarowsky, 2001), and SENSEVAL3 (Mihal-
cea and Edmonds, 2004). We selected all of the
SENSEVAL words that are also in the subjectivity
lexicon, and labeled their dictionary senses as S,
O, or B according to the annotation scheme de-
scribed above in Section 2. We did this subjectiv-
ity sense labeling according to the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3).
Among the words, we found that 11 are not
ambiguous - either they have only S or only O
senses (in the corresponding sense inventory), or
the senses of their instances in the SENSEVAL data
are all S or all O. So as not to inflate our results, we
removed those 11 from the data, leaving 39 words.
In addition, we excluded the senses labeled B (a to-
tal of 10 senses). This leaves a total of 372 senses:
9 words (64 senses) from SENSEVAL1, 18 words
(201 senses) from SENSEVAL2, and 12 words (107
senses) from SENSEVAL3.
192
Base Acc SP SR SF OP OR OF IB EB(%)
All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8
S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2
S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8
S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0
Table 1: Overall SWSD results (micro averages). Base is majority-class baseline; Acc is accuracy; SP,
SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF. IB is absolute
improvement in Acc over Base; EB is percent error reduction in Acc.
3.3 SWSD Experiments
In this section, we evaluate our SWSD system, and
compare its performance to an WSD system on the
same data.
Note that, although generally in the SENSEVAL
datasets, training and test data are provided sep-
arately, a few target words from SENSEVAL1 do
not have both training and testing data. Thus, we
opted to combine the training and test data into one
dataset, and then perform 10-fold cross validation
experiments.
For our classifier, we use the SVM classifier
from the Weka package (Witten and Frank., 2005)
with its default settings.
We were interested in how well the system
would perform on more and less ambiguous
words. Thus, we split the words into three sub-
sets according to their majority-class baselines,
and report separate results: S1 (9 words), S2 (18
words), and S3 (12 words) have majority-class
baselines in the intervals [50%,70%) , [70%,90%),
and [90%,100%), respectively.
Table 1 contains the results, giving the overall
results (micro averages), as well as results for the
subsets S1, S2, and S3.
The improvement for SWSD over baseline is
especially high for the less skewed set, S1. This
is very encouraging because these words are the
more ambiguous words, and thus are the ones that
most need SWSD (assuming the SENSEVAL pri-
ors are similar to the priors in the corpus). The
average error reduction over baseline for S1 words
is 54.2%. Even for the more skewed sets S2 and
S3, reductions are 32.8% and 28.0%, respectively,
with an overall reduction of 41.8%.
To compare SWSD with WSD, we re-ran the
10-fold cross validation experiments, but this time
using the original sense labels, rather than S
and O. The (micro-averaged) accuracy is 67.9%,
much lower than the overall accuracy for SWSD
(88.3%).
The positive results provide evidence that
SWSD is a feasible variant of WSD, and that the
S/O sense groupings are natural ones, since the
system is able to learn to distinguish between them
with high accuracy. There is also potential for im-
provement by using a richer feature set, including
subjectivity features.
4 Opinion Analysis with Subjectivity
Word Sense Disambiguation
In this section, we explore the promise of SWSD
for contextual subjectivity analysis. First, we pro-
vide evidence that a subjectivity lexicon can have
substantial coverage of the subjective expressions
in a corpus, yet still be responsible for significant
subjectivity sense ambiguity in that corpus. Then,
we exploit SWSD in several contextual opinion
analysis systems, comparing the performance of
sense-aware and non-sense-aware versions. They
are all variations of components of the Opinion-
Finder opinion recognition system.3
4.1 Coverage and Ambiguity of Lexicon
Entries in the MPQA Corpus
In this section, we consider the distribution of lex-
icon entries in the MPQA corpus.
The lexicon covers a substantial subset of the
subjective expressions in the corpus: 67.1% of the
subjective expressions contain one or more lexi-
con entries.
On the other hand, fully 42.9% of the instances
of the lexicon entries in the MPQA corpus are
not in subjective expressions. An instance that
is not in a subjective expression is, by definition,
being used with an objective sense. Thus, these
instances are false hits of subjectivity clues. As
mentioned above, the entries in the lexicon have
been pre-classified as either more (strongsubj) or
less (weaksubj) reliable. We see this difference re-
flected in their degree of ambiguity ? 53% of the
3Available at http://www.cs.pitt.edu/opin
193
weaksubj instances are false hits, while only 22%
of the strongsubj instances are.
The high coverage of the lexicon demonstrates
its potential usefulness for opinion analysis sys-
tems, while its degree of ambiguity, in the form of
false hits in a subjectivity annotated corpus, shows
the potential benefit to opinion analysis of per-
forming SWSD.
As mentioned above, our experiments involve
only lexicon entries that are covered by the SEN-
SEVAL data, as we did not perform manual sense
tagging for this work. We have hope to expand
the system?s coverage in the future, as more word-
sense tagged data is produced (e.g., ONTONOTES
(Hovy et al, 2006)). We also have evidence that a
moderate amount of manual annotation would be
worth the effort. For example, let us order the lexi-
con entries from highest to lowest by frequency in
the MPQA corpus. The top 20 are responsible for
25% of all false hits in the corpus; the top 40 are
responsible for 34%; and the top 80 are responsi-
ble for 44%. If the SWSD system could be trained
for these words, the potential impact on reducing
false hits could be substantial, especially consid-
ering the good performance of the SWSD system
on the more ambiguous words. Note that we do
not want to simply discard these clues. The top 20
cover 9.4% of all subjective expressions; the top
40 cover 15.4%; and the top 80 cover 29.5%. Note
that SWSD only needs the data annotated with the
coarse-grained binary labels, which should be less
time consuming to produce than full word sense
tags.
4.2 Contextual Classification
We found in Section 3.3 that SWSD is a feasible
task and then in Section 4.1 that there is a great
deal of subjectivity sense ambiguity in a standard
subjectivity-annotated corpus (MPQA). We now
turn to exploiting the results of SWSD to automat-
ically recognize subjectivity and sentiment in the
MPQA corpus.
A motivation for using the MPQA data is that
many types of classifiers have been evaluated on
it, and we can directly test the effect of SWSD on
these classifiers.
Note that, for the SWSD experiments, the num-
ber of words does not limit the amount of data,
as SENSEVAL provides data for each word. How-
ever, the only parts of the MPQA corpus for which
SWSD could affect performance is the subset con-
taining instances of the words in the SWSD sys-
tem?s coverage. Thus, for the classifiers in this
section, the data used is the SenMPQA dataset,
which consists of the sentences in the MPQA Cor-
pus that contain at least one instance of the 39 key-
words. There are 689 such sentences (containing,
in total, 723 instances of the 39 keywords).
Even though this dataset is smaller than the one
used above, it gives us enough data to draw con-
clusions according to McNemar?s test for statisti-
cal significance.
4.2.1 Rule-based Classifier
We first apply SWSD to the rule-based classifier
from (Riloff and Wiebe, 2003). The classifier,
which is a sentence-level S/O classifier, has low
subjective and objective recall but high subjective
and objective precision. It is useful for creating
training data for subsequent processing by apply-
ing it to large amounts of unannotated data.
The classifier is a good candidate for directly
measuring the effects of SWSD on contextual sub-
jectivity analysis, because it classifies sentences
only by looking for the presence of subjectivity
keywords. Performance will improve if false hits
can be ignored.
The classifier labels a sentence as S if it contains
two or more strongsubj clues. On the other hand,
it considers three conditions to classify a sentence
as O: there are no strongsubj clues in the current
sentence, there are together at most one strongsubj
clue in the previous and next sentence, and there
are together at most 2 weaksubj clues in the cur-
rent, previous, and next sentence. A sentence that
is not labeled S or O is labeled unknown.
The rule-based classifier is made sense aware
by making it blind to the target word instances la-
beled O by the SWSD system, as these represent
false hits of subjectivity keywords. We compare
this sense-aware method (SE), with the original
classifier (O
RB
), in order to see if SWSD would
improve performance. We also built another modi-
fied rule-based classifier RE to demonstrate the ef-
fect of randomly ignoring subjectivity keywords.
RE ignores a keyword instance randomly with a
probability of 0.429, the expected value of false
hits in the MPQA corpus. The results are listed in
Table 2.
The rule-based classifier looks for the presence
of the keywords to find subjective sentences and
for the absence of the keywords to find objective
sentences. It is obvious that a variant working on
194
Acc OP OR OF SP SR SF
O
RB
27.0 50.0 4.1 7.6 92.7 36.0 51.8
SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6
RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2
Table 2: Effect of SWSD on the rule-based classi-
fiers.
fewer keyword instances than O
RB
will always
have the same or higher objective recall and the
same or lower subjective recall than O
RB
. That is
the case for both SE and RE. The real benefit we
see is in objective precision, which is substantially
higher for SE than O
RB
. For our experiments, OP
gives a better idea of the impact of SWSD, be-
cause most of the keyword instances SWSD dis-
ambiguates are weaksubj clues, and weaksubj key-
words figure more prominently in objective classi-
fication. On the other hand, RE has both lower OP
and SP than O
RB
. Note that accuracy for all three
systems is low, because all unknown predictions
are counted as incorrect.
These findings suggest that SWSD performs
well on disambiguating keyword instances in the
MPQA corpus,4 and demonstrates a positive im-
pact of SWSD on sentence-level subjectivity clas-
sification.
4.2.2 Subjective/Objective Classifier
We now move to more fine-grained expression-
level subjectivity classification. Since sentences
often contain multiple subjective expressions,
expression-level classification is more informative
than sentence-level classification.
The classifier in this section is an implementa-
tion of the neutral/polar supervised classifier of
(Wilson et al, 2005a) (using the same features),
except that the classes are S/O rather than neu-
tral/polar. These classifiers label instances of lex-
icon entries. The gold standard is defined on the
MPQA Corpus as follows: If an instance is in a
subjective expression, it is contextually S. If the
instance is in an objective expression, it is contex-
tually O. We evaluate the system on the 723 clue
instances in the SenMPQA dataset.
We incorporate SWSD information into the
contextual subjectivity classifier in a straight-
forward fashion: outputs are modified according
to simple, intuitive rules.
4which we cannot evaluate directly, as the MPQA corpus
is not sense tagged.
Our strategy is defined by the relation between
sense subjectivity and contextual subjectivity and
involves two rules, R1 and R2.
We know that a keyword instance used with a
S sense must be in a subjective expression. R1 is
to simply trust SWSD: If the contextual classifier
labels an instance as O, but SWSD determines that
it has an S sense, then R1 flips the contextual clas-
sifier?s label to S.
Things are not as simple in the case of O senses,
since they may appear in both subjective and ob-
jective expressions. We will state R2, and then ex-
plain it: If the contextual classifier labels an in-
stance as S, but (1) SWSD determines that it has
an O sense, (2) the contextual classifier?s confi-
dence is low, and (3) there is no other subjective
keyword in the same expression, then R2 flips the
contextual classifier?s label to O. First, consider
confidence: though a keyword with an O sense
may appear in either subjective or objective ex-
pressions, it is more likely to appear in an objec-
tive expression. We assume that this is reflected
to some extent in the contextual classifier?s confi-
dence. Second, if a keyword with an O sense ap-
pears in a subjective expression, then the subjec-
tivity is not due to that keyword but rather due to
something else. Thus, the presence of another lex-
icon entry ?explains away? the presence of the O
sense in the subjective expression, and we do not
want SWSD to overrule the contextual classifier.
Only when the contextual classifier isn?t certain
and only when there isn?t another keyword does
R2 flip the label to O.
Our definition of low confidence is in terms
of the label weights assigned by BoosTexter
(Schapire and Singer, 2000), which is the under-
lying machine learning algorithm of the classifier.
We use the difference between the largest label
weight and the second largest label weight as a
measure of confidence, as suggested in the Boos-
Texter documentation. The threshold we use is
0.0008.5
We apply the contextual classifier and the
SWSD system to the data, and compare the per-
formance of the original system (O
S/O
) and three
sense-aware variants: one using only R1, one us-
5As will be noted below, we experimented with three
thresholds for the classifier in Section 4.2.3, with no signif-
icant difference in accuracy. Here, we simply adopt 0.0008,
without further experimentation. In addition, we did not ex-
periment with other conditions than those incorporated in the
two rules in this section and the two rules in Section 4.2.3
below.
195
Acc OP OR OF SP SR SF
O
S/O
75.4 68.0 62.9 65.4 79.2 82.7 80.9
R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4
R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0
R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8
Table 3: Effect of SWSD on the subjec-
tive/objective classifier
ing only R2, and one using both (R1R2). The re-
sults are in Table 3. The R1 variant shows an im-
provement of 2.3 points in accuracy (a 9.4% error
reduction). The R2 variant shows an improvement
of 3.6 points in accuracy (a 14.6% error reduc-
tion). Applying both rules (R1R2) gives an im-
provement of 5.9 percentage points in accuracy (a
24% error reduction).
In our case, a paired t-test is not appropriate
to measure statistical significance, as we are not
doing multiple runs. Thus, we apply McNemar?s
test, which is a non-parametric method for algo-
rithms that can be executed only once, meaning
training once and testing once (Dietterich, 1998).
For R1, the improvement in accuracy is statisti-
cally significant at the p < .05 level. For R2 and
R1R2, the improvement in accuracy is statistically
significant at the p < .01 level. Moreover, in all
cases, we see improvement in both objective and
subjective F-measure.
4.2.3 Contextual Polarity Classifier
We now apply SWSD to contextual polarity clas-
sification (positive/negative/neutral), in the hope
that avoiding false hits of subjectivity keywords
will also lead to performance improvement in con-
textual sentiment analysis.
We use an implementation of the classifier of
(Wilson et al, 2005a). This classifier labels in-
stances of lexicon entries. The gold standard is
defined on the MPQA Corpus as follows: If an
instance is in a positive subjective expression, it
is contextually positive (Ps); if in a negative sub-
jective expression, it is contextually negative (Ng);
and if it is in an objective expression or a neu-
tral subjective expression, then it is contextually
N(eutral). As above, we evaluate the system on
the keyword instances in the SenMPQA dataset.
Wilson et al use a two step approach. The first
step classifies keyword instances as being in a po-
lar (positive or negative) or a neutral context. The
first step is performed by the neutral/polar classi-
fier mentioned above in Section 4.2.2. The sec-
ond step decides the contextual polarity (positive
or negative) of the instances classified as polar in
the first step, and is performed by a separate clas-
sifier.
To make a sense-aware version of the system,
we use rules to change some of the answers of the
neutral/polar classifier.
Unfortunately, we cannot simply trust SWSD
when it labels a keyword as an S sense, because an
S sense might be in a N(eutral) expression (since
there are neutral subjective expressions). But, an
S sense is more likely to appear in a P(olar) ex-
pression. Thus, we consider confidence (rule R3):
If the contextual classifier labels an instance as N,
but SWSD determines it has an S sense and the
contextual classifier?s confidence is low,6 then R3
flips the contextual classifier?s label to P.
Rule R4 is analogous to R2 in the previous sec-
tion: If the contextual classifier labels an instance
as P, but (1) SWSD determines that it has an O
sense, (2) the contextual classifier?s confidence is
low, and (3) there is no other subjective keyword in
the same expression, then R2 flips the contextual
classifier?s label to N.
We compare the performance of the original
neutral/polar classifier (O
N/P
) and sense-aware
variants using R3 and R4. The results are in Table
4. This time, the table does not include a combined
method, because only R4 improves performance.
This is consistent with the finding in (Wilson et
al., 2005a) that most errors are caused by subjec-
tivity keywords with non-neutral prior polarity ap-
pearing in phrases with neutral contextual polarity.
R4 targets these cases. It is promising to see that
SWSD provides enough information to fix some of
them. There is a 2.6 point improvement in accu-
racy (a 12.4% error reduction). The improvement
in accuracy is statistically significant at the p <
.01 level with McNemar?s test. The improvement
in accuracy is accompanied by improvements in
both neutral and polar F-measure.
We wanted to see if the improvements in the
6As in the previous section, low confidence is defined
in terms of the difference between the largest label weight
and the second largest label weight assigned by BoosTexter.
We tried three thresholds, 0.0007, 0.0008, and 0.0009, re-
sulting in only a slight difference in accuracy: 0.0007 and
0.0009 both give 81.5 accuracy compared to 81.6 accuracy
for 0.0008. We report results using 0.0008, though the ac-
curacy using the other thresholds is statistically significantly
better than the accuracy of the original classifier at the same
level.
196
Acc NP NR NF NgP NgR NgF PsP PsR PsF
O
Ps/Ng/N
77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0
R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0
Table 5: Effect of SWSD on the contextual polarity classifier
Acc NP NR NF PP PR PF
O
N/P
79.0 81.5 92.5 86.7 65.8 40.7 50.3
R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8
R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3
Table 4: Effect of SWSD on the neutral/polar clas-
sifier
first step of Wilson et als system can be propa-
gated to their second step, yielding an overall im-
provement in positive /negative/neutral (Ps/Ng/N)
classification.
The sense-aware variant of the overall two-part
system is the same as the original except that we
apply R4 to the output of the first step (flipping
some of the neutral/polar classifier?s P labels to
N). Thus, since the second step in Wilson et al?s
classifier processes only those instances labeled P
in the first step, in the sense-aware system, fewer
instances are passed from the first to the second
step.
Table 5 reports results for the original sys-
tem (O
Ps/Ng/N
) and the sense-aware variant (R4).
These results are for the entire SenMPQA dataset,
not just those labeled P in the first step.
The accuracy improves 3 percentage points (a
13.4% error reduction). The improvement in accu-
racy is statistically significant at the p < .01 level
with McNemar?s test. We see the real benefit when
we look at the precision of the positive and neg-
ative classes. Negative precision goes from 60.4
to 82.1 and positive precision goes from 52.2 to
68.6, with no loss in recall. This is evidence that
the SWSD system is doing a good job of removing
some false hits of subjectivity clues that harm the
original version of the system.
5 Comparisons to Previous Work
Several researchers exploit lexical resources for
contextual subjectivity and sentiment analysis.
These systems typically look for the presence of
subjective or sentiment-bearing words in the text.
They may rely only on this information (e.g.,
(Turney, 2002; Whitelaw et al, 2005; Riloff and
Wiebe, 2003)), or they may combine it with addi-
tional information as well (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Bloom et al,
2007; Wilson et al, 2005a)). We apply SWSD to
some of those systems to show the effect of SWSD
on contextual subjectivity and sentiment analysis.
Another set of related work is on subjectivity
and polarity labeling of word senses (e.g. (Esuli
and Sebastiani, 2006; Andreevskaia and Bergler,
2006; Wiebe and Mihalcea, 2006; Su and Markert,
2008)). They label senses of words in a dictionary.
In comparison, we label senses of word instances
in a corpus.
Moreover, our work extends findings in (Wiebe
and Mihalcea, 2006) and (Su and Markert, 2008).
(Wiebe and Mihalcea, 2006) demonstrates that
subjectivity is a property that can be associated
with word senses. We show that it is a natural
grouping of word senses and that it provides a
principled way for clustering senses. They also
demonstrate that subjectivity helps with WSD. We
show that a coarse-grained WSD variant (SWSD)
helps with subjectivity and sentiment analysis.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) show that even reliable subjectiv-
ity clues have objective senses. We demonstrate
that this ambiguity is also prevalent in a corpus.
Several researchers (e.g., (Palmer et al, 2004;
Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)) work on reducing the granularity of sense
inventories for WSD. They aim for a more coarse-
grained sense inventory to overcome performance
shortcomings related to fine-grained sense distinc-
tions. Our work is similar in the sense that we
reduce all senses of a word to two senses (S/O).
The difference is the criterion driving the group-
ing. Related work concentrates on syntactic and
semantic similarity between senses to group them.
In contrast, our grouping is driven by subjectivity
with a specific application area in mind, namely
subjectivity and sentiment analysis.
6 Conclusions and Future Work
We introduced the task of subjectivity word sense
disambiguation (SWSD), and evaluated a super-
vised method inspired by research in WSD. The
197
system achieves high accuracy, especially on
highly ambiguous words, and substantially outper-
forms WSD on the same data. The positive results
provide evidence that SWSD is a feasible variant
of WSD, and that the S/O sense groupings are nat-
ural ones.
We also explored the promise of SWSD for con-
textual subjectivity analysis. We showed that a
subjectivity lexicon can have substantial coverage
of the subjective expressions in the corpus, yet
still be responsible for significant sense ambiguity.
This demonstrates the potential benefit to opin-
ion analysis of performing SWSD. We then ex-
ploit SWSD in several contextual opinion analysis
systems, including positive/negative/neutral senti-
ment classification. Improvements in performance
were realized for all of the systems.
We plan several future directions which promise
to further increase the impact of SWSD on sub-
jectivity and sentiment analysis. We will manu-
ally annotate a moderate number of strategically
chosen words, namely frequent ones which are
highly ambiguous. In addition, we will add fea-
tures to the SWSD system reflecting the subjec-
tivity of the surrounding context. Finally, there
are more sophisticated strategies to explore for
improving subjectivity and sentiment analysis via
SWSD than the simple, intuitive rules we began
with in this paper.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0840632
and #0840608. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
A. Andreevskaia and S. Bergler. 2006. Mining word-
net for a fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In (EACL-2006).
K. Bloom, N. Garg, and S. Argamon. 2007. Extracting
appraisal expressions. In HLT-NAACL 2007, pages
308?315, Rochester, NY.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895?1923.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In (LREC-06), Genova, IT.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, New York City.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
S.-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In (HLT/NAACL-06), pages
200?207, New York, New York.
R. Mihalcea and P. Edmonds, editors. 2004. Pro-
ceedings of SENSEVAL-3, Association for Compu-
tational Linguistics Workshop, Barcelona, Spain.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applica-
tions. In HLT-NAACL 2004 Workshop: 2nd Work-
shop on Scalable Natural Language Understanding,
Boston, Massachusetts.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In (ACL-04), pages 271?
278, Barcelona, ES. Association for Computational
Linguistics.
J. Preiss and D. Yarowsky, editors. 2001. Pro-
ceedings of SENSEVAL-2, Association for Compu-
tational Linguistics Workshop, Toulouse, France.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In (EMNLP-2003),
pages 105?112, Sapporo, Japan.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), Prague,
Czech Republic.
F. Su and K. Markert. 2008. From word to sense: a
case study of subjectivity recognition. In (COLING-
2008), Manchester.
198
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 417?424, Philadelphia.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Pro-
ceedings of CIKM-05, the ACM SIGIR Conference
on Information and Knowledge Management, Bre-
men, DE.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, Syd-
ney, Australia.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Anno-
tating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005a. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In (HLT/EMNLP-2005), pages 347?354,
Vancouver, Canada.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005b. OpinionFinder: A system for subjec-
tivity analysis. In Proc. Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP-
2005) Companion Volume (software demonstration).
T. Wilson. 2008. Fine-grained Subjectivity and Sen-
timent Analysis: Recognizing the Intensity, Polarity,
and Attitudes of private states. Ph.D. thesis, Intelli-
gent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-03), pages 129?
136, Sapporo, Japan.
199
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 347?354, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Paul Hoffmann
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
hoffmanp@cs.pitt.edu
Abstract
This paper presents a new approach to
phrase-level sentiment analysis that first
determines whether an expression is neu-
tral or polar and then disambiguates the
polarity of the polar expressions. With this
approach, the system is able to automat-
ically identify the contextual polarity for
a large subset of sentiment expressions,
achieving results that are significantly bet-
ter than baseline.
1 Introduction
Sentiment analysis is the task of identifying positive
and negative opinions, emotions, and evaluations.
Most work on sentiment analysis has been done at
the document level, for example distinguishing pos-
itive from negative reviews. However, tasks such
as multi-perspective question answering and sum-
marization, opinion-oriented information extraction,
and mining product reviews require sentence-level
or even phrase-level sentiment analysis. For exam-
ple, if a question answering system is to successfully
answer questions about people?s opinions, it must be
able to pinpoint expressions of positive and negative
sentiments, such as we find in the sentences below:
(1) African observers generally approved+ of his
victory while Western governments denounced?
it.
(2) A succession of officers filled the TV
screen to say they supported+ the people and that
the killings were ?not tolerable?.?
(3) ?We don?t hate+ the sinner,? he says,
?but we hate? the sin.?
A typical approach to sentiment analysis is to start
with a lexicon of positive and negative words and
phrases. In these lexicons, entries are tagged with
their a priori prior polarity: out of context, does
the word seem to evoke something positive or some-
thing negative. For example, beautiful has a positive
prior polarity, and horrid has a negative prior polar-
ity. However, the contextual polarity of the phrase
in which a word appears may be different from the
word?s prior polarity. Consider the underlined polar-
ity words in the sentence below:
(4) Philip Clapp, president of the National Environ-
ment Trust, sums up well the general thrust of the
reaction of environmental movements: ?There is no
reason at all to believe that the polluters are sud-
denly going to become reasonable.?
Of these words, ?Trust,? ?well,? ?reason,? and ?rea-
sonable? have positive prior polarity, but they are
not all being used to express positive sentiments.
The word ?reason? is negated, making the contex-
tual polarity negative. The phrase ?no reason at all
to believe? changes the polarity of the proposition
that follows; because ?reasonable? falls within this
proposition, its contextual polarity becomes nega-
tive. The word ?Trust? is simply part of a referring
expression and is not being used to express a senti-
ment; thus, its contextual polarity is neutral. Simi-
larly for ?polluters?: in the context of the article, it
simply refers to companies that pollute. Only ?well?
has the same prior and contextual polarity.
Many things must be considered in phrase-level
sentiment analysis. Negation may be local (e.g., not
good), or involve longer-distance dependencies such
as the negation of the proposition (e.g., does not
look very good) or the negation of the subject (e.g.,
347
no one thinks that it?s good). In addition, certain
phrases that contain negation words intensify rather
than change polarity (e.g., not only good but amaz-
ing). Contextual polarity may also be influenced by
modality (e.g., whether the proposition is asserted to
be real (realis) or not real (irrealis) ? no reason at all
to believe is irrealis, for example); word sense (e.g.,
Environmental Trust versus He has won the peo-
ple?s trust); the syntactic role of a word in the sen-
tence (e.g., polluters are versus they are polluters);
and diminishers such as little (e.g., little truth, lit-
tle threat). (See (Polanya and Zaenen, 2004) for a
more detailed discussion of contextual polarity in-
fluencers.)
This paper presents new experiments in automat-
ically distinguishing prior and contextual polarity.
Beginning with a large stable of clues marked with
prior polarity, we identify the contextual polarity of
the phrases that contain instances of those clues in
the corpus. We use a two-step process that employs
machine learning and a variety of features. The
first step classifies each phrase containing a clue as
neutral or polar. The second step takes all phrases
marked in step one as polar and disambiguates their
contextual polarity (positive, negative, both, or neu-
tral). With this approach, the system is able to auto-
matically identify the contextual polarity for a large
subset of sentiment expressions, achieving results
that are significantly better than baseline. In addi-
tion, we describe new manual annotations of contex-
tual polarity and a successful inter-annotator agree-
ment study.
2 Manual Annotation Scheme
To create a corpus for the experiments below, we
added contextual polarity judgments to existing an-
notations in the Multi-perspective Question Answer-
ing (MPQA) Opinion Corpus1, namely to the an-
notations of subjective expressions2. A subjective
expression is any word or phrase used to express
an opinion, emotion, evaluation, stance, speculation,
1The MPQA Corpus is described in (Wiebe et al, 2005) and
available at nrrc.mitre.org/NRRC/publications.htm.
2In the MPQA Corpus, subjective expressions are direct
subjective expressions with non-neutral expression intensity,
plus all the expressive subjective elements. Please see (Wiebe
et al, 2005) for more details on the existing annotations in the
MPQA Corpus.
etc. A general covering term for such states is pri-
vate state (Quirk et al, 1985). In the MPQA Cor-
pus, subjective expressions of varying lengths are
marked, from single words to long phrases.
For this work, our focus is on sentiment expres-
sions ? positive and negative expressions of emo-
tions, evaluations, and stances. As these are types of
subjective expressions, to create the corpus, we just
needed to manually annotate the existing subjective
expressions with their contextual polarity.
In particular, we developed an annotation
scheme3 for marking the contextual polarity of sub-
jective expressions. Annotators were instructed to
tag the polarity of subjective expressions as positive,
negative, both, or neutral. The positive tag is for
positive emotions (I?m happy), evaluations (Great
idea!), and stances (She supports the bill). The neg-
ative tag is for negative emotions (I?m sad), eval-
uations (Bad idea!), and stances (She?s against the
bill). The both tag is applied to sentiment expres-
sions that have both positive and negative polarity.
The neutral tag is used for all other subjective ex-
pressions: those that express a different type of sub-
jectivity such as speculation, and those that do not
have positive or negative polarity.
Below are examples of contextual polarity anno-
tations. The tags are in boldface, and the subjective
expressions with the given tags are underlined.
(5) Thousands of coup supporters celebrated (posi-
tive) overnight, waving flags, blowing whistles . . .
(6) The criteria set by Rice are the following: the
three countries in question are repressive (nega-
tive) and grave human rights violators (negative)
. . .
(7) Besides, politicians refer to good and evil
(both) only for purposes of intimidation and
exaggeration.
(8) Jerome says the hospital feels (neutral) no dif-
ferent than a hospital in the states.
The annotators were asked to judge the contex-
tual polarity of the sentiment that is ultimately be-
ing conveyed by the subjective expression, i.e., once
the sentence has been fully interpreted. Thus, the
subjective expression, they have not succeeded, and
3The annotation instructions are available at
http://www.cs.pitt.edu/?twilson.
348
will never succeed, was marked as positive in the
sentence, They have not succeeded, and will never
succeed, in breaking the will of this valiant people.
The reasoning is that breaking the will of a valiant
people is negative; hence, not succeeding in break-
ing their will is positive.
3 Agreement Study
To measure the reliability of the polarity annotation
scheme, we conducted an agreement study with two
annotators, using 10 documents from the MPQA
Corpus. The 10 documents contain 447 subjective
expressions. Table 1 shows the contingency table for
the two annotators? judgments. Overall agreement is
82%, with a Kappa (?) value of 0.72.
Neutral Positive Negative Both Total
Neutral 123 14 24 0 161
Positive 16 73 5 2 96
Negative 14 2 167 1 184
Both 0 3 0 3 6
Total 153 92 196 6 447
Table 1: Agreement for Subjective Expressions
(Agreement: 82%, ?: 0.72)
For 18% of the subjective expressions, at least one
annotator used an uncertain tag when marking po-
larity. If we consider these cases to be borderline
and exclude them from the study, percent agreement
increases to 90% and Kappa rises to 0.84. Thus, the
annotator agreement is especially high when both
are certain. (Note that all annotations are included
in the experiments described below.)
4 Corpus
In total, 15,991 subjective expressions from 425
documents (8,984 sentences) were annotated with
contextual polarity as described above. Of these sen-
tences, 28% contain no subjective expressions, 25%
contain only one, and 47% contain two or more. Of
the 4,247 sentences containing two or more subjec-
tive expressions, 17% contain mixtures of positive
and negative expressions, and 62% contain mixtures
of polar (positive/negative/both) and neutral subjec-
tive expressions.
The annotated documents are divided into two
sets. The first (66 documents/1,373 sentences/2,808
subjective expressions) is a development set, used
for data exploration and feature development. We
use the second set (359 documents/7,611 sen-
tences/13,183 subjective expressions) in 10-fold
cross-validation experiments, described below.
5 Prior-Polarity Subjectivity Lexicon
For the experiments in this paper, we use a lexicon of
over 8,000 subjectivity clues. Subjectivity clues are
words and phrases that may be used to express pri-
vate states, i.e., they have subjective usages (though
they may have objective usages as well). For this
work, only single-word clues are used.
To compile the lexicon, we began with a list of
subjectivity clues from (Riloff and Wiebe, 2003).
The words in this list were grouped in previous work
according to their reliability as subjectivity clues.
Words that are subjective in most contexts were
marked strongly subjective (strongsubj), and those
that may only have certain subjective usages were
marked weakly subjective (weaksubj).
We expanded the list using a dictionary and a
thesaurus, and also added words from the General
Inquirer positive and negative word lists (General-
Inquirer, 2000) which we judged to be potentially
subjective. We also gave the new words reliability
tags, either strongsubj or weaksubj.
The next step was to tag the clues in the lexicon
with their prior polarity. For words that came from
positive and negative word lists (General-Inquirer,
2000; Hatzivassiloglou and McKeown, 1997), we
largely retained their original polarity, either posi-
tive or negative. We assigned the remaining words
one of the tags positive, negative, both or neutral.
By far, the majority of clues, 92.8%, are
marked as having either positive (33.1%) or nega-
tive (59.7%) prior polarity. Only a small number of
clues (0.3%) are marked as having both positive and
negative polarity. 6.9% of the clues in the lexicon
are marked as neutral. Examples of these are verbs
such as feel, look, and think, and intensifiers such as
deeply, entirely, and practically. These words are in-
cluded because, although their prior polarity is neu-
tral, they are good clues that a sentiment is being
expressed (e.g., feels slighted, look forward to). In-
cluding them increases the coverage of the system.
349
6 Experiments
The goal of the experiments described below is to
classify the contextual polarity of the expressions
that contain instances of the subjectivity clues in
our lexicon. What the system specifically does is
give each clue instance its own label. Note that the
system does not try to identify expression bound-
aries. Doing so might improve performance and is a
promising avenue for future research.
6.1 Definition of the Gold Standard
We define the gold standard used to train and test the
system in terms of the manual annotations described
in Section 2.
The gold standard class of a clue instance that is
not in a subjective expression is neutral: since the
clue is not even in a subjective expression, it is not
contained in a sentiment expression.
Otherwise, if a clue instance appears in just one
subjective expression (or in multiple subjective ex-
pressions with the same contextual polarity), then
the class assigned to the clue instance is the class
of the subjective expression(s). If a clue appears
in at least one positive and one negative subjective
expression (or in a subjective expression marked as
both), then its class is both. If it is in a mixture of
negative and neutral subjective expressions, its class
is negative; if it is in a mixture of positive and neu-
tral subjective expressions, its class is positive.
6.2 Performance of a Prior-Polarity Classifier
An important question is how useful prior polarity
alone is for identifying contextual polarity. To an-
swer this question, we create a classifier that sim-
ply assumes that the contextual polarity of a clue in-
stance is the same as the clue?s prior polarity, and we
explore the classifier?s performance on the develop-
ment set.
This simple classifier has an accuracy of 48%.
From the confusion matrix given in Table 2, we see
that 76% of the errors result from words with non-
neutral prior polarity appearing in phrases with neu-
tral contextual polarity.
6.3 Contextual Polarity Disambiguation
The fact that words with non-neutral prior polarity
so frequently appear in neutral contexts led us to
Prior-Polarity Classifier
Neut Pos Neg Both Total
Neut 798 784 698 4 2284
Pos 81 371 40 0 492
Gold Neg 149 181 622 0 952
Both 4 11 13 5 33
Total 1032 1347 1373 9 3761
Table 2: Confusion matrix for the prior-polarity
classifier on the development set.
adopt a two-step approach to contextual polarity dis-
ambiguation. For the first step, we concentrate on
whether clue instances are neutral or polar in context
(where polar in context refers to having a contextual
polarity that is positive, negative or both). For the
second step, we take all clue instances marked as
polar in step one, and focus on identifying their con-
textual polarity. For both steps, we develop classi-
fiers using the BoosTexter AdaBoost.HM (Schapire
and Singer, 2000) machine learning algorithm with
5000 rounds of boosting. The classifiers are evalu-
ated in 10-fold cross-validation experiments.
6.3.1 Neutral-Polar Classification
The neutral-polar classifier uses 28 features, listed
in Table 3.
Word Features: Word context is a bag of three
word tokens: the previous word, the word itself, and
the next word. The prior polarity and reliability
class are indicated in the lexicon.
Modification Features: These are binary rela-
tionship features. The first four involve relationships
with the word immediately before or after: if the
word is a noun preceded by an adjective, if the pre-
ceding word is an adverb other than not, if the pre-
ceding word is an intensifier, and if the word itself
is an intensifier. A word is considered an intensifier
if it appears in a list of intensifiers and if it precedes
a word of the appropriate part-of-speech (e.g., an in-
tensifier adjective must come before a noun).
The modify features involve the dependency parse
tree for the sentence, obtained by first parsing the
sentence (Collins, 1997) and then converting the tree
into its dependency representation (Xia and Palmer,
2001). In a dependency representation, every node
in the tree structure is a surface word (i.e., there are
no abstract nodes such as NP or VP). The edge be-
tween a parent and a child specifies the grammatical
relationship between the two words. Figure 1 shows
350
Word Features Sentence Features Structure Features
word token strongsubj clues in current sentence: count in subject: binary
word part-of-speech strongsubj clues in previous sentence: count in copular: binary
word context strongsubj clues in next sentence: count in passive: binary
prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count
reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count
Modification Features weaksubj clues in next sentence: count Document Feature
preceeded by adjective: binary adjectives in sentence: count document topic
preceeded by adverb (other than not): binary adverbs in sentence (other than not): count
preceeded by intensifier: binary cardinal number in sentence: binary
is intensifier: binary pronoun in sentence: binary
modifies strongsubj: binary modal in sentence (other than will): binary
modifies weaksubj: binary
modified by strongsubj: binary
modified by weaksubj: binary
Table 3: Features for neutral-polar classification
The human rights
report
a
poses
substantial
challenge
to
USthe
interpretation
of
good and evil
det det
det
adj adj
objsubj
mod
mod
conj conjpobj
pobj
p
p
(pos) (neg)
(pos)
(neg)
(pos)
Figure 1: The dependency tree for the sentence The human
rights report poses a substantial challenge to the US interpre-
tation of good and evil. Prior polarity is marked in parentheses
for words that match clues from the lexicon.
an example. The modifies strongsubj/weaksubj fea-
tures are true if the word and its parent share an
adj, mod or vmod relationship, and if its parent is
an instance of a clue from the lexicon with strong-
subj/weaksubj reliability. The modified by strong-
subj/weaksubj features are similar, but look for rela-
tionships and clues in the word?s children.
Structure Features: These are binary features
that are determined by starting with the word in-
stance and climbing up the dependency parse tree
toward the root, looking for particular relationships,
words, or patterns. The in subject feature is true if
we find a subj relationship. The in copular feature is
true if in subject is false and if a node along the path
is both a main verb and a copular verb. The in pas-
sive features is true if a passive verb pattern is found
on the climb.
Sentence Features: These are features that were
found useful for sentence-level subjectivity classifi-
cation by Wiebe and Riloff (2005). They include
counts of strongsubj and weaksubj clues in the cur-
rent, previous and next sentences, counts of adjec-
tives and adverbs other than not in the current sen-
tence, and binary features to indicate whether the
sentence contains a pronoun, a cardinal number, and
a modal other than will.
Document Feature: There is one document fea-
ture representing the topic of the document. A doc-
ument may belong to one of 15 topics ranging from
specific (e.g., the 2002 presidential election in Zim-
babwe) to more general (e.g., economics) topics.
Table 4 gives neutral-polar classification results
for the 28-feature classifier and two simpler classi-
fiers that provide our baselines. The first row in the
table lists the results for a classifier that uses just
one feature, the word token. The second row shows
the results for a classifier that uses both the word to-
ken and the word?s prior polarity as features. The
results for the 28-feature classifier are listed in the
last row. The 28-feature classifier performs signifi-
cantly better (1-tailed t-test, p ? .05) than the two
simpler classifiers, as measured by accuracy, polar
F-measure, and neutral F-measure (? = 1). It has an
accuracy of 75.9%, with a polar F-measure of 63.4
and a neutral F-measure of 82.1.
Focusing on the metrics for polar expressions, it?s
interesting to note that using just the word token as a
feature produces a classifier with a precision slightly
better than the 28-feature classifier, but with a recall
that is 20% lower. Adding a feature for the prior
351
Word Features
word token
word prior polarity: positive, negative, both, neutral
Polarity Features
negated: binary
negated subject: binary
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conj polarity: positive, negative, neutral, both, notmod
general polarity shifter: binary
negative polarity shifter: binary
positive polarity shifter: binary
Table 6: Features for polarity classification
polarity improves recall so that it is only 4.4% lower,
but this hurts precision, which drops to 4.2% lower
than the 28-feature classifier?s precision. It is only
with all the features that we get the best result, good
precision with the highest recall.
The clues in the prior-polarity lexicon have
19,506 instances in the test set. According to the
28-feature neutral-polar classifier, 5,671 of these in-
stances are polar in context. It is these clue instances
that are passed on to the second step in the contex-
tual disambiguation process, polarity classification.
6.3.2 Polarity Classification
Ideally, this second step in the disambiguation
process would be a three-way classification task, de-
termining whether the contextual polarity is posi-
tive, negative or both. However, although the major-
ity of neutral expressions have been filtered out by
the neutral-polar classification in step one, a number
still remain. So, for this step, the polarity classifica-
tion task remains four-way: positive, negative, both,
and neutral.
Table 6 lists the features used by the polarity clas-
sifier. Word token and word prior polarity are un-
changed from the neutral-polar classifier. Negated
is a binary feature that captures whether the word is
being locally negated: its value is true if a negation
word or phrase is found within the four preceeding
words or in any of the word?s children in the de-
pendency tree, and if the negation word is not in a
phrase that intensifies rather than negates (e.g., not
only). The negated subject feature is true if the sub-
ject of the clause containing the word is negated.
The modifies polarity, modified by polarity, and
conj polarity features capture specific relationships
between the word instance and other polarity words
it may be related to. If the word and its parent in
the dependency tree share an obj, adj, mod, or vmod
relationship, the modifies polarity feature is set to
the prior polarity of the word?s parent (if the parent
is not in our prior-polarity lexicon, its prior polarity
is set to neutral). The modified by polarity feature
is similar, looking for adj, mod, and vmod relation-
ships and polarity clues within the word?s children.
The conj polarity feature determines if the word is
in a conjunction. If so, the value of this feature is its
sibling?s prior polarity (as above, if the sibling is not
in the lexicon, its prior polarity is neutral). Figure 1
helps to illustrate these features: modifies polarity is
negative for the word ?substantial,? modified by po-
larity is positive for the word ?challenge,? and conj
polarity is negative for the word ?good.?
The last three polarity features look in a window
of four words before, searching for the presence of
particular types of polarity influencers. General po-
larity shifters reverse polarity (e.g., little truth, lit-
tle threat). Negative polarity shifters typically make
the polarity of an expression negative (e.g., lack of
understanding). Positive polarity shifters typically
make the polarity of an expression positive (e.g.,
abate the damage).
The polarity classification results for this second
step in the contextual disambiguation process are
given in Table 5. Also listed in the table are results
for the two simple classifiers that provide our base-
lines. The first line in Table 5 lists the results for
the classifier that uses just one feature, the word to-
ken. The second line shows the results for the clas-
sifier that uses both the word token and the word?s
prior polarity as features. The last line shows the
results for the polarity classifier that uses all 10 fea-
tures from Table 6.
Mirroring the results from step one, the more
complex classifier performs significantly better than
the simpler classifiers, as measured by accuracy
and all of the F-measures. The 10-feature classi-
fier achieves an accuracy of 65.7%, which is 4.3%
higher than the more challenging baseline provided
by the word + prior polarity classifier. Positive F-
measure is 65.1 (5.7% higher); negative F-measure
is 77.2 (2.3% higher); and neutral F-measure is 46.2
(13.5% higher).
Focusing on the metrics for positive and negative
expressions, we again see that the simpler classifiers
352
Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut F
word token 73.6 45.3 72.2 55.7 89.9 74.0 81.2
word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.7
28 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1
Table 4: Results for Step 1 Neutral-Polar Classification
Positive Negative Both Neutral
Acc Rec Prec F Rec Prec F Rec Prec F Rec Prec F
word token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7
word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.7
10 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2
Table 5: Results for Step 2 Polarity Classification.
Experiment Features Removed
AB1 negated, negated subject
AB2 modifies polarity, modified by polarity
AB3 conj polarity
AB4 general, negative, and positive polarity shifters
Table 7: Features for polarity classification
take turns doing better or worse for precision and
recall. Using just the word token, positive preci-
sion is slightly higher than for the 10-feature clas-
sifier, but positive recall is 11.6% lower. Add the
prior polarity, and positive recall improves, but at
the expense of precision, which is 12.6% lower than
for the 10-feature classifier. The results for negative
expressions are similar. The word-token classifier
does well on negative recall but poorly on negative
precision. When prior polarity is added, negative
recall improves but negative precision drops. It is
only with the addition of the polarity features that we
achieve both higher precisions and higher recalls.
To explore how much the various polarity features
contribute to the performance of the polarity classi-
fier, we perform four experiments. In each experi-
ment, a different set of polarity features is excluded,
and the polarity classifier is retrained and evaluated.
Table 7 lists the features that are removed for each
experiment.
The only significant difference in performance in
these experiments is neutral F-measure when the
modification features (AB2) are removed. These
ablation experiments show that the combination of
features is needed to achieve significant results over
baseline for polarity classification.
7 Related Work
Much work on sentiment analysis classifies docu-
ments by their overall sentiment, for example deter-
mining whether a review is positive or negative (e.g.,
(Turney, 2002; Dave et al, 2003; Pang and Lee,
2004; Beineke et al, 2004)). In contrast, our ex-
periments classify individual words and phrases. A
number of researchers have explored learning words
and phrases with prior positive or negative polarity
(another term is semantic orientation) (e.g., (Hatzi-
vassiloglou and McKeown, 1997; Kamps and Marx,
2002; Turney, 2002)). In contrast, we begin with
a lexicon of words with established prior polarities,
and identify the contextual polarity of phrases in
which instances of those words appear in the cor-
pus. To make the relationship between that task
and ours clearer, note that some word lists used to
evaluate methods for recognizing prior polarity are
included in our prior-polarity lexicon (General In-
quirer lists (General-Inquirer, 2000) used for evalu-
ation by Turney, and lists of manually identified pos-
itive and negative adjectives, used for evaluation by
Hatzivassiloglou and McKeown).
Some research classifies the sentiments of sen-
tences. Yu and Hatzivassiloglou (2003), Kim and
Hovy (2004), Hu and Liu (2004), and Grefenstette et
al. (2001)4 all begin by first creating prior-polarity
lexicons. Yu and Hatzivassiloglou then assign a sen-
timent to a sentence by averaging the prior semantic
orientations of instances of lexicon words in the sen-
tence. Thus, they do not identify the contextual po-
larity of individual phrases containing clues, as we
4In (Grefenstette et al, 2001), the units that are classified are
fixed windows around named entities rather than sentences.
353
do in this paper. Kim and Hovy, Hu and Liu, and
Grefenstette et al multiply or count the prior po-
larities of clue instances in the sentence. They also
consider local negation to reverse polarity. However,
they do not use the other types of features in our
experiments, and they restrict their tags to positive
and negative (excluding our both and neutral cate-
gories). In addition, their systems assign one sen-
timent per sentence; our system assigns contextual
polarity to individual expressions. As seen above,
sentences often contain more than one sentiment ex-
pression.
Nasukawa, Yi, and colleagues (Nasukawa and Yi,
2003; Yi et al, 2003) classify the contextual polarity
of sentiment expressions, as we do. Thus, their work
is probably most closely related to ours. They clas-
sify expressions that are about specific items, and
use manually developed patterns to classify polarity.
These patterns are high-quality, yielding quite high
precision, but very low recall. Their system classi-
fies a much smaller proportion of the sentiment ex-
pressions in a corpus than ours does.
8 Conclusions
In this paper, we present a new approach to
phrase-level sentiment analysis that first determines
whether an expression is neutral or polar and then
disambiguates the polarity of the polar expressions.
With this approach, we are able to automatically
identify the contextual polarity for a large subset of
sentiment expressions, achieving results that are sig-
nificantly better than baseline.
9 Acknowledgments
This work was supported in part by the NSF under
grant IIS-0208798 and by the Advanced Research
and Development Activity (ARDA).
References
P. Beineke, T. Hastie, and S. Vaithyanathan. 2004. The sen-
timental factor: Improving review classification via human-
provided information. In ACL-2004.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifica-
tion of product reviews. In WWW-2003.
The General-Inquirer. 2000.
http://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.
G. Grefenstette, Y. Qu, J.G. Shanahan, and D.A. Evans. 2001.
Coupling niche browsers and affect analysis for an opinion
mining application. In RIAO-2004.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting the
semantic orientation of adjectives. In ACL-1997.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In KDD-2004.
J. Kamps and M. Marx. 2002. Words with attitude. In 1st
International WordNet Conference.
S-M. Kim and E. Hovy. 2004. Determining the sentiment of
opinions. In Coling 2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Capturing
favorability using natural language processing. In K-CAP
2003.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In ACL-2004.
L. Polanya and A. Zaenen. 2004. Contextual valence shifters.
In Working Notes ? Exploring Attitude and Affect in Text
(AAAI Spring Symposium Series).
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
P. Turney. 2002. Thumbs up or thumbs down? Semantic orien-
tation applied to unsupervised classification of reviews. In
ACL-2002.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expres-
sions of opinions and emotions in language. Language Re-
sources and Evalution (formerly Computers and the Human-
ities), 1(2).
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Senti-
ment analyzer: Extracting sentiments about a given topic us-
ing natural language processing techniques. In IEEE ICDM-
2003.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In EMNLP-2003.
354
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 923?930, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Multi-Perspective Question Answering Using the OpQA Corpus
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
We investigate techniques to support the
answering of opinion-based questions.
We first present the OpQA corpus of opin-
ion questions and answers. Using the cor-
pus, we compare and contrast the proper-
ties of fact and opinion questions and an-
swers. Based on the disparate characteris-
tics of opinion vs. fact answers, we argue
that traditional fact-based QA approaches
may have difficulty in an MPQA setting
without modification. As an initial step
towards the development of MPQA sys-
tems, we investigate the use of machine
learning and rule-based subjectivity and
opinion source filters and show that they
can be used to guide MPQA systems.
1 Introduction
Much progress has been made in recent years in
automatic, open-domain question answering (e.g.,
Voorhees (2001), Voorhees (2002), Voorhees and
Buckland (2003)). The bulk of the research in this
area, however, addresses fact-based questions like:
?When did McDonald?s open its first restaurant??
or ?What is the Kyoto Protocol??. To date, how-
ever, relatively little research been done in the area
of Multi-Perspective Question Answering (MPQA),
which targets questions of the following sort:
? How is Bush?s decision not to ratify the Kyoto Protocol
looked upon by Japan and other US allies?
? How do the Chinese regard the human rights record of the
United States?
In comparison to fact-based question answering
(QA), researchers understand far less about the prop-
erties of questions and answers in MPQA, and have
yet to develop techniques to exploit knowledge of
those properties. As a result, it is unclear whether
approaches that have been successful in the domain
of fact-based QA will work well for MPQA.
We first present the OpQA corpus of opinion ques-
tions and answers. Using the corpus, we compare
and contrast the properties of fact and opinion ques-
tions and answers. We find that text spans identi-
fied as answers to opinion questions: (1) are approx-
imately twice as long as those of fact questions, (2)
are much more likely (37% vs. 9%) to represent par-
tial answers rather than complete answers, (3) vary
much more widely with respect to syntactic cate-
gory ? covering clauses, verb phrases, prepositional
phrases, and noun phrases; in contrast, fact answers
are overwhelming associated with noun phrases, and
(4) are roughly half as likely to correspond to a sin-
gle syntactic constituent type (16-38% vs. 31-53%).
Based on the disparate characteristics of opinion
vs. fact answers, we argue that traditional fact-based
QA approaches may have difficulty in an MPQA
setting without modification. As one such modifi-
cation, we propose that MPQA systems should rely
on natural language processing methods to identify
information about opinions. In experiments in opin-
ion question answering using the OpQA corpus, we
find that filtering potential answers using machine
learning and rule-based NLP opinion filters substan-
tially improves the performance of an end-to-end
MPQA system according to both a mean reciprocal
rank (MRR) measure (0.59 vs. a baseline of 0.42)
923
and a metric that determines the mean rank of the
first correct answer (MRFA) (26.2 vs. a baseline of
61.3). Further, we find that requiring opinion an-
swers to match the requested opinion source (e.g.,
does <source> approve of the Kyoto Protocol) dra-
matically improves the performance of the MPQA
system on the hardest questions in the corpus.
The remainder of the paper is organized as fol-
lows. In the next section we summarize related
work. Section 3 describes the OpQA corpus. Sec-
tion 4 uses the OpQA corpus to identify poten-
tially problematic issues for handling opinion vs.
fact questions. Section 5 briefly describes an opin-
ion annotation scheme used in the experiments. Sec-
tions 6 and 7 explore the use of opinion information
in the design of MPQA systems.
2 Related Work
There is a growing interest in methods for the auto-
matic identification and extraction of opinions, emo-
tions, and sentiments in text. Much of the relevant
research explores sentiment classification, a text cat-
egorization task in which the goal is to assign to
a document either positive (?thumbs up?) or nega-
tive (?thumbs down?) polarity (e.g. Das and Chen
(2001), Pang et al (2002), Turney (2002), Dave et
al. (2003), Pang and Lee (2004)). Other research
has concentrated on analyzing opinions at, or below,
the sentence level. Recent work, for example, indi-
cates that systems can be trained to recognize opin-
ions, their polarity, their source, and their strength
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et al
(2004), Pang and Lee (2004), Wilson et al (2004),
Yu and Hatzivassiloglou (2003), Wiebe and Riloff
(2005)).
Related work in the area of corpus development
includes Wiebe et al?s (2005) opinion annotation
scheme to identify subjective expressions ? expres-
sions used to express opinions, emotions, sentiments
and other private states in text. Wiebe et al have
applied the annotation scheme to create the MPQA
corpus consisting of 535 documents manually an-
notated for phrase-level expressions of opinion. In
addition, the NIST-sponsored TREC evaluation has
begun to develop data focusing on opinions ? the
2003 Novelty Track features a task that requires sys-
tems to identify opinion-oriented documents w.r.t. a
specific issue (Voorhees and Buckland, 2003).
While all of the above work begins to bridge
the gap between text categorization and question
answering, none of the approaches have been em-
ployed or evaluated in the context of MPQA.
3 OpQA Corpus
To support our research in MPQA, we created the
OpQA corpus of opinion and fact questions and an-
swers. Additional details on the construction of the
corpus as well as results of an interannotator agree-
ment study can be found in Stoyanov et al (2004).
3.1 Documents and Questions
The OpQA corpus consists of 98 documents that ap-
peared in the world press between June 2001 and
May 2002. All documents were taken from the
aforementioned MPQA corpus (Wilson and Wiebe,
2003)1 and are manually annotated with phrase-
level opinion information, following the annotation
scheme of Wiebe et al (2005), which is briefly
summarized in Section 5. The documents cover
four general (and controversial) topics: President
Bush?s alternative to the Kyoto protocol (kyoto); the
US annual human rights report (humanrights); the
2002 coup d?etat in Venezuela (venezuela); and the
2002 elections in Zimbabwe and Mugabe?s reelec-
tion (mugabe). Each topic is covered by between 19
and 33 documents that were identified automatically
via IR methods.
Both fact and opinion questions for each topic
were added to the OpQA corpus by a volunteer not
associated with the current project. The volunteer
was provided with a set of instructions for creat-
ing questions together with two documents on each
topic selected at random. He created between six
and eight questions on each topic, evenly split be-
tween fact and opinion. The 30 questions are given
in Table 1 sorted by topic.
3.2 Answer annotations
Answer annotations were added to the corpus by two
annotators according to a set of annotation instruc-
1The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
The OpQA corpus is available upon request.
924
Kyoto
1 f What is the Kyoto Protocol about?
2 f When was the Kyoto Protocol adopted?
3 f Who is the president of the Kiko Network?
4 f What is the Kiko Network?
5 o Does the president of the Kiko Network approve of the US action concerning the Kyoto Protocol?
6 o Are the Japanese unanimous in their opinion of Bush?s position on the Kyoto Protocol?
7 o How is Bush?s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?
8 o How do European Union countries feel about the US opposition to the Kyoto protocol?
Human Rights
1 f What is the murder rate in the United States?
2 f What country issues an annual report on human rights in the United States?
3 o How do the Chinese regard the human rights record of the United States?
4 f Who is Andrew Welsdan?
5 o What factors influence the way in which the US regards the human rights records of other nations?
6 o Is the US Annual Human Rights Report received with universal approval around the world?
Venezuela
1 f When did Hugo Chavez become President?
2 f Did any prominent Americans plan to visit Venezuela immediately following the 2002 coup?
3 o Did anything surprising happen when Hugo Chavez regained power in Venezuela after he was
removed by a coup?
4 o Did most Venezuelans support the 2002 coup?
5 f Which governmental institutions in Venezuela were dissolved by the leaders of the 2002 coup?
6 o How did ordinary Venezuelans feel about the 2002 coup and subsequent events?
7 o Did America support the Venezuelan foreign policy followed by Chavez?
8 f Who is Vice-President of Venezuela?
Mugabe
1 o What was the American and British reaction to the reelection of Mugabe?
2 f Where did Mugabe vote in the 2002 presidential election?
3 f At which primary school had Mugabe been expected to vote in the 2002 presidential election?
4 f How long has Mugabe headed his country?
5 f Who was expecting Mugabe at Mhofu School for the 2002 election?
6 o What is the basis for the European Union and US critical attitude and adversarial action toward
Mugabe?
7 o What did South Africa want Mugabe to do after the 2002 election?
8 o What is Mugabe?s opinion about the West?s attitude and actions towards the 2002 Zimbabwe elec-
tion?
Table 1: Questions in the OpQA collection by topic.
f in column 1 indicates a fact question; o, an opinion
question.
tions.2 Every text segment that contributes to an
answer to any of the 30 questions is annotated as
an answer. In particular, answer annotations include
segments that constitute a partial answer. Partial an-
swers either (1) lack the specificity needed to consti-
tute a full answer (e.g., ?before May 2004? partially
answers the question When was the Kyoto protocol
ratified? when a specific date is known) or (2) need
to be combined with at least one additional answer
segment to fully answer the question (e.g., the ques-
tion Are the Japanese unanimous in their opposition
of Bush?s position on the Kyoto protocol? is an-
swered only partially by a segment expressing a sin-
gle opinion). In addition, annotators mark the min-
imum answer spans (e.g., ?a Tokyo organization,?
vs. ?a Tokyo organization representing about 150
Japanese groups?).
4 Characteristics of opinion answers
Next, we use the OpQA corpus to analyze and com-
pare the characteristics of fact vs. opinion questions.
Based on our findings, we believe that QA systems
based solely on traditional QA techniques are likely
2The annotation instructions are available
at http://www.cs.cornell.edu/ ves/
Publications/publications.htm.
to be less effective at MPQA than they are at tradi-
tional fact-based QA.
4.1 Traditional QA architectures
Despite the wide variety of approaches implied by
modern QA systems, almost all systems rely on the
following two steps (subsystems), which have em-
pirically proven to be effective:
? IR module. The QA system invokes an IR subsystem that
employs traditional text similarity measures (e.g., tf/idf)
to retrieve and rank document fragments (sentences or
paragraphs) w.r.t. the question (query).
? Linguistic filters. QA systems employ a set of filters
and text processing components to discard some docu-
ment fragments. The following filters have empirically
proven to be effective and are used universally:
Semantic filters prefer an answer segment that matches
the semantic class(es) associated with the question type
(e.g., date or time for when questions; person or organi-
zation for who questions).
Syntactic filters are also configured on the type of ques-
tion. The most common and effective syntactic filters se-
lect a specific constituent (e.g., noun phrase) according to
the question type (e.g., who question).
QA systems typically interleave the above two
subsystems with a variety of different processing
steps of both the question and the answer. The goal
of the processing is to identify text fragments that
contain an answer to the question. Typical QA sys-
tems do not perform any further text processing;
they return the text fragment as it occurred in the
text. 3
4.2 Corpus-based analysis of opinion answers
We hypothesize that QA systems that conform to
this traditional architecture will have difficulty han-
dling opinion questions without non-trivial modifi-
cation. In support of this hypothesis, we provide
statistics from the OpQA corpus to illustrate some of
the characteristics that distinguish answers to opin-
ion vs. fact questions, and discuss their implications
for a traditional QA system architecture.
Answer length. We see in Table 2 that the aver-
age length of opinion answers in the OpQA corpus
3This architecture is seen mainly in QA systems designed
for TREC?s ?factoid? and ?list? QA tracks. Systems competing
in the relatively new ?definition? or ?other? tracks have begun
to introduce new approaches. However, most such systems still
rely on the IR step and return the text fragment as it occurred in
the text.
925
Number of answers Length Number of partials
fact 124 5.12 12 (9.68%)
opinion 415 9.24 154 (37.11%)
Table 2: Number of answers, average answer length
(in tokens), and number of partial answers for
fact/opinion questions.
is 9.24 tokens, almost double that of fact answers.
Unfortunately, longer answers could present prob-
lems for some traditional QA systems. In particu-
lar, some of the more sophisticated algorithms that
perform additional processing steps such as logi-
cal verifiers (Moldovan et al, 2002) may be less ac-
curate or computationally infeasible for longer an-
swers. More importantly, longer answers are likely
to span more than a single syntactic constituent, ren-
dering the syntactic filters, and very likely the se-
mantic filters, less effective.
Partial answers. Table 2 also shows that over 37%
of the opinion answers were marked as partial vs.
9.68% of the fact answers. The implications of par-
tial answers for the traditional QA architecture are
substantial: an MPQA system will require an an-
swer generator to (1) distinguish between partial
and full answers; (2) recognize redundant partial an-
swers; (3) identify which subset of the partial an-
swers, if any, constitutes a full answer; (4) determine
whether additional documents need to be examined
to find a complete answer; and (5) asemble the final
answer from partial pieces of information.
Syntactic constituent of the answer. As discussed
in Section 4.1, traditional QA systems rely heav-
ily on the predicted syntactic and semantic class of
the answer. Based on answer lengths, we specu-
lated that opinion answers are unlikely to span a sin-
gle constituent and/or semantic class. This specula-
tion is confirmed by examining the phrase type as-
sociated with OpQA answers using Abney?s (1996)
CASS partial parser.4 For each question, we count
the number of times an answer segment for the ques-
tion (in the manual annotations) matches each con-
stituent type. We consider four constituent types
? noun phrase (n), verb phrase (v), prepositional
phrase (p), and clause (c) ? and three matching cri-
teria:
4The parser is available from
http://www.vinartus.net/spa/.
Fact OpinionQues- # of Matching Criteria syn Ques- # of Matching Criteria syn
tion answers ex up up/dn type tion answers ex up up/dn type
H 1 1 0 0 0 H 3 15 5 5 5 c
H 2 4 2 2 2 n H 5 24 5 5 10 n
H 4 1 0 0 0 H 6 123 17 23 52 n
K 1 48 13 14 24 n K 5 3 0 0 1
K 2 38 13 13 19 n K 6 34 6 5 12 c
K 3 1 1 1 1 c n K 7 55 9 8 19 c
K 4 2 1 1 1 n K 8 25 4 4 10 v
M 2 3 0 0 1 M 1 74 10 12 29 v
M 3 1 0 0 1 M 6 12 3 5 7 n
M 4 10 2 2 5 n M 7 1 0 0 0
M 5 3 1 1 2 c M 8 3 0 0 1V 1 4 3 3 4 n V 3 1 1 0 1 cV 2 1 1 1 1 n V 4 13 2 2 2 cV 5 3 0 1 1 V 6 9 2 2 5 c nV 8 4 2 4 4 n V 7 23 3 1 5
Cov- 124 39 43 66 Cov- 415 67 70 159
erage 31% 35% 53% erage 16% 17% 38%
Table 3: Syntactic Constituent Type for Answers in
the OpQA Corpus
1. The exact match criterion is satisfied only by answer seg-
ments whose spans exactly correspond to a constituent in
the CASS output.
2. The up criterion considers an answer to match a CASS
constituent if the constituent completely contains the an-
swer and no more than three additional (non-answer) to-
kens.
3. The up/dn criterion considers an answer to match a
CASS constituent if it matches according to the up crite-
rion or if the answer completely contains the constituent
and no more than three additional tokens.
The counts for the analysis of answer segment
syntactic type for fact vs. opinion questions are sum-
marized in Table 3. Results for the 15 fact ques-
tions are shown in the left half of the table, and
for the 15 opinion questions in the right half. The
leftmost column in each half provides the question
topic and number, and the second column indicates
the total number of answer segments annotated for
the question. The next three columns show, for each
of the ex, up, and up/dn matching criteria, respec-
tively, the number of annotated answer segments
that match the majority syntactic type among an-
swer segments for that question/criterion pair. Us-
ing a traditional QA architecture, the MPQA sys-
tem might filter answers based on this majority type.
The syn type column indicates the majority syntac-
tic type using the exact match criterion; two values
in the column indicate a tie for majority syntactic
type, and an empty syntactic type indicates that no
answer exactly matched any of the four constituent
types. With only a few exceptions, the up and up/dn
matching criteria agreed in majority syntactic type.
Results in Table 3 show a significant disparity be-
tween fact and opinion questions. For fact ques-
926
tions, the syntactic type filter would keep 31%, 35%,
or 53% of the correct answers, depending on the
matching criterion. For opinion questions, there is
unfortunately a two-fold reduction in the percentage
of correct answers that would remain after filtering
? only 16%, 17% or 38%, depending on the match-
ing criterion. More importantly, the majority syntac-
tic type among answers for fact questions is almost
always a noun phrase, while no single constituent
type emerges as a useful syntactic filter for opinion
questions (see the syn phrase columns in Table 3).
Finally, because semantic class information is gener-
ally tied to a particular syntactic category, the effec-
tiveness of traditional semantic filters in the MPQA
setting is unclear.
In summary, identifying answers to questions in
an MPQA setting within a traditional QA architec-
ture will be difficult. First, the implicit and explicit
assumptions inherent in standard linguistic filters are
consistent with the characteristics of fact- rather than
opinion-oriented QA. In addition, the presence of
relatively long answers and partial answers will re-
quire a much more complex answer generator than
is typically present in current QA systems.
In Sections 6 and 7, we propose initial steps to-
wards modifying the traditional QA architecture for
use in MPQA. In particular, we propose and evaluate
two types of opinion filters for MPQA: subjectiv-
ity filters and opinion source filters. Both types of
linguistic filters rely on phrase-level and sentence-
level opinion information, which has been manually
annotated for our corpus; the next section briefly de-
scribes the opinion annotation scheme.
5 Manual Opinion Annotations
Documents in our OpQA corpus come from the
larger MPQA corpus, which contains manual opin-
ion annotations. The annotation framework is de-
scribed in detail in (Wiebe et al, 2005). Here we
give a high-level overview.
The annotation framework provides a basis for
subjective expressions: expressions used to express
opinions, emotions, and sentiments. The framework
allows for the annotation of both directly expressed
private states (e.g., afraid in the sentence ?John is
afraid that Sue might fall,?) and opinions expressed
by the choice of words and style of language (e.g.,
it is about time and oppression in the sentence ?It is
about time that we end Saddam?s oppression?). In
addition, the annotations include several attributes,
including the intensity (with possible values low,
medium, high, and extreme) and the source of the
private state. The source of a private state is the per-
son or entity who holds or experiences it.
6 Subjectivity Filters for MPQA Systems
This section describes three subjectivity filters
based on the above opinion annotation scheme. Be-
low (in Section 6.3), the filters are used to remove
fact sentences from consideration when answering
opinion questions, and the OpQA corpus is used to
evaluate their effectiveness.
6.1 Manual Subjectivity Filter
Much previous research on automatic extraction of
opinion information performed classifications at the
sentence level. Therefore, we define sentence-level
opinion classifications in terms of the phrase-level
annotations. For our gold standard of manual opin-
ion classifications (dubbed MANUAL for the rest of
the paper) we will follow Riloff and Wiebe?s (2003)
convention (also used by Wiebe and Riloff (2005))
and consider a sentence to be opinion if it contains
at least one opinion of intensity medium or higher,
and to be fact otherwise.
6.2 Two Automatic Subjectivity Filters
As discussed in section 2, several research efforts
have attempted to perform automatic opinion clas-
sification on the clause and sentence level. We in-
vestigate whether such information can be useful for
MPQA by using the automatic sentence level opin-
ion classifiers of Riloff and Wiebe (2003) and Wiebe
and Riloff (2005).
Riloff and Wiebe (2003) use a bootstrapping al-
gorithm to perform a sentence-based opinion classi-
fication on the MPQA corpus. They use a set of high
precision subjectivity and objectivity clues to iden-
tify subjective and objective sentences. This data
is then used in an algorithm similar to AutoSlog-
TS (Riloff, 1996) to automatically identify a set of
extraction patterns. The acquired patterns are then
used iteratively to identify a larger set of subjective
and objective sentences. In our experiments we use
927
precision recall F
MPQA corpus RULEBASED 90.4 34.2 46.6
NAIVE BAYES 79.4 70.6 74.7
Table 4: Precision, recall, and F-measure for the two
classifiers.
the classifier that was created by the reimplemen-
tation of this bootstrapping process in Wiebe and
Riloff (2005). We will use RULEBASED to denote
the opinion information output by this classifier.
In addition, Wiebe and Riloff used the RULE-
BASED classifier to produce a labeled data set for
training. They trained a Naive Bayes subjectivity
classifier on the labeled set. We will use NAIVE
BAYES to refer to Wiebe and Riloff?s naive Bayes
classifier.5 Table 4 shows the performance of the
two classifiers on the MPQA corpus as reported by
Wiebe and Riloff.
6.3 Experiments
We performed two types of experiments using the
subjectivity filters.
6.3.1 Answer rank experiments
Our hypothesis motivating the first type of exper-
iment is that subjectivity filters can improve the an-
swer identification phase of an MPQA system. We
implement the IR subsystem of a traditional QA sys-
tem, and apply the subjectivity filters to the IR re-
sults. Specifically, for each opinion question in the
corpus 6 , we do the following:
1. Split all documents in our corpus into sentences.
2. Run an information retrieval algorithm7 on the set of all
sentences using the question as the query to obtain a
ranked list of sentences.
3. Apply a subjectivity filter to the ranked list to remove all
fact sentences from the ranked list.
We test each of the MANUAL, RULEBASED, and
NAIVE BAYES subjectivity filters. We compare the
rank of the first answer to each question in the
5Specifically, the one they label Naive Bayes 1.
6We do not evaluate the opinion filters on the 15 fact ques-
tions. Since opinion sentences are defined as containing at least
one opinion of intensity medium or higher, opinion sentences
can contain factual information and sentence-level opinion fil-
ters are not likely to be effective for fact-based QA.
7We use the Lemur toolkit?s standard tf.idf implementation
available from http://www.lemurproject.org/.
Topic Qnum Baseline Manual NaiveBayes Rulebased
Kyoto 5 1 1 1 1
6 5 4 4 3
7 1 1 1 1
8 1 1 1 1
Human 3 1 1 1 1
Rights 5 10 6 7 5
6 1 1 1 1
Venezuela 3 106 81 92 35
4 3 2 3 1
6 1 1 1 1
7 3 3 3 2
Mugabe 1 2 2 2 2
6 7 5 5 4
7 447 291 317 153
8 331 205 217 182
MRR : 0.4244 0.5189 0.5078 0.5856
MRFA: 61.3333 40.3333 43.7333 26.2
Table 5: Results for the subjectivity filters.
ranked list before the filter is applied, with the rank
of the first answer to the question in the ranked list
after the filter is applied.
Results. Results for the opinion filters are compared
to a simple baseline, which performs the informa-
tion retrieval step with no filtering. Table 5 gives the
results on the 15 opinion questions for the baseline
and each of the three subjectivity filters. The table
shows two cumulative measures ? the mean recip-
rocal rank (MRR) 8 and the mean rank of the first
answer (MRFA). 9
Table 5 shows that all three subjectivity filters out-
perform the baseline: for all three filters, the first
answer in the filtered results for all 15 questions is
ranked at least as high as in the baseline. As a result,
the three subjectivity filters outperform the baseline
in both MRR and MRFA. Surprisingly, the best per-
forming subjectivity filter is RULEBASED, surpass-
ing the gold standard MANUAL, both in MRR (0.59
vs. 0.52) and MRFA (40.3 vs. 26.2). Presum-
ably, the improvement in performance comes from
the fact that RULEBASED identifies subjective sen-
tences with the highest precision (and lowest recall).
Thus, the RULEBASED subjectivity filter discards
non-subjective sentences most aggressively.
6.3.2 Answer probability experiments
The second experiment, answer probability, be-
gins to explore whether opinion information can be
8The MRR is computed as the average of 1/r, where r is
the rank of the first answer.
9MRR has been accepted as the standard performance mea-
sure in QA, since MRFA can be strongly affected by outlier
questions. However, the MRR score is dominated by the results
in the high end of the ranking. Thus, MRFA may be more ap-
propriate for our experiments because the filters are an interme-
diate step in the processing, the results of which other MPQA
components may improve.
928
sentence
fact opinion
Manual fact 56 (46.67%) 64 (53.33%)
opinion 42 (10.14%) 372 (89.86%)
question Naive Bayes fact 49 (40.83%) 71 (59.17%)
opinion 57 (13.77%) 357 (86.23%)
Rulebased fact 96 (80.00%) 24 (20.00%)
opinion 184 (44.44%) 230 (55.56%)
Table 6: Answer probability results.
used in an answer generator. This experiment con-
siders correspondences between (1) the classes (i.e.,
opinion or fact) assigned by the subjectivity filters to
the sentences containing answers, and (2) the classes
of the questions the answers are responses to (ac-
cording to the OpQA annotations). That is, we com-
pute the probabilities (where ans = answer):
P(ans is in a C1 sentence | ans is the answer to a C2 ques-
tion) for all four combinations of C1=opinion, fact and
C2=opinion, fact.
Results. Results for the answer probability experi-
ment are given in Table 6. The rows correspond to
the classes of the questions the answers respond to,
and the columns correspond to the classes assigned
by the subjectivity filters to the sentences contain-
ing the answers. The first two rows, for instance,
give the results for the MANUAL criterion. MANUAL
placed 56 of the answers to fact questions in fact
sentences (46.67% of all answers to fact questions)
and 64 (53.33%) of the answers to fact questions in
opinion sentences. Similarly, MANUAL placed 42
(10.14%) of the answers to opinion questions in fact
sentences, and 372 (89.86%) of the answers to opin-
ion questions in opinion sentences.
The answer probability experiment sheds some
light on the subjectivity filter experiments. All three
subjectivity filters place a larger percentage of an-
swers to opinion questions in opinion sentences than
they place in fact sentences. However, the differ-
ent filters exhibit different degrees of discrimination.
Answers to opinion questions are almost always
placed in opinion sentences by MANUAL (89.86%)
and NAIVE BAYES (86.23%). While that aspect of
their performance is excellent, MANUAL and NAIVE
BAYES place more answers to fact questions in opin-
ion rather than fact sentences (though the percent-
ages are in the 50s). This is to be expected, because
MANUAL and NAIVE BAYES are more conservative
and err on the side of classifying sentences as opin-
ions: for MANUAL, the presence of any subjective
expression makes the entire sentence opinion, even
if parts of the sentence are factual; NAIVE BAYES
shows high recall but lower precision in recognizing
opinion sentences (see Table 4). Conversely, RULE-
BASED places 80% of the fact answers in fact sen-
tences and only 56% of the opinion answers in opin-
ion sentences. Again, the lower number of assign-
ments to opinion sentences is to be expected, given
the high precision and low recall of the classifier.
But the net result is that, for RULEBASED, the off-
diagonals are all less than 50%: it places more an-
swers to fact questions in fact rather than opinion
sentences (80%), and more answers to opinion ques-
tions in opinion rather than fact sentences (56%).
This is consistent with its superior performance in
the subjectivity filtering experiment.
In addition to explaining the performance of
the subjectivity filters, the answer rank experiment
shows that the automatic opinion classifiers can be
used directly in an answer generator module. The
two automatic classifiers rely on evidence in the sen-
tence to predict the class (the information extraction
patterns used by RULEBASED and the features used
by NAIVE BAYES). In ongoing work we investigate
ways to use this evidence to extract and summarize
the opinions expressed in text, which is a task simi-
lar to that of an answer generator module.
7 Opinion Source Filters for MPQA
Systems
In addition to subjectivity filters, we also define an
opinion source filter based on the manual opinion
annotations. This filter removes all sentences that
do not have an opinion annotation with a source that
matches the source of the question10. For this filter
we only used the MANUAL source annotations since
we did not have access to automatically extracted
source information. We employ the same Answer
Rank experiment as in 6.3.1, substituting the source
filter for a subjectivity filter.
Results. Results for the source filter are mixed.
The filter outperforms the baseline on some ques-
tions and performs worst on others. As a result the
MRR for the source filter is worse than the base-
10We manually identified the sources of each of the 15 opin-
ion questions.
929
line (0.4633 vs. 0.4244). However, the source fil-
ter exhibits by far the best results using the MRFA
measure, a value of 11.267. The performance im-
provement is due to the filter?s ability to recognize
the answers to the hardest questions, for which the
other filters have the most trouble (questions mu-
gabe 7 and 8). For these questions, the rank of the
first answer improves from 153 to 21, and from 182
to 11, respectively. With the exception of question
venezuela 3, which does not contain a clear source
(and is problematic altogether because there is only
a single answer in the corpus and the question?s
qualification as opinion is not clear) the source filter
always ranked an answer within the first 25 answers.
Thus, source filters can be especially useful in sys-
tems that rely on the presence of an answer within
the first few ranked answer segments and then in-
voke more sophisticated analysis in the additional
processing phase.
8 Conclusions
We began by giving a high-level overview of the
OpQA corpus. Using the corpus, we compared the
characteristics of answers to fact and opinion ques-
tions. Based on the different characteristics, we sur-
mise that traditional QA approaches may not be as
effective for MPQA as they have been for fact-based
QA. Finally, we investigated the use of machine
learning and rule-based opinion filters and showed
that they can be used to guide MPQA systems.
Acknowledgments We would like to thank Di-
ane Litman for her work eliciting the questions for
the OpQA corpus, and the anonymous reviewers
for their helpful comments.This work was supported
by the Advanced Research and Development Activ-
ity (ARDA), by NSF Grants IIS-0208028 and IIS-
0208798, by the Xerox Foundation, and by a NSF
Graduate Research Fellowship to the first author.
References
Steven Abney. 1996. Partial parsing via finite-state cascades.
In Proceedings of the ESSLLI ?96 Robust Parsing Workshop.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Ju-
rafsky. 2004. Automatic extraction of opinion propositions
and their holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting mar-
ket sentiment from stock message boards. In Proceedings of
the 8th Asia Pacific Finance Association Annual Conference.
Kushal Dave, Steve Lawrence, and David Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. In International World Wide
Web Conference, pages 519?528.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Laca-
tusu, A. Novischi, A. Badulescu, and O. Bolohan. 2002.
LCC tools for question answering. In Proceedings of TREC
2002.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of the ACL, pages 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of AAAI.
V. Stoyanov, C. Cardie, J. Wiebe, and D. Litman. 2004. Eval-
uating an opinion annotation scheme using a new Multi-
Perspective Question and Answer corpus. In 2004 AAAI
Spring Symposium on Exploring Attitude and Affect in Text.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of the ACL, pages 417?424.
E. Voorhees and L. Buckland. 2003. Overview of the
TREC 2003 Question Answering Track. In Proceedings of
TREC 12.
Ellen Voorhees. 2001. Overview of the TREC 2001 Question
Answering Track. In Proceedings of TREC 10.
Ellen Voorhees. 2002. Overview of the 2002 Question Answer-
ing Track. In Proceedings of TREC 11.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
Theresa Wilson and Janyce Wiebe. 2003. Annotating opinions
in the world press. 4th SIGdial Workshop on Discourse and
Dialogue (SIGdial-03).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you?
Finding strong and weak opinion clauses. In Proceedings of
AAAI.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identi-
fying the polarity of opinion sentences. In Proceedings of
EMNLP.
930
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
c? 2004 Association for Computational Linguistics
Learning Subjective Language
Janyce Wiebe? Theresa Wilson?
University of Pittsburgh University of Pittsburgh
Rebecca Bruce? Matthew Bell?
University of North Carolina University of Pittsburgh
at Asheville
Melanie Martin?
New Mexico State University
Subjectivity in natural language refers to aspects of language used to express opinions, evalua-
tions, and speculations. There are numerous natural language processing applications for which
subjectivity analysis is relevant, including information extraction and text categorization. The
goal of this work is learning subjective language from corpora. Clues of subjectivity are gener-
ated and tested, including low-frequency words, collocations, and adjectives and verbs identified
using distributional similarity. The features are also examined working together in concert. The
features, generated from different data sets using different procedures, exhibit consistency in
performance in that they all do better and worse on the same data sets. In addition, this article
shows that the density of subjectivity clues in the surrounding context strongly affects how likely
it is that a word is subjective, and it provides the results of an annotation study assessing the
subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion
piece recognition (a type of text categorization and genre detection) to demonstrate the utility of
the knowledge acquired in this article.
1. Introduction
Subjectivity in natural language refers to aspects of language used to express opin-
ions, evaluations, and speculations (Banfield 1982; Wiebe 1994). Many natural lan-
guage processing (NLP) applications could benefit from being able to distinguish
subjective language from language used to objectively present factual information.
Current extraction and retrieval technology focuses almost exclusively on the sub-
ject matter of documents. However, additional aspects of a document influence its
relevance, including evidential status and attitude (Kessler, Nunberg, Schu?tze 1997).
Information extraction systems should be able to distinguish between factual infor-
mation (which should be extracted) and nonfactual information (which should be
? Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260.
E-mail{wiebe,mbell}@cs.pitt.edu.
? Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260. Email: twilson@cs.pitt.edu.
? Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804.
E-mail: bruce@cs.unca.edu
? Department of Computer Science, New Mexico State University, Las Cruces, NM 88003. E-mail:
mmartin@cs.nmsu.edu.
Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for
publication: 23 January 2004
278
Computational Linguistics Volume 30, Number 3
discarded or labeled as uncertain). Question-answering systems should distinguish
between factual and speculative answers. Multi-perspective question answering aims
to present multiple answers to the user based upon speculation or opinions derived
from different sources (Carbonell 1979; Wiebe et al 2003). Multidocument summa-
rization systems should summarize different opinions and perspectives. Automatic
subjectivity analysis would also be useful to perform flame recognition (Spertus 1997;
Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intel-
lectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio
broadcasts (Barzialy et al 2000), review mining (Terveen et al 1997), review classifi-
cation (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy
1987), and clustering documents by ideological point of view (Sack 1995). In general,
nearly any information-seeking system could benefit from knowledge of how opin-
ionated a text is and whether or not the writer purports to objectively present factual
material.
To perform automatic subjectivity analysis, good clues must be found. A huge
variety of words and phrases have subjective usages, and while some manually de-
veloped resources exist, such as dictionaries of affective language (General-Inquirer
2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the atti-
tude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no
comprehensive dictionary of subjective language. In addition, many expressions with
subjective usages have objective usages as well, so a dictionary alone would not suffice.
An NLP system must disambiguate these expressions in context.
The goal of our work is learning subjective language from corpora. In this article,
we generate and test subjectivity clues and contextual features and use the knowledge
we gain to recognize subjective sentences and opinionated documents.
Two kinds of data are available to us: a relatively small amount of data manually
annotated at the expression level (i.e., labels on individual words and phrases) of Wall
Street Journal and newsgroup data and a large amount of data with existing document-
level annotations from the Wall Street Journal (opinion pieces, such as editorials and
reviews, versus nonopinion pieces). Both are used as training data to identify clues
of subjectivity. In addition, we cross-validate the results between the two types of
annotation: The clues learned from the expression-level data are evaluated against the
document-level annotations, and those learned using the document-level annotations
are evaluated against the expression-level annotations.
There were a number of motivations behind our decision to use document-level
annotations, in addition to our manual annotations, to identify and evaluate clues
of subjectivity. The document-level annotations were not produced according to our
annotation scheme and were not produced for the purpose of training and evaluating
an NLP system. Thus, they are an external influence from outside the laboratory. In
addition, there are a great number of these data, enabling us to evaluate the results
on a larger scale, using multiple large test sets. This and cross-training between the
two types of annotations allows us to assess consistency in performance of the various
identification procedures. Good performance in cross-validation experiments between
different types of annotations is evidence that the results are not brittle.
We focus on three types of subjectivity clues. The first are hapax legomena, the set
of words that appear just once in the corpus. We refer to them here as unique words.
The set of all unique words is a feature with high frequency and significantly higher
precision than baseline (Section 3.2).
The second are collocations (Section 3.3). We demonstrate a straightforward method
for automatically identifying collocational clues of subjectivity in texts. The method is
first used to identify fixed n-grams, such as of the century and get out of here. Interest-
279
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ingly, many include noncontent words that are typically on stop lists of NLP systems
(e.g., of, the, get, out, here in the above examples). The method is then used to identify
an unusual form of collocation: One or more positions in the collocation may be filled
by any word (of an appropriate part of speech) that is unique in the test data.
The third type of subjectivity clue we examine here are adjective and verb fea-
tures identified using the results of a method for clustering words according to dis-
tributional similarity (Lin 1998) (Section 3.4). We hypothesized that two words may
be distributionally similar because they are both potentially subjective (e.g., tragic, sad,
and poignant are identified from bizarre). In addition, we use distributional similarity
to improve estimates of unseen events: A word is selected or discarded based on the
precision of it together with its n most similar neighbors.
We show that the various subjectivity clues perform better and worse on the same
data sets, exhibiting an important consistency in performance (Section 4.2).
In addition to learning and evaluating clues associated with subjectivity, we ad-
dress disambiguating them in context, that is, identifying instances of clues that are
subjective in context (Sections 4.3 and 4.4). We find that the density of clues in the
surrounding context is an important influence. Using two types of annotations serves
us well here, too. It enables us to use manual judgments to identify parameters for
disambiguating instances of automatically identified clues. High-density clues are high
precision in both the expression-level and document-level data. In addition, we give
the results of a new annotation study showing that most high-density clues are in sub-
jective text spans (Section 4.5). Finally, we use the clues together to perform document-
level classification, to further demonstrate the utility of the acquired knowledge (Sec-
tion 4.6).
At the end of the article, we discuss related work (Section 5) and conclusions
(Section 6).
2. Subjectivity
Subjective language is language used to express private states in the context of a
text or conversation. Private state is a general covering term for opinions, evaluations,
emotions, and speculations (Quirk et al 1985). The following are examples of subjective
sentences from a variety of document types.
The first two examples are from Usenet newsgroup messages:
(1) I had in mind your facts, buddy, not hers.
(2) Nice touch. ?Alleges? whenever facts posted are not in your persona of
what is ?real.?
The next one is from an editorial:
(3) We stand in awe of the Woodstock generation?s ability to be unceasingly
fascinated by the subject of itself. (?Bad Acid,? Wall Street Journal,
August 17, 1989)
The next example is from a book review:
(4) At several different layers, it?s a fascinating tale. (George Melloan,
?Whose Spying on Our Computers?? Wall Street Journal, November 1,
1989)
280
Computational Linguistics Volume 30, Number 3
The last one is from a news story:
(5) ?The cost of health care is eroding our standard of living and sapping
industrial strength,? complains Walter Maher, a Chrysler
health-and-benefits specialist. (Kenneth H. Bacon, ?Business and Labor
Reach a Consensus on Need to Overhaul Health-Care System,? Wall
Street Journal, November 1, 1989)
In contrast, the following are examples of objective sentences, sentences without sig-
nificant expressions of subjectivity:
(6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a
share.
(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156
people killed in a 1987 crash, but claims against the jetliner?s maker are
being pursued, a federal judge said. (?Northwest Airlines Settles Rest of
Suits,? Wall Street Journal, November 1, 1989)
A particular model of linguistic subjectivity underlies the current and past re-
search in this area by Wiebe and colleagues. It is most fully presented in Wiebe and
Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994). It was developed to support NLP
research and combines ideas from several sources in fields outside NLP, especially
linguistics and literary theory. The most direct influences on the model were Dolezel
(1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda
(1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse),
Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic
description of opaque contexts), and especially Banfield (1982) (theory of subjectivity
versus communication).1
The remainder of this section sketches our conceptualization of subjectivity and
describes the annotation projects it underlies.
Subjective elements are linguistic expressions of private states in context. Subjec-
tive elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3)
and eroding, sapping, and complains in (5)). They may be single words (e.g., complains)
or more complex expressions (e.g., stand in awe, what a NP). Purely syntactic or mor-
phological devices may also be subjective elements (e.g., fronting, parallelism, changes
in aspect).
A subjective element expresses the subjectivity of a source, who may be the writer
or someone mentioned in the text. For example, the source of fascinating in (4) is
the writer, while the source of the subjective elements in (5) is Maher (according to
the writer). In addition, a subjective element usually has a target, that is, what the
subjectivity is about or directed toward. In (4), the target is a tale; in (5), the target of
Maher?s subjectivity is the cost of health care.
Note our parenthetical above??according to the writer??concerning Maher?s
subjectivity. Maher is not directly speaking to us but is being quoted by the writer.
Thus, the source is a nested source, which we notate (writer, Maher); this represents
the fact that the subjectivity is being attributed to Maher by the writer. Since sources
1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993),
Wiebe (1994), and Stein and Wright (1995).
281
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are not directly addressed by the experiments presented in this article, we merely
illustrate the idea here with an example, to give the reader an idea:
The Foreign Ministry said Thursday that it was ?surprised, to put it
mildly? by the U.S. State Department?s criticism of Russia?s human
rights record and objected in particular to the ?odious? section on
Chechnya. (Moscow Times, March 8, 2002]
Let us consider some of the subjective elements in this sentence, along with their
sources:
surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry)
to put it mildly: (writer, Foreign Ministry)
criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department)
objected: (writer, Foreign Ministry)
odious: (writer, Foreign Ministry)
Consider surprised, to put it mildly. This refers to a private state of the Foreign Ministry
(i.e., it is very surprised). This is in the context of The Foreign Ministry said, which is in
a sentence written by the writer. This gives us the three-level source (writer, Foreign
Ministry, Foreign Ministry). The phrase to put it mildly, which expresses sarcasm, is
attributed to the Foreign Ministry by the writer (i.e., according to the writer, the Foreign
Ministry said this). So its source is (writer, Foreign Ministry). The subjective element
criticism has a deeply nested source: According to the writer, the Foreign Ministry said
it is surprised by the U.S. State Department?s criticism.
The nested-source representation allows us to pinpoint the subjectivity in a sen-
tence. For example, there is no subjectivity attributed directly to the writer in the
above sentence: At the level of the writer, the sentence merely says that someone
said something and objected to something (without evaluating or questioning this).
If the sentence started The magnificent Foreign Ministry said. . . , then we would have an
additional subjective element, magnificent, with source (writer).
Note that subjective does not mean not true. Consider the sentence John criticized
Mary for smoking. The verb criticized is a subjective element, expressing negative eval-
uation, with nested source (writer, John). But this does not mean that John does not
believe that Mary smokes. (In addition, the fact that John criticized Mary is being
presented as true by the writer.)
Similarly, objective does not mean true. A sentence is objective if the language used
to convey the information suggests that facts are being presented; in the context of
the discourse, material is objectively presented as if it were true. Whether or not the
source truly believes the information, and whether or not the information is in fact
true, are considerations outside the purview of a theory of linguistic subjectivity.
An aspect of subjectivity highlighted when we are working with NLP applications
is ambiguity. Many words with subjective usages may be used objectively. Examples
are sapping and eroding. In (5), they are used subjectively, but one can easily imagine
objective usages, in a scientific domain, for example. Thus, an NLP system may not
merely consult a list of lexical items to accurately identify subjective language but
must disambiguate words, phrases, and sentences in context. In our terminology, a
potential subjective element (PSE) is a linguistic element that may be used to express
282
Computational Linguistics Volume 30, Number 3
Table 1
Data Sets and Annotations used in Experiments. Annotators M, MM, and T are
co-authors of this paper. D and R are not.
Name Source Number of Words Annotators Type of
annotation
WSJ-SE Wall Street Journal 18,341 D,M Subjective elements
NG-SE Newsgroup 15,413 M Subjective elements
NG-FE Newsgroup 88,210 MM,R Flame elements
OP1 Wall Street Journal 640,975 M,T Documents
Composed of 4 data sets: W9-4,W9-10,W9-22,W-33
OP2 Wall Street Journal 629,690 M,T Documents
Composed of 4 data sets: W9-2,W9-20,W9-21,W-23
subjectivity. A subjective element is an instance of a potential subjective element, in a
particular context, that is indeed subjective in that context (Wiebe 1994).
In this article, we focus on learning lexical items that are associated with subjec-
tivity (i.e., PSEs) and then using them in concert to disambiguate instances of them
(i.e., to determine whether the instances are subjective elements).
2.1 Manual Annotations
In our subjectivity annotation projects, we do not give the annotators lists of particular
words and phrases to look for. Rather, we ask them to label sentences according to
their interpretations in context. As a result, the annotators consider a large variety of
expressions when performing annotations.
We use data that have been manually annotated at the expression level, the sen-
tence level, and the document level. For diversity, we use data from the Wall Street
Journal Treebank as well as data from a corpus of Usenet newsgroup messages. Table
1 summarizes the data sets and annotations used in this article. None of the datasets
overlap. The annotation types listed in the table are those used in the experiments
presented in this article.
In our first subjectivity annotation project (Wiebe, Bruce, and O?Hara 1999; Bruce
and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus
(Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated
at the sentence level by multiple judges. The judges were instructed to classify a sen-
tence as subjective if it contained any significant expressions of subjectivity, attributed
to either the writer or someone mentioned in the text, and to classify the sentence as
objective, otherwise. After multiple rounds of training, the annotators independently
annotated a fresh test set of 500 sentences from WSJ-SE. They achieved an average
pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score
of 0.80 for the 85% of the test set for which the annotators were somewhat sure of
their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test
set for which the annotators were very sure of their judgments.
We later asked the same annotators to identify the subjective elements in WSJ-
SE. Specifically, each annotator was given the subjective sentences he identified in
283
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
the previous study and asked to put brackets around the words he believed caused
the sentence to be classified as subjective.2 For example (subjective elements are in
parentheses):
They paid (yet) more for (really good stuff).
(Perhaps you?ll forgive me) for reposting his response.
No other instructions were given to the annotators and no training was performed for
the expression-level task. A single round of tagging was performed, with no commu-
nication between annotators. There are techniques for analyzing agreement when an-
notations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera,
and Amorortu 1999), but our focus in this article is on words. Thus, our analyses are
at the word level: Each word is classified as either appearing in a subjective element
or not. Punctuation and numbers are excluded from the analyses. The kappa value for
word agreement in this study is 0.42.
Another two-level annotation project was performed in Wiebe et al (2001), this
time involving document-level and expression-level annotations of newsgroup data
(NG-FE in Table 1). In that project, we were interested in annotating flames, inflam-
matory messages in newsgroups or listservs. Note that inflammatory language is a
kind of subjective language. The annotators were instructed to mark a message as
a flame if the main intention of the message is a personal attack and the message
contains insulting or abusive language.
After multiple rounds of training, three annotators independently annotated a
fresh test set of 88 messages from NG-FE. The average pairwise percentage agreement
is 92% and the average pairwise kappa value is 0.78. These results are comparable to
those of Spertus (1997), who reports 98% agreement on noninflammatory messages
and 64% agreement on inflammatory messages.
Two of the annotators were then asked to identify the flame elements in the entire
corpus NG-FE. Flame elements are the subset of subjective elements that are perceived
to be inflammatory. The two annotators were asked to do this in the entire corpus, even
those messages not identified as flames, because messages that were not judged to be
flames at the document level may contain some individual inflammatory phrases. As
above, no training was performed for the expression-level task, and a single round of
tagging was performed, without communication between annotators. Agreement was
measured in the same way as in the subjective-element study above. The kappa value
for flame element annotations in corpus NG-FE is 0.46.
An additional annotation project involved a single annotator, who performed
subjective-element annotations on the newsgroup corpus NG-SE.
The agreement results above suggest that good levels of agreement can be achieved
at higher levels of classification (sentence and document), but agreement at the expres-
sion level is more challenging. The agreement values are lower for the expression-level
annotations but are still much higher than that expected by chance.
Note that our word-based analysis of agreement is a tough measure, because it
requires that exactly the same words be identified by both annotators. Consider the
following example from WSJ-SE:
D: (played the role well) (obligatory ragged jeans a thicket of long hair
and rejection of all things conventional)
2 We are grateful to Aravind Joshi for suggesting this level of annotation.
284
Computational Linguistics Volume 30, Number 3
M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long
hair and (rejection) of (all things conventional)
Judge D in the example consistently identifies entire phrases as subjective, while judge
M prefers to select discrete lexical items.
Despite such differences between annotators, the expression-level annotations
proved very useful for exploring hypotheses and generating features, as described
below.
Since this article was written, a new annotation project has been completed. A
10,000-sentence corpus of English-language versions of world news articles has been
annotated with detailed subjectivity information as part of a project investigating
multiple-perspective question answering (Wiebe et al 2003). These annotations are
much more detailed than the annotations used in this article (including, for example,
the source of each private state). The interannotator agreement scores for the new
corpus are high and are improvements over the results of the studies described above
(Wilson and Wiebe 2003).
The current article uses existing document-level subjective classes, namely edito-
rials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall Street
Journal. These are subjective classes in the sense that they are text categories for which
subjectivity is a key aspect. We refer to them collectively as opinion pieces. All other
types of documents in the Wall Street Journal are collectively referred to as nonopinion
pieces.
Note that opinion pieces are not 100% subjective. For example, editorials contain
objective sentences presenting facts supporting the writer?s argument, and reviews
contain sentences objectively presenting facts about the product beign reviewed. Sim-
ilarly, nonopinion pieces are not 100% objective. News reports present opinions and
reactions to reported events (van Dijk 1988); they often contain segments starting with
expressions such as critics claim and supporters argue. In addition, quoted-speech sen-
tences in which individuals express their subjectivity are often included (Barzilay et
al. 2000). For concreteness, let us consider WSJ-SE, which, recall, has been manually
annotated at the sentence level. In WSJ-SE, 70% of the sentences in opinion pieces
are subjective and 30% are objective. In nonopinion pieces, 44% of the sentences are
subjective and only 56% are objective. Thus, while there is a higher concentration of
subjective sentences in opinion versus nonopinion pieces, there are many subjective
sentences in nonopinion pieces and objective sentences in opinion pieces.
An inspection of some data reveals that some editorial and review articles are not
marked as such by the Wall Street Journal. For example, there are articles whose purpose
is to present an argument rather than cover a news story, but they are not explicitly
labeled as editorials by the Wall Street Journal. Thus, the opinion piece annotations of
data sets OP1 and OP2 in Table 1 have been manually refined. The annotation instruc-
tions were simply to identify any additional opinion pieces that were not marked as
such. To test the reliability of this annotation, two judges independently annotated
two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000
words. This is an ?annotation lite? task: With no training, the annotators achieved
kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall
Street Journal file.
3. Generating and Testing Subjective Features
3.1 Introduction
The goal in this section is to learn lexical subjectivity clues of various types, single
words as well as collocations. Some require no training data, some are learned us-
285
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
ing the expression-level subjective-element annotations as training data, and some
are learned using the document-level opinion piece annotations as training data (i.e.,
opinion piece versus nonopinion piece). All of the clues are evaluated with respect to
the document-level opinion piece annotations. While these evaluations are our focus,
because many more opinion piece than subjective-element data exist, we do evaluate
the clues learned from the opinion piece data on the subjective-element data as well.
Thus, we cross-validate the results both ways between the two types of annotations.
Throughout this section, we evaluate sets of clues directly, by measuring the pro-
portion of clues that appear in subjective documents or expressions, seeking those that
appear more often than expected. In later sections, the clues are used together to find
subjective sentences and to perform text categorization.
The following paragraphs give details of the evaluation and experimental design
used in this section.
The proportion of clues in subjective documents or expressions is their precision.
Specifically, the precision of a set S with respect to opinion pieces is
prec(S) =
number of instances of members of S in opinion pieces
total number of instances of members of S in the data
The precision of a set S with respect to subjective elements is
prec(S) =
number of instances of members of S in subjective elements
total number of instances of members of S in the data
In the above, S is a set of types (not tokens). The counts are of tokens (i.e., instances
or occurrences) of members of S.
Why use a set rather than individual items? Many good clues of subjectivity occur
with low frequency (Wiebe, McKeever, and Bruce 1998). In fact, as we shall see below,
uniqueness in the corpus is an informative feature for subjectivity classification. Thus,
we do not want to discard low-frequency clues, because they are a valuable source of
information, and we do not want to evaluate individual low-frequency lexical items,
because the results would be unreliable. Our strategy is thus to identify and evaluate
sets of words and phrases, rather than individual items.
What kinds of results may we expect? We cannot expect absolutely high precision
with respect to the opinion piece classifications, even for strong clues, for three reasons.
First, for our purposes, the data are noisy. As mentioned above, while the proportion
of subjective sentences is higher in opinion than in nonopinion pieces, the proportions
are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces
contain subjective sentences.
Second, we are trying to learn lexical items associated with subjectivity, that is,
PSEs. As discussed above, many words and phrases with subjective usages have ob-
jective usages as well. Thus, even in perfect data with no noise, we would not expect
100% precision. (This is the motivation for the work on density presented in section
4.4.)
Third, the distribution of opinions and nonopinions is highly skewed in favor of
nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion
pieces.
In this work, increases in precision over a baseline precision are used as evidence
that promising sets of PSEs have been found. Our main baseline for comparison is
the number of word instances in opinion pieces, divided by the total number of word
instances:
Baseline Precision =
number of word instances in opinion pieces
total number of word instances
286
Computational Linguistics Volume 30, Number 3
Table 2
Frequencies and increases in precision of unique
words in subjective-element data. Baseline
frequency is the total number of words, and
baseline precision is the proportion of words in
subjective elements.
WSJ-SE
D M
freq +prec +prec
Unique words 2,615 +.07 +.12
Baseline 18,341 .07 .08
Words and phrases with higher proportions than this appear more than expected in
opinion pieces.
To further evaluate the quality of a set of PSEs, we also perform the following
significance test. For a set of PSEs in a given data set, we test the significance of the
difference between (1) the proportion of words in opinion pieces that are PSEs and (2)
the proportion of words in nonopinion pieces that are PSEs, using the z-significance
test for two proportions.
Before we continue, there are a few more technical items to mention concerning
the data preparation and experimental design:
? All of the data sets are stemmed using Karp?s morphological analyzer
(Karp et al 1994) and part-of-speech tagged using Brill?s (1992) tagger.
? When the opinion piece classifications are used for training, the existing
classifications, assigned by the Wall Street Journal, are used. Thus, the
processes using them as training data may be applied to more data to
learn more clues, without requiring additional manual annotation.
? When the opinion piece data are used for testing, the manually refined
classifications (described at the end of Section 2.1) are used.
? OP1 and OP2 together comprise eight treebank files. Below, we often
give results separately for the component files, allowing us to assess the
consistency of results for the various types of clues.
3.2 Unique Words
In this section, we show that low-frequency words are associated with subjectivity in
both the subjective-element and opinion piece data. Apparently, people are creative
when they are being opinionated.
Table 2 gives results for unique words in subjective-element data. Recall that
unique words are those that appear just once in the corpus, that is, hapax legomena.
The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed
by the percentage-point improvements in precision over baseline for unique words in
subjective elements marked by two annotators (denoted as D and M in the table). The
second row gives baseline frequency and precisions. Baseline frequency is the total
number of words in WSJ-SE. Baseline precision for an annotator is the proportion of
words included in subjective elements by that annotator. Specifically, consider anno-
tator M. The baseline precision of words in subjective elements marked by M is 0.08,
287
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 3
Frequencies and increases in precision for words that appear exactly once in the data sets
composing OP1. For each data set, baseline frequency is the total number of words, and
baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11
Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
but the precision of unique words in these same annotations is 0.20, 0.12 points higher
than the baseline. This is a 150% improvement over the baseline.
The number of unique words in opinion pieces is also higher than expected. Table
3 compares the precision of the set of unique words to the baseline precision (i.e.,
the precision of the set of all words that appear in the corpus) in the four WSJ files
composing OP1. Before this analysis was performed, numbers were removed from the
data (we are not interested in the fact that, say, the number 163,213.01 appears just once
in the corpus). The number of words in each data set and baseline precisions are listed
at the bottom of the table. The freq columns give total frequencies. The +prec columns
show the percentage-point improvements in precision over baseline. For example, in
W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over
baseline of 0.16. The difference in the proportion of words that are unique in opinion
pieces and the proportion of words that are unique in nonopinion pieces is highly
significant, with p < 0.001 (z ? 22) for all of the data sets. Note that not only does the
set of unique words have higher than baseline precision, the set is a frequent feature.
The question arises, how does corpus size affect the precision of the set of unique
words? Presumably, uniqueness in a larger corpus is more meaningful than uniqueness
in a smaller one. The results in Figure 1 provide evidence that it is. The y-axis in Figure
1 represents increase in precision over baseline and the x-axis represents corpus size.
Five graphs are plotted, one for the set of words that appear exactly once (uniques),
one for the set of words that appear exactly twice ( freq2), one for the set of words that
appear exactly three times ( freq3), etc.
In Figure 1, increases in precision are given for corpora of size n, where n =
20, 40, . . . , 2420, 2440 documents. Each data point is an average over 25 sample corpora
of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in
which 9% of the documents are opinion pieces. The sample corpora were created by
randomly selecting documents from the large corpus, preserving the 9% distribution
of opinion pieces. At the smallest corpus size (containing 20 documents), the average
number of words is 9,617. At the largest corpus size (containing 2440 documents), the
average is 1,225,186 words.
As can be seen in the figure, the precision of unique and other low-frequency
words increases with corpus size, with increases tapering off at the largest corpus size
tested. Words with frequency 2 also realize a nice increase, although one that is not as
dramatic, in precision over baseline. Even words of frequency 3, 4, and 5 show modest
increases.
To help us understand the importance of low-frequency words in large as opposed
to small data sets, we can consider the following analogy. With collectible trading
cards, rare cards are the most valuable. However, if we have some cards and are
trying to determine thier value, looking in only a few packs of cards will not tell us if
288
Computational Linguistics Volume 30, Number 3
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
20 620 1220 1820 2420
Corpus Size (documents)
Increase in 
Precision
uniques freq2 freq3 freq4 freq5
Figure 1
Precision of low-frequency words as corpus size increases.
any of our cards are valuable. Only by looking at many packs of cards can we make
a determination as to which are the rare ones. Only in samples of sufficient size is
uniqueness informative.
The results in this section suggest that an NLP system using uniqueness features
to recognize subjectivity should determine uniqueness with respect to the test data
augmented with an additional store of (unannotated) data.
3.3 Identifying Potentially Subjective Collocations from Subjective-Element and
Flame-Element Annotations
In this section, we describe experiments in identifying potentially subjective colloca-
tions.
Collocations are selected from the subjective-element data (i.e., NG-SE, NG-FE, and
WSJ-SE), using the union of the annotators? tags for the data sets tagged by multiple
taggers. The results are then evaluated on opinion piece data.
The selection procedure is as follows. First, all 1-grams, 2-grams, 3-grams, and
4-grams are extracted from the data. In this work, each constituent of an n-gram is
a word-stem, part-of-speech pair. For example, (in-prep the-det can-noun) is a 3-gram
that matches trigrams consisting of preposition in, followed by determiner the, and
ending with noun can.
A subset of the n-grams are then selected based on precision. The precision of an
n-gram is the number of subjective instances of that n-gram in the data divided by
the total number of instances of that n-gram in the data. An instance of an n-gram is
subjective if each word occurs in a subjective element in the data.
n-grams are selected based on two criteria. First, the precision of the n-gram must
be greater than the baseline precision (i.e., the proportion of all word instances that
289
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
are in subjective elements). Second, the precision of the n-gram must be greater than
the maximum precision of its constituents. This criterion is used to avoid selecting
unnecessarily long collocations. For example, scumbag is a strongly subjective clue. If
be a scumbag does not have higher precision than scumbag alone, we do not want to
select it.
Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2.
(W1,W2) is identified as a potential subjective element if prec(W1, W2) ? 0.1 and:
prec(W1, W2) > max(prec(W1), prec(W2))
For trigrams, we extend the second condition as follows. Let (W1, W2, W3) be a trigram
consisting of consecutive words W1, W2, and W3. The condition is then
prec(W1, W2, W3) > max(prec(W1, W2), prec(W3))
or
prec(W1, W2, W3) > max(prec(W1), prec(W2, W3))
The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram
first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and
then with the maximum of the precisions of trigram (W1,W2,W3) and word W4. We
call the n-gram collocations identified as above fixed-n-grams.
We also define a type of collocation called a unique generalized n-gram (ugen-n-
gram). Such collocations have placeholders for unique words. As will be seen below,
these are our highest-precision features.
To find and select such generalized collocations, we first find every word that
appears just once in the corpus and replace it with a new word, UNIQUE (but re-
membering the part of speech of the original word). In essence, we treat the set of
single-instance words as a single, frequently occurring word (which occurs with var-
ious parts of speech). Precisely the same method used for extracting and selecting
n-grams above is used to obtain the potentially subjective collocations with one or
more positions filled by a UNIQUE, part-of-speech pair.
To test the ugen-n-grams extracted from the subjective-element training data using
the method outlined above, we assess their precision with respect to opinion piece
data. As with the training data, all unique words in the test data are replaced by
UNIQUE. When a ugen-n-gram is matched against the test data, the UNIQUE fillers
match words (of the appropriate parts of speech) that are unique in the test data.
Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns
identified as described above on the four data sets composing OP1. The freq columns
give total frequencies, and the +prec columns show the improvements in precision
from the baseline. The number of words in each data set and baseline precisions are
given at the bottom of the table. For all n-gram features besides the fixed-4-grams and
ugen-4-grams, the proportion of features in opinion pieces is significantly greater than
the proportion of features in nonopinion pieces.3
The question arises, how much overlap is there between instances of fixed-n-grams
and instances of ugen-n-grams? In the test data of Table 4, there are a total of 8,577
fixed-n-grams instances. Only 59 of these, fewer than 1% are contained (wholly or in
part) in ugen-n-gram instances. This small intersection set shows that two different
types of potentially subjective collocations are being recognized.
3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by the
number of words in opinion pieces and (2) the number of feature instances in nonopinion pieces
divided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets.
290
Computational Linguistics Volume 30, Number 3
Table 4
Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations
learned from the subjective-element data. For each data set, baseline frequency is the total
number of words, and baseline precision is the proportion of words in opinion pieces.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
fixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14
Randomly selected examples of our learned collocations that appear in the test
data are given in Tables 5 and 6. It is interesting to note that the unique generalized
collocations were learned from the training data by their matching different unique
words from the ones they match in the test data.
3.4 Generating Features from Document-Level Annotations Using Distributional
Similarity
In this section, we identify adjective and verb PSEs using distributional similarity.
Opinion-piece data are used for training, and (a different set of) opinion-piece data
and the subjective-element data are used for testing.
With distributional similarity, words are judged to be more or less similar based
on their distributional patterning in text (Lee 1999; Lee and Pereira 1999). Our
Table 5
Random sample of fixed-3-gram collocations in OP1.
one-noun of-prep his-det worst-adj of-prep all-det
quality-noun of-prep the-det to-prep do-verb so-adverb
in-prep the-det company-noun you-pronoun and-conj your-pronoun
have-verb taken-verb the-det rest-noun of-prep us-pronoun
are-verb at-prep least-adj but-conj if-prep you-pronoun
as-prep a-det weapon-noun continue-verb to-to do-verb
purpose-noun of-prep the-det could-modal have-verb be-verb
it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep
have-verb be-verb the-det do-verb something-noun about-prep
cause-verb you-pronoun to-to evidence-noun to-to back-adverb
that-prep you-pronoun are-verb i-pronoun be-verb not-adverb
of-prep the-det century-noun of-prep money-noun be-prep
291
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 6
Random sample of unique generalized collocations in OP1. U: UNIQUE.
Pattern Instances
U-adj as-prep: drastic as; perverse as; predatory as
U-adj in-prep: perk in; unsatisfying in; unwise in
U-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinate
U-noun back-adverb: cutting back; hearken back
U-verb U-adverb: coexist harmoniously; flouncing tiresomely
ad-noun U-noun: ad hoc; ad valorem
any-det U-noun: any over-payment; any tapings; any write-off
are-verb U-noun: are escapist; are lowbrow; are resonance
but-conj U-noun: but belch; but cirrus; but ssa
different-adj U-noun: different ambience; different subconferences
like-prep U-noun: like hoffmann; like manute; like woodchuck
national-adj U-noun: national commonplace; national yonhap
particularly-adverb U-adj: particularly galling; particularly noteworthy
so-adverb U-adj: so monochromatic; so overbroad; so permissive
this-det U-adj: this biennial; this inexcusable; this scurrilous
your-pronoun U-noun: your forehead; your manuscript; your popcorn
U-adj and-conj U-adj: arduous and raucous; obstreperous and abstemious
U-noun be-verb a-det: acyclovir be a; siberia be a
U-noun of-prep its-pronoun: outgrowth of its; repulsion of its
U-verb and-conj U-verb: wax and brushed; womanize and booze
U-verb to-to a-det: cling to a; trek to a
are-verb U-adj to-to: are opaque to; are subject to
a-det U-noun and-conj: a blindfold and; a rhododendron and
a-det U-verb U-noun: a jaundice ipo; a smoulder sofa
it-pronoun be-verb U-adverb: it be humanly; it be sooo
than-prep a-det U-noun: than a boob; than a menace
the-det U-adj and-conj: the convoluted and; the secretive and
the-det U-noun that-prep: the baloney that; the cachet that
to-to a-det U-adj: to a gory; to a trappist
to-to their-pronoun U-noun: to their arsenal; to their subsistence
with-prep an-det U-noun: with an alias; with an avalanche
292
Computational Linguistics Volume 30, Number 3
trainingPrec(s) is the precision of s in the training data
validationPrec(s) is the precision of s in the validation data
testPrec(s) is the precision of s in the test data
(similarly for trainingFreq, validationFreq, and testFreq)
S = the set of all adjectives (verbs) in the training data
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
retained = {}
For si in S:
if trainingPrec({si} ? Ci,n) > T:
retained = retained ? {si} ? Ci,n
RT,n = retained
ADJpses = {} (VERBpses = {})
for T in [0.01,0.04,. . .,0.70]:
for n in [2,3,. . .,40]:
if validationPrec(RT,n) ? 0.28 (0.23 for verbs)
and validationFreq(RT,n) ? 100:
ADJpses = ADJpses ? RT,n (VERBpses = VERBpses ? RT,n)
Results in Table 7 show testPrec(ADJpses) and testFreq(ADJpses).
Figure 2
Algorithm for selecting adjective and verb features using distributional similarity.
motivation for experimenting with it to identify PSEs was twofold. First, we hypoth-
esized that words might be distributionally similar because they share pragmatic us-
ages, such as expressing subjectivity, even if they are not close synonyms. Second,
as shown above, low-frequency words appear more often in subjective texts than ex-
pected. We did not want to discard all low-frequency words from consideration but
cannot effectively judge the suitability of individual words. Thus, to decide whether
to retain a word as a PSE, we consider the precision not of the individual word, but
of the word together with a cluster of words similar to it.
Many variants of distributional similarity have been used in NLP (Lee 1999; Lee
and Pereira 1999). Dekang Lin?s (1998) method is used here. In contrast to many
implementations, which focus exclusively on verb-noun relationships, Lin?s method
incorporates a variety of syntactic relations. This is important for subjectivity recogni-
tion, because PSEs are not limited to verb-noun relationships. In addition, Lin?s results
are freely available.
A set of seed words begins the process. For each seed si, the precision of the set
{si}?Ci,n in the training data is calculated, where Ci,n is the set of n words most similar
to si, according to Lin?s (1998) method. If the precision of {si} ? Ci,n is greater than a
threshold T, then the words in this set are retained as PSEs. If it is not, neither si nor
the words in Ci,n are retained. The union of the retained sets will be denoted RT,n, that
is, the union of all sets {si} ? Ci,n with precision on the training set > T.
In Wiebe (2000), the seeds (the sis) were extracted from the subjective-element
annotations in corpus WSJ-SE. Specifically, the seeds were the adjectives that appear
at least once in a subjective element in WSJ-SE. In this article, the opinion piece corpus
is used to move beyond the manual annotations and small corpus of the earlier work,
and a much looser criterion is used to choose the initial seeds: All of the adjectives
(verbs) in the training data are used.
The algorithm for the process is given in Figure 2. There is one small difference
for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for
293
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 7
Frequencies and increases in precision for adjective and verb features identified
using distributional similarity with filtering. For each test data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces.
Baseline ADJpses VERBpses
Training Validation Test freq prec freq +prec freq +prec
W9-10 W9-22
W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11
W9-10 W9-33
W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11
W9-22 W9-33
W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10
All pairings of W9-10,
W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15
adjectives versus 0.23 for verbs. These thresholds were determined using validation
data.
Seeds and their clusters are assessed on a training set for many parameter settings
(cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70
by .03). As mentioned above, each (n, T) parameter pair yields a set of adjectives RT,n,
that is, the union of all sets {si}?Ci,n with precision on the training set > T. A subset,
ADJpses, of those sets is chosen based on precision and frequency in a validation set.
Finally, the ADJpses are tested on the test set.
Table 7 shows the results for four opinion piece test sets. Multiple training-
validation data set pairs are used for each test set, as given in Table 7. The results
are for the union of the adjectives (verbs) chosen for each pair. The freq columns give
total frequencies, and the +prec columns show the improvements in precision from
the baseline. For each data set, the difference between the proportion of instances
of ADJpses in opinion pieces and the proportion in nonopinion pieces is significant
(p < 0.001, z ? 9.2). The same is true for VERBpses (p < 0.001, z ? 4.1).
In the interests of testing consistency, Table 8 shows the results of assessing the
adjective and verb features generated from opinion piece data (ADJpses and VERBpses
Table 8
Average frequencies and increases in precision in subjective-element data
of the sets tested in Table 7. The baselines are the precisions of
adjectives/verbs that appear in subjective elements in the
subjective-element data.
Adj baseline Verb baseline ADJpses VERBpses
freq prec freq prec freq +prec freq +prec
WSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10
WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13
NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08
294
Computational Linguistics Volume 30, Number 3
Table 9
Frequencies and increases in precision for all features. For each data set, baseline
frequency is the total number of words, and baseline precision is the proportion of
words in opinion pieces. freq: total frequency; +prec: increase in precision over baseline.
W9-04 W9-10 W9-22 W9-33
freq +prec freq +prec freq +prec freq +prec
Unique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11
Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05
ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17
Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05
ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15
Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 ?.07
ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25
Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12
Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11
Baseline 156421 .19 156334 .18 155135 .13 153634 .14
in Table 7) on the subjective-element data. The left side of the table gives baseline
figures for each set of subjective-element annotations. The right side of the table gives
the average frequencies and increases in precision over baseline for the ADJpses and
VERBpses sets on the subjective-element data. The baseline figures in the table are the
frequencies and precisions of the sets of adjectives and verbs that appear at least once
in a subjective element. Since these sets include words that appear just once in the
corpus (and thus have 100% precision), the baseline precision is a challenging one.
Testing the VERBpses and ADJpses on the subjective-element data reveals some inter-
esting consistencies for these subjectivity clues. The precision increases of the VERBpses
on the subjective-element data are comparable to their increases on the opinion piece
data. Similarly, the precision increases of the ADJpses on the subjective-element data
are as good as or better than the performance of this set of PSEs on the opinion piece
data. Finally, the precisions increases for the ADJpses are higher than for the VERBpses
on all data sets. This is again consistent with the higher performance of the ADJpses
sets in the opinion piece data sets.
4. Features Used in Concert
4.1 Introduction
In this section, we examine the various types of clues used together. In preparation for
this work, all instances in OP1 and OP2 of all of the PSEs identified as described in
Section 3 have been automatically identified. All training to define the PSE instances
in OP1 was performed on data separate from OP1, and all training to define the PSE
instances in OP2 was performed on data separate from OP2.
4.2 Consistency in Precision among Data Sets
Table 9 summarizes the results from previous sections in which the opinion piece data
are used for testing. The performance of the various features is consistently good or
bad on the same data sets: the performance is better for all features on W9-10 and
W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very
low frequency, and the verbs, which have low frequency in W9-10). This is so despite
the fact that the features were generated using different procedures and data: The
295
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at least
once in an SE (except not, will, be, have).
1. PSEinsts = the set of all instances of PSEs
2. HiDensity = {}
3. For P in PSEinsts:
4. leftWin(P) = the W words before P
5. rightWin(P) = the W words after P
6. density(P) = number of SEs whose first or last
word is in leftWin(P) or rightWin(P)
7. if density(P) ? T:
HiDensity = HiDensity ? {P}
8. prec(PSEinsts) =
number of PSEinsts in subject elements
|PSEinsts|
9. prec(HiDensity) =
number of HiDensity in subject elements
|HiDensity|
Figure 3
Algorithm for calculating density in subjective-element data.
adjectives and verbs were generated from WSJ document-level opinion piece classifi-
cations; the n-gram features were generated from newsgroup and WSJ expression-level
subjective-element classifications; and the unique unigram feature requires no training.
This consistency in performance suggests that the results are not brittle.
4.3 Choosing Density Parameters from Subjective-Element Data
In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on
how subjective the surrounding context is. We explore this idea in the current work,
assessing whether PSEs are more likely to be subjective if they are surrounded by sub-
jective elements. In particular, we experiment with a density feature to decide whether
or not a PSE instance is subjective: If a sufficient number of subjective elements are
nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded.
The density parameters are a window size W and a frequency threshold T.
In this section, we explore the density of manually annotated PSEs in subjective-
element data and choose density parameters to use in Section 4.4, in which we apply
them to automatically identified PSEs in opinion piece data.
The process for calculating density in the subjective-element data is given in Fig-
ure 3. The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that
appear at least once in a subjective element, with the exception of some stop words
(line 0 of Figure 3). Note that these PSEs depend only on the subjective-element man-
ual annotations, not on the automatically identified features used elsewhere in the
article or on the document-level opinion piece classes. PSEinsts is the set of PSE
instances to be disambiguated (line 1). HiDensity (initialized on line 2) will be the
subset of PSEinsts that are retained. In the loop, the density of each PSE instance
P is calculated. This is the number of subjective elements that begin or end in the
W words preceding or following P (line 6). P is retained if its density is at least T
(line 7).
Lines 8?9 of the algorithm assess the precision of the original (PSEinsts) and new
(HiDensity) sets of PSE instances. If prec(HiDensity) is greater than prec(PSEinsts), then
296
Computational Linguistics Volume 30, Number 3
Table 10
Most frequent entry in the top three precision intervals for each
subjective-element data set.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
Baseline freq 1,566 1,245 1,167 1,108 3,303
Baseline prec .49 .47 .41 .36 .51
Range .87?.92 .95?1.0 .95?1.0 .95?1.0 .95?1.0
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 76 12 1 1 3
prec .89 1.0 1.0 1.0 1.0
Range .82?.87 .90?.95 .73?.78 .51?.56 .67?.72
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 63 22 53 221 664
prec .84 .91 .78 .51 .67
Range .77?.82 .84?.89 .66?.71 .46?.51 .63?.67
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 292 42 53 358 1504
prec .78 .88 .68 .47 .63
there is evidence that the number of subjective elements near a PSE instance is related
to its subjectivity in context.
To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1
and WSJ-SE2) and annotations of the two judges are considered separately. WSJ-SE2-D,
for example, refers to D?s annotations of WSJ-SE2. The process in Figure 3 was repeated
for different parameter settings (T in [1, 2, 4, . . . , 48] and W in [1, 10, 20, . . . , 490]) on each
of the SE data sets. To find good parameter settings, the results for each data set were
sorted into five-point precision intervals and then sorted by frequency within each
interval. Information for the top three precision intervals for each data set are shown
in Table 10, specifically, the parameter values (i.e., T and W) and the frequency and
precision of the most frequent result in each interval. The intervals are in the rows
labeled Range. For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92,
0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92). The
top of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| and
prec(PSEinsts), respectively, in line 8 of Figure 3.
The parameter values exhibit a range of frequencies and precisions, with the ex-
pected trade-off between precision and frequency. We choose the following parameters
to test in Section 4.4: For each data set, for each precision interval whose lower bound
is at least 10 percentage points higher than the baseline for that data set, the top
two (T, W) pairs yielding the highest frequencies in that interval are chosen. Among
the five data sets, a total of 45 parameter pairs were so selected. This exercise was
completed once, without experimenting with different parameter settings.
4.4 Density for Disambiguation
In this section, density is exploited to find subjective instances of automatically iden-
tified PSEs. The process is shown in Figure 4. There are only two differences between
the algorithms in Figures 3 and 4. First, in Figure 3, density is defined in terms of
the number of subjective elements nearby. However, subjective-element annotations
are not available in test data. Thus in Figure 4, density is defined in terms of the
297
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
0. PSEinsts = the set of instances in the test
data of all PSEs described in Section 3
1. HiDensity = {}
2. For P in PSEinsts:
3. leftWin(P) = the W words before P
4. rightWin(P) = the W words after P
5. density(P) = number of PSEinsts whose first or last
word is in leftWin(P) or rightWin(P)
6. if density(P) ? T:
HiDensity = HiDensity ? {P}
7. prec(PSEinsts) = # of PSEinsts in OPs|PSEinsts|
8. prec(HiDensity) = # of HiDensity in OPs|HiDensity|
Figure 4
Algorithm for calculating density in opinion piece (OP) data
number of other PSE instances nearby, where PSEinsts consists of all instances of the
automatically identified PSEs described in Section 3, for which results are given in
Table 9.
Second, in Figure 4, we assess precision with respect to the document-level classes
(lines 7?8). The test data are OP1.
An interesting question arose when we were defining the PSE instances: What
should be done with words that are identified to be PSEs (or parts of PSEs) according
to multiple criteria? For example, sunny, radiant, and exhilarating are all unique in
corpus OP1, and are all members of the adjective PSE feature defined for testing
on OP1. Collocations add additional complexity. For example, consider the sequence
and splendidly, which appears in the test data. The sequence and splendidly matches
the ugen-2-gram (and-conj U-adj), and the word splendidly is unique. In addition, a
sequence may match more than one n-gram feature. For example, is it that matches
three fixed-n-gram features: is it, is it that, and it that.
In the current experiments, the more PSEs a word matches, the more weight it
is given. The hypothesis behind this treatment is that additional matches represent
additional evidence that a PSE instance is subjective. This hypothesis is realized as
follows: Each match of each member of each type of PSE is considered to be a PSE
instance. Thus, among them, there are 11 members in PSEinsts for the five phrases
sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches
mentioned above.
The process in Figure 4 was conducted with the 45 parameter pair values (T and
W) chosen from the subjective-element data as described in Section 4.3. Table 11 shows
results for a subset of the 45 parameters, namely, the most frequent parameter pair
chosen from the top three precision intervals for each training set. The bottom of the
table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts|
and prec(PSEinsts), respectively, in line 7 of Figure 4.
The density features result in substantial increases in precision. Of the 45 parameter
pairs, the minimum percentage increase over baseline is 22%. Fully 24% of the 45
parameter pairs yield increases of 200% or more; 38% yield increases between 100%
298
Computational Linguistics Volume 30, Number 3
Table 11
Results for high-density PSEs in test data OP1 using parameters chosen
from subjective-element data.
WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE
T, W 10, 20 12, 50 20, 50 14, 100 10, 10
freq 237 3,176 170 10,510 8
prec .87 .72 .97 .57 1.0
T, W 6, 10 12, 60 46, 190 22, 370 26, 90
freq 459 5,289 1,323 21,916 787
prec .68 .68 .95 .37 .92
T, W 12, 40 12, 80 18, 60 16, 310 8, 30
freq 1,398 9,662 906 24,454 3,239
prec .79 .58 .87 .34 .67
PSE baseline: freq = 30,938, prec = .28
and 199%, and 38% yield increases between 22% and 99%. In addition, the increases
are significant. Using the set of high-density PSEs defined by the parameter pair with
the least increase over baseline, we tested the difference in the proportion of PSEs
in opinion pieces that are high-density and the proportion of PSEs in nonopinion
pieces that are high-density. The difference between these two proportions is highly
significant (z = 46.2, p < 0.0001).
Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisions
decrease and the frequencies increase as we go down each column in Table 11. The
same pattern can be observed with all 45 parameter pairs (results not included here
because of space considerations). But the parameter pairs are ordered in Table 11
based on performance in the manually annotated subjective-element data, not based
on performance in the test data. For example, the entry in the first row, first column
(T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision
interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of
Figure 3). Thus, the relative precisions and frequencies of the parameter pairs are
carried over from the training to the test data. This is quite a strong result, given that
the PSEs in the training data are from manual annotations, while the PSEs in the test
data are our automatically identified features.
4.5 High-Density Sentence Annotations
To assess the subjectivity of sentences with high-density PSEs, we extracted the 133
sentences in corpus OP2 that contain at least one high-density PSE and manually
annotated them. We refer to these sentences as the system-identified sentences.
We chose the density-parameter pair (T, W = 12, 30), based on its precision and
frequency in OP1. This parameter setting yields results that have relatively high pre-
cision and low frequency. We chose a low-frequency setting to make the annotation
study feasible.
The extracted sentences were independently annotated by two judges. One is a
coauthor of this article (judge 1), and the other has performed subjectivity annota-
tion before, but is not otherwise involved in this research (judge 2). Sentences were
annotated according to the coding instructions of Wiebe, Bruce, and O?Hara (1999)
which, recall, are to classify a sentence as subjective if there is a significant expression
of subjectivity of either the writer or someone mentioned in the text, in the sentence.
299
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Table 12
Examples of system-identified sentences.
(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo
Somali soldiers.
(2.a) But now the refugees are streaming across the border and alarming the world. ss
(2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation. oo
(2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him: . . . ss
(3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss
at Hiltons and Holiday Inns, but creditably and with the air of someone for whom
?Ten Cents a Dance? was more than a bit autobiographical.
(3.b) ?It was an exercise of blending Michelle?s singing with Susie?s singing,? explained Ms. Stevens. oo
(4) Enlisted men and lower-grade officers were meat thrown into a grinder. ss
(5) ?If you believe in God and you believe in miracles, there?s nothing particularly crazy about that.? ss
(6) He was much too eager to create ?something very weird and dynamic,? ss
?catastrophic and jolly? like ?this great and coily thing? ?Lolita.?
(7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps ss
even inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition.
(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep ss
coming and, like the recent sightings, there?s often a detail or two that suggests they may
actually be a little on the dumb side.
(9) As for the women, they?re pathetic. ss
(10) At this point, the truce between feminism and sensationalism gets might uneasy. ss
(11) MMPI?s publishers say the test shouldn?t be used alone to diagnose ss
psychological problems or in hiring; it should be given in conjunction with other tests.
(12) While recognizing that professional environmentalists may feel threatened, ss
I intend to urge that UV-B be monitored whenever I can.
Table 13
Sentence annotation contingency table; judge 1 counts are in rows and
judge 2 counts are in columns.
Subjective Objective Unsure
Subjective 98 2 3
Objective 2 14 0
Unsure 2 11 1
In addition to the subjective and objective classes, a judge can tag a sentence as unsure
if he or she is unsure of his or her rating or considers the sentence to be borderline.
An equal number (133) of other sentences were randomly selected from the corpus
to serve as controls. The 133 system-identified sentences and the 133 control sentences
were randomly mixed together. The judges were asked to annotate all 266 sentences,
not knowing which were system-identified and which were control. Each sentence
was presented with the sentence that precedes it and the sentence that follows it in
the corpus, to provide some context for interpretation.
Table 12 shows examples of the system-identified sentences. Sentences classified
by both judges as objective are marked oo and those classified by both judges as
subjective are marked ss.
300
Computational Linguistics Volume 30, Number 3
Table 14
Examples of subjective sentences adjacent to system-identified sentences.
Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the
damp hand of Edek or Waldeck who, like me, were convinced that there was no God.
?The Japanese are amazed that a company like this exists in Japan,? says Kimindo
Kusaka, head of the Softnomics Center, a Japanese management-research organization.
And even if drugs were legal, what evidence do you have that the habitual drug user
wouldn?t continue to rob and steal to get money for clothes, food or shelter?
The moral cost of legalizing drugs is great, but it is a cost that apparently lies
outside the narrow scope of libertarian policy prescriptions.
I doubt that one exists.
They were upset at his committee?s attempt to pacify the program critics by
cutting the surtax paid by the more affluent elderly and making up the loss by
shifting more of the burden to the elderly poor and by delaying some benefits by a year.
Judge 1 classified 103 of the system-identified sentences as subjective, 16 as ob-
jective, and 14 as unsure. Judge 2 classified 102 of the system-identified sentences as
subjective, 27 as objective; and 4 as unsure. The contingency table is given in Table 13.4
The kappa value using all three classes is 0.60, reflecting the highly skewed distri-
bution in favor of subjective sentences, and the disagreement on the lower-frequency
classes (unsure and objective). Consistent with the findings in Wiebe, Bruce, and
O?Hara (1999), the kappa value for agreement on the sentences for which neither
judge is unsure is very high: 0.86.
A different breakdown of the sentences is illuminating. For 98 of the sentences (call
them SS), judges 1 and 2 tag the sentence as subjective. Among the other sentences, 20
appear in a block of contiguous system-identified sentences that includes a member of
SS. For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of
subjective sentences as they are. Similarly, (3.a) is in SS and (3.b) is in the same block.
Among the remaining 15 sentences, 6 are adjacent to subjective sentences that
were not identified by our system (so were not annotated by the judges). All of those
sentences contain significant expressions of subjectivity of the writer or someone men-
tioned in the text, the criterion used in this work for classifying a sentence as subjective.
Samples are shown in Table 14.
Thus, 93% of the sentences identified by the system are subjective or are near
subjective sentences. All the sentences, together with their tags and the sentences
adjacent to them, are available on the Web at www.cs.pitt.edu/? wiebe.
4.6 Using Features for Opinion Piece Recognition
In this section, we assess the usefulness of the PSEs identified in Section 3 and listed
in Table 9 by using them to perform document-level classification of opinion pieces.
Opinion-piece classification is a difficult task for two reasons. First, as discussed in Sec-
tion 2.1, both opinionated and factual documents tend to be composed of a mixture of
subjective and objective language. Second, the natural distribution of documents in our
data is heavily skewed toward nonopinion pieces. Despite these hurdles, using only
4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2
classified only 47 (36%) of them as subjective.
301
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
our PSEs, we achieve positive results in opinion-piece classification using the basic k-
nearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997).
Given a document, the basic KNN algorithm classifies the document according to
the majority classification of the document?s k closest neighbors. For our purposes, each
document is characterized by one feature, the count of all PSE instances (regardless
of type) in the document, normalized by document length in words. The distance
between two documents is simply the absolute value of the difference between the
normalized PSE counts for the two documents.
With leave-one-out cross-validation, the set of n documents to be classified is
divided into a training set of size n?1 and a validation set of size 1. The one document
in the validation set is then classified according to the majority classification of its k
closest-neighbor documents in the training set. This process is repeated until every
document is classified.
Which value to use for k is chosen during a preprocessing phase. During the pre-
processing phase, we run the KNN algorithm with leave-one-out cross-validation on
a separate training set, for odd values of k from 1 to 15. The value of k that results in
the best classification during the preprocessing phase is the one used for later KNN
classification.
For the classification experiment, the data set OP1 was used in the preprocess-
ing phase to select the value of k, and then classification was performed on the 1,222
documents in OP2. During training on OP1, k equal to 15 resulted in the best classifi-
cation. On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline
accuracy for choosing the most frequent class (nonopinion pieces) was 0.915. Our clas-
sification accuracy represents a 28% reduction in error and is significantly better than
baseline according to McNemar?s test (Everitt 1997).
The positive results from the opinion piece classification show the usefulness of
the various PSE features when used together.
5. Relation to Other Work
There has been much work in other fields, including linguistics, literary theory, psy-
chology, philosophy, and content analysis, involving subjective language. As men-
tioned in Section 2, the conceptualization underlying our manual annotations is based
on work in literary theory and linguistics, most directly Dolez?el (1973), Uspensky
(1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield
(1982). We also mentioned existing knowledge resources such as affective lexicons
(General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons
(e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]).
Such knowledge may be used in future work to complement the work presented in
this article, for example, to seed the distributional-similarity process described in Sec-
tion 3.4.
There is also work in fields such as content analysis and psychology on statisti-
cally characterizing texts in terms of word lists manually developed for distinctions
related to subjectivity. For example, Hart (1984) performs counts on a manually de-
veloped list of words and rhetorical devices (e.g., ?sacred? terms such as freedom)
in political speeches to explore potential reasons for public reactions. Anderson and
McMaster (1998) use fixed sets of high-frequency words to assign connotative scores
to documents and sections of documents along dimensions such as how pleasant,
acrimonious, pious, or confident, the text is.
What distinguishes our work from work on subjectivity in other fields is that
we focus on (1) automatically learning knowledge from corpora, (2) automatically
302
Computational Linguistics Volume 30, Number 3
performing contextual disambiguation, and (3) using knowledge of subjectivity in
NLP applications. This article expands and integrates the work reported in Wiebe and
Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al (2001) and Wiebe (2000).
Previous work in NLP on the same or related tasks includes sentence-level and
document-level subjectivity classifications. At the sentence level, Wiebe, Bruce, and
O?Hara (1999) developed a machine learning system to classify sentences as subjec-
tive or objective. The accuracy of the system was more than 20 percentage points
higher than a baseline accuracy. Five part-of-speech features, two lexical features, and
a paragraph feature were used. These results suggested to us that there are clues to
subjectivity that might be learned automatically from text and motivated the work
reported in the current article. The system was tested in 10-fold cross validation ex-
periments using corpus WSJ-SE, a small corpus of only 1,001 sentences. As discussed
in Section 1, a main goal of our current work is to exploit existing document-level
annotations, because they enable us to use much larger data sets, they were created
outside our research group, and they allow us to assess consistency of performance
by cross-validating between our manual annotations and the existing document-level
annotations. Because the document-level data are not annotated at the sentence level,
sentence-level classification is not highlighted in this article. The new sentence annota-
tion study to evaluate sentences with high-density features (Section 4.5) uses different
data from WSJ-SE, because some of the features (n-grams and density parameters)
were identified using WSJ-SE as training data.
Other previous work in NLP has addressed related document-level classifications.
Spertus (1997) developed a system for recognizing inflammatory messages. As men-
tioned earlier in the article, inflammatory language is a type of subjective language,
so the task she addresses is closely related to ours. She uses machine learning to
select among manually developed features. In contrast, the focus in our work is on
automatically identifying features from the data.
A number of projects investigating genre detection include editorials as one of the
targeted genres. For example, in Karlgren and Cutting (1994), editorials are one of fif-
teen categories, and in Kessler, Nunberg, and Schu?tze (1997), editorials are one of six.
Given the goal of these works to perform genre detection in general, they use low-level
features that are not specific to editorials. Neither shows significant improvements for
editorial recognition. Argamon, Koppel, and Avneri (1998) address a slightly different
task, though it does involve editorials. Their goal is to distinguish not only, for ex-
ample, news from editorials, but also these categories in different publications. Their
best results are distinguishing among the news categories of different publications;
their lowest results involve editorials. Because we focus specifically on distinguishing
opinion pieces from nonopinion pieces, our results are better than theirs for those
categories. In addition, in contrast to the above studies, the focus of our work is on
learning features of subjectivity. We perform opinion piece recognition in order to
assess the usefulness of the various features when used together.
Other previous NLP research has used features similar to ours for other NLP tasks.
Low-frequency words have been used as features in information extraction (Weeber,
Vos, and Baayen 2000) and text categorization (Copeck et al 2000). A number of
researchers have worked on mining collocations from text to extend lexicographic
resources for machine translation and word sense disambiguation (e.g., Smajda 1993;
Lin 1999; Biber 1993).
In Samuel, Carberry, and Vijay-Shanker?s (1998) work on identifying collocations
for dialog-act recognition, a filter similar to ours was used to eliminate redundant
n-gram features: n-grams were eliminated if they contained substrings with the same
entropy score as or a better entropy score than the n-gram.
303
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
While it is common in studies of collocations to omit low-frequency words and
expressions from analysis, because they give rise to invalid or unrealistic statistical
measures (Church and Hanks, 1990), we are able to identify higher-precision colloca-
tions by including placeholders for unique words (i.e., the ugen-n-grams). We are not
aware of other work that uses such collocations as we do.
Features identified using distributional similarity have previously been used for
syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994)
and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).
We are not aware of other work identifying and using density parameters as
described in this article.
Since our experiments, other related work in NLP has been performed. Some of
this work addresses related but different classification tasks. Three studies classify
reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002;
Dave, Lawrence, Pennock 2003). The input is assumed to be a review, so this task
does not include finding subjective documents in the first place. The first study listed
above (Turney 2002) uses a variation of the semantic similarity procedure presented
in Wiebe (2000) (Section 3.4). The third (Dave, Lawrence, and Pennock 2003) uses n-
gram features identified with a variation of the procedure presented in Wiebe, Wilson,
and Bell (2001) (Section 3.3). Tong (2001) addresses finding sentiment timelines, that
is, tracking sentiments over time in multiple documents. For clues of subjectivity, he
uses manually developed lexical rules, rather than automatically learning them from
corpora. Similarly, Gordon et al (2003) use manually developed grammars to detect
some types of subjective language. Agrawal et al (2003) partition newsgroup authors
into camps based on quotation links. They do not attempt to recognize subjective
language.
The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff
and Wiebe (2003) and Yu and Hatzivassiloglou (2003). The first two focus on finding
additional types of subjective clues (nouns and extraction patterns identified using
extraction pattern bootstrapping). Yu and Hatzivassiloglou (2003) perform opinion text
classification. They also use existing WSJ document classes for training and testing,
but they do not include the entire corpus in their experiments, as we do. Their opinion
piece class consists only of editorials and letters to the editor, and their nonopinion
class consists only of business and news. They report an average F-measure of 96.5%.
Our result of 94% accuracy on document level classification is almost comparable.
They also perform sentence-level classification.
We anticipate that knowledge of subjective language may be usefully exploited in
a number of NLP application areas and hope that the work presented in this article will
encourage others to experiment with subjective language in their applications. More
generally, there are many types of artificial intelligence systems for which state-of-
affairs types such as beliefs and desires are central, including systems that perform plan
recognition for understanding narratives (Dyer 1982; Lehnert et al 1983), for argument
understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from
different perspectives (Carbonell 1979), and for generating language under different
pragmatic constraints (Hovy 1987). Knowledge of linguistic subjectivity could enhance
the abilities of such systems to recognize and generate expressions referring to such
states of affairs in natural text.
6. Conclusions
Knowledge of subjective language promises to be beneficial for many NLP applica-
tions including information extraction, question answering, text categorization, and
304
Computational Linguistics Volume 30, Number 3
summarization. This article has presented the results of an empirical study in ac-
quiring knowledge of subjective language from corpora in which a number of fea-
ture types were learned and evaluated on different types of data with positive re-
sults.
We showed that unique words are subjective more often than expected and that
unique words are valuable clues to subjectivity. We also presented a procedure for au-
tomatically identifying potentially subjective collocations, including fixed collocations
and collocations with placeholders for unique words. In addition, we used the results
of a method for clustering words according to distributional similarity (Lin 1998) to
identify adjectival and verbal clues of subjectivity.
Table 9 summarizes the results of testing all of the above types of PSEs. All show
increased precision in the evaluations. Together, they show consistency in performance.
In almost all cases they perform better or worse on the same data sets, despite the
fact that different kinds of data and procedures are used to learn them. In addition,
PSEs learned using expression-level subjective-element data have precisions higher
than baseline on document-level opinion piece data, and vice versa.
Having a large stable of PSEs, it was important to disambiguate whether or not
PSE instances are subjective in the contexts in which they appear. We discovered that
the density of other potentially subjective expressions in the surrounding context is
important. If a clue is surrounded by a sufficient number of other clues, then it is
more likely to be subjective than if there were not. Parameter values were selected
using training data manually annotated at the expression level for subjective elements
and then tested on data annotated at the document level for opinion pieces. All of
the selected parameters led to increases in precision on the test data, and most lead to
increases over 100%. Once again we found consistency between expression-level and
document-level annotations. PSE sets defined by density have high precision in both
the subjective-element data and the opinion piece data. The large differences between
training and testing suggest that our results are not brittle.
Using a density feature selected from a training set, sentences containing high-
density PSEs were extracted from a separate test set, and manually annotated by two
judges. Fully 93% of the sentences extracted were found to be subjective or to be near
subjective sentences. Admittedly, the chosen density feature is a high-precision, low-
frequency one. But since the process is fully automatic, the feature could be applied to
more unannotated text to identify regions containing subjective sentences. In addition,
because the precision and frequency of the density features are stable across data sets,
lower-precision but higher-frequency options are available.
Finally, the value of the various types of PSEs was demonstrated with the task
of opinion piece classification. Using the k-nearest-neighbor classification algorithm
with leave-one-out cross-validation, a classification accuracy of 94% was achieved on
a large test set, with a reduction in error of 28% from the baseline.
Future work is required to determine how to exploit density features to improve
the performance of text categorization algorithms. Another area of future work is
searching for clues to objectivity, such as the politeness features used by Spertus (1997).
Still another is identifying the type of a subjective expression (e.g., positive or neg-
ative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on
classifying lexemes to the classification of instances in context (compare, e.g., ?great!?
and ?oh great.?)
In addition, it would be illuminating to apply our system to data annotated with
discourse trees (Carlson, Marcu, and Okurowski 2001). We hypothesize that most ob-
jective sentences identified by our system are dominated in the discourse by subjective
sentences and that we are moving toward identifying subjective discourse segments.
305
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Acknowledgments
We thank the anonymous reviewers for
their helpful and constructive comments.
This research was supported in part by the
Office of Naval Research under grants
N00014-95-1-0776 and N00014-01-1-0381.
References
Agrawal, Rakesh, Sridhar Rajagopalan,
Ramakrishnan Srikant, and Yirong Xu.
2003. Mining newsgroups using networks
arising from social behavior. In
Proceedings of the 12th International World
Wide Web Conference (WWW2003),
Budapest, May 20-24.
Alvarado, Sergio J., Michael G. Dyer, and
Margot Flowers. 1986. Editorial
comprehension in oped through
argument units. In Proceedings of the Fifth
National Conference on Artificial Intelligence
(AAAI-86), Philadelphia, August 11?15,
pages 250?256.
Anderson, Clifford W. and George C.
McMaster. 1989. Quantification of
rewriting by the Brothers Grimm: A
comparison of successive versions of
three tales. Computers and the Humanities,
23(4?5):341?346.
Aone, Chinatsu, Mila Ramos-Santacruz, and
William J. Niehaus. 2000. Assentor: An
NLP-based solution to e-mail monitoring.
In Proceedings of the 12th Innovative
Applications of Artificial Intelligence
Conference (IAAI-2000), Austin, TX,
August 1?3, pages 945?950.
Argamon, Shlomo, Moshe Koppel, and
Galit Avneri. 1998. Routing documents
according to style. In Proceedings of the
First International Workshop on Innovative
Internet Information Systems (IIIS-98), Pisa,
Italy, June 8?9.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
Barzilay, Regina, Michael Collins, Julia
Hirschberg, and Steve Whittaker. 2000.
The rules behind roles: Identifying
speaker role in radio broadcasts. In
Proceedings of the 17th National Conference on
Artificial Intelligence (AAAI-2000), Austin,
TX, July 30?August 3, pages 679?684.
Biber, Douglas. 1993. Co-occurrrence
patterns among collocations: A tool for
corpus-based lexical knowledge
acquisition. Computational Linguistics,
19(3):531?538.
Brill, Eric. 1992. A simple rule-based part of
speech tagger. In Proceedings of the 3rd
Conference on Applied Natural Language
Processing (ANLP-92), Trenton, Italy, April
1?3 pages 152?155.
Bruce, Rebecca and Janyce Wiebe. 1999.
Recognizing subjectivity: A case study of
manual tagging. Natural Language
Engineering, 5(2):187?205.
Carbonell, Jaime G. 1979. Subjective
Understanding: Computer Models of Belief
Systems. Ph.D. thesis, and Technical
Report no. 150, Department of Computer
Science, Yale University, New Haven, CT.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2001. Building a
discourse-tagged corpus in the
framework of rhetorical structure theory.
In Proceedings of the Second SIG dial
Workshop on Discourse and Dialogue
(SIGdial-2001), Aalborg, Denmark,
September 1?2, pages 30?39.
Chatman, Seymour. 1978. Story and
Discourse: Narrative Structure in Fiction and
Film. Cornell University Press, Ithaca, NY.
Church, Kenneth W. and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16:22?29.
Cohn, Dorrit. 1978. Transparent Minds:
Narrative Modes for Representing
Consciousness in Fiction. Princeton
University Press, Princeton, NJ.
Copeck, Terry, Kim Barker, Sylvain Delisle,
and Stan Szpakowicz. 2000. Automating
the measurement of linguistic features to
help classify texts as technical. In
Proceedings of the Seventh Conference on
Automatic NLP (TALN-2000), Lausanne,
Switzerland, October 16?18, pages
101?110.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word cooccurrence probabilities. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics
(ACL-94), Las Cruces, NM, June 27?30,
pages 272?278.
Dave, Kushal, Steve Lawrence, and
David M. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and
semantic classification of produce
reviews. In Proceedings of the 12th
International World Wide Web Conference
(WWW2003), Budapest, May 20?24.
Dolez?el, Lubomir. 1973. Narrative Modes in
Czech Literature. University of Toronto
Press, Toronto, Ontario, Canada.
Dyer, Michael G. 1982. Affect processing for
narratives. In Proceedings of the Second
National Conference on Artificial Intelligence
(AAAI-82), Pittsburgh, August 18?20,
pages 265?268.
Everitt, Brian S. 1977. The Analysis of
Contingency Tables. Chapman and Hall,
London.
306
Computational Linguistics Volume 30, Number 3
Fludernik, Monika. 1993. The Fictions of
Language and the Languages of Fiction.
Routledge, London.
Fodor, Janet Dean. 1979. The Linguistic
Description of Opaque Contexts, volume 13
of Outstanding Dissertations in Linguistics.
Garland, New York and London.
General-Inquirer, The. 2000. Available at
http://www.wjh.harvard.edu/?
inquirer/spreadsheet guide.htm.
Gordon, Andrew, Abe Kazemzadeh, Anish
Nair, and Milena Petrova. 2003.
Recognizing expressions of commonsense
psychology in English text. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03),
Sapporo, Japan, July 7?12, pages 208?215.
Hart, Roderick P. 1984. Systematic analysis
of political discourse: The development of
diction. In K. Sanders et al, editors,
Political Communication Yearbook: 1984.
Southern Illinois University Press,
Carbondale, pages 97?134.
Hatzivassiloglou, Vasileios and Kathy
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 12, pages 174?181.
Heise, David. 2000. Affect control theory.
Available at
http://www.indiana.edu/socpsy/ACT/
index.htm.
Hindle, Don. 1990. Noun classification from
predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-90), Pittsburgh, June 6?9, pages
268?275.
Hovy, Eduard. 1987. Generating Natural
Language under Pragmatic Constraints. Ph.D.
thesis, Yale University, New Haven, CT.
Karlgren, Jussi and Douglass Cutting. 1994.
Recognizing text genres with simple
metrics using discriminant analysis. In
Proceedings of the Fifteenth International
Conference on Computational Linguistics
(COLING-94), pages 1071?1075.
Karp, Daniel, Yves Schabes, Martin Zaidel,
and Dania Egedi. 1994. A freely available
wide coverage morphological analyzer for
English. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING-94), Nantes, France
pages 922?928.
Kaufer, David. 2000. Flaming: A white paper.
Available at www.eudora.com.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automatic
detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics (ACL-97),
Madrid, July 7?12, pages 32?38.
Kuroda, S.-Y. 1973. Where epistemology,
style and grammar meet: A case study
from the Japanese. In P. Kiparsky and
S. Anderson, editors, A Festschrift for
Morris Halle. Holt, Rinehart & Winston,
New York, pages 377?391.
Kuroda, S.-Y. 1976. Reflections on the
foundations of narrative theory?from a
linguistic point of view. In T. A. van Dijk,
editor, Pragmatics of Language and
Literature. North-Holland, Amsterdam,
pages 107?140.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), College Park, MD,
pages 25?32.
Lee, Lillian and Fernando Pereira. 1999.
Distributional similarity models:
Clustering vs. nearest neighbors. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD,
pages 33?40.
Lehnert, Wendy G., Michael Dyer, Peter
Johnson, C. J. Yang, and Steve Harley.
1983. BORIS: An Experiment in In-Depth
Understanding of Narratives. Artificial
Intelligence, 20:15?62.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics (ACL-98),
Montreal, August 10?14, pages 768?773.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), College Park, MD, pages
317?324.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge
sources for discourse segmentation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics
(ACL-95), Cambridge, MA, June 26?30,
pages 108?115.
Macleod, Catherine, Ralph Grishman, and
Adam Meyers. 1998. Complex syntax
reference manual. Technical report, New
York University.
Marcu, Daniel, Magdalena Romera, and
Estibaliz Amorrortu. 1999. Experiments in
constructing a corpus of discourse trees:
Problems, annotation choices, issues. In
Proceedings of the International Workshop on
Levels of Representation in Discourse
(LORID-99), Edinburgh, July 6?9 pages
71?78.
307
Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language
Marcus, Mitch, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mitchell, Tom. 1997. Machine Learning.
McGraw-Hill, Boston.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002),
Philadelphia, July 6?7, pages 79?86.
Quirk, Randolph, Sidney Greenbaum,
Geoffry Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, New York.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level Bootstrapping. In Proceedings
of the 16th National Conference on Artificial
Intelligence (AAAI-1999), Orlando, FL, July
18?22, pages 474?479.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for subjective
expressions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 105?112.
Riloff, Ellen, Janyce Wiebe, and Theresa
Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping. In
Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL-2003),
Edmonton, Alberta, Canada, May 31?June
1, pages 25?32.
Sack, Warren. 1995. Representing and
recognizing point of view. In Proceedings
of the AAAI Fall Symposium on AI
Applications in Knowledge Navigation and
Retrieval, Cambridge, MA, page 152.
Samuel, Ken, Sandra Carberry, and
K. Vijay-Shanker. 1998. Dialogue act
tagging with transformation-based
learning. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics (ACL-98), Montreal, August
10?14, pages 1150?1156.
Smajda, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19:143?177.
Spertus, Ellen. 1997. Smokey: Automatic
recognition of hostile messages. In
Proceedings of the Ninth Annual Conference
on Innovative Applications of Artificial
Intelligence (IAAI-97), Providence, RI, July
27?31, pages 1058?1065.
Stein, Dieter and Susan Wright, editors.
1995. Subjectivity and Subjectivisation.
Cambridge University Press, Cambridge.
Terveen, Loren, Will Hill, Brian Amento,
David McDonald, and Josh Creter. 1997.
Building task-specific interfaces to high
volume conversational data. In
Proceedings of the Conference on Human
Factors in Computing Systems (CHI-97), Los
Angeles, April 18?23, pages 226?233.
Teufel, Simone and Marc Moens. 2000.
What?s yours and what?s mine:
Determining intellectual attribution in
scientific texts. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing and the Workshop on
Very Large Corpora (EMNLP/VLC-2000),
Hong Kong, October 7?8, pages 9?17.
Tong, Richard. 2001. An operational system
for detecting and tracking opinions in
on-line discussions. In Working Notes of the
SIGIR Workshop on Operational Text
Classification, New Orleans, September
9?13, pages 1?6.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL-2000), Philadelphia, July 7?12, pages
417?424.
Uspensky, Boris. 1973. A Poetics of
Composition. University of California
Press, Berkeley, and Los Angeles.
van Dijk, Teun A. 1988. News as Discourse.
Erlbaum, Hillsdale, NJ.
Weeber, Marc, Rein Vos, and R. Harald
Baayen. 2000. Extracting the
lowest-frequency words: Pitfalls and
possibilities. Computational Linguistics,
26(3):301?317.
Wiebe, Janyce and Theresa Wilson. 2002.
Learning to disambiguate potentially
subjective expressions. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNLL-2002), Taipei, Taiwan,
pages 112?118.
Wiebe, Janyce. 1990. Recognizing Subjective
Sentences: A Computational Investigation of
Narrative Text. Ph.D. thesis, State
University of New York at Buffalo.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the 17th National Conference on Artificial
Intelligence (AAAI-2000), Austin, TX, July
30?August 3, pages 735?740.
Wiebe, Janyce, Eric Breck, Chris Buckley,
Claire Cardie, Paul Davis, Bruce Fraser,
Diane Litman, David Pierce, Ellen Riloff,
Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and
308
Computational Linguistics Volume 30, Number 3
organizing opinions expressed in the
world press. In Working Notes of the AAAI
Spring Symposium in New Directions in
Question Answering, Palo Alto, CA, pages
12?19.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second ACL SIGdial Workshop on Discourse
and Dialogue (SIGdial-2001), Aalborg,
Denmark, September 1?2, pages 186?195.
Wiebe, Janyce, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a
gold standard data set for subjectivity
classifications. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL-99), College
Park, MD, pages 246?253.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the Sixth
Workshop on Very Large Corpora (WVLC-98),
Montreal, August 15?16, pages 225?233.
Wiebe, Janyce and William J. Rapaport.
1986. Representing de re and de dicto belief
reports in discourse and narrative.
Proceedings of the IEEE, 74:1405?1413.
Wiebe, Janyce and William J. Rapaport.
1988. A computational theory of
perspective and reference in narrative. In
Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics
(ACL-88), Buffalo, NY, pages 131?138.
Wiebe, Janyce M. and William J. Rapaport.
1991. References in narrative text. Nou?s,
25(4):457?486.
Wiebe, Janyce, Theresa Wilson, and
Matthew Bell. 2001. Identifying
collocations for recognizing opinions. In
Proceedings of the ACL-01 Workshop on
Collocation: Computational Extraction,
Analysis, and Exploitation, Toulouse,
France, July 7, pages 24?31.
Wilson, Theresa and Janyce Wiebe. 2003.
Annotating opinions in the world press.
In Proceedings of the Fourth SIGdial Workshop
on Discourse and Dialogue (SIGdial-2003),
Sapporo, Japan, July 5?6, pages 13?22.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), Sapporo, Japan,
July 11?12, pages 129?136.
Exploiting Semantic Role Resources
for Preposition Disambiguation
Tom O?Hara?
University of Maryland, Baltimore County
Janyce Wiebe??
University of Pittsburgh
This article describes how semantic role resources can be exploited for preposition disambigua-
tion. The main resources include the semantic role annotations provided by the Penn Treebank
and FrameNet tagged corpora. The resources also include the assertions contained in the Fac-
totum knowledge base, as well as information from Cyc and Conceptual Graphs. A common
inventory is derived from these in support of definition analysis, which is the motivation for this
work.
The disambiguation concentrates on relations indicated by prepositional phrases, and is
framed as word-sense disambiguation for the preposition in question. A new type of feature for
word-sense disambiguation is introduced, usingWordNet hypernyms as collocations rather than
just words. Various experiments over the Penn Treebank and FrameNet data are presented, in-
cluding prepositions classified separately versus together, and illustrating the effects of filtering.
Similar experimentation is done over the Factotum data, including a method for inferring likely
preposition usage from corpora, as knowledge bases do not generally indicate how relationships
are expressed in English (in contrast to the explicit annotations on this in the Penn Treebank and
FrameNet). Other experiments are included with the FrameNet data mapped into the common
relation inventory developed for definition analysis, illustrating how preposition disambiguation
might be applied in lexical acquisition.
1. Introduction
English prepositions convey important relations in text. When used as verbal adjuncts,
they are the principal means of conveying semantic roles for the supporting entities
described by the predicate. Preposition disambiguation is a challenging problem. First,
prepositions are highly polysemous. A typical collegiate dictionary has dozens of
senses for each of the common prepositions. Second, the senses of prepositions tend
to be closely related to one another. For instance, there are three duplicate role assign-
ments among the twenty senses for of in The Preposition Project (Litkowski and
Hargraves 2006), a resource containing semantic annotations for common prepositions.
? Institute for Language and Information Technologies, Baltimore, MD 21250. E-mail:
tomohara@umbc.edu.
?? Department of Computer Science, Pittsburgh, PA 15260. E-mail: wiebe@cs.pitt.edu.
Submission received: 7 August 2006; accepted for publication: 21 February 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
Consider the disambiguation of the usages of on in the following sentences:
(1) The cut should be blocked on procedural grounds.
(2) The industry already operates on very thin margins.
The choice between the purpose and manner meanings for on in these sentences is
difficult. The purpose meaning seems preferred for sentence 1, as grounds is a type of
justification. For sentence 2, the choice is even less clear, though the manner meaning
seems preferred.
This article presents a new method for disambiguating prepositions using infor-
mation learned from annotated corpora as well as knowledge stored in declarative
lexical resources. The approach allows for better coverage and finer distinctions than
in previous work in preposition disambiguation. For instance, a traditional approach
would involvemanually developing rules for on that specify the semantic type of objects
associated with the different senses (e.g., time for temporal). Instead, we infer this based
on lexical associations learned from annotated corpora.
The motivation for preposition disambiguation is to support a system for lexical
acquisition (O?Hara 2005). The focus of the system is to acquire distinguishing infor-
mation for the concepts serving to define words. Large-scale semantic lexicons mainly
emphasize the taxonomic relations among the underlying concepts (e.g., is-a and part-
of ), and often lack sufficient differentiation among similar concepts (e.g., via attributes
or functional relations such as is-used-for). For example, in WordNet (Miller et al 1990),
the standard lexical resource for natural language processing, the only relations for
beagle andAfghan are that they are both a type of hound. Although the size difference can
be inferred from the definitions, it is not represented in the WordNet semantic network.
In WordNet, words are grouped into synonym sets called synsets, which represent
the underlying concepts and serve as nodes in a semantic network. Synsets are ordered
into a hierarchy using the hypernym relation (i.e., is-a). There are several other semantic
relations, such as part-whole, is-similar-to, and domain-of . Nonetheless, in version 2.1 of
WordNet, about 30% of the synsets for noun entries are not explicitly distinguished from
sibling synsets via semantic relations.
To address such coverage problems in lexicons, we have developed an empirical
approach to lexical acquisition, building upon earlier knowledge-based approaches in
dictionary definition analysis (Wilks, Slator, and Guthrie 1996). This involves a two-step
process: Definitions are first analyzed with a broad-coverage parser, and then the result-
ing syntactic relationships are disambiguated using statistical classification. A crucial
part of this process is the disambiguation of prepositions, exploiting online resources
with semantic role usage information. The main resources are the Penn Treebank
(PTB; Marcus et al 1994) and FrameNet (Fillmore, Wooters, and Baker 2001), two
popular corpora providing rich annotations on English text, such as the semantic roles
associatedwith prepositional phrases in context. In addition to the semantic role annota-
tions from PTB and FrameNet, traditional knowledge bases (KBs) are utilized to provide
training data for the relation classification. In particular, the FactotumKB (Cassidy 2000)
is used to provide additional training data for prepositions that are used to convey
particular relationships. Information on preposition usage is not explicitly encoded in
Factotum, so a new corpus analysis technique is employed to infer the associations.
Details on the lexical acquisition process, including application and evaluation, can
be found in O?Hara (2005). This article focuses on the aspects of this method relevant
to the processing of prepositions. In particular, here we specifically address preposition
152
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
disambiguation using semantic role annotations from PTB, FrameNet, and Factotum.
In each case, classification experiments are presented using the respective resources as
training data with evaluation via 10-fold cross validation.
This article is organized as follows. Section 2 presents background information on
the relation inventories used during classification, including one developed specifically
for definition analysis. Section 3 discusses the relation classifiers in depth with results
given for four different inventories. Section 4 discusses related work in relation disam-
biguation, and Section 5 presents our conclusions.
2. Semantic Relation Inventories
The representation of natural language utterances often incorporates the notion of
semantic roles, which are analogous to the slots in a frame-based representation. In
particular, there is an emphasis on the analysis of thematic roles, which serve to tie
the grammatical constituents of a sentence to the underlying semantic representation.
Thematic roles are also called case roles, because in some languages the grammatical
constituents are indicated by case inflections (e.g., ablative in Latin). As used here, the
term ?semantic role? refers to an arbitrary semantic relation, and the term ?thematic
role? refers to a relation intended to capture the semantics of sentences (e.g., event
participation).
Which semantic roles are used varies widely in Natural Language Processing
(NLP). Some systems use just a small number of very general roles, such as beneficiary.
At the other extreme, some systems use quite specific roles tailored to a particular
domain, such as catalyst in the chemical sense.
2.1 Background on Semantic Roles
Bruce (1975) presents an account of early case systems in NLP. For the most part,
those systems had limited case role inventories, along the lines of the cases defined by
Fillmore (1968). Palmer (1990) discusses some of the more contentious issues regarding
case systems, including adequacy for representation, such as reliance solely upon case
information to determine semantics versus the use of additional inference mechanisms.
Barker (1998) provides a comprehensive summary of case inventories in NLP, along
with criteria for the qualitative evaluation of case systems (generality, completeness, and
uniqueness). Linguistic work on thematic roles tends to use a limited number of roles.
Frawley (1992) presents a detailed discussion of twelve thematic roles and discusses
how they are realized in different languages.
During the shift in emphasis away from systems that work in small, self-contained
domains to those that can handle open-ended domains, there has been a trend towards
the use of larger sets of semantic primitives (Wilks, Slator, and Guthrie 1996). The
WordNet lexicon (Miller et al 1990) serves as one example of this. A synset is defined
in terms of its relations with any of the other 100,000+ synsets, rather than in terms of a
set of features like [?ANIMATE]. There has also been a shift in focus from deep under-
standing (e.g., story comprehension) facilitated by specially constructed KBs to shallow
surface-level analysis (e.g., text extraction) facilitated by corpus analysis. Both trends
seem to be behind the increase in case inventories in two relatively recent resources,
namely FrameNet (Fillmore, Wooters, and Baker 2001) and OpenCyc (OpenCyc 2002),
both of which define well over a hundred case roles. However, provided that the case
roles are well structured in an inheritance hierarchy, both paraphrasability and coverage
can be addressed by the same inventory.
153
Computational Linguistics Volume 35, Number 2
2.2 Inventories Developed for Corpus Annotation
With the emphasis on corpus analysis in computational linguistics, there has been a
shift away from relying on explicitly-coded knowledge towards the use of knowledge
inferred from naturally occurring text, in particular text that has been annotated by
humans to indicate phenomena of interest. For example, rather than manually devel-
oping rules for preferring one sense of a word over another based on context, the
most successful approaches have automatically learned the rules based on word-sense
annotations, as evidenced by the Senseval competitions (Kilgarriff 1998; Edmonds and
Cotton 2001).
The Penn Treebank version II (Marcus et al 1994) provided the first large-scale set
of case annotations for general-purpose text. These are very general roles, following
Fillmore (1968). The Berkeley FrameNet (Fillmore, Wooters, and Baker 2001) project
currently provides the most comprehensive set of semantic roles annotations. These are
at a much finer granularity than those in PTB, making them quite useful for applications
learning semantics from corpora. Relation disambiguation experiments for both of these
role inventories are presented subsequently.
2.2.1 Penn Treebank. The original PTB (Marcus, Santorini, and Marcinkiewicz 1993) pro-
vided syntactic annotations in the form of parse trees for text from theWall Street Journal.
This resource is very popular in computational linguistics, particularly for inducing
part-of-speech taggers and parsers. PTB version II (Marcus et al 1994) added 20 func-
tional tags, including a few thematic roles such as temporal, direction, and purpose. These
can be attached to any verb complement but normally occur with clauses, adverbs, and
prepositions.
For example, Figure 1 shows a parse tree using the extended annotation format.
In addition to the usual syntactic constituents such as NP and VP, function tags are
included. For example, the second NP gives the subject. This also shows that the first
prepositional phrase (PP) indicates the time frame, whereas the last PP indicates the
Sentence:
In 1982, Sports & Recreation?s managers and certain passive investors purchased the
company from Brunswick Corp. of Skokie, Ill.
Parse:
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) & (NP Recreation) ?s)
managers)
and (NP certain passive investors))
(VP purchased
(NP the company)
(PP-CLR from closely related
(NP (NP Brunswick Corp.)
(PP-LOC of locative
(NP (NP Skokie) , (NP Ill)))
))) .)
Figure 1
Penn Treebank II parse tree annotation sample. The functional tags are shown in boldface.
154
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 1
Frequency of Penn Treebank II semantic role annotations. Relative frequencies estimated over the
counts for unique assignments given in the PTB documentation (bkt tags.lst), and descriptions
based on Bies et al (1995). Omits low-frequency benefactive role. The syntactic role annotations
generally have higher frequencies; for example, the subject role occurs 49% of the time (out of
about 240,000 total annotations).
Role Freq. Description
temporal .113 indicates when, how often, or how long
locative .075 place/setting of the event
direction .026 starting or ending location (trajectory)
manner .021 indicates manner, including instrument
purpose .017 purpose or reason
extent .010 spatial extent
location. The second PP is tagged as closely-related, which is one of the miscellaneous
PTB function tags that are more syntactic in nature: ?[CLR] occupy somemiddle ground
between arguments and adjunct? (Bies et al 1995). Frequency information for the
semantic role annotations is shown in Table 1.
2.2.2 FrameNet. FrameNet (Fillmore, Wooters, and Baker 2001) is striving to develop an
English lexicon with rich case structure information for the various contexts that words
can occur in. Each of these contexts is called a frame, and the semantic relations that
occur in each frame are called frame elements (FE). For example, in the communica-
tion frame, there are frame elements for communicator, message, medium, and so forth.
FrameNet annotations occur at the phrase level instead of the grammatical constituent
level as in PTB. Figure 2 shows an example.
Table 2 displays the top 25 semantic roles by frequency of annotation. This shows
that the semantic roles in FrameNet can be quite specific, as with the roles cognizer,
evaluee, and addressee. In all, there are over 780 roles annotated with over 288,000 tagged
instances.
Sentence:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling stand-
alone workstations to communicate over public or private ISDN networks.
Annotation:
Hewlett-Packard Co has rolled out a new range of ISDN connectivity enabling
?C FE=?Communicator? PT=?NP??standalone workstations?/C?
to ?C TARGET=?y??communicate?/C?
?C FE=?Medium? PT=?PP??over public or private ISDN networks?/C? .
Figure 2
FrameNet annotation sample. The constituent (C) tags identify the phrases that have been
annotated. The frame element (FE) attributes indicate the semantic roles, and the phrase type
(PT) attributes indicate the traditional grammatical category for the phrase. For simplicity, this
example is formatted in the earlier FrameNet format, but the information is taken from the
latest annotations (lu5.xml).
155
Computational Linguistics Volume 35, Number 2
Table 2
Common FrameNet semantic roles. The top 25 of 773 roles are shown, representing nearly half of
the total annotations (about 290,000). Descriptions based on FrameNet 1.3 frame documentation.
Role Freq. Description
agent .037 person performing the intentional act
theme .031 object being acted on, affected, etc.
experiencer .029 being who has a physical experience, etc.
goal .028 endpoint of the path
speaker .028 individual that communicates the message
stimulus .026 entity that evokes response
manner .025 manner of performing an action, etc.
degree .024 degree to which event occurs
self-mover .023 volitional agent that moves
message .021 the content that is communicated
path .020 the trajectory of motion, etc.
cognizer .018 person who perceives the event
source .017 the beginning of the path
time .016 the time at which the situation occurs
evaluee .016 thing about which a judgment has been made
descriptor .015 attributes, traits, etc. of the entity
body-part .014 location on the body of the experiencer
content .014 situation or state-of-affairs that attention is focused on
topic .014 subject matter of the communicated message, etc.
item .012 entity whose scalar property is specified
target .011 entity which is hit by a projectile
garment .011 clothing worn
addressee .011 entity that receives a message from the communicator
protagonist .011 person to whom a mental property is attributed
communicator .010 the person who communicates a message
2.3 Other
A recent semantic role resource that is starting to attract interest is the Proposition Bank
(PropBank), developed at the University of Pennsylvania (Palmer, Gildea, and Kings-
bury 2005). It extends the Penn Treebank with information on verb subcategorization.
The focus is on annotating all verb occurrences and all their argument realizations that
occur in the Wall Street Journal, rather than select corpus examples as in FrameNet.
Therefore, the role inventory is heavily verb-centric, for example, with the generic labels
arg0 through arg4 denoting the main verbal arguments to avoid misinterpretations.
Verbal adjuncts are assigned roles based on PTB version II (e.g., argM-LOC and argM-
TMP). PropBank has been used as the training data in recent semantic role labeling
competitions as part of the Conferences on Computational Natural Language Learn-
ing (Carreras and Ma`rquez 2004, 2005). Thus, it is likely to become as influential as
FrameNet in computational semantics.
The Preposition Project similarly adds information to an existing semantic role
resource, namely FrameNet. It is being developed by CL Research (Litkowski and
Hargraves 2006) and endeavors to provide comprehensive syntactic and semantic in-
formation on various usages of prepositions, which often are not represented well
in semantic lexicons (e.g., they are not included at all in WordNet). The Preposition
Project uses the sense distinctions from the Oxford Dictionary of English and integrates
syntactic information about prepositions from comprehensive grammar references.
156
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
2.4 Inventories for Knowledge Representation
This section describes three case inventories: one developed for the Cyc KB (Lenat
1995), one used to define Conceptual Graphs (Sowa 1984), and one for the Factotum
KB (Cassidy 2000). The first two are based on a traditional knowledge representation
paradigm. With respect to natural language processing, these approaches are more
representative of the earlier approaches in which deep understanding is the chief goal.
Factotum is also based on a knowledge representation paradigm, but in a sense also
reflects the empirical aspect of the corpus annotation approach, because the annotations
were developed to address the relations implicit in Roget?s Thesaurus.
In this article, relation disambiguation experiments are only presented for Facto-
tum, given that the others do not readily provide sufficient training data. However, the
other inventories are discussed because each provides relation types incorporated into
the inventory used below for the definition analysis (see Section 3.5).
2.4.1 Cyc. The Cyc system (Lenat 1995) is the most ambitious knowledge representation
project undertaken to date, in development since 1984. The full Cyc KB is propri-
etary, which has hindered its adoption in natural language processing. However, to
encourage broader usage, portions of the KB have been made freely available to the
public. For instance, there is an open-source version of the system called OpenCyc
(www.opencyc.org), which covers the upper part of the KB and also includes the Cyc
inference engine, KB browser, and other tools. In addition, researchers can obtain access
to ResearchCyc, which contains most of the KB except for proprietary information (e.g.,
internal bookkeeping assertions).
Cyc uses a wide range of role types: very general roles (e.g., beneficiary); commonly
occurring situational roles (e.g., victim); and highly specialized roles (e.g., catalyst). Of
the 8,756 concepts in OpenCyc, 130 are for event-based roles (i.e., instances of actor-
slot) with 51 other semantic roles (i.e., other instances of role). Table 3 shows the most
commonly used event-based roles in the KB.
2.4.2 Conceptual Graphs. The Conceptual Graphs (CG) mechanism was introduced by
Sowa (1984) for knowledge representation as part of his Conceptual Structures theory.
The original text listed two dozen or so thematic relations, such as destination and
initiator. In all, 37 conceptual relations were defined. This inventory formed the basis
for most work in Conceptual Graphs. Recently, Sowa (1999) updated the inventory to
allow for better hierarchical structuring and to incorporate the important thematic roles
identified by Somers (1987). Table 4 shows a sample of these roles, along with usage
estimates based on corpus analysis (O?Hara 2005).
2.4.3 Factotum. The Factotum semantic network (Cassidy 2000) developed by Micra,
Inc., makes explicit many of the relations in Roget?s Thesaurus.1 Outside of proprietary
resources such as Cyc, Factotum is themost comprehensive KBwith respect to functional
relations, which are taken here to be non-hierarchical relations, excluding attributes.
OpenCyc does include definitions of many non-hierarchical relations. However, there
are not many instantiations (i.e., relationship assertions), because it concentrates on the
higher level of the ontology.
1 Factotum is based on the public domain version of Roget?s Thesaurus. The latter is freely available via
Project Gutenberg (http://promo.net/pg), thanks to Micra, Inc.
157
Computational Linguistics Volume 35, Number 2
Table 3
Most common event-based roles in OpenCyc. Descriptions based on comments from the
OpenCyc knowledge base (version 0.7). Relative frequencies based on counts obtained
via Cyc?s utility functions.
Role Freq. Description
done-by .178 relates an event to its ?doer?
performed-by .119 doer deliberately does act
object-of-state-change .081 object undergoes some kind of intrinsic change of state
object-acted-on .057 object is altered or affected in event
outputs-created .051 object comes into existence sometime during event
transporter .044 object facilitating conveyance of transportees
transportees .044 object being moved
to-location .041 where the moving object is found when event ends
object-removed .036 object removed from its previous location
inputs .036 pre-existing event participant destroyed or incorporated
into a new entity
products .035 object is one of the intended outputs of event
inputs-destroyed .035 object exists before event and is destroyed during event
from-location .034 where some moving-object in the move is found at the
beginning
primary-object-moving .033 object is in motion at some point during the event, and
this movement is focal
seller .030 agent sells something in the exchange
object-of-possession-transfer .030 rights to use object transferred from one agent to another
transferred-thing .030 object is being moved, transferred, or exchanged in the
event transfer
sender-of-info .030 sender is an agent who is the source of information
transferred
inputs-committed .028 object exists before event and continues to exist
afterwards, and as a result of event, object becomes
incorporated into something created during event
object-emitted .026 object is emitted from the emitter during the emission
event
The Factotum knowledge base is based on the 1911 version of Roget?s Thesaurus
and specifies the relations that hold between the Roget categories and the words listed
in each entry. Factotum incorporates information from other resources as well. For
instance, the Unified Medical Language System (UMLS) formed the basis for the initial
inventory of semantic relations, which was later revised during tagging.
Figure 3 shows a sample from Factotum. This illustrates that the basic Roget or-
ganization is still used, although additional hierarchical levels have been added. The
relations are contained within double braces (e.g., ?{{has subtype}}?) and generally
apply from the category to each word in the synonym list on the same line. For example,
the line with ?{{result of}}? indicates that conversion is the result of transforming,
as shown in the semantic relation listing that would be extracted. There are over 400
different relations instantiated in the knowledge base, which has over 93,000 assertions.
Some of these are quite specialized (e.g., has-brandname). In addition, there are quite a
few inverse relations, becausemost of the relations are not symmetrical. Certain features
of the knowledge representation are ignored during the relation extraction used later.
For example, relation specifications can have qualifier prefixes, such as an ampersand
to indicate that the relationship only sometimes holds.
158
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 4
Common semantic roles used in Conceptual Graphs. Inventory and descriptions based on
Sowa (1999, pages 502?510). The term situation is used in place of Sowa?s nexus (i.e., ?fact of
togetherness?), which also covers spatial structures. Freq. gives estimated relative frequencies
from O?Hara (2005).
Role Freq. Description
agent .267 entity voluntarily initiating an action
attribute .155 entity that is a property of some object
characteristic .080 types of properties of entities
theme .064 participant involved with but not changed
patient .061 participant undergoing structural change
location .053 participant of a spatial situation
possession .035 entity owned by some animate being
part .035 object that is a component of some object
origin .035 source of a spatial or ambient situation
experiencer .035 animate goal of an experience
result .032 inanimate goal of an act
instrument .027 resource used but not changed
recipient .019 animate goal of an act
destination .013 goal of a spatial process
point-in-time .011 participant of a temporal situation
path .011 resource of a spatial or ambient situation
accompaniment .011 object participating with another
effector .008 source involuntarily initiating an action
beneficiary .008 entity benefiting from event completion
matter .005 resource that is changed by the event
manner .005 entity that is a property of some process
source .003 present at beginning of activity
resource .003 material necessary for situation
product .003 present at end of activity
medium .003 resource for transmitting information
goal .003 final cause which is purpose or benefit
duration .003 resource of a temporal process
because .003 situation causing another situation
amount .003 a measure of some characteristic
Table 5 shows themost common relations in terms of usage in the semantic network,
and includes others that are used in the experiments discussed later.2 The relative
frequencies just reflect relationships explicitly labeled in the KB data file. For instance,
this does not account for implicit has-subtype relationships based on the hierarchical
organization of the thesaural groups (e.g., ?simple-change, has-subtype, conversion?).
The functional relations are shown in boldface. This excludes the meronym or part-
whole relations (e.g., is-conceptual-part-of ), in line with their classification by Cruse
(1986) as hierarchical relations. The reason for concentrating on the functional relations
is that these are more akin to the roles tagged in PTB and FrameNet.
The information in Factotum complements WordNet through the inclusion of more
functional relations (e.g., non-hierarchical relations such as uses and is-function-of ). For
comparison purposes, Table 6 shows the semantic relation usage in WordNet version
2 The database files and documentation for the semantic network are available from Micra, Inc., via
ftp://micra.com/factotum.
159
Computational Linguistics Volume 35, Number 2
Original data:
A. ABSTRACT RELATION
...
A6 CHANGE (R140 TO R152)
...
A6.1 SIMPLE CHANGE (R140)
...
A6.1.4 CONVERSION (R144)
#144. Conversion.
N. {{has subtype(change, R140)}} conversion, transformation.
{{has case: @R7, initial state, final state}}.
{{has patient: @R3a, object, entity}}.
{{result of}} {{has subtype(process, A7.7)}} converting, transforming.
{{has subtype}} processing.
transition.
Extracted relationships:
?change, has-subtype, conversion? ?change, has-subtype, transformation?
?conversion, has-case, initial state? ?conversion, has-case, final state?
?conversion, has-patient, object? ?conversion, has-patient, entity?
?conversion, is-result-of , converting? ?conversion, is-result-of , transforming?
?process, has-subtype, converting? ?process, has-subtype, transforming?
?conversion, has-subtype, processing?
Figure 3
Sample data from Factotum. Based on version 0.56 of Factotum.
2.1. As can be seen from the table, the majority of the relations are hierarchical.3
WordNet 2.1 averages just about 1.1 non-taxonomic properties per concept (includ-
ing inverses but excluding hierarchical relations such as has-hypernym and is-member-
meronym-of ). OpenCyc provides a much higher average at 3.7 properties per concept,
although with an emphasis on argument constraints and other usage restrictions. Fac-
totum averages 1.8 properties per concept, thus complementing WordNet in terms of
information content.4
2.5 Combining the Different Semantic Role Inventories
It is difficult to provide precise comparisons of the five inventories just discussed. This is
due both to the different nature of the inventories (e.g., developed for knowledge bases
as opposed to being derived from natural language annotations) and due to the way the
3 In WordNet, the is-similar-to relation for adjectives can be considered as hierarchical, as it links satellite
synsets to heads of adjective clusters (Miller 1998). For example, the satellite synsets for ?thirsty? and
?rainless? are both linked to the head synset for ?dry (vs. wet).?
4 These figures are derived by counting the number of relations excluding the instance and subset ones
and then dividing by the number of concepts (i.e., ratio of non-hierarchical relations to concepts). Cyc?s
comments and lexical assertions are also excluded, as these are implicit in Factotum and WordNet.
WordNet?s is-derived-from relations are omitted as lexical in nature (the figure otherwise would be 1.6).
160
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 5
Common Factotum semantic roles. These account for 80% of the instances. Boldface relations are
used in the experiments (Section 3.4.2).
Relation Freq. Description
has-subtype .401 inverse of is-a relation
is-property-of .077 object with given salient character
is-caused-by .034 force that is the origin of something
has-property .028 salient property of an object
has-part .022 a part of a physical object
has-high-intensity .018 intensifier for property or characteristic
has-high-level .017 implication of activity (e.g., intelligence)
is-antonym-of .016 generally used for lexical opposition
is-conceptual-part-of .015 parts of other entities (e.g., case relations)
has-metaphor .014 non-literal reference to the word
causesmental .013 motivation (causation in the mental realm)
uses .012 a tool needing active manipulation
is-performed-by .012 human actor for the event
performshuman .011 human role in performing some activity
is-function-of .011 artifact passively performing the function
has-result .010 more specific type of causes
has-conceptual-part .010 generalization of has-part
is-used-in .010 activity or desired effect for the entity
is-part-of .010 distinguishes part from group membership
causes .009 inverse of is-caused-by
has-method .009 method used to achieve some goal
is-caused-bymental .009 inverse of causesmental
has-consequence .008 causation due to a natural association
has-commencement .007 state that commences with the action
is-location-of .007 absolute location of an object
requires .004 object or sub-action needed for an action
is-studied-in .004 inquires into any field of study
is-topic-of .002 communication dealing with given subject
produces .002 what an action yields, generates, etc.
is-measured-by .002 instrument for measuring something
is-job-of .001 occupation title for a job function
is-patient-of .001 action that the object participates in
is-facilitated-by .001 object or sub-action aiding an action
is-biofunction-of .0003 biological function of parts of living things
was-performed-by .0002 is-performed-by occurring in the past
has-consequenceobject .0002 consequence for the patient of an action
is-facilitated-bymental .0001 trait that facilitates some human action
relation listings were extracted (e.g., just including event-based roles from OpenCyc).
As can be seen from Tables 2 and 3, FrameNet tends to refine the roles for agents (e.g.,
communicator) compared to OpenCyc, which in contrast has more refinements of the
object role (e.g., object-removed). The Concept Graphs inventory includes more emphasis
on specialization relations than the others, as can be seen from the top entries in Table 4
(e.g., attribute).
In the next section, we show how classifiers can be automatically developed for
the semantic role inventories just discussed. For the application to dictionary defin-
ition analysis, we need to combine the classifiers learned over PTB, FrameNet, and
Factotum. This can be done readily in a cascaded fashion with the classifier for the
most specific relation inventory (i.e., FrameNet) being used first and then the other
classifiers being applied in turn whenever the classification is inconclusive. This would
161
Computational Linguistics Volume 35, Number 2
Table 6
Semantic relation usage in WordNet. Relative frequencies for semantic relations in WordNet
(173,570 total instances). This table omits lexical relations, such as the is-derived-from relation
(71,914 instances). Frequencies based on analysis of database files for WordNet 2.1.
Relation Freq. Description
has-hypernym .558 superset relation
is-similar-to .130 similar adjective synset
is-member-meronym-of .071 constituent member
is-part-meronym-of .051 constituent part
is-pertainym-of .046 noun that adjective pertains to
is-antonym-of .046 opposing concept
has-topic-domain .038 topic domain for the synset
also-see .019 related entry (for adjectives and verbs)
has-verb-group .010 verb senses grouped by similarity
has-region-domain .008 region domain for the synset
has-attribute .007 related attribute category or value
has-usage-domain .007 usage domain for the synset
is-substance-meronym-of .004 constituent substance
entails .002 action entailed by the verb
causes .001 action caused by the verb
has-participle .001 verb participle
have the advantage that new resources could be integrated into the combined relation
classifier with minimal effort. However, the resulting role inventory would likely be
heterogeneous and might be prone to inconsistent classifications. In addition, the role
inventory could change whenever new annotation resources are incorporated, making
the overall definition analysis system somewhat unpredictable.
Alternatively, the annotations can be converted into a common inventory, and a
separate relation classifier induced over the resulting data. This has the advantage
that the target relation-type inventory remains stable whenever new sources of relation
annotations are introduced. In addition, the classifier will likely be more accurate as
there are more examples per relation type on average. The drawback, however, is that
annotations from new resources must first be mapped into the common inventory
before incorporation.
The latter approach is employed here. The common inventory incorporates some of
the general relation types defined by Gildea and Jurafsky (2002) for their experiments
in classifying semantic relations in FrameNet using a reduced relation inventory. They
defined 18 relations (including a special-case null role for expletives), as shown in
Table 7. These roles served as the starting point for the common relation inventory
we developed to support definition analysis (O?Hara 2005), with half of the roles used
as is and a few others mapped into similar roles. In total, twenty-six relations are
defined, including a few roles based on the PTB, Cyc, and Conceptual Graphs inven-
Table 7
Abstract roles defined by Gildea and Jurafsky based on FrameNet. Taken from Gildea and
Jurafsky (2002).
agent cause degree experiencer force goal
instrument location manner null path patient
percept proposition result source state topic
162
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 8
Inventory of semantic relations for definition analysis. This inventory is inspired by the roles in
Table 7 and is primarily based on FrameNet (Fillmore, Wooters, and Baker 2001) and Conceptual
Graphs (Sowa 1999); it also includes roles based on the PTB and Cyc inventories.
Relation Description
accompaniment entity that participates with another entity
agent entity voluntarily performing an action
amount quantity used as a measure of some characteristic
area region in which the action takes place
category general type or class of which the item is an instance
cause non-agentive entity that produces an effect
characteristic general properties of entities
context background for situation or predication
direction either spatial source or goal (same as in PTB)
distance spatial extent of motion
duration period of time that the situation applies within
experiencer entity undergoing some (non-voluntary) experience
goal location that an affected entity ends up in
instrument entity or resource facilitating event occurrence
location reference spatial location for situation
manner property of the underlying process
means action taken to affect something
medium setting in which an affected entity is conveyed
part component of entity or situation
path trajectory which is neither a source nor a goal
product entity present at end of event (same as Cyc products)
recipient recipient of the resource(s)
resource entity utilized during event (same as Cyc inputs)
source initial position of an affected entity
theme entity somehow affected by the event
time reference time for situation
tories. Table 8 shows this role inventory along with a description of each case. In
addition to traditional thematic relations, this includes a few specialization relations,
which are relevant to definition analysis. For example, characteristic corresponds to the
general relation from Conceptual Graphs for properties of entities; and category gen-
eralizes the corresponding FrameNet role, which indicates category type, to subsume
other FrameNet roles related to categorization (e.g., topic). Note that this inventory is
not meant to be definitive and has been developed primarily to address mappings from
FrameNet for the experiments discussed in Section 3.5. Thus, it is likely that additional
roles will be required when additional sources of semantic relations are incorporated
(e.g., Cyc). Themappingswere producedmanually by reviewing the role descriptions in
the FrameNet documentation and checking prepositional usages for each to determine
which of the common inventory roles might be most relevant. As some of the roles with
the same name have frame-specific meanings, in a few cases this involved conflicting
usages (e.g., body-part associated with both area and instrument), which were resolved in
favor of the more common usage.5
5 See www.cs.nmsu.edu/~tomohara/cl-prep-article/relation-mapping.html for the mapping,
covering cases occurring at least 50 times in FrameNet.
163
Computational Linguistics Volume 35, Number 2
3. Preposition Disambiguation
This section presents the results of our experiments on the disambiguation of relations
indicated by prepositional phrases. Results are given for PTB, FrameNet, and Factotum.
The PTB roles are general: For example, for the preposition for, there are six distinctions
(four, with low-frequency pruning). The PTB role disambiguation experiments thus
address a coarse form of sense distinction. In contrast, the FrameNet distinctions are
quite specific: there are 192 distinctions associatedwith for (21 with low-frequency prun-
ing); and, there are 17 distinctions in Factotum (15 with low-frequency pruning). Our
FrameNet and Factotum role disambiguation experiments thus address fine-grained
sense distinctions.
3.1 Overview
A straightforward approach for preposition disambiguation would be to use typical
word-sense disambiguation features, such as the parts-of-speech of surrounding words
and, more importantly, collocations (e.g., lexical associations). Although this can be
highly accurate, it tends to overfit the data and to generalize poorly. The latter is of
particular concern here as the training data is taken from a different genre than the
application data. For example, the PTB data is from newspaper text (specifically, Wall
Street Journal), but the lexical acquisition is based on dictionary definitions. We first
discuss how class-based collocations address this problem and then present the features
used in the experiments.
Before getting into technical details, an informal example will be used to motivate
the use of hypernym collocations. Consider the following purpose role examples, which
are similar to the first example from the introduction.
(3) This contention would justify dismissal of these actions onpurpose
prudential grounds.
(4) Ramada?s stock rose 87.5 cents onpurpose the news.
It turns out that grounds and news are often used as the prepositional object in PTB
when the sense for on is purpose (or reason). Thus, these words would likely be chosen as
collocations for this sense. However, for the sake of generalization, it would be better to
choose theWordNet hypernym subject matter, as that subsumes both words. This would
then allow the following sentence to be recognized as indicating purpose even though
censurewas not contained in the training data.
(5) Senator sets hearing onpurpose censure of Bush.
3.1.1 Class-Based Collocations via Hypernyms. To overcome data sparseness problems, a
class-based approach is used for the collocations, with WordNet synsets as the source
of the word classes. (Part-of-speech tags are a popular type of class-based feature used
in word sense disambiguation (WSD) to capture syntactic generalizations.) Recall that
the WordNet synset hierarchy can be viewed as a taxonomy of concepts. Therefore, in
addition to using collocations in the form of other words, we use collocations in the
form of semantic concepts.
Word collocation features are derived by making two passes over the training
data (e.g., ?on? sentences with correct role indicated). The first pass tabulates the
164
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
co-occurrence counts for each of the context words (i.e., those in a window around the
target word) paired with the classification value for the given training instance (e.g.,
the preposition sense from the annotation). These counts are used to derive conditional
probability estimates of each class value given co-occurrence of the various potential
collocates. The words exceeding a certain threshold are collected into a list associated
with the class value, making this a ?bag of words? approach. In the experiments dis-
cussed below, a potential collocate (coll) is selected whenever the conditional probability
for the class (C) value exceeds the prior probability by a factor greater than 20%:6
P(C|coll)? P(C)
P(C)
? .20 (1)
That is, for a given potential collocation word (coll) to be treated as one of the ac-
tual collocation words, the relative percent change of the class conditional probability
(P(C|coll)) versus the prior probability for the class value (P(C)) must be 20% or higher.
The second pass over the training data determines the value for the collocational feature
of each classification category by checking whether the current context window has any
of the associated collocation words. Note that for the test data, only the second pass is
made, using the collocation lists derived from the training data.
In generalizing this to a class-based approach, the potential collocational words are
replaced with each of their hypernym ancestors fromWordNet. The adjective hierarchy
is relatively shallow, so it is augmented by treating is-similar-to as has-hypernym. For
example, the synset for ?arid? and ?waterless? is linked to the synset for ?dry (vs.
wet).? Adverbs would be included, but there is no hierarchy for them. Because the co-
occurring words are not sense-tagged, this is done for each synset serving as a different
sense of the word. Likewise, in the case of multiple inheritance, each parent synset is
used. For example, given the co-occurring wordmoney, the counts would be updated as
if each of the following tokens were seen (grouped by sense).
1. { medium of exchange#1, monetary system#1, standard#1, criterion#1,
measure#2, touchstone#1, reference point#1, point of reference#1, ref-
erence#3, indicator#2, signal#1, signaling#1, sign#3, communication#2,
social relation#1, relation#1, abstraction#6 }
2. { wealth#4, property#2, belongings#1, holding#2, material possession#1,
possession#2 }
3. { currency#1, medium of exchange#1, monetary system#1, standard#1,
criterion#1, measure#2, touchstone#1, reference point#1, point of -
reference#1, reference#3, indicator#2, signal#1, signaling#1, sign#3,
communication#2, social relation#1, relation#1, abstraction#6 }
Thus, the word token money is replaced by 41 synset tokens. Then, the same two-pass
process just described is performed over the text consisting of the replacement tokens.
Although this introduces noise due to ambiguity, the conditional-probability selection
scheme (Wiebe, McKeever, and Bruce 1998) compensates by selecting hypernym synsets
that tend to co-occur with specific roles.
6 The 20% threshold is a heuristic that is fixed for all experiments. We tested automatic threshold derivation
for Senseval-3 and found that the optimal percentage differed across training sets. As values near 20%
were common, it is left fixed rather than adding an additional feature-threshold refinement step.
165
Computational Linguistics Volume 35, Number 2
Note that there is no preference in the system for choosing either specific or general
hypernyms. Instead, they are inferred automatically based on the word to be disam-
biguated (i.e., preposition for these experiments). Hypernyms at the top levels of the
hierarchy are less likely to be chosen, as they most likely occur with different senses for
the same word (as with relation#1 previously). However, hypernyms at lower levels
tend not to be chosen, as there might not be enough occurrences due to other co-
occurring words. For example, wealth#4 is unlikely to be chosen as a collocation for
the second sense of money, as only a few words map into it, unlike property#2. The
conditional-probability selection scheme (i.e., Equation (1)) handles this automatically
without having to encode heuristics about hypernym rank, and so on.
3.1.2 Classification Experiments. A supervised approach for word-sense disambiguation
is used following Bruce and Wiebe (1999).
For each experiment, stratified 10-fold cross validation is used: The classifiers are
repeatedly trained on 90% of the data and tested on the remainder, with the test sets
randomly selected to form a partition. The results described here were obtained using
the settings in Figure 4, which are similar to the settings used by O?Hara et al (2004)
in the third Senseval competition. The top systems from recent Senseval competitions
(Mihalcea 2002; Grozea 2004) use a variety of lexical features for WSD. Words in the im-
mediate context (Word?i) and their parts of speech (POS?i) are standard features. Word
collocations are also common, but there are various ways of organizing collocations into
features (Wiebe, McKeever, and Bruce 1998). We use the simple approach of having a
single binary feature per sense (e.g., role) that is set true whenever any of the associated
collocation words for that sense are encountered (i.e., per-class-binary).
The main difference of our approach from more typical WSD systems (Mihalcea,
Chklovski, and Kilgarriff 2004) concerns the hypernym collocations. The collocation
context section of Figure 4 shows that word collocations can occur anywhere in the
sentence, whereas hypernym collocations must occur within five words of the target
Features:
Prep: preposition being classified
POS?i: part-of-speech of word at offset i
Word?i: stem of word at offset i
WordCollr: context has word collocation for role r
HypernymCollr: context has hypernym collocation for role r
Collocation context:
Word: anywhere in the sentence
Hypernym: within 5 words of target preposition
Collocation selection:
Frequency: f (word) > 1
Conditional probability: P(C|coll) ? .50
Relative percent change: (P(C|coll)? P(C))/P(C) ? .20
Organization: per-class-binary
Model selection:
C4.5 Decision tree via Weka?s J4.8 classifier (Quinlan 1993; Witten and Frank 1999)
Figure 4
Feature settings used in preposition classification experiments. Aspects that differ from a typical WSD
system are italicized.
166
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
prepositions (i.e., a five-word context window).7 This reduced window size is used
to make the hypernym collocations more related to the prepositional object and the
modified term.
The feature settings in Figure 4 are used in three different configurations: word-
based collocations alone, hypernym collocations alone, and both collocations together.
Combining the two types generally produces the best results, because this balances the
specific clues provided by the word collocations with the generalized clues provided by
the hypernym collocations.
Unlike the general case for WSD, the sense inventory is the same for all the words
being disambiguated; therefore, a single classifier can be produced rather than indi-
vidual classifiers. This has the advantage of allowing more training data to be used
in the derivation of the clues indicative of each semantic role. However, if there were
sufficient annotations for particular preposition, then it would be advantageous to have
a dedicated classifier. For example, the prior probabilities for the roles would be based
on the usages for the given preposition. Therefore, we perform experiments illustrating
the difference when disambiguating prepositions with a single classifier versus the use
of separate classifiers.
3.2 Penn Treebank Classification Experiments
The first set of experiments deals with preposition disambiguation using PTB. When
deriving training data from PTB via the parse tree annotations, the functional tags as-
sociated with prepositional phrases are converted into preposition sense tags. Consider
the following excerpt from the sample annotation for PTB shown earlier:
(6)
(S (PP-TMP In (NP 1982)), temporal extent
(NP-SBJ grammatical subject
(NP (NP (NP Sports) & (NP Recreation) ?s)
managers) ...
Treating temporal as the preposition sense yields the following annotation:
(7) InTMP 1982, Sports & Recreation?s managers ...
The relative frequencies of the roles in the PTB annotations for PPs are shown in Ta-
ble 9. As can be seen, several of the roles do not occur often with PPs (e.g., extent). This
somewhat skewed distribution makes for an easier classification task than the one for
FrameNet.
3.2.1 Illustration with ?at.? As an illustration of the probabilities associated with class-
based collocations, consider the differences in the prior versus class-based conditional
probabilities for the semantic roles of the preposition at in the Penn Treebank (ver-
sion II). Table 10 shows the global probabilities for the roles assigned to at, along with
7 This window size was chosen after estimating that on average the prepositional objects occur within
2.3 ? 1.26 words of the preposition and that the average attachment site is within 3.0 ? 2.98 words. These
figures were produced by analyzing the parse trees for the semantic role annotations in the PTB.
167
Computational Linguistics Volume 35, Number 2
Table 9
Penn Treebank semantic roles for PPs. Omits low-frequency benefactive relation. Freq. is the relative
frequency of the role occurrence (36,476 total instances). Example usages are taken from
the corpus.
Role Freq. Example
locative .472 workers at a factory
temporal .290 expired atmidnight Tuesday
direction .149 has grown at a sluggish pace
manner .050 CDs aimed at individual investors
purpose .030 opened for trading
extent .008 declined by 14%
conditional probabilities for these roles given that certain high-level WordNet synsets
occur in the context. In a context referring to a concrete concept (i.e., entity#1), the
difference in the probability distributions for the locative and temporal roles shows that
the locative interpretation becomes even more likely. In contrast, in a context referring
to an abstract concept (i.e., abstraction#6), the difference in the probability distributions
for the same roles shows that the temporal interpretation becomes more likely. Therefore,
these class-based lexical associations capture commonsense usages of the preposition at.
3.2.2 Results. The classification results for these prepositions in the Penn Treebank show
that this approach is very effective. Table 11 shows the accuracy when disambiguating
the 14 prepositions using a single classifier with 6 roles. Table 11 also shows the per-
class statistics, showing that there are difficulties tagging the manner role (e.g., lowest
F-score). For the single-classifier case, the overall accuracy is 89.3%, using Weka?s J4.8
classifier (Witten and Frank 1999), which is an implementation of Quinlan?s (1993) C4.5
decision tree learner.
For comparison, Table 12 shows the results for individual classifiers created for the
prepositions annotated in PTB. A few prepositions only have small data sets, such as
of which is used more for specialization relations (e.g., category) than thematic ones.
This table is ordered by entropy, which measures the inherent ambiguity in the classes
as given by the annotations. Note that the Baseline column is the probability of the most
frequent sense, which is a common estimate of the lower bound for classification
Table 10
Prior and posterior probabilities of roles for ?at? in the Penn Treebank. P(R) is the relative frequency.
P(R|S) is the probability of the relation given that the synset occurs in the immediate context of
at. RPCR,S is the relative percentage change: (P(R|S)? P(R))/P(R).
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
locative 73.5 75.5 0.03 67.0 ?0.09
temporal 23.9 22.5 ?0.06 30.6 0.28
manner 2.0 1.5 ?0.25 2.0 0.00
direction 0.6 0.4 ?0.33 0.4 ?0.33
168
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 11
Overall preposition disambiguation results over Penn Treebank roles. A single classifier is used for all
the prepositions. # Instances is the number of role annotations. # Classes is the number of distinct
roles. Entropy measures non-uniformity of the role distributions. Baseline is estimated by the
most-frequent role. The Word Only experiment uses just word collocations, Hypernym Only just
uses hypernym collocations, and Both uses both types of collocations. Accuracy is average for
percent correct over ten trials in cross validation. STDEV is the standard deviation over the trials.
Experiment Accuracy STDEV
Word Collocations Only 88.1 0.88
Hypernym Collocations Only 88.2 0.43
Both Collocations 89.3 0.33
Data Set Characteristics
# Instances: 27,308
# Classes: 6
Entropy: 1.831
Baseline: 49.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
direction .953 .969 .960 .952 .967 .959 .956 .965 .961
extent .817 .839 .826 .854 .819 .834 .817 .846 .829
locative .879 .967 .921 .889 .953 .920 .908 .932 .920
manner .797 .607 .687 .790 .599 .680 .826 .558 .661
purpose .854 .591 .695 .774 .712 .740 .793 .701 .744
temporal .897 .776 .832 .879 .794 .834 .845 .852 .848
Table 12
Per-preposition disambiguation results over Penn Treebank roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
Prep Freq. Roles Entropy Baseline Word Hypernym Both
through 331 4 1.668 0.438 59.795 62.861 58.592
by 1290 7 1.575 0.479 87.736 88.231 86.655
as 220 3 1.565 0.405 95.113 96.377 96.165
between 87 4 1.506 0.483 77.421 81.032 70.456
of 30 3 1.325 0.567 63.182 82.424 65.606
out 76 4 1.247 0.711 70.238 76.250 63.988
for 1401 6 1.189 0.657 82.444 85.795 80.158
on 1915 5 1.181 0.679 85.998 88.720 79.428
in 14321 7 1.054 0.686 86.404 92.647 86.523
throughout 59 2 0.998 0.525 61.487 35.949 63.923
at 2825 5 0.981 0.735 84.178 90.265 85.561
across 78 2 0.706 0.808 75.000 78.750 77.857
from 1521 5 0.517 0.917 91.649 91.650 91.650
to 3074 5 0.133 0.985 98.732 98.537 98.829
Mean 1944.8 4.43 1.12 0.648 80.0 82.1 78.9
169
Computational Linguistics Volume 35, Number 2
experiments. When using preposition-specific classifiers, the hypernym collocations
surprisingly outperform the other configurations, most likely due to overfitting with
word-based clues: 82.1% versus 80.0% for the word-only case.
3.3 FrameNet Classification Experiments
The second set of experiments perform preposition disambiguation using FrameNet.
A similar preposition word-sense disambiguation experiment is carried out over the
FrameNet semantic role annotations involving prepositional phrases. Consider the sam-
ple annotation shown earlier:
(8) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling ?C FE=?Communicator? PT=?NP??standalone workstations?/C?
to ?C TARGET=?y??communicate?/C? ?C FE=?Medium? PT=?PP??over
public or private ISDN networks?/C?.
The prepositional phrase annotation is isolated and treated as the sense of the preposi-
tion. This yields the following sense annotation:
(9) Hewlett-Packard Co has rolled out a new range of ISDN connectivity
enabling standalone workstations to communicate overMedium public or
private ISDN networks.
Table 13 shows the distribution of common roles assigned to prepositional phrases. The
topic role is the most frequent case not directly covered in PTB.
3.3.1 Illustration with ?at.? See Table 14 for the most frequent roles out of the 124 cases
that were assigned to at, along with the conditional probabilities for these roles given
that certain high-level WordNet synsets occur in the context. In a context referring
to concrete entities, the role place becomes more prominent. However, in an abstract
context, the role time becomes more prominent. Thus, similar behavior to that noted for
PTB in Section 3.2.1 occurs with FrameNet.
3.3.2 Results. Table 15 shows the results of classification when all of the prepositions
are classified together. Due to the exorbitant number of roles (641), the overall results
are low. However, the combined collocation approach still shows slight improvement
(23.3% versus 23.1%). The FrameNet inventory contains many low-frequency relations
Table 13
Most common FrameNet semantic roles for PPs. Relative frequencies for roles assigned to
prepositional phrases in version 1.3 (66,038 instances), omitting cases below 0.01.
Role Freq. Role Freq. Role Freq.
goal .092 theme .022 whole .015
path .071 manner .021 individuals .013
source .043 area .018 location .012
topic .040 reason .018 ground .012
time .037 addressee .017 means .011
place .033 stimulus .017 content .011
170
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 14
Prior and posterior probabilities of roles for ?at? in FrameNet. Only the top 5 of 641 applicable roles
are shown. P(R) is the relative frequency for relation. P(R|S) is the probability of the relation given
that the synset occurs in the immediate context of at. RPCR,S is the relative percentage change:
(P(R|S) ? P(R))/P(R).
Synset
entity#1 abstraction#6
Relation P(R) P(R|S) RPCR,S P(R|S) RPCR,S
place 15.6 19.0 21.8 16.8 7.7
time 12.0 11.5 ?4.2 15.1 25.8
stimulus 6.6 5.0 ?24.2 6.6 0.0
addressee 6.1 4.4 ?27.9 3.3 ?45.9
goal 5.5 6.3 14.5 6.0 9.1
Table 15
Preposition disambiguation with all FrameNet roles. All 641 roles are considered. Entropymeasures
data set uniformity, and Baseline selects most common role.
Experiment Accuracy STDEV
Word Collocations Only 23.078 0.472
Hypernym Collocations Only 23.206 0.467
Both Collocations 23.317 0.556
Data Set Characteristics
# Instances: 65,550
# Classes: 641
Entropy: 6.785
Baseline: 9.3
that complicate this type of classification. By filtering out relations that occur in less than
1% of the role occurrences for prepositional phrases, substantial improvement results,
as shown in Table 16. Even with filtering, the classification is challenging (e.g., 18 classes
with entropy 3.82). Table 16 also shows the per-class statistics, indicating that the means
and place roles are posing difficulties for classification.
Table 17 shows the results when using individual classifiers, ordered by entropy.
This illustrates that the role distributions are more complicated than those for PTB,
yielding higher entropy values on average. In all, there are over 360 prepositions
with annotations, 92 with ten or more instances each. (Several of the low-frequency
cases are actually adverbs, such as anywhere, but are treated as prepositions during the
annotation extraction.) The results show that the word collocations produce slightly
better results: 67.8 versus 66.0 for combined collocations. Unlike the case with PTB,
the single-classifier performance is below that of the individual classifiers. This is
due to the fine-grained nature of the role inventory. When all the roles are considered
together, prepositions are sometimes being incorrectly classified using roles that have
not been assigned to them in the training data. This occurs when contextual clues are
stronger for a commonly used role than for the appropriate one. Given PTB?s small role
inventory, this problem does not occur in the corresponding experiments.
3.4 Factotum Classification Experiments
The third set of experiments deals with preposition disambiguation using Factotum.
Note that Factotum does not indicate the way the relationships are expressed in English.
171
Computational Linguistics Volume 35, Number 2
Table 16
Overall results for preposition disambiguation with common FrameNet roles. Excludes roles with less
than 1% relative frequency. Entropymeasures data set uniformity, and Baseline selects most
common role. Detailed per-class statistics are also included, averaged over the 10 folds.
Experiment Accuracy STDEV
Word Collocations Only 73.339 0.865
Hypernym Collocations Only 73.437 0.594
Both Collocations 73.544 0.856
Data Set Characteristics
# Instances: 32974
# Classes: 18
Entropy: 3.822
Baseline: 18.4
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
addressee .785 .332 .443 .818 .263 .386 .903 .298 .447
area .618 .546 .578 .607 .533 .566 .640 .591 .613
content .874 .618 .722 .895 .624 .734 .892 .639 .744
goal .715 .766 .739 .704 .778 .739 .703 .790 .743
ground .667 .386 .487 .684 .389 .494 .689 .449 .541
individuals .972 .947 .959 .961 .945 .953 .938 .935 .936
location .736 .524 .610 .741 .526 .612 .815 .557 .660
manner .738 .484 .584 .748 .481 .584 .734 .497 .591
means .487 .449 .464 .562 .361 .435 .524 .386 .441
path .778 .851 .812 .777 .848 .811 .788 .849 .817
place .475 .551 .510 .483 .549 .513 .474 .576 .519
reason .803 .767 .784 .777 .773 .774 .769 .714 .738
source .864 .980 .918 .865 .981 .919 .860 .978 .915
stimulus .798 .798 .797 .795 .809 .802 .751 .752 .750
theme .787 .811 .798 .725 .847 .779 .780 .865 .820
time .585 .665 .622 .623 .687 .653 .643 .690 .664
topic .831 .836 .833 .829 .842 .835 .856 .863 .859
whole .818 .932 .871 .807 .932 .865 .819 .941 .875
Similarly, WordNet does not indicate this, but it does include definition glosses. For
example,
(10)
Factotum:
?drying, is-function-of , drier?
WordNet:
dryalter remove the moisture from and make dry
dryerappliance an appliance that removes moisture
These definition glosses might be useful in certain cases for inferring the relation markers
(i.e., generalized case markers). As is, Factotum cannot be used to provide training data
for learning how the relations are expressed in English. This contrasts with corpus-
based annotations, such as PTB (Marcus et al 1994) and FrameNet (Fillmore, Wooters,
and Baker 2001), where the relationships are marked in context.
3.4.1 Inferring Semantic Role Markers. To overcome the lack of context in Factotum, the
relation markers are inferred through corpus checks, in particular through proximity
searches involving the source and target terms from the relationship (i.e., ?source,
172
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 17
Per-preposition disambiguation results over FrameNet roles. A separate classifier is used for each
preposition, excluding roles with less than 1% relative frequency. Freq gives the preposition
frequency, and Roles the number of senses. Entropy measures data set uniformity, and Baseline
selects most common role. The Word and Hypernym columns show results when including just
word and hypernym collocations, respectively, whereas Both includes both types. Each column
shows averages for percent correct over ten trials. The Mean row averages the values of the
individual experiments.
Prep Freq. Roles Entropy Baseline Word Hypernym Both
with 3758 25 4.201 19.6 59.970 57.809 61.924
of 7339 22 4.188 12.8 85.747 84.663 85.965
between 675 23 4.166 11.4 61.495 56.215 53.311
under 286 26 4.045 25.5 29.567 33.040 33.691
against 557 26 4.028 21.2 53.540 58.885 31.892
for 2678 22 3.988 22.6 58.135 58.839 39.809
by 3348 18 3.929 13.6 62.618 60.854 61.152
on 3579 22 3.877 18.1 61.011 57.671 60.838
at 2685 21 3.790 21.2 61.814 58.501 57.630
in 6071 18 3.717 18.7 54.253 49.953 53.880
as 1123 17 3.346 27.1 53.585 47.186 42.722
to 4741 17 3.225 36.6 71.963 77.751 72.448
behind 254 13 3.222 22.8 47.560 41.045 43.519
over 1157 16 3.190 27.8 47.911 48.548 50.337
after 349 16 2.837 45.8 62.230 65.395 61.944
around 772 15 2.829 45.1 52.463 52.582 49.357
from 3251 14 2.710 51.2 73.268 71.934 75.423
round 389 12 2.633 34.7 46.531 50.733 49.393
into 1923 14 2.208 62.9 79.175 77.366 80.846
during 242 10 2.004 63.6 71.067 75.200 68.233
like 570 9 1.938 62.3 82.554 79.784 85.666
through 1358 10 1.905 66.0 77.800 77.798 79.963
up 745 10 1.880 60.3 76.328 76.328 74.869
off 647 9 1.830 63.8 90.545 86.854 90.423
out 966 8 1.773 60.7 77.383 79.722 78.671
across 894 11 1.763 67.6 80.291 80.095 80.099
towards 673 10 1.754 67.9 65.681 71.171 65.517
down 965 7 1.600 63.2 81.256 81.466 79.141
along 723 9 1.597 72.5 87.281 86.862 86.590
about 1894 8 1.488 72.2 83.214 76.663 83.899
back 405 7 1.462 64.7 88.103 91.149 86.183
past 275 9 1.268 78.9 85.683 86.423 85.573
Mean 1727.9 14.8 2.762 43.8 67.813 67.453 65.966
relation, target?). For example, using AltaVista?s Boolean search,8 this can be done via
?source NEAR target.?
Unfortunately, this technique would require detailed post-processing of the Web
search results, possibly including parsing, in order to extract the patterns. As an ex-
pedient, common prepositions9 are included in a series of proximity searches to find
8 AltaVista?s Boolean search is available at www.altavista.com/sites/search/adv.
9 The common prepositions are determined from the prepositional phrases assigned functional
annotations in the Penn Treebank (Marcus et al 1994).
173
Computational Linguistics Volume 35, Number 2
the preposition occurring most frequently with the given terms. For instance, given the
relationship ?drying, is-function-of, drier?, the following searches would be performed.
(11) drying NEAR drier NEAR in
drying NEAR drier NEAR to
...
drying NEAR drier NEAR ?around?
To account for prepositions that occur frequently (e.g., of ), pointwise mutual infor-
mation (MI) statistics (Manning and Schu?tze 1999, pages 66?68) are used in place of the
raw frequency when rating the potential markers. These are calculated as follows:
MIprep = log2
P(X,Y)
P(X)? P(Y)
? log2
f (source NEAR target NEAR prep)
f (source NEAR target)? f (prep)
(2)
Such checks are done for the 25 most common prepositions to find the preposition
yielding the highest mutual information score. For example, the top three markers for
the ?drying, is-function-of, drier? relationship based on this metric are during, after, and
with.
3.4.2 Method for Classifying Functional Relations. Given the functional relationships in
Factotum along with the inferred relation markers, machine-learning algorithms can
be used to infer what relation most likely applies to terms occurring together with a
particular marker. Note that the main purpose of including the relation markers is to
provide clues for the particular type of relation. Because the source term and target
terms might occur in other relationships, associations based on them alone might not
be as accurate. In addition, the inclusion of these clue words (e.g., the prepositions)
makes the task closer to what would be done in inferring the relations from free text.
The task thus approximates preposition disambiguation, using the Factotum relations
as senses.
Figure 5 gives the feature settings used in the experiments. This is a version of
the feature set used in the PTB and FrameNet experiments (see Figure 4), simplified to
account for the lack of sentential context. Figure 6 contains sample feature specifications
from the experiments discussed in the next section. The top part shows the original
relationships from Factotum; the first example indicates that connaturalize causes simi-
larity. Also included is the most likely relation marker inferred for each instance. This
shows that ?n/a? is used whenever a preposition for a particular relationship cannot be
inferred. This happens in the first example because connaturalize is a rare term.
The remaining parts of Figure 6 illustrate the feature values that would be derived
for the three different experiment configurations, based on the inclusion of word and/or
hypernym collocations. In each case, the classification variable is given by relation.
For brevity, the feature specification only includes collocation features for the most
frequent relations. Sample collocations are also shown for the relations (e.g., vulgar-
ity for is-caused-by). In the word collocation case, the occurrence of similarity is used
to determine that the is-caused-by feature (WC1) should be positive (i.e., ?1?) for the
first two instances. Note that there is no corresponding hypernym collocation due to
conditional probability filtering. In addition, although new is not included as a word
collocation, one of its hypernyms, namely Adj:early#2, is used to determine that the
has-consequence feature (HC3) should be positive in the last instance.
174
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Context:
Source and target terms from relationship (?source, relation, target?)
Features:
POSsource: part-of-speech of the source term
POStarget: part-of-speech of the target term
Prep: preposition serving as relation marker (?n/a? if not inferable)
WordCollr: 1 iff context contains any word collocation for relation r
HypernymCollr: 1 iff context contains any hypernym collocation for relation r
Collocation selection:
Frequency: f (word) > 1
Relative percent change: (P(C|coll)? P(C))/P(C) ? .20
Organization: per-class-binary grouping
Model selection:
Decision tree using Weka?s J4.8 classifier (Witten and Frank 1999)
Figure 5
Features used in Factotum role classification experiments. Simplified version of Figure 4: Context
only consists of the source and target terms.
3.4.3 Results. To make the task more similar to the PTB and FrameNet cases covered
previously, only the functional relations in Factotum are used. These are determined
by removing the hierarchical relations (e.g., has-subtype and has-part) along with the
attribute relations (e.g., is-property-of ). In addition, in cases where there are inverse
functions (e.g., causes and is-caused-by), the most frequently occurring relation of each
inverse pair is used. This is done because the relation marker inference approach does
not account for argument order. The boldface relations in the listing shown earlier in
Table 5 are those used in the experiment. Only single-word source and target terms are
considered to simplify the WordNet hypernym lookup (i.e., no phrasals). The resulting
data set has 5,959 training instances. The data set alo includes the inferred relation
markers (e.g., one preposition per training instance), thus introducing some noise.
Figure 6 includes a few examples from this data set. This shows that the original
relationship ?similarity, is-caused-by, rhyme? from Factotum is augmented with the
by marker prior to classification. Again, these markers are inferred via Web searches
involving the terms from the original relationship.
Table 18 shows the results of the classification. The combined use of both collocation
types achieves the best overall accuracy at 71.2%, which is good considering that the
baseline of always choosing the most common relation (is-caused-by) is 24.2%. This com-
bination generalizes well by using hypernym collocations, while retaining specificity
via word collocations. The classification task is difficult, as suggested by the number
of classes, entropy, and baseline values all being comparable to the filtered FrameNet
experiment (see Table 16).
3.5 Common Relation Inventory Classification Experiments
The last set of experiments investigate preposition disambiguation using FrameNet
mapped into a reduced semantic role inventory. For the application to lexical acqui-
sition, the semantic role annotations are converted into the common relation inventory
discussed in Section 2.5. To apply the common inventory to the FrameNet data, anno-
tations using the 641 FrameNet relations (see Table 2) need to be mapped into those
175
Computational Linguistics Volume 35, Number 2
Relationships from Factotum with inferred markers:
Relationship Marker
?similarity, is-caused-by, connaturalize? n/a
?similarity, is-caused-by, rhyme? by
?approximate, has-consequence, imprecise? because
?new, has-consequence, patented? with
Word collocations only:
Relation POSs POSt Prep WC1 WC2 WC3 WC4 WC5 WC6 WC7
is-caused-by NN VB n/a 1 0 0 0 0 0 0
is-caused-by NN NN by 1 0 0 0 0 0 0
has-consequence NN JJ because 0 0 0 0 0 0 0
has-consequence JJ VBN with 0 0 0 0 0 0 0
Sample collocations:
is-caused-by {bitterness, evildoing, monochrome, similarity, vulgarity}
has-consequence {abrogate, frequently, insufficiency, nonplus, ornament}
Hypernym collocations only:
Relation POSs POSt Prep HC1 HC2 HC3 HC4 HC5 HC6 HC7
is-caused-by NN VB n/a 0 0 0 0 0 0 0
is-caused-by NN NN by 0 0 0 0 0 0 0
has-consequence NN JJ because 0 0 0 0 0 0 0
has-consequence JJ VBN with 0 0 1 0 0 0 0
Sample collocations:
is-caused-by {N:hostility#3, N:inelegance#1, N:humorist#1}
has-consequence {V:abolish#1, Adj:early#2, N:inability#1, V:write#2}
Both collocations:
Relation POSs POSt Prep WC1 ... WC7 HC1 HC2 HC3 ...
is-caused-by NN VB n/a 1 ... 0 0 0 0 ...
is-caused-by NN NN by 1 ... 0 0 0 0 ...
has-consequence NN JJ because 0 ... 0 0 0 0 ...
has-consequence JJ VBN with 0 ... 0 0 0 1 ...
Legend:
POSs & POSt are the parts of speech for the source and target terms; and
WCr & HCr are the word and hypernym collocations as follows:
1. is-caused-by 2. is-function-of 3. has-consequence 4. has-result
5. is-caused-bymental 6. is-performed-by 7. uses
Figure 6
Sample feature specifications for Factotum experiments. Each relationship from Factotum is
augmented with one relational marker inferred via Web searches, as shown at top of figure.
Three distinct sets of feature vectors are shown based on the type of collocation included,
omitting features for low-frequency relations.
176
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Table 18
Functional relation classification over Factotum. This uses the relational source and target terms
with inferred prepositions. The accuracy figures are averages based on 10-fold cross validation.
The gain in accuracy for the combined experiment versus the word experiment is statistically
significant at p < .01 (via a paired t-test).
Experiment Accuracy STDEV
Word Collocations Only 68.4 1.28
Hypernym Collocations Only 53.9 1.66
Both Collocations 71.2 1.78
Data Set Characteristics
# Instances: 5,959
# Classes: 21
Entropy: 3.504
Baseline: 24.2
using the 26 common relations shown in Table 8. Results for the classification of the
FrameNet data mapped into the common inventory are shown in Table 19. As can
be seen, the performance is well above that of the full classification over FrameNet
without filtering (see Table 15). Although the low-frequency role filtering yields the
highest performance (see Table 16), this comes at the expense of having half of the
training instances discarded. Corpus annotations are a costly resource, so such waste
is undesirable. Table 19 also shows the per-class statistics, indicating that the means,
direction, and part roles are handled poorly by the classifier. The latter two are due to the
relatively small training examples for the roles in question, which can be addressed
partly by refining the mapping from FrameNet. However, problems classifying the
means role occur with all classifiers discussed in this article, suggesting that that role
is too subtle to be classified with the feature set currently used.
The results in Table 19 also illustrate that the reduced, common-role inventory has
an additional advantage of improving performance in the classification, compared to a
cascaded approach. This occurs because several of the miscellaneous roles in FrameNet
cover subtle distinctions that are not relevant for definition analysis (e.g., cognizer and
addressee). The common inventory therefore strikes a balance between the overly general
roles in PTB, which are easy to classify, and the overly specialized roles in FrameNet,
which are quite difficult to classify. Nonetheless, a certain degree of classification diffi-
culty is inevitable in order for the inventory to provide adequate coverage of the dif-
ferent distinctions present in dictionary definitions. Note that, by using the annotations
from PTB and FrameNet, the end result is a general-purpose classifier, not one tied into
dictionary text. Thus, it is useful for other tasks besides definition analysis.
This classifier was used to disambiguate prepositions in the lexical acquisition
system we developed at NMSU (O?Hara 2005). Evaluation of the resulting distinctions
was performed by having the output of the system rated by human judges. Manu-
ally corrected results were also evaluated by the same judges. The overall ratings are
not high in both cases, suggesting that some of the distinctions being made are subtle.
For instance, for ?counterintelligence achieved by deleting any information of value?
from the definition of censoring, means is the preferred role for by, but manner is ac-
ceptable. Likewise, characteristic is the preferred role for of, but category is interpretable.
Thus, the judges differed considerably on these cases. However, as the ratings for
the uncorrected output were close to those for the corrected output, the approach is
promising to use for lexical acquisition. If desired, the per-role accuracy results shown
in Table 19 could be incorporated as confidence values assigned to particular relation-
ships extracted from definitions (e.g., 81% for those with source but only 21% when
means used).
177
Computational Linguistics Volume 35, Number 2
4. Related Work
The main contribution of this article concerns the classification methodology (rather
than the inventories for semantic roles), so we will only review other work related
to this aspect. First, we discuss similar work involving hypernyms. Then, we address
preposition classification proper.
Scott and Matwin (1998) use WordNet hypernyms for text classification. They
include a numeric density feature for any synset that subsumes words appearing in
the document, potentially yielding hundreds of features. In contrast, the hypernym
collocations discussed in Section 3.1.1 involve a binary feature for each of the relations
being classified, using indicative synsets based on the conditional probability test. This
test alleviates the need for their maximum height parameter to avoid overly general
hypernyms. Their approach, as well as ours, considers all senses of a word, distrib-
uting the alternative readings throughout the set of features. In comparison, Gildea
Table 19
Results for preposition disambiguation with common roles. The FrameNet annotations are mapped
into the common inventory from Table 8. Entropymeasures data set uniformity, and Baseline
selects most common role. Detailed per-class statistics are also included, averaged over the
10 folds.
Experiment Accuracy STDEV
Word Collocations Only 62.9 0.345
Hypernym Collocations Only 62.6 0.487
Both Collocations 63.1 0.639
Data Set Characteristics
# Instances: 59,615
# Classes: 24
Entropy: 4.191
Baseline: 12.2
Word Only Hypernym Only Both
Class Prec. Rec. F Prec. Rec. F Prec. Rec. F
accompaniment .630 .611 .619 .671 .605 .636 .628 .625 .626
agent .623 .720 .667 .639 .726 .677 .616 .731 .668
area .546 .475 .508 .541 .490 .514 .545 .501 .522
category .694 .706 .699 .695 .700 .697 .714 .718 .716
cause .554 .493 .521 .569 .498 .531 .540 .482 .509
characteristic .595 .468 .523 .607 .474 .530 .584 .490 .532
context .569 .404 .472 .577 .388 .463 .568 .423 .485
direction .695 .171 .272 .701 .189 .294 .605 .169 .260
duration .601 .465 .522 .589 .445 .503 .596 .429 .497
experiencer .623 .354 .449 .606 .342 .435 .640 .378 .474
goal .664 .683 .673 .662 .674 .668 .657 .680 .668
instrument .406 .339 .367 .393 .337 .360 .405 .370 .385
location .433 .557 .487 .427 .557 .483 .417 .553 .475
manner .493 .489 .490 .483 .478 .479 .490 .481 .485
means .235 .183 .205 .250 .183 .210 .254 .184 .212
medium .519 .306 .382 .559 .328 .412 .529 .330 .403
part .539 .289 .368 .582 .236 .323 .526 .301 .380
path .705 .810 .753 .712 .813 .759 .706 .795 .748
product .837 .750 .785 .868 .739 .788 .769 .783 .770
recipient .661 .486 .559 .661 .493 .563 .642 .482 .549
resource .613 .471 .530 .614 .458 .524 .618 .479 .539
source .703 .936 .802 .697 .936 .799 .707 .937 .806
theme .545 .660 .596 .511 .661 .576 .567 .637 .600
time .619 .624 .621 .626 .612 .619 .628 .611 .619
178
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
and Jurafsky (2002) instead just select the first sense for their hypernym features for
relation classification. They report marginal improvements using the features, whereas
configurations with hypernym collocations usually perform best in our preposition
disambiguation experiments.
Mohit and Narayanan (2003) use WordNet hypernyms to generalize patterns for
information extraction inferred from FrameNet annotations by distributing support
from terms co-occurring in annotations for frame elements to the terms for hypernyms.
However, they do not incorporate a filtering stage, as with our conditional probability
test. Mihalcea (2002) shows how hypernym information can be useful in deriving clues
for unsupervised WSD. Patterns for co-occurring words of a given sense are induced
from sense-tagged corpora. Each pattern specifies templates for the co-occurring words
in the immediate context window of the target word, as well as their corresponding
synsets if known (e.g., sense tagged or unambiguous), and similarly the hypernym
synsets if known. To disambiguate a word, the patterns for each of its senses are
evaluated in the context, and the sense with the most support is chosen.
The work here addresses relation disambiguation specifically with respect to those
indicated by prepositional phrases (i.e., preposition word-sense disambiguation). Until
recently, there has been little work on general-purpose preposition disambiguation.
Litkowski (2002) and Srihari, Niu, and Li (2001) present approaches using manually
derived rules. Both approaches account for only a handful of prepositions; in contrast,
for FrameNet we disambiguate 32 prepositions via individual classifiers and over 100
prepositions via the combined classifier. Liu and Soo (1993) present a heuristic approach
for relation disambiguation relying upon syntactic clues as well as occurrence of specific
prepositions. They assign roles to constituents of a sentence from corpus data provided
that sufficient instances are available. Otherwise, a human trainer is used to answer
questions needed by the system for the assignment. They report an 86% accuracy rate
for the assignment of roles to verbal arguments in about 5,000 processed sentences.
Alam (2004) sketches out how the preposition over might be disambiguated into one
of a dozen roles using features based on the head and complement, such as whether the
head is amovement verb orwhether the complement refers to a duration. These features
form the basis for a manually-constructed decision tree, which is interpreted by hand in
an evaluation over sentences from the British National Corpus (BNC), giving a precision
of 93.5%. Boonthum, Toida, and Levinstein (2006), building upon the work of Alam,
show how WordNet can be used to automate the determination of similar head and
complement properties. For example, if both the head and complement refer to people,
with should be interpreted as accompaniment. These features form the basis for a
disambiguation system using manually constructed rules accounting for ten commonly
occurring prepositions. They report a precision of 79% with a recall of 76% over an
inventory of seven roles in a post hoc evaluation that allows for partial correctness.
There have been a fewmachine-learning approaches that are more similar to the ap-
proach used here. Gildea and Jurafsky (2002) perform relation disambiguation using the
FrameNet annotations as training data. They include lexical features for the headword
of the phrase and the predicating word for the entire annotated frame (e.g., the verb
corresponding to the frame under which the annotations are grouped). They also use
several features derived from the output of a parser, such as the constituent type of the
phrase (e.g., NP), the grammatical function (e.g., subject), and a path feature listing part-
of-speech tags from the target word to the phrase being tagged. They report an accuracy
of 78.5% with a baseline of 40.6% over the FrameNet semantic roles. However, by
conditioning the classification on the predicatingword, the range of roles for a particular
classification instance is more limited than in the experiments presented in this article.
179
Computational Linguistics Volume 35, Number 2
Blaheta and Charniak (2000) use the PTB annotations for relation disambiguation. They
use a few parser-derived features, such as the constituent labels for nearby nodes and
part-of-speech for parent and grandparent nodes. They also include lexical features for
the head and alternative head (because prepositions are considered as the head by their
parser). As their classifier tags all adjuncts, they include the nominal and adverbial roles,
which are syntactic and more predictable than the roles occurring with prepositional
phrases.
There have been recent workshops featuring competitions for semantic role tagging
(Carreras and Ma`rquez 2004, 2005; Litkowski 2004). A common approach is to tag all
the semantic roles in a sentence at the same time to account for dependencies, such
as via Hidden Markov Models. To take advantage of accurate Support Vector Machine
classification, Pradhan et al (2005) instead use a postprocessing phrase based on trigram
models of roles. Their system incorporates a large variety of features, building upon sev-
eral different preceding approaches, such as including extensions to the path features
from Gildea and Jurafsky (2002). Their lexical features include the predicate root word,
headwords for the sentence constituents and PPs, as well as their first and last words.
Koomen et al (2005) likewise use a large feature set. They use an optimization phase to
maximize satisfaction of the constraints imposed by the PropBank data set, such as the
number of arguments for particular predicates (e.g., just two for stalk, arg0 and arg1).
Lastly, Ye and Baldwin (2006) show how filtering can be used to constrain the
hypernyms selected to serve as collocations, building upon our earlier work (O?Hara
and Wiebe 2003). They report 87.7% accuracy in a setup similar to ours over PTB
(i.e., a gain of 2 percentage points). They use a different type of collocation feature
than ours: having a binary feature for each potential collocation rather than a single
feature per class. That is, they useOver-Range Binary rather than Per-Class Binary (Wiebe,
McKeever, and Bruce 1998). Moreover, they include several hundred of these features,
rather than our seven (benefactive previously included), which is likely the main source
of improvement. Again, the per-class binary organization is a bag of words approach,
so it works well only with a limited number of potential collocations. Follow-up work
of theirs (Ye and Baldwin 2007) fared well in the recent preposition disambiguation
competition, held as part of SemEval-2007 (Litkowski and Hargraves 2007). Thus, an
immediate area for future work will be to incorporate such improved feature sets. We
will also investigate addressing sentential role constraints as in general semantic role
tagging.
5. Conclusion
This article shows how to exploit semantic role resources for preposition disambigua-
tion. Information about two different types of semantic role resources is provided. The
emphasis is on corpus-based resources providing annotations of naturally occurring
text. The Penn Treebank (Marcus et al 1994) covers general roles for verbal adjuncts and
FrameNet (Fillmore, Wooters, and Baker 2001) includes a wide range of domain-specific
roles for all verbal arguments. In addition, semantic role inventories from knowledge
bases are investigated. Cyc (Lehmann 1996) provides fine-grained role distinctions,
Factotum (Cassidy 2000) includes a variety of functional relations, and work in Concep-
tual Graphs (Sowa 1999) emphasizes roles for attributes. Relations from both types of
resources are considered when developing the inventory of relations used for definition
analysis, as shown in Table 8.
The disambiguation concentrates on relations indicated by prepositional phrases,
and is framed as word-sense disambiguation for the preposition in question. A new
180
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
type of feature for word-sense disambiguation is introduced, using WordNet hyper-
nyms as collocations rather than just words, as is typically done. The full feature set is
shown in Figure 4. Various experiments over the PTB and FrameNet data are presented,
including prepositions classified separately versus together, and illustrating the effects
of filtering. The main results in Tables 11 and 16 show that the combined use of word
and hypernym collocations generally achieves the best performance. For relationships
derived from knowledge bases, the prepositions and other relational markers need to
be inferred from corpora. A method for doing this is demonstrated using Factotum,
with results shown in Table 18. In addition, to account for granularity differences in the
semantic role inventories, the relations are mapped into a common inventory that was
developed based on the inventories discussed in the article. This allows for improved
classification in cases where inventories provide overly specialized relations, such as
those in FrameNet. Classification results are shown in Table 19.
The recent competitions on semantic relation labeling have highlighted the useful-
ness of incorporating a variety of clues for general-purpose relation disambiguation
(Carreras and Ma`rquez 2005). Some of the techniques developed here for preposition
disambiguation can likely help with relation disambiguation in general. For instance,
there are quite a few lexical features, such as in Pradhan et al (2005), which could be
extended to use semantic classes as with our hypernym collocations. In general it seems
that, when lexical features are used in supervised machine learning, it is likely that
corresponding class-based features based on hypernyms can be beneficial for improved
coverage.
Other aspects of this approach are geared specifically to our goal of supporting
lexical acquisition from dictionaries, which was the motivation for the emphasis on
preposition disambiguation. Isolating the preposition annotations allows the classifiers
to be more readily tailored to definition analysis, especially because predicate frames
are not assumed as with other FrameNet relation disambiguation. Future work will
investigate combining the general relation classifiers with preposition disambiguation
classifiers, such as is done in Ye and Baldwin (2006). Future work will also investigate
improvements to the application to definition analysis. Currently, FrameNet roles are
alwaysmapped to the same common inventory role (e.g., place to location). However, this
should account for the frame of the annotation and perhaps other context information.
Lastly, we will also look for more resources to exploit for preposition disambiguation
(e.g., ResearchCyc).
Acknowledgments
The experimentation for this article was
greatly facilitated though the use of
computing resources at New Mexico State
University. We are also grateful for the
extremely helpful comments provided
by the anonymous reviewers.
References
Alam, Yukiko Sasaki. 2004. Decision trees
for sense disambiguation of prepositions:
Case of over. In Proceedings of the
Computational Lexical Semantics Workshop,
pages 52?59, Boston, MA.
Barker, Ken. 1998. Semi-Automatic Recognition
of Semantic Relationships in English Technical
Texts. Ph.D. thesis, Department of
Computer Science, University of Ottawa.
Bies, Ann, Mark Ferguson, Karen Katz,
Robert MacIntyre, Victoria Tredinnick,
Grace Kim, Mary Ann Marcinkiewicz,
and Britta Schasberger. 1995. Bracketing
guidelines for Treebank II style:
Penn Treebank project. Technical
Report MS-CIS-95-06, University of
Pennsylvania.
Blaheta, Don and Eugene Charniak. 2000.
Assigning function tags to parsed text.
In Proceedings of the 1st Annual Meeting
of the North American Chapter of the
American Association for Computational
Linguistics (NAACL-2000), pages
234?240,Seattle, WA.
181
Computational Linguistics Volume 35, Number 2
Boonthum, Chutima, Shunichi Toida, and
Irwin B. Levinstein. 2006. Preposition
senses: Generalized disambiguation
model. In Proceedings of the Seventh
International Conference on Computational
Linguistics and Intelligent Text Processing
(CICLing-2006), pages 196?207,
Mexico City.
Bruce, Bertram. 1975. Case systems for
natural language. Artificial Intelligence,
6:327?360.
Bruce, Rebecca and Janyce Wiebe. 1999.
Decomposable modeling in natural
language processing. Computational
Linguistics, 25(2):195?208.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Cassidy, Patrick J. 2000. An investigation
of the semantic relations in the Roget?s
Thesaurus: Preliminary results. In
Proceedings of the First International
Conference on Intelligent Text Processing
and Computational Linguistics
(CICLing-2000), pages 181?204,
Mexico City.
Cruse, David A. 1986. Lexical Semantics.
Cambridge University Press, Cambridge.
Edmonds, Phil and Scott Cotton, editors.
2001. Proceedings of the Senseval 2 Workshop.
Association for Computational Linguistics,
Toulouse.
Fillmore, Charles. 1968. The case for case.
In Emmon Bach and Rovert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart and Winston, New York,
pages 1?88.
Fillmore, Charles J., Charles Wooters, and
Collin F. Baker. 2001. Building a large
lexical databank which provides
deep semantics. In Proceedings of the
Pacific Asian Conference on Language,
Information and Computation, pages 3?25,
Hong Kong.
Frawley, William. 1992. Linguistic Semantics.
Lawrence Erlbaum Associates,
Hillsdale, NJ.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Grozea, Cristian. 2004. Finding optimal
parameter settings for high performance
word sense disambiguation. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 125?128,
Barcelona.
Kilgarriff, Adam. 1998. Senseval: An exercise
in evaluating word sense disambiguation
programs. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC ?98),
pages 581?588, Granada.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Lehmann, Fritz. 1996. Big posets of
participatings and thematic roles. In
Peter W. Eklund, Gerard Ellis, and
Graham Mann, editors, Conceptual
Structures: Knowledge Representation as
Interlingua, Springer-Verlag, Berlin,
pages 50?74.
Lenat, Douglas B. 1995. Cyc: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38.
Litkowski, Kenneth C. 2002. Digraph
analysis of dictionary preposition
definitions. In Proceedings of the
SIGLEX/SENSEVAL Workshop on Word
Sense Disambiguation: Recent Successes
and Future Directions, pages 9?16,
Philadelphia, PA.
Litkowski, Kenneth C. 2004. Senseval-3 task:
Automatic labeling of semantic roles.
In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text,
pages 9?12, Barcelona.
Litkowski, Kenneth C. and Orin Hargraves.
2006. Coverage and inheritance in The
Preposition Project. In Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Liu, Rey-Long and Von-Wun Soo. 1993. An
empirical study on thematic knowledge
acquisition based on syntactic clues and
heuristics. In Proceedings of the 31st Annual
Meeting of the Association for Computational
182
O?Hara and Wiebe Exploiting Resources for Preposition Disambiguation
Linguistics (ACL-93), pages 243?250,
Columbus, OH.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Human Language
Technology Workshop, pages 110?115,
Plainsboro, NJ.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mihalcea, Rada. 2002. Instance based
learning with automatic feature
selection applied to word sense
disambiguation. In Proceedings of
the 19th International Conference on
Computational Linguistics (COLING-2002),
pages 1?7, Taiwan.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The Senseval-3
English lexical sample task. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25?28,
Barcelona.
Miller, George A., Richard Beckwith,
Christiane Fellbaum, Derek Gross, and
Katherine Miller. 1990. WordNet: An
on-line lexical database. International
Journal of Lexicography, 3(4): Special
Issue on WordNet.
Miller, Katherine. 1998. Modifiers in
WordNet. In Christiane Fellbaum,
editor,WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA,
pages 47?67.
Mohit, Behrang and Srini Narayanan.
2003. Semantic extraction with
wide-coverage lexical resources. In
Companion Volume of the Proceedings
of HLT-NAACL 2003 - Short Papers,
pages 64?66, Edmonton.
O?Hara, Thomas P. 2005. Empirical acquisition
of conceptual distinctions via dictionary
definitions. Ph.D. thesis, Department
of Computer Science, New Mexico
State University.
O?Hara, Tom, Rebecca Bruce, Jeff Donner,
and Janyce Wiebe. 2004. Class-based
collocations for word-sense
disambiguation. In Proceedings of
Senseval-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text,
pages 199?202, Barcelona.
O?Hara, Tom and Janyce Wiebe. 2003.
Classifying functional relations in
Factotum via WordNet hypernym
associations. In Proceedings of the
Fourth International Conference on
Intelligent Text Processing and
Computational Linguistics (CICLing-2003),
pages 347?359, Mexico City.
OpenCyc. 2002. OpenCyc release 0.6b.
Available at www.opencyc.org.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Palmer, Martha Stone. 1990. Semantic
Processing for Finite Domains. Cambridge
University Press, Cambridge.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005. Support
vector learning for semantic argument
classification.Machine Learning,
60(1?3):11?39.
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Scott, Sam and Stan Matwin. 1998. Text
classification using WordNet hypernyms.
In Proceedings of the COLING/ACL
Workshop on Usage of WordNet in Natural
Language Processing Systems, pages 38?44,
Montreal.
Somers, Harold L. 1987. Valency and Case in
Computational Linguistics. Edinburgh
University Press, Scotland.
Sowa, John F. 1984. Conceptual Structures in
Mind and Machines. Addison-Wesley,
Reading, MA.
Sowa, John F. 1999. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing,
Pacific Grove, CA.
Srihari, Rohini, Cheng Niu, and Wei Li.
2001. A hybrid approach for named
entity and sub-type tagging. In
Proceedings of the 6th Applied Natural
Language Processing Conference,
pages 247?254, Seattle.
Wiebe, Janyce, Kenneth McKeever, and
Rebecca F. Bruce. 1998. Mapping
collocational properties into machine
learning features. In Proceedings of the
6th Workshop on Very Large Corpora
(WVLC-98), pages 225?233, Montreal.
183
Computational Linguistics Volume 35, Number 2
Wilks, Yorick, Brian M. Slator, and Louise
Guthrie. 1996. Electric Words. MIT Press,
Cambridge, MA.
Witten, Ian H. and Eibe Frank. 1999.
Data Mining: Practical Machine
Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann,
San Francisco, CA.
Ye, Patrick and Timothy Baldwin. 2006.
Semantic role labeling of prepositional
phrases. ACM Transactions on Asian
Language Information Processing (TALIP),
5(3):228?244.
Ye, Patrick and Timothy Baldwin.
2007. MELB-YB: Preposition sense
disambiguation using rich semantic
features. In Proceedings of the Fourth
International Workshop on Semantic
Evaluations (SemEval-2007),
pages 241?244, Prague.
184
This article has been cited by:
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
Recognizing Contextual Polarity:
An Exploration of Features for Phrase-Level
Sentiment Analysis
Theresa Wilson?
University of Edinburgh
Janyce Wiebe??
University of Pittsburgh
Paul Hoffmann??
University of Pittsburgh
Many approaches to automatic sentiment analysis begin with a large lexicon of words marked
with their prior polarity (also called semantic orientation). However, the contextual polarity of
the phrase in which a particular instance of a word appears may be quite different from the
word?s prior polarity. Positive words are used in phrases expressing negative sentiments, or
vice versa. Also, quite often words that are positive or negative out of context are neutral in
context, meaning they are not even being used to express a sentiment. The goal of this work is to
automatically distinguish between prior and contextual polarity, with a focus on understanding
which features are important for this task. Because an important aspect of the problem is
identifying when polar terms are being used in neutral contexts, features for distinguishing
between neutral and polar instances are evaluated, as well as features for distinguishing between
positive and negative contextual polarity. The evaluation includes assessing the performance
of features across multiple machine learning algorithms. For all learning algorithms except
one, the combination of all features together gives the best performance. Another facet of the
evaluation considers how the presence of neutral instances affects the performance of features for
distinguishing between positive and negative polarity. These experiments show that the presence
of neutral instances greatly degrades the performance of these features, and that perhaps the
best way to improve performance across all polarity classes is to improve the system?s ability to
identify when an instance is neutral.
1. Introduction
Sentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on iden-
tifying positive and negative opinions, emotions, and evaluations expressed in natural
language. It has been a central component in applications ranging from recognizing
? School of Informatics, Edinburgh EH8 9LW, U.K. E-mail: twilson@inf.ed.ac.uk.
?? Department of Computer Science, Pittsburgh, PA 15260, USA. E-mail: {wiebe,hoffmanp}@cs.pitt.edu.
Submission received: 14 November 2006; revised submission received: 8March 2008; accepted for publication:
16 April 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
inflammatory messages (Spertus 1997), to tracking sentiments over time in online
discussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, and
Vaithyanathan 2002; Turney 2002). Although a great deal of work in sentiment analy-
sis has targeted documents, applications such as opinion question answering (Yu and
Hatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and re-
view mining to extract opinions about companies and products (Morinaga et al 2002;
Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis. For exam-
ple, if a question answering system is to successfully answer questions about people?s
opinions, it must be able not only to pinpoint expressions of positive and negative
sentiments, such as we find in sentence (1), but also to determine when an opinion is not
being expressed by a word or phrase that typically does evoke one, such as condemned
in sentence (2).
(1) African observers generally approved (positive) of his victory while
Western governments denounced (negative) it.
(2) Gavin Elementary School was condemned in April 2004.
A common approach to sentiment analysis is to use a lexicon with information
about which words and phrases are positive and which are negative. This lexicon may
be manually compiled, as is the case with the General Inquirer (Stone et al 1966), a
resource often used in sentiment analysis. Alternatively, the information in the lexicon
may be acquired automatically. Acquiring the polarity of words and phrases is itself
an active line of research in the sentiment analysis community, pioneered by the
work of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semantic
orientation of adjectives. Various techniques have been proposed for learning the
polarity of words. They include corpus-based techniques, such as using constraints
on the co-occurrence in conjunctions of words with similar or opposite polarity
(Hatzivassiloglou and McKeown 1997) and statistical measures of word association
(Turney and Littman 2003), as well as techniques that exploit information about lexical
relationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli and
Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.
Acquiring the polarity of words and phrases is undeniably important, and there
are still open research challenges, such as addressing the sentiments of different senses
of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on. However,
what the polarity of a given word or phrase is when it is used in a particular context is
another problem entirely. Consider, for example, the underlined positive and negative
words in the following sentence.
(3) Philip Clapp, president of the National Environment Trust, sums up well
the general thrust of the reaction of environmental movements: ?There is
no reason at all to believe that the polluters are suddenly going to become
reasonable.?
The first underlined word is Trust. Although many senses of the word trust express a
positive sentiment, in this case, the word is not being used to express a sentiment at
all. It is simply part of an expression referring to an organization that has taken on
the charge of caring for the environment. The adjective well is considered positive, and
indeed it is positive in this context. However, the same is not true for the words reason
400
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
and reasonable. Out of context, we would consider both of these words to be positive.1
In context, the word reason is being negated, changing its polarity from positive to
negative. The phrase no reason at all to believe changes the polarity of the proposition that
follows; because reasonable falls within this proposition, its polarity becomes negative.
The word polluters has a negative connotation, but here in the context of the discussion
of the article and its position in the sentence, polluters is being used less to express a
sentiment and more to objectively refer to companies that pollute. To clarify how the
polarity of polluters is affected by its subject role, consider the purely negative sentiment
that emerges when it is used as an object: They are polluters.
We call the polarity that would be listed for a word in a lexicon the word?s prior
polarity, and we call the polarity of the expression in which a word appears, con-
sidering the context of the sentence and document, the word?s contextual polarity.
Although words often do have the same prior and contextual polarity, many times
a word?s prior and contextual polarities differ. Words with a positive prior polarity
may have a negative contextual polarity, or vice versa. Quite often words that are
positive or negative out of context are neutral in context, meaning that they are not
even being used to express a sentiment. Similarly, words that are neutral out of context,
neither positive or negative, may combine to create a positive or negative expression in
context.
The focus of this work is on the recognition of contextual polarity?in particular,
disambiguating the contextual polarity of words with positive or negative prior polar-
ity. We begin by presenting an annotation scheme for marking sentiment expressions
and their contextual polarity in the Multi-perspective Question Answering (MPQA)
opinion corpus. We show that, given a set of subjective expressions (identified from
the existing annotations in the MPQA corpus), contextual polarity can be annotated
reliably.
Using the contextual polarity annotations, we conduct experiments in automatically
distinguishing between prior and contextual polarity. Beginning with a large lexicon of
clues tagged with prior polarity, we identify the contextual polarity of the instances
of those clues in the corpus. The process that we use has two steps, first classifying
each clue as being in a neutral or polar phrase, and then disambiguating the contextual
polarity of the clues marked as polar. For each step in the process, we experiment with a
variety of features and evaluate the performance of the features using several different
machine learning algorithms.
Our experiments reveal a number of interesting findings. First, being able to accu-
rately identify neutral contextual polarity?when a positive or negative clue is not being
used to express a sentiment?is an important aspect of the problem. The importance of
neutral examples has previously been noted for classifying the sentiment of documents
(Koppel and Schler 2006), but ours is the first work to explore how neutral instances
affect classifying the contextual polarity of words and phrases. In particular, we found
that the performance of features for distinguishing between positive and negative po-
larity greatly degrades when neutral instances are included in the experiments.
We also found that achieving the best performance for recognizing contextual po-
larity requires a wide variety of features. This is particularly true for distinguishing
1 It is open to question whether reason should be listed as positive in a sentiment lexicon, because the more
frequent senses of reason involve intention, not sentiment. However, any existing sentiment lexicon one
would start with will have some noise and errors. The task in this article is to disambiguate instances of
the entries in a given sentiment lexicon.
401
Computational Linguistics Volume 35, Number 3
between neutral and polar instances. Although some features help to increase polar or
neutral recall or precision, it is only the combination of features together that achieves
significant improvements in accuracy over the baselines. Our experiments show that for
distinguishing between positive and negative instances, features capturing negation are
clearly the most important. However, there is more to the story than simple negation.
Features that capture relationships between instances of clues also perform well, indi-
cating that identifying features that represent more complex interdependencies between
sentiment clues may be an important avenue for future research.
The remainder of this article is organized as follows. Section 2 gives an overview
of some of the things that can influence contextual polarity. In Section 3, we describe
our corpus and present our annotation scheme and inter-annotator agreement study
for marking contextual polarity. Sections 4 and 5 describe the lexicon used in our
experiments and how the contextual polarity annotations are used to determine the
gold-standard tags for instances from the lexicon. In Section 6, we consider what kind of
performance can be expected from a simple, prior-polarity classifier. Section 7 describes
the features that we use for recognizing contextual polarity, and our experiments
and results are presented in Section 8. In Section 9 we discuss related work, and we
conclude in Section 10.
2. Polarity Influencers
Phrase-level sentiment analysis is not a simple problem. Many things besides negation
can influence contextual polarity, and even negation is not always straightforward.
Negation may be local (e.g., not good), or involve longer-distance dependencies such as
the negation of the proposition (e.g., does not look very good) or the negation of the subject
(e.g., no one thinks that it?s good). In addition, certain phrases that contain negation words
intensify rather than change polarity (e.g., not only good but amazing). Contextual polarity
may also be influenced bymodality: whether the proposition is asserted to be real (realis)
or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g.,
Environmental Trust vs. He has won the people?s trust); the syntactic role of a word in the
sentence: whether the word is the subject or object of a copular verb (consider polluters
are versus they are polluters); and diminishers such as little (e.g., little truth, little threat).
Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarity
influencers. Many of these contextual polarity influencers are represented as features in
our experiments.
Contextual polarity may also be influenced by the domain or topic. For example,
the word cool is positive if used to describe a car, but it is negative if it is used to
describe someone?s demeanor. Similarly, a word such as fever is unlikely to be expressing
a sentiment when used in a medical context. We use one feature in our experiments to
represent the topic of the document.
Another important aspect of contextual polarity is the perspective of the person
who is expressing the sentiment. For example, consider the phrase failed to defeat
in the sentence Israel failed to defeat Hezbollah. From the perspective of Israel, failed
to defeat is negative. From the perspective of Hezbollah, failed to defeat is positive.
Therefore, the contextual polarity of this phrase ultimately depends on the perspec-
tive of who is expressing the sentiment. Although automatically detecting this kind
of pragmatic influence on polarity is beyond the scope of this work, this as well as
the other types of polarity influencers all are considered when annotating contextual
polarity.
402
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
3. Data and Annotations
For the experiments in this work, we need a corpus that is annotated comprehensively
for sentiment expressions and their contextual polarity. Rather than building a corpus
from scratch, we chose to add contextual polarity annotations to the existing annota-
tions in the Multi-perspective Question Answering (MPQA) opinion corpus2 (Wiebe,
Wilson, and Cardie 2005).
The MPQA corpus is a collection of English-language versions of news documents
from the world press. The documents contain detailed, expression-level annotations
of attributions and private states (Quirk et al 1985). Private states are mental and
emotional states; they include beliefs, speculations, intentions, and sentiments, among
others. Although sentiments are not distinguished from other types of private states
in the existing annotations, they are a subset of what already is annotated. This makes
the annotations in the MPQA corpus a good starting point for annotating sentiment
expressions and their contextual polarity.
3.1 Annotation Scheme
When developing our annotation scheme for sentiment expressions and contextual
polarity, there were three main questions to address. First, which of the existing annota-
tions in the MPQA corpus have the possibility of being sentiment expressions? Second,
which of the possible sentiment expressions actually are expressing sentiments? Third,
what coding scheme should be used for marking contextual polarity?
TheMPQA annotation scheme has four types of annotations: objective speech event
frames, two types of private state frames, and agent frames that are used for marking
speakers of speech events and experiencers of private states. A full description of
the MPQA annotation scheme and an agreement study evaluating key aspects of the
scheme are found in Wiebe, Wilson, and Cardie (2005).
The two types of private state frames, direct subjective frames and expressive sub-
jective element frames, are where we will find sentiment expressions. Direct subjective
frames are used to mark direct references to private states as well as speech events in
which private states are being expressed. For example, in the following sentences, fears,
praised, and said are all marked as direct subjective annotations.
(4) The U.S. fears a spill-over of the anti-terrorist campaign.
(5) Italian senator Renzo Gubert praised the Chinese government?s efforts.
(6) ?The report is full of absurdities,? he said.
The word fears directly refers to a private state; praised refers to a speech event
in which a private state is being expressed; and said is marked as direct subjective
because a private state is being expressed within the speech event referred to by
said. Expressive subjective elements indirectly express private states through the way
something is described or through a particular wording. In example (6), the phrase
full of absurdities is an expressive subjective element. Subjectivity (Banfield 1982; Wiebe
2 Available at http://nrrc.mitre.org/NRRC/publications.htm.
403
Computational Linguistics Volume 35, Number 3
1994) refers to the linguistic expression of private states, hence the names for the two
types of private state annotations.
All expressive subjective elements are included in the set of annotations that have
the possibility of being sentiment expressions, but the direct subjective frames to include
in this set can be pared down further. Direct subjective frames have an attribute, expres-
sion intensity, that captures the contribution of the annotated word or phrase to the
overall intensity of the private state being expressed. Expression intensity ranges from
neutral to high. In the given sentences, fears and praised have an expression intensity of
medium, and said has an expression intensity of neutral. A neutral expression intensity
indicates that the direct subjective phrase itself is not contributing to the expression
of the private state. If this is the case, then the direct subjective phrase cannot be
a sentiment expression. Thus, only direct subjective annotations with a non-neutral
expression intensity are included in the set of annotations that have the possibility of
being sentiment expressions. We call this set of annotations, the union of the expres-
sive subjective elements and the direct subjective frames with a non-neutral intensity,
the subjective expressions in the corpus; these are the annotations we will mark for
contextual polarity.
Table 1 gives a sample of subjective expressions marked in the MPQA corpus.
Although many of the words and phrases express what we typically think of as
sentiments, others do not, for example, believes, very definitely, and unconditionally and
without delay.
Now that we have identified which annotations have the possibility of being sen-
timent expressions, the next question is which of these annotated words and phrases
are actually expressing sentiments. We define a sentiment as a positive or negative
emotion, evaluation, or stance. On the left of Table 2 are examples of positive sentiments;
examples of negative sentiments are on the right.
Table 1
Sample of subjective expressions from the MPQA corpus.
victory of justice and freedom such a disadvantageous situation
grown tremendously must
such animosity not true at all
throttling the voice imperative for harmonious society
disdain and wrath glorious
so exciting disastrous consequences
could not have wished for a better situation believes
freak show the embodiment of two-sided justice
if you?re not with us, you?re against us appalling
vehemently denied very definitely
everything good and nice once and for all
under no circumstances shameful mum
most fraudulent, terrorist and extremist enthusiastically asked
number one democracy hate
seems to think gross misstatement
indulging in blood-shed and their lunaticism surprised, to put it mildly
take justice to pre-historic times unconditionally and without delay
so conservative that it makes Pat Buchanan look vegetarian
those digging graves for others, get engraved themselves
lost the reputation of commitment to principles of human justice
ultimately the demon they have reared will eat up their own vitals
404
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 2
Examples of positive and negative sentiments.
Positive sentiments Negative sentiments
Emotion I?m happy I?m sad
Evaluation Great idea! Bad idea!
Stance She supports the bill She?s against the bill
The final issue to address is the actual annotation scheme for marking contextual
polarity. The scheme we developed has four tags: positive, negative, both, and neutral.
The positive tag is used to mark positive sentiments. The negative tag is used to mark
negative sentiments. The both tag is applied to expressions in which both a positive and
negative sentiment are being expressed. Subjective expressions with positive, negative, or
both tags are our sentiment expressions. The neutral tag is used for all other subjective
expressions, including emotions, evaluations, and stances that are neither positive or
negative. Instructions for the contextual-polarity annotation scheme are available at
http://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt.
Following are examples from the corpus of each of the different contextual-polarity
annotations. Each underlinedword or phrase is a subjective expression that wasmarked
in the original MPQA annotations.3 In bold following each subjective expression is the
contextual polarity with which it was annotated.
(7) Thousands of coup supporters celebrated (positive) overnight, waving
flags, blowing whistles . . .
(8) The criteria set by Rice are the following: the three countries in question are
repressive (negative) and grave human rights violators (negative) . . .
(9) Besides, politicians refer to good and evil (both) only for purposes of
intimidation and exaggeration.
(10) Jerome says the hospital feels (neutral) no different than a hospital in the
states.
As a final note on the annotation scheme, annotators are asked to judge the con-
textual polarity of the sentiment that is ultimately being conveyed by the subjective
expression, that is, once the sentence has been fully interpreted. Thus, the subjective
expression, they have not succeeded, and will never succeed, is marked as positive in the
following sentence:
(11) They have not succeeded, and will never succeed (positive), in breaking
the will of this valiant people.
The reasoning is that breaking the will of a valiant people is negative, so to not succeed
in breaking their will is positive.
3 Some sentences contain additional subjective expressions that are not underlined as examples.
405
Computational Linguistics Volume 35, Number 3
Table 3
Contingency table for contextual polarity agreement.
Neutral Positive Negative Both Total
Neutral 123 14 24 0 161
Positive 16 73 5 2 96
Negative 14 2 167 1 184
Both 0 3 0 3 6
Total 153 92 196 6 447
Table 4
Contingency table for contextual polarity agreement, borderline cases removed.
Neutral Positive Negative Both Total
Neutral 113 7 8 0 128
Positive 9 59 3 0 71
Negative 5 2 156 1 164
Both 0 2 0 2 4
Total 127 70 167 3 367
3.2 Agreement Study
To measure the reliability of the polarity annotation scheme, we conducted an agree-
ment study with two annotators4 using 10 documents from the MPQA corpus. The 10
documents contain 447 subjective expressions. Table 3 shows the contingency table for
the two annotators? judgments. Overall agreement is 82%, with a kappa value of 0.72.
As part of the annotation scheme, annotators are asked to judge how certain they
are in their polarity tags. For 18% of the subjective expressions, at least one annotator
used the uncertain tag whenmarking polarity. If we consider these cases to be borderline
and exclude them from the study, percent agreement increases to 90% and kappa rises to
0.84. Table 4 shows the revised contingency table with the uncertain cases removed. This
shows that annotator agreement is especially highwhen both annotators are certain, and
that annotators are certain for over 80% of their tags.
Note that all annotations are included in the experiments.
3.3 Contextual Polarity Annotations
In total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of the
MPQA corpus were annotated with their contextual polarity as just described.5 Three
annotators carried out the task: the two who participated in the annotation study and
a third who was trained later.6 Table 5 gives the distribution of the contextual polarity
tags. Looking at this table, we see that a small majority of subjective expressions (54.6%)
4 Both annotators are authors of this article.
5 The revised version of the MPQA corpus with the contextual polarity annotations is available at
http://www.cs.pitt.edu/mpqa.
6 The third annotator received training until her reliability of performance on the task was comparable to
that of the first two annotators who participated in the study.
406
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 5
Distribution of contextual polarity tags.
Neutral Positive Negative Both Total
9,057 3,311 7,294 299 19,961
45.4% 16.6% 36.5% 1.5% 100%
are expressing a positive, negative, or both (positive and negative) sentiment. We refer to
these expressions as polar in context. Many of the subjective expressions are neutral
and do not express a sentiment. This suggests that, although sentiment is a major type
of subjectivity, distinguishing other prominent types of subjectivity will be important
for future work in subjectivity analysis.
As many NLP applications operate at the sentence level, one important issue to
consider is the distribution of sentences with respect to the subjective expressions
they contain. In the 11,112 sentences in the MPQA corpus, 28% contain no subjective
expressions, 24% contain only one, and 48% contain two or more. Of the 5,304 sentences
containing two or more subjective expressions, 17% contain mixtures of positive and
negative expressions, and 61% contain mixtures of polar (positive/negative/both) and
neutral subjective expressions.
4. Prior-Polarity Subjectivity Lexicon
For the experiments in this article, we use a lexicon of over 8,000 subjectivity clues.
Subjectivity clues are words and phrases that may be used to express private states. In
other words, subjectivity clues have subjective usages, though they may have objective
usages as well. For this work, only single-word clues are used.
To compile the lexicon, we began with the list of subjectivity clues from Riloff and
Wiebe (2003), which includes the positive and negative adjectives fromHatzivassiloglou
and McKeown (1997). The words in this list were grouped in previous work according
to their reliability as subjectivity clues. Words that are subjective in most contexts are
considered strong subjective clues, indicated by the strongsubj tag. Words that may
only have certain subjective usages are considered weak subjective clues, indicated by
the weaksubj tag.
We expanded the list using a dictionary and a thesaurus, and added words from the
General Inquirer positive and negative word lists (Stone et al 1966) that we judged to be
potentially subjective.7 We also gave the new words strongsubj and weaksubj reliability
tags. The final lexicon has a coverage of 67% of subjective expressions in the MPQA
corpus, where coverage is the percentage of subjective expressions containing one or
more instances of clues from the lexicon. The coverage of just sentiment expressions is
even higher: 75%.
The next step was to tag the clues in the lexicon with their prior polarity: positive,
negative, both, or neutral. A word in the lexicon is tagged as positive if out of context
it seems to evoke something positive, and negative if it seems to evoke something
negative. If a word has both positive and negative meanings, it is tagged with the
polarity that seems the most common. A word is tagged as both if it is at the same time
7 In the end, about 70% of the words from the General Inquirer positive word list and 80% of the words
from the negative word list were included in the subjectivity lexicon.
407
Computational Linguistics Volume 35, Number 3
both positive and negative. For example, the word bittersweet evokes something both
positive and negative. Words like brag are also tagged as both, because the one who is
bragging is expressing something positive, yet at the same time describing someone as
bragging is expressing a negative evaluation of that person. A word is tagged as neutral
if it does not evoke anything positive or negative.
For words that came from positive and negative word lists (Stone et al 1966;
Hatzivassiloglou and McKeown 1997), we largely retained their original polarity.
However, we did change the polarity of a word if we strongly disagreed with its
original class.8 For example, the word apocalypse is listed as positive in the General
Inquirer; we changed its prior polarity to negative for our lexicon.
By far, the majority of clues in the lexicon (92.8%) are marked as having either
positive (33.1%) or negative (59.7%) prior polarity. Only a small number of clues (0.3%)
are marked as having both positive and negative polarity. We refer to the set of clues
marked as positive, negative, or both as sentiment clues. A total of 6.9% of the clues in
the lexicon are marked as neutral. Examples of neutral clues are verbs such as feel, look,
and think, and intensifiers such as deeply, entirely, and practically. Although the neutral
clues make up a small proportion of the total words in the lexicon, we retain them for
our later experiments in recognizing contextual polarity because many of them are good
clues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on,
look forward to). Including them increases the coverage of the system.
At the end of the previous section, we considered the distribution of sentences in the
MPQA corpus with respect to the subjective expressions they contain. It is interesting
to compare that distribution with the distribution of sentences with respect to the
instances they contain of clues from the lexicon. We find that there are more sentences
with two or more clue instances (62%) than sentences with two or more subjective
expressions (48%). More importantly, many more sentences have mixtures of positive
and negative clue instances than actually have mixtures of positive and negative sub-
jective expressions. Only 880 sentences have a mixture of both positive and negative
subjective expressions, whereas 3,234 sentences have a mixture of positive and negative
clue instances. Thus, a large number of positive and negative instances are either neutral
in context, or they are combining to form more complex polarity expressions. Either
way, this provides strong evidence of the need to be able to disambiguate the contextual
polarity of subjectivity and sentiment clues.
5. Definition of the Gold Standard
In the experiments described in the following sections, the goal is to classify the con-
textual polarity of the expressions that contain instances of the subjectivity clues in our
lexicon. However, determining which clue instances are part of the same expression and
identifying expression boundaries are not the focus of this work. Thus, instead of trying
to identify and label each expression, in the following experiments, each clue instance
is labeled individually as to its contextual polarity.
We define the gold-standard contextual polarity of a clue instance in terms of the
manual annotations (Section 3) as follows. If a clue instance is not in a subjective
expression (and therefore not in a sentiment expression), its gold class is neutral. If
a clue instance appears in just one subjective expression or in multiple subjective
8 We decided on a different polarity for about 80 of the words in our lexicon that appeared on other
positive and negative word lists.
408
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
expressions with the same contextual polarity, its gold class is the contextual polarity
of the subjective expression(s). If a clue instance appears in a mixture of negative and
neutral subjective expressions, its gold class is negative; if it is in amixture of positive and
neutral subjective expressions, its gold class is positive. Finally, if a clue instance appears
in at least one positive and one negative subjective expression (or in a subjective ex-
pression marked as both), then its gold class is both. A clue instance can appear in more
than one subjective expression because in the MPQA annotation scheme, it is possible
for direct subjective frames and expressive subjective elements frames to overlap.
6. A Prior-Polarity Classifier
Before delving into the task of recognizing contextual polarity, an important question
to address is how useful prior polarity alone is for identifying contextual polarity. To
answer this question, we create a classifier that simply assumes the contextual polarity
of a clue instance is the same as the clue?s prior polarity. We explore this classifier?s
performance on a small amount of development data, which is not part of the data used
in the subsequent experiments.
This simple classifier has an accuracy of 48%. From the confusion matrix given in
Table 6, we see that 76% of the errors result from words with non-neutral prior polarity
appearing in phrases with neutral contextual polarity. Only 12% of the errors result from
words with neutral prior polarity appearing in expressions with non-neutral contextual
polarity, and only 11% of the errors come from words with a positive or negative prior
polarity appearing in expressions with the opposite contextual polarity. Table 6 also
shows that positive clues tend to be used in negative expressions far more often than
negative clues tend to be used in positive expressions.
Given that by far the largest number of errors come from clues with positive,
negative, or both prior polarity appearing in neutral contexts, we were motivated to try
a two-step approach to the problem of sentiment classification. The first step, Neutral?
Polar Classification, tries to determine if an instance is neutral or polar in context. The
second step, Polarity Classification, takes all instances that step one classified as polar,
and tries to disambiguate their contextual polarity. This two-step approach is illustrated
in Figure 1.
7. Features
The features used in our experiments were motivated both by the literature and by
exploration of the contextual-polarity annotations in our development data. A number
Table 6
Confusion matrix for the prior-polarity classifier on the development set.
Prior-Polarity Classifier
Neutral Positive Negative Both Total
Neutral 798 784 698 4 2284
Gold Positive 81 371 40 0 492
Class Negative 149 181 622 0 952
Both 4 11 13 5 33
Total 1032 1347 1373 9 3761
409
Computational Linguistics Volume 35, Number 3
Figure 1
Two-step approach to recognizing contextual polarity.
of features were inspired by the paper on contextual-polarity influencers by Polanyi
and Zaenan (2004). Other features are those that have been found useful in the past
for recognizing subjective sentences (Wiebe, Bruce, and O?Hara 1999; Wiebe and Riloff
2005).
7.1 Features for Neutral?Polar Classification
For distinguishing between neutral and polar instances, we use the features listed in
Table 7. For ease of description, we group the features into six sets: word features, gen-
eral modification features, polarity modification features, structure features, sentence
features, and one document feature.
Word Features In addition to the word token (the token of the clue instance being
classified), the word features include the parts of speech of the previous word, the word
itself, and the next word. The prior polarity and reliability class features represent those
pieces of information about the clue which are taken from the lexicon.
General Modification Features These are binary features that capture different
types of relationships involving the clue instance.
The first four features involve relationships with the word immediately before or af-
ter the clue instance. The preceded by adjective feature is true if the clue instance is a noun
preceded by an adjective. The preceded by adverb feature is true if the preceding word
is an adverb other than not. The preceded by intensifier feature is true if the preceding
word is an intensifier, and the self intensifier feature is true if the clue instance itself is an
intensifier. A word is considered to be an intensifier if it appears in a list of intensifiers
and if it precedes a word of the appropriate part of speech (e.g., an intensifier adjective
must come before a noun). The list of intensifiers is a compilation of those listed in Quirk
et al (1985), intensifiers identified from existing entries in the subjectivity lexicon, and
intensifiers identified during explorations of the development data.
The modifies/modifed by features involve the dependency parse tree of the sentence,
obtained by first parsing the sentence (Collins 1997) and then converting the tree into
its dependency representation (Xia and Palmer 2001). In a dependency representation,
every node in the tree structure is a surface word (i.e., there are no abstract nodes such
as NP or VP). The parent word is called the head, and its children are its modifiers. The
410
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 7
Features for neutral?polar classification.
Word Features
word token
word part of speech
previous word part of speech
next word part of speech
prior polarity: positive, negative, both, neutral
reliability class: strongsubj or weaksubj
General Modification Features
preceded by adjective: binary
preceded by adverb (other than not): binary
preceded by intensifier: binary
self intensifier: binary
modifies strongsubj: binary
modifies weaksubj: binary
modified by strongsubj: binary
modified by weaksubj: binary
Polarity Modification Features
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conjunction polarity: positive, negative, neutral, both, notmod
Structure Features
in subject: binary
in copular: binary
in passive: binary
Sentence Features
strongsubj clues in current sentence: 0, 1, 2, 3 (or more)
strongsubj clues in previous sentence: 0, 1, 2, 3 (or more)
strongsubj clues in next sentence: 0, 1, 2, 3 (or more)
weaksubj clues in current sentence: 0, 1, 2, 3 (or more)
weaksubj clues in previous sentence: 0, 1, 2, 3 (or more)
weaksubj clues in next sentence: 0, 1, 2, 3 (or more)
adjectives in sentence: 0, 1, 2, 3 (or more)
adverbs in sentence (other than not): 0, 1, 2, 3 (or more)
cardinal number in sentence: binary
pronoun in sentence: binary
modal in sentence (other than will): binary
Document Feature
document topic/domain
edge between a parent and a child specifies the grammatical relationship between the
two words. Figure 2 shows an example of a dependency parse tree. Instances of clues in
the tree are marked with the clue?s prior polarity and reliability class from the lexicon.
For each clue instance, the modifies/modifed by features capture whether there are
adj, mod, or vmod relationships between the clue instance and any other instances from
the lexicon. Specifically, the modifies strongsubj feature is true if the clue instance and
its parent share an adj, mod, or vmod relationship, and if its parent is an instance of
a strongsubj clue from the lexicon. The modifies weaksubj feature is the same, except
that it looks in the parent for an instance of a weaksubj clue. The modified by strongsubj
411
Computational Linguistics Volume 35, Number 3
Figure 2
The dependency tree for the sentence The human rights report poses a substantial challenge to the
U.S. interpretation of good and evil. Prior polarity and reliability class are marked in parentheses
for words that match clues from the lexicon.
feature is true for a clue instance if one of its children is an instance of a strongsubj
clue, and if the clue instance and its child share an adj, mod, or vmod relationship. The
modified by weaksubj feature is the same, except that it looks for instances of weaksubj
clues in the children. Although the adj and vmod relationships are typically local, the
mod relationship involves longer-distance as well as local dependencies. Figure 2 helps
to illustrate these features. The modifies weaksubj feature is true for substantial, because
substantial modifies challenge, which is an instance of a weaksubj clue. For rights, the
modifies weaksubj feature is false, because rightsmodifies report, which is not an instance
of a weaksubj clue. The modified by weaksubj feature is false for substantial, because it has
no modifiers that are instances of weaksubj clues. For challenge, the modified by weaksubj
feature is true because it is being modified by substantial, which is an instance of a
weaksubj clue.
Polarity Modification Features The modifies polarity, modified by polarity, and conj
polarity features capture specific relationships between the clue instance and other senti-
ment clues it may be related to. If the clue instance and its parent in the dependency tree
share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior
polarity of the parent. If the parent is not in the prior-polarity lexicon, its prior polarity
is considered neutral. If the clue instance is at the root of the tree and has no parent,
the value of the feature is notmod. The modified by polarity feature is similar, looking
for adj,mod, and vmod relationships and other sentiment clues in the children of the clue
instance. The conj polarity feature determines if the clue instance is in a conjunction. If so,
the value of this feature is its sibling?s prior polarity. As before, if the sibling is not in the
412
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
lexicon, its prior polarity is neutral. If the clue instance is not in a conjunction, the value
for this feature is notmod. Figure 2 also helps to illustrate thesemodification features. The
word substantial with positive prior polarity modifies the word challenge with negative
prior polarity. Therefore the modifies polarity feature is negative for substantial, and the
modified by polarity feature is positive for challenge. The words good and evil are in a con-
junction together; thus the conj polarity feature is negative for good and positive for evil.
Structure Features These are binary features that are determined by starting with
the clue instance and climbing up the dependency parse tree toward the root, looking
for particular relationships, words, or patterns. The in subject feature is true if we find
a subj relationship on the path to the root. The in copular feature is true if in subject is
false and if a node along the path is both a main verb and a copular verb. The in passive
feature is true if a passive verb pattern is found on the climb.
The in subject and in copular features were motivated by the intuition that the syn-
tactic role of a word may influence whether a word is being used to express a sentiment.
For example, consider the word polluters in each of the following two sentences.
(12) Under the application shield, polluters are allowed to operate if they have
a permit.
(13) ?The big-city folks are pointing at the farmers and saying you are
polluters . . . ?
In the first sentence, polluters is simply being used as a referring expression. In the
second sentence, polluters is clearly being used to express a negative evaluation of the
farmers. Themotivation for the in passive feature was previous work by Riloff andWiebe
(2003), who found that different words aremore or less likely to be subjective depending
on whether they are in the active or passive.
Sentence Features These are features that previously were found useful for
sentence-level subjectivity classification (Wiebe, Bruce, and O?Hara 1999; Wiebe and
Riloff 2005). They include counts of strongsubj and weaksubj clue instances in the cur-
rent, previous and next sentences, counts of adjectives and adverbs other than not in
the current sentence, and binary features to indicate whether the sentence contains a
pronoun, a cardinal number, and a modal other than will.
Document Feature There is one document feature representing the topic or domain
of the document. The motivation for this feature is that whether or not a word is
expressing a sentiment or even a private state in general may depend on the subject
of the discourse. For example, the words fever and sufferer may express a negative
sentiment in certain contexts, but probably not in a health or medical context, as is the
case in the following sentence.
(14) The disease can be contracted if a person is bitten by a certain tick or if a
person comes into contact with the blood of a congo fever sufferer.
In the creation of the MPQA corpus, about two-thirds of the documents were
selected to be on one of the 10 topics listed in Table 8. The documents for each topic were
identified by human searches and by an information retrieval system. The remaining
documents were semi-randomly selected from a very large pool of documents from
the world press. In the corpus, these documents are listed with the topic miscellaneous.
Rather than leaving these documents unlabeled, we chose to label them using the
413
Computational Linguistics Volume 35, Number 3
Table 8
Topics in the MPQA corpus.
Topic Description
argentina Economic collapse in Argentina
axisofevil U.S. President?s State of the Union Address
guantanamo Detention of prisoners in Guantanamo Bay
humanrights U.S. State Department Human Rights Report
kyoto Kyoto Protocol ratification
settlements Israeli settlements in Gaza and the West Bank
space Space missions of various countries
taiwan Relationship between Taiwan and China
venezuela Presidential coup in Venezuela
zimbabwe Presidential election in Zimbabwe
following general domain categories: economics, general politics, health, report events,
and war and terrorism.
7.2 Features for Polarity Classification
Table 9 lists the features that we use for step two, polarity classification. Word token,
word prior polarity, and the polarity-modification features are the same as described for
neutral?polar classification.
We use two features to capture two different types of negation. The negated feature
is a binary feature that is used to capture more local negations: Its value is true if a
negation word or phrase is found within the four words preceding the clue instance,
and if the negation word is not also in a phrase that acts as an intensifier rather than a
negator. Examples of phrases that intensify rather than negate are not only and nothing if
not. The negated subject feature captures a longer-distance type of negation. This feature
Table 9
Features for polarity classification.
Word Features
word token
word prior polarity: positive, negative, both, neutral
Negation Features
negated: binary
negated subject: binary
Polarity Modification Features
modifies polarity: positive, negative, neutral, both, notmod
modified by polarity: positive, negative, neutral, both, notmod
conj polarity: positive, negative, neutral, both, notmod
Polarity Shifters
general polarity shifter: binary
negative polarity shifter: binary
positive polarity shifter: binary
414
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
is true if the subject of the clause containing the clue instance is negated. For example,
the negated subject feature is true for support in the following sentence.
(15) No politically prudent Israeli could support either of them.
The last three polarity features look in a window of four words before the clue
instance, searching for the presence of particular types of polarity influencers. Gen-
eral polarity shifters reverse polarity (e.g., little truth, little threat). Negative polarity
shifters typically make the polarity of an expression negative (e.g., lack of understand-
ing). Positive polarity shifters typically make the polarity of an expression positive
(e.g., abate the damage). The polarity influencers that we used were identified through
explorations of the development data.
8. Experiments in Recognizing Contextual Polarity
We have two primary goals with our experiments in recognizing contextual polarity.
The first is to evaluate the features described in Section 7 as to their usefulness for
this task. The second is to investigate the importance of recognizing neutral instances?
recognizing when a sentiment clue is not being used to express a sentiment?for classi-
fying contextual polarity.
To evaluate features, we investigate their performance, both together and sep-
arately, across several different learning algorithms. Varying the learning algorithm
allows us to verify that the features are robust and that their performance is not the
artifact of a particular algorithm. We experiment with four different types of machine
learning: boosting, memory-based learning, rule learning, and support vector learning.
For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH. For rule
learning, we use Ripper (Cohen 1996). For memory-based learning, we use TiMBL
(Daelemans et al 2003b) IB1 (k-nearest neighbor). For support vector learning, we
use SVM-light and SVM-multiclass (Joachims 1999). SVM-light is used for the experi-
ments involving binary classification (neutral?polar classification), and SVM-multiclass
is used for experiments with more than two classes. These machine learning algorithms
were chosen because they have been used successfully for a number of natural language
processing tasks, and they represent several different types of learning.
For all of the classification algorithms except for SVM, the features for a clue in-
stance are represented as they are presented in Section 7. For SVM, the representations
for numeric and discrete-valued features are changed. Numeric features, such as the
count of strongsubj clue instances in a sentence, are scaled to range between 0 and 1.
Discrete-valued features, such as the reliability class feature, are converted into multiple
binary features. For example, the reliability class feature is represented by two binary
features: one for whether the clue instance is a strongsubj clue and one for whether the
clue instance is a weaksubj clue.
To investigate the importance of recognizing neutral instances, we perform two sets
of polarity classification (step two) experiments. First, we experiment with classifying
the polarity of all gold-standard polar instances?the clue instances identified as polar
in context by the manual polarity annotations. Second, we experiment with using the
polar instances identified automatically by the neutral?polar classifiers. Because the
second set of experiments includes the neutral instances misclassified in step one, we
can compare results for the two sets of experiments to see how the noise of neutral
instances affects the performance of the polarity features.
415
Computational Linguistics Volume 35, Number 3
All experiments are performed using 10-fold cross validation over a test set of
10,287 sentences from 494MPQA corpus documents. Wemeasure performance in terms
of accuracy, recall, precision, and F-measure. Accuracy is simply the total number of
instances correctly classified. Recall, precision, and F-measure for a given class C are
defined as follows. Recall is the percentage of all instances of class C correctly identified.
Rec(C) =
| instances of C correctly identified |
| all instances of C |
Precision is the percentage of instances classified as class C that are class C in truth.
Prec(C) =
| instances of C correctly identified |
| all instances identified as C |
F-measure is the harmonic mean of recall and precision.
F(C) =
2 ?Rec(C)? Prec(C)
Rec(C)+ Prec(C)
All results reported are averages over the 10 folds.
8.1 Neutral?Polar Classification
In our two-step process for recognizing contextual polarity, the first step is neutral?polar
classification, determining whether each instance of a clue from the lexicon is neutral or
polar in context. In our test set, there are 26,729 instances of clues from the lexicon. The
features we use for this step were listed above in Table 7 and described in Section 7.1.
In this section, we perform two sets of experiments. In the first, we compare
the results of neutral?polar classification using all the neutral?polar features against
two baselines. The first baseline uses just the word token feature. The second baseline
(word+priorpol) uses the word token and prior polarity features. In the second set of
experiments, we explore the performance of different sets of features for neutral?polar
classification.
Research has shown that the performance of learning algorithms for NLP tasks can
vary widely depending on their parameter settings, and that the optimal parameter
settings can also vary depending on the set of features being evaluated (Daelemans
et al 2003a; Hoste 2005). Although the goal of this work is not to identify the optimal
configuration for each algorithm and each set of features, we still want to make a rea-
sonable attempt to find a good configuration for each algorithm. To do this, we perform
10-fold cross validation of the more challenging baseline classifier (word+priorpol)
on the development data, varying select parameter settings. The results from those
experiments are then used to select the parameter settings for each algorithm. For
BoosTexter, we vary the number of rounds of boosting. For TiMBL, we vary the value
for k (the number of neighbors) and the distance metric (overlap or modified value
difference metric [MVDM]). For Ripper, we vary whether negative tests are disallowed
for nominal (-!n) and set (-!s) valued attributes and howmuch to simplify the hypothesis
(-S). For SVM, we experiment with linear, polynomial, and radial basis function kernels.
Table 10 gives the settings selected for the neutral?polar classification experiments for
the different learning algorithms.
416
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 10
Algorithm settings for neutral?polar classification.
Algorithm Settings
BoosTexter 2,000 rounds of boosting
TiMBL k=25, MVDM distance metric
Ripper -!n, -S 0.5
SVM linear kernel
8.1.1 Classification Results. The results for the first set of experiments are given in
Table 11. For each algorithm, we give the results for the two baseline classifiers, followed
by the results for the classifier trained using all the neutral?polar features. The results
shown in bold are significantly better than both baselines (two-sided t-test, p? 0.05) for
the given algorithm.
Working together, how well do the neutral?polar features perform? For BoosTexter,
TiMBL, and Ripper, the classifiers trained using all the features improve significantly
over the two baselines in terms of accuracy, polar recall, polar F-measure, and neutral
precision. Neutral F-measure is also higher, but not significantly so. These consistent
results across three of the four algorithms show that the neutral?polar features are
helpful for determining when a sentiment clue is actually being used to express a
sentiment.
Interestingly, Ripper is the only algorithm for which the word-token baseline per-
formed better than the word+priorpol baseline. Nevertheless, the prior polarity feature
is an important component in the performance of the Ripper classifier using all the
features. Excluding prior polarity from this classifier results in a significant decrease in
Table 11
Results for neutral?polar classification (step one).
Polar Neutral
Acc Rec Prec F Rec Prec F
BoosTexter
word token baseline 74.0 41.9 77.0 54.3 92.7 73.3 81.8
word+priorpol baseline 75.0 55.6 70.2 62.1 86.2 76.9 81.3
neutral?polar features 76.5 58.3 72.4 64.6 87.1 78.2 82.4
TiMBL
word token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8
word+priorpol baseline 74.6 48.2 73.7 58.3 90.0 74.9 81.7
neutral?polar features 76.5 59.5 71.7 65.0 86.3 78.5 82.3
Ripper
word token baseline 66.3 11.2 80.6 19.6 98.4 65.6 78.7
word+priorpol baseline 65.5 07.7 84.5 14.1 99.1 64.8 78.4
neutral?polar features 71.4 49.4 64.6 56.0 84.2 74.1 78.8
SVM
word token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8
word+priorpol baseline 75.6 54.5 72.5 62.2 88.0 76.8 82.0
neutral?polar features 75.3 52.6 72.7 61.0 88.5 76.2 81.9
417
Computational Linguistics Volume 35, Number 3
performance for every metric. Decreases range from 2.5% for neutral recall to 9.5% for
polar recall.
The best SVM classifier is the word+priorpol baseline. In terms of accuracy, this
classifier does not perform much worse than the BoosTexter and TiMBL classifiers that
use all the neutral?polar features: The SVM word+priorpol baseline classifier has an
accuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of
76.5%. However, the BoosTexter and TiMBL classifiers using all the features perform
notably better in terms of polar recall and F-measure. The BoosTexter and TiMBL
classifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline. Polar
F-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher. These increases are
significant for p ? 0.01.
8.1.2 Feature Set Evaluation. To evaluate the contribution of the various features for
neutral?polar classification, we perform a series of experiments in which different
sets of neutral?polar features are added to the word+priorpol baseline and new clas-
sifiers are trained. We then compare the performance of these new classifiers to the
word+priorpol baseline, with the exception of the Ripper classifiers, which we compare
to the higher word baseline. Table 12 lists the sets of features tested in these experiments.
The feature sets generally correspond to how the neutral?polar features are presented
in Table 7, although some of the groups are broken down into more fine-grained sets
that we believe capture meaningful distinctions.
Table 13 gives the results for these experiments. Increases and decreases for a
given metric as compared to the word+priorpol baseline (word baseline for Ripper)
are indicated by + or ?, respectively. Where changes are significant at the p ? 0.1 level,
++ or ? ? are used, and where changes are significant at the p ? 0.05 level, +++ or ? ? ?
are used. An ?nc? indicates no change (a change of less than ? 0.05) compared to the
baseline.
What does Table 13 reveal about the performance of various feature sets for neutral?
polar classification?Most noticeable is that no individual feature sets stand out as strong
performers. The only significant improvements in accuracy come from the PARTS-
OF-SPEECH and RELIABILITY-CLASS feature sets for Ripper. These improvements are
perhaps not surprising given that the Ripper baseline was much lower to begin with.
Very few feature sets show any improvement for SVM. Again, this is not unexpected
given that all the features together performed worse than the word+priorpol baseline
Table 12
Neutral?polar feature sets for evaluation.
Experiment Features
PARTS-OF-SPEECH parts of speech for clue instance, previous word, and next word
RELIABILITY-CLASS reliability class of clue instance
PRECEDED-POS preceded by adjective, preceded by adverb
INTENSIFY preceded by intensifier, self intensifier
RELCLASS-MOD modifies strongsubj/weaksubj, modified by strongsubj/weaksubj
POLARITY-MOD polarity-modification features
STRUCTURE structure features
CURSENT-COUNTS strongsubj/weaksubj clue instances in sentence
PNSENT-COUNTS strongsubj/weaksubj clue instances in previous/next sentence
CURSENT-OTHER adjectives/adverbs/cardinal number/pronoun/modal in sentence
TOPIC document topic
418
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 13
Results for neutral?polar feature ablation experiments.
Polar Neut Polar Neut
BoosTexter Acc F F Ripper Acc F F
PARTS-OF-SPEECH + ? + PARTS-OF-SPEECH +++ +++ ? ? ?
RELIABILITY-CLASS + ? + RELIABILITY-CLASS +++ +++ +
PRECEDED-POS nc ? nc PRECEDED-POS ? ? ?
INTENSIFY - nc - INTENSIFY ? ? ? ? ?
RELCLASS-MOD + ++ + RELCLASS-MOD + +++ +
POLARITY-MOD nc ? + POLARITY-MOD ? +++ ?
STRUCTURE ? ? ? ? + STRUCTURE ? + ?
CURSENT-COUNTS + ? ? ? + CURSENT-COUNTS ? ? +++ ? ? ?
PNSENT-COUNTS + ? ? ? + PNSENT-COUNTS ? ? ? +++ ? ? ?
CURSENT-OTHER nc ? + CURSENT-OTHER ? ? ? +++ ? ? ?
TOPIC + + + TOPIC ? +++ ? ? ?
Polar Neut Polar Neut
TiMBL Acc F F SVM Acc F F
PARTS-OF-SPEECH + +++ + PARTS-OF-SPEECH ? ? ? ? ? ?
RELIABILITY-CLASS + + nc RELIABILITY-CLASS + ? +
PRECEDED-POS nc + nc PRECEDED-POS nc nc nc
INTENSIFY nc nc nc INTENSIFY nc nc nc
RELCLASS-MOD + + + RELCLASS-MOD nc + nc
POLARITY-MOD + + + POLARITY-MOD ? ? ? ? ? ? ?
STRUCTURE nc + ? STRUCTURE ? + ?
CURSENT-COUNTS ? + ? CURSENT-COUNTS ? ? ?
PNSENT-COUNTS + +++ ? PNSENT-COUNTS ? ? ?
CURSENT-OTHER + +++ ? CURSENT-OTHER ? ? ?
TOPIC ? + ? TOPIC ? ? ?
Increases and decreases for a given metric as compared to the word+priorpol baseline
(word baseline for Ripper) are indicated by + or ?, respectively; ++ or ? ? indicates the
change is significant at the p < 0.1 level; +++ or ? ? ? indicates significance at the
p < 0.05 level; nc indicates no change.
for SVM. The performance of the feature sets for BoosTexter and TiMBL are perhaps
the most revealing. In the previous experiments using all the features together, these
algorithms produced classifiers with the same high performance. In these experiments,
six different feature sets for each algorithm show improvements in accuracy over the
baseline, yet none of those improvements are significant. This suggests that achieving
the highest performance for neutral?polar classification requires a wide variety of fea-
tures working together in combination.
We further test this result by evaluating the effect of removing the features that
produced either no change or a drop in accuracy from the respective all-feature classi-
fiers. For example, we train a TiMBL neutral?polar classifier using all the features except
for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC
feature sets, and then compare the performance of this new classifier to the TiMBL, all-
feature classifier. Although removing the non-performing features has little effect for
BoosTexter, performance does drop for both TiMBL and Ripper. The primary source of
this performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper.
419
Computational Linguistics Volume 35, Number 3
Although no feature sets stand out in Table 13 as far as giving an overall high
performance, there are some features that consistently improve performance across
the different algorithms. The reliability class of the clue instance (RELIABILITY-CLASS)
improves accuracy over the baseline for all four algorithms. It is the only feature that
does so. The RELCLASS-MOD features give improvements for all metrics for BoosTexter,
Ripper, and TiMBL, as well as improving polar F-measure for SVM. The PARTS-OF-
SPEECH features are also fairly consistent, improving performance for all the algorithms
except for SVM. There are also a couple of feature sets that consistently do not improve
performance for any of the algorithms: the INTENSIFY and PRECEDED-POS features.
8.2 Polarity Classification
For the second step of recognizing contextual polarity, we classify the polarity of all clue
instances identified as polar in step one. The features for polarity classification were
listed in Table 9 and described in Section 7.2.
We investigate the performance of the polarity features under two conditions:
(1) perfect neutral?polar recognition and (2) automatic neutral?polar recognition. For
condition 1, we identify the polar instances according to the gold-standard, manual
contextual-polarity annotations. In the test data, 9,835 instances of the clues from the
lexicon are polar in context according to the manual annotations. Experiments under
condition 1 classify these instances as having positive, negative, or both (positive and
negative) polarity. For condition 2, we take the best performing neutral?polar classifier
for each algorithm and use the output from those algorithms to identify the polar
instances. Because polar instances now are being identified automatically, there will be
noise in the form of misclassified neutral instances. Therefore, for experiments under
condition 2 we include the neutral class and perform four-way classification instead of
three-way. Condition 1 allows us to investigate the performance of the different polarity
features without the noise of misclassified neutral instances. Also, because the set of
polar instances being classified is the same for all the algorithms, condition 1 allows
us to compare the performance of the polarity features across the different algorithms.
However, condition 2 is themore natural one. It allows us to see how the noise of neutral
instances affects the performance of the polarity features.
The following sections describe three sets of experiments. First, we investigate the
performance of the polarity features used together for polarity classification under
condition 1. As before, the word and word+priorpol classifiers provide our baselines. In
the second set of experiments, we explore the performance of different sets of features
for polarity classification, again assuming perfect recognition of the polar instances.
Finally, we experiment with polarity classification using all the polarity features under
condition 2, automatic recognition of the polar instances.
As before, we use the development data to select the parameter settings for each al-
gorithm. The settings for polarity classification are given in Table 14. They were selected
based on the performance of the word+priorpol baseline classifier under condition 2.
8.2.1 Classification Results: Condition 1. The results for polarity classification using all the
polarity features, assuming perfect neutral?polar recognition for step one, are given in
Table 15. For each algorithm, we give the results for the two baseline classifiers, followed
by the results for the classifier trained using all the polarity features. For the metrics
where the polarity features perform statistically better than both baselines (two-sided
t-test, p ? 0.05), the results are given in bold.
420
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Table 14
Algorithm settings for polarity classification.
Algorithm Settings
BoosTexter 2,000 rounds of boosting
TiMBL k=1, MVDM distance metric
Ripper -!s, -S 0.5
SVM linear kernel
Table 15
Results for polarity classification (step two) using gold-standard polar instances.
Positive Negative Both
Acc Rec Prec F Rec Prec F Rec Prec F
BoosTexter
word token baseline 78.7 57.7 72.8 64.4 91.5 80.8 85.8 12.9 53.6 20.8
word+priorpol baseline 79.7 70.5 68.8 69.6 87.2 85.1 86.1 13.7 53.7 21.8
polarity features 83.2 76.7 74.3 75.5 89.7 87.7 88.7 11.8 54.2 19.4
TiMBL
word token baseline 78.5 63.3 69.2 66.1 88.6 82.5 85.4 14.1 51.0 22.1
word+priorpol baseline 79.4 69.7 68.4 69.1 87.0 84.8 85.9 14.6 53.5 22.9
polarity features 82.2 75.4 73.3 74.3 88.5 87.6 88.0 18.3 34.6 23.9
Ripper
word token baseline 70.0 14.5 74.5 24.3 98.3 69.7 81.6 09.1 74.4 16.2
word+priorpol baseline 78.9 75.5 65.2 70.0 83.8 86.4 85.1 09.8 75.4 17.4
polarity features 83.2 77.8 73.5 75.6 89.2 87.8 88.5 09.8 74.9 17.4
SVM
word token baseline 69.9 62.4 69.6 65.8 76.0 84.1 79.9 14.1 31.2 19.4
word+priorpol baseline 78.2 76.7 63.7 69.6 82.2 86.7 84.4 09.8 75.4 17.4
polarity features 81.6 74.9 71.1 72.9 88.1 86.6 87.3 09.5 77.6 16.9
Howwell do the polarity features perform working all together? For all algorithms,
the polarity classifier using all the features significantly outperforms both baselines
in terms of accuracy, positive F-measure, and negative F-measure. These consistent
improvements in performance across all four algorithms show that these features are
quite useful for polarity classification.
One interesting thing that Table 15 reveals is that negative polarity words are much
more straightforward to recognize than positive polarity words, at least in this corpus.
For the negative class, precisions and recalls for the word+priorpol baseline range from
82.2 to 87.2. For the positive class, precisions and recalls for the word+priorpol baseline
range from 63.7 to 76.7. However, it is with the positive class that polarity features seem
to help themost. With the addition of the polarity features, positive F-measure improves
by 5 points on average; improvements in negative F-measures average only 2.75 points.
8.2.2 Feature Set Evaluation. To evaluate the performance of the various features for
polarity classification, we again perform a series of ablation experiments. As before, we
start with the word+priorpol baseline classifier, add different sets of polarity features,
train new classifiers, and compare the results of the new classifiers to the baseline.
421
Computational Linguistics Volume 35, Number 3
Table 16
Polarity feature sets for evaluation.
Experiment Features
NEGATION negated, negated subject
POLARITY-MOD modifies polarity, modified by polarity, conjunction polarity
SHIFTERS general, negative, positive polarity shifters
Table 17
Results for polarity feature ablation experiments.
Positive Negative
Acc Rec Prec F Rec Prec F
BoosTexter
NEGATION +++ ++ +++ +++ +++ + +++
POLARITY-MOD ++ +++ + +++ + ++ +
SHIFTERS + + + + + + +
TiMBL
NEGATION +++ +++ +++ +++ +++ +++ +++
POLARITY-MOD + + + + ? + +
SHIFTERS + + + + ? + +
Ripper
NEGATION +++ ? ? +++ +++ +++ ? +++
POLARITY-MOD + +++ ++ +++ + + +
SHIFTERS + ? + + + ? +
SVM
NEGATION +++ ? +++ +++ +++ + +++
POLARITY-MOD + ? +++ + + ? +
SHIFTERS + ? + + + + +
Increases and decreases for a given metric as compared to the
word+priorpol baseline are indicated by + or ?, respectively;
++ or ? ? indicates the change is significant at the p < 0.1 level;
+++ or ? ? ? indicates significance at the p < 0.05 level.
Table 16 lists the sets of features tested in each experiment, and Table 17 shows the
results of the experiments. Results are reported as they were previously in Section 8.1.2,
with increases and decreases compared to the baseline for a given metric indicated by +
or ?, respectively.
Looking at Table 17, we see that all three sets of polarity features help to increase
performance as measured by accuracy and positive and negative F-measures. This is
true for all the classification algorithms. As we might expect, including the negation
features has the most marked effect on the performance of polarity classification, with
statistically significant improvements for most metrics across all the algorithms.9 The
9 Although the negation features give the best performance improvements of the three feature sets, these
classifiers still do not perform as well as the respective all-feature polarity classifiers for each algorithm.
422
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
polarity-modification features also seem to be important for polarity classification,
in particular for disambiguating the positive instances. For all the algorithms except
TiMBL, including the polarity-modification features results in significant improvements
for at least one of the positive metrics. The polarity shifters also help classification, but
they seem to be the weakest of the features: Including them does not result in significant
improvements for any algorithm.
Another question that is interesting to consider is how much the word token feature
contributes to polarity classification, given all the other polarity features. Is it enough
to know the prior polarity of a word, whether it is being negated, and how it is related
to other polarity influencers? To answer this question, we train classifiers using all the
polarity features except for word token. Table 18 gives the results for these classifiers;
for comparison, the results for the all-feature polarity classifiers are also given. Inter-
estingly, excluding the word token feature produces only small changes in the overall
results. The results for BoosTexter and Ripper are slightly lower, and the results for
SVM are practically unchanged. TiMBL actually shows a slight improvement, with the
exception of the both class. This provides further evidence of the strength of the polarity
features. Also, a classifier not tied to actual word tokens may potentially be a more
domain-independent classifier.
8.2.3 Classification Results: Condition 2. The experiments in Section 8.2.1 show that the
polarity features perform well under the ideal condition of perfect recognition of polar
instances. The next question to consider is how well the polarity features perform
under the more natural but less-than-perfect condition of automatic recognition of
polar instances. To investigate this, the polarity classifiers (including the baselines) for
each algorithm in these experiments start with the polar instances identified by the
best performing neutral?polar classifier for that algorithm (from Section 8.1.1). The
results for these experiments are given in Table 19. As before, statistically significant
improvements over both baselines are given in bold.
How well do the polarity features perform in the presence of noise from misclas-
sified neutral instances? Our first observation comes from comparing Table 15 with
Table 19: Polarity classification results are much lower for all classifiers with the noise
of neutral instances. Yet in spite of this, the polarity features still produce classifiers that
Table 18
Results for polarity classification without and with the word token feature.
Acc Pos F Neg F Both F
BoosTexter
excluding word token 82.5 74.9 88.0 17.4
all polarity features 83.2 75.5 88.7 19.4
TiMBL
excluding word token 83.2 75.9 88.4 17.3
all polarity features 82.2 74.3 88.0 23.9
Ripper
excluding word token 82.9 75.4 88.3 17.4
all polarity features 83.2 75.6 88.5 17.4
SVM
excluding word token 81.5 72.9 87.3 16.8
all polarity features 81.6 72.9 87.3 16.9
423
C
o
m
p
u
ta
tio
n
a
l
L
in
g
u
istics
V
o
lu
m
e
3
5
,
N
u
m
b
e
r
3
Table 19
Results for polarity classification (step two) using automatically identified polar instances.
Positive Negative Both Neutral
Acc R P F R P F R P F R P F
BoosTexter
word token 61.5 62.3 62.7 62.5 86.4 64.6 74.0 11.4 49.3 18.5 20.8 44.5 28.3
word+priorpol 63.3 70.0 57.9 63.4 81.3 71.5 76.1 12.5 47.3 19.8 30.9 47.5 37.4
polarity feats 65.9 73.6 62.2 67.4 84.9 72.3 78.1 13.4 40.7 20.2 31.0 50.6 38.4
TiMBL
word token 60.1 68.3 58.9 63.2 81.8 65.0 72.5 11.2 39.6 17.4 21.6 43.1 28.8
word+priorpol 61.0 73.2 53.4 61.8 80.6 69.8 74.8 12.7 41.7 19.5 23.0 44.2 30.3
polarity feats 64.4 75.3 58.6 65.9 81.1 73.0 76.9 16.9 32.7 22.3 32.1 50.0 39.1
Ripper
word token 54.4 22.2 69.4 33.6 95.1 50.7 66.1 00.0 00.0 00.0 21.7 76.5 33.8
word+priorpol 51.4 24.0 71.7 35.9 97.7 48.9 65.1 00.0 00.0 00.0 09.2 75.8 16.3
polarity feats 54.8 38.0 67.2 48.5 95.5 52.7 67.9 00.0 00.0 00.0 14.5 66.8 23.8
SVM
word token 64.5 70.0 60.9 65.1 70.9 74.9 72.9 16.6 41.5 23.7 53.3 51.0 52.1
word+priorpol 62.8 89.0 51.2 65.0 88.4 69.2 77.6 11.1 48.5 18.0 02.4 58.3 04.5
polarity feats 64.1 90.8 53.0 66.9 90.4 70.1 79.0 12.7 52.3 20.4 02.2 61.4 04.3
4
2
4
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
outperform the baselines. For three of the four algorithms, the classifier using all the
polarity features has the highest accuracy. For BoosTexter and TiMBL, the improvements
in accuracy over both baselines are significant. Also for all algorithms, using the polarity
features gives the highest positive and negative F-measures.
Because the set of polarity instances being classified by each algorithm is different,
we cannot directly compare the results from one algorithm to the next.
8.3 Two-step versus One-step Recognition of Contextual Polarity
Although the two-step approach to recognizing contextual polarity allows us to focus
our investigation on the performance of features for both neutral?polar classification
and polarity classification, the question remains: How does the two-step approach
compare to recognizing contextual polarity in a single classification step? The results
shown in Table 20 help to answer this question. The first row in Table 20 for each
algorithm shows the combined result for the two stages of classification. For BoosTexter,
TiMBL, and Ripper, this is the combination of results from using all the neutral?polar
features for step one, together with the results from using all of the polarity features for
step two.10 For SVM, this is the combination of results from the word+priorpol baseline
from step one, together with results for using all the polarity features for step two.
Recall that the word+priorpol classifier was the best neutral?polar classifier for SVM
(see Table 11). The second rows for BoosTexter, TiMBL, and Ripper show the results of
a single classifier trained to recognize contextual polarity using all the neutral?polar
and polarity features together. For SVM, the second row shows the results of classifying
the contextual polarity using just the word token feature. This classifier outperformed
all others for SVM. In the table, the best result for each metric for each algorithm is
highlighted in bold.
When comparing the two-step and one-step approaches, contrary to our expecta-
tions, we see that the one-step approach performs about as well or better than the
two-step approach for recognizing contextual polarity. For SVM, the improvement in
accuracy achieved by the two-step approach is significant, but this is not true for
the other algorithms. One fairly consistent difference between the two approaches is
that the two-step approach scores slightly higher for neutral F-measure, and the one-
step approach achieves higher F-measures for the polarity classes. The difference in
negative F-measure is significant for BoosTexter, TiMBL, and Ripper. The exception to
this is SVM. For SVM, the two-step approach achieves significantly higher positive and
negative F-measures.
One last question we consider is how much the neutral?polar features contribute
to the performance of the one-step classifiers. The third line in Table 20 for BoosTexter,
TiMBL, and Ripper gives the results for a one-step classifier trainedwithout the neutral?
polar features. Although the differences are not always large, excluding the neutral?
polar features consistently degrades performance in terms of accuracy and positive,
negative, and neutral F-measures. The drop in negative F-measure is significant for all
three algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL,
and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at the
p ? 0.1 level).
10 To clarify, Section 8.2.3 only reported results for instances identified as polar in step one. Here, we report
results for all clue instances, including the instances classified as neutral in step one.
425
Computational Linguistics Volume 35, Number 3
Table 20
Results for contextual polarity classification for both two-step and one-step approaches.
Acc Pos F Neg F Both F Neutral F
BoosTexter
two-step 74.5 47.1 57.5 12.9 83.4
one-step all feats 74.3 49.1 59.8 14.1 82.9
one-step ? neut-pol feats 73.3 48.4 58.7 16.3 81.9
TiMBL
two-step 74.1 47.6 56.4 13.8 83.2
one-step all feats 73.9 49.6 59.3 15.2 82.6
one-step ? neut-pol feats 72.5 49.5 56.9 21.6 81.4
Ripper
two-step 68.9 26.6 49.0 00.0 80.1
one-step all feats 69.5 30.2 52.8 14.0 79.4
one-step ? neut-pol feats 67.0 28.9 33.0 11.4 78.6
SVM
two-step 73.1 46.6 58.0 13.0 82.1
one-step 71.6 43.4 51.7 17.0 81.6
The modest drop in performance that we see when excluding the neutral?polar
features in the one-step approach seems to suggest that discriminating between neutral
and polar instances is helpful but not necessarily crucial. However, consider Figure 3.
In this figure, we show the F-measures for the positive, negative, and both classes for
the BoosTexter polarity classifier that uses the gold-standard neutral/polar instances
(from Table 15) and for the BoosTexter one-step polarity classifier that uses all features
(from Table 20). Plotting the same sets of results for the other three algorithms produces
very similar figures. The difference when the classifiers have to contend with the noise
from neutral instances is dramatic. Although Table 20 shows that there is room for
improvement across all the contextual polarity classes, Figure 3 shows us that perhaps
the best way to achieve these improvements is to improve the ability to discriminate the
neutral class from the others.
Figure 3
Chart showing the positive, negative, and both class F-measures for the BoosTexter classifier that
uses the gold-standard neutral/polar classes and the BoosTexter one-step classifier that uses all
the features.
426
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
9. Related Work
9.1 Phrase-Level Sentiment Analysis
Other researchers who have worked on classifying the contextual polarity of sentiment
expressions are Yi et al (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, and
Okumura (2006). Yi et al use a lexicon and manually developed patterns to classify
contextual polarity. Their patterns are high-quality, yielding quite high precision over
the set of expressions that they evaluate. Popescu and Etzioni use an unsupervised clas-
sification technique called relaxation labeling (Hummel and Zucker 1983) to recognize
the contextual polarity of words that are at the heads of select opinion phrases. They
take an iterative approach, using relaxation labeling first to determine the contextual
polarities of the words, then again to label the polarities of the words with respect to
their targets. A third stage of relaxation labeling then is used to assign final polarities to
the words, taking into consideration the presence of other polarity terms and negation.
Aswe do, Popescu and Etzioni use features that represent conjunctions and dependency
relations between polarity words. Suzuki et al use a bootstrapping approach to classify
the polarity of tuples of adjectives and their target nouns in Japanese blogs. Included
in the features that they use are the words that modify the adjectives and the word that
the adjective modifies. They consider the effect of a single negation term, the Japanese
equivalent of not.
Our work in recognizing contextual polarity differs from this research on
expression-level sentiment analysis in several ways. First, the set of expressions they
evaluate is limited either to those that target specific items of interest, such as products
and product features, or to tuples of adjectives and nouns. In contrast, we seek to classify
the contextual polarity of all instances of words from a large lexicon of subjectivity clues
that appear in the corpus. Included in the lexicon are not only adjectives, but nouns,
verbs, adverbs, and even modals.
Our work also differs from other research in the variety of features that we use. As
other researchers do, we consider negation and the words that directly modify or are
modified by the expression being classified. However, with negation, we have features
for both local and longer-distance types of negation, and we take care to count negation
terms only when they are actually being used to negate, excluding, for example, nega-
tion terms when they are used in phrases that intensify (e.g., not only). We also include
contextual features to capture the presence of other clue instances in the surrounding
sentences, and features that represent the reliability of clues from the lexicon.
Finally, a unique aspect of the work presented in this article is the evaluation of
different features for recognizing contextual polarity. We first presented the features
explored in this research in Wilson, Wiebe, and Hoffman (2005), but this work signif-
icantly extends that initial evaluation. We explore the performance of features across
different learning algorithms, and we evaluate not only features for discriminating
between positive and negative polarity, but features for determining when a word is
or is not expressing a sentiment in the first place (neutral in context). This is also the
first work to evaluate the effect of neutral instances on the performance of features for
discriminating between positive and negative contextual polarity.
9.2 Other Research in Sentiment Analysis
Recognizing contextual polarity is just one facet of the research in automatic senti-
ment analysis. Research ranges from work on learning the prior polarity (semantic
427
Computational Linguistics Volume 35, Number 3
orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kamps
and Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuli
and Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005;
Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa
2006) to characterizing the sentiment of documents, such as recognizing inflammatory
messages (Spertus 1997), tracking sentiment over time in online discussions (Tong
2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001;
Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and movie
reviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, and
Pennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai,
Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen
2006; Koppel and Schler 2006).
Identifying prior polarity is a different task than recognizing contextual polarity,
although the two tasks are complementary. The goal of identifying prior polarity is
to automatically acquire the polarity of words or phrases for listing in a lexicon. Our
work on recognizing contextual polarity begins with a lexicon of words with established
prior polarities and then disambiguates in the corpus the polarity being expressed
by the phrases in which instances of those words appear. To make the relationship
between that task and ours clearer, someword lists that are used to evaluatemethods for
recognizing prior polarity (positive and negative word lists from the General Inquirer
[Stone et al 1966] and lists of positive and negative adjectives created for evaluation by
Hatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon used
in our experiments.
For the most part, the features explored in this work differ from the ones used to
identify prior polarity with just a few exceptions. Using a feature to capture conjunc-
tions between clue instances was motivated in part by the work of Hatzivassiloglou and
McKeown (1997). They use constraints on the co-occurrence in conjunctions of words
with similar or opposite polarity to predict the prior polarity of adjectives. Esuli and
Sebastiani (2005) consider negation in some of their experiments involving WordNet
glosses. Takamura et al (2005) use negation words and phrases, including phrases such
as lack of that are members in our lists of polarity shifters, and conjunctive expressions
that they collect from corpora.
Esuli and Sebastiani (2006a) is the only work in prior-polarity identification to
include a neutral (objective) category and to consider a three-way classification between
positive, negative, and neutral words. Although identifying prior polarity is a different
task, they report a finding similar to ours, namely, that accuracy is lower when neutral
words are included.
Some research in sentiment analysis classifies the sentiments of sentences. Morinaga
et al (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004),
and Grefenstette et al (2004)11 all begin by first creating prior-polarity lexicons. Yu and
Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic
orientations of instances of lexicon words in the sentence. Thus, they do not identify the
contextual polarity of individual phrases containing clue instances, which is the focus
of this work. Morinaga et al only consider the positive or negative clue instance in
each sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, and
Grefenstette et al multiply or count the prior polarities of clue instances in the sentence.
11 In Grefenstette et al (2004), the units that are classified are fixed windows around named entities rather
than sentences.
428
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
These researchers also consider local negation to reverse polarity, with Morinaga et al
also taking into account the negating effect of words like insufficient. However, they
do not use the other types of features that we consider in our experiments. Kaji and
Kitsuregawa (2006) take a different approach to recognizing positive and negative
sentences. They bootstrap from information easily obtained in ?Pro? and ?Con?
HTML tables and lists, and from one high-precision linguistic pattern, to automatically
construct a large corpus of positive and negative sentences. They then use this corpus to
train a naive Bayes sentence classifier. In contrast to our work, sentiment classification
in all of this research is restricted to identifying only positive and negative sentences
(excluding our both and neutral categories). In addition, only one sentiment is assigned
per sentence; our system assigns contextual polarity to individual expressions, which
would allow for a sentence to be assigned to multiple sentiment categories. As we saw
when exploring the contextual polarity annotations, it is not uncommon for sentences
to contain more than one sentiment expression.
Classifying the sentiment of documents is a very different task than recognizing
the contextual polarity of words and phrases. However, some researchers have re-
ported findings about document-level classification that are similar to our findings
about phrase-level classification. Bai et al (2005) argue that dependencies among key
sentiment terms are important for classifying document sentiment. Similarly, we show
that features for capturing when clue instances modify each other are important for
phrase-level classification, in particular, for identifying positive expressions. Gamon
(2004) achieves his best results for document classification using a wide variety of
features, including rich linguistic features, such as features that capture constituent
structure, features that combine part-of-speech and semantic relations (e.g., sentence
subject or negated context), and features that capture tense information. We also achieve
our best results for phrase-level classification using a wide variety of features, many
of which are linguistically rich. Kennedy and Inkpen (2006) report consistently higher
results for document sentiment classification when select polarity influencers, including
negators and intensifiers, are included.12 Koppel and Schler (2006) demonstrate the
importance of neutral examples for document-level classification. In this work, we show
that being able to correctly identify neutral instances is also very important for phrase-
level sentiment analysis.
10. Conclusions and Future Work
Being able to determine automatically the contextual polarity of words and phrases is
an important problem in sentiment analysis. In the research presented in this article, we
tackle this problem and show that it is much more complex than simply determining
whether a word or phrase is positive or negative. In our analysis of a corpus with
annotations of subjective expressions and their contextual polarity, we find that positive
and negative words from a lexicon are used in neutral contexts much more often than
they are used in expressions of the opposite polarity. The importance of identifying
12 Das and Chen (2001), Pang, Lee, and Vaithyanathan (2002), and Dave, Lawrence, and Pennock (2003) also
represent negation. In their experiments, words which follow a negation term are tagged with a negation
marker and then treated as new words. Pang, Lee and Vaithyanathan report that representing negation in
this way slightly helps their results, whereas Dave, Lawrence, and Pennock report a slightly detrimental
effect. Whitelaw, Garg, and Argamon (2005) also represent negation terms and intensifiers. However, in
their experiments, the effect of negation is not separately evaluated, and intensifiers are not found to be
beneficial.
429
Computational Linguistics Volume 35, Number 3
when contextual polarity is neutral is further revealed in our classification experiments:
When neutral instances are excluded, the performance of features for distinguishing
between positive and negative polarity greatly improves.
A focus of this research is on understanding which features are important for
recognizing contextual polarity. We experiment with a wide variety of linguistically
motivated features, and we evaluate the performance of these features using several
different machine learning algorithms. Features for distinguishing between neutral and
polar instances are evaluated, as well as features for distinguishing between positive
and negative contextual polarity. For classifying neutral and polar instances, we find
that, although some features produce significant improvements over the baseline in
terms of polar or neutral recall or precision, it is the combination of features together
that is needed to achieve significant improvements in accuracy. For classifying positive
and negative contextual polarity, features for capturing negation prove to be the most
important. However, we find that features that also perform well are those that cap-
ture when a word is (or is not) modifying or being modified by other polarity terms.
This suggests that identifying features that represent more complex interdependencies
between polarity clues will be an important avenue for future research.
Another direction for future work will be to expand our lexicon using existing
techniques for acquiring the prior polarity of words and phrases. It follows that a larger
lexicon will have a greater coverage of sentiment expressions. However, expanding the
lexicon with automatically acquired prior-polarity tags may result in an even greater
proportion of neutral instances to contend with. Given the degradation in performance
created by the neutral instances, whether expanding the lexicon automatically will
result in improved performance for recognizing contextual polarity is an empirical
question.
Finally, the overall goal of our research is to use phrase-level sentiment analysis in
higher-level NLP tasks, such as opinion question answering and summarization.
Acknowledgments
We would like to thank the anonymous
reviewers for their valuable comments and
suggestions. This work was supported in
part by an Andrew Mellow Predoctoral
Fellowship, by the NSF under grant
IIS-0208798, by the Advanced Research and
Development Activity (ARDA), and by the
European IST Programme through the
AMIDA Integrated Project FP6-0033812.
References
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction from
WordNet glosses. In Proceedings of the 11th
Meeting of the European Chapter of the
Association for Computational Linguistics
(EACL-2006), pages 209?216, Trento.
Bai, Xue, Rema Padman, and Edoardo
Airoldi. 2005. On learning parsimonious
models for extracting consumer opinions.
In Proceedings of the 38th Annual Hawaii
International Conference on System
Sciences (HICSS?05) - Track 3, page 75.2,
Waikoloa, HI.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
Beineke, Philip, Trevor Hastie, and
Shivakumar Vaithyanathan. 2004. The
sentimental factor: Improving review
classification via human-provided
information. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-04),
pages 263?270, Barcelona.
Cohen, William W. 1996. Learning trees
and rules with set-valued features. In
Proceedings of the 13th National Conference
on Artificial Intelligence, pages 709?717,
Portland, OR.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
(ACL-97), pages 16?23, Madrid.
Daelemans, Walter, Ve?ronique Hoste,
Fien De Meulder, and Bart Naudts.
2003a. Combined optimization of feature
selection and algorithm parameter
430
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
interaction in machine learning of
language. In Proceedings of the 14th
European Conference on Machine Learning
(ECML-2003), pages 84?95,
Cavtat-Dubrovnik.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003b.
TiMBL: Tilburg Memory Based Learner,
version 5.0 Reference Guide. ILK Technical
Report 03-10, Induction of Linguistic
Knowledge Research Group, Tilburg
University. Available at http://ilk.uvt.
nl/downloads/pub/papers/ilk0310.pdf.
Das, Sanjiv Ranjan and Mike Y. Chen. 2001.
Yahoo! for Amazon: Sentiment parsing
from small talk on the Web. In Proceedings
of the August 2001 Meeting of the European
Finance Association (EFA), Barcelona,
Spain. Available at http://ssrn.com/
abstract=276189.
Dave, Kushal, Steve Lawrence, and David M.
Pennock. 2003. Mining the peanut
gallery: Opinion extraction and
semantic classification of product
reviews. In Proceedings of the 12th
International World Wide Web Conference
(WWW2003), Budapest. Available at
http://www2003.org.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation of
terms through gloss analysis. In
Proceedings of ACM SIGIR Conference on
Information and Knowledge Management
(CIKM-05), pages 617?624, Bremen.
Esuli, Andrea and Fabrizio Sebastiani. 2006a.
Determining term subjectivity and term
orientation for opinion mining. In
Proceedings the 11th Meeting of the European
Chapter of the Association for Computational
Linguistics (EACL-2006), pages 193?200,
Trento.
Esuli, Andrea and Fabrizio Sebastiani. 2006b.
SentiWordNet: A publicly available lexical
resource for opinion mining. In Proceedings
of LREC-06, the 5th Conference on Language
Resources and Evaluation, pages 417?422,
Genoa.
Gamon, Michael. 2004. Sentiment
classification on customer feedback data:
Noisy data, large feature vectors, and the
role of linguistic analysis. In Proceedings
of the 20th International Conference on
Computational Linguistics (COLING-2004),
pages 611?617, Geneva.
Grefenstette, Gregory, Yan Qu, James G.
Shanahan, and David A. Evans. 2004.
Coupling niche browsers and affect
analysis for an opinion mining application.
In Proceedings of the Conference Recherche
d?Information Assistee par Ordinateur
(RIAO-2004), pages 186?194, Avignon.
Hatzivassiloglou, Vasileios and Kathy
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL-97),
pages 174?181, Madrid.
Hoste, Ve?ronique. 2005. Optimization Issues in
Machine Learning of Coreference Resolution.
Ph.D. thesis, Language Technology Group,
University of Antwerp.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews. In
Proceedings of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining
2004 (KDD-2004), pages 168?177,
Seattle, WA.
Hummel, Robert A. and Steven W. Zucker.
1983. On the foundations of relaxation
labeling processes. IEEE Transactions on
Pattern Analysis and Machine Intelligence
(PAMI), 5(3):167?187.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In B. Scholkopf,
C. Burgess, and A. Smola, editors,
Advances in Kernel Methods ? Support Vector
Learning, pages 169?184. MIT Press,
Cambridge, MA.
Kaji, Nobuhiro and Masaru Kitsuregawa.
2006. Automatic construction of
polarity-tagged corpus from HTML
documents. In Proceedings of the
COLING/ACL 2006 Main Conference
Poster Sessions, pages 452?459, Sydney.
Kamps, Jaap and Maarten Marx. 2002.
Words with attitude. In Proceedings of the
1st International Conference on Global
WordNet, pages 332?341, Mysore.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2006), pages 355?363,
Sydney.
Kennedy, Alistair and Diana Inkpen. 2006.
Sentiment classification of movie reviews
using contextual valence shifters.
Computational Intelligence, 22(2):110?125.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-2004), pages 1267?1373, Geneva.
Koppel, Moshe and Jonathan Schler. 2006.
The importance of neutral examples for
learning sentiment. Computational
Intelligence, 22(2):100?109.
431
Computational Linguistics Volume 35, Number 3
Maybury, Mark T., editor. 2004. New
Directions in Question Answering. American
Association for Artificial Intelligence,
Menlo Park, CA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In Proceedings of the 8th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining (KDD-2002),
pages 341?349, Edmonton.
Mullen, Tony and Nigel Collier. 2004.
Sentiment analysis using support
vector machines with diverse
information sources. In Proceedings
of the Conference on Empirical Methods
in Natural Language Processing
(EMNLP-2004), pages 412?418,
Barcelona.
Nasukawa, Tetsuya and Jeonghee Yi.
2003. Sentiment analysis: Capturing
favorability using natural language
processing. In Proceedings of the 2nd
International Conference on Knowledge
Capture (K-CAP 2003), pages 70?77,
Sanibel Island, FL.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002),
pages 79?86, Philadelphia, PA.
Polanyi, Livia and Annie Zaenen. 2004.
Contextual valence shifters. InWorking
Notes of the AAAI Spring Symposium on
Exploring Attitude and Affect in Text:
Theories and Applications, pages 106?111,
The AAAI Press, Menlo Park, CA.
Popescu, Ana-Maria and Oren Etzioni.
2005. Extracting product features and
opinions from reviews. In Proceedings
of the Human Language Technologies
Conference/Conference on Empirical
Methods in Natural Language Processing
(HLT/EMNLP-2005), pages 339?346,
Vancouver.
Quirk, Randolph, Sidney Greenbaum,
Geoffry Leech, and Jan Svartvik. 1985.
A Comprehensive Grammar of the English
Language. Longman, New York.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for subjective
expressions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 105?112,
Sapporo.
Schapire, Robert E. and Yoram Singer. 2000.
BoosTexter: A boosting-based system for
text categorization.Machine Learning,
39(2/3):135?168.
Spertus, Ellen. 1997. Smokey: Automatic
recognition of hostile messages. In
Proceedings of the 8th Annual Conference
on Innovative Applications of Artificial
Intelligence (IAAI-97), pages 1058?1065,
Providence, RI.
Stone, Philip J., Dexter C. Dunphy,
Marshall S. Smith, and Daniel M. Ogilvie.
1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press,
Cambridge, MA.
Stoyanov, Veselin, Claire Cardie, and
Janyce Wiebe. 2005. Multi-perspective
question answering using the OpQA
corpus. In Proceedings of the Human
Language Technologies Conference/
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP-2005),
pages 923?930, Vancouver.
Suzuki, Yasuhiro, Hiroya Takamura, and
Manabu Okumura. 2006. Application of
semi-supervised learning to evaluative
expression classification. In Proceedings of
the 7th International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing-2006), pages 502?513,
Mexico City.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
emotional polarity of words using spin
model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL-05), pages 133?140,
Ann Arbor, MI.
Tong, Richard. 2001. An operational
system for detecting and tracking
opinions in online discussions. In
Working Notes of the SIGIR Workshop on
Operational Text Classification, pages 1?6,
New Orleans, LA.
Turney, Peter. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL-02), pages 417?424,
Philadelphia, PA.
Turney, Peter and Michael L. Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems
(TOIS), 21(4):315?346.
Whitelaw, Casey, Navendu Garg, and
Shlomo Argamon. 2005. Using appraisal
groups for sentiment analysis. In
Proceedings of the 14th ACM International
Conference on Information and Knowledge
432
Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity
Management (CIKM-2005), pages 625?631,
Bremen.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce, Rebecca Bruce, and
Thomas O?Hara. 1999. Development
and use of a gold standard data set
for subjectivity classifications. In
Proceedings of the 37th Annual Meeting
of the Association for Computational
Linguistics (ACL-99), pages 246?253,
College Park, MD.
Wiebe, Janyce and Rada Mihalcea.
2006. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1065?1072, Sydney.
Wiebe, Janyce and Ellen Riloff. 2005.
Creating subjective and objective sentence
classifiers from unannotated texts. In
Proceedings of the 6th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005),
pages 486?497, Mexico City.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation
(formerly Computers and the Humanities),
39(2/3):164?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffmann. 2005. Recognizing contextual
polarity in phrase-level sentiment
analysis. In Proceedings of the Human
Language Technologies Conference/
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP-2005),
pages 347?354, Vancouver.
Xia, Fei and Martha Palmer. 2001.
Converting dependency structures to
phrase structures. In Proceedings of the
Human Language Technology Conference
(HLT-2001), pages 1?5, San Diego, CA.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan
Bunescu, and Wayne Niblack. 2003.
Sentiment analyzer: Extracting sentiments
about a given topic using natural language
processing techniques. In Proceedings of the
3rd IEEE International Conference on Data
Mining (ICDM?03), pages 427?434,
Melbourne, FL.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136,
Sapporo.
433

Identifying Opinionated Sentences
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
twilson@cs.pitt.edu
David R. Pierce
Department of Computer Science
and Engineering
University of Buffalo
The State University of New York
drpierce@cse.buffalo.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
1 Introduction
Natural language processing applications that summa-
rize or answer questions about news and other discourse
need to process information about opinions, emotions,
and evaluations. For example, a question answering sys-
tem that could identify opinions in the news could answer
questions such as the following:
Was the 2002 presidential election in Zim-
babwe regarded as fair?
What was the world-wide reaction to the
2001 annual U.S. report on human rights?
In the news, editorials, reviews, and letters to the editor
are sources for finding opinions, but even in news reports,
segments presenting objective facts are often mixed with
segments presenting opinions and verbal reactions. This
is especially true for articles that report on controversial
or ?lightning rod? topics. Thus, there is a need to be able
to identify which sentences in a text actually contain ex-
pressions of opinions and emotions.
We demonstrate a system that identifies opinionated
sentences. In general, an opinionated sentence is a sen-
tence that contains a significant expression of an opin-
ion, belief, emotion, evaluation, speculation, or senti-
ment. The system was built using data and other re-
sources from a summer workshop on multi-perspective
question answering (Wiebe et al, 2003) funded under
ARDA NRRC.1
1This work was performed in support of the Northeast Re-
gional Research Center (NRRC) which is sponsored by the
Advanced Research and Development Activity in Information
Technology (ARDA), a U.S. Government entity which sponsors
and promotes research of import to the Intelligence Community
which includes but is not limited to the CIA, DIA, NSA, NIMA,
and NRO.
2 Opinion Recognition System
2.1 System Architecture
The opinion recognition system takes as input a URL
or raw text document and produces as output an HTML
version of the document with the opinionated sentences
found by the system highlighted in bold. Figure 2.1
shows a news article that was processed by the system.
When the opinion recognition system receives a docu-
ment, it first uses GATE (Cunningham et al, 2002) (mod-
ified to run in batch mode) to tokenize, sentence split,
and part-of-speech tag the document. Then the document
is stemmed and searched for features of opinionated lan-
guage. Finally, opinionated sentences are identified using
the features found, and they are highlighted in the output.
2.2 Features
The system uses a combination of manually and auto-
matically identified features. The manually identified
features were culled from a variety of sources, includ-
ing (Levin, 1993) and (Framenet, 2002). In addition to
features learned in previous work (Wiebe et al, 1999;
Wiebe et al, 2001), the automatically identified features
include new features that were learned using information
extraction techniques (Riloff and Jones, 1999; Thelen and
Riloff, 2002) applied to an unannotated corpus of world
news documents.
2.3 Evaluation
We evaluated the system component that identifies opin-
ionated sentences on a corpus of 109 documents (2200
sentences) from the world news. These articles were an-
notated for expressions of opinions as part of the summer
workshop on multi-perspective question answering. In
this test corpus, 59% of sentences are opinionated sen-
tences. By varying system settings, the opinionated sen-
tence recognizer may be tuned to be very precise (91%
precision), identifying only those sentences it is very sure
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 33-34
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Example of an article processed by the opinionated sentence recognition system. Sentences identified by the
system are highlighted in bold.
are opinionated (33% recall), or less precise (82% preci-
sion), identifing many more opinionated sentences (77%
recall), but also making more errors.
References
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Framenet. 2002. http://www.icsi.berkeley.edu/   framenet/.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
contexts. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99), pages 246?
253, University of Maryland, June. ACL.
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
D. Day, and M. Maybury. 2003. Recognizing and
organizing opinions expressed in the world press. In
Working Notes - New Directions in Question Answer-
ing (AAAI Spring Symposium Series).
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrating Knowledge for Subjectivity Sense Labeling
Yaw Gyamfi and Janyce Wiebe
University of Pittsburgh
{anti,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Abstract
This paper introduces an integrative approach
to automatic word sense subjectivity annota-
tion. We use features that exploit the hier-
archical structure and domain information in
lexical resources such as WordNet, as well as
other types of features that measure the sim-
ilarity of glosses and the overlap among sets
of semantically related words. Integrated in a
machine learning framework, the entire set of
features is found to give better results than any
individual type of feature.
1 Introduction
Automatic extraction of opinions, emotions, and
sentiments in text (subjectivity analysis) to support
applications such as product review mining, sum-
marization, question answering, and information ex-
traction is an active area of research in NLP.
Many approaches to opinion, sentiment, and sub-
jectivity analysis rely on lexicons of words that may
be used to express subjectivity. However, words may
have both subjective and objective senses, which is
a source of ambiguity in subjectivity and sentiment
analysis. We show that even words judged in pre-
vious work to be reliable clues of subjectivity have
significant degrees of subjectivity sense ambiguity.
To address this ambiguity, we present a method
for automatically assigning subjectivity labels to
word senses in a taxonomy, which uses new features
and integrates more diverse types of knowledge than
in previous work. We focus on nouns, which are
challenging and have received less attention in auto-
matic subjectivity and sentiment analysis.
A common approach to building lexicons for sub-
jectivity analysis is to begin with a small set of
seeds which are prototypically subjective (or posi-
tive/negative, in sentiment analysis), and then fol-
low semantic links in WordNet-like resources. By
far, the emphasis has been on horizontal relations,
such as synonymy and antonymy. Exploiting vertical
links opens the door to taking into account the infor-
mation content of ancestor concepts of senses with
known and unknown subjectivity. We develop novel
features that measure the similarity of a target word
sense with a seed set of senses known to be sub-
jective, where the similarity between two concepts
is determined by the extent to which they share in-
formation, measured by the information content as-
sociated with their least common subsumer (LCS).
Further, particularizing the LCS features to domain
greatly reduces calculation while still maintaining
effective features.
We find that our new features do lead to signif-
icant improvements over methods proposed in pre-
vious work, and that the combination of all features
gives significantly better performance than any sin-
gle type of feature alone.
We also ask, given that there are many approaches
to finding subjective words, if it would make sense
for word- and sense-level approaches to work in tan-
dem, or should we best view them as competing ap-
proaches? We give evidence suggesting that first
identifying subjective words and then disambiguat-
ing their senses would be an effective approach to
subjectivity sense labeling.
10
There are several motivations for assigning sub-
jectivity labels to senses. First, (Wiebe and Mi-
halcea, 2006) provide evidence that word sense la-
bels, together with contextual subjectivity analysis,
can be exploited to improve performance in word
sense disambiguation. Similarly, given subjectivity
sense labels, word-sense disambiguation may poten-
tially help contextual subjectivity analysis. In addi-
tion, as lexical resources such as WordNet are devel-
oped further, subjectivity labels would provide prin-
cipled criteria for refining word senses, as well as for
clustering similar meanings to create more course-
grained sense inventories.
For many opinion mining applications, polarity
(positive, negative) is also important. The overall
framework we envision is a layered approach: clas-
sifying instances as objective or subjective, and fur-
ther classifying the subjective instances by polar-
ity. Decomposing the problem into subproblems has
been found to be effective for opinion mining. This
paper addresses the first of these subproblems.
2 Background
We adopt the definitions of subjective and objective
from Wiebe and Mihalcea (2006) (hereafter WM).
Subjective expressions are words and phrases being
used to express opinions, emotions, speculations,
etc. WM give the following examples:
His alarm grew.
He absorbed the information quickly.
UCC/Disciples leaders roundly condemned the
Iranian President?s verbal assault on Israel.
What?s the catch?
Polarity (also called semantic orientation) is also
important to NLP applications in sentiment analysis
and opinion extraction. In review mining, for exam-
ple, we want to know whether an opinion about a
product is positive or negative. Even so, we believe
there are strong motivations for a separate subjec-
tive/objective (S/O) classification as well.
First, expressions may be subjective but not have
any particular polarity. An example given by (Wil-
son et al, 2005) is Jerome says the hospital feels
no different than a hospital in the states. An NLP
application system may want to find a wide range
of private states attributed to a person, such as their
motivations, thoughts, and speculations, in addition
to their positive and negative sentiments.
Second, distinguishing S and O instances has of-
ten proven more difficult than subsequent polarity
classification. Researchers have found this at vari-
ous levels of analysis, including the manual anno-
tation of phrases (Takamura et al, 2006), sentiment
classification of phrases (Wilson et al, 2005), sen-
timent tagging of words (Andreevskaia and Bergler,
2006b), and sentiment tagging of word senses (Esuli
and Sebastiani, 2006a). Thus, effective methods for
S/O classification promise to improve performance
for sentiment classification. In fact, researchers in
sentiment analysis have realized benefits by decom-
posing the problem into S/O and polarity classifica-
tion (Yu and Hatzivassiloglou, 2003; Pang and Lee,
2004; Wilson et al, 2005; Kim and Hovy, 2006).
One reason is that different features may be relevant
for the two subproblems. For example, negation fea-
tures are more important for polarity classification
than for subjectivity classification.
Note that some of our features require vertical
links that are present in WordNet for nouns and
verbs but not for other parts of speech. Thus we ad-
dress nouns (leaving verbs to future work). There
are other motivations for focusing on nouns. Rela-
tively little work in subjectivity and sentiment anal-
ysis has focused on subjective nouns. Also, a study
(Bruce and Wiebe, 1999) showed that, of the major
parts of speech, nouns are the most ambiguous with
respect to the subjectivity of their instances.
Turning to word senses, we adopt the definitions
from WM. First, subjective: ?Classifying a sense as
S means that, when the sense is used in a text or con-
versation, we expect it to express subjectivity; we
also expect the phrase or sentence containing it to
be subjective [WM, pp. 2-3].?
In WM, it is noted that sentences containing ob-
jective senses may not be objective, as in the sen-
tence Will someone shut that darn alarm off? Thus,
objective senses are defined as follows: ?Classifying
a sense as O means that, when the sense is used in a
text or conversation, we do not expect it to express
subjectivity and, if the phrase or sentence containing
it is subjective, the subjectivity is due to something
else [WM, p 3].?
The following subjective examples are given in
11
WM:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
The following objective examples are given in WM:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
WM performed an agreement study and report
that good agreement (?=0.74) can be achieved be-
tween human annotators labeling the subjectivity of
senses. For a similar task, (Su and Markert, 2008)
also report good agreement.
3 Related Work
Many methods have been developed for automati-
cally identifying subjective (opinion, sentiment, at-
titude, affect-bearing, etc.) words, e.g., (Turney,
2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004;
Taboada et al, 2006; Takamura et al, 2006).
Five groups have worked on subjectivity sense la-
beling. WM and Su and Markert (2008) (hereafter
SM) assign S/O labels to senses, while Esuli and Se-
bastiani (hereafter ES) (2006a; 2007), Andreevskaia
and Bergler (hereafter AB) (2006b; 2006a), and
(Valitutti et al, 2004) assign polarity labels.
WM, SM, and ES have evaluated their systems
against manually annotated word-sense data. WM?s
annotations are described above; SM?s are similar.
In the scheme ES use (Cerini et al, 2007), senses
are assigned three scores, for positivity, negativity,
and neutrality. There is no unambiguous mapping
between the labels of WM/SM and ES, first because
WM/SM use distinct classes and ES use numerical
ratings, and second because WM/SM distinguish be-
tween objective senses on the one hand and neutral
subjective senses on the other, while those are both
neutral in the scheme used by ES.
WM use an unsupervised corpus-based approach,
in which subjectivity labels are assigned to word
senses based on a set of distributionally similar
words in a corpus annotated with subjective expres-
sions. SM explore methods that use existing re-
sources that do not require manually annotated data;
they also implement a supervised system for com-
parison, which we will call SMsup. The other three
groups start with positive and negative seed sets and
expand them by adding synonyms and antonyms,
and traversing horizontal links in WordNet. AB, ES,
and SMsup additionally use information contained
in glosses; AB also use hyponyms; SMsup also uses
relation and POS features. AB perform multiple
runs of their system to assign fuzzy categories to
senses. ES use a semi-supervised, multiple-classifier
learning approach. In a later paper, (Esuli and Se-
bastiani, 2007), ES again use information in glosses,
applying a random walk ranking algorithm to a
graph in which synsets are linked if a member of
the first synset appears in the gloss of the second.
Like ES and SMsup, we use machine learning, but
with more diverse sources of knowledge. Further,
several of our features are novel for the task. The
LCS features (Section 6.1) detect subjectivity by
measuring the similarity of a candidate word sense
with a seed set. WM also use a similarity measure,
but as a way to filter the output of a measure of distri-
butional similarity (selecting words for a given word
sense), not as we do to cumulatively calculate the
subjectivity of a word sense. Another novel aspect
of our similarity features is that they are particular-
ized to domain, which greatly reduces calculation.
The domain subjectivity LCS features (Section 6.2)
are also novel for our task. So is augmenting seed
sets with monosemous words, for greater coverage
without requiring human intervention or sacrificing
quality. Note that none of our features as we specif-
ically define them has been used in previous work;
combining them together, our approach outperforms
previous approaches.
12
4 Lexicon and Annotations
We use the subjectivity lexicon of (Wiebe and Riloff,
2005)1 both to create a subjective seed set and to
create the experimental data sets. The lexicon is a
list of words and phrases that have subjective uses,
though only word entries are used in this paper (i.e.,
we do not address phrases at this point). Some en-
tries are from manually developed resources, includ-
ing the General Inquirer, while others were derived
from corpora using automatic methods.
Through manual review and empirical testing on
data, (Wiebe and Riloff, 2005) divided the clues into
strong (strongsubj) and weak (weaksubj) subjectiv-
ity clues. Strongsubj clues have subjective meanings
with high probability, and weaksubj clues have sub-
jective meanings with lower probability.
To support our experiments, we annotated the
senses2 of polysemous nouns selected from the lex-
icon, using WM?s annotation scheme described in
Section 2. Due to time constraints, only some of the
data was labeled through consensus labeling by two
annotators; the rest was labeled by one annotator.
Overall, 2875 senses for 882 words were anno-
tated. Even though all are senses of words from the
subjectivity lexicon, only 1383 (48%) of the senses
are subjective.
The words labeled strongsubj are in fact less am-
biguous than those labeled weaksubj in our analysis,
thus supporting the reliability classifications in the
lexicon. 55% (1038/1924) of the senses of strong-
subj words are subjective, while only 36% (345/951)
of the senses of weaksubj words are subjective.
For the analysis in Section 7.3, we form subsets
of the data annotated here to test performance of our
method on different data compositions.
5 Seed Sets
Both subjective and objective seed sets are used to
define the features described below. For seeds, a
large number is desirable for greater coverage, al-
though high quality is also important. We begin to
build our subjective seed set by adding the monose-
mous strongsubj nouns of the subjectivity lexicon
(there are 397 of these). Since they are monose-
mous, they pose no problem of sense ambiguity. We
1Available at http://www.cs.pitt.edu/mpqa
2In WordNet 2.0
then expand the set with their hyponyms, as they
were found useful in previous work by AB (2006b;
2006a). This yields a subjective seed set of 645
senses. After removing the word senses that belong
to the same synset, so that only one word sense per
synset is left, we ended up with 603 senses.
To create the objective seed set, two annotators
manually annotated 800 random senses from Word-
Net, and selected for the objective seed set the ones
they both agreed are clearly objective. This creates
an objective seed set of 727. Again we removed
multiple senses from the same synset leaving us with
722. The other 73 senses they annotated are added
to the mixed data set described below. As this sam-
pling shows, WordNet nouns are highly skewed to-
ward objective senses, so finding an objective seed
set is not difficult.
6 Features
6.1 Sense Subjectivity LCS Feature
This feature measures the similarity of a target sense
with members of the subjective seed set. Here, sim-
ilarity between two senses is determined by the ex-
tent to which they share information, measured by
using the information content associated with their
least common subsumer. For an intuition behind this
feature, consider this example. In WordNet, the hy-
pernym of the ?strong criticism? sense of attack is
criticism. Several other negative subjective senses
are descendants of criticism, including the relevant
senses of fire, thrust, and rebuke. Going up one
more level, the hypernym of criticism is the ?ex-
pression of disapproval? meaning of disapproval,
which has several additional negative subjective de-
scendants, such as the ?expression of opposition and
disapproval? sense of discouragement. Our hypoth-
esis is that the cases where subjectivity is preserved
in the hypernym structure, or where hypernyms do
lead from subjective senses to others, are the ones
that have the highest least common subsumer score
with the seed set of known subjective senses.
We calculate similarity using the information-
content based measure proposed in (Resnik, 1995),
as implemented in the WordNet::Similarity pack-
age (using the default option in which LCS values
are computed over the SemCor corpus).3 Given a
3http://search.cpan.org/dist/WordNet-Similarity/
13
taxonomy such as WordNet, the information con-
tent associated with a concept is determined as the
likelihood of encountering that concept, defined as
?log(p(C)), where p(C) is the probability of see-
ing concept C in a corpus. The similarity between
two concepts is then defined in terms of information
content as: LCSs(C1, C2) = max[?log(p(C))],
where C is the concept that subsumes both C1 and
C2 and has the highest information content (i.e., it is
the least common subsumer (LCS)).
For this feature, a score is assigned to a target
sense based on its semantic similarity to the mem-
bers of a seed set; in particular, the maximum such
similarity is used.
For a target sense t and a seed set S, we could
have used the following score:
Score(t, S) = max
s?S
LCSs(t, s)
However, several researchers have noted that sub-
jectivity may be domain specific. A version of
WordNet exists, WordNet Domains (Gliozzo et al,
2005), which associates each synset with one of the
domains in the Dewey Decimal library classifica-
tion. After sorting our subjective seed set into differ-
ent domains, we observed that over 80% of the sub-
jective seed senses are concentrated in six domains
(the rest are distributed among 35 domains).
Thus, we decided to particularize the semantic
similarity feature to domain, such that only the sub-
set of the seed set in the same domain as the tar-
get sense is used to compute the feature. This in-
volves much less calculation, as LCS values are cal-
culated only with respect to a subset of the seed set.
We hypothesized that this would still be an effec-
tive feature, while being more efficient to calculate.
This will be important when this method is applied
to large resources such as the entire WordNet.
Thus, for seed set S and target sense t which is
in domain D, the feature is defined as the following
score:
SenseLCSscore(t,D, S) = max
d?D?S
LCSs(t, d)
The seed set is a parameter, so we could have
defined a feature reflecting similarity to the objec-
tive seed set as well. Since WordNet is already
highly skewed toward objective noun senses, any
naive classifier need only guess the majority class
for high accuracy for the objective senses. We in-
cluded only a subjective feature to put more empha-
sis on the subjective senses. In the future, features
could be defined with respect to objectivity, as well
as polarity and other properties of subjectivity.
6.2 Domain Subjectivity LCS Score
We also include a feature reflecting the subjectivity
of the domain of the target sense. Domains are
assigned scores as follows. For domain D and seed
set S:
DomainLCSscore(D,S) =
aved?D?SMemLCSscore(d,D, S)
where:
MemLCSscore(d,D, S) =
max
di?D?S,di 6=d
LCSs(d, di)
The value of this feature for a sense is the score
assigned to that sense?s domain.
6.3 Common Related Senses
This feature is based on the intersection between the
set of senses related (via WordNet relations) to the
target sense and the set of senses related to members
of a seed set. First, for the target sense and each
member of the seed set, a set of related senses is
formed consisting of its synonyms, antonyms and di-
rect hypernyms as defined by WordNet. For a sense
s, R(s) is s together with its related senses.
Then, given a target sense t and a seed set S we
compute an average percentage overlap as follows:
RelOverlap(t, S) =
?
si?S
|R(t)?R(si)|
max (|R(t)|,|R(si)|)
|S|
The value of a feature is its score. Two features
are included in the experiments below, one for each
of the subjective and objective seed sets.
6.4 Gloss-based features
These features are Lesk-style features (Lesk, 1986)
that exploit overlaps between glosses of target and
seed senses. We include two types in our work.
6.4.1 Average Percentage Gloss Overlap
Features
For a sense s, gloss(s) is the set of stems in the
gloss of s (excluding stop words). Then, given a tar-
14
get sense t and a seed set S, we compute an average
percentage overlap as follows:
GlOverlap(t, S) =
?
si?S
|gloss(t)??r?R(si)gloss(r)|
max (|gloss(t)|,|?r?R(si)gloss(r)|)
|S|
As above, R(s) is considered for each seed sense
s, but now only the target sense t is considered, not
R(t). We did this because we hypothesized that the
gloss can provide sufficient context for a given target
sense, so that the addition of related words is not
necessary.
We include two features, one for each of the sub-
jective and objective seed sets.
6.4.2 Vector Gloss Overlap Features
For this feature we also consider overlaps of
stems in glosses (excluding stop words). The over-
laps considered are between the gloss of the tar-
get sense t and the glosses of R(s) for all s in a
seed set (for convenience, we will refer to these as
seedRelationSets).
A vector of stems is created, one for each stem
(excluding stop words) that appears in a gloss of
a member of seedRelationSets. If a stem in the
gloss of the target sense appears in this vector, then
the vector entry for that stem is the total count of
that stem in the glosses of the target sense and all
members of seedRelationSets.
A feature is created for each vector entry whose
value is the count at that position. Thus, these fea-
tures consider counts of individual stems, rather than
average proportions of overlaps, as for the previous
type of gloss feature.
Two vectors of features are used, one where the
seed set is the subjective seed set, and one where it
is the objective seed set.
6.5 Summary
In summary, we use the following features (here, SS
is the subjective seed set and OS is the objective
one).
1. SenseLCSscore(t,D, SS)
2. DomainLCSscore(D,SS)
3. RelOverlap(t, SS)
4. RelOverlap(t, OS)
5. GlOverlap(t, SS)
6. GlOverlap(t, OS)
Features Acc P R F
All 77.3 72.8 74.3 73.5
Standalone Ablation Results
All 77.3 72.8 74.3 73.5
LCS 68.2 69.3 44.2 54.0
Gloss vector 74.3 71.2 68.5 69.8
Overlaps 69.4 75.8 40.6 52.9
Leave-One-Out Ablation Results
All 77.3 72.8 74.3 73.5
LCS 75.2 70.9 70.6 70.7
Gloss vector 75.0 74.4 61.8 67.5
Overlaps 74.8 71.9 73.8 72.8
Table 1: Results for the mixed corpus (2354 senses,
57.82% O))
7. Vector of gloss words (SS)
8. Vector of gloss words (OS)
7 Experiments
We perform 10-fold cross validation experiments
on several data sets, using SVM light (Joachims,
1999)4 under its default settings.
Based on our random sampling of WordNet, it
appears that WordNet nouns are highly skewed to-
ward objective senses. (Esuli and Sebastiani, 2007)
argue that random sampling from WordNet would
yield a corpus mostly consisting of objective (neu-
tral) senses, which would be ?pretty useless as a
benchmark for testing derived lexical resources for
opinion mining [p. 428].? So, they use a mixture of
subjective and objective senses in their data set.
To create a mixed corpus for our task, we anno-
tated a second random sample from WordNet (which
is as skewed as the previously mentioned one). We
added together all of the senses of words in the lexi-
con which we annotated, the leftover senses from the
selection of objective seed senses, and this new sam-
ple. We removed duplicates, multiple senses from
the same synset, and any senses belonging to the
same synset in either of the seed sets. This resulted
in a corpus of 2354 senses, 993 (42.18%) of which
are subjective and 1361 (57.82%) of which are ob-
jective.
The results with all of our features on this mixed
corpus are given in Row 1 of Table 1. In Table 1, the
4http://svmlight.joachims.org/
15
first column identifies the features, which in this case
is all of them. The next three columns show overall
accuracy, and precision and recall for finding sub-
jective senses. The baseline accuracy for the mixed
data set (guessing the more frequent class, which is
objective) is 57.82%. As the table shows, the accu-
racy is substantially above baseline.5
7.1 Analysis and Discussion
In this section, we seek to gain insights by perform-
ing ablation studies, evaluating our method on dif-
ferent data compositions, and comparing our results
to previous results.
7.2 Ablation Studies
Since there are several features, we divided them
into sets for the ablation studies. The vector-of-
gloss-words features are the most similar to ones
used in previous work. Thus, we opted to treat
them as one ablation group (Gloss vector). The
Overlaps group includes the RelOverlap(t, SS),
RelOverlap(t, OS), GlOverlap(t, SS), and
GlOverlap(t, OS) features. Finally, the LCS
group includes the SenseLCSscore and the
DomainLCSscore features.
There are two types of ablation studies. In the
first, one group of features at a time is included.
Those results are in the middle section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using only the LCS fea-
tures. In comparison to performance when all fea-
tures are used, F-measure for the Overlaps and LCS
ablations is significantly different at the p < .01
level, and, for the Gloss Vector ablation, it is sig-
nificantly different at the p = .052 level (one-tailed
t-test). Thus, all of the features together have better
performance than any single type of feature alone.
In the second type of ablation study, we use all
the features minus one group of features at a time.
The results are in the bottom section of Table 1.
Thus, for example, the row labeled LCS in this sec-
tion is for an experiment using all but the LCS fea-
tures. F-measures for LCS and Gloss vector are sig-
nificantly different at the p = .056 and p = .014 lev-
els, respectively. However, F-measure for the Over-
laps ablation is not significantly different (p = .39).
5Note that, because the majority class is O, baseline recall
(and thus F-measure) is 0.
Data (#senses) Acc P R F
mixed (2354 57.8% O) 77.3 72.8 74.3 73.5
strong+weak (1132) 77.7 76.8 78.9 77.8
weaksubj (566) 71.3 70.3 71.1 70.7
strongsubj (566) 78.6 78.8 78.6 78.7
Table 2: Results for different data sets (all are 50% S,
unless otherwise notes)
These results provide evidence that LCS and Gloss
vector are better together than either of them alone.
7.3 Results on Different Data Sets
Several methods have been developed for identify-
ing subjective words. Perhaps an effective strategy
would be to begin with a word-level subjectivity lex-
icon, and then perform subjectivity sense labeling
to sort the subjective from objective senses of those
words. We also wondered about the relative effec-
tiveness of our method on strongsubj versus weak-
subj clues.
To answer these questions, we apply the full
model (again in 10-fold cross validation experi-
ments) to data sets composed of senses of polyse-
mous words in the subjectivity lexicon. To support
comparison, all of the data sets in this section have
a 50%-50% objective/subjective distribution.6 The
results are presented in Table 2.
For comparison, the first row repeats the results
for the mixed corpus from Table 1. The second
row shows results for a corpus of senses of a mix-
ture of strongsubj and weaksubj words. The corpus
was created by selecting a mixture of strongsubj and
weaksubj words, extracting their senses and the S/O
labels applied to them in Section 4, and then ran-
domly removing senses of the more frequent class
until the distribution is uniform. We see that the
results on this corpus are better than on the mixed
data set, even though the baseline accuracy is lower
and the corpus is smaller. This supports the idea
that an effective strategy would be to first identify
opinion-bearing words, and then apply our method
to those words to sort out their subjective and objec-
tive senses.
The third row shows results for a weaksubj subset
6As with the mixed data set, we removed from these data
sets multiple senses from the same synset and any senses in the
same synset in either of the seed sets.
16
Method P R F
Our method 56.8 66.0 61.1
WM, 60% recall 44.0 66.0 52.8
SentiWordNet mapping 60.0 17.3 26.8
Table 3: Results for WM Corpus (212 senses, 76% O)
Method A P R F
Our Method 81.3% 60.3% 63.3% 61.8%
SM CV* 82.4% 70.8% 41.1% 52.0%
SM SL* 78.3% 53.0% 57.4% 54.9%
Table 4: Results for SM Corpus (484 senses, 76.9% O)
of the strong+weak corpus and the fourth shows re-
sults for a strongsubj subset that is of the same size.
As expected, the results for the weaksubj senses
are lower while those for the strongsubj senses are
higher, as weaksubj clues are more ambiguous.
7.4 Comparisons with Previous Work
WM and SM address the same task as we do. To
compare our results to theirs, we apply our full
model (in 10-fold cross validation experiments) to
their data sets.7
Table 3 has the WM data set results. WM rank
their senses and present their results in the form of
precision recall curves. The second row of Table 3
shows their results at the recall level achieved by our
method (66%). Their precision at that level is sub-
stantially below ours.
Turning to ES, to create S/O annotations, we ap-
plied the following heuristic mapping (which is also
used by SM for the purpose of comparison): any
sense for which the sum of positive and negative
scores is greater than or equal to 0.5 is S, otherwise
it is O. We then evaluate the mapped tags against the
gold standard of WM. The results are in Row 3 of
Table 3. Note that this mapping is not fair to Sen-
tiWordNet, as the tasks are quite different, and we
do not believe any conclusions can be drawn. We
include the results to eliminate the possibility that
their method is as good ours on our task, despite the
differences between the tasks.
Table 4 has the results for the noun subset of SM?s
7The WM data set is available at
http://www.cs.pitt.edu/www.cs.pitt.edu/?wiebe. ES applied
their method in (2006b) to WordNet, and made the results
available as SentiWordNet at http://sentiwordnet.isti.cnr.it/.
data set, which is the data set used by ES, reanno-
tated by SM. CV* is their supervised system and
SL* is their best non-supervised one. Our method
has higher F-measure than the others.8 Note that the
focus of SM?s work is not supervised machine learn-
ing.
8 Conclusions
In this paper, we introduced an integrative approach
to automatic subjectivity word sense labeling which
combines features exploiting the hierarchical struc-
ture and domain information of WordNet, as well
as similarity of glosses and overlap among sets
of semantically related words. There are several
contributions. First, we learn several things. We
found (in Section 4) that even reliable lists of sub-
jective (opinion-bearing) words have many objec-
tive senses. We asked if word- and sense-level ap-
proaches could be used effectively in tandem, and
found (in Section 7.3) that an effective strategy is to
first identify opinion-bearing words, and then apply
our method to sort out their subjective and objective
senses. We also found (in Section 7.2) that the entire
set of features gives better results than any individ-
ual type of feature alone.
Second, several of the features are novel for
our task, including those exploiting the hierarchical
structure of a lexical resource, domain information,
and relations to seed sets expanded with monose-
mous senses.
Finally, the combination of our particular features
is effective. For example, on senses of words from
a subjectivity lexicon, accuracies range from 20 to
29 percentage points above baseline. Further, our
combination of features outperforms previous ap-
proaches.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation awards #0840632 and #0840608.
The authors are grateful to Fangzhong Su and Katja
Markert for making their data set available, and to
the three paper reviewers for their helpful sugges-
tions.
8We performed the same type of evaluation as in SM?s paper.
That is, we assign a subjectivity label to one word sense for each
synset, which is the same as applying a subjectivity label to a
synset as a whole as done by SM.
17
References
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006b. Sen-
timent tag extraction from wordnet glosses. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation.
Rebecca Bruce and Janyce Wiebe. 1999. Recognizing
subjectivity: A case study of manual tagging. Natural
Language Engineering, 5(2):187?205.
S. Cerini, V. Campagnoni, A. Demontis, M. Formentelli,
and C. Gandini. 2007. Micro-wnop: A gold standard
for the evaluation of automatically compiled lexical re-
sources for opinion mining. In Language resources
and linguistic theory: Typology, second language ac-
quisition, English linguistics. Milano.
Andrea Esuli and Fabrizio Sebastiani. 2006a. Determin-
ing term subjectivity and term orientation for opinion
mining. In 11th Meeting of the European Chapter of
the Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006b. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Conference
on Language Resources and Evaluation, Genova, IT.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 424?
431, Prague, Czech Republic, June.
A. Gliozzo, C. Strapparava, E. d?Avanzo, and
B. Magnini. 2005. Automatic acquisition of
domain specific lexicons. Tech. report, IRST, Italy.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burgess, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning, Cambridge, MA. MIT-Press.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twentieth
International Conference on Computational Linguis-
tics, pages 1267?1373, Geneva, Switzerland.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
Empirical Methods in Natural Language Processing,
pages 200?207, New York.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics , pages 271?278, Barcelona, ES. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In Proc. Inter-
national Joint Conference on Artificial Intelligence.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Conference on
Empirical Methods in Natural Language Processing,
pages 105?112.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics, Manchester.
M. Taboada, C. Anthony, and K. Voll. 2006. Methods
for creating semantic orientation databases. In Pro-
ceedings of 5th International Conference on Language
Resources and Evaluation .
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orienta-
tions of phrases. In Proceedings of the 11th Meeting
of the European Chapter of the Association for Com-
putational Linguistics , Trento, Italy.
P. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of re-
views. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
417?424, Philadelphia.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology Journal, 2(1):61?83.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Com-
putational Linguistics , pages 486?497, Mexico City,
Mexico.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technologies Conference/Conference on Empir-
ical Methods in Natural Language Processing , pages
347?354, Vancouver, Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Conference on Empirical Methods in Nat-
ural Language Processing , pages 129?136, Sapporo,
Japan.
18
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1065?1072,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Sense and Subjectivity
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
rada@cs.unt.edu
Abstract
Subjectivity and meaning are both impor-
tant properties of language. This paper ex-
plores their interaction, and brings empir-
ical evidence in support of the hypotheses
that (1) subjectivity is a property that can
be associated with word senses, and (2)
word sense disambiguation can directly
benefit from subjectivity annotations.
1 Introduction
There is growing interest in the automatic extrac-
tion of opinions, emotions, and sentiments in text
(subjectivity), to provide tools and support for var-
ious NLP applications. Similarly, there is continu-
ous interest in the task of word sense disambigua-
tion, with sense-annotated resources being devel-
oped for many languages, and a growing num-
ber of research groups participating in large-scale
evaluations such as SENSEVAL.
Though both of these areas are concerned with
the semantics of a text, over time there has been
little interaction, if any, between them. In this pa-
per, we address this gap, and explore possible in-
teractions between subjectivity and word sense.
There are several benefits that would motivate
such a joint exploration. First, at the resource
level, the augmentation of lexical resources such
as WordNet (Miller, 1995) with subjectivity labels
could support better subjectivity analysis tools,
and principled methods for refining word senses
and clustering similar meanings. Second, at the
tool level, an explicit link between subjectivity and
word sense could help improve methods for each,
by integrating features learned from one into the
other in a pipeline approach, or through joint si-
multaneous learning.
In this paper we address two questions about
word sense and subjectivity. First, can subjectiv-
ity labels be assigned to word senses? To address
this question, we perform two studies. The first
(Section 3) investigates agreement between anno-
tators who manually assign the labels subjective,
objective, or both to WordNet senses. The second
study (Section 4) evaluates a method for automatic
assignment of subjectivity labels to word senses.
We devise an algorithm relying on distributionally
similar words to calculate a subjectivity score, and
show how it can be used to automatically assess
the subjectivity of a word sense.
Second, can automatic subjectivity analysis be
used to improve word sense disambiguation? To
address this question, the output of a subjectivity
sentence classifier is input to a word-sense disam-
biguation system, which is in turn evaluated on the
nouns from the SENSEVAL-3 English lexical sam-
ple task (Section 5). The results of this experiment
show that a subjectivity feature can significantly
improve the accuracy of a word sense disambigua-
tion system for those words that have both subjec-
tive and objective senses.
A third obvious question is, can word sense dis-
ambiguation help automatic subjectivity analysis?
However, due to space limitations, we do not ad-
dress this question here, but rather leave it for fu-
ture work.
2 Background
Subjective expressions are words and phrases
being used to express opinions, emotions, evalu-
ations, speculations, etc. (Wiebe et al, 2005). A
general covering term for such states is private
state, ?a state that is not open to objective obser-
1065
vation or verification? (Quirk et al, 1985).1 There
are three main types of subjective expressions:2
(1) references to private states:
His alarm grew.
He absorbed the information quickly.
He was boiling with anger.
(2) references to speech (or writing) events ex-
pressing private states:
UCC/Disciples leaders roundly con-
demned the Iranian President?s verbal
assault on Israel.
The editors of the left-leaning paper at-
tacked the new House Speaker.
(3) expressive subjective elements:
He would be quite a catch.
What?s the catch?
That doctor is a quack.
Work on automatic subjectivity analysis falls
into three main areas. The first is identifying
words and phrases that are associated with sub-
jectivity, for example, that think is associated with
private states and that beautiful is associated with
positive sentiments (e.g., (Hatzivassiloglou and
McKeown, 1997; Wiebe, 2000; Kamps and Marx,
2002; Turney, 2002; Esuli and Sebastiani, 2005)).
Such judgments are made for words. In contrast,
our end task (in Section 4) is to assign subjectivity
labels to word senses.
The second is subjectivity classification of sen-
tences, clauses, phrases, or word instances in the
context of a particular text or conversation, ei-
ther subjective/objective classifications or posi-
tive/negative sentiment classifications (e.g.,(Riloff
and Wiebe, 2003; Yu and Hatzivassiloglou, 2003;
Dave et al, 2003; Hu and Liu, 2004)).
The third exploits automatic subjectivity anal-
ysis in applications such as review classification
(e.g., (Turney, 2002; Pang and Lee, 2004)), min-
ing texts for product reviews (e.g., (Yi et al, 2003;
Hu and Liu, 2004; Popescu and Etzioni, 2005)),
summarization (e.g., (Kim and Hovy, 2004)), in-
formation extraction (e.g., (Riloff et al, 2005)),
1Note that sentiment, the focus of much recent work in the
area, is a type of subjectivity, specifically involving positive
or negative opinion, emotion, or evaluation.
2These distinctions are not strictly needed for this paper,
but may help the reader appreciate the examples given below.
and question answering (e.g., (Yu and Hatzivas-
siloglou, 2003; Stoyanov et al, 2005)).
Most manual subjectivity annotation research
has focused on annotating words, out of context
(e.g., (Heise, 2001)), or sentences and phrases in
the context of a text or conversation (e.g., (Wiebe
et al, 2005)). The new annotations in this pa-
per are instead targeting the annotation of word
senses.
3 Human Judgment of Word Sense
Subjectivity
To explore our hypothesis that subjectivity may
be associated with word senses, we developed a
manual annotation scheme for assigning subjec-
tivity labels to WordNet senses,3 and performed
an inter-annotator agreement study to assess its
reliability. Senses are classified as S(ubjective),
O(bjective), or B(oth). Classifying a sense as S
means that, when the sense is used in a text or con-
versation, we expect it to express subjectivity; we
also expect the phrase or sentence containing it to
be subjective.
We saw a number of subjective expressions in
Section 2. A subset is repeated here, along with
relevant WordNet senses. In the display of each
sense, the first part shows the synset, gloss, and
any examples. The second part (marked with =>)
shows the immediate hypernym.
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in
anticipation of some specific pain or danger (usually ac-
companied by a desire to flee or fight))
He was boiling with anger.
seethe, boil ? (be in an agitated emotional state; ?The cus-
tomer was seething with anger?)
=> be ? (have the quality of being; (copula, used with an
adjective or a predicate noun); ?John is rich?; ?This is not
a good answer?)
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
That doctor is a quack.
quack ? (an untrained person who pretends to be a physician
and who dispenses medical advice)
=> doctor, doc, physician, MD, Dr., medico
Before specifying what we mean by an objec-
tive sense, we give examples.
3All our examples and data used in the experiments are
from WordNet 2.0.
1066
The alarm went off.
alarm, warning device, alarm system ? (a device that signals
the occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particu-
lar purpose; ?the device is small enough to wear on your
wrist?; ?a device intended to conserve water?)
The water boiled.
boil ? (come to the boiling point and change from a liquid to
vapor; ?Water boils at 100 degrees Celsius?)
=> change state, turn ? (undergo a transformation or a
change of position or action; ?We turned from Socialism
to Capitalism?; ?The people turned against the President
when he stole the election?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was
only 10 fish?)
=> indefinite quantity ? (an estimated quantity)
The duck?s quack was loud and brief.
quack ? (the harsh sound of a duck)
=> sound ? (the sudden occurrence of an audible event;
?the sound awakened them?)
While we expect phrases or sentences contain-
ing subjective senses to be subjective, we do not
necessarily expect phrases or sentences containing
objective senses to be objective. Consider the fol-
lowing examples:
Will someone shut that damn alarm off?
Can?t you even boil water?
While these sentences contain objective senses
of alarm and boil, the sentences are subjective
nonetheless. But they are not subjective due to
alarm and boil, but rather to punctuation, sentence
forms, and other words in the sentence. Thus, clas-
sifying a sense as O means that, when the sense is
used in a text or conversation, we do not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Finally, classifying a sense as B means it covers
both subjective and objective usages, e.g.:
absorb, suck, imbibe, soak up, sop up, suck up, draw, take in,
take up ? (take in, also metaphorically; ?The sponge absorbs
water well?; ?She drew strength from the minister?s words?)
Manual subjectivity judgments were added to
a total of 354 senses (64 words). One annotator,
Judge 1 (a co-author), tagged all of them. A sec-
ond annotator (Judge 2, who is not a co-author)
tagged a subset for an agreement study, presented
next.
3.1 Agreement Study
For the agreement study, Judges 1 and 2 indepen-
dently annotated 32 words (138 senses). 16 words
have both S and O senses and 16 do not (according
to Judge 1). Among the 16 that do not have both
S and O senses, 8 have only S senses and 8 have
only O senses. All of the subsets are balanced be-
tween nouns and verbs. Table 1 shows the contin-
gency table for the two annotators? judgments on
this data. In addition to S, O, and B, the annotation
scheme also permits U(ncertain) tags.
S O B U Total
S 39 O O 4 43
O 3 73 2 4 82
B 1 O 3 1 5
U 3 2 O 3 8
Total 46 75 5 12 138
Table 1: Agreement on balanced set (Agreement:
85.5%, ?: 0.74)
Overall agreement is 85.5%, with a Kappa (?)
value of 0.74. For 12.3% of the senses, at least
one annotator?s tag is U. If we consider these cases
to be borderline and exclude them from the study,
percent agreement increases to 95% and ? rises to
0.90. Thus, annotator agreement is especially high
when both are certain.
Considering only the 16-word subset with both
S and O senses (according to Judge 1), ? is .75,
and for the 16-word subset for which Judge 1 gave
only S or only O senses, ? is .73. Thus, the two
subsets are of comparable difficulty.
The two annotators also independently anno-
tated the 20 ambiguous nouns (117 senses) of the
SENSEVAL-3 English lexical sample task used in
Section 5. For this tagging task, U tags were not
allowed, to create a definitive gold standard for the
experiments. Even so, the ? value for them is 0.71,
which is not substantially lower. The distributions
of Judge 1?s tags for all 20 words can be found in
Table 3 below.
We conclude this section with examples of
disagreements that illustrate sources of uncer-
tainty. First, uncertainty arises when subjec-
tive senses are missing from the dictionary.
The labels for the senses of noun assault are
(O:O,O:O,O:O,O:UO).4 For verb assault there is
a subjective sense:
attack, round, assail, lash out, snipe, assault (attack in speech
or writing) ?The editors of the left-leaning paper attacked the
new House Speaker?
However, there is no corresponding sense for
4I.e., the first three were labeled O by both annotators. For
the fourth sense, the second annotator was not sure but was
leaning toward O.
1067
noun assault. A missing sense may lead an anno-
tator to try to see subjectivity in an objective sense.
Second, uncertainty can arise in weighing hy-
pernym against sense. It is fine for a synset to
imply just S or O, while the hypernym implies
both (the synset specializes the more general con-
cept). However, consider the following, which
was tagged (O:UB).
attack ? (a sudden occurrence of an uncontrollable condition;
?an attack of diarrhea?)
=> affliction ? (a cause of great suffering and distress)
While the sense is only about the condition, the
hypernym highlights subjective reactions to the
condition. One annotator judged only the sense
(giving tag O), while the second considered the
hypernym as well (giving tag UB).
4 Automatic Assessment of Word Sense
Subjectivity
Encouraged by the results of the agreement study,
we devised a method targeting the automatic an-
notation of word senses for subjectivity.
The main idea behind our method is that we can
derive information about a word sense based on in-
formation drawn from words that are distribution-
ally similar to the given word sense. This idea re-
lates to the unsupervised word sense ranking algo-
rithm described in (McCarthy et al, 2004). Note,
however, that (McCarthy et al, 2004) used the in-
formation about distributionally similar words to
approximate corpus frequencies for word senses,
whereas we target the estimation of a property of
a given word sense (the ?subjectivity?).
Starting with a given ambiguous word w, we
first find the distributionally similar words using
the method of (Lin, 1998) applied to the automat-
ically parsed texts of the British National Corpus.
Let DSW = dsw1, dsw2, ..., dswn be the list of
top-ranked distributionally similar words, sorted
in decreasing order of their similarity.
Next, for each sense wsi of the word w, we de-
termine the similarity with each of the words in the
list DSW , using a WordNet-based measure of se-
mantic similarity (wnss). Although a large num-
ber of such word-to-word similarity measures ex-
ist, we chose to use the (Jiang and Conrath, 1997)
measure, since it was found both to be efficient
and to provide the best results in previous exper-
iments involving word sense ranking (McCarthy
et al, 2004)5. For distributionally similar words
5Note that unlike the above measure of distributional sim-
Algorithm 1 Word Sense Subjectivity Score
Input: Word sense wi
Input: Distributionally similar words DSW = {dswj |j =
1..n}
Output: Subjectivity score subj(wi)
1: subj(wi) = 0
2: totalsim = 0
3: for j = 1 to n do
4: Instsj = all instances of dswj in the MPQA corpus
5: for k in Instsj do
6: if k is in a subj. expr. in MPQA corpus then
7: subj(wi) += sim(wi,dswj)
8: else if k is not in a subj. expr. in MPQA corpus
then
9: subj(wi) -= sim(wi,dswj)
10: end if
11: totalsim += sim(wi,dswj)
12: end for
13: end for
14: subj(wi) = subj(wi) / totalsim
that are themselves ambiguous, we use the sense
that maximizes the similarity score. The similar-
ity scores associated with each word dswj are nor-
malized so that they add up to one across all possi-
ble senses of w, which results in a score described
by the following formula:
sim(wsi, dswj) = wnss(wsi,dswj)?
i??senses(w)
wnss(wsi? ,dswj)
where
wnss(wsi, dswj) = max
k?senses(dswj)
wnss(wsi, dswkj )
A selection process can also be applied so that
a distributionally similar word belongs only to
one sense. In this case, for a given sense wi we
use only those distributionally similar words with
whom wi has the highest similarity score across all
the senses of w. We refer to this case as similarity-
selected, as opposed to similarity-all, which refers
to the use of all distributionally similar words for
all senses.
Once we have a list of similar words associated
with each sense wsi and the corresponding simi-
larity scores sim(wsi, dswj), we use an annotated
corpus to assign subjectivity scores to the senses.
The corpus we use is the MPQA Opinion Corpus,
which consists of over 10,000 sentences from the
world press annotated for subjective expressions
(all three types of subjective expressions described
in Section 2).6
ilarity which measures similarity between words, rather than
word senses, here we needed a similarity measure that also
takes into account word senses as defined in a sense inven-
tory such as WordNet.
6The MPQA corpus is described in (Wiebe et al, 2005)
and available at www.cs.pitt.edu/mpqa/databaserelease/.
1068
Algorithm 1 is our method for calculating sense
subjectivity scores. The subjectivity score is a
value in the interval [-1,+1] with +1 correspond-
ing to highly subjective and -1 corresponding to
highly objective. It is a sum of sim scores, where
sim(wi,dswj) is added for each instance of dswj
that is in a subjective expression, and subtracted
for each instance that is not in a subjective expres-
sion.
Note that the annotations in the MPQA corpus
are for subjective expressions in context. Thus, the
data is somewhat noisy for our task, because, as
discussed in Section 3, objective senses may ap-
pear in subjective expressions. Nonetheless, we
hypothesized that subjective senses tend to appear
more often in subjective expressions than objec-
tive senses do, and use the appearance of words in
subjective expressions as evidence of sense sub-
jectivity.
(Wiebe, 2000) also makes use of an annotated
corpus, but in a different approach: given a word
w and a set of distributionally similar words DSW,
that method assigns a subjectivity score to w equal
to the conditional probability that any member of
DSW is in a subjective expression. Moreover, the
end task of that work was to annotate words, while
our end task is the more difficult problem of anno-
tating word senses for subjectivity.
4.1 Evaluation
The evaluation of the algorithm is performed
against the gold standard of 64 words (354 word
senses) using Judge 1?s annotations, as described
in Section 3.
For each sense of each word in the set of 64
ambiguous words, we use Algorithm 1 to deter-
mine a subjectivity score. A subjectivity label is
then assigned depending on the value of this score
with respect to a pre-selected threshold. While a
threshold of 0 seems like a sensible choice, we per-
form the evaluation for different thresholds rang-
ing across the [-1,+1] interval, and correspond-
ingly determine the precision of the algorithm at
different points of recall7. Note that the word
senses for which none of the distributionally sim-
ilar words are found in the MPQA corpus are not
7Specifically, in the list of word senses ranked by their
subjectivity score, we assign a subjectivity label to the top N
word senses. The precision is then determined as the number
of correct subjectivity label assignments out of all N assign-
ments, while the recall is measured as the correct subjective
senses out of all the subjective senses in the gold standard
data set. By varying the value of N from 1 to the total num-
ber of senses in the corpus, we can derive precision and recall
curves.
included in this evaluation (excluding 82 senses),
since in this case a subjectivity score cannot be
calculated. The evaluation is therefore performed
on a total of 272 word senses.
As a baseline, we use an ?informed? random as-
signment of subjectivity labels, which randomly
assigns S labels to word senses in the data set,
such that the maximum number of S assignments
equals the number of correct S labels in the gold
standard data set. This baseline guarantees a max-
imum recall of 1 (which under true random condi-
tions might not be achievable). Correspondingly,
given the controlled distribution of S labels across
the data set in the baseline setting, the precision
is equal for all eleven recall points, and is deter-
mined as the total number of correct subjective as-
signments divided by the size of the data set8.
Number Break-even
Algorithm of DSW point
similarity-all 100 0.41
similarity-selected 100 0.50
similarity-all 160 0.43
similarity-selected 160 0.50
baseline - 0.27
Table 2: Break-even point for different algorithm
and parameter settings
There are two aspects of the sense subjectivity
scoring algorithm that can influence the label as-
signment, and correspondingly their evaluation.
First, as indicated above, after calculating the
semantic similarity of the distributionally similar
words with each sense, we can either use all the
distributionally similar words for the calculation
of the subjectivity score of each sense (similarity-
all), or we can use only those that lead to the high-
est similarity (similarity-selected). Interestingly,
this aspect can drastically affect the algorithm ac-
curacy. The setting where a distributionally simi-
lar word can belong only to one sense significantly
improves the algorithm performance. Figure 1
plots the interpolated precision for eleven points of
recall, for similarity-all, similarity-selected, and
baseline. As shown in this figure, the precision-
recall curves for our algorithm are clearly above
the ?informed? baseline, indicating the ability of
our algorithm to automatically identify subjective
word senses.
Second, the number of distributionally similar
words considered in the first stage of the algo-
rithm can vary, and might therefore influence the
8In other words, this fraction represents the probability of
making the correct subjective label assignment by chance.
1069
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Precision recall curves
selected
all
baseline
Figure 1: Precision and recall for automatic sub-
jectivity annotations of word senses (DSW=160).
output of the algorithm. We experiment with two
different values, namely 100 and 160 top-ranked
distributionally similar words. Table 2 shows the
break-even points for the four different settings
that were evaluated,9 with results that are almost
double compared to the informed baseline. As
it turns out, for weaker versions of the algorithm
(i.e., similarity-all), the size of the set of distribu-
tionally similar words can significantly impact the
performance of the algorithm. However, for the al-
ready improved similarity-selected algorithm ver-
sion, this parameter does not seem to have influ-
ence, as similar results are obtained regardless of
the number of distributionally similar words. This
is in agreement with the finding of (McCarthy et
al., 2004) that, in their word sense ranking method,
a larger set of neighbors did not influence the al-
gorithm accuracy.
5 Automatic Subjectivity Annotations for
Word Sense Disambiguation
The final question we address is concerned with
the potential impact of subjectivity on the quality
of a word sense classifier. To answer this ques-
tion, we augment an existing data-driven word
sense disambiguation system with a feature re-
flecting the subjectivity of the examples where the
ambiguous word occurs, and evaluate the perfor-
mance of the new subjectivity-aware classifier as
compared to the traditional context-based sense
classifier.
We use a word sense disambiguation system
that integrates both local and topical features.
9The break-even point (Lewis, 1992) is a standard mea-
sure used in conjunction with precision-recall evaluations. It
represents the value where precision and recall become equal.
Specifically, we use the current word and its part-
of-speech, a local context of three words to the
left and right of the ambiguous word, the parts-of-
speech of the surrounding words, and a global con-
text implemented through sense-specific keywords
determined as a list of at most five words occurring
at least three times in the contexts defining a cer-
tain word sense. This feature set is similar to the
one used by (Ng and Lee, 1996), as well as by a
number of SENSEVAL systems. The parameters
for sense-specific keyword selection were deter-
mined through cross-fold validation on the train-
ing set. The features are integrated in a Naive
Bayes classifier, which was selected mainly for
its performance in previous work showing that it
can lead to a state-of-the-art disambiguation sys-
tem given the features we consider (Lee and Ng,
2002).
The experiments are performed on the set of
ambiguous nouns from the SENSEVAL-3 English
lexical sample evaluation (Mihalcea et al, 2004).
We use the rule-based subjective sentence classi-
fier of (Riloff and Wiebe, 2003) to assign an S,
O, or B label to all the training and test examples
pertaining to these ambiguous words. This sub-
jectivity annotation tool targets sentences, rather
than words or paragraphs, and therefore the tool is
fed with sentences. We also include a surrounding
context of two additional sentences, because the
classifier considers some contextual information.
Our hypothesis motivating the use of a
sentence-level subjectivity classifier is that in-
stances of subjective senses are more likely to be
in subjective sentences, and thus that sentence sub-
jectivity is an informative feature for the disam-
biguation of words having both subjective and ob-
jective senses.
For each ambiguous word, we perform two sep-
arate runs: one using the basic disambiguation
system described earlier, and another using the
subjectivity-aware system that includes the addi-
tional subjectivity feature. Table 3 shows the re-
sults obtained for these 20 nouns, including word
sense disambiguation accuracy for the two differ-
ent systems, the most frequent sense baseline, and
the subjectivity/objectivity split among the word
senses (according to Judge 1). The words in the
top half of the table are the ones that have both S
and O senses, and those in the bottom are the ones
that do not. If we were to use Judge 2?s tags in-
stead of Judge 1?s, only one word would change:
source would move from the top to the bottom of
the table.
1070
Sense Data Classifier
Word Senses subjectivity train test Baseline basic + subj.
Words with subjective senses
argument 5 3-S 2-O 221 111 49.4% 51.4% 54.1%
atmosphere 6 2-S 4-O 161 81 65.4% 65.4% 66.7%
difference 5 2-S 3-O 226 114 40.4% 54.4% 57.0%
difficulty 4 2-S 2-O 46 23 17.4% 47.8% 52.2%
image 7 2-S 5-O 146 74 36.5% 41.2% 43.2%
interest 7 1-S 5-O 1-B 185 93 41.9% 67.7% 68.8%
judgment 7 5-S 2-O 62 32 28.1% 40.6% 43.8%
plan 3 1-S 2-O 166 84 81.0% 81.0% 81.0%
sort 4 1-S 2-O 1-B 190 96 65.6% 66.7% 67.7%
source 9 1-S 8-O 64 32 40.6% 40.6% 40.6%
Average 46.6% 55.6% 57.5%
Words with no subjective senses
arm 6 6-O 266 133 82.0% 85.0% 84.2%
audience 4 4-O 200 100 67.0% 74.0% 74.0%
bank 10 10-O 262 132 62.6% 62.6% 62.6%
degree 7 5-O 2-B 256 128 60.9% 71.1% 71.1%
disc 4 4-O 200 100 38.0% 65.6% 66.4%
organization 7 7-O 112 56 64.3% 64.3% 64.3%
paper 7 7-O 232 117 25.6% 49.6% 48.0%
party 5 5-O 230 116 62.1% 62.9% 62.9%
performance 5 5-O 172 87 26.4% 34.5% 34.5%
shelter 5 5-O 196 98 44.9% 65.3% 65.3%
Average 53.3% 63.5% 63.3%
Average for all words 50.0% 59.5% 60.4%
Table 3: Word Sense Disambiguation with and
without subjectivity information, for the set of am-
biguous nouns in SENSEVAL-3
For the words that have both S and O senses,
the addition of the subjectivity feature alone can
bring a significant error rate reduction of 4.3%
(p < 0.05 paired t-test). Interestingly, no improve-
ments are observed for the words with no subjec-
tive senses; on the contrary, the addition of the
subjectivity feature results in a small degradation.
Overall for the entire set of ambiguous words, the
error reduction is measured at 2.2% (significant at
p < 0.1 paired t-test).
In almost all cases, the words with both S and O
senses show improvement, while the others show
small degradation or no change. This suggests that
if a subjectivity label is available for the words in
a lexical resource (e.g. using Algorithm 1 from
Section 4), such information can be used to decide
on using a subjectivity-aware system, thereby im-
proving disambiguation accuracy.
One of the exceptions is disc, which had a small
benefit, despite not having any subjective senses.
As it happens, the first sense of disc is phonograph
record.
phonograph record, phonograph recording, record, disk, disc,
platter ? (sound recording consisting of a disc with continu-
ous grooves; formerly used to reproduce music by rotating
while a phonograph needle tracked in the grooves)
The improvement can be explained by observ-
ing that many of the training and test sentences
containing this sense are labeled subjective by the
classifier, and indeed this sense frequently occurs
in subjective sentences such as ?This is anyway a
stunning disc.?
Another exception is the noun plan, which did
not benefit from the subjectivity feature, although
it does have a subjective sense. This can perhaps
be explained by the data set for this word, which
seems to be particularly difficult, as the basic clas-
sifier itself could not improve over the most fre-
quent sense baseline.
The other word that did not benefit from the
subjectivity feature is the noun source, for which
its only subjective sense did not appear in the
sense-annotated data, leading therefore to an ?ob-
jective only? set of examples.
6 Conclusion and Future Work
The questions posed in the introduction concern-
ing the possible interaction between subjectivity
and word sense found answers throughout the pa-
per. As it turns out, a correlation can indeed be
established between these two semantic properties
of language.
Addressing the first question of whether subjec-
tivity is a property that can be assigned to word
senses, we showed that good agreement (?=0.74)
can be achieved between human annotators la-
beling the subjectivity of senses. When uncer-
tain cases are removed, the ? value is even higher
(0.90). Moreover, the automatic subjectivity scor-
ing mechanism that we devised was able to suc-
cessfully assign subjectivity labels to senses, sig-
nificantly outperforming an ?informed? baseline
associated with the task. While much work re-
mains to be done, this first attempt has proved
the feasibility of correctly assigning subjectivity
labels to the fine-grained level of word senses.
The second question was also positively an-
swered: the quality of a word sense disambigua-
tion system can be improved with the addition
of subjectivity information. Section 5 provided
evidence that automatic subjectivity classification
may improve word sense disambiguation perfor-
mance, but mainly for words with both subjective
and objective senses. As we saw, performance
may even degrade for words that do not. Tying
the pieces of this paper together, once the senses
in a dictionary have been assigned subjectivity la-
bels, a word sense disambiguation system could
consult them to decide whether it should consider
or ignore the subjectivity feature.
There are several other ways our results could
impact future work. Subjectivity labels would
be a useful source of information when manually
augmenting the lexical knowledge in a dictionary,
1071
e.g., when choosing hypernyms for senses or de-
ciding which senses to eliminate when defining a
coarse-grained sense inventory (if there is a sub-
jective sense, at least one should be retained).
Adding subjectivity labels to WordNet could
also support automatic subjectivity analysis. First,
the input corpus could be sense tagged and the
subjectivity labels of the assigned senses could be
exploited by a subjectivity recognition tool. Sec-
ond, a number of methods for subjectivity or sen-
timent analysis start with a set of seed words and
then search through WordNet to find other subjec-
tive words (Kamps and Marx, 2002; Yu and Hatzi-
vassiloglou, 2003; Hu and Liu, 2004; Kim and
Hovy, 2004; Esuli and Sebastiani, 2005). How-
ever, such searches may veer off course down ob-
jective paths. The subjectivity labels assigned to
senses could be consulted to keep the search trav-
eling along subjective paths.
Finally, there could be different strategies
for exploiting subjectivity annotations and word
sense. While the current setting considered a
pipeline approach, where the output of a subjec-
tivity annotation system was fed to the input of a
method for semantic disambiguation, future work
could also consider the role of word senses as a
possible way of improving subjectivity analysis,
or simultaneous annotations of subjectivity and
word meanings, as done in the past for other lan-
guage processing problems.
Acknowledgments We would like to thank
Theresa Wilson for annotating senses, and the
anonymous reviewers for their helpful comments.
This work was partially supported by ARDA
AQUAINT and by the NSF (award IIS-0208798).
References
K. Dave, S. Lawrence, and D. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proc.
WWW-2003, Budapest, Hungary. Available at
http://www2003.org.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-2005.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proc.
ACL-97, pages 174?181.
D. Heise. 2001. Project magellan: Collecting cross-
cultural affective meanings via the internet. Elec-
tronic Journal of Sociology, 5(3).
M. Hu and B. Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of ACM
SIGKDD.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical tax onomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
J. Kamps and M. Marx. 2002. Words with attitude. In
Proc. 1st International WordNet Conference.
S.M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. Coling 2004.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation
of knowledge sources and learning algo rithms for
word sense disambiguation. In Proc. EMNLP 2002.
D. Lewis. 1992. An evaluation of phrasal and clus-
tered representations on a text categorization task.
In Proceedings of ACM SIGIR.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL,
Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant senses in untagged text.
In Proc. ACL 2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Proc.
ACL/SIGLEX Senseval-3.
G. Miller. 1995. Wordnet: A lexical database. Com-
munication of the ACM, 38(11):39?41.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word se nse: An
examplar-based approach. In Proc. ACL 1996.
B. Pang and L. Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
riza tion based on minimum cuts. In Proc. ACL
2004.
A. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc. of
HLT/EMNLP 2005.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. EMNLP
2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting
subjectivity classification to improve information ex
traction. In Proc. AAAI 2005.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
perspective question answering using the opqa cor-
pus. In Proc. HLT/EMNLP 2005.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL 2002.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
J. Wiebe. 2000. Learning subjective adjectives from
corpora. In Proc. AAAI 2000.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natu ral language processing tech-
niques. In Proc. ICDM 2003.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP 2003.
1072
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976?983,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Multilingual Subjective Language via Cross-Lingual Projections
Rada Mihalcea and Carmen Banea
Department of Computer Science
University of North Texas
rada@cs.unt.edu, carmenb@unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
This paper explores methods for generating
subjectivity analysis resources in a new lan-
guage by leveraging on the tools and re-
sources available in English. Given a bridge
between English and the selected target lan-
guage (e.g., a bilingual dictionary or a par-
allel corpus), the methods can be used to
rapidly create tools for subjectivity analysis
in the new language.
1 Introduction
There is growing interest in the automatic extraction
of opinions, emotions, and sentiments in text (sub-
jectivity), to provide tools and support for various
natural language processing applications. Most of
the research to date has focused on English, which
is mainly explained by the availability of resources
for subjectivity analysis, such as lexicons and man-
ually labeled corpora.
In this paper, we investigate methods to auto-
matically generate resources for subjectivity analy-
sis for a new target language by leveraging on the
resources and tools available for English, which in
many cases took years of work to complete. Specif-
ically, through experiments with cross-lingual pro-
jection of subjectivity, we seek answers to the fol-
lowing questions.
First, can we derive a subjectivity lexicon for a
new language using an existing English subjectivity
lexicon and a bilingual dictionary? Second, can we
derive subjectivity-annotated corpora in a new lan-
guage using existing subjectivity analysis tools for
English and a parallel corpus? Finally, third, can we
build tools for subjectivity analysis for a new target
language by relying on these automatically gener-
ated resources?
We focus our experiments on Romanian, selected
as a representative of the large number of languages
that have only limited text processing resources de-
veloped to date. Note that, although we work with
Romanian, the methods described are applicable to
any other language, as in these experiments we (pur-
posely) do not use any language-specific knowledge
of the target language. Given a bridge between En-
glish and the selected target language (e.g., a bilin-
gual dictionary or a parallel corpus), the methods
can be applied to other languages as well.
After providing motivations, we present two ap-
proaches to developing sentence-level subjectivity
classifiers for a new target language. The first uses a
subjectivity lexicon translated from an English one.
The second uses an English subjectivity classifier
and a parallel corpus to create target-language train-
ing data for developing a statistical classifier.
2 Motivation
Automatic subjectivity analysis methods have been
used in a wide variety of text processing applica-
tions, such as tracking sentiment timelines in on-
line forums and news (Lloyd et al, 2005; Balog
et al, 2006), review classification (Turney, 2002;
Pang et al, 2002), mining opinions from product
reviews (Hu and Liu, 2004), automatic expressive
text-to-speech synthesis (Alm et al, 2005), text se-
mantic analysis (Wiebe and Mihalcea, 2006; Esuli
and Sebastiani, 2006), and question answering (Yu
and Hatzivassiloglou, 2003).
976
While much recent work in subjectivity analysis
focuses on sentiment (a type of subjectivity, namely
positive and negative emotions, evaluations, and
judgments), we opt to focus on recognizing subjec-
tivity in general, for two reasons.
First, even when sentiment is the desired focus,
researchers in sentiment analysis have shown that
a two-stage approach is often beneficial, in which
subjective instances are distinguished from objec-
tive ones, and then the subjective instances are fur-
ther classified according to polarity (Yu and Hatzi-
vassiloglou, 2003; Pang and Lee, 2004; Wilson et
al., 2005; Kim and Hovy, 2006). In fact, the prob-
lem of distinguishing subjective versus objective in-
stances has often proved to be more difficult than
subsequent polarity classification, so improvements
in subjectivity classification promise to positively
impact sentiment classification. This is reported in
studies of manual annotation of phrases (Takamura
et al, 2006), recognizing contextual polarity of ex-
pressions (Wilson et al, 2005), and sentiment tag-
ging of words and word senses (Andreevskaia and
Bergler, 2006; Esuli and Sebastiani, 2006).
Second, an NLP application may seek a wide
range of types of subjectivity attributed to a per-
son, such as their motivations, thoughts, and specu-
lations, in addition to their positive and negative sen-
timents. For instance, the opinion tracking system
Lydia (Lloyd et al, 2005) gives separate ratings for
subjectivity and sentiment. These can be detected
with subjectivity analysis but not by a method fo-
cused only on sentiment.
There is world-wide interest in text analysis appli-
cations. While work on subjectivity analysis in other
languages is growing (e.g., Japanese data are used in
(Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006)),
much of the work in subjectivity analysis has been
applied to English data. Creating corpora and lexical
resources for a new language is very time consum-
ing. In general, we would like to leverage resources
already developed for one language to more rapidly
create subjectivity analysis tools for a new one. This
motivates our exploration and use of cross-lingual
lexicon translations and annotation projections.
Most if not all work on subjectivity analysis has
been carried out in a monolingual framework. We
are not aware of multi-lingual work in subjectivity
analysis such as that proposed here, in which subjec-
tivity analysis resources developed for one language
are used to support developing resources in another.
3 A Lexicon-Based Approach
Many subjectivity and sentiment analysis tools rely
on manually or semi-automatically constructed lex-
icons (Yu and Hatzivassiloglou, 2003; Riloff and
Wiebe, 2003; Kim and Hovy, 2006). Given the suc-
cess of such techniques, the first approach we take
to generating a target-language subjectivity classi-
fier is to create a subjectivity lexicon by translating
an existing source language lexicon, and then build
a classifier that relies on the resulting lexicon.
Below, we describe the translation process and
discuss the results of an annotation study to assess
the quality of the translated lexicon. We then de-
scribe and evaluate a lexicon-based target-language
classifier.
3.1 Translating a Subjectivity Lexicon
The subjectivity lexicon we use is from Opinion-
Finder (Wiebe and Riloff, 2005), an English sub-
jectivity analysis system which, among other things,
classifies sentences as subjective or objective. The
lexicon was compiled from manually developed re-
sources augmented with entries learned from cor-
pora. It contains 6,856 unique entries, out of which
990 are multi-word expressions. The entries in the
lexicon have been labeled for part of speech, and for
reliability ? those that appear most often in subjec-
tive contexts are strong clues of subjectivity, while
those that appear less often, but still more often than
expected by chance, are labeled weak.
To perform the translation, we use two bilingual
dictionaries. The first is an authoritative English-
Romanian dictionary, consisting of 41,500 entries,1
which we use as the main translation resource for the
lexicon translation. The second dictionary, drawn
from the Universal Dictionary download site (UDP,
2007) consists of 4,500 entries written largely by
Web volunteer contributors, and thus is not error
free. We use this dictionary only for those entries
that do not appear in the main dictionary.
1Unique English entries, each with multiple Romanian
translations.
977
There were several challenges encountered in the
translation process. First, although the English sub-
jectivity lexicon contains inflected words, we must
use the lemmatized form in order to be able to trans-
late the entries using the bilingual dictionary. How-
ever, words may lose their subjective meaning once
lemmatized. For instance, the inflected form of
memories becomes memory. Once translated into
Romanian (as memorie), its main meaning is ob-
jective, referring to the power of retaining informa-
tion as in Iron supplements may improve a woman?s
memory.
Second, neither the lexicon nor the bilingual dic-
tionary provides information on the sense of the in-
dividual entries, and therefore the translation has to
rely on the most probable sense in the target lan-
guage. Fortunately, the bilingual dictionary lists the
translations in reverse order of their usage frequen-
cies. Nonetheless, the ambiguity of the words and
the translations still seems to represent an impor-
tant source of error. Moreover, the lexicon some-
times includes identical entries expressed through
different parts of speech, e.g., grudge has two sepa-
rate entries, for its noun and verb roles, respectively.
On the other hand, the bilingual dictionary does not
make this distinction, and therefore we have again
to rely on the ?most frequent? heuristic captured by
the translation order in the bilingual dictionary.
Finally, the lexicon includes a significant number
(990) of multi-word expressions that pose transla-
tion difficulties, sometimes because their meaning
is idiomatic, and sometimes because the multi-word
expression is not listed in the bilingual dictionary
and the translation of the entire phrase is difficult
to reconstruct from the translations of the individual
words. To address this problem, when a translation
is not found in the dictionary, we create one using
a word-by-word approach. These translations are
then validated by enforcing that they occur at least
three times on the Web, using counts collected from
the AltaVista search engine. The multi-word expres-
sions that are not validated in this process are dis-
carded, reducing the number of expressions from an
initial set of 990 to a final set of 264.
The final subjectivity lexicon in Romanian con-
tains 4,983 entries. Table 1 shows examples of en-
tries in the Romanian lexicon, together with their
corresponding original English form. The table
Romanian English attributes
??nfrumuset?a beautifying strong, verb
notabil notable weak, adj
plin de regret full of regrets strong, adj
sclav slaves weak, noun
Table 1: Examples of entries in the Romanian sub-
jectivity lexicon
also shows the reliability of the expression (weak or
strong) and the part of speech ? attributes that are
provided in the English subjectivity lexicon.
Manual Evaluation.
We want to assess the quality of the translated lexi-
con, and compare it to the quality of the original En-
glish lexicon. The English subjectivity lexicon was
evaluated in (Wiebe and Riloff, 2005) against a cor-
pus of English-language news articles manually an-
notated for subjectivity (the MPQA corpus (Wiebe et
al., 2005)). According to this evaluation, 85% of the
instances of the clues marked as strong and 71.5% of
the clues marked as weak are in subjective sentences
in the MPQA corpus.
Since there is no comparable Romanian corpus,
an alternate way to judge the subjectivity of a Ro-
manian lexicon entry is needed.
Two native speakers of Romanian annotated the
subjectivity of 150 randomly selected entries. Each
annotator independently read approximately 100 ex-
amples of each drawn from the Web, including a
large number from news sources. The subjectivity
of a word was consequently judged in the contexts
where it most frequently appears, accounting for its
most frequent meanings on the Web.
The tagset used for the annotations consists of
S(ubjective), O(bjective), and B(oth). A W(rong) la-
bel is also used to indicate a wrong translation. Table
2 shows the contingency table for the two annota-
tors? judgments on this data.
S O B W Total
S 53 6 9 0 68
O 1 27 1 0 29
B 5 3 18 0 26
W 0 0 0 27 27
Total 59 36 28 27 150
Table 2: Agreement on 150 entries in the Romanian
lexicon
Without counting the wrong translations, the
agreement is measured at 0.80, with a Kappa ? =
978
0.70, which indicates consistent agreement. After
the disagreements were reconciled through discus-
sions, the final set of 123 correctly translated entries
does include 49.6% (61) subjective entries, but fully
23.6% (29) were found in the study to have primar-
ily objective uses (the other 26.8% are mixed).
Thus, this study suggests that the Romanian sub-
jectivity clues derived through translation are less re-
liable than the original set of English clues. In sev-
eral cases, the subjectivity is lost in the translation,
mainly due to word ambiguity in either the source
or target language, or both. For instance, the word
fragile correctly translates into Romanian as fragil,
yet this word is frequently used to refer to breakable
objects, and it loses its subjective meaning of del-
icate. Other words, such as one-sided, completely
lose subjectivity once translated, as it becomes in
Romanian cu o singura latura?, meaning with only
one side (as of objects).
Interestingly, the reliability of clues in the English
lexicon seems to help preserve subjectivity. Out of
the 77 entries marked as strong, 11 were judged to be
objective in Romanian (14.3%), compared to 14 ob-
jective Romanian entries obtained from the 36 weak
English clues (39.0%).
3.2 Rule-based Subjectivity Classifier Using a
Subjectivity Lexicon
Starting with the Romanian lexicon, we developed
a lexical classifier similar to the one introduced by
(Riloff and Wiebe, 2003). At the core of this method
is a high-precision subjectivity and objectivity clas-
sifier that can label large amounts of raw text using
only a subjectivity lexicon. Their method is further
improved with a bootstrapping process that learns
extraction patterns. In our experiments, however, we
apply only the rule-based classification step, since
the extraction step cannot be implemented without
tools for syntactic parsing and information extrac-
tion not available in Romanian.
The classifier relies on three main heuristics to la-
bel subjective and objective sentences: (1) if two
or more strong subjective expressions occur in the
same sentence, the sentence is labeled Subjective;
(2) if no strong subjective expressions occur in a
sentence, and at most two weak subjective expres-
sions occur in the previous, current, and next sen-
tence combined, then the sentence is labeled Objec-
tive; (3) otherwise, if none of the previous rules ap-
ply, the sentence is labeled Unknown.
The quality of the classifier was evaluated on a
Romanian gold-standard corpus annotated for sub-
jectivity. Two native Romanian speakers (Ro1 and
Ro2) manually annotated the subjectivity of the sen-
tences of five randomly selected documents (504
sentences) from the Romanian side of an English-
Romanian parallel corpus, according to the anno-
tation scheme in (Wiebe et al, 2005). Agreement
between annotators was measured, and then their
differences were adjudicated. The baseline on this
data set is 54.16%, which can be obtained by as-
signing a default Subjective label to all sentences.
(More information about the corpus and annotations
are given in Section 4 below, where agreement be-
tween English and Romanian aligned sentences is
also assessed.)
As mentioned earlier, due to the lexicon projec-
tion process that is performed via a bilingual dictio-
nary, the entries in our Romanian subjectivity lex-
icon are in a lemmatized form. Consequently, we
also lemmatize the gold-standard corpus, to allow
for the identification of matches with the lexicon.
For this purpose, we use the Romanian lemmatizer
developed by Ion and Tufis? (Ion, 2007), which has
an estimated accuracy of 98%.2
Table 3 shows the results of the rule-based classi-
fier. We show the precision, recall, and F-measure
independently measured for the subjective, objec-
tive, and all sentences. We also evaluated a vari-
ation of the rule-based classifier that labels a sen-
tence as objective if there are at most three weak ex-
pressions in the previous, current, and next sentence
combined, which raises the recall of the objective
classifier. Our attempts to increase the recall of the
subjective classifier all resulted in significant loss in
precision, and thus we kept the original heuristic.
In its original English implementation, this sys-
tem was proposed as being high-precision but low
coverage. Evaluated on the MPQA corpus, it has
subjective precision of 90.4, subjective recall of
34.2, objective precision of 82.4, and objective re-
call of 30.7; overall, precision is 86.7 and recall is
32.6 (Wiebe and Riloff, 2005). We see a similar be-
havior on Romanian for subjective sentences. The
subjective precision is good, albeit at the cost of low
2Dan Tufis?, personal communication.
979
Measure Subjective Objective All
subj = at least two strong; obj = at most two weak
Precision 80.00 56.50 62.59
Recall 20.51 48.91 33.53
F-measure 32.64 52.52 43.66
subj = at least two strong; obj = at most three weak
Precision 80.00 56.85 61.94
Recall 20.51 61.03 39.08
F-measure 32.64 58.86 47.93
Table 3: Evaluation of the rule-based classifier
recall, and thus the classifier could be used to har-
vest subjective sentences from unlabeled Romanian
data (e.g., for a subsequent bootstrapping process).
The system is not very effective for objective classi-
fication, however. Recall that the objective classifier
relies on the weak subjectivity clues, for which the
transfer of subjectivity in the translation process was
particularly low.
4 A Corpus-Based Approach
Given the low number of subjective entries found in
the automatically generated lexicon and the subse-
quent low recall of the lexical classifier, we decided
to also explore a second, corpus-based approach.
This approach builds a subjectivity-annotated cor-
pus for the target language through projection, and
then trains a statistical classifier on the resulting
corpus (numerous statistical classifiers have been
trained for subjectivity or sentiment classification,
e.g., (Pang et al, 2002; Yu and Hatzivassiloglou,
2003)). The hypothesis is that we can eliminate
some of the ambiguities (and consequent loss of sub-
jectivity) observed during the lexicon translation by
accounting for the context of the ambiguous words,
which is possible in a corpus-based approach. Ad-
ditionally, we also hope to improve the recall of the
classifier, by addressing those cases not covered by
the lexicon-based approach.
In the experiments reported in this section, we
use a parallel corpus consisting of 107 documents
from the SemCor corpus (Miller et al, 1993) and
their manual translations into Romanian.3 The cor-
pus consists of roughly 11,000 sentences, with ap-
proximately 250,000 tokens on each side. It is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others.
3The translation was carried out by a Romanian native
speaker, student in a department of ?Foreign Languages and
Translations? in Romania.
Below, we begin with a manual annotation study
to assess the quality of annotation and preservation
of subjectivity in translation. We then describe the
automatic construction of a target-language training
set, and evaluate a classifier trained on that data.
Annotation Study.
We start by performing an agreement study meant
to determine the extent to which subjectivity is pre-
served by the cross-lingual projections. In the study,
three annotators ? one native English speaker (En)
and two native Romanian speakers (Ro1 and Ro2) ?
first trained on 3 randomly selected documents (331
sentences). They then independently annotated the
subjectivity of the sentences of two randomly se-
lected documents from the parallel corpus, account-
ing for 173 aligned sentence pairs. The annotators
had access exclusively to the version of the sen-
tences in their language, to avoid any bias that could
be introduced by seeing the translation in the other
language.
Note that the Romanian annotations (after all dif-
ferences between the Romanian annotators were ad-
judicated) of all 331 + 173 sentences make up the
gold standard corpus used in the experiments re-
ported in Sections 3.2 and 4.1.
Before presenting the results of the annotation
study, we give some examples. The following are
English subjective sentences and their Romanian
translations (the subjective elements are shown in
bold).
[en] The desire to give Broglio as many starts as
possible.
[ro] Dorint?a de a-i da lui Broglio ca?t mai multe
starturi posibile.
[en] Suppose he did lie beside Lenin, would it be
permanent ?
[ro] Sa? presupunem ca? ar fi as?ezat ala?turi de Lenin,
oare va fi pentru totdeauna?
The following are examples of objective parallel
sentences.
[en]The Pirates have a 9-6 record this year and the
Redbirds are 7-9.
[ro] Pirat?ii au un palmares de 9 la 6 anul acesta si
Pa?sa?rile Ros?ii au 7 la 9.
[en] One of the obstacles to the easy control of a
2-year old child is a lack of verbal communication.
[ro] Unul dintre obstacolele ??n controlarea unui
copil de 2 ani este lipsa comunica?rii verbale.
980
The annotators were trained using the MPQA
annotation guidelines (Wiebe et al, 2005). The
tagset consists of S(ubjective), O(bjective) and
U(ncertain). For the U tags, a class was also given;
OU means, for instance, that the annotator is uncer-
tain but she is leaning toward O. Table 4 shows the
pairwise agreement figures and the Kappa (?) calcu-
lated for the three annotators. The table also shows
the agreement when the borderline uncertain cases
are removed.
all sentences Uncertain removed
pair agree ? agree ? (%) removed
Ro1 & Ro2 0.83 0.67 0.89 0.77 23
En & Ro1 0.77 0.54 0.86 0.73 26
En & Ro2 0.78 0.55 0.91 0.82 20
Table 4: Agreement on the data set of 173 sentences.
Annotations performed by three annotators: one na-
tive English speaker (En) and two native Romanian
speakers (Ro1 and Ro2)
When all the sentences are included, the agree-
ment between the two Romanian annotators is mea-
sured at 0.83 (? = 0.67). If we remove the border-
line cases where at least one annotator?s tag is Un-
certain, the agreement rises to 0.89 with ? = 0.77.
These figures are somewhat lower than the agree-
ment observed during previous subjectivity anno-
tation studies conducted on English (Wiebe et al,
2005) (the annotators were more extensively trained
in those studies), but they nonetheless indicate con-
sistent agreement.
Interestingly, when the agreement is conducted
cross-lingually between an English and a Romanian
annotator, the agreement figures, although some-
what lower, are comparable. In fact, once the
Uncertain tags are removed, the monolingual and
cross-lingual agreement and ? values become al-
most equal, which suggests that in most cases the
sentence-level subjectivity is preserved.
The disagreements were reconciled first between
the labels assigned by the two Romanian annotators,
followed by a reconciliation between the resulting
Romanian ?gold-standard? labels and the labels as-
signed by the English annotator. In most cases, the
disagreement across the two languages was found
to be due to a difference of opinion about the sen-
tence subjectivity, similar to the differences encoun-
tered in monolingual annotations. However, there
are cases where the differences are due to the sub-
jectivity being lost in the translation. Sometimes,
this is due to several possible interpretations for the
translated sentence. For instance, the following sen-
tence:
[en] They honored the battling Billikens last night.
[ro] Ei i-au celebrat pe Billikens seara trecuta?.
is marked as Subjective in English (in context, the
English annotator interpreted honored as referring
to praises of the Billikens). However, the Romanian
translation of honored is celebrat which, while cor-
rect as a translation, has the more frequent interpre-
tation of having a party. The two Romanian annota-
tors chose this interpretation, which correspondingly
lead them to mark the sentence as Objective.
In other cases, in particular when the subjectivity
is due to figures of speech such as irony, the trans-
lation sometimes misses the ironic aspects. For in-
stance, the translation of egghead was not perceived
as ironic by the Romanian annotators, and conse-
quently the following sentence labeled Subjective in
English is annotated as Objective in Romanian.
[en] I have lived for many years in a Connecti-
cut commuting town with a high percentage of [...]
business executives of egghead tastes.
[ro] Am tra?it mult?i ani ??ntr-un oras? din apropiere de
Connecticut ce avea o mare proport?ie de [...] oa-
meni de afaceri cu gusturi intelectuale.
4.1 Translating a Subjectivity-Annotated
Corpus and Creating a Machine Learning
Subjectivity Classifier
To further validate the corpus-based projection of
subjectivity, we developed a subjectivity classifier
trained on Romanian subjectivity-annotated corpora
obtained via cross-lingual projections.
Ideally, one would generate an annotated Roma-
nian corpus by translating English documents man-
ually annotated for subjectivity such as the MPQA
corpus. Unfortunately, the manual translation of this
corpus would be prohibitively expensive, both time-
wise and financially. The other alternative ? auto-
matic machine translation ? has not yet reached a
level that would enable the generation of a high-
quality translated corpus. We therefore decided to
use a different approach where we automatically
annotate the English side of an existing English-
Romanian corpus, and subsequently project the an-
notations onto the Romanian side of the parallel cor-
981
Precision Recall F-measure
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 5: Precision, recall, and F-measure for the
two OpinionFinder classifiers, as measured on the
MPQA corpus.
pus across the sentence-level alignments available in
the corpus.
For the automatic subjectivity annotations, we
generated two sets of the English-side annotations,
one using the high-precision classifier and one using
the high-coverage classifier available in the Opinion-
Finder tool. The high-precision classifier in Opin-
ionFinder uses the clues of the subjectivity lexicon
to harvest subjective and objective sentences from
a large amount of unannotated text; this data is then
used to automatically identify a set of extraction pat-
terns, which are then used iteratively to identify a
larger set of subjective and objective sentences.
In addition, in OpinionFinder, the high-precision
classifier is used to produce an English labeled data
set for training, which is used to generate its Naive
Bayes high-coverage subjectivity classifier. Table
5 shows the performance of the two classifiers on
the MPQA corpus as reported in (Wiebe and Riloff,
2005). Note that 55% of the sentences in the MPQA
corpus are subjective ? which represents the baseline
for this data set.
The two OpinionFinder classifiers are used to la-
bel the training corpus. After removing the 504 test
sentences, we are left with 10,628 sentences that
are automatically annotated for subjectivity. Table
6 shows the number of subjective and objective sen-
tences obtained with each classifier.
Classifier Subjective Objective All
high-precision 1,629 2,334 3,963
high-coverage 5,050 5,578 10,628
Table 6: Subjective and objective training sentences
automatically annotated with OpinionFinder.
Next, the OpinionFinder annotations are pro-
jected onto the Romanian training sentences, which
are then used to develop a probabilistic classifier for
the automatic labeling of subjectivity in Romanian
sentences.
Similar to, e.g., (Pang et al, 2002), we use a
Naive Bayes algorithm trained on word features co-
occurring with the subjective and the objective clas-
sifications. We assume word independence, and we
use a 0.3 cut-off for feature selection. While re-
cent work has also considered more complex syn-
tactic features, we are not able to generate such fea-
tures for Romanian as they require tools currently
not available for this language.
We create two classifiers, one trained on each
data set. The quality of the classifiers is evaluated
on the 504-sentence Romanian gold-standard corpus
described above. Recall that the baseline on this data
set is 54.16%, the percentage of sentences in the cor-
pus that are subjective. Table 7 shows the results.
Subjective Objective All
projection source: OF high-precision classifier
Precision 65.02 69.62 64.48
Recall 82.41 47.61 64.48
F-measure 72.68 56.54 64.68
projection source: OF high-coverage classifier
Precision 66.66 70.17 67.85
Recall 81.31 52.17 67.85
F-measure 72.68 56.54 67.85
Table 7: Evaluation of the machine learning classi-
fier using training data obtained via projections from
data automatically labeled by OpinionFinder (OF).
Our best classifier has an F-measure of 67.85,
and is obtained by training on projections from
the high-coverage OpinionFinder annotations. Al-
though smaller than the 74.70 F-measure obtained
by the English high-coverage classifier (see Ta-
ble 5), the result appears remarkable given that no
language-specific Romanian information was used.
The overall results obtained with the machine
learning approach are considerably higher than
those obtained from the rule-based classifier (except
for the precision of the subjective sentences). This
is most likely due to the lexicon translation process,
which as mentioned in the agreement study in Sec-
tion 3.1, leads to ambiguity and loss of subjectivity.
Instead, the corpus-based translations seem to better
account for the ambiguity of the words, and the sub-
jectivity is generally preserved in the sentence trans-
lations.
5 Conclusions
In this paper, we described two approaches to gener-
ating resources for subjectivity annotations for a new
982
language, by leveraging on resources and tools avail-
able for English. The first approach builds a target
language subjectivity lexicon by translating an exist-
ing English lexicon using a bilingual dictionary. The
second generates a subjectivity-annotated corpus in
a target language by projecting annotations from an
automatically annotated English corpus.
These resources were validated in two ways.
First, we carried out annotation studies measuring
the extent to which subjectivity is preserved across
languages in each of the two resources. These stud-
ies show that only a relatively small fraction of the
entries in the lexicon preserve their subjectivity in
the translation, mainly due to the ambiguity in both
the source and the target languages. This is con-
sistent with observations made in previous work
that subjectivity is a property associated not with
words, but with word meanings (Wiebe and Mihal-
cea, 2006). In contrast, the sentence-level subjectiv-
ity was found to be more reliably preserved across
languages, with cross-lingual inter-annotator agree-
ments comparable to the monolingual ones.
Second, we validated the two automatically gen-
erated subjectivity resources by using them to build
a tool for subjectivity analysis in the target language.
Specifically, we developed two classifiers: a rule-
based classifier that relies on the subjectivity lexi-
con described in Section 3.1, and a machine learn-
ing classifier trained on the subjectivity-annotated
corpus described in Section 4.1. While the highest
precision for the subjective classification is obtained
with the rule-based classifier, the overall best result
of 67.85 F-measure is due to the machine learning
approach. This result is consistent with the anno-
tation studies, showing that the corpus projections
preserve subjectivity more reliably than the lexicon
translations.
Finally, neither one of the classifiers relies on
language-specific information, but rather on knowl-
edge obtained through projections from English. A
similar method can therefore be used to derive tools
for subjectivity analysis in other languages.
References
Alina Andreevskaia and Sabine Bergler. Mining wordnet for
fuzzy sentiment: Sentiment tag extraction from WordNet
glosses. In Proceedings of EACL 2006.
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005.
Emotions from text: Machine learning for text-based emo-
tion prediction. In Proceedings of HLT/EMNLP 2005.
Krisztian Balog, Gilad Mishne, and Maarten de Rijke. 2006.
Why are they excited? identifying and explaining spikes in
blog mood levels. In EACL-2006.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determining term
subjectivity and term orientation for opinion mining. In Pro-
ceedings the EACL 2006.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD.
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, and
Ruzhan Lu. 2005. A new method for sentiment classifi-
cation in text retrieval. In Proceedings of IJCNLP.
Radu Ion. 2007. Methods for automatic semantic disambigua-
tion. Applications to English and Romanian. Ph.D. thesis,
The Romanian Academy, RACAI.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of EMNLP 2006.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of HLT/NAACL
2006.
Levon Lloyd, Dimitrios Kechagias, and Steven Skiena. 2005.
Lydia: A system for large-scale news analysis. In Proceed-
ings of SPIRE 2005.
George Miller, Claudia Leacock, Tangee Randee, and Ross
Bunker. 1993. A semantic concordance. In Proceedings
of the DARPA Workshop on Human Language Technology.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP 2002.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of EMNLP
2003.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2006.
Latent variable models for semantic orientations of phrases.
In Proceedings of EACL 2006.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL 2002.
Universal Dictionary. 2007. Available at
www.dicts.info/uddl.php.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of COLING-ACL 2006.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing 2005 (invited paper). Available at
www.cs.pitt.edu/mpqarequest.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2/3):164?
210. Available at www.cs.pitt.edu/mpqa.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of HLT/EMNLP 2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP 2003.
983
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 226?234,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Recognizing Stances in Online Debates
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper presents an unsupervised opin-
ion analysis method for debate-side clas-
sification, i.e., recognizing which stance a
person is taking in an online debate. In
order to handle the complexities of this
genre, we mine the web to learn associa-
tions that are indicative of opinion stances
in debates. We combine this knowledge
with discourse information, and formu-
late the debate side classification task as
an Integer Linear Programming problem.
Our results show that our method is sub-
stantially better than challenging baseline
methods.
1 Introduction
This paper presents a method for debate-side clas-
sification, i.e., recognizing which stance a per-
son is taking in an online debate posting. In on-
line debate forums, people debate issues, express
their preferences, and argue why their viewpoint is
right. In addition to expressing positive sentiments
about one?s preference, a key strategy is also to
express negative sentiments about the other side.
For example, in the debate ?which mobile phone is
better: iPhone or Blackberry,? a participant on the
iPhone side may explicitly assert and rationalize
why the iPhone is better, and, alternatively, also ar-
gue why the Blackberry is worse. Thus, to recog-
nize stances, we need to consider not only which
opinions are positive and negative, but also what
the opinions are about (their targets).
Participants directly express their opinions,
such as ?The iPhone is cool,? but, more often, they
mention associated aspects. Some aspects are par-
ticular to one topic (e.g., Active-X is part of IE
but not Firefox), and so distinguish between them.
But even an aspect the topics share may distin-
guish between them, because people who are pos-
itive toward one topic may value that aspect more.
For example, both the iPhone and Blackberry have
keyboards, but we observed in our corpus that pos-
itive opinions about the keyboard are associated
with the pro Blackberry stance. Thus, we need to
find distinguishing aspects, which the topics may
or may not share.
Complicating the picture further, participants
may concede positive aspects of the opposing is-
sue or topic, without coming out in favor of it,
and they may concede negative aspects of the is-
sue or topic they support. For example, in the fol-
lowing sentence, the speaker says positive things
about the iPhone, even though he does not pre-
fer it: ?Yes, the iPhone may be cool to take it out
and play with and show off, but past that, it offers
nothing.? Thus, we need to consider discourse re-
lations to sort out which sentiments in fact reveal
the writer?s stance, and which are merely conces-
sions.
Many opinion mining approaches find negative
and positive words in a document, and aggregate
their counts to determine the final document po-
larity, ignoring the targets of the opinions. Some
work in product review mining finds aspects of a
central topic, and summarizes opinions with re-
spect to these aspects. However, they do not find
distinguishing factors associated with a preference
for a stance. Finally, while other opinion anal-
ysis systems have considered discourse informa-
tion, they have not distinguished between conces-
sionary and non-concessionary opinions when de-
termining the overall stance of a document.
This work proposes an unsupervised opinion
analysis method to address the challenges de-
scribed above. First, for each debate side, we mine
the web for opinion-target pairs that are associated
with a preference for that side. This information
is employed, in conjunction with discourse infor-
mation, in an Integer Linear Programming (ILP)
framework. This framework combines the individ-
ual pieces of information to arrive at debate-side
226
classifications of posts in online debates.
The remainder of this paper is organized as fol-
lows. We introduce our debate genre in Section 2
and describe our method in Section 3. We present
the experiments in Section 4 and analyze the re-
sults in Section 5. Related work is in Section 6,
and the conclusions are in Section 7.
2 The Debate Genre
In this section, we describe our debate data,
and elaborate on characteristic ways of express-
ing opinions in this genre. For our current
work, we use the online debates from the website
http://www.convinceme.net.1
In this work, we deal only with dual-sided,
dual-topic debates about named entities, for ex-
ample iPhone vs. Blackberry, where topic1 =
iPhone, topic2 =Blackberry, side1 = pro-iPhone,
and side2=pro-Blackberry.
Our test data consists of posts of 4 debates:
Windows vs. Mac, Firefox vs. Internet Explorer,
Firefox vs. Opera, and Sony Ps3 vs. Nintendo
Wii. The iPhone vs. Blackberry debate and two
other debates, were used as development data.
Given below are examples of debate posts. Post
1 is taken from the iPhone vs. Blackberry debate,
Post 2 is from the Firefox vs. Internet Explorer
debate, and Post 3 is from the Windows vs. Mac
debate:
(1) While the iPhone may appeal to younger
generations and the BB to older, there is no
way it is geared towards a less rich popula-
tion. In fact it?s exactly the opposite. It?s a
gimmick. The initial purchase may be half
the price, but when all is said and done you
pay at least $200 more for the 3g.
(2) In-line spell check...helps me with big
words like onomatopoeia
(3) Apples are nice computers with an excep-
tional interface. Vista will close the gap on
the interface some but Apple still has the
prettiest, most pleasing interface and most
likely will for the next several years.
2.1 Observations
As described in Section 1, the debate genre poses
significant challenges to opinion analysis. This
1http://www.forandagainst.com and
http://www.createdebate.com are other similar debating
websites.
subsection elaborates upon some of the complexi-
ties.
Multiple polarities to argue for a side. Debate
participants, in advocating their choice, switch
back and forth between their opinions towards the
sides. This makes it difficult for approaches that
use only positive and negative word counts to de-
cide which side the post is on. Posts 1 and 3 illus-
trate this phenomenon.
Sentiments towards both sides (topics) within a
single post. The above phenomenon gives rise
to an additional problem: often, conflicting sides
(and topics) are addressed within the same post,
sometimes within the same sentence. The second
sentence of Post 3 illustrates this, as it has opinions
about both Windows and Mac.
Differentiating aspects and personal prefer-
ences. People seldom repeatedly mention the
topic/side; they show their evaluations indirectly,
by evaluating aspects of each topic/side. Differen-
tiating aspects determine the debate-post?s side.
Some aspects are unique to one side/topic or the
other, e.g., ?3g? in Example 1 and ?inline spell
check? in Example 2. However, the debates are
about topics that belong to the same domain and
which therefore share many aspects. Hence, a
purely ontological approach of finding ?has-a? and
?is-a? relations, or an approach looking only for
product specifications, would not be sufficient for
finding differentiating features.
When the two topics do share an aspect (e.g., a
keyboard in the iPhone vs. Blackberry debate), the
writer may perceive it to be more positive for one
than the other. And, if the writer values that as-
pect, it will influence his or her overall stance. For
example, many people prefer the Blackberry key-
board over the iPhone keyboard; people to whom
phone keyboards are important are more likely to
prefer the Blackberry.
Concessions. While debating, participants often
refer to and acknowledge the viewpoints of the op-
posing side. However, they do not endorse this ri-
val opinion. Uniform treatment of all opinions in
a post would obviously cause errors in such cases.
The first sentence of Example 1 is an instance of
this phenomenon. The participant concedes that
the iPhone appeals to young consumers, but this
positive opinion is opposite to his overall stance.
227
DIRECT OBJECT Rule: dobj(opinion, target)
In words: The target is the direct object of the opinion
Example: I loveopinion1 Firefoxtarget1 and defendedopinion2 ittarget2
NOMINAL SUBJECT Rule: nsubj(opinion, target)
In words: The target is the subject of the opinion
Example: IEtarget breaksopinion with everything.
ADJECTIVAL MODIFIER Rule: amod(target, opinion)
In words: The opinion is an adjectival modifier of the target
Example: The annoyingopinion popuptarget
PREPOSITIONAL OBJECT Rule: if prep(target1,IN) ? pobj(IN, target2)
In words: The prepositional object of a known target is also a target of the same opinion
Example: The annoyingopinion popuptarget1 in IEtarget2 (?popup? and ?IE? are targets of ?annoying?)
RECURSIVE MODIFIERS Rule: if conj(adj2, opinionadj1) ? amod(target, adj2)
In words: If the opinion is an adjective (adj1) and it is conjoined with another adjective (adj2),
then the opinion is tied to what adj2 modifies
Example: It is a powerfulopinion(adj1) and easyopinion(adj2) applicationtarget
(?powerful? is attached to the target ?application? via the adjective ?easy?)
Table 1: Examples of syntactic rules for finding targets of opinions
3 Method
We propose an unsupervised approach to classify-
ing the stance of a post in a dual-topic debate. For
this, we first use a web corpus to learn preferences
that are likely to be associated with a side. These
learned preferences are then employed in conjunc-
tion with discourse constraints to identify the side
for a given post.
3.1 Finding Opinions and Pairing them with
Targets
We need to find opinions and pair them with tar-
gets, both to mine the web for general preferences
and to classify the stance of a debate post. We use
straightforward methods, as these tasks are not the
focus of this paper.
To find opinions, we look up words in a sub-
jectivity lexicon: all instances of those words are
treated as opinions. An opinion is assigned the
prior polarity that is listed for that word in the lex-
icon, except that, if the prior polarity is positive or
negative, and the instance is modified by a nega-
tion word (e.g., ?not?), then the polarity of that in-
stance is reversed. We use the subjectivity lexicon
of (Wilson et al, 2005),2 which contains approxi-
mately 8000 words which may be used to express
opinions. Each entry consists of a subjective word,
its prior polarity (positive (+), negative (?), neu-
tral (?)), morphological information, and part of
speech information.
To pair opinions with targets, we built a rule-
based system based on dependency parse informa-
tion. The dependency parses are obtained using
2Available at http://www.cs.pitt.edu/mpqa.
the Stanford parser.3 We developed the syntactic
rules on separate data that is not used elsewhere in
this paper. Table 1 illustrates some of these rules.
Note that the rules are constructed (and explained
in Table 1) with respect to the grammatical relation
notations of the Stanford parser. As illustrated in
the table, it is possible for an opinion to have more
than one target. In such cases, the single opin-
ion results in multiple opinion-target pairs, one for
each target.
Once these opinion-target pairs are created, we
mask the identity of the opinion word, replacing
the word with its polarity. Thus, the opinion-
target pair is converted to a polarity-target pair.
For instance, ?pleasing-interface? is converted to
interface+. This abstraction is essential for han-
dling the sparseness of the data.
3.2 Learning aspects and preferences from
the web
We observed in our development data that people
highlight the aspects of topics that are the bases
for their stances, both positive opinions toward as-
pects of the preferred topic, and negative opinions
toward aspects of the dispreferred one. Thus, we
decided to mine the web for aspects associated
with a side in the debate, and then use that infor-
mation to recognize the stances expressed in indi-
vidual posts.
Previous work mined web data for aspects as-
sociated with topics (Hu and Liu, 2004; Popescu
et al, 2005). In our work, we search for aspects
associated with a topic, but particularized to po-
larity. Not all aspects associated with a topic are
3http://nlp.stanford.edu/software/lex-parser.shtml.
228
side1 (pro-iPhone) side2 (pro-blackberry)
termp P (iPhone+|termp) P (blackberry?|termp) P (iPhone?|termp) P (blackberry+|termp)
storm+ 0.227 0.068 0.022 0.613
storm? 0.062 0.843 0.06 0.03
phone+ 0.333 0.176 0.137 0.313
e-mail+ 0 0.333 0.166 0.5
ipod+ 0.5 0 0.33 0
battery? 0 0 0.666 0.333
network? 0.333 0 0.666 0
keyboard+ 0.09 0.12 0 0.718
keyboard? 0.25 0.25 0.125 0.375
Table 2: Probabilities learned from the web corpus (iPhone vs. blackberry debate)
discriminative with respect to stance; we hypoth-
esized that, by including polarity, we would be
more likely to find useful associations. An aspect
may be associated with both of the debate top-
ics, but not, by itself, be discriminative between
stances toward the topics. However, opinions to-
ward that aspect might discriminate between them.
Thus, the basic unit in our web mining process is
a polarity-target pair. Polarity-target pairs which
explicitly mention one of the topics are used to an-
chor the mining process. Opinions about relevant
aspects are gathered from the surrounding context.
For each debate, we downloaded weblogs and
forums that talk about the main topics (corre-
sponding to the sides) of that debate. For ex-
ample, for the iPhone vs. Blackberry debate,
we search the web for pages containing ?iPhone?
and ?Blackberry.? We used the Yahoo search API
and imposed the search restriction that the pages
should contain both topics in the http URL. This
ensured that we downloaded relevant pages. An
average of 3000 documents were downloaded per
debate.
We apply the method described in Section
3.1 to the downloaded web pages. That is,
we find all instances of words in the lexicon,
extract their targets, and mask the words with
their polarities, yielding polarity-target pairs. For
example, suppose the sentence ?The interface
is pleasing? is in the corpus. The system
extracts the pair ?pleasing-interface,? which is
masked to ?positive-interface,? which we notate
as interface+. If the target in a polarity-target
pair happens to be one of the topics, we select the
polarity-target pairs in its vicinity for further pro-
cessing (the rest are discarded). The intuition be-
hind this is that, if someone expresses an opinion
about a topic, he or she is likely to follow it up
with reasons for that opinion. The sentiments in
the surrounding context thus reveal factors that in-
fluence the preference or dislike towards the topic.
We define the vicinity as the same sentence plus
the following 5 sentences.
Each unique target word targeti in the web cor-
pus, i.e., each word used as the target of an opin-
ion one or more times, is processed to generate the
following conditional probabilities.
P (topicqj |target
p
i ) =
#(topicqj , target
p
i )
#targetpi
(1)
where p = {+,? ,? } and q = {+,? ,? } denote the
polarities of the target and the topic, respectively;
j = {1, 2}; and i = {1...M}, where M is the
number of unique targets in the corpus. For exam-
ple, P (Mac+|interface+) is the probability that
?interface? is the target of a positive opinion that is
in the vicinity of a positive opinion toward ?Mac.?
Table 2 lists some of the probabilities learned
by this approach. (Note that the neutral cases are
not shown.)
3.2.1 Interpreting the learned probabilities
Table 2 contains examples of the learned proba-
bilities. These probabilities align with what we
qualitatively found in our development data. For
example, the opinions towards ?Storm? essen-
tially follow the opinions towards ?Blackberry;?
that is, positive opinions toward ?Storm? are usu-
ally found in the vicinity of positive opinions to-
ward ?Blackberry,? and negative opinions toward
?Storm? are usually found in the vicinity of neg-
ative opinions toward ?Blackberry? (for example,
in the row for storm+, P (blackberry+|storm+)
is much higher than the other probabilities). Thus,
an opinion expressed about ?Storm? is usually the
opinion one has toward ?Blackberry.? This is ex-
pected, as Storm is a type of Blackberry. A similar
example is ipod+, which follows the opinion to-
ward the iPhone. This is interesting because an
229
iPod is not a phone; the association is due to pref-
erence for the brand. In contrast, the probability
distribution for ?phone? does not show a prefer-
ence for any one side, even though both iPhone
and Blackberry are phones. This indicates that
opinions towards phones in general will not be
able to distinguish between the debate sides.
Another interesting case is illustrated by the
probabilities for ?e-mail.? People who like e-mail
capability are more likely to praise the Blackberry,
or even criticize the iPhone ? they would thus be-
long to the pro-Blackberry camp.
While we noted earlier that positive evaluations
of keyboards are associated with positive evalua-
tions of the Blackberry (by far the highest prob-
ability in that row), negative evaluations of key-
boards, are, however, not a strong discriminating
factor.
For the other entries in the table, we see that
criticisms of batteries and the phone network are
more associated with negative sentiments towards
the iPhones.
The possibility of these various cases motivates
our approach, in which opinions and their polar-
ities are considered when searching for associa-
tions between debate topics and their aspects.
3.3 Debate-side classification
Once we have the probabilities collected from the
web, we can build our classifier to classify the de-
bate posts.
Here again, we use the process described in Sec-
tion 3.1 to extract polarity-target pairs for each
opinion expressed in the post. Let N be the num-
ber of instances of polarity-target pairs in the post.
For each instance Ij (j = {1...N}), we look up
the learned probabilities of Section 3.2 to create
two scores, wj and uj :
wj = P (topic+1 |target
p
i ) + P (topic?2 |target
p
i ) (2)
uj = P (topic?1 |target
p
i ) + P (topic+2 |target
p
i ) (3)
where targetpi is the polarity-target type of which
Ij is an instance.
Score wj corresponds to side1 and uj corre-
sponds to side2. A point to note is that, if a tar-
get word is repeated, and it occurs in different
polarity-target instances, it is counted as a sepa-
rate instance each time ? that is, here we account
for tokens, not types. Via Equations 2 and 3, we
interpret the observed polarity-target instance Ij in
terms of debate sides.
We formulate the problem of finding the over-
all side of the post as an Integer Linear Program-
ming (ILP) problem. The side that maximizes the
overall side-score for the post, given all the N in-
stances Ij , is chosen by maximizing the objective
function
N
?
j=1
(wjxj + ujyj) (4)
subject to the following constraints
xj ? {0, 1}, ?j (5)
yj ? {0, 1}, ?j (6)
xj + yj = 1, ?j (7)
xj ? xj?1 = 0, j ? {2..N} (8)
yj ? yj?1 = 0, j ? {2..N} (9)
Equations 5 and 6 implement binary constraints.
Equation 7 enforces the constraint that each Ij can
belong to exactly one side. Finally, Equations 8
and 9 ensure that a single side is chosen for the
entire post.
3.4 Accounting for concession
As described in Section 2, debate participants of-
ten acknowledge the opinions held by the oppos-
ing side. We recognize such discourse constructs
using the Penn Discourse Treebank (Prasad et al,
2007) list of discourse connectives. In particu-
lar, we use the list of connectives from the Con-
cession and Contra-expectation category. Exam-
ples of connectives in these categories are ?while,?
?nonetheless,? ?however,? and ?even if.? We use
approximations to finding the arguments to the
discourse connectives (ARG1 and ARG2 in Penn
Discourse Treebank terms). If the connective is
mid-sentence, the part of the sentence prior to
the connective is considered conceded, and the
part that follows the connective is considered non-
conceded. An example is the second sentence of
Example 3. If, on the other hand, the connective
is sentence-initial, the sentence is split at the first
comma that occurs mid sentence. The first part is
considered conceded, and the second part is con-
sidered non-conceded. An example is the first sen-
tence of Example 1.
The opinions occurring in the conceded part are
interpreted in reverse. That is, the weights corre-
sponding to the sides wj and uj are interchanged
in equation 4. Thus, conceded opinions are effec-
tively made to count towards the opposing side.
230
4 Experiments
On http://www.convinceme.net, the html page for
each debate contains side information for each
post (side1 is blue in color and side2 is green).
This gives us automatically labeled data for our
evaluations. For each of the 4 debates in our test
set, we use posts with at least 5 sentences for eval-
uation.
4.1 Baselines
We implemented two baselines: the OpTopic sys-
tem that uses topic information only, and the
OpPMI system that uses topic as well as related
word (noun) information. All systems use the
same lexicon, as well as exactly the same pro-
cesses for opinion finding and opinion-target pair-
ing.
The OpTopic system This system considers
only explicit mentions of the topic for the opin-
ion analysis. Thus, for this system, the step
of opinion-target pairing only finds all topic+1 ,
topic?1 , topic+2 , topic?2 instances in the post
(where, for example, an instance of topic+1 is a
positive opinion whose target is explicitly topic1).
The polarity-topic pairs are counted for each de-
bate side according to the following equations.
score(side1) = #topic+1 + #topic?2 (10)
score(side2) = #topic?1 + #topic+2 (11)
The post is assigned the side with the higher score.
The OpPMI system This system finds opinion-
target pairs for not only the topics, but also for the
words in the debate that are significantly related to
either of the topics.
We find semantic relatedness of each noun in
the post with the two main topics of the debate
by calculating the Pointwise Mutual Information
(PMI) between the term and each topic over the
entire web corpus. We use the API provided by the
Measures of Semantic Relatedness (MSR)4 engine
for this purpose. The MSR engine issues Google
queries to retrieve documents and finds the PMI
between any two given words. Table 3 lists PMIs
between the topics and the words from Table 2.
Each noun k is assigned to the topic with the
higher PMI score. That is, if
PMI(topic1,k) > PMI(topic2,k) ?k= topic1
and if
4http://cwl-projects.cogsci.rpi.edu/msr/
PMI(topic2,k) > PMI(topic1,k) ?k= topic2
Next, the polarity-target pairs are found for the
post, as before, and Equations 10 and 11 are used
to assign a side to the post as in the OpTopic
system, except that here, related nouns are also
counted as instances of their associated topics.
word iPhone blackberry
storm 0.923 0.941
phone 0.908 0.885
e-mail 0.522 0.623
ipod 0.909 0.976
battery 0.974 0.927
network 0.658 0.961
keyboard 0.961 0.983
Table 3: PMI of words with the topics
4.2 Results
Performance is measured using the follow-
ing metrics: Accuracy ( #Correct#Total posts ), Precision
(#Correct#guessed), Recall ( #Correct#relevant ) and F-measure
( 2?Precision?Recall(Precision+Recall)).
In our task, it is desirable to make a pre-
diction for all the posts; hence #relevant =
#Total posts. This results in Recall and Accu-
racy being the same. However, all of the systems
do not classify a post if the post does not con-
tain the information it needs. Thus, #guessed ?
#Total posts, and Precision is not the same as
Accuracy.
Table 4 reports the performance of four systems
on the test data: the two baselines, our method
using the preferences learned from the web cor-
pus (OpPr) and the method additionally using dis-
course information to reverse conceded opinions.
The OpTopic has low recall. This is expected,
because it relies only on opinions explicitly toward
the topics.
The OpPMI has better recall than OpTopic;
however, the precision drops for some debates. We
believe this is due to the addition of noise. This re-
sult suggests that not all terms that are relevant to
a topic are useful for determining the debate side.
Finally, both of the OpPr systems are better than
both baselines in Accuracy as well as F-measure
for all four debates.
The accuracy of the full OpPr system improves,
on average, by 35 percentage points over the Op-
Topic system, and by 20 percentage points over the
231
OpPMI system. The F-measure improves, on aver-
age, by 25 percentage points over the OpTopic sys-
tem, and by 17 percentage points over the OpPMI
system. Note that in 3 out of 4 of the debates, the
full system is able to make a guess for all of the
posts (hence, the metrics all have the same values).
In three of the four debates, the system us-
ing concession handling described in Section 3.4
outperforms the system without it, providing evi-
dence that our treatment of concessions is effec-
tive. On average, there is a 3 percentage point
improvement in Accuracy, 5 percentage point im-
provement in Precision and 5 percentage point im-
provement in F-measure due to the added conces-
sion information.
OpTopic OpPMI OpPr OpPr
+ Disc
Firefox Vs Internet explorer (62 posts)
Acc 33.87 53.23 64.52 66.13
Prec 67.74 60.0 64.52 66.13
Rec 33.87 53.23 64.52 66.13
F1 45.16 56.41 64.52 66.13
Windows vs. Mac (15 posts)
Acc 13.33 46.67 66.67 66.67
Prec 40.0 53.85 66.67 66.67
Rec 13.33 46.67 66.67 66.67
F1 20.0 50.00 66.67 66.67
SonyPs3 vs. Wii (36 posts)
Acc 33.33 33.33 56.25 61.11
Prec 80.0 46.15 56.25 68.75
Rec 33.33 33.33 50.0 61.11
F1 47.06 38.71 52.94 64.71
Opera vs. Firefox (4 posts)
Acc 25.0 50.0 75.0 100.0
Prec 33.33 100 75.0 100.0
Rec 25.0 50 75.0 100.0
F1 28.57 66.67 75.0 100.0
Table 4: Performance of the systems on the test
data
5 Discussion
In this section, we discuss the results from the pre-
vious section and describe the sources of errors.
As reported in the previous section, the OpPr
system outperforms both the OpTopic and the
OpPMI systems. In order to analyze why OpPr
outperforms OpPMI, we need to compare Tables
2 and 3. Table 2 reports the conditional proba-
bilities learned from the web corpus for polarity-
target pairs used in OpPr, and Table 3 reports the
PMI of these same targets with the debate topics
used in OpPMI. First, we observe that the PMI
numbers are intuitive, in that all the words, ex-
cept for ?e-mail,? show a high PMI relatedness to
both topics. All of them are indeed semantically
related to the domain. Additionally, we see that
some conclusions of the OpPMI system are simi-
lar to those of the OpPr system, for example, that
?Storm? is more closely related to the Blackberry
than the iPhone.
However, notice two cases: the PMI values
for ?phone? and ?e-mail? are intuitive, but they
may cause errors in debate analysis. Because the
iPhone and the Blackberry are both phones, the
word ?phone? does not have any distinguishing
power in debates. On the other hand, the PMI
measure of ?e-mail? suggests that it is not closely
related to the debate topics, though it is, in fact, a
desirable feature for smart phone users, even more
so with Blackberry users. The PMI measure does
not reflect this.
The ?network? aspect shows a comparatively
greater relatedness to the blackberry than to the
iPhone. Thus, OpPMI uses it as a proxy for
the Blackberry. This may be erroneous, how-
ever, because negative opinions towards ?net-
work? are more indicative of negative opinions to-
wards iPhones, a fact revealed by Table 2.
In general, even if the OpPMI system knows
what topic the given word is more related to, it
still does not know what the opinion towards that
word means in the debate scenario. The OpPr sys-
tem, on the other hand, is able to map it to a debate
side.
5.1 Errors
False lexicon hits. The lexicon is word based,
but, as shown by (Wiebe and Mihalcea, 2006; Su
and Markert, 2008), many subjective words have
both objective and subjective senses. Thus, one
major source of errors is a false hit of a word in
the lexicon.
Opinion-target pairing. The syntactic rule-
based opinion-target pairing system is a large
source of errors in the OpPr as well as the base-
line systems. Product review mining work has ex-
plored finding opinions with respect to, or in con-
junction with, aspects (Hu and Liu, 2004; Popescu
et al, 2005); however, in our work, we need to find
232
information in the other direction ? that is, given
the opinion, what is the opinion about. Stoyanov
and Cardie (2008) work on opinion co-reference;
however, we need to identify the specific target.
Pragmatic opinions. Some of the errors are due
to the fact that the opinions expressed in the post
are pragmatic. This becomes a problem especially
when the debate post is small, and we have few
other lexical clues in the post. The following post
is an example:
(4) The blackberry is something like $150 and
the iPhone is $500. I don?t think it?s worth
it. You could buy a iPod separate and have
a boatload of extra money left over.
In this example, the participant mentions the
difference in the prices in the first sentence. This
sentence implies a negative opinion towards the
iPhone. However, recognizing this would require
a system to have extensive world knowledge. In
the second sentence, the lexicon does hit the word
?worth,? and, using syntactic rules, we can deter-
mine it is negated. However, the opinion-target
pairing system only tells us that the opinion is tied
to the ?it.? A co-reference system would be needed
to tie the ?it? to ?iPhone? in the first sentence.
6 Related Work
Several researchers have worked on similar tasks.
Kim and Hovy (2007) predict the results of an
election by analyzing forums discussing the elec-
tions. Theirs is a supervised bag-of-words sys-
tem using unigrams, bigrams, and trigrams as fea-
tures. In contrast, our approach is unsupervised,
and exploits different types of information. Bansal
et al (2008) predict the vote from congressional
floor debates using agreement/disagreement fea-
tures. We do not model inter-personal exchanges;
instead, we model factors that influence stance
taking. Lin at al (2006) identify opposing perspec-
tives. Though apparently related at the task level,
perspectives as they define them are not the same
as opinions. Their approach does not involve any
opinion analysis. Fujii and Ishikawa (2006) also
work with arguments. However, their focus is on
argument visualization rather than on recognizing
stances.
Other researchers have also mined data to learn
associations among products and features. In
their work on mining opinions in comparative sen-
tences, Ganapathibhotla and Liu (2008) look for
user preferences for one product?s features over
another?s. We do not exploit comparative con-
structs, but rather probabilistic associations. Thus,
our approach and theirs are complementary. A
number of works in product review mining (Hu
and Liu, 2004; Popescu et al, 2005; Kobayashi et
al., 2005; Bloom et al, 2007) automatically find
features of the reviewed products. However, our
approach is novel in that it learns and exploits as-
sociations among opinion/polarity, topics, and as-
pects.
Several researchers have recognized the im-
portant role discourse plays in opinion analysis
(Polanyi and Zaenen, 2005; Snyder and Barzilay,
2007; Somasundaran et al, 2008; Asher et al,
2008; Sadamitsu et al, 2008). However, previous
work did not account for concessions in determin-
ing whether an opinion supports one side or the
other.
More sophisticated approaches to identifying
opinions and recognizing their contextual polar-
ity have been published (e.g., (Wilson et al, 2005;
Ikeda et al, 2008; Sadamitsu et al, 2008)). Those
components are not the focus of our work.
7 Conclusions
This paper addresses challenges faced by opinion
analysis in the debate genre. In our method, fac-
tors that influence the choice of a debate side are
learned by mining a web corpus for opinions. This
knowledge is exploited in an unsupervised method
for classifying the side taken by a post, which also
accounts for concessionary opinions.
Our results corroborate our hypothesis that find-
ing relations between aspects associated with a
topic, but particularized to polarity, is more effec-
tive than finding relations between topics and as-
pects alone. The system that implements this in-
formation, mined from the web, outperforms the
web PMI-based baseline. Our hypothesis that ad-
dressing concessionary opinions is useful is also
corroborated by improved performance.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152. We would also like to thank
Vladislav D. Veksler for help with the MSR en-
gine, and the anonymous reviewers for their help-
ful comments.
233
References
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse:
A preliminary study. In Coling 2008: Companion
volume: Posters and Demonstrations, pages 5?8,
Manchester, UK, August.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of COLING: Companion vol-
ume: Posters.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Atsushi Fujii and Tetsuya Ishikawa. 2006. A sys-
tem for summarizing and visualizing arguments in
subjective documents: Toward supporting decision
making. In Proceedings of the Workshop on Senti-
ment and Subjectivity in Text, pages 15?22, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 241?248,
Manchester, UK, August.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In AAAI-2004.
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov,
and Manabu Okumura. 2008. Learning to shift
the polarity of words for sentiment classification. In
Proceedings of the Third International Joint Confer-
ence on Natural Language Processing (IJCNLP).
Soo-Min Kim and Eduard Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1056?1064.
Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and Yuji
Matsumoto. 2005. Opinion extraction using a
learning-based anaphora resolution technique. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing (IJCNLP-05),
poster, pages 175?180.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of the 10th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2006), pages 109?116, New York, New
York.
Livia Polanyi and Annie Zaenen. 2005. Contextual
valence shifters. In Computing Attitude and Affect
in Text. Springer.
Ana-Maria Popescu, Bao Nguyen, and Oren Et-
zioni. 2005. OPINE: Extracting product fea-
tures and opinions from reviews. In Proceedings
of HLT/EMNLP 2005 Interactive Demonstrations,
pages 32?33, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
R. Prasad, E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi,
L. Robaldo, and B. Webber, 2007. PDTB 2.0 Anno-
tation Manual.
Kugatsu Sadamitsu, Satoshi Sekine, and Mikio Ya-
mamoto. 2008. Sentiment analysis based on
probabilistic models using inter-sentence informa-
tion. In European Language Resources Associa-
tion (ELRA), editor, Proceedings of the Sixth In-
ternational Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, May.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
Proceedings of NAACL-2007.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Veselin Stoyanov and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
817?824, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Fangzhong Su and Katja Markert. 2008. From
word to sense: a case study of subjectivity recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
2008), Manchester, UK, August.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of COLING-ACL
2006.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT-EMNLP, pages
347?354, Vancouver, Canada.
234
A Corpus Study of Evaluative and Speculative Language
Janyce Wiebe

, Rebecca Bruce
y
, Matthew Bell

, Melanie Martin
z
, Theresa Wilson

University of Pittsburgh

, University of North Carolina at Asheville
y
, New Mexico State University
z
wiebe,mbell,twilson@cs.pitt.edu, bruce@cs.unca.edu, mmartin@cs.nmsu.edu
Abstract
This paper presents a corpus study
of evaluative and speculative language.
Knowledge of such language would be
useful in many applications, such as
text categorization and summarization.
Analyses of annotator agreement and of
characteristics of subjective language are
performed. This study yields knowl-
edge needed to design eective machine
learning systems for identifying subjec-
tive language.
1 Introduction
Subjectivity in natural language refers to aspects
of language used to express opinions and evalua-
tions (Baneld, 1982; Wiebe, 1994). Subjectivity
tagging is distinguishing sentences used to present
opinions and other forms of subjectivity (subjec-
tive sentences) from sentences used to objectively
present factual information (objective sentences).
This task is especially relevant for news report-
ing and Internet forums, in which opinions of var-
ious agents are expressed. There are numerous
applications for which subjectivity tagging is rele-
vant. Two are information retrieval and informa-
tion extraction. Current extraction and retrieval
technology focuses almost exclusively on the sub-
ject matter of documents. However, additional
aspects of a document inuence its relevance, in-
cluding, e.g., the evidential status of the material
presented, and the attitudes expressed about the
topic (Kessler et al, 1997). Knowledge of subjec-
tive language would also be useful in ame recog-
nition (Spertus, 1997; Kaufer, 2000), email clas-
sication (Aone et al, 2000), intellectual attribu-
tion in text (Teufel and Moens, 2000), recogniz-
ing speaker role in radio broadcasts (Barzilay et
al., 2000), review mining (Terveen et al, 1997),
generation and style (Hovy, 1987), clustering doc-
uments by ideological point of view (Sack, 1995),
and any other application that would benet from
knowledge of how opinionated the language is, and
whether or not the writer purports to objectively
present factual material.
To use subjectivity tagging in applications,
good linguistic clues must be found. As with many
pragmatic and discourse distinctions, existing lex-
ical resources are not comprehensively coded for
subjectivity. The goal of our current work is learn-
ing subjectivity clues from corpora. This paper
contributes to this goal by empirically examin-
ing subjectivity. We explore annotating subjectiv-
ity at dierent levels (expression, sentence, docu-
ment) and produce corpora annotated at dierent
levels. Annotator agreement is analyzed to un-
derstand and assess the viability of such annota-
tions. In addition, because expression-level anno-
tations are ne-grained and thus very informative,
these annotations are examined to gain knowledge
about subjectivity.
We also use our annotations and existing ed-
itorial annotations to generate and test features
of subjectivity. Altogether, the observations and
results of these studies provide valuable informa-
tion that will facilitate designing eective machine
learning systems for recognizing subjectivity.
The remainder of this paper rst provides back-
ground about subjectivity, then presents results
for document-level annotations, followed by an
analysis of expression-level annotations. Results
for features generated using document-level anno-
tations are next, ending with conclusions.
2 Subjectivity
Sentence (1) is an example of a simple subjective
sentence, and (2) is an example of a simple objec-
tive sentence:
1
(1) At several dierent layers, it's a fascinating
tale.
1
The term subjectivity is due to Ann Baneld
(1982). For references to work on subjectivity, please
see (Baneld, 1982; Fludernik, 1993; Wiebe, 1994;
Stein and Wright, 1995).
(2) Bell Industries Inc. increased its quarterly to
10 cents from 7 cents a share.
The main types of subjectivity are:
1. Evaluation. This category includes emotions
such as hope and hatred as well as evalua-
tions, judgements, and opinions. Examples
of expressions involving positive evaluation
are enthused, wonderful, and great product!.
Examples involving negative evaluation are
complained, you idiot!, and terrible product.
2. Speculation. This category includes anything
that removes the presupposition of events oc-
curring or states holding, such as speculation
and uncertainty. Examples of speculative ex-
pressions are speculated, and maybe.
Following are examples of strong negative
evaluative language from a corpus of Usenet
newsgroup messages:
(3a) I had in mind your facts, buddy, not hers.
(3b) Nice touch. \Alleges" whenever facts posted
are not in your persona of what is \real".
Following is an example of opinionated, edito-
rial language, taken from an editorial in the Wall
Street Journal:
(4) We stand in awe of the Woodstock genera-
tion's ability to be unceasingly fascinated by the
subject of itself.
Sentences (5) and (6) illustrate the fact that
sentences about speech events may be subjective
or objective:
(5) Northwest Airlines settled the remaining
lawsuits led on behalf of 156 people killed in
a 1987 crash, but claims against the jetliner's
maker are being pursued, a federal judge said.
(6) \The cost of health care is eroding our stan-
dard of living and sapping industrial strength,"
complains Walter Maher, a Chrysler health-and-
benets specialist.
In (5), the material about lawsuits and claims is
presented as factual information, and a federal
judge is given as the source of information. In
(6), in contrast, a complaint is presented. An NLP
system performing information extraction on (6)
should not treat the material in the quoted string
as factual information, with the complainer as a
source of information, whereas a corresponding
treatment of sentence (5) would be appropriate.
Subjective sentences often contain individual
expressions of subjectivity. Examples are fasci-
nating in (1), and eroding, sapping, and complains
in (6). The following paragraphs mention aspects
of subjectivity expressions that are relevant for
NLP applications.
First, although some expressions, such as !, are
subjective in all contexts, many, such as sapping
and eroding, may or may not be subjective, de-
pending on the context in which they appear. A
potential subjective element (PSE) is a linguistic
element that may be used to express subjectivity.
A subjective element is an instance of a potential
subjective element, in a particular context, that is
indeed subjective in that context (Wiebe, 1994).
Second, a subjective element expresses the sub-
jectivity of a source, who may be the writer or
someone mentioned in the text. For example, the
source of fascinating in (1) is the writer, while
the source of the subjective elements in (6) is Ma-
her. In addition, a subjective element has a tar-
get, i.e., what the subjectivity is about or directed
toward. In (1), the target is a tale; in (6), the tar-
get of Maher's subjectivity is the cost of health
care. These are examples of object-centric sub-
jectivity, which is about an object mentioned in
the text (other examples: \I love this project";
\The software is horrible"). Subjectivity may also
be addressee-oriented, i.e., directed toward the lis-
tener or reader (e.g., \You are an idiot").
Third, there may be multiple subjective ele-
ments in a sentence, possibly of dierent types
and attributed to dierent sources and targets.
For example, in (4), subjectivity of the Woodstock
generation is described (specically, its fascina-
tion with itself). In addition, subjectivity of the
writer is expressed (e.g., `we stand in awe'). As de-
scribed below, individual subjective elements were
annotated as part of this work, rening previous
work on sentence-level annotations. Finally, PSEs
may be complex expressions such as `village id-
iot', `powers that be', `You' NP, and `What a'
NP. There is a great variety of such expressions,
including many studied under the rubric of idioms
(see, for example, (Nunberg et al, 1994)). We ad-
dress learning such expressions in another project.
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999; Bruce and
Wiebe, 1999), a corpus of sentences from the Wall
Street Journal Treebank Corpus (Marcus et al,
1993) was manually annotated with subjectivity
classications by multiple judges. The judges were
instructed to consider a sentence to be subjective
if they perceived any signicant expression of sub-
jectivity (of any source) in the sentence, and to
consider the sentence to be objective, otherwise.
Agreement was summarized in terms of Cohen's
 (Cohen, 1960), which compares the total proba-
bility of agreement to that expected if the taggers'
classications were statistically independent (i.e.,
\chance agreement"). After two rounds of tag-
ging by three judges, an average pairwise  value
of .69 was achieved on a test set. The EM learn-
ing algorithm was used to produce corrected tags
representing the consensus opinions of the taggers
(Goodman, 1974; Dawid and Skene, 1979). An
automatic system to perform subjectivity tagging
was developed using the new tags as training and
testing data. In 10-fold cross validation experi-
ments, a probabilistic classier obtained an aver-
age accuracy on subjectivity tagging of 72.17%,
more than 20 percentage points higher than a
baseline accuracy obtained by always choosing the
more frequent class. Five part-of-speech features,
two lexical features, and a paragraph feature were
used.
To identify richer features, (Wiebe, 2000) used
Lin's (1998) method for clustering words accord-
ing to distributional similarity, seeded by a small
amount of detailed manual annotation, to auto-
matically identify adjective PSEs. There are two
parameters of this process, neither of which was
varied in (Wiebe, 2000): C, the cluster size con-
sidered, and FT , a ltering threshold, such that, if
the seed word and the words in its cluster have, as
a set, lower precision than the ltering threshold
on the training data, the entire cluster, includ-
ing the seed word, is ltered out. This process is
adapted for use in the current paper, as described
in section 7.
4 Choices in Annotation
In expression-level annotation, the judges rst
identify the sentences they believe are subjective.
They next identify the subjective elements in
the sentence, i.e., the expressions they feel are
responsible for the subjective classication. For
example (subjective elements are in parentheses):
They promised (yet) more for (really good stu).
(Perhaps you'll forgive me) for reposting his
response.
Subjective-element (expression-level) annota-
tions are probably the most natural. Ultimately,
we would like to recognize the subjective elements
in a text, and their types, targets, and sources.
However, both manual and automatic tagging at
this level are dicult because the tags are very
ne-grained, and there is no predetermined clas-
sication unit; a subjective element may be a sin-
gle word or a large expression. Thus, in the short
term, it is probably best to use subjective-element
annotations for knowledge acquisition (analysis,
training, feature generation) alone, and not target
automatic classication of subjective elements.
In this work, document-level subjectivity anno-
tations are text categories of which subjectivity
is a key aspect. We use three text categories:
editorials (Kessler et al, 1997), reviews, and
\ames", i.e., hostile messages (Spertus, 1997;
Kaufer, 2000). For ease of discussion, we group
editorials and reviews together under the term
opinion pieces.
There are benets to using such document-level
annotations. First, they are more directly re-
lated to applications (e.g., ltering hostile mes-
sages and mining reviews from Internet forums).
Second, there are existing annotations to be ex-
ploited, such as editorials and arts reviews marked
as such by newspapers, as well as on-line product
reviews accompanied by formal numerical ratings
(e.g., 4 on a scale from 1 to 5).
However, a challenging aspect of such data is
that opinion pieces and ames contain objective
sentences, while documents in other text cate-
gories contain subjective sentences. News reports
present reactions to and attitudes toward reported
events (van Dijk 1988); they often contain seg-
ments starting with expressions such as critics
claim and supporters argue. In addition, quoted-
speech sentences in which individuals express their
subjectivity are often included (Barzilay et al,
2000). On the other hand, editorials contain ob-
jective sentences presenting facts supporting the
writer's argument, and reviews contain sentences
objectively presenting facts about the product.
This \impure" aspect of opinionated text cate-
gories must be considered when such data is used
for training and testing. Some specic results are
given below in section 7.
We believe that sentence-level classications
will continue to provide an important level of
analysis. The sentence provides a prespeci-
ed classication unit
2
and, while sentence-level
judgements are not as ne-grained as subjective-
2
While sentence boundaries are not always unam-
biguous in unedited text or spoken language, the data
can always be segmented into sentence-like units be-
fore subjectivity tagging is performed.
element judgements, they do not involve the large
amount of noise we face with document-level an-
notations.
5 Document-Level Annotation
Results
5.1 Flame Annotations
In this study, newsgroup messages were assigned
the tags ame or not-ame. The corpus con-
sists of 1140 Usenet newsgroup messages, bal-
anced among the categories alt, sci, comp, and rec
in the Usenet hierarchy. The corpus was divided,
preserving the category balance, into a training set
of 778 messages and a test set of 362 messages.
The annotators were instructed to mark a mes-
sage as a ame if the \main intention of the mes-
sage is a personal attack, containing insulting or
abusive language." A number of policy decisions
were made in the instructions, dealing, primarily,
with included messages (part or all of a previous
message, included in the current message as part
of a reply). Some additional issues addressed in
the instructions were who the attack was directed
at, nonsense, sarcasm, humor, rants, and raves.
During the training phase, two annotators, MM
and R, participated in multiple rounds of tagging,
revising the annotation instructions as they pro-
ceeded. During the testing phase, MM and R in-
dependently annotated the test set, achieving a 
value on these messages of 0.69. A third annota-
tor, L, trained on 492 messages from the training
set, and then annotated 88 of the messages in the
test set. The pairwise  values on this set of 88
are: MM & R: 0.80; MM & L: 0.75; R & MM:
0.80; for an average pairwise  of .78.
This study provides evidence for the viability
of document-level ame annotation. We plan to
build a ame-recognition system in the future. As
will be seen below, MM and R also tagged this
data at the subjective-element level.
5.2 Opinion-Piece Classications
Our opinion-piece classications are built on exist-
ing annotations in the Wall Street Journal. Specif-
ically, there are articles explicitly identied to be
Editorials, Letters to the Editor, Arts & Leisure,
and Viewpoints; together, we call these opinion
pieces. This data is a good resource for subjectiv-
ity recognition. However, an inspection of some
data revealed that some editorials and reviews are
not marked as such. For example, there are arti-
cles written in the rst person, and the purpose of
the article is to present an argument rather than
cover a news story, but there is no explicit indi-
cation that they are editorials. To create high
quality test data, two judges manually annotated
WSJ data for opinion pieces. The instructions
were to nd any additional opinion pieces that
are not marked as such. The annotators also had
the option of disagreeing with the existing anno-
tations, but did not opt to do so in any instances.
One judge annotated all articles in four datasets
of the Wall Street Journal Treebank corpus (Mar-
cus et al, 1993) (W9-4, W9-10, W9-22, and W9-
33, each approximately 160K words) as well as
the corpus of Wall Street Journal articles used in
(Wiebe et al, 1999) (called WSJ-SE below). An-
other judge annotated all articles in two of the
datasets (W9-22 and W9-33).
This annotation task appears to be relatively
easy. With no training at all, the  values are very
high: .94 for dataset W9-33 and .95 for dataset
W9-22.
The agreement data for W9-22 is given in Table
1 in the form of a contingency table. In section
7, this data is used to generate and test candidate
potential subjective elements (PSEs).
6 Subjective-Element Annotation
Results and Analyses
6.1 Annotations and Data
These subsections analyze subjective element an-
notations performed on three datasets, WSJ-SE,
NG-FE, and NG-SE.
WSJ-SE is the corpus of 1001 sentences of the
Wall Street Journal Treebank Corpus referred to
above in section 3. Recall that the sentences of
this corpus were manually annotated with subjec-
tivity classications as described in (Wiebe et al,
1999; Bruce and Wiebe, 1999).
For this paper, two annotators (D and M ) were
asked to identify the subjective elements in WSJ-
SE. Specically, the taggers were given the sub-
jective sentences identied in the previous study,
and asked to put brackets around the words they
believe cause the sentence to be classied as sub-
jective.
Note that inammatory language is a kind of
subjective language. NG-FE is a subset of the
Usenet newsgroup corpus used in the document-
level ame-annotation study described in section
5.1. Specically, NG-FE consists of the 362-
message test set for taggers R and MM. For this
study, R and MM were asked to identify the ame
elements in NG-FE. Flame elements are the sub-
set of subjective elements that are perceived to
be inammatory. R and MM were asked to do
this in all 362 messages, because some messages
that were not judged to be ames at the message
level do contain individual inammatory phrases
Tagger 2
Op Not Op
Tagger 1 Op n
11
= 23 n
12
= 0 n
1+
= 23
Not Op n
21
= 2 n
22
= 268 n
2+
= 270
n
+1
= 25 n
+2
= 268 n
++
= 293
Table 1: Contingency Table for Opinion Piece Agreement in W9-22
(in these cases, the tagger does not believe that
these phrases express the main intent of the mes-
sage).
In addition to the above annotations, tagger M
performed subjective-element tagging on a dier-
ent set of Usenet newsgroup messages, corpus NG-
SE. The size of this corpus is 15413 words.
In datasets WSJ-SE and NG-SE, the taggers
were also asked to specify one of ve subjective
element types: e+ (positive evaluative), e  (neg-
ative evaluative), e? (some other type of evalua-
tion), u (uncertainty), and o (none of the above),
with the option to assign multiple types to an in-
stance. All corpora were stemmed (Karp et al,
1992) and part-of-speech tagged (Brill, 1992).
6.2 Agreement Among Taggers
There are techniques for analyzing agreement
when annotations involve segment boundaries
(Litman and Passonneau, 1995; Marcu et al,
1999), but our focus in this paper is on words.
Thus, our analyses are at the word level: each
word is classied as either appearing in a subjec-
tive element or not. Punctuation is excluded from
our analyses. The WSJ data is divided into two
subsets in this section, Exp1 and Exp2.
As mentioned above, in WSJ-SE Exp1 and
Exp2, the taggers also classied subjective ele-
ments with respect to the type of subjectivity
being expressed. Subjectivity type agreement is
again analyzed at the word level, but, in this anal-
ysis, only the words classied as belonging to sub-
jective elements by both taggers are considered.
Table 2 provides  values for word agreement
in NG-FE (the ame data) as well as for WSJ-SE
Exp1 and Exp2. The task of identifying subjec-
tive elements in a body of text is dicult, and the
agreement results reect this fact; agreement is
much stronger than that expected by chance, but
less than what we would like to see when verify-
ing a new classication. Further renement of the
coding manual is required. Additionally, it may be
possible to rene the classications automatically
using methods such as those described in (Wiebe
et al, 1999). In this analysis, we explore the pat-
terns of agreement exhibited by the taggers in an
eort to better understand the classication.
We begin by looking at word agreement. Word
agreement is higher in the ame experiment
(NG-FE) than it is in either WSJ experiment
(WSJ-SE Exp1 and Exp2). Looking at the WSJ
data provides one plausible explanation for the
lower word agreement in the WSJ experiments.
As exhibited in the subjective elements identied
for the single clause below,
D: (e+ played the role well) (e? obligatory
ragged jeans a thicket of long hair and rejection
of all things conventional)
M : (e+ well) (e? obligatory) (e- ragged) (e?
thicket) (e- rejection) (e- all things conventional)
tagger D consistently identies entire phrases
as subjective, while Tagger M prefers to select
discrete lexical items. This dierence in inter-
pretation of the tagging instructions does not
occur in the ame experiment. Nonetheless, even
within the ame data, there are many instances
where both taggers identify the same segment of
a sentence as forming a subjective element but
disagree on the boundaries of that segment, as in
the example below.
R: (classic case of you deliberately misinterpret-
ing my comments)
MM : (you deliberately misinterpreting my
comments)
These patterns of partial agreement are also evi-
dent in the  values for words from specic syn-
tactic categories (see Table 2 again). In the WSJ
data, agreement on determiners is particularly low
because they are often included as part of a phrase
by tagger D but typically not included in the spe-
cic lexical items chosen by tagger M. Interest-
ingly, in the WSJ experiments, the taggers most
frequently agreed on the selection of modals and
adjectives, while in the ame experiment, agree-
ment was highest on nouns and adjectives. The
high agreement on adjectives in both genres is con-
All Words Nouns Verbs Modals Adj's Adverbs Det's
NG-FE 0:4657 0:5213 0.4571 0:4008 0:5011 0:3576 0:4286
WSJ-SE, Exp1 0:4228 0:3999 0.4235 0:6992 0:6000 0:4328 0:2661
WSJ-SE, Exp2 0:3703 0:3705 0.4261 0:4298 0:4294 0:2256 0:1234
Table 2:  Values for Word Agreement
sistent with results from other work (Bruce and
Wiebe, 1999; Wiebe et al, 1999), but high agree-
ment on nouns in the ame data verses high agree-
ment on modals in the WSJ data suggests a genre
specic usage of these categories. This would be
the case if, for example, modals were most fre-
quently used to express uncertainty, a type of sub-
jectivity that would be relatively rare in ames.
Turning to subjective-element type, in both
WSJ experiments, the  values for type agreement
are comparable to those for word agreement. Re-
call that multiple types may be assigned to a single
subjective instance. All such instances in the WSJ
data are u in combination with an evaluative tag
(i.e., e+, e- and e?), and they are not common:
each tagger assigned multiple tags to fewer than
7% of the subjective instances. However, if partial
matches between type tags are recognized, i.e., if
they share a common tag, then the  values im-
prove signicantly. Table 3 shows both types of
results.
It is interesting to note the variation in type agree-
ment for words of dierent syntactic categories.
Agreement on adjectives is consistently high while
the agreement on the type of subjectivity ex-
pressed by modals and adverbs is consistently low.
This contrasts with the fact that word agreement
for modals, in particular, and, to a lesser extent,
adverbs was high. This lack of agreement sug-
gests that the type of subjectivity expressed by
adjectives is more easily distinguished than that
of modals or adverbs. This is particularly impor-
tant because the number of adjectives included in
subjective elements is high. In contrast, the num-
bers of modals and adverbs are relatively low.
Additional insight can be gained by combining
the 3 evaluative classications (i.e., e+, e- and
e?) to form a single tag, e, representing any
form of evaluative expression. Table 4 presents
type agreement results for the tag set e, u, o.
In contrasting Tables 3 and 4, it is surprising
to note that most of the  values decrease when
the distinction among the evaluative types is re-
moved. This suggests that the three evaluative
types are natural classications. Only for adverbs
does type agreement improve with the smaller
tag set; this indicates that it is dicult to dis-
tinguish the evaluative nature of adverbs. Note
also that agreement for modals is not impacted
by the change in tag sets. This fact supports the
hypothesis that modals are used primary to ex-
press uncertainty. As a nal point, we look at
patterns of agreement in type classication using
the models of symmetry, marginal homogeneity,
quasi-independence, and quasi-symmetry. Each
model tests for a specic pattern of agreement:
symmetry tests the interchangeability of taggers,
marginal homogeneity veries the absence of bias
among taggers, quasi-independence veries that
the taggers act independently when they disagree,
and quasi-symmetry tests for the presence of any
pattern in their disagreements. For a more com-
plete description of these models and their use
in analyzing intercoder reliability see (Bruce and
Wiebe, 1999). In short, the results presented in
Table 5 indicate that the taggers are not inter-
changeable: they exhibit biases in their type clas-
sications, and there is a pattern of correlated dis-
agreement in the assignment of the original type
tags. Surprisingly, the taggers appear to act in-
dependently when they disagree in assigning the
compressed type tags (i.e., tags e, u and o). This
shift in the pattern of disagreement between tag-
gers again suggests that the compression of the
evaluative tags was inappropriate. Additionally,
these ndings suggest that it may be possible to
automatically correct the type biases expressed
by the taggers using the technique described in
(Bruce and Wiebe, 1999), a topic that will be in-
vestigated in future work.
6.3 Uniqueness
Based on previous work (Wiebe et al, 1998), we
hypothesized that low-frequency words are associ-
ated with subjectivity. Table 6 provides evidence
that the number of unique words (words that ap-
pear just once) in subjective elements is higher
than expected. The rst row gives information
for all words and the second gives information for
words that appear just once. The gures in the
Num columns are total counts, and the gures in
the P columns give the proportion that appear in
subjective elements. The Agree columns give in-
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:4216 0:4228 0.2933 0:1422 0:5919 0:1207 0:5000
Partial Match 0:5156 0:4570 0.4447 0:3011 0:6607 0:3305 0:5000
Exp2 Full Match 0:3041 0:2353 0.2765 0:1429 0:5794 0:1207 0:0000
Partial Match 0:4209 0:2353 0.3994 0:3494 0:6719 0:4439 0:1429
Table 3:  Values for Type Agreement Using All Types in the WSJ Data
All Words Nouns Verbs Modals Adj's Adverbs Det's
Exp1 Full Match 0:3377 0:0440 0.1648 0:1968 0:5443 0:3810 0:0000
Partial Match 0:5287 0:1637 0.3765 0:4903 0:8125 0:3810 0:0000
Exp2 Full Match 0:2569 0:0000 0.1923 0:1509 0:4783 0:1707 0:1429
Partial Match 0:4789 0:0000 0.4167 0:4000 0:8056 0:7671 0:4000
Table 4:  Values for Type Agreement Using E,O,U in the WSJ Data
Sym. M.H. Q.S. Q.I.
Exp1 All Types G
2
112:351 92:447 19:904 66:771
Sig. 0:000 0:000 0:527 0:007
e,o,u G
2
85:478 84:142 1:336 12:576
Sig. 0:000 0:000 0:248 0:027
Exp2 All Types G
2
94:669 76:247 18:422 58:892
Sig. 0:000 0:000 0:241 0:001
e,o,u G
2
66:822 66:819 0:003 0:0003
Sig. 0:000 0:000 0:986 0:987
Table 5: Tests for Patterns of Agreement in WSJ Type-Tagged Data
WSJ-SE NG-FE
D M Agree Agree R MM
Num P Num P Num P Num P Num P Num P
All words 18341 .07 18341 .08 16857 .04 15413 .15 86279 .01 88210 .02
unique 2615 .14 2615 .20 2522 .15 2348 .17 5060 .07 4836 .03
Table 6: Proportions of Unique Words in Subjective Elements
formation for the subset of the corresponding data
set upon which the two annotators agree.
Comparison of rows 1 and 2 across columns
shows that the proportion of unique words that
are subjective is higher than the proportion of all
words that are subjective. In all cases, this dier-
ence in proportions is highly statistically signi-
cant.
6.4 Types and Context
An interesting question is, when a word appears
in multiple subjective elements, are those subjec-
tive elements all the same type? Table 7 shows
that a signicant portion are used in more than
one type. Each item considered in the table is a
word-POS pair that appears more than once in the
corpus. The gures shown are the total number of
word-POS items that appear more than once (the
columns labeled MultInst) and the proportion of
those items that appear in more than one type
of subjective element (the columns labeled Mult-
Type). These results highlight the need for contex-
tual disambiguation. For example, one thinks of
great as a positive evaluative term, but its polarity
depends on the context; it can be used negatively
evaluatively in a context such as \Just great." A
goal of performing subjective-element annotations
is to support learning such local contextual inu-
ences.
7 Generating and Testing PSEs
using Document-Level
Annotations
This section uses the opinion-piece annotations to
expand our set of PSEs beyond those that can be
derived from the subjective-element annotations.
Precision is used to assess feature quality. The
precision of feature F for class C is the number
of Fs that occur in units of class C over the total
number of Fs that occur anywhere in the data.
An important motivation for using the opinion-
piece data is that there is a large amount of it,
and manually rening existing annotations as de-
scribed in section 5.2 is much easier and more re-
liable than other types of subjectivity annotation.
However, we cannot expect absolutely high pre-
cisions for two reasons. First, the distribution of
opinions and non-opinions is highly skewed in fa-
vor of non-opinions. For example, in Table 1, tag-
ger 1 classies only 23 of 293 articles as opinion
pieces. Second, as discussed in section 4, opin-
ion pieces contain objective sentences and non
opinion-pieces contain subjective sentences. For
example, in WSJ-SE, which has been annotated
at the sentence and document levels, 70% of the
sentences in opinion pieces are subjective and 30%
are objective. In non-opinion pieces, 44% of the
sentences are subjective and only 56% are objec-
tive.
To give an idea of expected precisions, let us
consider the precision of subjective sentences with
respect to opinion pieces. Suppose that 15% of
the sentences in the dataset are in opinions, 85%
in non-opinions. Let us assume the proportions of
subjective and objective sentences in opinion and
non-opinion pieces given just above. Let N be the
total number of sentences. The desired precision
is the number of subjective sentences in opinions
over the total number of subjective sentences. It
is .22:
p=.15 * N * .70 / (.15 * N * .70 + .85 * N * .44).
In addition, we are assessing PSEs, which are
only potentially subjective; many have objective
as well as subjective uses.
Thus, even if precisions are much lower than 1,
we use increases in precision over a baseline as ev-
idence of promising PSEs. The baseline for com-
parison is the number of word instances in opin-
ion pieces, divided by the total number of word
instances. Table 8 shows the precisions for three
types of PSEs. The freq columns give total fre-
quencies, and the +prec columns show the im-
provements in precision from the baseline. The
baseline precisions are given at the bottom of the
table.
As mentioned above, (Wiebe, 2000) showed suc-
cess automatically identifying adjective PSEs us-
ing Lin's method, seeded by a small amount of de-
tailed manual annotations. Desiring to move away
from manually annotated data, for this paper the
same process is used, but the seed words are all
the adjectives (verbs) in the training data. In ad-
dition, in the current setting, there are no a priori
values to use for parameters C (cluster size) and
FT (ltering threshold), as there were in (Wiebe,
2000), and results vary with dierent parameter
settings. Thus, a train-validate-test process is ap-
propriate. In Table 8, the numbers given under,
e.g., W9-10, are the results obtained when W9-10
is used as the test set. One of the other datasets,
say W9-22, was used as the training set, meaning
that all the adjectives (verbs) in that dataset are
the seed words, and all ltering was performed us-
ing only that data. The seed-ltering process was
repeated with dierent settings of C and FT , pro-
ducing a dierent set of adjectives (verbs) for each
setting. A third dataset, say W9-33, was used as a
validation set, i.e., among all the sets of adjectives
generated from the training set, those with good
performance on the validation set were selected as
WSJ-SE-M WSJ-SE-D NG-SE-M
MultInst MultType MultInst MultType MultInst MultType
413 .17 378 .16 571 .29
Table 7: Word-POS-Types Used in Multiple Types of Subjective Elements
W9-10 W9-22 W9-33 W9-04
freq +prec freq +prec freq +prec freq +prec
adjectives 373 .21 1340 .11 2137 .09 2537 .14
verbs 721 .16 1436 .08 3139 .07 3720 .11
unique words 6065 .10 5441 .07 6045 .06 6171 .09
baseline precision .17 .13 .14 .18
freq: Total frequency +prec: Increase in precision over baseline
Table 8: Frequencies and Increases in Precision
the PSEs to test on the test set. A set was consid-
ered to have good performance on the validation
set if its precision is at least .25 and its frequency
is at least 100. Since this process is meant to
be a method for mining existing document-level
annotations for PSEs, the existing opinion-piece
annotations were used for training and validation.
Our manual opinion-piece annotations were used
for testing.
The row labeled unique words shows the preci-
sion on the test set of the individual words that
are unique in the test set. The increase over base-
line precision shows that low-frequency words can
be informative for recognizing subjectivity.
Note that the features all do better and worse
on the same data sets. This shows that the subjec-
tivity is somehow harder to identify in, say, W9-33
than in W9-10; it also shows an important consis-
tency among the features, even though they are
identied in dierent ways.
8 Conclusions
This paper presents the results of an empirical ex-
amination of subjectivity at the dierent levels of
a text: the expression level, the sentence level,
and the document level. While analysis of subjec-
tivity is perhaps most natural and precise at the
expression level, document-level annotations are
freely available from a number of sources and are
appropriate for many applications. The sentence-
level annotation is a workable intermediate level:
sentence-level judgments are not as ne-grained as
expression-level judgments, and they don't involve
the large amount of noise found at the document
level.
As part of this examination, we present a study
of annotator agreement characterizing the di-
culty of identifying subjectivity at the dierent
levels of a text. The results demonstrate that not
only can subjectivity be identied at the docu-
ment level with high reliability, but that it is also
possible to identify expression-level subjectivity,
albeit with lower reliability.
Using manual annotations, we are able to char-
acterize subjective language. At the expression
level, we found that it is natural to distinguish
among positively evaluative, negatively evalua-
tive, and speculative uses of a word. We also
found that subjective text contains a high pro-
portion of unique word occurrences, much more so
than ordinary text. Rather than ignoring or dis-
carding unique words, we demonstrate that the
occurrence of a unique word is a PSE. We also
found that agreement is higher for some syntac-
tic word classes, e.g., for adjectives in comparison
with determiners.
Finally, we are able to mine PSEs from text
tagged at the document level. Given the diculty
of evaluating PSEs in document-level subjectiv-
ity classication due to the mix of subjective and
objective sentences, the PSEs identied in this
study exhibit relatively high precision. In future
work, we will investigate document-level classi-
cation using these PSEs, as well as other methods
for extracting PSEs from text tagged at the doc-
ument level; methods to be investigated include
mutual-bootstrapping and/or co-training.
References
C. Aone, M. Ramos-Santacruz, and W. Niehaus.
2000. Assentor: An nlp-based solution to e-mail
monitoring. In Proc. IAAI-2000, pages 945{
950.
A. Baneld. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
R. Barzilay, M. Collins, J. Hirschberg, and
S. Whittaker. 2000. The rules behind roles:
Identifying speaker role in radio broadcasts. In
Proc. AAAI.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of the 3rd Conference on Ap-
plied Natural Language Processing (ANLP-92),
pages 152{155.
R. Bruce and J. Wiebe. 1999. Recognizing subjec-
tivity: A case study of manual tagging. Natural
Language Engineering, 5(2).
J. Cohen. 1960. A coecient of agreement for
nominal scales. Educational and Psychological
Meas., 20:37{46.
A. P. Dawid and A. M. Skene. 1979. Max-
imum likelihood estimation of observer error-
rates using the EM algorithm. Applied Statis-
tics, 28:20{28.
M. Fludernik. 1993. The Fictions of Language
and the Languages of Fiction. Routledge, Lon-
don.
L. Goodman. 1974. Exploratory latent structure
analysis using both identiable and unidenti-
able models. Biometrika, 61:2:215{231.
E. Hovy. 1987. Generating Natural Language un-
der Pragmatic Constraints. Ph.D. thesis, Yale
University.
D. Karp, Y. Schabes, M. Zaidel, and D. Egedi.
1992. A freely available wide coverage mor-
phological analyzer for English. In Proc. of
the 14th International Conference on Compu-
tational Linguistics (COLING-92).
D. Kaufer. 2000. Flaming: A White Paper.
www.eudora.com.
B. Kessler, G. Nunberg, and H. Schutze. 1997.
Automatic detection of text genre. In Proc.
ACL-EACL-97.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. COLING-ACL '98,
pages 768{773.
Diane J. Litman and R. J. Passonneau. 1995.
Combining multiple knowledge sources for dis-
course segmentation. In Proc. 33rd Annual
Meeting of the Association for Computational
Linguistics (ACL-95), pages 108{115. Associa-
tion for Computational Linguistics, june.
D. Marcu, M. Romera, and E. Amorrortu. 1999.
Experiments in constructing a corpus of dis-
course trees: Problems, annotation choices, is-
sues. In The Workshop on Levels of Represen-
tation in Discourse, pages 71{78.
M. Marcus, Santorini, B., and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The penn treebank. Computational Lin-
guistics, 19(2):313{330.
G. Nunberg, I. Sag, and T. Wasow. 1994. Idioms.
Language, 70:491{538.
W. Sack. 1995. Representing and recognizing
point of view. In Proc. AAAI Fall Symposium
on AI Applications in Knowledge Navigation
and Retrieval.
E. Spertus. 1997. Smokey: Automatic recogni-
tion of hostile messages. In Proc. IAAI.
D. Stein and S. Wright, editors. 1995. Subjectiv-
ity and Subjectivisation. Cambridge University
Press, Cambridge.
L. Terveen, W. Hill, B. Amento, D. McDonald,
and J. Creter. 1997. Building task-specic in-
terfaces to high volume conversational data. In
Proc. CHI 97, pages 226{233.
S. Teufel and M. Moens. 2000. What's yours and
what's mine: Determining intellectual attribu-
tion in scientic texts. In Proc. Joint SIGDAT
Converence on EMNLP and VLC.
J. Wiebe, K. McKeever, and R. Bruce. 1998.
Mapping collocational properties into machine
learning features. In Proc. 6th Workshop on
Very Large Corpora (WVLC-98), pages 225{
233, Montreal, Canada, August. ACL SIGDAT.
J. Wiebe, R. Bruce, and T. O'Hara. 1999. Devel-
opment and use of a gold standard data set for
subjectivity classications. In Proc. 37th An-
nual Meeting of the Assoc. for Computational
Linguistics (ACL-99), pages 246{253, Univer-
sity of Maryland, June. ACL.
J. Wiebe. 1994. Tracking point of view in narra-
tive. Computational Linguistics, 20(2):233{287.
J. Wiebe. 2000. Learning subjective adjectives
from corpora. In 17th National Conference on
Articial Intelligence (AAAI-2000).
           	  
                                    
 
    

  Learning Subjective Nouns using Extraction Pattern Bootstrapping?
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Abstract
We explore the idea of creating a subjectiv-
ity classifier that uses lists of subjective nouns
learned by bootstrapping algorithms. The goal
of our research is to develop a system that
can distinguish subjective sentences from ob-
jective sentences. First, we use two bootstrap-
ping algorithms that exploit extraction patterns
to learn sets of subjective nouns. Then we
train a Naive Bayes classifier using the subjec-
tive nouns, discourse features, and subjectivity
clues identified in prior research. The boot-
strapping algorithms learned over 1000 subjec-
tive nouns, and the subjectivity classifier per-
formed well, achieving 77% recall with 81%
precision.
1 Introduction
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculation. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
? This work was supported in part by the National Sci-
ence Foundation under grants IIS-0208798 and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Reseach Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Subjective language has been previously studied in
fields such as linguistics, literary theory, psychology, and
content analysis. Some manually-developed knowledge
resources exist, but there is no comprehensive dictionary
of subjective language.
Meta-Bootstrapping (Riloff and Jones, 1999) and
Basilisk (Thelen and Riloff, 2002) are bootstrapping al-
gorithms that use automatically generated extraction pat-
terns to identify words belonging to a semantic cate-
gory. We hypothesized that extraction patterns could
also identify subjective words. For example, the pat-
tern ?expressed <direct object>? often extracts subjec-
tive nouns, such as ?concern?, ?hope?, and ?support?.
Furthermore, these bootstrapping algorithms require only
a handful of seed words and unannotated texts for train-
ing; no annotated data is needed at all.
In this paper, we use the Meta-Bootstrapping and
Basilisk algorithms to learn lists of subjective nouns from
a large collection of unannotated texts. Then we train
a subjectivity classifier on a small set of annotated data,
using the subjective nouns as features along with some
other previously identified subjectivity features. Our ex-
perimental results show that the subjectivity classifier
performs well (77% recall with 81% precision) and that
the learned nouns improve upon previous state-of-the-art
subjectivity results (Wiebe et al, 1999).
2 Subjectivity Data
2.1 The Annotation Scheme
In 2002, an annotation scheme was developed
for a U.S. government-sponsored project with a
team of 10 researchers (the annotation instruc-
tions and project reports are available on the Web
at http://www.cs.pitt.edu/?wiebe/pubs/ardasummer02/).
                                                               Edmonton, May-June 2003
                                                    held at HLT-NAACL 2003 , pp. 25-32
                                            Proceeings of the Seventh CoNLL conference
The scheme was inspired by work in linguistics and
literary theory on subjectivity, which focuses on how
opinions, emotions, etc. are expressed linguistically in
context (Banfield, 1982). The scheme is more detailed
and comprehensive than previous ones. We mention only
those aspects of the annotation scheme relevant to this
paper.
The goal of the annotation scheme is to identify and
characterize expressions of private states in a sentence.
Private state is a general covering term for opinions, eval-
uations, emotions, and speculations (Quirk et al, 1985).
For example, in sentence (1) the writer is expressing a
negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.?
Sentence (2) reflects the private state of Western coun-
tries. Mugabe?s use of ?overwhelmingly? also reflects a
private state, his positive reaction to and characterization
of his victory.
(2) ?Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe?s presidential election.?
Annotators are also asked to judge the strength of each
private state. A private state can have low, medium, high
or extreme strength.
2.2 Corpus and Agreement Results
Our data consists of English-language versions of foreign
news documents from FBIS, the U.S. Foreign Broadcast
Information Service. The data is from a variety of publi-
cations and countries. The annotated corpus used to train
and test our subjectivity classifiers (the experiment cor-
pus) consists of 109 documents with a total of 2197 sen-
tences. We used a separate, annotated tuning corpus of
33 documents with a total of 698 sentences to establish
some experimental parameters.1
Each document was annotated by one or both of two
annotators, A and T. To allow us to measure interanno-
tator agreement, the annotators independently annotated
the same 12 documents with a total of 178 sentences. We
began with a strict measure of agreement at the sentence
level by first considering whether the annotator marked
any private-state expression, of any strength, anywhere
in the sentence. If so, the sentence should be subjective.
Otherwise, it is objective. Table 1 shows the contingency
table. The percentage agreement is 88%, and the ? value
is 0.71.
1The annotated data will be available to U.S. government
contractors this summer. We are working to resolve copyright
issues to make it available to the wider research community.
Tagger T
Subj Obj
Tagger A Subj nyy = 112 nyn = 16
Obj nny = 6 nnn = 44
Table 1: Agreement for sentence-level annotations
Tagger T
Subj Obj
Tagger A Subj nyy = 106 nyn = 9
Obj nny = 0 nnn = 44
Table 2: Agreement for sentence-level annotations, low-
strength cases removed
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define
a sentence as borderline if it has at least one private-
state expression identified by at least one annotator, and
all strength ratings of private-state expressions are low.
Table 2 shows the agreement results when such border-
line sentences are removed (19 sentences, or 11% of the
agreement test corpus). The percentage agreement in-
creases to 94% and the ? value increases to 0.87.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use in our experiments. A sentence is subjective if it
contains at least one private-state expression of medium
or higher strength. The second class, which we call ob-
jective, consists of everything else. Thus, sentences with
only mild traces of subjectivity are tossed into the objec-
tive category, making the system?s goal to find the clearly
subjective sentences.
3 Using Extraction Patterns to Learn
Subjective Nouns
In the last few years, two bootstrapping algorithms have
been developed to create semantic dictionaries by ex-
ploiting extraction patterns: Meta-Bootstrapping (Riloff
and Jones, 1999) and Basilisk (Thelen and Riloff, 2002).
Extraction patterns were originally developed for infor-
mation extraction tasks (Cardie, 1997). They represent
lexico-syntactic expressions that typically rely on shal-
low parsing and syntactic role assignment. For example,
the pattern ?<subject> was hired? would apply to sen-
tences that contain the verb ?hired? in the passive voice.
The subject would be extracted as the hiree.
Meta-Bootstrapping and Basilisk were designed to
learn words that belong to a semantic category (e.g.,
?truck? is a VEHICLE and ?seashore? is a LOCATION).
Both algorithms begin with unannotated texts and seed
words that represent a semantic category. A bootstrap-
ping process looks for words that appear in the same ex-
traction patterns as the seeds and hypothesizes that those
words belong to the same semantic class. The principle
behind this approach is that words of the same semantic
class appear in similar pattern contexts. For example, the
phrases ?lived in? and ?traveled to? will co-occur with
many noun phrases that represent LOCATIONS.
In our research, we want to automatically identify
words that are subjective. Subjective terms have many
different semantic meanings, but we believe that the same
contextual principle applies to subjectivity. In this sec-
tion, we briefly overview these bootstrapping algorithms
and explain how we used them to generate lists of subjec-
tive nouns.
3.1 Meta-Bootstrapping
The Meta-Bootstrapping (?MetaBoot?) process (Riloff
and Jones, 1999) begins with a small set of seed words
that represent a targeted semantic category (e.g., 10
words that represent LOCATIONS) and an unannotated
corpus. First, MetaBoot automatically creates a set of ex-
traction patterns for the corpus by applying and instanti-
ating syntactic templates. This process literally produces
thousands of extraction patterns that, collectively, will ex-
tract every noun phrase in the corpus. Next, MetaBoot
computes a score for each pattern based upon the num-
ber of seed words among its extractions. The best pat-
tern is saved and all of its extracted noun phrases are
automatically labeled as the targeted semantic category.2
MetaBoot then re-scores the extraction patterns, using the
original seed words as well as the newly labeled words,
and the process repeats. This procedure is called mutual
bootstrapping.
A second level of bootstrapping (the ?meta-? boot-
strapping part) makes the algorithm more robust. When
the mutual bootstrapping process is finished, all nouns
that were put into the semantic dictionary are re-
evaluated. Each noun is assigned a score based on how
many different patterns extracted it. Only the five best
nouns are allowed to remain in the dictionary. The other
entries are discarded, and the mutual bootstrapping pro-
cess starts over again using the revised semantic dictio-
nary.
3.2 Basilisk
Basilisk (Thelen and Riloff, 2002) is a more recent boot-
strapping algorithm that also utilizes extraction patterns
to create a semantic dictionary. Similarly, Basilisk be-
gins with an unannotated text corpus and a small set of
2Our implementation of Meta-Bootstrapping learns individ-
ual nouns (vs. noun phrases) and discards capitalized words.
seed words for a semantic category. The bootstrapping
process involves three steps. (1) Basilisk automatically
generates a set of extraction patterns for the corpus and
scores each pattern based upon the number of seed words
among its extractions. This step is identical to the first
step of Meta-Bootstrapping. Basilisk then puts the best
patterns into a Pattern Pool. (2) All nouns3 extracted by a
pattern in the Pattern Pool are put into a Candidate Word
Pool. Basilisk scores each noun based upon the set of
patterns that extracted it and their collective association
with the seed words. (3) The top 10 nouns are labeled as
the targeted semantic class and are added to the dictio-
nary. The bootstrapping process then repeats, using the
original seeds and the newly labeled words.
The main difference between Basilisk and Meta-
Bootstrapping is that Basilisk scores each noun based
on collective information gathered from all patterns that
extracted it. In contrast, Meta-Bootstrapping identifies
a single best pattern and assumes that everything it ex-
tracted belongs to the same semantic class. The second
level of bootstrapping smoothes over some of the prob-
lems caused by this assumption. In comparative experi-
ments (Thelen and Riloff, 2002), Basilisk outperformed
Meta-Bootstrapping. But since our goal of learning sub-
jective nouns is different from the original intent of the
algorithms, we tried them both. We also suspected they
might learn different words, in which case using both al-
gorithms could be worthwhile.
3.3 Experimental Results
The Meta-Bootstrapping and Basilisk algorithms need
seed words and an unannotated text corpus as input.
Since we did not need annotated texts, we created a much
larger training corpus, the bootstrapping corpus, by gath-
ering 950 new texts from the FBIS source mentioned
in Section 2.2. To find candidate seed words, we auto-
matically identified 850 nouns that were positively corre-
lated with subjective sentences in another data set. How-
ever, it is crucial that the seed words occur frequently
in our FBIS texts or the bootstrapping process will not
get off the ground. So we searched for each of the 850
nouns in the bootstrapping corpus, sorted them by fre-
quency, and manually selected 20 high-frequency words
that we judged to be strongly subjective. Table 3 shows
the 20 seed words used for both Meta-Bootstrapping and
Basilisk.
We ran each bootstrapping algorithm for 400 itera-
tions, generating 5 words per iteration. Basilisk gener-
ated 2000 nouns and Meta-Bootstrapping generated 1996
nouns.4 Table 4 shows some examples of extraction pat-
3Technically, each head noun of an extracted noun phrase.
4Meta-Bootstrapping will sometimes produce fewer than 5
words per iteration if it has low confidence in its judgements.
cowardice embarrassment hatred outrage
crap fool hell slander
delight gloom hypocrisy sigh
disdain grievance love twit
dismay happiness nonsense virtue
Table 3: Subjective Seed Words
Extraction Patterns Examples of Extracted Nouns
expressed <dobj> condolences, hope, grief,
views, worries, recognition
indicative of <np> compromise, desire, thinking
inject <dobj> vitality, hatred
reaffirmed <dobj> resolve, position, commitment
voiced <dobj> outrage, support, skepticism,
disagreement, opposition,
concerns, gratitude, indignation
show of <np> support, strength, goodwill,
solidarity, feeling
<subject> was shared anxiety, view, niceties, feeling
Table 4: Extraction Pattern Examples
terns that were discovered to be associated with subjec-
tive nouns.
Meta-Bootstrapping and Basilisk are semi-automatic
lexicon generation tools because, although the bootstrap-
ping process is 100% automatic, the resulting lexicons
need to be reviewed by a human.5 So we manually re-
viewed the 3996 words proposed by the algorithms. This
process is very fast; it takes only a few seconds to classify
each word. The entire review process took approximately
3-4 hours. One author did this labeling; this person did
not look at or run tests on the experiment corpus.
Strong Subjective Weak Subjective
tyranny scum aberration plague
smokescreen bully allusion risk
apologist devil apprehensions drama
barbarian liar beneficiary trick
belligerence pariah resistant promise
condemnation venom credence intrigue
sanctimonious diatribe distortion unity
exaggeration mockery eyebrows failures
repudiation anguish inclination tolerance
insinuation fallacies liability persistent
antagonism evil assault trust
atrocities genius benefit success
denunciation goodwill blood spirit
exploitation injustice controversy slump
humiliation innuendo likelihood sincerity
ill-treatment revenge peaceful eternity
sympathy rogue pressure rejection
Table 5: Examples of Learned Subjective Nouns
5This is because NLP systems expect dictionaries to have
high integrity. Even if the algorithms could achieve 90% ac-
curacy, a dictionary in which 1 of every 10 words is defined
incorrectly would probably not be desirable.
B M B ? M B ? M
StrongSubj 372 192 110 454
WeakSubj 453 330 185 598
Total 825 522 295 1052
Table 6: Subjective Word Lexicons after Manual Review
(B=Basilisk, M=MetaBootstrapping)
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 200 400 600 800 1000 1200 1400 1600 1800 2000
%
 o
f W
or
ds
 S
ub
jec
tiv
e
Number of Words Generated
?Basilisk?
?MetaBoot?
Figure 1: Accuracy during Bootstrapping
We classified the words as StrongSubjective, WeakSub-
jective, or Objective. Objective terms are not subjective at
all (e.g., ?chair? or ?city?). StrongSubjective terms have
strong, unambiguously subjective connotations, such as
?bully? or ?barbarian?. WeakSubjective was used for
three situations: (1) words that have weak subjective con-
notations, such as ?aberration? which implies something
out of the ordinary but does not evoke a strong sense of
judgement, (2) words that have multiple senses or uses,
where one is subjective but the other is not. For example,
the word ?plague? can refer to a disease (objective) or an
onslaught of something negative (subjective), (3) words
that are objective by themselves but appear in idiomatic
expressions that are subjective. For example, the word
?eyebrows? was labeled WeakSubjective because the ex-
pression ?raised eyebrows? probably occurs more often
in our corpus than literal references to ?eyebrows?. Ta-
ble 5 shows examples of learned words that were classi-
fied as StrongSubjective or WeakSubjective.
Once the words had been manually classified, we could
go back and measure the effectiveness of the algorithms.
The graph in Figure 1 tracks their accuracy as the boot-
strapping progressed. The X-axis shows the number of
words generated so far. The Y-axis shows the percent-
age of those words that were manually classified as sub-
jective. As is typical of bootstrapping algorithms, ac-
curacy was high during the initial iterations but tapered
off as the bootstrapping continued. After 20 words,
both algorithms were 95% accurate. After 100 words
Basilisk was 75% accurate and MetaBoot was 81% accu-
rate. After 1000 words, accuracy dropped to about 28%
for MetaBoot, but Basilisk was still performing reason-
ably well at 53%. Although 53% accuracy is not high for
a fully automatic process, Basilisk depends on a human
to review the words so 53% accuracy means that the hu-
man is accepting every other word, on average. Thus, the
reviewer?s time was still being spent productively even
after 1000 words had been hypothesized.
Table 6 shows the size of the final lexicons created
by the bootstrapping algorithms. The first two columns
show the number of subjective terms learned by Basilisk
and Meta-Bootstrapping. Basilisk was more prolific, gen-
erating 825 subjective terms compared to 522 for Meta-
Bootstrapping. The third column shows the intersection
between their word lists. There was substantial overlap,
but both algorithms produced many words that the other
did not. The last column shows the results of merging
their lists. In total, the bootstrapping algorithms produced
1052 subjective nouns.
4 Creating Subjectivity Classifiers
To evaluate the subjective nouns, we trained a Naive
Bayes classifier using the nouns as features. We also in-
corporated previously established subjectivity clues, and
added some new discourse features. In this section, we
describe all the feature sets and present performance re-
sults for subjectivity classifiers trained on different com-
binations of these features. The threshold values and fea-
ture representations used in this section are the ones that
produced the best results on our separate tuning corpus.
4.1 Subjective Noun Features
We defined four features to represent the sets of subjec-
tive nouns produced by the bootstrapping algorithms.
BA-Strong: the set of StrongSubjective nouns generated
by Basilisk
BA-Weak: the set of WeakSubjective nouns generated
by Basilisk
MB-Strong: the set of StrongSubjective nouns generated
by Meta-Bootstrapping
MB-Weak: the set of WeakSubjective nouns generated
by Meta-Bootstrapping
For each set, we created a three-valued feature based on
the presence of 0, 1, or ? 2 words from that set. We used
the nouns as feature sets, rather than define a separate
feature for each word, so the classifier could generalize
over the set to minimize sparse data problems. We will
refer to these as the SubjNoun features.
4.2 Previously Established Features
Wiebe, Bruce, & O?Hara (1999) developed a machine
learning system to classify subjective sentences. We ex-
perimented with the features that they used, both to com-
pare their results to ours and to see if we could benefit
from their features. We will refer to these as the WBO
features.
WBO includes a set of stems positively correlated with
the subjective training examples (subjStems) and a set
of stems positively correlated with the objective training
examples (objStems). We defined a three-valued feature
for the presence of 0, 1, or ? 2 members of subjStems
in a sentence, and likewise for objStems. For our exper-
iments, subjStems includes stems that appear ? 7 times
in the training set, and for which the precision is 1.25
times the baseline word precision for that training set.
objStems contains the stems that appear ? 7 times and
for which at least 50% of their occurrences in the training
set are in objective sentences. WBO also includes a bi-
nary feature for each of the following: the presence in the
sentence of a pronoun, an adjective, a cardinal number, a
modal other than will, and an adverb other than not.
We also added manually-developed features found by
other researchers. We created 14 feature sets represent-
ing some classes from (Levin, 1993; Ballmer and Bren-
nenstuhl, 1981), some Framenet lemmas with frame ele-
ment experiencer (Baker et al, 1998), adjectives manu-
ally annotated for polarity (Hatzivassiloglou and McKe-
own, 1997), and some subjectivity clues listed in (Wiebe,
1990). We represented each set as a three-valued feature
based on the presence of 0, 1, or ? 2 members of the set.
We will refer to these as the manual features.
4.3 Discourse Features
We created discourse features to capture the density of
clues in the text surrounding a sentence. First, we com-
puted the average number of subjective clues and objec-
tive clues per sentence, normalized by sentence length.
The subjective clues, subjClues, are all sets for which
3-valued features were defined above (except objStems).
The objective clues consist only of objStems. For sen-
tence S, let ClueRatesubj(S) = |subjClues in S||S| and
ClueRateobj(S) = |objStems in S||S| . Then we define
AvgClueRatesubj to be the average of ClueRate(S)
over all sentences S and similarly for AvgClueRateobj.
Next, we characterize the number of subjective and
objective clues in the previous and next sentences as:
higher-than-expected (high), lower-than-expected (low),
or expected (medium). The value for ClueRatesubj(S)
is high if ClueRatesubj(S) ? AvgClueRatesubj ? 1.3;
low if ClueRatesubj(S) ? AvgClueRatesubj/1.3; oth-
erwise it is medium. The values for ClueRateobj(S) are
defined similarly.
Using these definitions we created four features:
ClueRatesubj for the previous and following sen-
tences, and ClueRateobj for the previous and follow-
ing sentences. We also defined a feature for sentence
length. Let AvgSentLen be the average sentence length.
SentLen(S) is high if length(S) ? AvgSentLen?1.3;
low if length(S) ? AvgSentLen/1.3; and medium oth-
erwise.
4.4 Classification Results
We conducted experiments to evaluate the performance
of the feature sets, both individually and in various com-
binations. Unless otherwise noted, all experiments in-
volved training a Naive Bayes classifier using a particu-
lar set of features. We evaluated each classifier using 25-
fold cross validation on the experiment corpus and used
paired t-tests to measure significance at the 95% confi-
dence level. As our evaluation metrics, we computed ac-
curacy (Acc) as the percentage of the system?s classifica-
tions that match the gold-standard, and precision (Prec)
and recall (Rec) with respect to subjective sentences.
Acc Prec Rec
(1) Bag-Of-Words 73.3 81.7 70.9
(2) WBO 72.1 76.0 77.4
(3) Most-Frequent 59.0 59.0 100.0
Table 7: Baselines for Comparison
Table 7 shows three baseline experiments. Row (3)
represents the common baseline of assigning every sen-
tence to the most frequent class. The Most-Frequent
baseline achieves 59% accuracy because 59% of the sen-
tences in the gold-standard are subjective. Row (2) is
a Naive Bayes classifier that uses the WBO features,
which performed well in prior research on sentence-level
subjectivity classification (Wiebe et al, 1999). Row (1)
shows a Naive Bayes classifier that uses unigram bag-of-
words features, with one binary feature for the absence
or presence in the sentence of each word that appeared
during training. Pang et al (2002) reported that a similar
experiment produced their best results on a related clas-
sification task. The difference in accuracy between Rows
(1) and (2) is not statistically significant (Bag-of-Word?s
higher precision is balanced by WBO?s higher recall).
Next, we trained a Naive Bayes classifier using only
the SubjNoun features. This classifier achieved good
precision (77%) but only moderate recall (64%). Upon
further inspection, we discovered that the subjective
nouns are good subjectivity indicators when they appear,
but not every subjective sentence contains one of them.
And, relatively few sentences contain more than one,
making it difficult to recognize contextual effects (i.e.,
multiple clues in a region). We concluded that the ap-
propriate way to benefit from the subjective nouns is to
use them in tandem with other subjectivity clues.
Acc Prec Rec
(1) 76.1 81.3 77.4 WBO+SubjNoun+
manual+discourse
(2) 74.3 78.6 77.8 WBO+SubjNoun
(3) 72.1 76.0 77.4 WBO
Table 8: Results with New Features
Table 8 shows the results of Naive Bayes classifiers
trained with different combinations of features. The ac-
curacy differences between all pairs of experiments in
Table 8 are statistically significant. Row (3) uses only
the WBO features (also shown in Table 7 as a baseline).
Row (2) uses the WBO features as well as the SubjNoun
features. There is a synergy between these feature sets:
using both types of features achieves better performance
than either one alone. The difference is mainly precision,
presumably because the classifier found more and better
combinations of features. In Row (1), we also added the
manual and discourse features. The discourse features
explicitly identify contexts in which multiple clues are
found. This classifier produced even better performance,
achieving 81.3% precision with 77.4% recall. The 76.1%
accuracy result is significantly higher than the accuracy
results for all of the other classifiers (in both Table 8 and
Table 7).
Finally, higher precision classification can be obtained
by simply classifying a sentence as subjective if it con-
tains any of the StrongSubjective nouns. On our data, this
method produces 87% precision with 26% recall. This
approach could support applications for which precision
is paramount.
5 Related Work
Several types of research have involved document-level
subjectivity classification. Some work identifies inflam-
matory texts (e.g., (Spertus, 1997)) or classifies reviews
as positive or negative ((Turney, 2002; Pang et al, 2002)).
Tong?s system (Tong, 2001) generates sentiment time-
lines, tracking online discussions and creating graphs of
positive and negative opinion messages over time. Re-
search in genre classification may include recognition of
subjective genres such as editorials (e.g., (Karlgren and
Cutting, 1994; Kessler et al, 1997; Wiebe et al, 2001)).
In contrast, our work classifies individual sentences, as
does the research in (Wiebe et al, 1999). Sentence-level
subjectivity classification is useful because most docu-
ments contain a mix of subjective and objective sen-
tences. For example, newspaper articles are typically
thought to be relatively objective, but (Wiebe et al, 2001)
reported that, in their corpus, 44% of sentences (in arti-
cles that are not editorials or reviews) were subjective.
Some previous work has focused explicitly on learn-
ing subjective words and phrases. (Hatzivassiloglou and
McKeown, 1997) describes a method for identifying the
semantic orientation of words, for example that beauti-
ful expresses positive sentiments. Researchers have fo-
cused on learning adjectives or adjectival phrases (Tur-
ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe,
2000) and verbs (Wiebe et al, 2001), but no previous
work has focused on learning nouns. A unique aspect
of our work is the use of bootstrapping methods that ex-
ploit extraction patterns. (Turney, 2002) used patterns
representing part-of-speech sequences, (Hatzivassiloglou
and McKeown, 1997) recognized adjectival phrases, and
(Wiebe et al, 2001) learned N-grams. The extraction
patterns used in our research are linguistically richer pat-
terns, requiring shallow parsing and syntactic role assign-
ment.
In recent years several techniques have been developed
for semantic lexicon creation (e.g., (Hearst, 1992; Riloff
and Shepherd, 1997; Roark and Charniak, 1998; Cara-
ballo, 1999)). Semantic word learning is different from
subjective word learning, but we have shown that Meta-
Bootstrapping and Basilisk could be successfully applied
to subjectivity learning. Perhaps some of these other
methods could also be used to learn subjective words.
6 Conclusions
This research produced interesting insights as well as per-
formance results. First, we demonstrated that weakly
supervised bootstrapping techniques can learn subjec-
tive terms from unannotated texts. Subjective features
learned from unannotated documents can augment or en-
hance features learned from annotated training data us-
ing more traditional supervised learning techniques. Sec-
ond, Basilisk and Meta-Bootstrapping proved to be use-
ful for a different task than they were originally intended.
By seeding the algorithms with subjective words, the ex-
traction patterns identified expressions that are associated
with subjective nouns. This suggests that the bootstrap-
ping algorithms should be able to learn not only general
semantic categories, but any category for which words
appear in similar linguistic phrases. Third, our best sub-
jectivity classifier used a wide variety of features. Sub-
jectivity is a complex linguistic phenomenon and our evi-
dence suggests that reliable subjectivity classification re-
quires a broad array of features.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley
framenet project. In Proceedings of the COLING-ACL.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Clas-
sification: A Study in the Lexical Analysis of English
Speech Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge
and Kegan Paul, Boston.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 120?126.
C. Cardie. 1997. Empirical Methods in Information Ex-
traction. AI Magazine, 18(4):65?79.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In ACL-EACL
1997.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th Interna-
tional Conference on Computational Linguistics.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
COLING-94.
B. Kessler, G. Nunberg, and H. Schutze. 1997. Auto-
matic detection of text genre. In Proc. ACL-EACL-97.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the 16th National Conference on Ar-
tificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase
Co-occurrence Statistics for Semi-automatic Seman-
tic Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. IAAI.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing.
R. Tong. 2001. An operational system for detecting and
tracking opinions in on-line discussion. In SIGIR 2001
Workshop on Operational Text Classification.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Orientation Applied to Unsupervised Classification
of Reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99).
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe. 1990. Recognizing Subjective Sentences: A
Computational Investigation of Narrative Text. Ph.D.
thesis, State University of New York at Buffalo.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In 17th National Conference on Artificial Intelli-
gence.
Preposition Semantic Classification via PENN TREEBANK and FRAMENET
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003
tomohara@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper reports on experiments in clas-
sifying the semantic role annotations as-
signed to prepositional phrases in both the
PENN TREEBANK and FRAMENET. In
both cases, experiments are done to see
how the prepositions can be classified
given the dataset?s role inventory, using
standard word-sense disambiguation fea-
tures. In addition to using traditional word
collocations, the experiments incorporate
class-based collocations in the form of
WordNet hypernyms. For Treebank, the
word collocations achieve slightly better
performance: 78.5% versus 77.4% when
separate classifiers are used per preposi-
tion. When using a single classifier for
all of the prepositions together, the com-
bined approach yields a significant gain at
85.8% accuracy versus 81.3% for word-
only collocations. For FrameNet, the
combined use of both collocation types
achieves better performance for the indi-
vidual classifiers: 70.3% versus 68.5%.
However, classification using a single
classifier is not effective due to confusion
among the fine-grained roles.
1 Introduction
English prepositions convey important relations in
text. When used as verbal adjuncts, they are the prin-
ciple means of conveying semantic roles for the sup-
porting entities described by the predicate. Preposi-
tions are highly ambiguous. A typical collegiate dic-
tionary has dozens of senses for each of the common
prepositions. These senses tend to be closely related,
in contrast to the other parts of speech where there
might be a variety of distinct senses.
Given the recent advances in word-sense disam-
biguation, due in part to SENSEVAL (Edmonds and
Cotton, 2001), it would seem natural to apply the
same basic approach to handling the disambiguation
of prepositions. Of course, it is difficult to disam-
biguate prepositions at the granularity present in col-
legiate dictionaries, as illustrated later. Nonetheless,
in certain cases this is feasible.
We provide results for disambiguating preposi-
tions at two different levels of granularity. The
coarse granularity is more typical of earlier work in
computational linguistics, such as the role inventory
proposed by Fillmore (1968), including high-level
roles such as instrument and location. Recently, sys-
tems have incorporated fine-grained roles, often spe-
cific to particular domains. For example, in the Cyc
KB there are close to 200 different types of seman-
tic roles. These range from high-level roles (e.g.,
beneficiaries) through medium-level roles (e.g., ex-
changes) to highly specialized roles (e.g., catalyst).1
Preposition classification using two different se-
mantic role inventories are investigated in this pa-
per, taking advantage of large annotated corpora.
After providing background to the work in Sec-
tion 2, experiments over the semantic role anno-
tations are discussed in Section 3. The results
over TREEBANK (Marcus et al, 1994) are covered
first. Treebank include about a dozen high-level
roles similar to Fillmore?s. Next, experiments us-
ing the finer-grained semantic role annotations in
FRAMENET version 0.75 (Fillmore et al, 2001) are
1Part of the Cyc KB is freely available at www.opencyc.org.
presented. FrameNet includes over 140 roles, ap-
proaching but not quite as specialized as Cyc?s in-
ventory. Section 4 follows with a comparison to
related work, emphasizing work in broad-coverage
preposition disambiguation.
2 Background
2.1 Semantic roles in the PENN TREEBANK
The second version of the Penn Treebank (Marcus
et al, 1994) added additional clause usage informa-
tion to the parse tree annotations that are popular
for natural language learning. This includes a few
case-style relation annotations, which prove useful
for disambiguating prepositions. For example, here
is a simple parse tree with the new annotation for-
mat:
(S (NP-TPC-5 This)
(NP-SBJ every man)
(VP contains
(NP *T*-5)
(PP-LOC within
(NP him))))
This shows that the prepositional phrase (PP) is pro-
viding the location for the state described by the verb
phrase. Treating this as the preposition sense would
yield the following annotation:
This every man contains within
LOC
him
The main semantic relations in TREEBANK are
beneficiary, direction, spatial extent, manner, loca-
tion, purpose/reason, and temporal. These tags can
be applied to any verb complement but normally oc-
cur with clauses, adverbs, and prepositions. Fre-
quency counts for the prepositional phrase (PP) case
role annotations are shown in Table 1.
The frequencies for the most frequent preposi-
tions that have occurred in the prepositional phrase
annotations are shown later in Table 7. The table
is ordered by entropy, which measures the inherent
ambiguity in the classes as given by the annotations.
Note that the Baseline column is the probability of
the most frequent sense, which is a common esti-
mate of the lower bound for classification experi-
ments.
2.2 Semantic roles in FRAMENET
Berkeley?s FRAMENET (Fillmore et al, 2001)
project provides the most recent large-scale anno-
tation of semantic roles. These are at a much finer
granularity than those in TREEBANK, so they should
prove quite useful for applications that learn detailed
semantics from corpora. Table 2 shows the top se-
mantic roles by frequency of annotation. This il-
lustrates that the semantic roles in Framenet can be
quite specific, as in the roles cognizer, judge, and
addressee. In all, there are over 140 roles annotated
with over 117,000 tagged instances.
FRAMENET annotations occur at the phrase level
instead of the grammatical constituent level as in
TREEBANK. The cases that involve prepositional
phrases can be determined by the phrase-type at-
tribute of the annotation. For example, consider the
following annotation.
?S TPOS=?56879338??
?T TYPE=?sense2???/T?
Itpnp hadvhd aat0 sharpaj0
,pun pointedaj0 facenn1 andcjc
?C FE=?BodP? PT=?NP? GF=?Ext??
aat0 featheryaj0 tailnn1 thatcjt
?/C? ?C TARGET=?y?? archedvvd?/C?
?C FE=?Path? PT=?PP? GF=?Comp??
overavp?prp itsdps backnn1
?/C? .pun?/S?
The constituent (C) tags identify the phrases that
have been annotated. The target attribute indicates
the predicating word for the overall frame. The
frame element (FE) attribute indicates one of the se-
mantic roles for the frame, and the phrase type (PT)
attribute indicates the grammatical function of the
phrase. We isolate the prepositional phrase annota-
tion and treat it as the sense of the preposition. This
yields the following annotation:
It had a sharp, pointed face and a feathery
tail that arched over
Path
its back.
The annotation frequencies for the most frequent
prepositions are shown later in Table 8, again or-
dered by entropy. This illustrates that the role dis-
tributions are more complicated, yielding higher en-
tropy values on average. In all, there are over 100
prepositions with annotations, 65 with ten or more
instances each.
Tag Freq Description
pp-loc 17220 locative
pp-tmp 10572 temporal
pp-dir 5453 direction
pp-mnr 1811 manner
pp-prp 1096 purpose/reason
pp-ext 280 spatial extent
pp-bnf 44 beneficiary
Table 1: TREEBANK semantic roles for PP?s. Tag
is the label for the role in the annotations. Freq is
frequency of the role occurrences.
Tag Freq Description
Spkr 8310 speaker
Msg 7103 message
SMov 6778 self-mover
Thm 6403 theme
Agt 5887 agent
Goal 5560 goal
Path 5422 path
Cog 4585 cognizer
Manr 4474 manner
Src 3706 source
Cont 3662 content
Exp 3567 experiencer
Eval 3108 evaluee
Judge 3107 judge
Top 3074 topic
Other 2531 undefined
Cause 2306 cause
Add 2266 addressee
Src-p 2179 perceptual source
Phen 1969 phenomenon
Reas 1789 reason
Area 1328 area
Degr 1320 degree
BodP 1230 body part
Prot 1106 protagonist
Table 2: Common FRAMENET semantic roles. The
top 25 of 141 roles are shown.
3 Classification experiments
The task of selecting the semantic roles for the
prepositions can be framed as an instance of word-
sense disambiguation (WSD), where the semantic
roles serve as the senses for the prepositions.
A straightforward approach for preposition dis-
ambiguation would be to use standard WSD fea-
tures, such as the parts-of-speech of surrounding
words and, more importantly, collocations (e.g., lex-
ical associations). Although this can be highly ac-
curate, it will likely overfit the data and generalize
poorly. To overcome these problems, a class-based
approach is used for the collocations, with WordNet
high-level synsets as the source of the word classes.
Therefore, in addition to using collocations in the
form of other words, this uses collocations in the
form of semantic categories.
A supervised approach for word-sense disam-
biguation is used following Bruce and Wiebe (1999).
The results described here were obtained using the
settings in Figure 1. These are similar to the set-
tings used by O?Hara et al (2000) in the first
SENSEVAL competition, with the exception of the
hypernym collocations. This shows that for the hy-
pernym associations, only those words that occur
within 5 words of the target prepositions are con-
sidered.2
The main difference from that of a standard WSD
approach is that, during the determination of the
class-based collocations, each word token is re-
placed by synset tokens for its hypernyms in Word-
Net, several of which might occur more than once.
This introduces noise due to ambiguity, but given
the conditional-independence selection scheme, the
preference for hypernym synsets that occur for dif-
ferent words will compensate somewhat. O?Hara
and Wiebe (2003) provide more details on the ex-
traction of these hypernym collocations. The fea-
ture settings in Figure 1 are used in two different
configurations: word-based collocations alone, and
a combination of word-based and hypernym-based
collocations. The combination generally produces
2This window size was chosen after estimating that on aver-
age the prepositional objects occur within 2.35+/? 1.26 words
of the preposition and that the average attachment site is within
3.0 +/? 2.98 words. These figures were produced by ana-
lyzing the parse trees for the semantic role annotations in the
PENN TREEBANK.
Features:
POS?2 part-of-speech 2 words to left
POS?1: part-of-speech 1 word to left
POS+1: part-of-speech 1 word to right
POS+2: part-of-speech 2 words to right
Prep preposition being classified
WordColl
i
: word collocation for role i
HypernymColl
i
: hypernym collocation for role i
Collocation Context:
Word: anywhere in the sentence
Hypernym: within 5 words of target preposition
Collocation selection:
Frequency: f(word) > 1
CI threshold: p(c|coll)?p(c)
p(c)
>= 0.2
Organization: per-class-binary
Model selection:
overall classifier: Decision tree
individual classifiers: Naive Bayes
10-fold cross-validation
Figure 1: Feature settings used in the preposi-
tion classification experiments. CI refers to condi-
tional independence; the per-class-binary organiza-
tion uses a separate binary feature per role (Wiebe et
al., 1998).
the best results. This exploits the specific clues pro-
vided by the word collocations while generalizing to
unseen cases via the hypernym collocations.
3.1 PENN TREEBANK
To see how these conceptual associations are de-
rived, consider the differences in the prior versus
class-based conditional probabilities for the seman-
tic roles of the preposition ?at? in TREEBANK. Ta-
ble 3 shows the global probabilities for the roles as-
signed to ?at?. Table 4 shows the conditional prob-
Relation P(R) Example
locative .732 workers at a factory
temporal .239 expired at midnight Tuesday
manner .020 has grown at a sluggish pace
direction .006 CDs aimed at individual investors
Table 3: Prior probabilities of semantic relations for
?at? in TREEBANK. P (R) is the relative frequency.
Example usages are taken from the corpus.
Category Relation P(R|C)
ENTITY#1 locative 0.86
ENTITY#1 temporal 0.12
ENTITY#1 other 0.02
ABSTRACTION#6 locative 0.51
ABSTRACTION#6 temporal 0.46
ABSTRACTION#6 other 0.03
Table 4: Sample conditional probabilities of seman-
tic relations for ?at? in TREEBANK. Category is
WordNet synset defining the category. P (R|C) is
probability of the relation given that the synset cate-
gory occurs in the context.
Relation P(R) Example
addressee .315 growled at the attendant
other .092 chuckled heartily at this admission
phenomenon .086 gazed at him with disgust
goal .079 stationed a policeman at the gate
content .051 angry at her stubbornness
Table 5: Prior probabilities of semantic relations for
?at? in FRAMENET for the top 5 of 40 applicable
roles.
Category Relation P(R|C)
ENTITY#1 addressee 0.28
ENTITY#1 goal 0.11
ENTITY#1 phenomenon 0.10
ENTITY#1 other 0.09
ENTITY#1 content 0.03
ABSTRACTION#6 addressee 0.22
ABSTRACTION#6 other 0.14
ABSTRACTION#6 goal 0.12
ABSTRACTION#6 phenomenon 0.08
ABSTRACTION#6 content 0.05
Table 6: Sample conditional probabilities of seman-
tic relations for ?at? in FRAMENET
abilities for these roles given that certain high-level
WordNet categories occur in the context. These cat-
egory probability estimates were derived by tabulat-
ing the occurrences of the hypernym synsets for the
words occurring within a 5-word window of the tar-
get preposition. In a context with a concrete concept
(ENTITY#1), the difference in the probability dis-
tributions shows that the locative interpretation be-
comes even more likely. In contrast, in a context
with an abstract concept (ABSTRACTION#6), the
difference in the probability distributions shows that
the temporal interpretation becomes more likely.
Therefore, these class-based lexical associations re-
flect the intuitive use of the prepositions.
The classification results for these prepositions
in the PENN TREEBANK show that this approach is
very effective. Table 9 shows the results when all
of the prepositions are classified together. Unlike
the general case for WSD, the sense inventory is
the same for all the words here; therefore, a sin-
gle classifier can be produced rather than individ-
ual classifiers. This has the advantage of allowing
more training data to be used in the derivation of
the clues indicative of each semantic role. Good ac-
curacy is achieved when just using standard word
collocations. Table 9 also shows that significant
improvements are achieved using a combination of
both types of collocations. For the combined case,
the accuracy is 86.1%, using Weka?s J48 classifier
(Witten and Frank, 1999), which is an implementa-
tion of Quinlan?s (1993) C4.5 decision tree learner.
For comparison, Table 7 shows the results for indi-
vidual classifiers created for each preposition (using
Naive Bayes). In this case, the word-only colloca-
tions perform slightly better: 78.5% versus 77.8%
accuracy.
3.2 FRAMENET
It is illustrative to compare the prior probabilities
(i.e., P(R)) for FRAMENET to those seen earlier
for ?at? in TREEBANK. See Table 5 for the most
frequent roles out of the 40 cases that were as-
signed to it. This highlights a difference between
the two sets of annotations. The common tempo-
ral role from TREEBANK is not directly represented
in FRAMENET, and it is not subsumed by another
specific role. Similarly, there is no direct role cor-
responding to locative, but it is partly subsumed by
Dataset Statistics
Instances 26616
Classes 7
Entropy 1.917
Baseline 0.480
Experiment Accuracy STDEV
Word Only 81.1 .996
Combined 86.1 .491
Table 9: Overall results for preposition disambigua-
tion with TREEBANK semantic roles. Instances is
the number of role annotations. Classes is the
number of distinct roles. Entropy measures non-
uniformity of the role distributions. Baseline selects
the most-frequent role. The Word Only experiment
just uses word collocations, whereas Combined uses
both word and hypernym collocations. Accuracy is
average for percent correct over ten trials in cross
validation. STDEV is the standard deviation over the
trails. The difference in the two experiments is sta-
tistically significant at p < 0.01.
Dataset Statistics
Instances 27300
Classes 129
Entropy 5.127
Baseline 0.149
Experiment Accuracy STDEV
Word Only 49.0 0.90
Combined 49.4 0.44
Table 10: Overall results for preposition disam-
biguation with FRAMENET semantic roles. See Ta-
ble 9 for the legend.
Preposition Freq Entropy Baseline Word Only Combined
through 332 1.668 0.438 0.598 0.634
as 224 1.647 0.399 0.820 0.879
by 1043 1.551 0.501 0.867 0.860
between 83 1.506 0.483 0.733 0.751
of 30 1.325 0.567 0.800 0.814
out 76 1.247 0.711 0.788 0.764
for 1406 1.223 0.655 0.805 0.796
on 1927 1.184 0.699 0.856 0.855
throughout 61 0.998 0.525 0.603 0.584
across 78 0.706 0.808 0.858 0.748
from 1521 0.517 0.917 0.912 0.882
Total 6781 1.233 0.609 0.785 0.778
Table 7: Per-word results for preposition disambiguation with TREEBANK semantic roles. Freq gives the
frequency for the prepositions. Entropy measures non-uniformity of the role distributions. The Baseline
experiment selects the most-frequent role. The Word Only experiment just uses word collocations, whereas
Combined uses both word and hypernym collocations. Both columns show averages for percent correct over
ten trials. Total averages the values of the individual experiments (except for Freq).
Prep Freq Entropy Baseline Word Only Combined
between 286 3.258 0.490 0.325 0.537
against 210 2.998 0.481 0.310 0.586
under 125 2.977 0.385 0.448 0.440
as 593 2.827 0.521 0.388 0.598
over 620 2.802 0.505 0.408 0.526
behind 144 2.400 0.520 0.340 0.473
back 540 1.814 0.544 0.465 0.567
around 489 1.813 0.596 0.607 0.560
round 273 1.770 0.464 0.513 0.533
into 844 1.747 0.722 0.759 0.754
about 1359 1.720 0.682 0.706 0.778
through 673 1.571 0.755 0.780 0.779
up 488 1.462 0.736 0.736 0.713
towards 308 1.324 0.758 0.786 0.740
away 346 1.231 0.786 0.803 0.824
like 219 1.136 0.777 0.694 0.803
down 592 1.131 0.764 0.764 0.746
across 544 1.128 0.824 0.820 0.827
off 435 0.763 0.892 0.904 0.899
along 469 0.538 0.912 0.932 0.915
onto 107 0.393 0.926 0.944 0.939
past 166 0.357 0.925 0.940 0.938
Total 10432 1.684 0.657 0.685 0.703
Table 8: Per-word results for preposition disambiguation with FRAMENET semantic roles. See Table 7 for
the legend.
goal. This reflects the bias of FRAMENET towards
roles that are an integral part of the frame under con-
sideration: location and time apply to all frames, so
these cases are not generally annotated.
Table 9 shows the results of classification when
all of the prepositions are classified together. The
overall results are not that high due to the very large
number of roles. However, the combined colloca-
tion approach still shows slight improvement (49.4%
versus 49.0%). Table 8 shows the results when us-
ing individual classifiers. This shows that the com-
bined collocations produce better results: 70.3%
versus 68.5%. Unlike the case with Treebank, the
performance is below that of the individual classi-
fiers. This is due to the fine-grained nature of the
role inventory. When all the roles are considered to-
gether, prepositions are prone to being misclassified
with roles that they might not have occurred with in
the training data, such as whenever other contextual
clues are strong for that role. This is not a problem
with Treebank given its small role inventory.
4 Related work
Until recently, there has not been much work specif-
ically on preposition classification, especially with
respect to general applicability in contrast to spe-
cial purpose usages. Halliday (1956) did some early
work on this in the context of machine translation.
Later work in that area addressed the classification
indirectly during translation. In some cases, the is-
sue is avoided by translating the preposition into a
corresponding foreign function word without regard
to the preposition?s underlying meaning (i.e., direct
transfer). Other times an internal representation is
helpful (Trujillo, 1992). Taylor (1993) discusses
general strategies for preposition disambiguation us-
ing a cognitive linguistics framework and illustrates
them for ?over?. There has been quite a bit of work
in this area but mainly for spatial prepositions (Jap-
kowicz and Wiebe, 1991; Zelinsky-Wibbelt, 1993).
There is currently more interest in this type of
classification. Litkowski (2002) presents manually-
derived rules for disambiguating prepositions, in
particular for ?of?. Srihari et al (2001) present
manually-derived rules for disambiguating preposi-
tions used in named entities.
Gildea and Jurafsky (2002) classify seman-
tic role assignments using all the annotations in
FRAMENET, for example, covering all types of ver-
bal arguments. They use several features derived
from the output of a parser, such as the constituent
type of the phrase (e.g., NP) and the grammatical
function (e.g., subject). They include lexical fea-
tures for the headword of the phrase and the predi-
cating word for the entire annotated frame. They re-
port an accuracy of 76.9% with a baseline of 40.6%
over the FRAMENET semantic roles. However, due
to the conditioning of the classification on the pred-
icating word for the frame, the range of roles for a
particular classification is more limited than in our
case.
Blaheta and Charniak (2000) classify semantic
role assignments using all the annotations in TREE-
BANK. They use a few parser-derived features, such
as the constituent labels for nearby nodes and part-
of-speech for parent and grandparent nodes. They
also include lexical features for the head and al-
ternative head (since prepositions are considered as
the head by their parser). They report an accu-
racy of 77.6% over the form/function tags from the
PENN TREEBANK with a baseline of 37.8%,3 Their
task is somewhat different, since they address all ad-
juncts, not just prepositions, hence their lower base-
line. In addition, they include the nominal and ad-
verbial roles, which are syntactic and presumably
more predictable than the others in this group. Van
den Bosch and Bucholz (2002) also use the Tree-
bank data to address the more general task of assign-
ing function tags to arbitrary phrases. For features,
they use parts of speech, words, and morphological
clues. Chunking is done along with the tagging, but
they only present results for the evaluation of both
tasks taken together; their best approach achieves
78.9% accuracy.
5 Conclusion
Our approach to classifying prepositions according
to the PENN TREEBANK annotations is fairly accu-
rate (78.5% individually and 86.1% together), while
retaining ability to generalize via class-based lexi-
cal associations. These annotations are suitable for
3They target al of the TREEBANK function tags but give
performance figures broken down by the groupings defined in
the Treebank tagging guidelines. The baseline figure shown
above is their recall figure for the ?baseline 2? performance.
default classification of prepositions in case more
fine-grained semantic role information cannot be de-
termined. For the fine-grained FRAMENET roles,
the performance is less accurate (70.3% individu-
ally and 49.4% together). In both cases, the best
accuracy is achieved using a combination of stan-
dard word collocations along with class collocations
in the form of WordNet hypernyms.
Future work will address cross-dataset experi-
ments. In particular, we will see whether the word
and hypernym associations learned over FrameNet
can be carried over into Treebank, given a mapping
of the fine-grained FrameNet roles into the coarse-
grained Treebank ones. Such a mapping would be
similar to the one developed by Gildea and Jurafsky
(2002).
Acknowledgements
The first author is supported by a generous GAANN fellowship
from the Department of Education. Some of the work used com-
puting resources at NMSU made possible through MII Grants
EIA-9810732 and EIA-0220590.
References
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proc. NAACL-00.
Rebecca Bruce and Janyce Wiebe. 1999. Decomposable
modeling in natural language processing. Computa-
tional Linguistics, 25 (2):195?208.
A. Van den Bosch and S. Buchholz. 2002. Shallow pars-
ing on the basis of words only: A case study. In Pro-
ceedings of the 40th Meeting of the Association for
Computational Linguistics (ACL?02), pages 433?440.
Philadelphia, PA, USA.
P. Edmonds and S. Cotton, editors. 2001. Proceedings of
the SENSEVAL 2 Workshop. Association for Compu-
tational Linguistics.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Information and
Computation. Hong Kong.
C. Fillmore. 1968. The case for case. In Emmon Bach
and Rovert T. Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart and Winston, New York.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
M.A.K. Halliday. 1956. The linguistic basis of a
mechanical thesaurus, and its application to English
preposition classification. Mechanical Translation,
3(2):81?88.
Nathalie Japkowicz and Janyce Wiebe. 1991. Translat-
ing spatial prepositions using conceptual information.
In Proc. 29th Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL-91), pages 153?160.
K. C. Litkowski. 2002. Digraph analysis of dictionary
preposition definitions. In Proceedings of the Asso-
ciation for Computational Linguistics Special Interest
Group on the Lexicon. July 11, Philadelphia, PA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proc. ARPA Human Language Technology Workshop.
Tom O?Hara and Janyce Wiebe. 2003. Classifying func-
tional relations in Factotum viaWordNet hypernymas-
sociations. In Proc. Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2003).
TomO?Hara, JanyceWiebe, and Rebecca F. Bruce. 2000.
Selecting decomposable models for word-sense dis-
ambiguation: The GRLING-SDM system. Computers
and the Humanities, 34 (1-2):159?164.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, California.
Rohini Srihari, Cheng Niu, and Wei Li. 2001. A hybrid
approach for named entity and sub-type tagging. In
Proc. 6th Applied Natural Language Processing Con-
ference.
John R. Taylor. 1993. Prepositions: patterns of polysem-
ization and strategies of disambiguation. In Zelinsky-
Wibbelt (Zelinsky-Wibbelt, 1993).
Arturo Trujillo. 1992. Locations in the machine transla-
tion of prepositional phrases. In Proc. TMI-92, pages
13?20.
Janyce Wiebe, Kenneth McKeever, and Rebecca Bruce.
1998. Mapping collocational properties into machine
learning features. In Proc. 6th Workshop on Very
Large Corpora (WVLC-98), pages 225?233,Montreal,
Quebec, Canada. Association for Computational Lin-
guistics SIGDAT.
Ian H.Witten and Eibe Frank. 1999. DataMining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
Cornelia Zelinsky-Wibbelt, editor. 1993. The Semantics
of Prepositions: From Mental Processing to Natural
Language Processing. Mouton de Gruyter, Berlin.
Learning Extraction Patterns for Subjective Expressions?
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper presents a bootstrapping process
that learns linguistically rich extraction pat-
terns for subjective (opinionated) expressions.
High-precision classifiers label unannotated
data to automatically create a large training set,
which is then given to an extraction pattern
learning algorithm. The learned patterns are
then used to identify more subjective sentences.
The bootstrapping process learns many subjec-
tive patterns and increases recall while main-
taining high precision.
1 Introduction
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculations. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
?This work was supported by the National Science Founda-
tion under grants IIS-0208798, IIS-0208985, and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Research Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Some existing resources contain lists of subjective
words (e.g., Levin?s desire verbs (1993)), and some em-
pirical methods in NLP have automatically identified ad-
jectives, verbs, and N-grams that are statistically associ-
ated with subjective language (e.g., (Turney, 2002; Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe
et al, 2001)). However, subjective language can be ex-
hibited by a staggering variety of words and phrases. In
addition, many subjective terms occur infrequently, such
as strongly subjective adjectives (e.g., preposterous, un-
seemly) and metaphorical or idiomatic phrases (e.g., dealt
a blow, swept off one?s feet). Consequently, we believe
that subjectivity learning systems must be trained on ex-
tremely large text collections before they will acquire a
subjective vocabulary that is truly broad and comprehen-
sive in scope.
To address this issue, we have been exploring the use
of bootstrapping methods to allow subjectivity classifiers
to learn from a collection of unannotated texts. Our re-
search uses high-precision subjectivity classifiers to au-
tomatically identify subjective and objective sentences in
unannotated texts. This process allows us to generate a
large set of labeled sentences automatically. The sec-
ond emphasis of our research is using extraction patterns
to represent subjective expressions. These patterns are
linguistically richer and more flexible than single words
or N-grams. Using the (automatically) labeled sentences
as training data, we apply an extraction pattern learning
algorithm to automatically generate patterns represent-
ing subjective expressions. The learned patterns can be
used to automatically identify more subjective sentences,
which grows the training set, and the entire process can
then be bootstrapped. Our experimental results show that
this bootstrapping process increases the recall of the high-
precision subjective sentence classifier with little loss in
precision. We also find that the learned extraction pat-
terns capture subtle connotations that are more expressive
than the individual words by themselves.
This paper is organized as follows. Section 2 discusses
previous work on subjectivity analysis and extraction pat-
tern learning. Section 3 overviews our general approach,
describes the high-precision subjectivity classifiers, and
explains the algorithm for learning extraction patterns as-
sociated with subjectivity. Section 4 describes the data
that we use, presents our experimental results, and shows
examples of patterns that are learned. Finally, Section 5
summarizes our findings and conclusions.
2 Background
2.1 Subjectivity Analysis
Much previous work on subjectivity recognition has fo-
cused on document-level classification. For example,
(Spertus, 1997) developed a system to identify inflamma-
tory texts and (Turney, 2002; Pang et al, 2002) developed
methods for classifying reviews as positive or negative.
Some research in genre classification has included the
recognition of subjective genres such as editorials (e.g.,
(Karlgren and Cutting, 1994; Kessler et al, 1997; Wiebe
et al, 2001)).
In contrast, the goal of our work is to classify individ-
ual sentences as subjective or objective. Document-level
classification can distinguish between ?subjective texts?,
such as editorials and reviews, and ?objective texts,? such
as newspaper articles. But in reality, most documents
contain a mix of both subjective and objective sentences.
Subjective texts often include some factual information.
For example, editorial articles frequently contain factual
information to back up the arguments being made, and
movie reviews often mention the actors and plot of a
movie as well as the theatres where it?s currently playing.
Even if one is willing to discard subjective texts in their
entirety, the objective texts usually contain a great deal of
subjective information in addition to facts. For example,
newspaper articles are generally considered to be rela-
tively objective documents, but in a recent study (Wiebe
et al, 2001) 44% of sentences in a news collection were
found to be subjective (after editorial and review articles
were removed).
One of the main obstacles to producing a sentence-
level subjectivity classifier is a lack of training data. To
train a document-level classifier, one can easily find col-
lections of subjective texts, such as editorials and reviews.
For example, (Pang et al, 2002) collected reviews from
a movie database and rated them as positive, negative, or
neutral based on the rating (e.g., number of stars) given
by the reviewer. It is much harder to obtain collections of
individual sentences that can be easily identified as sub-
jective or objective. Previous work on sentence-level sub-
jectivity classification (Wiebe et al, 1999) used training
corpora that had been manually annotated for subjectiv-
ity. Manually producing annotations is time consuming,
so the amount of available annotated sentence data is rel-
atively small.
The goal of our research is to use high-precision sub-
jectivity classifiers to automatically identify subjective
and objective sentences in unannotated text corpora. The
high-precision classifiers label a sentence as subjective or
objective when they are confident about the classification,
and they leave a sentence unlabeled otherwise. Unanno-
tated texts are easy to come by, so even if the classifiers
can label only 30% of the sentences as subjective or ob-
jective, they will still produce a large collection of labeled
sentences. Most importantly, the high-precision classi-
fiers can generate a much larger set of labeled sentences
than are currently available in manually created data sets.
2.2 Extraction Patterns
Information extraction (IE) systems typically use lexico-
syntactic patterns to identify relevant information. The
specific representation of these patterns varies across sys-
tems, but most patterns represent role relationships sur-
rounding noun and verb phrases. For example, an IE
system designed to extract information about hijackings
might use the pattern hijacking of <x>, which looks for
the noun hijacking and extracts the object of the prepo-
sition of as the hijacked vehicle. The pattern <x> was
hijacked would extract the hijacked vehicle when it finds
the verb hijacked in the passive voice, and the pattern
<x> hijacked would extract the hijacker when it finds
the verb hijacked in the active voice.
One of our hypotheses was that extraction patterns
would be able to represent subjective expressions that
have noncompositional meanings. For example, consider
the common expression drives (someone) up the wall,
which expresses the feeling of being annoyed with some-
thing. The meaning of this expression is quite different
from the meanings of its individual words (drives, up,
wall). Furthermore, this expression is not a fixed word
sequence that could easily be captured by N-grams. It is
a relatively flexible construction that may be more gener-
ally represented as <x> drives <y> up the wall, where x
and y may be arbitrary noun phrases. This pattern would
match many different sentences, such as ?George drives
me up the wall,? ?She drives the mayor up the wall,?
or ?The nosy old man drives his quiet neighbors up the
wall.?
We also wondered whether the extraction pattern rep-
resentation might reveal slight variations of the same verb
or noun phrase that have different connotations. For ex-
ample, you can say that a comedian bombed last night,
which is a subjective statement, but you can?t express
this sentiment with the passive voice of bombed. In Sec-
tion 3.2, we will show examples of extraction patterns
representing subjective expressions which do in fact ex-
hibit both of these phenomena.
A variety of algorithms have been developed to au-
tomatically learn extraction patterns. Most of these
algorithms require special training resources, such as
texts annotated with domain-specific tags (e.g., Au-
toSlog (Riloff, 1993), CRYSTAL (Soderland et al,
1995), RAPIER (Califf, 1998), SRV (Freitag, 1998),
WHISK (Soderland, 1999)) or manually defined key-
words, frames, or object recognizers (e.g., PALKA (Kim
and Moldovan, 1993) and LIEP (Huffman, 1996)).
AutoSlog-TS (Riloff, 1996) takes a different approach,
requiring only a corpus of unannotated texts that have
been separated into those that are related to the target do-
main (the ?relevant? texts) and those that are not (the ?ir-
relevant? texts). Most recently, two bootstrapping algo-
rithms have been used to learn extraction patterns. Meta-
bootstrapping (Riloff and Jones, 1999) learns both extrac-
tion patterns and a semantic lexicon using unannotated
texts and seed words as input. ExDisco (Yangarber et al,
2000) uses a bootstrapping mechanism to find new ex-
traction patterns using unannotated texts and some seed
patterns as the initial input.
For our research, we adopted a learning process very
similar to that used by AutoSlog-TS, which requires only
relevant texts and irrelevant texts as its input. We describe
this learning process in more detail in the next section.
3 Learning and Bootstrapping Extraction
Patterns for Subjectivity
We have developed a bootstrapping process for subjec-
tivity classification that explores three ideas: (1) high-
precision classifiers can be used to automatically iden-
tify subjective and objective sentences from unannotated
texts, (2) this data can be used as a training set to auto-
matically learn extraction patterns associated with sub-
jectivity, and (3) the learned patterns can be used to grow
the training set, allowing this entire process to be boot-
strapped.
Figure 1 shows the components and layout of the boot-
strapping process. The process begins with a large collec-
tion of unannotated text and two high precision subjec-
tivity classifiers. One classifier searches the unannotated
corpus for sentences that can be labeled as subjective
with high confidence, and the other classifier searches
for sentences that can be labeled as objective with high
confidence. All other sentences in the corpus are left
unlabeled. The labeled sentences are then fed to an ex-
traction pattern learner, which produces a set of extrac-
tion patterns that are statistically correlated with the sub-
jective sentences (we will call these the subjective pat-
terns). These patterns are then used to identify more sen-
tences within the unannotated texts that can be classified
as subjective. The extraction pattern learner can then re-
train using the larger training set and the process repeats.
The subjective patterns can also be added to the high-
precision subjective sentence classifier as new features to
improve its performance. The dashed lines in Figure 1
represent the parts of the process that are bootstrapped.
In this section, we will describe the high-precision sen-
tence classifiers, the extraction pattern learning process,
and the details of the bootstrapping process.
3.1 High-Precision Subjectivity Classifiers
The high-precision classifiers (HP-Subj and HP-Obj) use
lists of lexical items that have been shown in previous
work to be good subjectivity clues. Most of the items are
single words, some are N-grams, but none involve syntac-
tic generalizations as in the extraction patterns. Any data
used to develop this vocabulary does not overlap with the
test sets or the unannotated data used in this paper.
Many of the subjective clues are from manually de-
veloped resources, including entries from (Levin, 1993;
Ballmer and Brennenstuhl, 1981), Framenet lemmas with
frame element experiencer (Baker et al, 1998), adjec-
tives manually annotated for polarity (Hatzivassiloglou
and McKeown, 1997), and subjectivity clues listed in
(Wiebe, 1990). Others were derived from corpora, in-
cluding subjective nouns learned from unannotated data
using bootstrapping (Riloff et al, 2003).
The subjectivity clues are divided into those that are
strongly subjective and those that are weakly subjective,
using a combination of manual review and empirical re-
sults on a small training set of manually annotated data.
As the terms are used here, a strongly subjective clue is
one that is seldom used without a subjective meaning,
whereas a weakly subjective clue is one that commonly
has both subjective and objective uses.
The high-precision subjective classifier classifies a sen-
tence as subjective if it contains two or more of the
strongly subjective clues. On a manually annotated test
set, this classifier achieves 91.5% precision and 31.9%
recall (that is, 91.5% of the sentences that it selected are
subjective, and it found 31.9% of the subjective sentences
in the test set). This test set consists of 2197 sentences,
59% of which are subjective.
The high-precision objective classifier takes a different
approach. Rather than looking for the presence of lexical
items, it looks for their absence. It classifies a sentence as
objective if there are no strongly subjective clues and at
most one weakly subjective clue in the current, previous,
and next sentence combined. Why doesn?t the objective
classifier mirror the subjective classifier, and consult its
own list of strongly objective clues? There are certainly
lexical items that are statistically correlated with the ob-
Known Subjective
Vocabulary
High?Precision Objective
Sentence Classifier (HP?Obj)
High?Precision Subjective
Sentence Classifier (HP?Subj)
Unannotated Text Collection
unlabeled sentences
unlabeled sentences
unlabeled sentences
Pattern?based Subjective
Sentence Classifier
Extraction Pattern
Learner
subjective
sentences
subjective sentences
objective sentences
subjective patterns
subjective patterns
Figure 1: Bootstrapping Process
jective class (examples are cardinal numbers (Wiebe et
al., 1999), and words such as per, case, market, and to-
tal), but the presence of such clues does not readily lead
to high precision objective classification. Add sarcasm
or a negative evaluation to a sentence about a dry topic
such as stock prices, and the sentence becomes subjec-
tive. Conversely, add objective topics to a sentence con-
taining two strongly subjective words such as odious and
scumbag, and the sentence remains subjective.
The performance of the high-precision objective classi-
fier is a bit lower than the subjective classifier: 82.6% pre-
cision and 16.4% recall on the test set mentioned above
(that is, 82.6% of the sentences selected by the objective
classifier are objective, and the objective classifier found
16.4% of the objective sentences in the test set). Al-
though there is room for improvement, the performance
proved to be good enough for our purposes.
3.2 Learning Subjective Extraction Patterns
To automatically learn extraction patterns that are associ-
ated with subjectivity, we use a learning algorithm similar
to AutoSlog-TS (Riloff, 1996). For training, AutoSlog-
TS uses a text corpus consisting of two distinct sets of
texts: ?relevant? texts (in our case, subjective sentences)
and ?irrelevant? texts (in our case, objective sentences).
A set of syntactic templates represents the space of pos-
sible extraction patterns.
The learning process has two steps. First, the syntac-
tic templates are applied to the training corpus in an ex-
haustive fashion, so that extraction patterns are generated
for (literally) every possible instantiation of the templates
that appears in the corpus. The left column of Figure 2
shows the syntactic templates used by AutoSlog-TS. The
right column shows a specific extraction pattern that was
learned during our subjectivity experiments as an instan-
tiation of the syntactic form on the left. For example, the
pattern <subj> was satisfied1 will match any sentence
where the verb satisfied appears in the passive voice. The
pattern <subj> dealt blow represents a more complex ex-
pression that will match any sentence that contains a verb
phrase with head=dealt followed by a direct object with
head=blow. This would match sentences such as ?The
experience dealt a stiff blow to his pride.? It is important
to recognize that these patterns look for specific syntactic
constructions produced by a (shallow) parser, rather than
exact word sequences.
SYNTACTIC FORM EXAMPLE PATTERN
<subj> passive-verb <subj> was satisfied
<subj> active-verb <subj> complained
<subj> active-verb dobj <subj> dealt blow
<subj> verb infinitive <subj> appear to be
<subj> aux noun <subj> has position
active-verb <dobj> endorsed <dobj>
infinitive <dobj> to condemn <dobj>
verb infinitive <dobj> get to know <dobj>
noun aux <dobj> fact is <dobj>
noun prep <np> opinion on <np>
active-verb prep <np> agrees with <np>
passive-verb prep <np> was worried about <np>
infinitive prep <np> to resort to <np>
Figure 2: Syntactic Templates and Examples of Patterns
that were Learned
1This is a shorthand notation for the internal representation.
PATTERN FREQ %SUBJ
<subj> was asked 11 100%
<subj> asked 128 63%
<subj> is talk 5 100%
talk of <np> 10 90%
<subj> will talk 28 71%
<subj> put an end 10 90%
<subj> put 187 67%
<subj> is going to be 11 82%
<subj> is going 182 67%
was expected from <np> 5 100%
<subj> was expected 45 42%
<subj> is fact 38 100%
fact is <dobj> 12 100%
Figure 3: Patterns with Interesting Behavior
The second step of AutoSlog-TS?s learning process ap-
plies all of the learned extraction patterns to the train-
ing corpus and gathers statistics for how often each
pattern occurs in subjective versus objective sentences.
AutoSlog-TS then ranks the extraction patterns using a
metric called RlogF (Riloff, 1996) and asks a human to
review the ranked list and make the final decision about
which patterns to keep.
In contrast, for this work we wanted a fully automatic
process that does not depend on a human reviewer, and
we were most interested in finding patterns that can iden-
tify subjective expressions with high precision. So we
ranked the extraction patterns using a conditional proba-
bility measure: the probability that a sentence is subjec-
tive given that a specific extraction pattern appears in it.
The exact formula is:
Pr(subjective | patterni) = subjfreq(patterni)freq(patterni)
where subjfreq(patterni) is the frequency of patterni
in subjective training sentences, and freq(patterni) is
the frequency of patterni in all training sentences. (This
may also be viewed as the precision of the pattern on the
training data.) Finally, we use two thresholds to select ex-
traction patterns that are strongly associated with subjec-
tivity in the training data. We choose extraction patterns
for which freq(patterni) ? ?1 and Pr(subjective |
patterni) ? ?2.
Figure 3 shows some patterns learned by our system,
the frequency with which they occur in the training data
(FREQ) and the percentage of times they occur in sub-
jective sentences (%SUBJ). For example, the first two
rows show the behavior of two similar expressions us-
ing the verb asked. 100% of the sentences that contain
asked in the passive voice are subjective, but only 63%
of the sentences that contain asked in the active voice are
subjective. A human would probably not expect the ac-
tive and passive voices to behave so differently. To un-
derstand why this is so, we looked in the training data
and found that the passive voice is often used to query
someone about a specific opinion. For example, here is
one such sentence from our training set: ?Ernest Bai Ko-
roma of RITCORP was asked to address his supporters on
his views relating to ?full blooded Temne to head APC?.?
In contrast, many of the sentences containing asked in
the active voice are more general in nature, such as ?The
mayor asked a newly formed JR about his petition.?
Figure 3 also shows that expressions using talk as a
noun (e.g., ?Fred is the talk of the town?) are highly cor-
related with subjective sentences, while talk as a verb
(e.g., ?The mayor will talk about...?) are found in a mix
of subjective and objective sentences. Not surprisingly,
longer expressions tend to be more idiomatic (and sub-
jective) than shorter expressions (e.g., put an end (to) vs.
put; is going to be vs. is going; was expected from vs. was
expected). Finally, the last two rows of Figure 3 show that
expressions involving the noun fact are highly correlated
with subjective expressions! These patterns match sen-
tences such as The fact is... and ... is a fact, which appar-
ently are often used in subjective contexts. This example
illustrates that the corpus-based learning method can find
phrases that might not seem subjective to a person intu-
itively, but that are reliable indicators of subjectivity.
4 Experimental Results
4.1 Subjectivity Data
The text collection that we used consists of English-
language versions of foreign news documents from FBIS,
the U.S. Foreign Broadcast Information Service. The
data is from a variety of countries. Our system takes
unannotated data as input, but we needed annotated data
to evaluate its performance. We briefly describe the man-
ual annotation scheme used to create the gold-standard,
and give interannotator agreement results.
In 2002, a detailed annotation scheme (Wilson and
Wiebe, 2003) was developed for a government-sponsored
project. We only mention aspects of the annotation
scheme relevant to this paper. The scheme was inspired
by work in linguistics and literary theory on subjectiv-
ity, which focuses on how opinions, emotions, etc. are
expressed linguistically in context (Banfield, 1982). The
goal is to identify and characterize expressions of private
states in a sentence. Private state is a general covering
term for opinions, evaluations, emotions, and specula-
tions (Quirk et al, 1985). For example, in sentence (1)
the writer is expressing a negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.?
Sentence (2) reflects the private state of Western coun-
tries. Mugabe?s use of overwhelmingly also reflects a pri-
vate state, his positive reaction to and characterization of
his victory.
(2) ?Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe?s presidential election.?
Annotators are also asked to judge the strength of each
private state. A private state may have low, medium, high
or extreme strength.
To allow us to measure interannotator agreement, three
annotators (who are not authors of this paper) indepen-
dently annotated the same 13 documents with a total of
210 sentences. We begin with a strict measure of agree-
ment at the sentence level by first considering whether
the annotator marked any private-state expression, of any
strength, anywhere in the sentence. If so, the sentence is
subjective. Otherwise, it is objective. The average pair-
wise percentage agreement is 90% and the average pair-
wise ? value is 0.77.
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define a
sentence as borderline if it has at least one private-state
expression identified by at least one annotator, and all
strength ratings of private-state expressions are low. On
average, 11% of the corpus is borderline under this def-
inition. When those sentences are removed, the average
pairwise percentage agreement increases to 95% and the
average pairwise ? value increases to 0.89.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use when evaluating our results. A sentence is subjec-
tive if it contains at least one private-state expression of
medium or higher strength. The second class, which we
call objective, consists of everything else.
4.2 Evaluation of the Learned Patterns
Our pool of unannotated texts consists of 302,163 indi-
vidual sentences. The HP-Subj classifier initially labeled
roughly 44,300 of these sentences as subjective, and the
HP-Obj classifier initially labeled roughly 17,000 sen-
tences as objective. In order to keep the training set rel-
atively balanced, we used all 17,000 objective sentences
and 17,000 of the subjective sentences as training data for
the extraction pattern learner.
17,073 extraction patterns were learned that have
frequency ? 2 and Pr(subjective | patterni) ? .60 on
the training data. We then wanted to determine whether
the extraction patterns are, in fact, good indicators of sub-
jectivity. To evaluate the patterns, we applied different
subsets of them to a test set to see if they consistently oc-
cur in subjective sentences. This test set consists of 3947
Figure 4: Evaluating the Learned Patterns on Test Data
sentences, 54% of which are subjective.
Figure 4 shows sentence recall and pattern (instance-
level) precision for the learned extraction patterns on the
test set. In this figure, precision is the proportion of pat-
tern instances found in the test set that are in subjective
sentences, and recall is the proportion of subjective sen-
tences that contain at least one pattern instance.
We evaluated 18 different subsets of the patterns, by
selecting the patterns that pass certain thresholds in the
training data. We tried all combinations of ?1 = {2,10}
and ?2 = {.60,.65,.70,.75,.80,.85,.90,.95,1.0}. The data
points corresponding to ?1=2 are shown on the upper line
in Figure 4, and those corresponding to ?1=10 are shown
on the lower line. For example, the data point correspond-
ing to ?1=10 and ?2=.90 evaluates only the extraction pat-
terns that occur at least 10 times in the training data and
with a probability ? .90 (i.e., at least 90% of its occur-
rences are in subjective training sentences).
Overall, the extraction patterns perform quite well.
The precision ranges from 71% to 85%, with the expected
tradeoff between precision and recall. This experiment
confirms that the extraction patterns are effective at rec-
ognizing subjective expressions.
4.3 Evaluation of the Bootstrapping Process
In our second experiment, we used the learned extrac-
tion patterns to classify previously unlabeled sentences
from the unannotated text collection. The new subjec-
tive sentences were then fed back into the Extraction Pat-
tern Learner to complete the bootstrapping cycle depicted
by the rightmost dashed line in Figure 1. The Pattern-
based Subjective Sentence Classifier classifies a sentence
as subjective if it contains at least one extraction pattern
with ?1?5 and ?2?1.0 on the training data. This process
produced approximately 9,500 new subjective sentences
that were previously unlabeled.
Since our bootstrapping process does not learn new ob-
jective sentences, we did not want to simply add the new
subjective sentences to the training set, or it would be-
come increasingly skewed toward subjective sentences.
Since HP-Obj had produced roughly 17,000 objective
sentences used for training, we used the 9,500 new sub-
jective sentences along with 7,500 of the previously iden-
tified subjective sentences as our new training set. In
other words, the training set that we used during the sec-
ond bootstrapping cycle contained exactly the same ob-
jective sentences as the first cycle, half of the same sub-
jective sentences as the first cycle, and 9,500 brand new
subjective sentences.
On this second cycle of bootstrapping, the extraction
pattern learner generated many new patterns that were not
discovered during the first cycle. 4,248 new patterns were
found that have ?1?2 and ?2?.60. If we consider only the
strongest (most subjective) extraction patterns, 308 new
patterns were found that had ?1?10 and ?2?1.0. This is
a substantial set of new extraction patterns that seem to
be very highly correlated with subjectivity.
An open question was whether the new patterns pro-
vide additional coverage. To assess this, we did a sim-
ple test: we added the 4,248 new patterns to the origi-
nal set of patterns learned during the first bootstrapping
cycle. Then we repeated the same analysis that we de-
pict in Figure 4. In general, the recall numbers increased
by about 2-4% while the precision numbers decreased by
less, from 0.5-2%.
In our third experiment, we evaluated whether the
learned patterns can improve the coverage of the high-
precision subjectivity classifier (HP-Subj), to complete
the bootstrapping loop depicted in the top-most dashed
line of Figure 1. Our hope was that the patterns would al-
low more sentences from the unannotated text collection
to be labeled as subjective, without a substantial drop in
precision. For this experiment, we selected the learned
extraction patterns that had ?1? 10 and ?2? 1.0 on the
training set, since these seemed likely to be the most reli-
able (high precision) indicators of subjectivity.
We modified the HP-Subj classifier to use extraction
patterns as follows. All sentences labeled as subjective
by the original HP-Subj classifier are also labeled as sub-
jective by the new version. For previously unlabeled sen-
tences, the new version classifies a sentence as subjective
if (1) it contains two or more of the learned patterns, or
(2) it contains one of the clues used by the original HP-
Subj classifier and at least one learned pattern. Table 1
shows the performance results on the test set mentioned
in Section 3.1 (2197 sentences) for both the original HP-
Subj classifier and the new version that uses the learned
extraction patterns. The extraction patterns produce a 7.2
percentage point gain in coverage, and only a 1.1 percent-
age point drop in precision. This result shows that the
learned extraction patterns do improve the performance
of the high-precision subjective sentence classifier, allow-
ing it to classify more sentences as subjective with nearly
the same high reliability.
HP-Subj HP-Subj w/Patterns
Recall Precision Recall Precision
32.9 91.3 40.1 90.2
Table 1: Bootstrapping the Learned Patterns into the
High-Precision Sentence Classifier
Table 2 gives examples of patterns used to augment the
HP-Subj classifier which do not overlap in non-function
words with any of the clues already known by the original
system. For each pattern, we show an example sentence
from our corpus that matches the pattern.
5 Conclusions
This research explored several avenues for improving the
state-of-the-art in subjectivity analysis. First, we demon-
strated that high-precision subjectivity classification can
be used to generate a large amount of labeled training data
for subsequent learning algorithms to exploit. Second, we
showed that an extraction pattern learning technique can
learn subjective expressions that are linguistically richer
than individual words or fixed phrases. We found that
similar expressions may behave very differently, so that
one expression may be strongly indicative of subjectivity
but the other may not. Third, we augmented our origi-
nal high-precision subjective classifier with these newly
learned extraction patterns. This bootstrapping process
resulted in substantially higher recall with a minimal loss
in precision. In future work, we plan to experiment with
different configurations of these classifiers, add new sub-
jective language learners in the bootstrapping process,
and address the problem of how to identify new objec-
tive sentences during bootstrapping.
6 Acknowledgments
We are very grateful to Theresa Wilson for her invaluable
programming support and help with data preparation.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL-98.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Classifi-
cation: A Study in the Lexical Analysis of English Speech
Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge and
Kegan Paul, Boston.
seems to be <dobj> I am pleased that there now seems to be broad political consensus . . .
underlined <dobj> Jiang?s subdued tone . . . underlined his desire to avoid disputes . . .
pretext of <np> On the pretext of the US opposition . . .
atmosphere of <np> Terrorism thrives in an atmosphere of hate . . .
<subj> reflect These are fine words, but they do not reflect the reality . . .
to satisfy <dobj> The pictures resemble an attempt to satisfy a primitive thirst for revenge . . .
way with <np> . . . to ever let China use force to have its way with . . .
bring about <np> ?Everything must be done by everyone to bring about de-escalation,? Mr Chirac added.
expense of <np> at the expense of the world?s security and stability
voiced <dobj> Khatami . . . voiced Iran?s displeasure.
turn into <np> . . . the surging epidemic could turn into ?a national security threat,? he said.
Table 2: Examples of Learned Patterns Used by HP-Subj and Sample Matching Sentences
M. E. Califf. 1998. Relational Learning Techniques for Natural
Language Information Extraction. Ph.D. thesis, Tech. Rept.
AI98-276, Artificial Intelligence Laboratory, The University
of Texas at Austin.
Dayne Freitag. 1998. Toward General-Purpose Learning for
Information Extraction. In Proceedings of the ACL-98.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting the
Semantic Orientation of Adjectives. In Proceedings of the
ACL-EACL-97.
S. Huffman. 1996. Learning information extraction pat-
terns from examples. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statistical, and
Symbolic Approaches to Learning for Natural Language
Processing, pages 246?260. Springer-Verlag, Berlin.
J. Karlgren and D. Cutting. 1994. Recognizing Text Genres
with Simple Metrics Using Discriminant Analysis. In Pro-
ceedings of the COLING-94.
B. Kessler, G. Nunberg, and H. Schu?tze. 1997. Automatic De-
tection of Text Genre. In Proceedings of the ACL-EACL-97.
J. Kim and D. Moldovan. 1993. Acquisition of Semantic Pat-
terns for Information Extraction from Corpora. In Proceed-
ings of the Ninth IEEE Conference on Artificial Intelligence
for Applications.
Beth Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sen-
timent Classification Using Machine Learning Techniques.
In Proceedings of the EMNLP-02.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for In-
formation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the AAAI-99.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Subjective
Nouns using Extraction Pattern Bootstrapping. In Proceed-
ings of the Seventh Conference on Computational Natural
Language Learning (CoNLL-03).
E. Riloff. 1993. Automatically Constructing a Dictionary for
Information Extraction Tasks. In Proceedings of the AAAI-
93.
E. Riloff. 1996. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the AAAI-96.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a Conceptual Dictionary. In Proceed-
ings of the IJCAI-95.
S. Soderland. 1999. Learning Information Extraction Rules for
Semi-Structured and Free Text. Machine Learning, 34(1-
3):233?272.
E. Spertus. 1997. Smokey: Automatic Recognition of Hostile
Messages. In Proceedings of the IAAI-97.
P. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Ori-
entation Applied to Unsupervised Classification of Reviews.
In Proceedings of the ACL-02.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development and
Use of a Gold Standard Data Set for Subjectivity Classifica-
tions. In Proceedings of the ACL-99.
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying Collo-
cations for Recognizing Opinions. In Proceedings of the
ACL-01 Workshop on Collocation: Computational Extrac-
tion, Analysis, and Exploitation.
J. Wiebe. 1990. Recognizing Subjective Sentences: A Compu-
tational Investigation of Narrative Text. Ph.D. thesis, State
University of New York at Buffalo.
J. Wiebe. 2000. Learning Subjective Adjectives from Corpora.
In Proceedings of the AAAI-00.
T. Wilson and J. Wiebe. 2003. Annotating Opinions in the
World Press. In Proceedings of the ACL SIGDIAL-03.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Automatic Acquisiton of Domain Knowledge for In-
formation Extraction. In Proceedings of COLING 2000.
Annotating Opinions in the World Press
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260, USA
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
In this paper we present a detailed
scheme for annotating expressions of
opinions, beliefs, emotions, sentiment
and speculation (private states) in the
news and other discourse. We explore
inter-annotator agreement for individ-
ual private state expressions, and show
that these low-level annotations are use-
ful for producing higher-level subjec-
tive sentence annotations.
1 Introduction
In this paper we present a detailed scheme for
annotating expressions of opinions, beliefs, emo-
tions, sentiment, speculation and other private
states in newspaper articles. Private state is a
general term that covers mental and emotional
states, which cannot be directly observed or ver-
ified (Quirk et al, 1985). For example, we can
observe evidence of someone else being happy,
but we cannot directly observe their happiness.
In natural language, opinions, emotions and other
private states are expressed using subjective lan-
guage (Banfield, 1982; Wiebe, 1994).
Articles in the news are composed of a mix-
ture of factual and subjective material. Writers
of editorials frequently include facts to support
their arguments, and news reports often mix seg-
ments presenting objective facts with segments
presenting opinions and verbal reactions (van
Dijk, 1988). However, natural language pro-
cessing applications that retrieve or extract infor-
mation from or that summarize or answer ques-
tions about news and other discourse have fo-
cused primarily on factual information and thus
could benefit from knowledge of subjective lan-
guage. Traditional information extraction and in-
formation retrieval systems could learn to concen-
trate on objectively presented factual information.
Question answering systems could identify when
an answer is speculative rather than certain. In
addition, knowledge of how opinions and other
private states are realized in text would directly
support new tasks, such as opinion-oriented in-
formation extraction (Cardie et al, 2003). The
ability to extract opinions when they appear in
documents would benefit multi-document sum-
marization systems seeking to summarize differ-
ent opinions and perspectives, as well as multi-
perspective question-answering systems trying to
answer opinion-based questions.
The annotation scheme we present in this paper
was developed as part of a U.S. government-
sponsored project (ARDA AQUAINT NRRC)1
to investigate multiple perspectives in question
answering (Wiebe et al, 2003). We implemented
the scheme in GATE2, a General Architecture
for Text Engineering (Cunningham et al, 2002).
General instructions for annotating opinions and
specific instructions for downloading and using
GATE to perform the annotations are available at
1This work was performed in support of the Northeast
Regional Research Center (NRRC) which is sponsored by
the Advanced Research and Development Activity in In-
formation Technology (ARDA), a U.S. Government entity
which sponsors and promotes research of import to the In-
telligence Community which includes but is not limited to
the CIA, DIA, NSA, NIMA, and NRO.
2GATE is freely available from the University of
Sheffield at http://gate.ac.uk.
http://www.cs.pitt.edu/ ?wiebe/pubs/ardasummer02.
The annotated data will be available to U.S. gov-
ernment contractors this summer. We are working
to resolve copyright issues to make it available to
the wider research community.
In developing this annotation scheme, we had
two goals. The first was to develop a represen-
tation for opinions and other private states that
was built on work in linguistics and literary the-
ory on subjectivity (please see (Banfield, 1982;
Fludernik, 1993; Wiebe, 1994; Stein and Wright,
1995) for references). The study of subjectiv-
ity in language focuses on how private states are
expressed linguistically in context. Our second
goal was to develop an annotation scheme that
would be useful for corpus-based research on
subjective language and for the development of
applications such as multi-perspective question-
answering systems. The annotation scheme that
resulted is more detailed and comprehensive than
previous ones for subjective language.
Our study of the annotations produced by the
annotation scheme gives two important results.
First, we find that trained annotators can consis-
tently perform detailed opinion annotations with
good agreement (0.81 Kappa). Second, the agree-
ment results are better than in previous sentence-
level annotation studies, suggesting that adding
detail can help the annotators perform more re-
liably.
In the sections that follow, we first review how
opinions and other private states are expressed in
language (section 2) and give a brief overview
of previous work in subjectivity tagging (section
3). We then describe our annotation scheme for
private state expressions (section 4) and give the
results of an annotation study (section 5). We
conclude with a discussion of our findings from
the annotation study and and future work (section
??). In the appendix, we give sample annotations
as well as a snapshot of the annotations in GATE.
2 Expressing Private States in Text
2.1 Private States, Speech Events, and
Expressive Subjective Elements
There are two main ways that private states are
expressed in language. Private states may be ex-
plicitly mentioned, or they may be expressed in-
directly by the types of words and the style of lan-
guage that a speaker or writer uses. An example
of an explicitly-mentioned private state is ?frus-
trated? in sentence (1).
(1) Western countries were left frus-
trated and impotent after Robert
Mugabe formally declared that he
had overwhelmingly won Zimbabwe?s
presidential election.
Although most often verbs, it is interesting to note
that explicit mentions of private states may also
be nouns, such as ?concern? in ?international con-
cern? and ?will? in ?will of the people.? They may
even be adjectives, such as ?fearful? in ?fearful
populace.?
The second way that private states are generally
expressed is indirectly using expressive subjective
elements (Banfield, 1982). For example, the pri-
vate states in sentences (2) and (3) are expressed
entirely by the words and the style of language
that is used.
(2) The time has come, gentlemen, for
Sharon, the assassin, to realize that in-
justice cannot last long.
(3) ?We foresaw electoral fraud but not
daylight robbery,? Tsvangirai said.
In (2), although the writer does not explicitly
say that he hates Sharon, his choice of words
clearly demonstrates a negative attitude. In sen-
tence (3), describing the election as ?daylight rob-
bery? clearly reflects the anger being experienced
by the speaker, Tsvangirai. As used in these sen-
tences, the phrases ?The time has come,? ?gentle-
men,? ?the assassin,? ?injustice cannot last long,?
?fraud,? and ?daylight robbery? are all expressive
subjective elements. Expressive subjective ele-
ments are used by people to express their frus-
tration, anger, wonder, positive sentiment, mirth,
etc., without explicitly stating that they are frus-
trated, angry, etc. Sarcasm and irony often in-
volve expressive subjective elements.
When looking for opinions and other private
states in text, an annotator must consider speech
events as well as explicitly-mentioned private
states. In this work, we use speech event to refer
to any event of speaking or writing. However, the
mere presence of a speech event does not indicate
a private state. Both sentences (3) above and (4)
below contain speech events indicated by ?said.?
As mentioned previously, sentence (3) is opinion-
ated, while in (4) the information is presented as
factual.
(4) Medical Department head Dr
Hamid Saeed said the patient?s blood
had been sent to the Institute for
Virology in Johannesburg for analysis.
For speech terms such as ?said,? ?added,? ?told,?
?announce,? and ?report,? an annotator deter-
mines if there is a private state mainly by looking
inside the scope of the speech term for expressive
subjective elements.
Occasionally, we also find private states that
are expressed by direct physical actions. We call
such actions private state actions. Examples are
booing someone, sighing heavily, shaking ones
fist angrily, waving ones hand dismissively, and
frowning. ?Applauding? in sentence (5) is an ex-
ample of a positive-evaluative private state action.
(5) As the long line of would-be voters
marched in, those near the front of the
queue began to spontaneously applaud
those who were far behind them.
2.2 Nested Sources
An important aspect of a private state or speech
event is its source. The source of a speech event
is the speaker or writer. The source of a private
state is the experiencer of the private state, i.e.,
the person whose opinion or emotion is being ex-
pressed. Obviously, the writer of an article is a
source, because he wrote the sentences compos-
ing the article, but the writer may also write about
other people?s private states and speech events,
leading to multiple sources in a single sentence.
For example, each of the following sentences has
two sources: the writer (because he wrote the sen-
tences), and Sue (because she is the source of a
speech event in (6) and of private states in (7) and
(8), namely thinking and being afraid).
(6) Sue said, ?The election was fair.?
(7) Sue thinks that the election was fair.
(8) Sue is afraid to go outside.
Note, however, that we don?t really know what
Sue says, thinks or feels. All we know is what the
writer tells us. Sentence (6), for example, does
not directly present Sue?s speech event but rather
Sue?s speech event according to the writer. Thus,
we have a natural nesting of sources in a sentence.
The nesting of sources may be quite deep and
complex. For example, consider sentence (9).
(9) The Foreign Ministry said Thursday
that it was ?surprised, to put it mildly?
by the U.S. State Department?s criti-
cism of Russia?s human rights record
and objected in particular to the ?odi-
ous? section on Chechnya.
There are three sources in this sentence: the
writer, the Foreign Ministry, and the U.S. State
Department. The writer is the source of the over-
all sentence. The remaining explicitly mentioned
private states and speech events in (9) have the
following nested sources:
said: (writer, Foreign Ministry)
surprised, to put it mildly:
(writer, Foreign Ministry, Foreign Ministry)
criticism:
(writer, Foreign Ministry, U.S. State Dept.)
objected: (writer, Foreign Ministry)
Expressive subjective elements may also have
nested sources. In sentence (9), ?to put it mildly?
and ?odious? are expressive subjective elements,
both with nested source (writer, Foreign Min-
istry). We might expect that an expressive subjec-
tive element always has the same nested source
as the immediately dominating private state or
speech term. Although this is the case for ?odi-
ous? in (9) (the nested source of ?odious? and
?objected? is the same), it is not the same for ?big-
ger than Jesus? in (10):
(10) ?It is heresy,? said Cao. ?The
?Shouters? claim they are bigger than
Jesus.?
The nested source of the subjectivity expressed
by ?bigger than Jesus? is Cao, while the nested
source of ?claim? is (writer, Cao, Shouters).3
3(10) is an example of a de re rather than de dicto propo-
sitional attitude report (Rapaport, 1986).
3 Previous Work on Subjectivity
Tagging
In previous work (Wiebe et al, 1999), a corpus of
sentences from the Wall Street Journal Treebank
Corpus (Marcus et al, 1993) was manually anno-
tated with subjectivity classifications by multiple
judges. The judges were instructed to classify a
sentence as subjective if it contained any signif-
icant expressions of subjectivity, attributed to ei-
ther the writer or someone mentioned in the text,
and to classify the sentence as objective, other-
wise. The judges rated the certainty of their an-
swers on a scale from 0 to 3.
Agreement in the study was summarized in
terms of Cohen?s Kappa (   ) (Cohen, 1960),
which compares the total probability of agree-
ment to that expected if the taggers? classifica-
tions were statistically independent (i.e., ?chance
agreement?). After two rounds of tagging by
three judges, an average pairwise   value of 0.69
was achieved on a test set. On average, the judges
rated 15% of the sentences as very uncertain (rat-
ing 0). When these sentences are removed, the
average pairwise   value is 0.79. When sentences
with uncertainty judgment 0 or 1 are removed (on
average 30% of the sentences), the average pair-
wise   is 0.88.
4 An Annotation Scheme for Private
States
The annotation scheme described in this section
is more detailed and comprehensive the previ-
ous ones for subjective language. In (Wiebe et
al., 1999), summary subjective/objective judg-
ments were performed at the sentence level. For
this work, annotators are asked to mark within
each sentence the word spans that indicate speech
events or that are expressions of private states.
For every span that an annotator marks, there are
a number of attributes the annotator may set to
characterize the annotation.
The annotation scheme has two main com-
ponents. The first is an annotation type for
explicitly-mentioned private states and speech
events. The second is an annotation type for ex-
pressive subjective elements. Table 1 lists the at-
tributes that may be assigned to these two types
of annotations. In addition, there is an annotation
Explicit private states/speech events
nested-source
onlyfactive: yes, no
overall-strength: low, medium, high, extreme
on-strength: neutral, low, medium, high, extreme
attitude-type: positive, negative, both (exploratory)
attitude-toward (exploratory)
is-implicit
minor
Expressive subjective elements
nested-source
strength: low, medium, high, extreme
attitude-type: positive, negative, other (exploratory)
Table 1: Attributes for the two main annotation
types. For attributes that take on one of a fixed set
of values, the set of possible values are given.
type, agent, that annotators may use to mark the
noun phrase (if one exists) of the source of a pri-
vate state or speech event.
4.1 Explicitly-mentioned Private State and
Speech Event Annotations
An important part of the annotation scheme is
represented by the onlyfactive attribute. This at-
tribute is marked on every private state and speech
event annotation. The onlyfactive attribute is used
to indicate whether the source of the private state
or speech event is indeed expressing an emo-
tion, opinion or other private state. By defini-
tion, any expression that is an explicit private state
(e.g., ?think?, ?believe,? ?hope,? ?want?) or a pri-
vate state mixed with speech (e.g., ?berate,? ?ob-
ject,? ?praise?) is onlyfactive=no. On the other
hand, neutral speech events (e.g., ?said,? ?added,?
?told?) may be either onlyfactive=yes or onlyfac-
tive=no, depending on their contents. For ex-
ample, the annotation for ?said? in sentence (3)
would be marked onlyfactive=no, but the annota-
tion for ?said? in sentence (4) would be marked
onlyfactive=yes (sentences in section 2).
Note that even if onlyfactive=no, the sentence
may express something the nested source believes
is factual. Consider the sentence ?John criti-
cized Mary for smoking.? John expresses a private
state (his negative evaluation of Mary?s smoking).
However, this does not mean that John does not
believe that Mary smokes.
Like the onlyfactive attribute, the nested-source
attribute is included on every private state and
speech event annotation. The nested source (i.e.,
(writer, Foreign Ministry, U.S. State Dept.)) is
typed in by the annotator.
When an annotation is marked onlyfactive=no,
additional attributes are used to characterize the
private state. The overall-strength attribute is
used to indicate the overall strength of the pri-
vate state (considering the explicit private state
or speech event phrase as well as everything in-
side its scope). It?s value may range from low
to extreme. The on-strength attribute is used to
measure the contribution made specifically by the
explicit private state or speech event phrase. For
example, the on-strength of ?said? is typically
neutral, the on-strength of ?criticize? is typically
medium, and the on-strength of ?vehemently de-
nied? is typically high or extreme. (As for all as-
pects of this annotation scheme, the annotators
are asked to make these judgments in context.)
A speech event that is onlyfactive=yes has on-
strength=neutral and no overall-strength. Thus,
there is no need to include the overall-strength
and on-strength attributes for onlyfactive=yes an-
notations.
4.1.1 Implicit Speech Event Annotations
Implicit speech events posed a problem when
we developed the annotation scheme. Implicit
speech events are speech events in the discourse
for which there is no explicit speech event phrase,
and thus no obvious place to attach the anno-
tation. For example, most of the writer?s sen-
tences do not include a phrase such as ?I say.?
Also, direct quotes are not always accompanied
by discourse parentheticals (such as ?, she said?).
Our solution was to add the is-implicit attribute to
the annotation type for private states and speech
events, which may then be used to mark implicit
speech event annotations.
4.1.2 Minor Private States and Speech
Events
Depending on its goals, an application may
need to identify all private state and speech event
expressions in a document, or it may want to find
only those opinions and other private states that
are significant and real in the discourse. By ?sig-
nificant?, we mean that a significant portion of the
contents of the private state or speech event are
given within the sentence where the annotation
is marked. By ?real?, we mean that the private
state or speech event is presented as an existing
event within the domain of discourse, e.g., it is
not hypothetical. We use the term minor for pri-
vate states and speech events that are not signif-
icant or not real. Annotators mark minor private
state and speech event annotations by including
the minor attribute.
The following sentences all contain one or
more minor private states or speech events (high-
lighted in bold).
(11) Such wishful thinking risks mak-
ing the US an accomplice in the de-
struction of human rights. (not signif-
icant)
(12) If the Europeans wish to influence
Israel in the political arena... (in a con-
ditional, so not real)
(13) ?And we are seeking a declara-
tion that the British government de-
mands that Abbasi should not face trial
in a military tribunal with the death
penalty.? (not real, i.e., the declaration
of the demand is just being sought)
(14) The official did not say how many
prisoners were on the flight. (not real
because the saying event did not occur)
(15) No one who has ever studied realist
political science will find this surpris-
ing. (not real since a specific ?surprise?
state is not referred to; note that the
subject noun phrase is attributive rather
than referential (Donnellan, 1966))
4.2 Expressive Subjective Element
Annotations
As with private state/speech event annotations,
the nested-source attribute is included on every
expressive subjective element annotation. In ad-
dition to marking the source of an expression, the
nested-source is also functioning as a link. Within
a sentence, the nested-source chains together all
the pieces that together indicate the overall pri-
vate state of a particular source.
In addition to nested-source, the strength at-
tribute is used to characterize expressive subjec-
tive element annotations. The strength of an ex-
pressive subjective element may range from low
to extreme (see Table 1).
4.3 Exploratory Attributes
We are exploring additional attributes that allow
an annotator to further characterize the type of
attitude being expressed by a private state. An
annotator may use the attitude-type attribute to
mark an onlyfactive=no private state/speech event
annotation or an expressive subjective element
annotation as positive or negative. An attitude-
toward attribute may also be included on private
state/speech event annotations to indicate the par-
ticular target of an evaluation, emotion, etc.
5 Annotation Study
The data in our study consists of English-
language versions of foreign news documents
from FBIS, the U.S. Foreign Broadcast Informa-
tion Service. The data is from a variety of publi-
cations and countries. To date, 252 articles have
been annotated with the scheme described in sec-
tion 4.
To measure agreement on various aspects of the
annotation scheme, three annotators (A, M, and
S) independently annotated 13 documents with a
total of 210 sentences. None of the annotators are
authors of this paper. The articles are from a vari-
ety of topics and were selected so that 1/3 of the
sentences are from news articles reporting on ob-
jective topics (objective articles), 1/3 of the sen-
tences are from news articles reporting on opin-
ionated topics (?hot-topic? articles), and 1/3 of
the sentences are from editorials.
In the instructions to the annotators, we asked
them to rate the annotation difficulty of each arti-
cle on a scale from 1 to 3, with 1 being the eas-
iest and 3 being the most difficult. The annota-
tors were not told which articles were objective
or which articles were editorials, only that they
were being given a variety of different articles to
annotate.
We hypothesized that the editorials would be
the hardest to annotate and that the objective ar-
ticles would be the easiest. The ratings that the
annotators assigned to the articles support this hy-
pothesis. The annotators rated an average of 44%
of the articles in the study as easy (rating 1) and
26% as difficult (rating 3). But, they rated an av-
erage of 73% of the objective articles as easy, and
89% of the editorials as difficult.
It makes intuitive sense that ?hot-topic? articles
would be more difficult to annotate than objective
articles and that editorials would be more difficult
still. Editorials and ?hot-topic? articles contain
many more expressions of private states, requir-
ing an annotator to make more judgments than
they would for objective articles.
5.1 Agreement for Expressive Subjective
Element Annotations
For annotations that involve marking spans of
text, such as expressive subjective element an-
notations, it is not unusual for two annotators to
identify the same expression in the text, but to
differ in how they mark the boundaries.4 For
example, both annotators A and M saw expres-
sive subjectivity in the phrase, ?such a disadvan-
tageous situation.? But, while A marked the entire
phrase as a single expressive subjective element,
M marked the individual words, ?such? and ?dis-
advantageous.? Because the annotators will iden-
tify a different number of annotations, as well as
different (but hopefully strongly overlapping) sets
of expressions, we need an agreement metric that
can measure agreement between sets of objects.
We use the   metric to measure agreement
for expressive subjective elements (and later for
private state/speech event annotations).
  is a directional measure of agreement. Let

and  be the sets of spans annotated by anno-
tators   and 	 . We compute the agreement of 	 to
  as:
 
 	







This measure of agreement corresponds to the no-
tion of precision and recall as used to evaluate, for
example, named entity recognition. The  
 	
metric corresponds to the recall if   is the gold-
standard and 	 the system, and to precision, if they
are reversed.
In the 210 sentences in the annotation study, the
annotators A, M, and S respectively marked 311,
352 and 249 expressive subjective elements. Ta-
ble 2 shows the pairwise agreement for these sets
of annotations. For example, M agrees with 76%
of the expressive subjective elements marked by
4In the coding instructions, we did not attempt to define
rules to try to enforce boundary agreement.
mother of terrorism
if the world has to rid itself from this menace, the perpetrators across the border had to be dealt with firmly
indulging in blood-shed and their lunaticism
ultimately the demon they have reared will eat up their own vitals
Table 3: Extreme strength expressive subjective elements
     	
  
  average
A M 0.76 0.72
A S 0.68 0.81
M S 0.59 0.74
0.72
Table 2: Inter-annotator Agreement: Expressive
subjective elements
A, and A agrees with 72% of the expressive
subjective elements marked by M. The average
agreement in Table 2 is the arithmetic mean of all
six   .
We hypothesized that the stronger the expres-
sion of subjectivity, the more likely the annota-
tors are to agree. To test this hypothesis, we mea-
sure agreement for the expressive subjective ele-
ments rated with a strength of medium or higher
by at least one annotator. This excludes on av-
erage 29% of the expressive subjective elements.
The average pairwise agreement rises to 0.80.
When measuring agreement for the expressive
subjective elements rated high or extreme, this ex-
cludes an average 65% of expressive subjective
elements, and the average pairwise agreement in-
creases to 0.88. Thus, annotators are more likely
to agree when the expression of subjectivity is
strong. Table 3 gives examples of expressive sub-
jective elements that at least one annotator rated
as extreme.
5.2 Agreement for Private State/Speech
Event Annotations
For private state and speech event annotations, we
again use   to measure agreement between the
sets of expressions identified by each annotator.
The three annotators, A, M, and S, respectively
marked 338, 285, and 315 explicit expressions of
private states and speech events. Implicit speech
events for the writer of course are excluded. Table
4 shows the pairwise agreement for these sets of
annotations.
The average pairwise agreement for explicit
private state and speech event expressions is 0.82,
     	    average
A M 0.75 0.91
A S 0.80 0.85
M S 0.86 0.75
0.82
Table 4: Inter-annotator Agreement: Explicitly-
mentioned private states and speech events
which indicates that they are easier to annotate
than expressive subjective elements.
5.3 Agreement for Attributes
In this section, we focus on the annotators? agree-
ment for judgments that reflect whether or not
an opinion, emotion, sentiment, speculation, or
other private state is being expressed. We con-
sider these judgments to be at the core of the an-
notation scheme. Two attributes, onlyfactive and
on-strength, carry information about whether a
private state is being expressed.
For onlyfactive judgments, we measure pair-
wise agreement between annotators for the set
of private state and speech event annotations that
both annotators identified. Because we are now
measuring agreement over the same set of objects
for each annotator, we use Kappa (   ) to capture
how well the annotators agree.
Table 5 shows the contingency table for the on-
lyfactive judgments made by annotators A and M.
The Kappa scores for all annotator pairs are given
in Table 7. For their onlyfactive judgments, i.e.,
whether or not an opinion or other private state
is being expressed, the annotators have an aver-
age pairwise Kappa of 0.81. Under Krippendorf?s
scale (Krippendorf, 1980), this allows for definite
conclusions.
With many judgments that characterize natural
language, one would expect that there are clear
cases as well as borderline cases, which would be
more difficult to judge. The agreement study in-
dicates that this is certainly true for private states.
In terms of our annotations, we define an explicit
private state or speech event to be borderline-
    

 
	
 
  

  Class-based Collocations for Word-Sense Disambiguation
Tom O?Hara
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
tomohara@cs.nmsu.edu
Rebecca Bruce
Department of Computer Science
University of North Carolina at Asheville
Asheville, NC 28804-3299
bruce@cs.unca.edu
Jeff Donner
Department of Computer Science
New Mexico State University
Las Cruces, NM 88003-8001
jdonner@cs.nmsu.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260-4034
wiebe@cs.pitt.edu
Abstract
This paper describes the NMSU-Pitt-UNCA
word-sense disambiguation system participat-
ing in the Senseval-3 English lexical sample
task. The focus of the work is on using seman-
tic class-based collocations to augment tradi-
tional word-based collocations. Three separate
sources of word relatedness are used for these
collocations: 1) WordNet hypernym relations;
2) cluster-based word similarity classes; and 3)
dictionary definition analysis.
1 Introduction
Supervised systems for word-sense disambigua-
tion (WSD) often rely upon word collocations
(i.e., sense-specific keywords) to provide clues
on the most likely sense for a word given the
context. In the second Senseval competition,
these features figured predominantly among the
feature sets for the leading systems (Mihalcea,
2002; Yarowsky et al, 2001; Seo et al, 2001).
A limitation of such features is that the words
selected must occur in the test data in order for
the features to apply. To alleviate this problem,
class-based approaches augment word-level fea-
tures with category-level ones (Ide and Ve?ronis,
1998; Jurafsky and Martin, 2000). When ap-
plied to collocational features, this approach ef-
fectively uses class labels rather than wordforms
in deriving the collocational features.
This research focuses on the determination
of class-based collocations to improve word-
sense disambiguation. We do not address refine-
ment of existing algorithms for machine learn-
ing. Therefore, a commonly used decision tree
algorithm is employed to combine the various
features when performing classification.
This paper describes the NMSU-Pitt-
UNCA system we developed for the third
Senseval competition. Section 2 presents an
overview of the feature set used in the system.
Section 3 describes how the class-based colloca-
tions are derived. Section 4 shows the results
over the Senseval-3 data and includes detailed
analysis of the performance of the various col-
locational features.
2 System Overview
We use a decision tree algorithm for word-sense
disambiguation that combines features from the
local context of the target word with other lex-
ical features representing the broader context.
Figure 1 presents the features that are used
in this application. In the first Senseval com-
petition, we used the first two groups of fea-
tures, Local-context features and Collocational
features, with competitive results (O?Hara et al,
2000).
Five of the local-context features represent
the part of speech (POS) of words immediately
surrounding the target word. These five fea-
tures are POS?i for i from -2 to +2 ), where
POS+1, for example, represents the POS of the
word immediately following the target word.
Five other local-context features represent
the word tokens immediately surrounding the
target word (Word?i for i from ?2 to +2).
Each Word?i feature is multi-valued; its values
correspond to all possible word tokens.
There is a collocation feature WordColl
s
de-
fined for each sense s of the target word. It
is a binary feature, representing the absence or
presence of any word in a set specifically chosen
for s. A word w that occurs more than once in
the training data is included in the collocation
set for sense s if the relative percent gain in the
conditional probability over the prior probabil-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Local-context features
POS: part-of-speech of target word
POS?i: part-of-speech of word at offset i
WordForm: target wordform
Word?i: stem of word at offset i
Collocational features
WordColl
s
: word collocation for sense s
WordColl
?
wordform of non-sense-specific
collocation (enumerated)
Class-based collocational features
HyperColl
s
: hypernym collocation for s
HyperColl
?,i
: non-sense-specific hypernym collo-
cation
SimilarColl
s
: similarity collocation for s
DictColl
s
: dictionary collocation for s
Figure 1: Features for word-sense disambigua-
tion. All collocational features are binary indi-
cators for sense s, except for WordColl
?
.
ity is 20% or higher:
(P (s|w) ? P (s))
P (s) ? 0.20.
This threshold was determined to be effective
via an optimization search over the Senseval-2
data. WordColl
?
represents a set of non-sense-
specific collocations (i.e., not necessarily indica-
tive of any one sense), chosen via the G2 criteria
(Wiebe et al, 1998). In contrast to WordColl
s
,
each of which is a separate binary feature, the
words contained in the set WordColl
?
serve as
values in a single enumerated feature.
These features are augmented with class-
based collocational features that represent in-
formation about word relationships derived
from three separate sources: 1) WordNet
(Miller, 1990) hypernym relations (HyperColl);
2) cluster-based word similarity classes (Simi-
larColl); and 3) relatedness inferred from dictio-
nary definition analysis (DictColl). The infor-
mation inherent in the sources from which these
class-based features are derived allows words
that do not occur in the training data context
to be considered as collocations during classifi-
cation.
3 Class-based Collocations
The HyperColl features are intended to capture
a portion of the information in the WordNet hy-
pernyms links (i.e., is-a relations). Hypernym-
based collocations are formulated by replacing
each word in the context of the target word (e.g.,
in the same sentence as the target word) with
its complete hypernym ancestry from WordNet.
Since context words are not sense-tagged, each
synset representing a different sense of a context
word is included in the set of hypernyms replac-
ing that word. Likewise, in the case of multiple
inheritance, each parent synset is included.
The collocation variable HyperColl
s
for each
sense s is binary, corresponding to the absence
or presence of any hypernym in the set chosen
for s. This set of hypernyms is chosen using the
ratio of conditional probability to prior prob-
ability as described for the WordColl
s
feature
above. In contrast, HyperColl
?,i
selects non-
sense-specific hypernym collocations: 10 sepa-
rate binary features are used based on the G2
selection criteria. (More of these features could
be used, but they are limited for tractability.)
For more details on hypernym collocations, see
(O?Hara, forthcoming).
Word-similarity classes (Lin, 1998) derived
from clustering are also used to expand the
pool of potential collocations; this type of se-
mantic relatedness among words is expressed in
the SimilarColl feature. For the DictColl fea-
tures, definition analysis (O?Hara, forthcoming)
is used to determine the semantic relatedness of
the defining words. Differences between these
two sources of word relations are illustrated by
looking at the information they provide for ?bal-
lerina?:
word-clusters:
dancer:0.115 baryshnikov:0.072
pianist:0.056 choreographer:0.049
... [18 other words]
nicole:0.041 wrestler:0.040
tibetans:0.040 clown:0.040
definition words:
dancer:0.0013 female:0.0013 ballet:0.0004
This shows that word clusters capture a wider
range of relatedness than the dictionary def-
initions at the expense of incidental associa-
tions (e.g., ?nicole?). Again, because context
words are not disambiguated, the relations for
all senses of a context word are conflated. For
details on the extraction of word clusters, see
(Lin, 1998); and, for details on the definition
analysis, see (O?Hara, forthcoming).
When formulating the features SimilarColl
and DictColl, the words related to each con-
text word are considered as potential colloca-
tions (Wiebe et al, 1998). Co-occurrence fre-
Sense Distinctions Precision Recall
Fine-grained .566 .565
Course-grained .660 .658
Table 1: Results for Senseval-3 test data.
99.72% of the answers were attempted. All fea-
tures from Figure 1 were used.
quencies f(s,w) are used in estimating the con-
ditional probability P (s|w) required by the rel-
ative conditional probability selection scheme
noted earlier. However, instead of using a unit
weight for each co-occurrence, the relatedness
weight is used (e.g., 0.056 for ?pianist?); and,
because a given related-word might occur with
more than one context word for the same target-
word sense, the relatedness weights are added.
The conditional probability of the sense given
the relatedness collocation is estimated by di-
viding the weighted frequency by the sum of all
such weighted co-occurrence frequencies for the
word:
P (s|w)? wf (s,w)?
s
?
wf (s?, w)
Here wf(s, w) stands for the weighted co-
occurrence frequency of the related-word collo-
cation w and target sense s.
The relatedness collocations are less reliable
than word collocations given the level of indi-
rection involved in their extraction. Therefore,
tighter constraints are used in order to filter out
extraneous potential collocations. In particular,
the relative percent gain in the conditional ver-
sus prior probability must be 80% or higher, a
threshold again determined via an optimization
search over the Senseval-2 data. In addition,
the context words that they are related to must
occur more than four times in the training data.
4 Results and Discussion
Disambiguation is performed via a decision tree
formulated using Weka?s J4.8 classifier (Witten
and Frank, 1999). For the system used in the
competition, the decision tree was learned over
the entire Senseval-3 training data and then ap-
plied to the test data. Table 1 shows the results
of our system in the Senseval-3 competition.
Table 2 shows the results of 10-fold cross-
validation just over the Senseval-3 training data
(using Naive Bayes rather than decision trees.)
To illustrate the contribution of the three types
Experiment Precision
?Local +Local
Local - .593
WordColl .490 .599
HyperColl .525 .590
DictColl .532 .570
SimilarColl .534 .586
HyperColl+WordColl .525 .611
DictColl+WordColl .501 .606
SimilarColl+WordColl .518 .596
All Collocations .543 .608
#Words: 57 Avg. Entropy: 1.641
Avg. #Senses: 5.3 Baseline: 0.544
Table 2: Results for Senseval-3 training data.
All values are averages, except #Words, which
is the number of distinct word types classified.
Baseline always uses the most-frequent sense.
of class-based collocations, the table shows re-
sults separately for systems developed using a
single feature type, as well as for all features in
combination. In addition, the performance of
these systems are shown with and without the
use of the local features (Local), as well as with
and without the use of standard word colloca-
tions (WordColl). As can be seen, the related-
word and definition collocations perform better
than hypernym collocations when used alone.
However, hypernym collocations perform bet-
ter when combined with other features. Fu-
ture work will investigate ways of ameliorat-
ing such interactions. The best overall system
(HyperColl+WordColl+Local) uses the com-
bination of local-context features, word colloca-
tions, and hypernym collocations. The perfor-
mance of this system compared to a more typi-
cal system for WSD (WordColl+Local) is sta-
tistically significant at p < .05, using a paired
t-test.
We analyzed the contributions of the various
collocation types to determine their effective-
ness. Table 3 shows performance statistics for
each collocation type taken individually over the
training data. Precision is based on the num-
ber of correct positive indicators versus the to-
tal number of positive indicators, whereas recall
is the number correct over the total number of
training instances (7706). This shows that hy-
pernym collocations are nearly as effective as
word collocations. We also analyzed the occur-
rence of unique positive indicators provided by
the collocation types over the training data. Ta-
Total Total
Feature #Corr. #Pos. Recall Prec.
DictColl 273 592 .035 .461
HyperColl 2932 6479 .380 .453
SimilarColl 528 1535 .069 .344
WordColl 3707 7718 .481 .480
Table 3: Collocation performance statistics.
Total #Pos. is number of positive indicators for
the collocation in the training data, and Total
#Corr. is the number of these that are correct.
Unique Unique
Feature #Corr. #Pos. Prec.
DictColl 110 181 .608
HyperColl 992 1795 .553
SimilarColl 198 464 .427
DictColl 1244 2085 .597
Table 4: Analysis of unique positive indicators.
Unique #Pos. is number of training instances
with the feature as the only positive indicator,
and Unique #Corr. is number of these correct.
ble 4 shows how often each feature type is pos-
itive for a particular sense when all other fea-
tures for the sense are negative. This occurs
fairly often, suggesting that the different types
of collocations are complementary and thus gen-
erally useful when combined for word-sense dis-
ambiguation. Both tables illustrate coverage
problems for the definition and related word
collocations, which will be addressed in future
work.
References
Nancy Ide and Jean Ve?ronis. 1998. Introduc-
tion to the special issue on word sense dis-
ambiguation: the state of the art. Computa-
tional Linguistics, 24(1):1?40.
Daniel Jurafsky and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, New Jersey.
Dekang Lin. 1998. Automatic retrieval
and clustering of similar words. In Proc.
COLING-ACL 98, pages 768?764, Montreal.
August 10-14.
Rada Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
the 19th International Conference on Com-
putational Linguistics (COLING 2002), Tai-
wan. August 26-30.
George Miller. 1990. Introduction. Interna-
tional Journal of Lexicography, 3(4): Special
Issue on WordNet.
Tom O?Hara, Janyce Wiebe, and Rebecca F.
Bruce. 2000. Selecting decomposable models
for word-sense disambiguation: The grling-
sdm system. Computers and the Humanities,
34(1-2):159?164.
Thomas P. O?Hara. forthcoming. Empirical ac-
quisition of conceptual distinctions via dictio-
nary definitions. Ph.D. thesis, Department of
Computer Science, New Mexico State Univer-
sity.
Hee-Cheol Seo, Sang-Zoo Lee, Hae-Chang
Rim, and Ho Lee. 2001. KUNLP sys-
tem using classification information model at
SENSEVAL-2. In Proceedings of the Second
International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-
2), pages 147?150, Toulouse. July 5-6.
Janyce Wiebe, Kenneth McKeever, and Re-
becca F. Bruce. 1998. Mapping collocational
properties into machine learning features. In
Proc. 6th Workshop on Very Large Corpora
(WVLC-98), pages 225?233, Montreal, Que-
bec, Canada. Association for Computational
Linguistics. SIGDAT.
Ian H. Witten and Eibe Frank. 1999. Data
Mining: Practical Machine Learning Tools
and Techniques with Java Implementations.
Morgan Kaufmann, San Francisco, CA.
David Yarowsky, Silviu Cucerzan, Radu Flo-
rian, Charles Schafer, and Richard Wicen-
towski. 2001. The Johns Hopkins SENSE-
VAL2 system descriptions. In Proceedings of
the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems
(SENSEVAL-2), pages 163?166, Toulouse.
July 5-6.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 53?60,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Annotating Attributions and Private States
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper describes extensions to a corpus
annotation scheme for the manual annotation
of attributions, as well as opinions, emotions,
sentiments, speculations, evaluations and other
private states in language. It discusses the
scheme with respect to the ?Pie in the Sky?
Check List of Desirable Semantic Information
for Annotation. We believe that the scheme is a
good foundation for adding private state anno-
tations to other layers of semantic meaning.
1 Introduction
This paper describes a fine-grained annotation scheme
for key components and properties of opinions, emo-
tions, sentiments, speculations, evaluations, and other
private states in text. We first give an overview of the
core scheme. We then describe recent extensions to the
scheme, namely refined annotations of attitudes and tar-
gets, or objects, of private states. Finally, we discuss re-
lated items from the ?Pie in the Sky? Check List of De-
sirable Semantic Information for Annotation, and related
work. We believe our scheme would provide a founda-
tion for adding private state annotations to other layers of
semantic and pragmatic meaning.
2 The Core Scheme
This section overviews the core of the annotation scheme.
Further details may be found in (Wilson and Wiebe,
2003; Wiebe et al, 2005).
2.1 Means of Expressing Private States
The goals of the annotation scheme are to represent inter-
nal mental and emotional states, and to distinguish sub-
jective information from material presented as fact. As
a result, the annotation scheme is centered on the no-
tion of private state, a general term that covers opinions,
beliefs, thoughts, feelings, emotions, goals, evaluations,
and judgments. As Quirk et al (1985) define it, a private
state is a state that is not open to objective observation
or verification: ?a person may be observed to assert that
God exists, but not to believe that God exists. Belief is in
this sense ?private?.? (p. 1181) Following literary theo-
rists such as Banfield (1982), we use the term subjectivity
for linguistic expressions of private states in the contexts
of texts and conversations.
We can further view private states in terms of their
functional components ? as states of experiencers hold-
ing attitudes, optionally toward targets. For example, for
the private state in the sentence John hates Mary, the ex-
periencer is ?John,? the attitude is ?hate,? and the target
is ?Mary.?
We create private state frames for three main types of
private state expressions in text:
? explicit mentions of private states
? speech events expressing private states
? expressive subjective elements
An example of an explicit mention of a private state is
?fears? in (1):
(1) ?The U.S. fears a spill-over,? said Xirao-
Nima.
An example of a speech event expressing a private state
is ?said? in (2):
(2) ?The report is full of absurdities,? Xirao-
Nima said.
Note that we use the term speech event to refer to both
speaking and writing events.
The phrase ?full of absurdities? in (2) above is an ex-
pressive subjective element (Banfield, 1982). Other ex-
amples can be found in (3):
(3) The time has come, gentlemen, for
Sharon, the assassin, to realize that injustice
cannot last long.
53
The private states in this sentence are expressed entirely
by the words and the style of language that is used. In
(3), although the writer does not explicitly say that he
hates Sharon, his choice of words clearly demonstrates
a negative attitude toward him. As used in these sen-
tences, the phrases ?The time has come,? ?gentlemen,?
?the assassin,? and ?injustice cannot last long? are all ex-
pressive subjective elements. Expressive subjective el-
ements are used by people to express their frustration,
anger, wonder, positive sentiment, etc., without explic-
itly stating that they are frustrated, angry, etc. Sarcasm
and irony often involve expressive subjective elements.
2.2 Private State Frames
We propose two types of private state frames: expressive
subjective element frames will be used to represent
expressive subjective elements; and direct subjective
frames will be used to represent both subjective speech
events (i.e., speech events expressing private states) and
explicitly mentioned private states. The frames have the
following attributes:
Direct subjective (subjective speech event or explicit
private state) frame:
? text anchor: a pointer to the span of text that rep-
resents the speech event or explicit mention of a pri-
vate state.
? source: the person or entity that expresses or expe-
riences the private state, possibly the writer.
? target: the target or topic of the private state, i.e.,
what the speech event or private state is about.
? properties:
? intensity: the intensity of the private state (low,
medium, high, or extreme).
? expression intensity: the contribution of the
speech event or private state expression itself
to the overall intensity of the private state. For
example, ?say? is often neutral, even if what is
uttered is not neutral, while ?excoriate? itself
implies a very strong private state.
? insubstantial: true, if the private state is not
substantial in the discourse. For example, a pri-
vate state in the context of a conditional often
has the value true for attribute insubstantial.
? attitude type: the type of attitude(s) compos-
ing the private state.
Expressive subjective element frame:
? text anchor: a pointer to the span of text that de-
notes the subjective or expressive phrase.
? source: the person or entity that is expressing the
private state, possibly the writer.
? properties:
? intensity: the intensity of the private state.
? attitude type
2.3 Objective Speech Event Frames
To distinguish opinion-oriented material from material
presented as factual, we also define objective speech
event frames. These are used to represent material that is
attributed to some source, but is presented as objective
fact. They include a subset of the slots in private state
frames:
Objective speech event frame:
? text anchor: a pointer to the span of text that de-
notes the speech event.
? source: the speaker or writer.
? target: the target or topic of the speech event, i.e.,
the content of what is said.
For example, an objective speech event frame is cre-
ated for ?said? in the following sentence (assuming no
undue influence from the context):
(4) Sargeant O?Leary said the incident took
place at 2:00pm.
That the incident took place at 2:00pm is presented as a
fact with Sargeant O?Leary as the source of information.
2.4 Agent Frames
The annotation scheme includes an agent frame for noun
phrases that refer to sources of private states and speech
events, i.e., for all noun phrases that act as the experi-
encer of a private state, or the speaker/writer of a speech
event. Each agent frame generally has two slots. The text
anchor slot includes a pointer to the span of text that de-
notes the noun phrase source. The source slot contains
a unique alpha-numeric ID that is used to denote this
source throughout the document. The agent frame as-
sociated with the first informative (e.g., non-pronominal)
reference to this source in the document includes an id
slot to set up the document-specific source-id mapping.
2.5 Nested Sources
The source of a speech event is the speaker or writer. The
source of a private state is the experiencer of the private
state, i.e., the person whose opinion or emotion is being
expressed. The writer of an article is always a source, be-
cause he or she wrote the sentences of the article, but the
writer may also write about other people?s private states
54
and speech events, leading to multiple sources in a single
sentence. For example, each of the following sentences
has two sources: the writer (because he or she wrote the
sentences), and Sue (because she is the source of a speech
event in (5) and of private states in (6) and (7)).
(5) Sue said, ?The election was fair.?
(6) Sue thinks that the election was fair.
(7) Sue is afraid to go outside.
Note, however, that we don?t really know what Sue says,
thinks or feels. All we know is what the writer tells us.
For example, Sentence (5) does not directly present Sue?s
speech event but rather Sue?s speech event according to
the writer. Thus, we have a natural nesting of sources in
a sentence.
In particular, private states are often filtered through
the ?eyes? of another source, and private states are of-
ten directed toward the private states of others. Consider
sentence (1) above and (8) following:
(8) China criticized the U.S. report?s criticism
of China?s human rights record.
In sentence (1), the U.S. does not directly state its fear.
Rather, according to the writer, according to Xirao-Nima,
the U.S. fears a spill-over. The source of the private state
expressed by ?fears? is thus the nested source ?writer,
Xirao-Nima, U.S.?. In sentence (8), the U.S. report?s crit-
icism is the target of China?s criticism. Thus, the nested
source for ?criticism? is ?writer, China, U.S. report?.
Note that the shallowest (left-most) agent of all nested
sources is the writer, since he or she wrote the sentence.
In addition, nested source annotations are composed of
the IDs associated with each source, as described in
the previous subsection. Thus, for example, the nested
source ?writer, China, U.S. report? would be represented
using the IDs associated with the writer, China, and the
report being referred to, respectively.
2.6 Examples
We end this section with examples of direct subjective,
expressive subjective element, and objective speech event
frames (sans target and attitude type attributes, which are
discussed in the next section).
First, we show the frames that would be associated
with sentence (9), assuming that the relevant source ID?s
have already been defined:
(9) ?The US fears a spill-over,? said Xirao-
Nima.
Objective speech event:
Text anchor: the entire sentence
Source: <writer>
Implicit: true
Objective speech event:
Text anchor: said
Source: <writer,Xirao-Nima>
Direct subjective:
Text anchor: fears
Source: <writer,Xirao-Nima,U.S.>
Intensity: medium
Expression intensity: medium
The first objective speech event frame represents that, ac-
cording to the writer, it is true that Xirao-Nima uttered
the quote and is a professor at the university referred
to. The implicit attribute is included because the writer?s
speech event is not explicitly mentioned in the sentence
(i.e., there is no explicit phrase such as ?I write?).
The second objective speech event frame represents
that, according to the writer, according to Xirao-Nima, it
is true that the US fears a spillover. Finally, when we drill
down to the subordinate clause we find a private state: the
US fear of a spillover. Such detailed analyses, encoded
as annotations on the input text, would enable a person
or an automated system to pinpoint the subjectivity in a
sentence, and attribute it appropriately.
Now, consider sentence (10):
(10) ?The report is full of absurdities,? Xirao-
Nima said.
Objective speech event:
Text anchor: the entire sentence
Source: <writer>
Implicit: true
Direct subjective:
Text anchor: said
Source: <writer,Xirao-Nima>
Intensity: high
Expression intensity: neutral
Expressive subjective element:
Text anchor: full of absurdities
Source: <writer,Xirao-Nima>
Intensity: high
The objective frame represents that, according to the
writer, it is true that Xirao-Nima uttered the quoted string.
The second frame is created for ?said? because it is a sub-
jective speech event: private states are conveyed in what
is uttered. Note that intensity is high but expression inten-
sity is neutral: the private state being expressed is strong,
but the specific speech event phrase ?said? does not it-
self contribute to the intensity of the private state. The
third frame is for the expressive subjective element ?full
of absurdities.?
3 Annotation Process
To date, over 11,000 sentences in 550 documents have
been annotated according to the annotation scheme de-
scribed above. The documents are English-language ver-
sions of news documents from the world press. The doc-
uments are from 187 different news sources in a variety
55
of countries. The original documents and their annota-
tions are available at
http://nrrc.mitre.org/NRRC/publications.htm.
The annotation process and inter-annotator agreement
studies are described in (Wiebe et al, 2005). Here, we
want to highlight two themes of the annotation instruc-
tions:
1. There are no fixed rules about how particular words
should be annotated. The instructions describe the
annotations of specific examples, but do not state
that specific words should always be annotated a cer-
tain way.
2. Sentences should be interpreted with respect to the
contexts in which they appear. The annotators
should not take sentences out of context and think
what they could mean, but rather should judge them
as they are being used in that particular sentence and
document.
We believe that these general strategies for annotation
support the creation of corpora that will be useful for
studying expressions of subjectivity in context.
4 Extensions: Attitude and Target
Annotations
Before we describe the new attitude and target annota-
tions, consider the following sentence.
(11) ?I think people are happy because Chavez
has fallen.?
This sentence contains two private states, represented by
direct subjective annotations anchored on ?think? and
?happy,? respectively.
The word ?think? is used to express an opinion about
what is true according to its source (a positive arguing
attitude type; see Section 4.1). The target of ?think? is
?people are happy because Chavez has fallen.?
The word ?happy? clearly expresses a positive attitude,
with target ?Chavez has fallen.? However, looking more
closely at the private state for ?happy,? we see that we
can also infer a negative attitude toward Chavez, from
the phrase ?happy because Chavez has fallen.?
Sentence (11) illustrates some of the things we need to
consider when representing attitudes and targets. First,
we see that more than one type of attitude may be in-
volved when a private state is expressed. In (11), there
are three (a positive attitude, a negative attitude, and a
positive arguing attitude). Second, more than one target
may be associated with a private state. Consider ?happy?
in (11). The target of the positive attitude is ?Chavez has
fallen,? while the target of the inferred negative attitude
is ?Chavez.?
Positive Attitudes Positive Arguing
Negative Attitudes Negative Arguing
Positive Intentions Speculation
Negative Intentions Other Attitudes
Table 1: Attitude Types
The representation also must support multiple targets
for a single attitude, as illustrated by Sentence (12):
(12) Tsvangirai said the election result was a
clear case of highway robbery by Mugabe, his
government and his party, Zanu-PF.
In (12), the phrase ?a clear case of highway robbery? ex-
presses a negative attitude of Tsvangirai. This negative
attitude has two targets: ?the election results? and ?Mu-
gabe, his government and his party, Zanu-PF.?
To capture the kind of detailed attitude and target in-
formation that we described above, we propose two new
types of annotations: attitude frames and target frames.
We describe these new annotations in Sections 4.2 and
4.3, but first we introduce the set of attitude types that we
developed for the annotation scheme.
4.1 Types of Attitudes
One of our goals in extending the annotation scheme for
private states was to develop a set of attitude types that
would be useful for NLP applications. It it also important
that the set of attitude types provide good coverage for the
range of possible private states. Working with our anno-
tators and looking at the private states already annotated,
we developed the set of attitude types listed in Table 1.
Below we give a brief description of each attitude
type, followed by an example. In each example, the span
of text that expresses the attitude type is in bold, and the
span of text that refers to the target of the attitude type (if
a target is given) is in angle brackets.
Positive Attitudes: positive emotions, evaluations, judg-
ments and stances.
(13) The Namibians went as far as to say
?Zimbabwe?s election system? was ?water
tight, without room for rigging?.
Negative Attitudes: negative emotions, evaluations,
judgments and stances.
(14) His disenfranchised supporters were
seething.
Positive Arguing: arguing for something, arguing that
something is true or so, arguing that something did hap-
pen or will happen, etc.
56
(15) Iran insists ?its nuclear program is purely
for peaceful purposes?.
Negative Arguing: arguing against something, arguing
that something is not true or not so, arguing that some-
thing did not happen or will not happen, etc.
(16) Officials in Panama denied that ?Mr.
Chavez or any of his family members had asked
for asylum?.
Positive Intentions: aims, goals, plans, and other overtly
expressed intentions.
(17) The Republic of China government be-
lieves in the US committment ?to separating
its anti-terrorism campaign from the Taiwan
Strait issue?, an official said Thursday.
Negative Intentions: expressing that something is not an
aim, not a goal, not an intention, etc.
(18) The Bush administration has no plans ?to
ease sanctions against mainland China?.
Speculation: speculation or uncertainty about what may
or may not be true, what may or may not happen, etc.
(19) ?The president is likely to endorse the
bill?.
Other Attitudes: other types of attitudes that do not fall
into one of the above categories.
(20) To the surprise of many, ?the dollar hit
only 2.4 pesos and closed at 2.1?.
4.2 Attitude Frames
With the introduction of the attitude frames, two issues
arise. First, which spans of text should the new atti-
tudes be anchored to? Second, how do we tie the attitude
frames back to the private states that they are part of?
The following sentence illustrates the first issue.
(21) The MDC leader said systematic cheating,
spoiling tactics, rigid new laws, and shear ob-
struction - as well as political violence and in-
timidation - were just some of the irregularities
practised by the authorities in the run-up to, and
during the poll.
In (21), there are 5 private state frames attributed
to the MDC leader: a direct subjective frame an-
chored to ?said,? and four expressive subjective ele-
ment frames anchored respectively to ?systematic cheat-
ing . . . obstruction,? ?as well as,? ?violence and intimida-
tion,? and ?just some of the irregularities.? We could cre-
ate an attitude frame for each of these private state frames,
but we believe the following is a better solution. For each
direct subjective frame, the annotator is asked to consider
the direct subjective annotation and everything within the
scope of the annotation when deciding what attitude types
are being expressed by the source of the direct subjective
frame. Then, for each attitude type identified, the an-
notator creates an attitude frame and anchors the frame
to whatever span of text completely captures the attitude
type. In to sentence (21), this results in just one attitude
frame being created to represent the negative attitude of
the MDC leader. The anchor for this attitude frame begins
with ?systematic cheating? and ends with ?irregularities.?
Turning to the second issue, tying attitude frames to
their private states, we do two things. First, we create a
unique ID for the attitude frame. Then, we change the
attitude type attribute on the direct subjective annotation
into a new attribute called an attitude link. We place the
attitude frame ID into the attitude link slot. The attitude
link slot can hold more then one attitude frame ID, allow-
ing us to represent a private state composed of more than
one type of attitude.
Because we expect the attitude annotations to overlap
with most of the expressive subjective element annota-
tions, we chose not to link attitude frames to expressive
subjective element frames. However, this would be pos-
sible to do should it become necessary.
The attitude frame has the following attributes:
Attitude frame:
? id: a unique alphanumeric ID for identifying the at-
titude annotation. The ID is used to link the attitude
annotation to the private state it is part of.
? text anchor: a pointer to the span of text that cap-
tures the attitude being expressed.
? attitude type: one of the attitude types listed in Ta-
ble 1.
? target link: one or more target annotation IDs (see
Section 4.3).
? intensity: the intensity of the attitude.
? properties:
? inferred: true, if the attitude is inferred.
? sarcastic: true, if the attitude is realized
through sarcasm.
? repetition: true, if the attitude is realized
through the repetition of words, phrases, or
syntax.
? contrast: true, if the attitude is realized only
through contrast with another attitude.
57
Of the four attitude-frame properties, inferred was al-
ready discussed. The property sarcastic marks attitudes
expressed using sarcasm. In general, we think this prop-
erty will be of interest for NLP applications working with
opinions. Detecting sarcasm may also help a system learn
to distinguish between positive and negative attitudes.
The sarcasm in Sentence (22), below, makes the word
?Great? an expression of a negative rather than a positive
attitude.
(22) ?Great, keep on buying dollars so there?ll
be more and more poor people in the country,?
shouted one.
The repetition and contrast properties are also for mark-
ing different ways in which an attitude might be realized.
We feel these properties will be useful for developing an
automatic system for recognizing different types of atti-
tudes.
4.3 Target Frames
The target frame is used to mark the target of each atti-
tude. A target frame has two slots, the id slot and the text
anchor slot. The id slot contains a unique alpha-numeric
ID for identifying the target annotation. We use the target
frame ID to link the target back to the attitude frame. The
attitude frame has a target-link slot that can hold one or
more target frame IDs. This allows us to represent when
a single attitude is directed at more than one target.
The text anchor slot has a pointer to the span of text that
denotes the target. If there is more than one reference to
the target in the sentence, the most syntactically relevant
reference is chosen.
To illustrate what we mean by syntactically relevant,
consider the following sentence.
(23) African observers generally approved of
?his victory? while Western governments de-
nounced ?it?.
The target of the two attitudes (in bold) in the above sen-
tence is the same entity in the discourse. However, al-
though we anchor the target for the first attitude to ?his
victory,? the anchor for the target of the second attitude is
the pronoun ?it.? As the direct object of the span that de-
notes the attitude ?denounced,? ?it? is more syntactically
relevant than ?his victory.?
4.4 Illustrative Examples
Figures 4.4 and 4.4 give graphical representations for the
annotations in sentences (11) and (12). With attitude
frame and target frame extensions, we are able to capture
more detail about the private states being expressed in the
text than the original core scheme presented in (Wiebe et
al., 2005).
5 Pie in the Sky Annotation
Among the items on the ?Pie in the Sky? Check List
of Desirable Semantic Information for Annotation, 1 the
most closely related are epistemic values (?attitude??),
epistemic, deontic, and personal attitudes. These all
fundamentally involve a self (Banfield, 1982), a subject
of consciousness who is the source of knowledge as-
sessments, judgments of certainty, judgments of obliga-
tion/permission, personal attitudes, and so on. Any ex-
plicit epistemic, deontic, or personal attitude expressions
are represented by us as private state frames, either direct
subjective frames (e.g., for verbs such as ?know? refer-
ring to an epistemic state) or expressive subjective ele-
ment frames (e.g., for modals such as ?must? or ?ought
to?). Importantly, many deontic, epistemic, and personal
attitude expressions do not directly express the speaker
or writer?s subjectivity, but are attributed by the speaker
or writer to agents mentioned in the text (consider, e.g.,
?John believes that Mary should quit her job?). Our frame
and nested-source representations were designed to sup-
port attributing subjectivity to appropriate sources. In fu-
ture work, additional attributes could be added to private
state frames to distinguish between, for example, deontic
and epistemic usages of ?must? and to represent different
epistemic values.
Other phenomena on the list overlap with subjectivity,
such as modality and social style/register. As mentioned
above, some modal expressions are subjective, such as
those expressing deontic or epistemic judgments. How-
ever, hypotheticals and future expressions need not be
subjective. For example, ?The company announced that
if its profits decrease in the next quarter, it will lay off
some employees? may easily be interpreted as presenting
objective fact. As for style, some are subjective by their
nature. One is the literary style represented thought, used
to present consciousness in fiction (Cohn, 1978; Banfield,
1982). Others are sarcastic or dismissive styles of speak-
ing or writing. In our annotation scheme, sentences per-
ceived to represent a character?s consciousness are repre-
sented with private-state frames, as are expressions per-
ceived to be sarcastic or dismissive. On the other hand,
some style distinctions, such as degree of formality, are
often realized in other ways than with explicit subjective
expressions (e.g., ?can?t? versus ?cannot?).
Polarity, another item on the checklist, also overlaps
with subjective positive and negative attitude types. Al-
though many negative and positive polarity words are sel-
dom used outside subjective expressions (such as ?hate?
and ?love?), others often are. For example, words such
as ?addicted? and ?abandoned? are included as negative
polarity terms in the General Inquirer lexicon (General-
Inquirer, 2000), but they can easily appear in objective
1Available at: http://nlp.cs.nyu.edu/meyers/frontiers/2005.html
58
 direct subjective frame
   text anchor: think
   source: <writer, I>
   intensity: medium
   expression intensity: medium
   attitude link: a10
 attitude frame
   id: a10
   text anchor: think 
   attitude type: positive arguing
   intensity: medium
   target link: t10
 direct subjective frame
   text anchor: are happy
   source: <writer, I, people>
   intensity: medium
   expression intensity: medium
   attitude link: a20    , a30
 target frame
   id: t30 
   text anchor: Chavez
 attitude frame
   id: a20
   text anchor: are happy
   attitude type: positive attitude
   intensity: medium
   target link: t20  target frame
   id: t20
   text anchor: Chavez has fallen
 target frame
   id: t10
   text anchor: people are happy 
     because Chavez has fallen 
  
 attitude frame
   id: a30
   text anchor: are happy because 
      Chavez has fallen
   attitude type: negative attitude
   intensity: medium
   inferred: true
   target link: t30
 objective speech event
   text anchor: the entire sentence
   source: <writer>
   implicit: true
Figure 1: Graphical representation of annotations for Sentence (11)
 direct subjective frame
   text anchor: said
   source: <writer, Tsvangirai>
   intensity: high
   expression intensity: neutral
   attitude link: a40
 attitude frame
   id: a40
   text anchor: clear case of highway robbery 
   attitude type: negative attitude
   intensity: high
   target link: t40    , t45
 target frame
   id: t40
   text anchor: election result
 target frame
   id: t45
   text anchor: Mugabe, his government 
       and his party, Zanu-PF
 objective speech event
   text anchor: the entire sentence
   source: <writer>
   implicit: true
 expressive subjective element frame
   source: <writer, Tsvangirai>
   text anchor: clear case of highway robbery 
   intensity: high
   
Figure 2: Graphical representation of annotations for Sentence (12)
59
sentences (e.g., ?Thomas De Quincy was addicted to
opium and lived in an abandoned shack?).
Integrating subjectivity with other layers of annotation
proposed in the ?Pie in the Sky? project would afford the
opportunity to investigate how they interact. It would
also enrich our subjectivity representations. While our
scheme promises to be a good base, much remains to be
added. For example, annotations of thematic roles and
co-reference would add needed structure to the target an-
notations, which are now only spans of text. In addi-
tion, temporal and modal annotations would flesh out the
insubstantial attribute, which is currently only a binary
marker. Furthermore, individual private state expressions
must be integrated with respect to the discourse context.
For example, which expressions of opinions oppose ver-
sus support one another? Which sentences presented as
objective fact are included to support a subjective opin-
ion? A challenging dimension to add to the ?Pie in the
Sky? project would be the deictic center as conceived of
in (Duchan et al, 1995), which consists of here, now, and
I reference points updated as the text or conversation un-
folds. Our annotation scheme was developed with this
framework in mind.
6 Related Work
The work most similar to ours is Appraisal Theory (Mar-
tin, 2000; White, 2002) from systemic functional linguis-
tics (see Halliday (19851994)). Both Appraisal Theory
and our annotation scheme are concerned with identify-
ing and characterizing expressions of opinions and emo-
tions in context. The two schemes, however, make differ-
ent distinctions. Appraisal Theory distinguishes different
types of positive and negative attitudes and also various
types of ?intersubjective positioning? such as attribution
and expectation. Appraisal Theory does not distinguish,
as we do, the different ways that private states may be ex-
pressed (i.e., directly, or indirectly using expressive sub-
jective elements). It also does not include a representa-
tion for nested levels of attribution.
In addition to Appraisal Theory, subjectivity annota-
tion of text in context has also been performed in Yu and
Hatzivassiloglou (2003), Bruce and Wiebe (1999), and
Wiebe et al (2004). The annotations in Yu and Hatzi-
vassiloglou (2003) are sentence-level subjective vs. ob-
jective and polarity judgments. The annotation schemes
used in Bruce and Wiebe (1999) and Wiebe et al (2004)
are earlier, much less detailed versions of the annotation
scheme presented in this paper.
7 Conclusion
We have described extensions to an annotation scheme
for private states and objective speech events in lan-
guage. We look forward to integrating and elaborating
this scheme with other layers of semantic meaning in the
future.
8 Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under grant IIS-0208798 and by the Ad-
vanced Research and Development Activity (ARDA).
References
A. Banfield. 1982. Unspeakable Sentences. Routledge and
Kegan Paul, Boston.
R. Bruce and J. Wiebe. 1999. Recognizing subjectivity: A case
study of manual tagging. Natural Language Engineering,
5(2):187?205.
D. Cohn. 1978. Transparent Minds: Narrative Modes for
Representing Consciousness in Fiction. Princeton Univer-
sity Press, Princeton, NJ.
J. Duchan, G. Bruder, and L. Hewitt, editors. 1995. Deixis
in Narrative: A Cognitive Science Perspective. Lawrence
Erlbaum Associates.
The General-Inquirer. 2000.
http://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.
M.A.K. Halliday. 1985/1994. An Introduction to Functional
Grammar. London: Edward Arnold.
J.R. Martin. 2000. Beyond exchange: APPRAISAL systems
in English. In Susan Hunston and Geoff Thompson, editors,
Evaluation in Text: Authorial stance and the construction of
discourse, pages 142?175. Oxford: Oxford University Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
P.R.R. White. 2002. Appraisal: The language of attitudi-
nal evaluation and intersubjective stance. In Verschueren,
Ostman, blommaert, and Bulcaen, editors, The Handbook
of Pragmatics, pages 1?27. Amsterdam/Philadelphia: John
Benjamins Publishing Company.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004.
Learning subjective language. Computational Linguistics,
30(3):277?308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expres-
sions of opinions and emotions in language. Language Re-
sources and Evalution (formerly Computers and the Human-
ities), 1(2).
T. Wilson and J. Wiebe. 2003. Annotating opinions in the
world press. In SIGdial-03.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identifying
the polarity of opinion sentences. In EMNLP-2003.
60
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 54?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Manual Annotation of Opinion Categories in Meetings  
 
 
Swapna Somasundaran1,    Janyce Wiebe1,    Paul Hoffmann2,    Diane Litman1 
            1Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260 
                          2Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260 
{swapna,wiebe,hoffmanp,litman}@cs.pitt.edu 
 
 
  
 
Abstract 
This paper applies the categories from an 
opinion annotation scheme developed for 
monologue text to the genre of multiparty 
meetings. We describe modifications to 
the coding guidelines that were required 
to extend the categories to the new type 
of data, and present the results of an in-
ter-annotator agreement study. As re-
searchers have found with other types of 
annotations in speech data, inter-
annotator agreement is higher when the 
annotators both read and listen to the data 
than when they only read the transcripts.   
Previous work exploited prosodic clues 
to perform automatic detection of speaker 
emotion (Liscombe et al 2003). Our 
findings suggest that doing so to recog-
nize opinion categories would be a prom-
ising line of work. 
1 Introduction 
Subjectivity refers to aspects of language that 
express opinions, beliefs, evaluations and specu-
lations (Wiebe et al 2005).  Many natural lan-
guage processing applications could benefit from 
being able to distinguish between facts and opin-
ions of various types, including speech-oriented 
applications such as meeting browsers, meeting 
summarizers, and speech-oriented question an-
swering (QA) systems. Meeting browsers could 
find instances in meetings where opinions about 
key topics are expressed. Summarizers could in-
clude strong arguments for and against issues, to 
make the final outcome of the meeting more un-
derstandable.  A preliminary user survey 
(Lisowska 2003) showed that users would like to 
be able to query meeting records with subjective 
questions like ?Show me the conflicts of opin-
ions between X and Y? , ?Who made the highest 
number of positive/negative comments? and 
?Give me all the contributions of participant X in 
favor of alternative A regarding the issue I.?  A 
QA system with a component to recognize opin-
ions would be able to help find answers to such 
questions. 
Consider the following example from a meet-
ing about an investment firm choosing which car 
to buy1. (In the examples, the words and phrases 
describing or expressing the opinion are under-
lined): 
(1)2 OCK: Revenues of less 
than a million and losses of 
like five million you know 
that's pathetic 
Here, the speaker, OCK, shows his strong nega-
tive evaluation by using the expression ?That?s 
pathetic.? 
(2) OCK: No it might just be 
a piece of junk cheap piece 
of junk that's not a good 
investment 
In (2), the speaker uses the term ?just a piece of 
junk? to express his negative evaluation and uses 
this to argue for his belief that it is ?not a good 
investment.? 
(3) OCK: Yeah I think that's 
the wrong image for an in-
vestment bank he wants sta-
bility and s safety and you 
don't want flashy like zip-
                                                 
1 Throughout this paper we take examples from a meeting 
where a group of people are deciding on a new car for an 
investment bank. The management wants to attract younger 
investors with a sporty car.  
2 We have presented the examples the way they were ut-
tered by the speaker. Hence they may show many false 
starts and repetitions. Capitalization was added to improve 
readability. 
54
ping around the corner kind 
of thing you know 
The example above shows that the speaker has a 
negative judgment towards the suggestion of a 
sports car (that was made in the previous turn) 
which is indicated by the words ?wrong image.? 
The speaker then goes on to positively argue for 
what he wants. He further argues against the cur-
rent suggestion by using more negative terms 
like ?flashy? and ?zipping around the corner.? 
The speaker believes that ?zipping around the 
corner? is bad as it would give a wrong impres-
sion of the bank to the customers. In the absence 
of such analyses, the decision making process 
and rationale behind the outcomes of meetings, 
which form an important part of the organiza-
tion?s memory, might remain unavailable. 
In this paper, we perform annotation of a 
meeting corpus to lay the foundation for research 
on opinion detection in speech. We show how 
categories from an opinion (subjectivity) annota-
tion scheme, which was developed for news arti-
cles, can be applied to the genre of multi-party 
meetings. The new genre poses challenges as it is 
significantly different from the text domain, 
where opinion analysis has traditionally been 
applied. Specifically, differences arise because:  
1) There are many participants interacting with 
one another, each expressing his or her own 
opinion, and eliciting reactions in the process. 
2) Social interactions may constrain how openly 
people express their opinions; i.e., they are often 
indirect in their negative evaluations. 
We also  explore the influence of speech on hu-
man perception of opinions.  
Specifically, we annotated some meeting data 
with the opinion categories Sentiment and Argu-
ing as defined in Wilson and Wiebe (2005). In 
our annotation we first distinguish whether a 
Sentiment or Arguing is being expressed. If one 
is, we then mark the polarity (i.e., positive or 
negative) and the intensity (i.e., how strong the 
opinion is). Annotating the individual opinion 
expressions is useful in this genre, because we 
see many utterances that have more than one 
type of opinion (e.g. (3) above). To investigate 
how opinions are expressed in speech, we divide 
our annotation into two tasks, one in which the 
annotator only reads the raw text, and the other 
in which the annotator reads the raw text and also 
listens to the speech. We measure inter-annotator 
agreement for both tasks.  
We found that the opinion categories apply 
well to the multi-party meeting data, although 
there is some room for improvement: the Kappa 
values range from 0.32 to 0.69.  As has been 
found for other types of annotations in speech, 
agreement is higher when the annotators both 
read and listen to the data than when they only 
read the transcripts. Interestingly, the advantages 
are more dramatic for some categories than oth-
ers.  And, in both conditions, agreement is higher 
for the positive than for the negative categories.  
We discuss possible reasons for these disparities. 
Prosodic clues have been exploited to perform 
automatic detection of speaker emotion (Lis-
combe et al 2003).  Our findings suggest that 
doing so to recognize opinion categories is a 
promising line of work.  
The rest of the paper is organized as follows: 
In Section 2 we discuss the data and the annota-
tion scheme and present examples. We then pre-
sent our inter-annotator agreement results in Sec-
tion 3, and in Section 4 we discuss issues and 
observations. Related work is described in Sec-
tion 5. Conclusions and Future Work are pre-
sented in Section 6. 
2 Annotation  
2.1 Data 
The data is from the ISL meeting corpus (Bur-
ger et al 2002).  We chose task oriented meet-
ings from the games/scenario and discussion 
genres, as we felt they would be closest to the 
applications for which the opinion analysis will 
be useful. The ISL speech is accompanied by 
rich transcriptions, which are tagged according to 
VERBMOBIL conventions. However, since real-
time applications only have access to ASR out-
put, we gave the annotators raw text, from which 
all VERBMOBIL tags, punctuation, and capitali-
zations were removed.  
In order to see how annotations would be af-
fected by the presence or absence of speech, we 
divided each raw text document into 2 segments. 
One part was annotated while reading the raw 
text only. For the annotation of the other part, 
speech as well as the raw text was provided.   
2.2 Opinion Category Definitions  
We base our annotation definitions on the 
scheme developed by Wiebe et al (2005) for 
news articles. That scheme centers on the notion 
of subjectivity, the linguistic expression of pri-
vate states. Private states are internal mental 
states that cannot be objectively observed or veri-
fied (Quirk et al 1985) and include opinions, 
beliefs, judgments, evaluations, thoughts, and 
feelings. Amongst these many forms of subjec-
55
tivity, we focus on the Sentiment and Arguing 
categories proposed by Wilson and Wiebe 
(2005). The categories are broken down by po-
larity and defined as follows:  
Positive Sentiments: positive emotions, 
evaluations, judgments and stances. 
(4) TBC: Well ca How about 
one of the the newer Cadil-
lac the Lexus is good 
In (4), taken from the discussion of which car to 
buy, the speaker uses the term ?good? to express 
his positive evaluation of the Lexus . 
Negative Sentiments: negative emotions, 
evaluations, judgments and stances. 
(5) OCK: I think these are 
all really bad choices 
In (5), the speaker expresses his negative evalua-
tion of the choices for the company car. Note that 
?really? makes the evaluation more intense.  
Positive Arguing:  arguing for something, ar-
guing that something is true or is so, arguing that 
something did happen or will happen, etc. 
(6) ZDN: Yeah definitely 
moon roof  
In (6), the speaker is arguing that whatever car 
they get should have a moon roof. 
Negative Arguing: arguing against some-
thing, arguing that something is not true or is not 
so, arguing that something did not happen or will 
not happen, etc. 
(7) OCK: Like a Lexus or 
perhaps a Stretch Lexus 
something like that but that 
might be too a little too 
luxurious 
In the above example, the speaker is using the 
term ?a little too luxurious? to argue against a 
Lexus for the car choice.  
In an initial tagging experiment, we applied 
the above definitions, without modification, to 
some sample meeting data. The definitions cov-
ered much of the arguing and sentiment we ob-
served. However, we felt that some cases of Ar-
guing that are more prevalent in meeting than in 
news data needed to be highlighted more, namely 
Arguing opinions that are implicit or that under-
lie what is explicitly said. Thus we add the fol-
lowing to the arguing definitions. 
Positive Arguing: expressing support for or 
backing the acceptance of an object, viewpoint, 
idea or stance by providing reasoning, justifica-
tions, judgment, evaluations or beliefs. This sup-
port or backing may be explicit or implicit. 
(8) MHJ: That's That's why I 
wanna What about the the 
child safety locks I think I 
think that would be a good 
thing because if our custom-
ers happen to have children  
Example (8) is marked as both Positive Arguing 
and Positive Sentiment. The more explicit one is 
the Positive Sentiment that the locks are good. 
The underlying Argument is that the company 
car they choose should have child safety locks. 
Negative Arguing: expressing lack of support 
for or attacking the acceptance of an object, 
viewpoint, idea or stance by providing reasoning, 
justifications, judgment, evaluations or beliefs. 
This may be explicit or implicit. 
(9) OCK: Town Car But it's a 
little a It's a little like 
your grandf Yeah your grand-
father would drive that 
Example (9) is explicitly stating who would drive 
a Town Car, while implicitly arguing against 
choosing the Town Car (as they want younger 
investors). 
2.3 Annotation Guidelines 
Due to genre differences, we also needed to 
modify the annotation guidelines. For each Argu-
ing or Sentiment the annotator perceives, he or 
she identifies the words or phrases used to ex-
press it (the text span), and then creates an anno-
tation consisting of the following. 
? Opinion Category and Polarity 
? Opinion Intensity 
? Annotator Certainty 
Opinion Category and Polarity: These are 
defined in the previous sub-section. Note that the 
target of an opinion is what the opinion is about. 
For example, the target of ?John loves baseball? 
is baseball.   An opinion may or may not have a 
separate target.  For example, ?want stability? in 
?We want stability? denotes a Positive Senti-
ment, and there is no separate target.  In contrast, 
?good? in ?The Lexus is good? expresses a Posi-
tive Sentiment and there is a separate target, 
namely the Lexus. 
In addition to Sentiments toward a topic of 
discussion, we also mark Sentiments toward 
other team members (e.g. ?Man you guys 
are so limited?). We do not mark 
agreements or disagreements as Sentiments, as 
these are different dialog acts (though they some-
times co-occur with Sentiments and Arguing).  
Intensity: We use a slightly modified version 
of Craggs and Wood's (2004) emotion intensity 
56
annotation scheme. According to that scheme, 
there are 5 levels of intensity. Level ?0? denotes 
a lack of the emotion (Sentiment or Arguing in 
our case), ?1? denotes traces of emotion, ?2? de-
notes a low level of emotion, ?3? denotes a clear 
expression while ?4? denotes a strong expres-
sion. Our intensity levels mean the same, but we 
do not mark intensity level 0 as this level implies 
the absence of opinion. 
If a turn has multiple, separate expressions 
marked with the same opinion tag (category and 
polarity), and all expressions refer to the same 
target, then the annotators merge all the expres-
sions into a larger text span, including the sepa-
rating text in between the  expressions. This re-
sulting text span has the same opinion tag as its 
constituents, and it has an intensity that is greater 
than or equal to the highest intensity of the con-
stituent expressions that were merged. 
Annotator Certainty: The annotators use this 
tag if they are not sure that a given opinion is 
present, or if, given the context, there are multi-
ple possible interpretations of the utterance and 
the annotator is not sure which interpretation is 
correct. This attribute is distinct from the Inten-
sity attribute, because the Intensity attribute indi-
cates the strength of the opinion, while the Anno-
tator Certainty attribute indicates whether the 
annotator is sure about a given tag (whatever the 
intensity is). 
2.4 Examples 
We conclude this section with some examples 
of annotations from our corpus.  
(10) OCK: So Lexun had reve-
nues of a hundred and fifty 
million last year and prof-
its of like six million.  
That's pretty good 
Annotation: Text span=That's 
pretty good Cate-
gory=Positive Sentiment In-
tensity=3 Annotator Cer-
tainty=Certain  
The annotator marked the text span ?That?s 
pretty good? as Positive Sentiment because this 
this expression is used by OCK to show his fa-
vorable judgment towards the company reve-
nues. The intensity is 3, as it is a clear expression 
of Sentiment.  
(11) OCK: No it might just 
be a piece of junk Cheap 
piece of junk that?s not a 
good investment 
Annotation1: Text span=it 
might just be a piece of 
junk Cheap piece of junk 
that?s not a good investment 
Category=Negative Sentiment 
Intensity=4 Annotator Cer-
tainty=Certain 
Annotation2: Text span=Cheap 
piece of junk that?s not a 
good investment Category 
=Negative Arguing Inten-
sity=3 Annotator Certainty 
=Certain  
In the above example, there are multiple expres-
sions of opinions. In Annotation1, the expres-
sions ?it might just be a piece of junk?, ?cheap 
piece of junk? and ?not a good investment? ex-
press negative evaluations towards the car choice 
(suggested by another participant in a previous 
turn). Each of these expressions is a clear case of 
Negative Sentiment (Intensity=3). As they are all 
of the same category and polarity and towards 
the same target, they have been merged by the 
annotator into one long expression of Inten-
sity=4. In Annotation2, the sub-expression 
?cheap piece of junk that is not a good invest-
ment? is also used by the speaker OCK to argue 
against the car choice. Hence the annotator has 
marked this as Negative Arguing.  
3 Guideline Development and Inter-
Annotator Agreement 
3.1 Annotator Training 
Two annotators (both co-authors) underwent 
three rounds of tagging. After each round, dis-
crepancies were discussed, and the guidelines 
were modified to reflect the resolved ambiguities. 
A total of 1266 utterances belonging to sections 
of four meetings (two of the discussion genre and 
two of the game genre) were used in this phase. 
3.2 Agreement  
The unit for which agreement was calculated 
was the turn. The ISL transcript provides demar-
cation of speaker turns along with the speaker ID. 
If an expression is marked in a turn, the turn is 
assigned the label of that expression. If there are 
multiple expressions marked within a turn with 
different category tags, the turn is assigned all 
those categories. This does not pose a problem 
for our evaluation, as we evaluate each category 
separately. 
A previously unseen section of a meeting con-
taining 639 utterances was selected and divided 
57
into 2 segments. One part of 319 utterances was 
annotated using raw text as the only signal, and 
the remaining 320 utterances were annotated us-
ing text and speech. Cohen?s Kappa (1960) was 
used to calculate inter-annotator agreement. We 
calculated inter-annotator agreement for both 
conditions: raw-text-only and raw-text+speech. 
This was done for each of the categories: Posi-
tive Sentiment, Positive Arguing, Negative Sen-
timent, and Negative Arguing. To evaluate a 
category, we did the following:  
? For each turn, if both annotators tagged 
the turn with the given category, or both 
did not tag the turn with the category, then 
it is a match.  
? Otherwise it is a mismatch 
Table 1 shows the inter-annotator Kappa val-
ues on the test set. 
 
Agreement (Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.54 0.60 
Negative Arguing 0.32 0.65 
Positive Sentiment 0.57 0.69 
Negative Sentiment 0.41 0.61 
Table 1 Inter-annotator agreement on different 
categories. 
 
With raw-text-only annotation, the Kappa 
value is in the moderate range according to 
Landis and Koch (1977), except for Negative 
Arguing for which it is 0.32. Positive Arguing 
and Positive Sentiment were more reliably de-
tected than Negative Arguing and Negative Sen-
timent. We believe this is because participants 
were more comfortable with directly expressing 
their positive sentiments in front of other partici-
pants.  Given only the raw text data, inter-
annotator reliability measures for Negative Argu-
ing and Negative Sentiment are the lowest. We 
believe this might be due to the fact that partici-
pants in social interactions are not very forthright 
with their Negative Sentiments and Arguing. 
Negative Sentiments and Arguing towards some-
thing may be expressed by saying that something 
else is better. For example, consider the follow-
ing response of one participant to another par-
ticipant?s suggestion of aluminum wheels for the 
company car 
(12) ZDN: Yeah see what kind 
of wheels you know they have 
to look dignified to go with 
the car 
The above example was marked as Negative Ar-
guing by one annotator (i.e., they should not get 
aluminum wheels) while the other annotator did 
not mark it at all. The implied Negative Arguing 
toward getting aluminum wheels can be inferred 
from the statement that the wheels should look 
dignified. However the annotators were not sure, 
as the participant chose to focus on what is desir-
able (i.e., dignified wheels). This utterance is 
actually both a general statement of what is de-
sirable, and an implication that aluminum wheels 
are not dignified. But this may be difficult to as-
certain with the raw text signal only.  
When the annotators had speech to guide their 
judgments, the Kappa values go up significantly 
for each category. All the agreement numbers for 
raw text+speech are in the substantial range ac-
cording to Landis and Koch (1977). We observe 
that with speech, Kappa for Negative Arguing 
has doubled over the Kappa obtained without 
speech. The Kappa for Negative Sentiment 
(text+speech) shows a 1.5 times improvement 
over the one with only raw text. Both these ob-
servations indicate that speech is able to help the 
annotators tag negativity more reliably. It is quite 
likely that a seemingly neutral sentence could 
sound negative, depending on the way words are 
stressed or pauses are inserted. Comparing the 
agreement on Positive Sentiment, we get a 1.2 
times improvement by using speech. Similarly, 
agreement improves by 1.1 times for Positive 
Arguing when speech is used. The improvement 
with speech for the Positive categories is not as 
high as compared to negative categories, which 
conforms to our belief that people are more 
forthcoming about their positive judgments, 
evaluations, and beliefs.  
In order to test if the turns where annotators 
were uncertain were the places that caused mis-
match, we calculated the Kappa with the annota-
tor-uncertain cases removed. The corresponding 
Kappa values are shown in Table 2 
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.52 0.63 
Negative Arguing 0.36 0.63 
Positive Sentiment 0.60 0.73 
Negative Sentiment 0.50 0.61 
Table-2 Inter-annotator agreement on different 
categories, Annotator Uncertain cases removed. 
 
The trends observed in Table 1 are seen in Ta-
ble 2 as well, namely annotation reliability im-
proving with speech. Comparing Tables 1 and 2, 
58
we see that for the raw text, the inter-annotator 
agreement goes up by 0.04 points for Negative 
Arguing and goes up by 0.09 points for Negative 
Sentiment. However, the agreement for Negative 
Arguing and Negative Sentiment on raw-text+ 
speech between Tables 1 and 2 remains almost 
the same. We believe this is  because we had 
20% fewer Annotator Uncertainty tags in the 
raw-text+speech annotation as compared to raw-
text-only, thus indicating that some types of un-
certainties seen in raw-text-only were resolved in 
the raw-text+speech due to the speech input. The 
remaining cases of Annotator Uncertainty could 
have been due to other factors, as discussed in 
the next section 
Table 3 shows Kappa with the low intensity 
tags removed. The hypothesis was that low in-
tensity might be borderline cases, and that re-
moving these might increase inter-annotator reli-
ability.  
 
Agreement ( Kappa) Raw Text only 
Raw Text 
+ Speech 
Positive Arguing 0.53 0.66 
Negative Arguing 0.26 0.65 
Positive Sentiment 0.65 0.74 
Negative Sentiment 0.45 0.59 
Table-3 Inter-annotator agreement on different 
categories, Intensity 1, 2 removed. 
 
Comparing Tables 1 and 3 (the raw-text col-
umns), we see that there is an improvement in 
the agreement on sentiment (both positive and 
negative) if the low intensity cases are removed.  
The agreement for Negative Sentiment (raw-text) 
goes up marginally by 0.04 points.  Surprisingly, 
the agreement for Negative Arguing (raw-text) 
goes down by 0.06 points. Similarly in raw-
text+speech results, removal of low intensity 
cases does not improve the agreement for Nega-
tive Arguing while hurting Negative Sentiment 
category (by 0.02 points). One possible explana-
tion is that it may be equally difficult to detect 
Negative categories at both low and high intensi-
ties. Recall that in (12) it was difficult to detect if 
there is  Negative Arguing at all. If the annotator 
decided that it is indeed a Negative Arguing, it is 
put at intensity level=3 (i.e., a clear case). 
4 Discussion 
There were a number of interesting subjectiv-
ity related phenomena in meetings that we ob-
served during our annotation. These are issues 
that will need to be addressed for improving in-
ter-annotator reliability. 
Global and local context for arguing: In the 
context of a meeting, participants argue for (posi-
tively) or against (negatively) a topic. This may 
become ambiguous when the participant uses an 
explicit local Positive Arguing and an implicit 
global Negative Arguing. Consider the following 
speaker turn, at a point in the meeting when one 
participant has suggested that the company car 
should have a moon roof and another participant 
has opposed it, by saying that a moon roof would 
compromise the headroom. 
(13) OCK: We wanna make sure 
there's adequate headroom 
for all those six foot six 
investors 
In the above example, the speaker OCK, in the 
local context of the turn, is arguing positively 
that headroom is important. However, in the 
global context of the meeting, he is arguing 
against the idea of a moon roof that was sug-
gested by a participant. Such cases occur when 
one object (or opinion) is endorsed which auto-
matically precludes another, mutually exclusive 
object (or opinion).  
Sarcasm/Humor: The meetings we analyzed 
had a large amount of sarcasm and humor. Issues 
arose with sarcasm due to our approach of mark-
ing opinions towards the content of the meeting 
(which forms the target of the opinion). Sarcasm 
is difficult to annotate because sarcasm can be 
1) On topic: Here the target is the topic of dis-
cussion and hence sarcasm is used as a Negative 
Sentiment. 
2) Off topic: Here the target is not a topic un-
der discussion, and the aim is to purely elicit 
laughter. 
3) Allied topic: In this case, the target is re-
lated to the topic in some way, and it?s difficult 
to determine if the aim of the sarcasm/humor was 
to elicit laughter or to imply something negative 
towards the topic.  
Multiple modalities: In addition to text and 
speech, gestures and visual diagrams play an im-
portant role in some types of meetings. In one 
meeting that we analyzed, participants were 
working together to figure out how to protect an 
egg when it is dropped from a long distance, 
given the materials they have. It was evident they 
were using some gestures to describe their ideas 
(?we can put tape like this?) and that they drew 
diagrams to get points across. In the absence of 
visual input, annotators would need to guess 
59
what was happening. This might further hurt the 
inter-annotator reliability. 
5 Related Work  
Our opinion categories are from the subjectiv-
ity schemes described in Wiebe et al (2005) and 
Wilson and Wiebe (2005). Wiebe et al (2005) 
perform expression level annotation of opinions 
and subjectivity in text. They define their annota-
tions as an experiencer having some type of atti-
tude (such as Sentiment or Arguing), of a certain 
intensity, towards a target. Wilson and Wiebe 
(2005) extend this basic annotation scheme to 
include different types of subjectivity, including 
Positive Sentiment, Negative Sentiment, Positive 
Arguing, and Negative Arguing. 
Speech was found to improve inter-annotator 
agreement in discourse segmentation of mono-
logs (Hirschberg and Nakatani 1996). Acoustic 
clues have been successfully employed for the 
reliable detection of the speaker?s emotions, in-
cluding frustration, annoyance, anger, happiness, 
sadness, and boredom (Liscombe et al 2003).  
Devillers et al (2003) performed perceptual tests 
with and without speech in detecting the 
speaker?s fear, anger, satisfaction and embar-
rassment.  Though related, our work is not con-
cerned with the speaker?s emotions, but rather 
opinions toward the issues and topics addressed 
in the meeting. 
Most annotation work in multiparty conversa-
tion has focused on exchange structures and dis-
course functional units like common grounding 
(Nakatani and Traum, 1998). In common ground-
ing research, the focus is on whether the partici-
pants of the discourse are able to understand each 
other, and not their opinions towards the content 
of the discourse. Other tagging schemes like the 
one proposed by Flammia and Zue (1997) focus 
on information seeking and question answering 
exchanges where one participant is purely seek-
ing information, while the other is providing it. 
The SWBD DAMSL (Jurafsky et al, 1997) an-
notation scheme over the Switchboard telephonic 
conversation corpus labels shallow discourse 
structures. The SWBD-DAMSL had a label ?sv? 
for opinions. However, due to poor inter-
annotator agreement, the authors discarded these 
annotations. The ICSI MRDA annotation scheme 
(Rajdip et al, 2003) adopts the SWBD DAMSL 
scheme, but does not distinguish between the 
opinionated and objective statements. The ISL 
meeting corpus (Burger and Sloane, 2004) is an-
notated with dialog acts and discourse moves like 
initiation and response, which in turn consist of 
dialog tags such as query, align, and statement. 
Their statement dialog category would not only 
include Sentiment and Arguing tags discussed in 
this paper, but it would also include objective 
statements and other types of subjectivity. 
?Hot spots? in meetings closely relate to our 
work because they find sections in the meeting 
where participants are involved in debates or 
high arousal activity (Wrede and Shriberg 2003). 
While that work distinguishes between high 
arousal and low arousal, it does not distinguish 
between  opinion or non-opinion or the different 
types of opinion. However, Janin et al (2004) 
suggest that there is a relationship between dia-
log acts and involvement, and that involved ut-
terances contain significantly more evaluative 
and subjective statements as well as extremely 
positive or negative answers. Thus we believe it 
may be beneficial for such works to make these 
distinctions. 
Another closely related work that finds par-
ticipants? positions regarding issues is argument 
diagramming (Rienks et al 2005). This ap-
proach, based on the IBIS system (Kunz and Rit-
tel 1970), divides a discourse into issues, and 
finds lines of deliberated arguments. However 
they do not distinguish between subjective and 
objective contributions towards the meeting. 
6 Conclusions and Future Work 
In this paper we performed an annotation 
study of opinions in meetings, and investigated 
the effects of speech. We have shown that it is 
possible to reliably detect opinions within multi-
party conversations. Our consistently better 
agreement results with text+speech input over 
text-only input suggest that speech is a reliable 
indicator of opinions. We have also found that 
Annotator Uncertainty decreased with speech 
input. Our results also show that speech is a more 
informative indicator for negative versus positive 
categories. We hypothesize that this is due to the 
fact the people express their positive attitudes 
more explicitly. The speech signal is thus even 
more important for discerning negative opinions. 
This experience has also helped us gain insights 
to the ambiguities that arise due to sarcasm and 
humor. 
Our promising results open many new avenues 
for research. It will be interesting to see how our 
categories relate to other discourse structures, 
both at the shallow level (agree-
ment/disagreement) as well as at the deeper level 
60
(intentions/goals). It will also be interesting to 
investigate how other forms of subjectivity like 
speculation and intention are expressed in multi-
party discourse. Finding prosodic correlates of 
speech as well as lexical clues that help in opin-
ion detection would be useful in building subjec-
tivity detection applications for multiparty meet-
ings.  
References 
Susanne Burger and Zachary A Sloane. 2004. The ISL 
Meeting Corpus: Categorical Features of Commu-
nicative Group Interactions. NIST Meeting Recog-
nition Workshop 2004, NIST 2004, Montreal, Can-
ada, 2004-05-17 
Susanne Burger, Victoria MacLaren and Hua Yu. 
2002. The ISL Meeting Corpus: The Impact of 
Meeting Type on Speech Style. ICSLP-2002. Den-
ver, CO: ISCA, 9 2002. 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Meas., 20:37?46. 
Richard Craggs and Mary McGee Wood. 2004. A 
categorical annotation scheme for emotion in the 
linguistic content of dialogue. Affective Dialogue 
Systems. 2004. 
Laurence Devillers, Lori Lamel and Ioana Vasilescu. 
2003. Emotion detection in task-oriented spoken 
dialogs. IEEE International Conference on Multi-
media and Expo (ICME). 
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey and 
Elizabeth Shriberg. 2003. ?Meeting Recorder Pro-
ject: Dialog Act Labeling Guide,? ICSI Technical 
Report TR-04-002, Version 3, October 2003 
Giovanni Flammia and Victor Zue. 1997. Learning 
The Structure of Mixed Initiative Dialogues Using 
A Corpus of Annotated Conversations. Eurospeech 
1997, Rhodes, Greece 1997, p1871?1874 
Julia Hirschberg and Christine Nakatani. 1996. A Pro-
sodic Analysis of Discourse Segments in Direction-
Giving Monologues Annual Meeting- Association 
For Computational Linguistics 1996, VOL 34, 
pages 286-293 
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhil-
lon, Jane Edwards, Javier Mac??as-Guarasa, Nelson 
Morgan, Barbara Peskin, Elizabeth Shriberg, An-
dreas Stolcke, Chuck Wooters and Britta Wrede. 
2004. ?The ICSI Meeting Project: Resources and 
Research,?  ICASSP-2004 Meeting Recognition 
Workshop. Montreal; Canada: NIST, 5 2004 
Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca, 
1997. Switchboard-DAMSL Labeling Project 
Coder?s Manual. 
http://stripe.colorado.edu/?jurafsky/manual.august1 
Werner Kunz and Horst W. J. Rittel. 1970. Issues as 
elements of information systems. Working Paper 
WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen der 
Planung, 1970 
Richard Landis and Gary Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data 
Biometrics, Vol. 33, No. 1 (Mar., 1977) , pp. 159-
174 
Agnes Lisowska. 2003. Multimodal interface design 
for the multimodal meeting domain: Preliminary 
indications from a query analysis study. Technical 
Report IM2.  Technical report, ISSCO/TIM/ETI. 
Universit de Genve, Switserland, November 2003. 
Jackson Liscombe, Jennifer Venditti and Julia 
Hirschberg. 2003. Classifying Subject Ratings of 
Emotional Speech Using Acoustic Features. Eu-
rospeech 2003. 
Christine Nakatani and David Traum. 1998. Draft: 
Discourse Structure Coding Manual version 
2/27/98 
Randolph Quirk, Sidney Greenbaum, Geoffry Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New 
York.s 
Rutger Rienks, Dirk Heylen and Erik van der Wei-
jden. 2005. Argument diagramming of meeting 
conversations. In Vinciarelli, A. and Odobez, J., 
editors, Multimodal Multiparty Meeting Process-
ing, Workshop at the 7th International Conference 
on Multimodal Interfaces, pages 85?92, Trento, It-
aly 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and 
Evaluation (formerly Computers and the Humani-
ties), volume 39, issue 2-3, pp. 165-210.  
Theresa Wilson and Janyce Wiebe. 2005. Annotating 
attributions and private states. ACL Workshop on 
Frontiers in Corpus Annotation II: Pie in the Sky.  
Britta Wrede and Elizabeth Shriberg. 2003. Spotting 
"Hotspots" in Meetings: Human Judgments and 
Prosodic Cues.  Eurospeech 2003, Geneva 
 
61
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440?448,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Feature Subsumption for Opinion Analysis
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
Lexical features are key to many ap-
proaches to sentiment analysis and opin-
ion detection. A variety of representations
have been used, including single words,
multi-word Ngrams, phrases, and lexico-
syntactic patterns. In this paper, we use a
subsumption hierarchy to formally define
different types of lexical features and their
relationship to one another, both in terms
of representational coverage and perfor-
mance. We use the subsumption hierar-
chy in two ways: (1) as an analytic tool
to automatically identify complex features
that outperform simpler features, and (2)
to reduce a feature set by removing un-
necessary features. We show that reduc-
ing the feature set improves performance
on three opinion classification tasks, espe-
cially when combined with traditional fea-
ture selection.
1 Introduction
Sentiment analysis and opinion recognition are ac-
tive research areas that have many potential ap-
plications, including review mining, product rep-
utation analysis, multi-document summarization,
and multi-perspective question answering. Lexi-
cal features are key to many approaches, and a va-
riety of representations have been used, including
single words, multi-word Ngrams, phrases, and
lexico-syntactic patterns. It is common for dif-
ferent features to overlap representationally. For
example, the unigram ?happy? will match all of
the texts that the bigram ?very happy? matches.
Since both features represent a positive sentiment
and the bigram matches fewer contexts than the
unigram, it is probably sufficient just to have the
unigram. However, there are many cases where
a feature captures a subtlety or non-compositional
meaning that a simpler feature does not. For exam-
ple, ?basket case? is a highly opinionated phrase,
but the words ?basket? and ?case? individually
are not. An open question in opinion analysis is
how often more complex feature representations
are needed, and which types of features are most
valuable. Our first goal is to devise a method to
automatically identify features that are represen-
tationally subsumed by a simpler feature but that
are better opinion indicators. These subjective ex-
pressions could then be added to a subjectivity lex-
icon (Esuli and Sebastiani, 2005), and used to gain
understanding about which types of complex fea-
tures capture meaningful expressions that are im-
portant for opinion recognition.
Many opinion classifiers are created by adopt-
ing a ?kitchen sink? approach that throws together
a variety of features. But in many cases adding
new types of features does not improve perfor-
mance. For example, Pang et al (2002) found that
unigrams outperformed bigrams, and unigrams
outperformed the combination of unigrams plus
bigrams. Our second goal is to automatically iden-
tify features that are unnecessary because similar
features provide equal or better coverage and dis-
criminatory value. Our hypothesis is that a re-
duced feature set, which selectively combines un-
igrams with only the most valuable complex fea-
tures, will perform better than a larger feature set
that includes the entire ?kitchen sink? of features.
In this paper, we explore the use of a subsump-
tion hierarchy to formally define the subsump-
tion relationships between different types of tex-
tual features. We use the subsumption hierarchy
in two ways. First, we use subsumption as an an-
440
alytic tool to compare features of different com-
plexities and automatically identify complex fea-
tures that substantially outperform their simpler
counterparts. Second, we use the subsumption hi-
erarchy to reduce a feature set based on represen-
tational overlap and on performance. We conduct
experiments with three opinion data sets and show
that the reduced feature sets can improve classifi-
cation performance.
2 The Subsumption Hierarchy
2.1 Text Representations
We analyze two feature representations that have
been used for opinion analysis: Ngrams and Ex-
traction Patterns. Information extraction (IE)
patterns are lexico-syntactic patterns that rep-
resent expressions which identify role relation-
ships. For example, the pattern ?<subj>
ActVP(recommended)? extracts the subject of
active-voice instances of the verb ?recommended?
as the recommender. The pattern ?<subj>
PassVP(recommended)? extracts the subject of
passive-voice instances of ?recommended? as the
object being recommended.
(Riloff and Wiebe, 2003) explored the idea
of using extraction patterns to represent more
complex subjective expressions that have non-
compositional meanings. For example, the expres-
sion ?drive (someone) up the wall? expresses the
feeling of being annoyed, but the meanings of the
words ?drive?, ?up?, and ?wall? have no emotional
connotations individually. Furthermore, this ex-
pression is not a fixed word sequence that can be
adequately modeled by Ngrams. Any noun phrase
can appear between the words ?drive? and ?up?, so
a flexible representation is needed to capture the
general pattern ?drives <NP> up the wall?.
This example represents a general phenomenon:
many expressions allow intervening noun phrases
and/or modifying terms. For example:
?stepped on <mods> toes?
Ex: stepped on the boss? toes
?dealt <np> <mods> blow?
Ex: dealt the company a decisive blow
?brought <np> to <mods> knees?
Ex: brought the man to his knees
(Riloff and Wiebe, 2003) also showed that syn-
tactic variations of the same verb phrase can be-
have very differently. For example, they found that
passive-voice constructions of the verb ?ask? had
a 100% correlation with opinion sentences, but
active-voice constructions had only a 63% corre-
lation with opinions.
Pattern Type Example Pattern
<subj> PassVP <subj> is satisfied
<subj> ActVP <subj> complained
<subj> ActVP Dobj <subj> dealt blow
<subj> ActInfVP <subj> appear to be
<subj> PassInfVP <subj> is meant to be
<subj> AuxVP Dobj <subj> has position
<subj> AuxVP Adj <subj> is happy
ActVP <dobj> endorsed <dobj>
InfVP <dobj> to condemn <dobj>
ActInfVP <dobj> get to know <dobj>
PassInfVP <dobj> is meant to be <dobj>
Subj AuxVP <dobj> fact is <dobj>
NP Prep <np> opinion on <np>
ActVP Prep <np> agrees with <np>
PassVP Prep <np> is worried about <np>
InfVP Prep <np> to resort to <np>
<possessive> NP <noun>?s speech
Figure 1: Extraction Pattern Types
Our goal is to use the subsumption hierarchy
to identify Ngram and extraction pattern features
that are more strongly associated with opinions
than simpler features. We used three types of fea-
tures in our research: unigrams, bigrams, and IE
patterns. The Ngram features were generated us-
ing the Ngram Statistics Package (NSP) (Baner-
jee and Pedersen, 2003).1 The extraction pat-
terns (EPs) were automatically generated using
the Sundance/AutoSlog software package (Riloff
and Phillips, 2004). AutoSlog relies on the Sun-
dance shallow parser and can be applied exhaus-
tively to a text corpus to generate IE patterns that
can extract every noun phrase in the corpus. Au-
toSlog has been used to learn IE patterns for the
domains of terrorism, joint ventures, and micro-
electronics (Riloff, 1996), as well as for opinion
analysis (Riloff and Wiebe, 2003). Figure 1 shows
the 17 types of extraction patterns that AutoSlog
generates. PassVP refers to passive-voice verb
phrases (VPs), ActVP refers to active-voice VPs,
InfVP refers to infinitive VPs, and AuxVP refers
1NSP is freely available for use under the GPL from
http://search.cpan.org/dist/Text-NSP. We discarded Ngrams
that consisted entirely of stopwords. We used a list of 281
stopwords.
441
to VPs where the main verb is a form of ?to be?
or ?to have?. Subjects (subj), direct objects (dobj),
PP objects (np), and possessives can be extracted
by the patterns.2
2.2 The Subsumption Hierarchy
We created a subsumption hierarchy that defines
the representational scope of different types of fea-
tures. We will say that feature A representation-
ally subsumes feature B if the set of text spans
that match feature A is a superset of the set of text
spans that match feature B. For example, the uni-
gram ?happy? subsumes the bigram ?very happy?
because the set of text spans that match ?happy?
includes the text spans that match ?very happy?.
First, we define a hierarchy of valid subsump-
tion relationships, shown in Figure 2. The 2Gram
node, for example, is a child of the 1Gram node
because a 1Gram can subsume a 2Gram. Ngrams
may subsume extraction patterns as well. Ev-
ery extraction pattern has at least one correspond-
ing 1Gram that will subsume it.3. For example,
the 1Gram ?recommended? subsumes the pattern
?<subj> ActVP(recommended)? because the pat-
tern only matches active-voice instances of ?rec-
ommended?. An extraction pattern may also
subsume another extraction pattern. For exam-
ple, ?<subj> ActVP(recommended)? subsumes
?<subj> ActVP(recommended) Dobj(movie)?.
To compare specific features we need to for-
mally define the representation of each type of
feature in the hierarchy. For example, the hierar-
chy dictates that a 2Gram can subsume the pattern
?ActInfVP <dobj>?, but this should hold only if
the words in the bigram correspond to adjacent
words in the pattern. For example, the 2Gram ?to
fish? subsumes the pattern ?ActInfVP(like to fish)
<dobj>?. But the 2Gram ?like fish? should not
subsume it. Similarly, consider the pattern ?In-
fVP(plan) <dobj>?, which represents the infini-
tive ?to plan?. This pattern subsumes the pattern
?ActInfVP(want to plan) <dobj>?, but it should
not subsume the pattern ?ActInfVP(plan to start)?.
To ensure that different features truly subsume
each other representationally, we formally define
each type of feature based on words, sequential
2However, the items extracted by the patterns are not ac-
tually used by our opinion classifiers; only the patterns them-
selves are matched against the text.
3Because every type of extraction pattern shown in Fig-
ure 1 contains at least one word (not including the extracted
phrases, which are not used as part of our feature representa-
tion).
dependencies, and syntactic dependencies. A se-
quential dependency between words wi and wi+1
means that wi and wi+1 must be adjacent, and that
wi must precede wi+1. Figure 3 shows the formal
definition of a bigram (2Gram) node. The bigram
is defined as two words with a sequential depen-
dency indicating that they must be adjacent.
Name = 2Gram
Constituent[0] = WORD1
Constituent[1] = WORD2
Dependency = Sequential(0, 1)
Figure 3: 2Gram Definition
A syntactic dependency between words wi and
wi+1 means that wi has a specific syntactic rela-
tionship to wi+1, and wi must precede wi+1. For
example, consider the extraction pattern ?NP Prep
<np>?, in which the object of the preposition at-
taches to the NP. Figure 4 shows the definition of
this extraction pattern in the hierarchy. The pat-
tern itself contains three components: the NP, the
attaching preposition, and the object of the prepo-
sition (which is the NP that the pattern extracts).
The definition also includes two syntactic depen-
dencies: the first dependency is between the NP
and the preposition (meaning that the preposition
syntactically attaches to the NP), while the second
dependency is between the preposition and the ex-
traction (meaning that the extracted NP is the syn-
tactic object of the preposition).
Name = NP Prep <np>
Constituent[0] = NP
Constituent[1] = PREP
Constituent[2] = NP EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Figure 4: ?NP Prep <np>? Pattern Definition
Consequently, the bigram ?affair with? will not
subsume the extraction pattern ?affair with <np>?
because the bigram requires the noun and preposi-
tion to be adjacent but the pattern does not. For ex-
ample, the extraction pattern matches the text ?an
affair in his mind with Countess Olenska? but the
bigram does not. Conversely, the extraction pat-
tern does not subsume the bigram either because
the pattern requires syntactic attachment but the
bigram does not. For example, the bigram matches
442
<subj> ActVP
<subj> ActInfVP
<subj> ActVP Dobj
<subj> PassVP
<subj> PassInfVP
InfVP <dobj>
ActInfVP <dobj>
PassInfVP <dobj>
1Gram
2Gram
<possessive> NP
<subj> AuxVP AdjP
<subj> AuxVP Dobj
ActVP <dobj>
ActVP Prep <np>
NP Prep <np>
PassVP Prep <np>
Subj AuxVP <dobj>
3Gram
ActVP Prep:OF <np>
InfVP Prep <np>
NP Prep:OF <np>
PassVP Prep:OF <np>
4Gram
InfVP Prep:OF <np>
Figure 2: The Subsumption Hierarchy
the sentence ?He ended the affair with a sense of
relief?, but the extraction pattern does not.
Figure 5 shows the definition of another ex-
traction pattern, ?InfVP <dobj>?, which includes
both syntactic and sequential dependencies. This
pattern would match the text ?to protest high
taxes?. The pattern definition has three compo-
nents: the infinitive ?to?, a verb, and the direct ob-
ject of the verb (which is the NP that the pattern
extracts). The definition also shows two syntac-
tic dependencies. The first dependency indicates
that the verb syntactically attaches to the infinitive
?to?. The second dependency indicates that the ex-
tracted NP syntactically attaches to the verb (i.e.,
it is the direct object of that particular verb).
The pattern definition also includes a sequen-
tial dependency, which specifies that ?to? must be
adjacent to the verb. Strictly speaking, our parser
does not require them to be adjacent. For exam-
ple, the parser allows intervening adverbs to split
infinitives (e.g., ?to strongly protest high taxes?),
and this does happen occasionally. But split in-
finitives are relatively rare, so in the vast major-
ity of cases the infinitive ?to? will be adjacent to
the verb. Consequently, we decided that a bigram
(e.g., ?to protest?) should representationally sub-
sume this extraction pattern because the syntac-
tic flexibility afforded by the pattern is negligi-
ble. The sequential dependency link represents
this judgment call that the infinitive ?to? and the
verb are adjacent in most cases.
For all of the node definitions, we used our best
judgment to make decisions of this kind. We tried
to represent major distinctions between features,
without getting caught up in minor differences that
were likely to be negligible in practice.
Name = InfVP <dobj>
Constituent[0] = INFINITIVE TO
Constituent[1] = VERB
Constituent[2] = DOBJ EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Dependency = Sequential(0, 1)
Figure 5: ?InfVP <dobj>? Pattern Definition
To use the subsumption hierarchy, we assign
each feature to its appropriate node in the hierar-
chy based on its type. Then we perform a top-
down breadth-first traversal. Each feature is com-
pared with the features at its ancestor nodes. If
a feature?s words and dependencies are a superset
of an ancestor?s words and dependencies, then it
is subsumed by the (more general) ancestor and
discarded.4 When the subsumption process is fin-
ished, a feature remains in the hierarchy only if
4The words that they have in common must also be in the
same relative order.
443
there are no features above it that subsume it.
2.3 Performance-based Subsumption
Representational subsumption is concerned with
whether one feature is more general than another.
But the purpose of using the subsumption hier-
archy is to identify more complex features that
outperform simpler ones. Applying the subsump-
tion hierarchy to features without regard to per-
formance would simply eliminate all features that
have a more general counterpart in the feature set.
For example, all bigrams would be discarded if
their component unigrams were also present in the
hierarchy.
To estimate the quality of a feature, we use In-
formation Gain (IG) because that has been shown
to work well as a metric for feature selection (For-
man, 2003). We will say that feature A be-
haviorally subsumes feature B if two criteria are
met: (1) A representationally subsumes B, and (2)
IG(A) ? IG(B) - ?, where ? is a parameter repre-
senting an acceptable margin of performance dif-
ference. For example, if ?=0 then condition (2)
means that feature A is just as valuable as fea-
ture B because its information gain is the same or
higher. If ?>0 then feature A is allowed to be a lit-
tle worse than feature B, but within an acceptable
margin. For example, ?=.0001 means that A?s in-
formation gain may be up to .0001 lower than B?s
information gain, and that is considered to be an
acceptable performance difference (i.e., A is good
enough that we are comfortable discarding B in
favor of the more general feature A).
Note that based on the subsumption hierarchy
shown in Figure 2, all 1Grams will always sur-
vive the subsumption process because they cannot
be subsumed by any other types of features. Our
goal is to identify complex features that are worth
adding to a set of unigram features.
3 Data Sets
We used three opinion-related data sets for our
analyses and experiments: the OP data set created
by (Wiebe et al, 2004), the Polarity data set5 cre-
ated by (Pang and Lee, 2004), and the MPQA data
set created by (Wiebe et al, 2005).6 The OP and
Polarity data sets involve document-level opinion
classification, while the MPQA data set involves
5Version v2.0, which is available at:
http://www.cs.cornell.edu/people/pabo/movie-review-data/
6Available at http://www.cs.pitt.edu/mpqa/databaserelease/
sentence-level classification.
The OP data consists of 2,452 documents from
the Penn Treebank (Marcus et al, 1993). Metadata
tags assigned by the Wall Street Journal define the
opinion/non-opinion classes: the class of any doc-
ument labeled Editorial, Letter to the Editor, Arts
& Leisure Review, or Viewpoint by the Wall Street
Journal is opinion, and the class of documents in
all other categories (such as Business and News)
is non-opinion. This data set is highly skewed,
with only 9% of the documents belonging to the
opinion class. Consequently, a trivial (but useless)
opinion classifier that labels all documents as non-
opinion articles would achieve 91% accuracy.
The Polarity data consists of 700 positive and
700 negative reviews from the Internet Movie
Database (IMDb) archive. The positive and neg-
ative classes were derived from author ratings ex-
pressed in stars or numerical values. The MPQA
data consists of English language versions of ar-
ticles from the world press. It contains 9,732
sentences that have been manually annotated for
subjective expressions. The opinion/non-opinion
classes are derived from the lower-level annota-
tions: a sentence is an opinion if it contains a sub-
jective expression of medium or higher intensity;
otherwise, it is a non-opinion sentence. 55% of the
sentences belong to the opinion class.
4 Using the Subsumption Hierarchy for
Analysis
In this section, we illustrate how the subsump-
tion hierarchy can be used as an analytic tool to
automatically identify features that substantially
outperform simpler counterparts. These features
represent specialized usages and expressions that
would be good candidates for addition to a sub-
jectivity lexicon. Figure 6 shows pairs of features,
where the first is more general and the second is
more specific. These feature pairs were identified
by the subsumption hierarchy as being representa-
tionally similar but behaviorally different (so the
more specific feature was retained). The IGain
column shows the information gain values pro-
duced from the training set of one cross-validation
fold. The Class column shows the class that the
more specific feature is correlated with (the more
general feature is usually not strongly correlated
with either class).
The top table in Figure 6 contains examples for
the opinion/non-opinion classification task from
444
Opinion/Non-Opinion Classification
ID Feature IGain Class Example
A1 line .0016 - . . . issue consists of notes backed by credit line receivables
A2 the line .0075 opin ...lays it on the line; ...steps across the line
B1 nation .0046 - . . . has 750,000 cable-tv subscribers around the nation
B2 a nation .0080 opin It?s not that we are spawning a nation of ascetics . . .
C1 begin .0006 - Campeau buyers will begin writing orders...
C2 begin with .0036 opin To begin with, we should note that in contrast...
D1 benefits .0040 - . . . earlier period included $235,000 in tax benefits.
DEP NP Prep(benefits to) .0090 opin . . . boon to the rich with no proven benefits to the economy
E1 due .0001 - . . . an estimated $ 1.23 billion in debt due next spring
EEP ActVP Prep(due to) .0038 opin It?s all due to the intense scrutiny...
Positive/Negative Sentiment Classification
ID Feature IGain Class Example
F1 short .0014 - to make a long story short...
F2 nothing short .0039 pos nothing short of spectacular
G1 ugly .0008 - ...an ugly monster on a cruise liner
G2 and ugly .0054 neg it?s a disappointment to see something this dumb and ugly
H1 disaster .0010 - ...rated pg-13 for disaster related elements
HEP AuxVP Dobj(be disaster) .0048 neg . . . this is such a confused disaster of a film
I1 work .0002 - the next day during the drive to work...
IEP ActVP(work) .0062 pos the film will work just as well...
J1 manages .0003 - he still manages to find time for his wife
JEP ActInfVP(manages to keep) .0054 pos this film manages to keep up a rapid pace
Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy.
(1 ? unigram; 2 ? bigram; EP ? extraction pattern)
the OP data. The more specific features are more
strongly correlated with opinion articles. Surpris-
ingly, simply adding a determiner can dramatically
change behavior. Consider A2. There are many
subjective idioms involving ?the line? (two are
shown in the table; others include ?toe the line?
and ?draw the line?), while objective language
about credit lines, phone lines, etc. uses the deter-
miner less often. Similarly, consider B2. Adding
?a? to ?nation? often corresponds to an abstract
reference used when making an argument (e.g.,
?a nation of ascetics?), whereas other instances
of ?nation? are used more literally (e.g., ?the 6th
largest in the nation?). 21% of feature B1?s in-
stances appear in opinion articles, while 70% of
feature B2?s instances are in opinion articles.
?Begin with? (C2) captures an adverbial phrase
used in argumentation (?To begin with...?) but
does not match objective usages such as ?will
begin? an action. The word ?benets? alone
(D1) matches phrases like ?tax benets? and ?em-
ployee benets? that are not opinion expressions,
while DEP typically matches positive senses of
the word ?benets?. Interestingly, the bigram
?benets to? is not highly correlated with opin-
ions because it matches infinitive phrases such
as ?tax benets to provide? and ?health benets
to cut?. In this case, the extraction pattern ?NP
Prep(benefits to)? is more discriminating than the
bigram for opinion classification. The extraction
pattern EEP is also highly correlated with opin-
ions, while the unigram ?due? and the bigram
?due to? are not.
The bottom table in Figure 6 shows feature
pairs identified for their behavioral differences on
the Polarity data set, where the task is to distin-
guish positive reviews from negative reviews. F2
and G2 are bigrams that behave differently from
their component unigrams. The expression ?noth-
ing short (of)? is typically used to express posi-
tive sentiments, while ?nothing? and ?short? by
themselves are not. The word ?ugly? is often used
as a descriptive modifier that is not expressing
a sentiment per se, while ?and ugly? appears in
predicate adjective constructions that are express-
ing a negative sentiment. The extraction pattern
HEP is more discriminatory than H1 because it
distinguishes negative sentiments (?the lm is a
disaster!?) from plot descriptions (?the disaster
movie...?). IEP shows that active-voice usages of
?work? are strong positive indicators, while the
unigram ?work? appears in a variety of both pos-
itive and negative contexts. Finally, JEP shows
that the expression ?manages to keep? is a strong
positive indicator, while ?manages? by itelf is
much less discriminating.
445
These examples illustrate that the subsumption
hierarchy can be a powerful tool to better under-
stand the behaviors of different kinds of features,
and to identify specific features that may be desir-
able for inclusion in specialized lexical resources.
5 Using the Subsumption Hierarchy to
Reduce Feature Sets
When creating opinion classifiers, people often
throw in a variety of features and trust the ma-
chine learning algorithm to figure out how to make
the best use of them. However, we hypothesized
that classifiers may perform better if we can proac-
tively eliminate features that are not necesary be-
cause they are subsumed by other features. In this
section, we present a series of experiments to ex-
plore this hypothesis. First, we present the results
for an SVM classifier trained using different sets
of unigram, bigram, and extraction pattern fea-
tures, both before and after subsumption. Next, we
evaluate a standard feature selection approach as
an alternative to subsumption and then show that
combining subsumption with standard feature se-
lection produces the best results of all.
5.1 Classification Experiments
To see whether feature subsumption can improve
classification performance, we trained an SVM
classifier for each of the three opinion data sets.
We used the SVMlight (Joachims, 1998) package
with a linear kernel. For the Polarity and OP data
we discarded all features that have frequency < 5,
and for the MPQA data we discarded features that
have frequency < 2 because this data set is sub-
stantially smaller. All of our experimental results
are averages over 3-fold cross-validation.
First, we created 4 baseline classifiers: a 1Gram
classifier that uses only the unigram features; a
1+2Gram classifier that uses unigram and bigram
features; a 1+EP classifier that uses unigram and
extraction pattern features, and a 1+2+EP classi-
fier that uses all three types of features. Next, we
created analogous 1+2Gram, 1+EP, and 1+2+EP
classifiers but applied the subsumption hierar-
chy first to eliminate unnecessary features be-
fore training the classifier. We experimented with
three delta values for the subsumption process:
?=.0005, .001, and .002.
Figures 7, 8, and 9 show the results. The sub-
sumption process produced small but consistent
improvements on all 3 data sets. For example, Fig-
ure 8 shows the results on the OP data, where all
of the accuracy values produced after subsumption
(the rightmost 3 columns) are higher than the ac-
curacy values produced without subsumption (the
Base[line] column). For all three data sets, the best
overall accuracy (shown in boldface) was always
achieved after subsumption.
Features Base ?=.0005 ?=.001 ?=.002
1Gram 79.8
1+2Gram 81.2 81.0 81.3 81.0
1+EP 81.7 81.4 81.4 82.0
1+2+EP 81.7 82.3 82.3 82.7
Figure 7: Accuracies on Polarity Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 97.5 - - -
1+2Gram 98.0 98.7 98.6 98.7
1+EP 97.2 97.8 97.9 97.9
1+2+EP 97.8 98.6 98.7 98.7
Figure 8: Accuracies on OP Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 74.8
1+2Gram 74.3 74.9 74.6 74.8
1+EP 74.4 74.6 74.6 74.6
1+2+EP 74.4 74.9 74.7 74.6
Figure 9: Accuracies on MPQA Data
We also observed that subsumption had a dra-
matic effect on the F-measure scores on the OP
data, which are shown in Figure 10. The OP data
set is fundamentally different from the other data
sets because it is so highly skewed, with 91% of
the documents belonging to the non-opinion class.
Without subsumption, the classifier was conser-
vative about assigning documents to the opinion
class, achieving F-measure scores in the 82-88
range. After subsumption, the overall accuracy
improved but the F-measure scores increased more
dramatically. These numbers show that the sub-
sumption process produced not only a more ac-
curate classifier, but a more useful classifier that
identifies more documents as being opinion arti-
cles.
For the MPQA data, we get a very small im-
provement of 0.1% (74.8% ? 74.9%) using sub-
sumption. But note that without subsumption the
performance actually decreased when bigrams and
446
Features Base ?=.0005 ?=.001 ?=.002
1Gram 84.5
1+2Gram 88.0 92.5 92.0 92.3
1+EP 82.4 86.9 87.4 87.4
1+2+EP 86.7 91.8 92.5 92.3
Figure 10: F-measures on OP Data
 97.6
 97.8
 98
 98.2
 98.4
 98.6
 98.8
 99
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 11: Feature Selection on OP Data
extraction patterns were added! The subsumption
process counteracted the negative effect of adding
the more complex features.
5.2 Feature Selection Experiments
We conducted a second series of experiments to
determine whether a traditional feature selection
approach would produce the same, or better, im-
provements as subsumption. For each feature, we
computed its information gain (IG) and then se-
lected the N features with the highest scores.7 We
experimented with values of N ranging from 1,000
to 10,000 in increments of 1,000.
We hypothesized that applying subsumption be-
fore traditional feature selection might also help to
identify a more diverse set of high-performing fea-
tures. In a parallel set of experiments, we explored
this hypothesis by first applying subsumption to
reduce the size of the feature set, and then select-
ing the best N features using information gain.
Figures 11, 12, and 13 show the results of these
experiments for the 1+2+EP classifiers. Each
graph shows four lines. One line corresponds to
the baseline classifier with no subsumption, and
another line corresponds to the baseline classifier
with subsumption using the best ? value for that
data set. Each of these two lines corresponds to
7In the case of ties, we included all features with the same
score as the Nth-best as well.
 78
 78.5
 79
 79.5
 80
 80.5
 81
 81.5
 82
 82.5
 83
 83.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 12: Feature Selection on Polarity Data
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.0005
Feature Selection
Subsumption ?=0.0005 + Feature Selection
Figure 13: Feature Selection on MPQA Data
just a single data point (accuracy value), but we
drew that value as a line across the graph for the
sake of comparison. The other two lines on the
graph correspond to (a) feature selection for dif-
ferent values of N (shown on the x-axis), and (b)
subsumption followed by feature selection for dif-
ferent values of N.
On all 3 data sets, traditional feature selection
performs worse than the baseline in some cases,
and it virtually never outperforms the best classi-
fier trained after subsumption (but without feature
selection). Furthermore, the combination of sub-
sumption plus feature selection generally performs
best of all, and nearly always outperforms feature
selection alone. For all 3 data sets, our best ac-
curacy results were achieved by performing sub-
sumption prior to feature selection. The best accu-
racy results are 99.0% on the OP data, 83.1% on
the Polarity data, and 75.4% on the MPQA data.
For the OP data, the improvement over baseline
for both accuracy and F-measure are statistically
significant at the p < 0.05 level (paired t-test). For
the MPQA data, the improvement over baseline is
447
statistically significant at the p < 0.10 level.
6 Related Work
Many features and classification algorithms have
been explored in sentiment analysis and opinion
recognition. Lexical cues of differing complexi-
ties have been used, including single words and
Ngrams (e.g., (Mullen and Collier, 2004; Pang et
al., 2002; Turney, 2002; Yu and Hatzivassiloglou,
2003; Wiebe et al, 2004)), as well as phrases
and lexico-syntactic patterns (e.g, (Kim and Hovy,
2004; Hu and Liu, 2004; Popescu and Etzioni,
2005; Riloff and Wiebe, 2003; Whitelaw et al,
2005)). While many of these studies investigate
combinations of features and feature selection,
this is the first work that uses the notion of sub-
sumption to compare Ngrams and lexico-syntactic
patterns to identify complex features that outper-
form simpler counterparts and to reduce a com-
bined feature set to improve opinion classification.
7 Conclusions
This paper uses a subsumption hierarchy of
feature representations as (1) an analytic tool
to compare features of different complexities,
and (2) an automatic tool to remove unneces-
sary features to improve opinion classification
performance. Experiments with three opinion
data sets showed that subsumption can improve
classification accuracy, especially when combined
with feature selection.
Acknowledgments
This research was supported by NSF Grants IIS-
0208798 and IIS-0208985, the ARDA AQUAINT
Program, and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
References
S. Banerjee and T. Pedersen. 2003. The Design, Imple-
mentation, and Use of the Ngram Statistics Package.
In Proc. Fourth Int?l Conference on Intelligent Text
Processing and Computational Linguistics.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-05.
G. Forman. 2003. An Extensive Empirical Study of
Feature Selection Metrics for Text Classification. J.
Mach. Learn. Res., 3:1289?1305.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. KDD-04.
T. Joachims. 1998. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. COLING-04.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Mullen and N. Collier. 2004. Sentiment Analysis
Using Support Vector Machines with Diverse Infor-
mation Sources. In Proc. EMNLP-04.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL-04.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proc. EMNLP-02.
A-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT-EMNLP-05.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff and J. Wiebe. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proc. EMNLP-
03.
E. Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction
in Three Domains. Artificial Intelligence, 85:101?
134.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL-02.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Proc.
CIKM-05.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Mar-
tin. 2004. Learning subjective language. Computa-
tional Linguistics, 30(3):277?308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3).
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP-03.
448
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 109?116, New York City, June 2006. c?2006 Association for Computational Linguistics
Which Side are You on? Identifying Perspectives at the Document and
Sentence Levels
Wei-Hao Lin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
whlin@cs.cmu.edu
Theresa Wilson, Janyce Wiebe
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
{twilson,wiebe}@cs.pitt.edu
Alexander Hauptmann
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
alex@cs.cmu.edu
Abstract
In this paper we investigate a new problem
of identifying the perspective from which
a document is written. By perspective we
mean a point of view, for example, from
the perspective of Democrats or Repub-
licans. Can computers learn to identify
the perspective of a document? Not every
sentence is written strongly from a per-
spective. Can computers learn to identify
which sentences strongly convey a partic-
ular perspective? We develop statistical
models to capture how perspectives are
expressed at the document and sentence
levels, and evaluate the proposed mod-
els on articles about the Israeli-Palestinian
conflict. The results show that the pro-
posed models successfully learn how per-
spectives are reflected in word usage and
can identify the perspective of a document
with high accuracy.
1 Introduction
In this paper we investigate a new problem of au-
tomatically identifying the perspective from which
a document is written. By perspective we mean
a ?subjective evaluation of relative significance, a
point-of-view.?1 For example, documents about the
Palestinian-Israeli conflict may appear to be about
the same topic but reveal different perspectives:
1The American Heritage Dictionary of the English Lan-
guage, 4th ed.
(1) The inadvertent killing by Israeli forces of
Palestinian civilians ? usually in the course of
shooting at Palestinian terrorists ? is
considered no different at the moral and ethical
level than the deliberate targeting of Israeli
civilians by Palestinian suicide bombers.
(2) In the first weeks of the Intifada, for example,
Palestinian public protests and civilian
demonstrations were answered brutally by
Israel, which killed tens of unarmed protesters.
Example 1 is written from an Israeli perspective;
Example 2 is written from a Palestinian perspec-
tive. Anyone knowledgeable about the issues of
the Israeli-Palestinian conflict can easily identify the
perspectives from which the above examples were
written. However, can computers learn to identify
the perspective of a document given a training cor-
pus?
When an issue is discussed from different per-
spectives, not every sentence strongly reflects the
perspective of the author. For example, the follow-
ing sentences were written by a Palestinian and an
Israeli.
(3) The Rhodes agreements of 1949 set them as
the ceasefire lines between Israel and the Arab
states.
(4) The green line was drawn up at the Rhodes
Armistice talks in 1948-49.
Examples 3 and 4 both factually introduce the back-
ground of the issue of the ?green line? without ex-
pressing explicit perspectives. Can we develop a
109
system to automatically discriminate between sen-
tences that strongly indicate a perspective and sen-
tences that only reflect shared background informa-
tion?
A system that can automatically identify the per-
spective from which a document is written will be
a valuable tool for people analyzing huge collec-
tions of documents from different perspectives. Po-
litical analysts regularly monitor the positions that
countries take on international and domestic issues.
Media analysts frequently survey broadcast news,
newspapers, and weblogs for differing viewpoints.
Without the assistance of computers, analysts have
no choice but to read each document in order to iden-
tify those from a perspective of interest, which is ex-
tremely time-consuming. What these analysts need
is to find strong statements from different perspec-
tives and to ignore statements that reflect little or no
perspective.
In this paper we approach the problem of learning
individual perspectives in a statistical framework.
We develop statistical models to learn how perspec-
tives are reflected in word usage, and we treat the
problem of identifying perspectives as a classifica-
tion task. Although our corpus contains document-
level perspective annotations, it lacks sentence-level
annotations, creating a challenge for learning the
perspective of sentences. We propose a novel sta-
tistical model to overcome this problem. The ex-
perimental results show that the proposed statisti-
cal models can successfully identify the perspective
from which a document is written with high accu-
racy.
2 Related Work
Identifying the perspective from which a document
is written is a subtask in the growing area of au-
tomatic opinion recognition and extraction. Sub-
jective language is used to express opinions, emo-
tions, and sentiments. So far, research in automatic
opinion recognition has primarily addressed learn-
ing subjective language (Wiebe et al, 2004; Riloff
et al, 2003), identifying opinionated documents (Yu
and Hatzivassiloglou, 2003) and sentences (Yu and
Hatzivassiloglou, 2003; Riloff et al, 2003), and dis-
criminating between positive and negative language
(Pang et al, 2002; Morinaga et al, 2002; Yu and
Hatzivassiloglou, 2003; Turney and Littman, 2003;
Dave et al, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005; Wilson et al, 2005). While by its
very nature we expect much of the language that is
used when presenting a perspective or point-of-view
to be subjective, labeling a document or a sentence
as subjective is not enough to identify the perspec-
tive from which it is written. Moreover, the ideol-
ogy and beliefs authors possess are often expressed
in ways other than positive or negative language to-
ward specific targets.
Research on the automatic classification of movie
or product reviews as positive or negative (e.g.,
(Pang et al, 2002; Morinaga et al, 2002; Turney
and Littman, 2003; Nasukawa and Yi, 2003; Mullen
and Collier, 2004; Beineke et al, 2004; Hu and Liu,
2004)) is perhaps the most similar to our work. As
with review classification, we treat perspective iden-
tification as a document-level classification task, dis-
criminating, in a sense, between different types of
opinions. However, there is a key difference. A pos-
itive or negative opinion toward a particular movie
or product is fundamentally different from an overall
perspective. One?s opinion will change from movie
to movie, whereas one?s perspective can be seen as
more static, often underpinned by one?s ideology or
beliefs about the world.
There has been research in discourse analysis that
examines how different perspectives are expressed
in political discourse (van Dijk, 1988; Pan et al,
1999; Geis, 1987). Although their research may
have some similar goals, they do not take a compu-
tational approach to analyzing large collections of
documents. To the best of our knowledge, our ap-
proach to automatically identifying perspectives in
discourse is unique.
3 Corpus
Our corpus consists of articles published on the
bitterlemonswebsite2. The website is set up to
?contribute to mutual understanding [between Pales-
tinians and Israelis] through the open exchange of
ideas.?3 Every week an issue about the Israeli-
Palestinian conflict is selected for discussion (e.g.,
2http://www.bitterlemons.org
3http://www.bitterlemons.org/about/
about.html
110
?Disengagement: unilateral or coordinated??), and
a Palestinian editor and an Israeli editor each con-
tribute one article addressing the issue. In addition,
the Israeli and Palestinian editors invite one Israeli
and one Palestinian to express their views on the
issue (sometimes in the form of an interview), re-
sulting in a total of four articles in a weekly edi-
tion. We choose the bitterlemons website for
two reasons. First, each article is already labeled
as either Palestinian or Israeli by the editors, allow-
ing us to exploit existing annotations. Second, the
bitterlemons corpus enables us to test the gen-
eralizability of the proposed models in a very real-
istic setting: training on articles written by a small
number of writers (two editors) and testing on arti-
cles from a much larger group of writers (more than
200 different guests).
We collected a total of 594 articles published on
the website from late 2001 to early 2005. The dis-
tribution of documents and sentences are listed in
Table 1. We removed metadata from all articles, in-
Palestinian Israeli
Written by editors 148 149
Written by guests 149 148
Total number of documents 297 297
Average document length 740.4 816.1
Number of sentences 8963 9640
Table 1: The basic statistics of the corpus
cluding edition numbers, publication dates, topics,
titles, author names and biographic information. We
used OpenNLP Tools4 to automatically extract sen-
tence boundaries, and reduced word variants using
the Porter stemming algorithm.
We evaluated the subjectivity of each sentence us-
ing the automatic subjective sentence classifier from
(Riloff and Wiebe, 2003), and find that 65.6% of
Palestinian sentences and 66.2% of Israeli sentences
are classified as subjective. The high but almost
equivalent percentages of subjective sentences in the
two perspectives support our observation in Sec-
tion 2 that a perspective is largely expressed using
subjective language, but that the amount of subjec-
tivity in a document is not necessarily indicative of
4http://sourceforge.net/projects/
opennlp/
its perspective.
4 Statistical Modeling of Perspectives
We develop algorithms for learning perspectives us-
ing a statistical framework. Denote a training corpus
as a set of documents Wn and their perspectives la-
bels Dn, n = 1, . . . ,N , where N is the total number
of documents in the corpus. Given a new document
W? with a unknown document perspective, the per-
spective D? is calculated based on the following con-
ditional probability.
P (D?|W? , {Dn,Wn}Nn=1) (5)
We are also interested in how strongly each sen-
tence in a document conveys perspective informa-
tion. Denote the intensity of the m-th sentence of
the n-th document as a binary random variable Sm,n.
To evaluate Sm,n, how strongly a sentence reflects
a particular perspective, we calculate the following
conditional probability.
P (Sm,n|{Dn,Wn}Nn=1) (6)
4.1 Na??ve Bayes Model
We model the process of generating documents from
a particular perspective as follows:
pi ? Beta(?pi, ?pi)
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Wn ? Multinomial(Ln, ?d)
First, the parameters pi and ? are sampled once from
prior distributions for the whole corpus. Beta and
Dirichlet are chosen because they are conjugate pri-
ors for binomial and multinomial distributions, re-
spectively. We set the hyperparameters ?pi, ?pi, and
?? to one, resulting in non-informative priors. A
document perspective Dn is then sampled from a bi-
nomial distribution with the parameter pi. The value
of Dn is either d0 (Israeli) or d1 (Palestinian). Words
in the document are then sampled from a multino-
mial distribution, where Ln is the length of the doc-
ument. A graphical representation of the model is
shown in Figure 1.
111
pi ?
Dn Wn
N
Figure 1: Na??ve Bayes Model
The model described above is commonly known
as a na??ve Bayes (NB) model. NB models have
been widely used for various classification tasks,
including text categorization (Lewis, 1998). The
NB model is also a building block for the model
described later that incorporates sentence-level per-
spective information.
To predict the perspective of an unseen document
using na??ve Bayes , we calculate the posterior distri-
bution of D? in (5) by integrating out the parameters,
? ?
P (D?, pi, ?|{(Dn,Wn)}Nn=1, W? )dpid? (7)
However, the above integral is difficult to compute.
As an alternative, we use Markov Chain Monte
Carlo (MCMC) methods to obtain samples from the
posterior distribution. Details about MCMC meth-
ods can be found in Appendix A.
4.2 Latent Sentence Perspective Model
We introduce a new binary random variable, S, to
model how strongly a perspective is reflected at the
sentence level. The value of S is either s1 or s0,
where s1 indicates a sentence is written strongly
from a perspective while s0 indicates it is not. The
whole generative process is modeled as follows:
pi ? Beta(?pi, ?pi)
? ? Beta(?? , ?? )
? ? Dirichlet(??)
Dn ? Binomial(1, pi)
Sm,n ? Binomial(1, ?)
Wm,n ? Multinomial(Lm,n, ?)
The parameters pi and ? have the same semantics as
in the na??ve Bayes model. S is naturally modeled as
a binomial variable, where ? is the parameter of S.
S represents how likely it is that a sentence strongly
conveys a perspective. We call this model the La-
tent Sentence Perspective Model (LSPM) because S
is not directly observed. The graphical model repre-
sentation of LSPM is shown in Figure 2.
pi ? ?
Dn
Sm,n Wm,n
N
Mn
Figure 2: Latent Sentence Perspective Model
To use LSPM to identify the perspective of a new
document D? with unknown sentence perspectives S?,
we calculate posterior probabilities by summing out
possible combinations of sentence perspective in the
document and parameters.
? ? ?
?
Sm,n
?
S?
P (D?, Sm,n, S?, pi, ?, ?| (8)
{(Dn,Wn)}Nn=1, W? )dpid?d?
As before, we resort to MCMC methods to sample
from the posterior distributions, given in Equations
(5) and (6).
As is often encountered in mixture models, there
is an identifiability issue in LSPM. Because the val-
ues of S can be permuted without changing the like-
lihood function, the meanings of s0 and s1 are am-
biguous. In Figure 3a, four ? values are used to rep-
resent the four possible combinations of document
perspective d and sentence perspective intensity s. If
we do not impose any constraints, s1 and s0 are ex-
changeable, and we can no longer strictly interpret
s1 as indicating a strong sentence-level perspective
and s0 as indicating that a sentence carries little or
no perspective information. The other problem of
this parameterization is that any improvement from
LSPM over the na??ve Bayes model is not necessarily
112
d0
?d0,s0
s0
?d0,s1
s1
d1
?d1,s0
s0
?d0,s0
s1
(a) s0 and s1 are not identifiable
s1
?d0,s1
d0
?d1,s1
d1 ?s0
s0
(b) sharing ?d1,s0 and
?d0,s0
Figure 3: Two different parameterization of ?
due to the explicit modeling of sentence-level per-
spective. S may capture aspects of the document
collection that we never intended to model. For ex-
ample, s0 may capture the editors? writing styles and
s1 the guests? writing styles in the bitterlemons
corpus.
We solve the identifiability problem by forcing
?d1,s0 and ?d0,s0 to be identical and reducing the
number of ? parameters to three. As shown in Fig-
ure 3b, there are separate ? parameters conditioned
on the document perspective (left branch of the tree,
d0 is Israeli and d1 is Palestinian), but there is single
? parameter when S = s0 shared by both document-
level perspectives (right branch of the tree). We as-
sume that the sentences with little or no perspective
information, i.e., S = s0, are generated indepen-
dently of the perspective of a document. In other
words, sentences that are presenting common back-
ground information or introducing an issue and that
do not strongly convey any perspective should look
similar whether they are in Palestinian or Israeli doc-
uments. By forcing this constraint, we become more
confident that s0 represents sentences of little per-
spectives and s1 represents sentences of strong per-
spectives from d1 and d0 documents.
5 Experiments
5.1 Identifying Perspective at the Document
Level
We evaluate three different models for the task
of identifying perspective at the document level:
two na??ve Bayes models (NB) with different infer-
ence methods and Support Vector Machines (SVM)
(Cristianini and Shawe-Taylor, 2000). NB-B uses
full Bayesian inference and NB-M uses Maximum
a posteriori (MAP). We compare NB with SVM not
only because SVM has been very effective for clas-
sifying topical documents (Joachims, 1998), but also
to contrast generative models like NB with discrimi-
native models like SVM. For training SVM, we rep-
resent each document as a V -dimensional feature
vector, where V is the vocabulary size and each co-
ordinate is the normalized term frequency within the
document. We use a linear kernel for SVM and
search for the best parameters using grid methods.
To evaluate the statistical models, we train them
on the documents in the bitterlemons corpus
and calculate how accurately each model predicts
document perspective in ten-fold cross-validation
experiments. Table 2 reports the average classi-
fication accuracy across the the 10 folds for each
model. The accuracy of a baseline classifier, which
randomly assigns the perspective of a document as
Palestinian or Israeli, is 0.5, because there are equiv-
alent numbers of documents from the two perspec-
tives.
Model Data Set Accuracy Reduction
Baseline 0.5
SVM Editors 0.9724
NB-M Editors 0.9895 61%
NB-B Editors 0.9909 67%
SVM Guests 0.8621
NB-M Guests 0.8789 12%
NB-B Guests 0.8859 17%
Table 2: Results for Identifying Perspectives at the
Document Level
The last column of Table 2 is error reduction
relative to SVM. The results show that the na??ve
Bayes models and SVM perform surprisingly well
on both the Editors and Guests subsets of the
bitterlemons corpus. The na??ve Bayes mod-
els perform slightly better than SVM, possibly be-
cause generative models (i.e., na??ve Bayes models)
achieve optimal performance with a smaller num-
ber of training examples than discriminative models
(i.e., SVM) (Ng and Jordan, 2002), and the size of
the bitterlemons corpus is indeed small. NB-B,
which performs full Bayesian inference, improves
113
on NB-M, which only performs point estimation.
The results suggest that the choice of words made
by the authors, either consciously or subconsciously,
reflects much of their political perspectives. Statis-
tical models can capture word usage well and can
identify the perspective of documents with high ac-
curacy.
Given the performance gap between Editors and
Guests, one may argue that there exist distinct edit-
ing artifacts or writing styles of the editors and
guests, and that the statistical models are capturing
these things rather than ?perspectives.? To test if the
statistical models truly are learning perspectives, we
conduct experiments in which the training and test-
ing data are mismatched, i.e., from different subsets
of the corpus. If what the SVM and na??ve Bayes
models learn are writing styles or editing artifacts,
the classification performance under the mismatched
conditions will be considerably degraded.
Model Training Testing Accuracy
Baseline 0.5
SVM Guests Editors 0.8822
NB-M Guests Editors 0.9327 43%
NB-B Guests Editors 0.9346 44%
SVM Editors Guests 0.8148
NB-M Editors Guests 0.8485 18%
NB-B Editors Guests 0.8585 24%
Table 3: Identifying Document-Level Perspectives
with Different Training and Testing Sets
The results on the mismatched training and test-
ing experiments are shown in Table 3. Both SVM
and the two variants of na??ve Bayes perform well
on the different combinations of training and testing
data. As in Table 2, the na??ve Bayes models per-
form better than SVM with larger error reductions,
and NB-B slightly outperforms NB-M. The high ac-
curacy on the mismatched experiments suggests that
statistical models are not learning writing styles or
editing artifacts. This reaffirms that document per-
spective is reflected in the words that are chosen by
the writers.
We list the most frequent words (excluding stop-
words) learned by the the NB-M model in Ta-
ble 4. The frequent words overlap greatly be-
tween the Palestinian and Israeli perspectives, in-
cluding ?state,? ?peace,? ?process,? ?secure? (?se-
curity?), and ?govern? (?government?). This is in
contrast to what we expect from topical text classi-
fication (e.g., ?Sports? vs. ?Politics?), in which fre-
quent words seldom overlap. Authors from differ-
ent perspectives often choose words from a simi-
lar vocabulary but emphasize them differently. For
example, in documents that are written from the
Palestinian perspective, the word ?palestinian? is
mentioned more frequently than the word ?israel.?
It is, however, the reverse for documents that are
written from the Israeli perspective. Perspectives
are also expressed in how frequently certain people
(?sharon? v.s. ?arafat?), countries (?international?
v.s. ?america?), and actions (?occupation? v.s. ?set-
tle?) are mentioned. While one might solicit these
contrasting word pairs from domain experts, our re-
sults show that statistical models such as SVM and
na??ve Bayes can automatically acquire them.
5.2 Identifying Perspectives at the Sentence
Level
In addition to identifying the perspective of a docu-
ment, we are interested in knowing which sentences
of the document strongly conveys perspective in-
formation. Sentence-level perspective annotations
do not exist in the bitterlemons corpus, which
makes estimating parameters for the proposed La-
tent Sentence Perspective Model (LSPM) difficult.
The posterior probability that a sentence strongly
covey a perspective (Example (6)) is of the most in-
terest, but we can not directly evaluate this model
without gold standard annotations. As an alterna-
tive, we evaluate how accurately LSPM predicts the
perspective of a document, again using 10-fold cross
validation. Although LSPM predicts the perspec-
tive of both documents and sentences, we will doubt
the quality of the sentence-level predictions if the
document-level predictions are incorrect.
The experimental results are shown in Table 5.
We include the results for the na??ve Bayes models
from Table 3 for easy comparison. The accuracy of
LSPM is comparable or even slightly better than that
of the na??ve Bayes models. This is very encouraging
and suggests that the proposed LSPM closely cap-
tures how perspectives are reflected at both the doc-
ument and sentence levels. Examples 1 and 2 from
the introduction were predicted by LSPM as likely to
114
Palestinian palestinian, israel, state, politics, peace, international, people, settle, occupation, sharon,
right, govern, two, secure, end, conflict, process, side, negotiate
Israeli israel, palestinian, state, settle, sharon, peace, arafat, arab, politics, two, process, secure,
conflict, lead, america, agree, right, gaza, govern
Table 4: The top twenty most frequent stems learned by the NB-M model, sorted by P (w|d)
Model Training Testing Accuracy
Baseline 0.5
NB-M Guests Editors 0.9327
NB-B Guests Editors 0.9346
LSPM Guests Editors 0.9493
NB-M Editors Guests 0.8485
NB-B Editors Guests 0.8585
LSPM Editors Guests 0.8699
Table 5: Results for Perspective Identification at the
Document and Sentence Levels
contain strong perspectives, i.e., large Pr(S? = s1).
Examples 3 and 4 from the introduction were pre-
dicted by LSPM as likely to contain little or no per-
spective information, i.e., high Pr(S? = s0).
The comparable performance between the na??ve
Bayes models and LSPM is in fact surprising. We
can train a na??ve Bayes model directly on the sen-
tences and attempt to classify a sentence as reflect-
ing either a Palestinian or Israeli perspective. A sen-
tence is correctly classified if the predicted perspec-
tive for the sentence is the same as the perspective
of the document from which it was extracted. Us-
ing this model, we obtain a classification accuracy of
only 0.7529, which is much lower than the accuracy
previously achieved at the document level. Identify-
ing perspectives at the sentence level is thus more
difficult than identifying perspectives at the docu-
ment level. The high accuracy at the document level
shows that LSPM is very effective in pooling evi-
dence from sentences that individually contain little
perspective information.
6 Conclusions
In this paper we study a new problem of learning to
identify the perspective from which a text is written
at the document and sentence levels. We show that
much of a document?s perspective is expressed in
word usage, and statistical learning algorithms such
as SVM and na??ve Bayes models can successfully
uncover the word patterns that reflect author per-
spective with high accuracy. In addition, we develop
a novel statistical model to estimate how strongly
a sentence conveys perspective, in the absence of
sentence-level annotations. By introducing latent
variables and sharing parameters, the Latent Sen-
tence Perspective Model is shown to capture well
how perspectives are reflected at the document and
sentence levels. The small but positive improvement
due to sentence-level modeling in LSPM is encour-
aging. In the future, we plan to investigate how con-
sistently LSPM sentence-level predictions are with
human annotations.
Acknowledgment
This material is based on work supported by
the Advanced Research and Development Activity
(ARDA) under contract number NBCHC040037.
A Gibbs Samplers
Based the model specification described in Sec-
tion 4.2 we derive the Gibbs samplers (Chen et al,
2000) for the Latent Sentence Perspective Model as
follows,
pi(t+1) ? Beta(?pi +
N
?
n=1
dn + d?(t+1),
?pi + N ?
N
?
n=1
dn + 1 ? d?(t+1))
? (t+1) ? Beta(?? +
N
?
n=1
Mn
?
m=1
sm,n +
M?
?
m=1
s?m,
?? +
N
?
n=1
Mn ?
N
?
n=1
Mn
?
m=1
sm,n + M? ?
M?
?
m=1
s?m)
115
?(t+1) ? Dirichlet(?? +
N
?
n=1
Mn
?
m=1
wm,n)
Pr(S(t+1)n,m = s1) ? P (Wm,n|Sm,n = 1, ?(t))
Pr(S(t+1)m,n = 1|?,Dn)
Pr(D?(t+1) = d1) ?
M?
?
m=1
dbinom(? (t+1)d )
M?
?
m=1
dmultinom(?d,m?(t))dbinom(pi(t))
where dbinom and dmultinom are the density func-
tions of binomial and multinomial distributions, re-
spectively. The superscript t indicates that a sample
is from the t-th iteration. We run three chains and
collect 5000 samples. The first half of burn-in sam-
ples are discarded.
References
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor:
Improving review classification via human-provided
information. In Proceedings of ACL-2004.
Ming-Hui Chen, Qi-Man Shao, and Joseph G. Ibrahim.
2000. Monte Carlo Methods in Bayesian Computa-
tion. Springer-Verlag.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW-2003.
Michael L. Geis. 1987. The Language of Politics.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD-2004.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML-1998.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-1998.
S. Morinaga, K. Yamanishi, K. Tateishi, and
T. Fukushima. 2002. Mining product reputations on
the web. In Proceedings of KDD-2002.
Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In Proceedings of EMNLP-2004.
T. Nasukawa and J. Yi. 2003. Sentiment analysis: Cap-
turing favorability using natural language processing.
In Proceedings of K-CAP 2003.
Andrew Y. Ng and Michael Jordan. 2002. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. In NIPS-2002, vol-
ume 15.
Zhongdang Pan, Chin-Chuan Lee, Joseph Man Chen, and
Clement Y.K. So. 1999. One event, three stories: Me-
dia narratives of the handover of hong kong in cultural
china. Gazette, 61(2):99?112.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP-2005, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of CoNLL-2003.
Peter Turney and Michael L. Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM TOIS, 21(4):315?346.
T.A. van Dijk. 1988. News as Discourse. Lawrence
Erlbaum, Hillsdale, NJ.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003.
116
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Discourse Level Opinion Relations: An Annotation Study
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Josef Ruppenhofer
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
josefr@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work proposes opinion frames as a repre-
sentation of discourse-level associations that
arise from related opinion targets and which
are common in task-oriented meeting dialogs.
We define the opinion frames and explain their
interpretation. Additionally we present an
annotation scheme that realizes the opinion
frames and via human annotation studies, we
show that these can be reliably identified.
1 Introduction
There has been a great deal of research in recent
years on opinions and subjectivity. Opinions have
been investigated at the phrase, sentence, and docu-
ment levels. However, little work has been carried
out at the level of discourse.
Consider the following excerpt from a dialog
about designing a remote control for a television (the
opinion targets - what the opinions are about - are
shown in italics).
(1) D:: And I thought not too edgy and like a box, more
kind of hand-held not as computery, yeah, more or-
ganic shape I think. Simple designs, like the last one
we just saw, not too many buttons . . .
Speaker D expresses an opinion in favor of a de-
sign that is simple and organic in shape, and against
an alternative design which is not. Several individ-
ual opinions are expressed in this passage. The first
is a negative opinion about the design being too edgy
and box-like, the next is a positive opinion toward
a hand-held design, followed by a negative opin-
ion toward a computery shape, and so on. While
we believe that recognizing individual expressions
of opinions, their properties, and components is im-
portant, we believe that discourse interpretation is
needed as well. It is by understanding the passage
as a discourse that we see edgy, like a box, com-
putery, and many buttons as descriptions of the type
of design D does not prefer, and hand-held, organic
shape, and simple designs as descriptions of the type
he does. These descriptions are not in general syn-
onyms/antonyms of one another; for example, there
are hand-held ?computery? devices and simple de-
signs that are edgy. The unison/opposition among
the descriptions is due to how they are used in the
discourse.
This paper focuses on such relations between the
targets of opinions in discourse. Specifically, we
propose opinion frames, which consist of two opin-
ions which are related by virtue of having united
or opposed targets. We believe that recognizing
opinion frames will provide more information for
NLP applications than recognizing their individual
components alone. Further, if there is uncertainty
about any one of the components, we believe opin-
ion frames are an effective representation incorpo-
rating discourse information to make an overall co-
herent interpretation (Hobbs, 1979; Hobbs, 1983).
To our knowledge, this is the first work to ex-
tend a manual annotation scheme to relate opinions
in the discourse. In this paper, we present opin-
ion frames, and motivate their usefulness through
examples. Then we provide an annotation scheme
for capturing these opinion frames. Finally we per-
form fine-grained annotation studies to measure the
human reliability in recognizing of these opinion
frames.
129
Opinion frames are presented in Section 2, our an-
notation scheme is described in Section 3, the inter-
annotator agreement studies are presented in Section
4, related work is discussed in Section 5, and conclu-
sions are in Section 6.
2 Opinion Frames
2.1 Introduction
The components of opinion frames are individual
opinions and the relationships between their targets.
We address two types of opinions, sentiment and
arguing. Following (Wilson and Wiebe, 2005; So-
masundaran et al, 2007), sentiment includes posi-
tive and negative evaluations, emotions, and judg-
ments, while arguing includes arguing for or against
something, and arguing that something should or
should not be done. In our examples, the lexical an-
chors revealing the opinion type (as the words are
interpreted in context) are indicated in bold face.
In addition, the text span capturing the target of the
opinion (again, as interpreted in context) is indicated
in italics.
(2) D:: . . . this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot. A
bit more durable and that can also be ergonomic and it
kind of feels a bit different from all the other remote
controls.
Speaker D expresses his preference for the rub-
bery material for the remote. He reiterates his opin-
ion with a number of positive evaluations like bit
more bouncy, bit more durable, ergonomic and
so on.
All opinions in this example are related to the oth-
ers via opinion frames by virtue of having the same
targets, i.e., the opinions are essentially about the
same things (the rubbery material for the remote).
For example, the opinions ergonomic and a bit dif-
ferent from all the other remote controls are re-
lated in a frame of type SPSPsame, meaning the first
opinion is a S(entiment) with polarity P(ositive); the
second also is a S(entiment) with polarity P(ositive);
and the targets of the opinions are in a same (target)
relation.
The specific target relations addressed in this pa-
per are the relations of either being the same or being
alternatives to one another. While these are not the
only possible relations, they are not infrequent, and
SPSPsame, SNSNsame, APAPsame, ANANsame,
SPAPsame, APSPsame, SNANsame, ANSNsame,
SPSNalt, SNSPalt, APANalt, ANAPalt,
SPANalt, SNAPalt, APSNalt, ANSPalt
SPSNsame, SNSPsame, APANsame, ANAPsame,
SPANsame, APSNsame, SNAPsame, ANSPsame,
SPSPalt, SNSNalt, APAPalt, ANANalt,
SPAPalt, SNANalt, APSPalt, ANSNalt
Table 1: Opinion Frames
they commonly occur in task-oriented dialogs such
as those in our data.
With four opinion type - polarity pairs (SN, SP,
AN, AP), for each of two opinion slots, and two pos-
sible target relations, we have 4 * 4 * 2 = 32 types
of frame, listed in Table 1.
In the remainder of this section, we elaborate fur-
ther on the same target relation (in 2.2) the alter-
native target relation (in 2.3) and explain a method
by which these relationships can be propagated (in
2.4). Finally, we illustrate the usefulness of opinion
frames in discourse interpretation (in 2.5).
2.2 Same Targets
Our notion of sameness for targets includes cases
of anaphora and ellipses, lexically similar items, as
well as less direct relations such as part-whole, sub-
set, inferable, and instance-class.
Looking at the opinion frames for Example 2 in
more detail, we separately list the opinions, followed
by the relations between targets.
Opinion Span - target Span Type
O1 bit more bouncy - it?s [t1] SP
O2 bit more durable - ellipsis [t2] SP
O3 ergonomic - that [t3] SP
O4 a bit different from all the other remote - it [t4] SP
Target - target Rel
t1 - t2 same
t1 - t3 same
t3 - t4 same
Ellipsis occurs with bit more durable. [t2] rep-
resents the (implicit) target of that opinion, and [t2]
has a same relation to [t1], the target of the bit more
bouncy opinion. (Note that the interpretation of the
first target, [t1], would require anaphora resolution
of its target span with a previous noun phrase, rub-
bery material.)
Let us now consider the following passage, in
which a meeting participant analyzes two leading re-
130
motes on the market.1
(3) D:: These are two leading remote controls at the mo-
ment. You know they?re grey, this one?s got loads of
buttons, it?s hard to tell from here what they actually
do, and they don?t look very exciting at all.
Opinion Span - target Span Rel
O1 leading - remote controls [t1] SP
O2 grey - they [t2] SN
O3 loads of buttons - this one [t3] SN
O4 hard to tell - they [t4] SN
O5 don?t look very exciting at all - they [t5] SN
Target - target Rel
t1 - t2 same
t2 - t3 same
t3 - t4 same
t5 - t1 same
Target [t2] is the set of two leading remotes, and [t3],
which is in a same relation with [t2], is one of those
remotes. Target [t4], which is also in a same rela-
tion with [t3], is an aspect of that remote, namely
its buttons. Thus, opinion O3 is directly about one
of the remotes, and indirectly about the set of both
remotes. Similarly, opinion O4 is directly about the
buttons of one of the remotes, and indirectly about
that remote itself.
2.3 Alternative Targets
The alt(ernative) target relation arises when multiple
choices are available, and only one can be selected.
For example, in the domain of TV remote controls,
the set of all shapes are alternatives to one another,
since a remote control may have only one shape at a
time. In such scenarios, a positive opinion regarding
one choice may imply a negative opinion toward the
rest of the choices, and vice versa.
As an example, let us now consider the follow-
ing passage (some intervening utterances have been
removed for clarity).
(4) C:: . . . shapes should be curved, so round shapes2
Nothing square-like.
C:: . . . So we shouldn?t have too square corners and
that kind of thing.
B:: Yeah okay. Not the old box look.
1In the other examples in this paper, the source (holder) of
the opinions is the speaker. The leading opinion in this example
is an exception: its source is implicit; it is a consensus opinion
that is not necessarily shared by the speaker (i.e., it is a nested
source (Wiebe et al, 2005)).
2In the context of the dialogs, the annotators read the ?so
round shapes? as a summary statement. Had the ?so? been inter-
preted as Arguing, the round shapes would have been annotated
as a target (and linked to curved).
Opinion Span - target Span Rel
O1 should be - curved [t1] AP
O2 Nothing - square-like [t2] AN
O3 shouldn?t have - square corners [t3] AN
O4 too - square corners [t3] SN
O5 Not - the old box look [t4] AN
O6 the old box look - the old box look [t4] SN
Target - target Rel
t1 -t2 alternatives
t2 - t3 same
t3 - t4 same
There is an alt relation between, for example,
[t1] and [t2]. Thus, we have an opinion frame be-
tween O1 and O2, whose type is APANalt. From
this frame, we understand that a positive opinion is
expressed toward something and a negative opinion
is expressed toward its alternative.
2.4 Link Transitivity
When individual targets are linked, they form a
chain-like structure. Due to this, a connecting path
may exist between targets that were not directly
linked by the human annotators. This path may be
traversed to create links between new pairs of tar-
gets - which in turn results in new opinion frame re-
lations. For instance, in Example 4, the frame with
direct relation is O1O2 APANalt. By following the
alt link from [t1] to [t2] and the same link from [t2]
to [t3], we have an alt link between [t1] and [t3],
and the additional frames O1O3 APANalt and O1O4
APSNalt. Repeating this process would finally link
speaker C?s opinion O1 with B?s opinion O6, yield-
ing a APSNalt frame.
2.5 Interpretation
This section illustrates two motivations for opinion
frames: they may unearth additional information
over and above the individual opinions stated in the
text, and they may contribute toward arriving at a
coherent interpretation (Hobbs, 1979; Hobbs, 1983)
of the opinions in the discourse.
Through opinion frames, opinions regarding
something not explicitly mentioned in the local con-
text and not even lexically related can become rel-
evant, providing more information about someone?s
opinions. This is particularly interesting when alt
relations are involved, as opinions towards one al-
ternative imply opinions of opposite polarity toward
the remaining options. For instance in Example 4
131
above, if we consider only the explicitly stated opin-
ions, there is only one (positive) opinion about the
curved shape, namely O1. However, the speaker ex-
presses several other opinions which reinforce his
positivity toward the curved shape. These are in
fact opinion frames in which the other opinion has
the opposite polarity as O1 and the target relation is
alt (for example frames such as O1O3 APANalt and
O1O4 APSNalt).
In the dialog, notice that speaker B agrees with
C and exhibits his own reinforcing opinions. These
would be similarly linked via targets resulting in
frames like O1O6 APSNalt.
Turning to our second point, arriving at a coher-
ent interpretation obviously involves disambigua-
tion. Suppose that some aspect of an individual
opinion, such as polarity, is unclear. If the discourse
suggests certain opinion frames, this may in turn re-
solve the underlying ambiguity. For instance in Ex-
ample 2, we see that out of context, the polarities of
bouncy and different from other remotes are un-
clear (bounciness and being different may be neg-
ative attributes for another type of object). How-
ever, the polarities of two of the opinions are clear
(durable and ergonomic). There is evidence in this
passage of discourse continuity and same relations,
such as the pronouns, the lack of contrastive cue
phrases, and so on. This evidence suggests that the
speaker expresses similar opinions throughout the
passage, making the opinion frame SPSPsame more
likely throughout. Recognizing the frames would re-
solve the polarity ambiguities of bouncy and differ-
ent.
Example 2 is characterized by opinion frames in
which the opinions reinforce one other. Interest-
ingly, interplays among different opinion types may
show the same type of reinforcement. As we an-
alyzed above, Example 4 is characterized by mix-
tures of opinion types, polarities, and target rela-
tions. However, the opinions are still unified in
the intention to argue for a particular type of shape.
There is evidence in this passage suggesting rein-
forcing frames: the negations are applied to targets
that are alternative to the desired option, and the pas-
sage is without contrastive discourse cues. If we
are able to recognize the best overall set of opinion
frames for the passage, the polarity ambiguities will
be resolved.
On the other hand, evidence for non-reinforcing
opinions would suggest other frames, potentially re-
sulting in different interpretations of polarity and re-
lations among targets. Such non-reinforcing associ-
ations between opinions and often occur when the
speaker is ambivalent or weighing pros and cons.
Table 1 lists the frames that occur in reinforcing sce-
narios in the top row, and the frames that occur in
non-reinforcing scenarios in the bottom row.
3 Annotation Scheme
Our annotation scheme began with the definition
and basics of the opinion annotation from previ-
ous work (Wilson and Wiebe, 2005; Somasundaran
et al, 2007). We then add to it the attributes and
components that are necessary to make an Opinion
Frame.
First, the text span that reveals the opinion expres-
sion is identified. Then, the text spans corresponding
to the targets are marked, if there exist any (we also
allow span-less targets). Then, the type and polar-
ity of the opinion in the context of the discourse is
marked. Finally the targets that are related (again
in the context of the discourse) are linked. Specif-
ically, the components that form the Annotation of
the frame are as follows:
Opinion Span: This is a span of text that reveals
the opinion.
Type: This attribute specifies the opinion type as ei-
ther Arguing or Sentiment.
Polarity: This attribute identifies the valence of an
opinion and can be one of: positive, negative,
neutral, both, unknown.
Target Span: This is a span of text that captures
what an opinion is about. This can be a propo-
sition or an entity.
Target Link: This is an attribute of a target and
records all the targets in the discourse that the
target is related to.
Link Type: The link between two targets is speci-
fied by this attribute as either same or alterna-
tive.
132
In addition to these definitions, our annotation man-
ual has guidelines detailing how to deal with gram-
matical issues, disfluencies, etc. Appendix A illus-
trates how this annotation scheme is applied to the
utterances of Example 4.
Links between targets can be followed in either
direction to construct chains. In this work, we
consider target relations to be commutative, i.e.,
Link(t1,t2) => Link(t2,t1). When a newly anno-
tated target is similar (or opposed) to a set of tar-
gets already participating in same relations, then the
same (or alt) link is made only to one of them - the
one that looks most natural. This is often the one
that is closest.
4 Annotation Studies
Construction of an opinion frame is a stepwise pro-
cess where first the text spans revealing the opinions
and their targets are selected, the opinion text spans
are classified by type and polarity and finally the
targets are linked via one of the possible relations.
We split our annotation process into these 3 intuitive
stages and use an evaluation that is most applicable
for the task at that stage.
Two annotators (both co-authors on the paper) un-
derwent training at each stage, and the annotation
manual was revised after each round of training. In
order to prevent errors incurred at earlier stages from
affecting the evaluation of later stages, the anno-
tators produced a consensus version at the end of
each stage, and used that consensus annotation as
the starting point for the next annotation stage. In
producing these consensus files, one annotator first
annotated a document, and the other annotator re-
viewed the annotations, making changes if needed.
This prevented any discussion between the annota-
tors from influencing the tagging task of the next
stage.
In the following subsections, we first introduce
the data and then present our results for annotation
studies for each stage, ending with discussion.
4.1 Data
The data used in this work is the AMI meeting cor-
pus (Carletta et al, 2005) which contains multi-
modal recordings of group meetings. We annotated
meetings from the scenario based meetings, where
Gold Exact Lenient Subset
ANN-1 53 89 87
ANN-2 44 76 74
Table 2: Inter-Annotator agreement on Opinion Spans
four participants collaborate to design a new TV
remote control in a series of four meetings. The
meetings represent different project phases, namely
project kick-off, functional design, conceptual de-
sign, and detailed design. Each meeting has rich
transcription and segment (turn/utterance) informa-
tion for each speaker. Each utterance consists of
one or more sentences. At each agreement stage we
used approximately 250 utterances from a meeting
for evaluation. The annotators also used the audio
and video recordings in the annotation of meetings.
4.2 Opinion Spans and Target Spans
In this step, the annotators selected text spans and
labeled them as opinion or target We calculated our
agreement for text span retrieval similar to Wiebe et
al. (2005). This agreement metric corresponds to
the Precision metric in information retrieval, where
annotations from one annotator are considered the
gold standard, and the other annotator?s annotations
are evaluated against it.
Table 2 shows the inter-annotator agreement (in
percentages). For the first row, the annotations pro-
duced by Annotator-1 (ANN-1) are taken as the gold
standard and, for the second row, the annotations
from annotator-2 form the gold standard. The ?Ex-
act? column reports the agreement when two text
spans have to match exactly to be considered cor-
rect. The ?Lenient? column shows the results if
an overlap relation between the two annotators? re-
trieved spans is also considered to be a hit. Wiebe
et al (2005) use this approach to measure agree-
ment for a (somewhat) similar task of subjectivity
span retrieval in the news corpus. Our agreement
numbers for this column is comparable to theirs. Fi-
nally, the third column, ?Subset?, shows the agree-
ment for a more strict constraint, namely, that one
of the spans must be a subset of the other to be con-
sidered a match. Two opinion spans that satisfy this
relation are ensured to share all the opinion words of
the smaller span.
The numbers indicate that, while the annotators
133
Gold Exact Lenient Subset
ANN-1 54 73 71
ANN-2 54 75 74
Table 3: Inter-Annotator agreement on Target Spans
Gold Exact Lenient Subset
ANN-1 74 87 87
ANN-2 76 90 90
Table 4: Inter-Annotator agreement on Targets with Per-
fect Opinion spans
do not often retrieve the exact same span, they
reliably retrieve approximate spans. Interestingly,
the agreement numbers between Lenient and Sub-
set columns are close. This implies that, in the cases
of inexact matches, the spans retrieved by the two
annotators are still close. They agree on the opinion
words and differ mostly on the inclusion of func-
tion words (e.g. articles) and observation of syntac-
tic boundaries.
In similar fashion, Table 3 gives the inter-
annotator agreement for target span retrieval. Ad-
ditionally, Table 4 shows the inter-annotator agree-
ment for target span retrieval when opinions that do
not have an exact match are filtered out. That is, Ta-
ble 4 shows results only for targets of the opinions
on which the annotators perfectly agree. As targets
are annotated with respect to the opinions, this sec-
ond evaluation removes any effects of disagreements
in the opinion detection task. As seen in Table 4, this
improves the inter-coder agreement.
4.3 Opinion Type and Polarity
In this step, the annotators began with the consensus
opinion span and target span annotations. We hy-
pothesized that given the opinion expression, deter-
mining whether it is Arguing or Sentiment would not
be difficult. Similarly, we hypothesized that target
information would make the polarity labeling task
clearer.
As every opinion instance is tagged with a type
Type Tagging Polarity Tagging
Accuracy 97.8% 98.5%
? 0.95 0.952
Table 5: Inter-Annotator agreement on Opinion Types
and Polarity
and polarity, we use Accuracy and Cohen?s Kappa
(?) metric (Cohen, 1960). The ? metric measures
the inter-annotator agreement above chance agree-
ment. The results, in Table 5, show that ? both for
type and polarity tagging is very high. This con-
firms our hypothesis that Sentiment and Arguing can
be reliably distinguished once the opinion spans are
known. Our polarity detection task shows an im-
provement in ? over a similar polarity assignment
task by Wilson et al (2005) for the news corpus (?
of 0.72). We believe this improvement can partly be
attributed to the target information available to our
annotators.
4.4 Target Linking
As an intuitive first step in evaluating target link-
ing, we treat target links in the discourse similarly to
anaphoric chains and apply methods developed for
co-reference resolution (Passonneau, 2004) for our
evaluation. Passonneau?s method is based on Krip-
pendorf?s ? metric (Krippendorff, 2004) and allows
for partial matches between anaphoric chains. In ad-
dition to this, we evaluate links identified by both
annotators for the type (same / alternative) labeling
task with the help of the ? metric.
Passonneau (2004) reports that in her co-reference
task on spoken monologs, ? varies with the diffi-
culty of the corpus (from 0.46 to 0.74). This is true
in our case too. Table 6 shows our agreement for
the four types of meetings in the AMI corpus: the
kickoff meeting (a), the functional design (b), the
conceptual design (c) and the detailed design (d).
Of the meetings, the kickoff meeting (a) we use
has relatively clear discussions. The conceptual de-
sign meeting (c) is the toughest, as as participants
are expressing opinions about a hypothetical (desir-
able) remote. In our detailed design meeting (d),
there are two final designs being evaluated. On an-
alyzing the chains from the two annotators, we dis-
covered that one annotator had maintained two sepa-
rate chains for the two remotes as there is no explicit
linguistic indication (within the 250 utterances) that
these two are alternatives. The second annotator, on
the other hand, used the knowledge that the goal
of the meeting is to design a single TV remote to
link them as alternatives. Thus by changing just
two links in the second annotator?s file to account
for this, our ? for this meeting went up from 0.52
134
Meeting: a b c d
Target linking (?) 0.79 0.74 0.59 0.52
Relation Labeling (?) 1 1 0.91 1
Table 6: Inter-Annotator agreement on Target relation
identification
to 0.70. We plan to further explore other evalua-
tion methodologies that account for severity of dif-
ferences in linking and are more relevant for our
task. Nonetheless, the resulting numbers indicate
that there is sufficient information in the discourse
to provide for reliable linking of targets.
The high ? for the relation type identification
shows that once the presence of a link is detected,
it is not difficult to determine if the targets are simi-
lar or alternatives to each other.
4.5 Discussion
Our agreement studies help to identify the aspects of
opinion frames that are straightforward, and those
that need complex reasoning. Our results indicate
that while the labeling tasks such as opinion type,
opinion polarity and target relation type are rel-
atively reliable for humans, retrieval of opinions
spans, target spans and target links is more difficult.
A common cause of annotation disagreement is
different interpretation of the utterance, particularly
in the presence of disfluencies and restarts. For ex-
ample consider the following utterance where a par-
ticipant is evaluating the drawing of another partici-
pant on the white board.
(5) It?s a baby shark , it looks to me, . . .
One annotator interpreted this ?it looks to me? as
an arguing for the belief that it was indeed a draw-
ing of a baby shark (positive Arguing). The sec-
ond annotator on the other hand looked at it as a
neutral viewpoint/evaluation (Sentiment) being ex-
pressed regarding the drawing. Thus even though
both annotators felt an opinion is being expressed,
they differed on its type and polarity.
There are some opinions that are inherently on the
borderline of Sentiment and Arguing. For example,
consider the following utterance where there is an
appeal to importance:
(6) Also important for you all is um the production cost
must be maximal twelve Euro and fifty cents.
Here, ?also important? might be taken as an assess-
ment of the high value of adhering to the budget (rel-
ative to other constraints), or simply as an argument
for adhering to the budget.
One potential source of problems to the target-
linking process consists of cases where the same
item becomes involved in more than one opposition.
For instance, in the example below, speaker D ini-
tially sets up an alternative between speech recog-
nition and buttons as a possible interface for navi-
gation. But later, speaker A re-frames the choice as
between having speech recognition only and having
both options. Connecting up all references to speech
recognition as a target respects the co-reference but
it also results in incorrect conclusions: the speech
recognition is an alternative to having both speech
recognition and buttons.
(7) A:: One thing is interesting is talking about speech
recognition in a remote control...
D:: ... So that we don?t need any button on the remote
control it would be all based on speech.
A:: ... I think that would not work so well. You wanna
have both options.
5 Related Work
Evidence from the surrounding context has been
used previously to determine if the current sentence
should be subjective/objective (Riloff et al, 2003;
Pang and Lee, 2004)) and adjacency pair informa-
tion has been used to predict congressional votes
(Thomas et al, 2006). However, these methods do
not explicitly model the relations between opinions.
Additionally, in our scheme opinions that are not
in the immediate context may be allowed to influ-
ence the interpretation of a given opinion via target
chains.
Polanyi and Zaenen (2006), in their discussion on
contextual valence shifters, have also observed the
phenomena described in this work - namely that a
central topic may be divided into subtopics in order
to perform evaluations, and that discourse structure
can influence the overall interpretation of valence.
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations with
a local aspect (or target) model to make a more in-
formed overall decision for sentiment classification.
The contrastive cue indicates a change in the senti-
ment polarity. In our scheme, their aspects would
be related as same and their high contrast relations
would result in frames such as SPSNsame, SNSP-
same. Additionally, our frame relations would link
sentiments across non-adjacent clauses, and make
connections via alt target relations.
135
Considering the discourse relation annotations in
the PDTB (Prasad et al, 2006), there can be align-
ment between discourse relations (like contrast) and
our opinion frames when the frames represent dom-
inant relations between two clauses. However, when
the relation between opinions is not the most promi-
nent one between two clauses, the discourse relation
may not align with the opinion frames. And when an
opinion frame is between two opinions in the same
clause, there would be no discourse relation counter-
part at all. Further, opinion frames assume particular
intentions that are not necessary for the establish-
ment of ostensibly similar discourse relations. For
example, we may not impose an opinion frame even
if there are contrastive cues. (Please refer to Ap-
pendix B for examples)
With regard to meetings, the most closely re-
lated work includes the dialog-related annotation
schemes for various available corpora of conversa-
tion (Dhillon et al (2003) for ICSI MRDA; Car-
letta et al (2005) for AMI ) As shown by Soma-
sundaran et al (2007), dialog structure information
and opinions are in fact complementary. We believe
that, like discourse relations, dialog information will
additionally help in arriving at an overall coherent
interpretation.
6 Conclusion and Future work
This is the first work that extends an opinion annota-
tion scheme to relate opinions via target relations.
We first introduced the idea of opinion frames as
a representation capturing discourse level relations
that arise from related opinion targets and which are
common in task-oriented dialogs such as our data.
We built an annotation scheme that would capture
these relationships. Finally, we performed extensive
inter-annotator agreement studies in order to find the
reliability of human judgment in recognizing frame
components. Our results and analysis provide in-
sights into the complexities involved in recognizing
discourse level relations between opinions.
Acknowledgments
This research was supported in part by the
Department of Homeland Security under grant
N000140710152.
References
J. Carletta, S. Ashby, and et al 2005. The AMI Meetings
Corpus. In Proceedings of Measuring Behavior Sym-
posium on ?Annotating and measuring Meeting Be-
havior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg. 2003.
Meeting recorder project: Dialog act labeling guide.
Technical report, ICSI Tech Report TR-04-002.
J. Hobbs. 1979. Coherence and coreference. Cognitive
Science, 3:67?90.
J. Hobbs, 1983. Why is Discourse Coherent?, pages 29?
70. Buske Verlag.
K. Krippendorff. 2004. Content Analysis: An Introduc-
tion to Its Methodology, 2nd Edition. Sage Publica-
tions, Thousand Oaks, California.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
R. J. Passonneau. 2004. Computing reliability for coref-
erence annotation. In LREC.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters, chapter 1. Computing Attitude and Affect in
Text: Theory and Applications. Springer.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, and B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. In Workshop on Sentiment and Subjectivity
in Text. ACL.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning sub-
jective nouns using extraction pattern bootstrapping.
In CoNLL 2003.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In SIG-
dial Workshop on Discourse and Dialogue 2007.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In EMNLP 2006.
J. Wiebe, T. Wilson, and C Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, pages 164?210.
T. Wilson and J. Wiebe. 2005. Annotating attributions
and private states. In Proceedings of ACL Workshop
on Frontiers in Corpus Annotation II: Pie in the Sky.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT-EMNLP 2005.
136
A Annotation Example
C:: . . . shapes should be curved, so round shapes. Nothing
square-like.
C:: . . . So we shouldn?t have too square corners and that kind
of thing.
B:: Yeah okay. Not the old box look.
Span Attributes
O1 should be type=Arguing; Polarity=pos; target=t1
t1 curved Link,type=(t2,alt)
O2 Nothing type=Arguing; Polarity=neg; target=t2
t2 square-like Link,type=(t1,alt),(t3,same)
O3 shouldn?t have type=Arguing; Polarity=neg; target=t3
O4 too type=Sentiment; Polarity=neg; target=t3
t3 square corners Link,type=(t2,same),(t4,same)
O5 Not type=Arguing; Polarity=neg; target=t4
t4 the old box look Link,type=(t3,same)
O6 the old box look type=Sentiment; Polarity=neg; target=t4
B Comparison between Opinion Frames
and Discourse Relations
Opinion frames can align with discourse relations
between clauses only when the frames represent the
dominant relation between two clauses (1); but not
when the opinions occur in the same clause (2); or
when the relation between opinions is not the most
prominent (3); or when two distinct targets are nei-
ther same nor alternatives (4).
(1) Non-reinforcing opinion frame (SNSP-
same); Contrast discourse relation
D :: And so what I have found and after a lot
of work actually I draw for you this schema
that can be maybe too technical for you but
is very important for me you know.
(2) Reinforcing opinion frame (SPSPsame); no
discourse relation
Thirty four percent said it takes too long
to learn to use a remote control, they want
something that?s easier to use straight away,
more intuitive perhaps.
(3) Reinforcing opinion frame (SPSPsame);
Reason discourse relation
She even likes my manga, actually the quote
is: ?I like it, because you like it, honey.?
(source: web)
(4) Unrelated opinions; Contrast discourse re-
lation
A :: Yeah, what I have to say about means.
The smart board is okay. Digital pen is hor-
rible. I dunno if you use it. But if you want
to download it to your computer, it?s doesn?t
work. No.
137
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 66?74,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Opinion Graphs for Polarity and Discourse Classification
?
Swapna Somasundaran
Univ. of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Galileo Namata
Univ. of Maryland
College Park, MD 20742
namatag@cs.umd.edu
Lise Getoor
Univ. of Maryland
College Park, MD 20742
getoor@cs.umd.edu
Janyce Wiebe
Univ. of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work shows how to construct
discourse-level opinion graphs to perform
a joint interpretation of opinions and dis-
course relations. Specifically, our opinion
graphs enable us to factor in discourse in-
formation for polarity classification, and
polarity information for discourse-link
classification. This inter-dependent frame-
work can be used to augment and im-
prove the performance of local polarity
and discourse-link classifiers.
1 Introduction
Much research in opinion analysis has focused on
information from words, phrases and semantic ori-
entation lexicons to perform sentiment classifica-
tion. While these are vital for opinion analysis,
they do not capture discourse-level associations
that arise from relations between opinions. To cap-
ture this information, we propose discourse-level
opinion graphs for classifying opinion polarity.
In order to build our computational model, we
combine a linguistic scheme opinion frames (So-
masundaran et al, 2008) with a collective classifi-
cation framework (Bilgic et al, 2007). According
to this scheme, two opinions are related in the dis-
course when their targets (what they are about) are
related. Further, these pair-wise discourse-level
relations between opinions are either reinforcing
or non-reinforcing frames. Reinforcing frames
capture reinforcing discourse scenarios where the
individual opinions reinforce one another, con-
tributing to the same opinion polarity or stance.
Non-reinforcing frames, on the other hand, cap-
ture discourse scenarios where the individual opin-
ions do not support the same stance. The indi-
vidual opinion polarities and the type of relation
?
This research was supported in part by the Department
of Homeland Security under grant N000140710152.
between their targets determine whether the dis-
course frame is reinforcing or non-reinforcing.
Our polarity classifier begins with information
from opinion lexicons to perform polarity classifi-
cation locally at each node. It then uses discourse-
level links, provided by the opinion frames, to
transmit the polarity information between nodes.
Thus the opinion classification of a node is not
just dependent on its local features, but also on the
class labels of related opinions and the nature of
these links. We design two discourse-level link
classifiers: the target-link classifier, which deter-
mines if a given node pair has unrelated targets (no
link), or if their targets have a same or alternative
relation, and the frame-link classifier, which deter-
mines if a given node pair has no link, reinforcing
or non-reinforcing link relation. Both these classi-
fiers too first start with local classifiers that use lo-
cal information. The opinion graph then provides
a means to factor in the related opinion informa-
tion into the link classifiers. Our approach enables
using the information in the nodes (and links) to
establish or remove links in the graph. Thus in-
formation flows to and fro between all the opinion
nodes and discourse-level links to achieve a joint
inference.
The paper is organized as follows: We first de-
scribe opinion graphs, a structure that can capture
discourse-level opinion relationships in Section 2,
and then describe our joint interpretation approach
to opinion analysis in Section 3. Next, we describe
our algorithm for joint interpretation in Section 4.
Our experimental results are reported in Section 5.
We discuss related work in Section 6 and conclude
in Section 7.
2 Discourse-Level Opinion Graphs
The pairwise relationships that compose opinion
frames can be used to construct a graph over opin-
ion expressions in a discourse, which we refer
to as the discourse-level opinion graph (DLOG).
66
Figure 1 Opinion Frame Annotations.
In this section, we describe these graphs and il-
lustrate their applicability to goal-oriented multi-
party conversations.
The nodes in the DLOG represent opinions, and
there are two kinds of links: target links and frame
links. Each opinion node has a polarity (positive,
negative or neutral) and type (sentiment or argu-
ing). Sentiment opinions are evaluations, feelings
or judgments about the target. Arguing opinions
argue for or against something. Target links are
labeled as either same or alternatives. Same links
hold between targets that refer to the same en-
tity or proposition, while alternative links hold be-
tween targets that are related by virtue of being op-
posing (mutually exclusive) options in the context
of the discourse. The frame links correspond to
the opinion frame relation between opinions.
We illustrate the construction of the opinion
graph with an example (Example 1, from Soma-
sundaran et al (2008)) from a multi-party meet-
ing corpus where participants discuss and design a
new TV remote control. The opinion expressions
are in bold and their targets are in italics. Notice
here that speaker D has a positive sentiment to-
wards the rubbery material for the TV remote.
(1) D:: ... this kind of rubbery material, it?s a bit more
bouncy, like you said they get chucked around a lot.
A bit more durable and that can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
All the individual opinions in this example are
essentially regarding the same thing ? the rub-
bery material. The speaker?s positive sentiment is
apparent from the text spans bit more bouncy,
bit more durable, ergonomic and a bit different
from all the other remote controls. The explicit
targets of these opinions (it?s, that, and it) and the
implicit target of ?a bit more durable? are thus all
linked with same relations.
Figure 1 illustrates the individual opinion anno-
tations, target annotations (shown in italics) and
the relations between the targets (shown in dotted
lines). Note that the target of a bit more durable
is a zero span ellipsis that refers back to the rub-
bery material. The opinion frames resulting from
the individual annotations make pairwise connec-
tions between opinion instances, as shown in bold
lines in the figure. For example, the two opinions
bit more bouncy and ergonomic, and the same
link between their targets (it?s and that), make up
an opinion frame. An opinion frame type is de-
rived from the details (type and polarity) of the
opinions it relates and the target relation involved.
Even though the different combinations of opin-
ion type (sentiment and arguing), polarity (posi-
tive and negative) and target links (same and al-
ternative) result in many distinct frames types (32
in total), they can be grouped, according to their
discourse-level characteristics, into the two cat-
egories reinforcing and non-reinforcing. In this
work, we only make this category distinction for
opinion frames and the corresponding frame links.
The next example (Example 2, also from So-
masundaran et al (2008)) illustrates an alterna-
tive target relation. In the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. In this passage, speaker C?s positive stance
towards the curved shape is brought out even more
strongly with his negative opinions toward the al-
ternative, square-like, shapes.
(2) C:: . . . shapes should be curved, so round shapes.
Nothing square-like.
.
.
.
C:: . . . So we shouldn?t have too square corners
and that kind of thing.
The reinforcing frames characteristically show
a reinforcement of an opinion or stance in the dis-
course. Both the examples presented above depict
a reinforcing scenario. In the first example, the
opinion towards the rubbery material is reinforced
by repeated positive sentiments towards it, while
in the second example the positive stance towards
the curved shapes is further reinforced by nega-
tive opinions toward the alternative option. Ex-
amples of non-reinforcing scenarios are ambiva-
lence between alternative options (for e.g., ?I like
the rubbery material but the plastic will be much
67
cheaper?) or mixed opinions about the same tar-
get (for e.g., weighing pros and cons ?The rubbery
material is good but it will be just so expensive?).
3 Interdependent Interpretation
Our interdependent interpretation in DLOGs is
motivated by the observation that, when two opin-
ions are related, a clear knowledge of the polarity
of one of them makes interpreting the other much
easier. For instance, suppose an opinion classi-
fier wants to find the polarity of all the opinion
expressions in Example 1. As a first step, it can
look up opinion lexicons to infer that words like
?bouncy?, ?durable? and ? ergonomic? are pos-
itive. However, ?a bit different ? cannot be re-
solved via this method, as its polarity can be dif-
ferent in different scenarios.
Suppose now we relate the targets of opinions.
There are clues in the passage that the targets are
related via the same relation; for instance they
are all third person pronouns occurring in adja-
cent clauses and sentences. Once we relate the
targets, the opinions of the passage are related via
target links in the discourse opinion graph. We
are also able to establish frames using the opinion
information and target link information wherever
they are available, i.e., a reinforcing link between
bit more bouncy and ergonomic. For the places
where all the information is not available (between
ergonomic and a bit different) there are multiple
possibilities. Depending on the polarity, either a
reinforcing frame (if a bit different has positive
polarity) or a non-reinforcing frame (if a bit dif-
ferent has negative polarity) can exist. There are
clues in the discourse that this passage represents
a reinforcing scenario. For instance there are rein-
forcing frames between the first few opinions, the
repeated use of ?and? indicates a list, conjunction
or expansion relation between clauses (according
to the Penn Discourse TreeBank (PDTB) (Prasad
et al, 2008)), and there is a lack of contrastive
clues that would indicate a change in the opin-
ion. Thus the reinforcing frame link emerges as
being the most likely candidate. This in turn dis-
ambiguates the polarity of a bit different. Thus,
by establishing target links and frame links be-
tween the opinion instances, we are able to per-
form a joint interpretation of the opinions.
The interdependent framework of this example
is iterative and dynamic ? the information in the
nodes can be used to change the structure (i.e.,
establish new links), and the structure provides a
framework to change node polarity. We build our
classification framework and feature sets with re-
spect to this general framework, where the node
labels as well as the structure of the graph are pre-
dicted in a joint manner.
Thus our interdependent interpretation frame-
work has three main units: an instance polarity
classifier (IPC), a target-link classifier (TLC), and
a frame-link classifier (FLC). IPC classifies each
node (instance), which may be a sentence, utter-
ance or an other text span, as positive, negative
or neutral. The TLC determines if a given node
pair has related targets and whether they are linked
by a same or alternative relation. The FLC deter-
mines if a given node pair is related via frames,
and whether it is a reinforcing or non-reinforcing
link. As we saw in the example, there are local
clues available for each unit to arrive at its classi-
fication. The discourse augments this information
to aid in further disambiguation.
4 Collective Classification Framework
For our collective classification framework, we
use a variant of the iterative classification al-
gorithm (ICA) proposed by Bilgic et al(2007).
It combines several common prediction tasks in
graphs: object classification (predicting the label
of an object) and link prediction (predicting the
existence and class of a link between objects).
For our tasks, object classification directly corre-
sponds to predicting opinion polarity and the link
prediction corresponds to predicting the existence
of a same or alternative target link or a reinforc-
ing or non-reinforcing frame link between opin-
ions. We note that given the nature of our problem
formulation and approach, we use the terms link
prediction and link classification interchangeably.
In the collective classification framework, there
are two sets of features to use. The first are local
features which can be generated for each object or
link, independent of the links in which they par-
ticipate, or the objects they connect. For example,
the opinion instance may contain words that oc-
cur in sentiment lexicons. The local features are
described in Section 4.2. The second set of fea-
tures, the relational features, reflect neighborhood
information in the graph. For frame link classifi-
cation, for example, there is a feature indicating
whether the connected nodes are predicted to have
the same polarity. The relational features are de-
68
scribed in Section 4.3.
4.1 DLOG-ICA Algorithm
Our variant of the ICA algorithm begins by pre-
dicting the opinion polarity, and link type using
only the local features. We then randomly order
the set of all opinions and links and, in turn, pre-
dict the polarity or class using the local features
and the values of the currently predicted relational
features based on previous predictions. We repeat
this until some stopping criterion is met. For our
experiments, we use a fixed number of 30 itera-
tions which was sufficient, in most of our datasets,
for ICA to converge to a solution. The pseudocode
for the algorithm is shown in Algorithm 4.1.
Algorithm 1 DLOG-ICA Algorithm
for each opinion o do {bootstrapping}
Compute polarity for o using local attributes
end for
for each target link t do {bootstrapping}
Compute label for t using local attributes
end for
for each frame link f do {bootstrapping}
Compute label for f using local attributes
end for
repeat {iterative classification}
Generate ordering I over all nodes and links
for each i in I do
if i is an opinion instance then
Compute polarity for i using local and
relational attributes
else if i is a target link then
Compute class for i using local and re-
lational attributes
else if i is a frame link then
Compute class for i using local and re-
lational attributes
end if
end for
until Stopping criterion is met
The algorithm is one very simple way of making
classifications that are interdependent. Once the
local and relational features are defined, a variety
of classifiers can be used. For our experiments, we
use SVMs. Additional details are provided in the
experiments section.
4.2 Local Features
For the local polarity classifier, we employ opin-
ion lexicons, dialog information, and unigram fea-
Feature Task
Time difference between the node pair TLC, FLC
Number of intervening instances TLC, FLC
Content word overlap between the node pair TLC,FLC
Focus space overlap between the node pair TLC, FLC
Bigram overlap between the node pair * TLC, FLC
Are both nodes from same speaker * TLC, FLC
Bag of words for each node TLC, FLC
Anaphoric indicator in the second node TLC
Adjacency pair between the node pair FLC
Discourse relation between node pair * FLC
Table 1: Features and the classification task it is used for;
TLC = target-link classification, FLC = Frame-link classifi-
cation
tures. We use lexicons that have been success-
fully used in previous work (the polarity lexicon
from (Wilson et al, 2005) and the arguing lexi-
con (Somasundaran et al, 2007)). Previous work
used features based on parse trees, e.g., (Wilson et
al., 2005; Kanayama and Nasukawa, 2006), but
our data has very different characteristics from
monologic texts ? the utterances and sentences are
much shorter, and there are frequent disfluencies,
restarts, hedging and repetitions. Because of this,
we cannot rely on parsing features. On the other
hand, in this data, we have dialog act information
1
(Dialog Acts), which we can exploit. Note that the
IPC uses only the Dialog Act tags (instance level
tags like Inform, Suggest) and not the dialog struc-
ture information.
Opinion frame detection between sentences has
been previously attempted (Somasundaran et al,
2008) by using features that capture discourse
and dialog continuity. Even though our link
classification tasks are not directly comparable
(the previous work performs binary classifica-
tion of frame-present/frame-absent between opin-
ion bearing sentences, while this work performs
three-way classification: no-link/reinforcing/non-
reinforcing between DA pairs), we adapt the fea-
tures for the link classification tasks addressed
here. These features depend on properties of the
nodes that the link connects. We also create some
new features that capture discourse relations and
lexical overlap.
Table 1 lists the link classification features.
New features are indicated with a ?*?. Continu-
ous discourse indicators, like time difference be-
tween the node pair and number of intervening
instances are useful for determining if the two
nodes can be related. The content word over-
1
Manual annotations for Dialog act tags and adjacency
pairs are available for the AMI corpus.
69
lap, and focus space overlap features (the focus
space for an instance is a list of the most recently
used NP chunks; i.e., NP chunks in that instance
and a few previous instances) capture the overlap
in topicality within the node pair; while the bi-
gram overlap feature captures the alignment be-
tween instances in terms of function words as well
as content words. The entity-level relations are
captured by the anaphoric indicator feature that
checks for the presence of pronouns such as it and
that in the second node in the node pair. The adja-
cency pair and discourse relation are actually fea-
ture sets that indicate specific dialog-structure and
discourse-level relations. We group the list of dis-
course relations from the PDTB into the following
sets: expansion, contingency, alternative, tempo-
ral, comparison. Each discourse relation in PDTB
is associated with a list of discourse connective
words.
2
Given a node pair, if the first word of the
later instance (or the last word first instance) is a
discourse connective word, then we assume that
this node is connecting back (or forward) in the
discourse and the feature set to which the connec-
tive belongs is set to true (e.g., if a latter instance
is ?because we should ...?, it starts with the con-
nective ?because?, and connects backwards via a
contingency relation). The adjacency pair feature
indicates the presence of a particular dialog struc-
ture (e.g., support, positive-assessment) between
the nodes.
4.3 Relational Features
In addition to the local features, we introduce re-
lational features (Table 2) that incorporate related
class information as well as transfer label informa-
tion between classifiers. As we saw in our example
in Figure 1, we need to know not only the polar-
ity of the related opinions, but also the type of the
relation between them. For example, if the frame
relation between ergonomic and a bit different is
non-reinforcing, then the polarity of a bit differ-
ent is likely to be negative. Thus link labels play
an important role in disambiguating the polarity.
Accordingly, our relational features transfer infor-
mation of class labels from other instances of the
same classifier as well as between different clas-
sifiers. Table 2 lists our relational features. Each
row represents a set of features. Features are gen-
erated for all combinations of x, y and z for each
2
The PDTB provides a list of discourse connectives and
the list of discourse relations each connective signifies.
row. For example, one of the features in the first
row is Number of neighbors with polarity type pos-
itive, that are related via a reinforcing frame link.
Thus each feature for the polarity classifier iden-
tifies neighbors for a given node via a specific re-
lation (z or y) and factors in their polarity values.
Similarly, both link classifiers use polarity infor-
mation of the node pair, and other link relations
involving the nodes of the pair.
5 Evaluation
We experimentally test our hypothesis that
discourse-level information is useful and non-
redundant with local information. We also wanted
to test how the DLOG performs for varying
amounts of available annotations: from full neigh-
borhood information to absolutely no neighbor-
hood information.
Accordingly, for polarity classification, we im-
plemented three scenarios: ICA-LinkNeigh, ICA-
LinkOnly and ICA-noInfo. The ICA-LinkNeigh
scenario measures the performance of the DLOG
under ideal conditions (full neighborhood infor-
mation) ? the structure of the graph (link infor-
mation) as well as the neighbors? class are pro-
vided (by an oracle). Here we do not need the
TLC, or the FLC to predict links and the Instance
Polarity Classifier (IPC) is not dependent on its
predictions from the previous iteration. On the
other hand, the ICA-noInfo scenario is the other
extreme, and has absolutely no neighborhood in-
formation. Each node does not know which nodes
in the network it is connected to apriori, and also
has no information about the polarity of any other
node in the network. Here, the structure of the
graph, as well as the node classes, have to be in-
ferred via the collective classification framework
described in Sections 3 and 4. The ICA-LinkOnly
is an intermediate condition, and is representative
of scenarios where the discourse relationships be-
tween nodes is known. Here we start with the link
information (from an oracle) and the IPC uses the
collective classification framework to infer neigh-
bor polarity information.
Similarly, we vary the amounts of neighbor-
hood information for the TLC and FLC classifiers.
In the ICA-LinkNeigh condition, TLC and FLC
have full neighborhood information. In the ICA-
noInfo condition, TLC and FLC are fully depen-
dent on the classifications of the previous rounds.
In the ICA-Partial condition, the TLC classifier
70
Feature
Opinion Polarity Classification
Number of neighbors with polarity type x linked via frame link z
Number of neighbors with polarity type x linked via target link y
Number of neighbors with polarity type x and same speaker linked via frame link z
Number of neighbors with polarity type x and same speaker linked via target link y
Target Link Classification
Polarity of the DA nodes
Number of other target links y involving the given DA nodes
Number of other target links y involving the given DA nodes and other same-speaker nodes
Presence of a frame link z between the nodes
Frame Link Classification
Polarity of the DA nodes
Number of other frame links z involving the given DA nodes
Number of other frame links z involving the given DA nodes and other same-speaker nodes
Presence of a target link y between the nodes
Table 2: Relational features: x ? {non-neutral (i.e., positive or negative), positive, negative}, y ? {same, alt}, z ?
{reinforcing, non-reinforcing}
uses true frame-links and polarity information,
and previous-stage classifications for information
about neighborhood target links; the FLC classi-
fier uses true target-links and polarity information,
and previous-stage classifications for information
about neighborhood frame-links.
5.1 Data
For our experiments, we use the opinion frame
annotations from previous work (Somasundaran
et al, 2008). These annotations consist of the
opinion spans that reveal opinions, their targets,
the polarity information for opinions, the labeled
links between the targets and the frame links be-
tween the opinions. The annotated data consists
of 7 scenario-based, multi-party meetings from the
AMI meeting corpus (Carletta et al, 2005). The
manual Dialog Act (DA) annotations, provided by
AMI, segment the meeting transcription into sep-
arate dialog acts. We use these DAs as nodes or
instances in our opinion graph.
A DA is assigned the opinion orientation of the
words it contains (for example, if a DA contains a
positive opinion expression, then the DA assigned
the positive opinion category). We filter out very
small DAs (DAs with fewer than 3 tokens, punctu-
ation included) in order to alleviate data skewness
problem in the link classifiers. This gives us a to-
tal of 4606 DA instances, of which 1935 (42%)
have opinions. Out of these 1935, 61.7% are posi-
tive, 30% are negative and the rest are neutral. The
DAs that do not have opinions are considered neu-
tral, and have no links in the DLOG. We create
DA pairs by first ordering the DAs by their start
time, and then pairing a DA with five DAs before
it, and five DAs after it. The classes for target-
link classification are no-link, same, alt. The gold
standard target-link class is decided for a DA pair
based on the target link between the targets of the
opinions contained in that pair. Similarly, the la-
bels for the frame-link labeling task are no-link,
reinforcing, non-reinforcing. The gold standard
frame link class is decided for a DA pair based on
the frame between opinions contained by that pair.
In our data, of the 4606 DAs, 1118 (24.27%) par-
ticipate in target links with other DAs, and 1056
(22.9%) form frame links. The gold standard data
for links, which has pair-wise information, has a
total of 22,925 DA pairs, of which 1371 (6%) pairs
have target links and 1264 (5.5%) pairs have frame
links.
We perform 7-fold cross-validation experi-
ments, using the 7 meetings. In each fold, 6 meet-
ings are used for training and one meeting is used
for testing.
5.2 Classifiers
Our baseline (Base) classifies the test data based
on the distribution of the classes in the training
data. Note that due to the heavily skewed nature of
our link data, this classifier performs very poorly
for minority class prediction, even though it may
achieve good overall accuracy.
For our local classifiers, we used the classifiers
from the Weka toolkit (Witten and Frank, 2002).
For opinion polarity, we used the Weka?s SVM
implementation. For the target link and frame link
classes, the huge class skew caused SVM to learn a
trivial model and always predict the majority class.
To address this, we used a cost sensitive classifier
in Weka where we set the cost of misclassifying a
less frequent class, A, to a more frequent class, B,
71
Base Local ICA
LinkNeigh LinkOnly noInfo
Acc 45.9 68.7 78.8 72.9 68.4
Class: neutral (majority class)
Prec 61.2 76.3 83.9 78.2 73.5
Rec 61.5 83.9 89.6 89.1 86.6
F1 61.1 79.6 86.6 83.2 79.3
Class: positive polarity
Prec 26.3 56.2 70.9 63.3 57.6
Rec 26.1 46.6 62.0 47.0 42.8
F1 25.8 50.4 65.9 53.5 48.5
Class: negative polarity
Prec 12.4 52.3 64.6 56.3 55.2
Rec 12.2 44.3 60.2 48.2 38.2
F1 12.2 46.0 61.9 51.2 43.9
Table 3: Performance of Polarity Classifiers
as |B|/|A| where |class| is the size of the class in
the training set. All other misclassification costs
are set to 1.
For our collective classification, we use the
above classifiers for local features (l) and use sim-
ilar, separate classifiers for relational features (r).
For example, we learned an SVM for predicting
opinion polarity using only the local features and
learned another SVM using only relational fea-
tures. For the ICA-noInfo condition, where we
use TLC and FLC classifiers, we combine the
predictions using a weighted combination where
P (class|l, r) = ? ? P (class|l) + (1 ? ?) ?
P (class|r). This allows us to vary the influence
each feature set has to the overall prediction. The
results for ICA-noInfo are reported on the best per-
forming ? (0.7).
5.3 Results
Our polarity classification results are presented
in Table 3, specifically accuracy (Acc), precision
(Prec), recall (Rec) and F-measure (F1). As we
can see, the results are mixed. First, we no-
tice that the Local classifier shows substantial im-
provement over the baseline classifier. This shows
that the lexical and dialog features we use are in-
formative of opinion polarity in multi-party meet-
ings.
Next, notice that the ICA-LinkNeigh classifier
performs substantially better than the Local clas-
sifier for all metrics and all classes. The accuracy
improves by 10 percentage points, while the F-
measure improves by about 15 percentage points
for the minority (positive and negative) classes.
This result confirms that our discourse-level opin-
ion graphs are useful and discourse-level informa-
tion is non-redundant with lexical and dialog-act
Base Local ICA
LinkNeigh Partial noInfo
TLC
Acc 88.5 85.8 98.1 98.2 86.3
P-M 33.3 35.9 76.1 76.1 36.3
R-M 33.3 38.1 78.1 78.1 38.1
F1-M 33.1 36.0 74.6 74.6 36.5
FLC
Acc 89.3 86.2 98.9 98.9 87.6
P-M 33.3 36.9 81.3 82.8 38.0
R-M 33.4 41.2 82.2 84.4 41.7
F1-M 33.1 37.2 80.7 82.3 38.1
Table 4: Performance of Link Classifiers
information.
The results for ICA-LinkOnly follow the same
trend as for ICA-LinkNeigh, with a 3 to 5 percent-
age point improvement. These results show that
even when the neighbors? classes are not known
a priori, joint inference using discourse-level rela-
tions helps reduce errors from local classification.
However, the performance of the ICA-noInfo
system, which is given absolutely no starting in-
formation, is comparable to the Local classifier for
the overall accuracy and F-measure metrics for the
neutral class. There is slight improvement in pre-
cision for both the positive and negative classes,
but there is a drop in their recall. The reason this
classifier does no better than the Local classifier is
because the link classifiers TLC and FLC predict
?none? predominantly due to the heavy class skew.
The performance of the link classifiers are re-
ported in Table 4, specifically the accuracy (Acc)
and macro averages over all classes for preci-
sion (P-M), recall (R-M) and F-measure (F1-M).
Due to the heavy skew in the data, accuracy
of all classifiers is high; however, the macro F-
measure, which depends on the F1 of the minor-
ity classes, is poor for the ICA-noInfo. Note,
however, that when we provide some (Partial) or
full (LinkNeigh) neighborhood information for the
Link classifiers, the performance of these classi-
fiers improve considerably. This overall observed
trend is similar to that observed with the polarity
classifiers.
6 Related Work
Previous work on polarity disambiguation has
used contextual clues and reversal words (Wil-
son et al, 2005; Kennedy and Inkpen, 2006;
Kanayama and Nasukawa, 2006; Devitt and Ah-
mad, 2007; Sadamitsu et al, 2008). However,
these do not capture discourse-level relations.
72
Polanyi and Zaenen (2006) observe that a cen-
tral topic may be divided into subtopics in or-
der to perform evaluations. Similar to Somasun-
daran et al (2008), Asher et al (2008) advo-
cate a discourse-level analysis in order to get a
deeper understanding of contextual polarity and
the strength of opinions. However, these works do
not provide an implementation for their insights.
In this work we demonstrate a concrete way that
discourse-level interpretation can improve recog-
nition of individual opinions and their polarities.
Graph-based approaches for joint inference in
sentiment analysis have been explored previously
by many researchers. The biggest difference be-
tween this work and theirs is in what the links
represent linguistically. Some of these are not
related to discourse at all (e.g., lexical similari-
ties (Takamura et al, 2007), morphosyntactic sim-
ilarities (Popescu and Etzioni, 2005) and word
based measures like TF-IDF (Goldberg and Zhu,
2006)). Some of these work on sentence cohesion
(Pang and Lee, 2004) or agreement/disagreement
between speakers (Thomas et al, 2006; Bansal
et al, 2008). Our model is not based on sen-
tence cohesion or structural adjacency. The re-
lations due to the opinion frames are based on
relationships between targets and discourse-level
functions of opinions being mutually reinforcing
or non-reinforcing. Adjacent instances need not be
related via opinion frames, while long distant rela-
tions can be present if opinion targets are same or
alternatives. Also, previous efforts in graph-based
joint inference in opinion analysis has been text-
based, while our work is over multi-party conver-
sations.
McDonald et al (2007) propose a joint model
for sentiment classification based on relations de-
fined by granularity (sentence and document).
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations
with a local aspect (topic) model. Their aspects
would be related as same and their high contrast
relations would correspond to (a subset of) the
non-reinforcing frames.
In the field of product review mining, senti-
ments and features (aspects or targets) have been
mined (for example, Yi et al (2003), Popescu and
Etzioni (2005), and Hu and Liu (2006)). More re-
cently there has been work on creating joint mod-
els of topic and sentiments (Mei et al, 2007; Titov
and McDonald, 2008) to improve topic-sentiment
summaries. We do not model topics; instead we
directly model the relations between targets. The
focus of our work is to jointly model opinion po-
larities via target relations. The task of finding co-
referent opinion topics by (Stoyanov and Cardie,
2008) is similar to our target link classification
task, and we use somewhat similar features. Even
though their genre is different, we plan to experi-
ment with their full feature set for improving our
TLC system.
Turning to collective classification, there have
been various collective classification frameworks
proposed (for example, Neville and Jensen (2000),
Lu and Getoor (2003), Taskar et al (2004),
Richardson and Domingos (2006)). In this pa-
per, we use an approach proposed by (Bilgic et
al., 2007) which iteratively predicts class and link
existence using local classifiers. Other joint mod-
els used in sentiment classification include the spin
model (Takamura et al, 2007), relaxation labeling
(Popescu and Etzioni, 2005), and label propaga-
tion (Goldberg and Zhu, 2006).
7 Conclusion
This work uses an opinion graph framework,
DLOG, to create an interdependent classifica-
tion of polarity and discourse relations. We em-
ployed this graph to augment lexicon-based meth-
ods to improve polarity classification. We found
that polarity classification in multi-party conver-
sations benefits from opinion lexicons, unigram
and dialog-act information. We found that the
DLOGs are valuable for further improving polar-
ity classification, even with partial neighborhood
information. Our experiments showed three to
five percentage points improvement in F-measure
with link information, and 15 percentage point
improvement with full neighborhood information.
These results show that lexical and discourse in-
formation are non-redundant for polarity classi-
fication, and our DLOG, that employs both, im-
proves performance.
We discovered that link classification is a dif-
ficult problem. Here again, we found that by us-
ing the DLOG framework, and using even partial
neighborhood information, improvements can be
achieved.
References
N. Asher, F. Benamara, and Y. Mathieu. 2008. Dis-
tilling opinion in discourse: A preliminary study.
73
COLING-2008.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. In COLING-
2008.
M. Bilgic, G. M. Namata, and L. Getoor. 2007. Com-
bining collective classification and link prediction.
In Workshop on Mining Graphs and Complex Struc-
tures at the IEEE International Conference on Data
Mining.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of the Measuring Behavior Symposium
on ?Annotating and measuring Meeting Behavior?.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based
approach. In ACL 2007.
A. B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
M. Hu and B. Liu. 2006. Opinion extraction and sum-
marization on the Web. In 21st National Conference
on Artificial Intelligence (AAAI-2006).
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP-2006, pages 355?363,
Sydney, Australia.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?
125.
Q. Lu and L. Getoor. 2003. Link-based classification.
In Proceedings of the International Conference on
Machine Learning (ICML).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In ACL 2007.
Q. Mei, X. Ling, M. Wondra, H. Su, and C Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In WWW ?07. ACM.
J. Neville and D. Jensen. 2000. Iterative classifica-
tion in relational data. In In Proc. AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters. Computing Attitude and Affect in Text:
Theory and Applications.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
R. Prasad, A. Lee, N. Dinesh, E. Miltsakaki, G. Cam-
pion, A. Joshi, and B. Webber. 2008. Penn dis-
course treebank version 2.0. Linguistic Data Con-
sortium.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Mach. Learn., 62(1-2):107?136.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008.
Sentiment analysis based on probabilistic models us-
ing inter-sentence information. In LREC?08.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Coling
2008.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Coling 2008.
H. Takamura, T. Inui, and M. Okumura. 2007. Extract-
ing semantic orientations of phrases from dictionary.
In HLT-NAACL 2007.
B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004.
Link prediction in relational data. In Neural Infor-
mation Processing Systems.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
ACL 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP 2005.
I. H. Witten and E. Frank. 2002. Data mining: practi-
cal machine learning tools and techniques with java
implementations. SIGMOD Rec., 31(1):76?77.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In ICDM-2003.
74
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 28?36,
Beijing, August 2010
Multilingual Subjectivity: Are More Languages Better?
Carmen Banea, Rada Mihalcea
Department of Computer Science
University of North Texas
carmenbanea@my.unt.edu
rada@cs.unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
While subjectivity related research in
other languages has increased, most of the
work focuses on single languages. This
paper explores the integration of features
originating from multiple languages into
a machine learning approach to subjectiv-
ity analysis, and aims to show that this
enriched feature set provides for more ef-
fective modeling for the source as well
as the target languages. We show not
only that we are able to achieve over
75% macro accuracy in all of the six lan-
guages we experiment with, but also that
by using features drawn from multiple
languages we can construct high-precision
meta-classifiers with a precision of over
83%.
1 Introduction
Following the terminology proposed by (Wiebe
et al, 2005), subjectivity and sentiment analysis
focuses on the automatic identification of private
states, such as opinions, emotions, sentiments,
evaluations, beliefs, and speculations in natural
language. While subjectivity classification labels
text as either subjective or objective, sentiment or
polarity classification adds an additional level of
granularity, by further classifying subjective text
as either positive, negative or neutral.
To date, a large number of text processing ap-
plications have used techniques for automatic sen-
timent and subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 1990), tracking sentiment timelines in on-line
forums and news (Balog et al, 2006; Lloyd et al,
2005), and mining opinions from product reviews
(Hu and Liu, 2004). In many natural language
processing tasks, subjectivity and sentiment clas-
sification has been used as a first phase filtering to
generate more viable data. Research that benefited
from this additional layering ranges from ques-
tion answering (Yu and Hatzivassiloglou, 2003),
to conversation summarization (Carenini et al,
2008), and text semantic analysis (Wiebe and Mi-
halcea, 2006; Esuli and Sebastiani, 2006a).
Although subjectivity tends to be preserved
across languages ? see the manual study in (Mi-
halcea et al, 2007), (Banea et al, 2008) hypoth-
esize that subjectivity is expressed differently in
various languages due to lexicalization, formal
versus informal markers, etc. Based on this obser-
vation, our research seeks to answer the following
questions. First, can we reliably predict sentence-
level subjectivity in languages other than English,
by leveraging on a manually annotated English
dataset? Second, can we improve the English sub-
jectivity classification by expanding the feature
space through the use of multilingual data? Sim-
ilarly, can we also improve the classifiers in the
other target languages? Finally, third, can we ben-
efit from the multilingual subjectivity space and
build a high-precision subjectivity classifier that
could be used to generate subjectivity datasets in
the target languages?
The paper is organized as follows. We intro-
duce the datasets and the general framework in
Section 2. Sections 3, 4, and 5 address in turn each
of the three research questions mentioned above.
Section 6 describes related literature in the area
of multilingual subjectivity. Finally, we draw our
conclusions in Section 7.
2 Multilingual Datasets
Corpora that are manually annotated for subjec-
tivity, polarity, or emotion, are available in only
select languages, since they require a consider-
able amount of human effort. Due to this im-
pediment, the focus of this paper is to create a
method for extrapolating subjectivity data devel-
28
SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
90.4% 34.2% 46.6% 82.4% 30.7% 44.7% 86.7% 32.6% 47.4%
Table 1: Results obtained with a rule-based subjectivity classifier on the MPQA corpus (Wiebe and
Riloff, 2005)
oped in a source language and to transfer it to
other languages. Multilingual feature spaces are
generated to create even better subjectivity classi-
fiers, outperforming those trained on the individ-
ual languages alone.
We use the Multi-Perspective Question An-
swering (MPQA) corpus, consisting of 535
English-language news articles from a variety
of sources, manually annotated for subjectivity
(Wiebe et al, 2005). Although the corpus is an-
notated at the clause and phrase levels, we use
the sentence-level annotations associated with the
dataset in (Wiebe and Riloff, 2005). A sentence
is labeled as subjective if it has at least one pri-
vate state of strength medium or higher. Other-
wise the sentence is labeled as objective. From the
approximately 9700 sentences in this corpus, 55%
of them are labeled as subjective, while the rest
are objective. Therefore, 55% represents the ma-
jority baseline on this corpus. (Wiebe and Riloff,
2005) apply both a subjective and an objective
rule-based classifier to the MPQA corpus data and
obtain the results presented in Table 1.1
In order to generate parallel corpora to MPQA
in other languages, we rely on the method we pro-
posed in (Banea et al, 2008). We experiment with
five languages other than English (En), namely
Arabic (Ar), French (Fr), German (De), Roma-
nian (Ro) and Spanish (Es). Our choice of lan-
guages is motivated by several reasons. First,
we wanted languages that are highly lexicalized
and have clear word delimitations. Second, we
were interested to cover languages that are simi-
lar to English as well as languages with a com-
pletely different etymology. Consideration was
given to include Asian languages, such as Chi-
nese or Japanese, but the fact that their script with-
1For the purpose of this paper we follow this abbreviation
style: Subj stands for subjective, Obj stands for objective,
and All represents overall macro measures, computed over
the subjective and objective classes; P, R, F, and MAcc cor-
respond to precision, recall, F-measure, and macro-accuracy,
respectively.
out word-segmentation preprocessing does not di-
rectly map to words was a deterrent. Finally, an-
other limitation on our choice of languages is the
need for a publicly available machine translation
system between the source language and each of
the target languages.
We construct a subjectivity annotated corpus
for each of the five languages by using machine
translation to transfer the source language data
into the target language. We then project the orig-
inal sentence level English subjectivity labeling
onto the target data. For all languages, other than
Romanian, we use the Google Translate service,2
a publicly available machine translation engine
based on statistical models. The reason Roma-
nian is not included in this group is that, at the
time when we performed the first experiments,
Google Translate did not provide a translation ser-
vice for this language. Instead, we used an al-
ternative statistical translation system called Lan-
guageWeaver,3 which was commercially avail-
able, and which the company kindly allowed us
to use for research purposes.
The raw corpora in the five target lan-
guages are available for download at
http://lit.csci.unt.edu/index.php/Downloads,
while the English MPQA corpus can be obtained
from http://www.cs.pitt.edu/mpqa.
Given the specifics of each language, we em-
ploy several preprocessing techniques. For Ro-
manian, French, English, German and Spanish,
we remove all the diacritics, numbers and punc-
tuation marks except - and ?. The exceptions are
motivated by the fact that they may mark contrac-
tions, such as En: it?s or Ro: s-ar (may be), and
the component words may not be resolved to the
correct forms. For Arabic, although it has a dif-
ferent encoding, we wanted to make sure to treat
it in a way similar to the languages with a Roman
2http://www.google.com/translate t
3http://www.languageweaver.com/
29
alphabet. We therefore use a library4 that maps
Arabic script to a space of Roman-alphabet letters
supplemented with punctuation marks so that they
can allow for additional dimensionality.
Once the corpora are preprocessed, each sen-
tence is defined by six views: one in the origi-
nal source language (English), and five obtained
through automatic translation in each of the tar-
get languages. Multiple datasets that cover all
possible combinations of six languages taken one
through six (a total of 63 combinations) are gen-
erated. These datasets feature a vector for each
sentence present in MPQA (approximately 9700).
The vector contains only unigram features in one
language for a monolingual dataset. For a mul-
tilingual dataset, the vector represents a cumu-
lation of monolingual unigram features extracted
from each view of the sentence. For example, one
of the combinations of six taken three is Arabic-
German-English. For this combination, the vector
is composed of unigram features extracted from
each of the Arabic, German and English transla-
tions of the sentence.
We perform ten-fold cross validation and train
Na??ve Bayes classifiers with feature selection on
each dataset combination. The top 20% of the fea-
tures present in the training data are retained. For
datasets resulting from combinations of all lan-
guages taken one, the classifiers are monolingual
classifiers. All other classifiers are multilingual,
and their feature space increases with each addi-
tional language added. Expanding the feature set
by encompassing a group of languages enables us
to provide an answer to two problems that can ap-
pear due to data sparseness. First, enough training
data may not be available in the monolingual cor-
pus alone in order to correctly infer labeling based
on statistical measures. Second, features appear-
ing in the monolingual test set may not be present
in the training set and therefore their information
cannot be used to generate a correct classification.
Both of these problems are further explained
through the examples below, where we make the
simplifying assumption that the words in italics
are the only potential carriers of subjective con-
tent, and that, without them, their surrounding
4Lingua::AR::Word PERL library.
contexts would be objective. Therefore, their as-
sociation with an either objective or subjective
meaning imparts to the entire segment the same
labeling upon classification.
To explore the first sparseness problem, let us
consider the following two examples extracted
from the English version of the MPQA dataset,
followed by their machine translations in German:
?En 1: rights group Amnesty Interna-
tional said it was concerned about the
high risk of violence in the aftermath?
?En 2: official said that US diplomats
to countries concerned are authorized
to explain to these countries?
?De 1: Amnesty International sagte, es
sei besorgt u?ber das hohe Risiko von
Gewalt in der Folgezeit?
?De 2: Beamte sagte, dass US-
Diplomaten betroffenen La?nder
berechtigt sind, diese La?nder zu
erkla?ren?
We focus our discussion on the word con-
cerned, which in the first example is used in its
subjective sense, while in the second it carries an
objective meaning (as it refers to a group of coun-
tries exhibiting a particular feature defined ear-
lier on in the context). The words in italics in
the German contexts represent the translations of
concerned into German, which are functionally
different as they are shaped by their surrounding
context. By training a classifier on the English ex-
amples alone, under the data sparseness paradigm,
the machine learning model may not differentiate
between the word?s objective and subjective uses
when predicting a label for the entire sentence.
However, appending the German translation to the
examples generates additional dimensions for this
model and allows the classifier to potentially dis-
tinguish between the senses and provide the cor-
rect sentence label.
For the second problem, let us consider two
other examples from the English MPQA and their
respective translations into Romanian:
?En 3: could secure concessions on Tai-
wan in return for supporting Bush on is-
sues such as anti-terrorism and?
30
Lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF MAcc
En 74.01% 83.64% 78.53% 75.89% 63.68% 69.25% 74.95% 73.66% 73.89% 74.72%
Ro 73.50% 82.06% 77.54% 74.08% 63.40% 68.33% 73.79% 72.73% 72.94% 73.72%
Es 74.02% 82.84% 78.19% 75.11% 64.05% 69.14% 74.57% 73.44% 73.66% 74.44%
Fr 73.83% 83.03% 78.16% 75.19% 63.61% 68.92% 74.51% 73.32% 73.54% 74.35%
De 73.26% 83.49% 78.04% 75.32% 62.30% 68.19% 74.29% 72.90% 73.12% 74.02%
Ar 71.98% 81.47% 76.43% 72.62% 60.78% 66.17% 72.30% 71.13% 71.30% 72.22%
Table 2: Na??ve Bayes learners trained on six individual languages
?En 4: to the potential for change
from within America. Supporting our
schools and community centres is a
good?
?Ro 3: ar putea asigura concesii cu
privire la Taiwan, ??n schimb pentru
sust?inerea lui Bush pe probleme cum ar
fi anti-terorismului s?i?
?Ro 4: la potent?ialul de schimbare din
interiorul Americii. Sprijinirea s?colile
noastre s?i centre de comunitate este un
bun?
In this case, supporting is used in both English ex-
amples in senses that are both subjective; the word
is, however, translated into Romanian through two
synonyms, namely sust?inerea and sprijinirea. Let
us assume that sufficient training examples are
available to strengthen a link between support-
ing and sust?inerea, and the classifier is presented
with a context containing sprijinirea, unseen in
the training data. A multilingual classifier may be
able to predict a label for the context using the co-
occurrence metrics based on supporting and ex-
trapolate a label when the context contains both
the English word and its translation into Roma-
nian as sprijinirea. For a monolingual classifier,
such an inference is not possible, and the fea-
ture is discarded. Therefore a multi-lingual classi-
fier model may gain additional strength from co-
occurring words across languages.
3 Question 1
Can we reliably predict sentence-level sub-
jectivity in languages other than English, by
leveraging on a manually annotated English
dataset?
In (Banea et al, 2008), we explored several meth-
ods for porting subjectivity annotated data from
a source language (English) to a target language
(Romanian and Spanish). Here, we focus on the
transfer of manually annotated corpora through
the usage of machine translation by projecting the
original sentence level annotations onto the gener-
ated parallel text in the target language. Our aim
is not to improve on that method, but rather to ver-
ify that the results are reliable across a number of
languages. Therefore, we conduct this experiment
in several additional languages, namely French,
German and Arabic, and compare the results with
those obtained for Spanish and Romanian.
Table 2 shows the results obtained using Na??ve
Bayes classifiers trained in each language individ-
ually, with a macro accuracy ranging from 71.30%
(for Arabic) to 73.89% (for English).5 As ex-
pected, the English machine learner outperforms
those trained on other languages, as the original
language of the annotations is English. However,
it is worth noting that all measures do not deviate
by more than 3.27%, implying that classifiers built
using this technique exhibit a consistent behavior
across languages.
4 Question 2
Can we improve the English subjectivity clas-
sification by expanding the feature space
through the use of multilingual data? Simi-
larly, can we also improve the classifiers in the
other target languages?
We now turn towards investigating the impact on
subjectivity classification of an expanded feature
space through the inclusion of multilingual data.
In order to methodically assess classifier behavior,
we generate multiple datasets containing all pos-
5Note that the experiments conducted in (Banea et al,
2008) were made on a different test set, and thus the results
are not directly comparable across the two papers.
31
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 74.59% 83.14% 78.63% 75.70% 64.97% 69.92% 75.15% 74.05% 74.28%
3 75.04% 83.27% 78.94% 76.06% 65.75% 70.53% 75.55% 74.51% 74.74%
4 75.26% 83.36% 79.10% 76.26% 66.10% 70.82% 75.76% 74.73% 74.96%
5 75.38% 83.45% 79.21% 76.41% 66.29% 70.99% 75.90% 74.87% 75.10%
6 75.43% 83.66% 79.33% 76.64% 66.30% 71.10% 76.04% 74.98% 75.21%
Table 3: Average measures for a particular number of languages in a combination (from one through
six) for Na??ve Bayes classifiers using a multilingual space
sible combinations of one through six languages,
as described in Section 2. We then train Na??ve
Bayes learners on the multilingual data and av-
erage our results per each group comprised of a
particular number of languages. For example, for
one language, we have the six individual classi-
fiers described in Section 3; for the group of three
languages, the average is calculated over 20 pos-
sible combinations; and so on.
Table 3 shows the results of this experiment.
We can see that the overall F-measure increases
from 73.08% ? which is the average over one lan-
guage ? to 75.21% when all languages are taken
into consideration (8.6% error reduction). We
measured the statistical significance of these re-
sults by considering on one side the predictions
made by the best performing classifier for one lan-
guage (i.e., English), and on the other side the
predictions made by the classifier trained on the
multilingual space composed of all six languages.
Using a paired t-test, the improvement was found
to be significant at p = 0.001. It is worth men-
tioning that both the subjective and the objective
precision measures increase to 75% when more
than 3 languages are considered, while the overall
recall level stays constant at 74%.
To verify that the improvement is due indeed
to the addition of multilingual features, and it is
not a characteristic of the classifier, we also tested
two other classifiers, namely KNN and Rocchio.
Figure 1 shows the average macro-accuracies ob-
tained with these classifiers. For all the classi-
fiers, the accuracies of the multilingual combina-
tions exhibit an increasing trend, as a larger num-
ber of languages is used to predict the subjectivity
annotations. The Na??ve Bayes algorithm has the
best performance, and a relative error rate reduc-
 0.6
 0.65
 0.7
 0.75
 0.8
 1  2  3  4  5  6
Number of languages
NBKNNRocchio
Figure 1: Average Macro-Accuracy per group of
languages (combinations of 6 taken one through
six)
tion in accuracy of 8.25% for the grouping formed
of six languages versus one, while KNN and Roc-
chio exhibit an error rate reduction of 5.82% and
9.45%, respectively. All of these reductions are
statistically significant.
In order to assess how the proposed multilin-
gual expansion improves on the individual lan-
guage classifiers, we select one language at a time
to be the reference, and then compute the aver-
age accuracies of the Na??ve Bayes learner across
all the language groupings (from one through six)
that contain the language. The results from this
experiment are illustrated in Figure 2. The base-
line in this case is represented by the accuracy ob-
tained with a classifier trained on only one lan-
guage (this corresponds to 1 on the X-axis). As
more languages are added to the feature space,
we notice a steady improvement in performance.
When the language of reference is Arabic, we ob-
tain an error reduction of 15.27%; 9.04% for Ro-
32
 0.72
 0.73
 0.74
 0.75
 0.76
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 2: Average macro-accuracy progression
relative to a given language
manian; 7.80% for German; 6.44% for French;
6.06% for Spanish; and 4.90 % for English. Even
if the improvements seem minor, they are consis-
tent, and the use of a multilingual feature set en-
ables every language to reach a higher accuracy
than individually attainable.
In terms of the best classifiers obtained for
each grouping of one through six, English pro-
vides the best accuracy among individual clas-
sifiers (74.71%). When considering all possible
combinations of six classifiers taken two, German
and Spanish provide the best results, at 75.67%.
Upon considering an additional language to the
mix, the addition of Romanian to the German-
Spanish classifier further improves the accuracy
to 76.06%. Next, the addition of Arabic results
in the best performing overall classifier, with an
accuracy of 76.22%. Upon adding supplemental
languages, such as English or French, no further
improvements are obtained. We believe this is
the case because German and Spanish are able to
expand the dimensionality conferred by English
alone, while at the same time generating a more
orthogonal space. Incrementally, Romanian and
Arabic are able to provide high quality features
for the classification task. This behavior suggests
that languages that are somewhat further apart are
more useful for multilingual subjectivity classifi-
cation than intermediary languages.
5 Question 3
Can we train a high precision classifier with a
good recall level which could be used to gen-
erate subjectivity datasets in the target lan-
guages?
Since we showed that the inclusion of multilingual
information improves the performance of subjec-
tivity classifiers for all the languages involved, we
further explore how the classifiers? predictions can
be combined in order to generate high-precision
subjectivity annotations. As shown in previous
work, a high-precision classifier can be used to
automatically generate subjectivity annotated data
(Riloff and Wiebe, 2003). Additionally, the data
annotated with a high-precision classifier can be
used as a seed for bootstrapping methods, to fur-
ther enrich each language individually.
We experiment with a majority vote meta-
classifier, which combines the predictions of the
monolingual Na??ve Bayes classifiers described in
Section 3. For a particular number of languages
(one through six), all possible combinations of
languages are considered. Each combination sug-
gests a prediction only if its component classifiers
agree, otherwise the system returns an ?unknown?
prediction. The averages are computed across all
the combinations featuring the same number of
languages, regardless of language identity.
The results are shown in Table 4. The
macro precision and recall averaged across groups
formed using a given number of languages are
presented in Figure 3. If the average monolingual
classifier has a precision of 74.07%, the precision
increases as more languages are considered, with
a maximum precision of 83.38% obtained when
the predictions of all six languages are consid-
ered (56.02% error reduction). It is interesting to
note that the highest precision meta-classifier for
groups of two languages includes German, while
for groups with more than three languages, both
Arabic and German are always present in the top
performing combinations. English only appears
in the highest precision combination for one, five
and six languages, indicating the fact that the pre-
dictions based on Arabic and German are more
robust.
We further analyze the behavior of each lan-
guage considering only those meta-classifiers that
include the given language. As seen in Figure 4,
all languages experience a boost in performance
33
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 76.88% 76.39% 76.63% 80.17% 54.35% 64.76% 78.53% 65.37% 70.69%
3 78.56% 72.42% 75.36% 82.58% 49.69% 62.02% 80.57% 61.05% 68.69%
4 79.61% 69.50% 74.21% 84.07% 46.54% 59.89% 81.84% 58.02% 67.05%
5 80.36% 67.17% 73.17% 85.09% 44.19% 58.16% 82.73% 55.68% 65.67%
6 80.94% 65.20% 72.23% 85.83% 42.32% 56.69% 83.38% 53.76% 64.46%
Table 4: Average measures for a particular number of languages in a combination (from one through
six) for meta-classifiers
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 1  2  3  4  5  6
Number of languages
Macro-PrecisionMacro-Recall
Figure 3: Average Macro-Precision and Recall
across a given number of languages
as a result of paired language reinforcement. Ara-
bic gains an absolute 11.0% in average precision
when considering votes from all languages, as
compared to the 72.30% baseline consisting of the
precision of the classifier using only monolingual
features; this represents an error reduction in pre-
cision of 66.71%. The other languages experi-
ence a similar boost, including English which ex-
hibits an error reduction of 50.75% compared to
the baseline. Despite the fact that with each lan-
guage that is added to the meta-classifier, the re-
call decreases, even when considering votes from
all six languages, the recall is still reasonably high
at 53.76%.
The results presented in table 4 are promis-
ing, as they are comparable to the ones obtained
in previous work. Compared to (Wiebe et al,
2005), who used a high-precision rule-based clas-
sifier on the English MPQA corpus (see Table 1),
our method has a precision smaller by 3.32%, but
a recall larger by 21.16%. Additionally, unlike
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 4: Average Macro-Precision relative to a
given language
(Wiebe et al, 2005), which requires language-
specific rules, making it applicable only to En-
glish, our method can be used to construct a high-
precision classifier in any language that can be
connected to English via machine translation.
6 Related Work
Recently, resources and tools for sentiment anal-
ysis developed for English have been used as
a starting point to build resources in other lan-
guages, via cross-lingual projections or mono-
lingual and multi-lingual bootstrapping. Several
directions were followed, focused on leveraging
annotation schemes, lexica, corpora and auto-
mated annotation systems. The English annota-
tion scheme developed by (Wiebe et al, 2005)
for opinionated text lays the groundwork for the
research carried out by (Esuli et al, 2008) when
annotating expressions of private state in the Ital-
ian Content Annotation Bank. Sentiment and
subjectivity lexica such as the one included with
34
the OpinionFinder distribution (Wiebe and Riloff,
2005), the General Inquirer (Stone et al, 1967), or
the SentiWordNet (Esuli and Sebastiani, 2006b)
were transfered into Chinese (Ku et al, 2006; Wu,
2008) and into Romanian (Mihalcea et al, 2007).
English corpora manually annotated for subjec-
tivity or sentiment such as MPQA (Wiebe et al,
2005), or the multi-domain sentiment classifica-
tion corpus (Blitzer et al, 2007) were subjected
to experiments in Spanish, Romanian, or Chinese
upon automatic translation by (Banea et al, 2008;
Wan, 2009). Furthermore, tools developed for En-
glish were used to determine sentiment or sub-
jectivity labeling for a given target language by
transferring the text to English and applying an
English classifier on the resulting data. The labels
were then transfered back into the target language
(Bautin et al, 2008; Banea et al, 2008). These ex-
periments are carried out in Arabic, Chinese, En-
glish, French, German, Italian, Japanese, Korean,
Spanish, and Romanian.
The work closest to ours is the one proposed
by (Wan, 2009), who constructs a polarity co-
training system by using the multi-lingual views
obtained through the automatic translation of
product-reviews into Chinese and English. While
this work proves that leveraging cross-lingual in-
formation improves sentiment analysis in Chinese
over what could be achieved using monolingual
resources alone, there are several major differ-
ences with respect to the approach we are propos-
ing here. First, our training set is based solely
on the automatic translation of the English corpus.
We do not require an in-domain dataset available
in the target language that would be needed for
the co-training approach. Our method is therefore
transferable to any language that has an English-to
target language translation engine. Further, we fo-
cus on using multi-lingual data from six languages
to show that the results are reliable and replicable
across each language and that multiple languages
aid not only in conducting subjectivity research in
the target language, but also in improving the ac-
curacy in the source language as well. Finally,
while (Wan, 2009) research focuses on polarity
detection based on reviews, our work seeks to de-
termine sentence-level subjectivity from raw text.
7 Conclusion
Our results suggest that including multilingual
information when modeling subjectivity can not
only extrapolate current resources available for
English into other languages, but can also improve
subjectivity classification in the source language
itself. We showed that we can improve an English
classifier by using out-of-language features, thus
achieving a 4.90% error reduction in accuracy
with respect to using English alone. Moreover, we
also showed that languages other than English can
achieve an F-measure in subjectivity annotation
of over 75%, without using any manually crafted
resources for these languages. Furthermore, by
combining the predictions made by monolingual
classifiers using a majority vote learner, we are
able to generate sentence-level subjectivity anno-
tated data with a precision of 83% and a recall
level above 50%. Such high-precision classifiers
may be later used not only to create subjectivity-
annotated data in the target language, but also to
generate the seeds needed to sustain a language-
specific bootstrapping.
To conclude and provide an answer to the ques-
tion formulated in the title, more languages are
better, as they are able to complement each other,
and together they provide better classification re-
sults. When one language cannot provide suffi-
cient information, another one can come to the
rescue.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
1990. Emotions from text: machine learning for text-
based emotion prediction. Intelligence.
Balog, Krisztian, Gilad Mishne, and Maarten De Rijke.
2006. Why Are They Excited? Identifying and Explain-
ing Spikes in Blog Mood Levels. In Proceedings of the
35
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), Trento,
Italy.
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual Subjectivity Analysis Using
Machine Translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), pages 127?135, Honolulu.
Bautin, Mikhail, Lohit Vijayarenu, and Steven Skiena. 2008.
International Sentiment Analysis for News and Blogs. In
Proceedings of the International Conference on Weblogs
and Social Media (ICWSM-2008), Seattle, Washington.
Blitzer, John, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational (ACL-2007), pages 440?447, Prague,
Czech Republic. Association for Computational Linguis-
tics.
Carenini, Giuseppe, Raymond T Ng, and Xiaodong Zhou.
2008. Summarizing Emails with Conversational Cohe-
sion and Subjectivity. In Proceedings of the Association
for Computational Linguistics: Human Language Tech-
nologies (ACL- HLT 2008), pages 353?361, Columbus,
Ohio.
Esuli, Andrea and Fabrizio Sebastiani. 2006a. Determining
Term Subjectivity and Term Orientation for Opinion Min-
ing. In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguistics
(EACL-2006), volume 2, pages 193?200, Trento, Italy.
Esuli, Andrea and Fabrizio Sebastiani. 2006b. SentiWord-
Net: A Publicly Available Lexical Resource for Opinion
Mining. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation, pages 417?422.
Esuli, Andrea, Fabrizio Sebastiani, and Ilaria C Urciuoli.
2008. Annotating Expressions of Opinion and Emotion
in the Italian Content Annotation Bank. In Proceedings
of the Sixth International Language Resources and Eval-
uation (LREC-2008), Marrakech, Morocco.
Hu, Minqing and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining (ACM-
SIGKDD-2004), pages 168?177, Seattle, Washington.
Ku, Lun-wei, Yu-ting Liang, and Hsin-hsi Chen. 2006.
Opinion Extraction, Summarization and Tracking in
News and Blog Corpora. In Proceedings of AAAI-2006
Spring Symposium on Computational Approaches to An-
alyzing Weblogs, number 2001, Boston, Massachusetts.
Lloyd, Levon, Dimitrios Kechagias, and Steven Skiena,
2005. Lydia : A System for Large-Scale News Analysis
( Extended Abstract ) News Analysis with Lydia, pages
161?166. Springer, Berlin / Heidelberg.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe. 2007.
Learning Multilingual Subjective Language via Cross-
Lingual Projections. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics
(ACL-2007), pages 976?983, Prague, Czech Republic.
Riloff, Ellen and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003), pages 105?112, Sap-
poro, Japan.
Stone, Philip J, Marshall S Smith, Daniel M Ogilivie, and
Dexter C Dumphy. 1967. The General Inquirer: A Com-
puter Approach to Content Analysis. /. The MIT Press,
1st edition.
Wan, Xiaojun. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), Singapore.
Wiebe, Janyce and Rada Mihalcea. 2006. Word Sense and
Subjectivity. In Proceedings of the joint conference of
the International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING-ACL-2006), Sydney, Australia.
Wiebe, Janyce and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unannotated
Texts. In Proceeding of CICLing-05, International Con-
ference on Intelligent Text Processing and Computational
Linguistics, pages 486?497, Mexico City, Mexico.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 39(2-
3):165?210.
Wu, Yejun. 2008. Classifying attitude by topic aspect for
English and Chinese document collections.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentence. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
36
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 79?88, Dublin, Ireland, August 23-29 2014.
Joint Inference and Disambiguation of Implicit Sentiments via
Implicature Constraints
Lingjia Deng
1
, Janyce Wiebe
1,2
, Yoonjung Choi
2
1
Intelligent Systems Program, University of Pittsburgh
2
Department of Computer Science, University of Pittsburgh
lid29@pitt.edu, wiebe@cs.pitt.edu, yjchoi@cs.pitt.edu
Abstract
This paper addresses implicit opinions expressed via inference over explicit sentiments and
events that positively/negatively affect entities (goodFor/badFor, gfbf events). We incorporate
the inferences developed by implicature rules into an optimization framework, to jointly improve
sentiment detection toward entities and disambiguate components of gfbf events. The framework
simultaneously beats the baselines by more than 10 points in F-measure on sentiment detection
and more than 7 points in accuracy on gfbf polarity disambiguation.
1 Introduction
Previous work in NLP on sentiment analysis has mainly focused on explicit sentiments. However, as
noted in (Deng and Wiebe, 2014), many opinions are expressed implicitly, as shown by this example:
Ex(1) The reform would lower health care costs, which would be a tremendous positive change across the entire
health-care system.
There is an explicit positive sentiment toward the event of ?reform lower costs?. However, in expressing
this sentiment, the writer also implies he is negative toward the ?costs?, since he?s happy to see the costs
being decreased. Moreover, the writer may be positive toward ?reform? since it contributes to the ?lower?
event. Such inferences may be seen as opinion-oriented implicatures (i.e., defeasible inferences)
1
.
We develop a set of rules for inferring and detecting implicit sentiments from explicit sentiments and
events such as ?lower? (Wiebe and Deng, 2014). In (Deng et al., 2013), we investigate such events,
defining a badFor (bf) event to be an event that negatively affects the theme and a goodFor (gf) event to
be an event that positively affects the theme of the event.
2
Here, ?lower? is a bf event. According to their
annotation scheme, goodFor/badFor (gfbf) events have NP agents and themes (though the agent may be
implicit), and the polarity of a gf event may be changed to bf by a reverser (and vice versa).
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s senti-
ments toward entities mentioned in the text. However, this requires resolving several ambiguities: (Q1)
Given a document, which spans are gfbf events? (Q2) Given a gfbf text span, what is its polarity, gf
or bf? (Q3) Is the polarity of a gfbf event being reversed? (Q4) Which NP in the sentence is the agent
and which is the theme? (Q5) What are the writer?s sentiments toward the agent and theme, positive
or negative? Fortunately, the implicature rules in (Deng and Wiebe, 2014) define dependencies among
these ambiguities. As in Ex(1), the sentiments toward the agent and theme, the sentiment toward the gfbf
event (positive or negative), and the polarity of the gfbf event (gf or bf) are all interdependent. Thus,
rather than having to take a pipeline approach, we are able to develop an optimization framework which
exploits these interdependencies to jointly resolve the ambiguities.
Specifically, we develop local detectors to analyze the four individual components of gfbf events,
(Q2)-(Q5) above. Then, we propose an Integer Linear Programming (ILP) framework to conduct global
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Specifically, we focus on generalized conversational implicature (Grice, 1967; Grice, 1989).
2
Compared to (Deng et al., 2013), we change the term ?object? to ?theme? as the later is more appropriate for this task.
79
inference, where the gfbf events and their components are variables and the interdependencies defined by
the implicature rules are encoded as constraints over relevant variables in the framework. The reason we
do not address (Q1) is that the gold standard we use for evaluation contains sentiment annotations only
toward the agents and themes of gfbf events. We are only able to evaluate true hits of gfbf events. Thus,
the input to the system is the set of the text spans marked as gfbf events in the corpus. The results show
that, compared to the local detectors, the ILP framework improves sentiment detection by more than 10
points in F-measure and disambiguating gfbf polarity by more than 7 points in the accuracy, without any
loss in accuracy for other two components.
2 Related Work
Most work in sentiment analysis focuses on classifying explicit sentiments and extracting explicit opinion
expressions, holders and targets (Wiebe et al., 2005; Johansson and Moschitti, 2013; Yang and Cardie,
2013). There is some work investigating features that directly indicate implicit sentiments (Zhang and
Liu, 2011; Feng et al., 2013). In contrast, we focus on how we can bridge between explicit and implicit
sentiments via inference. To infer the implicit sentiments related to gfbf events, some work mines various
syntactic patterns (Choi and Cardie, 2008), proposes linguistic templates (Zhang and Liu, 2011; Anand
and Reschke, 2010; Reschke and Anand, 2011), or generates a lexicon of patient polarity verbs (Goyal
et al., 2013). Different from their work, which do not cover all cases relevant to gfbf events, (Deng and
Wiebe, 2014) defines a generalized set of implicature rules and proposes a graph-based model to achieve
sentiment propagation between the agents and themes of gfbf events. However, that system requires
all of the gfbf information (Q1)-(Q4) to be input from the manual annotations; the only ambiguity it
resolves is sentiments toward entities. In contrast, the method in this paper tackles four ambiguities
simultaneously. Further, as we will see below in Section 6, the improvement over the local detectors by
the current method is greater than that by the previous method, even though it operates over the noisy
output of local components automatically.
Different from pipeline architectures, where each step is computed independently, joint inference has
often achieved better results. Roth and Yih (2004) formulate the task of information extraction using
Integer Linear Programming (ILP). Since then, ILP has been widely used in various tasks in NLP, in-
cluding semantic role labeling (Punyakanok et al., 2004; Punyakanok et al., 2008; Das et al., 2012),
joint extraction of opinion entities and relations (Choi et al., 2006; Yang and Cardie, 2013), co-reference
resolution (Denis and Baldridge, 2007), and summarization (Martins and Smith, 2009). The most similar
ILP model to ours is (Somasundaran and Wiebe, 2009), which improves opinion polarity classification
using discourse constraints in an ILP model. However, their work addresses discourse relations among
explicit opinions in different sentences.
3 GoodFor/BadFor Event and Implicature
This work addresses sentiments toward, in general, states and events which positively or negatively
affect entities. Deng et al. (2013) (hereafter DCW) identify a clear case that occurs frequently in opinion
sentences, namely the gfbf events mentioned above. As defined in DCW, a gf event is an event that
positively affects the theme of the event and a bf event is an event that negatively affects the theme.
According to the annotation schema, gfbf events have NP agents and themes (though the agent may be
implicit). In the sentence ?President Obama passed the bill?, the agent of the gf ?passed? is ?President
Obama? and the theme is ?the bill?. In the sentence ?The bill was denied?, the agent of the bf ?was
denied? is implicit. The polarity of a gf event may be changed to bf by a reverser (and vice versa). For
example, in ?The reform will not worsen the economy,? ?not? is a reverser and it reverses the polarity
from bf to gf.
3
The constraints we encode in the ILP framework described below are based on implicature rules in
(Deng and Wiebe, 2014). Table 1 gives two rule schemas, each of which defines four specific rules. In
3
DCW also introduce retainers. We don?t analyze retainers in this work since they do not affect the polarity of gfbfs, and
only 2.5% of gfbfs have retainers in the corpus.
80
s(gfbf) gfbf ? s(agent) s(theme) s(gfbf) gfbf ? s(agent) s(theme)
1 positive gf ? positive positive 3 positive bf ? positive negative
2 negative gf ? negative negative 4 negative bf ? negative positive
Table 1: Rule Schema 1 & Rule Schema 3 (Deng and Wiebe, 2014)
the table, s(?) = ? means that the writer?s sentiment toward ? is ?, where ? is a gfbf event, or the agent
or theme of a gfbf event, and ? is either positive or negative. P? Q means to infer Q from P.
Applying the rules to Ex(1): the writer expresses a positive sentiment (?positive?) toward a bf event
(?lower?), thus matching Case 3 in Table 1. We infer that the writer is positive toward the agent (?re-
form?) and negative toward the theme (?costs?). Two other rule schemas (not shown) make the same
inferences as Rule Schemas 1 and 3 but in the opposite direction. As we can see, if two entities partic-
ipate in a gf event, the writer has the same sentiment toward the agent and theme, while if two entities
participate in a bf event, the writer has opposite sentiments toward them. Later we use this observation
in our experiments.
4 Global Optimization Framework
Optimization is performed over two sets of variables. The first set is GFBF, containing a variable for
each gfbf event in the document. The other set is Entity, containing a variable for each agent or theme
candidate. Each variable k in GFBF has its corresponding agent and theme variables, i and j, in Entity.
The three form a triple unit, ?i, k, j?. The set Triple consists of each ?i, k, j?, recording the correspon-
dence between variables in GFBF and Entity. The goal of the framework is to assign optimal labels to
variables in Entity and GFBF. We first introduce how we recognize candidates for agents and themes,
then introduce the optimization framework, and then define local scores that are input to the framework.
4.1 Local Agents and Theme Candidates Detector
We extract two agent candidates and two theme candidates for each gfbf event (one each will ultimately
be chosen by the ILP model).
4
We use syntax, and the output of the SENNA (Collobert et al., 2011)
semantic role labeling tool. SENNA labels the A0 (subject), A1 (object), and A2 (indirect object) spans
for each predicate, if possible. To extract the semantic agent candidate: If SENNA labels a span as A0
of the gfbf event, we consider it as the semantic agent; if there is no A0 but A1 is labeled, we consider
A1; if there is no A0 or A1 but A2 is labeled, we consider A2. To extract the syntactic agent candidate,
we find the nearest noun in front of the gfbf span, and then extract any other word that depends on the
noun according to the dependency parse. Similarly, to extract the semantic theme candidate, we consider
A1, A2, A0 in order. To extract the syntactic theme candidate, the same procedure is conducted as for
the syntactic agent, but the nearest noun should be after the gfbf. If there is no A0, A1 or A2, then there
is only one agent candidate, implicit and only one theme candidate, null. We treat a null theme as an
incorrect span in the later evaluations. If the two agent (theme) candidate spans are the same, there is
only one candidate.
4.2 Integer Linear Programming Framework
We use Integer Linear Programming (ILP) to assign labels to variables. Variables in Entity will be
assigned positive or negative, representing the writer?s sentiments toward them. We may have two candi-
date agents for a gfbf and that we will choose between them. Thus, only one agent is assigned a positive
or negative label; the other is considered to be an incorrect agent of the gfbf (similarly for the theme can-
didates). Each variable in GFBF will be assigned the label gf or bf. Optionally, it may also be assigned
the label reversed. Label gf or bf is the polarity of the gfbf event; reversed is assigned if the polarity is
reversed (e.g., for ?not harmed?, the labels are bf and reversed).
The objective function of the ILP is:
4
This framework is able to handle any number of candidates. The methods we tried using more candidates did not perform
as well - the gain in recall was offset by larger losses in precision.
81
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
(1)
subject to
u
ic
? {0, 1}, ?i, c ?
ikj
, ?
ikj
? {0, 1},??i, k, j? ? Triple (2)
where L
i
is the set of labels given to ?i ? GFBF ? Entity. If i ? GFBF, L
i
is {gf, bf, reversed} ({gf,
bf, r}, for short). If i ? Entity, L
i
is {positive, negative} ({pos, neg}, for short). u
ic
is a binary in-
dicator representing whether the label c is assigned to the variable i. When an indicator variable is 1,
the corresponding label is selected. p
ic
is the score given by local detectors, introduced in the following
sections. Variables ?
ikj
and ?
ikj
are binary slack variables that correspond to the gfbf implicature con-
straints of ?i, k, j?. When a given slack variable is 1, the corresponding triple violates the implicature
constraints. Minimizing the objective function could achieve two goals at the same time. The first part
(?1 ?
?
i
?
c
p
ic
u
ic
) tries to select a set of labels that maximize the scores given by the local detectors.
The second part (
?
ikj
?
ikj
+
?
ikj
?
ikj
) aims at minimizing the cases where gfbf implicature constraints
are violated. Here we do not force each triple to obey the implicature constraints, but to minimize the
violating cases. For each variable, we have defined constraints:
?
c?L
GFBF
?
u
kc
= 1, ?k ? GFBF (3)
?
i?Entity
?i,k,j??Triple
?
c?L
Entity
u
ic
= 1,?k ? GFBF (4)
?
j?Entity,
?i,k,j??Triple
?
c?L
Entity
u
jc
= 1,?k ? GFBF (5)
where L
GFBF
?
in Equation (3) is a subset of L
GFBF
, consisting of {gf, bf}. Equation (3) means a
gfbf must be either gf or bf. But it is free to choose whether it is being reversed. Recall that we have two
agent candidates (a1,a2) for a gfbf. Thus we have four agent indicators in Equation (4): u
a1,pos
, u
a1,neg
,
u
a2,pos
and u
a2,neg
. Equation (4) ensures that three of them are 0 and one of them is 1. For instance,
u
a1,pos
assigned 1 means that candidate a1 is selected to be the agent span and pos is selected to be its
polarity. In this way, the framework disambiguates the agent span and sentiment polarity simultaneously.
(Similar comments apply for the theme candidates in Equation (5).)
According to the implicature rules in Table 1 in Section 3, the writer has the same sentiment toward
entities in a gf relation. Thus, for each triple unit ?i, k, j?, the gf constraints are applied via the following:
|
?
i,?i,k,j?
u
i,pos
?
?
j,?i,k,j?
u
j,pos
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (6)
|
?
i,?i,k,j?
u
i,neg
?
?
j,?i,k,j?
u
j,neg
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (7)
We use |u
k,gf
? u
k,r
| to represent whether this triple is gf. In Equation (6), if this value is 1, then the
triple should follow the gf constraints. In that case, ?
ikj
= 0 means that the triple doesn?t violate the
gf constraints, and |
?
i
u
i,pos
?
?
j
u
j,pos
| must be 0. Further, in this case,
?
i
u
i,pos
and
?
j
u
j,pos
are
constrained to be of the same value (both 1 or 0) ? that is, entities i and j must be both positive or both
not positive. However, if ?
ikj
= 1, Equation (6) does not constrain the values of the variables at all. If
|u
k,gf
? u
k,r
| is 0, representing that the triple is not gf, then Equation (6) does not constrain the values
of the variables. Similar comments apply to Equation (7).
In contrast, the writer has opposite sentiments toward entities in a bf relation.
|
?
i,?i,k,j?
u
i,pos
+
?
j,?i,k,j?
u
j,pos
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (8)
|
?
i,?i,k,j?
u
i,neg
+
?
j,?i,k,j?
u
j,neg
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
,?k ? GFBF (9)
We use |u
k,bf
? u
k,r
| to represent whether this triple is bf. In Equation (8), if a triple is bf and the
constraints are not violated, then |
?
i
u
i,pos
+
?
j
u
j,pos
? 1| must be 0. Further, in this case,
?
i
u
i,pos
82
ugf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
| u
gf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
|
A 1 0 0 1 0 C 0 1 0 0 1
B 0 1 1 1 0 D 1 0 1 0 1
Table 2: Truth table of being reversed or not (k is omitted)
and
?
j
u
j,pos
are constrained to be of the opposite value ? that is, if entity i is positive then entity j must
not be positive. Similar comments apply to Equation (9).
Note that above we use |u
k,gf
?u
k,r
| and |u
k,bf
?u
k,r
| to represent whether a triple is gf or bf. In Table
2, we show that they always take opposite values and that they are consistent with the actual polarities.
In Table 2, Case A means the triple is gf and Case B means the triple is bf but it is reversed. In both
cases, |u
gf
? u
r
| = 1, indicating that the triple should follow the gf constraints. Similarly for Case C
and Case D to follow the bf constraints.
4.3 Local GoodFor/BadFor Score: p
k,gf
, p
k,bf
We utilize a sense-level gfbf lexicon by (Choi et al., 2014). In total there are 6,622 gf senses and 3,290
bf senses. The gf lexicon covers 64% of the gf words in the corpus and the bf lexicon covers 42% of the
bf words. We then look up the gfbf span k in the gfbf lexicon. If k only appears in the gf lexicon, then
p
k,gf
= 1 ?  and p
k,bf
= . Here  = 0.0001, to prevent there being any 0 scores in our computation.
If k only appears in the bf lexicon, then p
k,bf
= 1 ?  and p
i,gf
= . If k appears in both the gf and bf
lexicon, and there are a senses in the gf lexicon and b senses in the bf lexicon, then p
k,gf
= a/(a + b)
and p
k,bf
= b/(a + b). If k is not in either lexicon, then p
k,gf
= p
k,bf
= . If there is more than one
word in the gfbf span, we take the maximum score.
4.4 Local Reversed Score: p
k,r
As introduced in Section 3, a reverser changes the polarity of a gfbf. First, we build reverser lexicons
from Wilson?s shifter lexicon (2008), namely the entries labeled as genshifter, negation, and shiftneg.
We create two lexicons: one with the verbs and the other with the non-verb entries, excluding nouns,
adjectives, and adverbs, since most non-verb reversers are prepositions or subordinating conjunctions.
There are 219 reversers in the entire corpus; 134 (61.19%) are instances of words in one of the two
lexicons. Based on the lexicon, we categorize reversers into three classes. Examples are shown below.
Ex(2) They will not be able to water down your coverage.
Ex(3) ... how a massive new bureaucracy will cut costs without hurting the old and the helpless.
Ex(4) The new law includes new rules to prevent insurance companies from overcharging patients.
Negation: An instance in this category is ?not? in Ex(2). If any word in the gfbf span has a neg
dependency relation according to the Stanford dependency parser, then we consider the gfbf to be negated
(i.e., reversed). In this case the path between the negator and the gfbf is labeled neg and the length of the
path is one.
Other Non-Verb: This category consists of words such as ?without? in Ex(3) (others are ?never? and
?few?, etc). These words lower the extent of the gfbf event. We look in the sentence for instances of
words in the non-verb reverser lexicon, which are not tagged as noun, verb, adj, or adv. For any found,
we examine the path in the dependency parse between the potential reverser and the gfbf span. If the
path has at least one of advmod, pcomp, cc, xcomp, nsubj, neg and the length of the path is less than four
(learnt from development set), the event is considered to be reversed.
Verb: In Ex(4), the verb ?prevent? stops the gfbf event ?overcharging? from happening. We call such
words Verb reverser (others are ?prohibit? and ?ban?, etc). We look in the sentence for instances of words
in the verb reverser lexicon. For any that appear before the gfbf span in the sentence, if the path has at
least one of xcomp, pcomp, obj and the length of the path is less than four, then the event is reversed.
For the triple ?companies, overcharging, patients? in Ex(4), though it is reversed by ?prevent?, the agent
of the reverser, which is ?law?, is different from the agent of the gfbf, which is ?companies?, so the bf
83
within the ?overcharging? event is not reversed.
5
Though we extract the Verb reversers to evaluate the
performance of recognizing a reverser, in the optimization framework, gfbf events with Verb reversers
are not considered to be reversed, since almost all Verb reversers introduce new agents.
Different from other scores, p
k,r
could be negative. According to the heuristics above, the probability
of a gfbf event being reversed decreases as the length of the path increases. We define p
k,r
so it is
inversely proportional to the length of the path. Further, to make sense of a gfbf triple ?agent, gfbf,
theme?, where, e.g., the local detectors label it ?pos, bf, pos?, the framework is choosing the smaller
one from (a) ?1 ? p
k,r
? u
k,r
(it has a reverser) versus (b) 1 ? ?
ikj
(it is an exception to the rules). The
framework assigns u
k,r
= 0 and ?
ikj
= 1 if ?1 ? p
k,r
> 1. It assigns u
k,r
= 1 and ?
ikj
= 0 if
?1 ? p
k,r
<= 1. For gfbf events which have Negation or Other Non-verb reversers, since we use the
length four as a threshold in the heuristics above, we define p
k,r
=
1
d
?
5
4
, so that ?1 ? p
k,r
=
5
4
?
1
d
> 1
if d > 4. For gfbf events for which no reverser word appears in the sentence, or those which only have
Verb reversers, p
k,r
= ?1 ?
5
4
(so ?1 ? p
k,r
> 1), so that the framework chooses case (b) (choosing the
gfbf event to be not reversed).
4.5 Local Sentiment Score: p
i,pos
, p
i,neg
In the corpus of DCW, only the writer?s sentiments toward the agents and the themes of gfbf events are
annotated. Thus, since there are many false negatives of sentiments toward entities, the corpus does
not support training a classifier. Therefore, we adopt the same local sentiment detector from (Deng
and Wiebe, 2014), using available resources to detect writer?s sentiments toward all agent and theme
candidates.
6
The sentiment scores range from 0.5 to 1.
5 Co-reference In the Framework
So far the constraints in the framework are within a gfbf triple. Consider the following example:
Ex(5) The reform will decrease the healthcare costs and improve the medical qualify as expected.
The two gfbfs, ?decrease? and ?improve? have the same agent, ?reform?. Thus, if there is more than
one gfbf in a sentence, and the path between the two gfbfs in dependency parse contains only conj or
xcomp, and there is no other noun between the latter gfbf and the conjunction, we assume the two agents
are the same and the sentiments toward them should be the same. Thus, for any i, j ? Entity, if i, j
co-refer
7
, or they are the same agent as described above, Coref(i, j) = 1 (otherwise 0). We add two
more constraints, similar to the gf constraints in Equations (6) and (7), as shown in Equation (10) and
(11). where ?
ij
is a slack variable, e(i) is the set of agent/theme candidates linked to the same gfbf as i
is. If Coref(i, j) = 0, Equations (10) and (11) do not constrain the variables. The objective function in
Equation (12) is updated to incorporate these new constraints.
|
?
e(i)
u
i,pos
?
?
e(j)
u
j,pos
|+ Coref(i, j) <= 1 + ?
ij
,?i, j ? Entity (10)
|
?
e(i)
u
i,neg
?
?
e(j)
u
j,neg
|+ Coref(i, j) <= 1 + ?
ij
, ?i, j ? Entity (11)
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
+
?
i,j?Entity
?
ij
(12)
6 Experiment and Performance
In this section we introduce the data we use, the baseline methods, the evaluations and the results. In
addition, we give examples illustrating how opinion inference may improve performances.
5
DCW defines here is a triple chain: ?law, prevent ?companies, overcharging, patients??. The reverser is changing the
polarity between ?law? and ?patients?, but it does not change the polarity between ?companies? and ?patients?.
6
We use Opinion Extractor (Johansson and Moschitti, 2013) , opinionFinder (Wilson et al., 2005), MPQA subjectivity
lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966) and a connotation lexicon (Feng et al., 2013), to detect
writer?s sentiments toward all agent and theme candidates, and all gfbf events. We adopt Rule 1 and Rule 3 to infer from the
sentiment toward event to the sentiment toward theme. Then we conduct a majority voting based on the results.
7
We use the co-reference resolution system from (Stoyanov et al., 2010).
84
6.1 Experiment Data
We use the ?Affordable Care Act? corpus of DCW, consisting of 134 online editorials and blogs. In total,
there are 1,762 annotated triples, out of which 692 are gf or retainers and 1,070 are bf or reversers. From
the writer?s perspective, 1,495 noun phrases are annotated positive, 1,114 noun phrases are negative
and the remaining 8 are neutral. This indicates that there are many opinions in the corpus. Out of 134
documents in the corpus, 3 do not have any annotation. 6 are used as a development set to develop the
heuristics in Sections 4 and 5. We use the remaining 125 for the experiments.
6.2 Baseline Methods and Evaluation Metrics
We compare the output of the global optimization framework with the outputs of baseline systems built
from the local detectors in Section 4. For the gfbf polarity and reverser ambiguities, the local detectors
directly provide a disambiguation result. For the agent/theme span and sentiment ambiguities, the local
sentiment detector assigns positive and negative scores to each candidate. The framework chooses among
the combined options. Thus, for comparison, we build a baseline system that combines the outputs of
the local agent/theme candidate detector and the local sentiment detector.
Recall from Section 4, a variable k ? GFBF has two agent candidates, a1 and a2 ? Entity. Together
there are four binary indicator variables: u
a1,pos
, u
a1,neg
, u
a2,pos
and u
a2,neg
. Among these indicator
variables whose corresponding local scores (e.g., p
a1,pos
is the score of u
a1,pos
) are larger than 0.5,
the baseline system (denoted Local) chooses the one with the largest local sentiment score. If there is
a tie, it prefers the variable representing the semantic candidate. If there is still a tie, it chooses the
variable representing the majority polarity (positive). If all the local scores of the four variables are
0.5 (neutral), Local fails to recognize any sentiment for that entity, so it assigns 0 to all the indicator
variables. Local+coref takes the maximum local score of the entities if they co-ref, and assigns each
entity the maximum score before disambiguation.
Another baseline, Majority, always chooses the semantic candidate and the majority polarity.
To evaluate the performance in detecting sentiment, we use precision, recall, and F-measure. We do
not take into account any agent or theme manually annotated as neutral (there are only 8).
P =
#(auto=gold & gold!=neutral)
#auto!=neutral
Accuracy = R =
#(auto=gold & gold!=neutral)
#gold!=neutral
F =
2*P*R
P+R
(13)
In the equations, auto is the system?s output and gold is the gold-standard label from annotations. Since
we don?t take into account any neutral agent or theme, #gold!=neutral equals to all nodes in the exper-
iment set. Thus accuracy is equal to recall. We only report recall here. Here we have two definitions
of auto=gold: (1) Strict evaluation means that, by saying auto=gold, the agent/theme must have the
same polarity and must be the same NP as the gold standard, and (2) Relaxed evaluation means the
agent/theme has the same polarity as the gold standard, regardless whether the span is correct or not.
Note that according to DCW, an implicit agent isn?t annotated with any sentiment. Thus, for an
implicit agent in gold, if auto outputs the span ?implicit?, we treat it as a correct span with correct
polarity, regardless what sentiment auto gives to it. If auto outputs any span other than ?implicit?, we
treat it as a wrong span with wrong polarity, regardless of its sentiment as well. For the theme span, if
auto outputs a ?null? theme candidate, we treat it as a wrong span but we evaluate its sentiment according
to gold.
To evaluate extracting candidate span, we use accuracy. The baseline for this task always chooses the
semantic candidate. To evaluate gfbf polarity and reverser, we also use accuracy.
Note that although we evaluate the performance in different tasks separately, the framework resolves
all the ambiguities at the same time.
6.3 Results
We report the performance results for (A) sentiment detection in Table 3, on two sets. One is the subset
containing the agents and themes where auto has the correct spans with gold. The other is the set of
all agents and themes. As shown in Table 3, ILP significantly improves performance, approximately
10-20 points on F-measure over different baselines. Though Local has a competitive precision with
85
correct span subset whole set, strict eval whole set, relaxed eval
P R F P R F P R F
1 ILP 0.6421 0.6421 0.6421 0.4401 0.4401 0.4401 0.5939 0.5939 0.5939
2 Local 0.6409 0.3332 0.4384 0.4956 0.2891 0.3652 0.5983 0.3490 0.4408
3 ILP+coref 0.6945 0.6945 0.6945 0.4660 0.4660 0.4660 0.6471 0.6471 0.6471
4 Local+coref 0.6575 0.3631 0.4678 0.5025 0.3103 0.3836 0.6210 0.3834 0.4741
5 Majority 0.5792 0.5792 0.5792 0.3862 0.3862 0.3862 0.5462 0.5462 0.5462
Table 3: Performances of sentiment detection
ILP, it has a much lower recall. That means the local sentiment detector cannot recognize implicit
sentiments toward most entities. But ILP is able to recognize more entities correctly. By adding coref,
performance improves for both ILP and Local. In comparison to (Deng and Wiebe, 2014), our current
method improves more in F-measure (2.43 points more) over local sentiment detector than the earlier
work, even though the earlier work takes the manual annotations of all the gfbf information as input.
In terms of the other tasks: For (B) agent/theme span, the baseline achieves 66.67% in accuracy, com-
pared to 68.54% and 67.10% for ILP and ILP+coref, respectively. For (C) gfbf polarity, the baseline
has an accuracy of 70.68%, whereas ILP achieves 77.25% and ILP+coref achieves 77.47%, respectively,
both 7 points higher. This improvement is interesting because it represents cases in which the optimiza-
tion framework is able to infer the correct polarity even though the gfbf span is not recognized by the
local detector (i.e., the span isn?t in the gfbf lexicon). For (D) reverser, the baseline is 88.07% in accu-
racy. ILP and ILP+coref are competitive with the baseline: 89% and 88.07% respectively. Note that both
our local detector and ILP surpass the majority class (not reversed) which has an accuracy of 86.60%.
Following (Akkaya et al., 2009), since ILP is unsupervised without multiple runs, we adopt McNe-
mar?s test to measure statistical significance of our improvements (Dietterich, 1998). In Table 3, the
improvements in recalls of Line 1 over 2, Line 3 over 4, and Lines 1&3 over 5 are statistically significant
at the p < .001 level. The improvements of Line 3 over 1 are statistically significant at the p < .005
level. For accuracy of gfbf polarity, the improvement is significant at the p < .001 level.
6.4 Examples
This sections gives simplified examples to illustrate how the framework can improve over the local
detectors. The explicit sentiment clues referred to in this section are from MPQA lexicon.
Ex(6) The reform would curb skyrocketing costs in the long run.
The local sentiment detector assigns ?costs? negative due to the single sentiment clue, ?skyrocketing?.
Since the agent and theme are in a bf triple, and the writer is negative toward that theme, we can infer
the writer is positive toward the agent. This illustrates how we improve recall on sentiments.
Ex(7) The supposedly costly reform will curb skyrocketing costs in the long run.
In Ex(7), agent ?reform? is labeled negative because ?costly? is a negative clue in the lexicon. (?sup-
posedly? is not in it.) However, in Ex(7), it is actually positive. The agent?s negative score is 0.6, and
its positive score is 0.5 due to the absence of a positive clue. Since the theme is negative too, by the bf
constraints, we expect to see a positive agent. If we were to assign negative to the agent, the objective
function would have -0.6 subjectivity score and +1 in violation penalty, together giving +0.4. If we as-
sign positive, the subjectivity score is -0.5, and there is no violation, resulting in a total score of -0.5.
Thus, the framework correctly chooses the positive label. This shows how we can improve precision on
sentiments.
Ex(8) The great reform will curb skyrocketing costs in the long run.
In this case, the agent is positive and the theme is negative. If the gfbf word ?curb? is not in the lexicon,
we could still infer its polarity. Given that the entities in the triple have different sentiments, to not violate
86
the implicature rules, the framework will assign it bf, or assign it gf along with reversed. However, there
is no reverser word in the sentence, so the reversed score p
r
= ?
5
4
. The framework will assign the
reverser indicator u
r
= 0, in order to avoid a gain in the objective function by ?1 ? p
r
? u
r
. Thus
the framework assigns the label bf to ?curb?. This is how the framework can improve the accuracy of
recognizing gfbf polarity.
7 Conclusion
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s
sentiments toward entities mentioned in the text. Using an unsupervised optimization framework that
incorporates gfbf implicature rules as constraints, our method improves over local sentiment recognition
by almost 20 points in F-measure and over all sentiment baselines by over 10 points in F-measure. The
global optimization framework jointly infers the polarity of gfbf events, whether or not they are reversed,
which candidate NPs are the agent and theme, and the writer?s sentiments toward them. In addition
to beating the baselines for sentiment detection, the framework significantly improves the accuracy of
gfbf polarity disambiguation. This work not only automatically utilizes gfbf information to improve
sentiment detection, it also proposes a framework for jointly solving various ambiguities related to gfbf
events.
Acknowledgement This work was supported in part by DARPA-BAA-12-47 DEFT grant. We would
like to thank the anonymous reviewers for their helpful feedback.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 190?199, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pranav Anand and Kevin Reschke. 2010. Verb classes as evaluativity functor classes. In Interdisciplinary Work-
shop on Verbs. The Identification and Representation of Verb Features.
Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 793?801, Honolulu, Hawaii, October. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Yoonjung Choi, Janyce Wiebe, and Lingjia Deng. 2014. Lexical acquisition for opinion inference: A sense-level
lexicon of benefactive and malefactive events. In 5th Workshop on Computational Approaches to Subjectivity,
Sentiment & Social Media Analysis.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493?2537, November.
Dipanjan Das, Andr?e FT Martins, and Noah A Smith. 2012. An exact dual decomposition algorithm for shallow
semantic parsing with constraints. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Evaluation, pages 209?217. Association for Computational
Linguistics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Meeting of the
European Chapter of the Association for Computational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude anno-
tation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers), pages 120?125, Sofia, Bulgaria, August. Association for Computational Linguistics.
87
Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using
integer programming. In Human Language Technologies 2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236?243,
Rochester, New York, April. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In Proceedings of the 51th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2013. A computational model for plot units. Computational
Intelligence, 29(3):466?488.
Herbert Paul Grice. 1967. Logic and conversation. The William James lectures.
Herbert Paul Grice. 1989. Studies in the Way of Words. Harvard University Press.
Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Compu-
tational Linguistics, 39(3).
Andr?e F. T. Martins and Noah a. Smith. 2009. Summarization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing
- ILP ?09, pages 1?9, Morristown, NJ, USA. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th international conference on Computational Linguistics,
page 1346. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics, 34(2):257?287.
Kevin Reschke and Pranav Anand. 2011. Extracting contextual evaluativity. In Proceedings of the Ninth Interna-
tional Conference on Computational Semantics, IWCS ?11, pages 370?374, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In CONLL.
Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 226?234, Suntec, Singapore, August. Association for Computational
Linguistics.
P.J. Stone, D.C. Dunphy, M.S. Smith, and D.M. Ogilvie. 1966. The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Corefer-
ence resolution with reconcile. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages
156?161, Stroudsburg, PA, USA. Association for Computational Linguistics.
Janyce Wiebe and Lingjia Deng. 2014. An account of opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation, 39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLP/EMNLP, pages 347?354.
Theresa Wilson. 2008. Fine-grained subjectivity analysis. Ph.D. thesis, Doctoral Dissertation, University of
Pittsburgh.
Bishan Yang and Claire Cardie. 2013. Joint Inference for Fine-grained Opinion Extraction. In Proceedings of
ACL, pages 1640?1649.
Lei Zhang and Bing Liu. 2011. Identifying noun product features that imply opinions. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
575?580, Portland, Oregon, USA, June. Association for Computational Linguistics.
88
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference
Yoonjung Choi and Janyce Wiebe
Department of Computer Science
University of Pittsburgh
yjchoi, wiebe@cs.pitt.edu
Abstract
Recently, work in NLP was initiated on a
type of opinion inference that arises when
opinions are expressed toward events
which have positive or negative effects
on entities (+/-effect events). This paper
addresses methods for creating a lexicon
of such events, to support such work on
opinion inference. Due to significant
sense ambiguity, our goal is to develop a
sense-level rather than word-level lexicon.
To maximize the effectiveness of different
types of information, we combine a
graph-based method using WordNet
1
relations and a standard classifier using
gloss information. A hybrid between the
two gives the best results. Further, we
provide evidence that the model is an
effective way to guide manual annotation
to find +/-effect senses that are not in the
seed set.
1 Introduction
Opinion mining (or sentiment analysis) identifies
positive or negative opinions in many kinds of
texts such as reviews, blogs, and news articles. It
has been exploited in many application areas such
as review mining, election analysis, and infor-
mation extraction. While most previous research
focusses on explicit opinion expressions, recent
work addresses a type of opinion inference that
arises when opinions are expressed toward events
which have positive or negative effects on enti-
ties (Deng et al., 2013; Deng and Wiebe, 2014).
We call such events +/-effect events.
2
Deng and
Wiebe (2014) show how sentiments toward one
1
WordNet 3.0, http://wordnet.princeton.edu/
2
While the term goodFor/badFor is used in previous pa-
pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,
2014), we have since decided that +/-effect is a better term.
entity may be propagated to other entities via
opinion inference rules. They give the following
example:
(1) The bill would curb skyrocketing
health care costs.
The writer expresses an explicit negative senti-
ment (by skyrocketing) toward the object (health
care costs). The event, curb, has a negative effect
on costs, since they are reduced. We can reason
that the writer is positive toward the event because
it has a negative effect on costs, toward which the
writer is negative. From there, we can reason that
the writer is positive toward the bill, since it is
the agent of the positive event. Deng and Wiebe
(2014) show that such inferences may be exploited
to significantly improve explicit sentiment analy-
sis systems.
However, to achieve its results, the system de-
veloped by Deng and Wiebe (2014) requires that
all instances of +/-effect events in the corpus be
manually provided as input. For the system to
be fully automatic, it needs to be able to recog-
nize +/-effect events automatically. This paper
addresses methods for creating lexicons of such
events, to support such work on opinion inference.
We have discovered that there is significant sense
ambiguity, meaning that words often have mix-
tures of senses among the classes +effect, -effect,
and Null. Thus, we develop a sense-level rather
than word-level lexicon.
One of our goals is to investigate whether
the +/-effect property tends to be shared among
semantically-related senses, and another is to
use a method that applies to all word senses, not
just to the senses of words in a given word-level
lexicon. Thus, we build a graph-based model in
which each node is a WordNet sense, and edges
represent semantic WordNet relations between
senses. In addition, we hypothesized that glosses
also contain useful information. Thus, we develop
1181
a supervised gloss classifier and define a hybrid
model which gives the best overall performance.
Finally, because all WordNet verb senses are
incorporated into the model, we investigate the
ability of the method to identify unlabeled senses
that are likely to be +/-effect senses. We find that
by iteratively labeling the top-weighted unlabeled
senses and rerunning the model, it may be used as
an effective method for guiding annotation efforts.
2 Background
There are many varieties of +/-effect events, in-
cluding creation/destruction (changes in states in-
volving existence), gain/loss (changes in states
involving possession), and benefit/injury (Anand
and Reschke, 2010; Deng et al., 2013). The cre-
ation, gain, and benefit classes are +effect events.
For example, baking a cake has a positive effect on
the cake because it is created;
3
increasing the tax
rate has a positive effect on the tax rate; and com-
forting the child has a positive effect on the child.
The antonymous classes of each are -effect events:
destroying the building has a negative effect on the
building; demand decreasing has a negative effect
on demand; and killing Bill has a negative effect
on Bill.
4
While sentiment (Esuli and Sebastiani, 2006;
Wilson et al., 2005; Su and Markert, 2009) and
connotation lexicons (Feng et al., 2011; Kang et
al., 2014) are related, sentiment, connotation, and
+/-effects are not the same; a single event may
have different sentiment and +/-effect polarities,
for example. Consider the following example:
perpetrate:
S: (v) perpetrate, commit, pull (perform
an act, usually with a negative connota-
tion) ?perpetrate a crime?; ?pull a bank
robbery?
This sense of perpetuate has a negative
connotation, and is an objective term in
SentiWordNet. However, it has a positive
effect on the object, a crime, since performing a
crime brings it into existence.
3
Deng et al. (2013) point out that +/-effect objects are not
equivalent to benefactive/malefactive semantic roles. An ex-
ample they give is She baked a cake for me: a cake is the ob-
ject of the +effect event baked as just noted, while me is the
filler of its benefactive semantic role (Ziga and Kittil, 2010).
4
Their annotation manual, which gives additional cases, is
available with the annotated data at http://mpqa.cs.pitt.edu/.
As we mentioned, the +/-effect ambiguity can-
not be avoided in a word-level lexicon. In the
+/-effect corpus of Deng et al. (2013),
5
+/-effect
events and their agents and objects are annotated
at the word level. In that corpus, 1,411 +/-effect in-
stances are annotated; 196 different +effect words
and 286 different -effect words appear in these
instances. Among them, 10 words appear in
both +effect and -effect instances, accounting for
9.07% of all annotated instances. They show that
+/-effect events (and the inferences that motivate
this work) appear frequently in sentences with ex-
plicit sentiment. Further, all instances of +/-effect
words that are not identified as +/-effect events are
false hits from the perspective of a recognition sys-
tem.
The following is an example of a word with
senses of different classes:
purge:
S: (v) purge (oust politically) ?Deng
Xiao Ping was purged several times
throughout his lifetime? -effect
S: (v) purge (clear of a charge) +effect
S: (v) purify, purge, sanctify (make pure
or free from sin or guilt) ?he left the
monastery purified? +effect
S: (v) purge (rid of impurities) ?purge
the water?; ?purge your mind? +effect
This is part of the WordNet output for the word
purge. In the first sense, the polarity is -effect
since it has a negative effect on the object, Deng
Xizo Ping. However, the other cases have positive
effect on the object. Moreover, although a word
may not have both +effect and -effect senses, it
may have mixtures of ((+effect or -effect) and
Null). A purely word-based approach is blind to
these cases.
3 Related Work
Lexicons are widely used in sentiment analysis
and opinion mining. Several works such as Hatzi-
vassiloglou and McKeown (1997), Turney and
Littman (2003), Kim and Hovy (2004), Strappar-
ava and Valitutti (2004), and Peng and Park (2011)
have tackled automatic lexicon expansion or ac-
quistion. However, in most such work, the lexi-
cons are word-level rather than sense-level.
5
Called the goodFor/badFor corpus in that paper.
1182
For the related (but different) tasks of de-
veloping subjectivity, sentiment and connota-
tion lexicons, some do take a sense-level ap-
proach. Esuli and Sebastiani (2006) construct
SentiWordNet. They assume that terms with
the same polarity tend to have similar glosses. So,
they first expand a manually selected seed set of
senses using WordNet lexical relations such as
also-see and direct antonymy and train two clas-
sifiers, one for positive and another for negative.
As features, a vectorial representation of glosses
is adopted. These classifiers were applied to all
WordNet senses to measure positive, negative, and
objective scores. In extending their work (Esuli
and Sebastiani, 2007), the PageRank algorithm is
applied to rank senses in terms of how strongly
they are positive or negative. In the graph, each
sense is one node, and two nodes are connected
when they contain the same words in their Word-
Net glosses. Moreover, a random-walk step is
adopted to refine the scores in their recent work
(Baccianella et al., 2010). In contrast, our ap-
proach uses WordNet relations and graph propa-
gation in addition to gloss classification.
Gyamfi et al. (2009) construct a classifier to la-
bel the subjectivity of word senses. The hierarchi-
cal structure and domain information in WordNet
are exploited to define features in terms of sim-
ilarity (using the LCS metric in Resnik (1995))
of target senses and a seed set of senses. Also,
the similarity of glosses in WordNet is consid-
ered. Even though they investigated the hierarchi-
cal structure by LCS values, WordNet relations are
not exploited directly.
Su and Markert (2009) adopt a semi-supervised
mincut method to recognize the subjectivity of
word senses. To construct a graph, each node cor-
responds to one WordNet sense and is connected
to two classification nodes (one for subjectivity
and another for objectivity) via a weighted edge
that is assigned by a classifier. For this classifier,
WordNet glosses, relations, and monosemous
features are considered. Also, several WordNet
relations (e.g., antonymy, similiar-to, direct
hypernym, etc.) are used to connect two nodes.
Although they make use of both WordNet glosses
and relations, and gloss information is utilized
for a classifier, this classifier is generated only
for weighting edges between sense nodes and
classification nodes, not for classifying all senses.
Kang et al. (2014) present a unified model that
assigns connotation polarities to both words and
senses. They formulate the induction process as
collective inference over pairwise-Markov Ran-
dom Fields, and apply loopy belief propagation
for inference. Their approach relies on selectional
preferences of connotative predicates; the polarity
of a connotation predicate suggests the polarity of
its arguments. We have not discovered an analo-
gous type of predicate for the problem we address.
Goyal et al. (2010) generate a lexicon of patient
polarity verbs (PPVs) that impart positive or neg-
ative states on their patients. They harvest PPVs
from a Web corpus by co-occurance with Kind and
Evil agents and by bootstrapping over conjunc-
tions of verbs. Riloff et al. (2013) learn positive
sentiment phrases and negative situation phrases
from a corpus of tweets with hashtag ?sarcasm?.
However, both of these methods are word-level
rather than sense-level.
Ours is the first NLP research into developing
a sense-level lexicon for events that have negative
or positive effects on entities.
4 +/-Effect Word-Level Seed Lexicon
and Sense Annotations
To create the corpus used in this work, we devel-
oped a word-level seed lexicon, and then manually
annotated all the senses of the words in that lexi-
con.
FrameNet
6
is based on a theory of meaning
called Frame Semantics. In FrameNet, a Lexical
Unit (LU) is a pairing of a word with a meaning,
i.e., it corresponds to a sense in WordNet. Each
LU of a polysemous word belongs to a different
semantic frame, which is a description of a type
of event, relation, or entity and, where appropri-
ate, its participants. For instance, in the Creating
frame, the definition is that a Cause leads to the
formation of a Created entity. It has a positive
effect on the object, Created entity. This frame
contains about 10 LUs such as assemble, create,
yield, and so on. FrameNet consists of about 1,000
semantic frames and about 10,000 LUs.
FrameNet is a useful resource to select +/-effect
words since each semantic frame covers multi-
ple LUs. We believe that using FrameNet to
find +/-effect words is easier than finding +/-effect
words without any information since words may
6
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
1183
be filtered by semantic frames. To select +/-effect
words, an annotator (who is not a co-author) first
identified promising frames as +/-effect and ex-
tracted all LUs from them. Then, he went through
them and picked out the LUs which he judged to
be +effect or -effect. In total, 736 +effect LUs and
601 -effect LUs were selected from 463 semantic
frames.
While Deng et al. (2013) and Deng and Wiebe
(2014) specifically focus on events affecting ob-
jects (i.e., themes), we do not want to limit the
lexicon to only that case. Sometimes, events have
positive or negative effects on agents or other en-
tities as well. Thus, in this paper, we consider
a sense to be +effect (-effect) if it has +effect
(-effect) on an entity, which may be the agent, the
theme, or some other entity.
In a previous paper (Choi et al., 2014), we con-
ducted a study of the sense-level +/-effect prop-
erty. For the evaluation, two annotators (who
are co-authors of that paper) independently anno-
tated senses of selected words, where some are
from pure +effect (-effect) words (i.e., all senses
of the words are classified into the same class)
and some are from mixed words (i.e., the words
have both +effect and -effect senses). In the agree-
ment study, we calculated percent agreement and
? (Artstein and Poesio, 2008), and achieved 0.84
percent agreement and 0.75 ? value.
For a seed set and an evaluation set in this pa-
per, we need annotated sense-level +/-effect data.
Mappings between FrameNet and WordNet are
not perfect. Thus, we opted to manually anno-
tate the senses of the words in the word-level lexi-
con. We first extracted all words from 736 +effect
LUs and 601 -effect LUs; this extracts 606 +effect
words and 537 -effect words (the number of words
is smaller than the number of LUs because one
word can have more than one LU). Among them,
14 words (e.g., crush, order, etc.) are in both the
+effect word set and the -effect word set. That is,
these words have both +effect and -effect mean-
ings. Recall that this annotator was focusing on
frames, not on words - he did not look at all the
senses of all the words. As we will see just below,
when all the senses of all the words are annotated,
a much higher percentage of the words have both
+effect and -effect senses. We will also see that
many of the senses are revealed to be Null, show-
ing that +effect vs. Null and -effect vs. Null ambi-
guities are quite prevalent.
A different annotator (a co-author) then went
through all senses of all the words from the pre-
vious step and manually annotated each sense as
to whether it is +effect, -effect, or Null. Note that
this annotator participated in an agreement study
with positive results in Choi et al. (2014).
For the experiments in this paper, we divided
this annotated data into two equal-sized sets. One
is a fixed test set that is used to evaluate both the
graph model and the gloss classifier. The other set
is used as a seed set by the graph model, and as a
training set by the gloss classifer. Table 1 shows
the distribution of the data. In total, there are 258
+effect senses, 487 -effect senses, and 880 Null
senses. To avoid too large a bias toward the Null
class,
7
we randomly chose half (i.e., the Null set
contains 440 senses). Half of each set is used as
seed and training data, and the other half is used
for evaluation.
+effect -effect Null
# annotated data 258 487 880
# Seed/TrainSet 129 243 220
# TestSet 129 244 220
Table 1: Distribution of annotated data.
5 Graph-based Semi-Supervised
Learning for WordNet Relations
WordNet (Miller et al., 1990) is organized by se-
mantic relations such as hypernymy, troponymy,
grouping, and so on. These semantic relations can
be used to build a network. Since the most fre-
quently encoded relation is the super-subordinate
relation, most verb senses are arranged into hi-
erarchies; verb senses towards the bottom of the
graph express increasingly specific manner. Thus,
by following this hierarchical information, we hy-
pothesized that +/-effect polarity tends to propa-
gate. We use a graph-based semi-supervised learn-
ing (GSSL) method to carry out the label propaga-
tion.
5.1 Graph Formulation
We formulate a graph for semi-supervised learning
as follows. Let G = {X,E,W} be the undirected
graph in which X is the set of nodes, E is the set
7
As mentioned in the introduction, we want our method
to be able to identify unlabeled senses that are likely to be
+/-effect senses (see Section 8); we resize the Null class to
support this goal.
1184
of edges, and W represents the edge weights (i.e.,
the weight of edge E
ij
is W
ij
). The weight matrix
is a non-negative matrix.
Each data point in X = {x
1
, ... ,x
n
} is one
sense. The labeled data of X is represented as
X
L
= {x
1
, ... ,x
l
} and the unlabeled data is rep-
resented as X
U
= {x
l+1
, ... ,x
n
}). The labeled
data X
L
is associated with labels Y
L
= {y
1
, ...
,y
l
}, where y
i
? {1, ..., c} (c is the number of
classes). As is typical in such settings, l  n:
n is 13,767, i.e., the number of verb senses in
WordNet. Seed/TrainSet in Table 1 is the labeled
data.
To connect two nodes, WordNet relations are
utilized. We first connect nodes by the hierar-
chical relations. Since hypernym relations repre-
sent more general senses and troponym relations
represent more specific verb senses, we hypothe-
sized that hypernyms or troponyms of a verb sense
tends to have its same polarity. Verb groups rela-
tions that represent verb senses having a similar
meaning are also promising. Even though verb-
group coverage is not large, its relations are reli-
able since they are manually grouped. The entail-
ment relation is defined as the verb Y is entailed
by X if you must be doing Y by doing X . Since
pairs connected by this relation are co-extensive,
we can assume that both are the same type of
event. The synonym relation is not used because
it is already defined in senses (i.e., each node in
the graph is a synset), and the antonym relation is
also not applied since the weight matrix should be
non-negative. The weight value of all edges is 1.0.
5.2 Label Propagation
Given a constructed graph, the label inference (or
prediction) task is to propagate the seed labels to
the unlabeled nodes. One of the classic GSSL la-
bel propagation methods is the local and global
consistency (LGC) method suggested by Zhou et
al. (2004). The LGC method is a graph transduc-
tion algorithm which is sufficiently smooth with
respect to the intrinsic structure revealed by known
labeled and unlabeled data. The cost function typ-
ically involves a tradeoff between the smoothness
of the predicted labels over the entire graph and
the accuracy of the predicted labels in fitting the
given labeled nodes X
L
. LGC fits in a univariate
regularization framework, where the output ma-
trix is treated as the only variable in optimization,
and the optimal solutions can be easily obtained by
solving a linear system. Thus, we adopt the LGC
method in this paper. Although there are some ro-
bust GSSL methods for handling noisy labels, we
do not need to handle noisy labels because our in-
put is the annotated data.
Let F be a n ? c matrix to save the output
values of label propagation. So, we can label
each instance x
i
as a label y
i
= argmax
j?c
F
ij
after the label propagation. The initial discrete la-
bel matrix Y , which is also n ? c, is defined as
Y
ij
= 1 if x
i
is labeled as y
i
= j in Y
L
, and
Y
ij
= 0 otherwise. The vertex degree matrix
D = diag([D
11
, ..., Dnn]) is defined by D
ii
=
?
n
j=1
W
ij
.
LGC defines the cost function Q which inte-
grates two penalty components, global smooth-
ness and local fitting (? is the regularization pa-
rameter):
Q =
1
2
n
?
i=1
n
?
j=1
W
ij
?
F
i
?
D
ii
?
F
j
?
D
jj
?
2
+?
n
?
i=1
?F
i
? Y
i
?
2
The first part of the cost function is the
smoothness constraint: a good classifying func-
tion should not change too much between nearby
points. That is, if x
i
and x
j
are connected with
an edge, the difference between them should be
small. The second is the fitting constraint: a good
classifying function should not change too much
from the initial label assignment. The final label
prediction matrix F can be obtained by minimiz-
ing the cost function Q.
5.3 Experimental Results
Note that, in the rest of this paper, all tables except
the last one give results on the same fixed test set
(TestSet in Table 1).
We can apply the graph model in two ways.
? UniGraph: All three classes (+effect, -effect,
and Null) are represented in one graph.
? BiGraph: Two separate graphs are first con-
structed and then combined. One graph is for
classifying +effect and Other (i.e., -effect or
Null). This graph is called +eGraph. The
other graph, called -eGraph, is for classify-
ing -effect and Other (i.e., +effect or Null).
1185
UniGraph BiGraph BiGraph*
baseline-
0.411
accuracy
accuracy 0.630 0.623 0.658
+effect P 0.621 0.610 0.642
R 0.655 0.647 0.680
F 0.637 0.628 0.660
-effect P 0.644 0.662 0.779
R 0.720 0.677 0.612
F 0.680 0.670 0.686
Null P 0.615 0.583 0.583
R 0.516 0.550 0.695
F 0.561 0.561 0.634
Table 2: Results of UniGraph, BiGraph, and Bi-
Graph*.
These are combined into one model as fol-
lows. Nodes that are labeled as +effect by
+eGraph and Other by -eGraph are regarded
as +effect, and nodes that are labeled as
-effect by -eGraph and Other by +eGraph are
regarded as -effect. If nodes are labeled as
+effect by +eGraph and -effect by -eGraph,
they are deemed to be Null. Nodes that are
labeled Other by both graphs are also consid-
ered as Null.
We had two motivations for experimenting
with the BiGraph model: (1) SVM, the super-
vised learning method used for gloss classifica-
tion, tends to have better performance on binary
classification tasks, and (2) the two graphs of the
combined model can ?negotiate? with each other
via constraints.
In Table 2, we calculate precision (P), recall (R),
and f-measure (F) for all three classes. The base-
line shown in the top row is the accuracy of a ma-
jority class classifier. The first two columns of Ta-
ble 2 show the results of UniGraph and BiGraph
when they are built using the hypernym, troponym,
and verb group relations. UniGraph outperforms
BiGraph in this experiment.
To improve the results by performing some-
thing possible with BiGraph (but not UniGraph),
constraints are added when determining the class.
As we explained, the label of instance x
i
is
determined by F
i
in the graph. When the label
of x
i
is decided to be j, we can say that its con-
fidence value is F
ij
. There are two constraints as
follows.
H+T +V +E
+effect P 0.653 0.642 0.651
R 0.660 0.680 0.683
F 0.656 0.660 0.667
-effect P 0.784 0.779 0.786
R 0.547 0.612 0.604
F 0.644 0.686 0.683
Null P 0.557 0.583 0.564
R 0.735 0.695 0.691
F 0.634 0.634 0.621
Table 3: Effect of each relation
? If a sense is labeled as +effect (-effect), but
the confidence value is less than a threshold,
we count it as Null.
? If a sense is labeled as both +effect and -effect
by BiGraph, we choose the label with the
higher confidence value only if the higher one
is larger than a threshold and the lower one is
less than a threshold.
The thresholds are determined on Seed/TrainSet
by running BiGraph several times with different
thresholds, and choosing the one that gives the
best performance on Seed/TrainSet. (The chosen
value is 0.025 for +effect and 0.03 for -effect).
As can be seen in Table 2, BiGraph with con-
straints (called BiGraph*) outperforms not only
BiGraph without any constraints but also Uni-
Graph. Especially, for BiGraph*, the recall of the
Null class is considerably increased, showing that
constraints not only help overall, but are particu-
larly important for detecting Null cases.
Table 3 gives ablation results, showing the con-
tribution of each WordNet relation in BiGraph*.
With only hierarchical information (i.e., hyper-
nym (H) and troponym (T) relations), it already
shows good performance for all classes. How-
ever, they cannot cover some senses. Among the
13,767 verb senses in WordNet, 1,707 (12.4%)
cannot be labeled because there are not sufficient
hierarchical links to propagate polarity informa-
tion. When adding the verb group (+V) rela-
tion, it shows improvement in both +effect and
-effect. Especially, the recall for +effect and
-effect is significantly increased. In addition, the
coverage of the 13,767 verb senses increases to
95.1%. For entailment (+E), whereas adding it
shows a slight improvement in +effect (and in-
creases coverage by 1.1 percentage points), the
1186
performance is decreased a little bit in the -effect
and Null classes. Since the average f-measure for
all classes is the highest with hypernym (H), tro-
ponym (T), and verb group (V) relations (not en-
tailment), we only consider these three relations
when constructing the graph.
6 Supervised Learning applied to
WordNet Glosses
In WordNet, each sense contains a gloss consist-
ing of a definition and optional example sentences.
Since a gloss consists of several words and there
are no direct links between glosses, we believe that
a word vector representation is appropriate to uti-
lize gloss information as in Esuli and Sebastiani
(2006). For that, we adopt an SVM classifier.
6.1 Features
Two different feature types are used.
Word Features (WF): The bag-of-words
model is applied. We do not ignore stop words
for several reasons. Since most definitions and ex-
amples are not long, each gloss contains a small
number of words. Also, among them, the total vo-
cabulary of WordNet glosses is not large. More-
over, some prepositions such as against are some-
times useful to determine the polarity (+effect or
-effect).
Sentiment Features (SF): Some glosses of
+effect (-effect) senses contain positive (negative)
words. For instance, the definition of {hurt#4,
injure#4} is ?cause damage or affect negatively.?
It contains a negative word, negatively. Since a
given event may positively (negatively) affect enti-
ties, some definitions or examples already contain
positive (negative) words to express this. Thus, as
features, we check how many positive (negative)
words a given gloss contains. To detect sentiment
words, the subjectivity lexicon provided by Wil-
son et al. (2005)
8
is utilized.
6.2 Gloss Classifier
We have three classes, +effect, -effect, and Null.
Since SVM shows better performance on binary
classification tasks, we generate two binary clas-
sifiers, one (+eClassifier) to determine whether
a given sense is +effect or Other, and another
(-eClassifier) to classify whether a given sense is
-effect or Other. Then, they are combined as in
BiGraph.
8
Available at http://mpqa.cs.pitt.edu/
6.3 Experimental Results
Seed/TrainSet in Table 1 is used to train the two
classifiers, and TestSet is utilized for the evalua-
tion. So, the training set for +eClassifier consists
of 129 +effect instances and 463 Other instances,
and the training set for -eClassifier contains 243
-effect instances and 349 Other instances. As a
baseline, we adopt a majority class classifier.
Table 4 shows the results on TestSet. Perfor-
mance is better for the -effect than for the +effect
class, perhaps because the -effect class has more
instances.
When sentiment features (SF) are added,
all metric values increase, providing evidence
that sentiment features are helpful to determine
+/-effect classes.
WF WF+SF
baseline accuracy 0.411
accuracy 0.509 0.539
+effect P 0.541 0.588
R 0.354 0.393
F 0.428 0.472
-effect P 0.616 0.672
R 0.500 0.511
F 0.552 0.580
Null P 0.432 0.451
R 0.612 0.657
F 0.507 0.535
Table 4: Results of the gloss classifier.
7 Hybrid Method
To use more combined knowledge, the gloss clas-
sifier and BiGraph* can be combined. That is, for
WordNet gloss information, the gloss classifier is
utilized, and for WordNet relations, BiGraph* is
used. With the Hybrid method, we can see not
only the effect of propagation by WordNet rela-
tions but also the usefulness of gloss information
and sentiment features. Also, while BiGraph*
cannot cover all senses in WordNet, the Hybrid
method can.
The outputs of the gloss classifier and Bi-
Graph* are combined as follows. The label of
the gloss classifier is one of +effect, -effect, Null,
or Both (when a given sense is classified as both
+effect by +eClassifier and -effect by -eClassifier).
Possible labels of BiGraph* are +effect, -effect,
Null, Both, or None (when a given sense is not
1187
labeled by BiGraph*). There are five rules:
? If both labels are +effect (-effect), it is +effect
(-effect).
? If one of them is Both and the other is +effect
(-effect), it is +effect (-effect).
? If the label of BiGraph* is None, believe the
label of the gloss classifier
? If both labels are Both, it is Null
? Otherwise, it is Null
The results for Hybrid are given in the first
row of the lower half of Table 5; the results for
BiGraph* are in the first row of the upper half,
for comparison. Generally, the Hybrid method
shows better performance than the gloss classifier
and BiGraph*. In the Hybrid method, since more
+/-effect senses are detected than by BiGraph*,
while precision is decreased, recall is increased
by more. However, by the same token, the over-
all performance for the Null class is decreased.
Actually, that is expected since the Null class is
determined by the Other class in the gloss clas-
sifier and BiGraph*. Through this experiment, we
see that the Hybrid method is better for classifying
+/-effect senses.
7.1 Model Comparison
To provide evidence for our assumption that dif-
ferent models are needed for different information
to maximize effectiveness, we compare the hy-
brid method with the supervised learning and the
graph-based learning (GSSL) methods, each uti-
lizing both WordNet relations and gloss informa-
tion.
Supervised Learning (onlySL): The gloss clas-
sifier is trained with word features and sentiment
features for WordNet Gloss. To exploit Word-
Net relations in supervised learning, especially
the hierarchical information, we use least com-
mon subsumer (LCS) values as in Gyamfi et al.
(2009), which, recall, performs supervised learn-
ing of subjective/objective senses. The values are
calculated as follows. For a target sense t and a
seed set S, the maximum LCS value between a
target sense and a member of the seed set is found
as:
Score(t, S) = max
s?S
LCS(t, s)
With this LCS feature and the features described
in Section 6, we run SVM on the same training and
test data. For LCS values, the similarity using the
information content proposed by Resnik (1995) is
measured. WordNet Similarity
9
package provides
pre-computed pairwise similarity values for that.
Table 6 shows results of onlySL. Compared to
Table 4, while +effect and Null classes show a
slight improvement, the performance is degraded
for -effect. This means that the added feature is
rather harmful to -effect. Even though the hierar-
chical feature is very helpful to expand +/-effect,
it is not helpful for onlySL since SVM cannot cap-
ture propagation according to the hierarchy.
Graph-based Learning (onlyGraph): In Sec-
tion 5, the graph is constructed by using Word-
Net relations. To apply WordNet gloss informa-
tion in onlyGraph, we calculate a cosine similarity
between glosses. If the similarity value is higher
than a threshold, two nodes are connected with this
similarity value. The threshold is determined by
training and testing on Seed/TrainSet (the chosen
value is 0.3).
Comparing Tables 2 and 6, BiGraph* generally
outperforms onlyGraph (the exception is precision
of +effect). By gloss similarity, many nodes are
connected to each other. However, since uncertain
connections can cause incorrect propagation in the
graph, this negatively affects the performance.
Through this experiment, we see that since each
type of information has a different character, we
need different models to maximize the effective-
ness of each type. Thus, the hybrid method with
different models can have better performance.
Hybrid onlySL onlyGraph
+effect P 0.610 0.584 0.701
R 0.735 0.400 0.364
F 0.667 0.475 0.480
-effect P 0.717 0.778 0.651
R 0.669 0.316 0.562
F 0.692 0.449 0.603
Null P 0.556 0.440 0.473
R 0.520 0.813 0.679
F 0.538 0.571 0.557
Table 6: Comparison to onlySL and onlyGraph.
9
WordNet Similarity,
http://wn-similarity.sourceforge.net/
1188
+effect -effect Null
P R F P R F P R F
BiGraph* Initial 0.642 0.680 0.660 0.779 0.612 0.686 0.583 0.695 0.634
1st 0.636 0.684 0.663 0.770 0.632 0.694 0.591 0.672 0.629
2nd 0.642 0.701 0.670 0.748 0.656 0.699 0.605 0.655 0.629
3rd 0.636 0.708 0.670 0.779 0.652 0.710 0.599 0.669 0.632
4th 0.681 0.674 0.678 0.756 0.674 0.712 0.589 0.669 0.626
Hybrid Initial 0.610 0.735 0.667 0.717 0.669 0.692 0.556 0.520 0.538
1st 0.614 0.713 0.672 0.728 0.681 0.704 0.562 0.523 0.542
2nd 0.613 0.743 0.672 0.716 0.697 0.706 0.559 0.497 0.526
3rd 0.616 0.739 0.672 0.717 0.706 0.712 0.559 0.494 0.525
4th 0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545
Table 5: Results of an iterative approach.
8 Guided Annotation
Recall that Seed/TrainSet and TestSet, the data
used so far, are all the senses of the words in a
word-level +/-effect lexicon. This section presents
evidence that our method can guide annotation ef-
forts to find other words that have +/-effect senses.
A bonus is that the method pinpoints particular
+/-effect senses of those words.
All unlabeled data are senses of words that are
not included in the original lexicon. Since pre-
sumably the majority of verbs do not have any
+/-effect senses, a sense randomly selected from
WordNet is very likely to be Null. We explore an
iterative approach to guided annotation, using Bi-
Graph* and Hybrid as the method for assigning
labels.
The system is initially created as described
above using Seed/TrainSet as the initial seed set.
Each iteration has four steps: 1) rank all unlabeled
data (i.e., the data other than TestSet and the cur-
rent seed set) based on the F
ij
confidence values
(see Section 5.3); 2) choose the top 5% and manu-
ally annotate them (the same annotator as above
did this); 3) add them to the seed set; 4) rerun
the system using the expanded seed set. We per-
formed four iterations in this paper.
The upper and lower parts of Table 5 show the
intial results and the results after each iteration for
BiGraph* and Hybrid. Recall that these are results
on the fixed set, TestSet. Overall for both mod-
els, f-measure increases for both the +effect and
-effect classes as more seeds are added, mainly
due to improvements in recall. The evaluation on
the fixed set is also useful in the annotation process
because it trades off +/-effect vs. Null annotations.
If the new manual annotations were biased, in that
they incorrectly label Null senses as +/-effect, then
the f-measure results would instead degrade on the
fixed TestSet, since the system is created each time
using the increased seed set.
We now consider the accuracy of the system
on the newly labeled annotated data in Step 2.
Note that our method is similar to Active Learn-
ing (Tong and Koller, 2001), in that both auto-
matically identify which unlabeled instances the
human should annotate next. However, in active
learning, the goal is to find instances that are diffi-
cult for a supervised learning system. In our case,
the goal is to find needles in the haystack of Word-
Net senses. In Step 3, we add the newly labeled
senses to the seed set, enabling the model to find
unlabeled senses close to the new seeds when the
system is rerun for the next iteration.
We assess the system?s accuracy on the newly
labeled data by comparing the system?s labels with
the human?s new labels. Accuracy for +effect and
-effect is calculated such as:
Accuracy
+effect
=
# annotated +effect
# top 5% +effect data
Accuracy
?effect
=
# annotated -effect
# top 5% -effect data
That is, the accuracy means that out of the top 5%
of the +effect (-effect) data as scored by the sys-
tem, what percentage are correct as judged by a
human annotator. Table 7 shows the accuracy for
each iteration in the top part and the number of
senses labeled in the bottom part. As can be seen,
the accuracies range between 60% and 78%; these
1189
values are much higher than what would be ex-
pected if labeling senses of words randomly cho-
sen from WordNet.
10
The annotator spent, on av-
erage, approximately an hour to label 100 senses.
For finding new words with +/-effect usages, it
would be much more cost-effective if a significant
percentage of the data chosen for annotation are
senses of words that in fact have +/-effect senses.
1st 2nd 3rd 4th
+effect 65.63% 62.50% 63.79% 59.83%
-effect 73.55% 73.97% 77.78% 70.30%
+effect 128 122 116 117
-effect 155 146 153 145
total 283 268 269 262
Table 7: Accuracy and frequency of the top 5% for
each iteration
9 Conclusion and Future Work
In this paper, we investigated methods for creat-
ing a sense-level +/-effect lexicon. To maximize
the effectiveness of each type of information, we
combined a graph-based method using WordNet
relations and a standard classifier using gloss in-
formation. A hybrid between the two gives the
best results. Further, we provide evidence that the
model is an effective way to guide manual anno-
tation to find +/-effect words that are not in the
seed word-level lexicon. This is important, as the
likelihood that a random WordNet sense (and thus
word) is +effect or -effect is not large.
So as not to limit the inferences that may be
drawn, our annotations include events that are
+effect or -effect either the agent or object. In fu-
ture work, we plan to exploit corpus-based meth-
ods using patterns as in Goyal et al. (2010) com-
bined with semantic role labeling to refine the lex-
icon to distinguish which is the affected entity.
Further, to actually exploit the acquired lexicon to
process corpus data, an appropriate coarse-grained
sense disambiguation process must be added, as
Akkaya et al. (2009) and Akkaya et al. (2011) did
for subjective/objective classification.
We hope the general methodology will be ef-
fective for other semantic properties. In opin-
ion mining and sentiment analysis this is partic-
10
For reference, in 5th iteration, the +effect accuracy is
60.18% and the -effect accuracy is 69.93%, and in 6th itera-
tion, the +effect accuracy is 59.81% and the -effect accuracy
is 69.12%.
ularly needed, because different meanings of pos-
itive and negative are appropriate for different ap-
plications. This is a way to create lexicons that are
customized with respect to one?s own definitions.
It would be promising to combine our method
with other methods to enable it to find +effect
and -effect senses that are outside the coverage
of WordNet. However, a WordNet-based lexicon
gives a substantial base to build from.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and National Sci-
ence Foundation grant #IIS-0916046. We would
like to thank the reviewers for their helpful sug-
gestions and comments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of sub-
jectivity word sense disambiguation on contextual
opinion analysis. In Proceedings of CoNLL 2011,
pages 87?96.
Pranna Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC, pages 2200?2204.
Yoonjung Choi, Lingjia Deng, and Janyce Wiebe.
2014. Lexical acquisition for opinion inference:
A sense-level lexicon of benefactive and malefac-
tive events. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis (WASSA), pages
107?112. Association for Computational Linguis-
tics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120?125.
1190
Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of implicit
sentiments via implicature constraints. In Proceed-
ings of COLING, page 7988.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of ACL, pages 424?431.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092?
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77?86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10?18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181.
Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin
Choi. 2014. Connotationwordnet: Learning conno-
tation over the word+sense network. In Proceedings
of the 52nd ACL, page 15441554.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367?1373.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235?312.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity. In Proceedings of 14th
IJCAI, pages 448?453.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704?714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083?1086.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of NAACL HLT 2009,
pages 1?9.
Simon Tong and Daphne Koller. 2001. Support vector
machin active learning with applications to text clas-
sification. Journal of Machine Learning Research,
2:45?66.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347?354.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scholkopf. 2004.
Learning with local and global consistency. Ad-
vances in Neural Information Processing Systems,
16:321?329.
Fernando Ziga and Seppo Kittil. 2010. Benefactives
and malefactives, Typological perspectives and case
studies. John Benjamins Publishing.
1191
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269?278,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Iterative Constrained Clustering for Subjectivity Word Sense
Disambiguation
Cem Akkaya, Janyce Wiebe
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is a supervised and application-
specific word sense disambiguation task
disambiguating between subjective and
objective senses of a word. Not sur-
prisingly, SWSD suffers from the knowl-
edge acquisition bottleneck. In this work,
we use a ?cluster and label? strategy to
generate labeled data for SWSD semi-
automatically. We define a new algo-
rithm called Iterative Constrained Cluster-
ing (ICC) to improve the clustering purity
and, as a result, the quality of the gener-
ated data. Our experiments show that the
SWSD classifiers trained on the ICC gen-
erated data by requiring only 59% of the
labels can achieve the same performance
as the classifiers trained on the full dataset.
1 Introduction
Subjectivity lexicons (e.g., (Turney, 2002;
Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al., 2007; Andreevskaia and Bergler,
2008; Agarwal et al., 2009)) play an important
role in opinion, sentiment, and subjectivity
analysis. These systems typically look for the
presence of clues in text. Recently, in (Akkaya
et al., 2009), we showed that subjectivity clues
are fairly ambiguous as to whether they express
subjectivity or not ? words in such lexicons may
have both subjective and objective usages. We
call this problem subjectivity sense ambiguity.
Consider the following sentence containing the
clue ?attack?:
(1) He was attacked by Milosevic for at-
tempting to carve out a new party from the
Socialists.
Knowing that ?attack? is a subjectivity clue with
negative polarity will help a system recognize the
negative sentiment in the sentence. But for (2), the
same information is simply misleading, because
the clue is used with an objective meaning.
(2) A new treatment based on training T-cells
to attack cancerous cells ...
Any opinion analysis system which relies on a
subjectivity lexicon will be misled by subjectiv-
ity clues used with objective senses (false hits).
In (Akkaya et al., 2009), we introduced the task,
Subjectivity Word Sense Disambiguation, which is
to automatically determine which word instances
in a corpus are being used with subjective senses,
and which are being used with objective senses.
SWSD can be considered as a coarse-grained
and application-specific word sense disambigua-
tion task. We showed that sense subjectivity in-
formation about clues can be fed to subjectiv-
ity and sentiment analysis resulting in substantial
improvement for both subjectivity and sentiment
analysis by avoiding false hits.
Although SWSD is a promising tool, it suf-
fers from the knowledge acquisition bottleneck.
SWSD is defined as a supervised task, and fol-
lows a targeted approach common in the WSD lit-
erature for performance reasons. This means, for
each target clue, a different classifier is trained re-
quiring separate training data for each target clue.
It is expensive and time-consuming to obtain an-
notated datasets to train SWSD classifiers limit-
ing scalability. As a countermeasure, in (Akkaya
et al., 2011), we showed that non-expert annota-
tions collected through Amazon Mechanical Turk
(MTurk) can replace expert annotations success-
fully and might be used to apply SWSD on a large
scale.
Although non-expert annotations are cheap and
fast, they still incur some cost. In this work, we
aim to reduce the human annotation effort needed
269
to generate the same amount of subjectivity sense
tagged data by using a ?cluster and label? strategy.
We hypothesize that we can obtain large sets of
labeled data by labelling clusters of instances of a
target word instead of single instances.
The main contribution of this work is a novel
constrained clustering algorithm called Iterative
Constrained Clustering (ICC) utilizing an active
constraint selection strategy. A secondary con-
tribution is a mixed word representation that is a
combination of previously proposed context rep-
resentations. We show that a ?cluster and label?
strategy relying on these two proposed compo-
nents generates training data of good purity. The
resulting data has sufficient purity to train reli-
able SWSD classifiers. SWSD classifiers trained
on only 59% of the data achieve the same perfor-
mance as classifiers trained on 100% of the data,
resulting in a significant reduction in the annota-
tion effort. Our results take SWSD another step
closer to large scale application.
2 Cluster and Label
Our approach is inspired by a method lexicogra-
phers commonly employ to create sense invento-
ries, where they create inventories based on ev-
idence found in corpora. They use concordance
information to mine frequent usage patterns. (Kil-
garriff, 1997) describes this process in detail. A
lexicographer collects usages of a word in cor-
pora and groups them into coherent sets. The in-
stances in a set should have more in common with
each other than with the instances in other sets,
according to the criteria the lexicographer consid-
ers. After generating the sets, the lexicographer
codes each set as a dictionary definition based on
the common attributes of the instances. Our goal
is similar. Instead of generating dictionary defini-
tions, we are only interested in generating coher-
ent sets of usages of a word, so that we can label
each induced set ? with its instances ? to obtain
labeled data for SWSD. Our high-level grouping
criterion is that the instances in a cluster should be
similar subjective (objective) usages of the word.
Training data for an SWSD classifier consists
of instances of the target word tagged as having
a subjective sense (S) or an objective sense (O)
(subjectivity sense tagged data). We train a dif-
ferent SWSD classifier for each target word as in
(Akkaya et al., 2009). Thus, we need a different
training dataset for each target word. Our ultimate
goal is to reduce the human annotation effort re-
quired to create training data for SWSD classifiers.
For this purpose, we utilize a ?cluster and label?
strategy relying on context clustering. Each in-
stance of a word is represented as a feature vector
(i.e., a context vector). The annotation process has
the following steps: (1) cluster the context vectors
of word instances, (2) label the induced clusters
as S or O, (3) propagate the given label to all in-
stances in a cluster.
The induced clusters represent different usage
patterns of a word. Thus, we build more than two
clusters, even though SWSD is a binary task. This
implies that two different instances of a word can
both be subjective, but end up in different clusters,
if they are different usages of the word.
Since we are labelling clusters as a whole, we
will introduce noise in the labeled data. Thus, in
developing the clustering process, we need to min-
imize that noise and find as pure clusters as possi-
ble.
The first step is to define the context representa-
tion of the instances. This is addressed in Section
3. Then, we turn in Section 4.2 to the clustering
process itself.
To evaluate our ?cluster and label? strategy, we
use two gold standard subjectivity sense tagged
datasets.
1
. The first one is called senSWSD gen-
erated in (Akkaya et al., 2009) and the second
one is called mturkSWSD generated in (Akkaya
et al., 2011). They consist of subjectivity sense
tagged data for disjoint sets of 39 and 90 words,
respectively. In this paper, we opt to use the
smaller dataset senSWSD as our development set,
on which we evaluate various context representa-
tions (in Section 3) and our proposed constrained
clustering algorithm (in Section 4.2). Then, on
mturkSWSD, we evaluate the quality of semi-
automatically generated data for SWSD classifi-
cation (in Section 4.3.2).
3 Context Representations
There has been much work on context representa-
tions of words for various NLP tasks. Clustering
word instances in order to discriminate senses of
a word is called Word Sense Discrimination. Con-
text representations for this task rely on two main
types of models: distributional semantic models
(DSM) and feature-based models.
1
Available at http://mpqa.cs.pitt.edu/
corpora
270
(Schutze, 1998), which is still a competi-
tive model for word-sense discrimination by con-
text clustering, relies on a distributional semantic
model (DSM) (Turney and Pantel, 2010; Sahlgren,
2006; Bullinaria and Levy, 2007). A DSM is usu-
ally a word-to-word co-occurrence matrix ? also
called semantic space ? such that each row repre-
sents the distribution of a target word in a large
text corpus. Each row gives the semantic sig-
nature of a word, which is basically a high di-
mensional numeric vector. Note that this high di-
mensional vector represents word types, not word
tokens. Thus, it cannot model a word instance
in context. For token-based treatment, (Schutze,
1998) utilizes a second-order representation by av-
eraging co-occurrence vectors of the words (cor-
responding to rows of the co-occurrence matrix)
that occur in that particular context. It is impor-
tant to note that (Schutze, 1998) uses an addi-
tive model for compositional representation. Re-
cently, in (Akkaya et al., 2012), we found that a
DSM built using multiplicative composition ? pro-
posed by (Mitchell and Lapata, 2010) for a differ-
ent task ? gives better performance than the model
described by (Schutze, 1998).
We test both methods in this paper, using the
same semantic space. The space is built from a
corpus consisting of 120 million tokens. The rows
of the space correspond to word forms and the
columns correspond to word lemmas present in the
corpus. We adopt the parameters for our semantic
space from (Mitchell and Lapata, 2010): window
size of 10 and dimension size of 2000 (i.e., the
2000 most frequent lemmas). We do not filter out
stop words, since they have been shown to be use-
ful for various semantic similarity tasks in (Bulli-
naria and Levy, 2007). We use positive point-wise
mutual information to compute values of the vec-
tor components, which has also been shown to be
favourable in (Bullinaria and Levy, 2007).
Purandere and Pedersen is the prominent repre-
sentative of feature-based models. (Purandare and
Pedersen, 2004) creates context vectors from local
feature representations similar to the feature vec-
tors found in supervised WSD. In this work, we
use the following features from (Mihalcea, 2002)
to build the local feature representation: (1) the
target word itself and its part of speech, (2) sur-
rounding context of 3 words and their part of
speech, (3) the head of the noun phrase, (4) the
first noun and verb before the target word, (5) the
first noun and verb after the target word.
skew local dsm add dsm mul mix rep
average 79.90 80.50 80.50 83.53 85.23
appear-v 53.83 54.85 54.85 57.40 69.39
fine-a 70.07 72.26 70.07 74.45 75.18
interest-n 54.41 54.78 55.88 81.62 81.62
restraint-n 70.45 71.97 75.00 71.21 81.82
Table 1: Evaluation of Various Context Representations
3.1 Evaluation of Context Representations
In this section, we evaluate context representations
for the context clustering task on the subjectivity
sense tagged data, senSWSD. The evaluation is
done separately for each word.
We use the same clustering algorithm for all
context representations: agglomerative hierarchi-
cal clustering with average linkage criteria. In all
our experiments throughout the paper, we fix the
cluster size to 7 as it is done in (Purandare and
Pedersen, 2004). We think that is reasonable num-
ber since SENSEVAL III reports that the average
number of senses per word is 6.47. We choose
cluster purity as our evaluation metric. To com-
pute cluster purity, we assign each cluster to a
sense label, which is the most frequent one in the
cluster. The number of the correctly assigned in-
stances divided by the number of all the clustered
instances gives us cluster purity.
Row 1 of Table 1 holds the cumulative results
over all the words in senSWSD (micro averages).
The table also reports detailed results for 4 sample
selected words from senSWSD. skew stands for
the percentage of the most frequent label. dsm add
is the representation based on (Schutze, 1998),
dsm mul stands for the representation as described
in (Akkaya et al., 2012) and local features is the
local feature representation based on (Purandare
and Pedersen, 2004). The results show that among
dsm mul, dsm add, and local features; dsm mul
performs the best.
When we look at the context clustering re-
sults for single words separately, we observe
that the performance of different representations
vary. There is not a single winner among all
words. Thus, perhaps choosing one single repre-
sentation for all the words is not optimal. Hav-
ing that in mind, we try merging the dsm mul
and local features representations. We leave out
dsm add representation, since both dsm mul and
dsm add rely on the same type of semantic infor-
mation (i.e., a DSM). We hypothesize that the two
271
representations, one relying on a semantic space
and the other relying on local WSD features, may
complement each other.
To merge the representations, we concatenate
the two feature vectors into one. First, however,
we normalize each vector to unit length, since the
individual vectors have different scales and would
have unequal contribution, otherwise. We call this
mixed representation mix rep.
In Table 1, we see that, overall, mix rep per-
forms better than all the other representations. The
improvement is statistically significant at the p <
.05 level on a paired t-test. We observe that, even
when mix rep does not perform the best, it is never
bad. mix rep is the winner or ties for the winner
for 25 out of 39 words. This number is 13, 13, and
15 for dsm add, dsm mul and local features, re-
spectively. For the words for which mix rep is not
the winner, it is, on average, 1.47 points lower than
the winner. This number is 4.22, 6.83, and 7.07
for the others. The results provide evidence that
mix rep is consistently good and reliable. Thus, in
our experiments, mix rep will be our choice as the
context representation.
4 Clustering Process
We now turn to the clustering process. In a ?clus-
ter and label? strategy, in order to be able to label
clusters, we need to annotate some of the instances
in each cluster. Then, we can accept the majority
label found in a cluster as its label. Thus, some
manual labelling is required, preferably a small
amount.
We propose to provide this small amount of an-
notated data prior to clustering, and then perform
semi-supervised clustering. This way the provided
labels will guide the clustering algorithm to gener-
ate the clusters that are more suitable for our end
task, namely clusters where subjective and objec-
tive instances are grouped together.
4.1 Constrained Clustering
Constrained clustering (Grira et al., 2004) also
known as semi-supervised clustering is a recent
development in the clustering literature. In addi-
tion to the similarity information required by un-
supervised clustering, constrained clustering re-
quires pairwise constraints. There are two types
of constraints: (1) must-link and (2) cannot-link
constraints. A must-link constraint dictates that
two instances should be in the same cluster and a
cannot-link dictates that two instances should not
be in the same cluster. In this work, we only con-
sider cannot-links, because of the definition of our
SWSD task. Two instances sharing the same label
do not need to be in the same cluster, since the in-
duced clusters represent different usage patterns of
a word. For example, two instances labeled S need
not be similar to each other. They can be different
usages, both having a subjective meaning. On the
other hand, if two instances are labeled having op-
posing labels, we do not want them to be in the
same cluster. Thus, we utilize cannot-links but not
must-links.
Constraints can be obtained from domain
knowledge or from available instance labels. In
our work, constraints are generated from instance
labels. Each instance pair with opposing labels is
considered to be cannot-linked.
There are two general strategies to incorporate
constraints into clustering. The first is to adapt
the similarity between instances (Xing et al., 2002;
Klein et al., 2002) by adjusting the underlying dis-
tance metric. The main idea is to make the dis-
tance between must-linked instances ? their neigh-
bourhoods ? smaller and the distance between
cannot-linked instances ? their neighbourhoods ?
larger. The second strategy is modifying the clus-
tering algorithm itself so that search is biased to-
wards a partitioning for which the constraints hold
(Wagstaff and Cardie, 2000; Basu et al., 2002;
Demiriz et al., 1999).
Our proposed constrained clustering method re-
lies on some ideas from (Klein et al., 2002). Thus,
we explain it in more detail. (Klein et al., 2002)
utilizes agglomerative hierarchical clustering with
complete-linkage. The algorithm imposes con-
straints by changing the distance matrix accord-
ing to the given constraints. The distances be-
tween must-linked instances are set to 0. That is
not enough by itself, since if two instances are
must-linked, other instances close to them should
also get closer to each other. This means there is
a need to propagate the constraints. This is done
by calculating the shortest path between all the in-
stances and updating the distance matrix accord-
ingly. To impose cannot-links, the distance be-
tween two cannot-linked instances is set to some
large number. The complete-linkage property
indirectly propagates the cannot-link constraints,
since it will not allow two clusters to be merged if
they contain instances that are cannot-linked.
Although previous work report on average sub-
272
stantial improvement in the clustering purity,
(Davidson et al., 2006) shows that even if the
constraints are generated from gold-standard data,
some constraint sets can decrease clustering pu-
rity. The results vary significantly depending on
the specific set of constraints used. To our knowl-
edge, there have been two approaches for select-
ing informative constraint sets (Basu et al., 2004;
Klein et al., 2002). The method described in
(Basu et al., 2004) uses the farthest-first traversal
scheme. That strategy is not suitable in our setting,
since we have only two labels. After selecting
just one instance from both labels, this method be-
comes the same as random selection. The strategy
described in (Klein et al., 2002) is more general.
At first, the hierarchical clustering algorithm fol-
lows in a unconstrained fashion until some moder-
ate number of clusters are remaining. Then, the al-
gorithm starts to request constraints between roots
whenever two clusters are merged.
4.2 Iterative Constrained Clustering
Our proposed algorithm is closely related to (Klein
et al., 2002). We share the same backbone:
(1) the agglomerative hierarchical clustering with
complete-linkage and (2) the mechanism to im-
pose cannot-link constraints described in Section
4.1. For our algorithm, we implement a second
mechanism for imposing constraints proposed by
(Xing et al., 2002) (Section 4.2.1) and use both
mechanisms in combination. We also propose a
novel constraint selection method (Section 4.2.2).
4.2.1 Imposing Constraints
(Klein et al., 2002) imposes cannot-link con-
straints by adjusting the distance between cannot-
linked pairs heuristically and by relying on com-
plete linkage for propagation. Although this ap-
proach was shown to be effective, we believe it
does not make full use of the provided constraints.
We believe that learning a new distance metric will
result in more reliable distance estimates between
all instances. For this purpose, we learn a Maha-
lanobis distance function following the method de-
scribed in (Davis et al., 2007). (Davis et al., 2007)
formulate the problem of distance metric learn-
ing as minimizing the differential relative entropy
between two multivariate Gaussians under con-
straints. Note that using distance metric learning
for imposing constraints was previously proposed
by (Xing et al., 2002). (Xing et al., 2002) pose
metric learning as a convex optimization problem.
The reason we choose the metric learning method
(Davis et al., 2007) over (Xing et al., 2002) is that
it is computationally more efficient.
(Klein et al., 2002) has a favourable property we
want to keep. The constraints are imposed strictly,
meaning that no cannot-linked instances can ap-
pear in the same cluster. I.e., they are hard con-
straints. In the case of metric learning, the con-
straints are not imposed strictly. In a new learned
distance metric, two cannot-linked instances will
be relatively distant, but there is no guarantee they
will not end up in the same cluster. Although we
think that metric learning makes a better use of
provided constraints, we do not want to lose the
benefit of hard constraints. Thus, we use both
mechanisms in combination to impose constraints.
We first learn a Mahalanobis distance based on the
provided constraints. Then, we compute distance
matrix and employ the mechanism proposed by
(Klein et al., 2002) on the learned distance matrix.
4.2.2 Active Constraint Generation
As mentioned before, the choice of the set of con-
straints affects the quality of the end clustering. In
this work, we define a novel method to choose in-
formative instances, which we believe will have
maximum impact on the end cluster quality, when
they are labeled and used to generate constraints
for our task. We use an iterative approach. Each
iteration consists of three steps: (1) generating
clusters by the process described in Section 4.2.1
imposing available constraints, (2) choosing the
most informative instance, considering the cluster
boundaries, and acquiring its label, (3) extending
the available constraints with the ones we generate
from the newly labeled instance.
We consider an instance to be informative if
there is a high probability that the knowledge of
its label may change the cluster boundaries. The
more probable that change is, the more informa-
tive is the instance. The basic idea is that if an
instance is in a cluster holding instances of type
a and it is close to another cluster holding in-
stances of type b, that instance is most likely mis-
clustered. Thus, it should be queried. Our hypoth-
esis is that, in each iteration, the algorithm will
choose the most problematic ? informative ? in-
stance that will end up changing cluster bound-
aries. This will result in each iteration in a more
reliable distance metric, which in return will pro-
vide more reliable estimates of problematic in-
stances in future iterations. The imposed con-
273
Algorithm 1 Iterative Constrained Clustering
1: C = cluster(I)
2: I
{L}
= labelprototypes(C)
3: while
?
?
I
{L}
?
?
< stop do
4: Con = createconstraints(I
{L}
)
5: Matrix
dist
= learnmetric(I,Con)
6: C = constraintedcluster(Matrix
dist
,Con)
7: L = labelmostinformative(C)
8: I
{L}
= I
{L}
? L
9: end while
10: propagatelabels(I
{L}
, C) {C...Clusters; Con...Constraints;
I...Instances; I
{L}
...Labeled Instances; Matrix
dist
...Distance Matrix}
straints will move the clustering in each iteration
towards better separation of S and O instances.
To define informativeness, we define a scoring
function, which is used to score each data point on
its goodness. The lower the score, the more likely
it is that the instance is mis-clustered. Choosing
the data point with the lowest score will likely
change clustering borders in the next iteration.
Our scoring function is based on the silhouette co-
efficient, a popular unsupervised cluster validation
metric to measure goodness (Tan et al., 2005) of
a cluster member. Basically, the silhouette score
assigns a cluster member that is close to another
cluster a lower score, and a cluster member that
is closer to the cluster center a higher score. That
is partly what we want. In addition, we do not
want to penalize a cluster member that is close to
another cluster having members with the same la-
bel. For this purpose, we calculate the silhouette
score only over clusters with an opposing label
(i.e., holding members with an opposing label). In
addition, we consider only instances labeled so far
when computing the score. We call this new coef-
ficient silh
const
. It is computed as follows: (1) for
an instance i, compute its average distance from
the other instances in its cluster x
i
which are al-
ready labeled, (2) for an instance i, compute its
average distance from the labeled instances of the
clusters from an opposing label and take the mini-
mum of these averages y
i
, (3) compute the silhou-
ette coefficient as (y
i
-x
i
) / max(y
i
,x
i
).
The silh
const
coefficient has favourable proper-
ties. First, it scores members that are close to
a cluster with an opposing label lower than the
members that are close to a cluster with the same
label. According to our definition, these mem-
bers are more informative. Figure 1 holds a sam-
ple cluster setting. The shape of a member de-
notes its label and its fill denotes whether or not it
has been queried. In this example, silh
const
scores
3 1 
2 
Figure 1: Behaviour of selection function
members 2 and 3 lower than 1. Thus, member 1
will not be selected, which is the right decision in
this example. Both members 2 and 3 are close to
clusters with an opposing label. In this example
silh
const
scores member 3 lower, which is farther
away from already labeled members in the clus-
ter. Thus, member 3 will be selected to be labeled.
This type of behaviour results in an explorative
strategy.
The active selection strategy proposed by (Klein
et al., 2002) is single pass. Thus, it does not have
the opportunity to observe the complete cluster
structure before choosing constraints. We hypoth-
esize that our strategy will provide more informa-
tive constraints, since it has the advantage of be-
ing able to base the decision of which constraints
to generate on fully observed cluster structure in
each iteration.
We call our proposed algorithm Iterative Con-
strained Clustering (ICC). In our final implemen-
tation, ICC starts by simply clustering the in-
stances without any constraints. The algorithm
queries the label of the prototypical member ?
the member closest to the cluster center ? of each
cluster. Then, the described iterations begin. Al-
gorithm 1 contains the complete ICC algorithm.
Note that line 6 is equivalent to the algorithm of
(Klein et al., 2002).
4.3 Experiments
This section gives details on experiments to evalu-
ate the purity of the semi-automatically generated
subjectivity sense tagged data by our ?cluster and
label? strategy. We carry out detailed analysis to
quantify the effect of the proposed active selec-
tion strategy and of metric learning on the purity
of the generated data. We compare our active se-
lection strategy to random selection and also to
(Klein et al., 2002). The comparison is done on
the senSWSD dataset. SenSWSD consists of three
subsets, SENSEVAL I,II and III. Since we devel-
274
Figure 2: Label Purity ? ICC vs. random selection
oped our active selection algorithm on the SEN-
SEVAL I subset, we use only SENSEVAL II and
III subsets for comparison. We apply ICC to each
word in the comparison set separately, and report
cumulative results for the purity of the generated
data. We report results for different percentages of
the queried data amount (e.g. 10% means that the
algorithm queried 10% of the data to create con-
straints). This way, we obtain a learning curve.
We fix the cluster number to 7 as in the context
representation experiments.
4.3.1 Effect of Active Selection Strategy
Figure 2 holds the comparison of ICC with
silh
const
selection to a random selection baseline.
?majority? stands for majority label frequency in
the dateset. We see that silh
const
performs better
than the random selection. By providing labels to
only 25% of the data, we can achieve 87.67% pure
fully labeled data.
For comparison, we also evaluate the perfor-
mance of (Klein et al., 2002) with their active con-
straint selection strategy as described in Section
4.1. Note that originally (Klein et al., 2002) re-
quests the constraint between two roots. In our
setting, it requests labels of the roots and then gen-
erates constraints from the obtained labels. Since
we have a binary task, querying labels makes more
sense. This has the advantage that more con-
straints from each request are obtained. More-
over, it allows a direct comparison to our algo-
rithm. (Klein et al., 2002) does not use any metric
learning. Thus, we run our algorithm also without
metric learning, in order to compare the effective-
ness of both active selection strategies fairly. In
Figure 3, we see that silh
const
performs better than
the active selection strategy described in (Klein et
al., 2002). We also see that metric learning results
Figure 3: Label Purity ? ICC vs. Klein
Figure 4: SWSD accuracy on ICC generated data
in a big improvement. In addition, metric learn-
ing results in a smoother learning curve, which is
a favourable property for a real-world application.
4.3.2 SWSD on semi-automatically generated
annotations
Now that we have a tool to generate training data
for SWSD, we want to evaluate it on the actual
SWSD task. We want to see if the obtained purity
is enough to create reliable SWSD classifiers. For
this purpose, we test ICC on mturkSWSD dataset.
For each word in our dataset, we conduct 10-
fold cross-validation experiments. ICC is ap-
plied to training folds to label instances semi-
automatically. We train SWSD classifiers on the
generated training fold labels and test the classi-
fiers on the corresponding test fold. We distin-
guish between queried instances and propagated
labels. The queried instances are weighted as
1 and the instances with propagated labels are
weighted by their silh
const
score, since that mea-
sure gives the goodness of an instance. The score
is defined between -1 and 1. This score is normal-
ized between 0 and 1, before it is used as a weight.
SVM classifiers from the Weka package (Witten
and Frank., 2005) with its default settings are used
275
as in (Akkaya et al., 2011).
We implement two baselines. The first is sim-
ple random sampling and the second is uncer-
tainty sampling, which is an active learning (AL)
method. We use ?simple margin? selection as de-
scribed in (Tong and Koller, 2001). It selects, in
each iteration, the instance closest to the decision
boundary of the trained SVM. Each method is run
until it reaches the accuracy of training fully on
the gold-standard data. ICC reaches that bound-
ary when provided only 59% of the labels in the
dataset. For uncertainty sampling and random
sampling, these values are 92% and 100%, respec-
tively. In Figure 4, we see the SWSD accuracy for
different queried data percentages. ?full? stands
for training fully on gold-standard data. We see
that training SWSD on semi-automatically labeled
data by ICC does consistently better than uncer-
tainty sampling and random sampling.
It is surprising to see that uncertainty sampling
overall does not do better than random sampling.
We believe that it might be because of sampling
bias. During AL, as more and more labels are
obtained, the training set quickly diverges from
the underlying data distribution. (Sch?utze et al.,
2006) states that AL can explore the feature space
in such a biased way that it can end up ignoring en-
tire clusters of unlabeled instances. We think that
SWSD is highly prone for the mentioned missed
cluster problem because of its unique nature. As
mentioned, SWSD is a binary task where we dis-
tinguish between subjective and objective usages
of a subjectivity word. Although the classifica-
tion is binary, the underlying usages are grouped
into multiple clusters corresponding to senses of
the word. It is possible that two groups of usages
which are represented quite differently in the fea-
ture space are both subjective or objective. More-
over, one usage group might be closer to a usage
group from the opposing label than to a group with
the same label.
We see that our method reduces the annotation
amount by 36% in comparison to uncertainty sam-
pling and by 41% in comparison to random sam-
pling to reach the performance of the SWSD sys-
tem trained on fully annotated data.
5 Related Work
One related line of research is constrained clus-
tering also known as semi-supervised clustering
(Xing et al., 2002; Wagstaff and Cardie, 2000;
Grira et al., 2004; Demiriz et al., 1999). It has
been applied to various datasets and tasks such
as image and document categorization. To our
knowledge, we are the first to utilize constrained
clustering for a difficult NLP task.
There have been only two previous works se-
lecting constraints for constrained clustering ac-
tively (Basu et al., 2004; Klein et al., 2002). The
biggest difference of our approach is that it is iter-
ative as opposed to single pass.
Active Learning (AL) (Settles, 2009; Settles
and Craven, 2008; Hwa, 2004; Tong and Koller,
2001) builds another important set of related work.
Our method is inspired by uncertainty sampling.
We accomplish active selection in the clustering
setting.
6 Conclusions
In this paper, we explore a ?cluster and la-
bel? strategy to reduce the human annotation ef-
fort needed to generate subjectivity sense-tagged
data. In order to keep the noise in the semi-
automatically labeled data minimal, we investigate
different feature space types and evaluate their ex-
pressiveness. More importantly, we define a new
algorithm called iterative constrained clustering
(ICC) with an active constraint selection strategy.
We show that we can obtain a fairly reliable la-
beled data when we utilize ICC.
We show that the active selection strategy
we propose outperforms a previous approach by
(Klein et al., 2002) for generating subjectivity
sense-tagged data. Training SWSD classifiers on
ICC generated data improves over random sam-
pling and uncertainty sampling (Tong and Koller,
2001). We can achieve on mturkSWSD 36% an-
notation reduction over uncertainty sampling and
41% annotation reduction over random sampling
in order to reach the performance of SWSD clas-
sifiers trained on fully annotated data.
To our knowledge, this work is the first applica-
tion of constrained clustering to a hard NLP prob-
lem. We showcase the power of constrained clus-
tering. We hope that the same ?cluster and label?
strategy will be applicable to Word Sense Disam-
biguation. This will be part of our future work.
7 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
276
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24?
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
190?199, Singapore, August. Association for Com-
putational Linguistics.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2012. Utilizing semantic composition in distribu-
tional semantic models for word sense discrimina-
tion and word sense disambiguation. In ICSC, pages
45?51.
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Over-
coming domain dependence in sentiment tagging.
In Proceedings of ACL-08: HLT, pages 290?298,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Sugato Basu, Arindam Banerjee, and R. Mooney.
2002. Semi-supervised clustering by seeding. In
In Proceedings of 19th International Conference on
Machine Learning (ICML-2002).
Sugato Basu, Arindam Banerjee, and Raymond J.
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In SDM.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
John Bullinaria and Joseph Levy. 2007. Ex-
tracting semantic representations from word
co-occurrence statistics: A computational
study. Behavior Research Methods, 39:510?526.
10.3758/BF03193020.
Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006.
Measuring constraint-set utility for partitional clus-
tering algorithms. In PKDD, pages 115?126.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In Proceedings of the 24th interna-
tional conference on Machine learning, ICML ?07,
pages 209?216, New York, NY, USA. ACM.
Ayhan Demiriz, Kristin Bennett, and Mark J. Em-
brechts. 1999. Semi-supervised clustering using
genetic algorithms. In In Artificial Neural Networks
in Engineering (ANNIE-99, pages 809?814. ASME
Press.
Nizar Grira, Michel Crucianu, and Nozha Boujemaa.
2004. Unsupervised and semi-supervised cluster-
ing: a brief survey. In in A Review of Ma-
chine Learning Techniques for Processing Multime-
dia Content, Report of the MUSCLE European Net-
work of Excellence.
Rebecca Hwa. 2004. Sample selection for statis-
tical parsing. Comput. Linguist., 30(3):253?276,
September.
Adam Kilgarriff. 1997. I dont believe in word senses.
Computers and the Humanities, 31(2):91?113.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the Twentieth International Conference on Compu-
tational Linguistics (COLING 2004), pages 1267?
1373, Geneva, Switzerland.
D. Klein, K. Toutanova, I.T. Ilhan, S.D. Kamvar, and
C. Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceed-
ings of the ACL Workshop on ?Word Sense Dis-
ambiguatuion: Recent Successes and Future Direc-
tions, pages 74?80, July.
R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL
2004), Boston.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2003), pages
105?112, Sapporo, Japan.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Hinrich Sch?utze, Emre Velipasaoglu, and Jan O. Ped-
ersen. 2006. Performance thresholding in practical
text classification. In Proceedings of the 15th ACM
international conference on Information and knowl-
edge management, CIKM ?06, pages 662?671, New
York, NY, USA. ACM.
277
H. Schutze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 1070?1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active Learning Literature Survey.
Technical Report 1648, University of Wisconsin?
Madison.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining, (First Edi-
tion). Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA.
Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. March.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02), pages 417?424, Philadelphia,
Pennsylvania.
Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning (ICML-2000), pages 1103?1110.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal taxonomies for sen-
timent analysis. In Proceedings of CIKM-05, the
ACM SIGIR Conference on Information and Knowl-
edge Management, Bremen, DE.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and
Stuart J. Russell. 2002. Distance metric learning
with application to clustering with side-information.
In NIPS, pages 505?512.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129?136, Sapporo, Japan.
278
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?385,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Sentiment Propagation via Implicature Constraints
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
Opinions may be expressed implicitly via
inference over explicit sentiments and
events that positively/negatively affect en-
tities (goodFor/badFor events). We in-
vestigate how such inferences may be
exploited to improve sentiment analysis,
given goodFor/badFor event information.
We apply Loopy Belief Propagation to
propagate sentiments among entities. The
graph-based model improves over explicit
sentiment classification by 10 points in
precision and, in an evaluation of the
model itself, we find it has an 89% chance
of propagating sentiments correctly.
1 Introduction
Previous research in sentiment analysis and
opinion extraction has largely focused on the
interpretation of explicitly stated opinions. How-
ever, many opinions are expressed implicitly
via opinion implicature (i.e., opinion-oriented
defeasible inference). Consider the following
sentence:
EX(1) The bill would lower health care costs, which would
be a tremendous positive change across the entire health-care
system.
The writer is clearly positive toward the idea of
lowering health care costs. But how does s/he feel
about the costs? If s/he is positive toward the idea
of lowering them, then, presumably, she is nega-
tive toward the costs themselves (specifically, how
high they are). The only explicit sentiment expres-
sion, tremendous positive change, is positive, yet
we can infer a negative attitude toward the object
of the event itself (i.e., health care costs).
Going further, since the bill is the agent of an
event toward which the writer is positive, we may
(defeasibly) infer that the writer is positive toward
the bill, even though there are no explicit senti-
ment expressions describing it.
Now, consider The bill would curb skyrocketing
health care costs. The writer expresses an explicit
negative sentiment (skyrocketing) toward the ob-
ject (health care costs) of the event. Note that
curbing costs, like lowering them, is bad for them
(the costs are reduced). We can reason that, be-
cause the event is bad for something toward which
the writer is negative, the writer is positive toward
the event. We can reason from there, as above,
that the writer is positive toward the bill, since it
is the agent of the positive event.
These examples illustrate how explicit sen-
timents toward one entity may be propagated
to other entities via opinion implicature rules.
The rules involve events that positively or nega-
tively affect entities. We call such events good-
For/badFor (hereafter gfbf )events.
This work investigates how gfbf event interac-
tions among entities, combined with opinion in-
ferences, may be exploited to improve classifica-
tion of the writer?s sentiments toward entities men-
tioned in the text. We introduce four rule schemas
which reveal sentiment constraints among gfbf
events and their agents and objects. Those con-
straints are incorporated into a graph-based model,
where a node represents an entity (agent/object),
and an edge exists between two nodes if the two
entities participate in one or more gfbf events with
each other. Scores on the nodes represent the ex-
plicit sentiments, if any, expressed by the writer
toward the entities. Scores on the edges are based
on constraints derived from the rules. Loopy Be-
lief Propagation (LBP) (Pearl, 1982) is applied to
377
accomplish sentiment propagation in the graph.
Two evaluations are performed. The first shows
that the graph-based model improves over an ex-
plicit sentiment classification system. The second
evaluates the graph-based model itself (and hence
the implicature rules), assessing its ability to cor-
rectly propagate sentiments to nodes whose polar-
ities are unknown. We find it has an 89% chance
of propagating sentiment values correctly.
This is the first paper to address this type of
sentiment propagation to improve sentiment anal-
ysis. To eliminate interference introduced by other
components, we use manually annotated gfbf in-
formation to build the graph. Thus, the evaluations
in this paper are able to demonstrate the promise
of the overall framework itself.
2 Related Work
Much work in sentiment analysis has been on
document-level classification. Since different sen-
timents may be expressed toward different entities
in a document, fine-grained analysis may be more
informative for applications.
However, fine-grained sentiment analysis re-
mains a challenging task for NLP systems. For
fully-automatic systems evaluated on the MPQA
corpus (Wiebe et al., 2005), for example, a recent
paper (Johansson and Moschitti, 2013) reports re-
sults that improve over previous work, yet the F-
measures are in the 40s and 50s.
Most work in NLP addresses explicit sentiment,
but some address implicit sentiment. For example,
(Zhang and Liu, 2011) identify noun product fea-
tures that imply opinions, and (Feng et al., 2013)
identify objective words that have positive or neg-
ative connotations. However, identifying terms
that imply opinions is a different task than senti-
ment propagation between entities. (Dasigi et al.,
2012) search for implicit attitudes shared between
authors, while we address inferences within a sin-
gle text.
Several papers apply compositional semantics
to determine polarity (e.g., (Moilanen and Pul-
man, 2007; Choi and Cardie, 2008; Moilanen et
al., 2010); see (Liu, 2012) for an overview). The
goal of such work is to determine one overall po-
larity of an expression or sentence. In contrast, our
framework commits to a holder having sentiments
toward various events and entities in the sentence,
possibly of different polarities.
The idea of gfbf events in sentiment analysis is
not entirely new. For example, two papers men-
tioned above (Zhang and Liu, 2011; Choi and
Cardie, 2008) include linguistic patterns for the
tasks that they address that include gfbf events,
but they don?t define general implicature rules re-
lating sentiments and gfbf events, agents, and ob-
jects as we do. Recently, in linguistics, Anand
and Reschke (2010; 2011) identify classes of
gfbf terms, and carry out studies involving artifi-
cially constructed gfbf triples and corpus exam-
ples matching fixed linguistic templates. Our work
focuses on gfbf triples in naturally-occurring data
and uses generalized implicature rules. Goyal et
al. (2012) generate a lexicon of patient polar-
ity verbs, which correspond to gfbf events whose
spans are verbs. Riloff et al. (2013) investigate
sarcasm where the writer holds a positive senti-
ment toward a negative situation. However, nei-
ther of these works performs sentiment inference.
Graph-based models have been used for various
tasks in sentiment analysis. Some work (Wang et
al., 2011; Tan et al., 2011) apply LBP on a graph
capturing the relations between users and tweets in
Twitter data . However, they assume the nodes and
the neighbors of nodes share the same sentiments.
In contrast, we don?t assume that neighbors share
the same sentiment, and the task we address is dif-
ferent.
3 Opinion Implicatures
This section describes the opinion-implicature
framework motivating the design of the graph-
based method for sentiment analysis proposed be-
low. The components of the framework are gfbf
events, explicit sentiments, and rules operating
over gfbf events and sentiments.
The definition of a gfbf event is from (Deng et
al., 2013). A GOODFOR event is an event that
positively affects an entity (similarly, for BADFOR
events). (Deng et al., 2013) point out that gfbf ob-
jects are not equivalent to benefactive/malefactive
semantic roles. An example they give is She baked
a cake for me: a cake is the object of GOOD-
FOR event baked (creating something is good for
it (Anand and Reschke, 2010)), while me is the
filler of its benefactive semantic role (Z?u?niga and
Kittil?a, 2010).
Four implicature rule schemas are relevant for
this paper.
1
Four individual rules are covered by
1
Implicatures ?normally accompany the utterances of a
given sentence unless special factors exclude that possibility
378
each schema. sent(?) = ? means that the writer?s
sentiment toward ? is ?, where ? is a GOODFOR
event, a BADFOR event, or the agent or object of
a gfbf event, and ? is either positive or negative
(pos or neg, for short). P? Q is to infer Q from P.
Rule1: sent(gfbf event)? sent(object)
1.1 sent(GOODFOR) = pos? sent(object) = pos
1.2 sent(GOODFOR) = neg? sent(object) = neg
1.3 sent(BADFOR) = pos? sent(object) = neg
1.4 sent(BADFOR) = neg? sent(object) = pos
Rule2: sent(object)? sent(gfbf event)
2.1 sent(object) = pos? sent(GOODFOR) = pos
2.2 sent(object) = neg? sent(GOODFOR) = neg
2.3 sent(object) = pos? sent(BADFOR) = neg
2.4 sent(object) = neg? sent(BADFOR) = pos
Rule3: sent(gfbf event)? sent(agent)
3.1 sent(GOODFOR) = pos? sent(agent) = pos
3.2 sent(GOODFOR) = neg? sent(agent) = neg
3.3 sent(BADFOR) = pos? sent(agent) = pos
3.4 sent(BADFOR) = neg? sent(agent) = neg
Rule4: sent(agent)? sent(gfbf event)
4.1 sent(agent) = pos? sent(GOODFOR) = pos
4.2 sent(agent) = neg? sent(GOODFOR) = neg
4.3 sent(agent) = pos? sent(BADFOR) = pos
4.4 sent(agent) = neg? sent(BADFOR) = neg
To explain the rules, we step through an example:
EX(2) Why would [President Obama] support [health care
reform]? Because [reform] could lower [skyrocketing health
care costs], and prohibit [private insurance companies] from
overcharging [patients].
Suppose a sentiment analysis system recognizes
only one explicit sentiment expression, skyrock-
eting. According to the annotations, there are
several gfbf events. Each is listed below in the
form ?agent, gfbf, object?.
E
1
: ?reform, lower, costs?
E
2
: ?reform, prohibit, E
3
?
E
3
: ?companies, overcharge, patients?
E
4
: ?Obama, support, reform?
In E
1
, from the negative sentiment expressed
by skyrocketing (the writer is negative toward the
(p. 39).? (Huddleston and Pullum, 2002)
costs because they are too high), and the fact that
costs is the object of a BADFOR event (lower),
Rule2.4 infers a positive attitude toward E
1
.
Now, Rule3.3 applies. We infer the writer is
positive toward the reform, since it is the agent
of E
1
, toward which the writer is positive.
E
2
illustrates the case where the object is an
event. Specifically, the object of E
2
is E
3
, a BAD-
FOR event (overcharging). As we can see, E
2
keeps E
3
from happening. Events such as E
2
are REVERSERs, because they reverse the polar-
ity of a gfbf event (from BADFOR to GOODFOR,
or vice versa). Note that REVERSERs may be seen
as BADFOR events, because they make their ob-
jects irrealis (i.e., not happen). Similarly, a RE-
TAINER such as help in ?help Mary save Bill? can
be viewed as a GOODFOR event. (We call a RE-
VERSER or a RETAINER an INFLUENCER.) In this
paper, RETAINERS are treated as GOODFOR events
and REVERSERS are treated as BADFOR events.
Above, we inferred that the writer is positive to-
ward reform, the agent of E
2
. By Rule 4.3, the
writer is positive towardE
2
; then by Rule 1.3, the
writer is negative toward E
3
, the object of E
2
.
For E
3
, using Rule 1.4 we know the writer is
positive toward patients and using Rule 3.4 we
know the writer is negative toward companies.
Turning to E
4
, support health care reform is
GOODFOR reform. We already inferred the writer
is positive toward reform. Rule 2.1 infers that the
writer is positive toward E
4
. Rule 3.1 then infers
that the writer is positive toward the agent of E
4
,
Obama.
In summary, we infer that the writer is positive
toward E
1
, health care reform, E
2
, patients, E
4
,
and Obama, and negative toward E
3
and private
insurance companies.
4 Data
We use the data described in (Deng et al., 2013),
2
which consists of 134 documents about a contro-
versial topic, ?the Affordable Care Act.? The doc-
uments are editorials and blogs, and are full of
opinions.
In the data, gfbf triples are annotated specifying
the spans of the gfbf event, its agent, and its object,
as well as the polarity of the gfbf event (GOODFOR
or BADFOR), and the writer?s attitude toward the
agent and object (positive, negative, or neutral).
Influencers are also annotated. The agents of gfbf
2
Available at http://mpqa.cs.pitt.edu
379
and influencer events are noun phrases. The ob-
ject of a gfbf event is a noun phrase, but the object
of an influencer is a gfbf event or another influ-
encer. A triple chain is a chain of zero or more
influencers ending in a gfbf event, where the ob-
ject of each element of the chain is the following
element in the chain. (e.g. in EX(2), the two event
prohibit and overcharging is a triple chain.)
In total, there are 1,762 annotated gfbf triples,
out of which 692 are GOODFOR or RETAINER
and 1,070 are BADFOR or REVERSER. From the
writer?s perspective, 1,495 noun phrases are anno-
tated positive, 1,114 are negative and the remain-
ing 8 are neutral. This is not surprising, given that
most of the sentences in the data are opinionated.
5 Graph-based Model
We propose a graph-based model of entities and
the gfbf relations between them to enable senti-
ment propagation between entities. In this section,
we introduce the definition of the graph (in 5.1),
the LBP algorithm (in 5.2), and the definition of
its functions for our task (in 5.3 and 5.4).
5.1 Definition of the Entity Graph
We define a gfbf entity graph EG = {N,E},
in which the node set N consists of nodes, each
representing an annotated noun phrase agent or
object span. The edge set E consists of edges,
each linking two nodes if they co-occur in a triple
chain with each other. Consider the triples of
EX(2) in Section 3 below.
E
1
: ?reform, lower, costs?
E
2
: ?reform, prohibit, E
3
?
E
3
: ?companies, overcharge, patients?
E
4
: ?Obama, support, reform?
The node of reform is linked to nodes of costs via
E
1
and Obama via E
4
.
3
Note that, for E
2
and
E
3
, the two are linked in a chain: ?reform, pro-
hibit, ?companies, overcharge, patients? ?. The
three nodes reform, companies and patients partic-
ipate in this triple chain; thus, pairwise edges ex-
ist among them. The edge linking companies and
patients is BADFOR (because of overcharging).
The edge linking reform and companies is also a
BADFOR since we treat a REVERSER as BADFOR.
3
This assumes that the two instances of ?reform? co-refer.
However, the system does not resolve co-reference ? the
methods that we tried did not improve overall performance.
The edge linking reform and patients encodes two
BADFOR events (prohibit-overcharge); computa-
tionally we say two BADFORs result in a GOOD-
FOR, so the edge linking the two is GOODFOR.
4
Given a text, we get the spans of gfbf events
and their agents and objects plus the polarities of
the events (GOODFOR/BADFOR) from the manual
annotations, and then build the graph upon them.
However, the manual annotations of the writer?s
sentiments toward the agents and objects are used
as the gold standard for evaluation.
5.2 Sentiment Inference via LBP
initialize all m
i?j
(pos) = m
i?j
(neg) = 1
repeat
foreach n
i
? N do
foreach n
j
? Neighbor(n
i
) do
foreach y ? pos, neg do
calculate m
i?j
(y)
normalize m
i?j
(pos) + m
i?j
(neg) = 1
until all m
i?j
stop changing;
for each n
i
? N assign its polarity as
argmax
y?pos,neg
?
i
(y) ?
?
n
k
?Neighbor(n
i
)
m
k?i
(y)
neutral, in case of a tie
Table 1: Loopy Belief Propagation
With graph EG containing cycles and no appar-
ent structure, we utilize an approximate collective
classification algorithm, loopy belief propagation
(LBP) (Pearl, 1982; Yedidia et al., 2005), to clas-
sify nodes through belief message passing. The
algorithm is shown in Table 1.
In LBP, each node has a score, ?
i
(y), and each
edge has a score, ?
ij
(y
i
, y
j
). In our case, ?
i
(y)
represents the writer?s explicit sentiment toward
n
i
. ?
ij
(y
i
, y
j
) is the score on edge e
ij
, represent-
ing the likelihood that node n
i
has polarity y
i
and
n
j
has polarity y
j
. The specific definitions of the
two functions are given in Sections 5.3 and 5.4.
LBP is an iterative message passing algorithm.
A message from n
i
to n
j
over edge e
ij
has
two values: m
i?j
(pos) is how much information
from node n
i
indicates node n
j
is positive, and
m
i?j
(neg) is how much information from node
n
i
indicates node n
j
is negative. In each iteration,
the two are normalized such that m
i?j
(pos) +
m
i?j
(neg) = 1. The message from n
i
to its
4
Also, GOODFOR+BADFOR=BADFOR; GOOD-
FOR+GOODFOR=GOODFOR
380
neighbor n
j
is computed as:
m
i?j
(pos) =
?
ij
(pos, pos)??
i
(pos)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(pos)+
?
ij
(neg, pos)??
i
(neg)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(neg)
(1)
m
i?j
(neg) =
?
ij
(neg, neg)??
i
(neg)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(neg)+
?
ij
(pos, neg)??
i
(pos)?
?
n
k
?Neighbor(n
i
)/n
j
m
k?i
(pos)
(2)
For example, the first part of Equation (1)
means that the positive message n
i
conveys to
n
j
(i.e., m
i?j
(pos)) comes from n
i
being pos-
itive itself (?
i
(pos)), the likelihood of edge e
ij
with its nodes n
i
being positive and n
j
being
positive (?
ij
(pos, pos)), and the positive mes-
sage n
i
?s neighbors (besides n
j
) convey to it
(
?
k?Neighbor(n
i
)/n
j
m
k?i
(pos)).
After convergence, the polarity of each node is
determined by its explicit sentiment and the mes-
sages its neighbors convey to it, as shown at the
end of the algorithm in Table 1.
By this method, we take into account both sen-
timents and the interactions between entities via
gfbf events in order to discover implicit attitudes.
Note that the node and edge scores are deter-
mined initially and do not change. Only m
i?j
changes from iteration to iteration.
5.3 ?
ij
(y
i
, y
j
): GFBF Implicature Relations
The score ?
i,j
encodes constraints based on the
gfbf relationships that nodes n
i
and n
j
participate
in, together with the implicature rules given above.
Rule schemas 1 and 3 infer sentiments to-
ward entities (agent/object) from sentiments to-
ward gfbf events. All cases covered by them are
shown in Table 2 (use s(?) to represent sent(?)).
Rule 3 Rule1
s(gfbf) gfbf type ? s(agent) s(object)
pos GOODFOR ? pos pos
neg GOODFOR ? neg neg
pos BADFOR ? pos neg
neg BADFOR ? neg pos
Table 2: Rule 1 & Rule 3
A table of Rule schemas 2 and 4 would be
exactly the same, except that the inference (?)
would be in the opposite direction (?).
From Table 2, we see that, regardless of the
writer?s sentiment toward the event, if the event
is GOODFOR, then the writer?s sentiment toward
the agent and object are the same, while if the
event is BADFOR, the writer?s sentiment toward
the agent and object are opposite. Thus, the event
type and the writer?s sentiments toward the agents
and objects give us constraints. Therefore, we de-
fine ?
ij
(pos, pos) and ?
ij
(neg, neg) to be 1 if the
two nodes are linked by a GOODFOR edge; oth-
erwise, it is 0; and we define ?
ij
(neg, pos) and
?
ij
(pos, neg) to be 1 if the two nodes are linked
by a BADFOR edge; otherwise, it is 0.
5.4 ?
i
(y): Explicit Sentiment Classifier
The score of a node, ?
i
(y), represents the sen-
timent explicitly expressed by the writer toward
that entity in the document. Since y ranges over
(pos, neg), each node has a positive and a nega-
tive score; the scores sum to 1. If it is a positive
node, then its positive value ranges from 0.5 to 1,
and its negative value ranges from 0 to 0.5 (sim-
ilarly for negative nodes). For any node without
explicit sentiment, both the positive and negative
values are 0.5, indicating a neutral node.
Thus, we build a sentiment classifier that takes a
node as input and outputs a positive and a negative
score. It is built from widely-used, freely available
resources: the OpinionFinder (Wilson et al., 2005)
and General Inquirer (Stone et al., 1966) lexicons
and the OpinionFinder system.
5
We also use a new
Opinion Extraction system (Johansson and Mos-
chitti, 2013) that shows better performance than
previous work on fine-grained sentiment analy-
sis,
6
and a new automatically developed connota-
tion lexicon (Feng et al., 2013).
7
We implement a weighted voting method
among these various sentiment resources. After
that, for nodes that have not yet been assigned po-
lar values (positive or negative), we implement a
simple local discourse heuristic to try to assign
them polar values.
The particular strategies were chosen based
only on a separate development set, which is not
5
http://mpqa.cs.pitt.edu and
http://www.wjh.harvard.edu/ inquirer/
6
As evaluated on the MPQA corpus. Note that the authors
ran their system for us on the data we use.
7
http://www.cs.stonybrook.edu/?ychoi/connotation
381
included in the data used in the experiments.
5.4.1 Explicit Sentiment Tools
Opinion Extraction outputs a polarity expression
with its source, and OpinionFinder outputs a po-
larity word. But neither of the tools extracts the
target. To extract the target, for each word in the
opinion expression, we select other words in the
sentence which are in a mod, obj dependency pars-
ing relation with it.
We match up the extracted expressions and the
gfbf annotations according to their offsets in the
text. For an opinion expression appearing in the
sentence with no gfbf annotation, if the root word
(in the dependency parse) of the expression span
is the same as the root word of a gfbf span, or the
root word of an agent span, or the root word of an
object span, we assume they match up. Then we
assign polarity as follows. If the expression refers
only to the agent or object, then the agent or object
is assigned the polarity of the expression. If the
expression covers the gfbf event and its object, we
assume the sentiment is toward the gfbf event and
then assign sentiment according to Rule schema 1
(sent(gfbf event)? sent(object)).
5.4.2 Lexicons
To classify the sentiment expressed within the
span of an agent or object, we check whether the
words in the span appear in one or more of the
lexicons.
8
If a lexicon finds both positive and neg-
ative words in the span, we resolve the conflict
by choosing the polarity of the root word in the
span. If the root word does not have a polar value,
we choose the majority polarity of the sentiment
words. If there are an equal number of positive
and negative words, the polarity is neutral.
5.4.3 Voting Scheme among Resources
All together we have two sentiment systems and
three lexicons. Before explicit sentiment classi-
fying, each node has a positive value of 0.5 and
a negative value of 0.5. We give the five votes
equal weight (0.1), and add the number of posi-
tive votes multiplied by 0.1 to the positive value,
and the number of negative votes multiplied by 0.1
to the negative value. After this addition, both val-
ues are in the range 0.5 to 1. If the positive value
is larger, we maintain the positive value and assign
8
The comparison is done after lemmatization, using the
wordNet lemmatization in NLTK, and with the same POS,
according to the Stanford POStagger toolkit.
the negative value to be 1-positive value (similarly
if the negative value is larger).
5.4.4 Discourse
For a sentence s, we assume the writer?s senti-
ments toward the gfbf events in the clauses of s,
the previous sentence, and the next sentence, are
the same. Consider EX(3):
EX(3) ... health-insurance regulations that will prohibit (a)
denying coverage for pre-existing conditions, (b) dropping
coverage if the client gets sick, and (c) capping insurance
company reimbursement...
EX(3) has three clauses, (a)-(c). Suppose the ex-
plicit sentiment classifier recognizes that event (a),
denying coverage for pre-existing conditions, is
negative and it does not find any other explicit sen-
timents in the sentence. The system assumes the
writer?s sentiments toward (b) and (c) are negative
as well.
After assigning all possible polarities to events
within a sentence, polarities are propagated to the
other still-neutral gfbf events in the previous and
next sentences.
Finally, event-level polarities are propagated to
still-neutral objects using Rule schema 1.
9
If there
is a conflict, we take the majority sentiment; if
there is a tie, the object remains neutral.
However, the confidence of the discourse voting
is smaller than the explicit sentiment voting, since
discourse structure is complex. If by discourse an
object node is classified as positive, the positive
value is 0.5 + random(0, 0.1) and the negative
value is 1-positive value. Thus, the positive value
of a positive node is larger than its negative value,
but not exceeding too much (similarly for negative
nodes).
6 Experiments and Results
6.1 Experiment Data
Of the 134 documents in the dataset, 6 were used
as a development set, and 3 do not have any anno-
tation. We use the remaining 125 for experiment.
6.2 Evaluation Metrics
To evaluate the performance of classifying the
writer?s sentiments toward agents and objects, we
9
Note that, in the gfbf entity graph, sentiments can be
propagated from objects to agents, conceptually via Rule
schemas 2 and 3. Thus, here we only classify objects.
382
define three metrics to evaluate performance. For
the entire dataset, accuracy evaluates the percent-
age of nodes that are classified correctly. Preci-
sion and recall are defined to evaluate polar (non-
neutral) classification.
Accuracy =
#node auto=gold
#nodes
(3)
Precision =
#node auto=gold & gold != neutral
#node auto != neutral
(4)
Recall =
#node auto=gold & gold != neutral
#node gold != neutral
(5)
In the equations, auto is the system?s output and
gold is the gold-standard label from annotation.
6.3 Overall Performance
In this section, we evaluate the performance of
the overall system. In 6.5, we evaluate the graph
model itself.
Two baselines are defined. One is assigning
the majority class label, which is positive, to all
agents/objects (Majority(+)). The second is as-
suming that agents/objects in a GOODFOR relation
are positive and agents/objects in a BADFOR rela-
tion are negative (GFBF ). In addition, we eval-
uate the explicit sentiment classifier introduced in
Section 5.4 (Explicit). The results are shown in
Table 3.
Accuracy Precision Recall
Majority(+) 0.5438 0.5621 0.5443
GFBF 0.5437 0.5523 0.5444
Explicit 0.3703 0.5698 0.3703
Graph-LBP 0.5412 0.6660 0.5419
Table 3: Performance of baselines and graph.
As can be seen,Majority andGFBF give ap-
proximately 56% precision. Explicit sentiment
classification alone performs hardly better in pre-
cision and much lower in recall. As mentioned
in Section 2, fine-grained sentiment analysis is
still very difficult for NLP systems. However, the
graph model improves greatly over Explicit in
both precision and recall. While recall of the graph
model is comparable to the Majority, precision
is much higher.
During the experiment, if the LBP does not con-
verge until 100 iterations, it is forced to stop. The
average number of iteration is 34.192.
6.4 Error Analysis
Table 4 shows the results of an error analysis to
determine what contributes to the graph model?s
errors.
1 wrong sentiment from voting 0.2132
2 wrong sentiment from discourse 0.0462
3 subgraph with wrong polarity 0.3189
4 subgraph with no polarity 0.4160
5 other 0.0056
Table 4: Errors for graph model.
Rows 1-2 are the error sources for nodes as-
signed a polar value before graph propagation.
Row 1 errors are due to the sentiment-voting sys-
tem, Row 2 are due to discourse processing.
Rows 3-4 are the error sources for nodes that
have not been assigned a polar value by Explicit.
Such a node receives a polar value only via prop-
agation from other nodes in its subgraph (i.e., the
connected component of the graph containing the
node). Row 5 is the percentage of other errors.
As shown in Rows 1-2, 25.94% of the errors
are due to Explicit. These may propagate incor-
rect labels to other nodes in the graph. As shown
in Row 3, 31.89% of the errors are due to nodes
not classified polar by Explicit, but given incor-
rect values because their subgraph has an incorrect
polarity. Row 4 shows that 41.60% of the errors
are due to nodes that are not assigned any polar
value. Given non-ideal input from sentiment anal-
ysis, how does the graph model increase precision
by 10 percentage points?
There are two main ways. For nodes which re-
main neutral after Explicit, they might be clas-
sified correctly via the graph. For nodes which
are given incorrect polar labels by Explicit, they
might be fixed by the graph. Table 5 shows the
best the graph model could do, given the noisy in-
put from Explicit. Over all of the nodes, more
propagated labels are incorrect than correct. How-
ever, if there are no incorrect, or more correct than
incorrect sentiments in the subgraph (connected
component), then many more of the propagated la-
bels are correct than incorrect. In all cases, more
of the changed labels are correct than incorrect.
6.5 Consistency and Isolated Performance of
Graph Model
The implicature rules are defeasible. In this sec-
tion we introduce an experiment to valid the con-
383
propagated propagated changed changed
label correct label incorrect correctly incorrectly
all subgraphs
399 536 424 274
subgraphs having no incorrect sentiment
347 41 260 23
subgraphs having more correct than incorrect sentiment
356 42 288 35
Table 5: Effects of graph model given Explicit
input
sistency of implicature rule. Recall that in Section
5.3, the definition of ?
i,j
is based on implicature
rules and sentiment is propagated based on ?
i,j
.
Thus, this is also an evaluation of the performance
of the graph model itself. We performed an experi-
ment to assess the chance of a node being correctly
classified only via the graph.
In each subgraph (connected component), we
assign one of the nodes in the subgraph with its
gold-standard polarity. Then we run LBP on the
subgraph and record whether the other nodes in
the subgraph are classified correctly or not. The
experiment is run on the subgraph |S| times, where
|S| is the number of nodes in the subgraph, so
that each node is assigned its gold-standard polar-
ity exactly once. Each node is given a propagated
value |S| ? 1 times, as each of the other nodes in
its subgraph receives its gold-standard polarity.
To evaluate the chance of a node given a correct
propagated label, we use Equations (6) and (7).
correct(a|b) =
{
1 a is correct
0 otherwise
(6)
correctness(a) =
?
b?S
a
,b6=a
correct(a|b)
|S
a
| ? 1
(7)
where S
a
is the set of nodes in a?s subgraph. Given
b being assigned its gold-standard polarity, if a is
classified correctly, then correct(a|b) is 1; other-
wise 0. |S
a
| is the number of nodes in a?s sub-
graph. correctness(a) is the percentage of as-
signments to a that are correct. If it is 1, then a
is correctly classified given the correct classifica-
tion of any single node in its subgraph.
For example, suppose there are three nodes in
a subgraph, A, B and C. For A we (1) as-
sign B its gold label and carry out propagation
on the subgraph, (2) assign C its gold label and
carry out propagation again, then (3) calculate
correctness(A). Then the same process is re-
peated for B and C.
Some subgraphs contain only two nodes, the
agent and the object. In this case, graph propa-
gation corresponds to single applications of two
implicature rules. Other subgraphs contain more
nodes. Two results are shown in Table 6. One is
the result on the whole experiment data, the other
is the result for all nodes whose subgraphs have
more than two nodes.
Dataset # subgraph correctness
all subgraphs 983 0.8874
multi-node subgraphs 169 0.9030
Table 6: Performance of graph model itself.
As we can see, a node has an 89% chance of
being correct if there is one correct explicit sub-
jectivity node in its subgraph. If we only consider
subgraphs with more than two nodes, the correct-
ness chance is higher. The results indicate that, if
given correct sentiments, the graph model will as-
sign the unknown nodes with correct labels 90% of
the time. Further, the results indicate that the im-
plicature rules are consistent for most of the times
across the corpus.
7 Conclusions
We developed a graph-based model based on
implicature rules to propagate sentiments among
entities. The model improves over explicit
sentiment classification by 10 points in precision
and, in an evaluation of the model itself, we find
it has an 89% chance of propagating sentiments
correctly. An important question for future work
is under what conditions do the implicatures
not go through in context. Two cases we have
discovered involve Rule schema 3: the inference
toward the agent is defeated if the action was
accidental or if the agent was forced to perform it.
We are investigating lexical clues for recognizing
such cases.
Acknowledgments. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank
Richard Johansson and Alessandro Moschitti for
running their Opinion Extraction systems on our
data.
384
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 793?801, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude us-
ing textual latent semantics. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 65?69, Jeju Island, Korea, July. Association
for Computational Linguistics.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive and malefactive event and writer
attitude annotation. In 51st Annual Meeting of the
Association for Computational Linguistics (ACL-
2013, short paper).
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Sofia, Bulgaria, Angust. Association for Com-
putational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages 466?488.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Karo Moilanen, Stephen Pulman, and Yue Zhang.
2010. Packed feelings and ordered sentiments:
Sentiment parsing with quasi-compositional polar-
ity sequencing and compression. In Proceedings of
the 1st Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2010),
pages 36?43.
J. Pearl. 1982. Reverend bayes on inference engines:
A distributed hierarchical approach. In Proceedings
of the American Association of Artificial Intelligence
National Conference on AI, pages 133?136, Pitts-
burgh, PA.
Kevin Reschke and Pranav Anand. 2011. Extracting
contextual evaluativity. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, IWCS ?11, pages 370?374, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 704?714, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
P.J. Stone, D.C. Dunphy, M.S. Smith, and D.M.
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press, Cam-
bridge.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment
analysis incorporating social networks. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1397?1405. ACM.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming zhou,
and Ming Zhang. 2011. Topic sentiment anaylsis in
twitter: A graph-based hashtag sentiment classifica-
tion appraoch. In CIKM, pages 1031?1040.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ann. Language Resources and
Evaluation, 39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP, pages
347?354.
Jonathan S Yedidia, William T Freeman, and Yair
Weiss. 2005. Constructing free-energy approx-
imations and generalized belief propagation algo-
rithms. Information Theory, IEEE Transactions on,
51(7):2282?2312.
Lei Zhang and Bing Liu. 2011. Identifying noun prod-
uct features that imply opinions. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 575?580, Portland, Oregon, USA, June.
Association for Computational Linguistics.
F. Z?u?niga and S. Kittil?a. 2010. Introduction. In
F. Z?u?niga and S. Kittil?a, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
385
Tutorial Abstracts of ACL 2012, page 4,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Multilingual Subjectivity and Sentiment Analysis
Rada Mihalcea
University of North Texas
Denton, Tx
rada@cs.unt.edu
Carmen Banea
University of North Texas
Denton, Tx
carmenbanea@my.unt.edu
Janyce Wiebe
University of Pittsburgh
Pittsburgh, Pa
wiebe@cs.pitt.edu
Abstract
Subjectivity and sentiment analysis focuses on
the automatic identification of private states,
such as opinions, emotions, sentiments, evalu-
ations, beliefs, and speculations in natural lan-
guage. While subjectivity classification labels
text as either subjective or objective, sentiment
classification adds an additional level of gran-
ularity, by further classifying subjective text as
either positive, negative or neutral.
While much of the research work in this
area has been applied to English, research
on other languages is growing, including
Japanese, Chinese, German, Spanish, Ro-
manian. While most of the researchers in
the field are familiar with the methods ap-
plied on English, few of them have closely
looked at the original research carried out in
other languages. For example, in languages
such as Chinese, researchers have been look-
ing at the ability of characters to carry sen-
timent information (Ku et al, 2005; Xiang,
2011). In Romanian, due to markers of po-
liteness and additional verbal modes embed-
ded in the language, experiments have hinted
that subjectivity detection may be easier to
achieve (Banea et al, 2008). These addi-
tional sources of information may not be avail-
able across all languages, yet, various arti-
cles have pointed out that by investigating a
synergistic approach for detecting subjectiv-
ity and sentiment in multiple languages at the
same time, improvements can be achieved not
only in other languages, but in English as
well. The development and interest in these
methods is also highly motivated by the fact
that only 27% of Internet users speak En-
glish (www.internetworldstats.com/stats.htm,
Oct 11, 2011), and that number diminishes
further every year, as more people across the
globe gain Internet access.
The aim of this tutorial is to familiarize the
attendees with the subjectivity and sentiment
research carried out on languages other than
English in order to enable and promote cross-
fertilization. Specifically, we will review work
along three main directions. First, we will
present methods where the resources and tools
have been specifically developed for a given
target language. In this category, we will
also briefly overview the main methods that
have been proposed for English, but which can
be easily ported to other languages. Second,
we will describe cross-lingual approaches, in-
cluding several methods that have been pro-
posed to leverage on the resources and tools
available in English by using cross-lingual
projections. Finally, third, we will show how
the expression of opinions and polarity per-
vades language boundaries, and thus methods
that holistically explore multiple languages at
the same time can be effectively considered.
References
C. Banea, R. Mihalcea, and J. Wiebe. 2008. A Boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC 2008, Marrakech, Morocco.
L. W. Ku, T. H. Wu, L. Y. Lee, and H. H. Chen. 2005.
Construction of an Evaluation Corpus for Opinion Ex-
traction. In Proceedings of NTCIR-5, Tokyo, Japan.
L. Xiang. 2011. Ideogram Based Chinese Sentiment
Word Orientation Computation. Computing Research
Repository, page 4, October.
4
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 120?125,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Benefactive/Malefactive Event and Writer Attitude Annotation
Lingjia Deng ?, Yoonjung Choi ?, Janyce Wiebe ??
? Intelligent System Program, University of Pittsburgh
? Department of Computer Science, University of Pittsburgh
?lid29@pitt.edu, ?{yjchoi,wiebe}@cs.pitt.edu
Abstract
This paper presents an annotation scheme
for events that negatively or positively
affect entities (benefactive/malefactive
events) and for the attitude of the writer
toward their agents and objects. Work on
opinion and sentiment tends to focus on
explicit expressions of opinions. However,
many attitudes are conveyed implicitly,
and benefactive/malefactive events are
important for inferring implicit attitudes.
We describe an annotation scheme and
give the results of an inter-annotator
agreement study. The annotated corpus is
available online.
1 Introduction
Work in NLP on opinion mining and sentiment
analysis tends to focus on explicit expressions of
opinions. Consider, however, the following sen-
tence from the MPQA corpus (Wiebe et al, 2005)
discussed by (Wilson and Wiebe, 2005):
(1) I think people are happy because
Chavez has fallen.
The explicit sentiment expression, happy, is pos-
itive. Yet (according to the writer), the people
are negative toward Chavez. As noted by (Wil-
son and Wiebe, 2005), the attitude toward Chavez
is inferred from the explicit sentiment toward the
event. An opinion-mining system that recognizes
only explicit sentiments would not be able to per-
ceive the negative attitude toward Chavez con-
veyed in (1). Such inferences must be addressed
for NLP systems to be able to recognize the full
range of opinions conveyed in language.
The inferences arise from interactions be-
tween sentiment expressions and events such as
fallen, which negatively affect entities (malefac-
tive events), and events such as help, which pos-
itively affect entities (benefactive events). While
some corpora have been annotated for explicit
opinion expressions (for example, (Kessler et
al., 2010; Wiebe et al, 2005)), there isn?t a
previously published corpus annotated for bene-
factive/malefactive events. While (Anand and
Reschke, 2010) conducted a related annotation
study, their data are artificially constructed sen-
tences incorporating event predicates from a fixed
list, and their annotations are of the writer?s
attitude toward the events. The scheme pre-
sented here is the first scheme for annotating, in
naturally-occurring text, benefactive/malefactive
events themselves as well as the writer?s attitude
toward the agents and objects of those events.
2 Overview
For ease of communication, we use the terms
goodFor and badFor for benefactive and malefac-
tive events, respectively, and use the abbreviation
gfbf for an event that is one or the other. There are
many varieties of gfbf events, including destruc-
tion (as in kill Bill, which is bad for Bill), cre-
ation (as in bake a cake, which is good for the
cake), gain or loss (as in increasing costs, which
is good for the costs), and benefit or injury (as in
comforted the child, which is good for the child)
(Anand and Reschke, 2010).
The scheme targets clear cases of gfbf events.
The event must be representable as a triple of con-
tiguous text spans, ?agent, gfbf, object?. The
agent must be a noun phrase, or it may be implicit
(as in the constituent will be destroyed). The ob-
ject must be a noun phrase.
120
Another component of the scheme is the influ-
encer, a word whose effect is to either retain or
reverse the polarity of a gfbf event. For example:
(2) Luckily Bill didn?t kill him.
(3) The reform prevented companies
from hurting patients.
(4) John helped Mary to save Bill.
In (2) and (3), didn?t and prevented, respectively,
reverse the polarity from badFor to goodFor (not
killing Bill is good for Bill; preventing companies
from hurting patients is good for the patients). In
(4), helped is an influencer which retains the polar-
ity (i.e., helping Mary to save Bill is good for Bill).
Examples (3) and (4) illustrate the case where an
influencer introduces an additional agent (reform
in (3) and John in (4)).
The agent of an influencer must be a noun
phrase or implicit. The object must be another in-
fluencer or a gfbf event.
Note that, semantically, an influencer can be
seen as good for or bad for its object. A reverser
influencer makes its object irrealis (i.e., not hap-
pen). Thus, it is bad for it. In (3), for example,
prevent is bad for the hurting event. A retainer in-
fluencer maintains its object, and thus is good for
it. In (4), for example, helped maintains the sav-
ing event. For this reason, influencers and gfbf
events are sometimes combined in the evaluations
presented below (see Section 4.2).
Finally, the annotators are asked to mark the
writer?s attitude towards the agents of the influ-
encers and gfbf events and the objects of the gfbf
events. For example:
(5) GOP Attack on Reform Is a Fight
Against Justice.
(6) Jettison any reference to end-of-life
counselling.
In (5), there are two badFor events: ?GOP, Attack
on, Reform? and ?GOP Attack on Reform,Fight
Against, Justice?. The writer?s attitude toward
both agents is negative, and his or her attitude
toward both objects is positive. In (6), the
writer conveys a negative attitude toward end-of-
life counselling. The coding manual instructs the
annotators to consider whether an attitude of the
writer is communicated or revealed in the particu-
lar sentence which contains the gfbf event.
3 Annotation Scheme
There are four types of annotations: gfbf event,
influencer, agent, and object. For gfbf events, the
agent, object, and polarity (goodFor or badFor) are
identified. For influencers, the agent, object and
effect (reverse or retain) are identified. For agents
and objects, the writer?s attitude is marked (posi-
tive, negative, or none). The annotator links agents
and objects to their gfbf and influencer annotations
via explicit IDs. When an agent is not mentioned
explicitly, the annotator should indicate that it is
implicit. For any span the annotator is not certain
about, he or she can set the uncertain option to be
true.
The annotation manual includes guidelines to
help clarify which events should be annotated.
Though it often is, the gfbf span need not be a
verb or verb phrase. We saw an example above,
namely (5). Even though attack on and fight
against are not verbs, we still mark them because
they represent events that are bad for the object.
Note that, Goyal et al (2012) present a method for
automatically generating a lexicon of what they
call patient polarity verbs. Such verbs correspond
to gfbf events, except that gfbf events are, concep-
tually, events, not verbs, and gfbf spans are not
limited to verbs (as just noted).
Recall from Section 2 that annotators should
only mark gfbf events that may be represented as a
triple, ?agent,gfbf,object?. The relationship should
be perceptible by looking only at the spans in the
triple. If, for example, another argument of the
verb is needed to perceive the relationship, the an-
notators should not mark that event.
(7) His uncle left him a massive amount
of debt.
(8) His uncle left him a treasure.
There is no way to break these sentences into
triples that follow our rules. ?His uncle, left, him?
doesn?t work because we cannot perceive the po-
larity looking only at the triple; the polarity de-
pends on what his uncle left him. ?His uncle, left
him, a massive amount of debt? isn?t correct: the
event is not bad for the debt, it is bad for him. Fi-
nally, ?His uncle, left him a massive amount of
debt, Null? isn?t correct, since no object is iden-
tified.
Note that him in (7) and (8) are both consid-
ered benefactive semantic roles (Zu?n?iga and Kit-
tila?, 2010). In general, gfbf objects are not equiva-
121
lent to benefactive/malefactive semantic roles. For
example, in our scheme, (7) is a badFor event and
(8) is a goodFor event, while him fills the benefac-
tive semantic role in both. Further, according to
(Zu?n?iga and Kittila?, 2010), me is the filler of the
benefactive role in She baked a cake for me. Yet,
in our scheme, a cake is the object of the good-
For event; me is not included in the annotations.
The objects of gfbf events are what (Zu?n?iga and
Kittila?, 2010) refer to as the primary targets of the
events, whereas, they state, beneficiary semantic
roles are typically optional arguments. The reason
we annotate only the primary objects (and agents)
is that the clear cases of attitude implicatures mo-
tivating this work (see Section 1) are inferences
toward agents and primary objects of gfbf events.
Turning to influencers, there may be chains of
them, where the ultimate polarity and agent must
be determined compositionally. For example, the
structure of Jack stopped Mary from trying to kill
Bill is a reverser influencer (stopped) whose object
is a retainer influencer (trying) whose object is, in
turn, a badFor event (kill). The ultimate polarity of
this event is goodFor and the ?highest level? agent
is Jack. In our scheme, all such chains of lengthN
are treated as N ? 1 influencers followed by a sin-
gle gfbf event. It will be up to an automatic system
to calculate the ultimate polarity and agent using
rules such as those presented in, e.g., (Moilanen
and Pulman, 2007; Neviarouskaya et al, 2010).
To save some effort, the annotators are not
asked to mark retainer influencers which do not in-
troduce new agents. For example, for Jack stopped
trying to kill Bill, there is no need to mark ?trying.?
Of course, all reverser influencers must be marked.
4 Agreement Study
To validate the reliability of the annotation
scheme, we conducted an agreement study. In this
section we introduce how we designed the agree-
ment study, present the evaluation method and
give the agreement results. Besides, we conduct
a second-step consensus study to further analyze
the disagreement.
4.1 Data and Agreement Study Design
For this study, we want to use data that is rich in
opinions and implicatures. Thus we used the cor-
pus from (Conrad et al, 2012), which consists of
134 documents from blogs and editorials about a
controversial topic, ?the Affordable Care Act?.
To measure agreement on various aspects of
the annotation scheme, two annotators, who are
co-authors, participated in the agreement study;
one of the two wasn?t involved in developing the
scheme. The new annotator first read the anno-
tation manual and discussed it with the first an-
notator. Then, the annotators labelled 6 docu-
ments and discussed their disagreements to recon-
cile their differences. For the formal agreement
study, we randomly selected 15 documents, which
have a total of 725 sentences. These documents do
not contain any examples in the manual, and they
are different from the documents discussed during
training. The annotators then independently anno-
tated the 15 selected documents.
4.2 Agreement Study Evaluation
We annotate four types of items (gfbf event, influ-
encer, agent, and object) and their corresponding
attributes. As noted above in Section 2, influencers
can also be viewed as gfbf events. Also, the two
may be combined together in chains. Thus, we
measure agreement for gfbf and influencer spans
together, treating them as one type. Then we
choose the subset of gfbf and influencer annota-
tions that both annotators identified, and measure
agreement on the corresponding agents and ob-
jects.
Sometimes the annotations differ even though
the annotators recognize the same gfbf event.
Consider the following sentence:
(9) Obama helped reform curb costs.
Suppose the annotations given by the annotators
were:
Ann 1. ?Obama, helped, curb?
?reform, curb, costs?
Ann 2. ?Obama, helped, reform?
The two annotators do agree on the ?Obama,
helped, reform? triple, the first one marking helped
as a retainer and the other marking it as a goodFor
event. To take such cases into consideration in our
evaluation of agreement, if two spans overlap and
one is marked as gfbf and the other as influencer,
we use the following rules to match up their agents
and objects:
? for a gfbf event, consider its agent and object
as annotated;
122
? for an influencer, assign the agent of the in-
fluencer?s object to be the influencer?s object,
and consider its agent as annotated and the
newly-assigned object. In (9), Ann 2?s anno-
tations remain the same and Ann 1?s become
?Obama, helped, reform? and ?reform, curb,
costs?.
We use the same measurement for agreement
for all types of spans. Suppose A is a set of an-
notations of a particular type and B is the set of
annotations of the same type from the other anno-
tator. For any text span a ? A and b ? B, the span
coverage c measures the overlap between a and b.
Two measures of c are adopted here.
Binary: As in (Wilson and Wiebe, 2003), if two
spans a and b overlap, the pair is counted as 1,
otherwise 0.
c1(a, b) = 1 if |a ? b| > 0
Numerical: (Johansson and Moschitti, 2013)
propose, for the pairs that are counted as 1 by c1, a
measure of the percentage of overlapping tokens,
c2(a, b) =
|a ? b|
|b|
where |a| is the number of tokens in span a, and ?
gives the tokens that two spans have in common.
As (Breck et al, 2007) point out, c2 avoids the
problem of c1, namely that c1 does not penalize a
span covering the whole sentence, so it potentially
inflates the results.
Following (Wilson and Wiebe, 2003), treat-
ing each set A and B in turn as the gold-
standard, we calculate the average F-measure, de-
noted agr(A,B). agr(A,B) is calculated twice,
once with c = c1 and once with c = c2.
match(A,B) =
?
a?A,b?B,
|a?b|>0
c(a, b)
agr(A||B) = match(A,B)|B|
agr(A,B) = agr(A||B) + agr(B||A)2
Now that we have the sets of annotations on
which the annotators agree, we use ? (Artstein
and Poesio, 2008) to measure agreement for the
attributes. We report two ? values: one for the
polarities of the gfbf events, together with the ef-
fects of the influencers, and one for the writer?s
gfbf & agent object
influencer
all anno- c1 0.70 0.92 1.00
tations c2 0.69 0.87 0.97
only c1 0.75 0.92 1.00
certain c2 0.72 0.87 0.98
consensus c1 0.85 0.93 0.99
study c2 0.81 0.88 0.98
Table 1: Span overlapping agreement agr(A,B)
in agreement study and consensus study.
polarity & effect attitude
all 0.97 0.89
certain 0.97 0.89
Table 2: ? for attribute agreement.
attitude toward the agents and objects. Note that,
as in Example (9), sometimes one annotator marks
a span as gfbf and the other marks it as an influ-
encer; in such cases we regard retain and goodfor
as the same attribute value and reverse and badfor
as the same value. Table 1 gives the agr values
and Table 2 gives the ? values.
4.3 Agreement Study Results
Recall that the annotator could choose whether
(s)he is certain about the annotation. Thus, we
evaluate two sets: all annotations and only those
annotations that both annotators are certain about.
The results are shown in the top four rows in Table
1.
The results for agents and objects in Table 1 are
all quite good, indicating that, given a gfbf or in-
fluencer, the annotators are able to correctly iden-
tify the agent and object.
Table 1 also shows that results are not signifi-
cantly worse when measured using c2 rather than
c1. This suggests that, in general, the annotators
have good agreement concerning the boundaries
of spans.
Table 2 shows that the ? values are high for both
sets of attributes.
4.4 Consensus Analysis
Following (Medlock and Briscoe, 2007), we ex-
amined what percentage of disagreement is due to
negligence on behalf of one or the other annota-
tor (i.e., cases of clear gfbfs or influencers that
were missed), though we conducted our consensus
123
study in a more independent manner than face-to-
face discussion between the annotators. For anno-
tator Ann1, we highlighted sentences for which
only Ann2 marked a gfbf event, and gave Ann1?s
annotations back to him or her with the highlights
added on top. For Ann2 we did the same thing.
The annotators reconsidered their highlighted sen-
tences, making any changes they felt they should,
without communicating with each other. There
could be more than one annotation in a highlighted
sentence; the annotators were not told the specific
number.
After re-annotating the highlighted sentences,
we calculate the agreement score for all the an-
notations. As shown in the last two rows in Table
1, the agreement for gfbf and influencer annota-
tions increases quite a bit. Similar to the claim
in (Medlock and Briscoe, 2007), it is reasonable
to conclude that the actual agreement is approx-
imately lower bounded by the initial values and
upper bounded by the consensus values, though,
compared to face-to-face consensus, we provide a
tighter upper bound.
5 Corpus and Examples
Recall from in Section 4.1 that we use the corpus
from (Conrad et al, 2012), which consists of 134
documents with a total of 8,069 sentences from
blogs and editorials about ?the Affordable Care
Act?. There are 1,762 gfbf and influencer annota-
tions. On average, more than 20 percent of the sen-
tences contain a gfbf event or an influencer. Out of
all gfbf and influencer annotations, 40 percent are
annotated as goodFor or retain and 60 percent are
annotated as badFor or reverse. For agents and ob-
jects, 52 percent are annotated as positive and 47
percent as negative. Only 1 percent are annotated
as none, showing that almost all the sentences (in
this corpus of editorials and blogs) which con-
tain gfbf annotations are subjective. The annotated
corpus is available online1.
To illustrate various aspects of the annotation
scheme, in this section we give several examples
from the corpus. In the examples below, words
in square brackets are agents or objects, words in
italics are influencers, and words in boldface are
gfbf events.
1. And [it] will enable [Obama and the
Democrats] - who run Washington - to get
1http://mpqa.cs.pitt.edu/
back to creating [jobs].
(a) Creating is goodFor jobs; the agent is
Obama and the Democrats.
(b) The phrase to get back to is a retainer in-
fluencer. But, the agent span is also Obama
and the Democrats, as the same with the
goodFor, so we don?t have to give an anno-
tation for it.
(c) The phrase enable is a retainer influencer.
Since its agent span is different (namely, it),
we do create an annotation for it.
2. [Repealing [the Affordable Care Act]] would
hurt [families, businesses, and our econ-
omy].
(a) Repealing is a badFor event since it de-
prives the object, the Affordable Care Act, of
its existence. In this case the agent is implicit.
(b) The agent of the badFor event hurt is the
whole phrase Repealing the Affordable Care
Act. Note that the agent span is in fact a noun
phrase (even though it refers to an event).
Thus, it doesn?t break the rule that all agent
gfbf spans should be noun phrases.
3. It is a moral obligation to end this indefensi-
ble neglect of [hard-working Americans].
(a) This example illustrates a gfbf that cen-
ters on a noun (neglect) rather than on a verb.
(b) It also illustrates the case when two words
can be seen as gfbf events: both end and ne-
glect of can be seen as badFor events. Fol-
lowing our specification, they are annotated
as a chain ending in a single gfbf event: end
is an influencer that reverses the polarity of
the badFor event neglect of.
6 Conclusion
Attitude inferences arise from interactions
between sentiment expressions and benefac-
tive/malefactive events. Corpora have been
annotated in the past for explicit sentiment ex-
pressions; this paper fills in a gap by presenting
an annotation scheme for benefactive/malefactive
events and the writer?s attitude toward the agents
and objects of those events. We conducted an
agreement study, the results of which are positive.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their helpful feedback.
124
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2683?
2688, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ?12, pages 80?88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages no?no.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sen-
timent corpus for the automotive domain. In 4th
Int?l AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2010. Recognition of affect, judg-
ment, and appreciation in text. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10, pages 806?814, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3):164?210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13?22.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
F. Zu?n?iga and S. Kittila?. 2010. Introduction. In
F. Zu?n?iga and S. Kittila?, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
125
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560?565,
Dublin, Ireland, August 23-24, 2014.
SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity
Carmen Banea, Di Chen,
Rada Mihalcea
?
University of Michigan
Ann Arbor, MI
Claire Cardie
Cornell University
Ithaca, NY
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
Abstract
This article presents our team?s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.
1 Introduction
Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.
To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).
?
{carmennb,chenditc,mihalcea}@umich.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).
On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.
3 System Description
3.1 Generic Features
Our system employs both knowledge and corpus-
based measures as detailed below.
560
Knowledge-based features
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN ), and Jiang & Conrath (Jiang
and Conrath, 1997) (JCN ).
Corpus based features
Our corpus based features are derived from a
deep learning vector space model that is able to
?understand? word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit.
1
Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors V
A
and V
B
, respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-
1
https://code.google.com/p/word2vec/
tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.
We use the DKPro Similarity package (B?ar et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).
3.2 Feature Variations
Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
& example}), c) expand left (L) & right (R)
(A={word glosses & examples}, B={sense gloss
& example}). After applying the Align method,
we obtain measures JNC, LIN , RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L & R (A={phrase glosses & exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV 2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features? weights.
561
a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.
2
These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.
3.3 Meta-learning
The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier
3
was trained for each lex-
ical level and applied to the test data sets.
2
Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.
3
Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.
4 Evaluations & Discussion
Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.
Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.
In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask?s evaluations, falling short
of the leading system by 0.025 correlation points.
Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass
562
 0
 0.1
 0.2
 0.3
 0.4
 0.5
w
o
rd
2s
en
se
Train Test
 0
 0.1
 0.2
 0.3
 0.4
 0.5
BL JNC
LIN
RES
W
TV
1
W
TV
2
G
P
JN
C
LIN
RES
W
TV
1
W
TV
2
G
P
ph
ra
se
2w
or
d
No expansion Expansion R Expansion L&R All
 0
 0.2
 0.4
 0.6
 0.8
s
e
n
te
n
ce
2p
hr
as
e
Train Test
BL WTV
1
W
TV
2
W
TV
3
W
TV
4
G
P
W
TV
1
W
TV
2
W
TV
3
W
TV
4
G
P
 0
 0.2
 0.4
 0.6
 0.8
pa
ra
gr
ap
h2
se
nt
en
ce
Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.
obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.
Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).
Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.
For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.
Considering the overall system performance,
SimCompass is remarkably versatile, ranking
among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.
5 Conclusion
We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
563
of the National Science Foundation or the Defense
Advanced Research Projects Agency.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121?126,
Sofia, Bulgaria.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211?257.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606?1611, Hyderabad, India.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.
Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033?1038, Genoa, Italy, July.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings
of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.
Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of Human Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746?
751, Atlanta, GA, USA.
George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39?41.
Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:
564
Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).
Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
1995. Roget?s II: The New Thesaurus. Houghton Mif-
flin.
Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
565
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116?124,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Recognizing Stances in Ideological On-Line Debates
Swapna Somasundaran
Dept. of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
swapna@cs.pitt.edu
Janyce Wiebe
Dept. of Computer Science and
The Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This work explores the utility of sentiment and
arguing opinions for classifying stances in ide-
ological debates. In order to capture arguing
opinions in ideological stance taking, we con-
struct an arguing lexicon automatically from
a manually annotated corpus. We build su-
pervised systems employing sentiment and ar-
guing opinions and their targets as features.
Our systems perform substantially better than
a distribution-based baseline. Additionally,
by employing both types of opinion features,
we are able to perform better than a unigram-
based system.
1 Introduction
In this work, we explore if and how ideologi-
cal stances can be recognized using opinion analy-
sis. Following (Somasundaran and Wiebe, 2009),
stance, as used in this work, refers to an overall po-
sition held by a person toward an object, idea or
proposition. For example, in a debate ?Do you be-
lieve in the existence of God?,? a person may take a
for-existence of God stance or an against existence
of God stance. Similarly, being pro-choice, believ-
ing in creationism, and supporting universal health-
care are all examples of ideological stances.
Online web forums discussing ideological and po-
litical hot-topics are popular.1 In this work, we are
1http://www.opposingviews.com,
http://wiki.idebate.org, http://www.createdebate.com and
http://www.forandagainst.com are examples of such debating
websites.
interested in dual-sided debates (there are two pos-
sible polarizing sides that the participants can take).
For example, in a healthcare debate, participants can
take a for-healthcare stance or an against-healthcare
stance. Participants generally pick a side (the web-
sites provide a way for users to tag their stance)
and post an argument/justification supporting their
stance.
Personal opinions are clearly important in ideo-
logical stance taking, and debate posts provide out-
lets for expressing them. For instance, let us con-
sider the following snippet from a universal health-
care debate. Here the writer is expressing a nega-
tive sentiment2 regarding the government (the opin-
ion spans are highlighted in bold and their targets,
what the opinions are about, are highlighted in ital-
ics).
(1) Government is a disease pretending to be its
own cure. [side: against healthcare]
The writer?s negative sentiment is directed toward
the government, the initiator of universal healthcare.
This negative opinion reveals his against-healthcare
stance.
We observed that arguing, a less well explored
type of subjectivity, is prominently manifested in
ideological debates. As used in this work, arguing is
a type of linguistic subjectivity, where a person is ar-
guing for or against something or expressing a belief
about what is true, should be true or should be done
2As used in this work, sentiment is a type of linguistic sub-
jectivity, specifically positive and negative expressions of emo-
tions, judgments, and evaluations (Wilson and Wiebe, 2005;
Wilson, 2007; Somasundaran et al, 2008).
116
in his or her view of the world (Wilson and Wiebe,
2005; Wilson, 2007; Somasundaran et al, 2008).
For instance, let us consider the following snippet
from a post supporting an against-existence of God
stance.
(2) Obviously that hasn?t happened, and to be
completely objective (as all scientists should
be) we must lean on the side of greatest evi-
dence which at the present time is for evolu-
tion. [side: against the existence of God]
In supporting their side, people not only express
their sentiments, but they also argue about what is
true (e.g., this is prominent in the existence of God
debate) and about what should or should not be done
(e.g., this is prominent in the healthcare debate).
In this work, we investigate whether sentiment
and arguing expressions of opinion are useful for
ideological stance classification. For this, we ex-
plore ways to capture relevant opinion information
as machine learning features into a supervised stance
classifier. While there is a large body of resources
for sentiment analysis (e.g., the sentiment lexicon
from (Wilson et al, 2005)), arguing analysis does
not seem to have a well established lexical resource.
In order to remedy this, using a simple automatic ap-
proach and a manually annotated corpus,3 we con-
struct an arguing lexicon. We create features called
opinion-target pairs, which encode not just the opin-
ion information, but also what the opinion is about,
its target. Systems employing sentiment-based and
arguing-based features alone, or both in combina-
tion, are analyzed. We also take a qualitative look
at features used by the learners to get insights about
the information captured by them.
We perform experiments on four different ideo-
logical domains. Our results show that systems us-
ing both sentiment and arguing features can perform
substantially better than a distribution-based base-
line and marginally better than a unigram-based sys-
tem. Our qualitative analysis suggests that opinion
features capture more insightful information than
using words alone.
The rest of this paper is organized as follows: We
first describe our ideological debate data in Section
2. We explain the construction of our arguing lexi-
con in Section 3 and our different systems in Section
3MPQA corpus available at http://www.cs.pitt.edu/mpqa.
4. Experiments, results and analyses are presented in
Section 5. Related work is in Section 6 and conclu-
sions are in Section 7.
2 Ideological Debates
Political and ideological debates on hot issues are
popular on the web. In this work, we analyze the fol-
lowing domains: Existence of God, Healthcare, Gun
Rights, Gay Rights, Abortion and Creationism. Of
these, we use the first two for development and the
remaining four for experiments and analyses. Each
domain is a political/ideological issue and has two
polarizing stances: for and against.
Table 2 lists the domains, examples of debate top-
ics within each domain, the specific sides for each
debate topic, and the domain-level stances that cor-
respond to these sides. For example, consider the
Existence of God domain in Table 2. The two
stances in this domain are for-existence of God and
against-existence of God. ?Do you believe in God?,
a specific debate topic within this domain, has two
sides: ?Yes!!? and ?No!!?. The former corresponds
to the for-existence of God stance and the latter maps
to the against-existence of God stance. The situa-
tion is different for the debate ?God Does Not Ex-
ist?. Here, side ?against? corresponds to the for-
existence of God stance, and side ?for? corresponds
to the against-existence of God stance.
In general, we see in Table 2 that, while specific
debate topics may vary, in each case the two sides
for the topic correspond to the domain-level stances.
We download several debates for each domain and
manually map debate-level stances to the stances
for the domain. Table 2 also reports the number
of debates, and the total number of posts for each
domain. For instance, we collect 16 different de-
bates in the healthcare domain which gives us a total
of 336 posts. All debate posts have user-reported
debate-level stance tags.
2.1 Observations
Preliminary inspection of development data gave us
insights which shaped our approach. We discuss
some of our observations in this section.
Arguing Opinion
We found that arguing opinions are prominent
when people defend their ideological stances. We
117
Domain/Topics stance1 stance2
Healthcare (16 debates, 336 posts) for against
Should the US have universal health-
care
Yes No
Debate: Public insurance option in
US health care
Pro Con
Existence of God (7 debates, 486
posts)
for against
Do you believe in God Yes!! No!!
God Does Not Exist against for
Gun Rights (18 debates, 566 posts) for against
Should Guns Be Illegal against for
Debate: Right to bear arms in the US Yes No
Gay Rights (15 debates, 1186 posts) for against
Are people born gay Yes No
Is homosexuality a sin No Yes
Abortion (13 debates, 618 posts) for against
Should abortion be legal Yes No
Should south Dakota pass the abor-
tion ban
No Yes
Creationism (15 debates, 729 posts) for against
Evolution Is A False Idea for against
Has evolution been scientifically
proved
It has
not
It has
Table 1: Examples of debate topics and their stances
saw an instance of this in Example 2, where the par-
ticipant argues against the existence of God. He ar-
gues for what (he believes) is right (should be), and
is imperative (we must). He employs ?Obviously?
to draw emphasis and then uses a superlative con-
struct (greatest) to argue for evolution.
Example 3 below illustrates arguing in a health-
care debate. The spans most certainly believe and
has or must do reveal arguing (ESSENTIAL, IM-
PORTANT are sentiments).
(3) ... I most certainly believe that there are
some ESSENTIAL, IMPORTANT things
that the government has or must do [side: for
healthcare]
Observe that the text spans revealing arguing can
be a single word or multiple words. This is differ-
ent from sentiment expressions that are more often
single words.
Opinion Targets
As mentioned previously, a target is what an
opinion is about. Targets are vital for determining
stances. Opinions by themselves may not be as in-
formative as the combination of opinions and tar-
gets. For instance, in Example 1 the writer supports
an against-healthcare stance using a negative senti-
ment. There is a negative sentiment in the example
below (Example 4) too. However, in this case the
writer supports a for-healthcare stance. It is by un-
derstanding what the opinion is about, that we can
recognize the stance.
(4) Oh, the answer is GREEDY insurance com-
panies that buy your Rep & Senator. [side: for
healthcare]
We also observed that targets, or in general items
that participants from either side choose to speak
about, by themselves may not be as informative as
opinions in conjunction with the targets. For in-
stance, Examples 1 and 3 both speak about the gov-
ernment but belong to opposing sides. Understand-
ing that the former example is negative toward the
government and the latter has a positive arguing
about the government helps us to understand the cor-
responding stances.
Examples 1, 3 and 4 also illustrate that there
are a variety of ways in which people support
their stances. The writers express opinions about
government, the initiator of healthcare and insur-
ance companies, and the parties hurt by government
run healthcare. Participants group government and
healthcare as essentially the same concept, while
they consider healthcare and insurance companies
as alternative concepts. By expressing opinions re-
garding a variety of items that are same or alternative
to main topic (healthcare, in these examples), they
are, in effect, revealing their stance (Somasundaran
et al, 2008).
3 Constructing an Arguing Lexicon
Arguing is a relatively less explored category in sub-
jectivity. Due to this, there are no available lexicons
with arguing terms (clues). However, the MPQA
corpus (Version 2) is annotated with arguing sub-
jectivity (Wilson and Wiebe, 2005; Wilson, 2007).
There are two arguing categories: positive arguing
and negative arguing. We use this corpus to gener-
ate a ngram (up to trigram) arguing lexicon.
The examples below illustrate MPQA arguing an-
notations. Examples 5 and 7 illustrate positive argu-
118
ing annotations and Example 6 illustrates negative
arguing.
(5) Iran insists its nuclear program is purely
for peaceful purposes.
(6) Officials in Panama denied that Mr. Chavez
or any of his family members had asked for
asylum.
(7) Putin remarked that the events in Chechnia
?could be interpreted only in the context
of the struggle against international terror-
ism.?
Inspection of these text spans reveal that arguing an-
notations can be considered to be comprised of two
pieces of information. The first piece of information
is what we call the arguing trigger expression. The
trigger is an indicator that an arguing is taking place,
and is the primary component that anchors the argu-
ing annotation. The second component is the ex-
pression that reveals more about the argument, and
can be considered to be secondary for the purposes
of detecting arguing. In Example 5, ?insists?, by it-
self, conveys enough information to indicate that the
speaker is arguing. It is quite likely that a sentence
of the form ?X insists Y? is going to be an arguing
sentence. Thus, ?insists? is an arguing trigger.
Similarly, in Example 6, we see two arguing trig-
gers: ?denied? and ?denied that?. Each of these can
independently act as arguing triggers (For example,
in the constructs ?X denied that Y? and ?X denied
Y?). Finally, in Example 7, the arguing annotation
has the following independent trigger expressions
?could be * only?, ?could be? and ?could?. The wild
card in the first trigger expression indicates that there
could be zero or more words in its place.
Note that MPQA annotations do not provide this
primary/secondary distinction. We make this dis-
tinction to create general arguing clues such as ?in-
sist?. Table 3 lists examples of arguing annotations
from the MPQA corpus and what we consider as
their arguing trigger expressions.
Notice that trigger words are generally at the be-
ginning of the annotations. Most of these are uni-
grams, bigrams or trigrams (though it is possible for
these to be longer, as seen in Example 7). Thus, we
can create a lexicon of arguing trigger expressions
Positive arguing annotations Trigger Expr.
actually reflects Israel?s determination ... actually
am convinced that improving ... am convinced
bear witness that Mohamed is his ... bear witness
can only rise to meet it by making ... can only
has always seen usama bin ladin?s ... has always
Negative Arguing Annotations Trigger Expr.
certainly not a foregone conclusion certainly not
has never been any clearer has never
not too cool for kids not too
rather than issuing a letter of ... rather than
there is no explanation for there is no
Table 2: Arguing annotations from the MPQA corpus and
their corresponding trigger expressions
by extracting the starting n-grams from the MPQA
annotations. The process of creating the lexicon is
as follows:
1. Generate a candidate Set from the annotations
in the corpus. Three candidates are extracted
from the stemmed version of each annotation:
the first word, the bigram starting at the first
word, and the trigram starting at the first word.
For example, if the annotation is ?can only rise
to meet it by making some radical changes?,
the following candidates are extracted from it:
?can?, ?can only? and ?can only rise?.
2. Remove the candidates that are present in the
sentiment lexicon from (Wilson et al, 2005) (as
these are already accounted for in previous re-
search). For example, ?actually?, which is a
trigger word in Table 3, is a neutral subjectivity
clue in the lexicon.
3. For each candidate in the candidate Set,
find the likelihood that it is a reliable indi-
cator of positive or negative arguing in the
MPQA corpus. These are likelihoods of the
form: P (positive arguing|candidate) =
#candidate is in a positive arguing span
#candidate is in the corpus
and P (negative arguing|candidate) =
#candidate is in a negative arguing span
#candidate is in the corpus
4. Make a lexicon entry for each candidate con-
sisting of the stemmed text and the two proba-
bilities described above.
This process results in an arguing lexicon
with 3762 entries, where 3094 entries have
119
P (positive arguing|candidate) > 0; and 668
entries have P (negative arguing|candidate) > 0.
Table 3 lists select interesting expressions from the
arguing lexicon.
Entries indicative of Positive Arguing
be important to, would be better, would need to, be just the, be
the true, my opinion, the contrast, show the, prove to be, only
if, on the verge, ought to, be most, youve get to, render, man-
ifestation, ironically, once and for, no surprise, overwhelming
evidence, its clear, its clear that, it be evident, it be extremely,
it be quite, it would therefore
Entries indicative of Negative Arguing
be not simply, simply a, but have not, can not imagine, we dont
need, we can not do, threat against, ought not, nor will, never
again, far from be, would never, not completely, nothing will,
inaccurate and, inaccurate and, find no, no time, deny that
Table 3: Examples of positive argu-
ing (P (positive arguing|candidate) >
P (negative arguing|candidate)) and negative
arguing (P (negative arguing|candidate) >
P (positive arguing|candidate))from the arguing
lexicon
4 Features for Stance Classification
We construct opinion target pair features, which are
units that capture the combined information about
opinions and targets. These are encoded as binary
features into a standard machine learning algorithm.
4.1 Arguing-based Features
We create arguing features primarily from our ar-
guing lexicon. We construct additional arguing fea-
tures using modal verbs and syntactic rules. The lat-
ter are motivated by the fact that modal verbs such
as ?must?, ?should? and ?ought? are clear cases of
arguing, and are often involved in simple syntactic
patterns with clear targets.
4.1.1 Arguing-lexicon Features
The process for creating features for a post using
the arguing lexicon is simple. For each sentence in
the post, we first determine if it contains a positive or
negative arguing expression by looking for trigram,
bigram and unigram matches (in that order) with the
arguing lexicon. We prevent the same text span from
matching twice ? once a trigram match is found, a
substring bigram (or unigram) match with the same
text span is avoided. If there are multiple arguing ex-
pression matches found within a sentence, we deter-
mine the most prominent arguing polarity by adding
up the positive arguing probabilities and negative ar-
guing probabilities (provided in the lexicon) of all
the individual expressions.
Once the prominent arguing polarity is deter-
mined for a sentence, the prefix ap (arguing positive)
or an (arguing negative) is attached to all the content
words in that sentence to construct opinion-target
features. In essence, all content words (nouns, verbs,
adjectives and adverbs) in the sentence are assumed
to be the target. Arguing features are denoted as ap-
target (positive arguing toward target) and an-target
(negative arguing toward target).
4.1.2 Modal Verb Features for Arguing
Modals words such as ?must? and ?should? are
usually good indicators of arguing. This is a small
closed set. Also, the target (what the arguing is
about) is syntactically associated with the modal
word, which means it can be relatively accurately
extracted by using a small set of syntactic rules.
For every modal detected, three features are cre-
ated by combining the modal word with its subject
and object. Note that all the different modals are
replaced by ?should? while creating features. This
helps to create more general features. For exam-
ple, given a sentence ?They must be available to
all people?, the method creates three features ?they
should?, ?should available? and ?they should avail-
able?. These patterns are created independently of
the arguing lexicon matches, and added to the fea-
ture set for the post.
4.2 Sentiment-based Features
Sentiment-based features are created independent of
arguing features. In order to detect sentiment opin-
ions, we use a sentiment lexicon (Wilson et al,
2005). In addition to positive (+) and negative (?)
words, this lexicon also contains subjective words
that are themselves neutral (=) with respect to po-
larity. Examples of neutral entries are ?absolutely?,
?amplify?, ?believe?, and ?think?.
We find the sentiment polarity of the entire sen-
tence and assign this polarity to each content word in
the sentence (denoted, for example, as target+). In
order to detect the sentence polarity, we use the Vote
120
and Flip algorithm from Choi and Cardie (2009).
This algorithm essentially counts the number of pos-
itive, negative and neutral lexicon hits in a given ex-
pression and accounts for negator words. The algo-
rithm is used as is, except for the default polarity
assignment (as we do not know the most prominent
polarity in the corpus). Note that the Vote and Flip
algorithm has been developed for expressions but we
employ it on sentences. Once the polarity of a sen-
tence is determined, we create sentiment features for
the sentence. This is done for all sentences in the
post.
5 Experiments
Experiments are carried out on debate posts from the
following four domains: Gun Rights, Gay Rights,
Abortion, and Creationism. For each domain, a cor-
pus with equal class distribution is created as fol-
lows: we merge all debates and sample instances
(posts) from the majority class to obtain equal num-
bers of instances for each stance. This gives us a
total of 2232 posts in the corpus: 306 posts for the
Gun Rights domain, 846 posts for the Gay Rights
domain, 550 posts for the Abortion domain and 530
posts for the Creationism domain.
Our first baseline is a distribution-based baseline,
which has an accuracy of 50%. We also construct
Unigram, a system based on unigram content infor-
mation, but no explicit opinion information. Un-
igrams are reliable for stance classification in po-
litical domains (as seen in (Lin et al, 2006; Kim
and Hovy, 2007)). Intuitively, evoking a particular
topic can be indicative of a stance. For example,
a participant who chooses to speak about ?child?
and ?life? in an abortion debate is more likely from
an against-abortion side, while someone speaking
about ?woman?, ?rape? and ?choice? is more likely
from a for-abortion stance.
We construct three systems that use opinion in-
formation: The Sentiment system that uses only the
sentiment features described in Section 4.2, the Ar-
guing system that uses only arguing features con-
structed in Section 4.1, and the Arg+Sent system
that uses both sentiment and arguing features.
All systems are implemented using a standard im-
plementation of SVM in the Weka toolkit (Hall et
al., 2009). We measure performance using the accu-
racy metric.
5.1 Results
Table 4 shows the accuracy averaged over 10 fold
cross-validation experiments for each domain. The
first row (Overall) reports the accuracy calculated
over all 2232 posts in the data.
Overall, we notice that all the supervised systems
perform better than the distribution-based baseline.
Observe that Unigram has a better performance than
Sentiment. The good performance of Unigram indi-
cates that what participants choose to speak about is
a good indicator of ideological stance taking. This
result confirms previous researchers? intuition that,
in general, political orientation is a function of ?au-
thors? attitudes over multiple issues rather than pos-
itive or negative sentiment with respect to a sin-
gle issue? (Pang and Lee, 2008). Nevertheless, the
Arg+Sent system that uses both arguing and senti-
ment features outperforms Unigram.
We performed McNemar?s test to measure the dif-
ference in system behaviors. The test was performed
on all pairs of supervised systems using all 2232
posts. The results show that there is a significant dif-
ference between the classification behavior of Uni-
gram and Arg+Sent systems (p < 0.05). The dif-
ference between classifications of Unigram and Ar-
guing approaches significance (p < 0.1). There is
no significant difference in the behaviors of all other
system pairs.
Moving on to detailed performance in each do-
main, we see that Unigram outperforms Sentiment
for all domains. Arguing and Arg+Sent outperform
Unigram for three domains (Guns, Gay Rights and
Abortion), while the situation is reversed for one do-
main (Creationism). We carried out separate t-tests
for each domain, using the results from each test fold
as a data point. Our results indicate that the perfor-
mance of Sentiment is significantly different from
all other systems for all domains. However there is
no significant difference between the performance of
the remaining systems.
5.2 Analysis
On manual inspection of the top features used by
the classifiers for discriminating the stances, we
found that there is an overlap between the content
words used by Unigram, Arg+Sent and Arguing. For
121
Domain (#posts) Distribution Unigram Sentiment Arguing Arg+Sent
Overall (2232) 50 62.50 55.02 62.59 63.93
Guns Rights (306) 50 66.67 58.82 69.28 70.59
Gay Rights (846) 50 61.70 52.84 62.05 63.71
Abortion (550) 50 59.1 54.73 59.46 60.55
Creationism (530) 50 64.91 56.60 62.83 63.96
Table 4: Accuracy of the different systems
example, in the Gay Rights domain, ?understand?
and ?equal? are amongst the top features in Uni-
gram, while ?ap-understand? (positive arguing for
?understand?) and ?ap-equal? are top features for
Arg+Sent.
However, we believe that Arg+Sent makes finer
and more insightful distinctions based on polarity of
opinions toward the same set of words. Table 5 lists
some interesting features in the Gay Rights domain
for Unigram and Arg+Sent. Depending on whether
positive or negative attribute weights were assigned
by the SVM learner, the features are either indicative
of for-gay rights or against-gay rights. Even though
the features for Unigram are intuitive, it is not ev-
ident if a word is evoked as, for example, a pitch,
concern, or denial. Also, we do not see a clear sep-
aration of the terms (for e.g., ?bible? is an indicator
for against-gay rights while ?christianity? is an indi-
cator for for-gay rights)
The arguing features from Arg+Sent seem to
be relatively more informative ? positive arguing
about ?christianity?, ?corinthians?, ?mormonism?
and ?bible? are all indicative of against-gay rights
stance. These are indeed beliefs and concerns that
shape an against-gay rights stance. On the other
hand, negative arguings with these same words de-
note a for-gay rights stance. Presumably, these oc-
cur in refutations of the concerns influencing the op-
posite side. Likewise, the appeal for equal rights
for gays is captured positive arguing about ?liberty?,
?independence?, ?pursuit? and ?suffrage?.
Interestingly, we found that our features also cap-
ture the ideas of opinion variety and same and alter-
native targets as defined in previous research (So-
masundaran et al, 2008) ? in Table 5, items that
are similar (e.g., ?christianity? and ?corinthians?)
have similar opinions toward them for a given stance
(for e.g., ap-christianity and ap-corinthians belong
to against-gay rights stance while an-christianity and
an-corinthians belong to for-gay rights stance). Ad-
ditionally, items that are alternatives (e.g. ?gay? and
?heterosexuality?) have opposite polarities associ-
ated with them for a given stance, that is, positive
arguing for ?heterosexuality? and negative arguing
for ?gay? reveal the the same stance.
In general, unigram features associate the choice
of topics with the stances, while the arguing features
can capture the concerns, defenses, appeals or de-
nials that signify each side (though we do not ex-
plicitly encode these fine-grained distinctions in this
work). Interestingly, we found that sentiment fea-
tures in Arg+Sent are not as informative as the argu-
ing features discussed above.
6 Related Work
Generally, research in identifying political view-
points has employed information from words in the
document (Malouf and Mullen, 2008; Mullen and
Malouf, 2006; Grefenstette et al, 2004; Laver et al,
2003; Martin and Vanberg, 2008; Lin et al, 2006;
Lin, 2006). Specifically, Lin et al observe that peo-
ple from opposing perspectives seem to use words
in differing frequencies. On similar lines, Kim and
Hovy (2007) use unigrams, bigrams and trigrams for
election prediction from forum posts. In contrast,
our work specifically employs sentiment-based and
arguing-based features to perform stance classifica-
tion in political debates. Our experiments are fo-
cused on determining how different opinion expres-
sions reinforce an overall political stance. Our re-
sults indicate that while unigram information is re-
liable, further improvements can be achieved in cer-
tain domains using our opinion-based approach. Our
work is also complementary to that by Greene and
Resnik (2009), which focuses on syntactic packag-
ing for recognizing perspectives.
122
For Gay Rights Against Gay Rights
Unigram Features
constitution, fundamental, rights, suffrage, pursuit, discrimina-
tion, government, happiness, shame, wed, gay, heterosexual-
ity, chromosome, evolution, genetic, christianity, mormonism,
corinthians, procreate, adopt
pervert, hormone, liberty, fidelity, naval, retarded, orientation, pri-
vate, partner, kingdom, bible, sin, bigot
Arguing Features from Arg+Sent
ap-constitution, ap-fundamental, ap-rights, ap-hormone,
ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap-
discrimination, an-government, ap-fidelity, ap-happiness,
an-pervert, an-naval, an-retarded, an-orientation, an-shame,
ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,
ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an-
christianity, an-mormonism, an-corinthians, an-bible, an-sin,
an-bigot, an-procreate, ap-adopt,
an-constitution, an-fundamental, an-rights, an-hormone,
an-liberty, an-independence, an-suffrage, an-pursuit, an-
discrimination, ap-government, an-fidelity, an-happiness,
ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame,
an-private, an-wed, an-gay, ap-heterosexuality, an-partner,
an-chromosome, an-evolution, an-genetic, ap-kingdom, ap-
christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin,
ap-bigot, ap-procreate, an-adopt
Table 5: Examples of features associated with the stances in Gay Rights domain
Discourse-level participant relation, that is,
whether participants agree/disagree has been found
useful for determining political side-taking (Thomas
et al, 2006; Bansal et al, 2008; Agrawal et
al., 2003; Malouf and Mullen, 2008). Agree-
ment/disagreement relations are not the main focus
of our work. Other work in the area of polarizing po-
litical discourse analyze co-citations (Efron, 2004)
and linking patterns (Adamic and Glance, 2005). In
contrast, our focus is on document content and opin-
ion expressions.
Somasundaran et al (2007b) have noted the use-
fulness of the arguing category for opinion QA. Our
tasks are different; they use arguing to retrieve rele-
vant answers, but not distinguish stances. Our work
is also different from related work in the domain of
product debates (Somasundaran and Wiebe, 2009)
in terms of the methodology.
Wilson (2007) manually adds positive/negative
arguing information to entries in a sentiment lexi-
con from (Wilson et al, 2005) and uses these as ar-
guing features. Our arguing trigger expressions are
separate from the sentiment lexicon entries and are
derived from a corpus. Our n-gram trigger expres-
sions are also different from manually created regu-
lar expression-based arguing lexicon for speech data
(Somasundaran et al, 2007a).
7 Conclusions
In this paper, we explore recognizing stances in ide-
ological on-line debates. We created an arguing lex-
icon from the MPQA annotations in order to recog-
nize arguing, a prominent type of linguistic subjec-
tivity in ideological stance taking. We observed that
opinions or targets in isolation are not as informative
as their combination. Thus, we constructed opinion
target pair features to capture this information.
We performed supervised learning experiments
on four different domains. Our results show that
both unigram-based and opinion-based systems per-
form better than baseline methods. We found that,
even though our sentiment-based system is able to
perform better than the distribution-based baseline,
it does not perform at par with the unigram system.
However, overall, our arguing-based system does as
well as the unigram-based system, and our system
that uses both arguing and sentiment features obtains
further improvement. Our feature analysis suggests
that arguing features are more insightful than uni-
gram features, as they make finer distinctions that
reveal the underlying ideologies.
References
Lada A. Adamic and Natalie Glance. 2005. The political
blogosphere and the 2004 u.s. election: Divided they
blog. In LinkKDD.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
123
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008).
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 590?598, Singapore,
August. Association for Computational Linguistics.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cocitation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 503?511, Boulder, Colorado, June. Association
for Computational Linguistics.
Gregory Grefenstette, Yan Qu, James G. Shanahan, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04, Avignon, FR.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
SIGKDD Explorations, Volume 11, Issue 1.
Soo-Min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1056?1064.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and sen-
tence levels. In Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL-
2006), pages 109?116, New York, New York.
Wei-Hao Lin. 2006. Identifying perspectives at the doc-
ument and sentence levels using statistical models. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Doctoral
Consortium, pages 227?230, New York City, USA,
June. Association for Computational Linguistics.
Robert Malouf and Tony Mullen. 2008. Taking sides:
Graph-based user classification for informal online po-
litical discourse. Internet Research, 18(2).
Lanny W. Martin and Georg Vanberg. 2008. A ro-
bust transformation procedure for interpreting political
text. Political Analysis, 16(1):93?100.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal po-
litical discourse. In AAAI 2006 Spring Symposium
on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, Vol. 2(1-2):pp. 1?135.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234, Suntec, Singapore, August. Association for
Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007a. Detecting arguing and sentiment in
meetings. In SIGdial Workshop on Discourse and Di-
alogue, Antwerp, Belgium, September.
Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,
and Veselin Stoyanov. 2007b. Qa with attitude: Ex-
ploiting opinion type analysis for improving question
answering in on-line discussions and the news. In In-
ternational Conference on Weblogs and Social Media,
Boulder, CO.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801?808, Manchester, UK, August.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 327?335, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of ACL
Workshop on Frontiers in Corpus Annotation II: Pie in
the Sky.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In hltemnlp2005, pages 347?354,
Vancouver, Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
124
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 195?203,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Alexander Conrad
University of Pittsburgh
conrada@cs.pitt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
Amazon Mechanical Turk (MTurk) is a mar-
ketplace for so-called ?human intelligence
tasks? (HITs), or tasks that are easy for hu-
mans but currently difficult for automated pro-
cesses. Providers upload tasks to MTurk
which workers then complete. Natural lan-
guage annotation is one such human intelli-
gence task. In this paper, we investigate us-
ing MTurk to collect annotations for Subjec-
tivity Word Sense Disambiguation (SWSD),
a coarse-grained word sense disambiguation
task. We investigate whether we can use
MTurk to acquire good annotations with re-
spect to gold-standard data, whether we can
filter out low-quality workers (spammers), and
whether there is a learning effect associated
with repeatedly completing the same kind of
task. While our results with respect to spam-
mers are inconclusive, we are able to ob-
tain high-quality annotations for the SWSD
task. These results suggest a greater role for
MTurk with respect to constructing a large
scale SWSD system in the future, promising
substantial improvement in subjectivity and
sentiment analysis.
1 Introduction
Many Natural Language Processing (NLP) systems
rely on large amounts of manually annotated data
that is collected from domain experts. The anno-
tation process to obtain this data is very laborious
and expensive. This makes supervised NLP systems
subject to a so-called knowledge acquisition bottle-
neck. For example, (Ng, 1997) estimates an effort of
16 person years to construct training data for a high-
accuracy domain independent Word Sense Disam-
biguation (WSD) system.
Recently researchers have been investigating
Amazon Mechanical Turk (MTurk) as a source of
non-expert natural language annotation, which is a
cheap and quick alternative to expert annotations
(Kaisser and Lowe, 2008; Mrozinski et al, 2008).
In this paper, we utilize MTurk to obtain training
data for Subjectivity Word Sense Disambiguation
(SWSD) as described in (Akkaya et al, 2009). The
goal of SWSD is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with ob-
jective senses. SWSD is a new task which suffers
from the absence of a substantial amount of anno-
tated data and thus can only be applied on a small
scale. SWSD has strong connections to WSD. Like
supervised WSD, it requires training data where tar-
get word instances ? words which need to be dis-
ambiguated by the system ? are labeled as having
an objective sense or a subjective sense. (Akkaya
et al, 2009) show that SWSD may bring substantial
improvement in subjectivity and sentiment analysis,
if it could be applied on a larger scale. The good
news is that training data for 80 selected keywords is
enough to make a substantial difference (Akkaya et
al., 2009). Thus, large scale SWSD is feasible. We
hypothesize that annotations for SWSD can be pro-
vided by non-experts reliably if the annotation task
is presented in a simple way.
The annotations obtained from MTurk workers
are noisy by nature, because MTurk workers are
not trained for the underlying annotation task. That
is why previous work explored methods to assess
annotation quality and to aggregate multiple noisy
annotations for high reliability (Snow et al, 2008;
Callison-Burch, 2009). It is understandable that not
every worker will provide high-quality annotations,
195
depending on their background and interest. Un-
fortunately, some MTurk workers do not follow the
annotation guidelines and carelessly submit annota-
tions in order to gain economic benefits with only
minimal effort. We define this group of workers
as spammers. We believe it is essential to distin-
guish between workers as well-meaning annotators
and workers as spammers who should be filtered out
as a first step when utilizing MTurk. In this work,
we investigate how well the built-in qualifications in
MTurk function as such a filter.
Another important question about MTurk workers
is whether they learn to provide better annotations
over time in the absence of any interaction and feed-
back. The presence of a learning effect may support
working with the same workers over a long time and
creating private groups of workers. In this work, we
also examine if there is a learning effect associated
with MTurk workers.
To summarize, in this work we investigate the fol-
lowing questions:
? Can MTurk be utilized to collect reliable train-
ing data for SWSD ?
? Are the built-in methods provided by MTurk
enough to avoid spammers ?
? Is there a learning effect associated with MTurk
workers ?
The remainder of the paper is organized as fol-
lows. In Section 2, we give general background in-
formation on the Amazon Mechanical Turk service.
In Section 3, we discuss sense subjectivity. In Sec-
tion 4, we describe the subjectivity word sense dis-
ambiguation task. In Section 5, we discuss the de-
sign of our experiment and our filtering mechanisms
for workers. In Section 6, we evaluate MTurk anno-
tations and relate results to our questions. In Section
7, we review related work. In Section 8, we draw
conclusions and discuss future work.
2 Amazon Mechanical Turk
Amazon Mechanical Turk (MTurk)1 is a market-
place for so-called ?human intelligence tasks,? or
HITs. MTurk has two kinds of users: providers and
1http://mturk.amazon.com
workers. Providers create HITs using the Mechan-
ical Turk API and, for a small fee, upload them to
the HIT database. Workers search through the HIT
database, choosing which to complete in exchange
for monetary compensation. Anyone can sign up as
a provider and/or worker. Each HIT has an associ-
ated monetary value, and after reviewing a worker?s
submission, a provider may choose whether to ac-
cept the submission and pay the worker the promised
sum or to reject it and pay the worker nothing. HITs
typically consist of tasks that are easy for humans
but difficult or impossible for computers to complete
quickly or effectively, such as annotating images,
transcribing speech audio, or writing a summary of
a video.
One challenge for requesters using MTurk is that
of filtering out spammers and other workers who
consistently produce low-quality annotations. In or-
der to allow requesters to restrict the range of work-
ers who can complete their tasks, MTurk provides
several types of built-in statistics, known as quali-
fications. One such qualification is approval rating,
a statistic that records a worker?s ratio of accepted
HITs compared to the total number of HITs sub-
mitted by that worker. Providers can require that a
worker?s approval rating be above a certain threshold
before allowing that worker to submit one of his/her
HITs. Country of residence and lifetime approved
number of HITs completed also serve as built-in
qualifications that providers may check before al-
lowing workers to access their HITs.2 Amazon also
allows providers to define their own qualifications.
Typically, provider-defined qualifications are used to
ensure that HITs which require particular skills are
only completed by qualified workers. In most cases,
workers acquire provider-defined qualifications by
completing an online test.
Amazon also provides a mechanism by which
multiple unique workers can complete the same HIT.
The number of times a HIT is to be completed is
known as the number of assignments for the HIT.
By having multiple workers complete the same HIT,
2According to the terms of use, workers are prohibited from
having more than one account, but to the writer?s knowledge
there is no method in place to enforce this restriction. Thus,
a worker with a poor approval rating could simply create a
new account, since all accounts start with an approval rating
of 100%.
196
Subjective senses:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
Objective senses:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
Figure 1: Subjective and objective word sense examples.
techniques such as majority voting among the sub-
missions can be used to aggregate the results for
some types of HITs, resulting in a higher-quality
final answer. Previous work (Snow et al, 2008)
demonstrates that aggregating worker submissions
often leads to an increase in quality.
3 Word Sense Subjectivity
(Wiebe and Mihalcea, 2006) define subjective ex-
pressions as words and phrases being used to ex-
press mental and emotional states, such as specula-
tions, evaluations, sentiments, and beliefs. Many ap-
proaches to sentiment and subjectivity analysis rely
on lexicons of such words (subjectivity clues). How-
ever, such clues often have both subjective and ob-
jective senses, as illustrated by (Wiebe and Mihal-
cea, 2006). Figure 1 provides subjective and objec-
tive examples of senses.
(Akkaya et al, 2009) points out that most sub-
jectivity lexicons are compiled as lists of keywords,
rather than word meanings (senses). Thus, subjec-
tivity clues used with objective senses ? false hits ?
are a significant source of error in subjectivity and
sentiment analysis. SWSD specifically deals with
this source of errors. (Akkaya et al, 2009) shows
that SWSD helps with various subjectivity and sen-
timent analysis systems by ignoring false hits.
4 Annotation Task
4.1 Subjectivity Word Sense Disambiguation
Our target task is Subjectivity Word Sense Disam-
biguation (SWSD). SWSD aims to determine which
word instances in a corpus are being used with sub-
jective senses and which are being used with ob-
jective senses. It can be considered to be a coarse-
grained application-specific WSD that distinguishes
between only two senses: (1) the subjective sense
and (2) the objective sense.
Subjectivity word sense annotation is done in the
following way. We try to keep the annotation task
for the worker as simple as possible. Thus, we do
not directly ask them if the instance of a target word
has a subjective or an objective sense (without any
sense inventory), because the concept of subjectivity
is fairly difficult to explain to someone who does not
have any linguistics background. Instead we show
MTurk workers two sets of senses ? one subjective
set and one objective set ? for a specific target word
and a text passage in which the target word appears.
Their job is to select the set that best reflects the
meaning of the target word in the text passage. The
specific sense set automatically gives us the subjec-
tivity label of the instance. This makes the annota-
tion task easier for them as (Snow et al, 2008) shows
that WSD can be done reliably by MTurk workers.
This approach presupposes a set of word senses that
have been annotated as subjective or objective. The
annotation of senses in a dictionary for subjectivity
is not difficult for an expert annotator. Moreover,
it needs to be done only once per target word, al-
lowing us to collect hundreds of subjectivity labeled
instances for each target word through MTurk.
In this annotation task, we do not inform the
MTurk workers about the nature of the sets. This
means the MTurk workers have no idea that they are
annotating subjectivity of senses; they are just se-
lecting the set which contains a sense matching the
usage in the sentence or being as similar to it as pos-
sible. This ensures that MTurk workers are not bi-
ased by the contextual subjectivity of the sentence
while tagging the target word instance.
197
Sense Set1 (Subjective)
{ look, appear, seem } ? give a certain impression or have a
certain outward aspect; ?She seems to be sleeping?; ?This ap-
pears to be a very difficult problem?; ?This project looks fishy?;
?They appeared like people who had not eaten or slept for a
long time?
{ appear, seem } ? seem to be true, probable, or apparent; ?It
seems that he is very gifted?; ?It appears that the weather in
California is very bad?
Sense Set2 (Objective)
{ appear } ? come into sight or view; ?He suddenly appeared
at the wedding?; ?A new star appeared on the horizon?
{ appear, come out } ? be issued or published, as of news in a
paper, a book, or a movie; ?Did your latest book appear yet??;
?The new Woody Allen film hasn?t come out yet?
{ appear, come along } ? come into being or existence, or ap-
pear on the scene; ?Then the computer came along and changed
our lives?; ?Homo sapiens appeared millions of years ago?
{ appear } ? appear as a character on stage or appear in a play,
etc.; ?Gielgud appears briefly in this movie?; ?She appeared in
?Hamlet? on the London
{ appear } ? present oneself formally, as before a (judicial) au-
thority; ?He had to appear in court last month?; ?She appeared
on several charges of theft?
Figure 2: Sense sets for target word ?appear?.
Below, we describe a sample annotation problem.
An MTurk worker has access to the following two
sense sets of the target word ?appear?, as seen in
Figure 2. The information that the first sense set is
subjective and second sense set is objective is not
available to the worker. The worker is presented
with the following text passage holding the target
word ?appear?.
It?s got so bad that I don?t even know what
to say. Charles |target| appeared |target|
somewhat embarrassed by his own behav-
ior. The hidden speech was coming, I
could tell.
In this passage, the MTurk worker should be able
to understand that ?appeared? refers to the outward
impression given by ?Charles?. This use of appear is
most similar to the first entry in sense set one; thus,
the correct answer for this problem is Sense Set-1.
4.2 Gold Standard
The gold standard dataset, on which we evaluate
MTurk worker annotations, is provided by (Akkaya
et al, 2009). This dataset (called subjSENSEVAL)
consists of target word instances in a corpus labeled
as S or O, indicating whether they are used with
a subjective or objective sense. It is based on the
lexical sample corpora from SENSEVAL1 (Kilgar-
riff and Palmer, 2000), SENSEVAL2 (Preiss and
Yarowsky, 2001), and SENSEVAL3 (Mihalcea and
Edmonds, 2004). SubjSENSEVAL consists of in-
stances for 39 ambiguous (having both subjective
and objective meanings) target words.
(Akkaya et al, 2009) also provided us with sub-
jectivity labels for word senses which are used in the
creation of subjSENSEVAL. Sense labels of the tar-
get word senses are defined on the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3). This means the target words
from SENSEVAL1 have their senses annotated in
the Hector dictionary, while the target words from
SENSEVAL2 and SENSEVAL3 have their senses
annotated in WordNet1.7. We make use of these la-
beled sense inventories to build our subjective and
objective sets of senses, which we present to the
MTurk worker as Sense Set1 and Sense Set2 re-
spectively. We want to have a uniform sense rep-
resentation for the words we ask subjectivity sense
labels for. Thus, we consider only SENSEVAL2 and
SENSEVAL3 subsets of subjSENSEVAL, because
SENSEVAL1 relies on a sense inventory other than
WordNet.
5 Experimental Design
We chose randomly 8 target words that have a distri-
bution of subjective and objective instances in sub-
jSENSEVAL with less skew than 75%. That is, no
more than 75% of a word?s senses are subjective or
objective. Our concern is that using skewed data
might bias the workers to choose from the more fre-
quent label without thinking much about the prob-
lem. Another important fact is that these words with
low skew are more ambiguous and responsible for
more false hits. Thus, these target words are the ones
for which we really need subjectivity word sense
disambiguation. For each of these 8 target words, we
select 40 passages from subjSENSEVAL in which
the target word appears, to include in our experi-
ments. Table 1 summarizes the selected target words
198
Word FLP Word FLP
appear 55% fine 72.5%
judgment 65% solid 55%
strike 62.5% difference 67.5%
restraint 70% miss 50%
Average 62.2%
Table 1: Frequent label percentages for target words.
and their label distribution. In this table, frequent la-
bel percentage (FLP) represents the skew for each
word. A word?s FLP is equal to the percent of the
senses that are of the most frequently occurring type
of sense (subjective or objective) for that word.
We believe this annotation task is a good candi-
date for attracting spammers. This task requires only
binary annotations, where the worker just chooses
from one of the two given sets, which is not a dif-
ficult task. Since it is easy to provide labels, we
believe that there will be a distinct line, with re-
spect to quality of annotations, between spammers
and mediocre annotators.
For our experiments, we created three different
HIT groups each having different qualification re-
quirements but sharing the same data. To be con-
crete, each HIT group consists of the same 320 in-
stances: 40 instances for each target word listed in
Table 1. Each HIT presents an MTurk worker with
four instances of the same word in a text passage
? this makes 80 HITs for each HIT group ? and
asks him to choose the set to which the activated
sense belongs. We know for each HIT the mapping
between sense set numbers and subjectivity. Thus,
we can evaluate each HIT response on our gold-
standard data, as discussed in Section 4.2. We pay
seven cents per HIT. We consider this to be generous
compensation for such a simple task.
There are many builtin qualifications in MTurk.
We concentrated only on three of them: location,
HIT approval rate, and approved HITs, as discussed
in Section 2. In our experience, these qualifications
are widely used for quality assurance. As mentioned
before, we created three different HIT groups in or-
der to see how well different built-in qualification
combinations do with respect to filtering spammers.
These groups ? starting from the least constrained to
the most constrained ? are listed in Table 2.
Group1 Location: USA
Group2 Location: USAHIT Approval Rate > 96%
Group3
Location: USA
HIT Approval Rate > 96%
Approved HITs > 500
Table 2: Constraints for each HIT group.
Group1 required only that the MTurk workers are
located in the US. This group is the least constrained
one. Group2 additionally required an approval rate
greater than 96%. Group3 is the most constrained
one, requiring a lifetime approved HIT number to
be greater than 500, in addition to the qualifications
in Group1 and Group2.
We believe that neither location nor approval rate
and location together is enough to avoid spammers.
While being a US resident does to some extent guar-
antee English proficiency, it does not guarantee well-
thought answers. Since there is no mechanism in
place preventing users from creating new MTurk
worker accounts at will and since all worker ac-
counts are initialized with a 100% approval rate, we
do not think that approval rate is sufficient to avoid
serial spammers and other poor annotators. We hy-
pothesize that the workers with high approval rate
and a large number of approved HITs have a reputa-
tion to maintain, and thus will probably be careful in
their answers. We think it is unlikely that spammers
will have both a high approval rate and a large num-
ber of completed HITs. Thus, we anticipated that
Group3?s annotations will be of higher quality than
those of the other groups.
Note that an MTurk worker who has access to the
HITs in one of the HIT groups also has access to
HITs in less constrained groups. For example, an
MTurk worker who has access to HITs in Group3
also has access to HITs in Group2 and Group1. We
did not prevent MTurk workers from working in
multiple HIT groups because we did not want to
influence worker behavior, but instead simulate the
most realistic annotation scenario.
In addition to the qualifications described above,
we also required each worker to take a qualification
test in order to prove their competence in the anno-
tation task. The qualification test consists of 10 sim-
199
Figure 3: Venn diagram illustrating worker distribution.
ple annotation questions identical in form to those
present in the HITs. These questions are split evenly
between two target words, ?appear? and ?restraint?.
There are a total of five subjective and five objective
usages in the test. We required an accuracy of 90%
in the qualification test, corresponding to a Kappa
score of .80, before a worker was allowed to submit
any of our HITs. If a worker failed to achieve a score
of 90% on an attempt, that worker could try the test
again after a delay of 4 hours.
We collected three sets of assignments within
each HIT group. In other words, each HIT was com-
pleted three times by three different workers in each
group. This gives us a total of 960 assignments in
each HIT group. A total of 26 unique workers par-
ticipated in the experiment: 17 in Group1, 17 in
Group2 and 8 in Group3. As mentioned before, a
worker is able to participate in all the groups for
which he is qualified. Thus the unique worker num-
bers in each group does not sum up to the total num-
ber of workers in the experiment, since some work-
ers participated in the HITs for more than one group.
Figure 3 summarizes how workers are distributed
between groups.
6 Evaluation
We are interested in how accurate the MTurk annota-
tions are with respect to gold-standard data. We are
also interested in how the accuracy of each group
differs from the others. We evaluate each group it-
self separately on the gold-standard data. Addition-
ally, we evaluate each worker?s performance on the
gold-standard data and inspect their distribution in
various groups.
6.1 Group Evaluation
As mentioned in the previous section, we collect
three annotations for each HIT. They are assigned to
respective trials in the order submitted by the work-
ers. The results are summarized in Table 3. Trials
are labeled as TX and MV is the majority vote an-
notation among the three trials. The final column
contains the baseline agreement where a worker la-
bels each instance of a word with the most frequent
label of that word in the gold-standard data. It is
clear from this table that, since worker accuracy
always exceeds the baseline agreement, subjectiv-
ity word sense annotation can be done reliably by
MTurk workers. This is very promising. Consid-
ering the low cost and low time required to obtain
MTurk annotations, a large scale SWSD is realis-
tic. For example, (Akkaya et al, 2009) shows that
the most frequent 80 lexicon keywords are respon-
sible for almost half of the false hits in the MPQA
Corpus3 (Wiebe et al, 2005; Wilson, 2008), a cor-
pus annotated for subjective expressions. Utilizing
MTurk to collect training data for these 80 lexicon
keywords will be quick and cheap and most impor-
tantly reliable.
When we compare groups with each other, we
see that the best trial result is achieved in Group3.
However, according to McNemar?s test (Dietterich,
1998), there is no statistically significant difference
between any trial of any group. On the other hand,
the best majority vote annotation is achieved in
Group2, but again there is no statistically significant
difference between any majority vote annotation of
any group. These results are surprising to us, since
we do not see any significant difference in the qual-
ity of the data throughout different groups.
6.2 Worker Evaluation
In this section, we evaluate all 26 workers and group
them as either spammers or well-meaning workers.
All workers who deviate from the gold-standard by a
3http://www.cs.pitt.edu/mpqa/
200
Group3 Group2 Group1 baseline
T1 T2 T3 MV T1 T2 T3 MV T1 T2 T3 MV
Accuracy 89.7 86.9 86.6 88.4 87.2 86.3 88.1 90.3 84.4 87.5 87.5 88.4 62.2
Kappa .79 .74 .73 .77 .74 .73 .76 .81 .69 .75 .75 .77
Table 3: Accuracy and kappa scores for each group of workers.
Threshold 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
Spammer Count
G1 2 2 2 2 2 4 7 9
G2 1 2 2 2 2 3 5 8
G3 0 0 0 0 0 0 2 2
Spammer Percentage
G1 12% 12% 12% 12% 12% 24% 41% 53%
G2 6% 12% 12% 12% 12% 12% 29% 42%
G3 0% 0% 0% 0% 0% 0% 25% 25%
Table 4: Spammer representation in groups.
large margin beyond a certain threshold will be con-
sidered to be spammers. As discussed in Section 5,
we require all participating workers to pass a quali-
fication test before answering HITs. Thus, we know
that they are competent to do subjectivity sense an-
notations, and providing consistently erroneous an-
notations means that they are probably spammers.
We think a kappa score of 0.6 is a good threshold
to distinguish spammers from well-meaning work-
ers. For this threshold, we had 2 spammers par-
ticipating in Group1, 2 spammers in Group2 and
0 spammers in Group3. Table 4 presents spammer
count and spammer percentage in each group for
various threshold values. We see that Group3 has
consistently fewer spammers and a smaller spammer
percentage. The lowest kappa scores for Group1,
Group2, and Group3 are .35, .40, and .69, respec-
tively. The mean kappa scores for Group1, Group2,
and Group3 are .73, .75, and .77, respectively.
These results indicate that Group3 is less prone
to spammers, apparently contradicting Section 6.1.
We see the reason when we inspect the data more
closely. It turns out that spammers contributed in
Group1 and Group2 only minimally. On the other
hand there are two mediocre workers (Kappa of
0.69) who submit around 1/3 of the HITs in Group3.
This behavior might be a coincidence. In the face of
contradicting results, we think that we need a more
extensive study to derive conclusions about the rela-
tion between spammer distribution and built-in qual-
ification.
6.3 Learning Effect
Expert annotators can learn to provide more accu-
rate annotations over time. (Passonneau et al, 2006)
reports a learning effect early in the annotation pro-
cess. This might be due to the formal and informal
interaction between annotators. Another possibility
is that the annotators might get used to the annota-
tion task over time. This is to be expected if there is
not an extensive training process before the annota-
tion takes place.
On the other hand, the MTurk workers have no
interaction among themselves. They do not receive
any formal training and do not have access to true
annotations except a few examples if provided by
the requester. These properties make MTurk work-
ers a unique annotation workforce. We are interested
if the learning effect common to expert annotators
holds in this unique workforce in the absence of any
interaction and feedback. That may justify working
with the same set of workers over a long time by
creating private groups of workers.
We sort annotations of a worker after the submis-
sion date. This way, we get for each worker an or-
dered list of annotations. We split the list into bins
of size 40 and we test for an increasing trend in
the proportion of successes over time. We use the
Chi-squared Test for binomial proportions (Rosner,
2006). Using this test, we find that all of the p-values
201
are substantially larger than 0.05. Thus, there is no
increasing trend in the proportion of successes and
no learning effect. This is true for both mediocre
workers and very reliable workers. We think that the
results may differ for harder annotation tasks where
the input is more complex and requires some adjust-
ment.
7 Related Work
There has been recently an increasing interest in
Amazon Mechanical Turk. Many researchers have
utilized MTurk as a source of non-expert natural
language annotation to create labeled datasets. In
(Mrozinski et al, 2008), MTurk workers are used to
create a corpus of why-questions and corresponding
answers on which QA systems may be developed.
(Kaisser and Lowe, 2008) work on a similar task.
They make use of MTurk workers to identify sen-
tences in documents as answers and create a corpus
of question-answer sentence pairs. MTurk is also
considered in other fields than natural language pro-
cessing. For example, (Sorokin and Forsyth, 2008)
utilizes MTurk for image labeling. Our ultimate goal
is similar; namely, to build training data (in our case
for SWSD).
Several studies have concentrated specifically on
the quality aspect of the MTurk annotations. They
investigated methods to assess annotation quality
and to aggregate multiple noisy annotations for high
reliability. (Snow et al, 2008) report MTurk an-
notation quality on various NLP tasks (e.g. WSD,
Textual Entailment, Word Similarity) and define
a bias correction method for non-expert annota-
tors. (Callison-Burch, 2009) uses MTurk workers
for manual evaluation of automatic translation qual-
ity and experiments with weighed voting to com-
bine multiple annotations. (Hsueh et al, 2009) de-
fine various annotation quality measures and show
that they are useful for selecting annotations leading
to more accurate classifiers. Our work investigates
the effect of built-in qualifications on the quality of
MTurk annotations.
(Hsueh et al, 2009) applies MTurk to get senti-
ment annotations on political blog snippets. (Snow
et al, 2008) utilizes MTurk for affective text annota-
tion task. In both works, MTurk workers annotated
larger entities but on a more detailed scale than we
do. (Snow et al, 2008) also provides a WSD anno-
tation task which is similar to our annotation task.
The difference is the MTurk workers are choosing
an exact sense not a sense set.
8 Conclusion and Future Work
In this paper, we address the question of whether
built-in qualifications are enough to avoid spam-
mers. The investigation of worker performances
indicates that the lesser constrained a group is the
more spammers it attracts. On the other hand, we did
not find any significant difference between the qual-
ity of the annotations for each group. It turns out that
workers considered as spammers contributed only
minimally. We do not know if it is just a coincidence
or if it is correlated to the task definition. We did not
get conclusive results. We need to do more extensive
experiments before arriving at conclusions.
Another aspect we investigated is the learning ef-
fect. Our results show that there is no improvement
in annotator reliability over time. We should not ex-
pect MTurk workers to provide more consistent an-
notations over time. This will probably be the case
in similar annotation tasks. For harder annotation
tasks (e.g. parse tree annotation) things may be dif-
ferent. An interesting follow-up would be whether
showing the answers of other workers on the same
HIT will promote learning.
We presented our subjectivity sense annotation
task to the worker in a very simple way. The an-
notation results prove that subjectivity word sense
annotation can be done reliably by MTurk workers.
This is very promising since the MTurk annotations
can be collected for low costs in a short time pe-
riod. This implies that a large scale general SWSD
component, which can help with various subjectivity
and sentiment analysis tasks, is feasible. We plan to
work with selected workers to collect new annotated
data for SWSD and use this data to train a SWSD
system.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation awards IIS-
0916046 and IIS-0917170 and by Department of
Homeland Security award N000140710152. The au-
thors are grateful to the three paper reviewers for
their helpful suggestions.
202
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 286?295, Morristown, NJ,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10:1895?1923.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Kaisser and John Lowe. 2008. Creat-
ing a research collection of question answer sen-
tence pairs with amazons mechanical turk. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a why-question corpus for develop-
ment and evaluation of an automatic QA-system. In
Proceedings of ACL-08: HLT, pages 443?451, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hwee Tou Ng. 1997. Getting serious about word sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why,What, and How?
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC).
Bernard Rosner. 2006. Fundamentals of Biostatistics.
Thompson Brooks/Cole.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
A. Sorokin and D. Forsyth. 2008. Utility data annotation
with amazon mechanical turk. pages 1 ?8, june.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In (ACL-06), Sydney, Australia.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
203
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 87?96,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Improving the Impact of Subjectivity Word Sense Disambiguation on
Contextual Opinion Analysis
Cem Akkaya, Janyce Wiebe, Alexander Conrad
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe,conrada}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is automatically determining which
word instances in a corpus are being used with
subjective senses, and which are being used
with objective senses. SWSD has been shown
to improve the performance of contextual
opinion analysis, but only on a small scale and
using manually developed integration rules.
In this paper, we scale up the integration of
SWSD into contextual opinion analysis and
still obtain improvements in performance,
by successfully gathering data annotated by
non-expert annotators. Further, by improving
the method for integrating SWSD into con-
textual opinion analysis, even greater benefits
from SWSD are achieved than in previous
work. We thus more firmly demonstrate the
potential of SWSD to improve contextual
opinion analysis.
1 Introduction
Often, methods for opinion, sentiment, and sub-
jectivity analysis rely on lexicons of subjective
(opinion-carrying) words (e.g., (Turney, 2002;
Whitelaw et al, 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al, 2007; Andreevskaia and Bergler, 2008;
Agarwal et al, 2009)). Examples of such words are
the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
However, even manually developed subjectiv-
ity lexicons have significant degrees of subjectivity
sense ambiguity (Su and Markert, 2008; Gyamfi et
al., 2009). That is, many clues in these lexicons have
both subjective and objective senses. This ambiguity
leads to errors in opinion and sentiment analysis, be-
cause objective instances represent false hits of sub-
jectivity clues. For example, the following sentence
contains the keywords from (1) used with objective
senses:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
Recently, in (Akkaya et al, 2009), we introduced
the task of subjectivity word sense disambiguation
(SWSD), which is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with objec-
tive senses. We developed a supervised system for
SWSD, and exploited the SWSD output to improve
the performance of multiple contextual opinion anal-
ysis tasks.
Although the reported results are promising, there
are three obvious shortcomings. First, we were able
to apply SWSD to contextual opinion analysis only
on a very small scale, due to a shortage of anno-
tated data. While the experiments show that SWSD
improves contextual opinion analysis, this was only
on the small amount of opinion-annotated data that
was in the coverage of our system. Two questions
arise: is it feasible to obtain greater amounts of
the needed data, and do SWSD performance im-
provements on contextual opinion analysis hold on a
87
larger scale. Second, the annotations in (Akkaya et
al., 2009) are piggy-backed on SENSEVAL sense-
tagged data, which are fine-grained word sense an-
notations created by trained annotators. A concern
is that SWSD performance improvements on con-
textual opinion analysis can only be achieved using
such fine-grained expert annotations, the availability
of which is limited. Third, (Akkaya et al, 2009) uses
manual rules to apply SWSD to contextual opinion
analysis. Although these rules have the advantage
that they transparently show the effects of SWSD,
they are somewhat ad hoc. Likely, they are not opti-
mal and are holding back the potential of SWSD to
improve contextual opinion analysis.
To address these shortcomings, in this paper, we
investigate (1) the feasibility of obtaining a substan-
tial amount of annotated data, (2) whether perfor-
mance improvements on contextual opinion analy-
sis can be realized on a larger scale, and (3) whether
those improvements can be realized with subjectiv-
ity sense tagged data that is not built on expert full-
inventory sense annotations. In addition, we explore
better methods for applying SWSD to contextual
opinion analysis.
2 Subjectivity Word Sense Disambiguation
2.1 Annotation Tasks
We adopt the definitions of subjective (S) and ob-
jective (O) from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2007). Subjective expressions
are words and phrases being used to express mental
and emotional states, such as speculations, evalua-
tions, sentiments, and beliefs. A general covering
term for such states is private state (Quirk et al,
1985), an internal state that cannot be directly ob-
served or verified by others. Objective expressions
instead are words and phrases that lack subjectivity.
The contextual opinion analysis experiments de-
scribed in Section 3 include both S/O and polar-
ity (positive,negative, neutral) classifications. The
opinion-annotated data used in those experiments is
from the MPQA Corpus (Wiebe et al, 2005; Wilson,
2007),1 which consists of news articles annotated for
subjective expressions, including polarity.
1Available at http://www.cs.pitt.edu/mpqa
2.1.1 Subjectivity Sense Labeling
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjectiv-
ity. As noted in (Wiebe and Mihalcea, 2006), sen-
tences containing objective senses may not be objec-
tive. Thus, objective senses are defined as follows:
Classifying a sense as O means that, when the sense
is used in a text or conversation, one does not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) performed agreement studies of the
scheme and report that good agreement can be
achieved between human annotators labeling the
subjectivity of senses (? values of 0.74 and 0.79, re-
spectively).
(Akkaya et al, 2009) followed the same annota-
tion scheme to annotate the senses of the words used
in the experiments. For this paper, we again use
the same scheme and annotate WordNet senses of
90 new words (the process of selecting the words is
described in Section 2.4).
2.1.2 Subjectivity Sense Tagging
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, in-
dicating whether they are used with a subjective or
objective sense.
Because there was no such tagged data at the time,
(Akkaya et al, 2009) created a data set by com-
bining two types of sense annotations: (1) labels of
senses within a dictionary as S or O (i.e., the subjec-
tivity sense labels of the previous section), and (2)
sense tags of word instances in a corpus (i.e., SEN-
SEVAL sense-tagged data).2 The subjectivity sense
labels were used to collapse the sense labels in the
sense-tagged data into the two new senses, S and O.
The target words (Akkaya et al, 2009) chose are the
words tagged in SENSEVAL that are also members
2Please see the paper for details on the SENSEVAL data
used in the experiments.
88
Sense Set1 (Subjective)
{ attack, round, assail, lash out, snipe, assault } ? attack in
speech or writing; ?The editors attacked the House Speaker?
{ assail, assault, set on, attack } ? attack someone emotionally;
?Nightmares assailed him regularly?
Sense Set2 (Objective)
{ attack } ? begin to injure; ?The cancer cells are attacking his
liver?; ?Rust is attacking the metal?
{ attack, aggress } ? take the initiative and go on the offensive;
?The visiting team started to attack?
Figure 1: Sense sets for target word ?attack? (abridged).
of the subjectivity lexicon of (Wilson et al, 2005;
Wilson, 2007).3 There are 39 such words. (Akkaya
et al, 2009) chose words from a subjectivity lexicon
because such words are known to have subjective
usages.
For this paper, subjectivity sense-tagged data was
obtained from the MTurk workers using the anno-
tation scheme of (Akkaya et al, 2010). A goal is to
keep the annotation task as simple as possible. Thus,
the workers are not directly asked if the instance of
a target word has a subjective or an objective sense,
because the concept of subjectivity would be diffi-
cult to explain in this setting. Instead the workers
are shown two sets of senses ? one subjective set and
one objective set ? for a specific target word and a
text passage in which the target word appears. Their
job is to select the set that best reflects the meaning
of the target word in the text passage. The set they
choose gives us the subjectivity label of the instance.
A sample annotation task is shown below. An
MTurk worker has access to two sense sets of the
target word ?attack? as seen in Figure 1. The S and
O labels appear here only for the purpose of this pa-
per; the workers do not see them. The worker is pre-
sented with the following text passage holding the
target word ?attack?:
Ivkovic had been a target of intra-party
feuding that has shaken the party. He was
attacked by Milosevic for attempting to
carve out a new party from the Socialists.
In this passage, the use of ?attack? is most similar
to the first entry in sense set one; thus, the correct
answer for this problem is Sense Set-1.
3Available at http://www.cs.pitt.edu/mpqa
(Akkaya et al, 2010) carried out a pilot study
where a subjectivity sense-tagged dataset was cre-
ated for eight SENSEVAL words through MTurk.
(Akkaya et al, 2010) evaluated the non-expert la-
bel quality against gold-standard expert labels which
were obtained from (Akkaya et al, 2009) relying
on SENSEVAL. The non-expert annotations are reli-
able, achieving ? scores around 0.74 with the expert
annotations.
For some words, there may not be a clean split be-
tween the subjective and objective senses. For these,
we opted for another strategy for obtaining MTurk
annotations. Rather than presenting the workers
with WordNet senses, we show them a set of objec-
tive usages, a set of subjective usages, and a text pas-
sage in which the target word appears. The workers?
job is to judge which set of usages the target instance
is most similar to.
2.2 SWSD System
We follow the same approach as in (Akkaya et al,
2009) to build our SWSD system. We train a differ-
ent supervised SWSD classifier for each target word
separately. This means the overall SWSD system
consists of as many SWSD classifiers as there are
target words. We utilize the same machine learning
features as in (Akkaya et al, 2009), which are com-
monly used in Word Sense Disambiguation (WSD).
2.3 Expert SWSD vs. Non-expert SWSD
Before creating a large subjectivity sense-tagged
corpus via MTurk, we want to make sure that non-
expert annotations are good enough to train reliable
SWSD classifiers. Thus, we decided to compare
the performance of a SWSD system trained on non-
expert annotations and on expert annotations. For
this purpose, we need a subjectivity sense-tagged
corpus where word instances are tagged both by ex-
pert and non-expert annotations. Fortunately, we
have such a corpus. As discussed in Section 3,
(Akkaya et al, 2009) created a subjecvitivity sense-
tagged corpus piggybacked on SENSEVAL. This
gives us a gold-standard corpus tagged by experts.
There is also a small subjectivity sense-tagged cor-
pus consisting of eight target words obtained from
non-expert annotators in (Akkaya et al, 2010). This
corpus is a subset of the gold-standard corpus from
(Akkaya et al, 2009) and it consists of 60 tagged
89
Acc p-value
SWSDGOLD 79.2 -
SWSDMJL 78.4 0.542
SWSDMJC 78.8 0.754
Table 1: Comparison of SWSD systems
instances for each target word.
Actually, (Akkaya et al, 2010) gathered three la-
bels for each instance. This gives us two options
to train the non-expert SWSD system: (1) training
the system on the majority vote labels (SWSDMJL)
(2) training three systems on the three separate la-
bel sets and taking the majority vote prediction
(SWSDMJC). Additionally, we train an expert SWSD
system (SWSDGOLD) ? a system trained on gold
standard expert annotations. All these systems are
trained on 60 instances of the eight target words for
which we have both non-expert and expert annota-
tions and are evaluated on the remaining instances
of the gold-standard corpus. This makes a total of
923 test instances for the eight target words with a
majority class baseline of 61.8.
Table 1 reports micro-average accuracy of each
system and the two-tailed p-value between the ex-
pert SWSD system and the two non-expert SWSD
systems. The p-value is calculated with McNemar?s
test. It shows that there is no statistically signif-
icant difference between classifiers trained on ex-
pert gold-standard annotations and non-expert anno-
tations. We adopt SWSDMJL in all our following ex-
periments, because it is more efficient.
2.4 Corpus Creation
For our experiments, we have multiple goals, which
effect our decisions on how to create the subjectiv-
ity sense-tagged corpus via MTurk. First, we want
to be able to disambiguate more target words than
(Akkaya et al, 2009). This way, SWSD will be able
to disambiguate a larger portion of the MPQA Cor-
pus allowing us to evaluate the effect of SWSD on
contextual opinion analysis on a larger scale. This
will also allow us to investigate additional integra-
tion methods of SWSD into contextual opinion anal-
ysis rather than simple ad hoc manual rules utilized
in (Akkaya et al, 2009). Second, we want to show
that we can rely on non-expert annotations instead of
expert annotations, which will make an annotation
effort on a larger-scale both practical and feasible,
timewise and costwise. Optimally, we could have
annotated via MTurk the same subjectivity sense-
tagged corpus from (Akkaya et al, 2009) in order to
compare the effect of a non-expert SWSD system on
contextual opinion analysis directly with the results
reported for an expert SWSD system in (Akkaya et
al., 2009). But, this would have diverted our re-
sources to reproduce the same corpus and contradict
our goal to extend the subjectivity sense-tagged cor-
pus to new target words. Moreover, we have already
shown in Section 2.3 that non-expert annotations can
be utilized to train reliable SWSD classifiers. It is
reasonable to believe that similar performance on
the SWSD task will reflect to similar improvements
on contextual opinion analysis. Thus, we decided
to prioritize creating a subjectivity sense-tagged cor-
pus for a totally new set of words. We aim to show
that the favourable results reported in (Akkaya et al,
2009) will still hold on new target words relying on
non-expert annotations.
We chose our target words from the subjectivity
lexicon of (Wilson et al, 2005), because we know
they have subjective usages. The contextual opin-
ion systems we want to improve rely on this lexicon.
We call the words in the lexicon subjectivity clues.
At this stage, we want to concentrate on the fre-
quent and ambiguous subjectivity clues. We chose
frequent ones, because they will have larger cov-
erage in the MPQA Corpus. We chose ambiguous
ones, because these clues are the ones that are most
important for SWSD. Choosing most frequent and
ambiguous subjectivity clues guarantees that we uti-
lize our limited resources in the most efficient way.
We judge a clue to be ambiguous if it appears more
than 25% and less than 75% of the times in a sub-
jective expression. We get these statistics by simply
counting occurrences in the MPQA Corpus inside
and outside of subjective expressions.
There are 680 subjectivity clues that appear in the
MPQA Corpus and are ambiguous. Out of those, we
selected the 90 most frequent that have to some ex-
tent distinct objective and subjective senses in Word-
Net, as judged by the co-authors. The co-authors an-
notated the WordNet senses of those 90 target words.
For each target word, we selected approximately 120
instances randomly from the GIGAWORD Corpus.
In a first phase, we collected three sets of MTurk an-
90
notations for the selected instances. In this phase,
MTurk workers base their judgements on two sense
sets they observe. This way, we get training data to
build SWSD classifiers for these 90 target words.
The quality of these classifiers is important, be-
cause we will exploit them for contextual opinion
analysis. Thus, we evaluate them by 10-fold cross-
validation. We split the target words into three
groups. If the majority class baseline of a word is
higher than 90%, it is considered as skewed (skewed
words have a performance at least as good as the ma-
jority class baseline). If a target word improves over
its majority class baseline by 25% in accuracy, it is
considered as good. Otherwise, it is considered as
mediocre. This way, we end up with 24 skewed, 35
good, and 31 mediocre words. There are many pos-
sible reasons for the less reliable performance for
the mediocre group. We hypothesize that a major
problem is the similarity between the objective and
subjective sense sets of a word, thus leading to poor
annotation quality. To check this, we calculate the
agreement between three annotation sets and report
averages. The agreement in the mediocre group is
78.68%, with a ? value of 0.57, whereas the aver-
age agreement in the good group is 87.51%, with
a ? value of 0.75. These findings support our hy-
pothesis. Thus, the co-authors created usage inven-
tories for the words in the mediocre group as de-
scribed in Section 2.1.1. We initiated a second phase
of MTurk annotations. We collect for the mediocre
group another three sets of MTurk annotations for
120 instances, this time utilizing usage inventories.
The 10-fold cross-validation experiments show that
nine of the 31 words in the mediocre group shift to
the good group. Only for these nine words, we ac-
cept the annotations collected via usage inventories.
For all other words, we use the annotations collected
via sense inventories. From now on, we will refer
to this non-expert subjectivity sense-tagged corpus
consisting of the tagged data for all 90 target words
as the MTurkSWSD Corpus (agreement on the entire
MTurkSWSD corpus is 85.54%, ?:0.71).
3 SWSD Integration
Now that we have the MTurkSWSD Corpus, we
are ready to evaluate the effect of SWSD on con-
textual opinion analysis. In this section, we ap-
ply our SWSD system trained on MTurkSWSD to
both expression-level classifiers from (Akkaya et al,
2009): (1) the subjective/objective (S/O) classifier
and (2) the contextual polarity classifier. Both clas-
sifiers are introduced in Section 3.1
Our SWSD system can disambiguate 90 target
words, which have 3737 instances in the MPQA
Corpus. We refer to this subset of the MPQA Corpus
as MTurkMPQA. This subset makes up the cover-
age of our SWSD system. Note that MTurkMPQA
is 5.2 times larger than the covered MPQA subset
in (Akkaya et al, 2009) referred as senMPQA. We
try different strategies to integrate SWSD into the
contextual classifiers. In Section 3.2, we follow the
same rule-based strategy as in (Akkaya et al, 2009)
for completeness. In Section 3.3, we introduce two
new learning strategies for SWSD integration out-
performing existing rule-based strategy. We evalu-
ate the improvement gained by SWSD on MTurkM-
PQA.
3.1 Contextual Classifiers
The original contextual polarity classifier is intro-
duced in (Wilson et al, 2005). We use the same im-
plementation as in (Akkaya et al, 2009). This classi-
fier labels clue instances in text as contextually neg-
ative/positive/neutral. The gold standard is defined
on the MPQA Corpus as follows. If a clue instance
appears in a positive expression, it is contextually
positive (Ps). If it appears in a negative expression,
it is contextually negative (Ng). If it is in an objec-
tive expression or in a neutral subjective expression,
it is contextually neutral (N). The contextual polar-
ity classifier consists of two separate steps. The first
step is an expression-level neutral/polar (N/P) clas-
sifier. The second step classifies only polar instances
further into positive and negative classes. This way,
the overall system performs a three-way classifica-
tion (Ng/Ps/N).
The subjective/objective classifier is introduced in
(Akkaya et al, 2009). It relies on the same machine
learning features as the N/P classifier (i.e. the first
step of the contextual polarity classifier). The only
difference is that the classes are S/O instead of N/P.
The gold standard is defined on the MPQA Corpus
in the following way. If a clue instance appears in
a subjective expression, it is contextually S. If it ap-
pears in an objective expression, it is contextually O.
Both contextual classifiers are supervised.
91
Baseline Acc OF SF
MTurkMPQA 52.4% (O)
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
senMPQA 63.1% (O)
OS/O 75.4 65.4 80.9
R1R2 81.3 75.9 84.8
Table 2: S/O classifier with and without SWSD.
3.2 Rule-Based SWSD Integration
(Akkaya et al, 2009) integrates SWSD into a con-
textual classifier by simple rules. The rules flip the
output of the contextual classifier if some conditions
hold. They make use of following information: (1)
SWSD output, (2) the contextual classifier?s confi-
dence and (3) the presence of another subjectivity
clue ? any clue from the subjectivity lexicon ? in the
same expression.
For the contextual S/O classifier, (Akkaya et al,
2009) defines two rules: one flipping the S/O classi-
fier?s output from O to S (R1) and one flipping from
S to O (R2). R1 is defined as follows : if the contex-
tual classifier decides a target word instance is con-
textually O and SWSD decides that it is used in a S
sense, then SWSD overrules the contextual S/O clas-
sifier?s output and flips it from O to S, because an
instance in a S sense will make the surrounding ex-
pression subjective. R2 is a little bit more complex.
It is defined as follows: If the contextual classifier la-
bels a clue instance as S but (1) SWSD decides that
it is used in an O sense, (2) the contextual classifier?s
confidence is low, and (3) there is no other subjec-
tivity clue in the same expression, then R2 flips the
contextual classifier?s output from S to O. The ra-
tionale behind R2 is that even if the target word in-
stance has an O sense, there might be another reason
(e.g. the presence of another subjectivity clue in the
same expression) for the expression enclosing it to
be subjective.
We use the exact same rules and adopt the same
confidence threshold. Table 2 holds the comparison
of the original contextual classifier and the classi-
fier with SWSD support on senMPQA as reported in
(Akkaya et al, 2009) and on MTurkMPQA. OS/O is
the original S/O classifier; R1R2 is the system with
SWSD support utilizing both rules. We report only
R1R2, since (Akkaya et al, 2009) gets highest im-
provement utilizing both rules.
Baseline Acc NF PF
MTurkMPQA 70.6% (P)
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
senMPQA 73.9% (P)
ON/P 79.0 86.7 50.3
R4 81.6 88.6 52.3
Table 3: N/P classifier with and without SWSD
In Table 2 we see that R1R2 achieves 4% percent-
age points improvement in accuracy over OS/O on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test. It
is accompanied with improvements both in subjec-
tive F-measure (SF) and objective F-measure (OF).
It is not possible to directly compare improvements
on senMPQA and MTurkMPQA since they are dif-
ferent subsets of the MPQA Corpus. SWSD support
brings 24% error reduction on senMPQA over the
original S/O classifier. In comparison, on MTurkM-
PQA, the error reduction is 12%. We see that the im-
provements on the large MTurkMPQA set still hold,
but not as strong as in (Akkaya et al, 2009).
(Akkaya et al, 2009) uses a similar rule to
make the contextual polarity classifier sense-aware.
Specifically, the rule is applied to the output of the
first step (N/P classifier). The rule, R4, flips P to N
and is analogous to R2. If the contextual classifier
labels a clue instance as P but (1) SWSD decides
that it is used in an O sense, (2) the contextual clas-
sifier?s confidence is low, and (3) there is no other
clue instance in the same expression, then R4 flips
the contextual classifier?s output from P to N.
Table 3 holds the comparison of the original N/P
classifier with and without SWSD support on sen-
MPQA as reported in (Akkaya et al, 2009) and on
MTurkMPQA. ON/P is the original N/P classifier; R4
is the system with SWSD support utilizing rule R4.
Since our main focus is not rule-based integration,
we did not run the second step of the polarity classi-
fier. We report the second step result below for the
learning-based SWSD integration in section 3.4.
In Table 3, we see that R4 achieves 2.2 percent-
age points improvement in accuracy over ON/P on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test.
It is accompanied with improvement only in objec-
tive F-measure (OF). SWSD support brings 12.4%
error reduction on senMPQA (Akkaya et al, 2009).
92
On MTurkMPQA, the error reduction is 8%. We see
that the rule-based SWSD integration still improves
both contextual classifiers on MTurkMPQA, but the
gain is not as large as on senMPQA. This might be
due to the brittleness of the rule-based integration.
3.3 Learning SWSD Integration
Now that we can disambiguate a larger portion of
the MPQA Corpus than in (Akkaya et al, 2009),
we can investigate machine learning methods for
SWSD integration to deal with the brittleness of the
rule-based integration. In this section, we introduce
two learning methods to apply SWSD to the contex-
tual classifiers. For the learning methods, we rely on
exactly the same information as the rule-based inte-
gration: (1) SWSD output, (2) the contextual clas-
sifier?s output, (3) the contextual classifier?s confi-
dence, and (4) the presence of another clue instance
in the same expression. The rationale is the same as
for the rule-based integration, namely to relate sense
subjectivity and contextual subjectivity.
3.3.1 Method1
In the first method, we extend the machine learn-
ing features of the underlying contextual classifiers
by adding (1) and (4) from above. We evaluate the
extended contextual classifiers on MTurkMPQA via
10-fold cross-validation. Tables 4 and 5 hold the
comparison of Method1 (EXTS/O, EXTN/P) to the
original contextual classifiers (OS/O, ON/P) and to the
rule-based SWSD integration (R1R2, R4). We see
substantial improvement for Method1. It achieves
39% error reduction over OS/O and 25% error reduc-
tion over ON/P. For both classifiers, the improvement
in accuracy over the rule-based integration is statisti-
cally significant at the p< .01 level with McNemar?s
test.
3.3.2 Method2
This method defines a third classifier that accepts
as input the contextual classifier?s output and the
SWSD output and predicts what the contextual clas-
sifier?s output should have been. We can think of
this third classifier as the learning counterpart of
the manual rules from Section 3.2, since it actu-
ally learns when to flip the contextual classifier?s
output considering SWSD evidence. Specifically,
this merger classifier relies on four machine learn-
ing features (1), (2), (3), (4) from above (the ex-
Acc OF SF
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
EXTS/O 80.0 81.4 78.3
MERGERS/O 78.2 80.3 75.5
Table 4: S/O classifier with learned SWSD integration
Acc NF PF
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
EXTN/P 79.1 85.7 61.1
MERGERN/P 80.4 86.7 62.8
Table 5: N/P classifier with learned SWSD integration
act same information used in rule-based integration).
Because it is a supervised classifier, we need train-
ing data where we have clue instances with cor-
responding contextual classifier and SWSD predic-
tions. Fortunately, we can use senMPQA for this
purpose. We train our merger classifier on senM-
PQA (we get contextual classifier predictions via 10-
fold cross-validation on the MPQA Corpus) and ap-
ply it to MTurkMPQA. We use SVM classifier from
the Weka package (Witten and Frank., 2005) with
its default settings. Tables 4 and 5 hold the com-
parison of Method2 (MERGERS/O, MERGERN/P) to
the original contextual classifiers (Oo/s, ON/P) and
the rule-based SWSD integration (R1R2, R4). It
achieves 29% error reduction over OS/O and 29% er-
ror reduction over ON/P. The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. Method2
performs better (statistically significant at the p <
.05 level) than Method1 for the N/P classifier but
worse (statistically significant at the p < .01 level)
for the S/O classifier.
3.4 Improving Contextual Polarity
Classification
We have seen that Method2 is the best method to
improve the N/P classifier, which is the first step
of the contextual polarity classifier. To assess the
overall improvement in polarity classification, we
run the second step of the contextual polarity clas-
sifier after correcting the first step with Method2.
Table 6 summarizes the improvement propagated to
93
Acc NF NgF PsF
MTurkMPQA
OPs/Ng/N 72.1 83.0 34.2 15.0
MERGERN/P 77.8 87.4 53.0 27.7
senMPQA
OPs/Ng/N 77.6 87.2 39.5 40.0
R4 80.6 89.1 43.2 44.0
Table 6: Polarity classifier with and without SWSD.
Ps/Ng/N classification. For comparison, we also
include results from (Akkaya et al, 2009) on sen-
MPQA. Method2 results in 20% error reduction in
accuracy over OPs/Ng/N (R4 achieves 13.4% error
reduction on senMPQA). The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. More im-
portantly, the F-measure for all the labels improves.
This indicates that non-expert MTurk annotations
can replace expert annotations for our end-goal ? im-
proving contextual opinion analysis ? while reduc-
ing time and cost requirements by a large margin.
Moreover, we see that the improvements in (Akkaya
et al, 2009) scale up to new subjectivity clues.
4 Related Work
One related line of research is to automatically
assign subjectivity and/or polarity labels to word
senses in a dictionary (Valitutti et al, 2004; An-
dreevskaia and Bergler, 2006; Wiebe and Mihalcea,
2006; Esuli and Sebastiani, 2007; Su and Markert,
2009). In contrast, the task in our paper is to auto-
matically assign labels to word instances in a corpus.
Recently, some researchers have exploited full
word sense disambiguation in methods for opinion-
related tasks. For example, (Mart??n-Wanton et al,
2010) exploit WSD for recognizing quotation polar-
ities, and (Rentoumi et al, 2009; Mart??n-Wanton et
al., 2010) exploit WSD for recognizing headline po-
larities. None of this previous work investigates per-
forming a coarse-grained variation of WSD such as
SWSD to improve their application results, as we do
in this work.
A notable exception is (Su and Markert, 2010),
who exploit SWSD to improve the performance on
a contextual NLP task, as we do. While the task
in our paper is subjectivity and sentiment analy-
sis, their task is English-Chinese lexical substitu-
tion. As (Akkaya et al, 2009) did, they anno-
tated word senses, and exploited SENSEVAL data
as training data for SWSD. They did not directly an-
notate words in context with S/O labels, as we do in
our work. Further, they did not separately evaluate a
SWSD system component.
Many researchers work on reducing the granular-
ity of sense inventories for WSD (e.g., (Palmer et al,
2004; Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)). Their criteria for grouping senses are syn-
tactic and semantic similarities, while the groupings
in work on SWSD are driven by the goals to improve
contextual subjectivity and sentiment analysis.
5 Conclusions and Future Work
In this paper, we utilized a large pool of non-expert
annotators (MTurk) to collect subjectivity sense-
tagged data for SWSD. We showed that non-expert
annotations are as good as expert annotations for
training SWSD classifiers. Moreover, we demon-
strated that SWSD classifiers trained on non-expert
annotations can be exploited to improve contextual
opinion analysis.
The additional subjectivity sense-tagged data en-
abled us to evaluate the benefits of SWSD on con-
textual opinion analysis on a corpus of opinion-
annotated data that is five times larger. Using the
same rule-based integration strategies as in (Akkaya
et al, 2009), we found that contextual opinion anal-
ysis is improved by SWSD on the larger datasets.
We also experimented with new learning strategies
for integrating SWSD into contextual opinion analy-
sis. With the learning strategies, we achieved greater
benefits from SWSD than the rule-based integration
strategies on all of the contextual opinion analysis
tasks.
Overall, we more firmly demonstrated the poten-
tial of SWSD to improve contextual opinion analy-
sis. We will continue to gather subjectivity sense-
tagged data, using sense inventories for words that
are well represented in WordNet for our purposes,
and with usage inventories for those that are not.
6 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
94
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 24?32. Asso-
ciation for Computational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 190?199, Singa-
pore, August. Association for Computational Linguis-
tics.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 195?203, Los Angeles, June. Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006).
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT, pages 290?298, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2009), pages
10?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, New York City.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twen-
tieth International Conference on Computational Lin-
guistics (COLING 2004), pages 1267?1373, Geneva,
Switzerland.
Tamara Mart??n-Wanton, Aurora Pons-Porrata, Andre?s
Montoyo-Guijarro, and Alexandra Balahur. 2010.
Opinion polarity detection - using word sense disam-
biguation to determine the polarity of opinions. In
ICAART 2010 - Proceedings of the International Con-
ference on Agents and Artificial Intelligence, Volume
1, pages 483?486.
R. Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applications.
In HLT-NAACL 2004 Workshop: 2nd Workshop on
Scalable Natural Language Understanding, Boston,
Massachusetts.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sentiment
analysis of figurative language using a word sense
disambiguation approach. In Proceedings of the In-
ternational Conference RANLP-2009, pages 370?375,
Borovets, Bulgaria, September. Association for Com-
putational Linguistics.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-2003), pages 105?
112, Sapporo, Japan.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008), Manch-
ester.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 1?9, Boulder, Colorado, June. Associ-
ation for Computational Linguistics.
95
Fangzhong Su and Katja Markert. 2010. Word sense
subjectivity for cross-lingual lexical substitution. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 357?
360, Los Angeles, California, June. Association for
Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-02), pages 417?424, Philadelphia, Pennsyl-
vania.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology, 2(1):61?83.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1065?1072, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques, Second Edi-
tion. Morgan Kaufmann, June.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2003), pages 129?136, Sapporo, Japan.
96
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 2,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Subjectivity Word Sense Disambiguation
(Abstract of Invited Talk)
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Sennott Square Building, Room 5409
210 S. Bouquet St., Pittsburgh, PA 15260, U.S.A.
wiebe@cs.pitt.edu
Abstract
Many approaches to opinion and sentiment
analysis rely on lexicons of words that may be
used to express subjectivity. These are com-
piled as lists of keywords, rather than word
meanings (senses). However, many keywords
have both subjective and objective senses.
False hits ? subjectivity clues used with objec-
tive senses ? are a significant source of error
in subjectivity and sentiment analysis. This
talk will focus on sense-level opinion and sen-
timent analysis. First, I will give the results
of a study showing that even words judged
in previous work to be reliable opinion clues
have significant degrees of subjectivity sense
ambiguity. Then, we will consider the task
of distinguishing between the subjective and
objective senses of words in a dictionary, and
the related task of creating ?usage inventories?
of opinion clues. Given such distinctions, the
next step is to automatically determine which
word instances in a corpus are being used
with subjective senses, and which are being
used with objective senses (we call this task
?SWSD?). We will see evidence that SWSD
is more feasible than full word sense disam-
biguation, because it is more coarse grained
? often, the exact sense need not be pin-
pointed, and that SWSD can be exploited to
improve the performance of opinion and sen-
timent analysis systems via sense-aware clas-
sification. Finally, I will discuss experiments
in acquiring SWSD data, via token-based con-
text discrimination where the context vector
representation is adapted to distinguish be-
tween subjective and objective contexts, and
the clustering process is enriched by pair-wise
constraints, making it semi-supervised.
2
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 80?88, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Recognizing Arguing Subjectivity and Argument Tags
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
Pittsburgh PA, 15260, USA
{conrada,wiebe,hwa}@cs.pitt.edu
Abstract
In this paper we investigate two distinct
tasks. The first task involves detecting ar-
guing subjectivity, a type of linguistic sub-
jectivity on which relatively little work has
yet to be done. The second task involves
labeling instances of arguing subjectivity
with argument tags reflecting the concep-
tual argument being made. We refer to
these two tasks collectively as ?recogniz-
ing arguments?. We develop a new anno-
tation scheme and assemble a new anno-
tated corpus to support our learning ef-
forts. Through our machine learning ex-
periments, we investigate the utility of a
sentiment lexicon, discourse parser, and
semantic similarity measures with respect
to recognizing arguments. By incorpo-
rating information gained from these re-
sources, we outperform a unigram baseline
by a significant margin. In addition, we ex-
plore a two-phase approach to recognizing
arguments, with promising results.
1 Introduction
Subjectivity analysis is a thriving field within
natural language processing. However, most
research into subjectivity has focused on sen-
timent with respect to concrete things such
as product debates (e.g., (Somasundaran and
Wiebe, 2009), (Yu et al, 2011)) and movie re-
views (e.g., (He et al, 2011), (Maas et al, 2011),
(Pang and Lee, 2004)). Analysis often follows
the opinion-target paradigm, in which expres-
sions of sentiment are assessed with respect to
the aspects of the object(s) under consideration
towards which they are targeted. For example,
in the domain of smartphone reviews, aspects
could include product features such as the key-
board, screen quality, and battery life.
Although sentiment analysis is interesting
and important in its own right, this paradigm
does not seem to be the best match for fine-
grained analysis of ideological domains. While
sentiment is also present in documents from
this domain, previous work (Somasundaran and
Wiebe, 2010) has found that arguing subjec-
tivity, a less-studied form of subjectivity, is
more frequently employed and more relevant
for a robust assessment of ideological positions.
Whereas sentiment conveys the polarity of a
writer?s affect towards a topic, arguing subjec-
tivity is a type of linguistic subjectivity in which
a person expresses a controversial belief about
what is true or what action ought to be taken
regarding a central contentious issue (Somasun-
daran, 2010). For example, consider this sen-
tence about health care reform:
(1) Almost everyone knows that we
must start holding insurance compa-
nies accountable and give Americans a
greater sense of stability and security
when it comes to their health care.
In a traditional opinion-target or sentiment-
topic paradigm, perhaps this sentence could be
labeled as containing a negative sentiment to-
wards a topic representing ?insurance compa-
nies?, or a positive sentiment towards a topic
representing ?stability? or ?security?. However,
a reader of a political editorial or blog may be
more interested in why the author is negative to-
80
wards insurers, and how the author proposes to
improve stability of the healthcare system. By
focusing on the arguments conveyed through ar-
guing subjectivity, we aim to capture these kind
of conceptual reasons an author provides when
arguing for his or her position.
However, identifying when someone is arguing
is only part of the challenge. Since arguing sub-
jectivity is used to express arguments, the next
natural step is to identify the argument being
expressed through each instance of arguing sub-
jectivity. To illustrate this distinction, consider
the following three example spans:
(2) the bill is a job destroyer
(3) President Obamas signature do-
mestic policy will throw 100,000 peo-
ple out of work come January
(4) he can?t expand his business be-
cause he can?t afford the burden of
Obamacare
Each of these examples contains arguing
subjectivity, but more importantly, each ex-
presses roughly the same idea, namely, that the
recently-passed health care reform bill will cause
economic harm. This latent, shared idea giving
rise to each of the three spans is what we mean
by ?argument tag?.
However, although all three are related, exam-
ple spans (2) and (3) are more similar than (4)
in terms of the notions they convey: while the
first two explicitly are concerned with the loss
of jobs, the last focuses on business expansion
and the economy as a whole. If we were to tag
these three spans with respect to the argument
that each is making, should they all receive the
same tag, or should (4)?s tag be different?
To address these challenges, we propose in this
work a new annotation scheme for identifying
arguing subjectivity and a hierarchical model for
organizing ?argument tags?. In our hierarchical
model, (4) would receive a different tag from (2)
and (3), but because of the tags? relatedness all
would share the same parent tag.
In addition to presenting this new scheme for
labeling arguing subjectivity, we also explore
sentiment, discourse, and distributional similar-
ity as tools to enhance identification and classi-
fication of arguing subjectivity. Finally, we also
investigate splitting the arguing subjectivity de-
tection task up into two distinct phases: iden-
tifying expressions of arguing subjectivity, and
labelling each such expression with an appropri-
ate argument tag.
Since no corpora annotated for arguing sub-
jectivity yet exist, we gather and annotate a cor-
pus of blog posts and op-eds about a contro-
versial topic, namely, the recently-passed ?Oba-
maCare? health care reform bill.
2 Annotation Scheme
We designed our annotation scheme with two
goals in mind: identifying all spans of text which
express arguing subjectivity, and labelling each
such span with an argument tag. To address
the first goal, our annotators manually identified
and annotated spans of text containing arguing
subjectivity using the GATE environment1. An-
notators were instructed to identify spans of 1
sentence or less in which a writer ?conveys a
controversial private state concerning what she
believes to be true or what action she believes
should be taken? concerning the health care re-
form debate. To train our annotators to recog-
nize arguing subjectivity, we performed several
rounds of practice on a separate dataset. Be-
tween each round, our annotators met to discuss
their annotations and resolve disagreements.
As a heuristic to help distinguish between bor-
derline sentences, we advised our annotators to
imagine disputants from each side writing the
sentence in isolation. If a disputant from either
side could conceivably write the sentence, then
the sentence is likely objective. For example,
statements of accepted facts and statistics gen-
erally fall into this category. However, if only
one side could conceivably be the author of the
sentence, it is highly likely that the sentence ex-
presses a controversial belief relevant to the de-
bate and thus should be labeled as subjective.
Next, the annotators labeled each arguing
span with an argument tag. As illustrated in
earlier examples, an argument tag represents a
1http://gate.ac.uk/
81
controversial abstract belief expressed through
arguing subjectivity. Since the meanings of
many tags may be related, we organize these
tags in a hierarchical ?stance structure?. A
stance structure is a tree-based data structure
containing all of the argument tags associated
with a particular debate, organizing those tags
using ?is-a? relationships. Our stance structure
contains two levels of argument tags: upper-
level ?primary? argument tags and lower-level
?secondary? tags. Each primary tag has one of
the stances (either ?pro? or ?anti? in our case)
as its parent, while each secondary tag has a
primary tag as its parent2.
Political science ?arguing dimension? ap-
proaches to debate framing analysis served, in
part, as an inspiration for our stance structure
(Baumgartner et al, 2008). Also, as illustrated
in Section 1, this approach permits us additional
flexibility, supporting classification at different
levels of specificity depending on the task at
hand and the amount of data available. We en-
vision a future scenario in which a community of
users collaboratively builds a stance structure to
represent a new topic or debate, or in which an-
alysts build a stance structure to categorize the
issues expressed towards a proposed law, such
as in the context of e-rulemaking (Cardie et al,
2008).
Because each stance contains a large number
of argument tags, we back-off from each sec-
ondary argument tag to its primary argument
parent for the classification experiments. We
chose to do this in order to ensure that we have
a sufficient amount of data with which to train
the classifier.
3 Dataset
For this study, we chose to focus on online ed-
itorials and blog posts concerning the ongoing
debate over health insurance reform legislation
in the United States. Our intuition is that blogs
and editorials represent a genre rich in both
2Our stance structure contains an additional ?aspect?
level consisting of a-priori categories adopted from politi-
cal science research. However, we do not utilize this level
of the stance structure in this work.
?pro? documents 37
?pro? sentences 1,222
?anti? documents 47
?anti? sentences 1,456
total documents 84
total sentences 2,678
Table 1: Dataset summary statistics.
arguing subjectivity
objective 683
subjective 588
argument labels
no label 683
improves healthcare access 130
improves healthcare affordability 104
people dont know truth
about bill
75
controls healthcare costs 54
improves quality of healthcare 52
helps economy 51
bill should be passed 43
other argument 79
Table 2: Arguing and argument label statistics for
the ?pro? stance.
subjectivity and arguments. We collected docu-
ments written both before and after the passage
of the final ?Patient Protection and Affordable
Care Act? bill using the ?Google Blog Search?3
and ?Daily Op Ed?4 search portals. By choosing
a relatively broad time window, from early 2009
to late 2011, we aimed to capture a wide range
of arguments expressed throughout the debate.
The focus of this paper is on sentence-level
argument detection rather than document-level
stance classification (e.g., (Anand et al, 2011),
(Park et al, 2011), (Somasundaran and Wiebe,
2010), (Burfoot et al, 2011)). We treat stance
classification as a separate step preceding argu-
ing subjectivity detection, and thus provide or-
acle stance labels for our data.
We treat documents written from the ?pro?
3http://www.google.com/blogsearch
4http://www.dailyoped.com/
82
arguing subjectivity
objective 913
subjective 575
argument labels
no label 913
diminishes quality of care 122
too expensive 67
unpopular 60
hurts economy 55
expands govt 52
bill is politically motivated 44
other reforms more appropriate 35
other argument 140
Table 3: Arguing and argument label statistics for
the ?anti? stance.
stance and documents written from the ?anti?
stance as separate datasets. Being written from
different positions, the two stances will have dif-
ferent argument labels and may employ different
styles of arguing subjectivity. Table 1 provides
an overview of the size of this dataset. Summary
statistics concerning the density of arguing and
argument labels in the two sides of the dataset
is presented in Tables 2 and 3. However, since
it can be difficult to summarize a complex ar-
gument in a short phrase, many of these labels
by themselves do not clearly convey the meaning
they are meant to represent. To better illustrate
the meanings of some of the more ambiguous la-
bels, Table 4 presents several annotated example
spans for some of the more unclear ambiguous
argument labels.
4 Agreement Study
One of our authors performed annotation of our
corpus, the broad outlines of which are sketched
in the previous section. However, to assess inter-
annotator agreement for this annotation scheme,
we recruited a non-author to independently an-
notate a subset of our corpus consisting of 384
sentences across 10 documents. This non-author
both identified spans of arguing subjectivity and
assigned argument tags. She was given a stance
structure from which to select argument tags.
improves healthcare access
?Our reform will prohibit insurance compa-
nies from denying coverage because of your
medical history.?
?Let?s also not overlook the news from last
week about the millions of younger Americans
who are getting coverage thanks to consumer
protections that are now in place.?
improves healthcare affordability
? new health insurance exchanges will offer
competitive, consumer-centered health insur-
ance marketplaces...?
?Millions of seniors can now afford medication
they would otherwise struggle to pay for.?
people dont know truth about bill
?...the cynics and the naysayers will continue
to exploit fear and concerns for political gain.?
?Republican leaders, who see opportunities
to gain seats in the elections, have made
clear that they will continue to peddle fictions
about a government takeover of the health
care system and about costs too high to bear.?
unpopular
?The 1,000-page monstrosity that emerged in
various editions from Congress was done in by
widespread national revulsion...?
?Support for ObamaCare?s repeal is broad,
and includes one group too often overlooked
during the health care debate: America?s doc-
tors.?
expands govt
?...the real goal of the health care overhaul
was to enact the largest entitlement program
in history...?
?the new bureaucracy the health care legisla-
tion creates is so complex and indiscriminate
that its size and cost is ?currently unknow-
able.? ?
bill is politically motivated
?...tawdry backroom politics were used to sell
off favors in exchange for votes.?
?From the wildly improper gifts to senators
like Nebraska?s Ben Nelson to this week?s
backroom deals for unions...?
Table 4: Example annotated spans for several argu-
ment labels.
83
metric recall precision f-measure
agr 0.677 0.690 0.683
kappa for overlapping annotations 0.689
Table 5: Inter-annotator span agr (top) and argu-
ment label kappa on overlapping spans (bottom).
In assessing inter-annotator agreement on this
subset of the corpus, we must address two levels
of agreement, arguing spans and argument tags.
At first glance, how to assess agreement
of annotated arguing spans is not obvious.
Because our annotation scheme did not enforce
strict boundaries, we hypothesized that both
annotators would both frequently see an in-
stance of arguing subjectivity within a local
region of text, but would disagree with respect
to where the arguing begins and ends. Thus, we
adopt from (Wilson and Wiebe, 2003) the agr
directional agreement metric to measure the
degree of annotation overlap. Given two sets
of spans A and B annotated by two different
annotators, this metric measures the fraction
of spans in A which at least partially overlap
with any spans in B. Specifically, agreement is
computed as:
agr(A B) = A matching BA
When A is the gold standard set of annota-
tions, agr is equivalent to recall. Similarly, when
B is the gold standard, agr is equivalent to pre-
cision. For this evaluation, we treat the dataset
annotated by our primary annotator as the gold
standard. Table 5 presents these agr scores and
f-measures for the arguing spans.
Second, we measure agreement with respect
to the argument tags assigned by the two an-
notators. Continuing to follow the methodol-
ogy of (Wilson and Wiebe, 2003), we look at
each pair of annotations, one from each anno-
tator, which share at least a partial overlap.
For each such pair, we assess whether the two
spans share the same primary argument tag.
Scores for primary argument label agreement in
terms of Cohen?s kappa are also presented in Ta-
ble 5. Since this kappa score falls within the
range of 0.67 ? K ? 0.8, according to Krippen-
dorf?s scale (Krippendorff, 2004) this allows us
to draw tentative conclusions concerning a sig-
nificant level of tag agreement.
5 Methods
As discussed earlier, recognizing arguments can
be thought of in terms of two related but dif-
ferent tasks: recognizing a type of subjectivity,
and labeling instances of that subjectivity with
tags. We refer to the binary arguing subjectiv-
ity detection task as ?arg?, and to the multi-
class argument labeling task as ?tag?. For the
?tag? task, we create eight classes: one for each
of the seven most-frequent labels, and an eighth
into which we agglomerate the remaining less-
frequent labels. We only consider the sentences
known to be subjective (via oracle information)
for the ?tag? task.
We also perform a ?combined? task. This
third task is conceptually similar to the ?tag?
task, except that all sentences are considered
rather than only the subjective sentences. In ad-
dition to the eight classes used by ?tag?, ?com-
bined? adds an additional class for non-arguing
sentences. Finally, we also perform a two-stage
?arg+tag? task. In this two-stage task, the in-
stances labeled as subjective by the ?arg? clas-
sifier are passed as input to the ?tag? classifier.
The intuition behind this two-phase approach is
that the features most useful for identifying ar-
guing subjectivity may not be the most useful
for discriminating between argument tags, and
vice versa. For all of our classification tasks,
we treat both the ?pro? and ?anti? stances
separately, building separate classifiers for each
stance for each of the above tasks.
In general, we perform single-label classifi-
cation at the sentence level. However, sen-
tences containing multiple labels pose a chal-
lenge. Since this was an early exploratory work
on a very difficult task, we decided to handle
this situation by splitting sentences containing
multiple labels into separate instances for the
purpose of learning, assigning a single label to
each instance. However, only about 3% of the
sentences in our corpus contained multiple la-
84
bels. Thus, replacing this splitting step in the
future with another method that does not re-
quire oracle information, such as choosing the
label which covers the most words in the sen-
tence, is a reasonable simplification of the task.
Since discourse actions, such as contrasting,
restating, and identifying causation, play a sub-
stantial role in arguing, we hypothesize that in-
formation about the discourse roles played by
a span of text will help improve classification.
Although discourse parsers historically haven?t
been found to be effective for subjectivity anal-
ysis, a new parser (Lin et al, 2010) trained on
the Penn Discourse TreeBank (PDTB) tagset
(Prasad et al, 2008) has recently been released.
Previous work has demonstrated that this parser
can reliably detect discourse relationships be-
tween adjacent sentences (Lin et al, 2011), and
the PDTB tagset, being relatively flat, is con-
ducive to feature engineering for our task.
To give a feeling for the kind of discourse re-
lations identified by this parser, the following
example illustrates a concession relation identi-
fied in the corpus by the parser. The italicized
text represents the concession, while the bolded
text indicates the overall point that the author
is making. The underlined word was identified
by the parser as an explicit concessionary clue.
(7) the health care reform legisla-
tion that President Obama now seems
likely to sign into law , while an
unlovely mess , will be remembered
as a landmark accomplishment .
Using this automatic information, we define
features indicating the discourse relationships by
which the instance is connected to surrounding
text. Specifically, the class of discourse rela-
tionship connecting the target instance to the
previous instance, the relationship connecting it
to the following instance, and any internal dis-
course relationships by which the parts of the
instance are connected to each other are each
added as features. Since PDTB contains many
fine-grained discourse relations, we replace each
discourse relationship type inferred by the dis-
course parser with the parent top-level PDTB
discourse relationship class. We arrive at a total
of 15 binary discourse relationship features: (4
top-level classes + ?other?) x (connects to pre-
vious + connects to following + internal connec-
tion) = 15. We refer to these features as ?rels?.
As illustrated in our earlier examples, while
arguing subjectivity is different from sentiment,
the two types of subjectivity are often related.
Thus, we investigate incorporating sentiment
information based on the presence of unigram
clues from a publically-available sentiment lexi-
con5 (Wilson, 2005). Each clue in the lexicon is
marked as being either ?strong? or ?weak?.
We found that this lexicon was producing
many false hits for positive sentiment. Thus, a
span containing a minimum of two positive clues
of which at least one is marked as ?strong?, or
three positive ?weak? clues, is augmented with a
feature indicating positive sentiment. For nega-
tive sentiment the threshold is slightly lower, at
one ?strong? clue or two ?weak? clues. These
features are referred to as ?senti?.
A challenge to argument tag assignment is the
broad diversity of language through which in-
dividual entities or specific actions may be ref-
erenced, as illustrated in Examples (2-4) from
Section 1. To address this problem, we in-
vestigate expanding each instance with terms
that are most similar, according to a distribu-
tional model generated from Wikipedia articles,
to the nouns and verbs present within the in-
stance (Pantel et al, 2009). We refer to these
features as ?expn?, where n is the number of
most-similar terms with which to expand the in-
stance for each noun or verb. We experiment
with values of n = 5 and n = 10.
Subjectivity classification of small units of
text, such as individual microblog posts (Jiang
et al, 2011) and sentences (Riloff et al, 2003),
has been shown to benefit from additional con-
text. Thus, we augment the feature representa-
tion of each target sentence with features from
the two preceding and two following sentences.
These additional features are modified so that
they do not fall within the same feature space
5downloaded from http://www.cs.pitt.edu/mpqa/
subj_lexicon.html
85
feat.
abbrev.
elaboration
unigram
senti 2 binary features indicating posi-
tive or negative sentiment based on
presence of lexicon clues
rels 15 binary features indicating kinds
of discourse relationships and how
they connect instance to surround-
ing text
exp5 for each noun and verb in instance,
expand instance with top 5 most
distributionally similar words
exp10 for each noun and verb in instance,
expand instance with top 10 most
distributionally similar words
Table 6: Overview of features used in the arguing
and argument experiments.
as the features representing the target sentence.
Using the Naive Bayes classifier within the
WEKA machine learning toolkit (Hall et al,
2009), we explore the impact of the features de-
scribed above on our four experiment configu-
rations. We perform our experiments using k-
fold cross-validation, where k equals the num-
ber of documents within the stance. The test
set for each fold consists of a single document?s
instances. For the ?pro? dataset k = 37, while
for the ?anti? dataset k = 47.
6 Results
Table 7 presents the accuracy scores from each of
our stand-alone classifiers across combinations
of feature sets. Each feature set consists of
unigrams augmented with the designated addi-
tional features, as described in Section 5. To
evaluate the ?tag? classifier in isolation, we use
oracle information to provide this classifier with
only the subjective instances. To assess signif-
icance of the performance differences between
feature sets, we used the Pearson Chi-squared
test with Yates continuity correction.
Expansion of nouns and verbs with
distributionally-similar terms (?exp5?, ?exp10?)
plays the largest role in improving classifier
features arg tag comb.
unigram baseline 0.610 0.425 0.458
senti 0.614 0.426 0.459
rels 0.614 0.422 0.462
senti, rels 0.618 0.424 0.465
exp5 0.635 0.522 0.482
exp5, senti 0.638 0.515 0.486
exp5, rels 0.640 0.522 0.484
exp5, senti, rels 0.643 0.516 0.484
exp10 0.645 0.517 0.488
exp10, senti 0.647 0.515 0.489
exp10, rels 0.642 0.512 0.490
exp10, senti, rels 0.644 0.513 0.490
Table 7: Classifier accuracy for differing feature sets.
Significant improvement (p < 0.05) over baseline is
boldfaced (0.05 < p < 0.1 italicized). Underline in-
dicates best performance per column.
performance. While differences between con-
figurations using ?exp5? versus ?exp10? were
generally not significant, all of the configu-
rations incorporating some version of term
expansion outperformed the unigram baseline
by either a statistically significant margin
(p < 0.05) or by a margin that approached
significance (0.05 < p < 0.1).
Sentiment features consistently produce im-
provements in accuracy for the ?arg? and ?com-
bined? tasks. While these improvements are
promising, the lack of a significant margin of im-
provement when incorporating sentiment is sur-
prising. Since sentiment lexicons are known to
be highly domain-dependent (Pan et al, 2010),
it may be the case that, having been learned
from a general news corpus, the sentiment lexi-
con employed in this work is not the best match
for the domain of ?ObamaCare? blogs and edito-
rials. Similarly, the discourse features also fail to
produce significant improvements in accuracy.
Finally, we aim to test our hypothesis that
separating the ?arg? and ?tag? phases results in
improvement beyond treating the two in a single
?combined? phase. The first step of our hierar-
chy involves normal classification of all sentences
using the ?arg? classifier. Next, all sentences
judged to contain arguing subjectivity by ?arg?
86
arg features tag features acc.
exp5, senti, rels
exp5 0.506
exp5, rels 0.506
exp10 0.501
exp10
exp5 0.514
exp5, rels 0.513
exp10 0.512
exp10, senti
exp5 0.514
exp5, rels 0.513
exp10 0.512
Table 8: Accuracies of two-stage classifiers across dif-
ferent combinations of feature sets for the ?arg? and
?tag? phases. Italics indicate improvement over the
top ?combined? configuration which approaches sig-
nificance (0.05 < p < 0.1). Underline indicates best
overall performance.
are passed to the ?tag? classifier to have an ar-
gument tag assigned. We choose three promis-
ing feature sets for the ?arg? and ?tag? phases,
based on best performance in isolation.
Results of this hierarchical experiment are
presented in Table 8. We evaluate the hi-
erarchical system against the best-performing
?combined? single-phase systems from Table 7.
While all of the hierarchical configurations beat
the best ?combined? classifier, none beats the
top combined classifier by a significant margin,
although the best configurations approach sig-
nificance (0.05 < p < 0.1).
7 Related Work
Much recent work in ideological subjectivity
detection has focused on detecting a writer?s
stance in domains of varying formality, such as
online forums, debating websites, and op-eds.
(Anand et al, 2011) demonstrates the usefulness
of dependency relations, LIWC counts (Pen-
nebaker et al, 2001), and information about re-
lated posts for this task. (Lin et al, 2006) ex-
plores relationships between sentence-level and
document-level classification for a stance-like
prediction task.
Among the literature on ideological subjectiv-
ity, perhaps most similar to our work is (Soma-
sundaran and Wiebe, 2010). This paper investi-
gates the impact of incorporating arguing-based
and sentiment-based features into binary stance
prediction for debate posts. Also closely related
to our work is (Somasundaran et al, 2007). To
support answering of opinion-based questions,
this work investigates the use of high-precision
sentiment and arguing clues for sentence-level
sentiment and arguing prediction.
Another active area of related research focuses
on identifying important aspects towards which
sentiment is expressed within a domain. (He
et al, 2011) approaches this problem through
topic modeling, extending the joint sentiment-
topic (JST) model which aims to simultaneously
learn sentiment and aspect probabilities for a
unit of text. (Yu et al, 2011) takes a different
approach, investigating thesaurus methods for
learning aspects based on groups of synonymous
nouns within product reviews.
8 Conclusion
In this paper, we explored recognizing argu-
ments in terms of arguing subjectivity and ar-
gument tags. We presented and evaluated a
new annotation scheme to capture arguing sub-
jectivity and argument tags, and annotated a
new dataset. Utilizing existing sentiment, dis-
course, and distributional similarity resources,
we explored ways in which these three forms
of knowledge could be used to enhance argu-
ment recognition. In particular, our empirical
results highlight the important role played by
distributional similarity in all phases of detect-
ing arguing subjectivity and argument tags. We
have also provided tentative evidence suggesting
that addressing the problem of recognizing argu-
ments in two separate phases may be beneficial
to overall classification accuracy.
9 Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation award
#0916046. We would like to thank Patrick Pan-
tel for sharing his thesaurus of distributionally
similar words from Wikipedia with us, Amber
Boydstun for insightful conversations about de-
bate frame categorization, and the anonymous
reviewers for their useful feedback.
87
References
Pranav Anand, Marilyn Walker, Rob Abbott,
Jean E. Fox Tree, Robeson Bowmani, and Michael
Minor. 2011. Cats rule and dogs drool!: Classi-
fying stance in online debate. In WASSA, pages
1?9, Portland, Oregon, June.
F.R. Baumgartner, S.D. Boef, and A.E. Boydstun.
2008. The decline of the death penalty and the dis-
covery of innocence. Cambridge University Press.
Clinton Burfoot, Steven Bird, and Timothy Bald-
win. 2011. Collective classification of congres-
sional floor-debate transcripts. In ACL, pages
1506?1515, Portland, Oregon, USA, June.
Claire Cardie, Cynthia Farina, Adil Aijaz, Matt
Rawding, and Stephen Purpura. 2008. A study in
rule-specific issue categorization for e-rulemaking.
In DG.O, pages 244?253.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics
for cross-domain sentiment classification. In ACL,
pages 123?131, Portland, Oregon, USA, June.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter
sentiment classification. In ACL, pages 151?160,
Portland, Oregon, USA, June.
K. Krippendorff. 2004. Content analysis: an intro-
duction to its methodology. Sage.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In CoNLL, pages 109?116.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In ACL, pages 997?1006, Port-
land, Oregon, USA, June.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher
Potts. 2011. Learning word vectors for sentiment
analysis. In ACL, pages 142?150, Portland, Ore-
gon, USA, June.
Sinno Jialin Pan, Xiaochuan Ni, Jian tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment.
In WWW.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In ACL,
pages 271?278, Barcelona, Spain, July.
Patrick Pantel, Eric Crestan, Arkady Borkovsky,
Ana-Maria Popescu, and Vishnu Vyas. 2009.
Web-scale distributional similarity and entity set
expansion. In EMNLP, pages 938?947, Morris-
town, NJ, USA.
Souneil Park, Kyung Soon Lee, and Junehwa Song.
2011. Contrasting opposing views of news arti-
cles on contentious issues. In ACL, pages 340?349,
Portland, Oregon, USA, June.
James W Pennebaker, Roger J Booth, and Martha E
Francis. 2001. Linguistic inquiry and word count
(liwc): Liwc2001. Linguistic Inquiry, (Mahwah,
NJ):0.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
In LREC, May.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In CoNLL, pages 25?32.
Swapna Somasundaran and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
AFNLP, pages 226?234.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line debates.
In CAAGET, pages 116?124.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with atti-
tude: Exploiting opinion type analysis for improv-
ing question answering in on-line discussions and
the news. In ICWSM.
Swampa Somasundaran. 2010. Discourse-Level Re-
lations for Opinion Analysis. Ph.D. thesis, Uni-
versity of Pittsburgh, USA.
Theresa Wilson and Janyce Wiebe. 2003. Annotat-
ing opinions in the world press. In SIGdial, pages
13?22.
Theresa Wilson. 2005. Recognizing contextual
polarity in phrase-level sentiment analysis. In
EMNLP, pages 347?354.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect ranking: Identifying
important product aspects from online consumer
reviews. In ACL, pages 1496?1505, Portland, Ore-
gon, USA, June.
88
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 8?17,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
An Investigation for Implicatures in Chinese :
Implicatures in Chinese and in English are similar !
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
Implicit opinions are commonly seen in
opinion-oriented documents, such as po-
litical editorials. Previous work have uti-
lized opinion inference rules to detect
implicit opinions evoked by events that
positively/negatively affect entities (good-
For/badFor) to improve sentiment analy-
sis for English text. Since people in differ-
ent languages may express implicit opin-
ions in different ways, in this work we in-
vestigate implicit opinions expressed via
goodFor/badFor events in Chinese. The
positive results have provided evidences
that such implicit opinions and inference
rules are similar in Chinese and in English.
Moreover, we have observed cases where
the inferences are blocked.
1 Introduction
In the opinion-oriented documents, many opin-
ions are expressed implicitly rather than explicitly.
Consider the following example from (Deng and
Wiebe, 2014):
EX(1.1) The reform would lower
health care costs, which would be a
tremendous positive change across the
entire health-care system.
There is an explicit positive sentiment (positive)
toward the event of reform lower costs. In express-
ing this sentiment, the writer implies he is nega-
tive toward the costs, because he?s happy to see the
costs being decreased. The writer may be positive
toward reform since it conducts the lower event.
Such inferences may be seen as opinion-oriented
implicatures (i.e., defeasible inferences)
1
.
1
Implicatures ?normally accompany the utterances of a
given sentence unless special factors exclude that possibility
(p. 39).? (Huddleston and Pullum, 2002)
We create an annotated corpus (denoted DCW
corpus) (Deng et al., 2013)
2
and generalizes such
events, defining a badFor (bf) event to be an
event that negatively affects the object and a good-
For (gf) event to be an event that positively af-
fects the object of the event. Here, lower is a
bf event. According to the annotation scheme,
goodFor/badFor (hereafter gfbf ) events have NP
agents and objects (though the agent may be im-
plicit), and the polarity of a gf event may be
changed to bf by a reverser (and vice versa).
We have developed a set of rules for inferring
implicit sentiments, from explicit sentiments and
gfbf events (Deng and Wiebe, 2014). We incor-
porate the rules into a graph-based model, which
significantly improves classifying the sentiments
toward agents and objects in the gfbf events.
The contribution of this work is investigating
implicatures in a second language, specifically in
Chinese. People in different languages may ex-
press implicit opinions in different ways, so it is
better to first assess similarity of implicatures in
the two languages, rather than to directly utilize
the English resources. In this work we conduct an
agreement study for gfbf information in Chinese.
The good agreement scores provide evidence for
the existence of similar implicature in Chinese.
During the analysis of disagreement, we have ob-
served interesting gfbf events triggered by Chinese
syntax, which are rare in English but common in
Chinese. We should provide additional guidance
for such events when developing a Chinese gfbf
manual in the future.
We run the graph-based model on the annotated
Chinese corpus. The good evaluation results sup-
port our hypothesis that the inference rules in En-
glish apply for Chinese. Moreover, we have ob-
served gfbf cases where the sentiment inferences
are blocked, which are similar to what we have
found in English (Wiebe and Deng, 2014).
2
Available at: http://mpqa.cs.pitt.edu/
8
Further, we analyze gfbf words and syntax of
agents/objects in Chinese. Our analysis shows that
it is feasible to extract components of Chinese gfbf
events utilizing the existing resources. In the last
section we briefly talk bout the Chinese explicit
sentiment analysis.
2 Related Work
In addition to researches focusing on explicit sen-
timents (Wiebe et al., 2005; Johansson and Mos-
chitti, 2013; Yang and Cardie, 2013), recently
there are work investigating features that directly
indicate implicit sentiments (Zhang and Liu, 2011;
Feng et al., 2013), or working on inferring implicit
opinions (Choi and Cardie, 2008; Zhang and Liu,
2011; Anand and Reschke, 2010; Reschke and
Anand, 2011; Goyal et al., 2013). Different from
their work, which do not cover all the inferences of
implicit opinions over explicit opinions and gfbf
events, we define a generalized set of inference
rules and incorporate the rules into a graph-based
model to achieve sentiment propagation between
the agents and objects of gfbf events (Deng and
Wiebe, 2014). The result shows that the graph-
based model itself is able to assign the unknown
nodes with correct labels 89% of the time.
Many works in Chinese sentiment analysis de-
velop heuristics for adapting methods in English
to methods appropriate for Chinese (Tsou et al.,
2005; Wang et al., 2007; Li and Sun, 2007). In-
stead of projecting English methods and resources
into Chinese versions, there are also works lever-
aging Chinese-English parallel corpus to assist
Chinese sentiment analysis. Wan (2008) trans-
lates Chinese sentiment sentences into English and
ensemble the sentiment classification results from
both English and Chinese sentiment classifiers.
Wan (2009) adopt co-training methods, utilizing
labeled English sentences and unlabelled Chinese
sentences. Lu et al. (2011) assumes parallel sen-
tences in different languages bear the same sen-
timent. They utilize unlabelled Chinese-English
parallel corpus to jointly improve sentiment clas-
sification in both languages. Boyd-Graber and
Resnik (2010) present a generative model, jointly
modeling topics that are consistent across lan-
guages, to improve sentiment rating predictions.
3 Implicature in Chinese
The definition of a gfbf event is from (Deng et
al., 2013). A goodFor (gf) event is an event that
positively affects an entity (similarly, for badFor
(bf) events). A gfbf triple has the structure of ?
agent, gfbf, object?, though the agent can be im-
plicit. For example, in the sentence from (Deng
et al., 2013), ?Repealing the Affordable Care Act
(ACA) would hurt our economy.?, there are two
gfbf triples. One is ?Repealing the ACA, hurt,
families, our economy?, which is a bf. The other
is ?implicit, Repealing, the ACA?, which is bf and
the agent is implicit. The DCW corpus contains
manually annotated gfbf events, the gfbf polari-
ties, the corresponding agents and objects and the
writer?s attitudes toward the agents and objects.
Because people in different languages may ex-
press their opinions in different ways. In this sec-
tion, we conduct an agreement study for Chinese
gfbf information in Section 3.1 and achieve good
agreement scores, reported in Section 3.2, which
provide supporting evidences for detecting Chi-
nese gfbf events. In the disagreement analysis,
we have observed interesting cases which are gfbf
events in semantics but are triggered by Chinese
own syntax. We explain the cases in Section 3.3.
3.1 Agreement Study Design
Data: We collect 100 political editorials from the
Opinion Column in the Chinese version of New
York Times
3
, where each political editorial has an
English version and a Chinese version. The Chi-
nese editorial is a translated and paraphrased ver-
sion of the corresponding English editorial, writ-
ten by professional translators. The English ver-
sion and the Chinese version are paragraph paral-
leled. In the previous agreement study of (Deng et
al., 2013), the annotators are asked to annotate the
whole document. Because not all the sentences
contain gfbf events and the documents are long,
a large proportion of disagreement we find that is
due to negligence. In order to reduce negligence
and provide a more dense data for annotation, first,
we collect a lexicon of English gfbf words in the
DCW corpus. Then we find the English sentences
containing English gfbf words and select the para-
graphs containing those sentences. The parallel
Chinese paragraphs are collected. Though a para-
graph may contain more than one sentence and
some sentences do not have gfbf events, it is much
more dense to annotate than the document as a
whole. When presenting data to the annotators, we
do not provide an isolated paragraph since it may
3
http://cn.nytimes.com/opinion/
9
lose the context information. Instead, we present
the original Chinese editorials and highlight the
selected paragraphs. The annotators are told to
read through the whole document but only need
to annotate the highlighted paragraphs.
Procedure: We adopt our English manual in
(Deng et al., 2013) to train the annotators. The
annotators read through the manual and several
Chinese gfbf examples. Then, the annotators la-
bel several paragraphs and discuss their disagree-
ments to reconcile their differences. For the for-
mal agreement study, we randomly selected 60
paragraphs, which have a total of 253 Chinese sen-
tences. These paragraphs are different from the
paragraphs discussed during training. The annota-
tors then independently annotated the 60 selected
paragraphs.
3.2 Agreement Study Evaluation and Result
We use the same measurement for agreement for
all types of spans. (The type is either gfbf, agent,
or object). Suppose A is a set of annotations of
a particular type and B is the set of annotations
of the same type from the other annotator. For
any text span a ? A and b ? B, the span cov-
erage c counts the percentage of overlapping Chi-
nese characters between a and b,
c(a, b) =
|a ? b|
|b|
(1)
where |a| is the number of characters in span a,
and ? gives the set of characters that two spans
have in common (Johansson and Moschitti, 2013).
Following (Wilson and Wiebe, 2003), we treat
each set A and B in turn as the gold-standard and
calculate the average F-measure (agr(A,B)).
agr(A||B) =
?
a?A,b?B,
|a?b|>0
c(a, b)
|B|
(2)
agr(A,B) =
agr(A||B) + agr(B||A)
2
(3)
Now that we have the sets of annotations on
which the annotators agree, we use ? (Artstein
and Poesio, 2008) to measure agreement for the at-
tributes. We report three ? values: one for the po-
larities of the gfbf events, and the other two for the
writer?s attitudes toward the agents and objects.
Three annotator participate in the agreement
study. All of them are Chinese graduate students
studying in US. One of them is the co-author
of this work (Anno 1), while the other two do
agr(A,B) gfbf agent object
Anno 1& 2 0.7929 0.9091 0.9091
Anno 1 & 3 0.7044 0.9524 1.0
gfbf agent object
? polarity attitude attitude
Anno 1 & 2 0.9385 0.7830 0.7238
Anno 1 & 3 0.8966 0.5913 0.8478
Table 1: Results for Agreement Study Analysis.
not know details of gfbf and implicature before
(Anno2, Anno3). Since Anno1 is familiar with this
work, we compare the other two?s annotations to
Anno1?s. In Table 1, the upper half is the agree-
ment for span overlapping (agr(A,B)), and the
lower half is the agreement for attribute (?).
The result have shown that the annotators have
good agreement scores, though our training period
is not long and our training data cover multiple
topics. In particular, the annotators agree quite
well on recognizing the agents and objects and
judging the polarity of gfbf events.
For recognizing gfbf events, we have found two
interesting gfbf cases caused by the Chinese syn-
tax that is different from English, elaborated in the
next section. Among the spans only one annota-
tor marks, one third is due to the two cases above;
one third are borderlines that could be marked; one
third are incorrect. For the spans two annotator
mark but the third doesn?t, we regard it as negli-
gence.
For judging the writer?s attitudes toward agents
and objects, we can see from Table 1 that Anno 2
and Anno 3 behave differently. This is understand-
able because we are marking the implicit opinions
of the writer. Though trained, different annotators
have different thresholds for judging whether an
opinion is expressed here. Some annotators may
be more sensitive than the others. If we don?t
count the spans that one annotator marks it as none
(i.e. neutral) but the other doesn?t, the ? scores in-
crease a lot, as Row Polar shows in Table 2. This
indicates that the annotators mainly disagree on
whether the sentiment is neutral or not, rather than
the polarity of opinions.
To further investigate whether the disagreement
is caused by Chinese, or is due to the annotators?
inherent different sensitivities of opinions, we ran-
domly select 5 documents from the DCW corpus,
delete the writer?s attitude toward agents and ob-
jects but keep the remaining annotations. The an-
10
Anno 1 & 2 Anno 1& 3
agent object agent object
Table 1 0.783 0.723 0.591 0.848
Polar 0.875 0.915 1 0.88
Eng 0.738 0.652 0.4633 0.8734
Table 2: ? for Agreement Study Analysis.
notators are then told to mark the attitudes. As
Row Eng in Table 2 shows, we have got consis-
tent agreement results within the same annotators
when they annotate in English and in Chinese.
This supports the idea that the differences between
the annotators are differences on the underlying
task, regardless of the language.
3.3 GoodFor/Badfor Triggered by Chinese
Syntax
During the analysis of disagreement, we have
found gfbf cases which are triggered by the Chi-
nese syntax that is different from English. Since
the annotators are trained by the English manual,
some annotators stay consistent with the English
syntax, but the others go beyond syntax and iden-
tify gfbf according to semantics and pragmatics,
which lead to disagreement. In this section we list
two major cases due to the Chinese own syntax.
This suggests that additional guidance to annotate
such cases should be added to the English manual
to develop a Chinese gfbf manual.
The first case is due to unclear expression of
passive voice in Chinese. In English, the noun
phrase that would be the object of an active sen-
tence (Our troops defeated the enemy) appears as
the subject of a sentence with passive voice (The
enemy was defeated by our troops)
4
. It is clear
that enemy is the object and our troops is the agent
in both sentences. However, this is not intuitive
for some Chinese sentences.
A Chinese example is ? ?????????
? ?, whose English translation is: ?The economic
potential ... appeared to be unleashed?. A word-
to-word translation would be ?...appeared to have
got unleashed?. In the two English versions, po-
tential is obviously the object of unleashed event.
However, some annotators analyze this sentence
according to syntax
5
. The dependency syntax be-
tween the object potential (??) and the gfbf un-
leash (??) is nsubj(??-5, ??-2) so it is not
4
http://en.wikipedia.org/wiki/English passive voice.
5
We use Stanford?s dependency parser in this work.
marked. Some annotators view from pragmatics
and read as a passive voice. Since there is no word
transformation of Chinese verbs for passive voice
(e.g. unleash changes to unleashed in English),
this raises disagreement.
The other case is related to one constraint de-
fined in (Deng et al., 2013). According to the
manual, the polarity of a gfbf triple must be de-
termined within the triple. As explained in the
manual, in the sentence ?Tom has left his cousin
a big trouble?, the triple ?Tom, left, his cousin? is
not a gfbf event, since we cannot judge whether
this event is good for or bad for his cousin without
knowing what Tom leaves to his cousin. While
in the sentence ?They decrease the manufacturing
costs?, the event decrease is a bf no matter how
many or by what means the costs are decreased.
However, a Chinese instance is, ??????
?? ?, whose translation is ?put the reform to
die?. Whether the event put (?) is good for or bad
for the object reform (??), depends on whether
the agent puts the reform to die or puts the reform
to revive, for instance. However, in Chinese, ?
is not main verb (Li and Thompson, 1989), the
object (??, reform) of the main verb (???
?, die) is placed after the function word (?), and
the verb is placed after the object, forming a sub-
ject?object?verb (SOV) sentence (Chao, 1968)
6
,
which is defined as ba structure (Chao, 1968; Li
and Thompson, 1989; Sybesma, 1992). Thus, in
Chinese the sentence is read as: ?kill the reform?,
which could be seen as a gfbf event. This structure
is very common in Chinese.
In conclusion, there are very similar implica-
tures in Chinese. However, in order to fully study
the gfbf events in Chinese, the manual should
be revised to provide guidance for annotating the
cases mentioned above.
4 Implicature Inference in Chinese
We propose a set of sentiment inference rules and
incorporate them into a graph-based model to con-
duct sentiment propagation among entities (agents
and objects) of gfbf events (Deng and Wiebe,
2014). In Section 4.1, we run this graph-based
model on the Chinese annotations. The positive
results of sentiment propagation support our hy-
pothesis that the inference rules apply for Chinese
as well. Further, we categorize interesting gfbf
cases where the inferences are blocked in Section
6
http://en.wikipedia.org/wiki/B%C7%8E construction.
11
4.2. From our observation, the blocking infer-
ences are similar to what we have found in English
(Wiebe and Deng, 2014).
4.1 Graph-based Model
In the graph-based model, a node represents an en-
tity (agent, or object), and an edge exists between
two nodes if the two entities participate in one or
more gfbf events with each other. Scores on the
nodes represent the explicit sentiments, if any, ex-
pressed by the writer toward the entities. Scores
on the edges are based on constraints derived from
the rules. Loopy Belief Propagation (Pearl, 1982;
Yedidia et al., 2005) is applied to accomplish sen-
timent propagation in the graph. Given a graph
built from manually annotations, an evaluation is
carried out to assess the ability to propagate sen-
timent of the model. In the study, for each sub-
graph (connected component), we assign one of
the nodes in the subgraph with its gold-standard
polarity. Then we run LBP on each node in the
subgraph. The experiment is run on the subgraph
|S| times, where |S| is the number of nodes in
the subgraph. Therefore, each node is assigned
its gold-standard polarity exactly once, and each
node is given a propagated value |S| ? 1 times, as
propagated by each of the other nodes in its sub-
graph. We use Equations (4) and (5) to evaluate
the chance of a node given a correct propagated
label.
correct(a|b) =
{
1 a is correct
0 otherwise
(4)
correctness(a) =
?
b?S
a
,b 6=a
correct(a|b)
|S
a
| ? 1
(5)
Here we run the graph-based model on the Chi-
nese annotations. The data we use include the
training and testing paragraphs in the agreement
study, in total 85 paragraphs, 341 sentences and
160 gfbf triples. Later we use this corpus of 160
gfbf triples for analysis (denoted Chinese gfbf cor-
pus). Since the edge scores of the model are de-
fined according to the inference rules, if the senti-
ments are propagated correctly, this is a good evi-
dence that the inference rules apply to Chinese.
The performances of the sentiment propagation
are really good, reported in Table 3. The model
has an 70%-83% chance of propagating senti-
ments correctly in Chinese. This gives us confi-
dence that the inference rules apply in Chinese and
Dataset # subgraph correctness
all subgraphs 136 0.7058
multi-node subgraphs 61 0.8251
Table 3: Performance of Graph-Based Model in
Chinese.
further we can utilize these rules to assist Chinese
sentiment analysis. Compared to the scores of
correctness reported in (Deng and Wiebe, 2014),
which are 0.8874 for all subgraphs and 0.9030 for
multi-node subgraphs, our scores are lower. We
analyze the reasons for the gap between our scores
in Chinese and in English in the next section.
4.2 Blocking the Inference
A wrong propagation indicates the inferences re-
lated to that propagation are blocked. During the
error analysis, we have found three interesting cat-
egories of cases where the inferences are blocked.
Interestingly, we have observed these cases in En-
glish as well (Wiebe and Deng, 2014). In other
words, we didn?t find any blocking case specific to
Chinese. The lower scores of correctness in Chi-
nese might be due to the smaller amount of exper-
iment data and more blocking cases in this corpus.
Irrealis: This category contains gfbf events that
haven?t or will not happen. One of the case is
when the agent tried to conduct the gfbf event,
but failed. In Ex(4.1), the agent and objective are
underlined and the gfbf event is boldfaced. By
the rules, the writer has the same sentiment to-
ward the agents and objects in gf events and op-
posite sentiments toward the agents and objects in
bf events (Deng and Wiebe, 2014). In Ex(4.1), the
writer is negative toward both the agent and the
object, though this is a bf event. This is because
the event counter does not exist due to the failure,
which is implied by intended to. The inferences
for gfbf events in this category are blocked be-
cause the writer expresses the sentiments toward
entities based on what they have done so far.
EX(4.1) ...monetary policy activism in-
tended to counter the cyclical bumps
and grinds of the free market.
Forced GFBF: This category contains gfbf events
whose agents don?t intend to do that or be-
ing forced to conduct the event. For exam-
ple, in Ex(4.2), though the triple ?Obama, delay,
mandate? is an event which does not happen, it
12
is different from Ex(4.1). Here, the agent Obama
is forced to conduct the delaying, though he does
not want to and the writer does not blame him
if he does so. For the entities involved in forced
events, (at least the writer believes the entities are
involuntary,) the forced event will not affect the
writer?s sentiments toward the entities so that the
inferences are blocked.
EX(4.2) Some of them even seem to
think that they can bully Mr. Obama into
delaying the individual mandate too.
Quoted GFBF: This category contains gfbf
events in the quotations. Consider the Ex(4.3),
where one of the gfbf triple is ?law, reduce,
amount of labor ?. In the original editorial, the
writer supports the law and the writer has a posi-
tive sentiment toward the number of jobs (because
he/she expects to see more job opportunities). But
merely from the annotated gfbf triple, it is inferred
that the law has negative effect since it reduces the
number of jobs. This is not contradictory with the
writer?s stance because the writer regards the event
as a deliberate misreading he/she doesn?t believe.
The actual agent of the event should be (misread-
ing, Obama). This example shows that inferences
of a triple in the quotation are blocked, or event
flipped, based on the writer?s sentiment toward the
agent saying the quotation. The agent in a quoted
gfbf is similar to the notion of nested source in
sentiment analysis (Wilson and Wiebe, 2003).
EX(4.3) Some of the job-killer scare
stories are based on a deliberate mis-
reading that estimated the law would
?reduce the amount of labor used in the
economy? by about 800,000 jobs.
In conclusion, the good performance in our pilot
study gives supporting evidence for our hypothe-
sis. That is, the inference rules apply for Chinese.
Moreover, there is no evidence showing that the
cases where the inferences are blocked only hap-
pen in Chinese.
5 Chinese GoodFor/BadFor Lexicon
Above all we have assessed the similarity of im-
plicatures and inference rules in Chinese and En-
glish. In the following sections, we will analyze
whether Chinese gfbf components could be cap-
tured by similar techniques in English.
Description Count (Percentage %)
Parallel Span 122 (76.25%)
Chinese Adding GFBF 10 (6.875%)
Chinese Adding Object 6 (3.75%)
English Out Of Triple 5 (3.125%)
English Neutral 6 (3.125%)
Paraphrase 11 (6.875%)
Table 4: Counts of Chinese-English Corresponds
In this section, we compare the gfbf spans in
the Chinese gfbf corpus and the English version,
to investigate the possibility of deriving a bilingual
gfbf lexicon. Though the Chinese and English ed-
itorials are paragraph paralleled, they are not sen-
tence paralleled, because an English sentence may
be translated into multiple Chinese sentences and
several English sentences may be merged into one
Chinese sentence. Therefore, instead of automatic
word-alignment, we manually pick up the English
parallel spans of the Chinese annotated gfbfs. The
correspondences of Chinese and English spans are
categorized in Table 4. We present pairs of ex-
amples from the Chinese gfbf corpus, beginning
with the original English sentence (Eng), followed
by another English sentence which is the word-by-
word translation of the Chinese sentence (Chi).
Parallel Span: This category contains instances
where the Chinese annotated gfbf spans have
the corresponding translations in the English sen-
tences, and the English spans are also gfbf words.
Chinese Adding GFBF: In the original English
sentence below, its own making is a noun phrase
rather than a gfbf verb used as a noun. However, in
the Chinese version, there is a clear triple, ?itself,
makes, a monetary prison?. In such case the Chi-
nese version adds a gfbf event into the sentence.
Eng: ...the Fed is domiciled in a monetary prison
of its own making.
Chi: ...the Fed is domiciled in a monetary prison
which itself makes.
Chinese Adding Object: As stated in the manual,
all gfbf triples should have objects. Thus, in the
original sentence below, we will not mark exclu-
sion because the object is implicit. However, the
Chinese version clearly states the object, patients.
Eng: ...no more exclusion based on pre-existing
conditions...
Chi: ...no more exclusion of the patients based on
pre-existing conditions...
13
English Out Of Triple: Recall from Section 3.3,
the gfbf polarity must be sufficient to perceive the
gfbf polarity within the triple. The ?the Fed, get,
unemployment? below cannot be considered as a
gfbf, since whether it is good for or bad for the
unemployment depending on whether it is below
6.5% or up 6.5%, for instance. On the contrary,
the Chinese version uses the word decrease, which
is a bf word, no matter how many percents are
changed.
Eng: If and when the Fed ? which now promises
to get unemployment below 6.5%...
Chi: If and when the Fed ? which now promises
to decrease the unemployment to 6.5%...
English Neutral: Sometimes the English word
doesn?t have a gfbf meaning but the Chinese word
has one, based on the translator?s interpretation of
the whole editorial, though the triple structures are
the same in English and Chinese versions.
Eng: We?ve had eight decades of increasingly
frenetic monetary policy activism...
Chi: We?ve been insisting increasingly frenetic
monetary policy activism for eight decades...
In the original English sentence, had eight
decades of is hardly regarded as a gfbf word.
However, in the translated version, the word in-
sisting is a gf word. The change of wording intro-
duces a new gfbf event into the sentence.
Paraphrase: There are other cases where the sen-
tences are paraphrased so largely that we cannot
find a corresponding parallel span of the annotated
Chinese span in the original English sentence. A
majority of cases in this category are gfbf events
triggered by the Chinese syntax in Section 3.3.
In conclusion, the percentage of 76.25% in Row
Parallel Span indicates that it is applicable to de-
rive a bilingual gfbf lexicon from a parallel cor-
pus. However, we need to take into consideration
the 23.75% mismatches for higher precision.
5.1 Chinese Reversers
The polarity of a gfbf event could be changed by
a reverser (Deng et al., 2013). A common class
of reversers is negation. For example, in the sen-
tence, ?the bill will not increase the costs?, the gf
increase is changed to be bf via the negation not.
In this section, we analyze the Chinese reversers.
All of the reversers in the Chinese gfbf corpus
happen to be negations. In the English sentences,
the negations are easily extracted by neg depen-
dency relation. About 50% of the Chinese nega-
tions are linked to the gfbf events via neg as well.
Among this half, there are two negations com-
monly seen. One is ? (Not), often labeled as AD
(adverb) in terms of Part-Of-Speech, the other is
?? (do not have), labeled as VV (verb), shown
below. The negation is underlined and the gfbf
event it negates is boldfaced.
EX(5.1)?/AD??/VV???/NN
EX(5.2)??/VV??/VV??/NN
For the other half, the error mostly arises from
segmentations. For the sentence below, though?
? (doesn?t have), often labeled as VB, could be
regarded as a complete token, if we segment the
two characters into two independent tokens, the
parse is more similar to the English one. Below
we only list the most relevant part of the parses.
Eng: He does n?t have ability control war budget
Eng dep: neg(have-4, n?t-3), root(ROOT-0, have-
4), dobj(have-4, ability-6)
Chi: ? ? ? ?? ?? ?? ??
wrong dep: root(ROOT-0, ??-2), nsubj(??-
4,??-3), dep(??-2,??-4)
correct dep: neg(?-3, ?-2), root(ROOT-0, ?-
3), nsubj(??-5,??-4)
In conclusion, it is feasible to recognize re-
versers in Chinese but it calls for a suitable word
segmentation as input.
6 Syntax of Agent/Object in Chinese
According to (Deng et al., 2013), the agent is the
entity conducting the gfbf event and the object is
the entity that the gfbf event affects. This defini-
tion is very similar to subject and (in)direct ob-
ject in semantic role labeling. Xue and Palmer
(2004) investigate the Chinese semantic role la-
beling. They utilize the PropBank and the con-
stituency parser. However, from a preliminary
analysis of constituency parse, we cannot distin-
guish the agent and object merely from the parse
tree, because the sentences in the editorials are
usually complicated and it is difficult to classify
whether a noun phrase (NP) constituency is agent
or object in terms of its position. Kozhevnikov
and Titov (2013) adopt a model transfer between
different languages using dependency parser. In
our case, the dependency parser has labels such
14
as ?nsubj? and ?dobj?, which are strong indica-
tions of agents and objects. Thus, we use the
Stanford dependency parser, which has both En-
glish and Chinese parsers, to analyze the syntax
of agents/objects in the gfbf events. We count the
types of dependencies on the path in a dependency
parse between the tokens of agents/objects and the
tokens of gfbf events in the DCW corpus and the
Chinese gfbf corpus.
Among all the dependency types, 19.57% of the
labels between agents and gfbfs are the ones spe-
cially designed for Chinese and 25.82% between
objects and gfbf are the ones specially designed
for Chinese. This indicates there is a consider-
able number of differences in dependency types.
Chang et al. (2009), who create the Chinese
parser, discuss the differences between Chinese
and English types, which are similar to our obser-
vations.
First, there are more nsubj in Chinese for
agents (21.53%) and more dobj in Chinese for ob-
jects (21.59%), compared to English (17.43% and
14.01%), which are easier for the parser to detect.
Second, the most common types specially de-
signed for Chinese are assm, assmod and cpm (in
total 12.23% for agents and 16.14% for objects).
The relations assm is associative marker, assmod
is associative modifier, and cpm is complemen-
tizer. These are defined because of the frequent us-
age of ? (whose, of) in Chinese. Though there is
not a direct mapping between Chinese and English
dependency types, they are similar to two common
types in English: prep and pobj (together 23.36%
for agents and 31.62% for objects).
Third, there are more rcmod in Chinese than
those in English. There are 7.05% and 6.5% rc-
mod in Chinese agents and objects, respectively.
But there are only 1.7% and 2.16% in English
agents and objects. The type rcomd is a relative
clause modifier. If a verb is used as the modifier
of a noun, it will be labelled rcmod. Instead, En-
glish writers tend to use more adjectives to mod-
ify nouns, which will be labeled amod (4.04% and
4.48%).
Fourth, there are 7.63% and 6.22% punct in
Chinese agents and object, compared to both 0%
in English. In addition, there are 3.36% and 3.31%
conj in English agents and objects. Chang et
al. (2009) explain that English use conjunctions
(conj) to link clauses while Chinese tend to use
punctuation. Another finding in our corpus is that,
translators tend to break down a long English sen-
tence into several Chinese clauses, linked by punc-
tuations.
For the other Chinese types, most of them are
modifiers, which may be grouped with similar En-
glish modifiers.
7 Chinese Explicit Sentiment Analysis
There are various available resources for Chinese
sentiment analysis, such as sentiment lexicon from
HowNet
7
, NTU Sentiment Dictionary (NTUSD)
(Ku and Chen, 2007)
8
and the sentiment lexi-
con from Tsinghua University (Li and Sun, 2007).
The sentiments recognized from lexicon hits are
explicit, meaning that the writers use sentiment
words to express his/her opinions. These explicit
sentiment results are provided to the graph-based
model as input. Note that the model plays a role
of sentiment inference, instead of directly detect-
ing sentiments from the text. The inferred senti-
ments are implicit, meaning that the writers ex-
press his/her opinions even without using a senti-
ment lexical clue.
8 Conclusion
In this work we investigate implicit opinions
expressed via goodFor/badFor events in Chinese.
The positive results have provided evidences
that such implicit opinions and inference rules
are similar in Chinese and English. There are
some gfbf events caused by the Chinese syntax,
guidance for which could be added to the current
English manual to develop a Chinese manual.
Moreover, there is no evidence showing that the
blocked inferences only happen in Chinese. We
also assess the feasibility of acquiring components
of gfbf events from Chinese text using current
available resources. In the future, it is promising
to utilize gfbf information to assist sentiment
analysis in Chinese.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank
Changsheng Liu and Fan Zhang for their anno-
tations in the agreement study, and thank anony-
mous reviewers for their feedback.
7
Available at: http://www.keenage.com/html/e index.html
8
Available at: http://nlg18.csie.ntu.edu.tw:8080/lwku/pub1.html
15
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
Yuen Ren Chao. 1968. A grammar of spoken Chinese.
Univ of California Press.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 793?801, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Meeting
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 120?125,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Sofia, Bulgaria, Angust. Association for Com-
putational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum?e III. 2013. A
computational model for plot units. Computational
Intelligence, 29(3):466?488.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190?1200, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Lun-Wei Ku and Hsin-Hsi Chen. 2007. Mining
opinions from the web: Beyond relevance retrieval.
Journal of the American Society for Information Sci-
ence and Technology, 58(12):1838?1850.
Jun Li and Maosong Sun. 2007. Experimental
study on sentiment classification of chinese review
using machine learning techniques. In Natural
Language Processing and Knowledge Engineering,
2007. NLP-KE 2007. International Conference on,
pages 393?400. IEEE.
Charles N Li and Sandra A Thompson. 1989. Man-
darin Chinese: A functional reference grammar.
Univ of California Press.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 320?330. Association
for Computational Linguistics.
J. Pearl. 1982. Reverend bayes on inference engines:
A distributed hierarchical approach. In Proceedings
of the American Association of Artificial Intelligence
National Conference on AI, pages 133?136, Pitts-
burgh, PA.
Kevin Reschke and Pranav Anand. 2011. Extracting
contextual evaluativity. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, IWCS ?11, pages 370?374, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rintje Pieter Eelke Sybesma. 1992. Causatives and
accomplishments: The case of Chinese ba, vol-
ume 1. Holland Institute of Generative Linguistics.
Benjamin KY Tsou, Raymond WM Yuen, Oi Yee
Kwong, TBY La, and Wei Lung Wong. 2005. Po-
larity classification of celebrity coverage in the chi-
nese press. In Proceedings of International Confer-
ence on Intelligence Analysis.
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 553?561, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
16
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 235?243. Association for Compu-
tational Linguistics.
Suge Wang, Yingjie Wei, Deyu Li, Wu Zhang, and
Wei Li. 2007. A hybrid method of feature se-
lection for chinese text sentiment classification. In
Fuzzy Systems and Knowledge Discovery, 2007.
FSKD 2007. Fourth International Conference on,
volume 3, pages 435?439. IEEE.
Janyce Wiebe and Lingjia Deng. 2014. An account of
opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language ann. Language Resources and
Evaluation, 39(2/3):164?210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13?22.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In EMNLP,
pages 88?94.
Bishan Yang and Claire Cardie. 2013. Joint Inference
for Fine-grained Opinion Extraction. In Proceed-
ings of ACL, pages 1640?1649.
Jonathan S Yedidia, William T Freeman, and Yair
Weiss. 2005. Constructing free-energy approx-
imations and generalized belief propagation algo-
rithms. Information Theory, IEEE Transactions on,
51(7):2282?2312.
Lei Zhang and Bing Liu. 2011. Identifying noun prod-
uct features that imply opinions. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 575?580, Portland, Oregon, USA, June.
Association for Computational Linguistics.
17
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 107?112,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Lexical Acquisition for Opinion Inference:
A Sense-Level Lexicon of Benefactive and Malefactive Events
Yoonjung Choi
1
, Lingjia Deng
2
, and Janyce Wiebe
1,2
1
Department of Computer Science
2
Intelligent Systems Program
University of Pittsburgh
yjchoi@cs.pitt.edu, lid29@pitt.edu, wiebe@cs.pitt.edu
Abstract
Opinion inference arises when opinions
are expressed toward states and events
which positive or negatively affect entities,
i.e., benefactive and malefactive events.
This paper addresses creating a lexicon of
such events, which would be helpful to in-
fer opinions. Verbs may be ambiguous,
in that some meanings may be benefac-
tive and others may be malefactive or nei-
ther. Thus, we use WordNet to create a
sense-level lexicon. We begin with seed
senses culled from FrameNet and expand
the lexicon using WordNet relationships.
The evaluations show that the accuracy of
the approach is well above baseline accu-
racy.
1 Introduction
Opinions are commonly expressed in many kinds
of written and spoken text such as blogs, reviews,
new articles, and conversation. Recently, there
have been a surge in reserach in opinion analy-
sis (sentiment analysis) research (Liu, 2012; Pang
and Lee, 2008).
While most past researches have mainly ad-
dressed explicit opinion expressions, there are a
few researches for implicit opinions expressed via
implicatures. Deng and Wiebe (2014) showed
how sentiments toward one entity may be prop-
agated to other entities via opinion implicature
rules. Consider The bill would curb skyrocketing
health care costs. Note that curb costs is bad for
the object costs since the costs are reduced. We
can reason that the writer is positive toward the
event curb since the event is bad for the object
health care costs which the writer expresses an ex-
plicit negative sentiment (skyrocketing). We can
reason from there that the writer is positive toward
the bill, since it is the agent of the positive event.
These implicature rules involve events that pos-
itively or negatively affect the object. Such events
are called malefactive and benefactive, or, for ease
of writing, goodFor (gf ) and badFor (bf ) (here-
after gfbf). The list of gfbf events and their polari-
ties (gf or bf) are necessary to develop a fully auto-
matic opinion inference system. On first thought,
one might think that we only need lists of gfbf
words. However, it turns out that gfbf terms may
be ambiguous ? a single word may have both gf
and bf meanings.
Thus, in this work, we take a sense-level ap-
proach to acquire gfbf lexicon knowledge, lead-
ing us to employ lexical resources with fine-
grained sense rather than word representations.
For that, we adopt an automatic bootstrapping
method which disambiguates gfbf polarity at the
sense-level utilizing WordNet, a widely-used lex-
ical resource. Starting from the seed set manually
generated from FrameNet, a rich lexicon in which
words are organized by semantic frames, we ex-
plore how gfbf terms are organized in WordNet via
semantic relations and expand the seed set based
on those semantic relations.
The expanded lexicon is evaluated in two ways.
First, the lexicon is evaluated against a corpus that
has been annotated with gfbf information at the
word level. Second, samples from the expanded
lexicon are manually annotated at the sense level,
which gives some idea of the prevalence of gfbf
lexical ambiguity and provides a basis for sense-
level evaluation. Also, we conduct the agreement
study. The results show that the expanded lexi-
con covers more than half of the gfbf instances
in the gfbf corpus, and the system?s accuracy, as
measured against the sense-level gold standard, is
substantially higher than baseline. In addition, in
the agreement study, the annotators achieve good
agreement, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
107
2 The GFBF Corpus
A corpus of blogs and editorials about the Afford-
able Care Act, a controversial topic, was manu-
ally annotated with gfbf information by Deng et
al. (2013)
1
. This corpus provides annotated gfbf
events and the agents and objects of the events. It
consists of 134 blog posts and editorials. Because
the Affordable Health Care Act is a controversial
topic, the data is full of opinions. In this corpus,
1,411 gfbf instances are annotated, each including
a gfbf event, its agent, and its object (615 gf in-
stances and 796 bf instances). 196 different words
appear in gf instances and 286 different words ap-
pear in bf instances; 10 words appear in both.
3 Sense-Level GFBF Ambiguity
A word may have one or more meanings. For
that, we use WordNet
2
, which is a large lexical
database of English (Miller et al., 1990). In Word-
Net, nouns, verbs, adjectives, and adverbs are or-
ganized by semantic relations between meanings
(senses). We assume that a sense is exactly one
of gf, bf, or neither. Since words often have more
than one sense, the polarity of a word may or may
not be consistent, as the following WordNet exam-
ples show.
? A word with only gf senses: encourage
S1: (v) promote, advance, boost, further, en-
courage (contribute to the progress or growth
of)
S2: (v) encourage (inspire with confidence;
give hope or courage to)
S3: (v) encourage (spur on)
? A word with only bf senses: assault
S1: (v) assail, assault, set on, attack (attack
someone physically or emotionally)
S2: (v) rape, ravish, violate, assault, dis-
honor, dishonour, outrage (force (someone)
to have sex against their will)
S3: (v) attack, round, assail, lash out, snipe,
assault (attack in speech or writing)
All senses of encourage are good for the object,
and all senses of assault are bad for the object.
The polarity is always same regardless of sense.
In such cases, for our purposes, which particular
sense is being used does not need to be determined
because any instance of the word will be good for
1
Available at http://mpqa.cs.pitt.edu/corpora/gfbf/
2
WordNet, http://wordnet.princeton.edu/
(bad for); that is, word-level approaches can work
well. However, word-level approaches are not ap-
plicable for all the words. Consider the following:
? A word with gf and neutral senses: inspire
S3: (v) prompt, inspire, instigate (serve as the
inciting cause of)
S4: (v) cheer, root on, inspire, urge, barrack,
urge on, exhort, pep up (spur on or encourage
especially by cheers and shouts)
S6: (v) inhale, inspire, breathe in (draw in
(air))
? A word with bf and neutral senses: neutral-
ize
S2: (v) neutralize, neutralise, nullify, negate
(make ineffective by counterbalancing the ef-
fect of)
S6: (v) neutralize, neutralise (make chemi-
cally neutral)
The words inspire and neutralize both have 6
senses (we list a subset due to space limitations).
For inspire, while S3 and S4 are good for the ob-
ject, S6 doesn?t have any polarity, i.e., it is a neu-
tral (we don?t think of inhaling air as good for the
air). Also, while S2 of neutralize is bad for the
object, S6 is neutral (neutralizing a solution just
changes its pH). Thus, if word-level approaches
are applied using these words, some neutral in-
stances may be incorrectly classified as gf or bf
events.
? A word with gf and bf senses: fight
S2: (v) fight, oppose, fight back, fight down,
defend (fight against or resist strongly)
S4: (v) crusade, fight, press, campaign, push,
agitate (exert oneself continuously, vigor-
ously, or obtrusively to gain an end or engage
in a crusade for a certain cause or person; be
an advocate for)
As mentioned in Section 2, 10 words are ap-
peared in both gf and bf instances. Since only
words and not senses are annotated in the corpus,
such conflicts arise. These 10 words account for
9.07% (128 instances) of all annotated instances.
One example is fight. In the corpus instance fight
for a piece of legislation, fight is good for the ob-
ject, a piece of legislation. This is S4. However,
in the corpus instance we need to fight this repeal,
the meaning of fight here is S2, so fight is bad for
the object, this repeal.
108
Thesefore, approaches for determining the gfbf
polarity of an instance that are sense-level instead
of word-level promise to have higher precision.
4 Lexicon Acquisition
In this section, we develop a sense-level gfbf lex-
icon by exploiting WordNet. The method boot-
straps from a seed lexicon and iteratively follows
WordNet relations. We consider only verbs.
4.1 Seed Lexicon
To preserve the corpus for evaluation, we created
a seed set that is independent from the corpus. An
annotator who didn?t have access to the corpus
manually selected gfbf words from FrameNet
3
in
the light of semantic frames. The annotator found
592 gf words and 523 bf words. Decomposing
each word into its senses in WordNet, there are
1,525 gf senses and 1,154 bf senses. 83 words ex-
tracted from FrameNet overlap with gfbf instances
in the corpus. For independence, those words were
discarded. Among the senses of the remaining
words, we randomly choose 200 gf senses and 200
bf senses.
4.2 Expansion Method
In WordNet, verb senses are arranged into hier-
archies, that is, verb senses towards the bottom
of the trees express increasingly specific manners.
Thus, we can follow hypernym relations to more
general senses and troponym relations to more spe-
cific verb senses. Since the troponym relation
refers to a specific elaboration of a verb sense, we
hypothesized that troponyms of a synset tends to
have its same polarity (i.e., gf or bf). We only con-
sider the direct troponyms in a single iteration. Al-
though the hypernym is a more general term, we
hypothesized that direct hypernyms tend to have
the the same or neutral polarity, but not the oppo-
site polarity. Also, the verb groups are promising;
even though the coverage is incomplete, we expect
the verb groups to be the most helpful.
WordNet Similarity
4
, is a facility that provides a
variety of semantic similarity and relatedness mea-
sures based on information found in the Word-
Net lexical database. We choose Jiang&Conrath
(1997) (jcn) method which has been found to be
effective for such tasks by NLP researchers. When
two concetps aren?t related at all, it returns 0. The
3
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
4
WN Similarity, http://wn-similarity.sourceforge.net/
more they are related, the higher the value is re-
tuned. We regarded words with similarity values
greater than 1.0 to be similar words.
Beginning with its seed set, each lexicon (gf and
bf) is expanded iteratively. On each iteration, for
each sense in the current lexicon, all of its direct
troponyms, direct hypernyms, and members of the
same verb group are extracted and added to the
lexicon for the next iteration. Similarity, for each
sense, all words with above-threshold jcn values
are added. For new senses that are extracted for
both the gf and bf lexicons, we ignore such senses,
since there is conflicting evidence (recall that we
assume a sense has only one polarity, even if a
word may have senses of different polarities).
4.3 Corpus Evaluation
In this section, we use the gfbf annotations in the
corpus as a gold standard. The annotations in the
corpus are at the word level. To use the annota-
tions as a sense-level gold standard, all the senses
of a word marked gf (bf) in the corpus are con-
sidered to be gf (bf). While this is not ideal, this
allows us to evaluate the lexicon against the only
corpus evidence available.
The 196 words that appear in gf instances in
the corpus have a total of 897 senses, and the 286
words that appear in bf instances have a total of
1,154 senses. Among them, 125 senses are con-
flicted: a sense of a word marked gf in the corpus
could be a member of the same synset as a sense
of a word marked bf in the corpus. For a more reli-
able gold-standard set, we ignored these conflicted
senses. Thus, the gold-standard set contains 772 gf
senses and 1,029 bf senses.
Table 1 shows the results after five iterations of
lexicon expansion. In total, the gf lexicon contains
4,157 senses and the bf lexicon contains 5,071
senses. The top half gives the results for the gf
lexicon and the bottom half gives the results for
the bf lexicon. In the table, gfOverlap means the
overlap between the senses in the lexicon in that
row and the gold-standard gf set, while bfOverlap
is the overlap between the senses in the lexicon in
that row and the gold-standard bf set. That is, of
the 772 senses in the gf gold standard, 449 (58%)
are in the gf expanded lexicon while 105 (14%)
are in the bf expanded lexicon.
Accuracy (Acc) for gf is calculated as #gfOver-
lap / (#gfOverlap + #bfOverlap) and bf is calcu-
lated as #bfOverlap / (#gfOverlap + #bfOverlap).
109
goodFor
#senses #gfOverlap #bfOverlap Acc
Total 4,157 449 176 0.72
WN Sim 1,073 134 75 0.64
Groups 242 69 24 0.74
Troponym 4,084 226 184 0.55
Hypernym 223 75 33 0.69
badFor
#senses #gfOverlap #bfOverlap Acc
Total 5,071 105 562 0.84
WN Sim 1,008 34 190 0.85
Groups 255 11 86 0.89
Troponym 4,258 66 375 0.85
Hypernym 286 16 77 0.83
Table 1: Results after lexicon expansion
Overall, accuracy is higher for the bf than the
gf lexicon. The results in the table are broken
down by semantic relation. Note that the individ-
ual counts do not sum to the totals because senses
of different words may actually be the same sense
in WordNet. The results for the bf lexicon are con-
sistently high over all semantic relations. The re-
sults for the gf lexicon are more mixed, but all re-
lations are valuable.
The WordNet Similarity is advantageous be-
cause it detects similar senses automatically, so
may provide coverage beyond the semantic rela-
tions coded in WordNet.
Overall, the verb group is the most informative
relation, as we suspected.
Although the gf-lexicon accuracy for the tro-
ponym relation is not high, it has the advantage
is that it yields the most number of senses. Its
lower accuracy doesn?t support our original hy-
pothesis. We first thought that verbs lower down in
the hierarchy would tend to have the same polar-
ity since they express specific manners character-
izing an event. However, this hypothesis is wrong.
Even though most troponyms have the same polar-
ity, there are many exceptions. For example, pro-
tect#v#1, which means the first sense of the verb
protect, has 18 direct troponyms such as cover
for#v#1, overprotect#v#2, and so on. protect#v#1
is a gf event because the meaning is ?shielding
from danger? and most troponyms are also gf
events. However, overprotect#v#2, which is one
of troponyms of protect#v#1, is a bf event.
For the hypernym relation, the number of de-
tected senses is not large because many were al-
ready detected in previous iterations (in general,
there are fewer nodes on each level as hypernym
links are traversed).
4.4 Sense Annotation Evaluation
For a more direct evaluation, two annotators, who
are co-authors, independently annotated a sample
of senses. We randomly selected 60 words among
the following classes: 10 pure gf words (i.e., all
senses of the words are classified by the expan-
sion method, and all senses are put into the gf lex-
icon), 10 pure bf words, 20 mixed words (i.e., all
senses of the words are classified by the expan-
sion method, and some senses are put into the gf
lexicon while others are put into the bf lexicon),
and 20 incomplete words (i.e., some senses of the
words are not classified by the expansion method).
The total number of senses is 151; 64 senses
are classified as gf, 56 senses are classified as bf,
and 31 senses are not classified. We included more
mixed than pure words to make the results of the
study more informative. Further, we wanted to in-
cluded non-classified senses as decoys for the an-
notators. The annotators only saw the sense en-
tries from WordNet. They didn?t know whether
the system classified a sense as gf or bf or whether
it didn?t classify it at all.
Table 2 evaluates the lexicons against the man-
ual annotations, and in comparison to the ma-
jority class baseline. The top half of the table
shows results when treating Anno1?s annotations
as the gold standard, and the bottom half shows
the results when treating Anno2?s as the gold stan-
dard. Among 151 senses, Anno1 annotated 56
senses (37%) as gf, 51 senses (34%) as bf, and
44 senses (29%) as neutral. Anno2 annotated 66
senses (44%) as gf, 55 senses (36%) as bf, and
30 (20%) senses as neutral. The incorrect cases
are divided into two sets: incorrect opposite con-
sists of senses that are classified as the opposite
polarity by the expansion method (e.g., the sense
is classified into gf, but annotator annotates it as
bf), and incorrect neutral consists of senses that
the expansion method classifies as gf or bf, but the
annotator marked it as neutral. We report the accu-
racy and the percentage of cases for each incorrect
case. The accuracies substantially improve over
baseline for both annotators and for both classes.
In Table 3, we break down the results into gfbf
classes. The gf accuracy measures the percentage
of correct gf senses out of all senses annotated as
gf according to the annotations (same as bf accu-
racy). As we can see, accuracy is higher for the
bf than the gf. The conclusion is consistent with
what we have discovered in Section 4.3.
110
By Anno1, 8 words are detected as mixed
words, that is, they contain both gf and bf senses.
By Anno2, 9 words are mixed words (this set in-
cludes the 8 mixed words of Anno1). Among
the randomly selected 60 words, the proportion of
mixed words range from 13.3% to 15%, according
to the two annotators. This shows that gfbf lexical
ambiguity does exist.
To measure agreement between the annotators,
we calculate two measures: percent agreement and
? (Artstein and Poesio, 2008). ? measures the
amount of agreement over what is expected by
chance, so it is a stricter measure. Percent agree-
ment is 0.84 and ? is 0.75.
accuracy % incorrect % incorrect base-
opposite neutral line
Anno1 0.53 0.16 0.32 0.37
Anno2 0.57 0.24 0.19 0.44
Table 2: Results against sense-annotated data
gf accuracy bf accuracy baseline
Anno1 0.74 0.83 0.37
Anno2 0.68 0.74 0.44
Table 3: Accuracy broken down for gfbf
5 Related Work
Lexicons are widely used in sentiment analysis
and opinion extraction. There are several previ-
ous works to acquire or expand sentiment lexi-
cons such as (Kim and Hovy, 2004), (Strapparava
and Valitutti, 2004), (Esuli and Sebastiani, 2006),
(Gyamfi et al., 2009), (Mohammad and Turney,
2010) and (Peng and Park, 2011). Such senti-
ment lexicons are helpful for detecting explicitly
stated opinions, but are not sufficient for recog-
nizing implicit opinions. Inferred opinions often
have opposite polarities from the explicit senti-
ment expressions in the sentence; explicit senti-
ments must be combined with benefactive, male-
factive state and event information to detect im-
plicit sentiments. There are few previous works
closest to ours. (Feng et al., 2011) build con-
notation lexicons that list words with connotative
polarity and connotative predicates. Goyal et al.
(2010) generate a lexicon of patient polarity verbs
that imparts positive or negative states on their pa-
tients. Riloff et al. (2013) learn a lexicon of nega-
tive situation phrases from a corpus of tweets with
hashtag ?sarcasm?.
Our work is complementary to theirs in that
their acquisition methods are corpus-based, while
we acquire knowledge from lexical resources.
Further, all of their lexicons are word level while
ours are sense level. Finally, the types of entries
among the lexicons are related but not the same.
Ours are specifically designed to support the au-
tomatic recognition of implicit sentiments in text
that are expressed via implicature.
6 Conclusion and Future Work
In this paper, we developed a sense-level gfbf
lexicon which was seeded by entries culled from
FrameNet and then expanded by exploiting se-
mantic relations in WordNet. Our evaluations
show that such lexical resources are promising for
expanding such sense-level lexicons. Even though
the seed set is completely independent from the
corpus, the expanded lexicon?s coverage of the
corpus is not small. The accuracy of the expanded
lexicon is substantially higher than baseline accu-
racy. Also, the results of the agreement study are
positive, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
However, there is still room for improvement.
We believe that gf/bf judgements of word senses
could be effectively crowd-sourced; (Akkaya et
al., 2010), for example, effectively used Ama-
zon Mechanical Turk (AMT) for similar coarse-
grained judgements. The idea would be to use au-
tomatic expansion methods to create a sense-level
lexicon, and then have AMT workers judge the
entries in which we have least confidence. This
would be much more time- and cost-effective.
The seed sets we used are small - only 400 total
senses. We believe it will be worth the effort to
create larger seed sets, with the hope to mine many
additional gfbf senses from WordNet.
To exploit the lexicon to recognize sentiments in
a corpus, the word-sense ambiguity we discovered
needs to be addressed. There is evidence that the
performance of word-sense disambiguation sys-
tems using a similar coarse-grained sense inven-
tory is much better than when the full sense inven-
tory is used (Akkaya et al., 2009; Akkaya et al.,
2011). That, coupled with the fact that our study
suggests that many words are unambiguous with
respect to the gfbf distinction, makes us hopeful
that gfbf information may be practically exploited
to improve sentiment analysis in the future.
111
7 Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s
Mechanical Turk, pages 195?203.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120?125.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092?
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77?86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10?18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of COLING.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367?1373.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235?312.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704?714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083?1086.
112
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 154?159,
Baltimore, Maryland, USA. June 27, 2014.
c
?2014 Association for Computational Linguistics
A Conceptual Framework for Inferring Implicatures
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Lingjia Deng
Intelligent Systems Program
University of Pittsburgh
lid29@pitt.edu
Abstract
While previous sentiment analysis re-
search has concentrated on the interpreta-
tion of explicitly stated opinions and atti-
tudes, this work addresses a type of opin-
ion implicature (i.e., opinion-oriented de-
fault inference) in real-world text. This
work describes a rule-based conceptual
framework for representing and analyzing
opinion implicatures. In the course of un-
derstanding implicatures, the system rec-
ognizes implicit sentiments (and beliefs)
toward various events and entities in the
sentence, often of mixed polarities; thus,
it produces a richer interpretation than is
typical in opinion analysis.
1 Introduction
This paper is a brief introduction to a framework
we have developed for sentiment inference (Wiebe
and Deng, 2014). Overall, the goal of this work is
to make progress toward a deeper automatic inter-
pretation of opinionated language by developing
computational models for the representation and
interpretation of opinion implicature (i.e., opinion-
oriented default inference) in language. In this
paper, we feature a rule-based implementation of
a conceptual framework for opinion implicatures,
specifically implicatures that arise in the presence
of explicit sentiments, and events that positively or
negatively affect entities (goodFor/badFor events).
To eliminate interference introduced by the noisy
output of automatic NLP components, the sys-
tem takes as input manually annotated explicit-
sentiment and event information, and makes in-
ferences based on that input information. Thus,
the purpose of this work is to provide a conceptual
understanding of (a type of) opinion implicature,
to provide a blueprint for realizing fully automatic
systems in the future.
Below, we give terminology, overview the rule-
based system, and then present the rule schemas.
Finally, via discussion of an example from the
MPQA opinion-annotated corpus (Wiebe et al.,
2005)
1
, we illustrate the potential of the frame-
work for recognizing implicit sentiments and
writer-level sentiments that are not anchored on
clear sentiment words, and for capturing inter-
dependencies among explicit and implicit senti-
ments.
We have developed a graph-based computa-
tional model implementing some rules introduced
below (Deng and Wiebe, 2014). Moreover, in on-
going work, we have proposed an optimization
framework to jointly extract and resolve the input
ambiguities.
2 Terminology
The building blocks of our opinion implicature
framework are subjectivity, inferred private states,
and benefactive/malefactive events and states.
Subjectivity. Following (Wiebe et al., 2005;
Wiebe, 1994), subjectivity is defined as the ex-
pression of private states in language, where pri-
vate states are mental and emotional states such as
speculations, sentiments, and beliefs (Quirk et al.,
1985). Subjective expressions (i.e., opinions) have
sources (or holders): the entity or entities whose
private states are being expressed. Again follow-
ing (Wiebe et al., 2005; Wiebe, 1994), a private
state is an attitude held by a source toward (op-
tionally) a target. Sentiment and belief are types
of attitudes. Subjectivity is the linguistic expres-
sion of private states. Subjectivity is a pragmatic
notion: as the sentence is interpreted in context,
a private state is attributed to a source in that con-
text (Banfield, 1982). By sentiment expression
or explicit sentiment, we mean a subjective ex-
pression where the attitude type of the expressed
1
Available at http://mpqa.cs.pitt.edu
154
private state is sentiment.
There are many types of linguistic clues that
contribute to recognizing subjective expressions
(Wiebe, 1994). In the clearest case, some word
senses give rise to subjectivity whenever they are
used in discourse (Wiebe and Mihalcea, 2006).
Other clues are not as definitive. For example, re-
searchers in NLP have begun to develop lexicons
of connotations (Feng et al., 2011), i.e., words as-
sociated with polarities out of context (e.g., war
has negative connotation and sunshine has positive
connotation (Feng et al., 2013)). However, words
may be used in context with polarities opposite to
their connotations, as in Ghenghis Kan likes war.
Inferred Private States and Opinion Implica-
tures. We address private states inferred from
other private states, where the attitude type of both
is sentiment. Inference is initiated by explicit sen-
timent subjectivity. We borrow the term implica-
ture from linguistics, specifically generalized con-
versational implicature. Grice (1967; 1989) intro-
duced the notion to account for how more can be
pragmatically communicated than what is strictly
said - what is implicated vs. what is said (Doran
et al., 2012). Generalized conversational implica-
tures are cancellable, or defeasible.
Analogously, we can treat subjectivity as part
of what is said,
2
and the private-state inferences
we address to be part of what is implicated.
Opinion implicatures are default inferences that
may not go through in context.
Benefactive/Malefactive Events and States.
This work addresses sentiments toward, in gen-
eral, states and events which positively or nega-
tively affect entities. Various lexical items and
semantic roles evoke such situations. We adopt
one clear case in this work (Deng et al., 2013):
?agent, event, object? triples, where event nega-
tively (badFor) or positively (goodFor) affects the
object. An event that is goodFor or badFor is a
gfbf event. Note that we have annotated a corpus
with gfbf information and the speaker?s sentiment
toward the agents and objects of gfbf events (Deng
et al., 2013).
3
2
While the focus in the literature on what is said is se-
mantics, Grice and people later working on the topic ac-
knowledge that what is said must include pragmatics such as
co-reference and indexical resolution (Doran et al., 2012),
and subjectivity arises from deixis (Bruder and Wiebe, 1995;
Stein and Wright, 1995). However, as long as what is said is
conceived of as only truth evaluable propositions, then it is
not exactly the notion for our setting.
3
Available at http://mpqa.cs.pitt.edu
3 Overview
In this section, we give an overview of the rule-
based system to provide an intuitive big picture of
what it can infer, instead of elaborating specific
rules, which will be introduced in Section 4.
The system includes default inference rules
which apply if there is no evidence to the contrary.
It requires as input explicit sentiment and gfbf in-
formation (plus any evidence that is contrary to the
inferences). The data structure of the input and the
output are described in Section 3.1. The rules are
applied repeatedly until no new conclusions can be
drawn. If a rule matches a sentiment or event that
is the target of a private state, the nesting struc-
ture is preserved when generating the conclusions.
We say that inference is carried out in private state
spaces, introduced in Section 3.2. Finally in Sec-
tion 3.3, an example is provided to illustrate what
the system is able to infer.
3.1 Data Structure
The system builds a graphical representation of
what it knows and infers about the meaning of
a sentence. A detailed knowledge representation
scheme is presented in (Wiebe and Deng, 2014).
Below is an example from the MPQA corpus.
Ex(1) [He] is therefore planning to trig-
ger [wars] here and there to revive [the
flagging arms industry].
There are two gfbf events in this sentence: ?He,
trigger, wars? and ?He, revive, arms industry?. The
system builds these nodes as input (as printed by
the system):
8 writer positive believesTrue
4 He revive flagging arms industry
6 writer positive believesTrue
1 He trigger wars
The system?s printout does not show all the
structure of a node. Consider node 8. It has a
source edge to the node representing the writer,
and a target edge to node 4, which in turn has an
agent edge to the node representing He and a ob-
ject edge to the node representing flagging arms
industry. The nodes also have attributes which
record, e.g., what type of node it is (node 8 is a
privateState and node 4 is a gfbf), polarity (if rele-
vant), etc.
The graph is directed. For example, node 4 is
a child of 8. A specification for the input is that
each root node must be a sentiment or believesTrue
155
node whose source is the writer. Inference pro-
ceeds by matching rules to the graph built so far
and, when a rule successfully fires, adding nodes
to the graph.
3.2 Private State Spaces
The approach adopted here follows work on rea-
soning in belief spaces and belief ascription in nat-
ural language (Martins and Shapiro, 1983; Rapa-
port, 1986; Slator and Wilks, 1987). Other than
private states of the writer, all propositions and
events must be the target of some private state. In
the simplest case, the writer believes the proposi-
tion or event he/she describes in the document, so
the proposition or event is nested under a writer
positive believesTrue node.
We want to carry out inferences within private
state spaces so that, for example, from S positive
believesTrue P, & P =? Q, the system may in-
fer S positive believesTrue Q. However, we are
working with sentiment, not only belief as in ear-
lier work, and we want to allow, as appropriate,
these types of inferences: from S sentiment to-
ward P, & P =?Q, infer S sentiment toward Q.
For example, if I?m upset my computer is infected
with a virus, then I?m also upset with the conse-
quences (e.g., that my files may be corrupted).
A private state space is defined by a path where
the root is a believesTrue or sentiment node whose
source is the writer, and each node on the path is a
believesTrue or sentiment node. Two paths define
the same private state space if, at each correspond-
ing position, they have the same attitude type, po-
larity, and source. P is in a private state space if P
is the target of the rightmost node on a path defin-
ing that space.
3.3 An Example
Now we have introduced the data structure and the
private state spaces, let?s see the potential conclu-
sions which the system can infer before we go into
the detailed rules in the next section.
Ex(2) However, it appears as if [the in-
ternational community (IC)] is tolerat-
ing [the Israeli] campaign of suppres-
sion against [the Palestinians].
The input nodes are the following.
writer negative sentiment
IC positive sentiment
Israeli suppression Palestinians
The gfbf event ?Israeli, suppression,
Palestinians? is a badFor event. According
to the writer, the IC is positive toward the event
in the sense that they tolerate (i.e., protect) it.
However and appears as if are clues that the
writer is negative toward IC?s positive sentiment.
Given these input annotations, the following are
the sentiments inferred by the system just toward
the entities in the sentence; note that many of the
sentiments are nested in private state spaces.
writer positive sentiment
Palestinians
writer negative sentiment
Israel
writer negative sentiment
IC
writer positive believesTrue
Israel negative sentiment
Palestinians
writer positive believesTrue
IC negative sentiment
Palestinians
writer positive believesTrue
IC positive sentiment
Israel
writer positive believesTrue
IC positive believesTrue
Israel negative sentiment
Palestinians
Note that for the sentiments between two enti-
ties other than the writer (e.g., Israel negative to-
ward Palestinians), they are nested under a writer
positive believesTrue node. This shows why we
need private state spaces. The writer expresses
his/her opinion that the sentiment from Israel to-
ward Palestinians is negative, which may not be
true outside the scope of this single document.
4 Rules
Rules include preconditions and conclusions.
They may also include assumptions (Hobbs et al.,
1993). For example, suppose a rule would suc-
cessfully fire if an entity S believes P. If the entity
S is not the writer but we know that the writer be-
lieves P, and there is no evidence to the contrary
(i.e. there is no evidence showing that the entity
S doesn?t believe P), then we?ll assume that S be-
lieves it as well, if a rule ?asks us to?.
Thus, our rules are conceptually of the form:
P1, ..., P j : A1, .., Ak/Q1, ..., Qm
where the P s are preconditions, the As are as-
sumptions, and the Qs are conclusions. For the Qs
to be concluded, the P s must already hold; there
156
must be a basis for assuming each A; and there
must be no evidence against any of the As or Qs.
Assumptions are indicated using the term ?As-
sume?, as in rule 10, which infers sentiment from
connotation:
rule10:
(Assume Writer positive ...
believesTrue) A gfbf T &
T?s anchor is in connotation lexicon =?
Writer sentiment toward T
The first line contains an assumption, the sec-
ond line contains a precondition, and the third con-
tains a conclusion.
rule8:
S positive believesTrue A gfbf T &
S sentiment toward T =?
S sentiment toward A gfbf T
For example, applying rule 8 to ?The bill would
curb skyrocketing health care costs,? from the
writer?s (S?s) negative sentiment toward the costs
(T) expressed by skyrocketing, we can infer the
writer is positive toward the event ?bill, curb,
costs? (A gfbf T) because it would decrease the
costs.
Note that, in rule 8, the inference is (senti-
ment toward object) =? (sentiment toward event).
Rules 1 and 2 infer in the opposite direction.
rule1:
S sentiment toward A gfbf T =?
S sentiment toward idea of A gfbf T
rule2:
S sentiment toward idea of A gfbf T =?
S sentiment toward T
For rule 1, why ?ideaOf A gfbf T?? Because the
purview of this work is making inferences about
attitudes, not about events themselves. Conceptu-
ally, ideaOf coerces an event into an idea, raising
it into the realm of private-state spaces. Reasoning
about the ideas of events avoids the classification
of whether the events are realis (i.e., whether they
did/will happen).
Rule 9 infers sentiment toward the agent in a
gfbf event.
rule9:
S sentiment toward A gfbf T &
A is a thing &
(Assume S positive believesTrue ...
substantial) A gfbf T =?
S sentiment toward A
By default, the system infers the event is in-
tentional and that the agent is positive toward the
event; if there is evidence against either, the infer-
ence should be blocked.
rule6:
A gfbf T, where A is animate =?
A intended A gfbf T
rule7:
S intended S gfbf T =?
S positive sentiment toward
ideaOf S gfbf T
So far, the preconditions have included only one
sentiment. Rule 3 applies when there are nested
sentiments, i.e., sentiments toward sentiments.
rule3.1:
S1 sentiment toward
S2 sentiment toward Z =?
S1 agrees/disagrees with S2 that
isGood/isBad Z &
S1 sentiment toward Z
rule3.2:
S1 sentiment toward
S2 pos/neg believesTrue substantial Z
=?
S1 agrees/disagrees with S2 that
isTrue/isFalse Z &
S1 pos/neg believesTrue substantial Z
rule3.3:
S1 sentiment toward
S2 pos/neg believesShould Z =?
S1 agrees/disagrees with S2 that
should/shouldNot Z &
S1 pos/neg believesShould Z
Among the subcases of rule 3, one shared con-
clusion is S1 agrees/disagrees with S2 *, which de-
pends on the sentiment from S1 toward S2. The
reason there are subcases is because the attitude
types of S2 are various, which determine the in-
ferred attitude type of S1.
By rule 3, given the sentiment between S1 and
S2, we can infer whether S1 and S2 agree. Simi-
larly, we can infer in the opposite direction, as rule
4 shows.
rule4:
S1 agrees/disagrees with S2 that
*
=?
S1 sentiment toward S2
Two other rules are given in (Wiebe and Deng,
2014).
5 Inferences for An Example from
MPQA Corpus
This section returns to the example from the
MPQA corpus in Section 3.1, illustrating some in-
teresting inference chains and conclusions.
Recall that the input for Ex(1) in Section 3.1 is:
8 writer positive believesTrue
4 He revive flagging arms industry
6 writer positive believesTrue
1 He trigger wars
157
The first inference is from connotation to sen-
timent since the word war is in the connotation
lexicon.
rule10:
(Assume Writer positive ...
believesTrue) A gfbf T &
T?s anchor is in connotation lexicon =?
Writer sentiment toward T
Assumptions:
6 writer positive believesTrue
1 He trigger wars
rule10 =? Infer:
17 writer negative sentiment
2 wars
From the writer?s negative sentiment toward
wars, the system infers a negative sentiment to-
ward trigger wars, since triggering wars is good-
For them:
rule8:
S positive believesTrue A gfbf T &
S sentiment toward T =?
S sentiment toward A gfbf T
Preconditions:
6 writer positive believesTrue
1 He trigger wars
17 writer negative sentiment
2 wars
rule8 =? Infer:
28 writer negative sentiment
1 He trigger wars
On the other hand, since the agent, He, is ani-
mate and there is no evidence to the contrary, the
system infers that the triggering event is inten-
tional, and that He is positive toward the idea of
his performing the event:
rule6 =? Infer:
38 writer negative sentiment
20 He positive intends
1 He trigger wars
rule7 =? Infer:
41 writer negative sentiment
25 He positive sentiment
26 ideaOf
1 He trigger wars
Continuing with inference, since the writer has
a negative sentiment toward the agent?s positive
sentiment, the system infers that the writer dis-
agrees with him (rule 3) and thus that the writer
is negative toward him (rule 4):
rule3.1:
S1 sentiment toward
S2 sentiment toward Z =?
S1 agrees/disagrees with S2 that
isGood/isBad Z &
S1 sentiment toward Z
Preconditions:
41 writer negative sentiment
25 He positive sentiment
26 ideaOf
1 He trigger wars
rule3.1 =? Infer:
50 writer disagrees with He that
49 isGood
26 ideaOf
1 He trigger wars
30 writer negative sentiment
26 ideaOf
1 He trigger wars
Then rule 4 works on node 50 and infers:
rule4 =? Infer:
55 writer negative sentiment
3 He
In addition to the sentiment related to the wars,
we have also drawn several conclusions of senti-
ment toward the arms industry. For example, one
of the output nodes related to the arms industry is:
32 writer positive believesTrue
31 He positive sentiment
5 flagging arms industry
The MPQA annotators marked the writer?s neg-
ative sentiment, choosing the long spans therefore
. . . industry and therefore planning . . . here and
there as attitude and expressive subjective element
spans, respectively. They were not able to pinpoint
any clear sentiment phrases. A machine learning
system trained on such examples would have diffi-
culty recognizing the sentiments. The system, re-
lying on the negative connotation of war and the
gfbf information in the sentence, is ultimately able
to infer several sentiments, including the writer?s
negative sentiment toward the trigger event.
6 Conclusions
While previous sentiment analysis research has
concentrated on the interpretation of explicitly
stated opinions and attitudes, this work addresses
opinion implicature (i.e., opinion-oriented default
inference) in real-world text. This paper described
a rule-based framework for representing and
analyzing opinion implicatures which we hope
will contribute to deeper automatic interpretation
of subjective language. In the course of under-
standing implicatures, the system recognizes
implicit sentiments (and beliefs) toward various
events and entities in the sentence, often of mixed
polarities; thus, it produces a richer interpretation
than is typical in opinion analysis.
Acknowledgements. This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their feedback.
158
References
Ann Banfield. 1982. Unspeakable Sentences. Rout-
ledge and Kegan Paul, Boston.
G. Bruder and J. Wiebe. 1995. Recognizing subjec-
tivity and identifying subjective characters in third-
person fictional narrative. In Judy Duchan, Gail
Bruder, and Lynne Hewitt, editors, Deixis in Nar-
rative: A Cognitive Science Perspective. Lawrence
Erlbaum Associates.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Meeting
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer at-
titude annotation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 120?125,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Ryan Doran, Gregory Ward, Meredith Larson, Yaron
McNabb, and Rachel E. Baker. 2012. A novel
experimental paradigm for distinguishing between
?what is said? and ?what is implicated?. Language,
88(1):124?154.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1092?1103, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash
of sentiment beneath the surface meaning. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1774?1784, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Herbert Paul Grice. 1967. Logic and conversation.
The William James lectures.
H Paul Grice. 1989. Studies in the Way of Words. Har-
vard University Press.
Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt,
and Paul Martin. 1993. Interpretation as abduction.
Artificial Intelligence, 63(1-2):69?142, October.
Jo?ao Martins and Stuart C. Shapiro. 1983. Reasoning
in multiple belief spaces. In IJCAI.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
William J. Rapaport. 1986. Logical foundations for
belief representation. Cognitive Science, 10(4):371?
422.
Brian M. Slator and Yorick Wilks. 1987. Towards se-
mantic structures from dictionary entries. Technical
Report MCCS-87-96, Computing Research Labora-
tory, NMSU.
Dieter Stein and Susan Wright, editors. 1995. Sub-
jectivity and Subjectivisation. Cambridge Univer-
sity Press, Cambridge.
Janyce Wiebe and Lingjia Deng. 2014. An account of
opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1065?1072, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Janyce Wiebe. 1994. Tracking point of view in narra-
tive. Computational Linguistics, 20(2):233?287.
159
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54?58,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List
Danielle Mowery
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
dlm31@pitt.edu
wiebe@cs.pitt.edu
Mindy Ross
University of California
San Diego
La Jolla, CA
mkross@ucsd.edu
Sumithra Velupillai
Stockholm University
Stockholm, SE
sumithra@dsv.su.se
Stephane Meystre
Wendy W Chapman
University of Utah
Salt Lake City, UT
stephane.meystre,
wendy.chapman@utah.edu
Abstract
An up-to-date problem list is useful for
assessing a patient?s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.
1 Introduction
In the late 1960?s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider?s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems
from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).
In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as ?maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care? (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).
In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or
54
concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.
The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.
2 Methods
In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.
2.1 Annotation Study
For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a ?CUI-less? la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned ?Healthy - no
problems? to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.
We assessed how reliably annotators agreed
with each other?s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:
true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)
false positive (FP) = the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations
55
false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations
Recall =
TP
(TP + FN)
(1)
Precision =
TP
(TP + FP )
(2)
F1-score =
2
(Recall ? Precision)
(Recall + Precision)
(3)
We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:
Spurious problems: procedures
solution: exclude non-problems via guidelines
Problem specificity: CUI specificity differences
solution: select most general CUIs
Conflicting status: negated vs. resolved
solution: select second reviewer?s status
CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful
We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.
2.2 Coverage Study
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a
FN as a patient problem CUI not occurring in the
vocabulary.
3 Results
We report the results of our annotation study on
the full set and vocabulary coverage study on the
training set.
3.1 Annotation Study
The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).
Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8
Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)
We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.
Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)
Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)
3.2 Coverage Study
In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22?10 DS,
10?4 ECHO, 6?2 RAD, and 4?1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG
56
by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as ?Healthy - no problems?. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.
Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)
Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE
4 Discussion
In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.
4.1 Annotation Study
Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding of patient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.
Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., ?cough improved then? and modal-
ity ?rule out pneumonia?, which are harder to
track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.
4.2 Coverage Study
We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).
5 Limitations
Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.
6 Conclusion
Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.
57
Acknowledgments
This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.
References
Center for Medicare and Medicaid Services. 2013.
EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.
Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296?298.
Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).
Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602?12.
Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032?1041.
National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.
Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.
Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687?691.
Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.
2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ?12, pages 130?139. Association
for Computational Linguistics.
Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.
58
