Proceedings of the ACL-HLT 2011 Student Session, pages 24?29,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Extracting and Classifying Urdu Multiword Expressions
Annette Hautli
Department of Linguistics
University of Konstanz, Germany
annette.hautli@uni-konstanz.de
Sebastian Sulger
Department of Linguistics
University of Konstanz, Germany
sebastian.sulger@uni-konstanz.de
Abstract
This paper describes a method for automati-
cally extracting and classifying multiword ex-
pressions (MWEs) for Urdu on the basis of a
relatively small unannotated corpus (around
8.12 million tokens). The MWEs are extracted
by an unsupervised method and classified into
two distinct classes, namely locations and per-
son names. The classification is based on sim-
ple heuristics that take the co-occurrence of
MWEs with distinct postpositions into account.
The resulting classes are evaluated against a
hand-annotated gold standard and achieve an
f-score of 0.5 and 0.746 for locations and
persons, respectively. A target application is
the Urdu ParGram grammar, where MWEs are
needed to generate a more precise syntactic
and semantic analysis.
1 Introduction
Multiword expressions (MWEs) are expressions
which can be semantically and syntactically idiosyn-
cratic in nature; acting as a single unit, their mean-
ing is not always predictable from their components.
Their identification is therefore an important task for
any Natural Language Processing (NLP) application
that goes beyond the analysis of pure surface struc-
ture, in particular for languages with few other NLP
tools available.
There is a vast amount of literature on extract-
ing and classifying MWEs automatically; many ap-
proaches rely on already available resources that aid
during the acquisition process. In the case of the
Indo-Aryan language Urdu, a lack of linguistic re-
sources such as annotated corpora or lexical knowl-
edge bases impedes the task of detecting and classi-
fying MWEs. Nevertheless, statistical measures and
language-specific syntactic information can be em-
ployed to extract and classify MWEs.
Therefore, the method described in this paper can
partly overcome the bottleneck of resource sparsity,
despite the relatively small size of the available cor-
pus and the simplistic approach taken. With the help
of heuristics as to the occurrence of Urdu MWEs with
characteristic postpositions and other cues, it is pos-
sible to cluster the MWEs into two groups: locations
and person names. It is also possible to detect junk
MWEs. The classification is then evaluated against a
hand-annotated gold standard of Urdu MWEs.
An NLP tool where the MWEs can be employed is
the Urdu ParGram grammar (Butt and King, 2007;
Bo?gel et al, 2007; Bo?gel et al, 2009), which is
based on the Lexical-Functional Grammar (LFG)
formalism (Dalrymple, 2001). For this task, differ-
ent types of MWEs need to be distinguished as they
are treated differently in the syntactic analysis.
The paper is structured as follows: Section 2 pro-
vides a brief review of related work, in particular
on MWE extraction in Indo-Aryan languages. Sec-
tion 3 describes our methodology, with the evalua-
tion following in Section 4. Section 5 presents the
Urdu ParGram Grammar and its treatment of MWEs,
followed by the discussion and the summary of the
paper in Section 6.
2 Related Work
MWE extraction and classification has been the focus
of a large amount of research. However, much work
24
has been conducted for well-resourced languages
such as English, benefiting from large enough cor-
pora (Attia et al, 2010), parallel data (Zarrie? and
Kuhn, 2009) and NLP tools such as taggers or depen-
dency parsers (Martens and Vandeghinste (2010),
among others) and lexical resources (Pearce, 2001).
Related work on Indo-Aryan languages has
mostly focused on the extraction of complex pred-
icates, with the focus on Hindi (Mukerjee et al,
2006; Chakrabarti et al, 2008; Sinha, 2009) and
Bengali (Das et al, 2010; Chakraborty and Bandy-
opadhyay, 2010). While complex predicates also
make up a large part of the verbal inventory in Urdu
(Butt, 1993), for the scope of this paper, we restrict
ourselves to classifying MWEs as locations or person
names and filter out junk bigrams.
Our approach deviates in several aspects to the re-
lated work in Indo-Aryan: First, we do not concen-
trate on specific POS constructions or dependency
relations, but use an unannotated middle-sized cor-
pus. For classification, we use simple heuristics by
taking the postpositions of the MWEs into account.
These can provide hints as to the nature of the MWE.
3 Methodology
3.1 Extraction and Identification of MWE
Candidates
The bigram extraction was carried out on a corpus of
around 8.12 million tokens of Urdu newspaper text,
collected by the Center for Research in Urdu Lan-
guage Processing (CRULP) (Hussain, 2008). We did
not perform any pre-processing such as POS tagging
or stop word removal.
Due to the relatively small size of our corpus, the
frequency cut-off for bigrams was set to 5, i.e. all
bigrams that occurred five times or more in the cor-
pus were considered. This rendered a list of 172,847
bigrams which were then ranked with the X2 asso-
ciation measure, using the UCS toolkit.1
The reasons for employing the X2 association
measure are twofold. First, papers using compara-
tively sized corpora reported encouraging results for
similar experiments (Ramisch et al, 2008; Kizito et
al., 2009). Second, initial manual comparison be-
tween MWE lists ranked according to all measures
1Available at http://www.collocations.de. See
Evert (2004) for documentation.
implemented in the UCS toolkit revealed the most
convincing results for the X2 test.
For the time being, we focus on bigram MWE
extraction. While the UCS toolkit readily supports
work on Unicode-based languages such as Urdu,
it does not support trigram extraction; other freely
available tools such as TEXT-NSP2 do come with
trigram support, but cannot handle Unicode script.
As a consequence, we currently implement our own
scripts to overcome these limitations.
3.2 Syntactic Cues
The clustering approach taken in this paper is based
on Urdu-specific syntactic information that can be
gathered straightforwardly from the corpus. Urdu
has a number of postpositions that can be used to
identify the nature of an MWE. Typographical cues
such as initial capital letters do not exist in the Urdu
script.
Locative postpositions The postposition QK (par)
either expresses location on something which has a
surface or that an object is next to something.3 In
addition, it expresses movement to a destination.
(1) ?


G


A? QK I. 
K. @ ?

K ?K
XA
	
K
nAdiyah t3ul AbEb par gAyI
Nadya Tel Aviv to go.Perf.Fem.Sg
?Nadya went to Tel Aviv.?
?
? (mEN) expresses location in or at a point in
space or time, whereas ?K (tak) denotes that some-
thing extends to a specific point in space. ?


?? (sE)
shows movement away from a certain point in space.
These postpositions mostly occur with locations
and are thus syntactic indicators for this type of
MWE. However, in special cases, they can also occur
with other nouns, in which case we predict wrong
results during classification.
Person-indicating syntactic cues To classify an
MWE as a person, we consider syntactic cues that
usually occur after such MWEs. The ergative marker
?


	
G (nE) describes an agentive subject in transitive
2Available at http://search.cpan.org/dist/
Text-NSP. See Banerjee and Pedersen (2003) for
documentation.
3The employed transliteration scheme is explained in Malik
et al (2010).
25
Locative Instr. Ergative Possessive Acc./Dat.
QK (par) ?
? (mEN) ?

K (tak) ?


?? (sE) ?


	
G (nE) A? (kA) ?


? (kE) ?


? (kI) ?? (kO)
LOC
? ? ? ?
? ? ? ? ?
PERS ? ? ?
? ? ? ? ? ?
JUNK ? ? ? ? ? ? ? ? ?
Table 1: Heuristics for clustering Urdu MWEs by different postpositions
sentences; therefore, it forms part of our heuristic
for finding person MWEs.
(2) @PA? ?? 	?
?AK
 ?

	
G ?K
XA
	
K
nAdiyah nE yAsIn kO mArA
Nadya Erg Yasin Acc hit.Perf.Masc.Sg
?Nadya hit Yasin.?
The same holds for the possessive markers
A? (kA), ?


? (kE) and ?


? (kI).
The accusative and dative case marker ?? (kO) is
also a possible indicator that the preceding MWE is
a person.
These cues can also appear with common nouns,
but the combination of MWE and syntactic cue hints
to a person MWE. However, consider cases such as
New Delhi said that the taxes will rise., where New
Delhi is treated as an agent with nE attached to it,
providing a wrong clue as to the nature of the MWE.
3.3 Classifying Urdu MWEs
The classification of the extracted bigrams is solely
based on syntactic information as described in the
previous section. For every bigram, the postpo-
sitions that it occurs with are extracted from the
corpus, together with the frequency of the co-
occurrence.
Table 1 shows which postpositions are expected
to occur with which type of MWE. The first stipula-
tion is that only bigrams that occur with one of the
locative postpositions plus the ablative/instrumental
marker ?


?? (sE) one or more times are considered
to be locative MWEs (LOC). In contrast, bigrams
are judged as persons (PERS) when they co-occur
with all postpositions apart from the locative post-
positions one or more times. If a bigram occurs with
none of the postpositions, it is judged as being junk
(JUNK). As a consequence this means that theoreti-
cally valid MWEs such as complex predicates, which
never occur with a postposition, are misclassified as
being JUNK.
Without any further processing, the resulting clus-
ters are then evaluated against a hand-annotated gold
standard, as described in the following section.
4 Evaluation
4.1 Gold Standard
Our gold standard comprises the 1300 highest
ranked Urdu multiword candidates extracted from
the CRULP corpus, using the X2 association mea-
sure. The bigrams are then hand-annotated by a na-
tive speaker of Urdu and clustered into the following
classes: locations, person names, companies, mis-
cellaneous MWEs and junk. For the scope of this
paper, we restrict ourselves to classifying MWEs as
either locations or person names,. This also lies in
the nature of the corpus: companies can usually be
detected by endings such as ?Corp.? or ?Ltd.?, as is
the case in English. However, these markers are of-
ten left out and are not present in the corpus at hand.
Therefore, they cannot be used for our clustering.
The class of miscellaneous MWEs contains complex
predicates that we do not attempt to deal with here.
In total, the gold standard comprises 30 compa-
nies, 95 locations, 411 person names, 512 miscella-
neous MWEs (mostly complex predicates) and 252
junk bigrams. We have not analyzed the gold stan-
dard any further, and restricting it to n< 1300 might
improve the evaluation results.
4.2 Results
The bigrams are classified according to the heuris-
tics outlined in Section 3.3. Evaluating against the
hand-annotated gold standard yields the results in
Table 2.
While the results are encouraging for persons with
an f-score of 0.746, there is still room for improve-
ment for locative MWEs. Part of the problem for per-
26
Precision Recall F-Score #total #found
LOC 0.453 0.558 0.5 95 43
PERS 0.727 0.765 0.746 411 298
JUNK 0.472 0.317 0.379 252 119
Table 2: Results for MWE clustering
son names is that Urdu names are generally longer
than two words, and as we have not considered tri-
grams yet, it is impossible to find a postposition after
an incomplete though generally valid name. Loca-
tions tend to have the same problem, however the
reasons for missing out on a large part of the loca-
tive MWEs are not quite clear and are currently being
investigated.
Junk bigrams can be detected with an f-score of
0.379. Due to the heterogeneous nature of the mis-
cellaneous MWEs (e.g., complex predicates), many
of them are judged as being junk because they never
occur with a postposition. If one could detect com-
plex predicate and, possibly, other subgroups from
the miscellaneous class, then classifying the junk
MWEs would become easier.
5 Integration into the Urdu ParGram
Grammar
The extracted MWEs are integrated into the Urdu
ParGram grammar (Butt and King, 2007; Bo?gel et
al., 2007; Bo?gel et al, 2009), a computational gram-
mar for Urdu running with XLE (Crouch et al, 2010)
and based on the syntax formalism of LFG (Dal-
rymple, 2001). XLE grammars are generally hand-
written and not acquired a machine learning pro-
cess or the like. This makes grammar development a
very conscious task and it is imperative to deal with
MWEs in order to achieve a linguistically valid and
deep syntactic analysis that can be used for an addi-
tional semantic analysis.
MWEs that are correctly classified according to the
gold standard are automatically integrated into the
multiword lexicon of the grammar, accompanied by
information about their nature (see example (3)).
In general, grammar input is first tokenized by a
standard tokenizer that separates the input string into
single tokens and replaces the white spaces with a
special token boundary symbol. Each token is then
passed through a cascade of finite-state morpholog-
ical analyzers (Beesley and Karttunen, 2003). For
MWEs, the matter is different as they are treated as
a single unit to preserve the semantic information
they carry. Apart from the meaning preservation, in-
tegrating MWEs into the grammar reduces parsing
ambiguity and parsing time, while the perspicuity of
the syntactic analyses is increased (Butt et al, 1999).
In order to prevent the MWEs from being inde-
pendently analyzed by the finite-state morphology,
a look-up is performed in a transducer which only
contains MWEs with their morphological informa-
tion. So instead of analyzing t3ul and AbEb sep-
arately, for example, they are analyzed as a sin-
gle item carrying the morphological information
+Noun+Location.4
(3) t3ul` AbEb: /t3ul` AbEb/ +Noun
+Location
The resulting stem and tag sequence is then
passed on to the grammar. See (4) for an example
and Figures 1 and 2 for the corresponding c- and
f-structure; the +Location tag in (3) is used to
produce the location analysis in the f-structure. Note
also that t3ul AbEb is displayed as a multiword
under the N node in the c-structure.
(4) ?


G


A? QK I. 
K. @ ?

K ?K
XA
	
K
nAdiyah t3ul AbEb par gAyI
Nadya Tel Aviv to go.Perf.Fem.Sg
?Nadya went to Tel Aviv.?
CS 1: ROOT
Sadj
S
KP
NP
N
nAdiyah
KP
NP
N
t3ul AbEb
K
par
VCmain
V
gAyI
Figure 1: C-structure for (4)
4The ` symbol is an escape character, yielding a literal white
space.
27
"nAdiyah t3ul AbEb par gAyI"
'gA<[1:nAdiyah]>'PRED
'nAdiyah'PRED
namePROPER-TYPEPROPERNSEM
properNSYN
NTYPE
CASE nom, GEND fem, NUM sg, PERS 31
SUBJ
't3ul AbEb'PRED
locationPROPER-TYPEPROPERNSEM
properNSYN
NTYPE
ADJUNCT-TYPE loc, CASE loc, NUM sg, PERS 321
ADJUNCT
ASPECT perf, MOOD indicativeTNS-ASP
CLAUSE-TYPE decl, PASSIVE -, VTYPE main42
Figure 2: F-structure for (4)
6 Discussion, Summary and Future Work
Despite the simplistic approach for extracting and
clustering Urdu MWEs taken in this paper, the re-
sults are encouraging with f-scores of 0.5 and 0.746
for locations and person names, respectively. We
are well aware that this paper does not present a
complete approach to classifying Urdu multiwords,
but considering the targeted tool, the Urdu ParGram
grammar, this methodology provides us with a set of
MWEs that can be implemented to improve the syn-
tactic analyses.
The methodology provided here can also guide
MWE work in other languages facing the same re-
source sparsity as Urdu, given that distinctive syn-
tactic cues are available in the language.
For Urdu, the syntactic cues are good indica-
tions of the nature of the MWE; future work on
this subtopic might prove beneficial to the clustering
regarding companies, complex predicates and junk
MWEs. Another area for future work is to extend
the extraction and classification to trigrams to im-
prove the results especially for locations and person
names. We also consider harvesting data sources
from the web such as lists of cities, common names
and companies in Pakistan and India. Such lists are
not numerous for Urdu, but they may nevertheless
help to generate a larger MWE lexicon.
Acknowledgments
We would like to thank Samreen Khan for annotat-
ing the gold standard, as well as the anonymous re-
viewers for their valuable comments. This research
was in part supported by the Deutsche Forschungs-
gemeinschaft (DFG).
References
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic
Extraction of Arabic Multiword Expressions. In Pro-
ceedings of the Workshop on Multiword Expressions:
from Theory to Applications (MWE 2010).
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation and Use of the Ngram Statistics
Package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics.
Kenneth Beesley and Lauri Karttunen. 2003. Finite State
Morphology. CSLI Publications, Stanford, CA.
Tina Bo?gel, Miriam Butt, Annette Hautli, and Sebastian
Sulger. 2007. Developing a Finite-State Morpholog-
ical Analyzer for Urdu and Hindi: Some Issues. In
Proceedings of FSMNLP07, Potsdam, Germany.
Tina Bo?gel, Miriam Butt, Annette Hautli, and Sebastian
Sulger. 2009. Urdu and the Modular Architecture of
ParGram. In Proceedings of the Conference on Lan-
guage and Technology 2009 (CLT09).
Miriam Butt and Tracy Holloway King. 2007. Urdu in
a Parallel Grammar Development Environment. Lan-
guage Resources and Evaluation, 41(2):191?207.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt. 1993. The Structure of Complex Predicates
in Urdu. Ph.D. thesis, Stanford University.
Debasri Chakrabarti, Vaijayanthi M. Sarma, and Pushpak
Bhattacharyya. 2008. Hindi Compound Verbs and
their Automatic Extraction. In Proceedings of COL-
ING 2008, pages 27?30.
Tanmoy Chakraborty and Sivaji Bandyopadhyay. 2010.
Identification of Reduplication in Bengali Corpus and
their Semantic Analysis: A Rule-Based Approach.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Theory to Applications (MWE 2010),
pages 72?75.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John T. Maxwell III, and Paula
Newman, 2010. XLE Documentation. Palo Alto Re-
search Center.
Mary Dalrymple. 2001. Lexical Functional Grammar,
volume 34 of Syntax and Semantics. Academic Press.
Dipankar Das, Santanu Pal, Tapabrata Mondal, Tanmoy
Chakraborty, and Sivaji Bandyopadhyay. 2010. Au-
tomatic Extraction of Complex Predicates in Bengali.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Theory to Applications (MWE 2010),
pages 37?45.
28
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
IMS, University of Stuttgart.
Sarmad Hussain. 2008. Resources for Urdu Language
Processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP?08.
John Kizito, Ismail Fahmi, Erik Tjong Kim Sang, Gosse
Bouma, and John Nerbonne. 2009. Computational
Linguistics and the History of Science. In Liborio
Dibattista, editor, Storia della Scienza e Linguistica
Computazionale. FrancoAngeli.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina Bo?gel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliter-
ating Urdu for a Broad-Coverage Urdu/Hindi LFG
Grammar. In Proceedings of the Seventh Conference
on International Language Resources and Evaluation
(LREC?10).
Scott Martens and Vincent Vandeghinste. 2010. An Effi-
cient, Generic Approach to Extracting Multi-Word Ex-
pressions from Dependency Trees. In Proceedings of
the Workshop on Multiword Expressions: from Theory
to Applications (MWE 2010), pages 84?87.
Amitabha Mukerjee, Ankit Soni, and Achla M. Raina.
2006. Detecting Complex Predicates in Hindi using
POS Projection across Parallel Corpora. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties (MWE
?06), pages 28?35.
David Pearce. 2001. Synonymy in Collocation Extrac-
tion. In WordNet and Other Lexical Resources: Appli-
cations, Extensions & Customizations, pages 41?46.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An Evaluation of Methods for
the Extraction of Multiword Expressions. In Proceed-
ings of the Workshop on Multiword Expressions: To-
wards a Shared Task for Multiword Expressions (MWE
2008).
R. Mahesh K. Sinha. 2009. Mining Complex Predicates
in Hindi Using a Parallel Hindi-English Corpus. In
Proceedings of the 2009 Workshop on Multiword Ex-
pressions, ACL-IJCNLP 2009, pages 40?46.
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Transla-
tional Correspondences for Pattern-Independent MWE
Identification. In Proceedings of the 2009 Workshop
on Multiword Expressions, ACL-IJCNLP 2009, pages
23?30.
29
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 550?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParGramBank: The ParGram Parallel Treebank
Sebastian Sulger and Miriam Butt
University of Konstanz, Germany
{sebastian.sulger|miriam.butt}@uni-konstanz.de
Tracy Holloway King
eBay Inc., USA
tracyking@ebay.com
Paul Meurer
Uni Research AS, Norway
paul.meurer@uni.no
Tibor Laczko? and Gyo?rgy Ra?kosi
University of Debrecen, Hungary
{laczko.tibor|rakosi.gyorgy}@arts.unideb.hu
Cheikh Bamba Dione and Helge Dyvik and Victoria Rose?n and Koenraad De Smedt
University of Bergen, Norway
dione.bamba@lle.uib.no, {dyvik|victoria|desmedt}@uib.no
Agnieszka Patejuk
Polish Academy of Sciences
aep@ipipan.waw.pl
O?zlem C?etinog?lu
University of Stuttgart, Germany
ozlem@ims.uni-stuttgart.de
I Wayan Arka* and Meladel Mistica+
*Australian National University and Udayana University, Indonesia
+Australian National University
wayan.arka@anu.edu.au, meladel.mistica@gmail.com
Abstract
This paper discusses the construction of
a parallel treebank currently involving ten
languages from six language families. The
treebank is based on deep LFG (Lexical-
Functional Grammar) grammars that were
developed within the framework of the
ParGram (Parallel Grammar) effort. The
grammars produce output that is maxi-
mally parallelized across languages and
language families. This output forms the
basis of a parallel treebank covering a
diverse set of phenomena. The treebank
is publicly available via the INESS tree-
banking environment, which also allows
for the alignment of language pairs. We
thus present a unique, multilayered paral-
lel treebank that represents more and dif-
ferent types of languages than are avail-
able in other treebanks, that represents
deep linguistic knowledge and that allows
for the alignment of sentences at sev-
eral levels: dependency structures, con-
stituency structures and POS information.
1 Introduction
This paper discusses the construction of a parallel
treebank currently involving ten languages that
represent several different language families, in-
cluding non-Indo-European. The treebank is based
on the output of individual deep LFG (Lexical-
Functional Grammar) grammars that were deve-
loped independently at different sites but within
the overall framework of ParGram (the Parallel
Grammar project) (Butt et al, 1999a; Butt et al,
2002). The aim of ParGram is to produce deep,
wide coverage grammars for a variety of lan-
guages. Deep grammars provide detailed syntactic
analysis, encode grammatical functions as well as
550
other grammatical features such as tense or aspect,
and are linguistically well-motivated. The Par-
Gram grammars are couched within the linguis-
tic framework of LFG (Bresnan, 2001; Dalrymple,
2001) and are constructed with a set of grammati-
cal features that have been commonly agreed upon
within the ParGram group. ParGram grammars are
implemented using XLE, an efficient, industrial-
strength grammar development platform that in-
cludes a parser, a generator and a transfer sys-
tem (Crouch et al, 2012). XLE has been devel-
oped in close collaboration with the ParGram
project. Over the years, ParGram has continu-
ously grown and includes grammars for Ara-
bic, Chinese, English, French, German, Georgian,
Hungarian, Indonesian, Irish, Japanese, Mala-
gasy, Murrinh-Patha, Norwegian, Polish, Spanish,
Tigrinya, Turkish, Urdu, Welsh and Wolof.
ParGram grammars produce output that has
been parallelized maximally across languages ac-
cording to a set of commonly agreed upon uni-
versal proto-type analyses and feature values. This
output forms the basis of the ParGramBank paral-
lel treebank discussed here. ParGramBank is con-
structed using an innovative alignment methodol-
ogy developed in the XPAR project (Dyvik et al,
2009) in which grammar parallelism is presup-
posed to propagate alignment across different pro-
jections (section 6). This methodology has been
implemented with a drag-and-drop interface as
part of the LFG Parsebanker in the INESS infras-
tructure (Rose?n et al, 2012; Rose?n et al, 2009).
ParGramBank has been constructed in INESS and
is accessible in this infrastructure, which also of-
fers powerful search and visualization.
In recent years, parallel treebanking1 has gained
in importance within NLP. An obvious applica-
tion for parallel treebanking is machine transla-
tion, where treebank size is a deciding factor for
whether a particular treebank can support a par-
ticular kind of research project. When conduct-
ing in-depth linguistic studies of typological fea-
tures, other factors such as the number of in-
cluded languages, the number of covered phe-
nomena, and the depth of linguistic analysis be-
come more important. The treebanking effort re-
ported on in this paper supports work of the lat-
ter focus, including efforts at multilingual depen-
dency parsing (Naseem et al, 2012). We have
1Throughout this paper ?treebank? refers to both phrase-
structure resources and their natural extensions to depen-
dency and other deep annotation banks.
created a parallel treebank whose prototype in-
cludes ten typologically diverse languages and re-
flects a diverse set of phenomena. We thus present
a unique, multilayered parallel treebank that rep-
resents more languages than are currently avail-
able in other treebanks, and different types of lan-
guages as well. It contains deep linguistic knowl-
edge and allows for the parallel and simultane-
ous alignment of sentences at several levels. LFG?s
f(unctional)-structure encodes dependency struc-
tures as well as information that is equivalent to
Quasi-Logical Forms (van Genabith and Crouch,
1996). LFG?s c(onstituent)-structure provides in-
formation about constituency, hierarchical rela-
tions and part-of-speech. Currently, ParGramBank
includes structures for the following languages
(with the ISO 639-3 code and language fam-
ily): English (eng, Indo-European), Georgian (kat,
Kartvelian), German (deu, Indo-European), Hun-
garian (hun, Uralic), Indonesian (ind, Austrone-
sian), Norwegian (Bokma?l) (nob, Indo-European),
Polish (pol, Indo-European), Turkish (tur, Altaic),
Urdu (urd, Indo-European) and Wolof (wol, Niger-
Congo). It is freely available for download under
the CC-BY 3.0 license via the INESS treebanking
environment and comes in two formats: a Prolog
format and an XML format.2
This paper is structured as follows. Section
2 discusses related work in parallel treebanking.
Section 3 presents ParGram and its approach to
parallel treebanking. Section 4 focuses on the tree-
bank design and its construction. Section 5 con-
tains examples from the treebank, focusing on ty-
pological aspects and challenges for parallelism.
Section 6 elaborates on the mechanisms for paral-
lel alignment of the treebank.
2 Related Work
There have been several efforts in parallel tree-
banking across theories and annotation schemes.
Kuhn and Jellinghaus (2006) take a mini-
mal approach towards multilingual parallel tree-
banking. They bootstrap phrasal alignments over
a sentence-aligned parallel corpus of English,
French, German and Spanish and report concrete
treebank annotation work on a sample of sen-
tences from the Europarl corpus. Their annotation
2http://iness.uib.no. The treebank is in the
public domain (CC-BY 3.0). The use of the INESS platform
itself is not subject to any licensing. To access the treebank,
click on ?Treebank selection? and choose the ParGram collec-
tion.
551
scheme is the ?leanest? possible scheme in that it
consists solely of a bracketing for a sentence in
a language (where only those units that play the
role of a semantic argument or modifier in a larger
unit are bracketed) and a correspondence relation
of the constituents across languages.
Klyueva and Marec?ek (2010) present a small
parallel treebank using data and tools from two
existing treebanks. They take a syntactically an-
notated gold standard text for one language and
run an automated annotation on the parallel text
for the other language. Manually annotated Rus-
sian data are taken from the SynTagRus treebank
(Nivre et al, 2008), while tools for parsing the cor-
responding text in Czech are taken from the Tec-
toMT framework (Popel and Z?abokrtsky?, 2010).
The SMULTRON project is concerned with con-
structing a parallel treebank of English, German
and Swedish. The sentences have been POS-tagged
and annotated with phrase structure trees. These
trees have been aligned on the sentence, phrase
and word level. Additionally, the German and
Swedish monolingual treebanks contain lemma in-
formation. The treebank is distributed in TIGER-
XML format (Volk et al, 2010).
Megyesi et al (2010) discuss a parallel English-
Swedish-Turkish treebank. The sentences in each
language are annotated morphologically and syn-
tactically with automatic tools, aligned on the
sentence and the word level and partially hand-
corrected.3
A further parallel treebanking effort is Par-
TUT, a parallel treebank (Sanguinetti and Bosco,
2011; Bosco et al, 2012) which provides depen-
dency structures for Italian, English and French
and which can be converted to a CCG (Combina-
tory Categorial Grammar) format.
Closest to our work is the ParDeepBank, which
is engaged in the creation of a highly paral-
lel treebank of English, Portuguese and Bulgar-
ian. ParDeepBank is couched within the linguistic
framework of HPSG (Head-Driven Phrase Struc-
ture Grammar) and uses parallel automatic HPSG
grammars, employing the same tools and imple-
mentation strategies across languages (Flickinger
et al, 2012). The parallel treebank is aligned on
the sentence, phrase and word level.
In sum, parallel treebanks have so far fo-
cused exclusively on Indo-European languages
3The paper mentions Hindi as the fourth language, but
this is not yet available: http://stp.lingfil.uu.
se/?bea/turkiska/home-en.html.
(with Turkish providing the one exception) and
generally do not extend beyond three or four
languages. In contrast, our ParGramBank tree-
bank currently includes ten typologically differ-
ent languages from six different language families
(Altaic, Austronesian, Indo-European, Kartvelian,
Niger-Congo, Uralic).
A further point of comparison with ParDeep-
Bank is that it relies on dynamic treebanks, which
means that structures are subject to change dur-
ing the further development of the resource gram-
mars. In ParDeepBank, additional machinery is
needed to ensure correct alignment on the phrase
and word level (Flickinger et al, 2012, p. 105).
ParGramBank contains finalized analyses, struc-
tures and features that were designed collabora-
tively over more than a decade, thus guaranteeing
a high degree of stable parallelism. However, with
the methodology developed within XPAR, align-
ments can easily be recomputed from f-structure
alignments in case of grammar or feature changes,
so that we also have the flexible capability of
allowing ParGramBank to include dynamic tree-
banks.
3 ParGram and its Feature Space
The ParGram grammars use the LFG formalism
which produces c(onstituent)-structures (trees)
and f(unctional)-structures as the syntactic anal-
ysis. LFG assumes a version of Chomsky?s Uni-
versal Grammar hypothesis, namely that all lan-
guages are structured by similar underlying prin-
ciples (Chomsky, 1988; Chomsky, 1995). Within
LFG, f-structures encode a language universal
level of syntactic analysis, allowing for crosslin-
guistic parallelism at this level of abstraction. In
contrast, c-structures encode language particular
differences in linear word order, surface morpho-
logical vs. syntactic structures, and constituency
(Dalrymple, 2001). Thus, while the Chomskyan
framework is derivational in nature, LFG departs
from this view by embracing a strictly representa-
tional approach to syntax.
ParGram tests the LFG formalism for its uni-
versality and coverage limitations to see how far
parallelism can be maintained across languages.
Where possible, analyses produced by the gram-
mars for similar constructions in each language are
parallel, with the computational advantage that the
grammars can be used in similar applications and
that machine translation can be simplified.
552
The ParGram project regulates the features and
values used in its grammars. Since its inception
in 1996, ParGram has included a ?feature com-
mittee?, which collaboratively determines norms
for the use and definition of a common multilin-
gual feature and analysis space. Adherence to fea-
ture committee decisions is supported technically
by a routine that checks the grammars for com-
patibility with a feature declaration (King et al,
2005); the feature space for each grammar is in-
cluded in ParGramBank. ParGram also conducts
regular meetings to discuss constructions, analy-
ses and features.
For example, Figure 1 shows the c-structure
of the Urdu sentence in (1) and the c-structure
of its English translation. Figure 2 shows the f-
structures for the same sentences. The left/upper
c- and f-structures show the parse from the En-
glish ParGram grammar, the right/lower ones from
Urdu ParGram grammar.4,5 The c-structures en-
code linear word order and constituency and thus
look very different; e.g., the English structure is
rather hierarchical while the Urdu structure is flat
(Urdu is a free word-order language with no evi-
dence for a VP; Butt (1995)). The f-structures, in
contrast, are parallel aside from grammar-specific
characteristics such as the absence of grammati-
cal gender marking in English and the absence of
articles in Urdu.6
(1) ? Aj J
K. Q?K
QK A 	JK @ ?

	
G
	
?A??
kisAn=nE apnA
farmer.M.Sg=Erg self.M.Sg
TrEkTar bEc-A
tractor.M.Sg sell-Perf.M.Sg
?Did the farmer sell his tractor??
With parallel analyses and parallel features, maxi-
mal parallelism across typologically different lan-
guages is maintained. As a result, during the con-
struction of the treebank, post-processing and con-
version efforts are kept to a minimum.
4The Urdu ParGram grammar makes use of a translitera-
tion scheme that abstracts away from the Arabic-based script;
the transliteration scheme is detailed in Malik et al (2010).
5In the c-structures, dotted lines indicate distinct func-
tional domains; e.g., in Figure 1, the NP the farmer and the
VP sell his tractor belong to different f-structures: the former
maps onto the SUBJ f-structure, while the latter maps onto the
topmost f-structure (Dyvik et al, 2009). Section 6 elaborates
on functional domains.
6The CASE feature also varies: since English does not
distinguish between accusative, dative, and other oblique
cases, the OBJ is marked with a more general obl CASE.
Figure 1: English and Urdu c-structures
We emphasize the fact that ParGramBank is
characterized by a maximally reliable, human-
controlled and linguistically deep parallelism
across aligned sentences. Generally, the result of
automatic sentence alignment procedures are par-
allel corpora where the corresponding sentences
normally have the same purported meaning as
intended by the translator, but they do not nec-
essarily match in terms of structural expression.
In building ParGramBank, conscious attention is
paid to maintaining semantic and constructional
parallelism as much as possible. This design fea-
ture renders our treebank reliable in cases when
the constructional parallelism is reduced even at f-
structure. For example, typological variation in the
presence or absence of finite passive constructions
represents a case of potential mismatch. Hungar-
ian, one of the treebank languages, has no produc-
tive finite passives. The most common strategy in
translation is to use an active construction with a
topicalized object, with no overt subject and with
3PL verb agreement:
(2) A fa?-t ki-va?g-t-a?k.
the tree-ACC out-cut-PAST-3PL
?The tree was cut down.?
In this case, a topicalized object in Hungarian has
to be aligned with a (topical) subject in English.
Given that both the sentence level and the phrase
level alignments are human-controlled in the tree-
bank (see sections 4 and 6), the greatest possible
parallelism is reliably captured even in such cases
of relative grammatical divergence.
553
Figure 2: Parallel English and Urdu f-structures
4 Treebank Design and Construction
For the initial seeding of the treebank, we focused
on 50 sentences which were constructed manu-
ally to cover a diverse range of phenomena (tran-
sitivity, voice alternations, interrogatives, embed-
ded clauses, copula constructions, control/raising
verbs, etc.). We followed Lehmann et al (1996)
and Bender et al (2011) in using coverage of
grammatical constructions as a key component for
grammar development. (3) lists the first 16 sen-
tences of the treebank. An expansion to 100 sen-
tences is scheduled for next year.
(3) a. Declaratives:
1. The driver starts the tractor.
2. The tractor is red.
b. Interrogatives:
3. What did the farmer see?
4. Did the farmer sell his tractor?
c. Imperatives:
5. Push the button.
6. Don?t push the button.
d. Transitivity:
7. The farmer gave his neighbor an old
tractor.
8. The farmer cut the tree down.
9. The farmer groaned.
e. Passives and traditional voice:
10. My neighbor was given an old tractor
by the farmer.
11. The tree was cut down yesterday.
12. The tree had been cut down.
13. The tractor starts with a shudder.
f. Unaccusative:
14. The tractor appeared.
g. Subcategorized declaratives:
15. The boy knows the tractor is red.
16. The child thinks he started the tractor.
The sentences were translated from English
into the other treebank languages. Currently, these
languages are: English, Georgian, German, Hun-
garian, Indonesian, Norwegian (Bokma?l), Polish,
Turkish, Urdu and Wolof. The translations were
done by ParGram grammar developers (i.e., expert
linguists and native speakers).
The sentences were automatically parsed with
ParGram grammars using XLE. Since the pars-
ing was performed sentence by sentence, our re-
sulting treebank is automatically aligned at the
sentence level. The resulting c- and f-structures
were banked in a database using the LFG Parse-
banker (Rose?n et al, 2009). The structures were
disambiguated either prior to banking using XLE
or during banking with the LFG Parsebanker and
its discriminant-based disambiguation technique.
The banked analyses can be exported and down-
loaded in a Prolog format using the LFG Parse-
banker interface. Within XLE, we automatically
convert the structures to a simple XML format and
make these available via ParGramBank as well.
The Prolog format is used with applications
which use XLE to manipulate the structures, e.g.
for further semantic processing (Crouch and King,
2006) or for sentence condensation (Crouch et al,
2004).
554
5 Challenges for Parallelism
We detail some challenges in maintaining paral-
lelism across typologically distinct languages.
5.1 Complex Predicates
Some languages in ParGramBank make extensive
use of complex predicates. For example, Urdu uses
a combination of predicates to express concepts
that in languages like English are expressed with
a single verb, e.g., ?memory do? = ?remember?,
?fear come? = ?fear?. In addition, verb+verb com-
binations are used to express permissive or as-
pectual relations. The strategy within ParGram is
to abstract away from the particular surface mor-
phosyntactic expression and aim at parallelism
at the level of f-structure. That is, monoclausal
predications are analyzed via a simple f-structure
whether they consist of periphrastically formed
complex predicates (Urdu, Figure 3), a simple
verb (English, Figure 4), or a morphologically de-
rived form (Turkish, Figure 5).
In Urdu and in Turkish, the top-level PRED
is complex, indicating a composed predicate. In
Urdu, this reflects the noun-verb complex predi-
cate sTArT kar ?start do?, in Turkish it reflects a
morphological causative. Despite this morphosyn-
tactic complexity, the overall dependency struc-
ture corresponds to that of the English simple verb.
(4) ?


?
f
A

KQ ? HPA

J ? ? ? Q

 ? K
Q

K P?

J K
 @P

X
DrAIvar TrEkTar=kO
driver.M.Sg.Nom tractor.M.Sg=Acc
sTArT kartA hE
start.M.Sg do.Impf.M.Sg be.Pres.3Sg
?The driver starts the tractor.?
(5) su?ru?cu? trakto?r-u? c?al?s?-t?r-?yor
driver.Nom tractor-Acc work-Caus-Prog.3Sg
?The driver starts the tractor.?
The f-structure analysis of complex predicates
is thus similar to that of languages which do not
use complex predicates, resulting in a strong syn-
tactic parallelism at this level, even across typo-
logically diverse languages.
5.2 Negation
Negation also has varying morphosyntactic sur-
face realizations. The languages in ParGramBank
differ with respect to their negation strategies.
Languages such as English and German use inde-
pendent negation: they negate using words such as
Figure 3: Complex predicate: Urdu analysis of (4)
Figure 4: Simple predicate: English analysis of (4)
adverbs (English not, German nicht) or verbs (En-
glish do-support). Other languages employ non-
independent, morphological negation techniques;
Turkish, for instance, uses an affix on the verb, as
in (6).
555
Figure 5: Causative: Turkish analysis of (5)
(6) du?g?me-ye bas-ma
button-Dat push-Neg.Imp
?Don?t push the button.?
Within ParGram we have not abstracted away
from this surface difference. The English not in
(6) functions as an adverbial adjunct that modifies
the main verb (see top part of Figure 6) and infor-
mation would be lost if this were not represented
at f-structure. However, the same cannot be said of
the negative affix in Turkish ? the morphological
affix is not an adverbial adjunct. We have there-
fore currently analyzed morphological negation as
adding a feature to the f-structure which marks the
clause as negative, see bottom half of Figure 6.
5.3 Copula Constructions
Another challenge to parallelism comes from co-
pula constructions. An approach advocating a uni-
form treatment of copulas crosslinguistically was
advocated in the early years of ParGram (Butt et
al., 1999b), but this analysis could not do justice to
the typological variation found with copulas. Par-
GramBank reflects the typological difference with
three different analyses, with each language mak-
ing a language-specific choice among the three
possibilities that have been identified (Dalrymple
et al, 2004; Nordlinger and Sadler, 2007; Attia,
2008; Sulger, 2011; Laczko?, 2012).
The possible analyses are demonstrated here
with respect to the sentence The tractor is red.
The English grammar (Figure 7) uses a raising ap-
proach that reflects the earliest treatments of cop-
ulas in LFG (Bresnan, 1982). The copula takes
a non-finite complement whose subject is raised
to the matrix clause as a non-thematic subject of
the copula. In contrast, in Urdu (Figure 8), the
Figure 6: Different f-structural analyses for nega-
tion (English vs. Turkish)
copula is a two-place predicate, assigning SUBJ
and PREDLINK functions. The PREDLINK function
is interpreted as predicating something about the
subject. Finally, in languages like Indonesian (Fig-
ure 9), there is no overt copula and the adjective is
the main predicational element of the clause.
Figure 7: English copula example
556
Figure 8: Urdu copula example
Figure 9: Indonesian copula example
5.4 Summary
This section discussed some challenges for main-
taining parallel analyses across typologically di-
verse languages. Another challenge we face is
when no corresponding construction exists in a
language, e.g. with impersonals as in the English
It is raining. In this case, we provide a translation
and an analysis of the structure of the correspond-
ing translation, but note that the phenomenon be-
ing exemplified does not actually exist in the lan-
guage. A further extension to the capabilities of
the treebank could be the addition of pointers from
the alternative structure used in the translation to
the parallel aligned set of sentences that corre-
spond to this alternative structure.
6 Linguistically Motivated Alignment
The treebank is automatically aligned on the sen-
tence level, the top level of alignment within Par-
GramBank. For phrase-level alignments, we use
the drag-and-drop alignment tool in the LFG Parse-
banker (Dyvik et al, 2009). The tool allows the
alignment of f-structures by dragging the index
of a subsidiary source f-structure onto the index
of the corresponding target f-structure. Two f-
structures correspond if they have translationally
matching predicates, and the arguments of each
predicate correspond to an argument or adjunct in
the other f-structure. The tool automatically com-
putes the alignment of c-structure nodes on the
basis of the manually aligned corresponding f-
structures.7
7Currently we have not measured inter-annotator agree-
ment (IAA) for the f-structure alignments. The f-structure
alignments were done by only one person per language pair.
We anticipate that multiple annotators will be needed for this
This method is possible because the c-structure
to f-structure correspondence (the ? relation) is
encoded in the ParGramBank structures, allow-
ing the LFG Parsebanker tool to compute which c-
structure nodes contributed to a given f-structure
via the inverse (??1) mapping. A set of nodes
mapping to the same f-structure is called a ?func-
tional domain?. Within a source and a target
functional domain, two nodes are automatically
aligned only if they dominate corresponding word
forms. In Figure 10 the nodes in each func-
tional domain in the trees are connected by whole
lines while dotted lines connect different func-
tional domains. Within a functional domain, thick
whole lines connect the nodes that share align-
ment; for simplicity the alignment is only indi-
cated for the top nodes. The automatically com-
puted c-structural alignments are shown by the
curved lines. The alignment information is stored
as an additional layer and can be used to ex-
plore alignments at the string (word), phrase (c-
)structure, and functional (f-)structure levels.
We have so far aligned the treebank pairs
English-Urdu, English-German, English-Polish
and Norwegian-Georgian. As Figure 10 illustrates
for (7) in an English-Urdu pairing, the English ob-
ject neighbor is aligned with the Urdu indirect ob-
ject (OBJ-GO) hamsAyA ?neighbor?, while the En-
glish indirect object (OBJ-TH) tractor is aligned
with the Urdu object TrEkTar ?tractor?. The c-
structure correspondences were computed auto-
matically from the f-structure alignments.
(7) AK
X Q?K
QK A 	K @QK ?? ?
G
A???f ?

	
?K @ ?

	
G
	
?A??
kisAn=nE apnE
farmer.M.Sg=Erg self.Obl
hamsAyE=kO purAnA
neighbor.M.Sg.Obl=Acc old.M.Sg
TrEkTar di-yA
tractor.M.Sg give-Perf.M.Sg
?The farmer gave his neighbor an old tractor.?
The INESS platform additionally allows for the
highlighting of connected nodes via a mouse-over
technique. It thus provides a powerful and flexible
tool for the semi-automatic alignment and subse-
task in the future, in which case we will measure IAA for this
step.
557
Figure 10: Phrase-aligned treebank example English-Urdu: The farmer gave his neighbor an old tractor.
quent inspection of parallel treebanks which con-
tain highly complex linguistic structures.8
7 Discussion and Future Work
We have discussed the construction of ParGram-
Bank, a parallel treebank for ten typologically
different languages. The analyses in ParGram-
Bank are the output of computational LFG Par-
Gram grammars. As a result of ParGram?s cen-
trally agreed upon feature sets and prototypical
analyses, the representations are not only deep
in nature, but maximally parallel. The representa-
tions offer information about dependency relations
as well as word order, constituency and part-of-
speech.
In future ParGramBank releases, we will pro-
vide more theory-neutral dependencies along with
the LFG representations. This will take the form of
triples (King et al, 2003). We also plan to provide
a POS-tagged and a named entity marked up ver-
sion of the sentences; these will be of use for more
general NLP applications and for systems which
use such markup as input to deeper processing.
8One reviewer inquires about possibilities of linking
(semi-)automatically between languages, for example using
lexical resources such as WordNets or Panlex. We agree that
this would be desirable, but unrealizable, since many of the
languages included in ParGramBank do not have a WordNet
resource and are not likely to achieve an adequate one soon.
Third, the treebank will be expanded to include
100 more sentences within the next year. We also
plan to include more languages as other ParGram
groups contribute structures to ParGramBank.
ParGramBank, including its multilingual sen-
tences and all annotations, is made freely avail-
able for research and commercial use under the
CC-BY 3.0 license via the INESS platform, which
supports alignment methodology developed in the
XPAR project and provides search and visualiza-
tion methods for parallel treebanks. We encourage
the computational linguistics community to con-
tribute further layers of annotation, including se-
mantic (Crouch and King, 2006), abstract knowl-
edge representational (Bobrow et al, 2007), Prop-
Bank (Palmer et al, 2005), or TimeBank (Mani
and Pustejovsky, 2004) annotations.
References
Mohammed Attia. 2008. A Unified Analysis of Cop-
ula Constructions. In Proceedings of the LFG ?08
Conference, pages 89?108. CSLI Publications.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar Engineering and Linguistic Hy-
pothesis Testing: Computational Support for Com-
plexity in Syntactic Analysis. In Emily M. Bender
and Jennifer E. Arnold, editors, Languages from a
Cognitive Perspective: Grammar, Usage and Pro-
cessing, pages 5?30. CSLI Publications.
558
Daniel G. Bobrow, Cleo Condoravdi, Dick Crouch,
Valeria de Paiva, Lauri Karttunen, Tracy Holloway
King, Rowan Nairn, Lottie Price, and Annie Zaenen.
2007. Precision-focused Textual Inference. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Cristina Bosco, Manuela Sanguinetti, and Leonardo
Lesmo. 2012. The Parallel-TUT: a multilingual and
multiformat treebank. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 1932?1938, Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Joan Bresnan. 1982. The Passive in Lexical Theory. In
Joan Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 3?86. The MIT Press.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishing.
Miriam Butt, Stefanie Dipper, Anette Frank, and
Tracy Holloway King. 1999a. Writing Large-
Scale Parallel Grammars for English, French and
German. In Proceedings of the LFG99 Conference.
CSLI Publications.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999b. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of the
COLING-2002 Workshop on Grammar Engineering
and Evaluation, pages 1?7.
Miriam Butt. 1995. The Structure of Complex Predi-
cates in Urdu. CSLI Publications.
Noam Chomsky. 1988. Lectures on Government and
Binding: The Pisa Lectures. Foris Publications.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press.
Dick Crouch and Tracy Holloway King. 2006. Seman-
tics via F-structure Rewriting. In Proceedings of the
LFG06 Conference, pages 145?165. CSLI Publica-
tions.
Dick Crouch, Tracy Holloway King, John T. Maxwell
III, Stefan Riezler, and Annie Zaenen. 2004. Ex-
ploiting F-structure Input for Sentence Condensa-
tion. In Proceedings of the LFG04 Conference,
pages 167?187. CSLI Publications.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman, 2012. XLE Documentation. Palo
Alto Research Center.
Mary Dalrymple, Helge Dyvik, and Tracy Holloway
King. 2004. Copular Complements: Closed or
Open? In Proceedings of the LFG ?04 Conference,
pages 188?198. CSLI Publications.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Helge Dyvik, Paul Meurer, Victoria Rose?n, and Koen-
raad De Smedt. 2009. Linguistically Motivated Par-
allel Parsebanks. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 71?82, Milan, Italy. EDU-
Catt.
Dan Flickinger, Valia Kordoni, Yi Zhang, Anto?nio
Branco, Kiril Simov, Petya Osenova, Catarina Car-
valheiro, Francisco Costa, and Se?rgio Castro. 2012.
ParDeepBank: Multiple Parallel Deep Treebank-
ing. In Proceedings of the 11th International Work-
shop on Treebanks and Linguistic Theories (TLT11),
pages 97?107, Lisbon. Edic?o?es Colibri.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald Kaplan. 2003. The
PARC700 Dependency Bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03).
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The Feature Space in Paral-
lel Grammar Writing. In Emily M. Bender, Dan
Flickinger, Frederik Fouvry, and Melanie Siegel, ed-
itors, Research on Language and Computation: Spe-
cial Issue on Shared Representation in Multilingual
Grammar Engineering, volume 3, pages 139?163.
Springer.
Natalia Klyueva and David Marec?ek. 2010. To-
wards a Parallel Czech-Russian Dependency Tree-
bank. In Proceedings of the Workshop on Anno-
tation and Exploitation of Parallel Corpora, Tartu.
Northern European Association for Language Tech-
nology (NEALT).
Jonas Kuhn and Michael Jellinghaus. 2006. Multilin-
gual Parallel Treebanking: A Lean and Flexible Ap-
proach. In Proceedings of the LREC 2006, Genoa,
Italy. ELRA/ELDA.
Tibor Laczko?. 2012. On the (Un)Bearable Lightness
of Being an LFG Style Copula in Hungarian. In Pro-
ceedings of the LFG12 Conference, pages 341?361.
CSLI Publications.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In
Proceedings of COLING, pages 711 ? 716.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina Bo?gel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliter-
ating Urdu for a Broad-Coverage Urdu/Hindi LFG
Grammar. In Proceedings of the Seventh Con-
ference on International Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
559
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral Discourse Models for Narrative Structure. In
Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 57?64.
Bea?ta Megyesi, Bengt Dahlqvist, E?va A?. Csato?, and
Joakim Nivre. 2010. The English-Swedish-Turkish
Parallel Treebank. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 629?637,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Joakim Nivre, Igor Boguslavsky, and Leonid Iomdin.
2008. Parsing the SynTagRus Treebank. In Pro-
ceedings of COLING08, pages 641?648.
Rachel Nordlinger and Louisa Sadler. 2007. Verb-
less Clauses: Revealing the Structure within. In An-
nie Zaenen, Jane Simpson, Tracy Holloway King,
Jane Grimshaw, Joan Maling, and Chris Manning,
editors, Architectures, Rules and Preferences: A
Festschrift for Joan Bresnan, pages 139?160. CSLI
Publications.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Proceedings
of the 7th International Conference on Advances in
Natural Language Processing (IceTAL 2010), pages
293?304.
Victoria Rose?n, Paul Meurer, and Koenraad de Smedt.
2009. LFG Parsebanker: A Toolkit for Building and
Searching a Treebank as a Parsed Corpus. In Pro-
ceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories (TLT7), pages 127?
133, Utrecht. LOT.
Victoria Rose?n, Koenraad De Smedt, Paul Meurer, and
Helge Dyvik. 2012. An Open Infrastructure for Ad-
vanced Treebanking. In META-RESEARCH Work-
shop on Advanced Treebanking at LREC2012, pages
22?29, Istanbul, Turkey.
Manuela Sanguinetti and Cristina Bosco. 2011. Build-
ing the Multilingual TUT Parallel Treebank. In Pro-
ceedings of Recent Advances in Natural Language
Processing, pages 19?28.
Sebastian Sulger. 2011. A Parallel Analysis of have-
Type Copular Constructions in have-Less Indo-
European Languages. In Proceedings of the LFG
?11 Conference. CSLI Publications.
Josef van Genabith and Dick Crouch. 1996. Direct and
Underspecified Interpretations of LFG f-structures.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), vol-
ume 1, pages 262?267, Copenhagen, Denmark.
Martin Volk, Anne Go?hring, Torsten Marek,
and Yvonne Samuelsson. 2010. SMUL-
TRON (version 3.0) ? The Stock-
holm MULtilingual parallel TReebank.
http://www.cl.uzh.ch/research/paralleltreebanks en.
html.
560
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 20?27,
Gothenburg, Sweden, April 26, 2014.
c
?2014 Association for Computational Linguistics
Automatic detection of causal relations in German multilogs
Tina B
?
ogel Annette Hautli-Janisz Sebastian Sulger Miriam Butt
Department of Linguistics
University of Konstanz
firstname.lastname@uni-konstanz.de
Abstract
This paper introduces a linguistically-
motivated, rule-based annotation system
for causal discourse relations in transcripts
of spoken multilogs in German. The over-
all aim is an automatic means of determin-
ing the degree of justification provided by
a speaker in the delivery of an argument
in a multiparty discussion. The system
comprises of two parts: A disambiguation
module which differentiates causal con-
nectors from their other senses, and a dis-
course relation annotation system which
marks the spans of text that constitute the
reason and the result/conclusion expressed
by the causal relation. The system is eval-
uated against a gold standard of German
transcribed spoken dialogue. The results
show that our system performs reliably
well with respect to both tasks.
1 Introduction
In general, causality refers to the way of know-
ing whether one state of affairs is causally related
to another.
1
Within linguistics, causality has long
been established as a central phenomenon for in-
vestigation. In this paper, we look at causality
from the perspective of a research question from
political science, where the notion is particularly
important when it comes to determining (a.o.) the
deliberative quality of a discussion. The notion of
deliberation is originally due to Habermas (1981),
who assumes that within a deliberative democ-
racy, stakeholders participating in a multilog, i.e.
a multi-party conversation, justify their positions
truthfully, rationally and respectfully and eventu-
ally defer to the better argument. Within polit-
ical science, the question arises whether actual
1
This work is part of the BMBF funded eHumanities
project VisArgue, an interdisciplinary cooperation between
political science, computer science and linguistics.
multilogs conducted in the process of a demo-
cratic decision making indeed follow this ideal
and whether/how one can use automatic means to
analyze the degree of deliberativity of a multilog
(Dryzek (1990; 2000), Bohman (1996), Gutmann
and Thompson (1996), Holzinger and Landwehr
(2010)). The disambiguation of causal discourse
markers and the determination of the relations they
entail is a crucial aspect of measuring the delibera-
tive quality of a multilog. In this paper, we develop
a system that is designed to perform this task.
We describe a linguistically motivated, rule-
based annotation system for German which disam-
biguates the multiple usages of causal discourse
connectors in the language and reliably annotates
the reason and result/conclusion relations that the
connectors introduce. The paper proceeds as fol-
lows: Section 2 briefly reviews related work on the
automatic extraction and annotation of causal rela-
tions, followed by a set of examples that illustrate
some of the linguistic patterns in German (Sec-
tion 3). We then introduce our rule-based anno-
tation system (Section 4) and evaluate it against a
hand-crafted gold standard in Section 5, where we
also present the results from the same annotation
task performed by a group of human annotators.
In Section 6, we provide an in-depth system error
analysis. Section 7 concludes the paper.
2 Related work
The automatic detection and annotation of causal-
ity in language has been approached from various
angles, for example by providing gold-standard,
(manually) annotated resources such as the Penn
Discourse Treebank for English (Prasad et al.,
2008), which was used, e.g., in the disambigua-
tion of English connectives by Pitler and Nenkova
(2009), the Potsdam Commentary Corpus for Ger-
man (Stede, 2004) and the discourse annotation
layer of Tu?ba-D/Z, a corpus of written German
text (Versley and Gastel, 2012). Training auto-
20
matic systems that learn patterns of causality (Do
et al., 2011; Mulkar-Mehta et al., 2011b, inter
alia) is a crucial factor in measuring discourse
coherence (Sanders, 2005), and is beneficial in
approaches to question-answering (Girju, 2003;
Prasad and Joshi, 2008).
With respect to automatically detecting causal
relations in German, Versley (2010) uses English
training data from the Penn Discourse Treebank in
order to train an English annotation model. These
English annotations can be projected to German
in an English-German parallel corpus and on the
basis of this a classifier of German discourse rela-
tions is trained. However, as previous studies have
shown (Mulkar-Mehta et al., 2011a, inter alia), the
reliability of detecting causal relations with auto-
matic means differs highly between different gen-
res. Our data consist of transcriptions of originally
spoken multilogs and this type of data differs sub-
stantially from newspaper or other written texts.
Regarding the disambiguation of German con-
nectives, Schneider and Stede (2012) carried out
a corpus study of 42 German discourse connec-
tives which are listed by Dipper and Stede (2006)
as exhibiting a certain degree of ambiguity. Their
results indicate that for a majority of ambigu-
ous connectives, plain POS tagging is not reliable
enough, and even contextual POS patterns are not
sufficient in all cases. This is the same conclu-
sion drawn by Dipper and Stede (2006), who also
state that off-the-shelf POS taggers are too unre-
liable for the task. They instead suggest a map-
ping approach for 9 out of the 42 connectives
and show that this assists considerably with dis-
ambiguation. As this also tallies with our experi-
ments with POS taggers, we decided to implement
a rule-based disambiguation module. This mod-
ule takes into account contextual patterns and fea-
tures of spoken communication and reliably de-
tects causal connectors as well as the reason and
result/conclusion discourse relations expressed in
the connected clauses.
3 Linguistic phenomenon
In general, causality can hold between single
concepts, e.g. between ?smoke? and ?fire?, or be-
tween larger phrases. The phrases can be put into
a causal relation via overt discourse connectors
like ?because? or ?as?, whereas other phrases en-
code causality implicitly by taking into account
world knowledge about the connected events. In
this paper, we restrict ourselves to the analysis of
explicit discourse markers; in particular we inves-
tigate the eight most frequent German causal con-
nectors, listed in Table 1. The markers of reason
on the left head a subordinate clause that describes
the cause of an effect stated in the matrix clause
(or in the previous sentence(s)). The markers of
result/conclusion on the other hand introduce a
clause that describes the overall effect of a cause
contained in the preceding clause/sentence(s). In
the genre of argumentation that we are working
with, the ?results? tend to be logical conclusions
that the speaker sees as following irrevocably from
the cause presented in the argument.
Reason Result
?because of? ?thus?
da daher
weil darum
denn deshalb
zumal deswegen
Table 1: German causal discourse connectors
The sentences in (1) and (2) provide exam-
ples of the phenomenon of explicit causal mark-
ers in German in our multilogs. Note that all
of the causal markers in Table 1 connect a re-
sult/conclusion with a cause/reason. The differ-
ence lies in which of these relations is expressed
in the clause headed by the causal connector.
The constructions in (1) and (2) exemplify this.
2
In (1), da ?since? introduces the reason for the con-
clusion in the matrix clause, i.e., the reason for
the travel times being irrelevant is that they are not
carried out as specified. In (2), daher ?thus? heads
the conclusion of the reason which is provided in
the matrix clause: Because the speaker has never
stated a fact, the accusation of the interlocutor is
not correct.
There are several challenges in the automatic
annotation of these relations. First, some of the
connectors can be ambiguous. In our case, four
out of the eight causal discourse connectors in Ta-
ble 1 are ambiguous (da, denn, daher and darum)
and have, in addition to their causal meaning, tem-
poral, locational or other usages. In example (3),
denn is used as a particle signaling disbelief, while
daher is used as a locational verb particle, having,
together with the verb ?to come?, the interpretation
2
These examples are taken from the Stuttgart 21 arbitra-
tion process, see section 5.1 for more information.
21
(1) Diese Fahrzeiten sind irrelevant, da sie so nicht gefahren werden.
Art.Dem travel time.Pl be.3.Pl irrelevant because they like not drive.Perf.Part be.Fut.3.Pl
Result/Conclusion Reason
?These travel times are irrelevant, because they are not executed as specified.?
(2) Das habe ich nicht gesagt, daher ist Ihr Vorwurf nicht richtig
Pron have.Pres.1.Sg I not say.Past.Part thus be.3.Sg you.Sg.Pol/Pl accusation not correct
Reason Result/Conclusion
?I did not say that, therefore your accusation is not correct.?
(3) Wie kommen Sie denn daher?
how come.Inf you.Sg.Pol then VPart
?What is your problem anyway?? (lit. ?In what manner are you coming here??)
(4) Da bin ich mir nicht sicher.
there be.Pres.1.Sg I I.Dat not sure
?I?m not sure about that.?
(5) Das kommt daher, dass keiner etwas sagt.
Pron come.Pres.3.Sg thus that nobody something say.Pres.3.Sg
Result/Conclusion Reason
?This is because nobody says anything.?
of ?coming from somewhere to where the speaker
is? (literally and metaphorically). In a second ex-
ample in (4), da is used as the pronominal ?there?.
Second, some of the causal connectors do not
always work the same way. In (5), the re-
sult/conclusion connector daher does not head
an embedded clause, rather it is part of the
matrix clause. In this case, the embedded
clause expresses the reason rather than the re-
sult/conclusion. A third challenge is the span of
the respective reason and result. While there are
some indications as to how to define the stretch
of these spans, there are some difficult challenges,
further discussed in the error analysis in Section 6.
In the following, we present the rule-based an-
notation system, which deals with the identifica-
tion of phrases expressing the result and reason,
along the lines illustrated in (1) and (2), as well as
with the disambiguation of causal connectors.
4 Rule-based annotation system
The automatic annotation system that we intro-
duce is based on a linguistically informed, hand-
crafted set of rules that deals with the disambigua-
tion of causal markers and the identification of
causal relations in text. As a first step, we divide
all of the utterances into smaller units of text in or-
der to be able to work with a more fine-grained
structure of the discourse. Following the liter-
ature, we call these discourse units. Although
there is no consensus in the literature on what ex-
actly a discourse unit consists of, it is generally
assumed that each discourse unit describes a sin-
gle event (Polanyi et al., 2004). Following Marcu
(2000), we term these elementary discourse units
(EDUs) and approximate the assumption made by
Polanyi et al. (2004) by inserting a boundary at
every punctuation mark and every clausal con-
nector (conjunctions, complementizers). Sentence
boundaries are additionally marked.
The annotation of discourse information is per-
formed at the level of EDUs. There are sometimes
instances in which a given relation such as ?rea-
son? spans multiple EDUs. In these cases, each of
the EDUs involved is marked/annotated individu-
ally with the appropriate relation.
In the following, we briefly lay out the two ele-
ments of the annotation system, namely the disam-
biguation module and the system for identifying
the causal relations.
22
4.1 Disambiguation
As shown in the examples above, markers like
da, denn, darum and daher ?because/thus? have a
number of different senses. The results presented
in Dipper and Stede (2006) indicate that POS tag-
ging alone does not help in disambiguating the
causal usages from the other functions, particu-
larly not for our data type, which includes much
noise and exceptional constructions that are not
present in written corpora. As a consequence, we
propose a set of rules built on heuristics, which
take into account a number of factors in the clause
in order to disambiguate the connector. To il-
lustrate the underlying procedure, (6) schematizes
part of the disambiguation rule for the German
causal connector da ?since?.
(6) IF da is not followed directly by a verb AND
no other particle or connector precedes da
AND
da is not late in the EDU THEN
da is a causal connector.
In total, the system comprises of 37 rules that
disambiguate the causal connectors shown in Ta-
ble 1. The evaluation in Section 5 shows that the
system performs well overall.
3
4.2 Relation identification
After disambiguation, a second set of rules anno-
tates discourse units as being part of the reason or
the result portion of a causal relation. One aspect
of deliberation is the assumption that participants
in a negotiation justify their positions. Therefore,
in this paper, we analyze causal relations within a
3
Two reviewers expressed interest in being able to access
our full set of rules. Their reasons were two-fold. For one,
sharing our rules would benefit a larger community. For an-
other, the reviewers cited concerns with respect to replicabil-
ity. With respect to the first concern, we will naturally be
happy to share our rule set with interested researchers. With
respect to the second concern, it is not clear to us that we
have understood it. As far as we can tell, what seems to be at
the root of the comments is a very narrow notion of replica-
bility, one which involves a freely available corpus in combi-
nation with a freely available automatic processing tool (e.g.,
a machine learning algorithm) that can then be used together
without the need of specialist language knowledge. We freely
admit that our approach requires specialist linguistic training,
but would like to note that linguistic analysis is routinely sub-
ject to replicability in the sense that given a set of data, the
linguistic analysis arrived at should be consistent across dif-
ferent sets of linguists. In this sense, our work is immediately
replicable. Moreover, given the publically available S21 data
set and the easily accessible and comprehensive descriptions
of German grammar, replication of our work is eminently
possible.
single utterance of a speaker, i.e., causal relations
that are expressed in a sequence of clauses which
a speaker utters without interference from another
speaker. As a consequence, the annotation system
does not take into account causal relations that are
split up between utterances of one speaker or ut-
terances of different speakers.
Nevertheless, the reason and result portion
of a causal relation can extend over multiple
EDUs/sentences and this means that not only EDUs
which contain the connector itself are annotated,
but preceding/following units that are part of the
causal relation also have to be marked. This in-
volves deep linguistic knowledge about the cues
that delimit or license relations, information which
is encoded in a set of heuristics that feed the 20 dif-
ferent annotation rules and mark the relevant units.
An example for a (simplified) relation annotation
is given in (7).
(7) IF result connector not in first EDU of sen-
tence AND
result connector not preceded by other con-
nector within same sentence THEN
mark every EDU from sentence beginning to
current EDU with reason.
ELSIF result connector in first EDU of sen-
tence THEN
mark every EDU in previous sentence with
reason UNLESS
encountering another connector.
5 Evaluation
The evaluation is split into two parts. On the one
hand, we evaluate the inter-annotator agreement
between five, minimally trained annotators (?5.2).
On the other hand, we evaluate the rule-based
annotation system against this hand-crafted gold-
standard (?5.3). Each evaluation is again split into
two parts: One concerns the successful identifica-
tion of the causal connectors. The other concerns
the identification of the spans of multilog that in-
dicate a result/conclusion vs. a reason.
5.1 Data
The underlying data comprises of two data sets,
the development and the test set. The develop-
ment set, on which the above-mentioned heuristics
for disambiguation and relation identification are
based, consists of the transcribed protocols of the
Stuttgart 21 arbitration process (henceforth: S21).
This public arbitration process took place in 2010
23
and was concerned with a railway and urban de-
velopment project in the German city of Stuttgart.
The project remains highly controversial and has
gained international attention. In total, the tran-
scripts contain around 265.000 tokens in 1330 ut-
terances of more than 70 participants.
4
The test set is based on different, but also tran-
scribed natural speech data, namely on experi-
ments simulating deliberative processes for estab-
lishing a governmental form for a hypothetical
new African country.
5
For testing, we randomly
collected utterances from two versions of the ex-
periment. Each utterance contained at least two
causal discourse connectors. In total, we extracted
60 utterances with an average length of 71 words.
There are a total of 666 EDUs and 105 instances
of the markers in Table 1. The composition of the
test set for each (possible) connector is in Table 2.
Reason Result
?because of? ?due to?
da 23 daher 10
weil 17 darum 11
denn 17 deshalb 12
zumal 4 deswegen 11
Total: 61 44
Table 2: Structure of the evaluation set
For the creation of a gold standard, the test set
was manually annotated by two linguistic experts.
238 out of 666 EDUs were marked as being part
of the reason of a causal relation, with the re-
sult/conclusion contributed by 180 EDUs. Out of
105 connectors found in the test set, 87 have a
causal usage. In 18 cases, the markers have other
functions.
5.2 Inter-annotator agreement
The task for the annotators comprised of two parts:
First, five students (undergraduates in linguistics)
had to decide wether an occurence of one of the
elements in Table 1 was a causal marker or not.
In a second step, they had to mark the bound-
aries for the reason and result/conclusion parts of
the causal relation, based on the boundaries of the
automatically generated EDUs. Their annotation
choice was not restricted by, e.g., instructing them
4
The transcripts are publicly available for down-
load under http://stuttgart21.wikiwam.de/
Schlichtungsprotokolle
5
These have been produced by our collaborators in polit-
ical science, Katharina Holzinger and Valentin Gold.
to choose a ?wider? or more ?narrow? span when
in doubt. These tasks served two purposes: On
the one hand, we were able to evaluate how easily
causal markers can be disambiguated from their
other usages and how clearly they introduce either
the reason or the result/conclusion of a causal re-
lation. On the other hand, we gained insights into
what span of discourse native speakers take to con-
stitute a result/conclusion and cause/reason.
For calculating the inter-annotator agreement
(IAA), we used Fleiss? kappa (Fleiss, 1971), which
measures the reliability of the agreement between
more than two annotators. In the disambiguation
task, the annotators? kappa is ? = 0.96 (?almost
perfect agreement?), which shows that the annota-
tors exhibit a high degree of confidence when dif-
ferentiating between causal and other usages of the
markers. When marking whether a connector an-
notates the reason or the result/conclusion portion
of a causal relation, the annotators have a kappa
of ? = 0.86. This shows that not only are anno-
tators capable of reliably disambiguating connec-
tors, they are also reliably labeling each connector
with the correct causal relation.
In evaluating the IAA of the spans, we mea-
sured three types of relations (reason, result and
no causal relation) over the whole utterance, i.e.
each EDU which is neither part of the result nor the
reason relation was tagged as having no causal re-
lation. We calculated four different ? values: one
for each relation type (vs. all other relation types),
and one across all relation types. The IAA fig-
ures are summarized in Table 3: For the causal
relation types, ?
Reason
=0.86 and ?
Result
=0.90 in-
dicate near-perfect agreement. ? is significantly
higher for causal EDUs than for non-causal (i.e.,
unmarked) EDUs (?
Non-causal
=0.82); this is in fact
expected since causal EDUs are the marked case
and are thus easier to identify for annotators in a
coherent manner.
IAA
?
Reason
0.86
?
Result
0.90
?
Non-causal
0.82
?
All
0.73
Table 3: IAA of span annotations
Across all relation types, ?
All
=0.73 indicates
?substantial agreement?. The drop in the agree-
ment is anticipated and mirrors the problem that
24
is generally found in the literature when evalu-
ating spans of discourse relations (Sporleder and
Lascarides, 2008). First, measuring ?
All
involves
three categories, whereas the other measures in-
volve two. Second, a preliminary error analysis
shows that there is substantial disagreement re-
garding the extent of both reason and result spans.
The examples in (8)?(9) illustrate this. While an-
notator 1 marks the result span (indicated by the
( S tag) as starting at the beginning of the sentence,
annotator 2 excludes the first EDU from the result
span.
6
In such cases, we thus register a mismatch
in the annotation of the first EDU.
Nevertheless, the numbers indicate a substantial
agreement. We thus conclude that the task we set
the annotators could be accomplished reliably.
5.3 System performance
In order to evaluate the automatic annotation sys-
tem described in Section 4, we match the system
output against the manually-annotated gold stan-
dard, calculating precision, recall and (balanced)
f-score of the annotation. For the disambiguation
of the connectors in terms of causal versus other
usages, the system performs as shown in Table 4
(the ? indicates the average of both values).
Precision Recall F-score
Causal 1 0.94 0.97
Non-causal 0.85 1 0.92
? 0.93 0.97 0.95
Table 4: Causal marker disambiguation
This result is very promising and shows that
even though the development data consists of data
from a different source, the patterns in the de-
velopment set are mirrored in the test set. This
means that the genre of the spoken exchange of
arguments in a multilog does not exhibit the dif-
ferences usually found when looking at data from
different genres, as Mulkar-Mehta et al. (2011a)
report when comparing newspaper articles from fi-
nance and sport.
For evaluating the annotated spans of reason
and result, we base the calculation on whether an
EDU is marked with a particular relation or not, i.e.
if the system marks an EDU as belonging to the
reason or result part of a particular causal marker
and the gold standard encodes the same informa-
tion, then the two discourse units match. As a con-
6
We use the | sign to indicate EDU boundaries.
sequence, spans which do not match perfectly, for
example in cases where their boundaries do not
match, are not treated as non-matching instances
as a whole, but are considered to be made up of
smaller units which match individually. Table 5
shows the results.
Precision Recall F-score
Reason 0.88 0.75 0.81
Result 0.81 0.94 0.87
? 0.84 0.84 0.84
Table 5: Results for relation identification
These results are promising insofar as the de-
tection of spans of causal relations is known to be
a problem. Again, this shows that development
and test set seem to exhibit similar patterns, de-
spite their different origins (actual political argu-
mentation vs. an experimental set-up). In the fol-
lowing, we present a detailed error analysis and
show that we find recurrent patterns of mismatch,
most of which can in principle be dealt with quite
straightforwardly.
6 Error analysis
Figure 1: Error analysis, in percent.
Figure 1 shows a pie chart in which each prob-
lem is identified and shown with its share in
the overall error occurrence. In total, the sys-
tem makes 26 annotation errors. Starting from
the top, empty connector position refers to struc-
tures which an annotator can easily define as rea-
son/result, but which do not contain an overt con-
nector. This causes the automatic annotation sys-
25
(8) Annotator 1:
( S Ich mo?chte an dieser Stelle einwerfen, | dass die Frage, ob ...
I would like.Pres.1.Sg at this point add.Inf that the question if ...
?I?d like to add at this point that the question if...
(9) Annotator 2:
Ich mo?chte an dieser Stelle einwerfen, | ( S dass die Frage, ob ...
I would like.Pres.1.Sg at this point add.Inf that the question if ...
?I?d like to add at this point that the question if...
tem to fail. The group of other connectors refers to
cases where a non-causal connector (e.g., the ad-
versative conjunction aber ?but?) signals the end
of the result/conclusion or cause span for a human
annotator. The presence of these other connectors
and their effect is not yet taken into account by the
automatic annotation system. The error group iaa
refers to the cases where we find a debatable dif-
ference of opinion with respect to the length of a
span. Speaker opinion refers to those cases where
a statement starts with expressions like ?I believe
/ I think / in my opinion etc.?. These are mostly
excluded from a relation span by human anno-
tators, but (again: as of yet) not by the system.
Span over several sentences refers to those cases
where the span includes several sentences. And
last, but not least, since the corpus consists of spo-
ken data, an external transcriptor had to transcribe
the speech signal into written text. Some low-level
errors in this category are missing sentence punc-
tuation. The human annotators were able to com-
pensate for this, but not the automatic system.
Roughly, three groups of errors can be distin-
guished. Some of the errors are relatively easy
to solve, by, e.g., adding another class of con-
nectors, by adding expressions or by correcting
the transcriptors script. A second group (span
over several sentences and empty connector po-
sition) needs a much more sophisticated system,
including deep linguistic knowledge on semantics,
pragmatics and notoriously difficult aspects of dis-
course analysis like anaphora resolution.
7 Conclusion
In conclusion, we have presented an automatic an-
notation system which can reliably and precisely
detect German causal relations with respect to
eight causal connectors in multilogs in which ar-
guments are exchanged and each party is trying to
convince the other of the rightness of their stance.
Our system is rule-based and takes into account
linguistic knowledge at a similar level as that used
by human annotators. Our work will directly ben-
efit research in political science as it can flow into
providing one measure for the deliberative qual-
ity of a multilog, namely, do interlocutors support
their arguments with reasons or not?
References
James Bohman. 1996. Public Deliberation: Plural-
ism, Complexity and Democracy. The MIT Press,
Cambridge, MA.
Stefanie Dipper and Manfred Stede. 2006. Disam-
biguating potential connectives. In Proceedings of
KONVENS (Conference on Natural Language Pro-
cessing) 2006.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally Supervised Event Causality Identifica-
tion. In Proceedings of EMNLP?11, pages 294?303.
John S. Dryzek. 1990. Discursive Democracy: Poli-
tics, Policy, and Political Science. Cambridge Uni-
versity Press, Cambridge, MA.
John S. Dryzek. 2000. Deliberative Democracy and
Beyond: Liberals, Critics, Contestations. Oxford
University Press, Oxford.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Roxana Girju. 2003. Automatic Detection of Causal
Relations for Question-Answering. In Proceedings
of the ACL Workshop on Multilingual summariza-
tion and question-answering, pages 76?83.
Amy Gutmann and Dennis Frank Thompson. 1996.
Democracy and Disagreement. Why moral conflict
cannot be avoided in politics, and what should be
done about it. Harvard University Press, Cam-
bridge, MA.
Ju?rgen Habermas. 1981. Theorie des kommunikativen
Handelns. Suhrkamp, Frankfurt am Main.
Katharina Holzinger and Claudia Landwehr. 2010. In-
stitutional determinants of deliberative interaction.
European Political Science Review, 2:373?400.
26
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, Mass.
Rutu Mulkar-Mehta, Andrew S. Gordon, Jerry Hobbs,
and Eduard Hovy. 2011a. Causal markers across
domains and genres of discourse. In The 6th Inter-
national Conference on Knowledge Capture.
Rutu Mulkar-Mehta, Christopher Welty, Jerry R.
Hoobs, and Eduard Hovy. 2011b. Using granularity
concepts for discovering causal relations. In Pro-
ceedings of the FLAIRS conference.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of ACL-IJCNLP, pages 13?16.
Livia Polanyi, Chris Culy, Martin van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Sen-
tential structure and discourse parsing. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 80?87.
Rashmi Prasad and Aravind Joshi. 2008. A Discourse-
based Approach to Generating Why-Questions from
Texts. In Proceedings of the Workshop on the Ques-
tion Generation Shared Task and Evaluation Chal-
lenge.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of LREC 2008, pages 2961?2968.
Ted Sanders. 2005. Coherence, Causality and Cog-
nitive Complexity in Discourse. In Proceedings of
SEM-05, First International Symposium on the Ex-
ploratiaon and Modelling of Meaning, pages 105?
114.
Angela Schneider and Manfred Stede. 2012. Ambi-
guity in German Connectives: A Corpus Study. In
Proceedings of KONVENS (Conference on Natural
Language Processing) 2012.
Caroline Sporleder and Alex Lascarides. 2008. Us-
ing Automatically Labelled Examples to Classify
Rhetorical Relations: An Assessment. Natural Lan-
guage Engineering, 14(3):369?416.
Manfred Stede. 2004. The Potsdam Commentary Cor-
pus. In In Proceedings of the ACL?04 Workshop on
Discourse Annotation, pages 96?102.
Yannick Versley and Anna Gastel. 2012. Linguistic
Tests for Discourse Relations in the Tu?ba-D/Z Cor-
pus of Written German. Dialogue and Discourse,
1(2):1?24.
Yannick Versley. 2010. Discovery of Ambiguous and
Unambiguous Discourse Connectives via Annota-
tion Projection. In Workshop on the Annotation and
Exploitation of Parallel Corpora (AEPC).
27
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 1?10,
Dublin, Ireland, August 23-29 2014.
Towards Identifying Hindi/Urdu Noun Templates in Support of a
Large-Scale LFG Grammar
Sebastian Sulger
Department of Linguistics
University of Konstanz
Germany
sebastian.sulger@uni-konstanz.de
Ashwini Vaidya
University of Colorado
Boulder, CO
80309 USA
vaidyaa@colorado.edu
Abstract
Complex predicates (CPs) are a highly productive predicational phenomenon in Hindi and Urdu
and present a challenge for deep syntactic parsing. For CPs, a combination of a noun and light
verb express a single event. The combinatorial preferences of nouns with one (or more) light
verb is useful for predicting an instance of a CP. In this paper, we present a semi-automatic
method to obtain noun groups based on their co-occurrences with light verbs. These noun groups
represent the likelihood of a particular noun-verb combination in a large corpus. Finally, in
order to encode this in an LFG grammar, we propose linking nouns with templates that describe
preferable combinations with light verbs.
1 Introduction
A problem that crops up repeatedly in shallow and deep syntactic parsing approaches to South Asian lan-
guages like Urdu and Hindi
1
is the proper treatment of complex predicates (CPs). In CPs, combinations
of more than one element are used to express an event (e.g., memory + do = remember). In Urdu/Hindi,
only about 700 simple verbs exist (Humayoun, 2006); the remaining verbal inventory consists of CPs.
CPs are encountered frequently in general language use, as well as in newspaper corpora. Thus, any NLP
application, whether shallow or deep, whether its goal be parsing, generation, question-answering or the
construction of lexical resources like WordNet (Bhattacharyya, 2010) encounters CPs sooner rather than
later.
There is a range of different elements that may combine with verbs to form a CP: verbs, nouns, prepo-
sitions, adjectives all occur in CPs. The constraints and productive mechanisms in verb-verb CPs are
comparatively well-understood (e.g, see Hook (1974), Butt (1995), Butt (2010) and references therein).
The domain of noun-verb CPs (N-V CPs) is less well understood, the standard theoretical reference being
Mohanan (1994). It is only recently that researchers have tried to come up with linguistic generaliza-
tions regarding N-V CPs, some by using manual methods and linguistic introspection (Ahmed and Butt,
2011), others using a combination of manual and statistical methods (Butt et al., 2012).
Ahmed and Butt (2011) have suggested that the combinatory possibilities of N-V combinations are in
part governed by the lexical semantic compatibility of the noun with the verb. Similar observations have
been made for English (Barrett and Davis, 2003; North, 2005). If this is true, then lexical resources
such as WordNet could be augmented with semantic specifications or feature information that can then
be used to determine dynamically whether a given N-V combination is licit or not.
Knowledge about this kind of lexical-semantic information is essential in computational grammars.
For example,lexicon entries and templates are required to define predicational classes. Implementing
such a grammar for a language that makes heavy use of CPs calls for two requirements. First, the lexical
items taking part in CP formation need to be present in the lexicon of the grammar; and second, the
grammar needs to be engineered in a way that represents the correct linguistic generalizations. Any ap-
1
Urdu is an Indo-Aryan language spoken primarily in Pakistan and parts of India, as well as in the South Asian diaspora. It
is structurally almost identical to Hindi, although the lexicon and orthography differs considerably.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
proach that is short of either of these requirements will result either in loss in coverage or overgeneration
of the grammar.
The Hindi/Urdu ParGram Grammar forms part of a larger international research effort, the ParGram
(Parallel Grammars) project (Butt et al., 1999; Butt et al., 2002; Butt and King, 2007). All of the gram-
mars in the ParGram project are couched within the LFG framework and are implemented using the
development platform XLE (Crouch et al., 2012). The grammars are developed manually and not via
learning methods, which allows for a theoretically sound analysis that is also efficient from a computa-
tional point of view. The Hindi/Urdu ParGram Grammar aims at covering both Hindi and Urdu, which is
a design decision that suggests itself due to the many structural conformities of the two languages (Butt
et al., 2002). One weakness of the grammar is its currently relatively small lexicon, compared to other
ParGram grammars. Adding to the lexicon is a critical step in extending the grammar coverage. This
is even more true for N-V CPs due to the high frequency of such constructions in running text. Thus,
we see the Hindi/Urdu ParGram Grammar as an ideal test bed for developing a lexical resource of Hindi
nouns.
This paper is a first step in terms of constructing such a lexical resource for Hindi nouns. Following
up on previous work, we assume that there are distinct groups of nouns; nouns that are part of a certain
group tend to co-occur with the same light verb(s) and differ in their usage from members of other
groups. Contrary to what has been done before, though, we do not dive into available corpora blindly to
identify the groupings. Instead, we make use of a manually annotated treebank for Hindi, the Hindi and
Urdu Treebank (HUTB, Bhatt et al. (2009)). Thus, we construct a seed list of nouns known to partake
in CP formation in the HUTB. Since the HUTB is limited in its coverage, we then turn to a large Hindi
corpus collected specifically for the present study and use clustering algorithms to put the nouns in the
seed list into groups, based on light verb co-occurrence.
2
Our aim is to arrive at a broad notion of noun similarity. If we can find groups of nouns that be-
have alike with respect to their light verbs, these groups can be included in an application such as the
Hindi/Urdu ParGram Grammar to boost coverage as well as precision. Note that this notion of noun
similarity is not the same as semantic classes in the sense of Levin (1993); however, it can serve as input
for future research into semantic noun classification.
2 N-V Complex Predicates in Hindi and Urdu
As mentioned above, CPs are an important means of forming verbal predication in Hindi and Urdu.
There is no single way of forming CPs; it is possible to find V-V CPs (Butt, 1995), ADJ-V combinations,
P-V CPs (Raza, 2011) and N-V CPs (Mohanan, 1994) (see Ahmed et al. (2012) for some examples of
each CP type.). In the present paper, we focus on identifying patterns of N-V CP formation. Here,
the noun contributes the main predicational content. The verb in such constructions is usually called a
light verb (Mohanan, 1994; Butt, 2003). The term represents the fact that these verbs are semantically
bleached and specify additional information about the predication, such as whether the predicate has an
agentive, telic or stative flavor. The light verb also determines the case marking on the subject, controls
agreement patterns and contributes tense and aspect information. This is illustrated in (1).
(1) a. nadya=ne kAhani yad k-i
Nadya.F.Sg=Erg story.F.Sg memory.F.Sg do-Perf.F.Sg
?Nadya remembered a/the story (agentively).? (lit. ?Nadya did memory of a/the story.?)
b. nadya=ko kAhani yad hE
Nadya.F.Sg=Dat story.F.Sg memory.F.Sg be.Pres.3.Sg
?Nadya remembers/knows a/the story.? (lit. ?At Nadya is memory of a/the story.?)
2
Note that despite the many structural conformities between Hindi and Urdu, the main difference between the two languages
is in the lexicon; Modern Standard Hindi vocabulary is based on Sanskrit, while Urdu draws from a Persio-Arabic lexicon.
This means that in principle, the methodology presented in this paper applied to Hindi needs to be applied to both languages
separately. The equivalent Urdu study is pending future work and currently faces two major obstacles. First, the Urdu portion
of the HUTB has not yet been released. Second, there is a major shortage of Urdu resources, with comparatively small corpora
becoming available only recently (Urooj et al., 2012). Readily available Urdu sources (e.g., Wikipedia) are of minor quality.
2
c. nadya=ko kAhani yad a-yi
Nadya.F.Sg=Dat story.F.Sg memory.F.Sg come-Perf.F.Sg
?Nadya remembered a/the story.? (lit. ?The memory of a/the story came to Nadya.?)
In all of the examples in (1), it is evident that the noun and the verb form a single predicational element.
The object kAhani ?story? is thematically licensed by the noun yad ?memory?, but it is not realized as a
genitive, as would be typical for arguments of nouns (and as in the English literal translations). Rather,
kAhani ?story? functions as the syntactic object of the joint predication (see Mohanan (1994) for details
on the argument structure and agreement patterns).
3 Previous Work
A recent study on the semantic classes of Persian N-V CPs using distributional vector-space methods
has shown that verb vectors are a very useful indicator of noun similarity (Taslimipoor et al., 2012). The
reported results are significantly better using the light verb dimension; Taslimipoor et al. (2012) state that
this affirms their original intuition that a verb-based vector space model can better capture similarities
across CPs. This finding is in agreement with our intuition that features based on light verbs best capture
generalizations about N-V CPs.
There have been two studies on noun similarity based on co-occurrence of noun and light verb. Ahmed
and Butt (2011) look at the light verbs kar ?do?, ho ?be?, hu- ?become? and identify three classes of nouns
based on co-occurrence patters. The first consists of psychological nouns that occur with all three light
verbs. The examples shown in (1) represent the class that is compatible with all of the light verbs
surveyed. Other CP classes may only be compatible with a subset of light verbs. The second and third
classes consist of nouns that are classified as more or less agentive in nature- based on their capacity to
form CPs with hu- ?become?. For example, the noun tamir ?construction? is only compatible with the
light verb kar ?do? but disallows hu- ?become?.
(2) a. b?lal=ne mAkan tAmir k?-ya
Bilal.M.Sg=Erg house.M.Sg construction.F.Sg do-Perf.M.Sg
?Bilal built a/the house.?
b. *b?lal=ko mAkan tAmir hE/hu-a
Bilal.M.Sg=Dat house.M.Sg construction.F.Sg be.Pres.3.Sg/be.Part-Perf.M.Sg
?Bilal built a/the house.?
In a follow-up study, Butt et al. (2012) attempted to identify Urdu N-V CPs automatically. After
filtering out the irrelevant combinations, they found that most nouns were either psychological nouns
(and occurred with all three light verbs) or nouns that were highly agentive and disallowed hu- ?become?.
However, one of the drawbacks of their method was the use of an untagged corpus, which required
extensive filtering in order to separate the light and non-light instances of these verbs.
We will draw upon the results of these two studies to motivate this present work. The classes identified
by Ahmed and Butt (2011) seem promising, but the corpus work was done manually, and the total size
of their data set is limited to 45 nouns. This can hardly serve as input to the development of a large-scale
noun lexicon for a grammar. In constructing a lexical resource, we thus take a different route in that we
try to expand the search space by using an external, manually-crafted resource and a larger set of light
verbs to come up with more substantial noun groups.
3
In addition, we circumvent the problems faced in
Butt et al. (2012)?s paper by filtering the list of nominal predicates in advance and by making use of a
tagged corpus.
3
One might argue that there are other features, beyond the light verbs, that one could use in identifying noun classes/groups,
e.g., additional arguments licensed, case marking, etc. The reason why we (and other researchers before us) limit ourselves
to the light verb occurrences is that Hindi and Urdu make rampant use of pro-drop (Kachru, 2006; Schmidt, 1999; Mohanan,
1994), which means that often, not all arguments are present in a sentence. Thus, the only reliable source of information about
the noun is in fact the light verb, since this is the only obligatory element aside from the noun itself.
3
4 Methodology
In order to build a lexical resource of semantically similar nouns, we first need to identify whether these
occur as part of a N-V CP. As we want to improve upon previous work, our aim was to include a large
number of nouns. The Hindi portion of the Hindi and Urdu Treebank (Bhatt et al., 2009) includes N-V
CPs that have been manually tagged with the dependency label POF (which stands for ?part of?). The
diagnostic criteria used for identifying CPs in the treebank is based on native speaker intuition. The POF
label is used for adjectives and adverbs as well as nouns. We extracted only POF cases that were nouns
only. This gave us an initial list of candidate nouns that were further filtered for spelling variations and
annotation errors. After this stage, we had a list of 1207 nouns, which we will refer to as our seed list.
The seed list consists of nouns that are a part of N-V CPs in the treebank.
Our aim was to include nouns that had at least 50 or more occurrences in order to ensure that we were
looking at the most well-attested noun and light verb co-occurrences. For this task, the Hindi Treebank
corpus (400,000 words) by itself would not be sufficient. For instance, if we applied our cutoff of 50
occurrences to the instances in the treebank, we would be left with only 20 nouns, which would not give
us any meaningful groups. Therefore, we chose to use a larger corpus (including the treebank) in order
to give us co-occurrence patterns for a noun from the seedlist. At the same time, we did not look at
co-occurrence patterns with any verb. Instead, we chose a list of the most frequent light verbs from the
treebank. This list is given below:
(3) ho ?be?, kar ?do?, de ?give?, le ?take?, rakh ?put?, lag ?attach?, a ?come?
Given the seed list and a short list of light verbs, our next step was the extraction of co-occurrences
from a larger corpus.
4.1 Extracting Co-occurrences from a Large Corpus
In order to obtain a larger corpus, we scraped two large online sources of Hindi: the BBC Hindi website
4
as well as the Hindi Wikipedia.
5
Along with the Hindi Treebank, this corpus contains about 21 million
tokens (BBC Hindi: ?7 million, Hindi Wikipedia: ?10 million, Hindi Treebank ?4 million); by includ-
ing the Wikipedia part, the resulting corpus extends beyond the newspaper domain. In a second step, the
corpus was automatically POS tagged using the tagger described in Reddy and Sharoff (2011).
We were interested in extracting co-occurrences that had the following pattern: seed list item + light
verb. A match would only occur if one of the light verbs occurred directly to the right of the noun (i.e.,
an item tagged as NN by the POS tagger). Our method therefore did not take into account any N-V CPs
that were syntactically flexible, i.e., when the noun and the light verb did not occur next to each other.
Those cases where the noun may be scrambled away from the light verb (e.g., topicalization of the noun)
are not numerous and occur rarely in the Hindi Treebank (only about 1% of the time).
6
4.2 Clustering & Evaluation
In the next step, a clustering algorithm was applied to the data. This was done using the clustering tool
described in Lamprecht et al. (2013). At the moment, the tool features two clustering algorithms: the
k-means algorithm (MacQueen, 1967) as well as the Greedy Variance Minimization (GVM) algorithm.
7
We made use of an automatic method using Hindi WordNet (Bhattacharyya, 2010) to choose the best
partition value. We followed the technique described in Van de Cruys (2006), which uses WordNet
relations to arrive at the most semantically coherent clusters. We define semantic coherence as the
similarity between items in a cluster, based on an overlap between their WordNet relations. Specifically,
for each k = 2?10, we iterated through the automatically generated clusters and performed the following
steps:
1. Using WordNet, we extracted synonyms, hypernyms and hyponyms for every word in a cluster.
4
http://www.bbc.co.uk/hindi
5
http://dumps.wikimedia.org/hiwiki
6
Mohanan (1994) even goes so far as to call the topicalization of nouns in N-V CPs ungrammatical.
7
http://code.google.com/p/tomgibara/
4
2 4 6 8 10
0.0
2
0.0
4
0.0
6
0.0
8
0.1
0
0.1
2
0.1
4
Results with GVM and Kmeans, with varying cutoffs
Number of clusters
Se
ma
nti
c c
oh
ere
nc
e
Kmeans;50
GVM;50
Kmeans;3
GVM;3
Figure 1: Choosing the best value for k. k-means with a frequency cutoff of 50 gives us the most
semantically coherent clusters for k = 5
2. A word that had the most semantic relations with every word in the cluster was chosen as its cen-
troid.
3. The co-hyponyms i.e., the hyponyms of the hypernyms for this centroid were extracted from Word-
Net (along with its synonyms, hypernyms and hyponyms).
4. In order to calculate precision for each cluster, we counted the number of words in that cluster that
overlapped with the words in the centroid?s relations.
We averaged the precision across all clusters for every k value. We found that precision for each
cluster gradually improved until we got the most semantically coherent partitions for k = 5 using k-
means, for 522 nouns occurring with a frequency of 50 and above. Table 1 shows the values for k using
our WordNet evaluation method, for k = 5? 9.
Frequency = 3 Frequency = 50
Size of k GVM k-means GVM k-means
5 0.049 0.060 0.107 0.122
6 0.066 0.055 0.121 0.119
7 0.089 0.056 0.104 0.110
8 0.084 0.089 0.108 0.109
9 0.082 0.081 0.095 0.097
Table 1: Semantic coherence values for k = 5? 9 for clustering algorithms GVM and k-means
In Figure 1, we have plotted the semantic coherence values against the number of clusters to show
the best results. The k-means algorithm performed only slightly better than GVM, and after k = 5, the
semantic coherence of the clusters declined again. As a point of comparison, we also plotted k-means
and GVM results for nouns that occurred more than 3 times in the data (i.e., using a far smaller cutoff).
In this configuration, the best results are achieved for a higher k value (i.e., between 7 or 8), but we
rejected this on the basis of a better semantic coherence value for k-means with a cutoff of 50.
5
Figure 2: Visualization for k = 5 clusters
5 Analysis
Lamprecht et al. (2013)?s tool is useful for visual cluster inspection; e.g., the tool created the visual
clustering in Figure 2 using the k-means algorithm with k = 5. The visualization enables the user to in-
spect the data points and derive initial generalizations. For example, Figure 2 shows a visualization with
colored circles that encode membership within a cluster. The larger circles represent cluster centroids.
The visualization enables us to see three most frequently occurring light verbs, viz. kar ?do?, ho ?be? and
de ?give?, represented by light green, dark blue and pink respectively. Many nouns alternate with ?do?
and ?be?, hence there is a visible continuum between the light green and dark blue data points. The two
clusters in the centre show a dark green cluster, consisting of only a handful of nouns that alternate with
the light verbs rakh ?keep?, lag ?attach? and a ?come?. The light blue cluster on the other hand is larger
and is dominated by the light verb le ?take?.
In order to further interpret the results of our study, we also referred to a secondary result from our
WordNet evaluation. While extracting the extent of overlap of the semantic relations, we also extracted
the ?semantic? centroids i.e. words that had the most semantic relations with every other word in the
cluster (see Section 4.2). For our best result of k = 5, these centroids also revealed semantic similarities
in the five clusters that we found. For instance, dynamic events that are inanimate and abstract and take
an agentive argument will lend themselves to combinations with kar ?do?. Similarly, events that include
the semantic property of ?transfer? will occur with de ?give? (although there is ostensibly an overlap here,
as these events invariably also require agentive arguments). The light verb ho ?be? occurs often with
nouns that denote mental states, resulting in an experiencer subject ? but this group also includes nouns
that alternate with kar. Less frequently occurring light verbs, especially rakh ?keep?, lag ?to attach? and
a ?come? show fewer alternations, as they do not occur in combination with all nouns and are grouped
together in this result. These light verbs often form N-V CPs with a more idiosyncratic meaning, in fact
Davison (2005) has argued that some of these light verbs may form ?incorporation idioms? (rather than
true N-V CPs).
The average figures of N-V CP co-occurrences for a certain cluster inform us about the likelihood of
a certain group of nouns to co-occur with a certain light verb. For instance, this noun grouping shows a
high likelihood of occurrence with kar ?do? and le ?take?, but not very likely at all with lag ?attach?. We
take this distribution to reflect a difference in the syntactic behavior of the nouns: While the productive
patterns indicate CP formation, the less productive patterns do not represent CPs at all. This is a finding
in line with Butt et al. (2012), who ended up deleting many low-frequency patterns which turned out
to be non-CP combinations. Similar tendencies can be derived for the five groups of nouns derived
6
from our clustering experiment above. This information is useful for a task like lexicon development
for a computational grammar. The following section therefore explores the possibility of encoding noun
group information in a computational Lexical Functional Grammar.
6 Noun Groups in Hindi/Urdu Grammar Development
Our experiments show that nouns appear with several different distributions, often with one dominant
light verb, but also with the possibility of occurring with one or two other light verbs. The clusters do
not represent absolute certainties about N-V CPs, but report tendencies of occurrences; e.g., the relative
frequencies of the cluster centroid for the noun group dominated by de ?give? is shown below.
(4) de ?give? 0.75, kar ?do? 0.08, le ?take? 0.06, ho ?be? 0.06, a ?come? 0.02, rakh ?keep? 0.02, lag
?attach? 0.01
In this section, we discuss the integration of our Hindi noun groupings into the grammar via the
construction of templates that can be augmented to model the relevant linguistic generalizations in terms
of constraints inspired by optimality theory (OT, Prince and Smolensky (2004)). A serious evaluation of
the effect on the grammar of adding in this lexical resource is planned for future work.
6.1 Templates in XLE
In XLE, grammar writers can define templates in a special section of the grammar that can be called
from the lexicon. Templates allow generalizations to be captured and, if necessary, changes to be made
only once, namely to the template itself (Butt et al., 1999; Dalrymple et al., 2004). Consider the template
in (5), which models intransitive verbs in English; these are represented in LFG terms as predicates that
apply to a single grammatical function, a subject. The lexical entry in (6) for the English intransitive
verb laugh calls up the INTRANS template; the argument supplied to the template is substituted for the
P(redicate) value inside the template definition.
(5) INTRANS(P) = (? PRED) = ? P<(? SUBJ)>?
@NOPASS.
(6) laugh V @(INTRANS laugh).
In ParGram grammars, the lexicons are generally organized so that each verb subcategorization frame
corresponds to a different template. Similarly, templates can be defined to encode a given set of general-
izations about how certain groups of nouns combine with different light verbs. Consider the N-V CPs in
(7). The noun ?sharA ?signal? forms part of the cluster dominated by de ?give? (i.e., the cluster with the
frequencies shown in (4)) and thus occurs most frequently with de ?give? as well as kar ?do?.
(7) a. nadya=ne b?lal=ko ?shara d?-ya
Nadya.F.Sg=Erg Bilal.M.Sg=Dat signal.M.Sg give-Perf.M.Sg
?Nadya signaled Bilal.? (lit. ?Nadya gave a signal to Bilal.?)
b. nadya=ne b?lal=ko ?shara k?-ya
Nadya.F.Sg=Erg Bilal.M.Sg=Acc signal.M.Sg do-Perf.M.Sg
?Nadya signaled Bilal.? (lit. ?Nadya made a signal towards Bilal.?)
The lexical entry of the noun ?shara ?signal? is given in (8).
8
The entry points to the template
NVGROUP2 which is defined as in (9). This version of the template constrains the verbal type of the
overall predication to be a CP either with the light verb de ?give? or with the light verb kar ?do?, or to not
be a CP at all. Thus, only light verb options with relative frequencies equaling or above 0.08 (i.e., 8%)
are accepted, an arbitrary threshold.
8
The transliteration scheme employed in the Hindi/Urdu ParGram Grammar is described in Malik et al. (2010).
7
(8) iSArA NOUN-S XLE (? PRED) = ?iSArA<(? OBJ)>?
@NVGROUP2.
(9) NVGROUP2 = { (? VTYPE COMPLEX-PRED-FORM) =c dE
|(? VTYPE COMPLEX-PRED-FORM) =c kar
| ? (? VTYPE COMPLEX-PRED-FORM)}
6.2 Preferred CPs
The template in (9), however, misses out on the fact that for all the groups identified, there are N-V com-
binations that are more productive (and thus more likely to be CP constructions) than other combinations
(which are more likely to be non-CP constructions, e.g., plain objects). In XLE, grammar developers can
model statistical generalizations using special marks that were inspired by Optimality Theory (Prince
and Smolensky, 2004). On top of the classical constraint system of existing LFG grammars, a separate
projection, o-structure, determines a preference ranking on the set of analyses for a given input sentence.
A relative ranking is specified for the constraints that appear in the o-projection, and this ranking serves
to determine the winner among the competing candidates. The constraints are also referred to as OT
marks and are overlaid on the existing grammar (Frank et al., 1998).
OT marks can be added in the appropriate place in the grammar to punish or prefer a certain analysis.
For example, (10) states that Mark1 is a member of the optimality projection. The order of preference of
a sequence of OT marks can be specified in the configuration section of the grammar; an example pref-
erence ordering is given in (11). Here, the list given in OPTIMALITYORDER shows the relative importance
of the marks. In this case Mark5 is the most important, and Mark1 is the least important. Marks that have
a + in front of them are preference marks. The more preference marks that an analysis has, the better. All
other marks are dispreference marks (the fewer, the better).
(10) ... Mark1 $ o::
*
...
(11) OPTIMALITYORDER Mark5 Mark4 Mark3 +Mark2 +Mark1.
Given the relative ordering of light verb tendencies in our noun groups, we can augment the templates
with OT marks that represent such tendencies. The noun template in (9) is changed in two ways. First,
all the light verbs are included; second, each disjunct is extended by two OT marks that represent the
statistical likelihood of this particular combination forming a CP or not.
9
The ordering of the marks is
shown in (13), where the mark cp-dispref is most severely punished, and the mark +cp-pref is most
strongly preferred. With an ordering like this, a CP analysis for (7a) is preferred, while a compositional
analysis is dispreferred by XLE; the inverse will apply to ?shara lAg, which is not a CP.
(12) NVGROUP2 = { { (? VTYPE COMPLEX-PRED-FORM) =c dE
cp-pref $ ::
*
| ? (? VTYPE COMPLEX-PRED-FORM)
non-cp-dispref $ ::
*
}
...
|(? VTYPE COMPLEX-PRED-FORM) =c lag}.
cp-dispref $ o::
*
| ? (? VTYPE COMPLEX-PRED-FORM)
non-cp-pref $ ::
*
} }.
(13) OPTIMALITYORDER cp-dispref non-cp-dispref +cp-pref +non-cp-pref.
9
For space reasons, only the disjuncts for de ?give? as well as lAg ?attach? are shown.
8
7 Conclusion
We have discussed a corpus study of Hindi/Urdu N-V CPs that makes use of a novel methodology in
terms of a noun seed list and an evaluation based on WordNet. We found that the k-means algorithm
with k = 5 and a frequency cutoff of 50 gave us the best result in terms of semantic coherence of the
resulting clusters. We are optimistic that the resulting noun groups can be used in different NLP settings
and have presented one such setting, the Hindi/Urdu ParGram Grammar, where lexical information about
nouns and their combinatory possibilities in CPs are vital for grammar extension.
Acknowledgements
We would like to thank the DAAD (Deutscher Akademischer Austausch Dienst) for sponsoring Ashwini
Vaidya?s research stay at the University of Konstanz and Dr Miriam Butt for hosting her. We are also
thankful to Dr Miriam Butt, Dr Martha Palmer and Christian Rohrdantz for their feedback on this paper.
Any errors that remain are our own.
References
Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In
Proceedings of the International Conference on Computational Semantics (IWCS 2011).
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Sebastian Sulger. 2012. A Reference Dependency Bank for An-
alyzing Complex Predicates. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U?gur Do?gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Evaluation (LREC?12). European Language Resources Associ-
ation (ELRA), May.
Leslie Barrett and Anthony R Davis. 2003. Diagnostics for determining compatibility in English support-verb-
nominalization pairs. In Proceedings of the 4th international conference on Computational Linguistics and
Intelligent text processing (CICLing 03).
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A Multi-
Representational and Multi-Layered Treebank for Hindi/Urdu. In Proceedings of the Third Linguistic Annota-
tion Workshop, pages 186?189, Suntec, Singapore, August. Association for Computational Linguistics.
Pushpak Bhattacharyya. 2010. IndoWordNet. In Proceedings of the Seventh Conference on International Lan-
guage Resources and Evaluation (LREC?10), pages 3785?3792.
Miriam Butt and Tracy Holloway King. 2007. Urdu in a Parallel Grammar Development Environment. Lan-
guage Resources and Evaluation: Special Issue on Asian Language Processing: State of the Art Resources and
Processing, 41.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia Ni?no, and Fr?ed?erique Segond. 1999. A Grammar Writer?s
Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The Parallel
Grammar Project. In Proceedings of the COLING-2002 Workshop on Grammar Engineering and Evaluation,
pages 1?7.
Miriam Butt, Tina B?ogel, Annette Hautli, Sebastian Sulger, and Tafseer Ahmed. 2012. Identifying Urdu Complex
Predication via Bigram Extraction. In In Proceedings of COLING 2012, Technical Papers, pages 409 ? 424,
Mumbai, India.
Miriam Butt. 1995. The Structure of Complex Predicates in Urdu. CSLI Publications.
Miriam Butt. 2003. The Light Verb Jungle. Harvard Working Papers in Linguistics, 9.
Miriam Butt. 2010. The Light Verb Jungle: Still Hacking Away. In Mengistu Amberber, Brett Baker, and Mark
Harvey, editors, Complex Predicates in Cross-Linguistic Perspective. Cambridge University Press.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan, Tracy Holloway King, John T. Maxwell III, and Paula Newman,
2012. XLE Documentation. Palo Alto Research Center.
9
Mary Dalrymple, Ronald M. Kaplan, and Tracy Holloway King. 2004. Linguistic Generalizations over De-
scriptions. In Miriam Butt and Tracy Holloway King, editors, Proceedings of the LFG04 Conference. CSLI
Publications.
Alice Davison. 2005. Phrasal predicates: How N combines with V in Hindi/Urdu. In Tanmoy Bhattacharya,
editor, Yearbook of South Asian Languages and Linguistics, pages 83?116. Mouton de Gruyter.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and John T. Maxwell III. 1998. Optimality Theory Style Con-
straint Ranking in Large-scale LFG Grammars. In Proceedings of the LFG98 Conference. CSLI Publications.
Peter Hook. 1974. The Compound Verb in Hindi. Center for South and Southeast Asian Studies, University of
Michigan.
Muhammad Humayoun. 2006. Urdu Morphology, Orthography and Lexicon Extraction. Master?s thesis, Depart-
ment of Computing Science, Chalmers University of Technology.
Yamuna Kachru. 2006. Hindi. John Benjamins.
Andreas Lamprecht, Annette Hautli, Christian Rohrdantz, and Tina B?ogel. 2013. A Visual Analytics System
for Cluster Exploration. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 109?114, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Beth Levin. 1993. English Verb Classes and Alternations. A Preliminary Investigation. The University of Chicago
Press.
James B. MacQueen. 1967. Some Methods for Classification and Analysis of Multivariate Observations. In
Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281?297. University
of California Press.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sarmad
Hussain, and Miriam Butt. 2010. Transliterating Urdu for a Broad-Coverage Urdu/Hindi LFG Grammar. In
Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010).
Tara Mohanan. 1994. Argument Structure in Hindi. CSLI Publications.
Ryan North. 2005. Computational Measures of the Acceptability of Light Verb Constructions. Ph.D. thesis,
University of Toronto.
Alan Prince and Paul Smolensky. 2004. Optimality Theory: Constraint Interaction in Generative Grammar.
Blackwell Publishing.
Ghulam Raza. 2011. Subcategorization Acquisition and Classes of Predication in Urdu. Ph.D. thesis, University
of Konstanz.
Siva Reddy and Serge Sharoff. 2011. Cross Language POS Taggers (and other Tools) for Indian Languages:
An Experiment with Kannada using Telugu Resources. In Proceedings of the Fifth International Workshop On
Cross Lingual Information Access, pages 11?19, Chiang Mai, Thailand, November. Asian Federation of Natural
Language Processing.
Ruth Laila Schmidt. 1999. Urdu: An Essential Grammar. Routledge.
Shiva Taslimipoor, Afsaneh Fazly, and Ali Hamzeh. 2012. Using Noun Similarity to Adapt an Acceptability
Measure for Persian Light Verb Constructions. In Nicoletta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis,
editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC?12),
Istanbul, Turkey, may. European Language Resources Association (ELRA).
S. Urooj, F. Jabeen, F. Adeeba, R. Parveen., and S. Hussain. 2012. Urdu Digest Corpus. In Proceedings of the
Conference on Language and Technology 2012, Lahore, Pakistan.
Tim Van de Cruys. 2006. Semantic Clustering in Dutch. In Proceedings of the Sixteenth Computational Linguis-
tics in Netherlands (CLIN), pages 17?32.
10
