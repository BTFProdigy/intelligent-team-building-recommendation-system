Proceedings of the 12th Conference of the European Chapter of the ACL, pages 710?718,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
A General, Abstract Model of Incremental Dialogue Processing
David Schlangen
Department of Linguistics
University of Potsdam, Germany
das@ling.uni-potsdam.de
Gabriel Skantze?
Dept. of Speech, Music and Hearing
KTH, Stockholm, Sweden
gabriel@speech.kth.se
Abstract
We present a general model and concep-
tual framework for specifying architec-
tures for incremental processing in dia-
logue systems, in particular with respect
to the topology of the network of modules
that make up the system, the way informa-
tion flows through this network, how in-
formation increments are ?packaged?, and
how these increments are processed by the
modules. This model enables the precise
specification of incremental systems and
hence facilitates detailed comparisons be-
tween systems, as well as giving guidance
on designing new systems.
1 Introduction
Dialogue processing is, by its very nature, incre-
mental. No dialogue agent (artificial or natural)
processes whole dialogues, if only for the simple
reason that dialogues are created incrementally, by
participants taking turns. At this level, most cur-
rent implemented dialogue systems are incremen-
tal: they process user utterances as a whole and
produce their response utterances as a whole.
Incremental processing, as the term is com-
monly used, means more than this, however,
namely that processing starts before the input is
complete (e.g., (Kilger and Finkler, 1995)). Incre-
mental systems hence are those where ?Each pro-
cessing component will be triggered into activity
by a minimal amount of its characteristic input?
(Levelt, 1989). If we assume that the character-
istic input of a dialogue system is the utterance
(see (Traum and Heeman, 1997) for an attempt to
define this unit), we would expect an incremental
system to work on units smaller than utterances.
Our aim in the work presented here is to de-
scribe and give names to the options available to
?The work reported here was done while the second au-
thor was at the University of Potsdam.
designers of incremental systems. We define some
abstract data types, some abstract methods that
are applicable to them, and a range of possible
constraints on processing modules. The notions
introduced here allow the (abstract) specification
of a wide range of different systems, from non-
incremental pipelines to fully incremental, asyn-
chronous, parallel, predictive systems, thus mak-
ing it possible to be explicit about similarities and
differences between systems. We believe that this
will be of great use in the future development of
such systems, in that it makes clear the choices
and trade-offs one can make. While we sketch our
work on one such system, our main focus here
is on the conceptual framework. What we are
not doing here is to argue for one particular ?best
architecture??what this is depends on the particu-
lar aims of an implementation/model and on more
low-level technical considerations (e.g., availabil-
ity of processing modules).1
In the next section, we give some examples of
differences in system architectures that we want to
capture, with respect to the topology of the net-
work of modules that make up the system, the
way information flows through this network and
how the modules process information, in partic-
ular how they deal with incrementality. In Sec-
tion 3, we present the abstract model that under-
lies the system specifications, of which we give an
example in Section 4. We close with a brief dis-
cussion of related work.
2 Motivating Examples
Figure 1 shows three examples of module net-
works, representations of systems in terms of their
component modules and the connections between
them. Modules are represented by boxes, and con-
nections by arrows indicating the path and the di-
1As we are also not trying to prove properties of the spec-
ified systems here, the formalisations we give are not sup-
ported by a formal semantics here.
710
rection of information flow. Arrows not coming
from or going to modules represent the global in-
put(s) and output(s) to and from the system.
Figure 1: Module Network Topologies
One of our aims here is to facilitate exact and
concise description of the differences between
module networks such as in the example. Infor-
mally, the network on the left can be described as
a simple pipeline with no parallel paths, the one in
the middle as a pipeline enhanced with a parallel
path, and the one on the right as a star-architecture;
we want to be able to describe exactly the con-
straints that define each type of network.
A second desideratum is to be able to specify
how information flows in the system and between
the modules, again in an abstract way, without
saying much about the information itself (as the
nature of the information depends on details of
the actual modules). The directed edges in Fig-
ure 1 indicate the direction of information flow
(i.e., whose output is whose input); as an addi-
tional element, we can visualise parallel informa-
tion streams between modules as in Figure 2 (left),
where multiple hypotheses about the same input
increments are passed on. (This isn?t meant to
imply that there are three actual communications
channels active. As described below, we will en-
code the parallelism directly on the increments.)
One way such parallelism may occur in an in-
cremental dialogue system is illustrated in Fig-
ure 2 (right), where for some stretches of an input
signal (a sound wave), alternative hypotheses are
entertained (note that the boxes here do not repre-
sent modules, but rather bits of incremental infor-
mation). We can view these alternative hypothe-
Figure 2: Parallel Information Streams (left) and
Alternative Hypotheses (right)
Figure 3: Incremental Input mapped to (less) in-
cremental output
Figure 4: Example of Hypothesis Revision
ses about the same original signal as being paral-
lel to each other (with respect to the input they are
grounded in).
We also want to be able to specify the ways in-
cremental bits of input (?minimal amounts of char-
acteristic input?) can relate to incremental bits of
output. Figure 3 shows one possible configuration,
where over time incremental bits of input (shown
in the left column) accumulate before one bit of
output (in the right column) is produced. (As for
example in a parser that waits until it can com-
pute a major phrase out of the words that are its
input.) Describing the range of possible module
behaviours with respect to such input/output rela-
tions is another important element of the abstract
model presented here.
It is in the nature of incremental processing,
where output is generated on the basis of incom-
plete input, that such output may have to be re-
vised once more information becomes available.
Figure 4 illustrates such a case. At time-step t1,
the available frames of acoustic features lead the
processor, an automatic speech recogniser, to hy-
pothesize that the word ?four? has been spoken.
This hypothesis is passed on as output. However,
at time-point t2, as additional acoustic frames have
come in, it becomes clear that ?forty? is a bet-
ter hypothesis about the previous frames together
with the new ones. It is now not enough to just
output the new hypothesis: it is possible that later
modules have already started to work with the hy-
pothesis ?four?, so the changed status of this hy-
pothesis has to be communicated as well. This is
shown at time-step t3. Defining such operations
and the conditions under which they are necessary
711
is the final aim of our model.
3 The Model
3.1 Overview
We model a dialogue processing system in an ab-
stract way as a collection of connected processing
modules, where information is passed between the
modules along these connections. The third com-
ponent beside the modules and their connections is
the basic unit of information that is communicated
between the modules, which we call the incremen-
tal unit (IU). We will only characterise those prop-
erties of IUs that are needed for our purpose of
specifying different system types and basic oper-
ations needed for incremental processing; we will
not say anything about the actual, module specific
payload of these units.
The processing module itself is modelled as
consisting of a Left Buffer (LB), the Processor
proper, and a Right Buffer (RB). When talking
about operations of the Processor, we will some-
times use Left Buffer-Incremental Unit (LB-IU)
for units in LB and Right Buffer-Incremental Unit
(RB-IU) for units in RB.
This setup is illustrated in Figure 4 above. IUs
in LB (here, acoustic frames as input to an ASR)
are consumed by the processor (i.e., is processed),
which creates an internal result, in the case shown
here, this internal result is posted as an RB-IU only
after a series of LB-IUs have accumulated. In our
descriptions below, we will abstract away from the
time processing takes and describe Processors as
relations between (sets of) LBs and RBs.
We begin our description of the model with the
specification of network topologies.
3.2 Network Topology
Connections between modules are expressed
through connectedness axioms which simply state
that IUs in one module?s right buffer are also in
another buffer?s left buffer. (Again, in an imple-
mented system communication between modules
will take time, but we abstract away from this
here.) This connection can also be partial or fil-
tered. For example, ?x(x ? RB1 ? NP (x) ?
x ? LB2) expresses that all and only NPs in mod-
ule one?s right buffer appear in module two?s left
buffer. If desired, a given RB can be connected to
more than one LB, and more than one RB can feed
into the same LB (see the middle example in Fig-
ure 1). Together, the set of these axioms define the
network topology of a concrete system. Different
topology types can then be defined through con-
straints on module sets and their connections. I.e.,
a pipeline system is one in which it cannot hap-
pen that an IU is in more than one right buffer and
more than one left buffer.
Note that we are assuming token identity here,
and not for example copying of data struc-
tures. That is, we assume that it indeed is the
same IU that is in the left and right buffers
of connected modules. This allows a spe-
cial form of bi-directionality to be implemented,
namely one where processors are allowed to make
changes to IUs in their buffers, and where these
changes automatically percolate through the net-
work. This is different to and independent of
the bi-directionality that can be expressed through
connectedness axioms.
3.3 Incremental Units
So far, all we have said about IUs is that they are
holding a ?minimal amount of characteristic input?
(or, of course, a minimal amount of characteris-
tic output, which is to be some other module?s in-
put). Communicating just these minimal informa-
tion bits is enough only for the simplest kind of
system that we consider, a pipeline with only a
single stream of information and no revision. If
more advanced features are desired, there needs to
be more structure to the IUs. In this section we de-
fine what we see as the most complete version of
IUs, which makes possible operations like hypoth-
esis revision, prediction, and parallel hypothesis
processing. (These operations will be explained in
the next section.) If in a particular system some of
these operations aren?t required, some of the struc-
ture on IUs can be simplified.
Informally, the representational desiderata are
as follows. First, we want to be able to repre-
sent relations between IUs produced by the same
processor. For example, in the output of an ASR,
two word-hypothesis IUs may stand in a succes-
sor relation, meaning that word 2 is what the ASR
takes to be the continuation of the utterance be-
gun with word 1. In a different situation, word 2
may be an alternative hypothesis about the same
stretch of signal as word 1, and here a different re-
lation would hold. The incremental outputs of a
parser may be related in yet another way, through
dominance: For example, a newly built IU3, rep-
resenting a VP, may want to express that it links
712
via a dominance relation to IU1, a V, and IU2, an
NP, which were both posted earlier. What is com-
mon to all relations of this type is that they relate
IUs coming from the same processor(s); we will
in this case say that the IUs are on the same level.
Information about these same level links will be
useful for the consumers of IUs. For example, a
parsing module consuming ASR-output IUs will
need to do different things depending on whether
an incoming IU continues an utterance or forms an
alternative hypothesis to a string that was already
parsed.
The second relation between IUs that we want
to capture cuts across levels, by linking RB-IUs to
those LB-IUs that were used by the processor to
produce them. For this we will say that the RB-IU
is grounded in LB-IU(s). This relation then tracks
the flow of information through the modules; fol-
lowing its transitive closure one can go back from
the highest level IU, which is output by the sys-
tem, to the input IU or set of input IUs on which it
is ultimately grounded. The network spanned by
this relation will be useful in implementing the re-
vision process mentioned above when discussing
Figure 4, where the doubt about a hypothesis must
spread to all hypotheses grounded in it.
Apart from these relations, we want IUs to carry
three other types of information: a confidence
score representing the confidence its producer had
in it being accurate; a field recording whether revi-
sions of the IU are still to be expected or not; and
another field recording whether the IU has already
been processed by consumers, and if so, by whom.
Formally, we define IUs as tuples IU =
?I,L,G,T , C,S,P?, where
? I is an identifier, which has to be unique for
each IU over the lifetime of a system. (That
is, at no point in the system?s life can there be
two or more IUs with the same ID.)
? L is the same level link, holding a statement
about how, if at all, the given IU relates to
other IUs at the same level, that is, to IUs pro-
duced by the same processor. If an IU is not
linked to any other IU, this slot holds the spe-
cial value ?.
The definition demands that the same level
links of all IUs belonging to the same larger
unit form a graph; the type of the graph will
depend on the purposes of the sending and
consuming module(s). For a one-best output
of an ASR it might be enough for the graph
to be a chain, whereas an n-best output might
be better represented as a tree (with all first
words linked to ?) or even a lattice (as in
Figure 2 (right)); the output of a parser might
require trees (possibly underspecified).
? G is the grounded in field, holding an ordered
list of IDs pointing to those IUs out of which
the current IU was built. For example, an IU
holding a (partial) parse might be grounded
in a set of word hypothesis IUs, and these in
turn might be grounded in sets of IUs holding
acoustic features. While the same level link
always points to IUs on the same level, the
grounded in link always points to IUs from
a previous level.2 The transitive closure of
this relation hence links system output IUs to
a set of system input IUs. For convenience,
we may define a predicate supports(x,y) for
cases where y is grounded in x; and hence
the closure of this relation links input-IUs to
the output that is (eventually) built on them.
This is also the hook for the mechanism that
realises the revision process described above
with Figure 4: if a module decides to re-
voke one of its hypotheses, it sets its confi-
dence value (see below) to 0; on noticing this
event, all consuming modules can then check
whether they have produced RB-IUs that link
to this LB-IU, and do the same for them. In
this way, information about revision will au-
tomatically percolate through the module net-
work.
Finally, an empty grounded in field can also
be used to trigger prediction: if an RB-IU has
an empty grounded in field, this can be under-
stood as a directive to the processor to find
evidence for this IU (i.e., to prove it), using
the information in its left buffer.
? T is the confidence (or trust) slot, through
which the generating processor can pass on
its confidence in its hypothesis. This then can
have an influence on decisions of the con-
suming processor. For example, if there are
parallel hypotheses of different quality (con-
fidence), a processor may decide to process
2The link to the previous level may be indirect. E.g.,
for an IU holding a phrase that is built out of previously
built phrases (and not words), this link may be expressed by
pointing to the same level link, meaning something like ?I?m
grounded in whatever the IUs are grounded in that I link to
on the same level link, and also in the act of combination that
is expressed in that same level link?.
713
(and produce output for) the best first.
A special value (e.g., 0, or -1) can be defined
to flag hypotheses that are being revoked by
a processor, as described above.
? C is the committed field, holding a Boolean
value that indicates whether the producing
module has committed to the IU or not, that
is, whether it guarantees that it will never re-
voke the IU. See below for a discussion of
how such a decision may be made, and how
it travels through the module network.
? S is the seen field. In this field consum-
ing processors can record whether they have
?looked at??that is, attempted to process?
the IU. In the simplest case, the positive fact
can be represented simply by adding the pro-
cessor ID to the list; in more complicated
setups one may want to offer status infor-
mation like ?is being processed by module
ID? or ?no use has been found for IU by
module ID?. This allows processors both to
keep track of which LB-IUs they have al-
ready looked at (and hence, to more easily
identify new material that may have entered
their LB) and to recognise which of its RB-
IUs have been of use to later modules, infor-
mation which can then be used for example
to make decisions on which hypothesis to ex-
pand next.
? P finally is the actual payload, the module-
specific unit of ?characteristic input?, which
is what is processed by the processor in order
to produce RB-IUs.
It will also be useful later to talk about the com-
pleteness of an IU (or of sets of IUs). This we de-
fine informally as its relation to (the type of) what
would count as a maximal input or output of the
module. For example, for an ASR module, such
maximally complete input may be the recording of
the whole utterance, for the parser maximal out-
put may be a parse of type sentence (as opposed
to one of type NP, for example).3 This allows us
to see non-incremental systems as a special case
of incremental systems, namely those with only
maximally complete IUs, which are always com-
mitted.
3This definition will only be used for abstractly classify-
ing modules. Practically, it is of course rarely possible to
know how complete or incomplete the already seen part of
an ongoing input is. Investigating how a dialogue system can
better predict completion of an utterance is in fact one of the
aims of the project in which this framework was developed.
3.4 Modules
3.4.1 Operations
We describe in this section operations that the pro-
cessors may perform on IUs. We leave open how
processors are triggered into action, we simply as-
sume that on receiving new LB-IUs or noticing
changes to LB or RB-IUs, they will eventually per-
form these operations. Again, we describe here the
complete set of operations; systems may differ in
which subset of the functions they implement.
purge LB-IUs that are revoked by their producer
(by having their confidence score set to the special
value) must be purged from the internal state of the
processor (so that they will not be used in future
updates) and all RB-IUs grounded in them must
be revoked as well.
Some reasons for revoking hypotheses have al-
ready been mentioned. For example, a speech
recogniser might decide that a previously output
word hypothesis is not valid anymore (i.e., is not
anymore among the n-best that are passed on). Or,
a parser might decide in the light of new evidence
that a certain structure it has built is a dead end,
and withdraw support for it. In all these cases, all
?later? hypotheses that build on this IU (i.e., all hy-
potheses that are in the transitive closure of this
IU?s support relation) must be purged. If all mod-
ules implement the purge operation, this revision
information will be guaranteed to travel through
the network.
update New LB-IUs are integrated into the in-
ternal state, and eventually new RB-IUs are built
based on them (not necessarily in the same fre-
quency as new LB-IUs are received; see Figure 3
above, and discussion below). The fields of the
new RB-IUs (e.g., the same level links and the
grounded in pointers) are filled appropriately. This
is in some sense the basic operation of a processor,
and must be implemented in all useful systems.
We can distinguish two implementation strate-
gies for dealing with updates: a) all state is thrown
away and results are computed again for the whole
input set. The result must then be compared with
the previous result to determine what the new out-
put increment is. b) The new information is in-
tegrated into internal state, and only the new out-
put increment is produced. For our purposes here,
we can abstract away from these differences and
assume that only actual increments are commu-
nicated. (Practically, it might be an advantage to
keep using an existing processor and just wrap it
714
into a module that computes increments by differ-
ences.)
We can also distinguish between modules along
another dimension, namely based on which types
of updates are allowed. To do so, we must first
define the notion of a ?right edge? of a set of
IUs. This is easiest to explain for strings, where
the right edge simply is the end of the string, or
for a lattice, where it is the (set of) smallest ele-
ment(s). A similar notion may be defined for trees
as well (compare the ?right frontier constraint?
of Polanyi (1988)). If now a processor only ex-
pects IUs that extend the right frontier, we can
follow Wire?n (1992) in saying that it is only left-
to-right incremental. Within what Wire?n (1992)
calls fully incremental, we can make more dis-
tinctions, namely according to whether revisions
(as described above) and/or insertions are allowed.
The latter can easily be integrated into our frame-
work, by allowing same-level links to be changed
to fit new IUs into existing graphs.
Processors can take supports information into
account when deciding on their update order. A
processor might for example decide to first try to
use the new information (in its LB) to extend struc-
tures that have already proven useful to later mod-
ules (that is, that support new IUs). For example,
a parser might decide to follow an interpretation
path that is deemed more likely by a contextual
processing module (which has grounded hypothe-
ses in the partial path). This may result in better
use of resources?the downside of such a strategy
of course is that modules can be garden-pathed.4
Update may also work towards a goal. As men-
tioned above, putting ungrounded IUs in a mod-
ule?s RB can be understood as a request to the
module to try to find evidence for it. For exam-
ple, the dialogue manager might decide based on
the dialogue context that a certain type of dialogue
act is likely to follow. By requesting the dialogue
act recognition module to find evidence for this
hypothesis, it can direct processing resources to-
wards this task. (The dialogue recognition mod-
ule then can in turn decide on which evidence it
would like to see, and ask lower modules to prove
this. Ideally, this could filter down to the interface
module, the ASR, and guide its hypothesis form-
ing. Technically, something like this is probably
easier to realise by other means.)
4It depends on the goals behind building the model
whether this is considered a downside or desired behaviour.
We finally note that in certain setups it may be
necessary to consume different types of IUs in one
module. As explained above, we allow more than
one module to feed into another modules LB. An
example where something like this could be useful
is in the processing of multi-modal information,
where information about both words spoken and
gestures performed may be needed to compute an
interpretation.
commit There are three ways in which a proces-
sor may have to deal with commits. First, it can
decide for itself to commit RB-IUs. For example,
a parser may decide to commit to a previously built
structure if it failed to integrate into it a certain
number of new words, thus assuming that the pre-
vious structure is complete. Second, a processor
may notice that a previous module has committed
to IUs in its LB. This might be used by the proces-
sor to remove internal state kept for potential re-
visions. Eventually, this commitment of previous
modules might lead the processor to also commit
to its output, thus triggering a chain of commit-
ments.
Interestingly, it can also make sense to let com-
mits flow from right to left. For example, if the
system has committed to a certain interpretation
by making a publicly observable action (e.g., an
utterance, or a multi-modal action), this can be
represented as a commit on IUs. This information
would then travel down the processing network;
leading to the potential for a clash between a re-
voke message coming from the left and the com-
mit directive from the right. In such a case, where
the justification for an action is revoked when the
action has already been performed, self-correction
behaviours can be executed.5
3.4.2 Characterising Module Behaviour
It is also useful to be able to abstractly describe the
relation between LB-IUs and RB-IUs in a module
or a collection of modules. We do this here along
the dimensions update frequency, connectedness
and completeness.
Update Frequency The first dimension we con-
sider here is that of how the update frequency of
LB-IUs relates to that of (connected) RB-IUs.
We write f:in=out for modules that guarantee
that every new LB-IU will lead to a new RB-IU
5In future work, we will explore in more detail if and
how through the implementation of a self-monitoring cycle
and commits and revokes the various types of dysfluencies
described for example by Levelt (1989) can be modelled.
715
(that is grounded in the LB-IU). In such a setup,
the consuming module lags behind the sending
module only for exactly the time it needs to pro-
cess the input. Following Nivre (2004), we can
call this strict incrementality.
f:in?out describes modules that potentially col-
lect a certain amount of LB-IUs before producing
an RB-IU based on them. This situation has been
depicted in Figure 3 above.
f:in?out characterises modules that update RB
more often than their LB is updated. This could
happen in modules that produce endogenic infor-
mation like clock signals, or that produce contin-
uously improving hypotheses over the same input
(see below), or modules that ?expand? their input,
like a TTS that produces audio frames.
Connectedness We may also want to distin-
guish between modules that produce ?island? hy-
potheses that are, at least when initially posted, not
connected via same level links to previously out-
put IUs, and those that guarantee that this is not
the case. For example, to achieve an f:in=out be-
haviour, a parser may output hypotheses that are
not connected to previous hypotheses, in which
case we may call the hypotheses ?unconnected?.
Conversely, to guarantee connectedness, a parsing
module might need to accumulate input, resulting
in an f:in?out behaviour.6
Completeness Building on the notion of com-
pleteness of (sets of) IUs introduced above, we
can also characterise modules according to how
the completeness of LB and RB relates.
In a c:in=out-type module, the most complete
RB-IU (or set of RB-IUs) is only as complete as
the most complete (set of) LB-IU(s). That is, the
module does not speculate about completions, nor
does it lag behind. (This may technically be diffi-
cult to realise, and practically not very relevant.)
More interesting is the difference between the
following types: In a c:in?out-type module, the
most complete RB-IU potentially lags behind the
most complete LB-IU. This will typically be the
case in f:in?out modules. c:in?out-type mod-
ules finally potentially produce output that is more
complete than their input, i.e., they predict contin-
uations. An extreme case would be a module that
always predicts complete output, given partial in-
put. Such a module may be useful in cases where
6The notion of connectedness is adapted from Sturt and
Lombardo (2005), who provide evidence that the human
parser strives for connectedness.
modules have to be used later in the processing
chain that can only handle complete input (that is,
are non-incremental); we may call such a system
prefix-based predictive, semi-incremental.
With these categories in hand, we can make
further distinctions within what Dean and Boddy
(1988) call anytime algorithms. Such algorithms
are defined as a) producing output at any time,
which however b) improves in quality as the al-
gorithm is given more time. Incremental mod-
ules by definition implement a reduced form of
a): they may not produce an output at any
time, but they do produce output at more times
than non-incremental modules. This output then
also improves over time, fulfilling condition b),
since more input becomes available and either
the guesses the module made (if it is a c:out?in
module) will improve or the completeness in
general increases (as more complete RB-IUs are
produced). Processing modules, however, can
also be anytime algorithms in a more restricted
sense, namely if they continuously produce new
and improved output even for a constant set of
LB-IUs, i.e. without changes on the input side.
(Which would bring them towards the f:out?in be-
haviour.)
3.5 System Specification
Combining all these elements, we can finally de-
fine a system specification as the following:
? A list of modules that are part of the system.
? For each of those a description in terms
of which operations from Section 3.4.1 the
module implements, and a characterisation of
its behaviour in the terms of Section 3.4.2.
? A set of axioms describing the connections
between module buffers (and hence the net-
work topology), as explained in Section 3.2.
? Specifications of the format of the IUs that
are produced by each module, in terms of the
definition of slots in Section 3.3.
4 Example Specification
We have built a fully incremental dialogue system,
called NUMBERS (for more details see Skantze
and Schlangen (2009)), that can engage in dia-
logues in a simple domain, number dictation. The
system can not only be described in the terms ex-
plained here, but it also directly instantiates some
of the data types described here.
716
Figure 5: The NUMBERS System Architecture
(CA = communicative act)
The module network topology of the system is
shown in Figure 5. This is pretty much a stan-
dard dialogue system layout, with the exception
that prosodic analysis is done in the ASR and that
dialogue management is divided into a discourse
modelling module and an action manager. As can
be seen in the figure, there is also a self-monitoring
feedback loop?the system?s actions are sent from
the TTS to the discourse modeller. The system
has two modules that interface with the environ-
ment (i.e., are system boundaries): the ASR and
the TTS.
A single hypothesis chain connects the mod-
ules (that is, no two same level links point to the
same IU). Modules pass messages between them
that can be seen as XML-encodings of IU-tokens.
Information strictly flows from LB to RB. All IU
slots except seen (S) are realised. The purge and
commit operations are fully implemented. In the
ASR, revision occurs as already described above
with Figure 4, and word-hypothesis IUs are com-
mitted (and the speech recognition search space is
cleared) after 2 seconds of silence are detected.
(Note that later modules work with all IUs from
the moment that they are sent, and do not have
to wait for them being committed.) The parser
may revoke its hypotheses if the ASR revokes the
words it produces, but also if it recovers from a
?garden path?, having built and closed off a larger
structure too early. As a heuristic, the parser
waits until a syntactic construct is followed by
three words that are not part of it until it com-
mits. For each new discourse model increment,
the action manager may produce new communica-
tive acts (CAs), and possibly revoke previous ones
that have become obsolete. When the system has
spoken a CA, this CA becomes committed, which
is recorded by the discourse modeller.
No hypothesis testing is done (that is, no un-
grounded information is put on RBs). All modules
have a f:in?out; c:in?out characteristic.
The system achieves a very high degree of
responsiveness?by using incremental ASR and
prosodic analysis for turn-taking decisions, it can
react in around 200ms when suitable places for
backchannels are detected, which should be com-
pared to a typical minimum latency of 750ms
in common systems where only a simple silence
threshold is used.
5 Related Work, Future Work
The model described here is inspired partially by
Young et al (1989)?s token passing architecture;
our model can be seen as a (substantial) general-
isation of the idea of passing smaller information
bits around, out of the domain of ASR and into the
system as a whole. Some of the characterisations
of the behaviour of incremental modules were in-
spired by Kilger and Finkler (1995), but again we
generalised the definitions to fit all kinds of incre-
mental modules, not just generation.
While there recently have been a number of
papers about incremental systems (e.g., (DeVault
and Stone, 2003; Aist et al, 2006; Brick and
Scheutz, 2007)), none of those offer general con-
siderations about architectures. (Despite its title,
(Aist et al, 2006) also only describes one particu-
lar setup.)
In future work, we will give descriptions of
these systems in the terms developed here. We
are also currently exploring how more cognitively
motivated models such as that of generation by
Levelt (1989) can be specified in our model. A
further direction for extension is the implementa-
tion of modality fusion as IU-processing. Lastly,
we are now starting to work on connecting the
model for incremental processing and ground-
ing of interpretations in previous processing re-
sults described here with models of dialogue-level
grounding in the information-state update tradi-
tion (Larsson and Traum, 2000). The first point
of contact here will be the investigation of self-
corrections, as a phenomenon that connects sub-
utterance processing and discourse-level process-
ing (Ginzburg et al, 2007).
Acknowledgments This work was funded by a grant in the
DFG Emmy Noether Programme. Thanks to Timo Baumann
and Michaela Atterer for discussion of the ideas reported
here, and to the anonymous reviewers for their very detailed
and helpful comments.
717
References
G.S. Aist, J. Allen, E. Campana, L. Galescu, C.A.
Gomez Gallo, S. Stoness, M. Swift, and M Tanen-
haus. 2006. Software architectures for incremental
understanding of human speech. In Proceedings of
the International Conference on Spoken Language
Processing (ICSLP), Pittsburgh, PA, USA, Septem-
ber.
Timothy Brick and Matthias Scheutz. 2007. Incremen-
tal natural language processing for HRI. In Proceed-
ings of the Second ACM IEEE International Confer-
ence on Human-Robot Interaction, pages 263?270,
Washington, DC, USA.
Thomas Dean and Mark Boddy. 1988. An analysis of
time-dependent planning. In Proceedings of AAAI-
88, pages 49?54. AAAI.
David DeVault and Matthew Stone. 2003. Domain
inference in incremental interpretation. In Proceed-
ings of ICOS 4: Workshop on Inference in Computa-
tional Semantics, Nancy, France, September. INRIA
Lorraine.
Jonathan Ginzburg, Raquel Ferna?ndez, and David
Schlangen. 2007. Unifying self- and other-repair.
In Proceeding of DECALOG, the 11th International
Workshop on the Semantics and Pragmatics of Dia-
logue (SemDial07), Trento, Italy, June.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical
Report RR-95-11, DFKI, Saarbru?cken, Germany.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language Engi-
neering, pages 323?340.
Willem J.M. Levelt. 1989. Speaking. MIT Press,
Cambridge, USA.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. pages 50?57, Barcelona,
Spain, July.
Livia Polanyi. 1988. A formal model of the structure
of discourse. Journal of Pragmatics, 12:601?638.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL 2009), Athens, Greece, April.
Patrick Sturt and Vincenzo Lombardo. 2005. Process-
ing coordinated structures: Incrementality and con-
nectedness. Cognitive Science, 29:291?305.
D. Traum and P. Heeman. 1997. Utterance units in
spoken dialogue. In E. Maier, M. Mast, and S. Lu-
perFoy, editors, Dialogue Processing in Spoken Lan-
guage Systems, Lecture Notes in Artificial Intelli-
gence. Springer-Verlag.
Mats Wire?n. 1992. Studies in Incremental Natural
Language Analysis. Ph.D. thesis, Linko?ping Uni-
versity, Linko?ping, Sweden.
S.J. Young, N.H. Russell, and J.H.S. Thornton. 1989.
Token passing: a conceptual model for con-
nected speech recognition systems. Technical re-
port CUED/FINFENG/TR 38, Cambridge Univer-
sity Engineering Department.
718
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 745?753,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Incremental Dialogue Processing in a Micro-Domain 
 
 
 Gabriel Skantze1 
 Dept. of Speech, Music and Hearing 
KTH, Stockholm, Sweden 
gabriel@speech.kth.se 
 David Schlangen 
Department of Linguistics 
University of Potsdam, Germany  
das@ling.uni-potsdam.de  
  
  
 
Abstract 
This paper describes a fully incremental dia-
logue system that can engage in dialogues 
in a simple domain, number dictation. Be-
cause it uses incremental speech recognition 
and prosodic analysis, the system can give 
rapid feedback as the user is speaking, with 
a very short latency of around 200ms.  Be-
cause it uses incremental speech synthesis 
and self-monitoring, the system can react to 
feedback from the user as the system is 
speaking. A comparative evaluation shows 
that na?ve users preferred this system over a 
non-incremental version, and that it was 
perceived as more human-like. 1 
1 Introduction 
A traditional simplifying assumption for spoken 
dialogue systems is that the dialogue proceeds 
with strict turn-taking between user and system. 
The minimal unit of processing in such systems 
is the utterance, which is processed in whole by 
each module of the system before it is handed on 
to the next. When the system is speaking an ut-
terance, it assumes that the user will wait for it to 
end before responding. (Some systems accept 
barge-ins, but then treat the interrupted utterance 
as basically unsaid.) 
Obviously, this is not how natural human-
human dialogue proceeds. Humans understand 
and produce language incrementally ? they use 
multiple knowledge sources to determine when it 
is appropriate to speak, they give and receive 
backchannels in the middle of utterances, they 
start to speak before knowing exactly what to 
say, and they incrementally monitor the listener?s 
reactions to what they say (Clark, 1996).  
                                                          
1
 The work reported in this paper was done while the first 
author was at the University of Potsdam. 
This paper presents a dialogue system, called 
NUMBERS, in which all components operate in-
crementally. We had two aims: First, to explore 
technical questions such as how the components 
of a modularized dialogue system should be ar-
ranged and made to interoperate to support in-
cremental processing, and which requirements 
incremental processing puts on dialogue system 
components (e.g., speech recognition, prosodic 
analysis, parsing, discourse modelling, action 
selection and speech synthesis).  Second, to in-
vestigate whether incremental processing can 
help us to better model certain aspects of human 
behaviour in dialogue systems ? especially turn-
taking and feedback ? and whether this improves 
the user?s experience of using such a system.   
2 Incremental dialogue processing  
All dialogue systems are ?incremental?, in some 
sense ? they proceed in steps through the ex-
change of ?utterances?. However, incremental 
processing typically means more than this; a 
common requirement is that processing starts 
before the input is complete and that the first 
output increments are produced as soon as possi-
ble (e.g., Kilger & Finkler, 1995). Incremental 
modules hence are those where ?Each processing 
component will be triggered into activity by a 
minimal amount of its characteristic input? 
(Levelt, 1989). If we assume that the ?character-
istic input? of a dialogue system is the utterance, 
this principle demands that ?minimal amounts? of 
an utterance already trigger activity. It should be 
noted though, that there is a trade-off between 
responsiveness and output quality, and that an 
incremental process therefore should produce 
output only as soon as it is possible to reach a 
desired output quality criterion.  
2.1 Motivations & related work 
The claim that humans do not understand and 
produce speech in utterance-sized chunks, but 
745
rather incrementally, can be supported by an 
impressive amount of psycholinguistic literature 
on the subject (e.g., Tanenhaus & Brown-
Schmidt, 2008; Levelt, 1989). However, when it 
comes to spoken dialogue systems, the dominant 
minimal unit of processing has been the utter-
ance. Moreover, traditional systems follow a 
very strict sequential processing order of utter-
ances ? interpretation, dialogue management, 
generation ? and there is most often no monitor-
ing of whether (parts of) the generated message 
is successfully delivered.  
Allen et al (2001) discuss some of the short-
comings of these assumptions when modelling 
more conversational human-like dialogue. First, 
they fail to account for the frequently found mid-
utterance reactions and feedback (in the form of 
acknowledgements, repetition of fragments or 
clarification requests). Second, people often 
seem to start to speak before knowing exactly 
what to say next (possibly to grab the turn), thus 
producing the utterance incrementally. Third, 
when a speaker is interrupted or receives feed-
back in the middle of an utterance, he is able to 
continue the utterance from the point where he 
was interrupted.  
Since a non-incremental system needs to proc-
ess the whole user utterance using one module at 
a time, it cannot utilise any higher level informa-
tion for deciding when the user?s turn or utter-
ance is finished, and typically has to rely only on 
silence detection and a time-out. Silence, how-
ever, is not a good indicator: sometimes there is 
silence but no turn-change is intended (e.g., hesi-
tations), sometimes there isn?t silence, but the 
turn changes (Sacks et al, 1974). Speakers ap-
pear to use other knowledge sources, such as 
prosody, syntax and semantics to detect or even 
project the end of the utterance. Attempts have 
been made to incorporate such knowledge 
sources for turn-taking decisions in spoken dia-
logue systems (e.g., Ferrer et al, 2002; Raux & 
Eskenazi, 2008). To do so, incremental dialogue 
processing is clearly needed. 
Incremental processing can also lead to better 
use of resources, since later modules can start to 
work on partial results and do not have to wait 
until earlier modules have completed processing 
the whole utterance. For example, while the 
speech recogniser starts to identify words, the 
parser can already add these to the chart. Later 
modules can also assist in the processing and for 
example resolve ambiguities as they come up. 
Stoness et al (2004) shows how a reference reso-
lution module can help an incremental parser 
with NP suitability judgements. Similarly, Aist et 
al. (2006) shows how a VP advisor could help an 
incremental parser.  
On the output side, an incremental dialogue 
system could monitor what is actually happening 
to the utterance it produces. As discussed by 
Raux & Eskenazi (2007), most dialogue manag-
ers operate asynchronously from the output com-
ponents, which may lead to problems if the 
dialogue manager produces several actions and 
the user responds to one of them. If the input 
components do not have any information about 
the timing of the system output, they cannot re-
late them to the user?s response. This is even 
more problematic if the user reacts (for example 
with a backchannel) in the middle of system 
utterances. The system must then relate the 
user?s response to the parts of its planned output 
it has managed to realise, but also be able to stop 
speaking and possibly continue the interrupted 
utterance appropriately. A solution for handling 
mid-utterance responses from the user is pro-
posed by Dohsaka & Shimazu (1997). For in-
cremental generation and synthesis, the output 
components must also cope with the problem of 
revision (discussed in more detail below), which 
may for example lead to the need for the genera-
tion of speech repairs, as discussed by Kilger & 
Finkler (1995). 
As the survey above shows, a number of stud-
ies have been done on incrementality in different 
areas of language processing. There are, how-
ever, to our knowledge no studies on how the 
various components could or should be inte-
grated into a complete, fully incremental dia-
logue system, and how such a system might be 
perceived by na?ve users, compared to a non-
incremental system. This we provide here. 
2.2 A general, abstract model 
The NUMBERS system presented in this paper can 
be seen as a specific instance (with some simpli-
fying assumptions) of a more general, abstract 
model that we have developed (Schlangen & 
Skantze, 2009). We will here only briefly de-
scribe the parts of the general model that are 
relevant for the exposition of our system. 
We model the dialogue processing system as a 
collection of connected processing modules. The 
smallest unit of information that is communi-
cated along the connections is called the incre-
mental unit (IU), the unit of the ?minimal 
amount of characteristic input?. Depending on 
what the module does, IUs may be audio frames, 
words, syntactic phrases, communicative acts, 
746
etc. The processing module itself is modelled as 
consisting of a Left Buffer (LB), the Processor 
proper, and a Right Buffer (RB). An example of 
two connected modules is shown in Figure 1. As 
IU1 enters the LB of module A, it may be con-
sumed by the processor. The processor may then 
produce new IUs, which are posted on the RB 
(IU2 in the example). As the example shows, the 
modules in the system are connected so that an 
IU posted on the RB in one module may be con-
sumed in the LB of another module. One RB 
may of course be connected to many other LB?s, 
and vice versa, allowing a range of different 
network topologies.    
 
 
Figure 1: Two connected modules. 
 
In the NUMBERS system, information is only 
allowed to flow from left to right, which means 
that the LB may be regarded as the input buffer 
and the RB as the output buffer. However, in the 
general model, information may flow in both 
directions.  
A more concrete example is shown in Figure 
2, which illustrates a module that does incre-
mental speech recognition. The IUs consumed 
from the LB are audio frames, and the IUs posted 
in the RB are the words that are recognised.  
 
 
Figure 2: Speech recognition as an example of incre-
mental processing. 
 
We identify three different generic module 
operations on IUs: update, purge and commit. 
First, as an IU is added to the LB, the processor 
needs to update its internal state. In the example 
above, the speech recogniser has to continuously 
add incoming audio frames to its internal state, 
and as soon as the recogniser receives enough 
audio frames to decide that the word ?four? is a 
good-enough candidate, the IU holding this word 
will be put on the RB (time-point t1). If a proces-
sor only expects IUs that extend the rightmost IU 
currently produced, we can follow Wir?n (1992) 
in saying that it is only left-to-right incremental.  
A fully incremental system (which we aim at 
here), on the other hand, also allows insertions 
and/or revisions.  
An example of revision is illustrated at time-
point t2 in Figure 2.  As more audio frames are 
consumed by the recogniser, the word ?four? is 
no longer the best candidate for this stretch of 
audio. Thus, the module must now revoke the IU 
holding the word ?four? (marked with a dotted 
outline) and add a new IU for the word ?forty?. 
All other modules consuming these IUs must 
now purge them from their own states and pos-
sibly revoke other IUs. By allowing revision, a 
module may produce tentative results and thus 
make the system more responsive. 
As more audio frames are consumed in the ex-
ample above, a new word ?five? is identified and 
added to the RB (time-point t3). At time-point t4, 
no more words are identified, and the module 
may decide to commit to the IUs that it has pro-
duced (marked with a darker shade). A commit-
ted IU is guaranteed to not being revoked later, 
and can hence potentially be removed from the 
processing window of later modules, freeing up 
resources. 
3 Number dictation: a micro-domain 
Building a fully incremental system with a be-
haviour more closely resembling that of human 
dialogue participants raises a series of new chal-
lenges. Therefore, in order to make the task more 
feasible, we have chosen a very limited domain ? 
what might be called a micro-domain (cf. Edlund 
et al, 2008): the dictation of number sequences. 
In this scenario, the user dictates a sequence of 
numbers (such as a telephone number or a credit 
card number) to the dialogue system. This is a 
very common situation in commercial telephone-
based dialogue systems, which however operate 
in a non-incremental manner: The user is first 
asked to read out the whole number sequence, 
which the system then confirms. Should the rec-
ognition be incorrect, the user has to repeat the 
whole sequence again. In an incremental version 
of this scenario, the system might give continu-
ous feedback (such as acknowledgements and 
clarification requests) as the user is reading the 
forty five 
forty five 
four t1
forty
 
four
 
forty 
forty five 
forty five 
t2
t3
t4
time left buffer processor right buffer 
four
 
IU2 
left buffer 
processor right buffer 
right buffer processor 
left buffer 
IU1 IU1 IU2 
IU3 IU2 
IU3 
module A 
module B
747
sequence. When the system repeats the sequence, 
the user is also given the opportunity to give 
feedback and make corrections at any time. Ob-
viously, such a dialogue will result in many more 
speaker-shifts, which in turn increases the need 
for the rapid turn-taking. Thus, while the number 
dictation task might appear simple at a first 
glance (very limited vocabulary, simple syntax 
and semantics), there are several issues ? not 
typically considered in most dialogue systems ? 
which an incremental and more human-like dia-
logue system must address. 
To explore dialogue phenomena in this do-
main, we gave 8 pairs of subjects the task of 
dictating number sequences to each other. The 
subjects were sitting in different rooms and used 
headsets to communicate. To induce more feed-
back behaviour, noise was randomly inserted in 
the utterances from the information giver (as in 
the setting used by Schlangen & Fern?ndez 
(2007)). The subjects were all speaking German 
(their native tongue). The recorded dialogues 
were then transcribed and annotated. The ob-
served dialogue strategies directly influenced the 
design of our system (see below). Table 1 shows 
an example of a dialogue between a user and the 
system and illustrates some of the phenomena we 
intended to model. 
 
U.01 two five three - 
S.02 okay 
U.03 one forty, five - 
S.04 sorry, do you mean one and then forty and 
then five? 
U.05 no, one forty five 
S.06 okay, one forty five 
U.07 yeah, and six eight four - 
S.08 okay 
U.09 seven twelve nine. 
S.10 so, from the start, two five three - 
U.11 yeah 
S.12 one forty five - 
S.13 six six four - 
U.14 no, six eight four 
S.15 okay, six eight four - 
S.16 and then seven twelve nine. 
U.17 that?s right 
Table 1: An example dialogue between a user and 
the NUMBERS system. 
 
Just like in this example, a common strategy 
for the subjects was to first let the information 
giver read the number sequence and then switch 
roles and let the information follower repeat it. 
Instead of reading out the whole number se-
quence at once, subjects tended to package 
pieces of information into what Clark (1996) 
refers to as installments (in this case small 
groups of numbers). After each installment, the 
other speaker may react by giving an acknowl-
edgement (as in S.02) a clarification request (as 
in S.04), a correction (as in U.14), or do nothing 
(as after S.12).  
As there are a lot of speaker shifts, there needs 
to be a mechanism for rapid turn taking. In the 
example above, the system must recognize that 
the last digit in U.01, U.03, U.05 and U.07 ends 
an installment and calls for a reaction, while the 
last digit in U.09 ends the whole sequence. One 
information source that has been observed to be 
useful for this is prosody (Koiso et al, 1998). 
When analysing the recorded dialogues, it 
seemed like mid-sequence installments most 
often ended with a prolonged duration and a 
rising pitch, while end-sequence installments 
most often ended with a shorter duration and a 
falling pitch. How prosody is used by the 
NUMBERS system for this classification is de-
scribed in section 4.2.  
4 The NUMBERS system components 
The NUMBERS system has been implemented 
using the HIGGINS spoken dialogue system 
framework (Skantze, 2007). All modules have 
been adapted and extended to allow incremental 
processing. It took us roughly 6 months to im-
plement the changes described here to a fully 
working baseline system. Figure 3 shows the 
architecture of the system2.  
 
  
Figure 3: The system architecture.  
CA = communicative act. 
 
This is pretty much a standard dialogue system 
layout, with some exceptions that will be dis-
cussed below. Most notably perhaps is that dia-
logue management is divided into a discourse 
modelling module and an action manager. As can 
                                                          
2
 A video showing an example run of the system has been 
uploaded to 
http://www.youtube.com/watch?v=_rDkb1K1si8 
Action  
Manager 
Discourse 
modeller 
ASR 
Semantic 
parser 
TTS Audio 
CAs 
Audio 
CAs + 
Words 
 
Words + 
Prosody
CAs + 
Entities 
CAs + 
Words 
748
be seen in the figure, the discourse modeller also 
receives information about what the system itself 
says. The modules run asynchronously in sepa-
rate processes and communicate by sending 
XML messages containing the IUs over sockets.  
We will now characterize each system module 
by what kind of IUs they consume and produce, 
as well as the criteria for committing to an IU.  
4.1 Speech recognition  
The automatic speech recognition module (ASR) 
is based on the Sphinx 4 system (Lamere et al, 
2003). The Sphinx system is capable of incre-
mental processing, but we have added support 
for producing incremental results that are com-
patible with the HIGGINS framework. We have 
also added prosodic analysis to the system, as 
described in 4.2. For the NUMBERS domain, we 
use a very limited context-free grammar accept-
ing number words as well as some expressions 
for feedback and meta-communication.  
An illustration of the module buffers is shown 
in Figure 2 above. The module consumes audio 
frames (each 100 msec) from the LB and pro-
duces words with prosodic features in the RB. 
The RB is updated every time the sequence of 
top word hypotheses in the processing windows 
changes. After 2 seconds of silence has been 
detected, the words produced so far are commit-
ted and the speech recognition search space is 
cleared. Note that this does not mean that other 
components have to wait for this amount of si-
lence to pass before starting to process or that the 
system cannot respond until then ? incremental 
results are produced as soon as the ASR deter-
mines that a word has ended.  
4.2 Prosodic analysis 
We implemented a simple form of prosodic 
analysis as a data processor in the Sphinx fron-
tend. Incremental F0-extraction is done by first 
finding pitch candidates (on the semitone scale) 
for each audio frame using the SMDSF algo-
rithm (Liu et al, 2005). An optimal path between 
the candidates is searched for, using dynamic 
programming (maximising candidate confidence 
scores and minimising F0 shifts). After this, me-
dian smoothing is applied, using a window of 5 
audio frames.  
In order for this sequence of F0 values to be 
useful, it needs to be parameterized. To find out 
whether pitch and duration could be used for the 
distinction between mid-sequence installments 
and end-sequence installments, we did a machine 
learning experiment on the installment-ending 
digits in our collected data. There were roughly 
an equal amount of both types, giving a majority 
class baseline of 50.9%. 
As features we calculated a delta pitch pa-
rameter for each word by computing the sum of 
all F0 shifts (negative or positive) in the pitch 
sequence. (Shifts larger than a certain threshold 
(100 cents) were excluded from the summariza-
tion, in order to sort out artefacts.) A duration 
parameter was derived by calculating the sum of 
the phoneme lengths in the word, divided by the 
sum of the average lengths of these phonemes in 
the whole data set. Both of these parameters 
were tested as predictors separately and in com-
bination, using the Weka Data Mining Software 
(Witten & Frank, 2005). The best results were 
obtained with a J.48 decision tree, and are shown 
in Table 2. 
 
Baseline 50.9% 
Pitch 81.2% 
Duration 62.4% 
Duration + Pitch 80.8% 
Table 2: The results of the installment classifica-
tion (accuracy). 
 
 As the table shows, the best predictor was 
simply to compare the delta pitch parameter 
against an optimal threshold. While the perform-
ance of 80.8% is significantly above baseline, it 
could certainly be better. We do not know yet 
whether the sub-optimal performance is due to 
the fact that the speakers did not always use 
these prosodic cues, or whether there is room for 
improvement in the pitch extraction and parame-
terization. 
Every time the RB of the ASR is updated, the 
delta pitch parameter is computed for each word 
and the derived threshold is used to determine a 
pitch slope class (rising/falling) for the word. 
(Note that there is no class for a flat pitch. This 
class is not really needed here, since the digits 
within installments are followed by no or only 
very short pauses.) The strategy followed by the 
system then is this: when a digit with a rising 
pitch is detected, the system plans to immedi-
ately give a mid-sequence reaction utterance, and 
does so if indeed no more words are received. If 
a digit with a falling pitch is detected, the system 
plans an end-of-sequence utterance, but waits a 
little bit longer before producing it, to see if there 
really are no more words coming in. In other 
words, the system bases its turn-taking decisions 
on a combination of ASR, prosody and silence-
thresholds, where the length of the threshold 
749
differs for different prosodic signals, and where 
reactions are planned already during the silence. 
(This is in contrast to Raux & Eskenazi (2008), 
where context-dependent thresholds are used as 
well, but only simple end-pointing is performed.) 
The use of prosodic analysis in combination 
with incremental processing allows the 
NUMBERS system to give feedback after mid-
sequence installments in about 200 ms. This 
should be compared with most dialogue systems 
which first use a silence threshold of about 750-
1500 msec, after which each module must proc-
ess the utterance. 
4.3 Semantic parsing 
For semantic parsing, the incremental processing 
in the HIGGINS module PICKERING (Skantze & 
Edlund, 2004) has been extended. PICKERING is 
based on a modified chart parser which adds 
automatic relaxations to the CFG rules for ro-
bustness, and produces semantic interpretations 
in the form of concept trees. It can also use fea-
tures that are attached to incoming words, such 
as prosody and timestamps. For example, the 
number groups in U.03 and U.05 in Table 1 ren-
der different parses due to the pause lengths be-
tween the words. 
The task of PICKERING in the NUMBERS do-
main is very limited. Essentially, it identifies 
communicative acts (CAs), such as number in-
stallments. The only slightly more complex pars-
ing is that of larger numbers such as ?twenty 
four?. There are also cases of ?syntactic ambigu-
ity?, as illustrated in U.03 in the dialogue exam-
ple above ("forty five" as "45" or "40 5"). In the 
NUMBERS system, only 1-best hypotheses are 
communicated between the modules, but 
PICKERING can still assign a lower parsing confi-
dence score to an ambiguous interpretation, 
which triggers a clarification request in S.04. 
Figure 4 show a very simple example of the 
incremental processing in PICKERING. The LB 
contains words with prosodic features produced 
by the ASR (compare with Figure 2 above). The 
RB consists of the CAs that are identified. Each 
time a word is added to the chart, PICKERING 
continues to build the chart and then searches for 
an optimal sequence of CAs in the chart, allow-
ing non-matching words in between. To handle 
revision, a copy of the chart is saved after each 
word has been added. 
 
 
Figure 4: Incremental parsing. There is a jump in time 
between t4 and t5. 
 
As can be seen at time-point t4, even if all 
words that a CA is based on are committed, the 
parser does not automatically commit the CA. 
This is because later words may still cause a 
revision of the complex output IU that has been 
built. As a heuristic, PICKERING instead waits 
until a CA is followed by three words that are not 
part of it until it commits, as shown at time-point 
t5. After a CA has been committed, the words 
involved may be cleared from the chart. This 
way, PICKERING parses a ?moving window? of 
words.  
4.4 Discourse modelling 
For discourse modelling, the HIGGINS module 
GALATEA (Skantze, 2008) has been extended to 
operate incrementally. The task of GALATEA is 
to interpret utterances in their context by trans-
forming ellipses into full propositions, indentify 
discourse entities, resolve anaphora and keep 
track of the grounding status of concepts (their 
confidence score and when they have been 
grounded in the discourse). As can be seen in 
Figure 3, GALATEA models both utterances from 
the user as well as the system. This makes it 
possible for the system to monitor its own utter-
ances and relate them to the user?s utterances, by 
using timestamps produced by the ASR and the 
speech synthesiser. 
In the LB GALATEA consumes CAs from both 
the user (partially committed, as seen in Figure 
4) and the system (always committed, see 4.6). 
In the RB GALATEA produces an incremental 
discourse model. This model contains a list of 
resolved communicative acts and list of resolved 
discourse entities. This model is then consulted 
by an action manager which decides what the 
system should do next. The discourse model is 
40 
forty 
forty five 
forty five 
forty 
forty five 
forty five 
40 
45 
45 
40 
45 
45 
three 62 3 45 
forty five sixty two three 
62 3 45 
t1
t2
t3
t4
time
four
 
four
 
4
 
4
 
t5
left buffer processor right buffer 
4
 
four
 
750
committed up to the point of the earliest non-
committed incoming CA. In the NUMBERS do-
main, the discourse entities are the number in-
stallments.  
4.5 Action management  
Based on the discourse model (from the LB), the 
action manager (AM) generates system actions 
(CAs) in semantic form (for GALATEA) with an 
attached surface form (for the TTS), and puts 
them on the RB. (In future extensions of the sys-
tem, we will add an additional generation module 
that generates the surface form from the semantic 
form.) In the NUMBERS system, possible system 
actions are acknowledgements, clarification re-
quests and repetitions of the number sequence. 
The choice of actions to perform is based on the 
grounding status of the concepts (which is repre-
sented in the discourse model). For example, if 
the system has already clarified the first part of 
the number sequence due to an ambiguity, it does 
not need to repeat this part of the sequence again. 
The AM also attaches a desired timing to the 
produced CA, relative to the end time of last user 
utterance. For example, if a number group with a 
final rising pitch is detected, the AM may tell the 
TTS to execute the CA immediately after the 
user has stopped speaking. If there is a falling 
pitch, it may tell the TTS to wait until 500 msec 
of silence has been detected from the user before 
executing the action. If the discourse model gets 
updated during this time, the AM may revoke 
previous CAs and replace them with new ones.  
4.6 Speech synthesis 
A diphone MBROLA text-to-speech synthesiser 
(TTS) is used in the system (Dutoit et al, 1996), 
and a wrapper for handling incremental process-
ing has been implemented. The TTS consumes 
words linked to CAs from the LB, as produced 
by the AM. As described above, each CA has a 
timestamp. The TTS places them on a queue, and 
prepares to synthesise and start sending the audio 
to the speakers. When the system utterance has 
been played, the corresponding semantic con-
cepts for the CA are sent to GALATEA. If the 
TTS is interrupted, the semantic fragments of the 
CA that corresponds to the words that were spo-
ken are sent. This way, GALATEA can monitor 
what the system actually says and provide the 
AM with this information. Since the TTS only 
sends (parts of) the CAs that have actually been 
spoken, these are always marked as committed.  
There is a direct link from the ASR to the TTS 
as well (not shown in Figure 3), informing the 
TTS of start-of-speech and end-of-speech events. 
As soon as a start-of-speech event is detected, 
the TTS stops speaking. If the TTS does not re-
ceive any new CAs from the AM as a conse-
quence of what the user said, it automatically 
resumes from the point of interruption. (This 
implements a "reactive behaviour" in the sense of 
(Brooks, 1991), which is outside of the control of 
the AM.)   
An example of this is shown in Table 1. After 
U.09, the AM decides to repeat the whole num-
ber sequence and sends a series of CAs to the 
TTS for doing this. After S.10, the user gives 
feedback in the form of an acknowledgement 
(U.11). This causes the TTS to make a pause. 
When GALATEA receives the user feedback, it 
uses the time-stamps to find out that the feedback 
is related to the number group in S.10 and the 
grounding status for this group is boosted. When 
the AM receives the updated discourse model, it 
decides that this does not call for any revision to 
the already planned series of actions. Since the 
TTS does not receive any revisions, it resumes 
the repetition of the number sequence in S.12. 
The TTS module is fully incremental in that it 
can stop and resume speaking in the middle of an 
utterance, revise planned output, and can inform 
other components of what (parts of utterances) 
has been spoken. However, the actual text-to-
speech processing is done before the utterance 
starts and not yet incrementally as the utterance 
is spoken, which could further improve the effi-
ciency of the system. This is a topic for future 
research, together with the generation of hidden 
and overt repair as discussed by Kilger & Finkler 
(1995).  
5 Evaluation  
It is difficult to evaluate complete dialogue sys-
tems such as the one presented here, since there 
are so many different components involved (but 
see M?ller et al (2007) for methods used). In our 
case, we?re interested in the benefits of a specific 
aspect, though, namely incrementality. No 
evaluation is needed to confirm that an incre-
mental system such as this allows more flexible 
turn-taking and that it can potentially respond 
faster ? this is so by design. However, we also 
want this behaviour to result in an improved user 
experience. To test whether we have achieved 
this, we implemented for comparison a non-
incremental version of the system, very much 
like a standard number dictation dialogue in a 
commercial application. In this version, the user 
751
is asked to read out the whole number sequence 
in one go. After a certain amount of silence, the 
system confirms the whole sequence and asks a 
yes/no question whether it was correct. If not, the 
user has to repeat the whole sequence.  
Eight subjects were given the task of using the 
two versions of the system to dictate number 
sequences (in English) to the system. (The sub-
jects were native speakers of German with a 
good command of English.) Half of the subjects 
used the incremental version first and the other 
half started with the non-incremental version. 
They were asked to dictate eight number se-
quences to each version, resulting in 128 dia-
logues. For each sequence, they were given a 
time limit of 1 minute. After each sequence, they 
were asked whether they had succeeded in dictat-
ing the sequence or not, as well as to mark their 
agreement (on a scale from 0-6) with statements 
concerning how well they had been understood 
by the system, how responsive the system was, if 
the system behaved as expected, and how hu-
man-like the conversational partner was. After 
using both versions of the system, they were also 
asked whether they preferred one of the versions 
and to what extent (1 or 2 points, which gives a 
maximum score of 16 to any version, when total-
ling all subjects).  
There was no significant difference between 
the two versions with regard to how many of the 
tasks were completed successfully. However, the 
incremental version was clearly preferred in the 
overall judgement (9 points versus 1). Only one 
of the more specific questions yielded any sig-
nificant difference between the versions: the 
incremental version was judged to be more hu-
man-like for the successful dialogues (5,2 on 
average vs. 4,5; Wilcoxon signed rank test; 
p<0.05).  
The results from the evaluation are in line with 
what could be expected. A non-incremental sys-
tem can be very efficient if the system under-
stands the number sequence the first time, and 
the ASR vocabulary is in this case very limited, 
which explains why the success-rate was the 
same for both systems. However, the incremental 
version was experienced as more pleasant and 
human-like. One explanation for the better rating 
of the incremental version is that the acknowl-
edgements encouraged the subjects to package 
the digits into installments, which helped the 
system to better read back the sequence using the 
same installments. 
6 Conclusions and future work 
To sum up, we have presented a dialogue system 
that through the use of novel techniques (incre-
mental prosodic analysis, reactive connection 
between ASR and TTS, fully incremental archi-
tecture) achieves an unprecedented level of reac-
tiveness (from a minimum latency of 750ms, as 
typically used in dialogue systems, down to one 
of 200ms), and is consequently evaluated as 
more natural than more typical setups by human 
users. While the domain we've used is relatively 
simple, there are no principled reasons why the 
techniques introduced here should not scale up. 
In future user studies, we will explore which 
factors contribute to the improved experience of 
using an incremental system. Such factors may 
include improved responsiveness, better install-
ment packaging, and more elaborate feedback. It 
would also be interesting to find out when rapid 
responses are more important (e.g. acknowl-
edgements), and when they may be less impor-
tant (e.g., answers to task-related questions). 
We are currently investigating the transfer of 
the prosodic analysis to utterances in a larger 
domain, where similarly instructions by the user 
can be given in installments. But even within the 
currently used micro-domain, there are interest-
ing issues still to be explored. In future versions 
of the system, we will let the modules pass paral-
lel hypotheses and also improve the incremental 
generation and synthesis. Since the vocabulary is 
very limited, it would also be possible to use a 
limited domain synthesis (Black & Lenzo, 2000), 
and explore how the nuances of different back-
channels might affect the dialogue. Another chal-
lenge that can be researched within this micro-
domain is how to use the prosodic analysis for 
other tasks, such as distinguishing correction 
from dictation (for example if U.14 in Table 1 
would not begin with a ?no?). In general, we 
think that this paper shows that narrowing down 
the domain while shifting the focus to the model-
ling of more low-level, conversational dialogue 
phenomena is a fruitful path. 
Acknowledgements 
This work was funded by a DFG grant in the 
Emmy Noether programme. We would also like 
to thank Timo Baumann and Michaela Atterer 
for their contributions to the project, as well as 
Anna Iwanow and Angelika Adam for collecting 
and transcribing the data used in this paper. 
752
References  
Aist, G., Allen, J. F., Campana, E., Galescu, L., 
G?mez Gallo, C. A., Stoness, S. C., Swift, M., & 
Tanenhaus, M. (2006). Software Architectures for 
Incremental Understanding of Human Speech. In 
Proceedings of Interspeech (pp. 1922-1925). Pitts-
burgh PA, USA. 
Allen, J. F., Ferguson, G., & Stent, A. (2001). An 
architecture for more realistic conversational sys-
tems. In Proceedings of the 6th international con-
ference on Intelligent user interfaces (pp. 1-8).  
Black, A., & Lenzo, K. (2000). Limited domain syn-
thesis. In Proceedings of ICSLP (pp. 410-415). 
Beijing, China. 
Brooks, R. A. (1991). Intelligence without representa-
tion. Artificial Intelligence, 47, 139-159. 
Clark, H. H. (1996). Using language. Cambridge, 
UK: Cambridge University Press. 
Dohsaka, K., & Shimazu, A. (1997). System architec-
ture for spoken utterance production in collabora-
tive dialogue. In Working Notes of IJCAI 1997 
Workshop on Collaboration, Cooperation and 
Conflict in Dialogue Systems.  
Dutoit, T., Pagel, V., Pierret, N., Bataille, F., & Vre-
ken, O. v. d. (1996). The MBROLA project: To-
wards a set of high-quality speech synthesizers free 
of use for non-commercial purposes. In Proceed-
ings of ICSLIP '96 (pp. 1393-1396).  
Edlund, J., Gustafson, J., Heldner, M., & Hjalmars-
son, A. (2008). Towards human-like spoken dialo-
gue systems. Speech Communication, 50(8-9), 630-
645. 
Ferrer, L., Shriberg, E., & Stolcke, A. (2002). Is the 
speaker done yet? Faster and more accurate end-of 
utterance detection using prosody. In Proceedings 
of ICSLP (pp. 2061-2064).  
Kilger, A., & Finkler, W. (1995). Incremental Gener-
ation for Real-Time Applications. Technical Report 
RR-95-11, German Research Center for Artificial 
Intelligence. 
Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., & 
Den, Y. (1998). An analysis of turn-taking and 
backchannels based on prosodic and syntactic fea-
tures in Japanese Map Task dialogs. Language and 
Speech, 41, 295-321. 
Lamere, P., Kwok, P., Gouvea, E., Raj, B., Singh, R., 
Walker, W., Warmuth, M., & Wolf, P. (2003). The 
CMU SPHINX-4 speech recognition system.. In 
Proceedings of the IEEE Intl. Conf. on Acoustics, 
Speech and Signal Processing. Hong Kong. 
Levelt, W. J. M. (1989). Speaking: From Intention to 
Articulation. Cambridge, Mass., USA: MIT Press. 
Liu, J., Zheng, T. F., Deng, J., & Wu, W. (2005). 
Real-time pitch tracking based on combined 
SMDSF. In Proceedings of Interspeech (pp. 301-
304). Lisbon, Portugal. 
M?ller, S., Smeele, P., Boland, H., & Krebber, J. 
(2007). Evaluating spoken dialogue systems ac-
cording to de-facto standards: A case study. Com-
puter Speech & Language, 21(1), 26-53. 
Raux, A., & Eskenazi, M. (2007). A multi-Layer 
architecture for semi-synchronous event-driven di-
alogue Management. In ASRU 2007. Kyoto, Ja-
pan.. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Sacks, H., Schwegloff, E., & Jefferson, G. (1974). A 
simplest systematics for the organization of turn-
taking for conversation. Language, 50, 696-735. 
Schlangen, D., & Fern?ndez, R. (2007). Speaking 
through a noisy channel: experiments on inducing 
clarification behaviour in human-human dialogue. 
In Proceedings of Interspeech 2007. Antwerp, Bel-
gium. 
Schlangen, D., & Skantze, G. (2009). A general, ab-
stract model of incremental dialogue processing. In 
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics (EACL-09). Athens, Greece. 
Skantze, G., & Edlund, J. (2004). Robust interpreta-
tion in the Higgins spoken dialogue system. In 
Proceedings of ISCA Tutorial and Research Work-
shop (ITRW) on Robustness Issues in Conversa-
tional Interaction. Norwich, UK. 
Skantze, G. (2007). Error Handling in Spoken Dialo-
gue Systems - Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing. 
Skantze, G. (2008). Galatea: A discourse modeller 
supporting concept-level error handling in spoken 
dialogue systems. In Dybkj?r, L., & Minker, W. 
(Eds.), Recent Trends in Discourse and Dialogue. 
Springer. 
Stoness, S. C., Tetreault, J., & Allen, J. (2004). In-
cremental parsing with reference interaction. In 
Proceedings of the ACL Workshop on Incremental 
Parsing (pp. 18-25).  
Tanenhaus, M. K., & Brown-Schmidt, S. (2008). 
Language processing in the natural world. In 
Moore, B. C. M., Tyler, L. K., & Marslen-Wilson, 
W. D. (Eds.), The perception of speech: from 
sound to meaning (pp. 1105-1122).  
Wir?n, M. (1992). Studies in Incremental Natural 
Language Analysis. Doctoral dissertation, 
Link?ping University, Link?ping, Sweden. 
Witten, I. H., & Frank, E. (2005). Data Mining: Prac-
tical machine learning tools and techniques. San 
Francisco: Morgan Kaufmann. 
 
753
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310?313,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Attention and Interaction Control in  
a Human-Human-Computer Dialogue Setting 
 
Gabriel Skantze 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
gabriel@speech.kth.se 
Joakim Gustafson 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
jocke@speech.kth.se 
 
 
 
Abstract 
This paper presents a simple, yet effective 
model for managing attention and interaction 
control in multimodal spoken dialogue sys-
tems. The model allows the user to switch at-
tention between the system and other hu-
mans, and the system to stop and resume 
speaking. An evaluation in a tutoring setting 
shows that the user?s attention can be effec-
tively monitored using head pose tracking, 
and that this is a more reliable method than 
using push-to-talk.  
1 Introduction 
Most spoken dialogue systems are based on the 
assumption that there is a clear beginning and 
ending of the dialogue, during which the user 
pays attention to the system constantly. However, 
as the use of dialogue systems is extended to 
settings where several humans are involved, or 
where the user needs to attend to other things 
during the dialogue, this assumption is obviously 
too simplistic (Bohus & Horvitz, 2009). When it 
comes to interaction, a strict turn-taking protocol 
is often assumed, where user and system wait for 
their turn and deliver their contributions in whole 
utterance-sized chunks. If system utterances are 
interrupted, they are treated as either fully 
delivered or basically unsaid. 
This paper presents a simple, yet effective 
model for managing attention and interaction 
control in multimodal (face-to-face) spoken dia-
logue systems, which avoids these simplifying 
assumptions. We also present an evaluation in a 
tutoring setting where we explore the use of head 
tracking for monitoring user attention, and com-
pare it with a more traditional method: push-to-
talk.  
2 Monitoring user attention 
In multi-party dialogue settings, gaze has been 
identified as an effective cue to help disambi-
guate the addressee of a spoken utterance 
(Vertegaal et al, 2001).  When it comes to hu-
man-machine interaction, Maglio et al (2000) 
showed that users tend to look at speech-
controlled devices when talking to them, even if 
they do not have the manifestation of an embo-
died agent. Bakx et al (2003) investigated the 
use of head pose for identifying the addressee in 
a multi-party interaction between two humans 
and an information kiosk. The results indicate 
that head pose should be combined with acoustic 
and linguistic features such as utterances length. 
Facial orientation in combination with speech-
related features was investigated by Katzenmaier 
et al (2004) in a human-human-robot interaction, 
confirming that a combination of cues was most 
effective. A common finding in these studies is 
that if a user does not look at the system while 
talking he is most likely not addressing it. How-
ever, when the user looks at the system while 
speaking, there is a considerable probability that 
she is actually addressing a bystander. 
3 The MonAMI Reminder 
This study is part of the 6th framework IP project 
MonAMI1. The goal of the MonAMI project is to 
develop and evaluate services for elderly and 
disabled people. Based on interviews with poten-
tial users in the target group, we have developed 
the MonAMI Reminder, a multimodal spoken 
dialogue system which can assist elderly and dis-
abled people in organising and initiating their 
daily activities (Beskow et al, 2009). The dia-
logue system uses Google Calendar as a back-
bone to answer questions about events. However, 
                                                 
1 http://www.monami.info/ 
310
it can also take the initiative and give reminders 
to the user.  
The MonAMI Reminder is based on the HIG-
GINS platform (Skantze, 2007). The architecture 
is shown in Figure 1. A microphone and a cam-
era are used for system input (speech recognition 
and head tracking), and a speaker and a display 
are used for system output (an animated talking 
head). This is pretty much a standard dialogue 
system architecture, with some exceptions. Di-
alogue management is split into a Discourse 
Modeller and an Action Manager, which consults 
the discourse model and decides what to do next. 
There is also an Attention and Interaction Con-
troller (AIC), which will be discussed next.  
 
Figure 1. The system architecture in the MonAMI 
Reminder. 
4 Attention and interaction model 
The purpose of the AIC is to act as a low level 
monitor and controller of the system?s speaking 
and attentional behaviour. The AIC uses a state-
based model to track the attentional and interac-
tional state of the user and the system, shown in 
Figure 2. The states shown in the boxes can be 
regarded as the combined state of the system 
(columns) and the user (rows)2. Depending on 
the combined state, events from input and output 
components will have different effects. As can be 
seen in the figure, some combination of states 
cannot be realised, such as the system and user 
speaking at the same time (if the user speaks 
while the system is speaking, it will automati-
cally change to the state INTERRUPTED). Of 
course, the user might speak while the system is 
speaking without the system detecting this, but 
                                                 
2 This is somewhat similar to the ?engagement state? used 
in Bohus & Horvitz (2009). 
the model should be regarded from the system?s 
perspective, not from an observer. 
The user?s attention is monitored using a cam-
era and an off-the-shelf head tracking software. 
As the user starts to look at the system, the state 
changes from NONATTENTIVE to ATTENTIVE. 
When the user starts to speak, a UserStartSpeak 
event from the ASR will trigger a change to the 
LISTENING state. The Action Manager might 
then trigger a SystemResponse event (together 
with what should be said), causing a change into 
the SPEAKING state. Now, if the user would look 
away while the system is speaking, the system 
would enter the HOLDING state ? the system 
would pause and then resume when the user 
looks back. If the user starts to speak while the 
system is speaking, the controller will enter the 
INTERRUPTED state. The Action Manager might 
then either decide to answer the new request, 
resume speaking (e.g., if there was just a back-
channel or the confidence was too low), or abort 
speaking (e.g., if the user told the system to shut 
up).  
There is also a CALLING state, in which the 
system might try to grab the user?s attention. 
This is very important for the current application 
when the system needs to remind the user about 
something.  
4.1 Incremental multimodal speech  
synthesis 
The speech synthesiser used must be capable of 
reporting the timestamp of each word in the 
synthesised string. These are two reasons for this. 
First, it must be possible to resume speaking 
after returning from the states INTERRUPTED and 
HOLDING. Second, the AIC is responsible for 
reporting what has actually been said by the 
system back to the Discourse Modeller for 
continuous self monitoring (there is a direct 
feedback loop as can be seen in Figure 1). This 
way, the Discourse Modeller may relate what the 
system says to what the user says on a high 
resolution time scale (which is necessary for 
handling phenomena such as backchannels, as 
discussed in Skantze & Schlangen, 2009).  
Currently, the system may pause and resume 
speaking at any word boundary and there is no 
specific prosodic modelling of these events. The 
synthesis of interrupted speech is something that 
we will need to improve. 
 
 
 
 
GALATEA:  
Discourse Modeller 
ASR 
PICKERING:  
Semantic Parsing 
Multimodal Speech  
Synthesis 
Utterance  
Generation 
Google  
Calendar 
Action  
Manager 
Attention and Inter-
action Controller 
Display Microphone Camera 
Head 
Tracker 
Speaker 
311
An animated talking head is shown on a display, 
synchronised with the synthesised speech 
(Beskow, 2003). The head is making small con-
tinuous movements (recorded from real human 
head movements), giving it a more life-like ap-
pearance. The head pose and facial gestures are 
triggered by the different states and events in the 
AIC, as can be seen in Figure 3. Thus, when the 
user approaches the system and starts to look at it, 
the system will look up, giving a clear signal that 
it is now attending to the user and ready to listen. 
5 Evaluation 
In the evaluation, we not only wanted to check 
whether the AIC model worked, but also to un-
derstand whether user attention could be effec-
tively modelled using head tracking. Similarly to 
Oh et al (2002), we wanted to compare ?look-to-
talk? with ?push-to-talk?. To do this, we used a 
human-human-computer dialogue setting, where 
a tutor was explaining the system to a subject 
(shown in Figure 4). Thus, the subject needed to 
frequently switch between speaking to the tutor 
and the system. A second version of the system 
was also implemented where the head tracker 
was not used, but where the subject instead 
pushed a button to switch between the attentional 
states (a sort-of push-to-talk). The tutor first ex-
plained both versions of the system to the subject 
and let her try both. The tutor gave the subjects 
hints on how to express themselves, but avoided 
to remind them about how to control the atten-
tion of the system, as this was what we wanted to 
test. After the introduction, the tutor gave the 
subject a task where both of them were supposed 
to find a suitable slot in their calendars to plan a 
dinner or lunch together. The tutor used a paper 
calendar, while the subject used the MonAMI 
Reminder. At the end of the experiment, the tutor 
interviewed the subject about her experience of 
using the system. 7 subjects (4 women and 3 men) 
were used in the evaluation, 3 lab members and 4 
elderly persons in the target group (recruited by 
the Swedish Handicap Institute).  
There was no clear consensus on which ver-
sion of the system was the best. Most subjects 
liked the head tracking version better when it 
worked but were frustrated when the head 
tracker occasionally failed. They reported that a 
combined version would perhaps be the best, 
where head pose could be the main method for 
handling attention, but where a button or a verbal 
call for attention could be used as a fall-back. 
When looking at the interaction from an objec-
tive point of view, however, the head tracking 
NonAttentive
Attentive Speaking
Listening
UserStartLook
SystemInitiative
SystemResponse
SystemStopSpeak
UserStopLook
Holding
Timeout
Interrupted
UserStartSpeak SystemIgnore (resume)
SystemResponse (restart)SystemIgnore
Calling
PausingSpeakingAttending
Not attending
Attending
Speaking
Not attending
SystemInitiative
UserStartLook 
SystemResponse
UserStartSpeak
UserStopLookUserStartLook (resume)
SystemStopSpeak
System
User
SystemAbortSpeak
Figure 2. The attention and interaction model. Dashed lines indicate events coming from input modules. Solid
lines indicate events from output modules. Note that some events and transitions are not shown in the figure. 
NonAttentive Attentive Listening SystemIgnore 
Figure 3. Examples of facial animations triggered by
the different states and events shown in Figure 2.  
 
312
version was clearly more successful in terms of 
number of misdirected utterances. When talking 
to the system, the subjects always looked at the 
system in the head tracking condition and never 
forgot to activate it in the push-to-talk condition. 
However, on average 24.8% of all utterances 
addressed to the tutor in the push-to-talk condi-
tion were picked up by the system, since the user 
had forgotten to deactivate it. The number of ut-
terances addressed to the tutor while looking at 
the system in the head tracking condition was 
significantly lower, only 5.1% on average (paired 
t-test; p<0.05).   
These findings partly contradict findings from 
previous studies, where head pose has not been 
that successful as a sole indicator when the user 
is looking at the system, as discussed in section 2 
above. One explanation for this might be that the 
subjects were explicitly instructed about how the 
system worked. Another explanation is the clear 
feedback (and entrainment) that the agent?s head 
pose provided. 
Two of the elderly subjects had no previous 
computer experience. During pre-interviews they 
reported that they were intimidated by com-
puters, and that they got nervous just thinking 
about having to operate them. However, after 
only a short tutorial session with the spoken in-
terface, they were able to navigate through a 
computerized calendar in order to find two 
empty slots. We think that having a human tutor 
that guides the user through their first interac-
tions with this kind of system is very important. 
One of the tutor?s tasks is to explain why the sys-
tem fails to understand out-of-vocabulary ex-
pressions. By doing this, the users? trust in the 
system is increased and they become less con-
fused and frustrated. We are confident that moni-
toring and modelling the user?s attention is a key 
component of spoken dialogue systems that are 
to be used in tutoring settings.   
Acknowledgements 
This research is supported by MonAMI, an Integrated 
Project under the European Commission?s 6th Frame-
work Program (IP-035147), and the Swedish research 
council project GENDIAL (VR #2007-6431). 
References 
Bakx, I., van Turnhout, K., & Terken, J. (2003). Fa-
cial orientation during multi-party interaction with 
information kiosks. In Proceedings of the Interact 
2003. 
Beskow, J., Edlund, J., Granstr?m, B., Gustafson, J., 
Skantze, G., & Tobiasson, H. (2009). The 
MonAMI Reminder: a spoken dialogue system for 
face-to-face interaction. In Proceedings of Inter-
speech 2009. 
Beskow, J. (2003). Talking heads - Models and appli-
cations for multimodal speech synthesis. Doctoral 
dissertation, KTH, Department of Speech, Music 
and Hearing, Stockholm, Sweden. 
Bohus, D., & Horvitz, E. (2009). Open-World Dialog: 
Challenges, Directions, and Prototype. In Proceed-
ings of IJCAI'2009 Workshop on Knowledge and 
Reasoning in Practical Dialogue Systems. Pasade-
na, CA. 
Katzenmaier, M., Stiefelhagen, R., Schultz, T., Rogi-
na, I., & Waibel, A. (2004). Identifying the Ad-
dressee in Human-Human-Robot Interactions 
based on Head Pose and Speech. In Proceedings of 
ICMI 2004. 
Maglio, P. P., Matlock, T., Campbell, C. S., Zhai, S., 
& Smith, B. A. (2000). Gaze and speech in atten-
tive user interfaces. In Proceedings of ICMI 2000.  
Oh, A., Fox, H., Van Kleek, M., Adler, A., Gajos, K., 
Morency, L-P., & Darrell, T. (2002). Evaluating 
Look-to-Talk: A Gaze-Aware Interface in a Col-
laborative Environment. In Proceedings of CHI 
2002. 
Skantze, G., & Schlangen, D. (2009). Incremental 
dialogue processing in a micro-domain. In Pro-
ceedings of EACL-09. Athens, Greece. 
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems ? Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing, 
Stockholm, Sweden. 
Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt, 
A. (2001). Eye gaze patterns in conversations: 
there is more to conversational agents than meets 
the eyes. In Proceedings of ACM Conf. on Human 
Factors in Computing Systems.   
Figure 4. The human-human-computer dialogue set-
ting used in the evaluation. The tutor is sitting on the 
left side and the subject on the right side 
313
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1?8,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Incremental Speech Generation in Dialogue Systems 
 
Gabriel Skantze 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
gabriel@speech.kth.se 
Anna Hjalmarsson 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
annah@speech.kth.se 
 
 
Abstract 
We present a first step towards a model of 
speech generation for incremental dialogue 
systems. The model allows a dialogue system 
to incrementally interpret spoken input, while 
simultaneously planning, realising and self-
monitoring the system response. The model 
has been implemented in a general dialogue 
system framework. Using this framework, we 
have implemented a specific application and 
tested it in a Wizard-of-Oz setting, comparing 
it with a non-incremental version of the same 
system. The results show that the incremental 
version, while producing longer utterances, 
has a shorter response time and is perceived 
as more efficient by the users. 
1 Introduction 
Speakers in dialogue produce speech in a piece-
meal fashion and on-line as the dialogue pro-
gresses. When starting to speak, dialogue partici-
pants typically do not have a complete plan of 
how to say something or even what to say. Yet, 
they manage to rapidly integrate information 
from different sources in parallel and simultane-
ously plan and realize new dialogue contribu-
tions. Moreover, interlocutors continuously self-
monitor the actual production processes in order 
to facilitate self-corrections (Levelt, 1989). Con-
trary to this, most spoken dialogue systems use a 
silence threshold to determine when the user has 
stopped speaking. The user utterance is then 
processed by one module at a time, after which a 
complete system utterance is produced and real-
ised by a speech synthesizer.  
This paper has two purposes. First, to present 
an initial step towards a model of speech genera-
tion that allows a dialogue system to incremen-
tally interpret spoken input, while simultaneously 
planning, realising and self-monitoring the sys-
tem response. The model has been implemented 
in a general dialogue system framework. This is 
described in Section 2 and 3. The second purpose 
is to evaluate the usefulness of incremental 
speech generation in a Wizard-of-Oz setting, us-
ing the proposed model. This is described in Sec-
tion 4. 
1.1 Motivation 
A non-incremental dialogue system waits until 
the user has stopped speaking (using a silence 
threshold to determine this) before starting to 
process the utterance and then produce a system 
response. If processing takes time, for example 
because an external resource is being accessed, 
this may result in a confusing response delay. An 
incremental system may instead continuously 
build a tentative plan of what to say as the user is 
speaking. When it detects that the user?s utter-
ance has ended, it may start to asynchronously 
realise this plan while processing continues, with 
the possibility to revise the plan if needed.  
There are many potential reasons for why dia-
logue systems may need additional time for 
processing. For example, it has been assumed 
that ASR processing has to be done in real-time, 
in order to avoid long and confusing response 
delays. Yet, if we allow the system to start 
speaking before input is complete, we can allow 
more accurate (and time-consuming) ASR proc-
essing (for example by broadening the beam). In 
this paper, we will explore incremental speech 
generation in a Wizard-of-oz setting. A common 
problem in such settings is the time it takes for 
the Wizard to interpret the user?s utterance 
and/or decide on the next system action, resulting 
in unacceptable response delays (Fraser & Gil-
bert, 1991). Thus, it would be useful if the sys-
tem could start to speak as soon as the user has 
finished speaking, based on the Wizard?s actions 
so far. 
1
1.2 Related work 
Incremental speech generation has been studied 
from different perspectives. From a psycholin-
guistic perspective, Levelt (1989) and others 
have studied how speakers incrementally pro-
duce utterances while self-monitoring the output, 
both overtly (listening to oneself speaking) and 
covertly (mentally monitoring what is about to 
be said). As deviations from the desired output is 
detected, the speaker may initiate  self-repairs. If 
the item to be repaired has already been spoken, 
an overt repair is needed (for example by using 
an editing term, such as ?sorry?). If not, the ut-
terance plan may be altered to accommodate the 
repair, a so-called covert repair. Central to the 
concept of incremental speech generation is that 
the realization of overt speech can be initiated 
before the speaker has a complete plan of what to 
say. An option for a speaker who does not know 
what to say (but wants to claim the floor) is to 
use hesitation phenomena such as filled pauses 
(?eh?) or cue phrases such as ?let?s see?.  
A dialogue system may not need to self-
monitor its output for the same reasons as hu-
mans do. For example, there is no risk of articu-
latory errors (with current speech synthesis tech-
nology). However, a dialogue system may utilize 
the same mechanisms of self-repair and hesita-
tion phenomena to simultaneously plan and real-
ise the spoken output, as there is always a risk 
for revision in the input to an incremental mod-
ule (as described in Section 2.1).  
There is also another aspect of self-monitoring 
that is important for dialogue systems. In a sys-
tem with modules operating asynchronously, the 
dialogue manager cannot know whether the in-
tended output is actually realized, as the user 
may interrupt the system. Also, the timing of the 
synthesized speech is important, as the user may 
give feedback in the middle of a system utter-
ance. Thus, an incremental, asynchronous system 
somehow needs to self-monitor its own output.   
From a syntactic perspective, Kempen & 
Hoenkamp (1987) and Kilger & Finkler (1995) 
have studied how to syntactically formulate sen-
tences incrementally under time constraints. 
Dohsaka & Shimazu (1997) describes a system 
architecture for incremental speech generation. 
However, there is no account for revision of the 
input (as discussed in Section 2.1) and there is no 
evaluation with users. Skantze & Schlangen 
(2009) describe an incremental system that partly 
supports incremental output and that is evaluated 
with users, but the domain is limited to number 
dictation. 
In this study, the focus is not on syntactic con-
struction of utterances, but on how to build prac-
tical incremental dialogue systems within limited 
domains that can handle revisions and produce 
convincing, flexible and varied speech output in 
on-line interaction with users.  
2 The Jindigo framework 
The proposed model has been implemented in 
Jindigo ? a Java-based open source framework 
for implementing and experimenting with incre-
mental dialogue systems (www.jindigo.net). We 
will here briefly describe this framework and the 
model of incremental dialogue processing that it 
is based on. 
2.1 Incremental units 
Schlangen & Skantze (2009) describes a general, 
abstract model of incremental dialogue process-
ing, which Jindigo is based on. In this model, a 
system consists of a network of processing mod-
ules. Each module has a left buffer, a processor, 
and a right buffer, where the normal mode of 
processing is to receive input from the left 
buffer, process it, and provide output in the right 
buffer, from where it is forwarded to the next 
module?s left buffer. An example is shown in 
Figure 1. Modules exchange incremental units 
(IUs), which are the smallest ?chunks? of infor-
mation that can trigger connected modules into 
action (such as words, phrases, communicative 
acts, etc). IUs are typically part of larger units: 
individual words are parts of an utterance; con-
cepts are part of the representation of an utter-
ance meaning. This relation of being part of the 
same larger unit is recorded through same-level 
links. In the example below, IU2 has a same-level 
link to IU1 of type PREDECESSOR, meaning that 
they are linearly ordered. The information that 
was used in creating a given IU is linked to it via 
grounded-in links. In the example, IU3 is 
grounded in IU1 and IU2, while IU4 is grounded 
in IU3. 
 
IU1 IU2
IU1 IU2
IU3 IU3
IU3
IU4
IU4
Module A
Module B
left buffer processor right buffer
left buffer processor right buffer
 
Figure 1: Two connected modules. 
2
A challenge for incremental systems is to han-
dle revisions. For example, as the first part of the 
word ?forty? is recognised, the best hypothesis 
might be ?four?. As the speech recogniser re-
ceives more input, it might need to revise its pre-
vious output, which might cause a chain of revi-
sions in all subsequent modules. To cope with 
this, modules have to be able to react to three 
basic situations: that IUs are added to a buffer, 
which triggers processing; that IUs that were er-
roneously hypothesized by an earlier module are 
revoked, which may trigger a revision of a mod-
ule?s own output; and that modules signal that 
they commit to an IU, that is, won?t revoke it 
anymore. 
Jindigo implements an efficient model for 
communicating these updates. In this model, IUs 
are associated with edges in a graph, as shown in 
Table 1. The graph may be incrementally 
amended without actually removing edges or 
vertices, even if revision occurs. At each time-
step, a new update message is sent to the con-
suming module. The update message contains a 
pair of pointers [C, A]: (C) the vertex from which 
the currently committed hypothesis can be con-
structed, and (A) the vertex from which the cur-
rently best tentative hypothesis can be con-
structed. In Jindigo, all modules run as threads 
within a single Java process, and therefore have 
access to the same memory space.  
2.2 A typical architecture 
A typical Jindigo system architecture is shown in 
Figure 2. The word buffer from the Recognizer 
module is parsed by the Interpreter module 
which tries to find an optimal sequence of top 
phrases and their semantic representations. These 
phrases are then interpreted in light of the current 
dialogue context by the Contextualizer module 
and are packaged as Communicative Acts (CAs). 
As can be seen in Figure 2, the Contextualizer 
also self-monitors Concepts from the system as 
they are spoken by the Vocalizer, which makes it 
possible to contextually interpret user responses 
to system utterances. This also makes it possible 
for the system to know whether an intended ut-
terance actually was produced, or if it was inter-
rupted. The current context is sent to the Action 
Manager, which generates a SpeechPlan that is 
sent to the Vocalizer. This is described in detail 
in the next section.  
Figure 2: A typical Jindigo system architecture. 
 
String Right buffer Update 
message 
t1: one w1 one w2
 
[w1, w2] 
t2: one five w1 one w2 five w3
 
[w1, w3] 
t3: one w1 one w2 five w3
 
[w1, w2] 
t4: one four five w1 one w2 five w3
five w5four w4
 
[w1, w5] 
t5: [commit] w1 one w2 five w3
five w5four w4
 
[w5,w5] 
Table 1: The right buffer of an ASR module, and up-
date messages at different time-steps. 
 
Figure 3: Incremental Units at different levels of processing. Some grounded-in relations are shown with dotted 
lines. W=Word, SS=SpeechSegment, SU=SpeechUnit, CA=Communicative Act. 
Interpreter
VAD
ASR
Action Manager
Vocalizer
Contextualizer
SpeechPlan
Speech
Segment
SU SU SU SU SU SU
Self
Delay
Other
Delay
W W W W W W W W W
P
CA
SS
Concept
CA
Response
To
SS
C Phrase Concept
Utterance Utterance
Utterance
Segment
US US
User System
SS
User
Vocalizer
Speech
Speech
Interpreter
Word
ContextualizerActionManager
Utterance
Segment
ASR
SpeechPlan
Context
Phrase
Concept
3
3 Incremental speech generation 
3.1 Incremental units of speech 
In order for user and system utterances to be in-
terpreted and produced incrementally, they need 
to be decomposed into smaller units of process-
ing (IUs). This decomposition is shown in Figure 
3. Using a standard voice activity detector 
(VAD) in the ASR, the user?s speech is chunked 
into Utterance-units. The Utterance bounda-
ries determine when the ASR hypothesis is 
committed. However, for the system to be able to 
respond quickly, the end silence threshold of 
these Utterances are typically too long. Therefore 
smaller units of the type UtteranceSegment 
(US) are detected, using a much shorter silence 
threshold of about 50ms. Such short silence 
thresholds allow the system to give very fast re-
sponses (such as backchannels). Information 
about US boundaries is sent directly from the 
ASR to the Vocalizer. As Figure 3 illustrates, the 
grounded-in links can be followed to derive the 
timing of IUs at different levels of processing.  
The system output is also modelled using IUs 
at different processing levels. The widest-
spanning IU on the output side is the 
SpeechPlan. The rendering of a SpeechPlan 
will result in a sequence of SpeechSegment?s, 
where each SpeechSegment represents a con-
tinuous audio rendering of speech, either as a 
synthesised string or a pre-recorded audio file. 
For example, the plan may be to say ?okay, a red 
doll, here is a nice doll?, consisting of three seg-
ments. Now, there are two requirements that we 
need to meet. First, the output should be varied: 
the system should not give exactly the same re-
sponse every time to the same request. But, as 
we will see, the output in an incremental system 
must also be flexible, as speech plans are incre-
mentally produced and amended. In order to re-
lieve the Action Manager of the burden of vary-
ing the output and making time-critical adjust-
ments, we model the SpeechPlan as a directed 
graph, where each edge is associated with a 
SpeechSegment, as shown in Figure 4. Thus, the 
Action Manager may asynchronously plan (a set 
of possible) responses, while the Vocalizer se-
lects the rendering path in the graph and takes 
care of time-critical synchronization. To control 
the rendering, each SpeechSegment has the 
properties optional, committing, selfDelay 
and otherDelay, as described in the next sec-
tion. It must also be possible for an incremental 
system to interrupt and make self-repairs in the 
middle of a SpeechSegment. Therefore, each 
SpeechSegment may also be decomposed into an 
array of SpeechUnit?s, where each SpeechUnit 
contains pointers to the audio rendering in the 
SpeechSegment. 
3.2 Producing and consuming SpeechPlans 
The SpeechPlan does not need to be complete 
before the system starts to speak. An example of 
this is shown in Figure 4. As more words are 
recognised by the ASR, the Action Manager may 
add more SpeechSegment?s to the graph. Thus, 
the system may start to say ?it costs? before it 
knows which object is being talked about.  
 
w1 how w2 much w3 is w4 the w5 doll w6
eh
well
s1
you can have it for
it costs
let?s say s3 s640 crowns
 
Figure 4: The right buffer of an ASR (top) and the 
SpeechPlan that is incrementally produced (bottom). 
Vertex s1 is associated with w1, s3 with w3, etc. Op-
tional, non-committing SpeechSegment?s are marked 
with dashed outline. 
The SpeechPlan has a pointer called 
finalVertex. When the Vocalizer reaches the 
finalVertex, the SpeechPlan is completely 
realised. If finalVertex is not set, it means that 
the SpeechPlan is not yet completely con-
structed. The SpeechSegment property 
optional tells whether the segment needs to be 
realised or if it could be skipped if the 
finalVertex is in sight. This makes it possible 
to insert floor-keeping SpeechSegment?s (such 
as ?eh?) in the graph, which are only realised if 
needed. The Vocalizer also keeps track of which 
SpeechSegment?s it has realised before, so that it 
can look ahead in the graph and realise a more 
varied output. Each SpeechSegment may carry a 
semantic representation of the segment (a 
Concept). This is sent by the Vocalizer to the 
Contextualizer as soon as the segment has been 
realised. 
The SpeechSegment properties selfDelay 
and otherDelay regulate the timing of the out-
put (as illustrated in Figure 3). They specify the 
number of milliseconds that should pass before 
the Vocalizer starts to play the segment, depend-
ing on the previous speaker. By setting the 
otherDelay of a segment, the Action Manager 
may delay the response depending on how cer-
tain it is that it is appropriate to speak, for exam-
ple by considering pitch and semantic complete-
ness. (See Raux & Eskenazi (2008) for a study 
4
on how such dynamic delays can be derived us-
ing machine learning.)  
If the user starts to speak (i.e., a new 
UtteranceSegment is initiated) as the system is 
speaking, the Vocalizer pauses (at a SpeechUnit 
boundary) and waits until it has received a new 
response from the Action Manager. The Action 
Manager may then choose to generate a new re-
sponse or simply ignore the last input, in which 
case the Vocalizer continues from the point of 
interruption. This may happen if, for example, 
the UtteranceSegment was identified as a back-
channel, cough, or similar. 
3.3 Self-repairs  
As Figure 3 shows, a SpeechPlan may be 
grounded in a user CA (i.e., it is a response to 
this CA). If this CA is revoked, or if the 
SpeechPlan is revised, the Vocalizer may initial-
ize a self-repair. The Vocalizer keeps a list of the 
SpeechSegment?s it has realised so far. If the 
SpeechPlan is revised when it has been partly 
realised, the Vocalizer compares the history with 
the new graph and chooses one of the different 
repair strategies shown in Table 2. In the best 
case, it may smoothly switch to the new plan 
without the user noticing it (covert repair). In 
case of a unit repair, the Vocalizer searches for a 
zero-crossing point in the audio segment, close to 
the boundary pointed out by the SpeechUnit.  
 
covert 
segment 
repair 
you are right it is blue
you are right they are blue
 
overt 
segment 
repair 
you are right it is blue
you are wrong it is redsorry
 
covert 
unit 
repair 
you are right it is blue
you are wrong it is red
 
overt 
unit 
repair 
you are right it is blue
you are wrong it is red
sorry
 
Table 2: Different types of self-repairs. The shaded 
boxes show which SpeechUnit?s have been realised, 
or are about to be realised, at the point of revision. 
The SpeechSegment property committing 
tells whether it needs to be repaired if the 
SpeechPlan is revised. For example, a filled 
pause such as ?eh? is not committing (there is no 
need to insert an editing term after it), while a 
request or an assertion usually is. If (parts of) a 
committing segment has already been realised 
and it cannot be part of the new plan, an overt 
repaired is made with the help of an editing term 
(e.g., ?sorry?). When comparing the history with 
the new graph, the Vocalizer searches the graph 
and tries to find a path so that it may avoid mak-
ing an overt repair. For example if the graph in 
Figure 4 is replaced with a corresponding one 
that ends with ?60 crowns?, and it has so far 
partly realised ?it costs?, it may choose the cor-
responding path in the new SpeechPlan, making 
a covert repair. 
4 A Wizard-of-Oz experiment 
A Wizard-of-Oz experiment was conducted to 
test the usefulness of the model outlined above. 
All modules in the system were fully functional, 
except for the ASR, since not enough data had 
been collected to build language models. Thus, 
instead of using ASR, the users? speech was 
transcribed by a Wizard. As discussed in section 
1.1, a common problem is the time it takes for 
the Wizard to transcribe incoming utterances, 
and thus for the system to respond. Therefore, 
this is an interesting test-case for our model. In 
order to let the system respond as soon as the 
user finished speaking, even if the Wizard hasn?t 
completed the transcription yet, a VAD is used. 
The setting is shown in Figure 5 (compare with 
Figure 2). The Wizard may start to type as soon 
as the user starts to speak and may alter whatever 
he has typed until the return key is pressed and 
the hypothesis is committed. The word buffer is 
updated in exactly the same manner as if it had 
been the output of an ASR.  
User VAD
Vocalizer
Speech
Speech
Interpreter
Word
ContextualizerActionManager
Utterance
Segment
Wizard
 
Figure 5: The system architecture used in the Wizard-
of-Oz experiment. 
For comparison, we also configured a non-
incremental version of the same system, where 
nothing was sent from the Wizard until he com-
5
mitted by pressing the return key. Since we did 
not have mature models for the Interpreter either, 
the Wizard was allowed to adapt the transcrip-
tion of the utterances to match the models, while 
preserving the semantic content. 
4.1 The DEAL domain 
The system that was used in the experiment was 
a spoken dialogue system for second language 
learners of Swedish under development at KTH, 
called DEAL (Hjalmarsson et al, 2007). The 
scene of DEAL is set at a flea market where a 
talking agent is the owner of a shop selling used 
goods. The student is given a mission to buy 
items at the flea market getting the best possible 
price from the shop-keeper. The shop-keeper can 
talk about the properties of goods for sale and 
negotiate about the price. The price can be re-
duced if the user points out a flaw of an object, 
argues that something is too expensive, or offers 
lower bids. However, if the user is too persistent 
haggling, the agent gets frustrated and closes the 
shop. Then the user has failed to complete the 
task.  
For the experiment, DEAL was re-
implemented using the Jindigo framework. Fig-
ure 6 shows the GUI that was shown to the user. 
 
 
Figure 6: The user interface in DEAL. The object on 
the table is the one currently in focus. Example ob-
jects are shown on the shelf. Current game score, 
money and bought objects are shown on the right. 
4.2 Speech segments in DEAL 
In a previous data collection of human-human 
interaction in the DEAL domain (Hjalmarsson, 
2008) it was noted that about 40% of the speaker 
turns were initiated with standardized lexical ex-
pressions (cue phrases) or filled pauses. Such 
speech segments commit very little semantically 
to the rest of the utterance and are therefore very 
useful as initiations of utterances, since such 
speech segments can be produced immediately 
after the user has stopped speaking, allowing the 
Wizard to exploit the additional time to tran-
scribe the rest of the utterance.  
The DEAL corpus was used to create utter-
ance initial speech segments for the experiment. 
The motivation to use speech segments derived 
from human recordings was to make the system 
sound convincing in terms of both lexical choice 
and intonation. In particular, we wanted a reper-
toire of different types of filled pauses and feed-
back expression such as ?eh? and ?mm? in order 
to avoid a system that sounds monotone and re-
petitive. First, a number of feedback expression 
such as ?ja?, ?a?, ?mm? (Eng: ?yes?), filled 
pauses such as ?eh?, ?ehm? and expressions used 
to initiate different domain specific speech acts 
(for example ?it costs? and ?let me see?) were 
extracted.  The segments were re-synthesized 
using Expros, a tool for experimentation with 
prosody in diphone voices (Gustafson & Edlund, 
2008). Based on manual transcriptions and sound 
files, Expros automatically extracts pitch, dura-
tion and intensity from the human voice and cre-
ates a synthetic version using these parameters. 
In the speech plan, these canned segments were 
mixed with generated text segments (for example 
references to objects, prices, etc) that were syn-
thesized and generated on-line with the same 
diphone voice. 
An example interaction with the incremental 
version of the system is shown in Table 3. S.11 
exemplifies a self-correction, where the system 
prepares to present another bid, but then realizes 
that the user?s bid is too low to even consider. A 
video (with subtitles) showing an interaction 
with one of the users can be seen at 
http://www.youtube.com/watch?v=cQQmgItIMvs. 
 
S.1 [welcome] [how may I help you] 
U.2 I want to buy a doll 
S.3 [eh] [here is] [a doll] 
U.4 how much is it? 
S.5 [eh] [it costs] [120 crowns] 
U.6 that is too expensive 
how much is the teddy bear? 
S.7 [well] [you can have it for] [let?s see] 
[40 crowns] 
U.8 I can give you 30 crowns 
S.9 [you could have it for] [37 crowns] 
U.10 I can give you 10 crowns 
S.11 [let?s say] [or, I mean] [that is way too 
little] 
Table 3: An example DEAL dialogue (translated from 
Swedish). Speech segments are marked in brackets. 
6
4.3 Experimental setup 
In order to compare the incremental and non-
incremental versions of the system, we con-
ducted an experiment with 10 participants, 4 
male and 6 female. The participants were given a 
mission: to buy three items (with certain charac-
teristics) in DEAL at the best possible price from 
the shop-keeper. The participants were further 
instructed to evaluate two different versions of 
the system, System A and System B. However, 
they were not informed how the versions dif-
fered. The participants were lead to believe that 
they were interacting with a fully working dia-
logue system and were not aware of the Wizard-
of-Oz set up. Each participant interacted with the 
system four times, first two times with each ver-
sion of the system, after which a questionnaire 
was completed. Then they interacted with the 
two versions again, after which they filled out a 
second questionnaire with the same questions. 
The order of the versions was balanced between 
subjects.  
The mid-experiment questionnaire was used to 
collect the participants? first opinions of the two 
versions and to make them aware of what type of 
characteristics they should consider when inter-
acting with the system the second time. When 
filling out the second questionnaire, the partici-
pants were asked to base their ratings on their 
overall experience with the two system versions. 
Thus, the analysis of the results is based on the 
second questionnaire. In the questionnaires, they 
were requested to rate which one of the two ver-
sions was most prominent according to 8 differ-
ent dimensions: which version they preferred; 
which was more human-like, polite, efficient, and 
intelligent; which gave a faster response and bet-
ter feedback; and with which version it was eas-
ier to know when to speak. All ratings were done 
on a continuous horizontal line with System A on 
the left end and System B on the right end. The 
centre of the line was labelled with ?no differ-
ence?.  
The participants were recorded during their in-
teraction with the system, and all messages in the 
system were logged.  
4.4 Results 
Figure 7 shows the difference in response time 
between the two versions. As expected, the in-
cremental version started to speak more quickly 
(M=0.58s, SD=1.20) than the non-incremental 
version (M=2.84s, SD=1.17), while producing 
longer utterances. It was harder to anticipate 
whether it would take more or less time for the 
incremental version to finish utterances. Both 
versions received the final input at the same 
time. On the one hand, the incremental version 
initiates utterances with speech segments that 
contain little or no semantic information. Thus, if 
the system is in the middle of such a segment 
when receiving the complete input from the 
Wizard, the system may need to complete this 
segment before producing the rest of the utter-
ance. Moreover, if an utterance is initiated and 
the Wizard alters the input, the incremental ver-
sion needs to make a repair which takes addi-
tional time. On the other hand, it may also start 
to produce speech segments that are semantically 
relevant, based on the incremental input, which 
allows it to finish the utterance more quickly. As 
the figure shows, it turns out that the average 
response completion time for the incremental 
version (M=5.02s, SD=1.54) is about 600ms 
faster than the average for non-incremental ver-
sion (M=5.66s, SD=1.50), (t(704)=5.56, 
p<0.001).  
 
0,00
1,00
2,00
3,00
4,00
5,00
6,00
start end length
Se
co
nd
s
inc
non
 
Figure 7: The first two column pairs show the average 
time from the end of the user?s utterance to the start 
of the system?s response, and from the end of the 
user?s utterance to the end of the system?s response. 
The third column pair shows the average total system 
utterance length (end minus start).  
In general, subjects reported that the system 
worked very well. After the first interaction with 
the two versions, the participants found it hard to 
point out the difference, as they were focused on 
solving the task. The marks on the horizontal 
continuous lines on the questionnaire were 
measured with a ruler based on their distance 
from the midpoint (labelled with ?no difference?) 
and normalized to a scale from -1 to 1, each ex-
treme representing one system version. A Wil-
coxon Signed Ranks Test was carried out, using 
these rankings as differences. The results are 
shown in Table 4. As the table shows, the two 
versions differed significantly in three dimen-
sions, all in favour of the incremental version. 
7
Hence, the incremental version was rated as 
more polite, more efficient, and better at indicat-
ing when to speak. 
 
 diff z-value p-value 
preferred 0.23 -1.24 0.214 
human-like 0.15 -0.76 0.445 
polite 0.40 -2.19 0.028* 
efficient 0.29 -2.08 0.038* 
intelligent 0.11 -0.70 0.484 
faster response 0.26 -1.66 0.097 
feedback 0.08 -0.84 0.400 
when to speak 0.35 -2.38 0.017* 
Table 4: The results from the second questionnaire. 
All differences are positive, meaning that they are in 
favour of the incremental version. 
A well known phenomena in dialogue is that 
of entrainment (or adaptation or alignment), that 
is, speakers (in both human-human and human-
computer dialogue) tend to adapt the conversa-
tional behaviour to their interlocutor (e.g., Bell, 
2003). In order to examine whether the different 
versions affected the user?s behaviour, we ana-
lyzed both the user utterance length and user re-
sponse time, but found no significant differences 
between the interactions with the two versions. 
5 Conclusions & Future work 
This paper has presented a first step towards in-
cremental speech generation in dialogue systems. 
The results are promising: when there are delays 
in the processing of the dialogue, it is possible to 
incrementally produce utterances that make the 
interaction more efficient and pleasant for the 
user.  
As this is a first step, there are several ways to 
improve the model. First, the edges in the 
SpeechPlan could have probabilities, to guide 
the path planning. Second, when the user has 
finished speaking, it should (in some cases) be 
possible to anticipate how long it will take until 
the processing is completed and thereby choose a 
more optimal path (by taking the length of the 
SpeechSegment?s into consideration). Third, a 
lot of work could be done on the dynamic gen-
eration of SpeechSegment?s, considering syntac-
tic and pragmatic constraints, although this 
would require a speech synthesizer that was bet-
ter at convincingly produce conversational 
speech. 
The experiment also shows that it is possible 
to achieve fast turn-taking and convincing re-
sponses in a Wizard-of-Oz setting. We think that 
this opens up new possibilities for the Wizard-of-
Oz paradigm, and thereby for practical develop-
ment of dialogue systems in general.  
6 Acknowledgements 
This research was funded by the Swedish research 
council project GENDIAL (VR #2007-6431). 
References 
Bell, L. (2003). Linguistic adaptations in spoken hu-
man-computer dialogues. Empirical studies of user 
behavior. Doctoral dissertation, Department of 
Speech, Music and Hearing, KTH, Stockholm. 
Dohsaka, K., & Shimazu, A. (1997). System architec-
ture for spoken utterance production in collaborative 
dialogue. In Working Notes of IJCAI 1997 Work-
shop on Collaboration, Cooperation and Conflict in 
Dialogue Systems.  
Fraser, N. M., & Gilbert, G. N. (1991). Simulating 
speech systems. Computer Speech and Language, 
5(1), 81-99. 
Gustafson, J., & Edlund, J. (2008). expros: a toolkit 
for exploratory experimentation with prosody in 
customized diphone voices. In Proceedings of Per-
ception and Interactive Technologies for Speech-
Based Systems (PIT 2008) (pp. 293-296). Ber-
lin/Heidelberg: Springer. 
Hjalmarsson, A., Wik, P., & Brusk, J. (2007). Dealing 
with DEAL: a dialogue system for conversation 
training. In Proceedings of SigDial (pp. 132-135). 
Antwerp, Belgium. 
Hjalmarsson, A. (2008). Speaking without knowing 
what to say... or when to end. In Proceedings of 
SIGDial 2008. Columbus, Ohio, USA. 
Kempen, G., & Hoenkamp, E. (1987). An incremental 
procedural grammar for sentence formulation. Cog-
nitive Science, 11(2), 201-258. 
Kilger, A., & Finkler, W. (1995). Incremental Gen-
eration for Real-Time Applications. Technical Re-
port RR-95-11, German Research Center for Artifi-
cial Intelligence. 
Levelt, W. J. M. (1989). Speaking: From Intention to 
Articulation. Cambridge, Mass., USA: MIT Press. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Schlangen, D., & Skantze, G. (2009). A general, ab-
stract model of incremental dialogue processing. In 
Proceedings of EACL-09. Athens, Greece. 
Skantze, G., & Schlangen, D. (2009). Incremental 
dialogue processing in a micro-domain. In Proceed-
ings of EACL-09. Athens, Greece. 
 
8
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 51?54,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Middleware for Incremental Processing in Conversational Agents
David Schlangen?, Timo Baumann?, Hendrik Buschmeier?, Okko Bu??
Stefan Kopp?, Gabriel Skantze?, Ramin Yaghoubzadeh?
?University of Potsdam ?Bielefeld University ?KTH, Stockholm
Germany Germany Sweden
david.schlangen@uni-potsdam.de
Abstract
We describe work done at three sites on
designing conversational agents capable of
incremental processing. We focus on the
?middleware? layer in these systems, which
takes care of passing around and maintain-
ing incremental information between the
modules of such agents. All implementa-
tions are based on the abstract model of
incremental dialogue processing proposed
by Schlangen and Skantze (2009), and the
paper shows what different instantiations
of the model can look like given specific
requirements and application areas.
1 Introduction
Schlangen and Skantze (2009) recently proposed
an abstract model of incremental dialogue process-
ing. While this model introduces useful concepts
(briefly reviewed in the next section), it does not
talk about how to actually implement such sys-
tems. We report here work done at three different
sites on setting up conversational agents capable
of incremental processing, inspired by the abstract
model. More specifically, we discuss what may
be called the ?middleware? layer in such systems,
which takes care of passing around and maintaining
incremental information between the modules of
such agents. The three approaches illustrate a range
of choices available in the implementation of such
a middle layer. We will make our software avail-
able as development kits in the hope of fostering
further research on incremental systems.1
In the next section, we briefly review the abstract
model. We then describe the implementations cre-
ated at Uni Bielefeld (BF), KTH Stockholm (KTH)
and Uni Potsdam (UP). We close with a brief dis-
cussion of similarities and differences, and an out-
look on further work.
1Links to the three packages described here can be found
at http://purl.org/net/Middlewares-SIGdial2010.
2 The IU-Model of Incremental Processing
Schlangen and Skantze (2009) model incremental
systems as consisting of a network of processing
modules. Each module has a left buffer, a proces-
sor, and a right buffer, where the normal mode of
processing is to take input from the left buffer, pro-
cess it, and provide output in the right buffer, from
where it goes to the next module?s left buffer. (Top-
down, expectation-based processing would work
in the opposite direction.) Modules exchange incre-
mental units (IUs), which are the smallest ?chunks?
of information that can trigger connected modules
into action. IUs typically are part of larger units;
e.g., individual words as parts of an utterance, or
frame elements as part of the representation of an
utterance meaning. This relation of being part of
the same larger unit is recorded through same level
links; the information that was used in creating a
given IU is linked to it via grounded in links. Mod-
ules have to be able to react to three basic situa-
tions: that IUs are added to a buffer, which triggers
processing; that IUs that were erroneously hypothe-
sised by an earlier module are revoked, which may
trigger a revision of a module?s own output; and
that modules signal that they commit to an IU, that
is, won?t revoke it anymore (or, respectively, expect
it to not be revoked anymore).
Implementations of this model then have to re-
alise the actual details of this information flow, and
must make available the basic module operations.
3 Sociable Agents Architecture
BF?s implementation is based on the ?D-Bus? mes-
sage bus system (Pennington et al, 2007), which
is used for remote procedure calls and the bi-
directional synchronisation of IUs, either locally
between processes or over the network. The bus sys-
tem provides proxies, which make the interface of
a local object accessible remotely without copying
data, thus ensuring that any access is guaranteed to
yield up-to-date information. D-Bus bindings exist
for most major programming languages, allowing
51
for interoperability across various systems.
IUs exist as objects implementing a D-Bus in-
terface, and are made available to other modules
by publishing them on the bus. Modules are ob-
jects comprising a main thread and right and left
buffers for holding own IUs and foreign IU proxies,
respectively. Modules can co-exist in one process
as threads or occupy one process each?even dis-
tributed across a network.
A dedicated Relay D-Bus object on the network
is responsible for module administration and up-
date notifications. At connection time, modules
register with the relay, providing a list of IU cat-
egories and/or module names they are interested
in. Category interests create loose functional links
while module interests produce more static ones.
Whenever a module chooses to publish informa-
tion, it places a new IU in its right buffer, while
removal of an IU from the right buffer corresponds
to retraction. The relay is notified of such changes
and in turn invokes a notification callback in all
interested modules synchronising their left buffers
by immediately and transparently creating or re-
moving proxies of those IUs.
IUs consist of the fields described in the abstract
model, and an additional category field which the
relay can use to identify the set of interested mod-
ules to notify. They furthermore feature an optional
custom lifetime, on the expiration of which they
are automatically retracted.
Incremental changes to IUs are simply realised
by changing their attributes: regardless of their lo-
cation in either a right or left buffer, the same setter
functions apply (e.g., set payload). These generate
relay-transported update messages which commu-
nicate the ID of the changed IU. Received update
messages concerning self-owned and remotely-
owned objects are discerned automatically to allow
for special treatment of own IUs. The complete
process is illustrated in Figure 1.
Current state and discussion. Our support for
bi-directional IU editing is an extension to the con-
cepts of the general model. It allows higher-level
modules with a better knowledge of context to re-
vise uncertain information offered by lower levels.
Information can flow both ways, bottom-up and
top-down, thus allowing for diagnostic and causal
networks linked through category interests.
Coming from the field of embodied conversa-
tional agents, and being especially interested in
modelling human-like communication, for exam-
A B
C
IU
IU proxy
Write access
Relay
Data access
Update notification
RBuf LBuf
Interest sets
Figure 1: Data access on the IU proxies is transparently dele-
gated over the D-Bus; module A has published an IU. B and C
are registered in the corresponding interest set, thus receiving
a proxy of this IU in their left buffer. When B changes the IU,
A and C receive update notifications.
ple for on-line production of listener backchannel
feedback, we constantly have to take incremen-
tally changing uncertain input into account. Using
the presented framework consistently as a network
communication layer, we are currently modelling
an entire cognitive architecture for virtual agents,
based on the principle of incremental processing.
The decision for D-Bus as the transportation
layer has enabled us to quickly develop ver-
sions for Python, C++ and Java, and produced
straightforward-to-use libraries for the creation of
IU-exchanging modules: the simplest fully-fledged
module might only consist of a periodically in-
voked main loop callback function and any subset
of the four handlers for IU events (added, removed,
updated, committed).
4 Inpro Toolkit
The InproTK developed at UP offers flexibility on
how tightly or loosely modules are coupled in a
system. It provides mechanisms for sending IU up-
dates between processes via a messaging protocol
(we have used OAA [Cheyer and Martin, 2001], but
other communication layers could also be used) as
well as for using shared memory within one (Java)
process. InproTK follows an event-based model,
where modules create events, for which other mod-
ules can register as Listeners. Module networks are
configured via a system configuration file which
specifies which modules listen to which.
Modules push information to their right, hence
the interface for inter-module communication is
called PushBuffer. (At the moment, InproTK only
implements left-to-right IU flow.) The PushBuffer
interface defines a hypothesis-change method
which a module will call for all its listening mod-
ules. A hypothesis change is (redundantly) charac-
terised by passing both the complete current buffer
state (a list of IUs) as well as the delta between
52
the previous and the current state, leaving listen-
ing modules a choice of how to implement their
internal update.
Modules can be fully event-driven, only trig-
gered into action by being notified of a hypothesis
change, or they can run persistently, in order to cre-
ate endogenous events like time-outs. Event-driven
modules can run concurrently in separate threads or
can be called sequentially by a push buffer (which
may seem to run counter the spirit of incremental
processing, but can be advantageous for very quick
computations for which the overhead of creating
threads should be avoided).
IUs are typed objects, where the base class IU
specifies the links (same-level, grounded-in) that
allow to create the IU network and handles the
assignment of unique IDs. The payload and addi-
tional properties of an IU are specified for the IU?s
type. A design principle here is to make all relevant
information available, while avoiding replication.
For instance, an IU holding a bit of semantic rep-
resentation can query which interval of input data
it is based on, where this information is retrieved
from the appropriate IUs by automatically follow-
ing the grounded-in links. IU networks ground out
in BaseData, which contains user-side input such
as speech from the microphone, derived ASR fea-
ture vectors, camera feeds from a webcam, derived
gaze information, etc., in several streams that can
be accessed based on their timing information.
Besides IU communication as described in the
abstract model, the toolkit also provides a separate
communication track along which signals, which
are any kind of information that is not seen as incre-
mental hypotheses about a larger whole but as infor-
mation about a single current event, can be passed
between modules. This communication track also
follows the observer/listener model, where proces-
sors define interfaces that listeners can implement.
Finally, InproTK also comes with an extensive
set of monitoring and profiling modules which can
be linked into the module network at any point and
allow to stream data to disk or to visualise it online
through a viewing tool (ANON 2009), as well as
different ways to simulate input (e.g., typed or read
from a file) for bulk testing.
Current state and discussion. InproTK is cur-
rently used in our development of an incremental
multimodal conversational system. It is usable in its
current state, but still evolves. We have built and in-
tegrated modules for various tasks (post-processing
of ASR output, symbolic and statistical natural lan-
guage understanding [ANON 2009a,b,c]). The con-
figuration system and the availability of monitoring
and visualisation tools enables us to quickly test
different setups and compare different implementa-
tions of the same tasks.
5 Jindigo
Jindigo is a Java-based framework for implement-
ing and experimenting with incremental dialogue
systems currently being developed at KTH. In
Jindigo, all modules run as separate threads within
a single Java process (although the modules them-
selves may of course communicate with external
processes). Similarly to InproTK, IUs are mod-
elled as typed objects. The modules in the system
are also typed objects, but buffers are not. Instead,
a buffer can be regarded as a set of IUs that are
connected by (typed) same-level links. Since all
modules have access to the same memory space,
they can follow the same-level links to examine
(and possibly alter) the buffer. Update messages
between modules are relayed based on a system
specification that defines which types of update
messages from a specific module go where. Since
the modules run asynchronously, update messages
do not directly invoke methods in other modules,
but are put on the input queues of the receiving
modules. The update messages are then processed
by each module in their own thread.
Jindigo implements a model for updating buffers
that is slightly different than the two previous ap-
proaches. In this approach, IUs are connected by
predecessor links, which gives each IU (words,
widest spanning phrases from the parser, commu-
nicative acts, etc), a position in a (chronologically)
ordered stream. Positional information is reified by
super-imposing a network of position nodes over
the IU network, with the IUs being associated with
edges in that network. These positional nodes then
give us names for certain update stages, and so
revisions can be efficiently encoded by reference
to these nodes. An example can make this clearer.
Figure 2 shows five update steps in the right buffer
of an incremental ASR module. By reference to po-
sitional nodes, we can communicate easily (a) what
the newest committed IU is (indicated in the figure
as a shaded node) and (b) what the newest non-
revoked or active IU is (i.e., the ?right edge? (RE);
indicated in the figure as a node with a dashed line).
So, the change between the state at time t1 and t2
is signalled by RE taking on a different value. This
53
Figure 2: The right buffer of an ASR module, and update
messages at different time-steps.
value (w3) has not been seen before, and so the
consuming module can infer that the network has
been extended; it can find out which IUs have been
added by going back from the new RE to the last
previously seen position (in this case, w2). At t3, a
retraction of a hypothesis is signalled by a return to
a previous state, w2. All consuming modules have
to do now is to return to an internal state linked
to this previous input state. Commitment is repre-
sented similarly through a pointer to the rightmost
committed node; in the figure, that is for example
w5 at t5.
Since information about whether an IU has been
revoked or committed is not stored in the IU it-
self, all IUs can (if desirable) be defined as im-
mutable objects. This way, the pitfalls of having
asynchronous processes altering and accessing the
state of the IUs may be avoided (while, however,
more new IUs have to be created, as compared to
altering old ones). Note also that this model sup-
ports parallel hypotheses as well, in which case the
positional network would turn into a lattice.
The framework supports different types of up-
date messages and buffers. For example, a parser
may incrementally send NPs to a reference reso-
lution (RR) module that has access to a domain
model, in order to prune the chart. Thus, informa-
tion may go both left-to-right and right-to-left. In
the buffer between these modules, the order be-
tween the NPs that are to be annotated is not im-
portant and there is no point in revoking such IUs
(since they do not affect the RR module?s state).
Current state and discussion. Jindigo uses con-
cepts from (Skantze, 2007), but has been rebuilt
from ground up to support incrementality. A range
of modules for ASR, semantic interpretation, TTS,
monitoring, etc., have been implemented within
the framework, allowing us to do experiments
with complete systems interacting with users. We
are currently using the framework to implement a
model of incremental speech production.
6 Discussion
The three implementations of the abstract IU model
presented above show that concrete requirements
and application areas result in different design de-
cisions and focal points.
While BF?s approach is loosely coupled and han-
dles exchange of IUs via shared objects and a me-
diating module, KTH?s implementation is rather
closely coupled and publishes IUs through a single
buffer that lies in shared memory. UP?s approach
is somewhat in between: it abstracts away from the
transportation layer and enables message passing-
based communication as well as shared memory
transparently through one interface.
The differences in the underlying module com-
munication infrastructure affect the way incremen-
tal IU updates are handled in the systems. In BF?s
framework modules holding an IU in one of their
buffers just get notified when one of the IU?s fields
changed. Conversely, KTH?s IUs are immutable
and new information always results in new IUs
being published and a change to the graph repre-
sentation of the buffer?but this allows an efficient
coupling of module states and cheap revoke op-
erations. Again, UP?s implementation lies in the
middle. Here both the whole new state and the delta
between the old and new buffer is communicated,
which leads to flexibility in how consumers can be
implemented, but also potentially to some commu-
nication overhead.
In future work, we will explore if further gener-
alisations can be extracted from the different im-
plementations presented here. For now, we hope
that the reference architectures presented here can
already be an inspiration for further work on incre-
mental conversational systems.
References
Adam Cheyer and David Martin. 2001. The open
agent architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1):143?148, March.
H. Pennington, A. Carlsson, and A. Larsson. 2007.
D-Bus Specification Version 0.12. http://dbus.free-
desktop.org/doc/dbus-specification.html.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of EACL 2009, Athens,
Greece.
Gabriel Skantze. 2007. Error Handling in Spoken Dia-
logue Systems. Ph.D. thesis, KTH, Stockholm, Swe-
den, November.
54
Proceedings of the SIGDIAL 2013 Conference, pages 163?172,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Exploring the effects of gaze and pauses  
in situated human-robot interaction  
 
Gabriel Skantze, Anna Hjalmarsson, Catharine Oertel 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
gabriel@speech.kth.se, annah@speech.kth.se, catha@kth.se 
 
  
 
Abstract 
In this paper, we present a user study where a 
robot instructs a human on how to draw a 
route on a map, similar to a Map Task. This 
setup has allowed us to study user reactions to 
the robot?s conversational behaviour in order 
to get a better understanding of how to gener-
ate utterances in incremental dialogue systems. 
We have analysed the participants' subjective 
rating, task completion, verbal responses, gaze 
behaviour, drawing activity, and cognitive 
load. The results show that users utilise the ro-
bot?s gaze in order to disambiguate referring 
expressions and manage the flow of the inter-
action. Furthermore, we show that the user?s 
behaviour is affected by how pauses are real-
ised in the robot?s speech.  
1 Introduction 
Dialogue systems have traditionally relied on 
several simplifying assumptions. When it comes 
to temporal resolution, the interaction has been 
assumed to take place with a strict turn-taking 
protocol, where each speaker takes discrete turns 
with noticeable gaps in between. While this as-
sumption simplifies processing, it fails to model 
many aspects of human-human interaction such 
as turn-taking with very short gaps or brief over-
laps and backchannels in the middle of utteranc-
es (Heldner & Edlund, 2010). Recently, re-
searchers have turned to more incremental mod-
els, where the dialogue is processed in smaller 
units (Schlangen & Skantze, 2011). On the out-
put side, this allows dialogue systems to start 
speaking before processing is complete, generat-
ing and synthesizing the response segment by 
segment, until the complete response is realised. 
If a segment is delayed, there will be a pause in 
the middle of the system?s speech. While previ-
ous studies have clearly shown the potential ben-
efits of incremental speech generation (Skantze 
& Hjalmarsson, 2012; Dethlefs et al, 2012; 
Buschmeier et al, 2012), there are few studies on 
how users react to pauses in the middle of the 
system?s speech.  
Apart from the real-time nature of spoken in-
teraction, spoken dialog technology has for a 
long time also neglected the physical space in 
which the interaction takes place. In application 
scenarios which involve situated interaction, 
such as human-robot interaction, there might be 
several users talking to the system at the same 
time (Bohus & Horvitz, 2010), and there might 
be physical objects in the surroundings that the 
user and the system refer to during the interac-
tion (Boucher et al, 2012). In such settings, gaze 
plays a very important role in the coordination of 
joint attention and turn-taking. However, it is not 
clear to what extent humans are able to utilize 
the gaze of a robot and respond to these cues.  
Here, we present a user study where a robot 
instructs a human on how to draw a route on a 
map, similar to a Map Task. The nature of this 
setting allows us to study the two phenomena 
outlined above. First, we want to understand how 
a face-to-face setting facilitates coordination of 
actions between a robot and a user, and how well 
humans can utilize the robot's gaze to disambig-
uate referring expressions in situated interaction. 
The second purpose of this study is to investigate 
how the system can either inhibit or encourage 
different types of user reactions while pausing by 
using filled pauses, gaze and syntactic complete-
ness.   
2 Background 
2.1 Gaze in situated interaction 
Gaze is one of the most studied visual cues in 
face-to-face interaction, and it has been associat-
ed with a variety of functions, such as managing 
attention (Vertegaal et al, 2001), expressing in-
timacy and exercising social control (Kleinke, 
163
1986), highlighting the information structure of 
the propositional content of speech (Cassell, 
1999) as well as coordinating turn-taking 
(Duncan, 1972). One of the most influential pub-
lications on this subject (Kendon, 1967) shows 
that speakers gaze away when initiating a new 
turn. At the end of a turn, in contrast, speakers 
shift their gaze towards their interlocutors as to 
indicate that the conversational floor is about to 
become available. Furthermore, it has been 
shown that gaze plays an important role in col-
laborative tasks. In a map task study by Boyle et 
al. (1994), it was shown that speakers in a face-
to-face setting interrupt each other less and use 
fewer turns, words, and backchannels per dia-
logue than speakers who can not see each other. 
A lot of research has also been done on how 
gaze can be used to facilitate turn-taking with 
robots (Mutlu et al, 2006; Al Moubayed et al, 
2013) and embodied conversational agents 
(Torres et al, 1997). Several studies have also 
explored situated human-robot interaction, where 
the interlocutors sit around a table with objects 
that can be referred to, thus constituting a shared 
space of attention (Yoshikawa et al, 2006; John-
son-Roberson et al, 2011). However, there are 
very few studies on how the robot?s gaze at ob-
jects in the shared visual scene may improve task 
completion in an interactive setting. One excep-
tion is a controlled experiment presented by 
Boucher et al (2012), where the iCub robot in-
teracted with human subjects. While the study 
showed that humans could utilize the robot?s 
gaze, the interaction was not that of a free con-
tinuous dialogue.  
Similarly to the study presented here, Nakano 
et al (2003) presented a system that describes a 
route to a user in a face-to-face setting. Based on 
studies of human-human interaction, they im-
plemented a model of face-to-face grounding. 
However, they did not provide a detailed analysis 
of the users? behaviour when interacting with 
this system. 
Even if we successfully manage to model hu-
man-like behaviour in a system, it is not certain 
to what extent humans react to these signals 
when interacting with a robot. In the current 
work, we investigate to what extent the robot?s 
gaze can be used to: (1) help the user disambigu-
ate referring expressions to objects in the shared 
visual scene, and (2) to either inhibit or encour-
age different types of user reactions while the 
system pauses or at turn endings. 
2.2 Pauses in the system's speech 
Speakers in dialogue produce speech piece by 
piece as the dialogue progresses. When starting 
to speak, dialogue participants typically do not 
have a complete plan of how to say something or 
even what to say. Yet, they manage to rapidly 
integrate information from different sources in 
parallel and simultaneously plan and realize new 
dialogue contributions (Levelt, 1989). Still, 
pauses occur frequently within utterances and it 
has been shown that these play a significant role 
in human-human dialogue (for an overview, see 
Rochester, 1973). For example, the timing and 
duration of pauses have important structural 
functions (Goldman-Eisler, 1972), pauses (filled 
and silent) are associated with high cognitive 
load and planning difficulties (Brennan & Wil-
liams, 1995), and whether a pause is detected or 
not does not only depend on duration but also on 
its linguistic context (Boomer & Dittmann, 
1962). 
Recently, several studies have looked into the 
possibilities of replicating the incremental behav-
iour of humans in human-machine interaction. 
Work on incremental speech generation has fo-
cused on the underlying system architecture 
(Schlangen & Skantze, 2011), how to incremen-
tally react to events that occur while realizing an 
utterance (Dohsaka & Shimazu, 1997, 
Buschmeier et al, 2012), and how to make the 
incremental processes more efficient in order to 
reduce the system?s response time (e.g. Dethlefs 
et al, 2012). In a recent study, we implemented a 
model of incremental speech generation in a dia-
logue system (Skantze & Hjalmarsson, 2012). By 
allowing the system to generate and synthesize 
the response segment by segment, the system 
could start to speak before the processing of the 
input was complete. However, if a system seg-
ment was delayed for some reason, the system 
generated a response based on the information 
obtained so far or by generating a pause (filled or 
unfilled). The system also employed self-repairs 
when the system needed to revise an already re-
alised speech segment. Despite these disfluencies 
(filled pauses and self-repairs), an evaluation of 
the system showed that in comparison to a non-
incremental version, the incremental version had 
a shorter response time and was perceived as 
more efficient by the users. 
However, pauses do not only have to be a 
side-effect of processing delays. Pauses could 
also be used wisely to chunk longer instructions 
into shorter segments, giving the user enough 
164
time to process the information. In this case, the 
system should instead invite user reactions dur-
ing the course of its utterance. In the current 
work, we investigate to what extent the system 
can use filled pauses, syntactic completeness and 
gaze as cues to either inhibit or encourage the 
user to react when the system pauses.  
3 Human-robot Map Task data 
Map Task is a well establish experimental para-
digm for collecting data on human-human dia-
logue [30]. Typically, an instruction-giver has a 
map with landmarks and a route, and is given the 
task of describing this route to an instruction-
follower, who has a similar map but without the 
route drawn on it. In a previous study, (Skantze, 
2012) we used this paradigm for collecting data 
on how humans elicit feedback in human-
computer dialogue. In that study, the human was 
the instruction-giver. In the current study, we use 
the same paradigm for a human-robot dialogue, 
but here the robot is the instruction-giver and the 
human is the instruction-follower. This has re-
sulted in a rich multi-modal corpus of various 
types of user reactions to the robot?s instructions, 
which vary across conditions.  
 
Figure 1: The experimental setup. 
3.1 A Map Task dialogue system 
The experimental setup is shown in Figure 1. 
The user is seated opposite to the robot head 
Furhat (Al Moubayed et al, 2013), developed at 
KTH. Furhat uses a facial animation model that 
is back-projected on a static mask. The head is 
mounted on a neck (with 3 degrees of freedom), 
which allows the robot to direct its gaze using 
both eye and head movements. The dialogue sys-
tem was implemented using the IrisTK frame-
work developed at KTH (Skantze & Al Mou-
bayed, 2012), which provides a set of modules 
for input and output, including control of Furhat 
(facial gestures, eye and head movements), as 
well as a statechart-based authoring language for 
controlling the flow of the interaction. For 
speech synthesis, we used the CereVoice unit 
selection synthesizer developed by CereProc 
(www.cereproc.com). 
Between the user and the robot lies a large 
map printed on paper. In addition, the user has a 
digital version of the map presented on a screen 
and is given the task to draw the route that the 
robot describes with a digital pen. However, the 
landmarks on the user?s screen are blurred and 
therefore the user also needs to look at the large 
map in order to identify the landmarks. This map 
thereby constitutes a target for joint attention. 
While the robot is describing the route, its gaze is 
directed at the landmarks under discussion (on 
the large map), which should help the user to 
disambiguate between landmarks. In a previous 
study, we have shown that human subjects can 
identify the target of Furhat's gaze with an accu-
racy that is very close to that of observing a hu-
man (Al Moubayed et al, 2013). At certain plac-
es in the route descriptions, the robot also looks 
up at the user. A typical interaction between the 
robot and a user is shown in Table 1. As the ex-
ample illustrates, each instruction is divided into 
two parts with a pause in between, which results 
in four phases per instruction: Part I, Pause, Part 
II and Release. Whereas user responses are not 
mandatory in the Pause phase (the system will 
continue anyway after a short silence threshold, 
as in U.2), the Release requires a verbal re-
sponse, after which the system will continue. We 
have explored three different realisations of 
pauses, which were systematically varied in the 
experiment: 
COMPLETE: Pauses preceded by a syntactically 
complete  phrase (R.5). 
INCOMPLETE: Pauses preceded by a syntactical-
ly incomplete phrase (R.9).  
FILLED: Pauses preceded by a filled pause (R.1). 
The phrase before the filled pause was some-
times incomplete and sometimes complete. 
To make the conditions comparable, the amount 
of information given before the pauses was bal-
anced between conditions. Thus, the incomplete 
phrases still contained an important piece of in-
formation and the pause was inserted in the be-
ginning of the following phrase (as in R.9).  
  
165
Table 1: An example interaction. 
Turn Activity Phase 
R.1 [gazing at map] continue towards the 
lights, ehm... 
Part I 
U.2 [drawing] Pause 
R.3 until you stand south of the stop 
lights [gazing at user] 
Part II 
U.4 [drawing] alright [gazing at robot] Release 
R.5 [gaze at map] continue and pass east 
of the lights... 
Part I 
U.6 okay [drawing] Pause 
R.7 ...on your way towards the tower 
[gaze at user] 
Part II 
U.8 Could you take that again? Release 
R.9 [gaze at map] Continue to the large 
tower, you pass... 
Part I 
U.10 [drawing] Pause 
R.11 ...east of the stop lights [gaze at user] Part II 
U.12 [drawing] okay, I am at the tower Release 
 
 
Figure 2: An example map. 
Given the current limitations of conversational 
speech recognition, and lack of data relevant for 
this task, we needed to employ some trick to be 
able to build a system that could engage in this 
task in a convincing way in order to evoke natu-
ral reactions from the user. One possibility would 
be to use a Wizard-of-Oz setup, but that was 
deemed to be infeasible for the time-critical be-
haviour that is under investigation here. Instead, 
we employed a trick similar to the one used in 
(Skantze, 2012). Although the users are told that 
the robot cannot see their drawing behaviour, the 
drawing on the digital map, together with a voice 
activity detector that detects the user?s verbal 
responses, is actually used by the system to se-
lect the next action. An example of a map can be 
seen in Figure 2. On the intended route (which 
obviously is not shown on the user?s screen), a 
number of hidden ?spots? were defined ? posi-
tions relative to some landmark (e.g. ?east of the 
field?). Each instruction from the system was 
intended to guide the user to the next hidden 
spot. Each map also contained an ambiguous 
landmark reference (as ?the tower? in the exam-
ple). 
Pilot studies showed that there were three 
basic kinds of verbal reactions from the user: (1) 
an acknowledgement of some sort, encouraging 
the system to continue, (2) a request for repeti-
tion, or (3) a statement that some misunderstand-
ing had occurred. By combining the length of the 
utterance with the information about the progres-
sion of the drawing, these could be distinguished 
in a fairly robust manner. How this was done is 
shown in Table 2. Notice that this scheme allows 
for both short and long acknowledgements (U.4, 
U.6 and U.12 in the example above), as well as 
clarification requests (U.8). It also allows us to 
explore misunderstandings, i.e. cases where the 
user thinks that she is at the right location and 
makes a short acknowledgement, while she is in 
fact moving in the wrong direction. Such prob-
lems are usually detected and repaired in the fol-
lowing turns, when the system continues with the 
instruction from the intended spot and the user 
objects with a longer response. This triggers the 
system to either RESTART the instruction from a 
previous spot where the user is known to have 
been ("I think that we lost each other, could we 
start again from where you were at the bus 
stop?"), or to explicitly CHECK whether the user 
is at the intended location ("Are you at the bus 
stop?"), which helps the user to correct the path.  
Table 2: The system?s action selection based on 
the user?s voice activity and drawing. 
User  
response 
Drawing Action 
Short/Long Continues to the 
next spot 
CONTINUE 
Short/Long Still at the same 
spot 
REPHRASE 
Short (<1s.) At the wrong spot CONTINUE (with  
misunderstanding) 
Long (>1s.) At the wrong spot RESTART or CHECK 
No resp. Any CHECK 
3.2 Experimental conditions  
In addition to the utterance-level conditions 
(concerning completeness) described above, 
three dialogue-level conditions were implement-
ed:  
CONSISTENT gaze (FACE): The robot gazes at 
the landmark that is currently being described 
during the phases Part I, Pause and Part II. In 
166
accordance with the findings in for example 
Kendon (1967), the robot looks up at the end 
of phase Part II, seeking mutual gaze with the 
user during the Release phase. 
RANDOM gaze (FACE): A random gaze behav-
iour, where the robot randomly shifts between 
looking at the map (at no particular landmark) 
and looking at the user, with an interval of 5-
10 seconds. 
NOFACE: The robot head was hidden behind a 
paper board so that the user could not see it, 
only hear the voice. 
3.3 Data collection and analysis 
We collected a corpus of 24 subjects interacting 
with the system, 20 males and 4 females between 
the ages of 21-47. Although none of them were 
native speakers, all of them had a high proficien-
cy in English. First, each subject completed a 
training dialogue and then six dialogues that 
were used for the analysis. For each dialogue, 
different maps were used. The subjects were di-
vided into three groups with 8 subjects in each:  
Group A: Three maps with the CONSISTENT 
(FACE) version and three maps with the 
NOFACE version. All pauses were 1.5 s. long. 
Group B: Three maps with the RANDOM (FACE) 
version and three maps with the NOFACE ver-
sion. All pauses were 1.5 s. long. 
Group C: Three maps with the CONSISTENT ver-
sion and three maps with the NOFACE ver-
sion. All pauses were 2-4 s. long (varied ran-
domly with a uniform distribution).  
For all groups, the order between the FACE 
and the NOFACE condition was varied and bal-
anced. Group A and Group B allow us to explore 
differences between the CONSISTENT and RAN-
DOM versions. This is important, since it is not 
evident to what extent the mere presence of a 
face affects the interaction and to what extent 
differences are due to a consistent gazing behav-
iour. Group C was added to the data collection 
since we wanted to be able to study users' behav-
iour during pauses in more detail. Thus, Group C 
will only be used to study within-group effects of 
different pause types and will not be compared 
against the other groups.  
After the subjects had interacted with the sys-
tem, they filled out a questionnaire. First, they 
were requested to rate with which version (FACE 
or NOFACE) it was easier to complete the task. 
Second, the participants were requested to rate 
whether the robot?s gaze was helpful or confus-
ing when it came to task completion, landmark 
identification and the timing of feedback. All 
ratings were done on a continuous horizontal line 
with either FACE or ?the gaze was helpful? on 
the left end and NOFACE or ?the gaze was con-
fusing? on the right end. The centre of the line 
was labelled with ?no difference?. 
During the experiments, the users? speech and 
face were recorded and all events in the system 
and the drawing activity were automatically 
logged. Afterwards, the users' voice activity that 
had been automatically detected online was 
manually corrected and transcribed. Using the 
video recordings, the users? gaze was also manu-
ally annotated, depending on whether the user 
was looking at the map, the screen or at the ro-
bot.  
 In this study, we also wanted to explore the 
possibility of measuring cognitive load in hu-
man-robot interaction using EDA (electrodermal 
activity). Hence, in an explorative manner, we 
investigated how the realisation of the system?s 
pauses and the presence of the face affected the 
cognitive costs of processing the system?s in-
structions. For measuring this, we used a weara-
ble EDA device, which exerts a direct current on 
the skin of the subject in order to measure skin 
conductance responses. For these measurements 
as well as the logging of the data the Q-Sensor 
developed by Affectiva 1  was used. The meas-
urements were taken from the fingertips of the 
subjects. The sampling rate was 8 Hz. All post 
processing was carried out in Ledalab2. We first 
applied the Butterworth filter and then carried 
out a Continuous Decomposition Analysis. All 
skin conductance responses (SCR) with a mini-
mum amplitude of 0.01 muS and a minimal dis-
tance of 700ms were used for further analysis. 
Due to problems with the EDA device, we only 
have data for six subjects in Group A, six in 
Group B and none in Group C.  
4 Results 
Analyses of the different measures used here re-
vealed that they were not normally distributed. 
We have therefore consistently used non-
parametric tests. All tests of significance are 
done using two-tailed tests at the .05 level.  
                                                 
1
 http://www.affectiva.com/ 
2
 http://www.ledalab.de/ 
167
4.1 Subjective ratings 
The questionnaire was used to analyse differ-
ences in subjective ratings between Group A and 
B. The marks on the horizontal continuous lines 
in the questionnaire were measured with a ruler 
based on their distance from the midpoint (la-
belled with ?no difference?) and normalized to a 
scale between 0 and 1. A Wilcoxon Signed 
Ranks Test was carried out, using these rankings 
as differences. The results show that the Con-
sistent version differed significantly from the 
midpoint (?no difference?) in four dimensions 
whereas there were no significant differences 
from the midpoint for RANDOM version. More 
specifically, Group A (CONSISTENT) (n=8) found 
it easier to complete the task in the face condi-
tion than in the no face condition (Mdn=0.88, 
Z=-2.54, p=.012). The same group thought that 
the robot?s gaze was helpful rather than confus-
ing when it came to task completion (Mdn=0.84, 
Z=-2.38, p=.017), landmark identification 
(Mdn=0.83, Z=-2.52, p=.012) and to decide 
when to give feedback (Mdn=0.66, Z=-1.99, 
p=.046). The results of the questionnaire are pre-
sented in Figure 3. 
 
Figure 3: The results from the questionnaire. The 
bars show the median rating for Group A (con-
sistent) and Group B (random). 
4.2 Task completion 
Apart from the subjective ratings, we also want-
ed to see whether the face-to-face setting affect-
ed task completion. In order to explore this, we 
analysed the time and number of utterances it 
took for the users to complete the maps. On av-
erage, the dialogues in Group A (CONSISTENT) 
were 2.5 system utterances shorter and 8.9 sec-
onds faster in the FACE condition than in the 
NOFACE condition. For Group B (RANDOM), the 
dialogues were instead 2.3 system utterances and 
17.3 seconds longer in the FACE condition 
(Mann-Whitney U-test, p<.05). Thus, it seems 
like the face facilitates the solving of the task, 
and that this is not just due to the mere presence 
of a face, but that the intelligent gaze behaviour 
actually contributes. In fact, the RANDOM gaze 
worsens the performance, possibly because sub-
jects spent time on trying to make sense of sig-
nals that did not provide any useful information. 
Looking at more local phenomena, it seems 
like there was also a noticeable difference when 
it comes to miscommunication. The dialogues in 
the RANDOM/FACE condition had a total of 18 
system utterances of the type RESTART (vs. 7 in 
CONSISTENT), and a total of 33 CHECK utteranc-
es (vs. 15 in CONSISTENT). A chi-square test 
shows that the differences are statistically signif-
icant (?2(1, N=25) = 4.8, p =.028; ?2(1, N=48) = 
6.75, p=.009). This indicates that the users that 
did not get the CONSISTENT gaze to a larger ex-
tent did not manage to follow the system?s in-
structions, most likely because they did not get 
guidance from the robot?s gaze in disambiguat-
ing referring expressions. 
4.3 Gaze behaviour 
In order to analyse the users? direction of atten-
tion during the dialogues, the manual annotation 
of the participants? gaze was analysed. First, we 
explored how the completion type of the robot's 
utterance affected the users? gaze. In this analy-
sis, FILLED and INCOMPLETE have been merged 
(since there was no difference in the users? gaze 
between these conditions). The percentage of 
gaze at the robot over the four different utterance 
phases for complete and incomplete utterances is 
plotted in Figure A in the Appendix. Note that 
the different phases actually are of different 
lengths depending on the actual content of the 
utterance and the length of the pause. However, 
these lengths have been normalized in order to 
make it possible to analyse the average user be-
haviour. For each phase, a Mann-Whitney U-test 
was conducted. The results show that the per-
centage of gaze at Furhat during the mid-
utterance pause is higher when the first part of 
the utterance is incomplete than when it is com-
plete (U=7573.0, p<.001). There were, however, 
no significant differences in gaze direction be-
tween complete and incomplete utterance during 
the other three phases (p>.05). This indicates that 
users gaze at the robot to elicit a continuation of 
the instruction when it is incomplete. 
Second, we wanted to explore if gaze direction 
can be used as a cue of whether the user will 
provide a verbal response in the pause or not. 
The percentage of gaze at the robot over the four 
utterance phases for system utterances with and 
0 0.5 1
RANDOM CONSISTENT
Did the robot?s gaze help you to 
understand which landmark he was 
talking about? (0=confusing, 1=helpful)
Did the robot?s gaze help you to 
complete the task?(0=confusing, 
1=helpful)
Did the robot?s gaze affect your 
decisions of when to give feedback?
(0=confusing, 1=helpful)
When was it easier to complete
the task? (noFace=0, face=1)
?No difference?
168
without user response in the pause is plotted in 
Figure B in the Appendix. For each phase, a 
Mann-Whitney U-test was conducted. The re-
sults show that the percentage of gaze at Furhat 
during the mid-utterance pause (U=1945.5, 
p=.008) and Part II (U=2090.0, p=.008) of the 
utterance is lower when the user gives a verbal 
response compared to when there is no response. 
There were however no significant differences in 
gaze direction between complete and incomplete 
utterance during the other two phases (p>.05). 
4.4 Verbal feedback behaviour 
Apart from the user?s gaze behaviour, we also 
wanted to see whether syntactic completeness 
before pauses had an effect on whether the users 
gave verbal responses in the pause. Figure 4 
shows the extent to which users gave feedback 
within pauses, depending on pause type and 
FACE/NOFACE condition. As can be seen, COM-
PLETE triggers more feedback, FILLED less feed-
back and INCOMPLETE even less. Interestingly, 
this difference is more distinct in the FACE con-
dition (?2(2, N=157) = 10.32, p<.01). In fact, the 
difference is not significant in the NOFACE con-
dition (p >.05).  
 
Figure 4: Presence of feedback depending on 
pause type (Group C). 
In Skantze et al (2013), we have also done a 
more thorough analysis of the verbal acknowl-
edgements from the users. The analysis shows 
that the prosody and lexical choice in these 
acknowledgements ("okay", "yes", "yeah", 
"mm", "mhm", "ah", "alright" and "oh") to some 
extent signal whether the drawing activity is 
about to be initiated or has been completed. The 
analysis also shows how these parameters are 
correlated to the perception of uncertainty. 
4.5 Drawing behaviour 
Whereas gaze and verbal responses can be re-
garded as communicative signals, the users were 
told that the robot could not observe their draw-
ing activity. However, the drawing of the route 
can be regarded as the purpose of the interaction 
and it is therefore important to understand how 
this is affected by the system?s behaviour under 
different conditions. First, we wanted to see how 
the completeness of the robot's utterance in com-
bination with the presence of the face affected 
the drawing activity. In this analysis, FILLED and 
INCOMPLETE have been merged (since there was 
no clear difference). The mean drawing activity 
over the four phases of the descriptions is plotted 
in Figure C in the Appendix. For each phase, a 
Kruskal-Wallis test was conducted showing that 
there is a significant difference between the con-
ditions in the Pause phase (H(3) = 28.8, p<.001). 
Post-hoc tests showed that FACE/INCOMPLETE 
has a lower drawing activity than the other con-
ditions, and that NOFACE/INCOMPLETE has a 
lower drawing activity than the COMPLETE con-
dition. Thus, INCOMPLETE phrases before pauses 
seem to have an inhibiting effect on the user?s 
drawing activity in general, but this effect ap-
pears to be much larger in the FACE condition. 
Second, we aimed to investigate to what ex-
tent the robot?s gaze at landmarks during ambig-
uous references helps users to discriminate be-
tween landmarks.  The mean drawing activity 
over the four phases of the descriptions of am-
biguous landmarks is plotted in Figure D in the 
Appendix. For each phase, a Kruskal-Wallis test 
was conducted showing that there is a significant 
difference between the conditions in the Part II 
phase (H(2)=10.2, p=.006). Post-hoc tests 
showed that CONSISTENT has a higher drawing 
activity than the RANDOM and NOFACE condi-
tions. However, there is no such difference when 
looking at non-ambiguous descriptions. This 
shows that robot?s gaze at the target landmark 
during ambiguous references makes it possible 
for the subjects to start to draw quicker.  
4.6 Cognitive load 
As mentioned above, we also wanted to study the 
cognitive costs of processing the system?s in-
structions, as measured with a wearable EDA 
device. For each system utterance part (Part I and 
Part II), we calculated the sum of the amplitudes 
of the skin conductance responses (SoSCR) dur-
ing the following three seconds. The SoSCR dur-
ing the pause, depending on pause type are 
shown in Figure 5. A Kruskal-Wallis test re-
vealed that there is an overall effect (H(2)=8.7, 
p=.13), and post-hoc tests showed that there is a 
significant difference between utterances which 
are incomplete and those with filled pauses, indi-
169
cating that the syntactic incompleteness without 
a filled pause leads to a higher cognitive load.  
We have no good explanation for this, and we do 
not know whether this is due to how the syntacti-
cally incomplete segments were realised by the 
synthesizer, or whether the same effect would 
appear in human-human interaction. 
 
Figure 5: EDA at different pause types (Group A 
and B). 
A similar analysis was done after both Part I and 
Part II to see if there is any difference in SoSCR 
between ambiguous and non-ambiguous refer-
ences in the different conditions, as shown in in 
Figure 6. No such differences were found for 
Group B, but for Group A, ambiguous references 
were followed by a higher SoSCR in the 
NOFACE condition, indicating that the robot?s 
gaze helps in disambiguating the referring ex-
pressions and reduces cognitive load (Mann-
Whitney U-test; U = 6585, p = .001).  
 
Figure 6: EDA for Group A (CONSISTENT). 
5 Conclusions and Discussion 
In this study, we have investigated to what extent 
the robot?s gaze can be used to: (1) help the user 
disambiguate referring expressions to objects in 
the shared visual scene, and (2) to either inhibit 
or encourage different types of user reactions 
while the system pauses. The  results show  that  
the robot?s gaze behaviour  was  rated  as  help-
ful  rather  than  confusing for  task completion,  
landmark  identification and feedback timing. 
These effects were not present when the robot 
used a random gaze behaviour. The efficiency of 
the gaze was further supported by the time it 
took to complete the task and the number of mis-
understandings. These results in combination 
with a faster drawing activity and lower cogni-
tive load when system?s reference was ambigu-
ous, suggest that the users indeed utilized the 
system?s gaze to discriminate between land-
marks.  
The second purpose of this study was to inves-
tigate to what extent filled pauses, syntactic 
completeness and gaze can be used as cues to 
either inhibit or encourage the user to react in 
pauses. First, the results show that pauses pre-
ceded by incomplete syntactic segments or filled 
pauses appear to inhibit user activity. Thus, our 
analyses of gaze and drawing activity show that 
users give less feedback, draw less and look at 
the robot to a larger extent when the preceding 
system utterance segment is incomplete than 
when it is complete. An interesting observation is 
that the inhibiting effect on drawing activity ap-
pears to be more pronounced in the face-to-face 
condition, which indicates that gaze also plays an 
important role here (since the robot looked down 
at the map during the pauses). Additionally, there 
is less cognitive load when the silence is preced-
ed by a filled pause. These results suggest that 
incomplete system utterances prevent further 
user processing; instead the user waits for more 
input from the system before starting to carry out 
the system?s instruction. After complete utter-
ance segments, however, there is more drawing 
activity and the user looks less at the robot, sug-
gesting that the user has already started to carry 
out the system?s instruction. 
The results presented in this study have impli-
cations for generating multimodal behaviours 
incrementally in dialogue systems for human-
robot interaction. Such a system should be able 
to generate speech and gaze intelligently in order 
to inhibit or encourage the user to act, depending 
on the state of the system's processing. In future 
studies, we plan to extend our previous model of 
incremental speech generation (Skantze & 
Hjalmarsson, 2012) with such capabilities.   
Acknowledgments 
Gabriel Skantze is supported by the Swedish research 
council (VR) project Incremental processing in mul-
timodal conversational systems (2011-6237). Anna 
Hjalmarsson is supported by the Swedish Research 
Council (VR) project Classifying and deploying paus-
es for flow control in conversational systems (2011-
6152). Catharine Oertel is supported by GetHomeSafe 
(EU 7th Framework STREP 288667).  
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Complete Incomplete Filled
So
SC
R 
(m
uS
)
170
References  
Al Moubayed, S., Skantze, G., & Beskow, J. (2013). 
The Furhat Back-Projected Humanoid Head - Lip 
reading, Gaze and Multiparty Interaction. Interna-
tional Journal of Humanoid Robotics, 10(1). 
Anderson, A., Bader, M., Bard, E., Boyle, E., 
Doherty, G., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, H., 
& Weinert, R. (1991). The HCRC Map Task corpus. 
Language and Speech, 34(4), 351-366. 
Bohus, D., & Horvitz, E. (2010). Facilitating multi-
party dialog with gaze, gesture, and speech. In 
Proc ICMI?10. Beijing, China. 
Boomer, D. S., & Dittmann, A. T. (1962). Hesitation 
pauses and juncture pauses in speech. Language 
and Speech, 5, 215-222. 
Boucher, J. D., Pattacini, U., Lelong, A., Bailly, G., 
Elisei, F., Fagel, S., Dominey, P. F., & Ventre-
Dominey, J. (2012). I reach faster when I see you 
look: gaze effects in human-human and human-
robot face-to-face cooperation. Frontiers in neuro-
robotics, 6. 
Boyle, E., Anderson, A., & Newlands, A. (1994). The 
effects of visibility on dialogue and performance 
in a cooperative problem solving task. Language 
and speech, 37(1), 1-20. 
Brennan, S., & Williams, M. (1995). The Feeling of 
Another's knowing: Prosody and Filled Pauses as 
Cues to Listeners about the Metacognitive States 
of Speakers. Journal of Memory and Language, 
34, 383-398. 
Buschmeier, H., Baumann, T., Dosch, B., Kopp, S., & 
Schlangen, D. (2012). Combining incremental 
language generation and incremental speech syn-
thesis for adaptive information presentation. In 
Proceedings of SigDial (pp. 295?303). Seoul, 
South Korea. 
Cassell, J. (1999). Nudge, nudge, wink, wink: Ele-
ments of face-toface conversation for embodied 
conversational agents. In Cassell, J., Suillivan, J., 
Prevost, S., & Churchill, E. (Eds.), Embodied 
Conversational Agents. Cambridge, MA: MIT 
Press. 
Dethlefs, N., Hastie, H., Rieser, V., & Lemon, O. 
(2012). Optimising Incremental Dialogue Deci-
sions Using Information Density for Interactive 
Systems. In Proceedings of the Conference on 
Empirical Methods in Natural Language Pro-
cessing (EMNLP) (pp. 82-93). Jeju, South Korea. 
Dohsaka, K., & Shimazu, A. (1997). System architec-
ture for spoken utterance production in collabora-
tive dialogue. In Working Notes of IJCAI 1997 
Workshop on Collaboration, Cooperation and 
Conflict in Dialogue Systems.  
Duncan, S. (1972). Some Signals and Rules for Tak-
ing Speaking Turns in Conversations. Journal of 
Personality and Social Psychology, 23(2), 283-
292. 
Goldman-Eisler, F. (1972). Pauses, clauses, sentences. 
Language and Speech, 15, 103-113. 
Heldner, M., & Edlund, J. (2010). Pauses, gaps and 
overlaps in conversations. Journal of Phonetics, 
38, 555-568. 
Johnson-Roberson, M., Bohg, J., Skantze, G., Gus-
tafson, J., Carlson, R., Rasolzadeh, B., & Kragic, 
D. (2011). Enhanced Visual Scene Understanding 
through Human-Robot Dialog. In IEEE/RSJ Inter-
national Conference on Intelligent Robots and 
Systems.  
Kendon, A. (1967). Some functions of gaze direction 
in social interaction. Acta Psychologica, 26, 22-
63. 
Kleinke, C. L. (1986). Gaze and eye contact: a re-
search review. Psychological Bulletin, 100, 78-
100. 
Mutlu, B., Forlizzi, J., & Hodgins, J. (2006). A story-
telling robot: Modeling and evaluation of human-
like gaze behavior. In Proceedings of 6th IEEE-
RAS International Conference on Humanoid Ro-
bots (pp. 518-523).  
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. 
(2003). Towards a model of face-to-face ground-
ing. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics (ACL 
2003) (pp. 553-561).  
Rochester, S. R. (1973). The significance of Pauses in 
Spontaneous Speech. Journal of Psycholinguistic 
Research, 2(1). 
Schlangen, D., & Skantze, G. (2011). A General, Ab-
stract Model of Incremental Dialogue Processing. 
Dialogue & Discourse, 2(1), 83-111. 
Skantze, G., & Al Moubayed, S. (2012). IrisTK: a 
statechart-based toolkit for multi-party face-to-
face interaction. In Proceedings of ICMI. Santa 
Monica, CA. 
Skantze, G., & Hjalmarsson, A. (2012). Towards In-
cremental Speech Generation in Conversational 
Systems. Computer Speech & Language, 27(1), 
243-262. 
Skantze, G., Oertel, C., & Hjalmarsson, A. (2013). 
User feedback in human-robot interaction: Proso-
dy, gaze and timing. In Proceedings of Inter-
speech.  
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
Torres, O., Cassell, J., & prevost, S. (1997). Modeling 
gaze behavior as a function of discourse structure. 
Proc. of the First International Workshop on Hu-
man-Computer Conversation. 
Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt, 
A. (2001). Eye gaze patterns in conversations: 
there is more to conversational agents than meets 
the eyes. In Proceedings of ACM Conf. on Human 
Factors in Computing Systems.  
Yoshikawa, Y., Shinozawa, K., Ishiguro, H., Hagita, 
N., & Miyamoto, T. (2006). Responsive robot 
gaze to interaction partner. In Proceedings of ro-
botics: Science and systems.  
171
Appendix 
 
 Part I Pause Part II Release 
Figure A: Average user gaze depending on pause type (Group C). 
 
 Part I Pause Part II Release 
Figure B: Average user gaze depending whether the user responds in the pause (Group A and B). 
 
 Part I Pause Part II Release 
Figure C: Average drawing activity depending on pause type and the presence of the face (Group C). 
 
 Part I Pause Part II Release 
Figure D: Average drawing activity during ambiguous references depending on condition (Group A and B). 
172
Proceedings of the SIGDIAL 2013 Conference, pages 366?368,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
The Map Task Dialogue System:  
A Test-bed for Modelling Human-Like Dialogue 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se  
  
 
  
 
Abstract 
The demonstrator presents a test-bed for 
collecting data on human?computer dia-
logue: a fully automated dialogue system 
that can perform Map Task with a user. 
In a first step, we have used the test-bed 
to collect human?computer Map Task di-
alogue data, and have trained various da-
ta-driven models on it for detecting feed-
back response locations in the user?s 
speech. One of the trained models has 
been tested in user interactions and was 
perceived better in comparison to a sys-
tem using a random model. The demon-
strator will exhibit three versions of the 
Map Task dialogue system?each using a 
different trained data-driven model of 
Response Location Detection.  
1 Introduction 
A common procedure in modelling human-like 
dialogue systems is to collect data on human?
human dialogue and then train models that pre-
dict the behaviour of the interlocutors. However, 
we think that it might be problematic to use a 
corpus of human?human dialogue as a basis for 
implementing dialogue system components. One 
problem is the interactive nature of the task. If 
the system produces a slightly different behav-
iour than what was found in the original data, 
this would likely result in a different behaviour 
in the interlocutor. Another problem is that hu-
mans are likely to behave differently towards a 
system as compared to another human (even if a 
more human-like behaviour is being modelled). 
Yet another problem is that much dialogue be-
haviour is optional and therefore makes the actu-
al behaviour hard to use as a gold standard. 
 
Figure 1: The Map Task system user interface 
To improve current systems, we need both a 
better understanding of the phenomena of human 
interaction, better computational models and bet-
ter data to build these models. An alternative ap-
proach that has proven to be useful is to train 
models on human?computer dialogue data col-
lected through Wizard-of-Oz studies (Dahlb?ck 
et al, 1993). However, the methodology might 
be hard to use when the issue under investigation 
is time-critical behaviour such as back-channels.  
A third alternative is to use a boot-strapping 
procedure, where more and more advanced (or 
human-like) versions of the system are built iter-
atively. After each iteration, users interact with 
the system and data is collected. This data is then 
used to train/improve data-driven models of in-
teraction in the system. A problem here, howev-
er, is how to build the first iteration of the sys-
tem, since many components, e.g., Automatic 
Speech Recognition (ASR), need some data to be 
useful at all.  
In this demonstration we present a test-bed for 
collecting data on time-critical human?computer 
dialogue phenomena: a fully automated dialogue 
system that can perform the Map Task with a 
366
user (Skantze, 2012). In a first step, following 
the boot-strapping procedure, we collected hu-
man?computer Map Task dialogue data using 
this test-bed and then trained various data-driven 
models on this data for detecting feedback re-
sponse locations in user?s speech. A trained 
model has been implemented and evaluated in 
interaction with users?in the same environment 
used for collecting the data (Meena et al, in 
press). The demonstrator will exhibit three ver-
sions of the Map Task dialogue system?each 
using a different trained data-driven model of 
Response Location Detection (RLD). 
2 The Map Task Dialogue System 
Map Task is a common experimental paradigm 
for studying human?human dialogue. In our set-
up, the user (the information giver) is given the 
task of describing a route on a map to the system 
(the information follower). The choice of Map 
Task is motivated partly because the system may 
allow the user to keep the initiative during the 
whole dialogue, and thus only produce responses 
that are not intended to take the initiative, most 
often some kind of feedback. Thus, the system 
might be described as an attentive listener.  
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the second iteration of the sys-
tem that uses a model trained on the collected 
data. To make the human?computer Map Task 
dialogue feasible without any full speech under-
standing we have implemented a trick: the user is 
presented with a map on a screen (see Figure 1) 
and instructed to move the mouse cursor along 
the route as it is being described. The user is told 
that this is for logging purposes, but the real rea-
son for this is that the system tracks the mouse 
position and thus knows what the user is current-
ly talking about. It is thereby possible to produce 
a coherent system behaviour without any speech 
recognition at all, only basic speech detection. 
This often results in a very realistic interaction1.  
The system uses a simple energy-based speech 
detector to chunk the user?s speech into inter-
pausal units (IPUs), that is, periods of speech that 
contain no sequence of silence longer than 200 
ms. Such a short threshold allows the system to 
give backchannels (seemingly) while the user is 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
speaking or take the turn with barely any gap. 
The end of an IPU is a candidate for the RLD 
model to identify whether it is a Response Loca-
tion (RL). 
 
Prosodic 
analysis
Dialogue 
manager
Map
Window
Speech 
detector
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
 
Figure 2: The basic components of the system. 
Each time the RLD detects a RL, the dialogue 
manager produces a response, depending on the 
current state of the dialogue and the position of 
the mouse cursor. Table 1 shows an example 
Map Task dialogue illustrating the different types 
of responses the system could produce. The dia-
logue manager always starts with an Introduction 
and ends with an Ending, once the mouse cursor 
has reached the destination. Between these, it 
selects from the other responses (Reprise Frag-
ment, Clarification and Backchannel), partly 
randomly, but also depending on the length of 
the last user turn (longer turn triggers Repeat or 
Restart) and the current mouse location (e.g., a 
Guess if mouse hovers over the same area for 
some time). 
Table 1: An example Map Task dialogue between a 
user (U) and system (S) [User utterances with ASR 
error(s) are indicated with a *]  
S Introduction Could you help me to find my way to 
the bus stop? 
U  start from the department store 
S Backchannel yeah 
U  and eh 
S Guess Should I start by going west? 
U  yeah do that 
U  then you will get to a meadow and 
when you get to the meadow 
*you get to a meadow until you get 
to the meadow 
S Repeat Eh, could you repeat that? 
U  you go straight and you see a mead-
ow on your right side 
S Clarification  
Request  
A green field? 
U  ehm yeah a field 
S Backchannel mhm 
U  pass the meadow and turn right so 
you are going north  
*the meadow and turn right to your 
going north 
S Backchannel okay 
U  you get to a restaurant 
367
S Reprise 
Fragment 
A restaurant 
U  then you have to turn right and that 
will take you to 
*you have turned right and at will 
take you turn 
S Restart Eh, I think I lost you at the restau-
rant, could you take that again? 
?   
U  at the junction go south and then 
you will get to the bus stop 
S Ending Okay, thanks a lot. 
3 Data-driven models of RLD 
Using the system described in the previous 
section a corpus of 50 human?computer Map 
Task dialogue was collected and used to train a 
data-driven model of RLD. Since we didn?t have 
a sophisticated model of RLD during the first 
iteration a na?ve model was used. This model 
would wait for a random period between 0 and 
800 ms after an IPU ended. If no new IPUs were 
initiated during this period, a RL was detected. 
Each IPU in the corpus was then manually la-
belled as either Hold (a response would be inap-
propriate) or Respond (a response is expected) 
type. On this data various models were trained 
on online extractable features?covering syntax, 
context and prosody. Table 2 illustrates the per-
formance of the various models. Going a step 
further, model #6 was deployed in the Map Task 
dialogue system (with an ASR component) and 
evaluated in user interactions. The result sug-
gests that the trained model provide for smooth 
turn-transitions in contrast to the Random model 
(Meena et al, in press). 
Table 2: Performance of various models of RLD 
[NB: Na?ve Bayes; SVM: Support Vector Machine; 
Models with * will be exhibited in the demonstration] 
# RLD model % accuracy (on ASR results) 
1* Random 50.79% majority class baseline 
2 Prosody 64.5% (SVM learner) 
3 Context 64.8% (SVM learner) 
4* 
Prosody 
+ Context 
69.1% (SVM learner) 
5 Syntax 81.1% (NB learner) 
6* 
Syntax 
+ Prosody  
+ Context 
82.0 % (NB learner) 
4 Future applications 
The Map Task test-bed presented here has the 
potential for modelling other human-like conver-
sational behaviour in dialogue systems: 
Clarification strategies: by deploying explicit 
(did you mean turn right?) and implicit (a reprise 
such as turn right) or elliptical (?right??) clarifi-
cation forms in the grounding process one could 
investigate the efficiency and effectively of these 
human-like clarification strategies.  
User utterance completion: It has been sug-
gested that completion of user utterances by a 
dialogue system would result in human-like con-
versational interactions. However, completing 
user?s utterance at every opportunity may not be 
the best strategy (DeVault et al, 2009). The pre-
sented system could be used to explore when it is 
appropriate to do so. We have observed in our 
data that the system dialogue acts Guess (cf. Ta-
ble 1) and Reprise often helped the dialogue pro-
ceed further ? by completing user utterances ? 
when the user had difficulty describing a land-
mark on a route. 
Visual cues: the system could be integrated in 
a robotic head, such as Furhat (Al Moubayed et 
al., 2013), and visual cues from the user could be 
used for improving the current model of RLD. 
This could be used further to explore the use of 
extra-linguistic system behaviours, such as head 
nods and facial gestures, as feedback responses. 
Acknowledgement 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237) 
References 
Al Moubayed, S., Skantze, G., & Beskow, J. (2013). 
The Furhat Back-Projected Humanoid Head - Lip 
reading, Gaze and Multiparty Interaction. Interna-
tional Journal of Humanoid Robotics, 10(1). 
Dahlb?ck, N., J?nsson, A., & Ahrenberg, L. (1993). 
Wizard of Oz studies ?  why and how. In Proceed-
ings from the 1993 International Workshop on In-
telligent User Interfaces (pp. 193-200).  
DeVault, D., Sagae, K., & Traum, D. (2009). Can I 
Finish? Learning When to Respond to Incremental 
Interpretation Results in Interactive Dialogue. In 
Proceedings of SIGdial (pp. 11-20). London, UK. 
Meena, R., Skantze, G., & Gustafson, J. (in press). A 
Data-driven Model for Timing Feedback in a Map 
Task Dialogue System. To be published in 14th 
Annual Meeting of the Special Interest Group on 
Discourse and Dialogue - SIGdial. Metz, France. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
368
Proceedings of the SIGDIAL 2013 Conference, pages 375?383,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
 
A Data-driven Model for Timing Feedback 
in a Map Task Dialogue System 
 
Raveesh Meena Gabriel Skantze Joakim Gustafson 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
raveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.se 
 
  
 
Abstract 
We present a data-driven model for de-
tecting suitable response locations in the 
user?s speech. The model has been 
trained on human?machine dialogue data 
and implemented and tested in a spoken 
dialogue system that can perform the 
Map Task with users. To our knowledge, 
this is the first example of a dialogue sys-
tem that uses automatically extracted 
syntactic, prosodic and contextual fea-
tures for online detection of response lo-
cations. A subjective evaluation of the 
dialogue system suggests that interac-
tions with a system using our trained 
model were perceived significantly better 
than those with a system using a model 
that made decisions at random. 
1 Introduction 
Traditionally, dialogue systems have rested on a 
very simple model for turn-taking, where the sys-
tem uses a fixed silence threshold to detect the 
end of the user?s utterance, after which the sys-
tem responds. However, this model does not cap-
ture human-human dialogue very accurately; 
sometimes a speaker just hesitates and no turn-
change is intended, sometimes the turn changes 
after barely any silence (Sacks et al, 1974). 
Therefore, such models can result in systems that 
interrupt the user or are perceived as unrespon-
sive. Related to the problem of turn-taking is that 
of backchannels (Yngve, 1970).  Backchannel 
feedback ? short acknowledgements such as uh-
huh or mm-hm ? are used by human interlocutors 
to signal continued attention to the speaker, 
without claiming the floor. If a dialogue system 
should be able to manage smooth turn-taking and 
back-channelling, it must be able to first identify 
suitable locations in the user?s speech to do so.  
Duncan (1972) found that human interlocutors 
continuously monitor several cues, such as con-
tent, syntax, intonation, paralanguage, and body 
motion, in parallel to manage turn-taking. Simi-
lar observations have been made in various other 
studies investigating the turn-taking and back-
channelling phenomena in human conversations. 
Ward (1996) has suggested that a low pitch re-
gion is a good cue that backchannel feedback is 
appropriate. On the other hand, Koiso et al 
(1998) have argued that both syntactic and pro-
sodic features make significant contributions in 
identifying turn-taking and back-channelling rel-
evant places. Cathcart et al (2003) have shown 
that syntax in combination with pause duration is 
a strong predictor for backchannel continuers.  
Gravano & Hirschberg (2009) observed that the 
likelihood of occurrence of a backchannel in-
creases with the number of syntactic and prosod-
ic cues conjointly displayed by the speaker. 
However, there is a general lack of studies on 
how such models could be used online in dia-
logue systems and to what extent that would im-
prove the interaction. There are two main prob-
lems in doing so. First, the data used in the stud-
ies mentioned above are from human?human 
dialogue and it is not obvious to what extent the 
models derived from such data transfers to hu-
man?machine dialogue. Second, many of the 
features used were manually extracted. This is 
especially true for the transcription of utterances, 
but several studies also rely on manually anno-
tated prosodic features.  
In this paper, we present a data-driven model 
of what we call Response Location Detection 
(RLD), which is fully online. Thus, it only relies 
375
on automatically extractable features?covering 
syntax, prosody and context. The model has been 
trained on human?machine dialogue data and has 
been implemented in a dialogue system that is in 
turn evaluated with users. The setting is that of a 
Map Task, where the user describes the route and 
the system may respond with for example 
acknowledgements and clarification requests.  
2 Background 
Two influential theories that have examined the 
turn-taking mechanism in human conversations 
are the signal-based mechanism of Duncan 
(1972) and the rule-based mechanism proposed 
by Sacks (1974). According to Duncan, ?the 
turn-taking mechanism is mediated through sig-
nals composed of clear-cut behavioural cues, 
considered to be perceived as discrete?. Duncan 
identified six discrete behavioural cues that a 
speaker may use to signal the intent to yield the 
turn. These behavioural cues are: (i) any devia-
tion from the sustained intermediate pitch level; 
(ii) drawl on the final syllable of a terminal 
clause; (iii) termination of any hand gesticulation 
or the relaxation of tensed hand position?during 
a turn; (iv) a stereotyped expression with trailing 
off effect; (v) a drop in pitch and/or loudness; 
and (vi) completion of a grammatical clause. Ac-
cording to the rule-based mechanism of Sacks 
(1974) turn-taking is regulated by applying rules 
(e.g. ?one party at a time?) at Transition-
Relevance Places (TRPs)?possible completion 
points of basic units of turns, in order to mini-
mize gaps and overlaps. The basic units of turns 
(or turn-constructional units) include sentential, 
clausal, phrasal, and lexical constructions. 
Duncan (1972) also suggested that speakers 
may display behavioural cues either singly or 
together, and when displayed together they may 
occur either simultaneously or in tight sequence. 
In his analysis, he found that the likelihood that a 
listener attempts to take the turn is higher when 
the cues are conjointly displayed across the vari-
ous modalities.  
While these theories have offered a function-
based account of turn-taking, another line of re-
search has delved into corpora-based techniques 
to build models for detecting turn-transition and 
feedback relevant places in speaker utterances.  
Ward (1996) suggested that a 110 millisecond 
(ms) region of low pitch is a fairly good predic-
tor for back-channel feedback in casual conver-
sational interactions. He also argued that more 
obvious factors, such as utterance end, rising in-
tonation, and specific lexical items, account for 
less than they seem to. He contended that proso-
dy alone is sometimes enough to tell you what to 
say and when to say. 
In their analysis of turn-taking and backchan-
nels based on prosodic and syntactic features, in 
Japanese Map Task dialogs, Koiso et al (1998) 
observed that some part-of-speech (POS) fea-
tures are strong syntactic cues for turn-change, 
and some others are strongly associated with no 
turn-change. Using manually extracted prosodic 
features for their analysis, they observed that 
falling and rising F0 patterns are related to 
changes of turn, and flat, flat-fall and rise-fall 
patterns are indications of the speaker continuing 
to speak. Extending their analysis to backchan-
nels, they asserted that syntactic features, such as 
filled pauses, alone might be sufficient to dis-
criminate when back-channelling is inappropri-
ate, whereas presence of backchannels is always 
preceded by certain prosodic patterns. 
Cathcart et al (2003) presented a shallow 
model for predicting the location of backchannel 
continuers in the HCRC Map Task Corpus 
(Anderson et al, 1991). They explored features 
such as POS, word count in the preceding speak-
er turn, and silence pause duration in their mod-
els. A model based on silence pause only insert-
ed a backchannel in every speaker pause longer 
than 900 ms and performed better than a word 
model that predicted a backchannel every sev-
enth word. A tri-gram POS model predicted that 
nouns and pronouns before a pause are the two 
most important cues for predicting backchannel 
continuers. The combination of the tri-gram POS 
model and pause duration model offered a five-
fold improvement over the others. 
Gravano & Hirschberg (2009) investigated 
whether backchannel-inviting cues differ from 
turn-yielding cues. They examined a number of 
acoustic features and lexical cues in the speaker 
utterances preceding smooth turn-changes, back-
channels, and holds. They have identified six 
measureable events that are strong predictors of a 
backchannel at the end of an inter-pausal unit: (i) 
a final rising intonation; (ii) a higher intensity 
level; (iii) a higher pitch level; (iv) a final POS 
bi-gram equal to ?DT NN?, ?JJ NN?, or ?NN 
NN?; (v) lower values of noise-to-harmonic rati-
os; and (vi) a longer IPU duration. They also ob-
served that the likelihood of a backchannel in-
creases in quadratic fashion with the number of 
cues conjointly displayed by the speaker. 
When it comes to using these features for 
making turn-taking decisions in dialogue sys-
376
tems, there is however, very little related work. 
One notable exception is Raux & Eskenazi 
(2008) who presented an algorithm for dynami-
cally setting endpointing silence thresholds based 
on features from discourse, semantics, prosody, 
timing, and speaker characteristics. The model 
was also applied and evaluated in the Let?s Go 
dialogue system for bus timetable information. 
However, that model only predicted the end-
pointing threshold based on the previous interac-
tion up to the last system utterance, it did not 
base the decision on the current user utterance to 
which the system response is to be made. 
In this paper, we train a model for online Re-
sponse Location Detection that makes a decision 
whether to respond at every point where a very 
short silence (200 ms) is detected. The model is 
trained on human?machine dialogue data taken 
from a first set of interactions with a system that 
used a very na?ve policy for Response Location 
Detection. The trained model is then applied to 
the same system, which has allowed us to evalu-
ate the model online in interaction with users.  
3 A Map Task dialogue system 
In a previous study, we presented a fully auto-
mated spoken dialogue system that can perform 
the Map Task with a user (Skantze, 2012). Map 
Task is a common experimental paradigm for 
studying human-human dialogue, where one sub-
ject (the information giver) is given the task of 
describing a route on a map to another subject 
(the information follower). In our case, the user 
acts as the giver and the system as the follower. 
The choice of Map Task is motivated partly be-
cause the system may allow the user to keep the 
initiative during the whole dialogue, and thus 
only produce responses that are not intended to 
take the initiative, most often some kind of feed-
back. Thus, the system might be described as an 
attentive listener.  
Implementing a Map Task dialogue system 
with full speech understanding would indeed be 
a challenging task, given the state-of-the-art in 
automatic recognition of conversational speech. 
In order to make the task feasible, we have im-
plemented a trick: the user is presented with a 
map on a screen (see Figure 1) and instructed to 
move the mouse cursor along the route as it is 
being described. The user is told that this is for 
logging purposes, but the real reason for this is 
that the system tracks the mouse position and 
thus knows what the user is currently talking 
about. It is thereby possible to produce a coher-
ent system behaviour without any speech recog-
nition at all, only basic speech detection. This 
often results in a very realistic interaction, as 
compared to what users are typically used to 
when interacting with dialogue systems?in our 
experiments, several users first thought that there 
was a hidden operator behind it1.  
 
 
Figure 1: The user interface, showing the map. 
The basic components of the system can be 
seen in Figure 2. Dashed lines indicate compo-
nents that were not part of the first iteration of 
the system (used for data collection), but which 
have been used in the model presented and eval-
uated here. The system uses a simple energy-
based speech detector to chunk the user?s speech 
into inter-pausal units (IPUs), that is, periods of 
speech that contain no sequence of silence longer 
than 200 ms. Such a short threshold allows the 
system to give backchannels (seemingly) while 
the user is speaking or take the turn with barely 
any gap. Similar to Gravano & Hirschberg 
(2009) and Koiso et al (1998), we define the end 
of an IPU as a candidate for the Response Loca-
tion Detection model to identify as a Response 
Location (RL). We use the term turn to refer to a 
sequence of IPUs which do not have any re-
sponses between them. 
 
 
Figure 2: The basic components of the system. 
                                                 
1 An example video can be seen at 
http://www.youtube.com/watch?v=MzL-B9pVbOE. 
Prosodic 
analysis
Dialogue 
manager
Map
Windo
Speech 
det ctor
Response
Location
Detector
Contextual
features
Prosodic
features
IPUs Response
Location
Mouse movements
Speech 
synthesizer
Response
ASR Syntactic features
377
Each time the RLD model detected a RL, the 
dialogue manager produced a Response, depend-
ing on the current state of the dialogue and the 
position of the mouse cursor. Table 1 shows the 
different types of responses the system could 
produce. The dialogue manager always started 
with an Introduction and ended with an Ending, 
once the mouse cursor had reached the destina-
tion. Between these, it selected from the other 
responses, partly randomly, but also depending 
on the length of the last user turn and the current 
mouse location. Longer turns often led to Restart 
or Repetition Requests, thus discouraging longer 
sequences of speech that did not invite the sys-
tem to respond. If the system detected that the 
mouse had been at the same place over a longer 
time, it pushed the task forward by making a 
Guess response. We also wanted to explore other 
kinds of feedback than just backchannels, and 
therefore added short Reprise Fragments and 
Clarification Requests (see for example Skantze 
(2007) for a discussion on these).  
Table 1: Different responses from the system 
Introduction ?Could you help me to find my way to 
the train station?? 
Backchannel ?Yeah?, ?Mhm?, ?Okay?, ?Uhu? 
Reprise  
Fragment  
?A station, yeah? 
Clarification  
Request  
?A station?? 
Restart ?Eh, I think I lost you at the hotel, how 
should I continue from there?? 
Repetition  
Request  
?Sorry, could you take that again?? 
Guess ?Should I continue above the church?? 
Ending ?Okay, thanks a lot.? 
 
A na?ve version of the system was used to col-
lect data. Since we initially did not have any so-
phisticated model of RLD, it was simply set to 
wait for a random period between 0 and 800 ms 
after an IPU ended. If no new IPUs were initiated 
during this period, a RL was detected, resulting 
in random response delays between 200 and 
1000 ms. Ten subjects participated in the data 
collection. Each subject did 5 consecutive tasks 
on 5 different maps, resulting in a total of 50 dia-
logues. 
Each IPU in the corpus was manually annotat-
ed into three categories: Hold (a response would 
be inappropriate), Respond (a response is ex-
pected) and Optional (a response would not be 
inappropriate, but it is perfectly fine not to re-
spond). Two human-annotators labelled the cor-
pus separately. For all the three categories the 
kappa score was 0.68, which is substantial 
agreement (Landis & Koch, 1977). Since only 
2.1% of all the IPUs in the corpus were identified 
for category Optional, we excluded them from 
the corpus and focused on the Respond and Hold 
categories only. The data-set contains 2272 IPUs 
in total; the majority of which belong to the class 
Respond (50.79%), which we take as our majori-
ty class baseline. Since the two annotators agreed 
on 87.20% of the cases, this can be regarded as 
an approximate upper limit for the performance 
expected from a model trained on this data. 
In (Skantze, 2012), we used this collected data 
to build an offline model of RLD that was 
trained on prosodic and contextual features. In 
this paper, we extend this work in three ways. 
First, we bring in Automatic Speech Recognition 
(ASR) for adding syntactic features to the model. 
Second, the model is implemented as a module 
in the dialogue system so that it can extract the 
prosodic features online. Third, we evaluate the 
performance of our RLD model against a base-
line system that makes a random choice, in a dia-
logue system interacting with users.  
In contrast to some related work (e.g. Koiso et 
al., 1998), we do not discriminate between loca-
tions for backchannels and turn-changes. Instead, 
we propose a general model for response loca-
tion detection. The reason for this is that the sys-
tem mostly plays the role of an attentive listener 
that produces utterances that are not intended to 
take the initiative or claim the floor, but only to 
provide different types of feedback (cf. Table 1). 
Thus, suitable response locations will be where 
the user invites the system to give feedback, re-
gardless of whether the feedback is simply an 
acknowledgement that encourages the system to 
continue, or a clarification request. Moreover, it 
is not clear whether the acknowledgements the 
system produces in this domain should really be 
classified as backchannels, since they do not only 
signal continued attention, but also that some 
action has been performed (cf. Clark, 1996). In-
deed, none of the annotators felt the need to mark 
relevant response locations within IPUs.  
4 A data-driven model for response lo-
cation detection 
The human?machine Map Task corpus described 
in the previous section was used for training a 
new model of RLD. We describe below how we 
extracted prosodic, syntactic and contextual fea-
tures from the IPUs. We test the contribution of 
these feature categories?individually as well as 
378
in combination, in classifying a given IPU as 
either Respond or Hold type. For this we explore 
the Na?ve Bayes (NB) and Support Vector Ma-
chine (SVM) algorithms in the WEKA toolkit 
(Hall et al, 2009). All results presented here are 
based on 10-fold cross-validation. 
4.1 Prosodic features 
Pitch and intensity (sampled at 10 ms) for each 
IPU were extracted using ESPS in 
Wavesurfer/Snack (Sj?lander & Beskow, 2000). 
The values were transformed to log scale and z-
normalized for each user. The final 200 ms 
voiced region was then identified for each IPU. 
For this region, the mean pitch, slope of the 
pitch (using linear regression)?in combination 
with the correlation coefficient r for the regres-
sion line, were used as features. In addition to 
these, we also used the duration of the voiced 
region as a feature. The last 500 ms of each IPU 
were used to obtain the mean intensity (also z-
normalised). Table 2 illustrates the power of pro-
sodic features, individually as well as collective-
ly (last row), in classifying an IPU as either Re-
spond or Hold type. Except for mean intensity all 
other features individually provide an improve-
ment over the baseline. The best accuracy, 
64.5%, was obtained by the SVM algorithm us-
ing all the prosodic features. This should be 
compared against the baseline of 50.79%. 
Table 2: Percentage accuracy of prosodic features 
in detecting response locations 
 Algorithm 
Feature(s) NB  SVM  
Mean pitch 60.3 62.7 
Pitch slope 59.0 57.8 
Duration 58.1 55.6 
Mean intensity 50.3 52.2 
Prosody (all combined) 63.3 64.5 
4.2 Syntactic features 
As lexico-syntactic features, we use the word 
form and part-of-speech tag of the last two 
words in an IPU. All the IPUs in the Map Task 
corpus were manually transcribed. To obtain the 
part-of-speech tag we used the LBJ toolkit 
(Rizzolo & Roth, 2010). Column three in Table 3 
illustrates the discriminatory power of syntactic 
features?extracted from the manual transcrip-
tion of the IPUs. Using the last two words and 
their POS tags, the Na?ve Bayes learner achieves 
the best accuracy of 83.6% (cf. row 7). While 
POS tag is a generic feature that would enable 
the model to generalize, using word form as a 
feature has the advantage that some words, such 
as yeah, are strong cues for predicting the Re-
spond class, whereas pause fillers, such as ehm, 
are strong predictors of the Hold class. 
Table 3: Percentage accuracy of syntactic features 
in detecting response locations 
  
Manual  
transcriptions 
ASR  
results 
# Feature(s) NB SVM NB SVM  
1 Last word (Lw) 82.5 83.9 80.8 80.9 
2 
Last word part-of-
speech (Lw-POS)  
79.4 79.5 74.5 74.6 
3 
Second last word 
(2ndLw) 
68.1 67.7 67.1 67.0 
4 
Second last word 
Part-of-speech 
(2ndLw-POS) 
66.9 66.5 65.8 66.1 
5 Lw + 2ndLw 82.3 81.5 80.8 80.6 
6 
Lw-POS 
+ 2ndLw-POS 
80.3 80.5 75.4 74.87 
7 
Lw + 2ndLw 
+ Lw-POS 
+ 2ndLw-POS 
83.6 81.7 79.7 79.7 
8 
Last word diction-
ary (Lw-Dict) 
83.4 83.4 78.0 78.0 
9 
Lw-Dict 
+ 2ndLw-Dict 
81.2 82.6 76.1 77.7 
10 
Lw + 2ndLw 
+ Lw-Conf 
+ 2ndLw-Conf  
82.3 81.5 81.1 80.5 
 
An RLD model for online predictions requires 
that the syntactic features are extracted from the 
output of a speech recogniser. Since speech 
recognition is prone to errors, an RLD model 
trained on manual transcriptions alone would not 
be robust when making predictions in noisy data. 
Therefore we train our RLD model on actual 
speech recognised results. To achieve this, we 
did an 80-20 split of the Map Task corpus into 
training and test sets respectively. The transcrip-
tions of IPUs in the training set were used to 
train the language model of the Nuance 9 ASR 
system. The audio recordings of the IPUs in the 
test set were then recognised by the trained ASR 
system. After performing five iterations of split-
ting, training and testing, we had obtained the 
speech recognised results for all the IPUs in the 
Map Task corpus. The mean word error rate for 
the five iterations was 17.22% (SD = 3.8%).  
Column four in Table 3 illustrates the corre-
sponding performances of the RLD model 
trained on syntactic features extracted from the 
best speech recognized hypotheses for the IPUs. 
With the introduction of a word error rate of 
17.22%, the performances of all the models us-
379
ing only POS tag feature decline. The perfor-
mances are bound to decline further with in-
crease in ASR errors. This is because the POS 
tagger itself uses the left context to make POS 
tag predictions. With the introduction of errors in 
the left context, the tagger?s accuracy is affected, 
which in turn affects the accuracy of the RLD 
models. However, this decline is not significant 
for models that use word form as a feature. This 
suggests that using context independent lexico-
syntactic features would still offer better perfor-
mance for an online model of RLD. We therefore 
also created a word class dictionary, which gen-
eralises the words into domain-specific classes in 
a simple way (much like a class-based n-gram 
model). Row 9 in Table 3 illustrates that using a 
dictionary instead of POS tag (cf. row 6) im-
proves the performance of the online model. We 
have also explored the use of word-level confi-
dence scores (Conf) from the ASR as another 
feature that could be used to reinforce a learning 
algorithm?s confidence in trusting the recognised 
words (cf. row 10 in Table 3).  
The best accuracy, 81.1%, for the online mod-
el of RLD is achieved by the Na?ve Bayes algo-
rithm using the features word form and confi-
dence score, for last two words in an IPU. 
4.3 Contextual features 
We have explored three discourse context fea-
tures: turn and IPU length (in words and se-
conds) and last system dialogue act. Dialogue 
act history information have been shown to be 
vital for predicting a listener response when the 
speaker has just responded to the listener?s clari-
fication request (Koiso et al (1998); Cathcart et 
al. 2003; Gravano & Hirschberg (2009); Skantze, 
2012). To verify if this rule holds in our corpus, 
we extracted turn length and dialogue act labels 
for the IPUs, and trained a J48 decision tree 
learner. The decision tree achieved an accuracy 
of 65.7%. One of the rules learned by the deci-
sion tree is: if the last system dialogue act is 
Clarification or Guess (cf. Table 1), and the turn 
word count is less than equal to 1, then Respond. 
In other words, if the system had previously 
sought a clarification, and the user has responded 
with a yes/no utterance, then a system response 
is expected. A more general rule in the decision 
tree suggests that: if the last system dialogue act 
was a Restart or Repetition Request, and if the 
turn word count is more than 4 then Respond 
otherwise Hold. In other words, the system 
should wait until it gets some amount of infor-
mation from the user.  
Table 4 illustrates the power of these contex-
tual features in discriminating IPUs, using the 
NB and the SVM algorithms. All the features 
individually provide improvement over the base-
line of 50.79%. The best accuracy, 64.8%, is 
achieved by the SVM learner using the features 
last system dialogue act and turn word count. 
Table 4: Percentage accuracy of contextual features 
in detecting response locations 
 
Manual 
transcriptions 
ASR  
results 
Features NB  SVM  NB  SVM  
Last system dialogue act 54.1 54.1 54.1 54.1 
Turn word count 61.8 61.9 61.5 62.9 
Turn length in seconds 58.4 58.8 58.4 58.8 
IPU word count 58.4 58.2 58.1 59.3 
IPU length in seconds 57.3 61.2 57.3 61.2 
Last system dialogue act 
+ Turn word count 
59.9 64.5 60.4 64.8 
 
4.4 Combined model 
Table 5 illustrates the performances of the RLD 
model using various feature category combina-
tions. It could be argued that the discriminatory 
power of prosodic and contextual feature catego-
ries is comparable. A model combining prosodic 
and contextual features offers an improvement 
over their individual performances. Using the 
three feature categories in combination, the Na-
?ve Bayes learner provided the best accuracy: 
84.6% (on transcriptions) and 82.0% (on ASR 
output). These figures are significantly better 
than the majority class baseline of 50.79% and 
approach the expected upper limit of 87.20% on 
the performance.  
Table 5: Percentage accuracy of combined models  
 
Manual  
transcriptions 
ASR  
results 
Feature categories NB SVM NB SVM 
Prosody  63.3 64.5 63.3 64.5 
Context  59.9 64.5 60.4 64.8 
Syntax  82.3 81.5 81.1 80.5 
Prosody + Context 67.7 70.2 67.5 69.1 
Prosody + Context 
+ Syntax 
84.6 77.2 82.0 77.1 
  
Table 6 illustrates that the Na?ve Bayes model 
for Response Location Detection trained on 
combined syntactic, prosodic and contextual fea-
tures, offers better precision (fraction of correct 
decisions in all model decisions) and recall (frac-
tion of all relevant decisions correctly made) in 
comparison to the SVM model. 
380
Table 6: Precision and Recall scores of the NB and 
the SVM learners trained on combined prosodic, con-
textual and syntactic features. 
Prediction class 
Precision (in %) Recall (in %) 
NB  SVM  NB  SVM  
Respond 81.0  73.0 87.0 84.0 
Hold 85.0 81.0 78.0 68.0 
 
5 User evaluation 
In order to evaluate the usefulness of the com-
bined model, we have performed a user evalua-
tion where we test the trained model in the Map 
Task dialogue system that was used to collect the 
corpus (cf. section 3). A version of the dialogue 
system was created that uses a Random model, 
which makes a random choice between Respond 
and Hold. The Random model thus approximates 
our majority class baseline (50.79% for Re-
spond). Another version of the system used the 
Trained model ? our data-driven model ? to 
make the decision. For both models, if the deci-
sion was a Hold, the system waited 1.5 seconds 
and then responded anyway if no more speech 
was detected from the user. 
We hypothesize that since the Random model 
makes random choices, it is likely to produce 
false-positive responses (resulting in overlap in 
interaction) as well as false-negative responses 
(resulting in gap/delayed response) in equal pro-
portion. The Trained model on the other hand 
would produce fewer overlaps and gaps.  
In order to evaluate the models, 8 subjects (2 
female, 6 male) were asked to perform the Map 
Task with the two systems. Each subject per-
formed five dialogues (which included 1 trial and 
2 tests) with each version of the system. This 
resulted in 16 test dialogues each for the two sys-
tems. The trial session was used to allow the us-
ers to familiarize themselves with the dialogue 
system. Also, the audio recording of the users? 
speech from this session was used to normalize 
the user pitch and intensity for the online prosod-
ic extraction. The order in which the systems and 
maps were presented to the subjects was varied 
over the subjects to avoid any ordering effect in 
the analysis.  
The 32 dialogues from the user evaluation 
were, on average, 1.7 min long (SD = 0.5 min). 
The duration of the interactions with the Random 
and the Trained model were not significantly 
different. A total of 557 IPUs were classified by 
the Random model whereas the Trained model 
classified 544 IPUs. While the Trained model 
classified 57.7% of the IPUs as Respond type the 
Random model classified only 48.29% of the 
total IPUs as Respond type, suggesting that the 
Random model was somewhat quieter.  
It turned out that it was very hard for the sub-
jects to perform the Map Task and at the same 
time make a valid subjective comparison be-
tween the two versions of the system, as we had 
initially intended. Therefore, we instead con-
ducted another subjective evaluation to compare 
the two systems. We asked subjects to listen to 
the interactions and press a key whenever a sys-
tem response was either lacking or inappropriate. 
The subjects were asked not to consider how the 
system actually responded, only evaluate the tim-
ing of the response. 
Eight users participated in this subjective 
judgment task. Although five of these were from 
the same set of users who had performed the 
Map Task, none of them got to judge their own 
interactions. The judges listened to the Map Task 
interactions in the same order as the users had 
interacted, including the trial session. Whereas it 
had been hard for the subjects who participated 
in the dialogues to characterize the two versions 
of the system, almost all of the judges could 
clearly tell the two versions apart. They stated 
that the Trained system provided for a smooth 
flow of dialogue. The timing of the IPUs was 
aligned with the timing of the judges? key-
presses in order to measure the numbers of IPUs 
that had been given inappropriate response deci-
sions. The results show that for the Random 
model, 26.75% of the RLD decisions were per-
ceived as inappropriate, whereas only 11.39% of 
the RLD decisions for the Trained model were 
perceived inappropriate. A two-tailed two-
sample t-test for difference in mean of the frac-
tion of inappropriate instances (key-press count 
divided by IPU count) for Random and Trained 
model show a clear significant difference (t = 
4.66, dF = 30, p < 0.001). 
We have not yet analysed whether judges pe-
nalized false-positives or false-negatives to a 
larger extent, this is left to future work. Howev-
er, some judges informed us that they did not 
penalize delayed response (false-negative), as the 
system eventually responded after a delay. In the 
context of a system trying to follow a route de-
scription, such delays could sometimes be ex-
pected and wouldn?t be unnatural. For other 
types of interactions (such as story-telling), such 
delays may on the other hand be perceived as 
unresponsive. Thus, the balance between false-
positives and false-negatives might need to be 
tuned depending on the topic of the conversation.  
381
6 Conclusion  
We have presented a data-driven model for de-
tecting response locations in the user?s speech. 
The model has been trained on human?machine 
dialogue data and has been integrated and tested 
in a spoken dialogue system that can perform the 
Map Task with users. To our knowledge, this is 
the first example of a dialogue system that uses 
automatically extracted syntactic, prosodic and 
contextual features for making online detection 
of response locations. The models presented in 
earlier works have used only prosody (Ward, 
1996), or combinations of syntax and prosody 
(Koiso et al, 1998), syntax and context (Cathcart 
et al, 2003), prosody and context (Skantze, 
2012), or prosody, context and semantics (Raux 
& Eskenazi (2008). Furthermore, we have evalu-
ated the usefulness of our model by performing a 
user evaluation of a dialogue system interacting 
with users. None of the earlier models have been 
tested in user evaluations. 
The significant improvement of the model 
gained by adding lexico-syntactic features such 
as word form and part-of-speech tag corroborates 
with earlier observations about the contribution 
of syntax in predicting response location (Koiso 
et al, 1998; Cathcart et al, 2003; Gravano & 
Hirschberg, 2009). While POS tag alone is a 
strong generic feature for making predictions in 
offline models its contribution to decision mak-
ing in online models is reduced due to speech 
recognition errors. This is because the POS tag-
ger itself uses the left context to make predic-
tions, and is not typically trained to handle noisy 
input. We have shown that using only the word 
form or a dictionary offers a better performance 
despite speech recognition errors. However, this 
of course results in a more domain-dependent 
model. 
Koiso et al, (1998), have shown that prosodic 
features contribute almost as strongly to response 
location prediction as the syntactic features. We 
do not find such results with our model. This 
difference could be partly attributed to inter-
speaker variation in the human?machine Map 
Task corpus used for training the models. All the 
users who participated in the corpus collection 
were non-native speakers of English. Also, our 
algorithm for extracting prosodic features is not 
as powerful as the manual extraction scheme 
used in (Koiso et al, 1998). Although prosodic 
and contextual features do not seem to improve 
the performance very much when syntactic fea-
tures are available, they are clearly useful when 
no ASR is available (70.2% as compared to the 
baseline of 50.79%).  
The subjective evaluation indicates that the in-
teractions with a system using our trained model 
were perceived as smoother (more accurate re-
sponses) as compared to a system using a model 
that makes a random choice between Respond 
and Hold. 
7 Future work 
Coordination problems in turn-transition and re-
sponsiveness have been identified as important 
short-comings of turn-taking models in current 
dialogue systems (Ward et al, 2005). In continu-
ation of the current evaluation exercise, we 
would next evaluate our Trained model?on an 
objective scale, in terms of its responsiveness 
and smoothness in turn-taking and back-
channels. An objective measure is the proportion 
of judge key-presses coinciding with false-
positive and false-negative model decisions. We 
argue that in comparison to the Random model 
our Trained model produces (i) fewer instances 
of false-negatives (gap/delayed response) and 
therefore has a faster response time, and (ii) few-
er instances of false-positives (overlap) and thus 
provides for smooth turn-transitions.  
We have so far explored syntactic, prosodic 
and contextual features for predicting response 
location. An immediate extension to our model 
would be to bring semantic features in the model. 
In Meena et al (2012) we have presented a data-
driven method for semantic interpretation of ver-
bal route descriptions into conceptual route 
graphs?a semantic representation that captures 
the semantics of the way human structure infor-
mation in route descriptions. Another possible 
extension is to situate the interaction in a face-to-
face Map Task between a human and a robot and 
add features from other modalities such as gaze. 
In a future version of the system, we do not 
only want to determine when to give responses 
but also what to respond. In order to do this, the 
system will need to extract the semantic concepts 
of the route directions (as described above) and 
utilize the confidence scores from the spoken 
language understanding component in order to 
select between different forms of clarification 
requests and acknowledgements.  
Acknowledgments 
This work is supported by the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
382
References 
Anderson, A., Bader, M., Bard, E., Boyle, E., 
Doherty, G., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, 
H., & Weinert, R. (1991). The HCRC Map Task 
corpus. Language and Speech, 34(4), 351-366. 
Cathcart, N., Carletta, J., & Klein, E. (2003). A shal-
low model of backchannel continuers in spoken di-
alogue. In 10th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics. Budapest. 
Clark, H. H. (1996). Using language. Cambridge, 
UK: Cambridge University Press. 
Duncan, S. (1972). Some Signals and Rules for Tak-
ing Speaking Turns in Conversations. Journal of 
Personality and Social Psychology, 23(2), 283-
292. 
Gravano, A., & Hirschberg, J. (2009). Backchannel-
inviting cues in task-oriented dialogue. In Proceed-
ings of Interspeech 2009 (pp. 1019-1022). Bright-
on, U.K. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P., & Witten, I. H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD Ex-
plorations, 11(1). 
Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., & 
Den, Y. (1998). An analysis of turn-taking and 
backchannels based on prosodic and syntactic fea-
tures in Japanese Map Task dialogs. Language and 
Speech, 41, 295-321. 
Landis, J., & Koch, G. (1977). The measurement of 
observer agreement for categorical data. Biomet-
rics, 33(1), 159-174. 
Meena, R., Skantze, G., & Gustafson, J. (2012). A 
Data-driven Approach to Understanding Spoken 
Route Directions in Human-Robot Dialogue. In 
Proceedings of Interspeech. Portland, OR, US. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Rizzolo, N., & Roth, D. (2010). Learning Based Java 
for Rapid Development of NLP Systems. Lan-
guage Resources and Evaluation. 
Sacks, H., Schegloff, E., & Jefferson, G. (1974). A 
simplest systematics for the organization of turn-
taking for conversation. Language, 50, 696-735. 
Sj?lander, K., & Beskow, J. (2000). WaveSurfer - an 
open source speech tool. In Yuan, B., Huang, T., & 
Tang, X. (Eds.), Proceedings of ICSLP 2000, 6th 
Intl Conf on Spoken Language Processing (pp. 
464-467). Beijing. 
Skantze, G. (2007). Error Handling in Spoken Dia-
logue Systems - Managing Uncertainty, Grounding 
and Miscommunication. Doctoral dissertation, 
KTH, Department of Speech, Music and Hearing. 
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
Ward, N., Rivera, A., Ward, K., & Novick, D. (2005). 
Root causes of lost time and user stress in a simple 
dialog system. In Proceedings of Interspeech 2005. 
Lisbon, Portugal. 
Ward, N. (1996). Using prosodic clues to decide when 
to produce backchannel utterances. In Proceedings 
of the fourth International Conference on Spoken 
Language Processing (pp. 1728-1731). Philadelph-
ia, USA. 
Yngve, V. H. (1970). On getting a word in edgewise. 
In Papers from the sixth regional meeting of the 
Chicago Linguistic Society (pp. 567-578). Chicago. 
 
383
Proceedings of the SIGDIAL 2014 Conference, pages 2?11,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Crowdsourcing Street-level Geographic Information Using a 
Spoken Dialogue System 
 
 
 Raveesh Meena Johan Boye Gabriel Skantze Joakim Gustafson 
KTH Royal Institute of Technology 
School of Computer Science and Communication  
Stockholm, Sweden 
{raveesh, jboye}@csc.kth.se, {gabriel, jocke}@speech.kth.se 
 
  
 
Abstract 
We present a technique for crowd-
sourcing street-level geographic infor-
mation using spoken natural language. In 
particular, we are interested in obtaining 
first-person-view information about what 
can be seen from different positions in 
the city. This information can then for 
example be used for pedestrian routing 
services. The approach has been tested in 
the lab using a fully implemented spoken 
dialogue system, and has shown promis-
ing results. 
1 Introduction 
Crowdsourcing is increasingly being used in 
speech processing for tasks such as speech data 
acquisition, transcription/labeling, and assess-
ment of speech technology, e.g. spoken dialogue 
systems (Parent & Eskenazi, 2011). However, 
we are not aware of any attempts where a dia-
logue system is the vehicle for crowdsourcing 
rather than the object of study, that is, where a 
spoken dialogue system is used to collect infor-
mation from a large body of users.  A task where 
such crowdsourcing dialogue systems would be 
useful is to populate geographic databases. While 
there are now open databases with geographic 
information, such as OpenStreetMap (Haklay & 
Weber, 2008), these are typically intended for 
map drawing, and therefore lack detailed street-
level information about city landmarks, such as 
colors and height of buildings, ornamentations, 
facade materials, balconies, conspicuous signs, 
etc. Such information could for example be very 
useful for pedestrian navigation (Tom & Denis, 
2003; Ross et al., 2004). With the current grow-
ing usage of smartphones, we might envisage a 
community of users using their phones to con-
tribute information to geographic databases, an-
notating cities to a great level of detail, using 
multi-modal method including speech. The key 
reason for using speech for map annotation is 
convenience; it is easy to talk into a mobile 
phone while walking down the street, so a user 
with a little experience will not be slowed down 
by the activity of interacting with a database. 
This way, useful information could be obtained 
that is really hard to add offline, sitting in front 
of one?s PC using a map interface, things like: 
Can you see X from this point? Is there a big 
sign over the entrance of the restaurant? What 
color is the building on your right? 
Another advantage of using a spoken dialogue 
system is that the users could be asked to freely 
describe objects they consider important in their 
current view. In this way, the system could learn 
new objects not anticipated by the system de-
signers, and their associated properties.   
In this paper we present a proof-of-concept 
study of how a spoken dialogue system could be 
used to enrich geographic databases by 
crowdsourcing. To our knowledge, this is the 
first attempt at using spoken dialogue systems 
for crowdsourcing in this way. In Section 2, we 
elaborate on the need of spoken dialogue systems 
for crowdsourcing geographic information. In 
Section 3 we describe the dialogue system im-
plementation. Section 4 presents our in-lab 
crowdsourcing experiment. We present an analy-
sis of crowd-sourced data in Section 5, and dis-
cuss directions for future work in Section 6. 
2 The pedestrian routing domain 
Routing systems have been around quite some 
time for car navigation, but systems for pedestri-
2
an routing are relatively new and are still in their 
nascent stage (Bartie & Mackaness, 2006; Krug 
et al., 2003; Janarthanam et al., 2012; Boye et al., 
2014). In the case of pedestrian navigation, it is 
preferable for way-finding systems to base their 
instructions on landmarks, by which we under-
stand distinctive objects in the city environment. 
Studies have shown that the inclusion of land-
marks into system-generated instructions for a 
pedestrian raises the user?s confidence in the sys-
tem, compared to only left-right instructions 
(Tom & Denis, 2003; Ross et al., 2004).  
Basing routing instructions on landmarks 
means that the routing system would, for exam-
ple, generate an instruction ?Go towards the red 
brick building? (where, in this case, ?the red 
brick building? is the landmark), rather than 
?Turn slightly left here? or ?Go north 200 me-
ters?. This strategy for providing instructions 
places certain requirements on the geographic 
database: It has to include many landmarks and 
many details about them as well, so that the sys-
tem can generate clear and un-ambiguous in-
structions. However, the information contained 
in current databases is still both sparse and 
coarse-grained in many cases.  
Our starting point is a pedestrian routing sys-
tem we designed and implemented, using the 
landmark-based approach to instruction-giving 
(Boye et al., 2014). The system performs visibil-
ity calculations whenever the pedestrian ap-
proaches a waypoint, in order to compute the set 
of landmarks that are visible for the user from his 
current position. OpenStreetMap (Haklay & We-
ber, 2008) is used as the data source. Figure 1 
shows a typical situation in pedestrian routing 
session. The blue dot indicates the user?s position 
and the blue arrow her direction. Figure 2 shows 
the same situation in a first-person perspective. 
The system can now compute the set of visible 
landmarks, such as buildings and traffic lights, 
along with distances and angles to those land-
marks. The angle to a building is given as an in-
terval in degrees relative to the direction of the 
user (e.g. 90? left to 30? left). This is exemplified 
in Figure 1, where four different buildings are in 
view (with field of view marked with numbers 
1?4). Landmarks that are not buildings are con-
sidered to be a single point, and hence the rela-
tive angle can be given as a single number. 
When comparing the map with the street view 
picture, it becomes obvious that the ?SEB? bank 
office is very hard to see and probably not very 
suitable to use as a landmark in route descrip-
tions. On the other hand, the database does not 
contain the fact that the building has six stories 
and a fa?ade made of yellow bricks, something 
that would be easily recognizable for the pedes-
trian. This is not due to any shortcoming of the 
OpenStreetMap database; it just goes to show 
that the database has been constructed with map 
drawing in mind, rather than pedestrian routing. 
There are also some other notable omissions in 
the database; e.g. the shop on the corner, visible 
right in front of the user, is not present in the da-
tabase. Since OpenStreetMap is crowd-sourced, 
there is no guarantee as to which information 
will be present in the database, and which will 
not. This also highlights the limitation of existing 
approaches to crowd-sourcing geographic infor-
mation: Some useful information is difficult to 
add off-line, using a map interface on a PC. On 
the other hand, it would be a straightforward 
matter given the kind of crowd-sourcing spoken 
dialogue system we present next. 
 
 
 
Figure 1: A pedestrian routing scenario 
  
 
 
Figure 2: The visual scene corresponding to the 
pedestrian routing scenario in Figure 1 
3 A dialogue system for crowd-sourcing 
To verify the potential of the ideas discussed 
above, we implemented a spoken dialogue sys-
tem that can engage in spoken conversation with 
3
users and learn details about landmarks in visual 
scenes (such as Figure 2). To identify the kind of 
details in a visual scene that the system could 
potentially ask the users, we first conducted a 
preliminary informal crowd-sourcing dialogue: 
one person (the receiver), was instructed to seek 
information that could be useful for pedestrian 
navigation from the other person (the giver).  
The receiver only had access to information 
available in the maps from OpenStreetMap, as in 
Figure 1, but without any marking of field of 
views, whereas the giver only had access to the 
corresponding visual scene (as in Figure 2). In-
teraction data from eight such dialogues (from 
four participants, and four different visual 
scenes) suggested that in a city environment, 
buildings are prominent landmarks and much of 
the interaction involves their properties such as 
color, number of stories, color of roof, signs or 
ornamentations on buildings, whether it has 
shops, etc. Seeking further details on mentioned 
signs, shops, and entities (whether mapped or 
unmapped) proved to be a useful strategy to ob-
tain information. We also noted that asking for 
open-ended questions, such as ?Is there anything 
else in this scene that I should be aware of?? 
towards the end has the potential of revealing 
unknown landmarks and details in the map.  
Obtaining specific details about known objects 
from the user corresponds to slot-filling in a dia-
logue system, where the dialogue system seeks a 
value for a certain slot (= attribute). By engaging 
in an open-ended interaction the system could 
also obtain general details to identify new slot-
value pairs. Although slots could be in some cas-
es be multi-valued (e.g., a building could have 
both color red and yellow), we have here made 
the simplifying assumption that they are single 
valued. Since users may not always be able to 
specify values for slots we treat no-value as a 
valid slot-value for all type of slots.  
We also wanted the system to automatically 
learn the most reliable values for the slots, over 
several interactions. As the system interacts with 
new users, it is likely that the system will obtain 
a range of values for certain slots. The variability 
of the answers could appear for various reasons: 
users may have differences in perception about 
slot-values such as colors, some users might 
misunderstand what building is being talked 
about, and errors in speech recognition might 
result in the wrong slot values. Some of these 
values may therefore be in agreement with those 
given by other users, while some may differ 
slightly or be in complete contradiction. Thus the 
system should be able to keep a record of all the 
various slot-values obtained (including the dis-
puted ones), identify slot-values that need to be 
clarified, and engage in a dialogue with users for 
clarification. 
In view of these requirements, we have de-
signed our crowd-sourcing dialogue system to be 
able to (1) take and retain initiative during the 
interactions for slot-filling, (2) behave as a re-
sponsive listener when engaging in open-ended 
dialogue, and (3) ask wh? and yes?no questions 
for seeking and clarifying slot-values, respective-
ly. Thus when performing the slot-filling task, 
the system mainly asks questions, acknowledges, 
or clarifies the concepts learned for the slot-
values. Apart from requesting repetitions, the 
user cannot ask any questions or by other means 
take the initiative. A summary of all the attrib-
utes and corresponding system prompts is pre-
sented in Appendix A. 
The top half of Figure 3 illustrates the key 
components of the dialogue system. The Dia-
logue Manager queries the Scene Manager (SM) 
for slots to be filled or slot-values to be clarified, 
engages in dialogue with users to learn/clarify 
slot-values, and informs the SM about the values 
obtained for these slots. The SM manages a list 
of scenes and the predefined slots ? for each type 
of landmark in visual scenes ? that need to be 
filled, maintains a record of slot-values obtained 
from all the users, and identifies slot-values with 
majority vote as the current reliable slot-value. 
To achieve these objectives, the scene manager 
uses an XML representation of visual scenes. In 
this representation, landmarks (e.g., buildings, 
junctions, etc.) ? automatically acquired through 
the OpenStreetMap database and the visibility 
computations mentioned in Section 2  ? are 
stored as scene-objects (cf. Figure 4). 
 
 
 
Figure 3: Dialogue system architecture 
 
The Dialogue Manager (DM) uses scene-
object attributes, such as type, angle or interval 
of a building, to generate referential expressions, 
such as ?Do you see a building on the far left?? 
4
or ?Do you see a shop on the left?? to draw the 
users? attention to the intended landmark in the 
scene. During the course of interaction, the Sce-
ne Manager (SM) extends scene-objects with a 
set of predefined attributes (= slots) that we iden-
tified in the preliminary study, along with their 
various slot-values (cf. Figure 5). For each slot, 
the SM keeps a record of slot-values obtained 
through wh? questions as well as the ones dis-
puted by the users in yes?no questions (cf. ob-
tained and disputed tags in the XML), and 
uses their tally to identify the slot-value in major-
ity. The system assumes this slot-value (or one of 
them in case of a tie) as its best estimate of a 
slot-value pair, which it could clarify with anoth-
er user using a yes?no query. During the slot-
filling mode the DM switches to open-ended in-
teraction mode to seek general details (using 
prompts such as ?Could you describe it/them??), 
if the user suggests/agrees that there are signs 
on/at a scene-object, or a building has shops or 
restaurants. Once all the slots for all the scene-
objects in a visual scene have been queried, the 
DM once again switches to the open-ended inter-
action mode and queries the users whether there 
are any other relevant signs or landmarks that the 
system may have missed and should be aware of. 
On completion of the open-ended queries the SM 
selects the next visual scene, and the DM engag-
es in a new dialogue.  
 
<scene xmlns="cityCS.scene" name=" view7.jpg" lat="59.34501" 
lon="18.0614" fovl="-60" fovr="60" bearing="320" dist="100"> 
    <scene-object> 
        <id>35274588</id> <type>building</type> 
        <from>-60</from> <end>-39</end> 
    </scene-object> 
    <scene-object> 
        <id>538907080</id> <type>shop</type> 
        <distance>34.82</distance> 
        <angle>-39</angle> <bearing>281</bearing> 
    </scene-object> 
    <scene-object> 
        <id>280604</id> <type>building</type> 
        <from>-38</from> <end>6</end> 
    </scene-object> 
    <scene-object> 
        <id>193906</id> <type>traffic_signals</type> 
        <distance>40.77</distance> 
        <angle>-14</angle> <bearing>306</bearing> 
    </scene-object> 
    ... 
</scene> 
Figure 4: XML representation of visual scenes 
 
For speech recognition and semantic interpre-
tation the system uses a context-free grammar 
with semantic tags (SRGS1), tailored for the do-
main. The output of semantic interpretation is a 
concept. If the concept type matches the type of 
the slot, the dialogue manager informs the scene 
manager about the obtained slot-value. If the 
                                                 
1 http://www.w3.org/TR/speech-grammar/ 
concept type is inappropriate the DM queries the 
user once more (albeit using different utterance 
forms). If still no appropriate concept is learned 
the DM requests the SM for the next slot and 
proceeds with the dialogue. For speech synthesis, 
we use the CereVoice system developed by 
CereProc2. The dialogue system has been imple-
mented using the IrisTK framework (Skantze & 
Al Moubayed, 2012). 
 
<scene-object> 
    <id>35274588</id> <type>building</type> 
    <from>-60</from> <end>-39</end> 
    <slot slotName="VISIBLE">?    </slot> 
    <slot slotName="COLOR"> 
     <obtained> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u01" asrCnf="0.06" qType="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="no-value"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.46" qType ="WH"/> 
         </userlist> 
       </value> 
       <value slotValue="Gray"> 
         <userlist> 
           <usrDtls uid="u03" asrCnf="0.19" qType ="WH"/> 
         </userlist> 
       </value> 
     </obtained> 
     <disputed> 
       <value slotValue="Green"> 
         <userlist> 
           <usrDtls uid="u02" asrCnf="0.92" qType ="YN"/> 
         </userlist> 
       </value> 
     </disputed> 
    </slot> 
    <slot slotName="STORIES">?    </slot> 
    <slot slotName="ROOF_COLOR">?    </slot> 
    ? 
</scene-object> 
 
Figure 5: Every slot-value is recorded  
 
In contrast to the slot-filling mode, when en-
gaging in an open-ended interaction, the system 
leaves the initiative to the user and behaves as a 
responsive listener. That is, the system only pro-
duces feedback responses, such as backchannels 
(e.g., okay, mh-hmm, uh-huh), repetition requests 
for longer speaker turns (e.g., could you repeat 
that?), or continuation prompts such as ?any-
thing else?? until the user is finished speaking. 
Unless the system recognized an explicit closing 
statement from the user (e.g., ?I can?t?), the sys-
tem encourages the user to continue the descrip-
tions for 2 to 4 turns (chosen randomly). 
To detect appropriate locations in users? 
speech where the system should give feedback 
response, the system uses a trained data-driven 
model (Meena et al., 2013). When the voice ac-
tivity detector detects a silence of 200 ms in us-
ers? speech, the model uses prosodic, contextual 
and lexico-syntactic features from the preceding 
speech segment to decide whether the system 
                                                 
2 https://www.cereproc.com/ 
5
should produce a feedback response. The lower 
half of Figure 3 shows the additional components 
of the dialogue system used in open-ended inter-
action mode. In this mode, the ASR system uses 
a language model that is trained on interactions 
from a related domain (verbal route descrip-
tions), in parallel to the SRGS grammar.  
4 In-lab crowd-sourcing experiment  
Nine visual scenes (wide-angle pictures in first-
person perspective and taken in Stockholm city, 
cf. Figure 2) were used for the task of 
crowdsourcing. Fifteen human participants (4 
females and 11 males) participated in the 
crowdsourcing exercise. All participants either 
studied or worked at the School of Computer 
Science and Communication, KTH, Stockholm. 
Participants were placed in front of a computer 
display and were told that the system will engage 
them in a spoken conversation to seek or clarify 
details about landmarks and other objects in vis-
ual scenes. They were told that the details would 
be used for pedestrian routing and therefore they 
are free to choose and specify details (in open-
ended questions) that they thought would be use-
ful when giving route instructions to another per-
son. 
Each participant did the nine visual scenes in 
the same order, with a 1 minute pause between 
each of them. The first visual scene was used as 
a trial in order to familiarize participants with the 
interaction scenario. For this reason, the trial in-
teraction was specifically designed to engage the 
participants in both wh? and yes?no type ques-
tions. We also wanted to investigate whether the 
use of wh? or yes?no questions alone has any 
impact on the interactions. For the remaining 
eight interactions, the dialogue system therefore 
alternatively used wh? and yes?no questions 
throughout the whole dialogue. The order of que-
ry types was switched for each successive partic-
ipant. Thus visual scenes presented with wh? 
queries to a participant pi were presented to par-
ticipant pi+1 with only yes?no queries, and vice-
versa. After each interaction participants were 
asked (1) whether the system appeared to be fa-
miliar with the visual scene; and (2) whether the 
interactions went smoothly. 
An example of a conversation with wh?type 
queries is presented in Table 1. The second col-
umn indicates the various types of utterances 
(dialogue acts) used by the system during this 
interaction. Note that during the slot-filling 
mode, the system has the initiative, whereas in 
the open-ended queries (cf. rows 15?22 in Table 
1) the system gives the initiative to the partici-
pant and only produces feedback responses. Ta-
ble 2 presents a conversation with yes?no queries 
following the conversation in Table 1. In a con-
versation with only yes?no  queries, a wh? query 
may be presented if a slot has been observed 
with no-value (cf. rows 9?10 in Table 1) or when 
the participant disputes the system?s current best 
estimate (cf. rows 6?9 in Table 2).  
 
S Introduction Could you help me with some de-
tails about this view? 
P  Yes 
S wh? query Do you see a building on the far 
left? 
P  Yes 
S Backchannel Mh-hmm 
S wh? query What color does the building have? 
P  Cream color [Green color] 
S Reprise Green 
S wh? query How many floors is this building? 
P  I don?t know [I do no] 
 ?  
S wh? query Are there any important signs in 
this scene that I may have missed? 
P  No 
S Backchannel Uh-huh 
S wh? query Is there anything peculiar in this 
scene that I should be familiar with 
P  There is a bike painted on street 
just in front of me 
[ is the right pink university in front 
of may ] 
S Backchannel Anything else? 
P  Yeah its white paint [Yes light pink] 
S Backchannel Mh-hmm 
P  That?s it [that that] 
S Backchannel Ehm 
P  I am done [okay] 
S Ending Okay, thanks 
 
Table 1: An example interaction with wh? que-
ries for the visual scene in Figure 2. S: system, P: 
participant (here, u01 from Figure 5). ASR re-
sults are shown within square brackets. 
 
S Introduction Could you help me with some details 
about this view? 
P  Yes 
S yes?no  
query 
Is the building on the far left visible to 
you? 
P  Yes 
S Backchannel Mh-hmm 
S yes?no  
query 
Does this building have green color? 
P  No 
S Backchannel Uh-huh 
S wh? query But than what color is the building? 
6
P  white [light] 
S Repeat Sorry, what color did you say? 
P  white [light] 
S wh? query How many stories is this building? 
P  I don?t know [I do rental] 
S Backchannel Ehm 
 ?  
S wh? query Are there any important signs in this 
scene that I may have missed? 
P  Ehm no [nine no] 
S Backchannel Ehm 
S yes?no  
query 
I am informed that there are other 
important landmarks or structures in 
this scene that I should be aware of, 
is it true? 
P  No 
S Backchannel uh-huh 
S Ending Okay, thanks 
 
Table 2: An example interaction with yes?no 
queries corresponding to the visual scene in Fig-
ure 2. S: system, P: participant (here u02 from 
Figure 5). ASR results are shown within square 
brackets. 
5 Data analysis 
We analyzed the data (15 8 interactions) col-
lected from the experiment along the following 
tracks: first, we compare the majority value of 
the slots to the ground truth as given by a human 
annotator; second, we explore how the ground 
truth of slot-values could be estimated automati-
cally; third, we also analyzed the instances where 
the participants disputed the system?s current 
estimate of slot-values; and fourth, we examined 
the post-experimental questionnaires.  
5.1 Rate of learning slot-values 
A total of 197 slots were learned in the exper-
iment. We analyzed how many slot-values had 
been correctly retrieved after 1, 2? 15 users. In 
Figure 6, the curve ?Majority? illustrates the 
fraction of slot-values correctly learned with 
each new user, under the assumption that the 
slot-values with majority votes ? from all the 15 
users ? constitute the ground truth. Thus after 
interacting with the first user the system had ob-
tained 67.0% of slot-values correctly (according 
to the majority) and 96.4% of slot-values after 
interacting with the first six users. Another eight 
users, or fourteen in total, were required to learn 
all the slot-values correctly. The progression 
curve thus provides an estimate of how many 
users are required to achieve a specific percent-
age of slot-values correctly if majority is to be 
considered the ground truth. The curve ?Not-in-
Majority? indicates the number of slot with val-
ues that were not in the majority. Thus after in-
teracting with the first user 20.8% of slot-values 
the system had obtained were not in majority and 
could be treated as incorrect. Note that the curves 
Majority and Not-in-Majority do not sum up to 
100%, this is because we consider no-value as a 
valid slot-value, and treat the slot as unfilled. For 
example, 12.2% of the slots remained unfilled 
after interacting with the first user.  
 
 
 
Figure 6: Rate of learning slot-values with two differ-
ent estimates of ground truth 
 
We also investigated how close the majority is 
to the actual truth. A human annotator (one of the 
coauthors) labeled all the obtained slot-values as 
either sensible or insensible, based on the com-
bined knowledge from the corresponding maps, 
the visual scenes, and the set of obtained values. 
Thus a slot could have many sensible values. For 
example, various parts of a building could be 
painted in different colors. The progression 
curves ?Sensible? and ?Insensible? in Figure 6 
illustrate the fraction of total slots for which the 
learned values were actually correct and incor-
rect, respectively. While the curve for sensible 
values follows the same pattern as the progres-
sion curve for majority as the estimate of ground 
truth, the percent of slot-values that were actually 
correct is always lower than the majority as 
ground truth, and it never reached 100%. The 
constant gap between the two curves suggests 
that some slot-values learned by the majority 
were not actually the ground truth. What led the 
majority into giving incorrect slot-values is left 
as a topic for future work. 
As mentioned earlier, much of the slot-filling 
interaction involved buildings and their proper-
ties. Figure 7 illustrates that sensible values for 
most slots, pertaining to whether a building is 
visible, whether it is residential, whether it has 
shops, and the color of roof were obtained by 
interacting with only few participants. In con-
trast, properties such as color of the building and 
7
number of stories required many more partici-
pants. This could be attributed to the fact that 
participants may have differences in perception 
about slot-values. As regards to whether there are 
signs on buildings, we observed that the recall is 
relatively low. This is largely due to lack of 
common ground among participants about what 
could be considered a sign. Our intentions with 
designing this prompt was to retrieve any peculi-
ar detail on the building that is easy to locate: for 
us a sign suggesting a name of restaurant is as 
useful as the knowledge that the building has 
blue sunshade on the windows. Some partici-
pants understood this while other didn?t. 
 
 
 
Figure 7: Learning rate of various slots for land-
mark type building  
5.2 Estimated ground truth of slot-values 
The 15 subjects in the in-lab experiment were all 
asked for the same information. In a real applica-
tion, however, we want the system to only ask 
for slots for which it has insufficient or conflict-
ing information. If the ground truth of a certain 
slot-value pair can be estimated with a certainty 
exceeding some threshold (given the quality re-
quirements of the database, say 0.8), the system 
can consider the matter settled, and need not ask 
about that slot again. We therefore want to esti-
mate the ground truth of slot-values along with a 
certainty measure. To this end, we use the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014), which is based on the 
probabilistic approach for supervised learning 
when we have multiple annotators providing la-
bels (possibly noisy) but no absolute gold stand-
ard, presented in Raykar et al. (2009). 
Using this approach, a question concerning the 
color of a building, say with ID 24, (e.g. ?What 
color is the building??) would be translated into 
several binary predicates COLOR_Red(24), 
COLOR_Brown(24), COLOR_Orange(24), etc. 
The justification for this binary encoding is that 
the different color values are not mutually exclu-
sive: A building might of course have more than 
one color, and in many cases more than one color 
name might be appropriate even though the 
building has only one dominating color (e.g. to 
describe the color either as ?brown? and ?red? 
might be acceptable to most people). Figure 8 
shows the incremental estimates for different 
colors for a certain building (OpenStreetMap ID 
163966736) after 1, 2? 15 subjects had been 
asked. The answer from the first subject was er-
roneously recognized as ?pink?. The next 9 sub-
jects all referred to the building as ?brown?. 
Among the final subjects, 3 subjects referred to 
building as ?red?, and 2 subjects as ?brown?. The 
final truth estimates are 0.98 for ?brown?, 0.002 
for ?red?, and 0.00005 for ?pink?. The diagram 
shows that if the certainty threshold is set to 0.8, 
the value ?brown? would have been established 
already after 4 subjects. 
 
 
 
Figure 8: Probabilities of different estimated ground 
truth values for the color of a certain building 
5.3 Disputed slot-values 
We also examined all system questions of 
yes?no type that received negative answers, i.e. 
instances where the participants disputed the sys-
tem?s current best estimate (based on majority 
vote) of a slot-value. Among the 95 such in-
stances, the system?s current best estimate was 
actually insensible only on 43 occasions. In 30 of 
these instances the participants provided a recti-
fied slot-value that was sensible. For the remain-
ing 13 instances the new slot-values proposed by 
the participant were actually insensible. There 
were 52 instances of false disputations, i.e. the 
system?s current estimate of a slot-value was 
sensible, but the participants disputed it. 6 of the-
se occurrences were due to errors in speech 
recognition, but for the remaining 46 occasions, 
error in grounding the intended landmark (15), 
users? perception of slot-values (3), and ambigui-
ty in what the annotator terms as sensible slot-
values (28), (e.g. whether there are signs on a 
building (as discussed in Section 5.1)) were iden-
8
tified as the main reasons. This suggests that 
slots (i.e. attributes) that are often disputed may 
not be easily understood by users. 
5.4 Post-experimental questionnaire 
As described above, the participants filled in a 
questionnaire after each interaction. They were 
asked to rate the system?s familiarity with the 
visual scene based on the questions asked. A 
Mann?Whitney U test suggests that participants? 
perception of the system?s familiarity with the 
visual scene was significantly higher for interac-
tions with yes?no queries than interactions with 
wh? queries (U=1769.5, p= 0.007). This result 
has implications for the design choice for sys-
tems that provide as well as ask for information 
from users. For example, a pedestrian routing 
system can already be used to offer routing in-
structions as well as crowdsourcing information. 
The system is more likely to give an impression 
of familiarity with the surrounding, to the user, 
by asking yes?no type questions than wh?
questions. This may influence a user?s confi-
dence or trust in using the routing system.  
Since yes?no questions expect a ?yes? or 
?no? in response, we therefore hypothesized that 
interactions with yes?no questions would be per-
ceived smoother in comparison to interactions 
with wh? questions. However, a Mann?Whitney 
U test suggests that the participants perceived no 
significant difference between the two interac-
tion types (U=1529.0, p= 0.248). Feedback 
comments from participants suggest that abrupt 
ending of open-ended interactions by the system 
(due to the simplistic model of detecting whether 
the user has anything more to say) gave users an 
impression that the system is not allowing them 
to speak. 
6 Discussion and future work 
We have presented a proof-of-concept study on 
using a spoken dialogue system for crowd-
sourcing street-level geographic information. To 
our knowledge, this is the first attempt at using 
spoken dialogue systems for crowdsourcing in 
this way. The system is fully automatic, in the 
sense that it (i) starts with minimal details ? ob-
tained from OpenStreetMap ? about a visual sce-
ne, (ii) prompts users with wh? questions to ob-
tain values for a predefined set of attributes; and 
(iii) assumes attribute-values with majority vote 
as its beliefs, and engages in yes?no questions 
with new participants to confirm them. In a data 
collection experiment, we have observed that 
after interacting with only 6 human participants 
the system acquires more than 80% of the slots 
with actually sensible values. 
We have also shown that the majority vote (as 
perceived by the system) could also be incorrect. 
To mitigate this, we have explored the use of the 
CityCrowdSource Trust software package 
(Dickens & Lupu, 2014) for obtaining the proba-
bilistic estimate of the ground truth of slot-values 
in a real crowd-sourcing system. However, it is 
important not only to consider the ground truth 
probabilities per se, but also on how many con-
tributing users the estimate is based and the qual-
ity of information obtained. We will explore the-
se two issues in future work. 
We have observed that through open-ended 
prompts, the system could potentially collect a 
large amount of details about the visual scenes. 
Since we did not use any automatic interpretation 
of these answers, we transcribed key concepts in 
participants? speech in order to obtain an esti-
mate of this. However, it is not obvious how to 
quantify the number of concepts. For example, 
we have learned that in Figure 2, at the junction 
ahead, there is: a traffic-sign, a speed-limit sign, 
a sign with yellow color, a sign with red color, a 
sign with red boarder, a sign that is round, a sign 
with some text, the text says 50. These are details 
obtained in pieces from various participants. 
Looking at Figure 2 one can see that these pieces 
when put together refer to the speed-limit sign 
mounted on the traffic-signal at the junction. 
How to assimilate these pieces together into a 
unified concept is a task that we have left for fu-
ture work. 
Acknowledgement 
We would like to thank the participants of the in-
lab crowd-sourcing experiment. This work is 
supported by the EIT KIC project 
?CityCrowdSource?, and the Swedish research 
council (VR) project Incremental processing in 
multimodal conversational systems (2011-6237).  
Reference 
Bartie, P. J., & Mackaness, W. A. (2006). Develop-
ment of a Speech-Based Augmented Reality Sys-
tem to Support Exploration of Cityscape. Transac-
tions in GIS, 10(1), 63-86. 
Boye, J., Fredriksson, M., G?tze, J., Gustafson, J., & 
K?nigsmann, J. (2014). Walk This Way: Spatial 
Grounding for City Exploration. In Mariani, J., 
Rosset, S., Garnier-Rizet, M., & Devillers, L. 
9
(Eds.), Natural Interaction with Robots, Knowbots 
and Smartphones (pp. 59-67). Springer New York. 
Dickens, L., & Lupu, E. (2014). Trust service final 
deliverable report. Technical Report, Imperial Col-
lege, UK. 
Haklay, M., & Weber, P. (2008). OpenStreetMap: 
User-Generated Street Maps. IEEE Pervasive 
Computing, 7(4), 12-18. 
Janarthanam, S., Lemon, O., Liu, X., Bartie, P., 
Mackaness, W., Dalmas, T., & Goetze, J. (2012). 
Integrating Location, Visibility, and Question-
Answering in a Spoken Dialogue System for Pe-
destrian City Exploration. In Proceedings of the 
13th Annual Meeting of the Special Interest Group 
on Discourse and Dialogue (pp. 134-136). Seoul, 
South Korea: Association for Computational Lin-
guistics. 
Krug, K., Mountain, D., & Phan, D. (2003). Webpark: 
Location-based services for mobile users in pro-
tected areas.. GeoInformatics, 26-29. 
Parent, G., & Eskenazi, M. (2011). Speaking to the 
Crowd: Looking at Past Achievements in Using 
Crowdsourcing for Speech and Predicting Future 
Challenges. In INTERSPEECH (pp. 3037-3040). 
ISCA. 
Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Flor-
in, C., Valadez, G. H., Bogoni, L., & Moy, L. 
(2009). Supervised Learning from Multiple Ex-
perts: Whom to Trust when Everyone Lies a Bit. In 
Proceedings of the 26th Annual International Con-
ference on Machine Learning (pp. 889-896). New 
York, NY, USA: ACM. 
Ross, T., May, A., & Thompson, S. (2004). The Use 
of Landmarks in Pedestrian Navigation Instructions 
and the Effects of Context. In Brewster, S., & Dun-
lop, M. (Eds.), Mobile Human-Computer Interac-
tion - MobileHCI 2004 (pp. 300-304). Springer 
Berlin Heidelberg. 
Skantze, G., & Al Moubayed, S. (2012). IrisTK: a 
statechart-based toolkit for multi-party face-to-face 
interaction. In Proceedings of ICMI. Santa Monica, 
CA. 
Tom, A., & Denis, M. (2003). Referring to Landmark 
or Street Information in Route Directions: What 
Difference Does It Make?. In Kuhn, W., Worboys, 
M., & Timpf, S. (Eds.), Spatial Information Theo-
ry. Foundations of Geographic Information Sci-
ence (pp. 362-374). Springer Berlin Heidelberg. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
10
Appendix A 
The table below lists slots (= landmark attributes) and the corresponding wh? and yes?no system questions. For 
attributes marked with * the dialogue manager switches to open-ended interaction mode. 
 
Slot (=attribute ) System wh? questions System yes?no questions 
Visible: whether a particular 
landmark is visible from this 
view. 
? Do you see a building on the far left? 
? Do you see another building in front of 
you? 
? Is there a junction on the right? 
? Do you see a traffic-signal ahead? 
? Is the building on the far right visible to 
you? 
? I think there is another building in front of 
you, do you see it? 
? Can you see the junction on the right? 
? Are you able to see the traffic-signal 
ahead? 
Color of the building 
? What color does the building have? 
? What color is the building? 
? I think this building is red in color, what do 
you think? 
? Does this building have red color? 
Size of the building (in num-
ber of stories) 
? How many floors do you think are 
there in this building 
? How many stories is this building 
? I think there are six floors in this building, 
what do you think? 
? Is this building six storied? 
Color of the building?s roof 
? What color does the roof of this build-
ing have? 
? What color is the roof of this building? 
? I think the roof of this building is orange in 
color, what do you think? 
? Do you think that the roof of this building 
is orange? 
Signs or ornamentation on the 
building 
? Do you see any signs or decorations 
on this building? 
? I think there is a sign or some decoration 
on this building, do you see it? 
? There may be a sign or a name on this 
building, do you see it? 
Shops or restaurants in the 
building 
? Are there any shops or restaurants in 
this building? 
? I am informed that there are some shops or 
restaurants in this building, is it true? 
? I think there are some shops or restaurants 
in this building, what do you think? 
Signs at landmarks 
? Are there any important signs at the 
junction/crossing? 
? I believe there is a sign at this junc-
tion/crossing, do you see it? 
? Do you see the sign at this junc-
tion/crossing? 
*Description of sign  
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
? Could you describe this sign? 
? What does this sign look like? 
? Does the sign say something? 
*Signs in the visual scene 
 
? Are there any important signs in this 
scene that I may have missed? 
? Have I missed any relevant signs in 
this scene? 
? There are some important signs in this 
scene that could be useful for my 
knowledge, am I right? 
? I am informed that there are some signs in 
this scene that are relevant for me, is it 
true? 
*Landmarks in the visual sce-
ne 
 
? Are there any other important build-
ings or relevant structures in this scene 
that I should be aware of? 
? Is there anything particular in this 
scene that I should be familiar with? 
? Have I missed any relevant buildings 
or landmarks in this scene? 
? I am informed that there are some im-
portant landmarks or structures in this sce-
ne that I should be aware of, is it true? 
? I have been told that there are some other 
things in this scene that I are relevant for 
me, is it true? 
? I believe I have missed some relevant 
landmarks in this scene, am I right? 
*Description of unknown 
landmarks e.g. shop, restau-
rant, building, etc. 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
? Could you describe it? 
? Could you describe them? 
? How do they look like? 
 
11
