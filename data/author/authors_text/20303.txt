Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 568?572, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NILC USP: A Hybrid System for Sentiment Analysis in Twitter Messages
Pedro P. Balage Filho and Thiago A. S. Pardo
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Science, University of Sa?o Paulo
Sa?o Carlos - SP, Brazil
{balage, taspardo}@icmc.usp.br
Abstract
This paper describes the NILC USP system
that participated in SemEval-2013 Task 2:
Sentiment Analysis in Twitter. Our system
adopts a hybrid classification process that
uses three classification approaches: rule-
based, lexicon-based and machine learning
approaches. We suggest a pipeline architec-
ture that extracts the best characteristics from
each classifier. Our system achieved an F-
score of 56.31% in the Twitter message-level
subtask.
1 Introduction
Twitter and Twitter messages (tweets) are a modern
way to express sentiment and feelings about aspects
of the world. In this scenario, understanding the sen-
timent contained in a message is of vital importance
in order to understand users behavior and for mar-
ket analysis (Java et al, 2007; Kwak et al, 2010).
The research area that deals with the computational
treatment of opinion, sentiment and subjectivity in
texts is called sentiment analysis (Pang et al, 2002).
Sentiment analysis is usually associated with a
text classification task. Sentiment classifiers are
commonly categorized in two basic approaches:
lexicon-based and machine learning (Taboada et al,
2011). A lexicon-based classifier uses a lexicon to
provide the polarity, or semantic orientation, of each
word or phrase in the text. A machine learning clas-
sifier learns features (usually the vocabulary) from
annotated corpus or labeled examples.
In this paper, we present a hybrid system for senti-
ment classification in Twitter messages. Our system
combines three different approaches: rule-based,
lexicon-based and machine learning. The purpose of
our system is to better understand the use of a hybrid
system in Twitter text and to verify the performance
of this approach in an open evaluation contest.
Our system participated in SemEval-2013 Task
2: Sentiment Analysis in Twitter (Wilson et al,
2013). The task objective was to determine the sen-
timent contained in Twitter messages. The task in-
cluded two sub-tasks: a expression-level classifi-
cation (Task A) and a message-level classification
(Task B). Our system participated in Task B. In this
task, for a given message, our system should classify
it as positive, negative, or neutral.
Our system was coded using Python and the
CLiPS Pattern library (De Smedt and Daelemans,
2012). This last library provides the part-of-speech
tagger and the SVM algorithm used in this work1.
2 Related work
Despite the significant number of works in senti-
ment analysis, few works have approached Twit-
ter messages. Agarwal et al (2011) explored new
features for sentiment classification of twitter mes-
sages. Davidov et al (2010) studied the use of
hashtags and emoticons in sentiment classification.
Diakopoulos and Shamma (2010) analyzed the peo-
ple?s sentiment on Twitter for first U.S. presidential
debate in 2008.
The majority of works in sentiment analysis uses
either machine learning techniques or lexicon-based
1Our system code is freely available at
http://github.com/pedrobalage/SemevalTwitterHybridClassifier
568
techniques. However, some few works have pre-
sented hybrid approaches. Ko?nig and Brill (2006)
propose a hybrid classifier that utilizes human rea-
soning over automatically discovered text patterns to
complement machine learning. Prabowo and Thel-
wall (2009) evaluates the effectiveness of different
classifiers. This study showed that the use of multi-
ple classifiers in a hybrid manner could improve the
effectiveness of sentiment analysis.
3 System architecture
Our system is organized in four main components:
normalization, rule-based classifier, lexicon-based
classifier and machine learning classifier. These
components are connected in a pipeline architecture
that extracts the best characteristics from each com-
ponent. The Figure 1 shows the system architecture.
Figure 1: System architecture
In this pipeline architecture, each classifier, in a
sequential order, evaluates the Twitter message. In
each step, the classifier may determine the polarity
class of the message if a certain degree of confidence
is achieved. If the classifier may not achieve this
confidence threshold, the classifier in the next step
is called. The machine learning classifier is the last
step in the process. It is responsible to determine the
polarity if the previous classifiers failed to achieve
the confidence level required to classification. The
normalization component is responsible to correct
and normalize the text before the classifiers use it.
This architecture improves the classification pro-
cess because it takes advantage of the multiple ap-
proaches. For example, the rule-based classifier is
the most reliable classifier. It achieves good results
when the text is matched by a high-confidence rule.
However, due the freedom of language, rules may
not match 100% of the unseen examples, conse-
quently it has a low recall rate.
Lexicon-based classifiers, for example, are very
confident in the process to determine if a text is polar
or neutral. Using sentiment lexicons, we can deter-
mine that sentences containing sentiment words are
polar and sentences that do not contain such words
are neutral. Moreover, the presence of a high num-
ber of positive or negative words in the text may be
a strong indicative of the polarity.
Finally, machine learning is known to be highly
domain adaptive and to be able to find deep corre-
lations (Taboada et al, 2011). This last classifier
might provide the final decision when the previous
methods failed. In the following sub-sections, we
describe in more details the components in which
our system is based on. In the next section, we ex-
plain how the confidence level was determined.
3.1 Normalization and rule-based classifier
The normalization module is in charge of correcting
and normalizing the texts. This module performs the
following operations:
? Elements such as hashtags, urls and mentions
are transformed into a consistent set of codes;
? Emoticons are grouped into representative
categories (such as happy, sad, laugh) and con-
verted to particular codes;
? Signals of exaltation (such as repetitive excla-
mation marks) are recognized;
? A simple misspelling correction is performed;
? Part-of-speech tagging is performed.
The rule-based classifier is very simple. The only
rules applied here are concerned to the emoticons
found in the text. Empirically, we evidenced that
positive emoticons are an important indicative of
positiveness in texts. Likewise, negative emoticons
569
indicate negativeness tendency. This module re-
turns the number of positive and negative emoticons
matched in the text.
3.2 Lexicon-based classifier
The lexicon-based classifier is based on the idea that
the polarity of a text can be summarized by the sum
of the individual polarity values of each word or
phrase present in the text. In this assumption, a
sentiment lexicon identifies polar words and assigns
polarity values to them (known as semantic orienta-
tions).
In our system, we used the sentiment lexicon pro-
vided by SentiStrength (Thelwall et al, 2010). This
lexicon provides an emotion vocabulary, an emoti-
cons list, a negation list and a booster word list.
In our algorithm, we sum the semantic orienta-
tions of each individual word in the text. If the word
is negated, the polarity is inverted. If the word is in-
tensified (boosted), we increase its polarity by a fac-
tor determined in the sentiment lexicon. A lexicon-
based classifier usually assumes the signal of the fi-
nal score as the sentiment class: positive, negative
or neutral (score zero).
3.3 Machine learning classifier
The machine learning classifier uses labeled exam-
ples to learn how to classify new instances. The
algorithm learns by using features extracted from
these examples. In our classifier, we used the SVM
algorithm provided by CLiPS Pattern. The features
used by the classifier are bag-of-words, the part-of-
speech set, and the existence of negation in the sen-
tence.
4 Hybrid approach and tuning
The organization from SemEval-2013 Task 2: Senti-
ment Analysis in Twitter provided three datasets for
the task (Wilson et al, 2013). A training dataset
(TrainSet), with 6,686 messages2, a development
dataset (DevSet), with 1,654 messages, and two test-
ing datasets (TestSets), with 3,813 (Twitter TestSet)
and 2,094 (SMS TestSet) messages each.
As we said in the previous section, our system is
a pipeline of classifiers where each classifier may
2The number of messages may differ from other participants
because the data was collected by crawling
assign a sentiment class if it achieves a particular
confidence threshold. This confidence threshold is a
fixed value we set for each system in order to have
a decision boundary. This decision was made by in-
specting the results table obtained with the develop-
ment set, as shown below.
Table 1 shows how the rule-based classifier per-
formed in the development dataset. The classifier
score consists in the difference between the num-
ber of positive emoticons and the number of nega-
tive emoticons found in the message. For example,
for score of -1 we had 22 negative, 4 neutral and 2
positive messages.
Table 1: Correlation between the rule-based classifier
scores and the gold standard classes in the DevSet
Rule-based Gold Standard Class
classifier score Negative Neutral Positive
-1 22 4 2
0 311 708 496
1 7 24 71
2 2 4
3 to 6 1 2
Inspecting the Table 1 we adjusted the rule-based
classifier boundary to decide when the score is dif-
ferent from zero. For values greater than zero, the
classifier will assign the positive class and, for val-
ues below zero, the classifier will assign the negative
class. For values equal zero, the classifier will call
the lexicon-based classifier.
Table 2 is similar to the Table 1, but it now shows
the scores obtained by the lexicon-based classifier
for the development set. This score is the message
semantic orientation computed by the sum of the se-
mantic orientation for each individual word.
Inspecting Table 2, we adjusted the lexicon-based
classifier to assign the positive class when the total
score is greater than 3 and negative class when the
total score is below -3. Moreover, we evidenced that,
compared to the other classifiers, the lexicon-based
classifier had better performance to determine the
neutral class. Therefore, we adjusted the lexicon-
based classifier to assign the neutral class when the
total score is zero. For any other values, the machine
learning classifier is called.
Finally, Table 3 shows the confusion matrix for
the machine learning classifier in the development
570
Table 2: Correlation between the lexicon-based classifier
score and the gold standard classes in the DevSet
Lexicon-based Gold Standard Class
classifier scores Negative Neutral Positive
-11 to -6 26 8 4
-5 15 6 4
-4 31 20 9
-3 32 24 5
-2 57 86 22
-1 25 31 20
0 74 354 115
1 26 70 42
2 28 87 103
3 12 29 81
4 8 9 56
5 2 6 42
6 to 13 4 9 72
dataset. The machine learning classifier does not
operate with a confidence threshold, so no decisions
were made for this classifier. We see that machine
learning classifier does not have a good accuracy
in general. Our hybrid approach proposed aims to
overcome this problem. Next section shows the re-
sults achieved for the Semeval test dataset.
Table 3: Confusion matrix for the machine learning clas-
sifier in the DevSet
Machine learning Gold Standard Class
classifier class Negative Neutral Positive
negative 35 6 11
neutral 232 595 262
positive 73 138 302
5 Results
Table 4 shows the results obtained by each individ-
ual classifier and the hybrid classifier for the test
dataset. In the task, the systems were evaluated with
the average F-Score obtained for positive and nega-
tive classes3. We see that the Hybrid approach could
improve in relation to each classifier score, confirm-
ing our hypothesis.
3Semeval-2013 Task 2: Sentiment Analysis in Twitter com-
pares the systems by the average F-score for positive and nega-
tive classes. For more information see Wilson et al (2013)
Table 4: Average F-score (positive and negative) obtained
by each classifier and the hybrid approach
Classifier Twitter TestSet SMS TestSet
Rule-based 0.1437 0.0665
Lexicon-Based 0.4487 0.4282
Machine Learning 0.4999 0.4029
Hybrid Approach 0.5631 0.5012
Table 5 shows the results in terms of precision,
recall and F-score for each class by the hybrid clas-
sifier in the Twitter dataset. Inspecting our algo-
rithm for the Twitter dataset, we had 277 examples
classified by the rule-based classifier, 2,312 by the
lexicon-based classifier and 1,224 the by machine
learning classifier. The results for the SMS dataset
had similar values.
Table 5: Results for Twitter TestSet
Class Precision Recall F-Score
positive 0.6935 0.6145 0.6516
negative 0.5614 0.4110 0.4745
neutral 0.6152 0.7427 0.6729
6 Conclusion
We described a hybrid classification system used for
Semeval-2013 Task 2: Sentiment Analysis in Twit-
ter. This paper showed how a hybrid classifier might
take advantage of multiple sentiment analysis ap-
proaches and how these approaches perform in a
Twitter dataset.
A future direction of this work would be im-
proving each individual classifier. In our system,
we used simple methods for each employed classi-
fier. Thus, we believe the hybrid classification tech-
nique applied might achieve even better results. This
strengthens our theory that hybrid techniques might
outperform the current state-of-art in sentiment anal-
ysis.
Acknowledgments
We would like to thank the organizers for their work
constructing the dataset and overseeing the task. We
also would like to thank FAPESP and CNPq for fi-
nancial support.
571
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tom De Smedt and Walter Daelemans. 2012. Pattern for
python. The Journal of Machine Learning Research,
13:2063?2067.
Nicholas A. Diakopoulos and David A. Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI
?10, pages 1195?1198, New York, NY, USA. ACM.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we twitter: understanding microblogging
usage and communities. In Proceedings of the 9th We-
bKDD and 1st SNA-KDD 2007 workshop on Web min-
ing and social network analysis, WebKDD/SNA-KDD
?07, pages 56?65, New York, NY, USA. ACM.
Arnd Christian Ko?nig and Eric Brill. 2006. Reducing the
human overhead in text categorization. In Proceed-
ings of the 12th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?06, pages 598?603, New York, NY, USA. ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - EMNLP ?02, pages 79?86,
Morristown, NJ, USA, July. Association for Computa-
tional Linguistics.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Informet-
rics, 3(2):143?157.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267?307, June.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in short
strength detection informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558, December.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
572
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 428?432,
Dublin, Ireland, August 23-24, 2014.
NILC USP: An Improved Hybrid System for Sentiment Analysis in
Twitter Messages
Pedro P. Balage Filho, Lucas Avanc?o, Thiago A. S. Pardo, Maria G. V. Nunes
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Sciences, University of S?ao Paulo
S?ao Carlos - SP, Brazil
{balage, taspardo, gracan}@icmc.usp.br avanco@usp.br
Abstract
This paper describes the NILC USP sys-
tem that participated in SemEval-2014
Task 9: Sentiment Analysis in Twitter, a
re-run of the SemEval 2013 task under the
same name. Our system is an improved
version of the system that participated in
the 2013 task. This system adopts a hybrid
classification process that uses three clas-
sification approaches: rule-based, lexicon-
based and machine learning. We sug-
gest a pipeline architecture that extracts
the best characteristics from each classi-
fier. In this work, we want to verify how
this hybrid approach would improve with
better classifiers. The improved system
achieved an F-score of 65.39% in the Twit-
ter message-level subtask for 2013 dataset
(+ 9.08% of improvement) and 63.94% for
2014 dataset.
1 Introduction
Twitter is an important platform of social com-
munication. The analysis of the Twitter messages
(tweets) offers a new possibility to understand so-
cial behavior. Understanding the sentiment con-
tained in such messages showed to be very impor-
tant to understand user behavior and also to as-
sist market analysis (Java et al., 2007; Kwak et al.,
2010).
Sentiment analysis, the area in charge of study-
ing how sentiments and opinions are expressed in
texts, is usually associated with text classification
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tasks. Sentiment classifiers are commonly cate-
gorized in two basic approaches: lexicon-based
and machine learning approaches (Taboada et al.,
2011). A lexicon-based classifier uses a lexicon
to provide the polarity, or semantic orientation, of
each word or phrase in the text. A machine learn-
ing classifier uses features (usually the vocabulary
in the texts) obtained from labeled examples to
classify the texts according to their polarity.
In this paper, we present a hybrid system for
sentiment classification in Twitter messages. Our
system combines the lexicon-based and machine
learning approaches, as well as uses simple rules
to aid in the process. Our system participated in
SemEval-2014 Task 9: Sentiment Analysis in Twit-
ter (Rosenthal et al., 2014), a re-run for the Se-
mEval 2013 task under the same name (Nakov et
al., 2013). The task goal was to determine the sen-
timent contained in tweets. The task included two
sub-tasks: a expression-level classification (Task
A) and a message-level classification (Task B).
Our system participated only in Task B, where, for
a given message, it should classify it as positive,
negative, or neutral.
The system presented is an improved version of
the system submitted for Semeval 2013. Our pre-
vious system had demonstrated that a hybrid ap-
proach could achieve good results (F-measure of
56.31%), even if we did not use the state-of-the-
art algorithms for each approach (Balage Filho and
Pardo, 2013). In this way, this work aims to ver-
ify how much this hybrid system could improve in
relation to the previous one by including modifica-
tions on both lexicon-based and machine learning
approaches.
428
2 Related work
The analysis of Tweets has gained lots of interest
recently. One evidence is the expressive number
of participants in the SemEval-2013 Task 2: Sen-
timent Analysis in Twitter (Nakov et al., 2013).
There were a total of 149 submissions from 44
teams. The best performing system on twitter
dataset for task B was reported by Mohammad et
al. (2013) with an F-mesaure of 69.02%. Their
system used a machine learning approach and a
very rich feature set. They showed that the best
results were achieved using a built-in positive and
negative lexicon and a bag-of-words as features.
Other important system in Semeval 2013 was
reported by Malandrakis et al. (2013). The authors
presented a hybrid system for twitter sentiment
analysis combining two approaches: a hierarchi-
cal model based on an affective lexicon and a lan-
guage modeling approach. The system achieved
an F-mesaure of 60.14%.
Most work in sentiment analysis uses either ma-
chine learning or lexicon-based techniques. How-
ever, few studies have shown promising results
with the hybrid approach. K?onig and Brill (2006)
proposed a hybrid classifier that uses human rea-
soning over automatically discovered text patterns
to complement machine learning. Prabowo and
Thelwall (2009) evaluated the effectiveness of dif-
ferent classifiers. Their study showed that the use
of multiple classifiers in a hybrid manner could
improve the effectiveness of sentiment analysis.
3 System Architecture
Our system is described as a pipeline solution of
four main processes: normalization, rule-based
classification, lexicon-based classification and ma-
chine learning classification. This is the same ar-
chitecture presented by our system in 2013.
This pipeline architecture works as a back-off
model. In this model, each classifier tries to clas-
sify the tweets by using the underlying approach.
If a certain degree of confidence is achieved, the
classifier will provide the final sentiment class for
the message. Otherwise, the next classifier will
continue the classification task. The last possibil-
ity is the machine learning classifier, responsible
to deliver the class when the previous two could
not achieve the confidence level. We decided to
use this back-off model instead of a voting system,
for example, due to the high precision achieved for
the rule-based and the lexicon-based classifiers.
The aim of this pipeline architecture is to im-
prove the classification process. In Balage Filho
and Pardo (2013), we have shown that this hybrid
classification approach may outperform the indi-
vidual approaches.
In the following subsections, we detail the com-
ponents of our system. In the next section, we ex-
plain how the confidence level was determined.
3.1 Normalization and Rule-based Classifier
The normalization module is responsible for nor-
malizing and tagging the texts. This module per-
forms the following operations:
? Hashtags, urls and mentions are transformed
into codes;
? Emoticons are grouped into representative
categories (such as ?happy?, ?sad?, ?laugh?)
and are converted to particular codes;
? Part-of-speech tagging is performed by using
the Ark-twitter NLP (Owoputi et al., 2013)
The rule-based classifier is designed to provide
rules that better impact the precision than the re-
call. In our 2014 system, we decided to use the
same rule-based classifier from the 2013 system.
The rules in this classifier only verify the pres-
ence of emoticons in the text. Empirically, we
evidenced that the use of emoticons indicates the
actual polarity of the message. In this module,
we consider the number of positive and negative
emoticons found in the text to determine its clas-
sification.
3.2 Lexicon-based Classifier
The lexicon-based classifier is based on the idea
that the polarity of a text can be given by the sum
of the individual polarity values of each word or
phrase present in the text. For this, a sentiment lex-
icon identifies polarity words and assigns polarity
values to them (known as semantic orientations).
In the 2013 system, we had used SentiStrength
lexicon (Thelwall et al., 2010). In 2014, we
improved our lexicon-based classifier by using
a larger sentiment lexicon. We used the senti-
ment lexicon provided by Opinion-Lexicon (Hu
and Liu, 2004) and a list of sentiment hashtags
provided by the NRC Hashtag Sentiment Lexicon
(Mohammad et al., 2013). For dealing with nega-
tion, we used a handcrafted list of negative words.
429
In our algorithm, the semantic orientations of
each individual word in the text are added up.
In this approach, the algorithm searches for each
word in the lexicon and only the words that were
found are returned. We associate the value +1 to
the positive words, and -1 to the negative words.
If a polarity word is negated, its value is inverted.
This lexicon-based classifier assumes the signal of
the final score as the sentiment class (positive or
negative) and the score zero as neutral.
3.3 Machine Learning Classifier
The machine learning classifier uses labeled ex-
amples to learn how to classify new instances.
The features used for this 2014 system were com-
pletely changed from 2013 system. We inspired
our machine learning module in the work reported
by Mohammad et al. (2013). The features used by
the classifier are:
1. unigrams, bigrams and trigrams
2. the presence of negation
3. the presence of three or more characters in
the words
4. the sequence of three or more punctuation
marks
5. the number of words with all letters in upper-
case
6. the total number of each tag present in the
text
7. the number of positive words computed by
the lexicon-based method
8. the number of negative words computed by
the lexicon-based method
We use a Linear Kernel SVM classifier provided
by the python sckit-learn library with C=0.005
1
.
4 Hybrid Approach and Tuning
The organization from SemEval-2014 Task 9: Sen-
timent Analysis in Twitter provided four datasets
for the task: a training dataset (TrainSet) with
9675 messages directly retrieved from Twitter; a
development dataset (DevSet), with 1654 mes-
sages; the testing dataset from 2013 run, which
was not used; and the testing dataset for 2014
1
Available at http://scikit-learn.org/
with 8987 messages. The 2014 testing dataset was
composed of 5 different sources:
? Twitter2013: Twitter test data from 2013 run
? SMS2013: SMS test data from 2013 run
? Twitter2014: 2000 tweets
? LiveJournal2014: 2000 sentences from Live-
Journal blogs
? Twitter2014Sarcasm: 100 tweets that contain
sarcasm
As we said in the previous section, our system is
a pipeline of classifiers where each classifier may
assign a sentiment class if it achieves a particu-
lar confidence threshold score. This confidence
score is a fixed value set for each system in or-
der to have a decision boundary. This decision
was made by inspecting the results obtained for the
development set. Tables 1 and 2 shows how the
rule-based and lexicon-based classifiers perform
for the development dataset in terms of score. The
score obtained by the rule-based classifier consists
of the difference between the number of positive
emoticons and the number of negative emoticons
found in the messages. The score obtained by the
lexicon-based classifier represents the total seman-
tic orientation obtained by the algorithm by adding
up the semantic orientation for their lexicon.
Inspecting Table 1, for the best threshold, we
adjusted the rule-based classifier boundary to de-
cide when the score is different from zero. For
values greater than zero, the classifier will assign
the positive class and, for values below zero, the
classifier will assign the negative class. For values
equal to zero, the classifier will call the lexicon-
based classifier.
Table 1: Correlation between the rule-based clas-
sifier scores and the gold standard classes in the
DevSet
Rule-based Gold Standard Class
classifier score Negative Neutral Positive
-1 22 3 3
0 311 709 495
1 7 26 73
2 0 0 2
3 to 6 0 1 2
Inspecting Table 2, for the best threshold, we
adjusted the lexicon-based classifier to assign the
430
positive class when the total score is greater than
1 and negative class when the total score is below
-2. For any other values, the classifier will call the
machine learning classifier.
Table 2: Correlation between the lexicon-based
classifier score and the gold standard classes in the
devset
Lexicon-based Gold Standard Class
classifier scores Negative Neutral Positive
-7 to -4 2 0 0
-3 10 4 0
-2 48 18 7
-1 111 99 35
0 108 432 178
1 48 143 210
2 11 39 104
3 to 5 3 4 47
As the machine learning classifier is responsible
for the final stage, we did not have to decide any
threshold for this classifier. However, we empiri-
cally identified a bias toward the positive class (the
negative class was barely chosen). In order to cor-
rect this problem, we setup the machine learning
classifier to decide for the negative class whenever
the SVM score for this class is bigger than -0.4.
Next section shows the results achieved for the Se-
meval test dataset.
5 Results
Table 3 shows the results obtained by each individ-
ual classifier and by the hybrid classifier for the
Twitter2014 messages in the testset. In the task,
the systems were evaluated with the average F-
score obtained for positive and negative classes.
Table 3: Average F-score (positive and negative)
obtained by each classifier and the hybrid ap-
proach for the Twitter2014 testset
Classifier Twitter2014 Testset
Rule-based 14.03
Lexicon-Based 47.55
Machine Learning 63.36
Hybrid Approach 63.94
Table 4 shows the improvement of the system
over the 2013 run. Unlike last year, we notice that
the performance of this hybrid system is very close
to the performance of the machine-learning.
Table 4: Comparison of the average F-score (pos-
itive and negative) obtained by each classifier and
the hybrid approach for the Twitter2013 testset for
2013 and 2014 versions
Classifier 2013 system 2014 system
Rule-based 14.37 13.31
Lexicon-Based 44.87 46.80
Machine Learning 49.99 63.75
Hybrid Approach 56.31 65.39
Table 5 shows the scores for each source in the
testset. Last column shows our system rank among
the 50 systems that participated in the competition.
For the entire testing dataset, our algorithm had
503 (5%) examples classified by the rule-based
classifier, 3204 (36%) by the lexicon-based classi-
fier and 5280 (59%) by the machine learning clas-
sifier.
6 Conclusion
We described our improved hybrid classification
system used for Semeval-2014 Task 9: Sentiment
Analysis in Twitter. This work showed that this
hybrid classifier can be improved as its modules
are too. However, we noticed that, improving the
lexicon and machine learning modules, the overall
score tends towards the machine learning score.
The source code produced for the experiment is
available at https://github.com/pedrobalage.
Acknowledgments
We would like to thank the organizers for their
work in constructing the dataset and in the over-
seeing of the task. We also would like to
thank FAPESP and SAMSUNG for supporting
this work.
References
Pedro Balage Filho and Thiago Pardo. 2013.
NILC USP: A Hybrid System for Sentiment Analy-
sis in Twitter Messages. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 568?572, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
431
Table 5: Results for Twitter TestSet
TestSet Source Majority Baseline Our Score Best Result Our Rank
Twitter2013 29.2 65.39 72.12 15th
SMS2013 19.0 61.35 70.28 16th
Twitter2014 34.6 63.94 70.96 19th
LiveJournal2014 27.2 69.02 74.84 18th
Twitter2014Sarcasm 27.7 42.06 58.16 34th
Akshay Java, Xiaodan Song, Tim Finin, and Belle
Tseng. 2007. Why we twitter: understanding mi-
croblogging usage and communities. In Proceed-
ings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network anal-
ysis, WebKDD/SNA-KDD ?07, pages 56?65, New
York, NY, USA. ACM.
Arnd Christian K?onig and Eric Brill. 2006. Reducing
the human overhead in text categorization. In Pro-
ceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?06, pages 598?603, New York, NY, USA.
ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is twitter, a social network
or a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexan-
dros Potamianos, and Shrikanth Narayanan. 2013.
SAIL: A hybrid approach to sentiment analysis. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 438?442, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in
short strength detection informal text. Journal of the
American Society for Information Science and Tech-
nology, 61(12):2544?2558, December.
432
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 433?436,
Dublin, Ireland, August 23-24, 2014.
NILC USP: Aspect Extraction using Semantic Labels
Pedro P. Balage Filho and Thiago A. S. Pardo
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Sciences, University of S?ao Paulo
S?ao Carlos - SP, Brazil
{balage, taspardo}@icmc.usp.br
Abstract
This paper details the system NILC USP
that participated in the Semeval 2014: As-
pect Based Sentiment Analysis task. This
system uses a Conditional Random Field
(CRF) algorithm for extracting the aspects
mentioned in the text. Our work added se-
mantic labels into a basic feature set for
measuring the efficiency of those for as-
pect extraction. We used the semantic
roles and the highest verb frame as fea-
tures for the machine learning. Overall,
our results demonstrated that the system
could not improve with the use of this se-
mantic information, but its precision was
increased.
1 Introduction
Sentiment analysis, or opinion mining, has gained
lots of attention lately. The importance of this
field of study is linked with the grown of informa-
tion in the internet and the commercial attention it
brought.
According to Liu et al. (2010), there are two
kinds of information available in the internet: facts
and opinions. Facts are objective statements about
entities and events in the world. Opinions are sub-
jective statements that reflect people?s sentiments
or perceptions about the entities and events. Ac-
cording to Liu, by that time, there was a lot of at-
tention on the processing of facts but little work
had been done on the processing of opinions.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Three levels of analysis for sentiment analysis
are known (Liu, 2012): document level, sentence
level and aspect level. The aspect-based sentiment
analysis is the name of the research topic that aims
to extract the sentiments about the aspects present
in the text.
This work presents a system evaluated in the
SemEval Task4: Aspect Based Sentiment Analy-
sis shared task (Pontiki et al., 2014). Our system
participated only in subtask 1: Aspect Term Ex-
traction. In this subtask, given a text, the system
should extract all aspects that are present. There
were two different domains for this task: restau-
rants and laptops.
The goal of our system was to verify how se-
mantic labels used in machine learning classifica-
tion would improve the aspect extraction task. For
this goal, we used two kinds of semantic labels:
the semantic roles (Palmer et al., 2010) and the se-
mantic frames (Baker et al., 1998).
Liu et al. (2012) categorizes the works for as-
pect extraction in four types, regarding the ap-
proach they follow, using: frequent terms, infre-
quent terms, machine learning, and topic model-
ing. This work uses a machine learning approach
that consists in training a sequential labeling algo-
rithm for aspect detection and extraction.
In what follows, we present some related work
in Section 2. Section 3 and 4 introduce our system
and report the achieved results. Some conclusions
are presented in Section 5.
2 Related work
Jin and Hovy (2009) reported one the first works
using sequential labeling for aspect extraction. In
this work, the authors used a Lexicalized Hidden
Markov Model to learn patterns to extract aspects
and opinions. Jakob and Gurevych (2010) trained
433
a Conditional Random Field for aspect extraction.
In this work, the authors report the results for a sin-
gle domain and a cross domain experiment. They
show that even in other domains the method could
be good.
Kim and Hovy (2006) explored the semantic
structure of a sentence, anchored to an opinion
bearing verb or adjective. Their method uses se-
mantic role labeling as an intermediate step to la-
bel an opinion holder and topic using data from
FrameNet.
Houen (2011) presented a system for opinion
mining with semantic analysis. The author ex-
plores the use of the semantic frame-based ana-
lyzer FrameNet (Baker et al., 1998) for modeling
features in a machine learning approach. The au-
thor found that the FrameNet information was not
helpful in this classifier.
3 System Description
Our system uses a sequential labeling algorithm.
In our work, we use the Conditional Random Field
(Lafferty et al., 2001) algorithm provided by the
CRF++ tool
1
.
For training the sequential labeling algorithm,
we give as input features for each word in the cor-
pus. The algorithm will then learn how to classify
those words. In our approach, the possible classes
are: True, representing an aspect word; and False,
representing the remaining words.
The goal of our system was to evaluate the per-
formance of the semantic labels for the task. In
order to model our system, we built a feature set
consisting of 6 features.
1. the word
2. the part-of-speech
3. the chunk
4. the named-entity category
5. the semantic role label (SRL)
6. the most generic frame in FrameNet
The use of the first four features is consistent
with the best approaches in aspect-based senti-
ment analysis. The last two features are the ones
we are testing in our work.
In order to extract the features, we used two im-
portant tools: the Senna (Collobert et al., 2011), a
1
Available at http://crfpp.googlecode.com/
semantic role labeling system, and the ARK SE-
MAFOR, a Semantic Analyzer of Frame Repre-
sentations (Das et al., 2010).
The Senna system uses a deep learning neural
network (Collobert, 2011) to provide several pre-
dictions for natural language processing. The sys-
tem output is represented in the CONLL format,
the same used in CRF++.
Our first 5 features were directly provided by
the Senna output. In these features, we decided to
keep the IOBE information since the initial exper-
iments showed the results were better with it than
without.
Our fifth feature, the semantic role label, was
retrieved from Senna as well. In the corresponding
paper, they reported Senna could achieve a F1 of
75% for the SRL task.
The example below shows how the features
were represented. In this example, we are only
showing four features: the word, the part-of-
speech, the chunk and the SRL. The classes are
in the last column.
WORD POS CHUNK SRL IS_ASPECT?
Great JJ B-NP B-A0 False
laptop NN E-NP E-A0 False
that WDT S-NP S-R-A0 False
offers VBZ S-VP S-V False
many JJ B-NP B-A1 False
great JJ I-NP I-A1 False
features NNS E-NP E-A1 True
! . O - False
The last feature was retrieved by ARK SE-
MAFOR tool. ARK SEMAFOR uses a probabilis-
tic frame-semantic parsing using the FrameNet re-
source. The ARK SEMAFOR output is the anal-
ysis of the frames present in the text for a given
verb. As our feature set has only word related fea-
tures, we decided to use the most upper level struc-
ture in the frame. In case of multiple verbs in the
sentence, we used the structure for the verb that is
closest to the word of interest.
The following example shows how the frames
were added into the training model. We limit to
show only the word, frame and the class. For train-
ing, we used the full training set with the six fea-
tures plus the class.
WORD FRAME IS_ASPECT?
I Shopping False
shopped Shopping False
around Relational_quantity False
before Relational_quantity False
434
buying Relational_quantity False
. O False
The organization from SemEval-2014 Task 4:
Aspect Based Sentiment Analysis provided two
domains for evaluation: restaurants and laptops.
For each domain, the organization provided three
datasets: a trainset, a devset and a testset.
We executed our algorithm with C pa-
rameter equal to 4.0. The experiment
code is fully available at the weblink
https://github.com/pedrobalage/
4 Results
Tables 1 and 2 show our system results for the
restaurants and laptops domains respectively. In
these tables, the results are discriminated by the
feature sets that were used. The reader may see
that a ?+ Frame? system, for example, stands for
all the features discriminated above (Word, POS,
Chunk, NR, SRL) plus the Frame feature. The last
line shows the results scored by our system in the
SemEval shared task with all the features. We also
show the results for the baseline system provided
by the shared task (Pontiki et al., 2014).
Table 1: Results for restaurants domain
System Precision Recall F1-mesaure
Baseline 52.54 42.76 47.15
Word + POS 83.76 68.69 75.48
+ Chunk 83.38 68.16 75.01
+ NE 83.45 68.07 74.98
+ SRL 82.79 67.46 74.34
+ Frame 87.72 34.03 49.04
Table 2: Results for laptops domain
System Precision Recall F1-mesaure
Baseline 44.31 29.81 35.64
Word + POS 80.87 39.44 53.03
+ Chunk 78.83 39.29 52.44
+ NE 79.93 39.60 52.96
+ SRL 78.22 38.99 52.04
+ Frame 83.62 14.83 25.19
Comparing with the baseline, we may noticed
that our submitted system (+Frame) outperformed
the baseline for the restaurants domain but it did
not outperformed the baseline for the laptops do-
main (considering F1 mesaure).
When we look in detail for the inclusion of fea-
tures in our feature set, we may notice that, at ev-
ery new feature, the precision goes up, but the re-
call goes down. We believe this is due to the be-
haviour of the conditional random field algorithm
for compensating for a sparser feature set.
In general, the semantic labels (SRL and Frame)
could not improve the results. However, if we
are interested only on precision, these features are
helpful. This may be the case in scenarios where a
lot of information is available, as in the web, and
we want to be sure about the retrieved informa-
tion. Certainly, there is a conflict between preci-
sion and computational complexity, since the se-
mantic features are more expensive to be achieved
(in relation to the usual simpler features that may
be used).
Despite of that, we judge to be necessary to con-
duct more experiments in order to better evaluate
the impact of semantic labels in the aspect extrac-
tion task.
5 Conclusion
We presented an aspect extraction system built on
a conditional random field algorithm. We used
a rich feature set with the semantic roles and
the FrameNet upper frames for each word. We
have showed that the semantic labels may help to
achieve a more precise classifier, but it did not help
to improve the overall F-measure of the system.
Regarding the shared task, our system achieved
the second best precision value among the com-
peting systems, but the lowest recall value. Future
work should investigate ways of also improving
recall without penalty for the achieved precision.
Acknowledgments
We would like to thank the organizers for their
work constructing the dataset and overseeing the
task. We also would like to thank FAPESP for the
financial support.
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1, pages 86?90. Association for Computa-
tional Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
435
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Geoffrey J. Gordon and
David B. Dunson, editors, Proceedings of the Four-
teenth International Conference on Artificial Intel-
ligence and Statistics (AISTATS-11), volume 15,
pages 224?232. Journal of Machine Learning Re-
search - Workshop and Conference Proceedings.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Semafor 1.0: A probabilistic
frame-semantic parser. Technical report, Language
Technologies Institute, School of Computer Science,
Carnegie Mellon University.
S?ren Houen. 2011. Opinion Mining with Seman-
tic Analysis. Ph.D. thesis, Department of Computer
Science, University of Copenhagen.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1035?1045.
Wei Jin and Hung Hay Ho. 2009. A Novel Lexicalized
HMM-based Learning Framework for Web Opinion
Mining. In L?eon Bottou and Michael Littman, ed-
itors, Proceedings of International Conference on
Machine Learning (ICML-2009), ICML ?09, pages
1?8. ACM, ACM Press.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Bing Liu. 2010. Sentiment Analysis and Subjectivity.
In N Indurkhya and F J Damerau, editors, Handbook
of Natural Language Processing, number 1, chap-
ter 28, pages 627?666. CRC Press, Taylor and Fran-
cis Group, Boca Raton.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1?103.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation, SemEval 2014, Dublin, Ireland.
436
