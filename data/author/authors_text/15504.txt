Proceedings of the EACL 2009 Student Research Workshop, pages 28?36,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
Finding Word Substitutions Using a Distributional Similarity Baseline
and Immediate Context Overlap
Aurelie Herbelot
University of Cambridge
Computer Laboratory
J.J. Thompson Avenue
Cambridge
ah433@cam.ac.uk
Abstract
This paper deals with the task of find-
ing generally applicable substitutions for a
given input term. We show that the output
of a distributional similarity system base-
line can be filtered to obtain terms that are
not simply similar but frequently substi-
tutable. Our filter relies on the fact that
when two terms are in a common entail-
ment relation, it should be possible to sub-
stitute one for the other in their most fre-
quent surface contexts. Using the Google
5-gram corpus to find such characteris-
tic contexts, we show that for the given
task, our filter improves the precision of a
distributional similarity system from 41%
to 56% on a test set comprising common
transitive verbs.
1 Introduction
This paper looks at the task of finding word substi-
tutions for simple statements in the context of KB
querying. Let us assume that we have a knowl-
edge base made of statements of the type ?subject
? verb ? object?:
1. Bank of America ? acquire ? Merrill Lynch
2. Lloyd?s ? buy ? HBOS
3. Iceland ? nationalise ? Kaupthing
Let us also assume a simple querying facility,
where the user can enter a word and be presented
with all statements containing that word, in a typ-
ical search engine fashion. If we want to return all
acquisition events present in the knowledge base
above (as opposed to nationalisation events), we
might search for ?acquire?. This will return the
first statement (about the acquisition of Merrill
Lynch) but not the second statement about HBOS.
Ideally, we would like a system able to generate
words similar to our query, so that a statement
containing the verb ?buy? gets returned when we
search for ?acquire?.
This problem is closely related to the clustering
of semantically similar terms, which has received
much attention in the literature. Systems that
perform such clustering usually do so under the
assumption of distributional similarity (Harris,
1954) which state that two words appearing
in similar contexts will be close in meaning.
This observation is statistically useful and has
contributed to successful systems within two
approaches: the pattern-based approach and the
feature vector approach (we describe those two
approaches in the next section). The definition
of similarity used by those systems is fairly
wide, however. Typically, a query on the verb
?produce? will return verbs such as ?export?, ?im-
port? or ?sell?, for instance (see DIRT demo from
http://demo.patrickpantel.com/Content/Lex
Sem/paraphrase.htm, Lin and Pantel, 2001.)
This fairly wide notion of similarity is not fully
appropriate for our word substitutions task: al-
though cats and dogs are similar types of enti-
ties, querying a knowledge base for ?cat? shouldn?t
return statements about dogs; statements about
Siamese, however, should be acceptable. So, fol-
lowing Dagan and Glickman (2004), we refine our
concept of similarity as that of entailment, defined
here as the relation whereby the meaning of a word
w1 is ?included? in the meaning of word w2 (prac-
tically speaking, we assume that the ?meaning? of
a word is represented by the contexts in which it
appears and require that if w1 entails w2, the con-
texts of w2 should be a subset of the contexts of
w1). Given an input term w, we therefore attempt
to extract words which either entail or are entailed
by w. (We do not extract directionality at this
stage.)
28
The definition of entailment usually implies that
an entailing word must be substitutable for the en-
tailed one, in some contexts at least. Here, we con-
sider word substitution queries in cases where no
additional contextual information is given, so we
cannot assume that possible, but rare, substitutions
will fit the query intended by the user (?believe?
correctly entails ?buy? in some cases but we can
be reasonably sure that the query ?buy? is meant
in the ?purchase? sense.) We thus require that our
output will fit the most common contexts. For in-
stance, given the query ?kill?, we want to return
?murder? but not ?stop?. Given ?produce?, we want
to return both ?release? and ?generate? but not ?fab-
ricate? or ?hatch?.1 Taking this into account, we
generally define substitutability as the ability of a
word to replace another one in a given sentence
without changing the meaning or acceptability of
the sentence, and this in the most frequent cases.
(By acceptability, we mean whether the sentence
is likely to be uttered by a native speaker of the
language under consideration.)
In order to achieve both entailment and general
substitutability, we propose to filter the output of
a conventional distributional similarity system us-
ing a check for lexical substitutability in frequent
contexts. The idea of the filter relies on the ob-
servation that entailing words tend to share more
frequent immediate contexts than just related ones.
For instance, when looking at the top 200 most fre-
quent Google 3-gram contexts (Brants and Franz,
2006) appearing after the terms ?kill?, ?murder?
and ?abduct?, we find that ?kill? and ?murder? share
54 while ?kill? and ?abduct? only share 2, giving
us the indication that as far as usage is concerned,
?murder? is closer to ?kill? than ?abduct?. Addi-
tionally, context frequency provides a way to iden-
tify substitutability for the most common uses of
the word, as required.
In what follows, we briefly present related
work, and introduce our corpus and algorithm, in-
cluding a discussion of our ?immediate context
overlap? filter. We then review the results of an
experiment on the extraction of entailment pairs
1In fact, we argue that even in systems where context is
available, searching for all entailing words is not necessary an
advantage: consider the query ?What does Dole produce?? to
a search engine. The verb ?fabricate? entails ?produce? in the
correct sense of the word, but because of its own polysemy,
and unless an expensive layer of WSD is added to the system,
it will return sentences such as ?Dole fabricated stories about
her opponent?, which is clearly not the information that the
user was looking for.
for 30 input verbs.
2 Previous Work
2.1 Distributional Similarity
2.1.1 Principles
Systems using distributional similarity usually fall
under two approaches:
1. The pattern-based approach (e.g. Ravichad-
ran and Hovy, 2002). The most significant
contexts for an input seed are extracted as
features and those features used to discover
words related to the input (under the assump-
tion that words appearing in at least one sig-
nificant context are similar to the seed word).
There is also a non-distributional strand of
this approach: it uses Hearst-like patterns
(Hearst, 1992) which are supposed to indi-
cate the presence of two terms in a certain re-
lation - most often hyponymy or meronymy
(see Chklovski and Pantel, 2004).
2. The feature vector approach (e.g. Lin and
Pantel, 2001). This method fully embraces
the definition of distributional similarity by
making the assumption that two words ap-
pearing in similar sets of features must be re-
lated.
2.1.2 Limitations
The problems of the distributional similarity as-
sumption are well-known: the facts that ?a bank
lends money? and ?Smith?s brother lent him
money? do not imply that banks and brothers are
similar entities. This effect becomes particularly
evident in cases where antonyms are returned by
the system; in those cases, a very high distribu-
tional similarity actually corresponds to opposite
meanings. Producing an output ranked accord-
ing to distributional similarity scores (weeding out
anything under a certain threshold) is therefore
not sufficient to retain good precisions for many
tasks. Some work has thus focused on a re-ranking
strategies (see Geffet and Dagan, 2004 and Gef-
fet and Dagan, 2005, who improve the output of a
distributional similarity system for an entailment
task using a web-based feature inclusion check,
and comment that their filtering produces better
outputs than cutting off the similarity pairs with
the lowest ranking.)
29
2.2 Extraction Systems
Prominent entailment rule acquisition systems in-
clude DIRT (Lin and Pantel, 2001), which uses
distributional similarity on a 1 GB corpus to iden-
tify semantically similar words and expressions,
and TEASE (Szpektor et al, 2004), which ex-
tracts entailment relations from the web for a given
word by computing characteristic contexts for that
word.
Recently, systems that combine both pattern-
based and feature vector approaches have also
been presented. Lin et al (2003) and Pantel and
Ravichandran (2004) have proposed to classify the
output of systems based on feature vectors using
lexico-syntactic patterns, respectively in order to
remove antonyms from a related words list and to
name clusters of related terms.
Even more related to our work, Mirkin et al
(2006) integrate both approaches by constructing
features for the output of both a pattern-based and
a vector-based systems, and by filtering incorrect
entries with a supervised SVM classifier. (The
pattern-based approach uses a set of manually-
constructed patterns applied to a web search.)
In the same vein, Geffet and Dagan (2005) fil-
ter the result of a pattern-based system using fea-
ture vectors. They get their features out of an 18
million word corpus augmented by a web search.
Their idea is that for any pair of potentially simi-
lar words, the features of the entailed one should
comprise all the features of the entailing one.
The main difference between our work and the
last two quoted papers is that we add a new layer
of verification: we extract pairs of verbs using au-
tomatically derived semantic patterns, perform a
first stage of filtering using the semantic signa-
tures of each word and apply a final stage of filter-
ing relying on surface substitutability, which we
name ?immediate context overlap? method. We
also experiment with a smaller size corpus to pro-
duce our distributional similarity baseline (a sub-
set of Wikipedia) in an attempt to show that a good
semantic parse and adequate filtering can provide
reasonable performance even on domains where
data is sparse. Our method does not need man-
ually constructed patterns or supervised classifier
training.
2.3 Evaluation
The evaluation of KB or ontology extraction sys-
tems is typically done by presenting human judges
with a subset of extracted data and asking them to
annotate it according to certain correctness crite-
ria. For entailment systems, the annotation usu-
ally relies on two tests: whether the meaning of
one word entails the other one in some senses of
those words, and whether the judges can come up
with contexts in which the words are directly sub-
stitutable. Szpektor et al (2007) point out the dif-
ficulties in applying those criteria. They note the
low inter-annotator agreements obtained in previ-
ous studies and propose a new evaluation method
based on precise judgement questions applied to
a set of relevant contexts. Using their methods,
they evaluate the DIRT (Lin and Pantel, 2001) and
TEASE (Szpektor et al, 2004) algorithms and ob-
tain upper bound precisions of 44% and 38% re-
spectively on 646 entailment rules for 30 transitive
verbs. We follow here their methodology to check
the results obtained via the traditional annotation.
3 The Data
The corpus used for our distributional similar-
ity baseline consists of a subset of Wikipedia to-
talling 500 MB in size, parsed first with RASP2
(Briscoe et al, 2006) and then into a Robust Min-
imal Recursion Semantics form (RMRS, Copes-
take, 2004) using a RASP-to-RMRS converter.
The RMRS representation consists of trees (or tree
fragments when a complete parse is not possible)
which comprise, for each phrase in the sentence, a
semantic head and its arguments. For instance, in
the sentence ?Lloyd?s rescues failing bank?, three
subtrees can be extracted:
lemma:rescue arg:ARG1 var:Lloyd?s
which indicates that ?Lloyd?s? is subject of the
head ?rescue?,
lemma:rescue arg:ARG2 var:bank
which indicates that ?bank? is object of the head
?rescue?, and
lemma:failing arg:ARG1 var:bank
which indicates that the argument of ?failing? is
?bank?.
Note that any tree can be transformed into
a feature for a particular lexical item by re-
placing the slot containing the word with a
hole: lemma:rescue arg:ARG2 var:bank be-
comes lemma:hole arg:ARG2 var:bank, a po-
tentially characteristic context for ?rescue?.
All the experiments reported in this paper con-
cern transitive verbs. In order to speed up
processing, we reduced the RMRS corpus to a
30
list of relations with a verbal head and at least
two arguments: lemma:verb-query arg:ARG1
var:subject arg:ARG2 var:object. Note that
we did not force noun phrases in the second ar-
gument of the relations and for instance, the verb
?say? was both considered as taking a noun or a
clause as second argument (?to say a word?, ?to
say that the word is...?).
4 A Baseline
We describe here our baseline, a system based on
distributional similarity.
4.1 Step 1 - Pattern-Based Pair Extraction
The first step of our algorithm uses a pattern-based
approach to get a list of potential entailing pairs.
For each word w presented to the system, we ex-
tract all semantic patterns containing w. Those se-
mantic patterns are RMRS subtrees consisting of a
semantic head and its children (see Section 3). We
then calculate the Pointwise Mutual Information
between each pattern p and w:
pmi(p, w) = log
(
P (p, w)
P (p)P (w)
)
(1)
where P (p) and P (w) are the probabilities of oc-
currence of the pattern and the instance respec-
tively and P (p, w) is the probability that they ap-
pear together.
PMI is known to have a bias towards less fre-
quent events. In order to counterbalance that bias,
we apply a simple logarithm function to the results
as a discount:
d = log (cwp + 1) (2)
where cwp is the cooccurrence count of an instance
and a pattern.
We multiply the original PMI value by this dis-
count to find the final PMI. We then select the n
patterns with highest PMIs and use them as rele-
vant semantic contexts to find all terms t that also
appear in those contexts. The result of this step
is a list of potential entailment relations, w ? t1
... w ? tx (we do not know the direction of the
entailment).
4.2 Step 2 - Feature vector Comparison
This step takes the output of the pattern-based ex-
traction and applies a first filter to the potential en-
tailment pairs. The filter relies on the idea that
two words that are similar will have similar fea-
ture vectors (see Geffet and Dagan, 2005). We de-
fine here the feature vector of word w as the list of
semantic features containing w, together with the
PMI of each feature in relation to w as a weight.
For each pair of words (w1, w2) we extract the
feature vectors of both w1 and w2 and calculate
their similarity using the measure of Lin (1998).
Pairs with a similarity under a certain threshold are
weeded out. (We use 0.007 in our experiments ?
the value was found by comparing precisions for
various thresholds in a set of initial experiments.)
As a check of how the Lin measure performed
on our Wikipedia subset using RMRS features,
we reproduced the Miller and Charles experi-
ment (1991) which consists in asking humans to
rate the similarity of 30 noun pairs. The experi-
ment is a standard test for semantic similarity sys-
tems (see Jarmasz and Szpakowicz, 2003; Lin,
1998; Resnik, 1995 and Hirst and St Onge, 1998
amongst others). The correlations obtained by pre-
vious systems range between the high 0.6 and the
high 0.8. Those systems rely on edge counting us-
ing manually-created resources such as WordNet
and the Roget?s Thesaurus. We are not actually
aware of results obtained on totally automated sys-
tems (apart from a baseline computed by Strube
and Ponzetto, 2006, using Google hits, which re-
turn a correlation of 0.26.)
Applying our feature vector step to the Miller
and Charles pairs, we get a correlation of 0.38,
way below the edge-counting systems. It turns out,
however, that this low result is at least partially due
to data sparsity: when ignoring the pairs contain-
ing at least one word with frequency under 200
(8 of them, which means ending up with 22 pairs
left out of the initial 30), the correlation goes up
to 0.69. This is in line with the edge-counting sys-
tems and shows that our baseline system produces
a decent approximation of human performance, as
long as enough data is supplied. 2
Two issues remain, though. First, fine-grained
results cannot be obtained over a general corpus:
we note that the pairs ?coast-forest? and ?coast-
hill? get very similar scores using distributional
similarity while the latter is ranked twice as high
as the former by humans. Secondly, distribu-
2It seems then that in order to maintain precision to a
higher level on our corpus, we could simply disregard pairs
with low-frequency words. (We decided here, however, that
this would be unacceptable from the point of view of recall
and did not attempt to do so.)
31
tional methods promise to identify ?semantically
similar? words, as do the Miller and Charles ex-
periment and edge-counting systems. However,
as pointed out in the introduction, there is still
a gap between general similarity and entailment:
?coast? and ?hill? are indeed similar in some way
but never substitutable. Our baseline is therefore
constrained by a theoretical problem that further
modules must solve.
5 Immediate Context Overlap
Our immediate context overlap module acts as a
filter for the system described as our baseline. The
idea is that, out of all pairs of ?similar? words,
we want to find those that express entailment in
at least one direction. So for instance, given the
pairs ?kill ? murder? and ?kill ? abduct?, we would
like to keep the former and filter the latter out. We
can roughly explain why the second pair is not ac-
ceptable by saying that, although the semantics of
the two words are close (they are both about an act
of violence conducted against somebody), they are
not substitutable in a given sentence.
To satisfy substitutability, we generally specify
that if w1 entails w2, then there should be surface
contexts where w2 can replace w1, with the substi-
tution still producing an acceptable utterance (see
our definition of acceptability in the introduction).
We further suggest that if one word can substitute
the other in frequent immediate contexts, we have
the basis to believe that entailment is possible in
at least one common sense of the words ? while
if substitution is impossible or rare, we can doubt
the presence of an entailment relation, at least in
common senses of the terms. This can be made
clearer with an example. We show in Table 1 some
of the most frequent trigrams to appear after the
verbs ?to kill?, ?to murder? and ?to abduct? (those
trigrams were collected from the Google 5-gram
corpus.) It is immediately noticeable that some
contexts are not transferable from one term to the
other: phrases such as ?to murder and forcibly
recruit someone?, or ?to abduct cancer cells? are
impossible ? or at least unconventional. We also
show in italic some common immediate contexts
between the three words. As pointed out in the in-
troduction, when looking at the top 200 most fre-
quent contexts for each term, we find that ?kill?
and ?murder? share 54 while ?kill? and ?abduct?
only share 2, giving us the indication that as far as
usage is concerned, ?murder? is closer to ?kill? than
?abduct?. Furthermore, by looking at frequency of
occurrence, we partly answer our need to find sub-
stitutions that work in very frequent sentences of
the language.
The Google 5-gram corpus gives the frequency
of each of its n-grams, allowing us to check substi-
tutability on the 5-grams with highest occurrence
counts for each potential entailment pair returned
by our baseline. For each pair (w1, w2) we select
the m most frequent contexts for both w1 and w2
and simply count the overlap between both lists. If
there is any overlap, we keep the pair; if the over-
lap is 0, we weed it out (the low threshold helps
our recall to remain acceptable). We experiment
with left and right contexts, i.e. with the query
term at the beginning and the end of the n-gram,
and with various combinations (see Section 6).
6 Results
The results in this section are produced by ran-
domly selecting 30 transitive verbs out of the 500
most frequent in our Wikipedia corpus and using
our system to extract non-directional entailment
pairs for those verbs, following a similar experi-
ment by Szpektor et al (2007). We use a list of
n = 30 features in Step 1 of the baseline. We eval-
uate the results by first annotating them according
to a broad definition of entailment: if the annota-
tor can think of any context where one word of
the pair could replace the other, preserving sur-
face form and semantics, then the two words are
in an entailment relation. (Note again that we do
not consider the directionality of entailment at this
stage.) We then re-evaluate our best score using
the Szpektor et al method (2007), which we think
is more suited for checking true substitutability. 3
The baseline described in Section 4 produces
301 unique pairs, 124 of which we judge correct
using our broad entailment definition, yielding a
precision of 41%. The average number of rela-
tions extracted for each input term is thus 4.1.
Tables 2 and 3 show our results at the end of
the immediate context overlap step. Table 2 re-
port results using the m = 50 most frequent con-
texts for each word in the pair while Table 3 uses
an expanded list of 200 contexts. Precision is the
3Although no direct comparison with the works
of Szpektor et al or Lin and Pantel is provided
in this paper, we are in the process of evaluating
our results against the TEASE output (available at
http://www.cs.biu.ac.il/?szpekti/TEASE co
llection.zip) through a web-based annotation task.
32
Table 1: Immediate Contexts for ?kill?, ?murder? and ?abduct?
kill murder abduct
two birds with babies that life her and make
cancer cells and his wife and an innocent man
a mocking bird thousands of innocent unsuspecting people and
or die for women and children suspects in foreign
or be killed her husband and a young girl
another human being in the name and forcibly recruit
thousands of people in connection with a teenage girl
in the name another human being and kill her
his wife and tens of thousands a child from
members of the the royal family women and children
number of correct relations amongst all those re-
turned. Recall is calculated with regard to the 124
pairs judged correct at the end of the previous step
(i.e., this is not true recall but recall relative to the
baseline results.)
We experimented with six different set-ups:
1- right context: the four words following the
query term are used as context
2- left context: the four words preceding the
query term are used as context
3- right and left contexts: the best contexts
(those with highest frequencies) are selected
out of the concatenation of both right and left
context lists
4- concatenation: the concatenation of the re-
sults obtained from 1 and 2
5- inclusion: the inclusion set of the results from
1 and 2, that is, the pairs judged correct by
both the right context and left context meth-
ods.
6- right context with ?to?: identical to 1 but the
5-gram is required to start with ?to?. This
ensures that only the verb form of the query
term is considered but has the disadvantage
of effectively transforming 5-grams into 4-
grams.
Our best overall results comes from using 50
immediate contexts starting with ?to?, right con-
text only: we obtain 56% precision on a recall of
85% calculated on the results of the previous step.
Table 2: Results using 50 immediate contexts
Context Used Precision Recall F Returned Correct
Left 48% 63% 54% 164 78
Right 62% 26% 36% 52 32
Left and Right 53% 52% 52% 122 65
Concatenation 48% 70% 57% 181 87
Inclusion 67% 19% 30% 36 24
Right + ?to? 56% 85% 68% 187 105
Table 3: Results using 200 immediate contexts
Context Used Precision Recall F Returned Correct
Left 44% 86% 58% 244 107
Right 54% 60% 57% 137 74
Left and Right 46% 85% 60% 228 105
Concatenation 44% 92% 60% 260 114
Inclusion 55% 53% 54% 121 66
Right + ?to? 48% 97% 64% 248 120
6.1 Instance-Based Evaluation
We then recalculate our best precision following
the method introduced in Szpektor et al (2007).
This approach consists in extracting, for each po-
tential entailment relation X-verb1-Y?X-verb2-
Y, 15 sentences in which verb1 appears and ask
annotators to provide answers to three questions:
1. Is the left-hand side of the relation entailed
by the sentence? If so...
2. When replacing verb1 with verb2, is the sen-
tence still likely in English? If so...
33
3. Does the sentence with verb1 entail the sen-
tence with verb2?
We show in Table 4 some potential annotations
at various stages of the process.
For each pair, Szpektor et al then calculate a
lower-bound precision as
Plb =
nEntailed
nLeftHandEntailed
(3)
where nEntailed is the number of entailed sentence
pairs (the annotator has answered ?yes? to the third
question) and nLeftHandEntailed is the number of
sentences where the left-hand relation is entailed
(the annotator has answered ?yes? to the first ques-
tion). They also calculate an upper-bound preci-
sion as
Pub =
nEntailed
nAcceptable
(4)
where nAcceptable is the number of acceptable
verb2 sentences (the annotator has answered ?yes?
to the second question). A pair is deemed to con-
tain an entailment relation if the precision for that
particular pair is over 80%.
The authors comment that a large proportion of
extracted sentences lead to a ?left-hand side not en-
tailed? answer. In order to counteract that effect,
we only extract sentences without modals or nega-
tion from our Wikipedia corpus and consequently
only require 10 sentences per relation (only 11%
of our sentences have a ?non-entailed? left-hand
side relation against 43% for Szpektor et al).
We obtain an upper bound precision of 52%,
which is slightly lower than the one initially cal-
culated using our broad definition of entailment,
showing that the more stringent evaluation is use-
ful when checking for general substitutability in
the returned pairs. When we calculate the lower
bound precision, however, we obtain a low 10%
precision due to the large number of sentences
judged as ?unlikely English sentences? after sub-
stitution (they amount to 33% of all examples with
a left-hand side judged ?entailed?). This result il-
lustrates the need for a module able to check sen-
tence acceptability when applying the system to
true substitution tasks. Fortunately, as we explain
in the next section, it also takes into account re-
quirements that are only necessary for generation
tasks, and are therefore irrelevant to our querying
task.
7 Discussion
Our main result is that the immediate context over-
lap step dramatically increases our precision (from
41% to 56%), showing that a more stringent notion
of similarity can be achieved when adequately fil-
tering the output of a distributional similarity sys-
tem. However, it also turns out that looking at
the most frequent contexts of the word to substi-
tute does not fully solve the issue of surface ac-
ceptability (leading to a high number of ?right-
hand side not entailed? annotations). We argue,
though, that the issue of producing an acceptable
English sentence is a generation problem separate
from the extraction task. Some systems, in fact,
are dedicated to related problems, such as identi-
fying whether the senses of two synonyms are the
same in a particular lexical context (see Dagan et
al., 2006). As far as our needs are concerned in
the task of KB querying, we only require accurate
searching capabilities as opposed to generational
capabilities: the expansion of search terms to in-
clude impossible strings is not a problem in terms
of result.
Looking at the immediate context overlaps re-
turned for each pair by the system, we find that the
overlap (the similarity) can be situated at various
linguistic layers:
? in the semantics of the verb?s object: ?a
new album? is something that one would fre-
quently ?record? or ?release?. The phrase
boosts the similarity score between ?record?
and ?release? in their music sense.
? in the clausal information of the right context:
a context starting with a clause introduced by
?that? is likely to be preceded by a verb ex-
pressing cognition or discourse. The tri-gram
?that there is? increases the similarity of pairs
such as ?say - argue?.
? in the prepositional information of the right
context: ?about? is the preposition of choice
after cognition verbs such as ?think? or ?won-
der?. The context ?about the future? helps the
score of the pair ?think - speculate? in the cog-
nitive sense (note that ?speculate? in a finan-
cial sense would take the preposition ?on?.)
Some examples of overlaps are shown in Ta-
ble 5.
We also note that the system returns a fair pro-
portion of vacuous contexts such as ?one of the? or
34
Table 4: Annotation Examples Following the Szpektor et al Method
Word Pair Sentence Question 1 Question 2 Question 3
acquire ? buy Lloyds acquires HBOS yes yes (Lloyds buys HBOS) yes
acquire ? praise Lloyds acquires HBOS yes yes (Lloyds praises HBOS) no
acquire ? spend Lloyds acquires HBOS yes no (*Lloyds spends HBOS) ?
acquire ? buy Lloyds may acquire HBOS no ? ?
Table 5: Sample of Immediate Context Overlaps
think ? speculate say ? claim describe ? characterise
about the future that it is the nature of
about what the that there is the effects of
about how the that it was it as a
that they were the effect of
that they have the role of
that it has the quality of
the impact of
the dynamics of
?part of the? which contribute to the score of many
pairs. Our precision would probably benefit from
excluding such contexts.
We note that as expected, using a larger set of
contexts leads to better recall and decreased pre-
cision. The best precision is obtained by return-
ing the inclusion set of both left and right contexts
results, but at a high cost in recall. Interestingly,
we find that the right context of the verb is far
more telling than the left one (potentially, objects
are more important than subjects). This is in line
with results reported by Alfonseca and Manandhar
(2002).
Our best results yield an average of 3.4 relations
for each input term. It is in the range reported
by the authors of the TEASE system (Szpektor et
al., 2004) but well below the extrapolated figures
of over 20 relations in Szpektor et al, 2007. We
point out, however, that we only search for sin-
gle word substitutions, as opposed to single and
multi-word substitutions for Szpektor et al. Fur-
thermore, our experiments are performed on 500
MB of text only, against 1 GB of news data for
the DIRT system and the web for the TEASE al-
gorithm. More data may help our recall, as well as
bootstrapping over our best precision system.
We show a sample of our results in Table 6. The
pairs with an asterisk were considered incorrect at
human evaluation stage.
Table 6: Sample of Extracted Pairs
bring ? attract make - earn
*call ? form *name - delegate
change ? alter offer - provide
create ? generate *perform - discharge
describe ? characterise produce ? release
develop ? generate record ? count
*do ? behave *release ? announce
feature ? boast *remain ? comprise
*find ? indicate require ? demand
follow ? adopt say ? claim
*grow ? contract tell ? assure
*increase - decline think ? believe
leave - abandon *use ? abandon
8 Conclusion
We have presented here a system for the extrac-
tion of word substitutions in the context of KB
querying. We have shown that the output of a
distributional similarity baseline can be improved
by filtering it using the idea that two words in an
entailment relation are substitutable in immediate
surface contexts. We obtained a precision of 56%
(52% using our most stringent evaluation) on a test
set of 30 transitive verbs, and a yield of 3.4 rela-
tions per verb.
We also point out that relatively good precisions
can be obtained on a parsed medium-sized corpus
of 500 MB, although recall is certainly affected.
We note that our current implementation does
not always satisfy the requirement for substi-
tutability for generation tasks and point out that
the system is therefore limited to our intended use,
which involves search capabilities only.
We would like to concentrate in the future on
providing a direction for the entailment pairs ex-
tracted by the system. We also hope that recall
could possibly improve using a larger set of fea-
tures in the pattern-based step (this is suggested
also by Szpektor et al, 2004), together with ap-
35
propriate bootstrapping.
Acknowledgements
This work was supported by the UK Engineer-
ing and Physical Sciences Research Council (EP-
SRC: EP/P502365/1). I would also like to thank
my supervisor, Dr Ann Copestake, for her support
throughout this project, as well as the anonymous
reviewers who commented on this paper.
References
Enrique Alfonseca and Suresh Manandhar. 2002. Ex-
tending a Lexical Ontology by a Combination of
Distributional Semantics Signatures. In Proceed-
ings of EKAW 2002, pp. 1?7, 2002.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadel-
phia, 2006.
Edward Briscoe, John Carroll and Rebecca Watson.
2006. The Second Release of the RASP System. In
Proceedings of the COLING/ACL 2006 Interactive
Presentation Sessions, Sydney, Australia, 2006.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining The Web for Fine-Grained Se-
mantic Verb Relations. Proceedings of EMNLP-04,
Barcelona, Spain, 2004.
Ann Copestake. 2004. Ro-
bust Minimal Recursion Semantics.
www.cl.cam.ac.uk/?aac10/papers/rmrs
draft.pdf.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic Textual Entailment: Generic Applied Modelling
of Language Variability. Proceedings of The PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France, 2004.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein and Carlo Strapparava. 2006. Direct
Word Sense Matching for Lexical Substitution. Pro-
ceedings of COLING-ACL 2006, 17-21 Jul 2006,
Sydney, Australia.
Maayan Geffet and Ido Dagan. 2004. Feature Vector
Quality and Distributional Similarity. Proceedings
Of the 20th International Conference on Computa-
tional Linguistics, 2004.
Maayan Geffet and Ido Dagan. 2005. The Distri-
butional Inclusion Hypothesises and Lexical Entail-
ment. In Proceedings Of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pp. 107?114, 2005.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and Semantic Similarity. In Proceedings
of International Conference RANLP?03, pp. 212?
219, 2003.
Zelig Harris. Distributional Structure. In Word, 10,
No. 2?3, pp. 146?162, 1954.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. Proceedings of
COLING-92, pp.539?545, 1992.
Graeme Hirst and David St-Onge. 1998. Lexical
Chains As Representations of Context for the Detec-
tion and Correction of Malapropisms. In ?WordNet?,
Ed. Christiane Fellbaum, Cambridge, MA: The MIT
Press, 1998.
Dekang Lin. 2003. An Information-Theoretic Defini-
tion of Similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, pp. 296?
304, 1998.
Dekang Lin, Shaojun Zhao, Lijuan Qin and Ming
Zhou. 2003. Identifying Synonyms among Distribu-
tionally Similar Words. In Proceedings of IJCAI-03,
Acapulco, Mexico, 2003.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
ACM 2001, 2001.
George Miller and Walter Charles. 2001. Contextual
Correlates of Semantic Similarity. In Language and
Cognitive Processes, 6(1), pp. 1?28, 1991.
Shachar Mirkin, Ido Dagan and Maayan Geffet. 2004.
Integrating Pattern-Based and Distributional Simi-
larity Methods for Lexical Entailment Acquisition.
In Proceedings of COLING/ACL, Sydney, Aus-
tralia, pp.579?586, 2006.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labelling Semantic Classes. In Proceed-
ings of HLT/NAACL04, Boston, MA, pp 321328,
2004.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. Proceedings of ACL, 2002.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
Proceedings of IJCAI?95, 1995.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-Based Acquisition
of Entailment Relations. In Proceedings of EMNLP?
2004, pp. 41?48, 2004.
Idan Szpektor, Eyal Shnarch and Ido Dagan. 2007.
Instance-Based Evaluation of Entailment Rule Ac-
quisition. In Proceedings of ACL?07, 2007.
Michael Strube and Simone Ponzetto. 2006. WikiRe-
late! Computing Semantic Relatedness Using
Wikipedia. In Proceedings of AAAI?06, pp. 1219?
1224, 2006.
36
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 440?445,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Measuring semantic content in distributional vectors
Aure?lie Herbelot
EB Kognitionswissenschaft
Universita?t Potsdam
Golm, Germany
aurelie.herbelot@cantab.net
Mohan Ganesalingam
Trinity College
University of Cambridge
Cambridge, UK
mohan0@gmail.com
Abstract
Some words are more contentful than oth-
ers: for instance, make is intuitively more
general than produce and fifteen is more
?precise? than a group. In this paper,
we propose to measure the ?semantic con-
tent? of lexical items, as modelled by
distributional representations. We inves-
tigate the hypothesis that semantic con-
tent can be computed using the Kullback-
Leibler (KL) divergence, an information-
theoretic measure of the relative entropy
of two distributions. In a task focus-
ing on retrieving the correct ordering of
hyponym-hypernym pairs, the KL diver-
gence achieves close to 80% precision but
does not outperform a simpler (linguis-
tically unmotivated) frequency measure.
We suggest that this result illustrates the
rather ?intensional? aspect of distributions.
1 Introduction
Distributional semantics is a representation of lex-
ical meaning that relies on a statistical analysis
of the way words are used in corpora (Curran,
2003; Turney and Pantel, 2010; Erk, 2012). In
this framework, the semantics of a lexical item is
accounted for by modelling its co-occurrence with
other words (or any larger lexical context). The
representation of a target word is thus a vector in a
space where each dimension corresponds to a pos-
sible context. The weights of the vector compo-
nents can take various forms, ranging from sim-
ple co-occurrence frequencies to functions such as
Pointwise Mutual Information (for an overview,
see (Evert, 2004)).
This paper investigates the issue of comput-
ing the semantic content of distributional vectors.
That is, we look at the ways we can distribution-
ally express that make is a more general verb than
produce, which is itself more general than, for
instance, weave. Although the task is related to
the identification of hyponymy relations, it aims
to reflect a more encompassing phenomenon: we
wish to be able to compare the semantic content of
words within parts-of-speech where the standard
notion of hyponymy does not apply (e.g. preposi-
tions: see with vs. next to or of vs. concerning)
and across parts-of-speech (e.g. fifteen vs. group).
The hypothesis we will put forward is that se-
mantic content is related to notions of relative en-
tropy found in information theory. More specif-
ically, we hypothesise that the more specific a
word is, the more the distribution of the words
co-occurring with it will differ from the baseline
distribution of those words in the language as a
whole. (A more intuitive way to phrase this is that
the more specific a word is, the more information
it gives us about which other words are likely to
occur near it.) The specific measure of difference
that we will use is the Kullback-Leibler divergence
of the distribution of words co-ocurring with the
target word against the distribution of those words
in the language as a whole. We evaluate our hy-
pothesis against a subset of the WordNet hierar-
chy (given by (Baroni et al 2012)), relying on the
intuition that in a hyponym-hypernym pair, the hy-
ponym should have higher semantic content than
its hypernym.
The paper is structured as follows. We first
define our notion of semantic content and moti-
vate the need for measuring semantic content in
distributional setups. We then describe the im-
plementation of the distributional system we use
in this paper, emphasising our choice of weight-
ing measure. We show that, using the compo-
440
nents of the described weighting measure, which
are both probability distributions, we can calculate
the relative entropy of a distribution by inserting
those probability distributions in the equation for
the Kullback-Leibler (KL) divergence. We finally
evaluate the KL measure against a basic notion of
frequency and conclude with some error analysis.
2 Semantic content
As a first approximation, we will define seman-
tic content as informativeness with respect to de-
notation. Following Searle (1969), we will take
a ?successful reference? to be a speech act where
the choice of words used by the speaker appropri-
ately identifies a referent for the hearer. Glossing
over questions of pragmatics, we will assume that
a more informative word is more likely to lead to
a successful reference than a less informative one.
That is, if Kim owns a cat and a dog, the identify-
ing expression my cat is a better referent than my
pet and so cat can be said to have more semantic
content than pet.
While our definition relies on reference, it also
posits a correspondence between actual utterances
and denotation. Given two possible identifying ex-
pressions e1 and e2, e1 may be preferred in a par-
ticular context, and so, context will be an indicator
of the amount of semantic content in an expres-
sion. In Section 5, we will produce an explicit
hypothesis for how the amount of semantic con-
tent in a lexical item affects the contexts in which
it appears.
A case where semantic content has a direct cor-
respondence with a lexical relation is hyponymy.
Here, the correspondence relies entirely on a basic
notion of extension. For instance, it is clear that
hammer is more contentful than tool because the
extension of hammer is smaller than that of tool,
and therefore more discriminating in a given iden-
tifying expression (See Give me the hammer ver-
sus Give me the tool). But we can also talk about
semantic content in cases where the notion of ex-
tension does not necessarily apply. For example,
it is not usual to talk of the extension of a prepo-
sition. However, in context, the use of a preposi-
tion against another one might be more discrim-
inating in terms of reference. Compare a) Sandy
is with Kim and b) Sandy is next to Kim. Given a
set of possible situations involving, say, Kim and
Sandy at a party, we could show that b) is more
discriminating than a), because it excludes the sit-
uations where Sandy came to the party with Kim
but is currently talking to Kay at the other end of
the room. The fact that next to expresses physi-
cal proximity, as opposed to just being in the same
situation, confers it more semantic content accord-
ing to our definition. Further still, there may be a
need for comparing the informativeness of words
across parts of speech (compare A group of/Fifteen
people was/were waiting in front of the town hall).
Although we will not discuss this in detail, there
is a notion of semantic content above the word
level which should naturally derive from compo-
sition rules. For instance, we would expect the
composition of a given intersective adjective and
a given noun to result into a phrase with a seman-
tic content greater than that of its components (or
at least equal to it).
3 Motivation
The last few years have seen a growing interest in
distributional semantics as a representation of lex-
ical meaning. Owing to their mathematical inter-
pretation, distributions allow linguists to simulate
human similarity judgements (Lund, Burgess and
Atchley, 1995), and also reproduce some of the
features given by test subjects when asked to write
down the characteristics of a given concept (Ba-
roni and Lenci, 2008). In a distributional semantic
space, for instance, the word ?cat? may be close to
?dog? or to ?tiger?, and its vector might have high
values along the dimensions ?meow?, ?mouse? and
?pet?. Distributional semantics has had great suc-
cesses in recent years, and for many computational
linguists, it is an essential tool for modelling phe-
nomena affected by lexical meaning.
If distributional semantics is to be seen as
a general-purpose representation, however, we
should evaluate it across all properties which we
deem relevant to a model of the lexicon. We con-
sider semantic content to be one such property. It
underlies the notion of hyponymy and naturally
models our intuitions about the ?precision? (as op-
posed to ?vagueness?) of words.
Further, semantic content may be crucial in
solving some fundamental problems of distribu-
tional semantics. As pointed out by McNally
(2013), there is no easy way to define the notion
of a function word and this has consequences for
theories where function words are not assigned
a distributional representation. McNally suggests
that the most appropriate way to separate function
441
from content words might, in the end, involve tak-
ing into account how much ?descriptive? content
they have.
4 An implementation of a distributional
system
The distributional system we implemented for this
paper is close to the system of Mitchell and La-
pata (2010) (subsequently M&L). As background
data, we use the British National Corpus (BNC) in
lemmatised format. Each lemma is followed by a
part of speech according to the CLAWS tagset for-
mat (Leech, Garside, and Bryant, 1994). For our
experiments, we only keep the first letter of each
part-of-speech tag, thus obtaining broad categories
such as N or V. Furthermore, we only retain words
in the following categories: nouns, verbs, adjec-
tives and adverbs (punctuation is ignored). Each
article in the corpus is converted into a 11-word
window format, that is, we are assuming that con-
text in our system is defined by the five words pre-
ceding and the five words following the target.
To calculate co-occurrences, we use the follow-
ing equations:
freqci =
?
t
freqci,t (1)
freqt =
?
ci
freqci,t (2)
freqtotal =
?
ci,t
freqci,t (3)
The quantities in these equations represent the
following:
freqci,t frequency of the context word ciwith the target word t
freqtotal total count of word tokens
freqt frequency of the target word t
freqci frequency of the context word ci
As in M&L, we use the 2000 most frequent
words in our corpus as the semantic space dimen-
sions. M&L calculate the weight of each context
term in the distribution as follows:
vi(t) =
p(ci|t)
p(ci)
= freqci,t ? freqtotalfreqt ? freqci
(4)
We will not directly use the measure vi(t) as it
is not a probability distribution and so is not suit-
able for entropic analysis; instead our analysis will
be phrased in terms of the probability distributions
p(ci|t) and p(ci) (the numerator and denominator
in vi(t)).
5 Semantic content as entropy: two
measures
Resnik (1995) uses the notion of information con-
tent to improve on the standard edge counting
methods proposed to measure similarity in tax-
onomies such as WordNet. He proposes that the
information content of a term t is given by the self-
information measure ? log p(t). The idea behind
this measure is that, as the frequency of the term
increases, its informativeness decreases. Although
a good first approximation, the measure cannot be
said to truly reflect our concept of semantic con-
tent. For instance, in the British National Corpus,
time and see are more frequent than thing or may
and man is more frequent than part. However, it
seems intuitively right to say that time, see and
man are more ?precise? concepts than thing, may
and part respectively. Or said otherwise, there is
no indication that more general concepts occur in
speech more than less general ones. We will there-
fore consider self-information as a baseline.
As we expect more specific words to be more
informative about which words co-occur with
them, it is natural to try to measure the specificity
of a word by using notions from information the-
ory to analyse the probability distribution p(ci|t)
associated with the word. The standard notion
of entropy is not appropriate for this purpose, be-
cause it does not take account of the fact that the
words serving as semantic space dimensions may
have different frequencies in language as a whole,
i.e. of the fact that p(ci) does not have a uniform
distribution. Instead we need to measure the de-
gree to which p(ci|t) differs from the context word
distribution p(ci). An appropriate measure for this
is the Kullback-Leibler (KL) divergence or rela-
tive entropy:
DKL(P?Q) =
?
i
ln(P (i)Q(i))P (i) (5)
By taking P (i) to be p(ci|t) and Q(i) to be p(ci)
(as given by Equation 4), we calculate the rela-
tive entropy of p(ci|t) and p(ci). The measure is
clearly informative: it reflects the way that t mod-
ifies the expectation of seeing ci in the corpus.
We hypothesise that when compared to the distri-
bution p(ci), more informative words will have a
442
more ?distorted? distribution p(ci|t) and that the
KL divergence will reflect this.1
6 Evaluation
In Section 2, we defined semantic content as a no-
tion encompassing various referential properties,
including a basic concept of extension in cases
where it is applicable. However, we do not know
of a dataset providing human judgements over the
general informativeness of lexical items. So in or-
der to evaluate our proposed measure, we inves-
tigate its ability to retrieve the right ordering of
hyponym pairs, which can be considered a subset
of the issue at hand.
Our assumption is that if X is a hypernym of
Y , then the information content in X will be lower
than in Y (because it has a more ?general? mean-
ing). So, given a pair of words {w1, w2} in a
known hyponymy relation, we should be able to
tell which of w1 or w2 is the hypernym by com-
puting the respective KL divergences.
We use the hypernym data provided by (Baroni
et al 2012) as testbed for our experiment.2 This
set of hyponym-hypernym pairs contains 1385 in-
stances retrieved from the WordNet hierarchy. Be-
fore running our system on the data, we make
slight modifications to it. First, as our distributions
are created over the British National Corpus, some
spellings must be converted to British English: for
instance, color is replaced by colour. Second, five
of the nouns included in the test set are not in the
BNC. Those nouns are brethren, intranet, iPod,
webcam and IX. We remove the pairs containing
those words from the data. Third, numbers such as
eleven or sixty are present in the Baroni et alset as
nouns, but not in the BNC. Pairs containing seven
such numbers are therefore also removed from the
data. Finally, we encounter tagging issues with
three words, which we match to their BNC equiv-
alents: acoustics and annals are matched to acous-
tic and annal, and trouser to trousers. These mod-
ifications result in a test set of 1279 remaining
pairs.
We then calculate both the self-information
measure and the KL divergence of all terms in-
1Note that KL divergence is not symmetric:
DKL(p(ci|t)?p(ci))) is not necessarily equal to
DKL(p(ci)?p(ci|t)). The latter is inferior as a few
very small values of p(ci|t) can have an inappropriately large
effect on it.
2The data is available at http://clic.cimec.
unitn.it/Files/PublicData/eacl2012-data.
zip.
cluded in our test set. In order to evaluate the sys-
tem, we record whether the calculated entropies
match the order of each hypernym-hyponym pair.
That is, we count a pair as correctly represented
by our system if w1 is a hypernym of w2 and
KL(w1) < KL(w2) (or, in the case of the
baseline, SI(w1) < SI(w2) where SI is self-
information).
Self-information obtains 80.8% precision on the
task, with the KL divergence lagging a little be-
hind with 79.4% precision (the difference is not
significant). In other terms, both measures per-
form comparably. We analyse potential reasons
for this disappointing result in the next section.
7 Error analysis
It is worth reminding ourselves of the assumption
we made with regard to semantic content. Our
hypothesis was that with a ?more general? target
word t, the p(ci|t) distribution would be fairly
similar to p(ci).
Manually checking some of the pairs which
were wrongly classified by the KL divergence re-
veals that our hypothesis might not hold. For ex-
ample, the pair beer ? beverage is classified in-
correctly. When looking at the beverage distri-
bution, it is clear that it does not conform to our
expectations: it shows high vi(t) weights along
the food, wine, coffee and tea dimensions, for in-
stance, i.e. there is a large difference between
p(cfood) and p(cfood|t), etc. Although beverage
is an umbrella word for many various types of
drinks, speakers of English use it in very partic-
ular contexts. So, distributionally, it is not a ?gen-
eral word?. Similar observations can be made for,
e.g. liquid (strongly associated with gas, presum-
ably via coordination), anniversary (linked to the
verb mark or the noun silver), or again projectile
(co-occurring with weapon, motion and speed).
The general point is that, as pointed out else-
where in the literature (Erk, 2013), distributions
are a good representation of (some aspects of) in-
tension, but they are less apt to model extension.3
So a term with a large extension like beverage
may have a more restricted (distributional) inten-
sion than a word with a smaller extension, such as
3We qualify ?intension? here, because in the sense of a
mapping from possible worlds to extensions, intension can-
not be said to be provided by distributions: the distribution of
beverage, it seems, does not allow us to successfully pick out
all beverages in the real world.
443
beer.4
Contributing to this issue, fixed phrases, named
entities and generally strong collocations skew our
distributions. So for instance, in the jewelry distri-
bution, the most highly weighted context is mental
(with vi(t) = 395.3) because of the music album
Mental Jewelry. While named entities could eas-
ily be eliminated from the system?s results by pre-
processing the corpus with a named entity recog-
niser, the issue is not so simple when it comes to
fixed phrases of a more compositional nature (e.g.
army ant): excluding them might be detrimental
for the representation (it is, after all, part of the
meaning of ant that it can be used metaphorically
to refer to people) and identifying such phrases is
a non-trivial problem in itself.
Some of the errors we observe may also be
related to word senses. For instance, the word
medium, to be found in the pair magazine ?
medium, can be synonymous with middle, clair-
voyant or again mode of communication. In the
sense of clairvoyant, it is clearly more specific
than in the sense intended in the test pair. As dis-
tributions do not distinguish between senses, this
will have an effect on our results.
8 Conclusion
In this paper, we attempted to define a mea-
sure of distributional semantic content in or-
der to model the fact that some words have a
more general meaning than others. We com-
pared the Kullback-Leibler divergence to a sim-
ple self-information measure. Our experiments,
which involved retrieving the correct ordering of
hyponym-hypernym pairs, had disappointing re-
sults: the KL divergence was unable to outperform
self-information, and both measures misclassified
around 20% of our testset.
Our error analysis showed that several factors
contributed to the misclassifications. First, distri-
butions are unable to model extensional properties
which, in many cases, account for the feeling that a
word is more general than another. Second, strong
collocation effects can influence the measurement
of information negatively: it is an open question
which phrases should be considered ?words-with-
spaces? when building distributions. Finally, dis-
4Although it is more difficult to talk of the extension of
e.g. adverbials (very) or some adjectives (skillful), the general
point is that text is biased towards a certain usage of words,
while the general meaning a competent speaker ascribes to
lexical items does not necessarily follow this bias.
tributional representations do not distinguish be-
tween word senses, which in many cases is a de-
sirable feature, but interferes with the task we sug-
gested in this work.
To conclude, we would like to stress that we do
not think another information-theoretic measure
would perform hugely better than the KL diver-
gence. The point is that the nature of distributional
vectors makes them sensitive to word usage and
that, despite the general assumption behind dis-
tributional semantics, word usage might not suf-
fice to model all aspects of lexical semantics. We
leave as an open problem the issue of whether a
modified form of our ?basic? distributional vectors
would encode the right information.
Acknowledgements
This work was funded by a postdoctoral fellow-
ship from the Alexander von Humboldt Founda-
tion to the first author, and a Title A Fellowship
from Trinity College, Cambridge, to the second
author.
References
Baroni, Marco, and Lenci, Alessandro. 2008. Con-
cepts and properties in word spaces. In Alessan-
dro Lenci (ed.), From context to meaning: Distribu-
tional models of the lexicon in linguistics and cog-
nitive science (Special issue of the Italian Journal of
Linguistics 20(1)), pages 55?88.
Baroni, Marco, Raffaella Bernardi, Ngoc-Quynh Do
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL2012), pages 23?32.
Baroni, Marco, Raffaella Bernardi, and Roberto Zam-
parelli. 2012. Frege in Space: a Program for Com-
positional Distributional Semantics. Under review.
Curran, James. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh,
Scotland, UK.
Erk, Katrin. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6:10:635?653.
Erk, Katrin. 2013. Towards a semantics for distribu-
tional representations. In Proceedings of the Tenth
International Conference on Computational Seman-
tics (IWCS2013).
Evert, Stefan. 2004. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. thesis,
University of Stuttgart.
444
Leech, Geoffrey, Roger Garside, and Michael Bryant.
1994. Claws4: The tagging of the british national
corpus. In Proceedings of the 15th International
Conference on Computational Linguistics (COLING
94), pages 622?628, Kyoto, Japan.
Lund, Kevin, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th annual conference of the Cognitive Science So-
ciety, Vol. 17, pages 660?665.
McNally, Louise. 2013. Formal and distributional se-
mantics: From romance to relationship. In Proceed-
ings of the ?Towards a Formal Distributional Seman-
tics? workshop, 10th International Conference on
Computational Semantics (IWCS2013), Potsdam,
Germany. Invited talk.
Mitchell, Jeff and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429, November.
Resnik, Philipp. 1995. Using information content
to evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Con-
ference on Artificial Intelligence (IJCAI-95), pages
448?453.
Searle, John R. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge University Press.
Turney, Peter D. and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
445
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 73?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotating Underquantification
Aurelie Herbelot
University of Cambridge
Cambridge, United Kingdom
ah433@cam.ac.uk
Ann Copestake
University of Cambridge
Cambridge, United Kingdom
aac10@cam.ac.uk
Abstract
Many noun phrases in text are ambigu-
ously quantified: syntax doesn?t explicitly
tell us whether they refer to a single en-
tity or to several, and what portion of the
set denoted by the Nbar actually takes part
in the event expressed by the verb. We
describe this ambiguity phenomenon in
terms of underspecification, or rather un-
derquantification. We attempt to validate
the underquantification hypothesis by pro-
ducing and testing an annotation scheme
for quantification resolution, the aim of
which is to associate a single quantifier
with each noun phrase in our corpus.
1 Quantification resolution
We are concerned with ambiguously quantified
noun phrases (NPs) and their interpretation, as il-
lustrated by the following examples:
1. Cats are mammals = All cats...
2. Cats have four legs = Most cats...
3. Cats were sleeping by the fire = Some cats...
4. The beans spilt out of the bag = Most/All of
the beans...
5. Water was dripping through the ceiling =
Some water...
We are interested in quantification resolution,
that is, the process of giving an ambiguously quan-
tified NP a formalisation which expresses a unique
set relation appropriate to the semantics of the ut-
terance. For instance, we wish to arrive at:
6. All cats are mammals.
|???| = |?|where ? is the set of all cats and
? the set of all mammals.
Resolving the quantification value of NPs is im-
portant for many NLP tasks. Let us imagine an in-
formation extraction system having retrieved the
triples ?cat ? is ? mammal? and ?cat ? chase ?
mouse? for inclusion in a factual database about
felines. The problem with those representation-
poor triples is that they do not contain the nec-
essary information about quantification to answer
such questions as ?Are all cats mammals?? or ?Do
all cats chase mice?? Or if they attempt to answer
those queries, they give the same answer to both.
Ideally, we would like to annotate such triples with
quantifiers which have a direct mapping to proba-
bility adverbs:
7. All cats are mammals AND Tom is a cat ?
Tom is definitely a mammal.
8. Some cats chase mice AND Tom is a cat ?
Tom possibly chases mice.
Adequate quantification is also necessary for in-
ference based on word-level entailment: an exis-
tentially quantified NP can be replaced by a suit-
able hypernym but this is not possible in non-
existential cases: (Some) cats are in my garden
entails (Some) animals are in my garden but (All)
cats are mammals doesn?t imply that (All) animals
are mammals.
In Herbelot (to appear), we provide a formal
semantics for ambiguously quantified NPs, which
relies on the idea that those NPs exhibit an under-
specified quantifier, i.e. that for each NP in a cor-
pus, a set relation can be agreed upon. Our formal-
isation includes a placeholder for the quantifier?s
set relation. In line with inference requirements,
we assume a three-fold partitioning of the quan-
tificational space, corresponding to the natural lan-
guage quantifiers some, most and all (in addition
to one, for the description of singular, unique enti-
ties). The corresponding set relations are:
9. some(?, ?) is true iff 0 < |? ? ?| < |?? ?|
10. most(?, ?) is true iff |???| ? |???| < |?|
11. all(?, ?) is true iff |? ? ?| = |?|
This paper is an attempt to show that our for-
malisation lends itself to evaluation by human an-
notation. The labels produced will also serve as
training and test sets for an automatic quantifica-
tion resolution system.
73
2 Under(specified) quantification
Before we present our annotation scheme, we will
spell out the essential idea behind what we call un-
derquantification.
The phenomenon of ambiguous quantification
overlaps with genericity (see Krifka et al 1995,
for an introduction to genericity). Generic NPs
are frequently expressed syntactically as bare plu-
rals, although they occur in definite and indefinite
singulars too, as well as bare singulars. There
are many views on the semantics of generics
(e.g. Carlson, 1995; Pelletier and Asher, 1997;
Heyer, 1990; Leslie, 2008) but one of them is that
they quantify (Cohen, 1996), although, puzzlingly
enough, not always with the same quantifier:
12. Frenchmen eat horsemeat = Some/Relatively-
many Frenchmen... (For the relatively many
reading, see Cohen, 2001.)
13. Cars have four wheels = Most cars...
14. Typhoons arise in this part of the Pacific =
Some typhoons... OR Most/All typhoons...
This behaviour has so far prevented linguists
from agreeing on a single formalisation for all
generics. The only accepted assumption is that an
operator GEN exists, which acts as a silent quan-
tifier over the restrictor (subject) and matrix (ver-
bal predicate) of the generic statement. The formal
properties of GEN are however subject to debate:
in particular, it is not clear which natural language
quantifier it would map onto (some view it as most,
but this approach requires some complex domain
restriction to deal with sentences such as 12).
In this paper, we take a different approach
which sidesteps some of the intractable prob-
lems associated with the literature on generics and
which also extends to definite plurals. Instead of
talking of ambiguous quantification, we will talk
of underspecified quantification, or underquan-
tification. By this, we mean that the bare plural,
rather than exhibiting a silent, GEN quantifier,
simply features a placeholder in the logical form
which must be filled with the appropriate quan-
tifier (e.g., uq(x, cat?(x), sleep?(x)), where uq is
the placeholder quantifier). This account caters
for the facts that so-called generics can so easily
be quantified via traditional quantifiers, thatGEN
is silent in all known languages, and it explains
also why it is the bare form which has the high-
est productivity, and can refer to a range of quan-
tified sets, from existentials to universals. Using
the underquantification hypothesis, we can para-
phrase any generic of the form ?X does Y? as ?there
is a set of things X, a certain number of which
do Y? (note the partitive construction). Such a
paraphrase allows us to also resolve ambiguously
quantified definite plurals, which have tradition-
ally been associated with universals, outside of the
genericity phenomenon (e.g. Lyons, 1999).
Because of space constraints, we will not give
our formalisation for underquantification in this
paper (see Herbelot,to appear, for details). It in-
volves a representation of the partitive construct
exemplified above and requires knowledge of the
distributive or collective status of the verbal pred-
icate. We also argue that if generics can always be
quantified, their semantics may involve more than
quantification. So we claim that in certain cases, a
double formalisation of the NP as a quantified en-
tity and a kind is desirable. We understand kinds
in the way proposed by Chierchia (1998), that is
as the plurality of all instances denoted by a given
word in the world under consideration. Under the
kind reading, we can interpret 12 as meaning Col-
lectively, the group of all Frenchmen has the prop-
erty of eating horsemeat.
3 Motivation
3.1 Linguistic motivation
It is usual to talk of ?annotation? generically, to
cover any process that involves humans using a set
of guidelines to mark some specific linguistic phe-
nomenon in some given text. However, we would
argue that, when considering the aims of an anno-
tation task and its relation to the existing linguistic
literature, it becomes possible to distinguish be-
tween various types of annotation. Further, we
will show that our own effort situates itself in a
little studied relation to formal semantics.
The most basic type of annotation is the
one where computational linguists mark large
amounts of textual data with well-known and well-
understood labels. The production of tree banks
like the Penn Treebank (Marcus et al 1993) makes
use of undisputed linguistic categories such as
parts of speech. The aim is to make the computer
learn and use irrefutable bits of linguistics. (Note
that, despite agreement, the representation of those
categories may differ: see for example the range
of available parts of speech tag sets.) This type
of task mostly involves basic syntactic knowledge,
but can be taken to areas of syntax and seman-
74
tics where the studied phenomena have a (some-
what) clear, agreed upon definition (Kingsbury et
al, 2002). We must clarify that in those cases, the
choice of a formalism may already imply a certain
theoretical position ? leading to potential incom-
patibilities between formalisms. However, the cat-
egories for such annotation are themselves fixed:
there is a generally agreed broad understanding of
concepts such as noun phrases and coordination.
Another type of annotation concerns tasks
where the linguistic categories at play are not
fixed. One example is discourse annotation ac-
cording to rhetorical function (Teufel et al 2006)
where humans are asked to differentiate between
several discursive categories such as ?contrast? or
?weakness?. In such a task, the computational lin-
guist develops a theory where different states or
values are associated with various phenomena. In
order to show that the world functions according to
the model presented, experimentation is required.
This usually takes the form of an annotation task
where several human subjects are required to mark
pieces of text following guidelines inferred from
the model. The intuition behind the annotation ef-
fort is that agreement between humans support the
claims of the theory (Teufel, in press). In particu-
lar, it may confirm that the phenomena in question
indeed exist and that the values attributed to them
are clearly defined and distinguishable. The work
is mostly of a descriptive nature ? it creates phe-
nomenological definitions that encompass bits of
observable language.
Our own work is similar to the latter type of
annotation in that it is trying to capture a phe-
nomenon that is still under investigation in the lin-
guistic literature. However, it is also different be-
cause the categories we use are fixed by language:
the quantifiers some, most and all exist and we as-
sume that their definition is agreed upon by speak-
ers of English. What we are trying to investigate
is whether those quantifiers should be used at all
in the context of ambiguous quantification.
The type of annotation carried out in this pa-
per can be said to have more formal aims than the
tasks usually attempted in computational linguis-
tics. In particular, it concerns itself with some of
the broad claims made by formal semantics: its
model-theoretical view and the use of generalised
quantifiers to formalise noun phrases.
In Section 1, we assumed that quantifiers de-
note relations between sets and presented the task
of quantification resolution as choosing the ?cor-
rect? set relation for a particular noun phrase in a
particular sentence ? implying some sort of truth
value at work throughout the process: the correct
set relation produces the sentence with truth value
1 while the other set relations produce a truth value
of 0. What we declined to discuss, though, is the
way that those reference sets were selected in nat-
ural language, i.e. we didn?t make claims about
what model, or models, are used by humans when
they compute the truth value of a given quantified
statement. The annotation task may not answer
this question but it should help us ascertain to what
extent humans share a model of the world.
In Section 2, we also argued that all subject
generic noun phrases could be analysed in terms
of quantification. That is, an (underspecified) gen-
eralised quantifier is at work in sentences that con-
tain such generic NPs. It is expected that if the
annotation is feasible and shows good agreement
between annotators, the quantification hypothesis
would be confirmed. Thus, annotation may allow
us to make semantic claims such as ?genericity
does quantify?. Note that the categories we assume
are intuitive and do not depend on a particular rep-
resentation: it is possible to reuse our annotation
with a different formalism as long as the theoreti-
cal assumption of quantification is agreed upon.
We are not aware of any annotation work in
computational linguistics that contributes to vali-
dating (or invalidating) a particular formal theory.
In that respect, the experiments presented in this
paper are of a slightly different nature than the
standard research on annotation (despite the fact
that, as we will show in the next section, they also
aim at producing data for a language analysis sys-
tem).
3.2 Previous work on genericity annotation
The aim of our work being the production of an au-
tomatic quantification resolution system, we need
an annotated corpus to train and test our machine
learning algorithm. There is no corpus that we
know of which would give us the required data.
The closest contestants are the ACE corpus (2008)
and the GNOME corpus (Poesio, 2000) which
both focus on the phenomenon of genericity, as de-
scribed in the linguistic literature. Unfortunately,
neither of those corpora are suitable for use in a
general quantification task.
The ACE corpus only distinguishes between
75
?generic? and ?specific? entities. The classification
proposed by the authors of the corpus is there-
fore a lot broader than the one we are attempt-
ing here and there is no direct correspondence
between their labels and natural language quanti-
fiers: we have shown in Section 2 that genericity
didn?t map to a particular division of the quantifi-
cational space. Furthermore, the ACE guidelines
contradict to some extent the literature on generic-
ity. They require for instance that a generic men-
tion be quantifiable with all, most or any. This
implies that statements such as Mosquitoes carry
malaria either refer to a kind only (i.e. they are
not quantified) or are not generic at all. Further,
despite the above reference to quantification, the
authors seem to separate genericity and universal
quantification as two antithetical phenomena, as
shown by the following quote: ?Even if the au-
thor may intend to use a GEN reading, if he/she
refers to all members of the set rather than the set
itself, use the SPC tag?.
The GNOME annotation scheme is closer in
essence to the literature on genericity and much
more detailed than the ACE guidelines. However,
the scheme distinguishes only between generic
and non-generic entities, as in the ACE corpus
case, and the corpus itself is limited to three gen-
res: museum labels, pharmaceutical leaflets, and
tutorial dialogues. The guidelines are therefore
tailored to the domains under consideration; for
instance, bare noun phrases are said to be typically
generic. This restricted solution has the advantage
of providing good agreement between annotators
(Poesio, 2004 reports a Kappa value of 0.82 for
this annotation).
4 Annotation corpus
We use as corpus a snapshot of the English ver-
sion of the online encyclopaedia Wikipedia.1 The
choice is motivated by the fact that Wikipedia can
be taken as a fairly balanced corpus: although it is
presented as an encyclopaedia, it contains a wide
variety of text ranging from typical encyclopaedic
descriptions to various types of narrative texts
(historical reconstructions, film ?spoilers?, fiction
summaries) to instructional material like rules of
games. Further, each article in Wikipedia is writ-
ten and edited by many contributors, meaning that
speaker heterogeneity is high. We would also ex-
pect an encyclopaedia to contain relatively many
1http://www.wikipedia.org
generics, allowing us to assess how our quantifi-
cational reading fares in a real annotation task. Fi-
nally, the use of an open resource means that the
corpus can be freely distributed.2
In order to create our annotation corpus, we first
isolated the first 100,000 pages in our snapshot
and parsed them into a Robust Minimal Recur-
sion Semantics (RMRS) representation (Copes-
take, 2004) using first the RASP parser (Briscoe
et al 2006) and the RASP to RMRS converter
(Ritchie, 2004). We then extracted all construc-
tions of the type Subject-Verb-Object from the ob-
tained corpus and randomly selected 300 of those
?triples? to be annotated. Another 50 random
triples were selected for the purpose of annotation
training (see Section 7.1).
We show in Figure 1 an example of an anno-
tation instance produced by the parser pipeline.
The data provided by the system consists of the
triple itself, followed by the argument structure
of that triple, including the direct dependents of
its constituents, the number and tense information
for each constituent, the file from which the triple
was extracted and the original sentence in which
it appeared. The information provided to annota-
tors is directly extracted from that representation.
(Note that the examples were not hand-checked,
and some parsing errors may have remained.)
5 Evaluating the annotation
In an annotation task, two aspects of agreement are
important when trying to prove or refute a partic-
ular linguistic model: stability and reproducibility
(Krippendorf, 1980). Reproducibility refers to the
consistency with which humans apply the scheme
guidelines, i.e. to the so-called inter-annotator
agreement. Stability relates to whether the same
annotator will consistently produce the same an-
notations at different points in time. The measure
for stability is called intra-annotator agreement.
Both measures concern the repeatability of an an-
notation experiment.
In this work, agreement is calculated for each
pair of annotators according to the Kappa mea-
sure. There are different versions of Kappa de-
pending on how multiple annotators are treated
and how the probabilities of classes are calculated
to establish the expected agreement between anno-
tators, Pr(e): we use Fleiss? Kappa (Fleiss, 1971),
which allows us to compute agreement between
2For access, contact the first author.
76
digraph G211 {
"TRIPLE: weed include pigra" [shape=box];
include -> weed [label="ARG1 n"];
include -> pigra [label="ARG2 n"];
invasive -> weed [label="ARG1 n"];
compound_rel -> pigra [label="ARG1 n"];
compound_rel -> mimosa [label="ARG2 n"];
"DNT INFO: lemma::include() tense::present lpos::v (arg::ARG1 var::weed() num::pl pos::)
(arg::ARG2 var::pigra() num::sg pos::)" [shape=box];
"FILE: /anfs/bigtmp/newr1-50/page101655" [shape=box];
"ORIGINAL: Invasive weeds include Mimosa pigra, which covers 80,000 hectares
of the Top End, including vast areas of Kakadu. " [shape=box]; }
Figure 1: Example of annotation instance
multiple annotators.
6 An annotation scheme for
quantification resolution
6.1 Scheme structure
Our complete annotation scheme can be found in
Herbelot (to appear). The scheme consists of five
parts. The first two present the annotation material
and the task itself. Some key definitions are given.
The following part describes the various quantifi-
cation classes to be used in the course of the an-
notation. Participants are then given detailed in-
structions for the labelling of various grammatical
constructs. Finally, in order to keep the demand
on the annotators? cognitive load to a minimum,
the last part reiterates the annotation guidelines in
the form of diagrammatic decision trees.
In the next sections, we give a walk-through of
the guidelines and definitions provided.
6.2 Material
Our annotators are first made familiar with the ma-
terial provided to them. This material consists
of 300 entries comprising a single sentence and
a triple Subject-Verb-Object which helps the an-
notator identify which subject noun phrase in the
sentence they are requested to label (the ?ORIG-
INAL? and ?TRIPLE? lines in the parser output ?
see Figure 1). No other context is provided. This
is partly to make the task shorter (letting us anno-
tate more instances) and partly to allow for some
limited comparison between human and machine
performance (by restricting the amount of infor-
mation given to our annotators, we force them ? to
some extent ? to use the limited information that
would be available to an automatic quantification
resolution system, e.g. syntax).
6.3 Definitions
In our scheme, we introduce the annotators to the
concepts of quantification and kind.3
Quantification is described in simple terms, as
the process of ?paraphrasing the noun phrase in
a particular sentence using an unambiguous term
expressing some quantity?. An example is given.
15. Europeans discovered the Tuggerah Lakes in
1796 = Some Europeans discovered the Tug-
gerah Lakes in 1796.
We only allow the three quantifiers some, most
and all. In order to keep the number of classes
to a manageable size, we introduce the additional
constraint that the process of quantification must
yield a single quantifier. We force the annotator
to choose between the three proposed options and
introduce priorities in cases of doubt: most has pri-
ority over all, some has priority over the other two
quantifiers. This ensures we keep a conservative
attitude with regard to inference (see Section 1).
Kinds are presented as denoting ?the group in-
cluding all entities described by the noun phrase
under consideration?, that is, as a supremum. (As
mentioned in Section 2, the verbal predicate ap-
plies collectively to that supremum in the corre-
sponding formalisation.)
Quantification classes are introduced in a sep-
arate part of the scheme. We define the five la-
bels SOME, MOST, ALL, ONE and QUANT (for al-
ready quantified noun phrases) and give examples
for each one of them.
We try, as much as possible, to keep annotators
away from performing complex reference resolu-
tion. Their first task is therefore to simply attempt
3Distributivity and collectivity are also introduced in the
scheme because they are a necessary part of our proposed for-
malisation. However, as this paper focuses on the annotation
of quantification itself, we will not discuss this side of the
annotation task.
77
to paraphrase the existing sentence by appending
a relevant quantifier to the noun phrase to be anno-
tated. In some cases, however, this is impossible
and no quantifier yields a correct English sentence
(this often happens in collective statements). To
help our annotators make decisions in those cases,
we ask them to distinguish what the noun phrase
might refer to when they first hear it and what it
refers to at the end of the sentence, i.e., when the
verbal predicate has imposed further constraints
on the quantification of the NP.
6.4 Guidelines
Guidelines are provided for five basic phrase
types: quantified noun phrases, proper nouns, plu-
rals, non-bare singulars and bare singulars.
6.4.1 Quantified noun phrases
This is the simplest case: a noun phrase that is
already quantified such as some people, 6 million
inhabitants or most of the workers. The annotator
simply marks the noun phrase with a QUANT label.
6.4.2 Proper nouns
Proper nouns are another simple case. But be-
cause what annotators understand as a proper noun
varies, we provide a definition. We note first that
proper nouns are often capitalised. It should how-
ever be clear that, while capitalised entities such
as Mary, Easter Island or Warner Bros refer to
singular, unique objects, others refer to groups or
instances of those groups: The Chicago Bulls, a
Roman. The latter can be quantified:
16. The Chicago Bulls won last week. (ALL ?
collective)
17. A Roman shows courage in battle. (MOST ?
distributive)
We define proper nouns as noun phrases that
?contain capitalised words and refer to a concept
which doesn?t have instances?. All proper nouns
are annotated as ONE.
6.4.3 Plurals
Plurals must be appropriately quantified and the
annotators must also specify whether they are
kinds or not. This last decision can simply be
made by attempting to paraphrase the sentence
with either a definite singular or an indefinite sin-
gular ? potentially leading to a typical generic
statement.
6.4.4 (Non-bare) singulars
Like plurals, singulars must be tested for a kind
reading. This is done by attempting to pluralise the
noun phrase. If pluralisation is possible, then the
kind interpretation is confirmed and quantification
is performed. If not (certain non-mass terms have
no identifiable parts), the singular refers to a single
entity and is annotated as ONE.
6.4.5 Bare singulars
We regard bare singulars as essentially plural, un-
der the linguistic assumption of non-overlapping
atomic parts ? for instance, water is considered a
collection of H2O molecules, rice is regarded as
a collection of grains of rice, etc (see Chierchia,
1998). In order to make this relation clear, we
ask annotators to try and paraphrase bare singulars
with an (atomic part) plural equivalent and follow,
as normal, the decision tree for plurals:
18. Free software allows users to co-operate in
enhancing and refining the programs they use
? Open source programs allow users...
When the paraphrase is impossible (as in certain
non-mass terms which have no identifiable parts),
the noun phrase is deemed a unique entity and la-
belled ONE.
7 Implementation and results
7.1 Task implementation
Three annotators were used in our experiment.
One annotator was one of the authors; the
other two annotators were graduate students (non-
linguists), both fluent in English. The two grad-
uate students were provided with individual train-
ing sessions where they first read the annotation
guidelines, had the opportunity to ask for clarifi-
cations, and subsequently annotated, with the help
of the author, the 50 noun phrases in the train-
ing set. The actual annotation task was performed
without communication with the scheme author or
the other annotators.
7.2 Kappa evaluation
We made an independence assumption between
quantification value and kind value, and evaluated
agreement separately for each type of annotation.
Intra-annotator agreement was calculated over
the set of annotations produced by one of the au-
thors. The original annotation experiment was re-
produced at three months? interval and Kappa was
78
Class Kind Quantification
Kappa 0.85 0.84
Table 1: Intra-annotator agreements for both tasks
Class Kind Quantification
Kappa 0.67 0.72
Table 2: Inter-annotator agreements for both tasks
computed between the original set and the new set.
Table 1 shows results over 0.8 for both tasks, cor-
responding to ?perfect agreement? according to the
Landis and Koch classification (1977). This indi-
cates that the stability of the scheme is high.
Table 2 shows inter-annotator agreements of
over 0.6 for both tasks, which correspond to ?sub-
stantial agreement?. This result must be taken with
caution, though. Although it shows good agree-
ment overall, it is important to ascertain in what
measure it holds for separate classes. In an ef-
fort to report such per class agreement, we cal-
culate Kappa values for each label by evaluating
each class against all others collapsed together (as
suggested by Krippendorf, 1980).
Table 3 indicates that substantial agreement is
maintained for separate classes in the kind annota-
tion task. Table 4, however, suggests that, if agree-
ment is perfect for the ONE and QUANT classes,
it is very much lower for the SOME, MOST and
ALL classes. While it is clear that the latter three
are the most complex to analyse, we can show
that the lower results attached to them are partly
due to issues related to Kappa as a measure of
agreement. Feinstein and Cicchetti (1990), fol-
lowed by Di Eugenio and Glass (2004) proved
that Kappa is subject to the effect of prevalence
and that different marginal distributions can lead
to very different Kappa values for the same ob-
served agreement. It can be shown, in particu-
lar, that an unbalanced, symmetrical distribution
of the data produces much lower figures than bal-
anced or unbalanced, asymmetrical distributions
because the expected agreement gets inflated. Our
confusion matrices indicate that our data falls into
the category of unbalanced, symmetrical distribu-
tion: the classes are not evenly distributed but an-
notators agree on the relative prevalence of each
class. Moreover, in the quantification task itself,
the ONE class covers roughly 50% of the data.
This means that, when calculating per class agree-
Class KIND NOT-KIND QUANT
Kappa 0.63 0.71 0.88
Table 3: Per class inter-annotator agreement for
the kind annotation
Class ONE SOME MOST ALL QUANT
Kappa 0.81 0.45 0.44 0.51 0.88
Table 4: Per class inter-annotator agreement for
the quantification annotation
ment, we get an approximately balanced distri-
bution for the ONE label and an unbalanced, but
still symmetrical, distribution for the other labels.
This leads to the expected agreement being rather
low for the ONE class and very high for the other
classes. Table 5 reproduces the per class agree-
ment figures obtained for the quantification task
but shows, in addition, the observed and expected
agreements for each label. Although the observed
agreement is consistently close to, or over, 0.9, the
Kappa values differ widely in conjunction with ex-
pected agreement. This results in relatively low re-
sults for SOME, MOST and ALL (the QUANT label
has nearly perfect agreement and therefore doesn?t
suffer from prevalence).
Class Kappa Pr(a) Pr(e)
ONE 0.814 0.911 0.521
SOME 0.445 0.893 0.808
MOST 0.438 0.931 0.877
ALL 0.509 0.867 0.728
QUANT 0.884 0.987 0.885
Table 5: The effect of prevalence on per class
agreement, quantification task. Pr(a) is the ob-
served agreement between annotators, Pr(e) the
expected agreement.
With regard to the purpose of creating a gold
standard for a quantification resolution system, we
also note that out of 300 quantification annota-
tions, there are only 14 cases in which a majority
decision cannot be found, i.e., at least two anno-
tators agreed in 95% of cases. Thus, despite some
low Kappa results, the data can adequately be used
for the production of training material.4
4As far as such data ever can be: Reidsma and Carletta,
2008, show that systematic disagreements between annota-
tors will produce bad machine learning, regardless of the
Kappa obtained on the data.
79
In Section 8, we introduce difficulties encoun-
tered by our subjects, as related in post-annotation
discussions. We focus on quantification.
8 Annotation issues
8.1 Reference
Although we tried to make the task as simple as
possible for the annotators by asking them to para-
phrase the sentences that they were reading, they
were not free from having to work out the refer-
ent of the NP (consciously or unconsciously) and
we have evidence that they did not always pick
the same referent, leading to disagreements at the
quantification stage. Consider the following:
19. Subsequent annexations by Florence in the
area have further diminished the likelihood of
incorporation.
In the course of post-annotation discussions, it
became clear that not all annotators had chosen the
same referent when quantifying the subject NP in
the first clause. One annotator had chosen as refer-
ent subsequent annexations, leading to the reading
Some subsequent annexations, conducted by Flo-
rence in the area, have further diminished the like-
lihood of incorporation. The other two annotators
had kept the whole NP as referent, leading to the
reading All the subsequent annexations conducted
by Florence in the area have further diminished
the likelihood of incorporation.
8.2 World knowledge
Being given only one sentence as context for the
NP to quantify, annotators sometimes lacked the
world knowledge necessary to make an informed
decision. This is illustrated by the following:
20. The undergraduate schools maintain a non-
restrictive Early Action admissions pro-
gramme.
Discussion revealed that all three annotators had
a different interpretation of what the mentioned
Early Action programme might refer to, and of
the duties of the undergraduate schools with re-
gard to it. This led to three different quantifica-
tions: SOME, MOST and ALL.
8.3 Interaction with time
The existence of interactions between NP quantifi-
cation and what we will call temporal quantifica-
tion is not surprising: we refer to the literature on
genericity and in particular to Krifka et al(1995)
who talk of characteristic predication, or habitual-
ity, as a phenomenon encompassed by genericity.
We do not intend to argue for a unified theory of
quantification, as temporal quantification involves
complexities which are beyond the scope of this
work. However, the interactions observed between
temporality and NP quantification might explain
further disagreements in the annotation task. The
following is a sentence that contains a temporal
adverb (sometimes) and that produced some dis-
agreement amongst annotators:
21. Scottish fiddlers emulating 18th-century
playing styles sometimes use a replica of the
type of bow used in that period.
Two annotators labelled the subject of that sen-
tence as MOST, while the third one preferred
SOME. In order to understand the issue, consider
the following, related, statement:
22. Mosquitoes sometimes carry malaria.
This sentence has the possible readings: Some
mosquitoes carry malaria or Mosquitoes, from
time to time in their lives, carry malaria. The first
reading is clearly the preferred one.
The structure of (21) is identical to that of (22)
and it should therefore be taken as similarly am-
biguous: it either means that some of the Scottish
fiddlers emulating 18th-century playing styles use
a replica of the bow used in that period, or that a
Scottish fiddler who emulates 18th-century play-
ing styles, from time to time, uses a replica of such
a bow. The two readings may explain the labels
given to that sentence by the annotators.
9 Conclusion
Taking prevalence effects into account, we believe
that our agreement results can be taken as evidence
that underquantification is analysable in a consis-
tent way by humans. We also consider them as
strong support for our claim that ?genericity quan-
tifies?. Our scheme could however be refined fur-
ther. In a future version, we would add guidelines
regarding the selection of the referent of the noun
phrase, encourage the use of external resources to
obtain the context of a given sentence (or simply
provide the actual context of the sentence), and
give some pointers as to how to resolve issues or
ambiguities caused by temporal quantification.
80
References
ACE. 2008. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Entities, Version 6.6
2008.06.13. Linguistic Data Consortium.
Edward Briscoe, John Carroll and Rebecca Watson.
2006. ?The Second Release of the RASP System?.
In Proceedings of the COLING/ACL 2006 Interac-
tive Presentation Sessions, Sydney, Australia, 2006.
Gregory Carlson. 1995. ?Truth-conditions of Generics
Sentences: Two Contrasting Views?. In Gregory N.
Carlson and Francis Jeffrey Pelletier, Editors, The
Generic Book, pages 224 ? 237. Chicago University
Press.
Gennaro Chierchia. 1998. ?Reference to kinds across
languages?. Natural Language Semantics, 6:339?
405.
Ariel Cohen. 1996. Think Generic: The Meaning
and Use of Generic Sentences. Ph.D. Dissertation.
Carnegie Mellon University at Pittsburgh. Published
by CSLI, Stanford, 1999.
Ann Copestake. 2004. ?Robust Minimal Recursion
Semantics?. www.cl.cam.ac.uk/?aac10/
papers/rmrsdraft.pdf.
Barbara Di Eugenio and Michael Glass. 2004. ?The
kappa statistic: a second look?. Computational Lin-
guistics, 30(1):95?101.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
?High agreement but low kappa: I. The problems of
two paradoxes?. Journal of Clinical Epidemiology,
43(6):543?549.
Joseph Fleiss. 1971. ?Measuring nominal scale agree-
ment among many raters?. Psychological Bulletin,
76(5):378-382.
Aurelie Herbelot. To appear. Underspecified quantifi-
cation. Ph.D. Dissertation. Computer Laboratory,
University of Cambridge, United Kingdom.
Gerhard Heyer. 1990. ?Semantics and Knowledge
Representation in the Analysis of Generic Descrip-
tions?. Journal of Semantics, 7(1):93?110.
Paul Kingsbury, Martha Palmer and Mitch Marcus.
2002. ?Adding Semantic Annotation to the Penn
TreeBank?. In Proceedings of the Human Language
Technology Conference (HLT 2002), San Diego,
California, pages 252?256.
Manfred Krifka, Francis Jeffry Pelletier, Gregory N.
Carlson, Alice ter Meulen, Godehard Link and Gen-
naro Chierchia. 1995. ?Genericity: An Introduc-
tion?. In Gregory N. Carlson and Francis Jeffry
Pelletier, Editors. The Generic Book, pages 1?125.
Chicago: Chicago University Press.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Newbury Park, CA:
Sage.
J. Richard Landis and Gary G. Koch. 1977. ?The
Measurement of Observer Agreement for Categor-
ical Data?. Biometrics, 33:159?174.
Sara-Jane Leslie. 2008. ?Generics: Cognition and Ac-
quisition.? Philosophical Review, 117(1):1?47.
Christopher Lyons. 1999. Definiteness. Cambridge
University Press, Cambridge, UK.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. ?Building a large annotated
corpus of english: The penn treebank?. Computa-
tional Linguistics, 19(2):313?330.
Francis Jeffry Pelletier and Nicolas Asher. 1997.
?Generics and defaults?. In: Johan van Benthem and
Alice ter Meulen, Editors, Handbook of Logic and
Language, pages 1125?1177. Amsterdam: Elsevier.
Massimo Poesio. 2000. ?The GNOME annota-
tion scheme manual?, Fourth Version. http:
//cswww.essex.ac.uk/Research/nle/
corpora/GNOME/anno_manual_4.htm
Massimo Poesio. 2004. ?Discourse Annotation and Se-
mantic Annotation in the GNOME Corpus?. In: Pro-
ceedings of the ACL Workshop on Discourse Anno-
tation, Barcelona, Spain.
Dennis Reidsma and Jean Carletta. 2008. ?Reliability
measurement without limits?. Computational Lin-
guistics, 34(3), pages 319?326.
Anna Ritchie. 2004. ?Compatible RMRS Repre-
sentations from RASP and the ERG?. http:
//www.cl.cam.ac.uk/TechReports/
UCAM-CL-TR-661.
Simone Teufel, Advaith Siddharthan, Dan Tidhar.
2006. ?An annotation scheme for citation function?.
In: Proceedings of Sigdial-06, Sydney, Australia,
pages 80?87.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Summarisation and Citation
Indexing. CSLI Publications. In press.
81
Formalising and specifying underquantification
Aurelie Herbelot
University of Cambridge
ah433@cam.ac.uk
Ann Copestake
University of Cambridge
aac@cl.cam.ac.uk
Abstract
This paper argues that all subject noun phrases can be given a quantified formalisation in terms
of the intersection between their denotation set and the denotation set of their verbal predicate. The
majority of subject noun phrases, however, are only implicitely quantified and the task of retrieving
the most plausible quantifier for a given NP is non-trivial. We propose a formalisation which captures
the underspecification of the quantifier in subject NPs and we show that this formalisation is widely
applicable, including in statements involving kinds. We then present a baseline for a quantification
resolution system using syntactic features as basis for classification. Although the syntactic baseline
provides a respectable 78% precision, our error analysis shows that obtaining true performance on
the task requires information beyond syntax.
1 Quantification resolution
Most subject noun phrases in English are not explicitly quantified. Still, humans are able to give them
quantificational interpretations in context:
1. Cats are mammals = All cats...
2. Cats have four legs = Most cats...
3. Cats were sleeping by the fire = Some cats...
4. The beans spilt out of the bag = Most/All of the beans...
5. Water was dripping through the ceiling = Some water...
We refer to this process as quantification resolution, that is, the process of giving an implicitely quan-
tified NP a formalisation which expresses a unique set relation appropriate to the semantics of the utter-
ance. For instance, the most plausible resolution of 1 can be expressed as:
6. All cats are mammals.
|? ? ?| = |?| where ? is the set of all cats and ? the set of all mammals.
Resolving the quantification value of NPs is important for many NLP tasks, in particular for infer-
ence. We would like to be able to automatically perform the type of interpretations shown in 1 to 5.
It will allow us to draw conclusions such as If (all) cats are mammals and Tom is a cat, then Tom is a
mammal and If (some) cats are in my garden, then (some) animals are in my garden.1
The task of quantification resolution involves finding a semantic representation that goes beyond what
is directly obtainable from a sentence?s syntactic composition. We can write the(x, cat?(x), sleep?(x))
as we would write some(x, cat?(x), sleep?(x))2, but while the quantification semantics of some can be
1The type of entailment relying on word substitution is dependent on quantification: (All) cats are mammals doesn?t imply
that (All) animals are mammals.
2We use here a generalised quantifier notation were the first argument of the quantifier is the bound variable.
165
fully defined (given a singular NP, we are talking of one entity only), that of the cannot: in a singu-
lar NP introduced by the, the referent can either be a single entity or a plurality with various possible
quantificational interpretations (cf The cat is sleeping vs The cat is a mammal).
This paper is an attempt to provide a formal semantics for implicitely quantified NPs which a) sup-
ports the type of inferences required by NLP, b) has good empirical coverage (beyond ?standard? lin-
guistic examples), c) lends itself to evaluation by human annotation and d) can be derived automatically.
We draw on work in formal linguistics, but by formulating the problem as quantification resolution,
we obtain an account which is more tractable from an NLP perspective. We also present preliminary
experiments that automate quantification resolution using a syntax-driven classifier.
2 Under(specified) quantification
The phenomenon of ambiguous quantification overlaps with genericity. Generic NPs have tradition-
ally been described as referring to kinds (Krifka et al, 1995) and one of their most frequent syntactic
expressions is the bare plural, although they occur in definite and indefinite singulars too, as well as
bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and
Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although,
puzzlingly enough, not always with the same quantifier:
7. Dogs are in my garden = Some dogs...
8. Frenchmen eat horsemeat = Some/Relatively-many Frenchmen... (For the relatively many reading,
see Cohen, 2001.)
9. Cars have four wheels = Most cars...
This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics.
Note that relegating the various readings to a matter of pragmatics, formalising all bare plurals using an
existential, is no solution as we are then unable to explain the semantic difference between, for instance,
Mosquitoes carry malaria and Some mosquitoes carry malaria. The only accepted assumption is that
an operator GEN exists, which acts as a silent quantifier over the restrictor (subject) and matrix (verbal
predicate) of the generic statement.
In this paper, we take an approach which sidesteps some of the intractable problems associated with
the literature on generics and which also extends to definite plurals, as discussed below. Instead of
talking of ambiguous quantification, we will talk of underspecified quantification, or underquantifi-
cation. By this, we mean that the bare plural, rather than exhibiting a silent, GEN quantifier, simply
features a placeholder in the logical form which must be filled with the appropriate quantifier (e.g.,
uq(x, cat?(x), sleep?(x)), where uq is the placeholder quantifier). This account caters for the facts that
so-called generics can so easily be quantified via traditional quantifiers, that GEN is silent in all known
languages, and it explains also why it is the bare form which has the highest productivity, and can denote
a range of quantified entities, from existentials to universals. Using the underquantification hypothesis,
we can paraphrase any generic of the form ?X does Y? as ?there is a set of things X, a certain number of
which do Y? (note the partitive construction).
We now turn to definite plurals which have traditionally been thought to be outside of the genericity
phenomenon and associated with universals (e.g., Lyons, 1999). Definite plurals do exhibit a range of
quantificational behaviour and thus we argue that they should be studied as underquantified forms too.
Consider the following, from Dowty (1987):
10. At the end of the press conference, the reporters asked the president questions.
Dowty remarks that it is not necessary that all reporters ask questions for the sentence to be true. In fact,
it is only necessary that some of them did. Dowty says: ?The question of how many members of the
group referent of a definite NP must have the distributive property is in part lexically determined and in
part determined by the context, and only rarely is every member required to have these properties.?
Following the existential reading, we can write:
166
11. some(x, reporter?(x), askQuestion?(x))
The problem is that for Dowty, the NP refers to a ?group?, i.e., to the reporters as a whole, and not to
specific reporters. We don?t want to say ?there is a small set of reporters, each of which asked a question?;
we want to say ?there is a large set of reporters ? all those present at the press conference ? and some
of them asked a question?, i.e., we want to use a partitive construction. We follow Brogaard?s (2007)
account of definite plurals as partitive constructions, where she examines the following:
12. The students asked questions.
Brogaard argues that, given X , the denotation of the students, a subset Y of X is selected via the quan-
tifier some and that the verbal predicate applies (distributively) to Y . A similar account can be given
of (10): there is a set of reporters, and a certain number of elements in that set (some reporters) asked
questions ? which is our desired reading. Note that all definite plurals can have this interpretation (e.g.,
possessives and demonstratives also).
We will next argue that the partitive construct observed in definite plurals can be generally applied to
subject NPs and we will propose a single formalisation for all underquantified statements.
3 Formalisation
3.1 Link?s notation (1983)
In what follows, we briefly define each item of notation used in this work, as taken from Link (1983).
We illustrate the main points via examples over a closed worldW containing three cats (Kitty, Sylvester
and Bagpuss).
The background assumption for our formalisation is that, following Link, plurals can be represented
as lattices. The star sign ? generates all individual sums of members of the extension of predicate P . So
if P is cat?, the extension of ?P is a join-semilattice representing all possible sums of cats in the world
under consideration. The join-semilattice of cats in worldW is shown in Fig 1.
Figure 1: Join-semilattice of all cats in worldW
The sign ? is the sum operator. ?xPx represents the sum, or supremum, of all objects that are ?P .
??xPx represents the proper sum of Ps, that is, the supremum of all objects that are proper plural
predicates of P . The sum includes (non-plural) individuals such asK or S while the proper sum doesn?t.
In worlds where there is more than one object in the extension of ?P , ?xPx = ??xPx: e.g., in Fig 1,
the sum of all cats is the same as the proper sum of all cats, i.e., the set {K,S,B}. (Compare this with a
world where there is only one cat, say Kitty: then ?xPx = {K} while ??xPx = ?).
The product sign
?
expresses an individual-part relation. The ? sign in combination with ? indi-
cates atomic part. Following Chierchia (1998), we assume the same underlying lattice for both mass
terms and count nouns, so we use the
?
and ? operators for formalising quantification over mass entities.
3.2 Collective and distributive predicates
Some predicates are collective: they refer to a group as a whole and not to its instances (13). Other
predicates are always distributive (14):
167
13. Antelopes gather near water holes (*Andy the antelope gathers near water holes.)
14. Three soldiers were asleep (Tom was asleep, Bill was asleep, Cornelia was asleep.)
Most verbal phrases, though, are ?mixed predicates? that accept both readings:
15. Three soldiers stole wine from the canteen.
(Tom, Bill and Cornelia went together to the canteen to steal wine or Tom, Bill and Cornelia each
stole wine from the canteen.)
Collective predicates can be a source of confusion when trying to directly apply quantification to an
ambiguously quantified NP:
16. (*Some/Most/All) Americans elect a new president every five years.
Quantifying 16 seems initially impossible in shallow form: we cannot write all(x,american?(x),electPres?(x))
as it seems to imply distributivity. However, we refer to the reporter example (10) and the latent partitive
construct that we suggested existed in that (distributive) sentence. By similarity, we can say that there
is a set X of Americans able to vote, and a subset Y of those ? which in this case is selected by the
quantifier all and is therefore equal to X ? collectively elects the president.
3.3 Formalising the partitive construct
Following Link (1998) for the formalisation of collective and distributive predicates, we can write, for
10 and 16:
17. X = ??x reporterAtPressConference?(x) ? ?Y [Y ?X ? ?z[z ??Y ?askques?(z)]]
18. X = ??xvotingAmerican?(x) ? ?Y [Y ?X?electPresident?(Y )]3
For the collective case, we just apply the verbal predicate collectively.
We can then add the quantifier resolution. We assume a three-fold partitioning of the quantificational
space, corresponding to the natural language quantifiers some, most and all (in addition to one, for the
description of singular, unique entities). The corresponding set relations are:
19. if some(?, ?) then 0 < |? ? ?|
20. ifmost(?, ?) then |?? ?| ? |? ? ?|
21. if all(?, ?) then |?? ?| = 0
These set relations can be expressed in terms of the sets involved in the partitive construction: in 16,
ifX is the set of all Americans able to vote, Y the subset ofX selected by the quantifier, and Z the set of
all things that elect the president, then Y actually represents the intersection X ? Z. We can thus write:
22. X = ??x reporterAtPressConference?(x)? ?Y [Y ?X ? ?z[z ??Y ?askques?(z)]? (0 < |Y |)]
23. X = ??x votingAmerican?(x) ? ?Y [Y ?X?electPresident?(Y ) ? (|X ? Y | = 0)]
The same principle applies to mass nouns. We show below a distributive example.
24. Water was dripping through the ceiling.
X = ??x water?(x) ? ?Y [Y ?X ? ?z[z ??Y ?dripThroughCeiling?(z)] ? (0 < |Y |)]
We thus write the underspecified quantifier as:
25. X = ??x P ?(x) ? ?Y [Y ?X ?Q(Y )] ? quantConstraint(X,Y )]
where the quantConstraint ensures the correct cardinality of Y for various quantifiers and the predicateQ
applies distributively or collectively depending on the semantics of the sentence. X and Y respectively
denote the Nbar and NP referents in the quantified paraphrase of the statement.
3Note that in the two examples, we have restricted X to the relevant set of entities. We will not investigate here how this
particular reference resolution takes place.
168
4 Kinds
In order to argue that our formalisation is applicable to all subject noun phrases, we must briefly come
back to the case of generics which, in some linguistic accounts, are not seen as quantified (Carlson,
1977).4 According to those accounts, the subject NP in sentences such as The cat is a mammal (the
kind) can be regarded as an entity similar to proper nouns. The generic reading of the sentence then
takes a straightforward subject/predicate formalisation of the type mammal?(cat?). The main argument
in favour of such a representation is the existence of sentences where the verbal predicate seems to only
be applicable to a species rather than to its instances:
26. The dodo is extinct.
Such cases, we claim, do not preclude quantification. We use the accounts of Chierchia (1998) and
Krifka (2004), where a kind is defined as a function that returns the greatest element of the extension of
the property relevant to that kind: Kind(X) = ??x X ?(x). This gives us the following for 26:
27. X = ??x dodo?(x) ? ?Y [Y ?X ? extinct?(Y ) ? (|Y ?X| = 0)]
We stress however that we do not deny the validity of representations that involve a simple sub-
ject/predicate structure. It should be clear that the sentence The cat is a mammal has an interpretation
where the species ?cat? is attributed the property of being a mammal. What we argue is simply that the
meaning of the sentence also includes a quantificational aspect. We want, after all, to be able to make
natural inferences about individual cats: if the cat is a mammal then Tom the cat is a mammal. We believe
that both quantification and a subject/predicate formalisation are necessary to fully render the semantics
of such sentences. We will also argue in Section 7 that for the purposes of computational linguistics, it
is actually desirable to formalise the quantificational aspect separately, as part of the full semantics.
We should also note that the genericity phenomenon is usually seen as encompassing habitual con-
structions (Krifka et al, 1995). Our quantificational account of kinds will not necessarily be applicable
to quantification of events and we do not wish to make any claims with regard to habituality in this paper.
For completeness, we will however point out that, following Chierchia (1995) on indefinites, we see
quantification adverbs as able to bind, and therefore quantify over individuals: according to this view,
the most felicitous reading of Mosquitoes sometimes carry malaria is Some mosquitoes carry malaria,
formalisable with 25.
5 Automatic quantification: first attempts
To our knowledge, no attempt at the automatic specification of quantification has been made before. In
consequence, we start our investigation with the simplest possible type of machine learning algorithm,
using as determining features the direct syntactic context of the statement to be quantified. The general
idea of such a system is that grammatical information such as the number of a subject noun phrase and
the tense of its verbal predicate may be statistically related to its classification.
5.1 Gold standard
We built a gold standard by re-using and expanding the quantification annotations we produced in Herbe-
lot and Copestake (2010). This small corpus, which contains randomly extracted Wikipedia5 sentences,
provides 300 instances of triply annotated subject noun phrases. The categories used for annotation are
the natural language quantifiers ONE, SOME, MOST, ALL and the label QUANT (for noun phrases of the
type some cats, most turtles or more than 37 unicorns which, being explicitly quantified, do not enter our
underquantification account and must be marked with a separate label). In order to convert the multiple
4A more comprehensive discussion can be found in Herbelot (2010).
5http://www.wikipedia.org/
169
annotations to a gold standard, we used majority opinion when it was available and negotiation in cases
of complete disagreement. There were only 14 cases where a majority opinion cannot be obtained.
The main issue with the resulting gold standard is its relatively small size. The 300 data points it
provides are clearly insufficient for machine learning, but the annotation process is time-consuming and
we do not have the resources to set up a large-scale annotation effort. As a trade-off, the first author
of this paper annotated a further 300 noun phrases, thus doubling the size of the gold standard. As a
precaution, we ran the classifier presented later in this section over the original gold standard and over
the new annotations; no substantial difference in performance between the two runs was found.
Table 1 shows the class distribution of our five quantification labels over the 600 instances of the
extended gold standard.
Class Number of instances Percentage of corpus
ONE 367 61%
SOME 53 9%
MOST 34 6%
ALL 102 17%
QUANT 44 7%
Table 1: Class distribution over 600 instances
We note, first, that the number of explicitly quantified noun phrases amounts to only 7% of the an-
notation set. This shows that the resolution of underquantification has potentially high value for NLP
systems. Next, we remark that 61% of all instances simply denote a single entity, leaving 32% to under-
quantified plurals ? 189 instances. This imbalance is problematic for the machine learning task that we
set out to achieve. First, it means that the training data available for SOME, MOST and ALL annotations
is comparably sparse. Secondly, it implies that the baseline for our future classifier is relatively high:
assuming a most frequent class baseline, we must beat 61% precision.
5.2 Quantifying with syntax
Most of the remarks that can be found in the literature on the relation between syntax and quantification
have been written with respect to the generic versus non-generic distinction. Although we have moved
away from the terminology on genericity, the two following examples show the potential promises ?
and hurdles ? of using syntax to induce quantification annotations.
? Noun phrases which act as subjects of simple past tense verbs are usually non-generic: A cow says
?moo? / A cow said ?moo? (Gelman, 2004). However, the so-called ?historic past? is an exception
to this rule: The woolly mammoth roamed the earth many years ago.
? The combination of a bare plural and present tense is a prototypical indication of genericity: Tigers
are massive (Cimpian and Markman, 2008). But news headlines behave differently: Cambridge
students steal cow.
We informally investigate the distribution of various grammatical constructions with respect to quan-
tification, as obtained from our gold standard. Although some constructions give a clear majority to one
or another label, that majority is not always overwhelming. For instance, consistently annotating bare
plurals followed by a past tense as SOME would result in a precision of only 54%. It is therefore unclear
how accurate a classifier based only on syntax can be. (Note that the quantification phenomenon is un-
derstood to be semantically complex and that syntax is only one of many features used in the annotation
guidelines produced in Herbelot and Copestake, 2010.)
170
5.3 Features
We give the system article and number information for the noun phrase to be quantified, as well as the
tense of the verbal predicate following it. In order to cater for proper nouns, we also indicate whether the
head of the noun phrase is capitalised or not. Article, number and capitalisation information is similarly
provided for the object of the verb. All features are automatically extracted from the Robust Minimal
Recursion Semantics (RMRS, Copestake, 2004) representation of the sentence in which the noun phrase
appears (obtained via a RASP parse, Briscoe et al, 2006). The following shows an example of a feature
line for a particular noun phrase (the sentence in which the noun phrase appears is also given):
ORIGINAL: [His early blues influences] included artists such as Robert
Johnson, Bukka White, Skip James and Sleepy John Estes.
FEATURES: past,possessive,plural,nocap,bare,plural,nocap
Note that articles belonging to the same class are labelled according to that class: all possessive
articles, for instance, are simply marked as ?possessive?. This is the same for demonstrative articles.
5.4 Experiments and results
The aim of this work is not only to produce an automatic quantification system, but also, if possible,
to learn about the linguistic phenomena surrounding the underspecification of quantification. Because
of this, we choose a tree-based classifier which has the advantage of letting us see the rules that are
created by the system and thereby may allow us to make some linguistic observations with regard to the
cooccurrence of certain quantification classes with certain grammatical constructions. We use an off-the-
shelf implementation of the C4.5 classifier (Quinlan, 1993) included in the Weka data mining software.6
We perform a 6-fold cross-validation on the gold standard and report class precision, recall and F-score.
Class Precision Recall F-score
ONE 86% (362/422) 99% (362/367) 92%
SOME 60% (25/42) 47% (25/53) 53%
MOST 33% (2/6) 6% (2/34) 10%
ALL 53% (57/108) 56% (57/102) 54%
QUANT 100% (22/22) 50% (22/44) 67%
Table 2: Class precision and recall for the quantification task
The C4.5 classifier gives 78% overall precision to the quantification task. Tables 2 shows per class
results for the three tasks. The figures in brackets indicate the number of true positives for a particular
class, followed by the total number of instances annotated by the system as instances of that class. The
classifier performs extremely well with the ONE class, reaching 92% F-score. Already quantified noun
phrases yield perfect precision and mediocre recall, as might be expected since we do not provide the
system with a list of quantifiers. The system performs less well with the labels SOME, MOST and ALL.
In order to understand the distribution of errors, we perform a detailed analysis on the first fold of
our data. Out of 100 instances, the classifier assigns 25 to an incorrect class. The majority of those
errors (44%) are due to the fact that the classifier labels all singulars as ONE, missing out on generic
interpretations and in particular on the plural reading of mass terms: out of 11 errors, 5 are linked to
a bare singular). The next most frequent type of error, covering another 16% of incorrectly classified
instances, comes from already quantified noun phrases being labelled as another class. These errors
affect the recall of the QUANT class and the precision of the SOME, MOST and ALL labels in particular
(most of those errors occur in plural noun phrases). The coarseness of the rules is again to blame for
the remaining errors: looking at the decision tree produced by the classifier, we observe that all bare
6http://www.cs.waikato.ac.nz/ml/weka/
171
plurals followed by a present tense, as well as all definite plurals, are labelled as universals, while all
bare plurals followed by a past tense are labelled as SOME. This accounts for a further 7 errors. The last
three incorrect assignments are due to a dubious capitalisation rule.
5.5 Correspondence with linguistics
We observe that most definite plurals (including demonstratives and possessives) are classified as either
MOST or ALL. This fits the linguistic notion of a definite as being essentially universal (Lyons, 1999) but
also misses out on the correct quantification of statements such as 10.
We note also that non-capitalised bare plurals followed by a present tense are similarly classed as
ALL. This echoes the observation that the combination of bare plural and present is a typical manifes-
tation of genericity (if one understands genericity as a quantification phenomenon close to universality).
When followed by past or perfect tenses, an existential quantification with SOME is however preferred.
One of the puzzles opened by the classifier?s decision trees is the use of the direct object feature to
distinguish between MOST and ALL in the case of some definite plurals. Given Sentences 28 and 29, our
classifier would label the first one as ALL and the second one as MOST.
28. My cats like the armchair. ALL
29. My cats like the armchairs. MOST
At first glance, the rule seems to be a mere statistical effect of our data. We will however remark
that statements like 29 are reserved a special section in Link (1998), where they are introduced as ?rela-
tional plural sentences?. One of Link?s claims is that those sentences warrant four collective/distributive
combinations ? as opposed to two only in the case where the object is an individual. So we can say in
Sentence 29 that a collective of cats likes a collective of armchairs, or that this collective of cats likes
each armchair individually, etc. This proliferation of interpretations makes uncertainties more likely with
regard to who likes what, and to the quantification of the subject and object.
For now, we will simply conclude that, although a simple syntax-based classifier is able to classify
certain constructs with high precision, other constructs are beyond its capabilities. Further, it is difficult
to see how improvements can be made to the current classification without venturing outside of the
grammatical context. For instance, it seems practically impossible to improve on the high-precision rule
specifying that every singular noun phrase should be classified as ONE. Due to space constraints, we
will not report any further experiments in this paper. However, preliminary investigations into the use of
lexical similarity to resolve quantification ambiguity can be found in Herbelot (2010).
6 Previous work
The general framework of this proposal is an underspecification account close to that described in Pinkal
(1996) or Egg (2010). Computational approaches to underspecified quantification have so far focused
on the genericity phenomenon. Leaving aside the question of annotation, which is treated in Herbelot
and Copestake (2010), research on genericity can be classified within two strands: theoretical research
on defeasible reasoning and extraction of common sense knowledge. Attempts to model defeasible
reasoning were made in the 1980s with, for instance, the developments of default logic (Reiter, 1980)
and non-monotonic logic (McDermott and Doyle, 1982). With information extraction as aim, Suh et al
(2006) attempt to retrieve ?common sense? statements from Wikipedia. They posit that common sense
is contained in generic sentences. Their system, however, makes simplifying assumptions with regard to
syntax: in particular, all bare plurals (and bare plurals only) are considered generic. In general, common
sense extraction systems tend to restrict the data they mine to avoid the problem of identifying genericity
(e.g., Voelker et al, 2007).
172
7 Conclusion, with some remarks on semantics
We have shown in this paper that subject noun phrases that are not explicitly quantified could be rep-
resented in an underspecified form. We have also argued that this formalisation is applicable to all
constructs, including so-called generics. We have introduced a syntax-based classifier for quantification
resolution and discussed the limits of an approach relying on compositional information only.
We acknowledge that our quantificational account of noun phrases, and especially of generics, does
not satisfy the common requirement that a formalisation be a full description of the semantic particu-
larities of a linguistic phenomenon. We think, however, that this requirement has led to over-restrictive
approaches. One of the debates surrounding generics, for instance, relates to whether they should be
given a ?rules and regulations? or an inductivist truth condition (Carlson, 1995). Our view is that it would
be a mistake to exclude either interpretation. Burton-Roberts? (1977) A gentleman opens doors for ladies
clearly has normative force and without doubt, also allows the hearer to make their own conclusions with
regard to the intersection between the set of all gentlemen and the set of people opening doors for ladies.
Our view of semantics is that it is a layered system and that specifying the quantification semantics
of a noun phrase does not mean providing the full semantics of that noun phrase. It may be argued that
the ideal semantics of generics should be unified and integrate all possible aspects of meaning. But such
a theory is yet to be developed for genericity and, from a computational point of view, may not even
be desirable: a modular representation of meaning allows us to only formalise the aspects that we are
interested in for a particular task, leaving the rest out.
The approach presented here can be said to implement the idea of ?slacker? semantics (Copestake,
2009) in that a) our experiments try to derive a specification from compositional information only and
b) we only attempt to specify one aspect of the meaning of noun phrases (quantification), leaving other
aspects unspecified. In the future, we would like to take away some of the slack in a) by using lexical
semantics in the specification of quantification. In order to do this, a much larger corpus should be
created for the training and testing of the system, and this will be our next task.
References
Briscoe, T., J. Carroll, and R. Watson (2006). The second release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation sessions, Morristown, NJ, USA, pp. 77?80.
Brogaard, B. (2007). The But Not All: A Partitive Account of Plural Definite Descriptions. Mind &
Language 22(4), 402?426.
Burton-Roberts, N. (1977). Generic sentences and analyticity. Studies in Language 1, 155?196.
Carlson, G. N. (1977). Reference to Kinds in English. Ph. D. thesis, University of Massachusetts at
Amherst.
Carlson, G. N. (1995). Truth-Conditions of Generic Sentences: Two Contrasting Views. In G. N. Carlson
and F. J. Pelletier (Eds.), The Generic Book, pp. 224?237. Chicago: University of Chicago Press.
Chierchia, G. (1995). Individual-level predicates as inherent generics. In G. N. Carlson and F. J. Pelletier
(Eds.), The Generic Book, pp. 176?223. Chicago: University of Chicago Press.
Chierchia, G. (1998). Reference to kinds across languages. Natural Language Semantics 6, 339?405.
Cimpian, A. and E. M. Markman (2008). Preschool children?s use of cues to generic meaning. Cogni-
tion 107(1), 19?53.
Cohen, A. (1996). Think Generic: The Meaning and Use of Generic Sentences. Ph. D. thesis, Carnegie-
Mellon University at Pittsburgh.
173
Copestake, A. (2004). Robust Minimal Recursion Semantics. http://www.cl.cam.ac.uk/
?aac10/papers/rmrsdraft.pdf.
Copestake, A. (2009). Slacker semantics : why superficiality , dependency and avoidance of commitment
can be the right way to go. In Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics, Athens, Greece, pp. 1?9.
Dowty, D. (1987). Collective predicates, distributive predicates and all. In F. Marshall, A. Miller, and
Z.-s. Zhang (Eds.), The Third Eastern States Conference on Linguistics, Columbus, pp. 97?115. The
Ohio State University, Department of Linguistics.
Egg, M. (2010). Semantic Underspecification. Language and Linguistics Compass 4(3), 166?181.
Gelman, S. A. (2004). Learning words for kinds: Generic noun phrases in acquisition. In D. Hall and
S. Waxman (Eds.), Weaving a lexicon. Cambridge, MA: MIT Press.
Herbelot, A. (2010). Underspecified quantification. Ph. D. thesis, University of Cambridge.
Herbelot, A. and A. Copestake (2010). Annotating underquantification. In Proceedings of the Fourth
Linguistic Annotation Workshop, Uppsala, Sweden, pp. 73?81.
Heyer, G. (1990). Semantics and Knowledge Representation in the Analysis of Generic Descriptions.
Journal of Semantics 7(1), 93?110.
Krifka, M. (2004). Bare NPs: Kind-referring, Indefinites, Both, or Neither? In O. Bonami and P. Cabredo
Hofherr (Eds.), Empirical Issues in Formal Syntax and Semantics, pp. 111?132.
Krifka, M., F. J. Pelletier, G. N. Carlson, A. ter Meulen, G. Chierchia, and G. Link (1995). Genericity:
An Introduction. In G. N. Carlson and F. J. Pelletier (Eds.), The Generic Book, pp. 1?125. Chicago:
Chicago University Press.
Leslie, S.-J. (2008). Generics: Cognition and Acquisition. Philosophical Review 117(1), 1?47.
Link, G. (1983). The Logical Analysis of Plurals and Mass Terms: a Lattice-Theoretical Approach. In
R. Bauerle, C. Schwarze, and A. von Stechow (Eds.), Meaning, Use, and Interpretation of Language,
pp. 302?323. Berlin: de Gruyter.
Link, G. (1998). Plural. In Algebraic Semantics in Language and Philosophy. Stanford: CSLI Publica-
tions.
Lyons, C. (1999). Definiteness. Cambridge: Cambridge University Press.
McDermott, D. and J. Doyle (1982). Non-monotonic Logic I. Artificial Intelligence 13, 41?72.
Pelletier, F. J. and N. Asher (1997). Generics and Defaults. In J. van Bethem and A. ter Meulen (Eds.),
Handbook of Logic and Language, pp. 1125?1177. Amsterdam: Elsevier.
Pinkal, M. (1996). Radical Underspecification. In P. Dekker and M. Stokhof (Eds.), Proceedings of the
10th Amsterdam Colloquium, Amsterdam, pp. 479?498. de Gruyter.
Quinlan, J. (1993). Programs for Machine Learning. San Francisco, CA: Morgan Kaufmann.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence 13(1-2), 81?132.
Suh, S., H. Halpin, and E. Klein (2006). Extracting Common Sense Knowledge from Wikipedia. In
Proceedings of the International Semantic Web Conference (ISWC-06). Workshop on Web Content
Mining with Human Language Technology, Athens, GA.
Voelker, J., P. Hitzler, and P. Cimiano (2007). Acquisition of OWL DL Axioms from Lexical Resources.
In Proceedings of the Fourth European conference on The Semantic Web: Research and Applications,
Innsbruck, Austria, pp. 670?685. Springer Verlag.
174
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 45?53,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Exciting and interesting: issues in the generation of binomials
Ann Copestake
Computer Laboratory,
University of Cambridge,
15 JJ Thomson Avenue,
Cambridge, CB3 0FD, UK
ann.copestake@cl.cam.ac.uk
Aure?lie Herbelot
Institut fu?r Linguistik,
Universita?t Potsdam,
Karl-Liebknecht-Stra?e 24-25
D-14476 Golm, Germany
herbelot@uni-potsdam.de
Abstract
We discuss the preferred ordering of elements
of binomials (e.g., conjunctions such as fish
and chips, lager and lime, exciting and in-
teresting) and provide a detailed critique of
Benor and Levy?s probabilistic account of En-
glish binomials. In particular, we discuss the
extent to which their approach is suitable as
a model of language generation. We describe
resources we have developed for the investi-
gation of binomials using a combination of
parsed corpora and very large unparsed cor-
pora. We discuss the use of these resources in
developing models of binomial ordering, con-
centrating in particular on the evaluation is-
sues which arise.
1 Introduction
Phrases such as exciting and interesting and gin and
tonic (referred to in the linguistics literature as bi-
nomials) are generally described as having a seman-
tics which makes the ordering of the conjuncts irrel-
evant. For instance, exciting and interesting might
correspond to exciting?(x)? interesting?(x) which is
identical in meaning to interesting?(x)?exciting?(x).
However, in many cases, the binomial is realized
with a preferred ordering, and in some cases this
preference is so strong that the reverse is perceived
as highly marked and may even be difficult to under-
stand. For example, tonic and gin has a corpus fre-
quency which is a very small fraction of that of gin
and tonic. Such cases are referred to as irreversible
binomials, although the term is sometimes used
only for the fully lexicalised, non-compositional ex-
amples, such as odds and ends.
Of course, realization techniques that utilize very
large corpora to decide on word ordering will tend to
get the correct ordering for such phrases if they have
been seen sufficiently frequently in the training data.
But the phenomenon is nevertheless of some practi-
cal interest because rare and newly-coined phrases
can still demonstrate a strong ordering preference.
For instance, the ordering found in the names of
mixed drinks, where the alcoholic component comes
first, applies not just to the conventional examples
such as gin and tonic, but also to brandy and coke,
lager and lime, sake and grapefruit and (hopefully)
unseen combinations such as armagnac and black-
currant.1 A second issue is that data from an un-
parsed corpus can be misleading in deciding on bi-
nomial order. Furthermore, our own interest is pre-
dominantly in developing plausible computational
models of human language generation, and from this
perspective, using data from extremely large cor-
pora to train a model is unrealistic. Binomials are
a particularly interesting construction to look at be-
cause they raise two important questions: (1) to what
extent does lexicalisation/establishment of phrases
play a role in determining order? and (2) is a detailed
lexical semantic classification required to accurately
predict order?
As far as we are aware, the problem of developing
a model of binomial ordering for language genera-
tion has not previously been addressed. However,
Benor and Levy (2006) have published an important
and detailed paper on binomial ordering which we
draw on extensively in this work. Their research
has the objective of determining how the various
constraints which have been proposed in the lin-
guistic literature might interact to determine bino-
1One of our reviewers very helpfully consulted a bartender
about this generalization, and reports the hypothesis that the al-
cohol always comes first because it is poured first. However,
there is the counter-example gin and bitters (another name for
pink gin), where the bitters are added first (unless the drink is
made in a cocktail shaker, in which case ordering is irrelevant).
45
mial ordering as observed in a corpus. We present
a critical evaluation of that work here, in terms of
the somewhat different requirements for a model for
language generation.
The issues that we concentrate on in this paper
are necessary preliminaries to constructing corpus-
based models of binomial reversibility and ordering.
These are:
1. Building a suitable corpus of binomials.
2. Developing a corpus-based technique for eval-
uation.
3. Constructing an initial model to test the evalu-
ation methodology.
In ?2, we provide a brief overview of some of
the factors affecting binomial ordering and discuss
Benor and Levy?s work in particular. ?3 discusses
evaluation issues and motivates some of the deci-
sions we made in deciding on the resources we have
developed, described in ?4. ?5 illustrates the evalua-
tion of a simple model of binomial ordering.
2 Benor and Levy?s account
We do not have space here for a proper discussion of
the extensive literature on binomials, or indeed for a
full discussion of Benor and Levy?s paper (hence-
forth B+L) but instead summarise the aspects which
are most important for the current work.
For convenience, we follow B+L in referring to
the elements of an ordered binomial as A and B.
They only consider binomials of the form ?A and
B? where A and B are of the same syntactic cate-
gory. Personal proper names were excluded from
their analysis. Because they required tagged data,
they used a combination of Switchboard, Brown and
the Wall Street Journal portion of the Penn Treebank
to extract binomials, selecting 411 binomial types
and all of the corresponding tokens (692 instances).
B+L investigate a considerable number of con-
straints on binomial ordering which have been dis-
cussed in the linguistics literature. They group the
features they use into 4 classes: semantic, word
frequency, metrical and non-metrical phonological.
We will not discuss the last class here, since they
found little evidence that it was relevant once the
other features were taken into account. The metri-
cal constraints were lapse (2 consecutive weak syl-
lables are generally avoided), length (A should not
have more syllables than B) and stress (B should not
have ultimate (primary) stress: this feature was actu-
ally found to overlap almost entirely with lapse and
length). The frequency constraint is that B should
not be more frequent than A, based on corpus spe-
cific counts of frequency (unsurprisingly, frequency
correlates with the length feature).
The semantic constraints are less straightforward
since the linguistics literature has discussed many
constraints and a variety of possible generalisations.
B+L use:
Markedness Divided into Relative formal,
which includes cases like flowers and roses
(more general term first) among others and
Perception-based, which is determined by
extra-linguistic knowledge, including cases
like see and hear (seeing is more salient).
B should not be less marked than A. Un-
fortunately markedness is too complex to
summarise adequately here. It is clear that it
overlaps with other constraints in some cases,
including frequency, since unmarked terms
tend to be more frequent.
Iconicity Sequence ordering of events, numbered
entities and so on (e.g., shot and killed, eighth
and ninth). If there is such a sequence, the bi-
nomial ordering should mirror it.
Power Power includes gender relationships (dis-
cussed below), hierarchical relationships (e.g.,
clergymen and parishioners), the ?condiment
rule? (e.g., fish and chips) and so on. B should
not be more powerful than A.
Set Open Construction This is used for certain
conventional cases where a given A may occur
with multiple Bs: e.g., nice and.
Pragmatic A miscellaneous context-dependent
constraint, used, for instance, where the
binomial ordering mirrors the ordering of other
words in the sentence.
B+L looked at the binomials in sentential context
to assign the semantic constraints. The iconicity
46
constraint, in particular, is context-dependent. For
example, although the sequence ninth and eighth
looks as though it violates iconicity, we found that
a Google search reveals a substantial number of in-
stances, many of which refer to the ninth and eighth
centuries BC. In this case, iconicity is actually ob-
served, if we assume that temporal ordering deter-
mines the constraint, rather than the ordering of the
ordinals.
The aspect of binomials which has received most
attention in the literature is the effect of gender:
words which refer to (human) males tend to pre-
cede those referring to females. For instance (with
Google 3-gram percentages for binomials with the
masculine term first): men and women (85%), boys
and girls (80%), male and female (91%) (exceptions
are father and mother (51%) and mothers and fa-
thers (33%)). There is also an observed bias towards
predominantly male names preceding female names.
B+L, following previous authors, take gender as an
example of the Power feature. For reasons of space
we can only touch on this issue very superficially,
but it illustrates a distinction between semantic fea-
tures which we think important. Iconicity generally
refers to a sequence of real world events or enti-
ties occuring in a particular order, hence its context-
dependence. For verbs, at least, there is a truth con-
ditional effect of the ordering of the binomial: shot
and killed does not mean the same thing as killed
and shot. Power, on the other hand, is supposed to
be about a conventional relationship between the en-
tities. Even if we are currently more interested in
chips rather than fish or biscuits rather than tea, we
will still tend to refer to fish and chips and tea and
biscuits. The actual ordering may depend on cul-
ture,2 but the assumption is that, within a particular
community, the power relationship which the bino-
mial ordering depends on is fixed.
B+L analyse the effects of all the features in de-
tail, and look at a range of models for combining
features, with logistic regression being the most suc-
cessful. This predicts the ordering of 79.2% of the
binomial tokens and 76.7% of the types. When se-
mantic constraints apply, they tend to outrank the
metrical constraints. B+L found that iconicity, in
2Our favourite example is an English-French parallel text
where the order of Queen Elizabeth and President Mitterand is
reversed in the French.
particular, is a very strong predictor of binomial or-
der.
B+L?s stated assumption is that a speaker/writer
knows they want to generate a binomial with the
words A and B and decides on the order based on
the words and the context. It is this order that they
are trying to predict. Of course, it is clear that some
binomials are non-compositional multiword expres-
sions (e.g., odds and ends) which are listed in con-
ventional dictionaries. These can be thought of as
?words with spaces? and, we would argue that the
speaker does not have a choice of ordering in such
cases. B+L argue that using a model which listed
the fixed phrases would be valid in the prediction of
binomial tokens, but not binomial types. We do not
think this holds in general and return to the issue in
?3.
B+L?s work is important in being the first account
which examines the effect of the postulated con-
straints in combination. However, from our perspec-
tive (which is of course quite different from theirs),
there are a number of potential problems. The first is
data sparsity: the vast majority of binomial types in
their data occur only once. It is impossible to know
whether both orderings are frequent for most types.
Furthermore, the number of binomial types is rather
small for full investigation of semantic features: e.g.,
Power is marked on only 26 types. The second is-
sue is that the combined models which B+L exam-
ine are, in effect, partially trained on the test data, in
that the relative contribution of the various factors is
optimized on the test data itself. Thirdly, the seman-
tic factors which B+L consider have no independent
verification: they were assigned by the authors for
the binomials under consideration, a methodology
which makes it impossible to avoid the possibility of
bias. There was some control over this, in that it was
done independently by the two authors with subse-
quent discussion to resolve disagreements. How-
ever, we think that it would be hard to avoid the
possibility of bias in the ?Set open? and ?Pragmatic?
constraints in particular. Some of the choices seem
unintuitive: e.g., we are unsure why there is a Power
annotation on broccoli and cauliflower, and why go
and vote would be marked for Iconicity while went
and voted is not. It seems to us that the defini-
tion of some of these semantic factors in the liter-
ature (markedness and power in particular) is suf-
47
ficiently unclear for reproducible annotation of the
type now expected in computational linguistics to be
extremely difficult.
Both for practical and theoretical reasons, we are
interested in investigating alternative models which
rely on a corpus instead of explicit semantic fea-
tures. Native speakers are aware of some lexicalised
and established binomials (see (Sag et al 2002) for a
discussion of lexicalisation vs establishment in mul-
tiword expressions), and will tend to generate them
in the familiar order. Instead of explicit features be-
ing learned for the unseen cases, we want to investi-
gate the possible role of analogy to the known bino-
mials. For instance, if tea and biscuits is known,
coffee and cake might be generated in that order-
ing by semantic analogy. The work presented in
this paper is essentially preparatory to such experi-
ments, although we will discuss an extremely simple
corpus-based model in ?5.
3 Evaluating models of binomial ordering
In this section, we discuss what models of binomial
ordering should predict and how we might evaluate
those predictions.
The first question is to decide precisely what we
are attempting to model. B+L take the position that
the speaker/writer has in mind the two words of the
binomial and chooses to generate them in one order
or other in a particular context, but this seems prob-
lematic for the irreversible binomials and, in any
case, is not directly testable. Alternatively we can
ask: Given a corpus of sentences where the binomi-
als have been replaced with unordered pairs of AB,
can we generate the ordering actually found? Both
of these are essentially token-based evaluations, al-
though we could additionally count binomial types,
as B+L do.
One problem with these formulations is that, to do
them justice, our models would really have to incor-
porate features from the surrounding context. Fac-
tors such as postmodification of the binomial affect
the ordering. This type of evaluation would clearly
be the right one if we had a model of binomials in-
corporated into a general realisation model, but it is
not clear it is suitable for looking at binomials in iso-
lation.
Perhaps more importantly, to model the irre-
versible or semi-irreversible binomials, we should
take into account the order and degree of reversibil-
ity of particular binomial types. It seems problem-
atic to formulate the generation of a lexicalised bino-
mial, such as odds and ends, as a process of deciding
on the order of the components, since the speaker
must have the term in mind as a unit. In terms
of the corpus formulation, given the pair AB, the
first question in deciding how to realise the phrase
is whether the order is actually fixed. The case
of established but compositional binomials, such as
fish and chips, is slightly less clear, but there still
seem good grounds for regarding it as a unit (Cruse,
1986). Furthermore, in evaluating a token-based re-
alisation model, we should not penalise the wrong
ordering of a reversible binomial as severely as if
the binomial were irreversible. From these perspec-
tives, developing a model of ordering of binomial
types should be a preliminary to developing a model
of binomial tokens. Context would be important in
properly modelling the iconicity effect, but is less
of an issue for the other ordering constraints. And
even though iconicity is context-dependent, there is
a very strongly preferred ordering for many of the
binomial types where iconicity is relevant.
Thus we argue that it is appropriate to look at the
question: Given two words A, B which can be con-
joined, what order do we find most frequently in a
corpus? Or, in order to look at degree of reversibil-
ity: What proportion of the two orderings do we find
in a corpus? This means that we require relatively
large corpora to obtain good estimates in order to
evaluate a model.
Of course, if we are interested in analogical mod-
els of binomial ordering, as mentioned at the end of
?2, we need a reasonably large corpus of binomials
to develop the model. Ideally this should be a dif-
ferent corpus from the one used for evaluation. We
note that some experiments on premodifier order-
ing have found a considerable drop in performance
when testing on a different domain (Shaw and Hatzi-
vassiloglou, 1999). Using a single corpus split into
training and test data would, of course, be problem-
atic when working with binomial types. We have
thus developed a relatively novel methodology of us-
ing an automatically parsed corpus in combination
with frequencies from Web data. This is discussed
in the next section.
48
4 Binomial corpora and corpus
investigation
In this section, we describe the resources we have
developed for investigating binomials and address-
ing some of the evaluation questions introduced in
the previous section. We then present an initial anal-
ysis of some of the corpus data.
4.1 Benor and Levy data
The appendix of B+L?s paper3 contains a list of the
binomials they looked at, plus some of their markup.
Although the size of the B+L dataset is too small
for many purposes, we found it useful to consider
it as a clean source of binomial types for our initial
corpus investigation and evaluation. We produced a
version of this list excluding the 10 capitalised ex-
amples: some of these seem to arise from sentence
initial capitals while others are proper names which
we decided to exclude from this study. We produced
a manually lemmatised version of the list, which re-
sults in a slightly reduced number of binomial types:
e.g., bought and sold and buy and sell correspond to
a single type. The issue of lemmatisation is slightly
problematic in that a few examples are lexicalised
with particular inflections, such as been and gone.
However, our use of parsed data meant that we had
to use lemmatization decisions which were compat-
ible with the parser.
4.2 Wikipedia and the Google n-gram corpus
In line with B+L, we assume that binomials are
made of two conjuncts with the same part of speech.
It is not possible to use an unparsed corpus to ex-
tract such constructions automatically: first, the raw
text surrounding a conjunction may not correspond
to the actual elements of the coordination (e.g., the
trigram dictionary and phrase in She bought a dic-
tionary and phrase book); second, the part of speech
information is not available. Using a parsed corpus,
however, has disadvantages: in particular, it limits
the amount of data available and, consequently, the
number of times that a given type can be observed.
In this section, we discuss the use of Wikipedia,
which is small enough for parsing to be tractable but
3http://idiom.ucsd.edu/?rlevy/papers/
binomials-sem-alpha-formatted
which turns out to have a fairly representative distri-
bution of binomials. The latter point is demonstrated
by comparison with a large dataset: the Google n-
gram corpus (Brants and Franz, 2006). Although
the Google data is not suitable for the actual task
of extracting binomials, because it is not parsed, we
hypothesize it is usable to predict the preferred or-
der of a given binomial and to estimate the extent to
which it is reversible.
In order to build a corpus of binomials, we process
the parsed Wikipedia dump produced by Kummer-
feld et al(2010). The parse consists of grammatical
relations of the following form:
(gr word1 x word2 y ... wordn z)
where gr is the name of the grammatical relation,
word1...n are the arguments of the relation, and
x, y...z are the positions of the arguments in the sen-
tence. The lemmatised forms of the arguments, as
well as their part of speech, are available separately.
We used the first one million and coordinations in
the corpus in these experiments. The conjuncts are
required to have the same part of speech and to di-
rectly precede and follow the coordination. The lat-
ter requirement ensures that we retrieve true binomi-
als (phrases, as opposed to distant coordinates). For
each binomial in this data, we record a frequency
and whether it is found in the reverse order in the
same dataset. The frequency of the reverse ordering
is similarly collected. Since we intend to compare
the Wikipedia data to a larger, unparsed corpus, we
merge the counts of all possible parts of speech for
a given type in a given ordering, so the counts for
European and American as nouns and as adjectives,
for instance, are added together. We also record
the preferred ordering (the one with the highest fre-
quency) of the binomial and the ratio of the frequen-
cies as an indication of (ir)reversibility. In line with
our treatment of the B+L data, we disregarded the
binomials that coordinate proper names, but noted
that a large proportion of proper names found in
the Wikipedia data cannot be found in the Google
data.4 The Google corpus also splits (most) hyphen-
4This suggests that the Google n-gram corpus does not con-
tain much (if any) of the Wikipedia data: the particular dump
of Wikipedia from which the parsed data is extracted being in
any case several years later than the date that the Google n-gram
corpus was produced.
49
ated words. Since hyphenation is notoriously irreg-
ular in English, we disregarded all binomials con-
taining hyphenated words. The resulting data con-
tains 279136 unique binomial types. Around 7600
of those types have a frequency of 10 or more in our
Wikipedia subset. As expected, this leaves a large
amount of data with low frequency.
We then attempt to verify how close the sparse
Wikipedia data is to the Google 3-gram corpus. For
each binomial obtained from Wikipedia, we retrieve
the frequency of both its orderings in the Google
data and, as before, calculate the ratio of the frequen-
cies in the larger corpus. The procedure involves
converting the lemmatised forms in the Wikipedia
parse back into surface forms. Rather than using
a morphological generator, which would introduce
noise in our data, we search for the surface forms as
they appeared in the original Wikipedia data, as well
as for the coordinated base forms (this ensures high
recall in cases where the original frequency is low).
So for example, given the one instance of the bino-
mial ?sadden and anger? in Wikipedia, appearing as
Saddened and angered in the corpus, we search for
Saddened and angered, sadden and anger and anger
and sadden.
Around 30% of the Wikipedia binomials are not
in the Google data. We manually spot checked a
number of those and confirmed that they were un-
available from the Google data, regardless of inflec-
tion. Examples of binomials not found in the n-gram
corpus include dagger and saber, sagacious and
firm and (rather surprisingly) gay and flamboyant.
19% of the Wikipedia binomials have a different
preferred order in the Google corpus. As expected,
most of those have a low frequency in Wikipedia.
For the binomials with an occurrence count over 40,
the agreement on ordering is high (around 96%).
Furthermore, many of those disagreements are not
?real? in that they concern binomials found with a
high dispreferred to preferred order ratio. Disre-
garding cases where this ratio is over 0.3 lowers the
initial disagreement figure to 7%. We will argue in
?4.4 that true irreversibility can be shown to roughly
correspond to a ratio of 0.1. At this cutoff, the per-
centage of disagreements between the two corpora
is only 2%. Thus we found no evidence that the
encyclopaedic nature of Wikipedia has a significant
skewing effect on the frequencies. We thus believe
that Wikipedia is a suitable dataset for training an
automatic binomial ordering system.
4.3 Lexicalisation
Our basic methodology for investigation of lexi-
calisation was to check online dictionaries for the
phrases. However, deciding whether a binomial
should be regarded as a fixed phrase is not entirely
straightforward. For instance, consider warm and
fuzzy. At first sight, it might appear compositional,
but the particular use of fuzzy, referring to feelings,
is not the usual one. While warm and fuzzy is not
listed in most dictionaries we have examined, it has
an entry in the Urban Dictionary5 and is used in ex-
amples illustrating that particular usage of fuzzy in
the online Merriam-Webster.6 Another case from
the B+L data is nice and toasty, which again is used
in a Merriam-Webster example.7
We therefore used a manual search procedure
to check for lexicalisation of the B+L binomials.
We used a broad notion of lexicalisation, treat-
ing a phrase as lexicalised if it occurred as an en-
try in one or more online English dictionaries us-
ing Google search. We included a few phrases as
semi-lexicalised when they were given in examples
in dictionaries produced by professional lexicogra-
phers, but this was, to some extent, a subjective
decision. Since such a search is time-consuming,
we only checked examples which one of us (a na-
tive British English speaker) intuitively considered
might be lexicalised. We first validated that this
would not cause too great a loss of recall by check-
ing a small subset of the B+L data exhaustively: this
did not reveal any additional examples.
Using these criteria, we found 39 lexicalised bi-
nomial types in the B+L data, of which 7 were
semi-lexicalised.8 The phrases backwards and for-
wards, backward and forward, day and night, salt
and pepper and in and out are lexicalised (or semi-
lexicalised) in both orders.
5http://www.urbandictionary.com/
6http://www.merriam-webster.com/
7The convention of indicating semi-fixed phrases in exam-
ples is quite common in lexicography, especially in dictionaries
intended for language learners.
8There are 40 tokens, because cut and dry and cut and dried
are both lexicalised. An additional example, foot-loose and
fancy-free, might be included, but we did not find it in any dic-
tionary with that hyphenation.
50
4.4 Reversibility and corpus evidence
There are a number of possible reasons why a partic-
ular binomial type AB might (almost) always appear
in one ordering (A and B or B and A):
1. The phrase A and B (B and A) might be fully
lexicalised (word with spaces).
2. The binomial might have a compositional
meaning, but have a conventional ordering. A
particular binomial AB might be established
with that ordering (e.g., gin and tonic is es-
tablished for most British and American speak-
ers) or might belong to a conventional pattern
(e.g., armagnac and blackcurrant, sole and ar-
tichokes).
3. The binomial could refer to a sequence of real
world events or entities which almost invari-
ably occur in a particular order. For example,
shot and killed has a frequency of 241675 in
the Google 3-gram corpus, as opposed to 158
for killed and shot. This ratio is larger that that
of many of the lexicalised binomials.
Relatively few of the binomials from the B+L data
are completely irreversible according to the Google
3-gram data. There are instances of the reverse of
even obviously fixed phrases, such as odds and ends.
Of course, there is no available context in the 3-gram
data, but we investigated some of these cases by on-
line search for the reversed phrases. This indicates
a variety of sources of noise, including wordplay
(e.g., Beckett?s play Ends and Odds), different word
senses (e.g., toasty and nice occurs when toasty is
used to describe wine) and false positives from hy-
phenated words etc.
We can obtain a crude estimate of extent to which
binomials which should be irreversible actually turn
up in the ?wrong? order by looking at the clearly lex-
icalised phrases discussed in ?4.3. Excluding the
cases where both orders are lexicalised, the mean
proportion of inverted cases is about 3%. There are
a few outliers, such as there and back and now and
then which have more than 10% inverted: however,
these all involve very frequent closed class words
which are more likely to show up in spurious con-
texts. We therefore tentatively conclude that up to
10% of the tokens of a open-class irreversible bino-
mial could be inverted in the 3-gram corpus, but that
we can take higher ratios as evidence for a degree of
genuine reversibility.
5 An initial model
We developed an initial n-gram-based model for or-
dering using the Wikipedia-derived counts. The ap-
proach is very similar to that presented in (Malouf,
2000) for adjective ordering. We use the observed
order of binomials where possible and back off to
counts of a lexeme?s position as first or second con-
junct over all binomials (i.e., we use what Malouf
refers to as positional probabilities).
To be more precise, assume that the task is to pre-
dict the order a ? b or b ? a for a given lexeme pair
a,b. We use the notation C(a and b) and C(b and a)
to refer to the counts in a given corpus of the two
orderings of the binomial (i.e., we count all inflec-
tions of a and b). C(a and) refers to the count of all
binomials with the lexeme a as the first conjunct,
C(and a) all binomials with a as the second con-
junct, and so on. We predict a ? b
if C(a and b) > C(b and a)
or C(a and b) = C(a and b)
and
C(a and)C(and b) > C(b and)C(and a)
and conversely for b ? a. Most of the cases where
the condition C(a and b) = C(a and b) is true occur
when C(a and b) = C(a and b) = 0 but we also
use the positional probabilities to break ties in the
counts. We could, of course, define this in terms of
probability estimates and investigate various forms
of smoothing and interpolation, but for our initial
purposes it is adequate to see how this very simple
model behaves.
We obtained counts for the model from the
Wikipedia-derived data and evaluated it on the bino-
mial types derived from B+L (as described in ?4.1).
There were only 9 cases where there was no pre-
diction, so for the sake of simplicity, we default to
alphabetic ordering in those cases. In Table 1, we
show the results evaluating against the B+L major-
ity decision and against the Google 3-gram majority.
Because not all the B+L binomials are found in the
Google data, the numbers of binomial types evalu-
ated against the Google data is slightly lower. In
51
addition to the overall figures, we also show the rela-
tive accuracy of the bigram prediction vs the backoff
and the different accuracies on the lexicalised and
non-lexicalised data. In Table 2, we group the re-
sults according to the ratio of the less frequent order
in the Google data and by frequency.
Unsurprisingly, performance on more frequent bi-
nomials and lexicalised binomials is better and the
bigram performance, where available, is better than
the backoff to positional probabilities. The scores
when evaluated on the Google corpus are generally
higher than those on the B+L counts, as expected
given the noise created by the data sparsity in B+L
combined with the effect of frequency.
One outcome from our experiments is that it does
not seem essential to treat the lexicalised examples
separately from the high frequency, low reversibil-
ity cases. Since determining lexicalisation is time-
consuming and error-prone, this is a useful result.
The model described does not predict whether or
not a given binomial is irreversible, but our analy-
sis of the data strongly suggests that this would be
important in developing more realistic models. An
obvious extension would be to generate probability
estimates of orderings and to compare these with the
observed Google 3-gram data.
Although n-gram models are completely stan-
dard in computational linguistics, their applicabil-
ity to modelling human performance on a task is
not straightforward. Minimally, if we were to pro-
pose that humans were using such a model as part
of their decision on binomial ordering, it would be
necessary to demonstrate that the counts we are re-
lying on correspond to data which it is plausible to
assume that a human could have been exposed to.
This is not a trivial consideration. We would, of
course, expect to obtain higher scores on this task by
using counts derived from the Google n-gram cor-
pus rather than from Wikipedia, but this would be
completely unrealistic from a psycholinguistic per-
spective. We should emphasize, therefore, that the
model presented here is simply intended as an initial
exercise in developing distributional models of bi-
nomial ordering, which allows us to check whether
the resources we have developed might be an ade-
quate basis for more serious modelling and whether
the evaluation schemes are reasonable.
6 Conclusion
We have demonstrated that we can make use of a
combination of corpora to build resources for devel-
opment and evaluation of models of binomial order-
ing.9 One novel aspect is our use of an automatically
parsed corpus, another is the use of combined cor-
pora. If binomial ordering is primarily determined
by universal linguistic factors, we would not expect
the relative frequency to differ very substantially be-
tween large corpora. The cases where we did ob-
serve differences in preferred ordering between the
Wikipedia and Google data are predominantly ones
where the Wikipedia frequency is low or the bino-
mial is highly reversible. We have investigated sev-
eral properties of binomials using this data and pro-
duced a simple initial model. We tested this on the
relatively small number of binomials used by Benor
and Levy (2006), but in future work we will evalu-
ate on a much larger subset of our corpus. Our in-
tention is to develop further models which use anal-
ogy (morphological and distributional semantic sim-
ilarity) to known binomials to predict degree of re-
versibility and ordering. This will allow us to inves-
tigate whether human performance can be modelled
without the use of explicit semantic features.
We briefly touched on Malouf?s (2000) work on
prenominal adjective ordering in our discussion of
the initial model. There are some similarities be-
tween these tasks, and in fact adjectives in binomials
tend to occur in the same order when they appear as
prenominal adjectives (e.g., cold and wet and cold
wet are preferred over the inverse orders). However,
the binomial problem is considerably more complex.
Binomials are much more variable because they in-
volve all the main syntactic categories. Furthermore,
adjective ordering is considerably easier to investi-
gate because an unparsed corpus can be used, the se-
mantic features which have been postulated are more
straightforward than for binomials and lexicalisation
of adjective sequences is not an issue. We hypoth-
esize that it should be possible to develop similar
analogical models for adjective ordering and bino-
mials which could be relevant for other construc-
tions where ordering is only partially determined
by syntax. In the long term, we would like to in-
9Available from http://www.cl.cam.ac.uk/
research/nl/nl-download/binomials/
52
n B+L n Google accuracy B+L (%) accuracy Google (%)
Overall 380 305 69 79
Bigram 187 185 79 89
Pos Prob 184 117 61 65
Unknown 9 3 33 0
Lexicalised 34 34 87 94
Non-lexicalised 346 271 67 77
Table 1: Evaluation of initial model, showing effects of lexicalisation. (n B+L and n Google indicates the number of
binomial types evaluated)
n accuracy B+L (%) accuracy Google (%)
Google count 0 75 59 -
1?1000 71 56 68
1001?10000 81 70 67
> 10000 153 80 91
Google ratio 0 11 64 64
0?0.1 41 94 93
0.1?0.25 33 75 85
> 0.25 220 68 76
Table 2: Evaluation of initial model, showing effects of frequency and reversibility.
vestigate using such models in conjunction with a
grammar-based realizer (cf (Velldal, 2007), (Cahill
and Riester, 2009)). However, for an initial inves-
tigation of the role of semantics and lexicalisation,
looking at the binomial construction in isolation is
more tractable.
Acknowledgments
This work was partially supported by a fellowship
to Aure?lie Herbelot from the Alexander von Hum-
boldt Foundation. We are grateful to the reviewers
for their comments.
References
Sarah Benor and Roger Levy. 2006. The Chicken or the
Egg? A Probabilistic Analysis of English Binomials.
Language, 82 233?78.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram Corpus Version 1.1. LDC2006T13.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pp.
817-825, Suntec, Singapore. Association for Compu-
tational Linguistics.
D. Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Jonathan K. Kummerfeld, Jessika Rosener, Tim Daw-
born, James Haggerty, James R. Curran, Stephen
Clark. 2010. Faster parsing by supertagger adapta-
tion Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, Uppsala,
Sweden, pages 345?355
Rob Malouf. 2000. The order of prenominal adjectives
in natural language generation. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2000), Hong Kong.
Ivan Sag, Tim Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLING 2002), pages 1?15,
Mexico City, Mexico.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 135?143, College Park, Maryland.
Eric Velldal. 2007. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
53
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 45?54,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Distributional techniques for philosophical enquiry
Aure?lie Herbelot
Institut fu?r Linguistik
Universita?t Potsdam
Karl-Liebknecht-Str. 24-25
14476 Golm, Germany
aurelie.herbelot@cantab.net
Eva von Redecker
Institut fu?r Philosophie
Humboldt Universita?t
Unter den Linden 6
10099 Berlin, Germany
redeckee@hu-berlin.de
Johanna Mu?ller
Institut fu?r Philosophie
Humboldt Universita?t
Unter den Linden 6
10099 Berlin, Germany
johannammueller@gmail.com
Abstract
This paper illustrates the use of distribu-
tional techniques, as investigated in compu-
tational semantics, for supplying data from
large-scale corpora to areas of the human-
ities which focus on the analysis of con-
cepts. We suggest that the distributional no-
tion of ?characteristic context? can be seen
as evidence for some representative tenden-
cies of general discourse. We present a case
study where distributional data is used by
philosophers working in the areas of gen-
der studies and intersectionality as confir-
mation of certain trends described in pre-
vious work. Further, we highlight that dif-
ferent models of phrasal distributions can
be compared to support the claim of inter-
sectionality theory that ?there is more to a
phrase than the intersection of its parts?.
1 Introduction
Research in the social sciences rely heavily on lin-
guistic analysis. Since what came to be called the
?linguistic turn? (Rorty, 1967) researchers across
all humanities subjects have been highly aware of
the fact that our access to the world, let alne cul-
tural artefacts, is mediated by language and cast
in our conceptual scheme.
Guided by the theory originating in Wittgen-
stein?s Philosophical Investigations (1953), one
of the basic assumptions in contemporary analytic
philosophy is now that the meaning of words is
given by their usage in ordinary language. Con-
ceptual analysis, i.e. the process of making ex-
plicit the rules that guide the applicability of a cer-
tain term, consequently forms a major occupation
of the philosophical profession. The method ex-
emplified by the French philosopher Michel Fou-
cault ? discourse analysis ? has become paradig-
matic in the social sciences and cultural studies
and constitutes a diachronic, historical version of
the linguistic turn. One of the fundamental as-
sumptions of this approach is that different eras
produce different frameworks (or ?episteme? in
Foucault?s terminology; see Foucault 1970) for
understanding reality. Such frameworks manifest
themselves as discursive patterns or specific for-
mulations. According to Foucault, social power
and the silencing of deviating utterances guaran-
tee the temporary stability of a particular social
regime. A similar methodology was pursued by
the ?Cambridge School? of political theorists and
historians, who tried to trace back the emergence
of concepts like ?state? or ?liberty? not only to the
ideas of a few canonical thinkers but to the or-
dinary use of those terms at the time (hence this
approach is called ?contextualism?, see Pocock
1975; Skinner 1998).
So far, such research has relied both on ex-
tensive manual work and on linguistic introspec-
tion. Manual methods, however, have clear draw-
backs: they are time-consuming, expensive and
likely to introduce bias in the data. This paper
suggests that distributional techniques, as used
in computational lexical semantics, may hold the
key to automating the process of discourse analy-
sis just described. We present a case study in phi-
losophy, where two standard problems (the anal-
ysis of power in gender structures and the is-
sue of so-called intersectionality) are reviewed
in the light of distributional data. Not only do
the produced distributions offer a rational way
to highlight characteristic relationships between
concepts, using an amount of data far greater than
what could be annotated manually, but we show
45
that building on the relatively recent and novel re-
search in composing distributions (Clark and Pul-
man, 2007), we can computationally illustrate the
main thesis advocated by researchers on intersec-
tionality.
This paper has a slightly unconventional for-
mat. We hope to exemplify a certain type of possi-
ble collaboration between computational linguis-
tics and the humanities, which is less about pro-
viding an application to solve a particular prob-
lem than about drawing parallels between certain
linguistic representations, known to have certain
properties, and humanities-based theories. We
felt that a fair amount of philosophical back-
ground was needed to show the relevance of our
system?s data to the particular type of investiga-
tion presented here. Therefore, a comprehensive
philosophical introduction is given in ?2. We then
describe the system and corpus underpinning our
research (?3 and 4) and discuss the theoretical as-
pects of lexical composition from a computational
point of view, drawing parallels with the philo-
sophical theory of intersectionality. Sections 5
and 6 discuss the worth of the data from a philo-
sophical point of view.
2 Two philosophical problems
2.1 Gender and power
For a feminist philosopher, as for many people
working in critical theory, the aim of research
is twofold. First, to understand a given social
structure, as for example the dynamics of gen-
der relations and identities; second, to transform
that structure towards greater freedom and equal-
ity. From its formation as an academic discipline
in the second half of the 20th century onwards,
one of the main concerns of feminist theory has
been to show how social and institutional (man-
made) factors have shaped what we are some-
times inclined to see as ?normal? or ?natural? gen-
der identities. This approach is called social con-
structivism, because it ascribes the causal role for
how gender identities emerge to processes of so-
cial construction. A prototypical example of this
viewpoint, Simone de Beauvoir?s famous claim
that one is not born a woman but becomes one
(Beauvoir, 1949) led to the distinction between bi-
ological ?sex? and social ?gender?. Such work has
created an interest in the historical contingencies
which decide what counts as a properly mascu-
line or feminine identity. But while competing bi-
ologistic explanations of gender differences natu-
rally have something they can point to (neurones,
genes, or anatomy) social constructivist theories
have sometimes lacked hard evidence for their
claims. As a result, there has been a constant re-
currence of theories claiming one natural cause
for all aspects of gendered behaviour ? and this,
despite the fact that every single one has been
proved wrong by the scientific community (Fine,
2010).
This state of affairs delineates a desideratum
for feminist philosophy: giving more evidence
of the cultural factors which instill gendered be-
haviour and associations in humans. While a lot
of the cultural information concerning gender is
visual (cinema, magazine covers, advertisement,
etc), and the ways to initiating people into specific
codes of gendered behavior can be non-verbal, the
content of our notions of gender are vastly repre-
sented in text. Following this insight, Simone de
Beauvoir reviewed a corpus of five modern nov-
els to extract the characteristic aspects of what she
coined the ?condition feminine? (Beauvoir, 1949).
More recently, Judith Butler (1990) tried to ex-
plain how the use of certain concepts ? gender,
sex, desire, sexual practice ? and associated no-
tions were consolidating the dominant, binary dis-
tinction between masculinity and femininity.
The studies just mentioned, though central to
their field, can always be met with suspicion. Ob-
jections such as This is not how I use the word
or You simply looked at books that distorted the
understanding of the phenomenon in question can
only be met on the grounds of large-scale data.
As a result, there have been attempts in historical
research to produce statistical databases on gen-
dered distributions. Hausen (1976), for example,
focused on the turn of the 18th century in Ger-
many, when a particular modern bourgeois under-
standing of gender roles is said to have emerged
together with a new organisation of labour. How-
ever, the manual tasks involved in preparing such
data are time-consuming and tedious and conse-
quently, this type of work still has a limited cov-
erage. We want to argue in this paper that entrust-
ing the production of such resources to computa-
tional corpus linguistics would a) provide the on-
going philosophical investigation with an appro-
priate amount of data and b) help overcome the
issues linked to the selective nature of the sources
46
a human reader might choose as relevant.
2.2 Intersectionality
Coined by the legal scientist Kimberle? Crenshaw
(1991), the term intersectionality has spread into
the humanites and social sciences as a perspec-
tive which has widened the scope of research on
inequality and oppression within social contexts.
Scientists working with the intersectional per-
spective claim that the combination (intersection)
of various forms of inequality (for example be-
ing black in a white dominated environment or
being a women in an environment dominated by
men) makes a qualitative difference not only to
the self-perception/identity of social actors, but
also to the way they are addressed through poli-
tics, legislation and other instiutions (Ngan-Ling
Chow, 2011).
The founding case out of which Kimberle?
Crenshaw developed the concept of intersection-
ality was a law suit that black women filed against
the hiring policy of General Motors. Within
the traditionally race and gender segregated au-
tomobile industry, women were only allowed to
work in customer service or other office jobs
while African-Americans were confined to fac-
tory work. As a consequence, African-American
women faced the problem of being denied both
office jobs and factory work. The women filed a
lawsuit for discrimination. But the case was dis-
missed on the grounds that the plaintiffs hadn?t
been able to proof that they had been discrim-
inated for either racial or sexist reasons. This
case demonstrated our general additive under-
standing of discrimination: we act against sex-
ism and we act against racism but we fail to ad-
dress cases where they interact. The court was un-
able to take this interaction into account and Cren-
shaw made the case for a reform of the US anti-
discrimination-law, which was based on the situ-
ation of white women in gender-dependent cases
and of men in race-dependent cases.
So the aim of intersectionality is to localise
and make visible discrimination that traditional
thought cannot conceive of, in particular where
various forms of oppression or inequality over-
lap. The claim is that social categories can only
be explained in relation to other categories that
define us as social beings within a society. For
instance, what it means to be in the situation of
a black woman can only be gauged in relation to
the political, cultural, socio-economic, religious,
etc. background of that woman, and not with
respect to being a woman or being a person of
colour in isolation. Indeed, these different fac-
tors can override, exacerbate, conflict with each
other, or simply run in parallel when they come
to interact. Additionally, intersectionality also re-
minds us that the meaning of social categories
also depends on historical eras. This proliferation
of factors increases the complexity a researcher
is confronted with to a point where it is doubt-
ful whether the intersectional perspective can be
transformed into a manageable methodology at
all. Dealing with this level of complexity using
automatically produced data might be of help on
two levels:
1. the synchronic level, at which the intersec-
tionality researcher seeks to grasp the qual-
itative differences between the concepts of,
say, woman and black woman.
2. the diachronic level, at which historians
working with intersectionality research con-
ceptual change over texts from various eras.
In this work, we concentrate on the conceptual
aspects of the synchronic level. In what follows,
we will attempt to show that the intersectional ap-
proach can indeed be illustrated by the linguistic
data obtained from a large contemporary corpus.
3 A distributional semantics system
3.1 Distributional semantics
Presented as a complement to model-theoretic se-
mantics, distributional semantics aims to repre-
sent lexical meaning as a function of the con-
texts in which a given word appears (Wittgen-
stein, 1953; see also Harris, 1954, credited with
the ?distributional hypothesis? which states that
words which are similar in meaning occur in sim-
ilar contexts).
Following this idea, some work in computa-
tional linguistics (starting with Harper, 1965) has
been devoted to building and evaluating models
which represent words as distributions, i.e., vec-
tors in a multidimensional space where each di-
mension corresponds to a potential context for a
lexical item (Curran, 2003). The notion of con-
text itself has been studied to try and determine
which representations work best for various tasks.
47
Word windows (Lund and Burgess, 1996), depen-
dencies (Pado? and Lapata, 2007) and syntactic re-
lations (Grefenstette, 1994) have been proposed.
In our work, we use as context the words
appearing in the same sentence as the query.
This simple model is attractive from the point
of view of using distributional techniques across
the wide range of texts considered by humani-
ties researchers. It ensures that a corpus is pro-
cessable as long as it is digitalised ? regardless
of the language it is written in and the era it be-
longs to. Given that resources for parsing rare
languages and older states of modern languages
are still scarce, the word-based model has the ad-
vantage of flexibility.
Another issue in producing distributions relates
to weighing the various dimensions. A number of
possibilities have been suggested. Binary mod-
els attribute a weight of 1 to a context if it co-
occurs at least once with the term that the distribu-
tion must represent, and 0 otherwise. Frequency-
based models use as weights the number of co-
occurrences of a particular context with the term
under consideration. More complex models use
functions like mutual information, which attempt
to represent how ?characteristic? a particular con-
text is for the term rather than how ?frequent?
it is in conjunction with that term. The notion
of a characteristic context is particularly impor-
tant to us, as we wish to provide conceptual rep-
resentations (distributions) which mirror what a
?standard? individual would associate with a given
word. To achieve this, frequency models are not
sufficient. Words like do, also, new, etc co-occur
with many terms but are in no meaningful rela-
tion with those terms. Instead, we want to choose
a function which gives high weights to contexts
that appear frequently with the term to be mod-
elled and not very frequently with other terms.
By doing this, we will have a way to describe
salient associations for a particular concept. In
?3.3, we will spell out such a function, borrowed
from Mitchell and Lapata (2010).
3.2 Intersectionality in linguistic terms
It has been suggested that in order to integrate
distributional semantics with model theoretic for-
malisms, methods should be found to compose
the distributions of single words (Clark and Pul-
man, 2007). It is clear that the representation of
carnivorous mammal in formal semantics can be
written as carnivorous?(x)?mammal?(x) but it is
less clear how the lexical semantics of the phrase
should be described in distributional terms.
The work done so far on distributional com-
positionality has focused on finding equivalents
for the well-known formal semantics notion of
intersection. All models assume that the inter-
sective composition of two elements should re-
turn a distribution, i.e. a lexical meaning, which
is made of the individual distributions, or mean-
ings, of those elements. But there are differences
in how those models are evaluated. Two cate-
gories can be drawn: models designed to emu-
late the distribution of the resulting phrase itself,
as it would be observed given a large enough cor-
pus (Guevara, 2010 and 2011; Baroni and Zam-
parelli, 2010), and those which only focus on
the composition operation and try to produce an
adequate representation of the semantic intersec-
tion of the phrase?s components, independently
from the phrasal distribution (Mitchell and La-
pata, 2010; Grefenstette and Sadrzadeh, 2011).
The former, which we will refer to as phrasal
models are trained and evaluated against phrases?
distributions while the latter, intersective models,
call for task-based evaluations (for instance, sim-
ilarity ratings: see Mitchell and Lapata, 2010).
We argue that phrasal and intersective models
are bound to produce different aspects of mean-
ing. Consider, for instance, the phrase big city.
Principles of semantic intersection tell us that a
big city is a city which is big. This is a correct
statement and one which should come out of the
composition of the big distribution and the city
distribution1. But arguably, there is more to the
meaning of the phrase (see Partee, 1994, for a
discussion of non-intersective adjectival phrases).
We expect people to readily associate concepts
like loud, underground, light, show, crowd to the
idea of a big city. Our hypothesis is that this
?extra? (non-intersective) meaning can be clearly
observed in phrasal distributions while it is, in
some sense, ?hidden? in distributions which are
the result of a purely intersective operation (be-
cause the entire distributions of the two compo-
nents are used, and not just the contexts relevant
to the particular association of big and city).
This observation, made at the linguistic level,
1For the sake of this argument, we will ignore the sugges-
tion that the gradable adjective might affect the intersective
status of the phrase
48
is also the foundation of intersectionality theory.
The argument, presented in Section 2, is that the
prejudices attached to black women, for instance,
(that is, the way that the concept black woman
is understood and used) are different from the
simple combination of the prejudices attached to
black people and women separately. So although
it is correct to say that a black woman is a woman
who is black (in the relevant sense of black), the
concept reaches further.
If the basic tenet of intersectionality theory
holds, and if we accept that distributions are a
valuable approximation of lexical meaning, we
would expect that the phrasal distribution of,
say, black woman would significantly differ from
its compositional distribution. Further, such a
significant difference would also have linguistic
relevance, as it would indicate the need to take
phrasal distributions into account when ?comput-
ing meaning? via distributional techniques. Both
phrasal models and intersective models could be
said to contribute to a complete and accurate rep-
resentation.
In Section 6, we will make a first step in in-
vestigating this issue by thoroughly analysing the
phrasal and compositional distributions of black
woman.
3.3 System description
The two systems we use in this paper have the
same basis. They produce a distribution for a
phrase based on a raw Wikipedia2 snapshot, pre-
processed to remove the wiki markup (Flickinger
et al 2010). Distributions are vectors in a space
S made of 10000 dimensions which correspond
to the 10000 most frequent words in the corpus.
The distribution of a word or phrase in the corpus
is taken to be the collection of all words that co-
occur with that word or phrase within a single sen-
tence (we use a list of stop words to discard func-
tion words, etc). The weight of each co-occurring
term in the distribution is given by a function bor-
rowed from Mitchell and Lapata (2010):
wi(t) =
p(ci|t)
p(ci)
=
freqci,t ? freqall
freqt ? freqci
(1)
where wi(t) is the weight of word t for dimension
i, freqci,t is the count of the co-occurrences of a
2http://www.wikipedia.org/
context word ci with t, freqall is the total num-
ber of words in the corpus, freqt and freqci are
respectively t and ci?s frequencies in the corpus.
We choose an intersective model based on mul-
tiplication, as this operation has been shown to
give excellent results in previous experiments
(Erk and Pado?, 2008; Mitchell and Lapata, 2010):
the distributions of the two components of the
phrase are multiplied in a point-wise fashion to
give the final distribution. This corresponds to the
model p=uv of Mitchell and Lapata.
As for the phrasal model, the final distribution
is simply the distribution obtained from looking
at the co-occurrences for the phrase itself.
The data passed on to the philosophers for fur-
ther consideration takes the form of a list of the
100 most ?characteristic? contexts for the query,
that is, the 100 words with heighest weights in the
distribution, filtered as described in ?3.3.1.
3.3.1 Filtering the results
One potential issue with our implementation
is that words belonging to frequent named enti-
ties end up in the top characteristic contexts for
the query. So for instance, wonder is one of the
most characteristic contexts for woman because
of the comic character Wonder Woman. Arguably,
such contexts should only be retrieved if they
are as significant in their ?non-name? form as in
their named entity form (e.g. if the string won-
der woman was significantly more frequent than
Wonder Woman, there would be a case for retain-
ing it in the results). We filter the relevant named
entities out using the following heuristic:
1. We call the query q and its capitalised ver-
sion Q. Let c1...cn be the top characteristic
contexts for q and C1...Cn their capitalised
equivalent.
2. For each context ck in c1...cn:
(a) We compute the corpus frequencies of
the patterns qck, ckq, qwck and ckwq,
where w is an intervening word. sk is
the sum of those frequencies.
(b) Similarly, we compute the corpus fre-
quencies of the patterns QCk, CkQ,
QwCk and CkwQ, where w is an in-
tervening word. Sk is the sum of those
frequencies.
49
(c) r is the ratio Sk/sk. If r is over a cer-
tain threshold t, we remove ck from the
characteristic contexts list.
In our experiments, we use t = 0.6. This
threshold allows us to successfully remove many
spurious contexts. For example, wonder and
spider are deleted from the results for woman
(among others) and isle3, elephant and iron from
the results for man.
4 The corpus
The number of experiments that can be devised
using distributional techniques is only limited by
the number of digitalised corpora available to re-
searchers in the humanities. It is easy to imag-
ine a range of comparative studies showing the
conceptual differences highlighted by the use of
a word or phrase at various times, in various
countries, or in various communities. The aim
of this study is to analyse the discursive use of
some concepts over a fairly large sample. We
chose the English Wikipedia4 as our corpus be-
cause of its size and because of its large contrib-
utor base (around 34,000 ?active editors? in De-
cember 20115). Wikipedia?s encyclopedic content
also makes it less explicitly biased than raw Inter-
net text, although we have to be aware of implicit
bias: most of Wikipedia?s contributors are male
and the encyclopedia?s content is heavily skewed
towards items of popular culture (Cohen, 2011).
The latter point is unproblematic as long as it is
acknowledged in the discussion of the results.
In the next two sections, we analyse the dis-
tributions obtained for the phrases man, woman,
black woman and Asian woman. It is worth men-
tioning that more data was extracted from our cor-
pus, which, due to space constraints, we will not
discuss here. The broad claims made with respect
to the above four noun phrases, however, are con-
sistent with the rest of our observations.
5 Discussing gender
This section discusses the produced distribu-
tions from the perspective of gender theory. The
aim of the discussion is to illustrate the type of
3As in Isle of Man
4http://en.wikipedia.org/wiki/Main Page
5http://stats.wikimedia.org/EN/SummaryEN.htm
information that may be relevant for discourse
analysis. Note that it follows the philosophical
methodology highlighted in Haslanger (2005) for
conceptual analysis.
Table 1 shows the most characteristic contexts
for woman and man, after filtering. There are
three levels on which the data discussed here
could be usefully interpreted within feminist re-
search ? the conceptual, the constructivist and the
deconstructivist. We will concentrate on the first
two.
In recent years a strong trend in Gender Studies
has emphasised that our investigations shouldn?t
repeat the historical bias which regards men as
?universal? (or default) and women as ?particular?,
as a specific ?other? to be investigated (Honegger,
1991). An advantage of our automatically pro-
duced data is that it returns just as much material
to focus on the cultural fabrication of masculinity
as on that of femininity. The male position proves
to indeed carry a broader variety of seemingly
non-gendered contexts, for instance, wise, inno-
cent, sin, fat, courage, salvation, genius, worthy
and rescued ? none of which is characteristic of
woman. But what is most striking is the strong oc-
currence of military contexts. We find enlisted at
the top of the list, followed by wounded, IRA, of-
ficers, militia, regiments, garrison, platoons, ca-
sualties, recruits and diverse military ranks. It
is sometimes unclear what counts as ?military-
related? (see killing, brave). We would have to
go back to the original text to investigate this. But
we see here very clearly how attributes that rank
high when it comes to defining stereotypical mas-
culinity and might be thought as ?general? charac-
teristics clearly owe their prominence to the mili-
tary cluster. The characteristic contexts list seems
to give distilled evidence to what has as yet still
only been partly analyzed in socio-historical re-
search, namely how the norms of masculinity are
to a large extent of military descent (for the Ger-
man context see Frevert, 2001). Brave, angry,
courage, cruel are all things that Wikipedia ? just
like popular imagination ? won?t associate with
women.
The meaning of woman seems to revolve
around the three interrelated clusters of reproduc-
tion, sex and love. Pregnant and pregnancy rank
very high, as well as reproduction-related terms
such as abortion, children and mothers. There
are more sexual terms (sexually, sexual, sexual-
50
Woman Man
women, woman, pregnant, feminist, abortion,
womens, men, husbands, elderly, pregnancy, sex-
ually, rape, breast, gender, equality, minorities,
lesbian, wives, beautiful, attractive, pornogra-
phy, dressed, sexual, marry, sexuality, dress, est.,
wear, young, sex, african-american, naked, com-
fort, homosexual, discrimination, priesthood,
womens, violence, loved, children, clothes,
man, male, marriages, hair, mysterious, wearing,
homeless, loves, boyfriend, wore, her., ladies,
mistress, lover, attitudes, hiv, advancement, re-
lationships, homosexuality, wealthy, mothers,
worn, murdered, ordained, mortal, unnamed,
girls, depicts, slavery, lonely, female, equal, can-
cer, goddess, roles, abuse, kidnapped, priests,
portrayal, witch, divorce, screening, clothing,
murders, husband, romantic, forbidden, loose,
excluded
men, man, enlisted, women, wise, homosex-
ual, wounded, gay, woman, dressed, young, el-
derly, ira, homeless, wives, brave, angry, offi-
cers, marry, marched, sexually, wealthy, killed,
wounds, innocent, militia, homosexuality, mans,
mysterious, god, tin, elves, mortal, ladies, wear-
ing, priesthood, sin, con, courage, fat, equal-
ity, numbering, regiments, garrison, numbered,
brotherhood, murdered, rape, lonely, platoon,
casualties, knew, recruits, reinforcements, re-
cruited, blind, loved, sexual, sex, thousand,
mask, clothes, salvation, commanded, loves,
lover, sick, detachment, genius, cruel, gender,
killing, col., lt., drunk, worthy, tall, flank, con-
victed, surrendered, contingent, rescued, naked
Table 1: Most characteristic contexts for woman and man, after filtering
ity, sex) in the characteristic list for woman and
mentions of loved, loves, lover are higher up than
in the results obtained for man. Further, a va-
riety of terms, mostly absent from the man list,
create a close link between femininity and rela-
tionality: husband(s), marriage (though, further
down, divorce comes up too), boyfriend and re-
lationships. While beautiful, attractive, comfort
and romantic might at least suggest that positive
sentiments are attached to the inbuilt feminine re-
lationality, another set of female contexts high-
lights the very vulnerability inscribed in the clus-
ter around intimacy: rape, pornography, violence,
slavery, abuse, kidnapped quantitatively capture a
correlation between relationality, sexuality and vi-
olence which characterises the use of the lexeme
woman.
Another set of exclusively feminine concepts
which at first sight seem to create interesting sin-
gular contexts ? breast, comfort, hair, HIV, can-
cer ? are united by reference to a physicality that
seems, apart from the wounds apparently con-
tracted in war, peculiarly absent in man. Such
clustering sheds light on the fact that certain asso-
ciations ?stick? to women and not to men. Though
it takes two to marry or divorce and have chil-
dren, those exclusively form contexts for woman.
Most dramatically, this can be observed when it
comes to rape. Though the majority of cases im-
ply a male perpetrator, rape is very high up, in
12th position, in the female list (that is before any
mention of love), while it is returned as character-
istic of men only to the extent that loneliness or
brotherhood are, at rank 49.
These observations highlight the kind of as-
sociations implicitly present in discursive data ?
whether retrieved by machines or humans. We do
not learn how matters are ?in fact? but simply in-
tegrate the linguistic patterns most characteristic
for a certain phenomenon. This, again, does have
tremendous effects on reality ? so-called ?con-
structive? effects. Indeed, when it comes to phe-
nomena that touch on human self-understanding,
discourse implies more than a mirror of mean-
ing. It partakes in the making of real identities.
It provides the material people craft their self-
understanding from and model their behavior af-
ter. It is this very effect of our characteristic usage
of language which prompts social philosophers to
ascribe ?power? to language.
6 Discussing intersectionality
Table 2 shows the most characteristic con-
texts for the phrase black woman after filtering,
as given by the intersective and phrasal mod-
els. We should point out that the phrase black
51
Multiplicative model Phrasal model
stripes, makeup, pepper, hole, racial, white,
woman, spots, races, women, whites, holes,
colours, belt, shirt, african-american, pale, yel-
low, wears, powder, coloured, wear, wore,
colour, dressed, racism, leather, colors, hair, col-
ored, trim, shorts, silk, throat, patch, jacket,
dress, metal, scarlet, worn, grey, wearing, shoes,
purple, native, gray, breast, slaves, color, vein,
tail, hat, painted, uniforms, collar, dark, coat, fur,
olive, bear, boots, paint, red, lined, canadiens,
predominantly, slavery
racism, feminist, women?s, slavery, negro, ide-
ology, tyler, filmmaker, african-american, ain?t,
elderly, whites, nursing, patricia, abbott, glo-
ria, freeman, terrestrial, shirley, profession, ju-
lia, abortion, diane, possibilities, argues, re-
union, hiv, blacks, inability, indies, sexually,
giuseppe, perry, vince, portraits, prevention, bea-
con, gender, attractive, tucker, fountain, riley,
beck, comfortable, stern, paradise, twist, anthol-
ogy, brave, protective, lesbian, domestic, feared,
breast, collective, barbara, liberation, racial, rosa,
riot, aunt, equality, rape, lawyers, playwright,
white, argued, documentary, carol, isn?t, expe-
riences, witch, men, spoke, slaves, depicted,
teenage, photos, resident, lifestyle, aids, com-
mons, slave, freedom, exploitation, clerk, tired,
romantic, harlem, celebrate, quran, interred, star-
gate, alvin, ada, katherine, immense
Table 2: Most characteristic contexts for black woman. Multiplicative and phrasal model, after filtering
woman/women only occurs 384 times in our cor-
pus, so the vector obtained through the phrasal
model suffers from some data sparsity problems.6
In particular, overall infrequent events are given
high weights by our algorithm, resulting in a rel-
atively high number of surnames being present in
the produced vector. Despite this issue, a num-
ber of observations can be made, which agree
with both our linguistic and philosophical expec-
tations.
We first considered to what extent the phrasal
and multiplicative models emphasised the char-
acteristics already present in their compo-
nents?distributions. We found that the top con-
texts for the phrasal distribution of black woman
only overlap 17 times with the top contexts of
woman and 9 times with the top contexts for
black. The multiplicative model produces an
overlap of only 12 items with woman but 64 with
black.7 This highlights a large conceptual differ-
6We should add that this small number of occurrences
is in itself significant, and mirrors problems of social
marginalisation. We note that the phrase African-American
woman/women is even sparser, with 236 occurrences in our
corpus.
7The weights in the black vector clearly override those
from the woman vector. Mitchell and Lapata (2010) discuss
possible improvements to the multiplicative model which in-
volve putting a positive weight on the noun component when
performing the composition.
ence between the phrase seen as a single entity
and its components. In contrast, the composition
of the constituents via the multiplicative model re-
turns a distribution fairly close to the distribution
of those constituents.
We looked next at the characteristic contexts
that were particular to each representation. We
found that the phrasal model vector presents 73
terms which are absent from the top contexts
for black and woman. In contrast, none of the
terms in the top contexts of the multiplied vec-
tor is specific to the composed phrase: all of
them are either highly characteristic of black or
woman (e.g. racial, African-American and breast,
dress).8 This indicates that the salient contexts for
the phrase are very different from the associations
commonly made with its constituents.
Finally, we observed that the vector obtained
through the phrasal model only overlaps 8 times
with the composed one. The shared words are
racial, white, whites, African-American, racism,
breast, slaves and slavery. Again, we can con-
clude from this that the two representations cap-
ture very different aspect of meaning, although
they both retrieve some high-level associations
with the concept of race and race-related history.
8Because our system does not perform any sense disam-
biguation, we return contexts such as pepper for black pep-
per, hole for black hole, etc.
52
From the point of view of intersectionality, our
results confirm the basic claim of the theory: there
are cases where the discourse on individuals be-
longing to two different social groups is radically
different than the discourse pertaining to those so-
cial groups taken separately.
In addition, the data supports further arguments
made by intersectionality researchers. In particu-
lar, comparing the distributions for woman, black
woman and Asian woman9 shows that colour or
ethnicity has a crucial impact on how women are
represented. Looking at woman, the word rape
appears at position 12, but it appears much fur-
ther down the list in black woman and not at all in
Asian woman. At the same time the word nurs-
ing is only associated with black women while
pornography hits position number 3, shortly fol-
lowed by exotic and passive when we look at
Asian woman. These three words do not occur
in the top contexts for woman or black woman.
This indicates that we are getting results concern-
ing sexuality which depend on ?ethnical? connota-
tion: white women are shown as victims of abuse
(rape), black women as responsible for nursing,
and Asian women represented as passive and ob-
jects of pornography.
Just looking at this data (and there would be
a lot more to analyse) we can find a connection
with what the latest historical research working
with intersectionality has brought to light: his-
torians have shown that the historical discourse
on prostitution has increased and reinforced racist
stereotypes and prejudices. Whyte (2012) shows,
(looking at the ?white slavery? panic of the early
twentieth century which is a key point in the his-
tory of prostitution) that the construction of the
white, innocent victim of prostitution is central to
the creation of the myth of the ?white slavery? in
many ways and that it has shaped the construction
and understanding of contemporary human traf-
ficking. The broader history of slavery (partic-
ularly in the North American context) forms the
backdrop for ?writing out? women of colour as
victims of sexual violence. This is appropriately
illustrated by our data.
9Due to space constraints, we are not showing the distri-
bution of Asian woman.
7 Conclusion
This paper sought to demonstrate that linguistic
representations of the type used in distributional
semantics may provide useful data to humanities
researchers who analyse discursive trends. We
presented a case study involving two subfields of
philosophy: gender theory and intersectionality.
We hope to have shown that a) distributional
data is a useful representation of social phenom-
ena which have been described by theorists and
social scientists but never linguistically observed
on a large scale b) this data lends itself to a fine-
grained analysis of such phenomena, as exem-
plified by the discussions in ?5 and ?6. Further,
we have highlighted that the philosophical theory
of intersectionality can be illustrated, at least for
some concepts, via a quantitative analysis of the
output of different distributional models. We sug-
gest that this observation should be investigated
further from the point of view of computational
linguistics: there may be some aspect of mean-
ing which is not expressed by those distributional
compositional models that do not take phrasal dis-
tributions into account (i.e. additive, multiplica-
tive, circular convolution models).
A natural extension of this work would be to
design experiments focusing on particular types
of discourse and corpora, and pursue conceptual
analysis at the diachronic level. This presupposes
the existence of digitalised corpora which may not
be available at this point in time. Efforts should
therefore be made to acquire the needed data. We
leave this as future work.
Acknowledgments
This work was in part enabled by an Alexander
von Humboldt Fellowship for Postdoctoral Re-
searchers awarded to the first author. We would
like to thank the Alexander von Humboldt Foun-
dation for their financial support.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.
Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP10).
Simone de Beauvoir. 1949. Le deuxie`me sexe. Galli-
mard, Paris.
53
Judith Butler. 1990. Gender Trouble. Feminism and
the Subversion of Identity. Routledge, New York.
Stephen Clark and Stephen Pulman. 2007. Combin-
ing Symbolic and Distributional Models of Mean-
ing. Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pp.52?55. Stanford, CA.
Noam Cohen. 2011. Define Gender Gap? Look Up
Wikipedias Contributor List. The New York Times,
31 January 2011, pp.A1 New York edition.
Kimberle? Crenshaw. 1991. Mapping the Mar-
gins: Intersectionality, Identity Politics, and Vio-
lence against Women of Color. Stanford Law Re-
view, 43:6, pp. 1241?1299.
James Curran. 2003. From Distributional to Semantic
Similarity. PhD dissertation. Institute for Commu-
nicating and Collaborative Systems. School of In-
formatics. University of Edinburgh, Scotland, UK.
Katrin Erk and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing. Hon-
olulu, HI.
Cordelia Fine. 2008. Delusions of Gender. The Real
Science Behind Sex Differences. Icon Books. Lon-
don.
Dan Flickinger and Stephan Oepen and Gisle Ytrestol
2010. WikiWoods: Syntacto-Semantic Annotation
for English Wikipedia. Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC10).
Michel Foucault. 1966. Les mots et les choses. Galli-
mard, Paris.
Ute Frevert. 2001. Die kasernierte Nation. Militrdi-
enst und Zivilgesellschaft in Deutschland. Beck,
Munich.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental Support for a Categorical
Compositional Distributional Model of Meaning.
Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pp.
1394?1404. Edinburgh, Scotland, UK.
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Kluwer Academic
Publishers.
Emiliano Guevara. 2010. A Regression Model of
Adjective-Noun Compositionality in Distributional
Semantics. Proceedings of the 2010 Workshop on
Geometrical Models of Natural Language Seman-
tics, ACL 2010, pp. 33?37. Uppsala, Sweden.
Emiliano Guevara. 2011. Computing Semantic Com-
positionality in Distributional Semantics. Proceed-
ings of the Ninth International Conference on Com-
putational Semantics (IWCS 2011), pp. 135?144.
Oxford, England, UK.
Kenneth E. Harper. 1965. Measurement of similar-
ity between nouns. Proceedings of the 1965 con-
ference on Computational linguistics (COLING 65),
pp. 1?23. Bonn, Germany.
Zelig Harris. 1954. Distributional Structure. Word,
10(2-3):146?162.
Sally Haslanger. 2005. What Are We Talking About?
The Semantics and Politics of Social Kinds Hypa-
tia, 20:10?26.
Karin Hausen. 1976. Die Polarisierung der
?Geschlechtscharaktere?. Eine Spiegelung der
Dissoziation von Erwerbs- und Familienleben.
Sozialgeschichte der Familie in der Neuzeit Eu-
ropas. Conze and Werner, Editors, pp. 363?393
Klett-Cotta, Stuttgart.
Claudia Honegger. 1991. Die Ordnung der
Geschlechter: Die Wissenschaften vom Menschen
und das Weib, 17501850. Campus, Frankfurt am
Main and New York.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28, pp. 203?208.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(2010):1388?1429.
Esther Ngan-Ling Chow. 2011. Analyzing Gender,
Intersectionality, and multiple inequalities: global,
transnational and local contexts. Esther Ngan-Ling
Chow, Marcia Texler Segal and Tan Lin, Editors.
Emerald Group Publishing.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2),
161?199.
Barbara Partee. 1994. Lexical Semantics and Compo-
sitionality Invitation to Cognitive Science, second
edition. Part I: Language. Daniel Osherson, Gen-
eral Editor. Lila Gleitman and Mark Liberman, Ed-
itors. MIT Press.
John G.A. Pocock. 1975. The Machiavellian moment.
Florentine political thought and the Atlantic repub-
lican tradition. Princeton University Press, Prince-
ton, NJ.
Richard Rorty. 1967. The Linguistic Turn: Recent Es-
says in Philosophical Method. Richard Rorty, Edi-
tor. University of Chicago Press, Chicago.
Quentin Skinner. 1998. Liberty before liberalism.
Cambridge University Press, Cambridge, UK.
Dominic Widdows. 2008. Semantic Vector Products:
Some Initial Investigations. Second AAAI Sympo-
sium on Quantum Interaction. Oxford, UK.
Christine Whyte. 2012. Praise be, prostitutes as the
women we are not ? Using intersectionality to anal-
yse race, class and gender in history. Intersektion-
alita?t und Kritik. Vera Kallenberg, Jennifer Meyer,
Johanna M. Mu?ller, Editors. Wiesdaben. Forthcom-
ing.
Ludwig Wittgenstein. 1953. Philosophische Unter-
suchungen. Blackwell, Oxford, UK.
54
