Abstract 
Cross-Language Multimedia Information Retrieval 
Sharon Flank 
emotion, Inc. 
2600 Park Tower Dr., Vienna, VA 22180 USA 
sharon.flank@emotion.com 
Simple measures can achieve high-accuracy 
cross-language r trieval in carefully chosen 
applications. Image retrieval is one of those 
applications, with results ranging from 68% 
of human translator performance for 
German, to 100% for French. 
1 Introduction 
contain strings of keywords. Typical queries 
are, as in most Web search applications, two 
to three words in length. At this point, all of 
the captions are in English. eMotion hosts a 
large database of images for sale and for 
licensing, PictureQuest. At least 10% of 
PictureQuest's user base is outside the 
United States. The tests were performed on 
the PictureQuest database of approximately 
400,000 images. 
Information is increasingly global, and the 
need to access it crosses language barriers. 
The topic of this paper, cross-language 
information retrieval, concerns the automatic 
retrieval of text in one language via a query 
in a different language. A considerable 
body of literature has grown up around 
cross-language information retrieval (e.g. 
Grefenstette 1998, TREC-7 1999). There 
are two basic approaches. Either the query 
can be translated, or each entire document 
can be translated into the same language as 
the query. The accuracy of retrieval across 
languages, however, is generally not good. 
One of the weaknesses that plagues cross- 
language retrieval is that we do not have a 
good sense of who the users are, or how best 
to interact with them. 
In this paper we describe a multimedia 
application for which cross-language 
information retrieval works particularly 
well. eMotion, Inc. has developed a natural 
language information retrieval application 
that retrieves images, such as photographs, 
based on short textual descriptions or 
captions. The captions are typically one to 
three sentences, although they may also 
Recent Web utilization data for PictureQuest 
indicate that of the 10% of users from 
outside the United States, a significant 
portion come from Spanish-speaking, 
French-speaking, and German-speaking 
countries. It is expected that adding 
appropriate language interfaces and listing 
PictureQuest in foreign-language search 
engines will dramatically increase non- 
English usage. 
The Cross-Language Multimedia 
Retrieval Application 
This paper offers several original 
contributions to the literature on cross- 
language information retrieval. First, the 
choice of application is novel, and 
significant because it simplifies the language 
problem enough to make it tractable. 
Because the objects retrieved are images and 
not text, they are instantly comprehensible 
to the user regardless of language issues. 
This fact makes it possible for users to 
perform a relevance assessment without he 
need for any kind of translation. More 
important, users themselves can select 
objects of interest, without recourse to 
translation. The images are, in fact, 
13 
associated with caption information, but, 
even in the monolingual system, few users 
ever even view the captions. It should be 
noted that most of the images in 
PictureQuest are utilized for advertising and 
publishing, rather than for news 
applications. Users of history and news 
photos do tend to check the captions, and 
often users in publishing will view the 
captions. For advertising, however, what the 
image itself conveys is far more important 
than the circumstances under which it was 
created. 
Another significant contribution of this 
paper is the inclusion of a variety of 
machine translation systems. None of the 
systems tested is a high-end machine 
translation system: all are freely available on 
the Web. 
Another key feature of this paper is the 
careful selection of an accuracy measure 
appropriate to the circumstances of the 
application. The standard measure, percent 
of monolingual performance achieved, is 
used, with a firm focus on precision. In this 
application, users are able to evaluate only 
what they see, and generally have no idea 
what else is present in the collection. As a 
result, precision is of far more interest o 
customers than recall. Recall is, however, of 
interest to image suppliers, and in any case it 
would not be prudent to optimize for 
precision without taking into account the 
recall tradeoff. 
The PictureQuest application avoids several 
of the major stumbling blocks that stand in 
the way of high-accuracy cross-language 
retrieval. Ballesteros and Croft (1997) note 
several pitfalls common to cross-language 
information retrieval: 
(1) The dictionary may not contain 
specialized vocabulary (particularly 
bilingual dictionaries). 
(2) Dictionary translations are inherently 
ambiguous and add extraneous terms 
to the query. 
(3) Failure to translate multi-term 
concepts as phrases reduces 
effectiveness. 
In the PictureQuest application, these pitfalls 
are minimized because the queries are short, 
not paragraph-long descriptions as in TREC 
(see, e.g., Voorhees and Harman 1999). 
This would be a problem for a statistical 
approach, since the queries present little 
context, but, since we are not relying on 
context (because reducing ambiguity is not 
our top priority) it makes our task simpler. 
Assuming that the translation program keeps 
multi-term concepts intact, or at least that it 
preserves the modifier-head structure, we 
can successfully match phrases. The 
captions (i.e. the documents o be retrieved) 
are mostly in sentences, and their phrases 
are intact. The phrase recognizer identifies 
meaningful phrases (e.g. fire engine) and 
handles them as a unit. The pattern matcher 
recognizes core noun phrases and makes it 
more likely that hey will match correctly. 
Word choice can be a major issue as well for 
cross-language retrieval systems. Some 
ambiguity problems can be resolved through 
the use of a part-of-speech tagger on the 
captions. As Resnik and Yarowsky (in 
press) observe, part-of-speech tagging 
considerably reduces the word sense 
disambiguation problem. However, some 
ambiguity remains. For example, the 
decision to translate a word as car, 
automobile, or vehicle, may dramatically 
affect retrieval accuracy. The PictureQuest 
14 
system uses a semantic net based on 
WordNet (Fellbaum 1998) to expand terms. 
Thus a query for car or automobile will 
retrieve ssentially identical results; vehicle 
will be less accurate but will still retrieve 
many of the same images. So while word 
choice may be a significant consideration for 
a system like that of Jang et al, 1999, its 
impact on PictureQuest is minimal. 
The use of WordNet as an aid to information 
retrieval is controversial, and some studies 
indicate it is more hindrance than help (e.g. 
Voorhees 1993, 1994, Smeaton, Kelledy and 
O'Donnell 1995). WordNet uses extremely 
fine-grained distinctions, which can interfere 
with precision even in monolingual 
information retrieval. In a cross-language 
application, the additional senses can add 
confounding mistranslations. If, on the 
other hand, WordNet expansion is 
constrained, the correct ranslation may be 
missed, lowering recall. In the PictureQuest 
application, we have tuned WordNet 
expansion levels and the corresponding 
weights attached to them so that WordNet 
serves to increase recall with minimal 
impact on precision (Flank 2000). This 
tuned expansion appears to be beneficial in 
the cross-language application as well. 
Gilarranz, Gonzalo and Verdejo (1997) 
point out that, for cross-language 
information retrieval, some precision is lost 
in any case, and WordNet is more likely to 
enhance cross-linguistic than monolingual 
applications. 
In fact, Smeaton and Quigley (1996) 
conclude that WordNet is indeed helpful in 
image retrieval, in particular because image 
captions are too short for statistical analysis 
to be useful. This insight is what led us to 
develop a proprietary image retrieval engine 
in the first place: fine-grained linguistic 
analysis is more useful that a statistical 
approach in a caption averaging some thirty 
words. (Our typical captions are longer than 
those reported in Smeaton and Quigley 
1996). 
3 Translation Methodology 
We performed preliminary testing using two 
translation methodologies. For the initial 
tests, we chose European languages: French, 
Spanish, and German. Certainly this choice 
simplifies the translation problem, but in our 
case it also reflects the most pressing 
business need for translation. For the 
French, Spanish, and German tests, we used 
Systran as provided by AltaVista 
(Babelfish); we also tested several other 
Web translation programs. We used native 
speakers to craft queries and then translated 
those queries either manually or 
automatically and submitted them to 
PictureQuest. The resulting image set was 
evaluated for precision and, in a limited 
fashion, for recall. 
The second translation methodology 
employed was direct dictionary translation, 
tested only for Spanish. We used the same 
queries for this test. Using an on-line 
Spanish-English dictionary, we selected, for 
each word, the top (top-frequency) 
translation. We then submitted this word- 
by-word translation to PictureQuest. 
(Unlike AltaVista, this method spell- 
corrected letters entered without the 
necessary diacritics.) Evaluation proceeded 
in the same manner. The word-by-word 
method introduces a weakness in phrase 
recognition: any phrase recognition 
capabilities in the retrieval system are 
defeated if phrases are not retained in the 
input. We can assume that the non-English- 
speaking user will, however, recognize 
phrases in her or his own language, and look 
15 
them up as phrases where possible. Thus we 
can expect at least those multiword phrases 
that have a dictionary entry to be correctly 
understood. We still do lose the noun 
phrase recognition capabilities in the 
retrieval system, further confounded by the 
fact that in Spanish adjectives follow the 
nouns they modify. In the hombre de 
negocios example in the data below, both 
AltaVista and Langenscheidt correctly 
identify the phrase as multiword, and 
translate it as businessman rather than man 
of businesses. 
The use of phrase recognition has been 
shown to be helpful, and, optimally, we 
would like to include it. Hull and 
Grefenstette 1996 showed the upper bound 
of the improvements possible by using 
lexicalized phrases. Every phrase that 
appeared was added to the dictionary, and 
that tactic did aid retrieval. Both statistical 
co-occurrence and syntactic phrases are also 
possible approaches. Unfortunately, the 
extra-system approach we take here relies 
heavily on the external machine translation 
to preserve phrases intact. If AltaVista (or, 
in the case of Langenscheidt, he user) 
recognizes a phrase and translates it as a 
unit, the translation is better and retrieval is 
likely to be better. If, however, the 
translation mistakenly misses a phrase, 
retrieval quality is likely to be worse. As for 
compositional noun phrases, if the 
translation preserves normal word order, 
then the PicmreQuest-internal oun phrase 
recognition will take effect. That is, ifjeune 
fille translates as young girl, then 
PictureQuest will understand that young is 
an adjective modifying girl. In the more 
difficult case, if the translation preserves the 
correct order in translating la selva africana, 
i.e. the African jungle, then noun phrase 
recognition will work. If, however, it comes 
out as the jungle African, then retrieval will 
be worse. In the architecture d scribed here, 
fixing this problem requires access to the 
internals of the machine translation program. 
4 Evaluation 
Evaluating precision and recall on a large 
corpus is a difficult task. We used the 
evaluation methods detailed in Flank 1998. 
Precision was evaluated using a crossing 
measure, whereby any image ranked higher 
than a better match was penalized. Recall 
per se was measured only with respect o a 
defined subset of the images. Ranking 
incorporates some recall measures into the 
precision score, since images ranked too low 
are a recall problem, and images marked too 
high are a precision problem. If there are 
three good matches, and the third shows up 
as #4, the bogus #3 is a precision problem, 
and the too-low #4 is a recall problem. 
For evaluation of the overall cross-language 
retrieval performance, we simply measured 
the ratio between the cross-language and 
monolingual retrieval accuracy (C/M%). 
This is standard; see, for example, Jang et al 
1999. 
Table 1 illustrates the percentage of 
monolingual retrieval performance we 
achieved for the translation tests performed. 
In this instance, we take the precision 
performance of the human-translated queries 
and normalize it to 100%, and adjust the 
other translation modalities relative to the 
human baseline. 
Language Raw 
Precision (%) 
French (Human) 80 
French 86 
(AltaVista) 
French 66 
(Transparent 
Language) 
C/M 
(%) 
100 
100 
83 
16 
Language Raw 
Precision (%) 
French (Intertran) 44 
Spanish (Human) 90 
Spanish 53 
(AltaVista) 
63 Spanish 
(Langenscheidt 
Bilingual 
Dictionary) 
German (Human) 80 
German 54 
(AltaVista) 
C/M 
(%) 
55 
100 
59 
70 
100 
68 
Several other factors make the PictureQuest 
application a particularly good application 
for machine translation technology. Unlike 
document ranslation, there is no need to 
match every word in the description; useful 
images may be retrieved even if a word or 
two is lost. There are no discourse issues at 
all: searches never use anaphora, and no one 
cares if the translated query sounds good or 
not. 
In addition, the fact that the objects being 
retrieved were images greatly simplified the 
endeavor. Under normal circumstances, 
developing a user-friendly interface is a 
major challenge. Users with only limited (or 
nonexistent) reading knowledge of the 
language of the documents need a way to 
determine, first, which ones are useful, and 
second, what they say. In the PictureQuest 
application, however, the retrieved assets are 
images. Users can instantly assess which 
images meet heir needs. 
In conclusion, it appears that simple on-line 
translation of queries can support effective 
cross-language information retrieval, for 
certain applications. We showed how an 
image retrieval application eliminates ome 
of the problems of cross-language r trieval, 
and how carefully tuned WordNet expansion 
simplifies word choice issues. We used a 
variety of machine translation systems, none 
of them high-end and all of them free, and 
nonetheless achieved commercially viable 
results. 
5 Appendix: Data 
Source Example Score 
Human men repairing road 100 
AV men repairing wagon 0 
Lang. man repair oad 100 
Human woman wearing red 100 
shopping in store 
AV woman dressed red buying 90 (2 of 
in one tends 20 bad) 
Lang. woman clothe red buy in wearing 
shop red is lost 
75 (5 of 
20 bad) 
Human cars driving on the 100 
highway 
AV cars handling by the 80' (4 of 
freeway 20 bad) 
Lang. cart handle for the 0 
expressway 
Human lions hunting in the 80 (1 of 5 
African forest bad) 
AV lions hunting in the 80 (1 of 5 
African forest bad) 
Lang. lion hunt in thejungle 45 (11 of 
gSt \] I 20 bad) 
~'~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  I~:~ i ~ 
Human juggler using colorful balls 67 (1 of 3 
bad) 
AV juggler with using balls of 50 (4 of 8 
colors bad) 
Lang. juggler by means of use (0; 1 
ball colour should be 
there) 
17 
Source Example Score 
Human blonde children playing 90(#3 
with marbles should be 
#1; 
remainder 
of top 20 
ok) 
AV blond children playing 90 (2 of 
with marbles 20 bad) 
Lang. young fair play by means 50 (1 of 2 
of marble bad) 
Human buying power 
AV spending power 45 (11 of 
20 bad) 
Lang. 
AV 
purchasing power 100 
successful businessman i 60 (8 of 
office 20 bad) 
Lang. successful businessman i 6 (8 of 20 
office bad) 
Human mother and daughter 100 (but 
baking bread in the kitchen no full 
matches) 
AV mother and daughter 30 (14 of 
\[horneando-removed\] 20 bad) 
bread in the kitchen 
Lang. mother and child bake 100 (but 
bread in the kitchen no full 
matches) 
Human old age and loneliness 100 
AV oldness and solitude 0 
Lang. old age and loneliness 100 
5.1 Spanish 
Human translations, tested on PictureQuest: 
90% (normalize to 100%) 
AltaVista: 53% (59% normalized) 
Langenscheidt, word-by-word: 63% (70% 
normalized) 
5.1.1 AltaVista 
For AltaVista, we left out the words that 
AltaVista didn't translate. 
5.1.2 Langenscheidt 
Langenscheidt, word-by-word: 63% (70% 
normalized) 
For the Langenscheidt word-by-word, we 
used the bilingual dictionary to translate 
each word separately as if we knew no 
English at all, and always took the first 
translation. We made the following 
adjustments: 
1. Left out "una," since Langenscheidt 
mapped it to "unir" rather than to either a or 
one 
2. Translated "e" as and instead of  e 
5.2 French 
Human translations, tested on PictureQuest: 
80% 
AltaVista: 86% (100% normalized) 
Transparent Language (freetranslation.com): 
66% (83% normalized) 
Intertran (www.intertran.net:2000): 44% 
(55% normalized) 
\[French examples originally drawn from 
http ://humanities.uchicago.edu/ARTFL/proj 
ects/academie/1835.searchform.html: 
French-French\] 
Source : Example Score 
~,, ~ i!, ~ii~l! "  ~:s~:: ~ ~'~  ~ 
Human signs of the zodiac 100 
AV signs of the zodiac 100 
TrLang sign zodiaque 0 
IntrTran 
Human 
\[signes\] any zodiac 
fish in water 
100 
30 (14 of 20 
bad) 
AV fish in water 30 (14 of 20 
bad) 
TrLang fish in water 30 (14 of 20 
bad) 
fish at water IntrTran 30 (14 of 20 
bad) 
18 
Source Example Score 
i 
Human painful earaches lO0 
AV Painful earaches 100 
TrLang the painful ear evil 0 
the \[manx\] \[doreille\]' 0 
distressing 
to take a rabbit by the 
ears 
To take a rabbit by the 
IntrTran 
,~ ~ ~ii ~ 
Human 
AV 
65 (7 of 20 
bad) 
65 (7 of 20 
bad) ears 
TrLang take a rabbit by the ears 65 (7 of 20 
bad) 
IntrTran 
Human 
capture a bunny by the 
ears 
cat which lives in wood 
80 (1 of 5 
bad) 
%~!,~:,.' i~: ~'" 
45 (11 of 20 
bad) 
AV Cat which lives in wood 45 (11 of 20 
bad) 
TrLang cat that lives in wood 65 (7 of 20 
bad) 
cat thanksgiving lives at 
the forest 
to leave a house 
IntrTran 
Human 
70 (6 of 20 
bad) 
60 (8 of 20 
bad) 
AV To leave a house 60 (8 of 20 
bad) 
TrLang to go out of a house 95 (1 of 20 
bad) 
IntrTran come out dune' dwelling 90 (18 of 20 
house bad) 
Human carpenter's tool 95 (1 of 20 
bad) 
AV Instrument of carpenter 100 
TrLang instrument of carpenter 100 
I IntrTran implement any carpenter 35 (13 of 20 
bad) 
Human to play the violin 100 
AV to play of the violin 100 
TrLang to play the violin 100 
IntrTran gamble any violin 0 
Human pleasures of the body 100 
Source Example Score 
AV Pleasures of the body 100 
100 TrLang 
IntrTran 
the pleasures of the body 
the delight any body 
Human a girl eats fruit 
AV a girl eats fruit 100 
TrLang a girl eats fruit 100 
IntrTran a girl am eating any fruit 65 (7 of 20 
bad) 
0 
100 
5.3 German 
Human translations, tested on PictureQuest:  
80% (100% normal ized)  
AltaVista 54% (68% normal ized)  
Source Example Score 
Human boys golf course 95 
AV golf course 95 
Human artificial paradise 100 
AV artificial paradiese 0 
Human solar energy for automobiles 95 
AV solar energy for auto 95 ........................ ~, , ,~ :~,,~ . ~.~ ~ ~ ~; : .  , .  ~<.~ 
Human hiking through the forest 90 
AV migrations by the forest 0 
Human an elephant in a zoo 25 
(#17 
should 
be #2) 
AV elephant in the zoo 100 
............... i!~ n = ~!~ ~ ~ 
Human the synthesis of I00 
desoxyribonucleic acid 
AV the synthesis of the 0 
Desoxynribonukleinsaeure 
Human black cars 100 
AV black auto 100 
Human playing together 60 
young together play 
19 
Source Example Score 
Human women in blue 65 
AV Ladies in blue 75 
Human woman at work 65 
AV Ladies on work 40 
6 Acknowledgements 
I am grateful to Doug Oard for comments on 
an earlier version of  this paper. 
7 References 
Ballesteros, Lisa, and W. Bruce Croft, 1997. "Phrasal 
Translation and Query Expansion Techniques for 
Cross-Language Information Retrieval," in AAAI 
Spring Symposium on Cross-Language Text and 
Speech Retrieval, Stanford University, Palo Alto, 
California, March 24-26, 1997. 
Fellbaum, Christiane, ed., 1998. WordNet: An 
Electronic Lexical Database. Cambridge, MA: MIT 
Press. 
Flank, Sharon. 2000. "Does WordNet Improve 
Multimedia Information Retrieval?" Working paper? 
Flank, Sharon. 1998? "A Layered Approach to NLP- 
Based Information Retrieval," in Proceedings of 
COLING-ACL, 36th Annual Meeting of the 
Association for Computational Linguistics, Montreal, 
Canada, 10-14 August 1998. 
Gilarranz, Julio, Julio Gonzalo and Felisa Verdejo. 
1997. "An Approach to Conceptual Text Retrieval 
Using the EuroWordNet Multilingual Semantic 
Database," in AAAI Spring Symposium on Cross- 
Language Text and Speech Retrieval, Stanford 
University, Palo Alto, California, March 24-26, 
1997. (http://www.clis.umd.edu/dlrg/filter/sss/papers) 
Grefenstette, Gregory, ed., 1998. Cross-Language 
Information Retrieval. Norwell, MA: Kluwer. 
Hull, David A. and Gregory Grefenstette, 1996. 
"Experiments in Multilingual Information Retrieval," 
m Proceedin s o the 19 th L ? " g f nternational Conference 
on Research and Development in Information 
Retrieval (SIGIR96) Zurich, Switzerland. 
Jang, Myung-Gil, Sung Hyon Myaeng, and Se 
Young Park, 1999. "Using Mutual Information to 
Resolve Query Translation Ambiguities and Query 
Term Weighting," in Proceedings of 37 th Annual 
Meeting of the Association for Computational 
Linguistics, College Park, Maryland. 
McCarley, J. Scott, 1999. "Should We Translate the 
Documents or the Queries in Cross-Language 
Information Retrieval?" 
Resnik, Philip and Yarowsky, David, in press. 
"Distinguishing Systems and Distinguishing Sense: 
New Evaluation Methods for Word Sense 
Disambiguation," Natural Language Engineering. 
Smeaton, Alan F., F. Kelledy and R. O'Donnell, 
1995. "TREC-4 Experiments at Dublin City 
University: Thresholding Posting Lists, Query 
Expansion with WordNet and POS Tagging of 
Spanish," in Donna K. Harman (ed.) NIST Special 
Publication 500-236: The Fourth Text REtrieval 
Conference (TREC-4), Gaithersburg, MD, USA: 
Department of Commerce, National Institute of 
Standards and Technology. 
(http://trec.nist.gov/pubs/trec4/t4_proceedings.html) 
Smeaton, Alan F. and I. Quigley, 1996. "Experiments 
on Using Semantic Distances Between Words in 
Image Caption Retrieval," in Proceedings of the 19 th 
International Conference on Research and 
Development in Information Retrieval (SIGIR96) 
Zurich, Switzerland. 
Voorhees, Ellen M. 1994. "Query Expansion Using 
Lexical-Semantic Relations," in Proceedings of the 
17 th International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
pp. 61-70. 
Voorhees, Ellen M. 1993. "Using WordNet to 
Disambiguate Word Senses for Text Retrieval," in 
Proceedings of the 16 th International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pp. 171-180. 
Voorhees, Ellen M. and Donna K. Harman, editors, 
1999? The 7 th Text Retrieval Conference (TREC- 7). 
20 
Sentences vs. Phrases: Syntactic Complexity in Multimedia 
Information Retrieval 
Sharon Flank 
emotion, Inc. 
2600 Park Tower Dr., Ste. 600, Vienna, VA 22180 
sharon.flank@emotion.corn 
Abstract 
In experiments on a natural language 
information retrieval system that retrieves 
images based on textual captions, we show 
that syntactic complexity actually aids 
retrieval. We compare two types of 
captioned images, those characterized with 
full sentences in English, and those 
characterized by lists of words and phrases. 
The full-sentence captions show a 15% 
increase in retrieval accuracy over the word- 
list captions. We conclude that the syntactic 
complexity may be of use in fact because it 
decreases semantic ambiguity: the word-list 
captions may be syntactically simple, but 
they are semantically confusingly complex. 
1 Introduction 
In this paper, we describe experiments 
conducted on an image retrieval system, 
PictureQuest, which uses text captions to 
characterize images. The text captions are 
of two types. Optimally, they consist of a 
prose description of the image, generally 
two to three sentences, with perhaps three or 
four additional words or phrases that 
describe emotional or non-literal image 
content, e.g. 
Two little girls play with blocks. The 
younger girl, wearing a blue shirt, laughs 
and prepares to knock over the tower that 
the older girl has constructed The older 
girl, dressed in a red shirt, winces in 
anticipation. 
Siblings, cooperation, rivalry 
Some of the captions in PictureQuest are not 
as well-behaved. They may contain legacy 
data or data shared with a keyword-retrieval 
system. They are optimized for exact-match 
retrieval, and, as such, consist of lists of 
words or, at best, a few short phrases mixed 
in with long lists of words. The same image 
might appear with the following caption: 
girl, girls, little girl little girls, block, 
blocks, play, playing, plays, blue, red, shirt, 
tower, knock; over, construct, construction, 
siblings, cooperation, rivalry 
PictureQuest relies on several natural 
language processing techniques to enhance 
retrieval accuracy. It contains a part-of- 
speech tagger, morphological nalyzer, noun 
phrase pattern matcher, semantic expansion 
based on WordNet, and special processing 
for names and locations. These have been 
tuned to perform most effectively on caption 
text of the first type, i.e. sentences. The 
following chart illustrates how these 
linguistic processes operate - or fail to 
operate - on syntactic units. 
Tagger dog-N herding-V sheep- 
N 
dog-N,V; herding-N,V; sheep-N 
Morphology dog herd-ING sheep (same) 
NP Pattems small child wearing a small, child, wearing, hat 
hat green, swirls (modifiers de-coupled from head 
green swirls nouns) 
cat jumping into the air: 
cat-N (7 senses) 
jumping-V (13 senses) 
air-N (13 senses) 
Semantic 
Expansion 
(WordNet- 
based) 
cat, jumping, air 
cat-N,V (9 senses) 
jumping-N,V,Adj (16 senses) 
air-N,V,Adj (20 senses) 
Names George Bush, A1 Gore George, Bush, AI, Gore (matches bush, gore) 
Locations Arlington, Virginia Arlington, Virginia (matches other Arlingtons in 
New England other states) 
New, England (matches England, new) 
2 Complexity Measures 
2.1 Competing Complexity Measures 
How do we determine what syntactic 
complexity is? Does it relate to depth? 
Nesting? Various definitions have been 
used in the various research communities: 
Alzheimer's research, normal and abnormal 
child language acquisition, speech and 
hearing, English teaching, second language 
teaching and acquisition, and theoretical 
linguistics of various persuasions (see, e.g., 
MacDonald 1997; Rosen 1974; Bar-Hillel et 
al. 1967). Fortunately, for the purposes of 
our investigation, we are dealing with broad 
distinctions that would foster agreement 
even among those with different definitions 
of complexity. For the captioned ata, in 
one case, the data are in full sentences. The 
average sentence l ngth is approximately ten 
words, and the average number of sentences 
is between two and three. In the other case, 
the data are either in lists of single words, or 
in lists of single words with a few two-word 
or three-word phrases included, but with no 
sentences whatsoever. Regardless of the 
exact measure of syntactic omplexity used, 
it is clear that sentences are syntactically 
more complex than word lists or even phrase 
lists. 
2.2 Query Complexity 
The standard query length for Web 
applications i between two and three words, 
and our experience with PictureQuest 
confirms that observation. In comparisons 
with other text-based image retrieval 
applications, including keyword systems, 
query complexity is important: one-word 
queries work equally well on keyword 
systems and on linguistically-enhanced 
natural language processing systems. The 
difference comes with longer queries, and in 
particular with syntactic phrases. (Boolean 
three-word queries, e.g. A and B; A or B, do 
not show much difference.) The more 
complex queries (and, in fact, the queries 
that show PictureQuest off to best 
advantage) consist either of a noun phrase or 
are of the form NP V-ing NP. The table 
below summarizes the differences in query 
complexity for natural anguage information 
retrieval as compared to keyword-only 
information retrieval. 
2 
one word, e.g. 
zlephant 
Boolean, e.g. rhino 
9r rhinoceros 
NP V-ing NP, e.g. 
girl leading ahorse 
noun phrase, e.g. 
black woman in a 
white hat 
Both are equally good 
Both are equally good, 
assuming they both 
recognize the meaning of 
the Boolean operator 
NLIR shows some 
improvement 
NLIR shows major 
improvement; keyword 
retrieval scrambles 
modifiers randomly 
2.3 Semantic Complexity 
Semantic complexity is more difficult to 
evaluate, but we can make certain 
observations. Leaving noun phrases intact 
makes a text more semantically complex 
than deconstructing those noun phrases: 
rubber baby buggy bumpers is more 
semantically complex than a simple list of 
nouns and attributes, ince there are various 
modification ambiguities in the longer 
version that are not present once it has been 
reduced to rubber, baby buggy, bumpers (or 
rubber, baby, buggy, bumpers, for that 
matter). 
As for the names of people and locations, 
one could argue that the intact syntactic 
units (AI Gore; George Bush; Arlington, 
Virginia; New England) are semantically 
simpler, since they resolve ambiguity and 
eliminate the spurious readings gore, bush, 
Arlington \[Massachusetts\], new England. 
Nonetheless, we would argue that they are 
syntactically more complex when intact. 
The PictureQuest system uses a WordNet- 
based semantic net to expand the caption 
data. To some extent, the syntactic 
measures (part-of-speech tagging, noun 
phrase pattern matching, name and location 
identification) serve to constrain the 
semantic expansion, since they eliminate 
some possible semantic expansions based on 
syntactic factors. One could interpret he 
word-list captions, then, not as syntactically 
less complex, but rather as semantically ess 
constrained, therefore more ambiguous and 
thus more complex. This view would, 
perhaps, restore the more intuitive notion 
that complexity should lead to worse rather 
than better esults. 
3 Experiments 
While the sentence captions are syntactically 
more complex, by almost any measure, they 
contain more information than the legacy 
word list captions. Specifically, the part-of- 
speech tagger and the noun phrase pattern 
matcher are essentially useless with the 
word lists, since they rely on syntactic 
patterns that are not present. We therefore 
hypothesized that our retrieval accuracy 
would be lower with the legacy word list 
captions than with the sentence captions. 
We performed two sets of experiments, one 
with legacy word list captions and the other 
with sentence captions. Fortunately, the 
corpus can be easily divided, since it is 
possible to select image providers with 
either full sentence or word list captions, and 
limit the search to those providers. In order 
to ensure that we did not introduce a bias 
because of the quality of captioning for a 
particular provider, we aggregated scores 
from at least hree providers in each test. 
Because the collection is large and live, and 
includes ranked results, we selected a 
modified version of precision at 20 rather 
than a manual gold standard precision/recall 
test. We chose this evaluation path for the 
following reasons: 
? Ranking image relevance was difficult 
for humans 
? The collection was large and live, i.e. 
changing daily 
? The modified measure more accurately 
reflected user evaluations 
-" 3 
We performed experiments initially with 
manual ranking, and found that it was 
impossible to get reliable cross-coder 
judgements for ranked results. That is, we 
could get humans to assess whether an 
image should or should not have been 
included, but the rankings did not yield 
agreement. Complicating the problem was 
the fact that we had a large collection 
(400,000+ images), and creating a test 
subset meant that most queries would 
generate almost no relevant results. Finally, 
we wanted to focus more on precision than 
on  recall, because our work with users had 
made it clear that precision was far more 
important in this application. 
To evaluate precision at 20 for this 
collection, we used the crossing measure 
introduced in Flank 1998. The crossing 
measure (in which any image ranked above 
another, better-matching image counts as an 
error) is both finer-grained and better suited 
to a ranking application in which user 
evaluations are not binary. We calibrated 
the crossing measure (on a subset of the 
queries) as follows: 
Precision at 20 Images for 
All Terms 
53 
Precision at 5 Images for All 59 
Terms 
Precision at 20 Images for 100 
Any Term 
Crossing Measure at 20 91 
i Images I 
That is, we calculated the precision "for all 
terms" as a binary measure with respect to a 
query, and scored an error if any terms in the 
query were not matched. For the "any term" 
precision measure, we scored an error only 
if the image failed to match any term in the 
query in such a way that a user would 
consider it a partial match. 
Thus, for example, for an "all terms" match, 
tall glass of beer succeeded only when the 
images howed (and captions mentioned) all 
three terms tall, glass, and beer, or their 
synonyms. For an "any-term" match, tall or 
glass or beer or a direct synonym would 
need to be present (but not, say, glasses). 
(For two of the test queries, fewer than 20 
images were retrieved, so the measure is, 
more precisely, R-precision: precision at the 
number of documents retrieved or at 20 or 5, 
whichever is less. 
4 Results 
We found a statistically significant 
difference in retrieval quality between the 
syntactically simple word list captions and 
the syntactically complex sentence captions. 
The word list captions cored 74.6% on our 
crossing measure, while the sentence 
captions cored 89.5%. 
We performed one test comparing one-word 
and two-word queries on sentence versus 
word list captions. The sentence captions 
showed little difference: 82.7% on the one- 
word queries, and 80% on the two-word 
queries. The word-list captions, however, 
were dramatically worse on two-word 
queries (70.5%) than on one-word queries 
(89.7%). 
Overall 74.6% 89.5% 
1-word 89.7% 82.7% 
2-word 7015% 80% 
5 Conclusion 
Our experiments indicate that, in an 
information retrieval system tuned to 
recognize and reward matches using 
syntactic information, syntactic omplexity 
yields better results than syntactically 
4 
mixed-up "word salad." One can interpret 
these results from a semantic complexity 
standpoint, since the syntactically simple 
captions all include considerably more 
semantic ambiguity, unconstrained as they 
are from a syntactic standpoint. This 
observation leads us to an additional 
conclusion about the relationship between 
syntactic and semantic complexity: in this 
instance, at least, the relationship is inverse 
rather than direct. The word-list captions 
are syntactically simple but, as a result, 
since syntactic factors are not available to 
limit ambiguity, semantically more complex 
than the same information presented in a 
more syntactically complex fashion, i.e. in 
sentences. 
6 References 
Bar-Hillel, Y., A. Kasher and E. Shamir 1967. 
"Measures of Syntactic Complexity," in Machine 
Translation, A.D. Booth, ed. Amsterdam: North- 
Holland, pp. 29-50. 
Flank, Sharon, 1998. "A Layered Approach to NLP- 
Based Information Retrieval," in Proceedings of 
COLING-ACL, 36th Annual Meeting of the 
Association for Computational Linguistics, Montreal, 
Canada, 10-14 August 1998. 
MacDonald, M.C. 1997. Language and Cognitive 
Processes: Special Issue on Lexical Representations 
and Sentence Processing, 12, pp. 121-399. 
Rosen, B.K. 1974. "Syntactic Complexity," in 
Information and Control 24, pp. 305-335. 
