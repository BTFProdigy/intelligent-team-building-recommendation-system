Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
Abstract
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
1 Introduction
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
1.1 Decoder
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al, 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
1.2 Evaluation
In this paper we report results using the BLEU met-
ric (Papineni et al, 2002), however as the evaluation
criterion in GALE is HTER (Snover et al, 2006), we
also report in TER (Snover et al, 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
1.3 Training Data
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al, 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
77
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
2 Number Tagging
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged ? 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
Table 1: Number tagging experiments, BLEU/TER
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
3 Scaling up to Large Data
3.1 Language Model
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
Table 3: LM interpolation weights per source
3.2 Speeding up Model Training
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al, 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
78
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
3.3 Translation Model
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
Table 4: In-domain only or all training data, BLEU/TER
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
4 POS-based Reordering
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
4.1 Learning Reordering Rules
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n > 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
4.2 Applying Reordering Rules
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
3http://nlp.stanford.edu/software/lex-parser.shtml
79
ordering approach an advantage over a simple jump
model with a sliding window.
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
Table 5: Reordering lattice decoding in BLEU/TER
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
5 Summary
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.Improvements in BLEU
242526
272829
303132
33
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
Figure 1: Improvements for MT06 in BLEU
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
Acknowledgments
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
References
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
80
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 216?223,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Distributed Language Modeling for N -best List Re-ranking
Ying Zhang Almut Silja Hildebrand Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Ave. Pittsburgh, PA 15213, U.S.A.
{joy+, silja+, vogel+}@cs.cmu.edu
Abstract
In this paper we describe a novel dis-
tributed language model for N -best list
re-ranking. The model is based on the
client/server paradigm where each server
hosts a portion of the data and provides
information to the client. This model al-
lows for using an arbitrarily large corpus
in a very efficient way. It also provides
a natural platform for relevance weighting
and selection. We applied this model on
a 2.97 billion-word corpus and re-ranked
the N -best list from Hiero, a state-of-the-
art phrase-based system. Using BLEU as a
metric, the re-ranked translation achieves
a relative improvement of 4.8%, signifi-
cantly better than the model-best transla-
tion.
1 Introduction
Statistical language modeling has been widely
used in natural language processing applications
such as Automatic Speech Recognition (ASR),
Statistical Machine Translation (SMT) (Brown et
al., 1993) and Information Retrieval (IR) (Ponte
and Croft, 1998).
Conventional n-gram language modeling
counts the frequency of all the n-grams in a
corpus and calculates the conditional probabilities
of a word given its history of n ? 1 words
P (wi|wi?1i?n+1). As the corpus size increases,
building a high order language model offline
becomes very expensive if it is still possible
(Goodman, 2000).
In this paper, we describe a new approach of
language modeling using a distributed comput-
ing paradigm. Distributed language modeling can
make use of arbitrarily large training corpora and
provides a natural way for language model adap-
tation.
We applied the distributed LM to the task of re-
ranking the N -best list in statistical machine trans-
lation and achieved significantly better translation
quality when measured by the BLEU metric (Pap-
ineni et al, 2001).
2 N -best list re-ranking
When translating a source language sentence f
into English, the SMT decoder first builds a trans-
lation lattice over the source words by applying the
translation model and then explores the lattice and
searches for an optimal path as the best translation.
The decoder uses different models, such as the
translation model, n-gram language model, fertil-
ity model, and combines multiple model scores to
calculate the objective function value which favors
one translation hypothesis over the other (Och et
al., 2004).
Instead of outputting the top hypothesis e(1)
based on the decoder model, the decoder can out-
put N (usually N = 1000) alternative hypotheses
{e(r)|r = 1, . . . , N} for one source sentence and
rank them according to their model scores.
Figure 1 shows an example of the output from a
SMT system. In this example, alternative hypoth-
esis e(2) is a better translations than e(1) according
to the reference (Ref) although its model score is
lower.
SMT models are not perfect, it is unavoidable
to have a sub-optimal translation output as the
model-best by the decoder. The objective of N -
best list re-ranking is then to re-rank the trans-
lation hypotheses using features which are not
used during decoding so that better translations
can emerge as ?optimal? translations. Our exper-
216
f : , 2001#?)?I9]??{/G??
Ref: Since the terrorist attacks on the United States in 2001
e(1): since 200 year , the united states after the terrorist
attacks in the incident
e(2): since 2001 after the incident of the terrorist attacks on
the united states
e(3): since the united states 2001 threats of terrorist attacks
after the incident
e(4): since 2001 the terrorist attacks after the incident
e(5): since 200 year , the united states after the terrorist
attacks in the incident
Figure 1: An example of N -best list.
iments (section 5.1) have shown that the oracle-
best translation from a typical N -best list could be
6 to 10 BLEU points better than the model-best
translation.
In this paper we use the distributed language
model on very large data to re-rank the N -best list.
2.1 Sentence likelihood
The goal of a language model is to determine
the probability, or in general the ?likelihood? of
a word sequence w1 . . . wm (wm1 for short) given
some training data. The standard language model-
ing approach breaks the sentence probability down
into:
P (wm1 ) =
?
i
P (wi|wi?11 ) (1)
Under the Markov or higher order Markov process
assumption that only the closest n? 1 words have
real impact on the choice of wi, equation 1 is ap-
proximated to:
P (wm1 ) =
?
i
P (wi|wi?1i?n+1) (2)
The probability of a word given its history can be
approximated with the maximum likelihood esti-
mate (MLE) without any smoothing:
P (wi|wi?1i?n+1) ?
C(wii?n+1)
C(wi?1i?n+1)
(3)
In addition to the standard n-gram probability
estimation, we propose 3 sentence likelihood met-
rics.
? L0: Number of n-grams matched.
The simplest metric for sentence likelihood is
to count how many n-grams in this sentence
can be found in the corpus.
L0(wm1 ) =
?
i,j
i?j
?(wji ) (4)
?(wji ) =
{
1 : C(wji ) > 0
0 : C(wji ) = 0
(5)
For example, L0 for sentence in figure 2 is 52
because 52 n-grams have non-zero counts.
? Ln1 : Average interpolated n-gram conditional
probability.
Ln1 (wm1 ) =
( m?
i=1
n?
k=1
?kP (wi|wi?1i?k+1)
) 1
m
(6)
P (wi|wi?1i?k+1) is approximated from the n-
gram counts (Eq. 3) without any smoothing.
?k is the weight for k-gram conditional prob-
ability,
??k = 1.
Ln1 is similar to the standard n-gram LM
except the probability is averaged over the
words in the sentence to prevent shorter sen-
tences being favored unfairly.
? L2: Sum of n-gram?s non-compositionality
For each matched n-gram, we consider all
the possibilities to cut/decompose it into two
short n-grams, for example ?the terrorist at-
tacks on the united states? could be decom-
posed into (?the?, ?terrorist attacks on the
united states?) or (?the terrorist?, ?attacks
on the united states?), ... , or (?the ter-
rorist attacks on the united?, ?states?). For
each cut, calculate the point-wise mutual in-
formation (PMI) between the two short n-
grams. The one with the minimal PMI
is the most ?natural? cut for this n-gram.
The PMI over the natural cut quantifies the
non-compositionality Inc of an n-gram wji .
The higher the value of Inc(wji ) the more
likely wji is a meaningful constituent, in other
words, it is less likely that wji is composed
from two short n-grams just by chance (Ya-
mamoto and Church, 2001).
Define L2 formally as:
L2(wm1 ) =
?
i,j
i?j
Inc(wji ) (7)
217
Inc(wji ) =
?
?
?
min
k
I(wki ;wjk+1) : C(wji ) > 0
0 : C(wji ) = 0
(8)
I(wki ;wjk+1) = log
P (wji )
P (wki )P (wjk+1)
(9)
3 Distributed language model
The fundamental information required to calculate
the likelihood of a sentence is the frequency of n-
grams in the corpus. In conventional LM train-
ing, all the counts are collected from the corpus D
and saved to disk for probability estimation. When
the size of D becomes large and/or n is increased
to capture more context, the count file can be too
large to be processed.
Instead of collecting n-gram counts offline, we
index D using a suffix array (Manber and Myers,
1993) and count the occurrences of wii?n+1 in D
on the fly.
3.1 Calculate n-gram frequency using suffix
array
For a corpus D with N words, locating all the oc-
currences of wii?n+1 takes O(logN ). Zhang and
Vogel (2005) introduce a search algorithm which
locates all the m(m+ 1)/2 embedded n-grams in
a sentence of m words within O(m ? logN ) time.
Figure 2 shows the frequencies of all the embed-
ded n-grams in sentence ?since 2001 after the in-
cident of the terrorist attacks on the united states?
matched against a 26 million words corpus. For
example, unigram ?after? occurs 4.43?104 times,
trigram ?after the incident? occurs 106 times. The
longest n-gram that can be matched is the 8-gram
?of the terrorist attacks on the united states? which
occurs 7 times in the corpus.
3.2 Client/Server paradigm
To load the corpus and its suffix array index into
the memory, each word token needs 8 bytes. For
example, if the corpus has 50 million words,
400MB memory is required. For the English1 Gi-
gaWord2 corpus which has 2.7 billion words, the
1Though we used English data for our experiments in this
paper, the approach described here is language independent.
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2005T12
total memory required is 22GB. It is practically
impossible to fit such data into the memory of any
single machine.
To make use of the large amount of data, we
developed a distributed client/server architecture
for language modeling. Client/server is the most
common paradigm of distributed computing at
present (Leopold, 2001). The paradigm describes
an asymmetric relationship between two type of
processes, of which one is the client, and the other
is the server. The server process manages some re-
sources and offers a service which can be used by
other processes. The client is a process that needs
the service in order to accomplish its task. It sends
a request to the server and asks for the execution
of a task that is covered by the service.
We split the large corpus D into d non-
overlapping chunks. One can easily verify that for
any n-gram wii?n+1 the count of its occurrences in
D is the sum of its occurrences in all the chunks,
i.e.,
C(wii?n+1)|D =
?
d
C(wii?n+1)|Dd (10)
Each server3 loads one chunk of the corpus with
its suffix array index. The client sends an English
sentence w1 . . . wm to each of the servers and re-
quests for the count information of all the n-grams
in the sentence. The client collects the count infor-
mation from all the servers, sums up the counts for
each n-gram and then calculates the likelihood of
the sentence.
The client communicates with the servers via
TCP/IP sockets. In our experiments, we used
150 servers running on 26 computers to serve one
client. Multiple clients can be served at the same
time if needed. The process of collecting counts
and calculating the sentence probabilities takes
about 1 to 2 ms for each English sentence (average
length 23.5 words). With this architecture, we can
easily make use of larger corpora by adding addi-
tional data servers. In our experiments, we used all
the 2.7 billion word data in the English Gigaword
corpus without any technical difficulties.
3A server is a special program that provides services to
client processes. It runs on a physical computer but the con-
cept of server should not be confused with the actual machine
that runs it. In practice, one computer usually hosts multiple
servers at the same time.
218
n since 2001 after the incident of the terrorist attacks on the united states
1 2.19?104 7559 4.43?104 1.67?106 2989 6.9?105 1.67?106 6160 9278 2.7?105 1.67?106 5.1?104 3.78?104
2 165 105 1.19?104 1892 34 2.07?105 807 1398 1656 5.64?104 3.72?104 3.29?104
3 6 56 106 6 3 162 181 216 545 605 2.58?104
4 0 0 0 1 0 35 67 111 239 424
5 0 0 0 0 0 15 34 77 232
6 0 0 0 0 0 10 23 76
7 0 0 0 0 0 7 23
8 0 0 0 0 0 7
Figure 2: Frequencies of all the embedded n-grams in sentence ?since 2001 after the incident of the
terrorist attacks on the united states.?
4 ?More data is better data? or
?Relevant data is better data?
Although statistical systems usually improve with
more data, performance can decrease if additional
data does not fit the test data. There have been
debates in the data-driven NLP community as to
whether ?more data is better data? or ?relevant
data is better data?. For N -best list re-ranking, the
question becomes: ?should we use all the data to
re-rank the hypotheses for one source sentence, or
select some corpus chunks that are believed to be
relevant to this sentence??
Various relevance measures are proposed in
(Iyer and Ostendorf, 1999) including content-
based relevance criteria and style-based criteria. In
this paper, we use a very simple relevance metric.
Define corporaDd?s relevance to a source sentence
ft as:
R(Dd, ft) =
N?
r=1
L0(e(r)t )|Dd (11)
R(Dd, ft) estimates how well a corpus Dd can
cover the n-grams in the N -best list of a source
sentence. The higher the coverage, the more rele-
vant Dd is.
In the distributed LM architecture, the client
first sends N translations of ft to all the servers.
From the returned n-gram matching information,
client calculates R(Dd, ft) for each server, and
choose the most relevant (e.g., 20) servers for ft.
The n-gram counts returned from these relevant
servers are summed up for calculating the likeli-
hood of ft. One could also assign weights to the n-
gram counts returned from different servers during
the summation so that the relevant data has more
impact than the less-relevant ones.
5 Experiments
We used the N -best list generated by the Hiero
SMT system (Chiang, 2005). Hiero is a statis-
tical phrase-based translation model that uses hi-
erarchical phrases. The decoder uses a trigram
language model trained with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) on a 200 mil-
lion words corpus. The 1000-best list was gen-
erated on 919 sentences from the MT03 Chinese-
English evaluation set.
All the data from the English Gigaword corpus
plus the English side of the Chinese-English bilin-
gual data available from LDC are used. The 2.97
billion words data is split into 150 chunks, each
has about 20 million words. The original order
is kept so that each chunk contains data from the
same news source and a certain period of time.
For example, chunk Xinhua2003 has all the Xin-
hua News data from year 2003 and NYT9499 038
has the last 20 million words from the New York
Times 1994-1999 corpus. One could split the
data into larger(smaller) chunks which will require
less(more) servers. We choose 20 million words as
the size for each chunk because it can be loaded by
our smallest machine and it is a reasonable granu-
larity for selection.
In total, 150 corpus information servers run on
26 machines connected by the standard Ethernet
LAN. One client sends each English hypothesis
translations to all 150 servers and uses the returned
information to re-rank. The whole process takes
about 600 seconds to finish.
We use BLEU scores to measure the transla-
tion accuracy. A bootstrapping method is used to
calculate the 95% confidence intervals for BLEU
(Koehn, 2004; Zhang and Vogel, 2004).
5.1 Oracle score of the N -best list
Because of the spurious ambiguity, there are only
24,612 unique hypotheses in the 1000-best list, on
average 27 per source sentence. This limits the po-
tential of N -best re-ranking. Spurious ambiguity
is created by the decoder where two hypotheses
generated from different decoding path are con-
sidered different even though they have identical
word sequences. For example, ?the terrorist at-
tacks on the united states? could be the output of
decoding path [the terrorist attacks][on the united
219
states] and [the terrorist attacks on] [the united
states].
We first calculate the oracle score from the N -
best list to verify that there are alternative hypothe-
ses better than the model-best translation. The or-
acle best translations are created by selecting the
hypothesis which has the highest sentence BLEU
score for each source sentence. Yet a critical prob-
lem with BLEU score is that it is a function of
the entire test set and does not give meaningful
scores for single sentences. We followed the ap-
proximation described in (Collins et al, 2005) to
get around this problem. Given a test set with T
sentences, N hypotheses are generated for each
source sentence ft. Denote e(r)t as the r-th ranked
hypothesis for ft. e(1)t is the model-best hypoth-
esis for this sentence. The baseline BLEU scores
are calculated based on the model-best translation
set {e(1)t |t = 1, . . . , T}.
Define the BLEU sentence-level gain for e(r)t
as:
GBLEUe(r)t =
BLEU{e(1)1 , e(1)2 , . . . , e(r)t , . . . , e(r)T }
? BLEU{e(1)1 , e(1)2 , . . . , e(1)t , . . . , e(r)T }
GBLEUe(r)t calculates the gain if we switch the
model-best hypothesis e(1)t using e(r)t for sentence
ft and keep the translations for the rest of the test
set untouched.
With the estimated sentence level gain for each
hypothesis, we can construct the oracle best trans-
lation set by selecting the hypotheses with the
highest BLEU gain for each sentence. Oracle best
BLEU translation set is: {e(r?t )t |t = 1, . . . , T}
where r?t = argmaxr GBLEUe(r)t .
Model-best
Score Confidence Interval Oracle
BLEU 31.44 [30.49, 32.33] 37.48
Table 1: BLEU scores for the model-best and
oracle-best translations.
Table 1 shows the BLEU score of the approxi-
mated oracle best translation. The oracle score is
7 points higher than the model-best scores even
though there are only 27 unique hypotheses for
each sentence on average. This confirms our ob-
servation that there are indeed better translations
in the N -best list.
5.2 Training standard n-gram LM on large
data for comparison
Besides comparing the distributed language model
re-ranked translations with the model-best transla-
tions, we also want to compare the distributed LM
with the the standard 3-gram and 4-gram language
models on the N -best list re-ranking task.
Training a standard n-gram model for a 2.9 bil-
lion words corpora is much more complicated and
tedious than setting up the distributed LM. Be-
cause of the huge size of the corpora, we could
only manage to train a test-set specific n-gram LM
for this experiment.
First, we split the corpora into smaller chunks
and generate n-gram count files for each chunk.
Each count file is then sub-sampled to entries
where all the words are listed in the vocabulary
of the N -best list (5,522 word types). We merge
all the sub-sampled count files into one and train
the standard language model based on it.
We manage to train a 3-gram LM using the
2.97 billion-word corpus. Resulting LM requires
2.3GB memory to be loaded for the re-ranking ex-
periment.
A 4-gram LM for this N -best list is of 13 GB
in size and can not be fit into the memory. We
split the N -best list into 9 parts to reduce the vo-
cabulary size of each sub N -best list to be around
1000 words. The 4-gram LM tailored for each sub
N -best list is around 1.5 to 2 GB in size.
Training higher order standard n-gram LMs
with this method requires even more partitions of
the N -best list to get smaller vocabularies. When
the vocabulary becomes too small, the smoothing
could fail and results in unreliable LM probabili-
ties.
Adapting the standard n-gram LM for each in-
dividual source sentence is almost infeasible given
our limited computing resources. Thus we do not
have equivalent n-gram LMs to be compared with
the distributed LM for conditions where the most
relevant data chunks are used to re-rank the N -best
list for a particular source sentence.
5.3 Results
Table 2 lists results of the re-ranking experiments
under different conditions. The re-ranked trans-
lation improved the BLEU score from 31.44 to
220
32.64, significantly better than the model-best
translation.
Different metrics are used under the same data
situation for comparison. L0, though extremely
simple, gives quite nice results on N -best list re-
ranking. With only one corpus chunk (the most
relevant one) for each source sentence, L0 im-
proved the BLEU score to 32.22. We suspect that
L0 works well because it is inline with the nature
of BLEU score. BLEU measures the similarity be-
tween the translation hypothesis and human refer-
ence by counting how many n-grams in MT can
be found in the references.
Instead of assigning weights 1 to all the
matched n-grams in L0, L2 weights each n-gram
by its non-compositionality. For all data condi-
tions, L2 consistently gives the best results.
Metric family L1 is close to the standard n-gram
LM probability estimation. Because no smoothing
is used, L31 performance (32.00) is slightly worse
than the standard 3-gram LM result (32.22). On
the other hand, increasing the length of the history
in L1 generally improves the performance.
Figure 3 shows the BLEU score of the re-ranked
translation when using different numbers of rele-
vant data chunks for each sentence. The selected
data chunks may differ for each sentences. For
example, the 2 most relevant corpora for sentence
1 are Xinhua2002 and Xinhua2003 while for sen-
tence 2 APW2003A and NYT2002D are more rel-
evant. When we use the most relevant data chunk
(about 20 million words) to re-rank the N -best list,
36 chunks of data will be used at least once for
919 different sentences, which accounts for about
720 million words in total. Thus the x-axis in fig-
ure 3 should not be interpreted as the total amount
of data used but the number of the most relevant
corpora used for each sentence.
All three metrics in figure 3 show that using
all data together (150 chunks, 2.97 billion words)
does not give better discriminative powers than us-
ing only some relevant chunks. This supports our
argument in section 4 that relevance selection is
helpful in N -best list re-ranking. In some cases
the re-ranked N -best list has a higher BLEU score
after adding a supposedly ?less-relevant? corpus
chunk and a lower BLEU score after adding a
?more-relevant? chunk. This indicates that the rel-
evance measurement (Eq. 11) is not fully reflect-
ing the real ?relevance? of a data chunk for a sen-
tence. With a better relevance measurement one
 32.15
 32.2
 32.25
 32.3
 32.35
 32.4
 32.45
 32.5
 32.55
 32.6
 32.65
 32.7
 0  20  40  60  80  100  120  140  160
Ble
u S
cor
e
Number of corpus chunks used for each source sentence (*20M=corpus size used)
"L0"
"L1"
"L2"
Figure 3: BLEU score of the re-ranked best hy-
pothesis vs. the number of the most relevant cor-
pus chunks used to re-rank the n-best list for each
sentences. L0: number of n-grams matched; L1:
average interpolated n-gram conditional probabil-
ity; L2: sum of n-grams? non-compositionality.
would expect to see the curves in figure 3 to be
much smoother.
6 Related work and discussion
Yamamoto and Church (2001) used suffix arrays
to compute the frequency and location of an n-
gram in a corpus. The frequencies are used to find
?interesting? substrings which have high mutual
information.
Soricut et al (2002) build a Finite State Ac-
ceptor (FSA) to compactly represent all possible
English translations of a source sentence accord-
ing to the translation model. All sentences in a
big monolingual English corpus are then scanned
by this FSA and those accepted by the FSA are
considered as possible translations for the source
sentence. The corpus is split into hundreds of
chunks for parallel processing. All the sentences
in one chunk are scanned by the FSA on one pro-
cessor. Matched sentences from all chunks are
then used together as possible translations. The
assumption of this work that possible translations
of a source sentence can be found as exact match
in a big monolingual corpus is weak even for very
large corpus. This method can easily fail to find
any possible translation and return zero proposed
translations.
Kirchhoff and Yang (2005) used a factored 3-
gram model and a 4-gram LM (modified KN
smoothing) together with seven system scores to
re-rank an SMT N -best. They improved the
translation quality of their best baseline (Spanish-
221
# of Relevant Chunks per. Sent 1 2 5 10 20 150
3-gram KN 32.22 32.08
4-gram KN 32.22 32.53
L0 32.27 32.38 32.40 32.47 32.51 32.48
L31 32.00 32.14 32.14 32.15 32.16
L41 32.18 32.36 32.28 32.44 32.41
L51 32.21 32.33 32.35 32.41 32.37
L61 32.19 32.22 32.37 32.45 32.40 32.41
L71 32.22 32.29 32.37 32.44 32.40
L2 32.29 32.52 32.61 32.55 32.64 32.56
Table 2: BLEU scores of the re-ranked translations. Baseline score = 31.44
English) from BLEU 30.5 to BLEU 31.0.
Iyer and Ostendorf (1999) select and weight
data to train language modeling for ASR. The data
is selected based on its relevance for a topic or the
similarity to data known to be in the same domain
as the test data. Each additional document is clas-
sified to be in-domain or out-of-domain accord-
ing to cosine distance with TF-IDF term weights,
POS-tag LM and a 3-gram word LM. n-gram
counts from the in-domain and the additionally se-
lected out-of-domain data are then combined with
an weighting factor. The combined counts are
used to estimate a LM with standard smoothing.
Hildebrand et al (2005) use information re-
trieval to select relevant data to train adapted trans-
lation and language models for an SMT system.
Si et al (2002) use unigram distribution simi-
larity to select the document collection which is
most relevant to the query documents. Their work
is mainly focused on information retrieval appli-
cation.
7 Conclusion and future work
In this paper, we presented a novel distributed
language modeling solution. The distributed LM
is capable of using an arbitrarily large corpus
to estimate the n-gram probability for arbitrarily
long histories. We applied the distributed lan-
guage model to N -best re-ranking and improved
the translation quality by 4.8% when evaluated by
the BLEU metric. The distributed LM provides a
flexible architecture for relevance selection, which
makes it possible to select data for each individual
test sentence. Our experiments have shown that
relevant data has better discriminative power than
using all the data.
We will investigate different relevance weight-
ing schemes to better combine n-gram statistics
from different data sources. We are planning to
integrate the distributed LM in the statistical ma-
chine translation decoder in the near future.
8 Acknowledgement
We would like to thank Necip Fazil Ayan and
Philip Resnik for providing Hiero system?s N -best
list and allowing us to use it for this work.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270, Ann Arbor,
MI, June 2005. ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540, Ann Arbor, MI, June.
J. Goodman. 2000. A bit of progress in language
modeling. Technical report, Microsoft Research, 56
Fuchun Peng.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT conference ?Practical applications of ma-
chine translation?, pages 133?142, Budapest, May.
R. Iyer and M. Ostendorf. 1999. Relevance weighting
for combining multi-domain data for n-gram lan-
guage modeling. Comptuer Speech and Language,
13(3):267?282.
222
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 125?128, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain, July.
Claudia Leopold. 2001. Parallel and Distributed Com-
puting: A Survey of Models, Paradigms and Ap-
proaches. John Wiley & Sons, Inc., New York, NY,
USA.
Udi Manber and Gene Myers. 1993. Suffix arrays:
a new method for on-line string searches. SIAM J.
Comput., 22(5):935?948.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of the 2004 Meeting of
the North American chapter of the Association for
Computational Linguistics (NAACL-04), Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-
022), IBM Research Division, Thomas J. Watson
Research Center.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Re-
search and Development in Information Retrieval,
pages 275?281.
Luo Si, Rong Jin, Jamie Callan, and Paul Ogilvie.
2002. A language modeling framework for resource
selection and results merging. In CIKM ?02: Pro-
ceedings of the eleventh international conference
on Information and knowledge management, pages
391?397, New York, NY, USA. ACM Press.
Radu Soricut, Kevin Knight, and Daniel Marcu. 2002.
Using a large monolingual corpus to improve trans-
lation accuracy. In AMTA ?02: Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, pages 155?164,
London, UK. Springer-Verlag.
Mikio Yamamoto and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and doc-
ument frequency for all substrings in a corpus. Com-
put. Linguist., 27(1):1?30.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalu-
ation metrics. In Proceedings of The 10th Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, October.
Ying Zhang and Stephan Vogel. 2005. An effi-
cient phrase-to-phrase alignment model for arbitrar-
ily long phrase and large corpora. In Proceedings
of the Tenth Conference of the European Associa-
tion for Machine Translation (EAMT-05), Budapest,
Hungary, May. The European Association for Ma-
chine Translation.
223
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 47?50,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
CMU System Combination for WMT?09
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, USA
vogel@cs.cmu.edu
Abstract
This paper describes the CMU entry for
the system combination shared task at
WMT?09. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from several MT systems.
The sentence level features are indepen-
dent from the MT systems involved. To
compensate for various n-best list sizes in
the workshop shared task including first-
best-only entries, we normalize one of our
high-impact features for varying sub-list
size. We combined restricted data track
entries in French - English, German - En-
glish and Hungarian - English using pro-
vided data only.
1 Introduction
For the combination of machine translation sys-
tems there have been two main approaches de-
scribed in recent publications. One uses confusion
network decoding to combine translation systems
as described in (Rosti et al, 2008) and (Karakos et
al., 2008). The other approach selects whole hy-
potheses from a combined n-best list (Hildebrand
and Vogel, 2008).
Our setup follows the approach described in
(Hildebrand and Vogel, 2008). We combine the
output from the available translation systems into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on a development set to determine feature
weights and re-rank the joint n-best list.
2 Features
For our entries to the WMT?09 we used the fol-
lowing feature groups:
? Language model score
? Word lexicon scores
? Sentence length features
? Rank feature
? Normalized n-gram agreement
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008). We use two sentence length features, which
are the ratio of the hypothesis length to the length
of the source sentence and the difference between
the hypothesis length and the average length of
the hypotheses in the n-best list for the respec-
tive source sentence. We also use the rank of the
hypothesis in the original system?s n-best list as a
feature.
2.1 Normalized N-gram Agreement
The participants of the WMT?09 shared transla-
tion task provided output from their translation
systems in various sizes. Most submission were
1st-best translation only, some submitted 10-best
up to 300-best lists.
In preliminary experiments we saw that adding
a high scoring 1st-best translation to a joint n-best
list composed of several larger n-best lists does not
yield the desired improvement. This might be due
to the fact, that hypotheses within an n-best list
originating from one single system (sub-list) tend
to be much more similar to each other than to hy-
potheses from another system. This leads to hy-
potheses from larger sub-lists scoring higher in the
n-best list based features, e.g. because they collect
more n-gram matches within their sub-list, which
?supports? them the more the larger it is.
Previous experiments on Chinese-English
showed, that the two feature groups with the
highest impact on the combination result are the
language model and the n-best list based n-gram
agreement. Therefore we decided to focus on the
n-best list n-gram agreement for exploring sub-list
47
size normalization to adapt to the data situation
with various n-best list sizes.
The n-gram agreement score of each n-gram in
the target sentence is the relative frequency of tar-
get sentences in the n-best list for one source sen-
tence that contain the n-gram e, independent of
the position of the n-gram in the sentence. This
feature represents the percentage of the transla-
tion hypotheses, which contain the respective n-
gram. If a hypothesis contains an n-gram more
than once, it is only counted once, hence the max-
imum for the agreement score a(e) is 1.0 (100%).
The agreement score a(e) for each n-gram e is:
a(e) =
C
L
(1)
where C is the count of the hypotheses containing
the n-gram and L is the size of the n-best list for
this source sentence.
To compensate for the various n-best list sizes
provided to us we modified the n-best list n-gram
agreement by normalizing the count of hypotheses
that contain the n-gram by the size of the sub-list
it came from. It can be viewed as either collecting
fractional counts for each n-gram match, or as cal-
culating the n-gram agreement percentage for each
sub-list and then interpolating them. The normal-
ized n-gram agreement score anorm(e) for each n-
gram e is:
anorm(e) =
1
P
P?
j=1
Cj
Lj
(2)
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram e in the
sublist pj and Lj is the size of the sublist pj .
For the extreme case of a sub-list size of one
the fact of finding an n-gram in that hypothesis
or not has a rather strong impact on the normal-
ized agreement score. Therefore we introduce a
smoothing factor ? in a way that it has an increas-
ing influence the smaller the sub-list is:
asmooth(e) =
1
P
P?
j=1
[
Cj
Lj
(1?
?
Lj
)
]
+
[
Lj ? Cj
Lj
?
Lj
] (3)
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram in the
sublist pj and Lj is the size of the sublist pj . We
used an initial value of ? = 0.1 for our experi-
ments.
In all three cases the score for the whole hypoth-
esis is the sum over the word scores normalized
by the sentence length. We use n-gram lengths
n = 1..6 as six separate features.
3 Preliminary Experiments
Arabic-English
For the development of the modification on the n-
best list n-gram agreement feature we used n-best
lists from three large scale Arabic to English trans-
lation systems. We evaluate using the case insen-
sitive BLEU score for the MT08 test set with four
references, which was unseen data for the individ-
ual systems as well as the system combination. Ta-
ble 1 shows the initial scores of the three input sys-
tems.
system MT08
A 47.47
B 46.33
C 44.42
Table 1: Arabic-English Baselines: BLEU
To compare the behavior of the combination
result for different n-best list sizes we combined
the 100-best lists from systems A and C and then
added three n-best list sizes from the middle sys-
tem B into the combination: 1-best, 10-best and
full 100-best. For each of these four combination
options we ran the hypothesis selection using the
plain version of the n-gram agreement feature a as
well as the normalized version without anorm and
with smoothing asmooth .
combination a anorm asmooth
A & C 48.04 48.09 48.13
A & C & B1 47.84 48.34 48.21
A & C & B10 48.29 48.33 48.47
A & C & B100 48.91 48.95 49.02
Table 2: Combination results: BLEU on MT08
The modified feature has as expected no impact
on the combination of n-best lists of the same size
(see Table 2), however it shows an improvement
of BLEU +0.5 for the combination with the 1st-
best from system B. The smoothing seems to have
no significant impact for this dataset, but differ-
ent smoothing factors will be investigated in the
future.
48
4 Workshop Results
To train our language models and word lexica
we only used provided data. Therefore we ex-
cluded systems from the combination, which were
to our knowledge using unrestricted training data
(google). We did not include any contrastive sys-
tems.
We trained the statistical word lexica on the par-
allel data provided for each language pair1. For
each combination we used two language models,
a 1.2 giga-word 3-gram language model, trained
on the provided monolingual English data and a 4-
gram language model trained on the English part
of the parallel training data of the respective lan-
guages. We used the SRILM toolkit (Stolcke,
2002) for training.
For each of the three language pairs we submit-
ted a combination that used the plain version of the
n-gram agreement feature as well as one using the
normalized smoothed version.
The provided system combination development
set, which we used for tuning our feature weights,
was the same for all language pairs, 502 sentences
with only one reference.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore results
for TER and METEOR are not very meaningful
here and only reported for completeness.
4.1 French-English
14 systems were submitted to the restricted data
track for the French-English translation task. The
scores on the combination development set range
from BLEU 27.56 to 15.09 (case insensitive eval-
uation).
We received n-best lists from five systems, a
300-best, a 200-best two 100-best and one 10-best
list. We included up to 100 hypotheses per system
in our joint n-best list.
For our workshop submission we combined the
top nine systems with the last system scoring
24.23 as well as all 14 systems. Comparing the
results for the two combinations of all 14 systems
(see Table 3), the one with the sub-list normaliza-
tion for the n-gram agreement feature gains +0.8
1http://www.statmt.org/wmt09/translation-
task.html#training
BLEU on unseen data compared to the one with-
out normalization.
system dev test TER Meteor
best single 27.56 26.88 56.32 52.68
top 9 asmooth 29.85 28.07 55.23 53.90
all 14 asmooth 30.39 28.46 55.12 54.35
all 14 29.49 27.65 55.41 53.74
Table 3: French-English Results: BLEU
Our system combination via hypothesis selec-
tion could improve the translation quality by +1.6
BLEU on the unseen test set compared to the best
single system.
A  177 B* 434 C  104
177 434 104
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100% N     7
M   18
L    16
K    12
J    10
I*  264
H    41
G  110
F* 423
E* 584
D* 562
C  104
B* 434
A  177*
*
*
*
*
Figure 1: Contributions of the individual systems
to the final translation.
Figure 1 shows, how many hypotheses were
contributed by the individual systems to the fi-
nal translation (unseen data). The systems A to
N are ordered by their BLEU score on the devel-
opment set. The systems which provided n-best
lists, marked with a star in the diagram, clearly
dominate the selection. The low scoring systems
contribute very little as expected.
4.2 German-English
14 systems were submitted to the restricted data
track for the German-English translation task. The
scores on the combination development set range
49
from BLEU 27.56 to 7 (case insensitive evalua-
tion). The two lowest scoring systems at BLEU
11 and 7 were so far from the rest of the systems
that we decided to exclude them, assuming an er-
ror had occurred.
Within the remaining 12 submissions were four
n-best lists, three 100-best and one 10-best.
For our submissions we combined the top seven
systems between BLEU 22.91 and 20.24 as well as
the top 12 systems where the last one of those was
scoring BLEU 16.00 on the development set. For
this language pair the combination with the nor-
malized n-gram agreement also outperforms the
one without by +0.8 BLEU (see Table 4).
system dev test TER Meteor
best single 22.91 21.03 61.87 47.96
top 7 asmooth 25.13 22.86 60.73 49.71
top 12 asmooth 25.32 22.98 60.72 50.01
top 12 25.12 22.20 60.95 49.33
Table 4: German-English Results: BLEU
Our system combination via hypothesis selec-
tion could improve translation quality by +1.95
BLEU on the unseen test set over the best single
system.
4.3 Hungarian-English
Only three systems were submitted for the
Hungarian-English translation task. Scores on the
combination development set ranged from BLEU
13.63 to 10.04 (case insensitive evaluation). Only
the top system provided an n-best list. We used
100-best hypotheses.
system dev test TER Meteor
best single 13.63 12.73 68.75 36.76
3 sys asmooth 14.98 13.74 72.34 38.20
3 sys 14.14 13.18 74.29 37.52
Table 5: Hungarian-English Results: BLEU
We submitted combinations of the three systems
by using the modified smoothed n-gram agree-
ment feature and the plain version of the n-gram
agreement feature. Here also the normalized ver-
sion of the feature gives an improvement of +0.56
BLEU with an overall improvement of +1.0 BLEU
over the best single system (see Table 5).
5 Summary
It is beneficial to include more systems, even if
they are more than 7 points BLEU behind the best
system, as the comparison to the combinations
with fewer systems shows.
In the mixed size data situation of the workshop
the modified feature shows a clear improvement
for all three language pairs. Different smoothing
factors should be investigated for these data sets
in the future.
Acknowledgments
We would like to thank the participants in the
WMT?09 workshop shared translation task for
providing their data, especially n-best lists.
References
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254?261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of ACL-08: HLT, Short Papers, pages
81?84, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hy-
pothesis alignment for building confusion networks
with application to machine translation system com-
bination. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
50
Proceedings of the Second Workshop on Statistical Machine Translation, pages 197?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
The ISL Phrase-Based MT System for the 2007 ACL Workshop on
Statistical Machine Translation
M. Paulik1,2, K. Rottmann2, J. Niehues2, S. Hildebrand 1,2 and S. Vogel1
1Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA
2Institut fu?r Theoretische Informatik, Universita?t Karlsruhe (TH), Karlsruhe, Germany
{paulik|silja|vogel}@cs.cmu.edu ; {jniehues|rottmann}@ira.uka.de
Abstract
In this paper we describe the Interactive Sys-
tems Laboratories (ISL) phrase-based ma-
chine translation system used in the shared
task ?Machine Translation for European
Languages? of the ACL 2007 Workshop on
Statistical Machine Translation. We present
results for a system combination of the
ISL syntax-augmented MT system and the
ISL phrase-based system by combining and
rescoring the n-best lists of the two systems.
We also investigate the combination of two
of our phrase-based systems translating from
different source languages, namely Spanish
and German, into their common target lan-
guage, English.
1 Introduction
The shared task of the ACL 2007 Workshop on Sta-
tistical Machine Translation focuses on the auto-
matic translation of European language pairs. The
workshop provides common training sets for trans-
lation model training and language model training
to allow for easy comparison of results between the
participants.
Interactive Systems Laboratories participated in the
English ? Spanish Europarl and News Commen-
tary task as well as in the English ? German Eu-
roparl task. This paper describes the phrase-based
machine translation (MT) system that was applied
to these tasks. We also investigate the feasibility
of combining the ISL syntax-augmented MT system
(Zollmann et al, 2007) with our phrase-based sys-
tem by combining and rescoring the n-best lists pro-
duced by both systems for the Spanish ? English
Europarl task. Furthermore, we apply the same com-
bination technique to combine two of our phrase-
based systems that operate on different source lan-
guages (Spanish and German), but share the same
target language (English).
The paper is organized as follows. In section 2 we
give a general description of our phrase-based sta-
tistical machine translation system. Section 3 gives
an overview of the data and of the final systems
used for the English ? Spanish Europarl and News
Commentary tasks, along with corresponding per-
formance numbers. Section 4 shows the data, final
systems and results for the English ? German Eu-
roparl task. In Section 5, we present our experiments
involving a combination of the syntax-augmented
MT system with the phrase-based MT system and a
combination of the Spanish ? English and German
? English phrase-based systems.
2 The ISL Phrase-Based MT System
2.1 Word and Phrase Alignment
Phrase-to-phrase translation pairs are extracted by
training IBM Model-4 word alignments in both di-
rections, using the GIZA++ toolkit (Och and Ney,
2000), and then extracting phrase pair candidates
which are consistent with these alignments, start-
ing from the intersection of both alignments. This
is done with the help of phrase model training
code provided by University of Edinburgh during
the NAACL 2006 Workshop on Statistical Machine
Translation (Koehn and Monz, 2006). The raw rel-
197
ative frequency estimates found in the phrase trans-
lation tables are then smoothed by applying modi-
fied Kneser-Ney discounting as explained in (Foster
et al, 2006). The resulting phrase translation tables
are pruned by using the combined translation model
score as determined by Minimum Error Rate (MER)
optimization on the development set.
2.2 Word Reordering
We apply a part-of-speech (POS) based reordering
scheme (J. M. Crego et al, 2006) to the POS-tagged
source sentences before decoding. For this, we use
the GIZA++ alignments and the POS-tagged source
side of the training corpus to learn reordering rules
that achieve a (locally) monotone alignment. Fig-
ure 1 shows an example in which three reordering
rules are extracted from the POS tags of an En-
glish source sentence and its corresponding Span-
ish GIZA++ alignment. Before translation, we con-
struct lattices for every source sentence. The lattices
include the original source sentence along with all
the reorderings that are consistent with the learned
rules. All incoming edges of the lattice are anno-
tated with distortion model scores. Figure 2 gives an
example of such a lattice. In the subsequent lattice
decoding step, we apply either monotone decoding
or decoding with a reduced local reordering window,
typically of size 2.
2.3 Decoder and MER Training
The ISL beam search decoder (Vogel, 2003) com-
bines all the different model scores to find the best
translation. Here, the following models were used:
? The translation model, i.e. the phrase-to-
phrase translations extracted from the bilingual
corpus, annoted with four translation model
scores. These four scores are the smoothed for-
ward and backward phrase translation proba-
bilities and the forward and backward lexical
weights.
? A 4-gram language model. The SRI language
model toolkit was used to train the language
model and we applied modified Kneser-Ney
smoothing.
? An internal word reordering model in addition
to the already described POS-based reordering.
  
We all agree on thatPRP DT VB IN DTEn {4} esto {5} estamos {1} todos {2} de {} acuerdo {3}
? PRP DT VB IN DT :   4 ? 5 ? 1 ? 2 ? 3? PRP DT VB:   2 ? 3 ? 1 ? PRP DT VB IN:   3 ? 4 ? 1 ? 2
Figure 1: Rule extraction for the POS-based reorder-
ing scheme.
This internal reordering model assigns higher
costs to longer distance reordering.
? Simple word and phrase count models. The
former is essentially used to compensate for
the tendency of the language model to prefer
shorter translations, while the latter can give
preference to longer phrases, potentially im-
proving fluency.
The ISL SMT decoder is capable of loading
several language models (LMs) at the same time,
namely n-gram SRI language models with n up to
4 and suffix array language models (Zhang and Vo-
gel, 2006) of arbitrary length. While we typically
see gains in performance for using suffix array LMs
with longer histories, we restricted ourselves here to
one 4-gram SRI LM only, due to a limited amount
of available LM training data. The decoding process
itself is organized in two stages. First, all available
word and phrase translations are found and inserted
into a so-called translation lattice. Then the best
combination of these partial translations is found
by doing a best path search through the translation
lattice, where we also allow for word reorderings
within a predefined local reordering window.
To optimize the system towards a maximal BLEU
or NIST score, we use Minimum Error Rate (MER)
Training as described in (Och, 2003). For each
model weight, MER applies a multi-linear search
on the development set n-best list produced by the
system. Due to the limited numbers of translations
in the n-best list, these new model weights are sub-
optimal. To compensate for this, a new full trans-
lation is done. The resulting new n-best list is then
merged with the old n-best list and the optimization
process is repeated. Typically, the translation quality
converges after three iterations.
198
1
20 3
honourable1.0000
Members1.0000 honourable0.3299
Members0.6701 6
75 8
we1.0000
have1.0000
have0.9175
a0.08254,1.0000 a1.0000
?
?Honourable Members, we have a challenging agenda?
Figure 2: Example for a source sentence lattice from
the POS-based reordering scheme.
English Spanish
sentence pairs 1259914
unique sent. pairs 1240151
sentence length 25.3 26.3
words 31.84 M 33.16 M
vocabulary 266.9 K 346.3 K
Table 1: Corpus statistics for the English/Spanish
Europarl corpus.
3 Spanish? English Europarl and News
Commentary Task
3.1 Data and Translation Tasks
The systems for the English ? Spanish translation
tasks were trained on the sentence-aligned Europarl
corpus (Koehn, 2005). Detailed corpus statistics can
be found in Table 1. The available parallel News
Commentary training data of approximately 1 mil-
lion running words for both languages was only
used as additional language model training data, to
adapt our in-domain (Europarl) system to the out-of-
domain (News Commentary) task.
The development sets consist of 2000 Europarl
sentences (dev-EU) and 1057 News Commentary
sentences (dev-NC). The available development-
test data consists of 2 x 2000 Europarl sentences
(devtest-EU and test06-EU) and 1064 News Com-
mentary sentences (test06-NC). All development
and development-test sets have only one reference
translation per sentence.
3.2 Data Normalization
The ACL shared task is very close in form and con-
tent to the Final Text Editions (FTE) task of the TC-
STAR (TC-STAR, 2004) evaluation. For this rea-
son, we decided to apply a similar normalization
scheme to the training data as was applied in our TC-
STAR verbatim SMT system. Although trained on
?verbatimized? data that did not contain any num-
bers, but rather had all numbers and dates spelled
out, it yielded consistently better results than our
TC-STAR FTE SMT system. When translating FTE
content, the verbatim system treated all numbers as
unknown words, i.e. they were left unchanged dur-
ing translation. To compensate for this, we applied
extended postprocessing to the translations that con-
ducts the necessary conversions between Spanish
and English numbers, e.g. the conversion of deci-
mal comma in Spanish to decimal point in English.
Other key points which we adopted from this nor-
malization scheme were the tokenization of punc-
tuation marks, the true-casing of the first word of
each sentence, as well as extended cleaning of the
training data. The latter mainly consisted of the re-
moval of sections with a highly unbalanced source
to target words ratio and the removal of unusual
string combinations and document references, like
for example ?B5-0918/2000?, ?(COM(2000) 335 -
C5-0386/2000 - 2000/0143(CNS))?, etc.
Based on this normalization scheme, we trained and
optimized a baseline in-domain system on accord-
ingly normalized source and reference sentences.
For optimization, we combined the available de-
velopment sets for the Europarl task and the News
Commentary task. In order to further improve
the applied normalization scheme, we experimented
with replacing all numbers with the string ?NMBR?,
rather than spelling them out and by replacing all
document identifiers with the string ?DCMNT?,
rather than deleting them. This was first done for
the language model training data only, and then for
all data, i.e. for the bilingual training data and for
the development set source and reference sentences.
In the latter case, the respective tags were again re-
placed by the correct numbers and document identi-
fiers during postprocessing. Table 2 shows the case
sensitive BLEU scores for the three normalization
approaches on the English ? Spanish Europarl and
News Commentary development sets. These scores
were computed with the official NIST scoring script
against the original (not normalized) references.
3.3 In-domain System
As mentioned above, we combined the Europarl and
News Commentary development sets when optimiz-
ing the in-domain system. This resulted in only one
199
Task baseline LM only all data
Europarl 30.94 31.20 31.26
News Com. 31.28 31.39 31.73
Table 2: Case sensitive BLEU scores on the in-
domain and out-of-domain development sets for the
three different normalization schemes.
Task Eng ? Spa Spa ? Eng
dev-EU 31.29 31.77
dev-NC 31.81 31.12
devtest-EU 31.01 31.40
test06-EU 31.87 31.76
test06-NC 30.23 29.22
Table 3: Case sensitive BLEU scores for the final
English ? Spanish in-domain systems.
set of scaling factors, i.e. the in-domain system
applies the same scaling factors for translating in-
domain data as for translating out-of-domain data.
Our baseline system applied only monotone lattice
decoding. For our final in-domain system, we used a
local reordering window of length 2, which accounts
for the slightly higher scores when compared to the
baseline system. The BLEU scores for both trans-
lation directions on the different development and
development-test sets can be found in Table 3.
3.4 Out-of-domain System
In order to adapt our in-domain system towards the
out-of-domain News Commentary task, we consid-
ered two approaches based on language model adap-
tation. First, we interpolated the in-domain LM
with an out-of-domain LM computed on the avail-
able News Commentary training data. The inter-
polation weights were chosen such as to achieve a
minimal LM perplexity on the out-of-domain de-
velopment set. For both languages, the interpo-
lation weights were approximately 0.5. Our sec-
ond approach was to simply load the out-of-domain
LM as an additional LM into our decoder. In both
cases, we optimized the translation system on the
out-of-domain development data only. For the sec-
ond approach, MER optimization assigned three to
four times higher scaling factors to the consider-
ably smaller out-domain LM than to the original in-
domain LM. Table 4 shows the results in BLEU on
the out-of-domain development and development-
test sets for both translation directions. While load-
Eng ? Spa Spa ? Eng
Task interp 2 LMs interp 2 LMs
dev-NC 33.31 33.28 32.61 32.70
test06-NC 32.55 32.15 30.73 30.55
Table 4: Case sensitive BLEU scores for the final
English ? Spanish out-of-domain systems.
ing a second LM gives similar or slightly better re-
sults on the development set during MER optimiza-
tion, we see consistently worse results on the unseen
development-test set. This, in the context of the rela-
tively small amount of development data, can be ex-
plained by stronger overfitting during optimization.
4 English? German Europarl Task
The systems for the English ? German translation
tasks were trained on the sentence-aligned Europarl
corpus only. The complete corpus consists of ap-
proximately 32 million English and 30 million Ger-
man words.
We applied a similar normalization scheme to the
training data as for the English ? Spanish system.
The main difference was that we did not replace
numbers and that we removed all document refer-
ences. In the translation process, the document ref-
erences were treated as unknown words and there-
fore left unchanged. As above, we trained and op-
timized a first baseline system on the normalized
source and reference sentences. However, we used
only the Europarl task development set during opti-
mization. To achieve further improvements on the
German ? English task, we applied a compound
splitting technique. The compound splitting was
based on (Koehn and Knight, 2003) and was applied
on the lowercased source sentences. The words gen-
erated by the compound splitting were afterwards
true-cased. Instead of replacing a compound by
its separate parts, we added a parallel path into the
source sentence lattices used for translation. The
source sentence lattices were augmented with scores
on their edges indicating whether each edge repre-
sents a word of the original text or if it was gener-
ated during compound splitting.
Table 5 shows the case-sensitive BLEU scores for
the final German ? English systems. In contrast
to the English ? Spanish systems, we used only
monotonous decoding on the lattices containing the
200
task Eng ? Ger Ger ? Eng
dev-EU 18.58 23.85
devtest-EU 18.50 23.87
test06-EU 18.39 23.88
Table 5: Case sensitive BLEU scores for the final
English ? German in-domain systems.
syntactical reorderings.
5 System Combination via n-best List
Combination and Rescoring
5.1 N-best List Rescoring
For n-best list rescoring we used unique 500-best
lists, which may have less than 500 entries for
some sentences. In this evaluation, we used sev-
eral features computed from different information
sources such as features from the translation sys-
tem, additional language models, IBM-1 word lex-
ica and the n-best list itself. We calculated 4 fea-
tures from the IBM-1 word lexica: the word proba-
bility sum as well as the maximum word probabil-
ity in both language directions. From the n-best list
itself, we calculated three different sets of scores.
A position-dependent word agreement score as de-
scribed in (Ueffing and Ney, 2005) with a position
window instead of the Levenshtein alignment, the
n-best list n-gram probability as described in (Zens
and Ney, 2006) and a position-independent n-gram
agreement, which is a variation on the first two. To
tune the feature combination weights, we used MER
optimization.
Rescoring the n-best lists from our individual sys-
tems did not give significant improvements on the
available unseen development-test data. For this rea-
son, we did not apply n-best list rescoring to the indi-
vidual systems. However, we investigated the feasi-
bility of combining two different systems by rescor-
ing the joint n-best lists of both systems. The corre-
sponding results are described in the following sec-
tions.
5.2 Combining Syntax-Augmented MT and
Phrase-Based MT
On the Spanish ? English in-domain task, we par-
ticipated not only with the ISL phrase-based SMT
system as described in this paper, but also with
the ISL syntax-augmented system. The syntax-
task PHRA SYNT COMB
dev-EU 31.77 32.48 32.77
test06-EU 31.76 32.15 32.27
Table 6: Results for combining the syntax-
augmented system (SYNT) with the phrase-based
system (PHRA).
augmented system was trained on the same normal-
ized data as the phrase-based system. However, it
was optimized on the in-domain development set
only. More details on the syntax-augmented system
can be found in (Zollmann et al, 2007). Table 6
lists the respective BLEU scores of both systems as
well as the BLEU score achieved by combining and
rescoring the individual 500-best lists.
5.3 Combining MT Systems with Different
Source Languages
(Och and Ney, 2001) describes methods for trans-
lating text given in multiple source languages into a
single target language. The ultimate goal is to im-
prove the translation quality when translating from
one source language, for example English into mul-
tiple target languages, such as Spanish and German.
This can be done by first translating the English doc-
ument into German and then using the translation as
an additional source, when translating to Spanish.
Another scenario where a multi-source translation
becomes desirable was described in (Paulik et al,
2005). The goal was to improve the quality of au-
tomatic speech recognition (ASR) systems by em-
ploying human-provided simultaneous translations.
By using automatic speech translation systems to
translate the speech of the human interpreters back
into the source language, it is possible to bias the
source language ASR system with the additional
knowledge. Having these two frameworks in mind,
we investigated the possibility of combining our in-
domain German ? English and Spanish ? English
translation systems using n-best list rescoring. Ta-
ble 7 shows the corresponding results. Even though
the German ? English translation performance was
approximately 8 BLEU below the translation perfor-
mance of the Spanish ? English system, we were
able to improve the final translation performance by
up to 1 BLEU.
201
task Spa ? Eng Ger ? Eng Comb.
dev-EU 31.77 23.85 32.76
devtest-EU 31.40 23.87 32.41
test06-EU 31.76 23.88 32.51
Table 7: Results for combining the Spanish ? En-
glish and German ? English phrase-based systems
on the in-domain tasks.
6 Conclusion
We described the ISL phrase-based statistical ma-
chine translation systems that were used for the 2007
ACL Workshop on Statistical Machine Translation.
Using the available out-of-domain News Commen-
tary task training data for language model adapta-
tion, we were able to significantly increase the per-
formance on the out-of-domain task by 2.3 BLEU
for English ? Spanish and by 1.3 BLEU for Span-
ish ? English. We also showed the feasibility of
combining different MT systems by combining and
rescoring their resprective n-best lists. In particular,
we focused on the combination of our phrase-based
and syntax-augmented systems and the combination
of two phrase-based systems operating on different
source languages. While we saw only a minimal im-
provement of 0.1 BLEU for the phrase-based and
syntax-augmented combination, we gained up to 1
BLEU, in case of the multi-source translation.
References
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
Smoothing for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
J. M. Crego et al 2006. N-gram-based SMT System
Enhanced with Reordering Patterns. In Proc. of the
Workshop on Statistical Machine Translation, pages
162?165, New York, USA.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proc. of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 187?193, Budapest, Hungary.
P. Koehn and C. Monz. 2006. Manual and Automatic
Evaluation of Machine Translation between European
Langauges. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York,
USA.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
Hongkong, China.
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. In Proc. of Machine Translation Summit,
pages 253?258, Santiago de Compostela, Spain.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160 ? 167, Sapporo, Japan.
M. Paulik, S. Stueker, C. Fuegen, T. Schultz, T. Schaaf,
and A. Waibel. 2005. Speech Translation Enhanced
Automatic Speech Recognition. In Proc. of the Work-
shop on Automatic Speech Recognition and Under-
standing, San Juan, Puerto Rico.
TC-STAR. 2004. Technology and Corpora for Speech to
Speech Translation. http://www.tc-star.org.
N. Ueffing and H. Ney. 2005. Word-Level Con-
fidence Estimation for Machine Translation using
Phrase-Based Translation Models. In Proc. of HLT
and EMNLP, pages 763?770, Vancouver, British
Columbia, Canada.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Proc. of Int. Conf. on Natural Lan-
guage Processing and Knowledge Engineering, Bei-
jing, China.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 72?77, New York, USA.
Y. Zhang and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Processing.
In the Technical Report CMU-LTI-06-010, Pittsburgh,
USA.
A. Zollmann, A. Venugopal, M. Paulik, and S. Vogel.
2007. The Syntax Augmented MT (SAMT) system
at the Shared Task for the 2007 ACL Workshop on
Statistical Machine Translation. In Proc. of ACL 2007
Workshop on Statistical MachineTranslation, Prague,
Czech Republic.
202
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307?310,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CMU System Combination via Hypothesis Selection for WMT?10
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, USA
vogel@cs.cmu.edu
Abstract
This paper describes the CMU entry for
the system combination shared task at
WMT?10. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from the input MT sys-
tems, where available. The sentence level
features used are independent from the
MT systems involved. Compared to the
baseline we added source-to-target word
alignment based features and trained sys-
tem weights to our feature set. We com-
bined MT systems for French - English
and German - English using provided data
only.
1 Introduction
For the combination of machine translation sys-
tems there have been several approaches described
in recent publications. One uses confusion net-
works formed along a skeleton sentence to com-
bine translation systems as described in (Rosti et
al., 2008) and (Karakos et al, 2008). A different
approach described in (Heafield et al, 2009) is not
keeping the skeleton fixed when aligning the sys-
tems. Another approach selects whole hypotheses
from a combined n-best list (Hildebrand and Vo-
gel, 2008).
Our setup follows the latter approach. We com-
bine the output from the submitted translation sys-
tems, including n-best lists where available, into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on the provided development data to de-
termine feature weights and re-rank the joint n-
best list. We train to maximize BLEU.
2 Features
For our entries to the WMT?09 we used the follow-
ing feature groups (in parenthesis are the number
of separate feature values per group):
? Language model scores (3)
? Word lexicon scores (6)
? Sentence length features (3)
? Rank feature (1)
? Normalized n-gram agreement (6)
? Source-target word alignment features (6)
? Trained system weights (no. of systems)
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008) and details on the rank feature and the nor-
malized n-gram agreement can be found in (Hilde-
brand and Vogel, 2009). We use three sentence
length features, which are the ratio of the hypoth-
esis length to the length of the source sentence,
the diversion of this ratio from the overall length
ratio of the bilingual training data and the differ-
ence between the hypothesis length and the av-
erage length of the hypotheses in the n-best list
for the respective source sentence. The system
weights are trained together with the other feature
weights during MERT using a binary feature per
system. To the feature vector for each hypothe-
sis one feature per input system is added; for each
hypothesis one of the features is one, indicating
which system it came from, all others are zero.
2.1 Source-Target Word Alignment Features
We trained the IBM word alignment models up
to model 4 using the GIZA++ toolkit (Och and
Ney, 2003) on the bilingual training corpus. Then
a forced alignment algorithm utilizes the trained
models to align each source sentence to each trans-
lation hypothesis in its respective n-best list.
We use the alignment score given by the word
alignment models, the number of unaligned words
307
and the number of NULL aligned words, all nor-
malized by the sentence length, as three separate
features. We calculate these alignability features
for both language directions.
3 Experiments
In the WMT shared translation task only a very
small number of participants submitted n-best
lists, e.g. in the German-English track there were
only four n-best lists among the 16 submissions.
Our combination method is proven to work signif-
icantly better when n-best lists are available.
For all our experiments on the data from
WMT?09, which was available for system combi-
nation development as well as the WMT?10 shared
task data we used the same setup and the same sta-
tistical models.
To train our language models and word lexica
we only used provided data. We trained the sta-
tistical word lexica on the parallel data provided
for each language pair1. For each combination we
used three language models: a 4-gram language
model trained on the English part of the parallel
training data, a 1.2 giga-word 3-gram language
model trained on the provided monolingual En-
glish data, and an interpolated 5-gram language
model trained on the English GigaWord corpus.
We used the SRILM toolkit (Stolcke, 2002) for
training. We chose to train three separate LMs
for the three corpora, so the feature weight train-
ing can automatically determine the importance of
each corpus for this task. The reason for training
only a 3-gram LM from the wmt10 monolingual
data was simply that there were not sufficient time
and resources available to train a bigger model.
For each of the two language pairs we compared
a combination that used the word alignment fea-
tures, or trained system weights or both of these
feature groups in addition to the features described
in (Hildebrand and Vogel, 2009) which serves a
baseline for this set of experiments.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore TER
results might not be very meaningful here and are
only reported for completeness.
1http://www.statmt.org/wmt10/translation-
task.html#training
3.1 French-English data from WMT?09
We used 14 systems from the restricted data track
of the WMT?09 including five n-best lists. The
scores of the individual systems for the combina-
tion tuning set range from BLEU 27.93 for the best
to 15.09 for the lowest ranked individual system
(case insensitive evaluation).
system tune test
best single 27.93 / 56.53 27.21 / 56.99
baseline 30.17 / 54.76 28.89 / 55.74
+ wrd al 30.67 / 54.34 28.69 / 55.67
+ sys weights 29.71 / 55.45 28.07 / 56.18
all features 30.30 / 54.53 28.37 / 55.77
Table 1: French-English Results: BLEU / TER
The combination outperforms the best single
system by 1.7 BLEU points. Here adding the 14
binary features for training system weights with
MERT hurts the combinations performance on the
unseen data. The reason for this might be the
rather small tuning set of 502 sentences with one
reference. Adding the word alignment features
does not improve the result either, the difference
to the baseline is at the noise level.
3.2 German-English data from WMT?09
For our experiments on the development data for
German-English we used the top 12 systems, scor-
ing between BLEU 23.01 and BLEU 16.06, ex-
cluding systems known to use data beyond the pro-
vided data. Within those 12 system outputs were
four n-best lists, three of which were 100-best and
one was 10-best.
system tune test
best single 23.01 / 60.52 21.44 / 62.33
baseline 26.28 / 58.69 23.62 / 60.49
+ wrd al 26.25 / 59.13 23.42 / 61.11
+ sys weights 26.78 / 58.48 23.28 / 60.80
all features 26.81 / 58.12 23.51 / 60.25
Table 2: German-English Results: BLEU / TER
Our system combination via hypothesis selec-
tion could improve translation quality by +2.2
BLEU over the best single system on the unseen
test set. Again, the differences between the four
different feature sets are not significant on the un-
seen test set.
308
3.3 French-English WMT?10 system
combination shared task
Out of 14 systems submitted to the French-English
translation task, we combined the top 11 systems,
the best of which scored 28.58 BLEU and the last
24.16 BLEU on the tuning set. There were only
three n-best lists among the submissions. We in-
cluded up to 100 hypotheses per system in our
joint n-best list.
system tune test
best sys. 28.58 / 54.17 29.98 / 52.62 / 53.88
baseline 30.67 / 52.62 29.94 / 52.53 / -
+ w. al 30.69 / 52.76 29.97 / 52.76 / 53.76
+ sys w. 30.90 / 52.44 29.79 / 52.84 / 54.05
all feat. 31.10 / 52.06 29.80 / 52.86 / 53.67
Table 3: French-English Results: BLEU / TER /
MaxSim
Our system combination via hypothesis selec-
tion could not improve the translation quality com-
pared to the best single system on the unseen data.
Adding any of the new feature groups to the base-
line does not change the result of the combination
significantly. This result could be explained by the
fact, that due to computational problems and time
constraints we were not able to train our models on
the whole provided French-English training data.
This should only affect the lexicon and word align-
ment feature groups though.
3.4 German-English WMT?10 system
combination shared task
For the German-English combination we used 13
out of the 16 submitted systems, which scored be-
tween BLEU 25.01 to BLEU 19.76 on the tuning
set. Our combination could improve translation
quality by +1.64 BLEU compared to the best sys-
tem.
system tune test
best sys. 25.01 / 58.34 23.89 / 59.14 / 51.10
baseline 26.47 / 56.89 25.44 / 57.96 / -
+ w. al 26.37 / 57.02 25.25 / 58.34 / 50.72
+ sys w. 27.67 / 56.05 25.53 / 57.70 / 51.06
all feat. 27.66 / 56.35 25.25 / 57.86 / 50.83
Table 4: German-English Results: BLEU / TER /
MaxSim
The word alignment features seem to hurt per-
formance slightly, which might be due to the more
	 
	   	 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 373?379,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
MT Quality Estimation: The CMU System for WMT?13
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Qatar Computing Research Institute
Doha, Qatar
svogel@qf.org.qa
Abstract
In this paper we present our entry to the
WMT?13 shared task: Quality Estima-
tion (QE) for machine translation (MT).
We participated in the 1.1, 1.2 and 1.3
sub-tasks with our QE system trained on
features from diverse information sources
like MT decoder features, n-best lists,
mono- and bi-lingual corpora and giza
training models. Our system shows com-
petitive results in the workshop shared
task.
1 Introduction
As MT becomes more and more reliable, more
people are inclined to use automatically translated
texts. If coming across a passage that is obviously
a mistranslation, any reader would probably start
to doubt the reliability of the information in the
whole article, even though the rest might be ad-
equately translated. If the MT system had a QE
component to mark translations as reliable or pos-
sibly erroneous, the reader would know to use in-
formation from passages marked as bad transla-
tions with caution, while still being able to trust
other passages. In post editing a human translator
could use translation quality annotation as an indi-
cation to whether editing the MT output or trans-
lating from scratch might be faster. Or he could
use this information to decide where to start in or-
der to improve the worst passages first or skip ac-
ceptable passages altogether in order to save time.
Confidence scores can also be useful for applica-
tions such as cross lingual information retrieval or
question answering. Translation quality could be
a valuable ranking feature there.
Most previous work in the field estimates con-
fidence on the sentence level (e.g. Quirk et
al. (2004)), some operate on the word level (e.g.
Ueffing and Ney (2007), Sanchis et al (2007),
and Bach et al (2011)), whereas Soricut and Echi-
habi (2010) use the document level.
Various classifiers and regression models have
been used in QE in the past. Gandrabur and Foster
(2003) compare single layer to Multi Layer Per-
ceptron (MLP), Quirk et al (2004) report that Lin-
ear Regression (LR) produced the best results in
a comparison of LR, MLP and SVM, Gamon et
al. (2005) use SVM, Soricut and Echihabi (2010)
find the M5P tree works best among a number of
regression models, while Bach et al (2011) define
the problem as a word sequence labeling task and
use MIRA.
The QE shared task was added to the WMT
evaluation campaign in 2012 (Callison-Burch et
al., 2012), providing standard training and test
data for system development.
2 WMT?13 Shared Task
In this WMT Shared Task for Quality Estima-
tion1 there were tasks for sentence and word level
QE. We participated in all sub-tasks for Task 1:
Sentence-level QE.
Task 1.1: Scoring and ranking for post-editing
effort focuses on predicting HTER per segment
for the translations of one specific MT system.
Task 1.2: System selection/ranking required to
predict a ranking for up to five translations of
the same source sentence by different MT sys-
tems. The training data provided manual labels for
ranking including ties. Task 1.3: Predicting post-
editing time participants are asked to predict the
time in seconds a professional translator will take
to post edit each segment.
1http://www.statmt.org/wmt13/quality-estimation-
task.html
373
Besides the training data with labels, for each
of these tasks additional resources were provided.
These include bilingual training corpora, language
models, 1000-best lists, models from giza and
moses training and various other statistics and
models depending on task and language pair.
3 Features
3.1 Language Models
To calculate language model (LM) features, we
train traditional n-gram language models with n-
gram lengths of four and five using the SRILM
toolkit (Stolcke, 2002). We calculate our features
using the KenLM toolkit (Heafield, 2011). We
normalize all our features with the target sentence
length to get an average word feature score, which
is comparable for translation hypotheses of differ-
ent length. In addition to the LM probability we
record the average n-gram length found in the lan-
guage model for the sentence, the total number of
LM OOVs and OOVs per word, as well as the
maximum and the minimum word probability of
the sentence, six features total.
We use language models trained on source lan-
guage data and target language data to measure
source sentence difficulty as well as translation
fluency.
3.2 Distortion Model
The moses decoder uses one feature from a dis-
tance based reordering model and six features
from a lexicalized reordering model: Given a
phrase pair, this model considers three events
Monotone, Swap, and Discontinuous in two direc-
tions Left and Right. This results in six events:
LM (left-monotone), LS (left-swap), LD (left-
discontinuous) and RM (right-monotone), RS,
RD.
These distortion features are calculated for each
phrase. For a total sentence score we normalize by
the phrase count for each of the seven features.
3.3 Phrase Table
From the phrase table we use the features from
the moses decoder output: inverse phrase trans-
lation probability, inverse lexical weighting, di-
rect phrase translation probability and direct lex-
ical weighting. For a total sentence score we nor-
malize by the phrase count. We use the number
of phrases used to generate the hypothesis and the
average phrase length as additional features, six
features total.
3.4 Statistical Word Lexica
From giza training we use IBM-4 statistical word
lexica in both directions. We use six probabil-
ity based features as described in Hildebrand and
Vogel (2008): Normalized probability, maximum
word probability and word deletion count from
each language direction.
To judge the translation difficulty of each word
in the source sentence we collect the number of
lexicon entries for each word similar to Gandrabur
and Foster (2003). The intuition is, that a word
with many translation alternatives in the word-to-
word lexicon is difficult to translate while a word
with only a few translation choices is easy to trans-
late.
In fact it is not quite this straight forward. There
are words in the lexicon, which have many lex-
icon entries, but the probability for them is not
very equally distributed. One entry has a very
high probability while all others have a very low
one - not much ambiguity there. Other words
on the other hand have several senses in one lan-
guage and therefore are translated frequently into
two or three different words in the target language.
There the top entries in the lexicon might each
have about 30% probability. To capture this be-
havior we do not only count the total number of
entries but also the number of entries with a prob-
ability over a threshold of 0.01.
For example one word with a rather high num-
ber of different translations in the English-Spanish
statistical lexicon is the period (.) with 1570 en-
tries. It has only one translation with a probability
over the threshold which is the period (.) in Span-
ish at a probability of 0.9768. This shows a clear
choice and rather little ambiguity despite the high
number of different translations in the lexicon.
For each word we collect the number of lexi-
con entries, the number of lexicon entries over the
threshold, the highest probability from the lexicon
and whether or not the word is OOV. If a word has
no lexicon entry with a probability over the thresh-
old we exclude the word from the lexicon for this
purpose and count it as an OOV. As sentence level
features we use the sum of the word level features
normalized by the sentence length as well as the
total OOV count for the sentence, which results in
five features.
374
3.5 Sentence Length Features
The translation difficulty of a source sentence is
often closely related to the sentence length, as
longer sentences tend to have a more complex
structure. Also a skewed ratio between the length
of the source sentence and its translation can be an
indicator for a bad translation.
We use plain sentence length features, namely
the source sentence length, the translation hypoth-
esis length and their ratio as introduced in Quirk
(2004).
Similar to Blatz et al (2004) we use the n-best
list as an information source. We calculate the av-
erage hypothesis length in the n-best list for one
source sentence. Then we compare the current hy-
pothesis to that and calculate both the diversion
from that average as well as their ratio. We also
calculate the source-target ratio to this average hy-
pothesis length.
To get a representative information on the
length relationship of translations from the source
and target languages in question, we use the par-
allel training corpus. We calculate the overall lan-
guage pair source to target sentence length ratio
and record the diversion of the current hypothesis?
source-target ratio from that.
The way sentences are translated from one lan-
guage to another might differ depending on how
complex the information is, that needs to be con-
veyed, which in turn might be related to the sen-
tence length and the ratio between source and
translation. As a simple way of capturing this
phenomenon we divide the parallel training cor-
pus into three classes (short, medium, long) by
the length of the source language sentence. The
boundaries of these classes are the mean 26.84
plus and minus the standard deviation 14.54 of the
source sentence lengths seen in the parallel cor-
pus. We calculate the source/target length ratio for
each of the three classes separately. The resulting
statistics for the parallel training corpora can be
found in Table 1. For English - Spanish the ratio
for all classes is close to one, for other language
pairs these differ more clearly.
As features for each hypothesis we use a binary
indicator for its membership to each class and its
deviation from the length ratio of its class. This
results in 12 sentence length related features in to-
tal.
En train
number of sentences 1,714,385
average length 26.84
standard deviation 14.54
class short 0 - 12.29
class medium 12.29 - 41.38
class long 41.38 - 100
s/t ratio overall 0.9624
s/t ratio for short 0.9315
s/t ratio for medium 0.9559
s/t ratio for long 0.9817
Table 1: Sentence Length Statistics for the
English-Spanish Parallel Corpus
3.6 Source Language Word and Bi-gram
Frequency Features
The length of words is often related to whether
they are content words and how frequently they
are used in the language. Therefore we use the
maximum and average word length as features.
Similar to Blatz et al (2004) we sort the vo-
cabulary of the source side of the training corpus
by occurrence frequency and then divide it into
four parts, each of which covers 25% of all to-
kens. As features we use the percentage of words
in the source sentence that fall in each quartile.
Additionally we use the number and percentage of
source words in the source sentence that are OOV
or very low frequency, using count 2 as threshold.
We also collect all bigram statistics for the cor-
pus and calculate the corresponding features for
the source sentence based on bigrams. This adds
up to fourteen features from source word and cor-
pus statistics.
3.7 N-Best List Agreement & Diversity
We use the three types of n-best list based features
described in Hildebrand and Vogel (2008): Posi-
tion Dependent N-best List Word Agreement, Po-
sition independent N-best List N-gram Agreement
and N-best List N-gram Probability.
To measure n-best list diversity, we compare
the top hypothesis to the 5th, 10th, 100th, 200th,
300th, 400th and 500th entry in the n-best list
(where they exist) to see how much the transla-
tion changes throughout the n-best list. We calcu-
late the Levenshtein distance (Levenshtein, 1966)
between the top hypothesis and the three lower
ranked ones and normalize by the sentence length
375
of the first hypothesis. We also record the n-best
list size and the size of the vocabulary in the n-
best list for each source sentence normalized by
the source sentence length.
Fifteen agreement based and nine diversity
based features add up to 24 n-best list based fea-
tures.
3.8 Source Parse Features
The intuition is that a sentence is harder to trans-
late, if its structure is more complicated. A sim-
ple indicator for a more complex sentence struc-
ture is the presence of subclauses and also the
length of any clauses and subclauses. To obtain the
clause structure, we parse the source language sen-
tence using the Stanford Parser2 (Klein and Man-
ning, 2003). Features are: The number of clauses
and subclauses, the average clause length, and the
number of sentence fragments found. If the parse
does not contain a clause tag, it is treated as one
clause which is a fragment.
3.9 Source-Target Word Alignment Features
A forced alignment algorithm utilizes the trained
alignment models from the MT systems GIZA
(Och and Ney, 2003) training to align each source
sentence to each translation hypothesis.
We use the score given by the word alignment
models, the number of unaligned words and the
number of NULL aligned words, all normalized
by the sentence length, as three separate features.
We calculate those for both language directions.
Hildebrand and Vogel (2010) successfully applied
these features in n-best list re-ranking.
3.10 Cohesion Penalty
Following the cohesion constraints described in
Bach et al (2009) we calculate a cohesion penalty
for the translation based on the dependency parse
structure of the source sentence and the word
alignment to the translation hypothesis. To obtain
these we use the Stanford dependency parser (de
Marneffe et al, 2006) and the forced alignment
from Section 3.9.
For each head word we collect all dependent
words and also their dependents to form each com-
plete sub-tree. Then we project each sub-tree onto
the translation hypothesis using the alignment. We
test for each sub-tree, whether all projected words
in the translation are next to each other (cohesive)
2http://nlp.stanford.edu/software/lex-parser.shtml
or if there are gaps. From the collected gaps we
subtract any unaligned words. Then we count the
number of gaps as cohesion violations as well as
how many words are in each gap. We go recur-
sively up the tree, always including all sub-trees
for each head word. If there was a violation in
one of the sub-trees it might be resolved by adding
in its siblings, but if the violation persists, it is
counted again.
4 Classifiers
For all experiments we used the Weka3 data min-
ing toolkit described in Hall et. al. (2009) to com-
pare four different classifiers: Linear Regression
(LR), M5P tree (M5Ptree), Multi Layer Percep-
tron (MLP) and Support Vector Machine for Re-
gression (SVM). Each of these has been identi-
fied as effective in previous publications. All but
one of the Weka default settings proved reliable,
changing the learning rate for the MLP from de-
fault: 0.3 to 0.01 improved the performance con-
siderably. We report Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE) for all re-
sults.
5 Experiment Results
For Tasks 1.1 and 1.3 we used the 1000-best out-
put provided. As first step we removed duplicate
entries in these n-best list. This brought the size
down to an average of 152.9 hypotheses per source
sentence for the Task 1.1 training data, 172.7 on
the WMT12 tests set and 204.3 hypotheses per
source sentence on the WMT13 blind test data.
The training data for task 1.3 has on average 129.0
hypothesis per source sentence, the WMT13 blind
test data 129.8.
In addition to our own features described above
we extracted the 17 features used in the WMT12
baseline for all sub-tasks via the software provided
for the WMT12-QE shared task.
5.1 Task 1.1
Task 1.1 is to give a quality score between 0 and
1 for each segment in the test set, predicting the
HTER score for the segment and also to give a
rank for each segment, sorting the entire test set
from best quality of translation to worst.
For Task 1.1 our main focus was the scoring
task. We did submit a ranking for the blind test
3http://www.cs.waikato.ac.nz/ml/weka/
376
wmt12-test: WMT12 manual quality labels
WMT12 best system: Language Weaver 0.61 - 0.75
WMT12 baseline system 0.69 - 0.82
feat. set #feat LR M5Pt MLP SVM
full 117 0.617 - 0.755 0.618 - 0.756 0.619 - 0.773 0.609 - 0.750
no WMT12-base 100 0.618 - 0.766 0.618 - 0.767 0.603 - 0.757 0.611 - 0.761
slim 69 0.621 - 0.767 0.621 - 0.766 0.614 - 0.768 0.627 - 0.773
wmt12-test: HTER
full 117 0.125 - 0.162 0.126 - 0.163 0.122 - 0.156 0.121 - 0.156
no WMT12-base 100 0.124 - 0.160 0.123 - 0.159 0.125 - 0.159 0.121 - 0.155
slim 69 0.125 - 0.161 0.126 - 0.161 0.124 - 0.159 0.123 - 0.158
wmt13-test: HTER
WMT12 baseline system 0.148 - 0.182
full 117 0.146 - 0.183 0.147 - 0.185 0.156 - 0.199 0.142 - 0.180
no WMT12-base 100 0.144 - 0.180 0.144 - 0.180 0.156 - 0.203 0.139 - 0.176
slim 69 0.147 - 0.182 0.147 - 0.181 0.153 - 0.194 0.142 - 0.177
Table 2: Task 1.1: Results in MAE and RMSE on the WMT12 test set for WMT12 manual labels as well
as WMT13 HTER as target class and the WMT13 test set for HTER
set as well, which resulted from simply sorting the
test set by the estimated HTER per segment.
In Table 2 we show the results for some ex-
periments comparing the performance of differ-
ent feature sets and classifiers. For development
we used the WMT12-QE test set and both the
WMT12 manual labels as well as HTER as target
class. We compared the impact of removing the
17 WMT12-baseline features ?no WMT12-base?
and training a ?slim? system by removing nearly
half the features, which showed to have a smaller
impact on the overall performance in preliminary
experiments. Among the removed features are
n-best list based features, redundant features be-
tween ours, the moses based and the base17 fea-
tures and some less reliable features like e.g. the
lexicon deletion features, who?s thresholds need to
be calibrated carefully for each new language pair.
We submitted the full+MLP and the no-WMT12-
base+SVM output to the shared task, shown in
bold in the table.
The official result for our system for task 1.1
on the WMT13 blind data is MAE 13.84, RMSE
17.46 for the no-WMT12-base+SVM system and
MAE 15.25 RMSE 18.97 for the full+MLP sys-
tem. Surprising here is the fact that our full system
clearly outperforms the 17-feature baseline on the
WMT12 test set, but is behind it on the WMT13
blind test set. (Baseline bb17 SVM: MAE 14.81,
RMSE 18.22) Looking at the WMT13 test set re-
sults, we should have chosen the slim+SVM sys-
tem variant.
5.2 Task 1.2
Task 1.2 asks to rank different MT systems by
translation quality on a segment by segment basis.
Since the manually annotated ranks in task 1.2
allowed ties, we treated them as quality scores and
ran the same QE system on this data as we did
for task 1.1. We submitted the full-MLP output
with the only difference that for this data set the
decoder based features were not available. We
rounded the predicted ranks to integer. Since the
training data contains many ties we did not employ
a strategy to resolve ties.
As a contrastive approach we ran the hypothe-
sis selection system described in Hildebrand and
Vogel (2010) using the BLEU MT metric as rank-
ing criteria. For this system it would have been
very beneficial to have access to the n-best lists
for the different system?s translations. The BLEU
score for the translation listed as the first system
for each source sentence would be 30.34 on the
entire training data. We ran n-best list re-ranking
using MERT (Och, 2003) for two feature sets: The
full feature set, 100 features in total and a slim fea-
ture set with 59 features. For the slim feature set
we removed all features that are solely based on
377
the source sentence, since those have no impact on
re-ranking an n-best list. The BLEU score for the
training set improved to 45.25 for the full feature
set and to 45.76 for the slim system. Therefore
we submitted the output of the slim system to the
shared task. This system does not predict ranks
directly, but estimates ranking according to BLEU
gain on the test set. Therefore the new ranking is
always ranks 1-5 without ties.
The official result uses Kendalls tau with and
without ties penalized. Our two submissions
score: ?0.11 /?0.11 for the BLEU optimized sys-
tem and?0.63 / 0.23 for the classifier system. The
classifier system is the best submission in the ?ties
ignored? category.
5.3 Task 1.3
Task 1.3 is to estimate post editing time on a per
segment basis.
In absence of a development test set we used
10-fold cross-validation on the training data to de-
termine the best feature set and classifier for the
two submissions. Table 3 shows the results on our
preliminary tests for four classifiers and three fea-
ture sets. The ?no pr.? differs from the full fea-
ture set only by removing the provided features, in
this case the 17 WMT12-baseline features and the
?translator ID? and ?nth in doc? features. For the
?slim? system run the feature set size was cut in
half in order to prevent overfitting to the training
data since the training data set is relatively small.
We used the same criteria as in Task 1.1. For
the shared task we submitted the full+SVM and
slim+LR variants, shown in bold in the table.
The official result for our entries on the WMT13
blind set in MAE and RMSE are: 53.59 - 92.21 for
the full system and 51.59 - 84.75 for the slim sys-
tem. The slim system ranks 3rd for both metrics
and outperforms the baseline at 51.93 - 93.36.
6 Conclusions
In this WMT?13 QE shared task we submitted to
the 1.1, 1.2 and 1.3 sub-tasks. In development we
focused on the scoring type tasks.
In general there don?t seem to be significant dif-
ferences between the different classifiers.
Surprising is the fact that our full system for
task 1.1 clearly outperforms the 17-feature base-
line on the WMT12 test set, but is behind it on
the WMT13 blind test set. This calls into ques-
tion whether the performance on the WMT12 test
set was the right criterium for selecting a system
variant for submission.
The relative success of the ?slim? system vari-
ant over the full feature set shows that our system
would most likely benefit from a sophisticated fea-
ture selection method. We plan to explore this in
future work.
References
Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009.
Cohesive constraints in a beam search phrase-based
decoder. In HLT-NAACL (Short Papers), pages 1?4.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 211?219, Portland, Oregon, USA, June. As-
sociation for Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. Technical report,
Final report JHU / CLSP 2003 Summer Workshop,
Baltimore.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC-06.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level mt evaluation without refer-
ence translations: Beyond language modeling. In
In European Association for Machine Translation
(EAMT.
Simona Gandrabur and George Foster. 2003. Con-
fidence estimation for translation prediction. In In
Proceedings of CoNLL-2003, page 102.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
378
feat. set #feat class. 10-fold cross train WMT13 test
full 119 LR 45.73 - 73.52 39.74 - 63.92 54.45 - 88.68
full 119 M5Pt 44.49 - 74.05 35.81 - 57.36 50.05 - 85.22
full 119 MLP 48.05 - 75.68 41.03 - 68.70 54.38 - 88.93
full 119 SVM 40.88 - 73.61 34.70 - 69.69 53.74 - 92.26
no pr 100 LR 46.06 - 74.94 40.39 - 66.00 52.13 - 86.68
no pr 100 M5Pt 43.80 - 74.30 36.80 - 59.47 50.86 - 87.42
no pr 100 MLP 47.70 - 75.41 39.85 - 68.30 52.39 - 87.93
no pr 100 SVM 41.35 - 74.68 35.59 - 70.99 52.87 - 92.22
slim 59 LR 44.72 - 73.86 41.14 - 67.44 51.71 - 84.83
slim 59 M5Pt 43.77 - 74.43 35.26 - 56.84 57.75 - 102.68
slim 59 MLP 46.98 - 74.38 40.35 - 69.79 51.06 - 85.48
slim 59 SVM 40.42 - 74.47 36.88 - 71.59 51.09 - 90.18
Table 3: Task 1.3: Results in MAE and RMSE for 10-fold cross validation and the whole training set as
well as the WMT13 blind test set
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254?261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Almut Silja Hildebrand and Stephan Vogel. 2010.
CMU system combination via hypothesis selec-
tion for WMT?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 307?310. Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Christopher B. Quirk. 2004. Training a sentence-
level machine translation confidence measure. In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation, pages 825?
828, Lisbon, Portugal, May. LREC.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
379
