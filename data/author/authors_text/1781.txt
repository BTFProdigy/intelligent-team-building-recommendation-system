Linking syntactic and semantic arguments in a dependency-based formalism
Christian Korthals and Ralph Debusmann
Computational Linguistics
Universita?t des Saarlandes, Geb. 17
Postfach 15 11 50
66041 Saarbu?cken, Germany
(cnkortha|rade)@coli.uni-sb.de
Abstract
We propose a formal characterization of variation
in the syntactic realization of semantic arguments,
using hierarchies of syntactic relations and thematic
roles, and a mechanism of lexical inheritance to ob-
tain valency frames from individual linking types.
We embed the formalization in the new lexicalized,
dependency-based grammar formalism of Topologi-
cal Dependency Grammar (TDG) (Duchier and De-
busmann, 2001). We account for arguments that can
be alternatively realized as a NP or a PP, and model
thematic role alternations. We also treat auxiliary
constructions, where the correspondance between
syntactic and semantic argumenthood is indirect.1
1 Introduction
This paper deals with the mapping (or linking) of se-
mantic predicate-argument structure to surface syn-
tactic realizations. We present a formal architecture
in the framework of a multi-dimensional, heavily
lexicalized, efficiently parsable dependency formal-
ism (Duchier and Debusmann, 2001), which uses
lexical inheritance as a means to explicitly model
syntactic variation. We concentrate on variation be-
tween prepositional phrases and nominal phrases
which realize verbal arguments, and remedy prob-
lems that occur with this kind of variation in recent
approaches like the HPSG linking architecture pro-
posed by (Davis, 1998).
Section 2 presents and analyses some of the prob-
lematic data we can model, English dative shift, op-
tional complements and thematic role alternations.
Section 3 compares the HPSG account with less for-
mal valency or dependency based approaches and
comments on the shortcomings, focusing on the
treatment of PP complements. We then present a
new account in the formal framework of Topologi-
1The authors wish to thank Denys Duchier and Geert-Jan
Kruijff for lots of helpful comments on this paper.
cal Dependency Grammar (TDG) by adding a new
representational level (thematic structure addition-
ally to ID structure) to the framework in Section
4.1 and introducing the concept of a valency frame
in the TDG inheritance lexicon (Sections 4.2 and
4.3). We then show how we use syntactic role hi-
erarchies to account for the data in a linguistically
concise way and define admissibility conditions for
a TDG derivation. Section 5 contrasts our analysis
of the dative shift construction with the analysis of
thematic role alternations.
2 Linguistic Data
Insights from corpus studies (e.g. the NEGRA
treebank for German (Skut et al, 1998), or the
material annotated in the Framenet (Baker et al,
1998) project on the basis of The Bank of English
(Cobuild, 2001) show that the syntactic patterns
specific verbs occur with vary stongly. Not only do
we observe different patterns for different verbs, but
also alternative patterns with the same verbs. (1)
to (6) illustrate the well-known dative shift (Levin,
1993):45 phenomenon, which occurs with a re-
stricted class of verbs only. While the distinction
between (2) and (4) can be explained in terms of
lexical semantics, even semantically closely related
verbs as English give and deliver can differ in their
syntactic behaviour, as the contrast between (1) and
(5) shows.
(1) [The postman] gave [him] [a package].
(2) [The postman] gave [a package] [to him].
(3) [The postman] charged [him] [5 Euros].
(4) *[The postman] charged [5 Euros] [to him].
(5) *[The postman] delivered [him] [a package].
(6) [The postman] delivered [a package] [to him].
In contrast to (Davis, 1998):3:562 for instance,
we do not assume a difference in meaning between
(1) and (2).3 Therefore, in order to compute a se-
mantics from this data without spurious ambigu-
ity, we must be able to express the semantic gen-
eralisation that to him and him realize the same
semantic argument. It is useful to employ the-
matic roles in the lexicon and in grammatical de-
scriptions for this purpose.4 See e.g. (Tarvainen,
1987), (Helbig, 1989), (Sgall et al, 1986) or the
Framenet project (Baker et al, 1998) for differ-
ent application-oriented sets of thematic roles (or
?frame elements?). For discussion see e.g. (Helbig,
1995) or (Kruijff, 2001), for criticism see (Dowty,
1991). We can also use thematic roles to structure
verbs into an ontology, as e.g. attempted by (Helbig,
1995), (Davis, 1998) or (Baker et al, 1998) in order
to make semantic predictions for syntactic valency
patterns. For instance, it is a regularity in English
that verbs of charging do not show the dative shift
(Levin, 1993), while verbs of change of possession
sometimes do.
Now consider the set of German examples in (7)
to (11), which all roughly express the proposition
Peter robs her of her money. All of the patterns are
attested in the NEGRA corpus (Skut et al, 1998),
but (10) cannot be found.
(7) [Peter]
Peter
beraubt
robs
[sie]
her+ACC
[ihres
her+GEN
Bargelds].
cash+GEN
?Peter robs her of her cash.?
(8) [Peter]
Peter
beraubt
robs
[sie]
her+ACC
[um
of
ihr
her
Bargeld].
cash
?Peter robs her of her cash.?
(9) [Peter] beraubt [sie].
(10) *[Peter] beraubt.
(11) [Peter]
Peter
raubt.
robs
?Peter steals.?
The data illustrates that it can be a lexical prop-
erty of verbs to allow or prohibit omission of their
2We cite the chapter and the page num-
ber of the online postscript version at
http://www-csli.stanford.edu/?tdavis/
3We expect English give to have at least two separate mean-
ings (a) cause someone to physically have something and (b) to
cause someone trouble, pain, etc. with different lexical entries,
following established lexicographic practice. While the lexical
entry for meaning (b) will exhibit the syntactic pattern illus-
trated by (1) only (*To give headache to someone), the entry
for meaning (a) exhibits both the patterns in (1) and (2).
4Note that we do not commit ourselves to a specific set of
thematic roles in this paper.
complements (Levin, 1993):33, (Helbig, 1995):99.
Therefore, we will analyse syntactic arguments in
terms of optionality and obligatoriness. Note that
this distinction is not predictable from the thematic
roles realized by the syntactic elements (e.g. by
distinguishing inner and outer roles in (Sgall et
al., 1986) and (Tarvainen, 1987)) nor by the syn-
tactic form or even function of the syntactic ele-
ments. Neither is the distinction between obliga-
tory and optional elements the same as the comple-
ment/adjunct distinction.
We analyse (1) to (6) as alternative realizations
of a thematic role, because one semantic argument
(the PATIENT) can either be realized as indirect ob-
ject or PP, while the THEME is always realized as a
direct object NP. Compare this data to alternations
as in (12) and (13). Here, additionally, one syntac-
tic function (direct object) is open for either of two
thematic roles (Levin, 1993).
(12) [He] cleared [the dirt] [from the table].
(13) [He] cleared [the table] [of the dirt].
We will show in Section 4 how we can account
for the data illustrated in this section in a lexical-
ized dependency grammar formalism and show that
the linguistic view taken above helps to reduce re-
dundancy in the lexicon.
3 Alternative approaches
The approach taken in this paper formalizes notions
that have only been informally employed in depen-
dency grammar. (Helbig, 1995):167 defines valency
frames on a formal syntactic and functional syntac-
tic level, a thematic role level and a logical level in
his 6-level-model, but only informally and for the
purpose of learners? dictionaries. There is a long
tradition in German lexicography which has pro-
duced a number of valency dictionaries (e.g. (Hel-
big and Schenkel, 1975), (Fischer, 1997), (Engel
and Schumacher, 1976)). The syntactic analyses in
these dictionaries are compatible with our model,
but they do not provide a thematic role level.
(Melcuk, 1988) characterizes valency frames in
a similar fashion (94), but uses informal additions
in natural language to constrain the possible pat-
terns. Also (Melcuk, 1988) assumes different levels
of representation. A shortcoming of the syntactic
level in (Melcuk, 1988) is, though, that his syntactic
classes are dependent on the specific lexical item,
and therefore problematic to define. The approach
we will take resembles LFG (Kaplan and Bresnan,
1982) (Bresnan, 2001) in that it assumes syntactic
relations.
(Davis, 1998) has recently proposed a linking the-
ory in the formal framework of HPSG. He sepa-
rates syntax and semantics by postulating thematic
roles under the CONTENT feature of his HPSG ar-
chitecture (Pollard and Sag, 1994), and syntac-
tic characterizations of the arguments under CAT-
EGORY|ARG-ST and CATEGORY|SUBCAT. He has
separate hierarchies of syntactic patterns (intrans,
trans, ditrans, 5:32) and semantic classes (subtypes
of RELATION, 5:72). These hierarchies interact by
a set of linking constraints and yield a hierarchy
of predicators (5:41), which specifies possible link-
ings of thematic roles to syntactic arguments. While
(Helbig, 1995) obviously employs a large role set,
(Davis, 1998) has only 6 roles, and moves thematic
roles further down into semantics than we assume
by postulating them on an event level, which ?ef-
fectively amounts to a limited amount of seman-
tic decomposition? (5:39). The shortcoming of the
model is that the syntactic patterns assumed are very
sparse indeed with only three transitivity classes.
Due to this, semantic predictions can be made only
for NP-complements, while PPs must be treated by
a separate mechanism (?content sharing account?).
Thus, there is no specific prediction for the preposi-
tional complement in English dative shift construc-
tions. The advantage of Davis?s model, in contrast,
is the lexical inheritance architecture which is a for-
mal means to capture generalizations.
4 Formalization
We formalize our idea of linking and valency frames
as an extension of the new lexicalized, dependency-
based grammar formalism of Topological Depen-
dency Grammar (TDG) (Duchier and Debusmann,
2001), (Debusmann, 2001). So far, TDG is only
concerned with syntax: every TDG analysis con-
sists of an unordered dependency tree (ID tree) and
an ordered and projective topology tree (LP tree).
We only describe a subset of the full TDG grammar
formalism (e.g. completely ignoring any issues con-
cerning word order) and extend it with the notion of
a thematic graph (TH graph). We call the version of
TDG described in this paper TDGTH.
4.1 Thematic graphs
Peter will live in Taipei
subj vbse
ppin
pcomp
(14)
What is a ?thematic graph?? We illustrate this
notion by an example. (14) is an ID tree analysis for
the sentence Peter will live in Taipei: We show the
corresponding TH graph in (15). Here, Peter is the
patient of will live and in Taipei the locative. Note
that we collapsed the auxiliary will and its verbal
complement live into a single node, and also the PP
in Taipei:
Peter will live in Taipei
th loc (15)
4.2 The lexicon
This section deals with the TDGTH-lexicon. We as-
sume a finite set of syntactic roles R and a finite set
of thematic roles T . We write ? for a syntactic role
in R and ? for a thematic role in T . ? = {!,?} is the
set of optionality flags pi. A = {valID,valTH, link} is
the set of lexical features ?, and E the set of lexical
entries e, having the following signature:5
?
?
valID : 2R ??
valTH : 2T ??
link : 2T ?R
?
?
E is a lattice of TDGTH-lexicon entries; lexical en-
tries either correspond to words or to lexical types
which can be inherited (see below).
The value of feature valID is a set of pairs (?,pi)
of syntactic roles and an optionality flag modeling
the concept of syntactic valency. The value of valTH
a set of pairs (?,pi) of thematic roles and an option-
ality flag (thematic valency). For convenience, we
write ?pi for (?,pi), and ?pi for (?,pi). The value of
link is a set of pairs (?,?) of thematic and syntactic
roles, expressing the mapping between them. We
call a pair in this set a linking.
5We write ?(e) to denote the value of feature ? at lexical
entry e.
eat :
?
?
?
valID : {subj!,objd?}
valTH : {ag!, th?}
link : {(ag,subj),
(th,objd)}
?
?
?
(16)
As an example, (16) is a lexical entry for finite
eat: eat has an obligatory subject (subj) and an op-
tional direct object (objd) in its syntactic valency. Its
thematic valency contains an obligatory AGENT and
an optional THEME. The link-feature defines two
linkings: one links the AGENT to the subject and
the THEME to the direct object.
4.3 Lexical inheritance
We introduce a mechanism of lexical inheritance.
We write e = e1 u . . .u en for ?lexical entry e in-
herits from lexical entries e1, . . . ,en?, and define in-
heritance as the set union of the individual features?
values:
e1 u . . .u en =
?
?
valID : valID(e1)? . . .? valID(en)
valTH : valTH(e1)? . . .? valTH(en)
link : link(e1)? . . .? link(en)
?
?
We can now use lexical inheritance to model our
notion of valency frames. We introduce the notion
of a linking type as a lexical entry that does not spec-
ify any other lexical attributes besides valID, valTH
and link. Such linking types specify a partial va-
lency frame from which we can build complete va-
lency frames by inheritance. For instance, consider
the following two linking types:
l ag subj :
?
?
valID : {subj!}
valTH : {ag!}
link : {(ag,subj)}
?
? (17)
l th objd :
?
?
valID : {objd?}
valTH : {th?}
link : {(th,objd)}
?
? (18)
The linking type l ag subj maps the agent to the
subject, and l th objd the theme to the direct object.
Out of the two, we can construct our lexical entry
for eat by lexical inheritance:
eat = l ag subj u l th objd (19)
which amounts precisely to the lexical entry dis-
played in (16) above. We call the values of the three
features valID, valTH and link in a lexical entry ob-
tained by inheriting from linking types a valency
frame.
4.4 Role hierarchies
We arrange the set R of syntactic roles in a role
hierarchy modeled as a meet semi-lattice. Here is
an example cut-out of the role hierarchy:
dativeshift ppdirectional ppspatial
obji ppto ppinto ppin ppunder
(20)
We write ? v ?? for ? is a specialization of ?? (i.e.
? is below ?? in the hierarchy).
We employ the role hierarchy to model alterna-
tive realizations in the sense of section 2: e.g. us-
ing the hierarchy above, dativeshift can be realized
as either obji or ppto but not by either ppdirectional,
ppinto or ppin. Note that certain roles (ppto, ppinto,
etc.) will be realized by only two lexical entries,
viz. the prepositions to and into respectively, while
other roles like subj, obji or objd can be realized by
a large set of lexical entries.
In the same fashion, we arrange the set T of the-
matic roles in a role hierarchy, but in this article we
keep this hierarchy completely flat.
Lexical entry constraint. To forbid that different
thematic roles are mapped to the same syntactic role
realization, we add a condition for well-formed lex-
ical entries: for every lexical entry e, the value of
its link-feature, link(e) = {(?1,?1), . . .(?n,?n)} must
not include two syntactic roles ?i, ? j (1? i 6= j ? n)
such that ?i v ? j.
4.5 TDGTH analyses
We can now define a TDGTH-analysis as a tuple
(V,EID,?,?,ETH). It consists of the finite set V
of nodes w, the finite set EID of ID edges EID ?
V ?V ?R , where R is the set of syntactic roles, and
the lexical assignment function ? : V ? E assigning
lexical entries to nodes. We write w1???IDw2 for
an ID edge from node w1 to node w2 labeled with
syntactic role ?.
Collapsing nodes. As in the example ID tree and
TH graph-analyses in (14) and (15) above, we
would like to be able to collapse sets of nodes in
the ID tree into single nodes in the TH graph. We
introduce a collapsing principle into the grammar
formalism, according to which the node of an auxil-
iary verb, a preposition or a determiner will collapse
with its daughter.
To capture this idea, we posit an equivalence re-
lation ?, and write V/? for the set of equivalence
classes over V . An equivalence class directly cor-
responds to a ?collapsed node? in the TH graph.
ETH ? (V/?)? (V/?)?T is the finite set of of TH
edges, and we write w for the equivalence class con-
taining w. w1???THw2 is a TH edge from node w1
to node w2 labeled with ?.
When we collapse nodes, we also collapse their
lexical entries: the value of the lexical feature ? of
a collapsed node w1 = {w1, . . . ,wn} is the set union
of the values assigned to the individual nodes:
?(w1) = ?(w1)? . . .??(wn) (21)
In the example TH graph in (15) above, the two
nodes will and live are collapsed into the equiva-
lence class will live. We assume that will is mapped
to the following lexical entry:
will :
?
?
valID : {subj!,vbse!}
valTH : {}
link : {}
?
? (22)
Here, we use the independent definition of the valID,
valTH and link features in order to express that func-
tion words like auxiliaries or prepositions realize
syntactic arguments which are semantically depen-
dent on different lexical items. This also allows for
an elegant treatment of semantically void syntactic
arguments as fake-reflexives and non-referential it.6
Infinitive live has this lexical entry:
live :
?
?
?
valID : {ppspatial!}
valTH : {ag!, loc!}
link : {(ag,subj)
(loc,ppspatial)}
?
?
?
(23)
When the two nodes collapse into one (will live),
these are the values of valTH and link:7
valTH(will live) = {ag!, loc!}
link(will live) = {(ag,subj),(loc,ppspatial)}
Valency constraints. The well-formedness con-
ditions of a TDGTH-analysis are stated in terms
of lexicalized constraints of syntactic valency, the-
matic valency and linking. The syntactic valency
6Fake-reflexives occur in German and are reflexive pro-
nouns that do not have any semantic content, as e.g. the Ger-
man verb sich fu?rchten (be afraid, sich is the reflexive pro-
noun). The linking type for a fake reflexive will only specify
valID : reflpronoun, but empty valTH and link.
7We omit those features that are not relevant to the TH graph
according to its well-formedness conditions. In particular, the
value of the valID feature is not relevant to the TH graph.
constraint restricts the number of outgoing edges of
each node in the ID tree: if (?, !) ? valID(w), then w
must have precisely one outgoing edge labeled with
?? v ?, at most one if (?,?) ? valID(w), and none
otherwise. Thus, (?, !) stands for an obligatory ?-
complement of w and (?,?) for an optional one. The
thematic valency constraint is defined analogously.
Linking constraint. An edge in the TH graph is
only licensed if it satisfies the lexicalized linking
constraint. It states that an edge w1???THw2 in
the TH graph is licensed only if there is a linking
(??,??) ? link(w1) and an edge w?1???IDw?2 in the
ID tree such that w1? = w1, w2? = w2, ? v ?? and
? v ??.
Consider the example ID tree and TH graph
analyses in (14) and (15) above. The edge
will live?loc?THin Taipei is mandated by the the-
matic valency of will live, but it must also be li-
censed by the linking principle: indeed there is a
linking (loc,ppspatial) in link(will live) and an ID
edge live?ppin?IDin such that live = will live and
in = in Taipei, and loc v loc and ppin v ppspatial.
5 Application
This section describes the linguistic examples from
section 2 within the framework developed above.
We define a linking type for the English dative
shift construction as follows, realizing our notion of
an alternative realization from section 2.
l pat dativeshift :
?
?
valID : {dativeshift!}
valTH : {pat!}
link : {(pat,dativeshift)}
?
?
(24)
As can be seen from the role hierarchy in (20),
the syntactic role dativeshift can either be realized as
obji or ppto. This linking type will be inherited by all
English verbs that exhibit the dative-shift alternative
realization. For instance, it is inherited by the lexi-
cal entry for the English verb give (cf. examples (1)
and (2)). Additionally, give inherits from the link-
ing type l ag subj defined in (17). l ag subj will be
shared among a large set of other verbs which re-
alize their agents as subjects, thus reducing redun-
dancy in the lexicon.
gives = l ag subj u
l pat dativeshift u
l th objd
(25)
The lexicon entry for deliver ((5) and (6)) will
differ from the one for give by inheriting from
l pat obji instead of l pat dativeshift and thus not
allow for a dative shift.
For the German data in examples (7) to (11) we
define a syntactic role hierarchy in the same fashion,
where genitive and ppum are below a role ppumgen.
Then, the lexical entry for berauben inherits from a
linking type l theme ppumgen.
In contrast to alternative realizations, alternations
as in examples (12) and (13) realize two different
thematic roles (pat and th) as the same syntactic role
(objd). By the lexical entry constraint in section 4.4,
there cannot be a single lexical entry for both alter-
nants of clear. We therefore model finite forms of
clear by two separate valency frames (we skip the
definitions of some of the linking types).
clears = l ag subj u
l pat objd u
l th ppof
(26)
clears = l ag subj u
l th objd u
l pat ppfrom
(27)
6 Conclusion
We proposed an architecture that abstractly cap-
tures reocurring patterns in the way in which dif-
ferent lexical items syntactically realize their se-
mantic arguments. We focused on interchange-
ability between prepositional phrases and nominal
phrases. We therefore defined a hierarchy of the-
matic roles, and a separate hierarchy of syntactic
functions, clearly separating between syntax and se-
mantics.
We enriched the framework of Topological De-
pendency Grammar (Duchier and Debusmann,
2001) (TDG) with a third level of representation,
thematic structure, and defined well formedness
conditions on the thematic structure and on the re-
lation between thematic structure (TH) and syntac-
tic dominance (ID) structure. This enabled us to
present a formal definition of the concept of multi-
level valency frames. We demonstrated how such
valency frames can be assembled using the lexical
inheritance model of TDG, in order to reduce lex-
ical redundancy. We also proposed a treatment of
auxiliary constructions using a notion of node col-
lapsing.
We applied our formalism to English dative shift
constructions, variation between NP and PP in Ger-
man, optional complements, semantically empty el-
ements, and thematic role alternations.
Our approach makes weaker theoretical predic-
tions about ?alternations? as opposed to ?alternative
realizations? than the approach in (Davis, 1998), but
is more powerful in the treatment of PPs. This is
partly due to the choice of an underlying depen-
dency formalism, because dependency structures
lead to concise and semantically flat parse trees.
Our approach is data-driven in the sense that a wider
range of syntactic patterns can be mapped to se-
mantic arguments. Thus, it lends itself easy to
techniques of automatically acquiring lexica from
syntactically and semantically annotated corpora,
which start developing at present ((Baker et al,
1998), (Skut et al, 1998)).
It is possible to include a mechanism that makes
stronger predictions about alternations of the clear
type, or about passivization. Another phenomenon
we would like to tackle in this framework are raising
and control constructions. We think we can use a
concept similar to node collapsing for them.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Christian Boitet and Pete Whitelock, editors,
36th ACL and 17th ICCL Proceedings, pages 86?
90, San Francisco, California. Morgan Kaufmann
Publishers.
Joan Bresnan. 2001. Lexical Functional Syntax.
Blackwell.
Cobuild. 2001. The bank of english.
http://titania.cobuild.collins.co.uk/
boe info.html.
Tony Davis. 1998. Linking as constraints in the hi-
erarchical lexicon. Chicago UP.
Ralph Debusmann. 2001. A declarative grammar
formalism for dependency grammar. Master?s
thesis, University of the Saarland.
David Dowty. 1991. Thematic proto-roles and ar-
gument selection. Language, 67(547?619).
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In ACL 2001
Proceedings, Toulouse, France.
Ulrich Engel and Helmut Schumacher. 1976.
Kleines Valenzlexikon deutscher Verben.
Forschungsbericht IDS Mannheim.
Klaus Fischer. 1997. German-English Verb Va-
lency. Narr.
Gerhard Helbig and Wolfgang Schenkel. 1975.
Wo?rterbuch zur Valenz und Distribution
deutscher Verben. VEB Bibliographisches
Institut.
Gerhard Helbig. 1989. Deutsche Grammatik. VEB
Enzyklopa?die.
Gerhard Helbig. 1995. Probleme der Valenz- und
Kasustheorie. Narr.
Ron Kaplan and Joan Bresnan. 1982. Lexical func-
tional grammar: A formal system for grammati-
cal representation. In The mental representation
of grammatical relations. MIT Press.
Geert-Jan Kruijff. 2001. A categorial-modal log-
ical architecture of informativity. Ph.D. thesis,
Charles University, Prague.
Beth Levin. 1993. English verb classes and alter-
nations. Chicago UP.
Igor Melcuk. 1988. Dependency syntax: Theory
and practice. Albany: State Univ of NY.
Carl Pollard and Ivan A. Sag. 1994. Head Driven
Phrase Structure Grammar. Chicago UP.
Petr Sgall, Eva Hajicova, and Jarmila Panenova.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel.
Wojciech Skut, Thorsten Brants, Brigitte Krenn,
and Hans Uszkoreit. 1998. A linguistically in-
terpreted corpus of German newspaper text. In
Proceedings of the ESSLLI Workshop on Recent
Advances in Corpus Annotation.
Kalevi Tarvainen. 1987. Cases in the framework of
dependency grammar. In Concepts of Case. Narr.
A Relational Syntax-Semantics Interface Based on Dependency Grammar
Ralph Debusmann Denys Duchier? Alexander Koller Marco Kuhlmann Gert Smolka Stefan Thater
Saarland University, Saarbr?cken, Germany ?LORIA, Nancy, France
{rade|kuhlmann|smolka}@ps.uni-sb.de, duchier@loria.fr, {koller|stth}@coli.uni-sb.de
Abstract
We propose a syntax-semantics interface that
realises the mapping between syntax and se-
mantics as a relation and does not make func-
tionality assumptions in either direction. This
interface is stated in terms of Extensible De-
pendency Grammar (XDG), a grammar formal-
ism we newly specify. XDG?s constraint-based
parser supports the concurrent flow of informa-
tion between any two levels of linguistic rep-
resentation, even when only partial analyses are
available. This generalises the concept of under-
specification.
1 Introduction
A key assumption of traditional syntax-semantics
interfaces, starting with (Montague, 1974), is that
the mapping from syntax to semantics is functional,
i. e. that once we know the syntactic structure of a
sentence, we can deterministically compute its se-
mantics.
Unfortunately, this assumption is typically not
justified. Ambiguities such as of quantifier scope
or pronominal reference are genuine semantic am-
biguities; that is, even a syntactically unambigu-
ous sentence can have multiple semantic readings.
Conversely, a common situation in natural language
generation is that one semantic representation can
be verbalised in multiple ways. This means that the
relation between syntax and semantics is not func-
tional at all, but rather a true m-to-n relation.
There is a variety of approaches in the litera-
ture on syntax-semantics interfaces for coping with
this situation, but none of them is completely sat-
isfactory. One way is to recast semantic ambiguity
as syntactic ambiguity by compiling semantic dis-
tinctions into the syntax (Montague, 1974; Steed-
man, 1999; Moortgat, 2002). This restores function-
ality, but comes at the price of an artificial blow-
up of syntactic ambiguity. A second approach is to
assume a non-deterministic mapping from syntax
to semantics as in generative grammar (Chomsky,
1965), but it is not always obvious how to reverse
the relation, e. g. for generation. For LFG, the oper-
ation of functional uncertaintainty allows for a re-
stricted form of relationality (Kaplan and Maxwell
III, 1988). Finally, underspecification (Egg et al,
2001; Gupta and Lamping, 1998; Copestake et al,
2004) introduces a new level of representation,
which can be computed functionally from a syntac-
tic analysis and encapsulates semantic ambiguity in
a way that supports the enumeration of all semantic
readings by need.
In this paper, we introduce a completely rela-
tional syntax-semantics interface, building upon the
underspecification approach. We assume a set of
linguistic dimensions, such as (syntactic) immedi-
ate dominance and predicate-argument structure; a
grammatical analysis is a tuple with one component
for each dimension, and a grammar describes a set
of such tuples. While we make no a priori function-
ality assumptions about the relation of the linguistic
dimensions, functional mappings can be obtained
as a special case. We formalise our syntax-seman-
tics interface using Extensible Dependency Gram-
mar (XDG), a new grammar formalism which gen-
eralises earlier work on Topological Dependency
Grammar (Duchier and Debusmann, 2001).
The relational syntax-semantics interface is sup-
ported by a parser for XDG based on constraint pro-
gramming. The crucial feature of this parser is that
it supports the concurrent flow of possibly partial in-
formation between any two dimensions: once addi-
tional information becomes available on one dimen-
sion, it can be propagated to any other dimension.
Grammaticality conditions and preferences (e. g. se-
lectional restrictions) can be specified on their nat-
ural level of representation, and inferences on each
dimension can help reduce ambiguity on the oth-
ers. This generalises the idea of underspecifica-
tion, which aims to represent and reduce ambiguity
through inferences on a single dimension only.
The structure of this paper is as follows: in Sec-
tion 2, we give the general ideas behind XDG, its
formal definition, and an overview of the constraint-
based parser. In Section 3, we present the relational
syntax-semantics interface, and go through exam-
ples that illustrate its operation. Section 4 shows
how the semantics side of our syntax-semantics in-
terface can be precisely related to mainstream se-
mantics research. We summarise our results and
point to further work in Section 5.
2 Extensible Dependency Grammar
This section presents Extensible Dependency
Grammar (XDG), a description-based formalism
for dependency grammar. XDG generalizes previ-
ous work on Topological Dependency Grammar
(Duchier and Debusmann, 2001), which focussed
on word order phenomena in German.
2.1 XDG in a Nutshell
XDG is a description language over finite labelled
graphs. It is able to talk about two kinds of con-
straints on these structures: The lexicon of an XDG
grammar describes properties local to individual
nodes, such as valency. The grammar?s principles
express constraints global to the graph as a whole,
such as treeness. Well-formed analyses are graphs
that satisfy all constraints.
An XDG grammar allows the characterisation
of linguistic structure along several dimensions of
description. Each dimension contains a separate
graph, but all these graphs share the same set of
nodes. Lexicon entries synchronise dimensions by
specifying the properties of a node on all dimen-
sions at once. Principles can either apply to a single
dimension (one-dimensional), or constrain the rela-
tion of several dimensions (multi-dimensional).
Consider the example in Fig. 1, which shows an
analysis for a sentence of English along two dimen-
sions of description, immediate dominance (ID) and
linear precedence (LP). The principles of the under-
lying grammar require both dimensions to be trees,
and the LP tree to be a ?flattened? version of the ID
tree, in the sense that whenever a node v is a tran-
sitive successor of a node u in the LP tree, it must
also be a transitive successor of u in the ID tree. The
given lexicon specifies the potential incoming and
required outgoing edges for each word on both di-
mensions. The word does, for example, accepts no
incoming edges on either dimension and must there-
fore be at the root of both the ID and the LP tree. It is
required to have outgoing edges to a subject (subj)
and a verb base form (vbse) in the ID tree, needs
fillers for a subject (sf) and a verb complement field
(vcf) in the LP tree, and offers an optional field for
topicalised material (tf). All these constraints are
satisfied by the analysis, which is thus well-formed.
2.2 Formalisation
Formally, an XDG grammar is built up of dimen-
sions, principles, and a lexicon, and characterises a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
s
u
b
j
v
b
s
e
o
b
j
what does John eat
s
f
v
c
f
what does John eat
t
f
word inID outID inLP outLP
what {obj?} {} {tf?} {}
does {} {subj,vbse} {} {tf?,sf,vcf}
John {subj?,obj?} {} {sf?,of?} {}
eat {vbse?} {obj} {vcf?} {}
Figure 1: XDG analysis of ?what does John eat?
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments (or
lexical entries). A D-structure, representing an anal-
ysis on dimension D, is a triple (V,E,F) of a set V
of nodes, a set E ?V ?V ?Lab of directed labelled
edges, and an assignment F : V ? (Fea ? Val) of
lexical entries to nodes. V and E form a graph. We
write StrD for the set of all possible D-structures.
The principles characterise subsets of StrD that have
further dimension-specific properties, such as being
a tree, satisfying assigned valencies, etc. We assume
that the elements of Pri are finite representations of
such subsets, but do not go into details here; some
examples are shown in Section 3.2.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Str1??? ??Strn
where all dimensions share the same set of nodes V .
Multi-dimensional principles work just like one-
dimensional principles, except that they specify
subsets of Ana, i. e. couplings between dimensions
(e. g. the flattening principle between ID and LP in
Section 2.1). The lexicon Lex ? Lex1 ? ?? ? ? Lexn
constrains all dimensions at once. An XDG analysis
is licenced by Lex iff (F1(w), . . . ,Fn(w)) ? Lex for
every node w ?V .
In order to compute analyses for a given input, we
model it as a set of input constraints (Inp), which
again specify a subset of Ana. The parsing prob-
lem for XDG is then to find elements of Ana that
are licenced by Lex and consistent with Inp and
Pri. Note that the term ?parsing problem? is tradi-
tionally used only for inputs that are sequences of
words, but we can easily represent surface realisa-
tion as a ?parsing? problem in which Inp specifies a
semantic dimension; in this case, a ?parser? would
compute analyses that contain syntactic dimensions
from which we can read off a surface sentence.
2.3 Constraint Solver
The parsing problem of XDG has a natural read-
ing as a constraint satisfaction problem (CSP) (Apt,
2003) on finite sets of integers; well-formed anal-
yses correspond to the solutions of this problem.
The transformation, whose details we omit due to
lack of space, closely follows previous work on ax-
iomatising dependency parsing (Duchier, 2003) and
includes the use of the selection constraint to effi-
ciently handle lexical ambiguity.
We have implemented a constraint solver for
this CSP using the Mozart/Oz programming system
(Smolka, 1995; Mozart Consortium, 2004). This
solver does a search for a satisfying variable assign-
ment. After each case distinction (distribution), it
performs simple inferences that restrict the ranges
of the finite set variables and thus reduce the size
of the search tree (propagation). The successful
leaves of the search tree correspond to XDG anal-
yses, whereas the inner nodes correspond to partial
analyses. In these cases, the current constraints are
too weak to specify a complete analysis, but they
already express that some edges or feature values
must be present, and that others are excluded. Partial
analyses will play an important role in Section 3.3.
Because propagation operates on all dimensions
concurrently, the constraint solver can frequently
infer information about one dimension from infor-
mation on another, if there is a multi-dimensional
principle linking the two dimensions. These infer-
ences take place while the constraint problem is be-
ing solved, and they can often be drawn before the
solver commits to any single solution.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an NP-
complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential, but the average-case complex-
ity for the hand-crafted grammars we experimented
with is often better than this result suggests. We
hope there are useful fragments of XDG that would
guarantee polynomial worst-case complexity.
3 A Relational Syntax-Semantics Interface
Now that we have the formal and processing frame-
works in place, we can define a relational syntax-
semantics interface for XDG. We will first show
how we encode semantics within the XDG frame-
work. Then we will present an example grammar
(including some principle definitions), and finally
go through an example that shows how the rela-
tionality of the interface, combined with the con-
currency of the constraint solver, supports the flow
of information between different dimensions.
3.1 Representing Meaning
We represent meaning within XDG on two dimen-
sions: one for predicate-argument structure (PA),
every student reads a book
s
u
b
j
d
e
t
o
b
j
d
e
t
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
i. ID-tree ii. PA-structure
s
every student reads a book
s
r
r
s
every student reads a book
s
r
r
iii. scope trees
Figure 2: Two analyses for the sentence ?every stu-
dent reads a book.?
and one for scope (SC). The function of the PA di-
mension is to abstract over syntactic idiosyncrasies
such as active-passive alternations or dative shifts,
and to make certain semantic dependencies e. g. in
control constructions explicit; it deals with concepts
such as agent and patient, rather than subject and ob-
ject. The purpose of the SC dimension is to reflect
the structure of a logical formula that would repre-
sent the semantics, in terms of scope and restriction.
We will make this connection explicit in Section 4.
In addition, we assume an ID dimension as above.
We do not include an LP dimension only for ease of
presentation; it could be added completely orthogo-
nally to the three dimensions we consider here.
While one ID structure will typically correspond
to one PA structure, each PA structure will typically
be consistent with multiple SC structures, because
of scope ambiguities. For instance, Fig. 2 shows the
unique ID and PA structures for the sentence ?Ev-
ery student reads a book.? These structures (and the
input sentence) are consistent with the two possi-
ble SC-structures shown in (iii). Assuming a David-
sonian event semantics, the two SC trees (together
with the PA-structure) represent the two readings of
the sentence:
? ?e.?x.student(x) ??y.book(y)? read(e,x,y)
? ?e.?y.book(y)??x.student(x) ? read(e,x,y)
3.2 A Grammar for a Fragment of English
The lexicon for an XDG grammar for a small frag-
ment of English using the ID, PA, and SC dimensions
is shown in Fig. 3. Each row in the table specifies a
(unique) lexical entry for each part of speech (deter-
miner, common noun, proper noun, transitive verb
and preposition); there is no lexical ambiguity in
this grammar. Each column specifies a feature. The
meaning of the features will be explained together
inID outID inPA outPA inSC outSC
DET {subj?,obj?,pcomp?} {det!} {ag?,pat?,arg?} {quant!} {r?,s?,a?} {r!,s!}
CN {det?} {prep?} {quant?} {mod?} {r?,s?,a?} {}
PN {subj?,obj?,pcomp?} {prep?} {ag?,pat?,arg?} {mod?} {r?,s?,a?} {r?,s!}
TV {} {subj!,obj!,prep?} {} {ag!,pat!, instr?} {r?,s?,a?} {}
PREP {prep?} {pcomp!} {mod?, instr?} {arg!} {r?,s?,a?} {a!}
link codom contradom
DET {quant 7? {det}} {quant 7? {r}} {}
CN,PN {mod 7? {prep}} {} {mod 7? {a}}
TV {ag 7? {subj},pat 7? {obj}, instr 7? {prep}} {} {ag 7? {s},pat 7? {s}, instr 7? {a}}
PREP {arg 7? {pcomp}} {} {arg 7? {s}}
Figure 3: The example grammar fragment
with the principles that use them.
The ID dimension uses the edge labels LabID =
{det,subj,obj,prep,pcomp} resp. for determined
common noun,1 subject, object, preposition, and
complement of a preposition. The PA dimension
uses LabPA = {ag,pat,arg,quant,mod, instr}, resp.
for agent, patient, argument of a modifier, common
noun pertaining to a quantifier, modifier, and instru-
ment; and SC uses LabSC = {r,s,a} resp. for restric-
tion and scope of a quantifier, and for an argument.
The grammar also contains three one-dimen-
sional principles (tree, dag, and valency), and
three multi-dimensional principles (linking, co-
dominance, and contra-dominance).
Tree and dag principles. The tree principle re-
stricts ID and SC structures to be trees, and the
dag principle restricts PA structures to be directed
acyclic graphs.
Valency principle. The valency principle, which
we use on all dimensions, states that the incom-
ing and outgoing edges of each node must obey the
specifications of the in and out features. The possi-
ble values for each feature ind and outd are subsets
of Labd ? {!,?,?}. `! specifies a mandatory edge
with label `, `? an optional one, and `? zero or more.
Linking principle. The linking principle for di-
mensions d1,d2 constrains how dependents on d1
may be realised on d2. It assumes a feature linkd1,d2
whose values are functions that map labels from
Labd1 to sets of labels from Labd2 , and is specified
by the following implication:
v
l
?d1 v
? ? ?l? ? linkd1,d2(v)(l) : v
l?
?d2 v
?
Our grammar uses this principle with the link fea-
ture to constrain the realisations of PA-dependents in
the ID dimension. In Fig. 2, the agent (ag) of reads
must be realised as the subject (subj), i. e.
1We assume on all dimensions that determiners are the
heads of common nouns. This makes for a simpler relationship
between the syntactic and semantic dimensions.
reads ag?PA every ? reads
subj
? ID every
Similarly for the patient and the object. There
is no instrument dependent in the example, so this
part of the link feature is not used. An ergative verb
would use a link feature where the subject realises
the patient; Control and raising phenomena can also
be modelled, but we cannot present this here.
Co-dominance principle. The co-dominance
principle for d1,d2 relates edges in d1 to dominance
relations in the same direction in d2. It assumes a
feature codomd1,d2 mapping labels in Labd1 to sets
of labels in Labd2 and is specified as
v
l
?d1 v
? ? ?l? ? codomd1,d2(v)(l) : v
l?
???d2v
?
Our grammar uses the co-dominance principle on
dimension PA and SC to express, e. g., that the
propositional contribution of a noun must end up in
the restriction of its determiner. For example, for the
determiner every of Fig. 2 we have:
every quant? PA student ? every
r
???SCstudent
Contra-dominance principle. The contra-domi-
nance principle is symmetric to the co-dominance
principle, and relates edges in d1 to dominance
edges into the opposite direction in d2. It assumes
a feature contradomd1,d2 mapping labels of Labd1 to
sets of labels from Labd2 and is specified as
v
l
?d1 v
? ?
?l? ? contradomd1,d2(v)(l) : v?
l?
???d2v
Our grammar uses the contra-dominance principle
on dimensions PA and SC to express, e. g., that pred-
icates must end up in the scope of the quantifiers
whose variables they refer to. Thus, for the transi-
tive verb reads of Fig. 2, we have:
reads ag?PA every ? every
s
???SCreads
reads pat?PA a ? a
s
???SCreads
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
p
r
e
p
Mary saw a student with a book
a
g
p
a
t
q
u
a
n
t
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
i
n
s
t
r
a
s
Mary saw a student with a book
a
g
p
a
t
a
r
g
a
r
g
q
u
a
n
t
Mary saw a student with a book
s
s
r
s
r
Mary saw a student with a book
s
u
b
j
o
b
j
d
e
t
a
r
g
d
e
t
m
o
d
p
r
e
p
a
i. Partial analysis
ii. verb attachment
iii. noun attachment
ID
PA
SC
Figure 4: Partial description (left) and two solutions (right) for ?Mary saw a student with a book.?
3.3 Syntax-Semantics Interaction
It is important to note at this point that the syntax-
semantics interface we have defined is indeed re-
lational. Each principle declaratively specifies a set
of admissible analyses, i. e. a relation between the
structures for the different dimensions, and the anal-
yses that the complete grammar judges grammatical
are simply those that satisfy all principles. The role
of the lexicon is to provide the feature values which
parameterise the principles defined above.
The constraint solver complements this relation-
ality by supporting the use of the principles to move
information between any two dimensions. If, say,
the left-hand side of the linking principle is found to
be satisfied for dimension d1, a propagator will infer
the right-hand side and add it to dimension d2. Con-
versely, if the solver finds that the right-hand side
must be false for d2, the negation of the left-hand
side is inferred for d1. By letting principles interact
concurrently, we can make some very powerful in-
ferences, as we will demonstrate with the example
sentence ?Mary saw a student with a book,? some
partial analyses for which are shown in Fig. 4.
Column (i) in the figure shows the state after the
constraint solver finishes its initial propagation, at
the root of the search tree. Even at this point, the va-
lency and treeness principles have conspired to es-
tablish an almost complete ID-structure. By the link-
ing principle, the PA-structure has been determined
similarly closely. The SC-structure is still mostly un-
determined, but by the co- and contra-dominance
principles, the solver has already established that
some nodes must dominate others: A dotted edge
with label s in the picture means that the solver
knows there must be a path between these two nodes
which starts with an s-edge. In other words, the
solver has computed a large amount of semantic in-
formation from an incomplete syntactic analysis.
Now imagine some external source tells us that
with is a mod-child of student on PA, i. e. the anal-
ysis in (iii). This information could come e. g. from
a statistical model of selectional preferences, which
will judge this edge much more probable than an
instr-edge from the verb to the preposition (ii).
Adding this edge will trigger additional inferences
through the linking principle, which can now infer
that with is a prep-child of student on ID. In the other
direction, the solver will infer more dominances on
SC. This means that semantic information can be
used to disambiguate syntactic ambiguities, and se-
mantic information such as selectional preferences
can be stated on their natural level of representation,
rather than be forced into the ID dimension directly.
Similarly, the introduction of new edges on SC
could trigger a similar reasoning process which
would infer new PA-edges, and thus indirectly also
new ID-edges. Such new edges on SC could come
from inferences with world or discourse knowledge
(Koller and Niehren, 2000), scope preferences, or
interactions with information structure (Duchier and
Kruijff, 2003).
4 Traditional Semantics
Our syntax-semantics interface represents seman-
tic information as graphs on the PA and SC dimen-
sions. While this looks like a radical departure from
traditional semantic formalisms, we consider these
graphs simply an alternative way of presenting more
traditional representations. We devote the rest of the
paper to demonstrating that a pair of a PA and a SC
structure can be interpreted as a Montague-style for-
mula, and that a partial analysis on these two di-
mensions can be seen as an underspecified semantic
description.
4.1 Montague-style Interpretation
In order to extract a standard type-theoretic expres-
sion from an XDG analysis, we assign each node v
two semantic values: a lexical value L(v) represent-
ing the semantics of v itself, and a phrasal value
P(v) representing the semantics of the entire SC-
subtree rooted at v. We use the SC-structure to de-
termine functor-argument relationships, and the PA-
structure to establish variable binding.
We assume that nodes for determiners and proper
names introduce unique individual variables (?in-
dices?). Below we will write ??v?? to refer to the in-
dex of the node v, and we write ?` to refer to the
node which is the `-child of the current node in the
appropriate dimension (PA or SC). The semantic lex-
icon is defined as follows; ?L(w)? should be read as
?L(v), where v is a node for the word w?.
L(a) = ?P?Q?e.?x(P(x)?Q(x)(e))
L(book) = book?
L(with) = ?P?x.(with?(???arg??)(x)?P(x))
L(reads) = read?(???pat??)(???ag??)
Lexical values for other determiners, common
nouns, and proper names are defined analogously.
Note that we do not formally distinguish event
variables from individual variables. In particular,
L(with) can be applied to either nouns or verbs,
which both have type ?e, t?.
We assume that no node in the SC-tree has more
than one child with the same edge label (which our
grammar guarantees), and write n(`1, . . . , `k) to in-
dicate that the node n has SC-children over the edge
labels `1, . . . , `k. The phrasal value for n is defined
(in the most complex case) as follows:
P(n(r,s)) = L(n)(P(?r))(? ??n??.P(?s))
This rule implements Montague?s rule of quan-
tification (Montague, 1974); note that ? ??n?? is a
binder for the variable ??n??. Nodes that have no
s-children are simply functionally applied to the
phrasal semantics of their children (if any).
By way of example, consider the left-hand SC-
structure in Fig. 2. If we identify each node by the
word it stands for, we get the following phrasal
@
@
every
?
@
@
a
?
@
@
read var
var
student
book
r
s
s
r
every student reads a book
r
r
s
s
every student reads a book
a
g
a
r
g
p
a
t
a
r
g
Figure 5: A partial SC-structure and its correspond-
ing CLLS description.
value for the root of the tree:
L(a)(L(book))(?x.L(every)(L(student)
(?y.read?(y)(x)))),
where we write x for ??a?? and y for ??every??. The
arguments of read? are x and y because every and
a are the arg and pat children of reads on the PA-
structure. After replacing the lexical values by their
definitions and beta-reduction, we obtain the fa-
miliar representation for this semantic reading, as
shown in Section 3.1.
4.2 Underspecification
It is straightforward to extend this extraction of
type-theoretic formulas from fully specified XDG
analyses to an extraction of underspecified seman-
tic descriptions from partial XDG analyses. We will
briefly demonstrate this here for descriptions in the
CLLS framework (Egg et al, 2001), which sup-
ports this most easily. Other underspecification for-
malisms could be used too.
Consider the partial SC-structure in Fig. 5, which
could be derived by the constraint solver for the
sentence from Fig. 2. We can obtain a CLLS con-
straint from it by first assigning to each node of
the SC-structure a lexical value, which is now a part
of the CLLS constraint (indicated by the dotted el-
lipses). Because student and book are known to be r-
daughters of every and a on SC, we plug their CLLS
constraints into the r-holes of their mothers? con-
straints. Because we know that reads must be dom-
inated by the s-children of the determiners, we add
the two (dotted) dominance edges to the constraint.
Finally, variable binding is represented by the bind-
ing constraints drawn as dashed arrows, and can be
derived from PA exactly as above.
5 Conclusion
In this paper, we have shown how to build a fully re-
lational syntax-semantics interface based on XDG.
This new grammar formalism offers the grammar
developer the possibility to represent different kinds
of linguistic information on separate dimensions
that can be represented as graphs. Any two dimen-
sions can be linked by multi-dimensional principles,
which mutually constrain the graphs on the two di-
mensions. We have shown that a parser based on
concurrent constraint programming is capable of in-
ferences that restrict ambiguity on one dimension
based on newly available information on another.
Because the interface we have presented makes
no assumption that any dimension is more ?basic?
than another, there is no conceptual difference be-
tween parsing and generation. If the input is the sur-
face sentence, the solver will use this information
to compute the semantic dimensions; if the input is
the semantics, the solver will compute the syntactic
dimensions, and therefore a surface sentence. This
means that we get bidirectional grammars for free.
While the solver is reasonably efficient for many
(hand-crafted) grammars, it is an important goal
for the future to ensure that it can handle large-
scale grammars imported from e.g. XTAG (XTAG
Research Group, 2001) or induced from treebanks.
One way in which we hope to achieve this is to iden-
tify fragments of XDG with provably polynomial
parsing algorithms, and which contain most useful
grammars. Such grammars would probably have to
specify word orders that are not completely free,
and we would have to control the combinatorics
of the different dimensions (Maxwell and Kaplan,
1993). One interesting question is also whether dif-
ferent dimensions can be compiled into a single di-
mension, which might improve efficiency in some
cases, and also sidestep the monostratal vs. multi-
stratal distinction.
The crucial ingredient of XDG that make rela-
tional syntax-semantics processing possible are the
declaratively specified principles. So far, we have
only given some examples for principle specifi-
cations; while they could all be written as Horn
clauses, we have not committed to any particular
representation formalism. The development of such
a representation formalism will of course be ex-
tremely important once we have experimented with
more powerful grammars and have a stable intuition
about what principles are needed.
At that point, it would also be highly interest-
ing to define a (logic) formalism that generalises
both XDG and dominance constraints, a fragment of
CLLS. Such a formalism would make it possible to
take over the interface presented here, but use dom-
inance constraints directly on the semantics dimen-
sions, rather than via the encoding into PA and SC
dimensions. The extraction process of Section 4.2
could then be recast as a principle.
Acknowledgements
We thank Markus Egg for many fruitful discussions
about this paper.
References
K. Apt. 2003. Principles of Constraint Programming.
Cambridge University Press.
N. Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2004. Minimal recursion semantics. an introduction.
Journal of Language and Computation. To appear.
D. Duchier and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of linear
precedence. In ACL 2001, Toulouse.
D. Duchier and G.-J. M. Kruijff. 2003. Information
structure in topological dependency grammar. In
EACL 2003.
D. Duchier. 2003. Configuration of labeled trees un-
der lexicalized constraints and principles. Research
on Language and Computation, 1(3?4):307?336.
M. Egg, A. Koller, and J. Niehren. 2001. The Constraint
Language for Lambda Structures. Logic, Language,
and Information, 10:457?485.
V. Gupta and J. Lamping. 1998. Efficient linear logic
meaning assembly. In COLING/ACL 1998.
R. M. Kaplan and J. T. Maxwell III. 1988. An algorithm
for functional uncertainty. In COLING 1988, pages
297?302, Budapest/HUN.
A. Koller and J. Niehren. 2000. On underspecified
processing of dynamic semantics. In Proceedings of
COLING-2000, Saarbr?cken.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In ACL 2002, Philadelphia/USA.
J. T. Maxwell and R. M. Kaplan. 1993. The interface
between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
R. Montague. 1974. The proper treatment of quantifica-
tion in ordinary english. In Richard Thomason, editor,
Formal Philosophy. Selected Papers of Richard Mon-
tague, pages 247?271. Yale University Press, New
Haven and London.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science. Na-
ture Publishing Group, MacMillan. To appear.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
G. Smolka. 1995. The Oz Programming Model. In
Computer Science Today, Lecture Notes in Computer
Science, vol. 1000, pages 324?343. Springer-Verlag.
M. Steedman. 1999. Alternating quantifier scope in
CCG. In Proc. 37th ACL, pages 301?308.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
Topological Dependency Trees:
A Constraint-Based Account of Linear Precedence
Denys Duchier
Programming Systems Lab
Universita?t des Saarlandes, Geb. 45
Postfach 15 11 50
66041 Saarbru?cken, Germany
duchier@ps.uni-sb.de
Ralph Debusmann
Computational Linguistics
Universita?t des Saarlandes, Geb. 17
Postfach 15 11 50
66041 Saarbru?cken, Germany
rade@coli.uni-sb.de
Abstract
We describe a new framework for de-
pendency grammar, with a modular de-
composition of immediate dependency
and linear precedence. Our approach
distinguishes two orthogonal yet mutu-
ally constraining structures: a syntactic
dependency tree and a topological de-
pendency tree. The syntax tree is non-
projective and even non-ordered, while
the topological tree is projective and
partially ordered.
1 Introduction
Linear precedence in so-called free word order
languages remains challenging for modern gram-
mar formalisms. To address this issue, we pro-
pose a new framework for dependency gram-
mar which supports the modular decomposition
of immediate dependency and linear precedence.
Duchier (1999) formulated a constraint-based ax-
iomatization of dependency parsing which char-
acterized well-formed syntax trees but ignored is-
sues of word order. In this article, we develop a
complementary approach dedicated to the treat-
ment of linear precedence.
Our framework distinguishes two orthogonal,
yet mutually constraining structures: a syntactic
dependency tree (ID tree) and a topological de-
pendency tree (LP tree). While edges of the ID
tree are labeled by syntactic roles, those of the
LP tree are labeled by topological fields (Bech,
1955). The shape of the LP tree is a flattening of
the ID tree?s obtained by allowing nodes to ?climb
up? to land in an appropriate field at a host node
where that field is available. Our theory of ID/LP
trees is formulated in terms of (a) lexicalized con-
straints and (b) principles governing e.g. climbing
conditions.
In Section 2 we discuss the difficulties pre-
sented by discontinuous constructions in free
word order languages, and briefly touch on the
limitations of Reape?s (1994) popular theory of
?word order domains?. In Section 3 we introduce
the concept of topological dependency tree. In
Section 4 we outline the formal framework for
our theory of ID/LP trees. Finally, in Section 5
we illustrate our approach with an account of the
word-order phenomena in the verbal complex of
German verb final sentences.
2 Discontinuous Constructions
In free word order languages, discontinuous con-
structions occur frequently. German, for example,
is subject to scrambling and partial extraposition.
In typical phrase structure based analyses, such
phenomena lead to e.g. discontinuous VPs:
(1) (dass)
(that)
einen
a
Mann
manacc
Maria
Marianom
zu
to
lieben
love
versucht
tries
whose natural syntax tree exhibits crossing edges:
S
NP V
VP
NP V
DET N
(dass) einen Mann Maria zu lieben versucht
Since this is classically disallowed, discontinu-
ous constituents must often be handled indirectly
through grammar extensions such as traces.
Reape (1994) proposed the theory of word or-
der domains which became quite popular in the
HPSG community and inspired others such as
Mu?ller (1999) and Kathol (2000). Reape distin-
guished two orthogonal tree structures: (a) the un-
ordered syntax tree, (b) the totally ordered tree of
word order domains. The latter is obtained from
the syntax tree by flattening using the operation
of domain union to produce arbitrary interleav-
ings. The boolean feature [??] of each node con-
trols whether it must be flattened out or not. In-
finitives in canonical position are assigned [?+]:
(dass)
S
NP
Maria
VP[?+]
NP[??]
DET
einen
N
Mann
V
zu lieben
V
versucht
Thus, the above licenses the following tree of
word order domains:
(dass)
S
NP
DET
einen
N
Mann
NP
Maria
V
zu lieben
V
versucht
Extraposed infinitives are assigned [??]:
(dass)
S
NP
Maria
V
versucht
VP[??]
NP
DET
einen
N
Mann
V
zu lieben
As a consequence, Reape?s theory correctly pre-
dicts scrambling (2,3) and full extraposition (4),
but cannot handle the partial extraposition in (5):
(2) (dass) Maria einen Mann zu lieben versucht
(3) (dass) einen Mann Maria zu lieben versucht
(4) (dass) Maria versucht, einen Mann zu lieben
(5) (dass) Maria einen Mann versucht, zu lieben
3 Topological Dependency Trees
Our approach is based on dependency grammar.
We also propose to distinguish two structures: (a)
a tree of syntactic dependencies, (b) a tree of topo-
logical dependencies. The syntax tree (ID tree) is
unordered and non-projective (i.e. it admits cross-
ing edges). For display purposes, we pick an ar-
bitrary linear arrangement:
(dass) Maria einen Mann zu lieben versucht
det
objec
t z
uvinfsubject
The topological tree (LP tree) is partially ordered
and projective:
(dass) Maria einen Mann zu lieben versucht
n
d
n v
vdf
mfmf vc
Its edge labels are called (external) fields and are
totally ordered: df ? mf ? vc. This induces a
linear precedence among the daughters of a node
in the LP tree. This precedence is partial because
daughters with the same label may be freely per-
muted.
In order to obtain a linearization of a LP tree,
it is also necessary to position each node with
respect to its daughters. For this reason, each
node is also assigned an internal field (d, n, or v)
shown above on the vertical pseudo-edges. The
set of internal and external fields is totally or-
dered: d ? df ? n ? mf ? vc ? v
Like Reape, our LP tree is a flattened version of
the ID tree (Reape, 1994; Uszkoreit, 1987), but
the flattening doesn?t happen by ?unioning up?;
rather, we allow each individual daughter to climb
up to find an appropriate landing place. This idea
is reminiscent of GB, but, as we shall see, pro-
ceeds rather differently.
4 Formal Framework
The framework underlying both ID and LP trees
is the configuration of labeled trees under valency
(and other) constraints. Consider a finite set L
of edge labels, a finite set V of nodes, and E ?
V ? V ? L a finite set of directed labeled edges,
such that (V,E) forms a tree. We write w???`w?
for an edge labeled ` from w to w?. We define the
`-daughters `(w) of w ? V as follows:
`(w) = {w? ? V | w???`w? ? E}
We write L? for the set of valency specifications ?`
defined by the following abstract syntax:
?` ::= ` | `? | `? (` ? L)
A valency is a subset of L?. The tree (V,E) satis-
fies the valency assignment valency : V ? 2L? if
for all w ? V and all ` ? L:
` ? valency(w) ? |`(w)| = 1
`? ? valency(w) ? |`(w)| ? 1
`? ? valency(w) ? |`(w)| ? 0
otherwise ? |`(w)| = 0
4.1 ID Trees
An ID tree (V,EID, lex, cat, valencyID) consists
of a tree (V,EID) with EID ? V ? V ?R, where
the set R of edge labels (Figure 1) represents syn-
tactic roles such as subject or vinf (bare infinitive
argument). lex : V ? Lexicon assigns a lexi-
cal entry to each node. An illustrative Lexicon is
displayed in Figure 1 where the 2 features cats
and valencyID of concern to ID trees are grouped
under table heading ?Syntax?. Finally, cat and
valencyID assign a category and an R? valency to
each node w ? V and must satisfy:
cat(w) ? lex(w).cats
valencyID(w) = lex(w).valencyID
(V,EID) must satisfy the valencyID assignment as
described earlier. For example the lexical entry
for versucht specifies (Figure 1):
valencyID(versucht) = {subject, zuvinf}
Furthermore, (V,EID) must also satisfy the
edge constraints stipulated by the grammar
(see Figure 1). For example, for an edge
w?????det w? to be licensed, w? must be assigned
category det and both w and w? must be assigned
the same agreement.1
4.2 LP Trees
An LP tree (V,ELP, lex, valencyLP, fieldext, fieldint)
consists of a tree (V,ELP) with ELP ?
V ? V ? Fext, where the set Fext of edge
labels represents topological fields (Bech, 1955):
df the determiner field, mf the ?Mittelfeld?, vc
1Issues of agreement will not be further considered in this
paper.
the verbal complement field, xf the extraposition
field. Features of lexical entries relevant to LP
trees are grouped under table heading ?Topology?
in Figure 1. valencyLP assigns a F?ext valency
to each node and is subject to the lexicalized
constraint:
valencyLP(w) = lex(w).valencyLP
(V,ELP) must satisfy the valencyLP assignment
as described earlier. For example, the lexical en-
try for zu lieben2 specifies:
valencyLP(zu lieben2) = {mf?, xf?}
which permits 0 or more mf edges and at most
one xf edge; we say that it offers fields mf and xf.
Unlike the ID tree, the LP tree must be projective.
The grammar stipulates a total order on Fext,
thus inducing a partial linear precedence on each
node?s daughters. This order is partial because
all daughters in the same field may be freely per-
muted: our account of scrambling rests on free
permutations within the mf field. In order to ob-
tain a linearization of the LP tree, it is necessary
to specify the position of a node with respect to its
daughters. For this reason each node is assigned
an internal field in Fint. The set Fext ? Fint is to-
tally ordered:
d ? df ? n ? mf ? vc ? v ? xf
In what (external) field a node may land and
what internal field it may be assigned is deter-
mined by assignments fieldext : V ? Fext and
fieldint : V ? Fint which are subject to the lexi-
calized constraints:
fieldext(w) ? lex(w).fieldext
fieldint(w) ? lex(w).fieldint
For example, zu lieben1 may only land in field vc
(canonical position), and zu lieben2 only in xf (ex-
traposed position). The LP tree must satisfy:
w???`w? ? ELP ? ` = fieldext(w?)
Thus, whether an edge w???`w? is licensed de-
pends both on valencyLP(w) and on fieldext(w?).
In other words: w must offer field ` and w? must
accept it.
For an edge w???`w? in the ID tree, we say that
w is the head of w?. For a similar edge in the LP
Grammar Symbols
C = {det , n, vfin, vinf , vpast, zuvinf} (Categories)
R = {det, subject, object, vinf, vpast, zuvinf} (Syntactic Roles)
Fext = {df, mf, vc, xf} (External Topological Fields)
Fint = {d, n, v} (Internal Topological Fields)
d ? df ? n ? mf ? vc ? v ? xf (Topological Ordering)
Edge Constraints
w?????????det w? ? cat(w?) = det ? agr(w) = agr(w?)
w?????????subject w? ? cat(w?) = n ? agr(w) = agr(w?) ? NOM
w?????????object w? ? cat(w?) = n ? agr(w?) ? ACC
w?????????vinf w? ? cat(w?) = vinf
w?????????vpast w? ? cat(w?) = vpast
w?????????zuvinf w? ? cat(w?) = zuvinf
Lexicon
Word Syntax Topology
cats valencyID fieldint fieldext valencyLP
einen {det} {} {d} {df} {}
Mann {n} {det} {n} {mf} {df?}
Maria {n} {} {n} {mf} {}
lieben {vinf} {object?} {v} {vc} {}
geliebt {vpast} {object?} {v} {vc} {}
ko?nnen1 {vinf} {vinf} {v} {vc} {vc?}
ko?nnen2 {vinf , vpast} {vinf} {v} {xf} {mf?, vc?, xf?}
wird {vfin} {subject, vinf} {v} {vc} {mf?, vc?, xf?}
haben {vinf} {vpast} {v} {xf} {mf?, vc?, xf?}
hat {vinf} {subject, vpast} {v} {vc} {mf?, vc?, xf?}
zu lieben1 {zuvinf} {object?} {v} {vc} {}
zu lieben2 {zuvinf} {object?} {v} {xf} {mf?, xf?}
versucht {vfin} {subject, zuvinf} {v} {vc} {mf?, vc?, xf?}
Figure 1: Grammar Fragment
tree, we say that w is the host of w? or that w?
lands on w. The shape of the LP tree is a flat-
tened version of the ID tree which is obtained by
allowing nodes to climb up subject to the follow-
ing principles:
Principle 1 a node must land on a transitive
head2
Principle 2 it may not climb through a barrier
We will not elaborate the notion of barrier which
is beyond the scope of this article, but, for exam-
ple, a noun will prevent a determiner from climb-
ing through it, and finite verbs are typically gen-
eral barriers.
2This is Bro?cker?s terminology and means a node in the
transitive closure of the head relation.
Principle 3 a node must land on, or climb higher
than, its head
Subject to these principles, a node w? may climb
up to any host w which offers a field licensed by
fieldext(w?).
Definition. An ID/ LP analysis is a tuple (V,
EID, ELP, lex, cat, valencyID, valencyLP, fieldext,
fieldint) such that (V,EID, lex, cat, valencyID) is
an ID tree and (V,ELP, lex, valencyLP, fieldext,
fieldint) is an LP tree and all principles are sat-
isfied.
Our approach has points of similarity with
(Bro?ker, 1999) but eschews modal logic in fa-
vor of a simpler and arguably more perspicuous
constraint-based formulation. It is also related
to the lifting rules of (Kahane et al, 1998), but
where they choose to stipulate rules that license
liftings, we opt instead for placing constraints on
otherwise unrestricted climbing.
5 German Verbal Phenomena
We now illustrate our theory by applying it to the
treatment of word order phenomena in the verbal
complex of German verb final sentences. We as-
sume the grammar and lexicon shown in Figure 1.
These are intended purely for didactic purposes
and we extend for them no claim of linguistic ad-
equacy.
5.1 VP Extraposition
Control verbs like versuchen or versprechen al-
low their zu-infinitival complement to be option-
ally extraposed. This phenomenon is also known
as optional coherence.
(6) (dass) Maria einen Mann zu lieben versucht
(7) (dass) Maria versucht, einen Mann zu lieben
Both examples share the following ID tree:
(dass) Maria einen Mann zu lieben versucht
det
objec
t z
uvinfsubject
Optional extraposition is handled by having two
lexical entries for zu lieben. One requires it to
land in canonical position:
fieldext(zu lieben1) = {vc}
the other requires it to be extraposed:
fieldext(zu lieben2) = {xf}
In the canonical case, zu lieben1 does not offer
field mf and einen Mann must climb to the finite
verb:
(dass) Maria einen Mann zu lieben versucht
n
d
n v
vdf
mfmf vc
In the extraposed case, zu lieben2 itself offers
field mf:
(dass) Maria versucht einen Mann zu lieben
n
v
d
n
v
mf
df
mf
xf
5.2 Partial VP Extraposition
In example (8), the zu-infinitive zu lieben is extra-
posed to the right of its governing verb versucht,
but its nominal complement einen Mann remains
in the Mittelfeld:
(8) (dass) Maria einen Mann versucht, zu lieben
In our account, Mann is restricted to land in an mf
field which both extraposed zu lieben2 and finite
verb versucht offer. In example (8) the nominal
complement simply climbed up to the finite verb:
(dass) Maria einen Mann versucht zu lieben
n
d
n
v
v
mf
df
mf xf
5.3 Obligatory Head-final Placement
Verb clusters are typically head-final in German:
non-finite verbs precede their verbal heads.
(9) (dass)
(that)
Maria
Marianom
einen
a
Mann
manacc
lieben
love
wird
will
(10)*(dass) Maria einen Mann wird lieben
The ID tree for (9) is:
(dass) Maria einen Mann lieben wird
subject
det
obje
ct
vinf
The lexical entry for the bare infinitive lieben re-
quires it to land in a vc field:
fieldext(lieben) = {vc}
therefore only the following LP tree is licensed:3
(dass) Maria einen Mann lieben wird
n
d
n v
v
mf
df
mf vc
where mf ? vc ? v, and subject and ob-
ject, both in field mf, remain mutually unordered.
Thus we correctly license (9) and reject (10).
5.4 Optional Auxiliary Flip
In an auxiliary flip construction (Hinrichs and
Nakazawa, 1994), the verbal complement of an
auxiliary verb, such as haben or werden, follows
rather than precedes its head. Only a certain class
of bare infinitive verbs can land in extraposed po-
sition. As we illustrated above, main verbs do
not belong to this class; however, modals such as
ko?nnen do, and may land in either canonical (11)
or in extraposed (12) position. This behavior is
called ?optional auxiliary flip?.
(11) (dass)
(that)
Maria
Maria
einen
a
Mann
man
lieben
love
ko?nnen
can
wird
will
(that) Maria will be able to love a man
(12) (dass) Maria einen Mann wird lieben ko?nnen
Both examples share the following ID tree:
(dass) Maria einen Mann wird lieben ko?nnen
subject
det
object
vinf
vinf
Our grammar fragment describes optional auxil-
iary flip constructions in two steps:
? wird offers both vc and xf fields:
valencyID(wird) = {mf?, vc?, xf?}
? ko?nnen has two lexical entries, one canonical
and one extraposed:
fieldext(ko?nnen1) = {vc}
fieldext(ko?nnen2) = {xf}
3It is important to notice that there is no spurious ambi-
guity concerning the topological placement of Mann: lieben
in canonical position does not offer field mf; therefore Mann
must climb to the finite verb.
Thus we correctly account for examples (11) and
(12) with the following LP trees:
(dass) Maria einen Mann lieben ko?nnen wird
n
d
n
v
v
v
mf
df
mf
vc
vc
(dass) Maria einen Mann wird lieben ko?nnen
n
d
n
v
v
v
mf
df
mf
vc
xf
The astute reader will have noticed that other LP
trees are licensed for the earlier ID tree: they are
considered in the section below.
5.5 V-Projection Raising
This phenomenon related to auxiliary flip de-
scribes the case where non-verbal material is in-
terspersed in the verb cluster:
(13) (dass) Maria wird einen Mann lieben ko?nnen
(14)*(dass) Maria lieben einen Mann ko?nnen wird
(15)*(dass) Maria lieben ko?nnen einen Mann wird
The ID tree remains as before. The NP einen
Mann must land in a mf field. lieben is in canon-
ical position and thus does not offer mf, but
both extraposed ko?nnen2 and finite verb wird do.
Whereas in (12), the NP climbed up to wird, in
(13) it climbs only up to ko?nnen.
(dass) Maria wird einen Mann lieben ko?nnen
n
v
d
n v
v
mf
df
mf vc
xf
(14) is ruled out because ko?nnen must be in the
vc of wird, therefore lieben must be in the vc
of ko?nnen, and einen Mann must be in the mf of
wird. Therefore, einen Mann must precede both
lieben and ko?nnen. Similarly for (15).
5.6 Intermediate Placement
The Zwischenstellung construction describes
cases where the auxiliary has been flipped but its
verbal argument remains in the Mittelfeld. These
are the remaining linearizations predicted by our
theory for the running example started above:
(16) (dass) Maria einen Mann lieben wird ko?nnen
(17) (dass) einen Mann Maria lieben wird ko?nnen
where lieben has climbed up to the finite verb.
5.7 Obligatory Auxiliary Flip
Substitute infinitives (Ersatzinfinitiv) are further
examples of extraposed verbal forms. A sub-
stitute infinitive exhibits bare infinitival inflec-
tion, yet acts as a complement of the perfectizer
haben, which syntactically requires a past partici-
ple. Only modals, AcI-verbs such as sehen and
lassen, and the verb helfen can appear in substi-
tute infinitival inflection.
A substitute infinitive cannot land in canonical
position; it must be extraposed: an auxiliary flip
involving a substitute infinitive is called an ?oblig-
atory auxiliary flip?.
(18) (dass)
(that)
Maria
Maria
einen
a
Mann
man
hat
has
lieben
love
ko?nnen
can
(that) Maria was able to love a man
(19) (dass) Maria hat einen Mann lieben ko?nnen
(20)*(dass) Maria einen Mann lieben ko?nnen hat
These examples share the ID tree:
(dass) Maria einen Mann hat lieben ko?nnen
subject
det
object
xvinf
vinf
hat subcategorizes for a verb in past participle in-
flection because:
valencyID(hat) = {subject, vpast}
and the edge constraint for w??????vpast w? requires:
cat(w?) = vpast
This is satisfied by ko?nnen2 which insists on being
extraposed, thus ruling (20) out:
fieldext(ko?nnen2) = {xf}
Example (18) has LP tree:
(dass) Maria einen Mann hat lieben ko?nnen
n
d
n
v
v
v
mf
df
mf xf
vc
In (18) einen Mann climbs up to hat, while in (19)
it only climbs up to ko?nnen.
5.8 Double Auxiliary Flip
Double auxiliary flip constructions occur when
an auxiliary is an argument of another auxiliary.
Each extraposed verb form offers both vc and mf:
thus there are more opportunities for verbal and
nominal arguments to climb to.
(21) (dass) Maria wird haben einen Mann lieben
ko?nnen
(that) Maria will have been able to love a man
(22) (dass) Maria einen Mann wird haben lieben
ko?nnen
(23) (dass) Maria wird einen Mann lieben haben
ko?nnen
(24) (dass) Maria einen Mann wird lieben haben
ko?nnen
(25) (dass) Maria einen Mann lieben wird haben
ko?nnen
These examples have ID tree:
Maria einen Mann wird haben lieben ko?nnen
subject
det
object
vinf
vinf
vpast
and (22) obtains LP tree:
Maria einen Mann wird haben lieben ko?nnen
n
d
n
v
v
v
v
mf
df
mf xf
vc
xf
5.9 Obligatory Coherence
Certain verbs like scheint require their argument
to appear in canonical (or coherent) position.
(26) (dass)
(that)
Maria
Maria
einen
a
Mann
man
zu
to
lieben
love
scheint
seems
(that) Maria seems to love a man
(27)*(dass) Maria einen Mann scheint, zu lieben
Obligatory coherence may be enforced with the
following constraint principle: if w is an obliga-
tory coherence verb and w? is its verbal argument,
then w? must land in w?s vc field. Like barri-
ers, the expression of this principle in our gram-
matical formalism falls outside the scope of the
present article and remains the subject of active
research.4
6 Conclusions
In this article, we described a treatment of lin-
ear precedence that extends the constraint-based
framework for dependency grammar proposed by
Duchier (1999). We distinguished two orthogo-
nal, yet mutually constraining tree structures: un-
ordered, non-projective ID trees which capture
purely syntactic dependencies, and ordered, pro-
jective LP trees which capture topological depen-
dencies. Our theory is formulated in terms of (a)
lexicalized constraints and (b) principles which
govern ?climbing? conditions.
We illustrated this theory with an application to
the treatment of word order phenomena in the ver-
bal complex of German verb final sentences, and
demonstrated that these traditionally challenging
phenomena emerge naturally from our simple and
elegant account.
Although we provided here an account spe-
cific to German, our framework intentionally per-
mits the definition of arbitrary language-specific
topologies. Whether this proves linguistically ad-
equate in practice needs to be substantiated in fu-
ture research.
Characteristic of our approach is that the for-
mal presentation defines valid analyses as the so-
lutions of a constraint satisfaction problem which
is amenable to efficient processing through con-
straint propagation. A prototype was imple-
mented in Mozart/Oz and supports a parsing
4we also thank an anonymous reviewer for pointing out
that our grammar fragment does not permit intraposition
mode as well as a mode generating all licensed
linearizations for a given input. It was used to
prepare all examples in this article.
While the preliminary results presented here
are encouraging and demonstrate the potential of
our approach to linear precedence, much work re-
mains to be done to extend its coverage and to
arrive at a cohesive and comprehensive grammar
formalism.
References
Gunnar Bech. 1955. Studien u?ber das deutsche Ver-
bum infinitum. 2nd unrevised edition published
1983 by Max Niemeyer Verlag, Tu?bingen (Linguis-
tische Arbeiten 139).
Norbert Bro?ker. 1999. Eine Dependenzgrammatik
zur Kopplung heterogener Wissensquellen. Lin-
guistische Arbeiten 405. Max Niemeyer Verlag,
Tu?bingen/FRG.
Denys Duchier. 1999. Axiomatizing dependency
parsing using set constraints. In Sixth Meeting on
the Mathematics of Language, Orlando/FL, July.
Erhard Hinrichs and Tsuneko Nakazawa. 1994. Lin-
earizing AUXs in German verbal complexes. In
Nerbonne et al (Nerbonne et al, 1994), pages 11?
37.
Sylvain Kahane, Alexis Nasr, and Owen Rambow.
1998. Pseudo-projectivity: a polynomially parsable
non-projective dependency grammar. In Proc.
ACL/COLING?98, pages 646?52, Montre?al.
Andreas Kathol. 2000. Linear Syntax. Oxford Uni-
versity Press.
Igor Melc?uk. 1988. Dependency Syntax: Theory and
Practice. The SUNY Press, Albany, N.Y.
Stefan Mu?ller. 1999. Deutsche Syntax deklara-
tiv. Head-Driven Phrase Structure Grammar fu?r
das Deutsche. Linguistische Arbeiten 394. Max
Niemeyer Verlag, Tu?bingen/FRG.
John Nerbonne, Klaus Netter, and Carl Pollard, edi-
tors. 1994. German in Head-Driven Phrase Struc-
ture Grammar. CSLI, Stanford/CA.
Mike Reape. 1994. Domain union and word order
variation in German. In Nerbonne et al (Nerbonne
et al, 1994), pages 151?197.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI, Stanford/CA.
Multiword expressions as dependency subgraphs
Ralph Debusmann
Programming Systems Lab
Saarland University
Postfach 15 11 50
66041 Saarbru?cken, Germany
rade@ps.uni-sb.de
Abstract
We propose to model multiword expres-
sions as dependency subgraphs, and re-
alize this idea in the grammar formal-
ism of Extensible Dependency Gram-
mar (XDG). We extend XDG to lexi-
calize dependency subgraphs, and show
how to compile them into simple lexical
entries, amenable to parsing and gener-
ation with the existing XDG constraint
solver.
1 Introduction
In recent years, dependency grammar (DG)
(Tesnie`re, 1959; Sgall et al, 1986; Mel?c?uk, 1988)
has received resurgent interest. Core concepts
such as grammatical functions, valency and the
head-dependent asymmetry have now found their
way into most grammar formalisms, including
phrase structure-based ones such as HPSG, LFG
and TAG. This renewed interest in DG has also
given rise to new grammar formalisms based di-
rectly on DG (Nasr, 1995; Heinecke et al, 1998;
Bro?ker, 1999; Gerdes and Kahane, 2001; Kruijff,
2001; Joshi and Rambow, 2003).
A controversy among DG grammarians cir-
cles around the question of assuming a 1:1-
correspondence between words and nodes in the
dependency graph. This assumption simplifies
the formalization of DGs substantially, and is of-
ten required for parsing. But as soon as se-
mantics comes in, the picture changes. Clearly,
the 1:1-correspondence between words and nodes
does not hold anymore for multiword expressions
(MWEs), where one semantic unit, represented
by a node in a semantically oriented dependency
graph, corresponds not to one, but to more than
one word.
Most DGs interested in semantics have thus
weakened the 1:1-assumption, starting with
Tesnie`re?s work. Tesnie`re proposed the con-
cept of nuclei to group together sets of nodes.
FGD, on the other hand, allows for the dele-
tion of solely syntactically motivated nodes in the
tectogrammatical structure. Similarly, in MTT,
nodes present on the syntactic structures can be
deleted on the semantic structure. This can happen
e.g. through paraphrasing rules implemented by
lexical functions (Mel?c?uk, 1996). Unfortunately,
these attempts to break the 1:1-correspondence
have not yet been formalized in a declarative way.
Extensible Dependency Grammar (XDG) is a
new grammar formalism based on Topological
Dependency Grammar (TDG) (Duchier and De-
busmann, 2001). From TDG, it inherits declara-
tive word order constraints, the ability to distin-
guish multiple dimensions of linguistic descrip-
tion, and an axiomatization as a constraint sat-
isfaction problem (Duchier, 2003) solvable using
constraint programming (Apt, 2003). One of the
benefits of this axiomatization is that the linear or-
der of the words can be left underspecified, with
the effect that the constraint solver can be applied
for both parsing and generation.
XDG solving is efficient at least for the smaller-
scale example grammars tested so far, but these
good results hinge substantially on the assump-
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 56-63
tion of a 1:1-correspondence between words and
nodes. As XDG has been created to cover not only
syntax but also semantics, we have no choice but
to weaken the 1:1-correspondence.
In this paper, we outline a way to break out of
the 1:1-straightjacket, without sacrificing the po-
tential for efficient parsing and generation. We in-
troduce a new layer of lexical organization called
groups above the basic XDG lexicon, allowing us
to describe MWEs as tuples of dependency sub-
graphs. Groups can be compiled into simple lexi-
cal entries, which can then be used by the already
existing XDG solver for parsing and generation.
With groups, we can omit nodes present in the syn-
tactic dimensions in the semantic dimensions, and
thus get away from the 1:1-correspondence.
Groups are motivated by the continuity hypothe-
sis of (Kay and Fillmore, 1999), assuming the con-
tinuity of the lexicon and the construction. They
can also be regarded as a declarative formalization
of Mel?c?uk?s paraphrasing rules, or as a realization
of the extended domain of locality of TAG (Joshi,
1987) in terms of dependency grammar, as also
proposed in (Nasr, 1996) and (Joshi and Rambow,
2003).
The structure of this paper is as follows. We in-
troduce XDG in section 2. In section 3, we intro-
duce groups, the new layer of lexical organization
required for the modeling of MWEs, and in sec-
tion 4, we show how to compile groups into sim-
ple lexical entries amenable for parsing and gener-
ation. Section 5 concludes the paper and outlines
future work.
2 XDG
In this section, we explain the intuitions behind
XDG, before proceeding with its formalization,
and a description of the XDG solver for parsing
and generation.
2.1 XDG intuitions
Extensible Dependency Grammar (XDG) is a new
grammar formalism generalizing Topological De-
pendency Grammar (TDG) (Duchier and Debus-
mann, 2001). XDG characterizes linguistic struc-
ture along arbitrary many dimensions of descrip-
tion. All dimensions correspond to labeled graphs,
sharing the same set of nodes but having different
edges.
The well-formedness conditions for XDG anal-
yses are determined by principles. Principles can
either be one-dimensional, applying to a single
dimension only, or multi-dimensional, constrain-
ing the relation of several dimensions. Basic one-
dimensional principles are treeness and valency.
Multi-dimensional principles include climbing (as
in (Duchier and Debusmann, 2001); one dimen-
sion must be a flattening of another) and linking
(e.g. to specify how semantic arguments must be
realized syntactically).
The lexicon plays a central role in XDG. For
each node, it provides a set of possible lexical en-
tries (feature structures) serving as the parameters
for the principles. Because a lexical entry con-
strains all dimensions simultaneously, it can also
help to synchronize the various dimensions, e.g.
with respect to valency. For instance, a lexical en-
try could synchronize syntactic and semantic di-
mensions by requiring a subject in the syntax, and
an agent in the semantics.
As an example, we show in (1) an analysis
for the sentence He dates her, along two dimen-
sions of description, immediate dominance (ID)
and predicate-argument structure (PA). We display
the ID part of the analysis on the left, and the PA
part on the right:1
.
He dates her
objsubj
.
He dates her
arg1
arg2
(1)
The set of edge labels on the ID dimension in-
cludes subj for subject and obj for object. On
the PA dimension, we have arg1 and arg2 stand-
ing for the argument slots of semantic predicates.2
The ID part of the analysis states that He is the sub-
ject, and her the object of dates. The PA part states
that He is the first argument and her the second ar-
gument of dates.
1For lack of space, we omit the dimension of linear prece-
dence (LP) from the presentation in this paper. For this di-
mension, we use the same theory as for TDG (Duchier and
Debusmann, 2001).
2We could also use some set of thematic roles for the PA
edge labels, but since the assumption of thematic roles is very
controversial, we decided to choose more neutral labels.
The principles of the underlying grammar re-
quire that the ID part of each analysis is a tree,
and the PA part a directed acyclic graph (dag).34
In addition, we employ the valency principle on
both dimensions, specifying the licensed incom-
ing and outgoing edges of each node. The only
employed multi-dimensional principle is the link-
ing principle, specifying how semantic arguments
are realized syntactically.
Figure 1 shows the lexicon of the underlying
grammar. Each lexical entry corresponds to both
a word and a semantic literal. inID and outID
parametrize the valency principle on the ID di-
mension. inID specifies the licensed incoming,
and outID the licensed outgoing edges. E.g.
He licenses zero or one incoming edges labeled
subj, and no outgoing edges. inPA and outPA
parametrize the valency principle on the PA dimen-
sion. E.g. dates licenses no incoming edges, and
requires precisely one outgoing edge labeled arg1
and one labeled arg2. link parametrizes the multi-
dimensional linking principle. E.g. dates syntac-
tically realizes its first argument by a subject, the
second argument by an object.
Observe that all the principles are satisfied in
(1), and hence the analysis is well-formed. Also
notice that we can use the same grammar and lex-
icon for both parsing (from words) and generation
(from semantic literals).
2.2 XDG formalization
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principle, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab, Fea, Val, Pri)
of a set Lab of edge labels, a set Fea of fea-
tures, a set Val of feature values, and a set of one-
dimensional principles Pri. A lexicon for the di-
mension D is a set Lex ? Fea ? Val of total fea-
ture assignments called lexical entries. An analy-
sis on dimension D is a triple (V,E, F ) of a set V
of nodes, a set E ? V ? V ? Lab of directed la-
beled edges, and an assignment F : V ? (Fea ?
Val) of lexical entries to nodes. V and E form a
3In the following, we will call the ID part of an analysis
ID tree, and the PA part PA dag.
4The PA structure is a dag and not a tree because we
want it to reflect the re-entrancy e.g. in control constructions,
where the same subject is shared by more than one node.
graph. We write AnaD for the set of all possible
analyses on dimension D. The principles charac-
terize subsets of AnaD. We assume that the ele-
ments of Pri are finite representations of such sub-
sets.
An XDG grammar ((Labi, Feai, Vali, Prii)ni=1,
Pri, Lex) consists of n dimensions, multi-
dimensional principles Pri, and a lexicon Lex.
An XDG analysis (V,Ei, Fi)ni=1 is an element of
Ana = Ana1 ? ? ? ? ? Anan where all dimensions
share the same set of nodes V . We call a dimen-
sion of a grammar grammar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex ? Lex1 ? ? ? ? ? Lexn
constrains all dimensions at once, thereby syn-
chronizing them. An XDG analysis is licensed by
Lex iff (F1(v), . . . , Fn(v)) ? Lex for every node
v ? V .
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are li-
censed by Lex, and consistent with Inp and Pri.
The input constraints e.g. determine whether XDG
solving is to be used for parsing or generation. For
parsing, they specify a sequence of words, and for
generation, a multiset of semantic literals.
2.3 XDG solver
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of inte-
gers, where well-formed analyses correspond to
the solutions of the CSP (Duchier, 2003). We
have implemented an XDG solver (Debusmann,
2003) using the Mozart-Oz programming system
(Mozart Consortium, 2004).
XDG solving operates on all dimensions con-
currently. This means that the solver can infer in-
formation about one dimension from information
on another, if there is either a multi-dimensional
principle linking the two dimensions, or by the
synchronization induced by the lexical entries. For
instance, not only can syntactic information trig-
ger inferences in syntax, but also vice versa.
Because XDG allows us to write grammars
with completely free word order, XDG solving is
an NP-complete problem (Koller and Striegnitz,
word literal inID outID inPA outPA link
He he? {subj?} {} {arg1?, arg2?} {} {}
dates date? {} {subj!, obj!} {} {arg1!, arg2!} {arg1 7? {subj}, arg2 7? {obj}}
her she? {obj?} {} {arg1?, arg2?} {} {}
Figure 1: Lexicon for the sentence He dates her.
2002). This means that the worst-case complex-
ity of the solver is exponential. The average-case
complexity of many smaller-scale grammars that
we have experimented with seems polynomial, but
it remains to be seen whether we can scale this up
to large-scale grammars.
3 Groups
In this section, we consider MWE paraphrases and
propose to model them as tuples of dependency
subgraphs called groups. We start with an exam-
ple: Consider (3), a paraphrase of (2):
He dates her. (2)
He takes her out. (3)
We display the XDG analysis of (3) in (4).
Again, the ID tree is on the left, and the PA dag
on the right:5
.
He takes her out
obj partsubj
.
He takes her out
arg1
arg2
(4)
This example demonstrates that we cannot sim-
ply treat MWEs as contiguous word strings and
include those in the lexicon, since the MWE takes
out is interrupted by the object her in (3). Instead,
we choose to implement the continuity hypothe-
sis of (Kay and Fillmore, 1999) in terms of DG,
and model MWEs as dependency subgraphs. We
realize this idea by a new layer of lexical orga-
nization called groups. A group is tuple of de-
pendency subgraphs covering one or more nodes,
where each component of the tuple corresponds to
a grammar dimension. We call a group compo-
nent group dimension. We display the group cor-
responding to dates in (5), and the one correspond-
5In the ID tree, part stands for a particle.
ing to takes out in (6).
.
dates
objsubj
.
dates
arg
1 arg2
(5)
.
takes out
obj partsubj
.
takes out
arg
1 arg2
(6)
Groups can leave out nodes present in the ID
dimension on the PA dimension. E.g. in (6), the
node corresponding to the particle out is present
on the ID dimension, but left out on the PA dimen-
sion. In this way, groups help us to weaken the
1:1-correspondence between words and nodes.
We can make use of groups also to handle more
complicated MWE paraphrases. Consider (8) be-
low, a support verb construction paraphrasing (7):
He argues with her. (7)
He has an argument with her. (8)
In (8), has is only a support verb; the semantic
head of the construction is the noun argument. We
display the ID tree and PA dag of (7) in (9). The ID
tree of (8) is in (10), and the PA dag in (11):6
.
He argues with her
pobjsubj
pcomp
.
He argues with her
arg1 arg2
(9)
6In the ID trees, pobj stands for a prepositional object,
pcomp for the complement of a preposition, pmod for a
prepositional modifier, and det for a determiner.
.He has an argument with her
objsubj
det
pmod
pcomp
(10).
He has an argument with her
arg1 arg2
(11)
In (9), the node corresponding to with is deleted
in the PA dag. In (11), the support verb construc-
tion leads to the deletion of three nodes (corre-
sponding to resp. has, an and with). These dele-
tions are reflected in the corresponding groups.
The group corresponding to argues with is dis-
played in (12), and the group corresponding to has
an argument with in (13) (ID) and (14) (PA):
.
argues with
pobjsubj
pcomp
.
argues with
arg1 arg2 (12)
.
has an argument with
objsubj
det
pmod
pcomp
(13).
has an argument with
arg1 arg2
(14)
Groups can capture difficult constructions such
as the support verb construction above in an el-
egant and transparent way, without having to re-
sort to complicated machinery. The key aspect
of groups is their multi-dimensionality, describing
tuples of dependency subgraphs spanning over a
shared set of nodes. This sharing enables groups
to express interdependencies between the differ-
ent dimensions. For instance in (13) and (14), the
noun argument, the object of the support verb has
in the ID dimension, is the semantic head in the PA
dimension.
4 Compilation
In this section, we show how to compile groups
into simple lexical entries. The benefit of this is
that we can retain XDG in its entirety, i.e. we can
retain its formalization and its axiomatization as
a constraint satisfaction problem. This means we
can also retain the implementation of the XDG
solver, and continue to use it for parsing and gen-
eration.
4.1 Node deletion
The 1:1-correspondence between words and nodes
is a key assumption of XDG. It requires that on
each dimension, each word must correspond to
precisely one node in the dependency graph. The
groups shown in the previous section clearly vio-
late this assumption as they allow nodes present on
the ID dimension to be omitted on the PA dimen-
sion.
The first step of the compilation aims to accom-
modate this deletion of nodes. To this end, we
assume for each analysis an additional root node,
e.g. corresponding to the full stop at the end of the
sentence. Each root in an XDG dependency graph
becomes a daughter of this additional root node,
the edge labeled root. The trick for deleting nodes
is now the following: Each deleted node also be-
comes a daughter of the additional root node, but
with a special node label indicating that this node
is regarded to be deleted (del).
As an example, here is the PA dag for example
(3) including the additional root node:
.
He takes her out .
arg1
arg2
delroot
(15)
4.2 Dependency subgraphs
The second step of the compilation is to compile
the dependency subgraphs into lexical entries for
individual words. To this end, we make use of the
valency principle. For each edge from v to v ? la-
beled l within a group, we require that v has an
outgoing edge labeled l, and that v? licenses an in-
coming edge labeled l. I.e. we include l! in the out
specification of the lexical entry for v, and l? in
the in specification of the lexical entry for v ?.
4.3 Group coherence
The final step of the compilation is about ensur-
ing group coherence, i.e. to ensure that the inner
nodes of a group dimension (neither the root nor
the leaves) stay within this group in the depen-
dency analysis. In other words, group coherence
make sure that the inner nodes of a group dimen-
sion cannot become daughters of nodes of a dif-
ferent group. In our support verb example, group
coherence ensures that e.g. that the determiner an
cannot become the determiner of a noun of an-
other group. We realize this idea through a new
XDG principle called group coherence principle.
This principle must hold on all dimensions of the
grammar.
Given a set of group identifiers Group , the prin-
ciple assumes two new features: group : Group ,
and outgroups : Lab ? 2Group . For each node v,
group(v) denotes the group identifier of the group
of v. For each edge within a group from v to
v? labeled l, i.e. for each edge to an inner node,
outgroups(v)(l) denotes the singleton set contain-
ing the group of both v and v?. For each edge
from v labeled l which goes outside of a group,
outgroups(v)(l) = Group , i.e. the edge can end
in any other group. We formalize the group coher-
ence principle as follows, where v l? v? denotes
an edge from v to v? labeled l:
v l? v? ? group(v?) ? outgroups(v)(l)
(16)
4.4 Examples
For illustration, we display the lexical entries of
two compiled groups: The group argues with
(with identifier g1), and the group has an argu-
ment with (with identifier g2), resp. in Figure 2 and
Figure 3. We omit the specification of outgroups
for the PA dimension for lack of space, and since it
is not relevant for the example: In all groups, there
are no edges which stay within a group in the PA
dimension.
4.5 Parsing and generation
We can use the same group lexicon for parsing and
generation, but we have to slightly adapt the com-
pilation for the generation case.
For parsing, we can use the XDG parser as be-
fore, without any changes.
For generation, we assume a set Sem of seman-
tic literals, multisets of which must be verbalized.
To do this, we assume a function groups : Sem ?
2Group , mapping each semantic literal to a set of
groups which verbalize it.
Now before using the XDG solver for genera-
tion, we have to make sure for each semantic lit-
eral s to be verbalized that the groups which can
potentially verbalize it have the same number of
nodes. For this, we calculate the maximum num-
ber n of syntactic nodes for the groups assigned
to s, and fill up the groups having less syntactic
nodes with deleted nodes. Then for XDG solving,
we introduce precisely n nodes for literal s. Using
groups for generation thus comes at the expense of
having to introduce additional nodes.7
As an example, consider we want to verbal-
ize the semantic literal argue. groups(argue) =
{g1, g2}, i.e. either groups g1 or g2 can verbalize
it. The maximum number n of syntactic nodes for
the two groups is 4 for g2 (has an argument with).
Hence, we have to fill up the groups having less
syntactic nodes with deleted nodes. In this case,
we have to add two deleted nodes to the group g1
(argue with) to get to four nodes. The resulting
lexical entries encoding g1 are displayed in Fig-
ure 4. The lexical entries for g2 stay the same as
in Figure 3.
After this step is done, we can use the existing
XDG solver to generate from the semantic literals
precisely the two possible verbalizations (7) and
(8), as intended.
7We should emphasize that n is not at all unrestricted.
For each semantic literal s to be verbalized, we have to intro-
duce only as many nodes as are contained in the largest group
which can verbalize s.
word literal group outgroupsID inID outID inPA outPA link
argues argue? g1 {pobj 7? {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1 7? {subj}arg2 7? {pcomp}}
with argue? g1 {} {pobj?} {pcomp!} {del?} {} {}
Figure 2: Lexical entries encoding the group for argues with
word literal group outgroupsID inID outID inPA outPA link
has argue? g2 {obj 7? {g2}} {root?} {subj!, obj!} {del?} {} {}
an argue? g2 {} {det?} {} {del?} {} {}
argument argue? g2 { det 7? {g2}pmod 7? {g2}} {obj?} {det!, pmod!} {root?} {arg1!, arg2!} {
arg1 7? {subj}
arg2 7? {pcomp}}
with argue? g2 {} {pmod?} {pcomp!} {del?} {} {}
Figure 3: Lexical entries encoding the group for has an argument with
5 Conclusion
We extended the XDG grammar formalism with
a means to break out of the straightjacket of the
1:1-correspondence between words and nodes. To
this end, we proposed the new notion of groups,
allowing to enrich the XDG lexicon with tuples
of dependency subgraphs. We illustrated how to
tackle complicated MWEs such as support verb
constructions with this new idea, and how to com-
pile groups into simple lexical entries.
We see two main benefits of our approach. The
first is that we can retain the XDG formalization,
and also its axiomatization as a constraint satisfac-
tion problem, in its entirety. Thus, we can simply
continue to use the existing XDG solver for pars-
ing and generation. The second benefit is that we
can use the same group lexicon for both parsing
and generation, the only difference being that for
generation, we have to slightly adapt the compila-
tion into lexical entries.
There are many issues which have remained un-
touched in this paper. For one, we did not talk
about our treatment of word order in this paper for
lack of space. Word order is among the best re-
searched issues in the context of XDG. For a thor-
ough discussion about word order in XDG, we re-
fer to (Duchier and Debusmann, 2001) and (De-
busmann, 2001).
Another issue is that of the relation between
groups and the meta-grammatical functionality
of the XDG lexicon, offering lexical inheritance,
templates and also disjunction in the sense of
crossings (Candito, 1996) to lexically state lin-
guistic generalizations. How well groups can be
integrated with this meta-grammatical functional-
ity is an open question.
XDG research has so far mainly been focused
on parsing, only to a very small extent on genera-
tion, and to no extent at all on Machine Translation
(MT). There is still a lot to do on both of the lat-
ter topics, and even for parsing, we are only at the
beginning. Although with our smaller-scale exam-
ple grammars, parsing and generation takes poly-
nomial time, we have yet to find out how we can
scale this up to large-scale grammars. We have
started importing and inducing large-scale gram-
mars from existing resources, but can so far only
speculate about if and how we can parse them ef-
ficiently. In a related line of research, we are also
working on the incorporation of statistical infor-
mation (Dienes et al, 2003) to help us to guide the
search for solutions. This could improve perfor-
mance because we would only have to search for
a few good analyses instead of enumerating hun-
dreds of them.
Acknowledgements
The idea for groups and this paper was triggered
by three events in fall 2003. The first was a work-
shop where Peter Dienes, Stefan Thater and me
worked out how to do generation with XDG. The
second was a presentation on the same workshop
by Aravind Joshi and Owen Rambow of their en-
coding of DG in TAG, and the third was a talk
by Charles Fillmore titled Multiword Expressions:
An Extremist Approach. I?d like to thank all of
them. And I?d like to thank all the others involved
with XDG, in alphabetical order: Ondrej Bojar,
Denys Duchier, Alexander Koller, Geert-Jan Krui-
word literal group outgroupsID inID outID inPA outPA link
argues argue? g1 {pobj 7? {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1 7? {subj}arg2 7? {pcomp}}
with argue? g1 {} {pobj?} {pcomp!} {del?} {} {}
 argue? g1 {} {del?} {} {del?} {} {}
 argue? g1 {} {del?} {} {del?} {} {}
Figure 4: Lexical entries for generation, encoding the group for argues with
jff, Vladislav Kubon, Marco Kuhlmann, Joachim
Niehren, Martin Platek and Gert Smolka for their
support and many helpful discussions.
References
Krzysztof R. Apt. 2003. Principles of Constraint Pro-
gramming. Cambridge University Press.
Norbert Bro?ker. 1999. Eine Dependenzgrammatik
zur Kopplung heterogener Wissensquellen. Lin-
guistische Arbeiten 405. Max Niemeyer Verlag,
Tu?bingen/GER.
Marie-H e`le?ne Candito. 1996. A principle-based hier-
archical representation of LTAG. In Proceedings of
COLING 1996, Kopenhagen/DEN.
Ralph Debusmann. 2001. A declarative grammar for-
malism for dependency grammar. Master?s thesis,
University of Saarland.
Ralph Debusmann. 2003. A parser system for extensi-
ble dependency grammar. In Denys Duchier, editor,
Prospects and Advances in the Syntax/Semantics In-
terface, pages 103?106. LORIA, Nancy/FRA.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical A* Dependency Parsing. In
Prospects and Advances in the Syntax/Semantics In-
terface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001. Topo-
logical dependency trees: A constraint-based ac-
count of linear precedence. In Proceedings of ACL
2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled trees
under lexicalized constraints and principles. Re-
search on Language and Computation, 1(3?4):307?
336.
Kim Gerdes and Sylvain Kahane. 2001. Word order
in german: A formal dependency grammar using a
topological hierarchy. In Proceedings of ACL 2001,
Toulouse/FRA.
Johannes Heinecke, Ju?rgen Kunze, Wolfgang Menzel,
and Ingo Schro?der. 1998. Eliminative parsing with
graded constraints. In Proceedings of COLING/ACL
1998, pages 526?530, Montreal/CAN.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on tree adjoin-
ing grammar. In Proceedings of MTT 2003, pages
207?216, Paris/FRA.
Aravind K. Joshi. 1987. An introduction to tree-
adjoining grammars. In Alexis Manaster-Ramer, ed-
itor, Mathematics of Language, pages 87?115. John
Benjamins, Amsterdam/NL.
Paul Kay and Charles J. Fillmore. 1999. Grammati-
cal constructions and linguistic generalizations: the
what?s x doing y? construction. Language, 75:1?33.
Alexander Koller and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
ACL 2002, Philadelphia/USA.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Ar-
chitecture of Informativity. Ph.D. thesis, Charles
University, Prague/CZ.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Igor Mel?c?uk. 1996. Lexical functions: a tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography
and Natural Language Processing. John Benjamins.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
Alexis Nasr. 1995. A formalism and a parser for lexi-
calised dependency grammars. In 4th International
Workshop on Parsing Technologies, pages 186?195,
Prague/CZ.
Alexis Nasr. 1996. Un syste`me de reformulation au-
tomatique de phrases fonde? sur la The?orie Sens-
Texte: application aux langues contro?le?es. Ph.D.
thesis, Universite? Paris 7.
Petr Sgall, Eva Hajicova, and Jarmila Panevova. 1986.
The Meaning of the Sentence in its Semantic and
Pragmatic Aspects. D. Reidel, Dordrecht/NL.
Lucien Tesni e`re. 1959. El?ements de Syntaxe Struc-
turale. Klincksiek, Paris/FRA.
Extensible Dependency Grammar: A New Methodology
Ralph Debusmann
Programming Systems Lab
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
rade@ps.uni-sb.de
Denys Duchier
?Equipe Calligramme
LORIA ? UMR 7503
Campus Scientifique, B. P. 239
54506 Vand?uvre le`s Nancy CEDEX
France
duchier@loria.fr
Geert-Jan M. Kruijff
Computational Linguistics
Saarland University
Postfach 15 11 50
66041 Saarbru?cken
Germany
gj@coli.uni-sb.de
Abstract
This paper introduces the new grammar formalism
of Extensible Dependency Grammar (XDG), and
emphasizes the benefits of its methodology of ex-
plaining complex phenomena by interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. This has the potential to increase
modularity with respect to linguistic description and
grammar engineering, and to facilitate concurrent
processing and the treatment of ambiguity.
1 Introduction
We introduce the new grammar formalism of Exten-
sible Dependency Grammar (XDG). In XDG, com-
plex phenomena arise out of the interaction of sim-
ple principles on multiple dimensions of linguis-
tic description. In this paper, we point out how
this novel methodology positions XDG in between
multi-stratal approaches like LFG (Bresnan and Ka-
plan, 1982) and MTT (Mel?c?uk, 1988), see also
(Kahane, 2002), and mono-stratal ones like HPSG
(Pollard and Sag, 1994), attempting to combine
their benefits and avoid their problems.
It is the division of linguistic analyses into dif-
ferent dimensions which makes XDG multi-stratal.
On the other, XDG is mono-stratal in that its princi-
ples interact to constrain all dimensions simultane-
ously. XDG combines the benefits of these two po-
sitions, and attempts to circumvent their problems.
From multi-stratal approaches, XDG adopts a high
degree of modularity, both with respect to linguis-
tic description as well as for grammar engineering.
This also facilitates the statement of cross-linguistic
generalizations. XDG avoids the problem of placing
too high a burden on the interfaces, and allows in-
teractions between all and not only adjacent dimen-
sions. From mono-stratal approaches, XDG adopts
a high degree of integration, facilitating concurrent
processing and the treatment of ambiguity. At the
same time, XDG does not lose its modularity.
XDG is a descendant of Topological Depen-
dency Grammar (TDG) (Duchier and Debusmann,
2001), pushing the underlying methodology further
by generalizing it in two aspects:
? number of dimensions: two in TDG (ID and
LP), arbitrary many in XDG
? set of principles: fixed in TDG, extensible
principle library in XDG
The structure of this paper is as follows: In ?2, we
introduce XDG and the XDG solver used for pars-
ing and generation. In ?3, we introduce a number
of XDG principles informally, before making use of
them in an idealized example grammar in ?4. In ?5
we argue why XDG has the potential to be an im-
provement over multi-stratal and mono-stratal ap-
proaches, before we conclude in ?6.
2 Extensible Dependency Grammar
In this section, we introduce XDG formally and
mention briefly the constraint-based XDG solver for
parsing and generation.
2.1 Formalization
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principles, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab,Fea,Val,Pri) of
a set Lab of edge labels, a set Fea of features, a set
Val of feature values, and a set of one-dimensional
principles Pri. A lexicon for the dimension D is a
set Lex ? Fea ? Val of total feature assignments
called lexical entries. An analysis on dimension
D is a triple (V,E,F) of a set V of nodes, a set
E ? V ?V ?Lab of directed labeled edges, and an
assignment F : V ? (Fea ? Val) of lexical entries
to nodes. V and E form a graph. We write AnaD for
the set of all possible analyses on dimension D. The
principles characterize subsets of AnaD. We assume
that the elements of Pri are finite representations of
such subsets.
An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,
Lex) consists of n dimensions, multi-dimensional
principles Pri, and a lexicon Lex. An XDG analysis
(V,Ei,Fi)ni=1 is an element of Ana = Ana1 ? ?? ? ?
Anan where all dimensions share the same set of
nodes V . We call a dimension of a grammar gram-
mar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex ? Lex1??? ??Lexn con-
strains all dimensions at once, thereby synchroniz-
ing them. An XDG analysis is licensed by Lex iff
(F1(v), . . . ,Fn(v)) ? Lex for every node v ?V .
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are licensed
by Lex, and consistent with Inp and Pri. The input
constraints determine whether XDG solving is to be
used for parsing or generation. For parsing, they
specify a sequence of words, and for generation, a
multiset of semantic literals.
2.2 Solver
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of integers,
where well-formed analyses correspond to the solu-
tions of the CSP (Duchier, 2003). We have imple-
mented an XDG solver using the Mozart-Oz pro-
gramming system.
XDG solving operates on all dimensions concur-
rently. This means that the solver can infer informa-
tion about one dimension from information on an-
other, if there is either a multi-dimensional principle
linking the two dimensions, or by the synchroniza-
tion induced by the lexical entries. For instance, not
only can syntactic information trigger inferences in
syntax, but also vice versa.
Because XDG allows us to write grammars with
completely free word order, XDG solving is an
NP-complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the
solver is exponential. The average-case complexity
of many smaller-scale grammars that we have ex-
perimented with seems polynomial, but it remains
to be seen whether we can scale this up to large-
scale grammars.
3 Principles
The well-formedness conditions of XDG analy-
ses are stipulated by principles. Principles are
parametrizable, e.g. by the dimensions on which
they are applied, or by lexical features. They can
be lexicalized or non-lexicalized, and can be one-
dimensional or multi-dimensional. Principles are
taken from an extensible principle library, and we
introduce some of the most important principles in
the following.
3.1 Tree principle
tree(i) The analysis on dimension i must be a tree.
The tree principle is non-lexicalized and
parametrized by the dimension i.
3.2 Dag principle
dag(i) The analysis on dimension i must be a di-
rected acyclic graph.
The dag principle is non-lexicalized and
parametrized by the dimension i.
3.3 Valency principle
valency(i, ini,outi) All nodes on dimension i must
satisfy their in and out specifications.
The valency principle is lexicalized and serves
to lexically describe dependency graphs. It is
parametrized by the dimension i, the in specification
ini and the out specification outi. For each node, ini
stipulates the licensed incoming edges, and outi the
licensed outgoing edges.
In the example grammar lexicon part in Figure 1
below, the in specification is inID and outID is the
out specification on the ID dimension. For the com-
mon noun Roman, the in specification licenses zero
or one incoming edges labeled subj, and zero or one
incoming edges labeled obj ({subj?,obj?}), i.e. it
can be either a subject or an object. The out specifi-
cation requires precisely one outgoing edge labeled
det ({det!}), i.e. it requires a determiner.
3.4 Government principle
government(i,casesi,governi) All edges in dimen-
sion i must satisfy the government specification of
the mother.
The government principle is lexicalized. Its pur-
pose is to constrain the case feature of a depen-
dent.1 It is parametrized by the dimension i, the
cases specification casesi and the government spec-
ification govern. cases assigns to each word a set of
possible cases, and govern a mapping from labels to
sets of cases.
In Figure 1, the cases specification for the deter-
miner den is {acc} (i.e. it can only be accusative).
By its government specification, the finite verb ver-
sucht requires its subject to exhibit nominative case
(subj 7? {nom}).
3.5 Agreement principle
agreement(i,casesi,agreei) All edges in dimension
i must satisfy the agreement specification of the
mother.
1We restrict ourselves to the case feature only for simplicity.
In a fully-fledged grammar, the government principle would be
used to constrain also other morphological aspects like number,
person and gender.
The agreement principle is lexicalized. Its pur-
pose is to enforce the case agreement of a daugh-
ter.2 It is parametrized by dimension i, the lexical
cases specification casesi, assigning to each word a
set of possible cases, and the agreement specifica-
tion agreei, assigning to each word a set of labels.
As an example, in Figure 1, the agreement spec-
ification for the common noun Roman is {det}, i.e.
the case of the common noun must agree with its
determiner.
3.6 Order principle.
order(i,oni,?i) On dimension i, 1) each node must
satisfy its node labels specification, 2) the order of
the daughters of each node must be compatible with
?i, and 3) the node itself must be ordered correctly
with respect to its daughters (using its node label).
The order principle is lexicalized. It is
parametrized by the dimension i, the node labels
specification oni mapping each node to set of labels
from Labi, and the total order ?i on Labi.
Assuming the node labels specification given in
Figure 2, and the total order in (5), the tree in (11)
satisfies the order principle.3 For instance for the
node versucht: 1) The node label of versucht is lbf,
satisfying the node labels specification. 2) The order
of the daughters Roman (under the edge labeled vf),
Peter (mf) and lesen (rbf) is compatible with the
total order prescribing vf ? mf ? rbf. 3) The node
versucht itself is ordered correctly with respect to its
daughters (the total order prescribes vf ? lbf ?mf).
3.7 Projectivity principle
projectivity(i) The analysis on dimension i must be
projective.
The projectivity principle is non-lexicalized. Its
purpose is to exclude non-projective analyses.4 It is
parametrized by dimension i.
3.8 Climbing principle
climbing(i, j) The graph on dimension i must be
flatter than the graph on dimension j.
The climbing principle is non-lexicalized and
two-dimensional. It is parametrized by the two di-
mensions i and j.
For instance, the tree in (11) is flatter than the
corresponding tree in (10). This concept was intro-
duced as lifting in (Kahane et al, 1998).
2Again, we restrict ourselves to case for simplicity.
3The node labels are defined in (2) below.
4The projectivity principle of course only makes sense in
combination with the order principle.
3.9 Linking principle
linking(i, j, linki, j) All edges on dimension i must
satisfy the linking specification of the mother.
The linking principle is lexicalized and two-
dimensional. It is parametrized by the two dimen-
sions i and j, and by the linking specification linki, j ,
mapping labels from Labi to sets of labels from
Lab j . Its purpose is to specify how dependents on
dimension i are realized by (or linked to) dependents
on dimension j.
In the lexicon part in Figure 3, the linking spec-
ification for the transitive verb lesen requires that
its agent on the PA dimension must be realized by a
subject (ag 7? {subj}), and the patient by an object
(pat 7? {obj}).
The linking principle is oriented. Symmetric
linking could be gained simply by using the linking
principle twice (in both directions).
4 Example grammar
In this section, we elucidate XDG with an example
grammar fragment for German. With it, we demon-
strate three aspects of the methodology of XDG:
? How complex phenomena such as topicaliza-
tion and control arise by the interaction of sim-
ple principles on different dimensions of lin-
guistic description.
? How the high degree of integration helps to re-
duce ambiguity.
? How the high degree of modularity facilitates
the statement of cross-linguistic generaliza-
tions.
Note that this grammar fragment is an idealized ex-
ample, and does not make any claims about XDG as
a grammar theory. Its purpose is solely to substan-
tiate our points about XDG as a framework. More-
over, the grammar is fully lexicalized for simplicity.
However, XDG of course allows the grammar writer
to formulate lexical abstractions using inheritance
(like in HPSG) or crossings (Candito, 1996).
4.1 Dimensions
The grammar fragment make use of two dimen-
sions: Immediate Dominance (ID) and Linear
Precedence (LP). The models on the ID dimension
are unordered, syntactic dependency trees whose
edge labels correspond to syntactic functions like
subject and object. On the LP dimension, the mod-
els are ordered, projective topological dependency
trees whose edge labels are topological fields like
Vorfeld and Mittelfeld.
4.2 Labels
The set LabID of labels on the ID dimension is:
LabID = {det,subj,obj,vinf ,part} (1)
These correspond resp. to determiner, subject, ob-
ject, infinitive verbal complement, and particle.
The set LabLP of labels on the LP dimension is:
LabLP = {detf,nounf,vf, lbf,mf,partf, rbf} (2)
Corresponding resp. to determiner field, noun field,
Vorfeld, left bracket field, Mittelfeld, particle field,
and right bracket field.
4.3 Principles
On the ID dimension, we make use of the following
one-dimensional principles:
tree(ID)
valency(ID, inID,outID)
government(ID,casesID,governID)
agreement(ID,casesID,agreeID)
(3)
The LP dimension uses the following principles:
tree(LP)
valency(LP, inLP,outLP)
order(LP,onLP,?LP)
projectivity(LP)
(4)
where the total order ?LP is defined as:
detf ? nounf ? vf ? lbf ? mf ? partf ? rbf (5)
We make use of the following multi-dimensional
principles:
climbing(LP, ID)
linking(LP, ID) (6)
4.4 Lexicon
We split the lexicon into two parts. The ID and LP
parts are displayed resp. in Figure 15 and Figure 2.
The LP part includes also the linking specification
for the LP,ID-application of the linking principle.6
4.5 Government and agreement
Our first example is the following sentence:
Peter versucht einen Roman zu lesen.
Peter tries aacc novel to read.
Peter tries to read a novel.
(7)
We display the ID analysis of the sentence below:
.
Peter versucht einen Roman zu lesen
subj vinf
partobj
det
(8)
5Here, stands for ?don?t care?, this means e.g. for the verb
versucht that it has unspecified case.
6We do not make use of the linking specification for the
German grammar fragment (the mappings are all empty), but
we will do so as we switch to Dutch in ?4.8 below.
Here, Peter is the subject of versucht. lesen is the in-
finitival verbal complement of versucht, zu the parti-
cle of lesen, and Roman the object of lesen. Finally,
einen is the determiner of Roman.
Under our example grammar, the sentence is un-
ambiguous, i.e. the given ID tree is the only possible
one. Other ID trees are ruled out by the interaction
of the principles on the ID dimension. For instance,
the government and agreement principles conspire
to rule out the reading where Roman is the subject of
versucht (and Peter the object). How? By the agree-
ment principle, Roman must be accusative, since it
agrees with its accusative determiner einen. By the
government principle, the subject of versucht must
be nominative, and the object of lesen accusative.
Thus Roman, by virtue of being accusative, cannot
become the subject of versucht. The only other op-
tion for it is to become the object of lesen. Conse-
quently, Peter, which is unspecified for case, must
become the subject of versuchen (versuchen must
have a subject by the valency principle).
4.6 Topicalization
Our second example is a case of topicalization,
where the object has moved into the Vorfeld, to the
left of the finite verb:
Einen Roman versucht Peter zu lesen. (9)
Here is the ID tree and the LP tree analysis:
.
Einen Roman versucht Peter zu lesen
subj vinf
partobj
det
(10)
.
Einen Roman versucht Peter zu lesen
detf
nounf
lbf
nounf
partf
rbf
vf
detf
mf rbf
part
f
(11)
The ID tree analysis is the same as before, except
that the words are shown in different positions. In
the LP tree, Roman is in the Vorfeld of versucht, Pe-
ter in the Mittelfeld, and lesen in the right bracket
field. versucht itself is (by its node label) in the left
bracket field. Moreover, Einen is in the determiner
field of Roman, and zu in the particle field of lesen.
Again, this is an example demonstrating how
complex phenomena (here: topicalization) are ex-
plained by the interaction of simple principles. Top-
icalization does not have to explicitly taken care of,
it is rather a consequence of the interacting princi-
ples. Here, the valency, projectivity and climbing
inID outID casesID governID agreeID
den {det?} {} {acc} {} {}
Roman {subj?,obj?} {det!} {nom,dat,acc} {} {det}
Peter {subj?,obj?} {} {nom,dat,acc} {} {}
versucht {} {subj!,vinf!} {subj 7? {nom}} {}
zu {part?} {} {} {}
lesen {vinf?} {obj!} {obj 7? {acc}} {}
Figure 1: Lexicon for the example grammar fragment, ID part
inLP outLP onLP linkLP,ID
den {detf?} {} {detf} {}
Roman {vf?,mf?} {detf!} {nounf} {}
Peter {vf?,mf?} {} {nounf} {}
versucht {} {vf?,mf?, rbf?} {lbf} {}
zu {partf?} {} {partf} {}
lesen {rbf?} {} {rbf} {}
Figure 2: Lexicon for the example grammar fragment, LP part
principles conspire to bring about the ?climbing up?
of the NP Einen Roman from being the daughter of
lesen in the ID tree to being the daughter of versucht
in the LP tree: The out specification of lesen does
not license any outgoing edge. Hence, Roman must
become the daughter of another node. The only pos-
sibility is versucht. The determiner Einen must then
also ?climb up? because Roman is its only possi-
ble mother. The result is an LP tree which is flat-
ter with respect to the ID tree. The LP tree is also
projective. If it were not be flatter, then it would
be non-projective, and ruled out by the projectivity
principle.
4.7 Negative example
Our third example is a negative example, i.e. an un-
grammatical sentence:
?Peter einen Roman versucht zu lesen. (12)
This example is perfectly legal on the unordered ID
dimension, but has no model on the LP dimension.
Why? Because by its LP out specification, the finite
verb versucht allows only one dependent to the left
of it (in its Vorfeld), and here we have two. The
interesting aspect of this example is that although
we can find a well-formed ID tree for it, this ID tree
is never actually generated. The interactions of the
principles, viz. here of the principles on the LP di-
mension, rule out the sentence before any full ID
analysis has been found.
4.8 From German to Dutch
For the fourth example, we switch from German to
Dutch. We will show how to use the lexicon to con-
cisely capture an important cross-linguistic general-
ization. We keep the same grammar as before, but
with two changes, arising from the lesser degree of
inflection and the higher reliance on word order in
Dutch:
? The determiner een is not case-marked but
can be either nominative, dative or accusative:
casesID = {nom,dat,acc}.
? The Vorfeld of the finite verb probeert cannot
be occupied by an object (but only by an ob-
ject): linkLP,ID = {vf 7? {subj}}.7
Now to the example, a Dutch translation of (7):
Peter probeert een roman te lezen.
Peter tries a novel to read.
Peter tries to read a novel.
(13)
We get only one analysis on the ID dimension,
where Peter is the subject and roman the object.
An analysis where Peter is the object of lezen and
roman the subject of probeert is impossible, as in
the German example. The difference is, however,
how this analysis is excluded. In German, the ac-
cusative inflection of the determiner einen triggered
the agreement and the government principle to rule
it out. In Dutch, the determiner is not inflected.
The unwanted analysis is excluded on the grounds
of word order instead: By the linking principle, the
Vorfeld of probeert must be filled by a subject, and
not by an object. That means that Peter in the Vor-
feld (to the left of probeert) must be a subject, and
consequently, the only other choice for roman is that
it becomes the object of lezen.
4.9 Predicate-Argument Structure
Going towards semantics, we extend the grammar
with another dimension, Predicate-Argument Struc-
ture (PA), where the models are not trees but di-
rected acyclic graphs (dags), to model re-entrancies
7Of course, this is an idealized assumption. In fact, given
the right stress, the Dutch Vorfeld can be filled by objects.
e.g. caused by control constructions. Thanks to the
modularity of XDG, the PA part of the grammar is
the same for German and Dutch.
The set LabPA of labels on the PA dimension is:
LabPA = {ag,pat,prop} (14)
Corresponding resp. to agent, patient and proposi-
tion.
The PA dimension uses the following one-
dimensional principles:
dag(PA)
valency(PA, inPA,outPA) (15)
Note that we re-use the valency principle again, as
we did on the ID and LP dimensions.
And also the following multi-dimensional princi-
ples:
climbing(ID, PA)
linking(PA, ID) (16)
Here, we re-use the climbing and linking princi-
ples. That is, we state that the ID tree is flatter
than the corresponding PA dag. This captures rais-
ing and control, where arguments of embedded infi-
nite verbs can ?climb up? and become arguments of
a raising or control verb, in the same way as syntac-
tic arguments can ?climb up? from ID to LP. We use
the linking principle to specify how semantic argu-
ments are to be realized syntactically (e.g. the agent
as a subject etc.). We display the PA part of the lex-
icon in Figure 3.8
Here is an example PA dag analysis of example
sentence (7):
.
Peter versucht einen Roman zu lesen
ag
prop
patag
(17)
Here, Peter is the agent of versucht, and also the
agent of lesen. Furthermore, lesen is a proposition
dependent of versucht, and Roman is the patient of
lesen.
Notice that the PA dag is indeed a dag and not a
tree since Peter has two incoming edges: It is simul-
taneously the agent of versucht and of lesen. This
is enforced by by the valency principle: Both ver-
sucht and lesen require an agent. Peter is the only
word which can be the agent of both, because it is a
subject and the agents of versucht and lesen must
be subjects by the linking principle. The climb-
ing principle ensures that predicate arguments can
8Notice that we specify linking lexically, allowing us to
capture deviations from the typical linking patterns. Still, we
can also accommodate linking generalizations using lexical ab-
stractions.
be ?raised? on the ID structure with respect to the
PA structure. Again, this example demonstrates that
XDG is able to reduce a complex phenomenon such
as control to the interaction of per se fairly simple
principles such as valency, climbing and linking.
5 Comparison
This section includes a more in-depth comparison
of XDG with purely multi- and mono-stratal ap-
proaches.
Contrary to multi-stratal approaches like LFG or
MTT, XDG is more integrated. For one, it places
a lighter burden the interfaces between the dimen-
sions. In LFG for instance, the ? -mapping from c-
structure to f-structure is rather specific, and has to
be specifically adapted to new c-structures, e.g. in
order to handle a new construction with a different
word order. That is, not only the grammar rules for
the c-structure need to be adapted, but also the inter-
face between c- and f-structure. In XDG, complex
phenomena arise out of the interaction of simple,
maximally general principles. To accommodate the
new construction, the grammar would ideally only
need to be adapted on the word order dimension.
Furthermore, XDG allows interactions of rela-
tional constraints between all dimensions, not only
between adjacent ones (like c- and f-structure),
and in all directions. For one, this gets us bi-
directionality for free. Secondly, the interactions
of XDG have the potential to help greatly in reduc-
ing ambiguity. In multi-stratal approaches, ambigu-
ity must be duplicated throughout the system. E.g.
suppose there are two candidate c-structures in LFG
parsing, but one is ill-formed semantically. Then
they can only be ruled out after duplicating the am-
biguity on the f-structure, and then filtering out the
ill-formed structure on the semantic ? -structure. In
XDG on the other hand, the semantic principles can
rule out the ill-formed analysis much earlier, typ-
ically on the basis of a partial syntactic analysis.
Thus, ill-formed analyses are never duplicated.
Contrary to mono-stratal ones, XDG is more
modular. For one, as (Oliva et al, 1999) note,
mono-stratal approaches like HPSG usually give
precedence to the syntactic tree structure, while
putting the description of other aspects of the anal-
ysis on the secondary level only, by means of fea-
tures spread over the nodes of the tree. As a result,
it becomes a hard task to modularize grammars. Be-
cause syntax is privileged, the phenomena ascribing
to semantics cannot be described independently, and
whenever the syntax part of the grammar changes,
the semantics part needs to be adapted. In XDG, no
dimension is privileged to another. Semantic phe-
inPA outPA linkPA,ID
den {} {} {}
Roman {ag?,pat?} {} {}
Peter {ag?,pat?} {} {}
versucht {} {ag!,prop!} {ag 7? {subj},prop 7? {vinf}}
zu {} {} {}
lesen {prop?} {ag!,pat!} {ag 7? {subj},pat 7? {obj}}
Figure 3: Lexicon of the example grammar fragment, PA part
nomena can be described much more independently
from syntax. This facilitates grammar engineering,
and also the statement of cross-linguistic general-
izations. Assuming that the semantics part of a
grammar stay invariant for most natural languages,
in order to accommodate a new language, ideally
only the syntactic parts would need to be changed.
6 Conclusion
In this paper, we introduced the XDG grammar
framework, and emphasized that its new methodol-
ogy places it in between the extremes of multi- and
mono-stratal approaches. By means of an idealized
example grammar, we demonstrated how complex
phenomena are explained as arising from the in-
teraction of simple principles on numerous dimen-
sions of linguistic description. On the one hand, this
methodology has the potential to modularize lin-
guistic description and grammar engineering, and
to facilitate the statement of linguistic generaliza-
tions. On the other hand, as XDG is a inherently
concurrent architecture, inferences from any dimen-
sion can help reduce the ambiguity on others.
XDG is a new grammar formalism, and still has
many open issues. Firstly, we need to continue work
on XDG as a framework. Here, one important goal
is to find out what criteria we can give to restrict the
principles. Secondly, we need to evolve the XDG
grammar theory, and in particular the XDG syntax-
semantics interface. Thirdly, for practical use, we
need to improve our knowledge about XDG solv-
ing (i.e. parsing and generation). So far, our only
good results are for smaller-scale handwritten gram-
mars, and we have not good results yet for larger-
scale grammars induced from treebanks (NEGRA,
PDT) or converted from other grammar formalisms
(XTAG). Finally, we need to incorporate statistics
into the picture, e.g. to guide the search for solu-
tions, in the vein of (Dienes et al, 2003).
References
Joan Bresnan and Ronald Kaplan. 1982. Lexical-
functional grammar: A formal system for gram-
matical representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 173?281. The MIT Press, Cam-
bridge/USA.
Marie-He`le?ne Candito. 1996. A principle-based hi-
erarchical representation of LTAG. In Proceed-
ings of COLING 1996, Kopenhagen/DEN.
Peter Dienes, Alexander Koller, and Marco
Kuhlmann. 2003. Statistical A* Dependency
Parsing. In Prospects and Advances in the Syn-
tax/Semantics Interface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
based account of linear precedence. In Proceed-
ings of ACL 2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled
trees under lexicalized constraints and principles.
Research on Language and Computation, 1(3?
4):307?336.
Sylvain Kahane, Alexis Nasr, and Owen Ram-
bow. 1998. Pseudo-projectivity: a polynomi-
ally parsable non-projective dependency gram-
mar. In 36th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 1998),
Montre?al/CAN.
Sylvain Kahane. 2002. Grammaire d?Unification
Sens-Texte: Vers un mode`le mathe?matique ar-
ticule? de la langue. Universite? Paris 7. Docu-
ment de synthe`se de l?habilitation a` diriger les
recherches.
Alexander Koller and Kristina Striegnitz. 2002.
Generation as dependency parsing. In Proceed-
ings of ACL 2002, Philadelphia/USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Karel Oliva, M. Andrew Moshier, and Sabine
Lehmann. 1999. Grammar engineering for the
next millennium. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium
1999 ?Closing the Millennium?, Beijing/CHI.
Tsinghua University Press.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago/USA.
