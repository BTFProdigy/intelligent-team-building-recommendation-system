Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056?1065,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
A Compact Forest for Scalable Inference
over Entailment and Paraphrase Rules
Roy Bar-Haim
?
, Jonathan Berant
?
, Ido Dagan
?
?
Computer Science Department, Bar-Ilan University, Ramat Gan 52900, Israel
{barhair,dagan}@cs.biu.ac.il
?
The Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israel
jonatha6@post.tau.ac.il
Abstract
A large body of recent research has been
investigating the acquisition and applica-
tion of applied inference knowledge. Such
knowledge may be typically captured as
entailment rules, applied over syntactic
representations. Efficient inference with
such knowledge then becomes a funda-
mental problem. Starting out from a for-
malism for entailment-rule application we
present a novel packed data-structure and
a corresponding algorithm for its scalable
implementation. We proved the validity of
the new algorithm and established its effi-
ciency analytically and empirically.
1 Introduction
Applied semantic inference is concerned with de-
riving target meanings from texts. In the textual
entailment framework, this is reduced to infer-
ring a textual statement (the hypothesis h) from
a source text (t). Traditional formal semantics
approaches perform such inferences over logi-
cal forms derived from the text. By contrast,
most practical NLP applications operate over shal-
lower representations such as parse trees, possibly
supplemented with limited semantic information
about named entities, semantic roles etc.
Most commonly, inference over such represen-
tations is made by applying some kind of transfor-
mations or substitutions to the tree or graph rep-
resenting the text. Such transformations may be
generally viewed as entailment (inference) rules,
which capture semantic knowledge about para-
phrases, lexical relations such as synonyms and
hyponyms, syntactic variations etc. Such knowl-
edge is either composed manually, e.g. WordNet
(Fellbaum, 1998), or learned automatically.
A large body of work has been dedicated to
learning paraphrases and entailment rules, e.g.
(Lin and Pantel, 2001; Shinyama et al, 2002;
Szpektor et al, 2004; Bhagat and Ravichandran,
2008), identifying appropriate contexts for their
application (Pantel et al, 2007) and utilizing them
for inference (de Salvo Braz et al, 2005; Bar-
Haim et al, 2007). Although current avail-
able rule bases are still quite noisy and incom-
plete, the progress made in recent years suggests
that they may become increasingly valuable for
text understanding applications. Overall, applied
knowledge-based inference is a prominent line of
research gaining much interest, with recent exam-
ples including the series of workshops on Knowl-
edge and Reasoning for Answering Questions
(KRAQ)
1
and the planned evaluation of knowledge
resources in the forthcoming 5
th
Recognizing Tex-
tual Entailment challenge (RTE-5)
2
.
While many applied systems utilize semantic
knowledge via such inference rules, their use is
typically limited, application-specific, and quite
heuristic. Formalizing these practices seems im-
portant for applied semantic inference research,
analogously to the role of well-formalized mod-
els in parsing and machine translation. Bar-Haim
et al (2007) made a step in this direction by in-
troducing a generic formalism for semantic infer-
ence over parse trees. Their formalism uses entail-
ment rules as a unifying representation for various
types of inference knowledge, allowing unified in-
ference as well. In this formalism, rule application
has a clear, intuitive interpretation as generating a
new sentence parse (a consequent), semantically
entailed by the source sentence. The inferred con-
sequent may be subject to further rule applications
1
http://www.irit.fr/recherches/ILPL/kraq09.html
2
http://www.nist.gov/tac/2009/RTE/
1056
and so on. In their implementation, each conse-
quent was generated explicitly as a separate tree.
Following this line of work, our long-term re-
search goal is to investigate effective utilization
of entailment rules for inference. While the for-
malism of Bar-Haim et al provides a princi-
pled framework for modeling such inferences,
its implementation using explicit generation of
consequents raises severe efficiency issues, since
the number of consequents may grow exponen-
tially in the number of rule applications. Con-
sider, for example, the sentence ?Children are
fond of candies.?, and the following entailment
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?. The number of derivable
sentences (including the source sentence) would
be 2
3
as each rule can either be applied or not, in-
dependently. Indeed, we found that this exponen-
tial explosion leads to poor scalability in practice.
Intuitively, we would like that each rule applica-
tion would add just the entailed part (e.g. kids) to a
packed sentence representation. Yet, we still want
the resulting structure to represent a set of entailed
sentences, rather than some mixture of sentence
fragments whose semantics is unclear.
As discussed in section 5, previous work pro-
posed only partial solutions to this problem. In this
paper we present a novel data structure, termed
compact forest, and a corresponding inference al-
gorithm, which efficiently generate and represent
all consequents while preserving the identity of
each individual one (section 3). Our work is
inspired by previous work on packed represen-
tations in other fields, such as parsing, genera-
tion and machine translation (section 5). As
we follow a well-defined formalism, we could
prove that all inference operations of Bar-Haim
et al are equivalently applied over the compact
forest. We compare inference cost over compact
forests to explicit consequent generation both the-
oretically (section 3.4), illustrating an exponential-
to-linear complexity ratio, and empirically (sec-
tion 4), showing improvement by orders of magni-
tude. These results suggest that our data-structure
and algorithm are both valid and scalable, open-
ing up the possibility to investigate large-scale en-
tailment rule application within a well-formalized
framework.
2 Inference Framework
This section briefly presents a (simplified) descrip-
tion of the tree transformations inference formal-
ism of Bar-Haim et al (2007). Given a source text,
syntactically parsed, and a set of entailment rules
representing tree transformations, the formalism
defines the set of consequents derivable from the
text using the rules. Each consequent is obtained
through a sequence of rule applications, each gen-
erates an intermediate parse tree, similar to a proof
process in logic.
More specifically, sentences are represented as
dependency trees, where nodes are annotated with
lemma and part-of-speech, and edges are anno-
tated with dependency relation. A rule ?L ? R?
is primarily composed of two templates, termed
left-hand-side (L), and right-hand-side (R). Tem-
plates are dependency subtrees which may con-
tain POS-tagged variables, matching any lemma.
Figure 1(a) shows passive-to-active transforma-
tion rule, and (b) illustrates its application.
A rule application generates a derived tree d
from a source tree s through the following steps:
L matching: First, a match of L in the source
tree s is sought. In our example, the variable V is
matched in the verb see, N1 is matched in Mary
and N2 is matched in John.
R instantiation: Next, a copy of R is generated
and its variables are instantiated according to their
matching node in L. In addition, a rule may spec-
ify alignments, defined as a partial function from
L nodes to R nodes. An alignment indicates that
for each modifier m of the source node that is not
part of the rule structure, the subtree rooted at m
should also be copied as a modifier of the target
node. In addition to defining alignments explic-
itly, each variable in L is implicitly aligned to its
counterpart in R. In our example, the alignment
between the V nodes implies that yesterday (mod-
ifying see) should be copied to the generated sen-
tence, and similarly beautiful (modifying Mary) is
copied for N1.
Derived tree generation: Let r be the instanti-
ated R, along with its descendants copied from L
through alignment, and l be the subtree matched
by L. The formalism has two methods for gen-
erating the derived tree d: substitution and intro-
duction, as specified by the rule type. Substitution
rules specify modification of a subtree of s, leav-
ing the rest of s unchanged. Thus, d is formed by
copying s while replacing l (and the descendants
1057
LV VERB
obj
ssf
f
f
f
f
f
f
f
f
f
be

by
++
X
X
X
X
X
X
X
X
X
X
R
V VERB
subj
ssf
f
f
f
f
f
f
f
f
f
obj
++
X
X
X
X
X
X
X
X
X
X
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(a) Passive to active transformation (substitution rule)
ROOT
i

see VERB
obj
qqc
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
be
ssf
f
f
f
f
f
f
f
f
f
by

mod
++
X
X
X
X
X
X
X
X
X
X
Mary NOUN
mod

be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i

see VERB
subj
rre
e
e
e
e
e
e
e
e
e
e
obj

mod
,,
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
John NOUN Mary NOUN
mod

yesterday NOUN
beautiful ADJ
Source: Beautiful Mary was seen by John yesterday. Derived: John saw beautiful Mary yesterday.
(b) Application of passive to active transformation
Figure 1: An inference rule application. POS and relation labels are based on Minipar (Lin, 1998)
of l?s nodes) with r. This is the case for the pas-
sive rule, as well as for lexical rules such as ?buy
? purchase?. By contrast, introduction rules are
used to make inferences from a subtree of s, while
the other parts of s are ignored and do not affect d.
A typical example is inferring a proposition em-
bedded as a relative clause in s. In this case, the
derived tree d is simply taken to be r.
In addition to inference rules, the formalism in-
cludes annotation rules which add features to ex-
isting parse tree nodes. These rules have been used
for identifying contexts that affect the polarity of
predicates.
As shown by Bar-Haim et al, this concise, well
defined formalism allows unified representation of
diverse types of knowledge which are commonly
used for applied semantic inference.
3 Efficient Inference over Compact Parse
Forests
As shown in the introduction, explicit genera-
tion of consequents (henceforth explicit inference)
leads to an exponential explosion of the number
of generated trees. In this section we present our
efficient implementation for this formalism. Our
implementation is based on a novel data structure,
termed compact forest (Section 3.1), which com-
pactly represents a large set of trees. Each rule
application generates explicitly only the nodes of
the rule right-hand-side while the rest of the con-
sequent tree is shared with the source, which also
reduces the number of redundant rule applications.
As we shall see, this novel representation is based
primarily on disjunction edges, an extension of
dependency edges that specify a set of alterna-
tive edges of multiple trees. Section 3.2 presents
an efficient algorithm for inference over compact
forests, followed by a discussion of its correctness
and complexity (sections 3.3 and 3.4).
3.1 The Compact Forest Data Structure
A compact forestF represents a set of dependency
trees. Figure 2 shows an example of a compact
forest, containing both the source and derived sen-
tences of Figure 1. We first define a more general
data structure for directed graphs, and then narrow
the definition to the case of trees.
A Compact Directed Graph (cDG) is a pair
G = (V, E) where V is a set of nodes and E is a
set of disjunction edges (d-edges). Let D be a
set of dependency relations. A d-edge d is a triple
(S
d
, rel
d
, T
d
), where S
d
and T
d
are disjoint sets
of source nodes and target nodes; rel
d
: S
d
? D
is a function specifying the dependency relation
corresponding to each source node. Graphically,
d-edges are shown as point nodes, with incoming
edges from source nodes and outgoing edges to
target nodes. For instance, let d be the bottom-
most d-edge in Figure 3. Then S
d
= {of, like},
T
d
= {candy, sweet}, rel(of ) = pcomp-n, and
rel(like) = obj.
A d-edge represents, for each s
i
? S
d
, a set of
1058
ROOT
i
John
see
by objbe mod
by
pcomp-n
beautiful
Mary
mod
be yesterday
see
subj
objmod
Figure 2: A compact forest containing both the
source and derived sentences of Figure 1. Parts
of speech are omitted.
alternative directed edges {(s
i
, t
j
) : t
j
? T
d
}, all
of which are labeled with the same relation given
by rel
d
(s
i
). Each of these edges, termed embed-
ded edge (e-edge), would correspond to a differ-
ent graph represented in G. In the previous exam-
ple, the e-edges are like
obj
???candy, like
obj
???sweet,
of
pcomp?n
???????candy and of
pcomp?n
???????sweet (notice
that the definition implies that all source nodes in
S
d
have the same set of alternative target nodes
T
d
). d is called an outgoing d-edge of a node v if
v ? S
d
and an incoming d-edge of v if v ? T
d
.
A Compact Directed Acyclic Graph (cDAG) is a
cDG that contains no cycles of e-edges.
A DAG G rooted in a node v ? V of a cDAG
G is embedded in G if it can be derived as follows:
we initialize G with v alone; then, we expand v
by choosing exactly one target node t ? T
d
from
each outgoing d-edge d of v, and adding t and the
corresponding e-edge (v, t) to G. This expansion
process is repeated recursively for each new node
added to G.
Each such set of choices results in a different
DAG with v as its only root. In Figure 2, we may
choose to connect the root either to the left see,
resulting in the source passive sentence, or to the
right see, resulting in the derived active sentence.
A Compact Forest F is a cDAG with a single
root r (i.e. r has no incoming d-edges) where all
the embedded DAGs rooted in r are trees. This set
of trees, termed embedded trees, comprise the set
of trees represented by F .
Figure 3 shows another example for a compact
ROOT
i
child
be
pred
fond
subjmod
of
pcomp-n
candy
like
subj
obj
kid
sweet
Figure 3: A compact forest representing the 2
3
sentences derivable from the sentence ?children
are fond of candies? using the following three
rules: ?children?kids?, ?candies?sweets?, and ?X
is fond of Y?X likes Y?.
forest efficiently representing the 2
3
sentences re-
sulting from three independently-applied rules.
3.2 The Inference Process
Next, we describe the algorithm implementing the
inference process of Section 2 over the compact
forest (henceforth, compact inference), illustrating
it through Figures 1 and 2.
Forest initialization F is initialized with the
set of dependency trees representing the text sen-
tences, with their roots connected under the forest
root as the target nodes of a single d-edge. Depen-
dency edges are transformed trivially to d-edges
with a single source and target. Annotation rules
are applied at this stage to the initial F . The black
part of Figure 2 corresponds to the initial forest.
Rule application comprises the following steps:
L matching: L is matched in F if there exists
an embedded tree t in F such that L is matched
in t, as in Section 2. We denote by l the subtree
of t in which L was matched. As in section 2, the
match in our example is (V,N1, N2)=(see, Mary,
John). Notice that this definition does not allow l
to be scattered over multiple embedded trees.
As the target nodes of a d-edge specify alterna-
tives for the same position in the tree, their parts-
of-speech are expected to be of substitutable types.
1059
In this paper we further assume that all target
nodes of the same d-edge have the same part-of-
speech
3
. Consequently, variables that are leaves in
L and may match a certain target node of a d-edge
d are mapped to the whole set of target nodes T
d
rather than to a single node. This yields a compact
representation of multiple matches, and prevents
redundant rule applications. For instance, given
a compact representation of ?{Children/kids} are
fond of {candies/sweeets}? (cf. Figure 3), the rule
?X is fond of Y?X likes Y? will be matched and
applied only once, rather than four times (for each
combination of matching X and Y ).
Derived tree generation: A template r consist-
ing of R while excluding variables that are leaves
of both L and R (termed dual leaf-variables)
4
is
generated and inserted into F . In case of a substi-
tution rule (as in our example), r is set as an alter-
native to l by adding r?s root to T
d
, where d is the
incoming d-edge of l?s root. In case of an intro-
duction rule, it is set as an alternative to the other
trees in the forest by adding r?s root to the target
node set of the forest root?s outgoing d-edge. In
our example, r is the gray node (still labeled with
the variable V ) , and it becomes an additional tar-
get node of the d-edge entering the original (left)
see.
Variable instantiation: Each variable in r (i.e.
a non-dual leaf) is instantiated according to its
match in L (as in Section 2), e.g. V is instantiated
with see. As specified above, if the variable is a
leaf inL is not a dual leaf then it is matched in a set
of nodes, and hence each of them should be instan-
tiated in r. This is decomposed into a sequence
of simpler operations: first, r is instantiated with a
representative from the set, and then we apply (ad-
hoc) lexical substitution rules for creating a new
node for each other node in the set
5
.
Alignment sharing: Modifiers of aligned nodes
are shared (rather than copied) as follows. Given
a node n
L
in l aligned to a node n
R
in r, and an
outgoing d-edge d of n
L
which is not part of l, we
share d between n
L
and n
R
by adding n
R
to S
d
3
This is the case in our current implementation, which is
based on the coarse tag-set of Minipar (Lin, 1998).
4
With the following exceptions: variables that are the
only node in R (and hence are both the root and a leaf), and
variables with additional alignments (other than the implicit
alignment between their occurrences in L and R) are not con-
sidered dual-leaves.
5
Notice that these nodes, in addition to the usual align-
ment with their source nodes in l, share the same daughters
in r.
and setting rel
d
(n
R
) = rel
d
(n
L
). This is illus-
trated by the sharing of yesterday in Figure 2. We
also copy annotation features from n
L
to n
R
.
We note at this point that the instantiation of
variables that are not dual leaves (e.g. V in our
example) cannot be shared because they typically
have different modifiers at the two sides of the
rule. Yet, their modifiers which are not part of
the rule are shared through the alignment opera-
tion (recall that common variables are always con-
sidered aligned). Dual leaf variables, on the other
hand, might be shared, as described next, since the
rule doesn?t specify any modifiers for them.
Dual leaf variable sharing: This final step is
performed analogously to alignment sharing. Sup-
pose that a dual leaf variable X is matched in a
node v in l whose incoming d-edge is d. Then
we simply add the parent p of X in r to S
d
and
set rel
d
(p) to the relation between p and X (in
R). Since v itself is shared, its modifiers become
shared as well, implicitly implementing the align-
ment operation. The subtrees beautiful Mary and
John are shared this way for variablesN1 andN2.
Applying the rule in our example added only
a single node and linked it to four d-edges, com-
pared to duplicating the whole tree in explicit in-
ference.
3.3 Correctness
In this section we present two theorems, which
prove that the inference algorithm is a valid imple-
mentation of the inference formalism of Section 2.
Due to space limitations, the proofs themselves
are omitted, and instead we outline their general
scheme.
We first argue that performing any sequence of
rule applications over the set of initial trees results
in a compact forest:
Theorem 1: The compact inference process
generates a compact forest.
Proof scheme: We prove by induction on the
number of rule applications. Initialization gen-
erates a single-rooted cDAG, whose embedded
DAGs are all trees, as required. We then prove that
if applying a rule on a compact forest creates a cy-
cle or an embedded DAG that is not a tree, then
such a cycle or a non-tree DAG already existed
prior to rule application, in contradiction with the
inductive assumption. A crucial observation for
this proof is that for any path from a node u to a
node v that passes through r, where u and v are
1060
outside r, there is also an analogous path from u
to v that passes through l instead, QED.
Next, we argue that the inference process over a
compact forest is complete and sound, i.e., it gen-
erates the set of consequents derivable from a text
according to the inference formalism.
Theorem 2: Given a rule base R and a set of
initial trees T , a tree t is embedded in a compact
forest derivable from T by the compact inference
process? t is a consequent of T according to the
inference formalism.
Proof scheme: We first show completeness by
induction on the number of explicit rule applica-
tions. Let t
n+1
be a tree derived from a tree t
n
using the rule r
n
according to the inference for-
malism. The inductive assumption asserts that t
n
is embedded in some derivable compact forest F .
It is easy to verify that applying r
n
to F will yield
a compact forest F
?
in which t
n+1
is embedded.
Next, we show soundness by induction on the
number of rule applications over the compact for-
est. Let t
n+1
be a tree represented in some derived
compact forest F
n+1
. F
n+1
was derived from the
compact forest F
n
, using the rule r
n
. It can be
shown that F
n
represents a tree t
n
, such that ap-
plying r
n
on t
n
will yield t
n+1
according to the
formalism. The inductive assumption asserts that
t
n
is a consequent in the inference formalism and
therefore t
n+1
is a consequent as well, QED.
These two theorems guarantee that the compact
inference process is valid - i.e., it yields a compact
forest that represents the set of consequents deriv-
able from a given text by a given rule set.
3.4 Complexity
In this section we explain why compact inference
exponentially reduces the time and space com-
plexity in typical scenarios.
We consider a set of rule matches in a tree T
independent if their matched left-hand-sides (ex-
cluding dual-leaf variables) do not overlap in T ,
and their application over T can be chained in any
order. For example, the three rule matches pre-
sented in Figure 3 are independent.
Let us consider explicit inference first. Assume
we start with a single tree T with k independent
rules matched. Applying k rules will yield 2
k
trees, since any subset of the rules might be ap-
plied to T . Therefore, the time and space com-
plexity of applying k independent rule matches is
?(2
k
). Applying more rules on the newly derived
Compact Explicit Ratio
Time (msec) 61 24,184 396
Rule applications 12 123 10
Node count 69 5,901 86
Edge endpoints 141 11,552 82
Table 1: Compact vs. explicit inference, us-
ing generic rules. Results are averaged per text-
hypothesis pair.
consequents behaves in a similar manner.
Next, we examine compact inference. Apply-
ing a rule using compact inference adds the right-
hand-side of the rule and shares with it existing
d-edges. Since that the size of the right-hand-side
and the number of outgoing d-edges per node are
practically bounded by low constants, applying k
rules on a tree T yields a linear increase in the size
of the forest. Thus, the resulting size isO(|T |+k),
as we can see from Figure 3.
The time complexity of rule application is com-
posed of matching the rule in the forest and apply-
ing the matched rule. Applying a matched rule is
linear in its size. Matching a rule of size r in a
forest F takes O(|F|
r
) time even when perform-
ing an exhaustive search for matches in the forest.
Since r tends to be quite small and can be bounded
by a low constant, this already gives polynomial
time complexity. In practice, indexing the forest
nodes, as well as the typical low connectivity of
the forest, result in a very fast matching procedure,
as illustrated in the empirical evaluation, described
next.
4 Empirical Evaluation
This section reports empirical evaluation of the ef-
ficiency of compact inference, tested in the recog-
nizing textual entailment setting using the RTE-3
and RTE-4 datasets (Giampiccolo et al, 2007; Gi-
ampiccolo et al, 2009). These datasets consist of
(text, hypothesis) pairs, which need to be classi-
fied as entailing/non entailing. Our first experi-
ment shows, using a small rule set, that compact
inference outperforms explicit inference by orders
of magnitude (Section 4.1). The second experi-
ment shows that compact inference scales well to
a full-blown RTE setting with several large-scale
rule bases, where up to hundreds of rules are ap-
plied for a text (Section 4.2).
1061
4.1 Compact vs. Explicit Inference
To compare explicit and compact inference we
randomly sampled 100 pairs from the RTE-3 de-
velopment set, and parsed the text in each pair
using Minipar (Lin, 1998). We used a set of
manually-composed entailment rules for inference
over generic linguistic phenomena such as pas-
sive, conjunction, relative clause, apposition, pos-
sessives, and determiners, which contains a few
dozens of rules. To make a fair comparison, we
aimed to make the explicit inference implementa-
tion reasonably efficient, e.g. by preventing gen-
eration of the same tree by different permutations
of the same rule applications. Both configurations
perform rule application iteratively, until no new
matches are found. In each iteration we first find
all rule matches and then apply all matching rules.
We compare run time, number of rule applications,
and the overall generated size of nodes and edges,
where edge size is represented by the sum of its
endpoints.
The results are summarized in Table 1. As ex-
pected, the results show that compact inference is
by orders of magnitude more efficient than explicit
inference. To avoid memory overflow, inference
was terminated after reaching 100,000 nodes. 3
out of the 100 pairs reached that limit with explicit
inference, while the maximal node count for com-
pact inference was only 268. The number of rule
applications is reduced thanks to the sharing of
common subtrees in the compact forest, by which
a single rule application operates simultaneously
over a large number of embedded trees. The re-
sults suggest that scaling to larger rule bases and
longer inference chains would be feasible for com-
pact inference, but prohibitive for explicit infer-
ence.
4.2 Application to an RTE System
Experimental setting The goal of the second
experiment was to assess that compact inference
scales well for broad entailment rule bases. In
this experiment we used the Bar-Ilan RTE system
(Bar-Haim et al, 2009). The system operates in
two primary stages: Inference, in which entail-
ment rules are applied to the initial compact forest
F , aiming to bring it closer to the hypothesis H,
and Classification, in which a set of features is ex-
tracted from the resulting F and from H and fed
into an SVM classifier, which determines entail-
ment.
The classification setting and its features are
quite typical for the RTE literature. They include
lexical and structural measures for the coverage of
H by F , where high coverage is assumed to cor-
relate with entailment, as well as features aiming
to detect inconsistencies between F and H such
as incompatible arguments for the same predicate
or incompatible verb polarity (see below). For a
complete feature description, see (Bar-Haim et al,
2009).
Rule Bases In addition to the generic rules de-
scribed in Section 4.1, the following large-scale
sources for entailment rules were used: Wikipeda:
We used the lexical rulebase of Shnarch et al
(2009), who extracted rules such as ?Janis Joplin
? singer? from Wikipedia based on both its meta-
data (e.g. links and redirects) and text defini-
tions, using patterns such as ?X is a Y?. Word-
Net: We extracted from WordNet (Fellbaum,
1998) lexical rules based on synonyms, hyper-
nyms and derivation relations. DIRT: The DIRT
algorithm (Lin and Pantel, 2001) learns from a
corpus entailment rules between binary predicates,
e.g. ?X is fond of Y?X likes Y?. We used the
version described in (Szpektor and Dagan, 2007),
which learns canonical rule forms. Argument-
MappedWordNet (AmWN):A resource for entail-
ment rules between verbal and nominal predicates
(Szpektor and Dagan, 2009), including their argu-
ment mapping, based on WordNet and NomLex-
plus (Meyers et al, 2004), verified statistically
through intersection with the unary-DIRT algo-
rithm (Szpektor and Dagan, 2008). In total, these
rule bases represent millions of rules. Polarity An-
notation Rules: We compiled a small set of anno-
tation rules for marking the polarity of predicates
as negative or unknown due to verbal negation,
modal verbs, conditionals etc. (Bar-Haim et al,
2009).
Search In this work we focus on efficient rep-
resentation of the search space, leaving for future
work the complementary problem of devising ef-
fective search heuristics over our representation.
In the current experiment we implemented a sim-
ple search strategy, in the spirit of (de Salvo Braz
et al, 2005): first, we applied three exhaustive iter-
ations of generic rules. Since these rules have low
fan-out (few possible right-hand-sides for a given
left-hand-side) it is affordable to apply and chain
them more freely. We then perform a single itera-
tion of all other lexical and lexical-syntactic rules,
1062
applying them only if their L part was matched in
F and their R part was matched inH.
The system was trained over the RTE-3 devel-
opment set, and tested on both RTE-3 test set and
RTE-4 (which includes only a test set).
Results Table 2 provides statistics on rule appli-
cation using all rule bases, over the RTE-3 devel-
opment set and the RTE-4 dataset
6
. Overall, the
primary result is that the compact forest indeed ac-
commodates well extensive rule application from
large-scale rule bases. The resulting forest size is
kept small, even in the maximal cases which were
causing memory overflow for explicit inference.
The accuracies obtained in this experiment and
the overall contribution of rule-based inference are
shown in Table 3. The results on RTE-3 are quite
competitive: compared to our 66.4%, only 3 teams
out of the 26 who participated in RTE-3 scored
higher than 67%, and three more systems scored
between 66% and 67%. The results for RTE4 rank
9-10 out of 26, with only 6 teams scoring higher by
more than 1%. Overall, these results validate that
the setting of our experiment represents a state-of-
the-art system.
Inference over the rule bases utilized in our
experiment improved the accuracy on both test
sets. The contribution was more prominent for
the RTE-4 dataset. These results illustrate a typ-
ical contribution of current knowledge sources for
current RTE systems. This contribution is likely
to increase with current and near future research,
on topics such as extending and improving knowl-
edge resources, applying them only in seman-
tically suitable contexts, improved classification
features and broader search strategies. As for our
current experiment, we may conclude that the goal
of assessing the compact forest scalability in a
state-of-the-art setting was achieved
7
.
Finally, Tables 4 and 5 illustrate the usage and
contribution of individual rule bases. Table 4
shows the distribution of rule applications over the
various rule bases. Table 5 presents ablation study
showing the marginal accuracy gain for each rule
base. These results show that each of the rule
bases is applicable for a large portion of the pairs,
and contributes to the overall accuracy.
6
Running time is omitted since most of it was dedicated
to rule fetching, which was rather slow for our available im-
plementation of some resources. The elapsed time was a few
seconds per (t, h) pair.
7
We note that common RTE research issues, such as im-
proving accuracy, fall out of the scope of the current paper.
RTE3-Dev RTE4
Avg. Max. Avg. Max.
Rule applications 14 275 15 110
Node count 71 606 80 357
Edge endpoints 155 1,741 173 1,062
Table 2: Application of compact inference to the
RTE-3 Dev. and RTE-4 datasets, using all rule
types.
Accuracy
Test set No inference Inference ?
RTE3 64.6% 66.4% 1.8%
RTE4 57.5% 60.6% *3.1%
Table 3: Inference contribution to RTE perfor-
mance. The system was trained on the RTE-3 de-
velopment set. * indicates statistically significant
difference (at level p < 0.02, using McNemar?s
test).
Rule base RTE3-Dev RTE4
Rules App Rules App
WordNet 0.6 1.2 0.6 1.1
AmWN 0.3 0.4 0.3 0.4
Wikipedia 0.6 1.7 0.6 1.3
DIRT 0.5 0.7 0.5 1.0
Generic 4.7 10.4 5.4 11.5
Polarity 0.2 0.2 0.2 0.2
Table 4: Average number of rule applications per
(t, h) pair, for each rule base. App counts each rule
application, while Rules ignores multiple matches
of the same rule in the same iteration.
Rule base ?Accuracy (RTE4)
WordNet 0.8%
AmWN 0.7%
Wikipedia 1.0%
DIRT 0.9%
Generic 0.4%
Polarity 0.9%
Table 5: Contribution of various rule bases. Re-
sults show accuracy loss on RTE-4, obtained for
removing each rule base (ablation tests).
5 Related Work
This section discusses related work on applying
knowledge-based transformations within RTE sys-
tems, as well as on using packed representations in
other NLP tasks.
RTE Systems Previous RTE systems usually re-
stricted both the type of allowed transformations
and the search space. Systems based on lexical
(word-based or phrase-based) matching of h in t
typically applied only lexical rules (without vari-
1063
ables), where both sides of the rule are matched
directly in t and h (Haghighi et al, 2005; Mac-
Cartney et al, 2008). The inference formalism
we use is more expressive, allowing also syntac-
tic and lexical-syntactic transformations as well as
rule chaining.
Hickl (2008) derived from a given (t, h) pair
a small set of discourse commitments, which are
quite similar to the kind of consequents we derive
by our syntactic and lexical-syntactic rules. The
commitments were generated by several different
tools and techniques, compared to our generic uni-
fied inference process, and commitment genera-
tion efficiency was not discussed.
Braz et al (2005) presented a semantic infer-
ence framework which ?augments? the text repre-
sentation with only the right-hand-side of an ap-
plied rule, and in this respect is similar to ours.
However, in their work, both rule application and
the semantics of the resulting ?augmented? struc-
ture were not fully specified. In particular, the dis-
tinction between individual consequents was lost
in the augmented graph. By contrast, our com-
pact inference is fully formalized and is provably
equivalent to an expressive, well-defined formal-
ism operating over individual trees, and each in-
ferred consequent can be recovered from the com-
pact forest.
Packed representations Packed representations
in various NLP tasks share common principles,
which also underly our compact forest: factor-
ing out common substructures and representing
choice as local disjunctions. Applying this gen-
eral scheme to individual problems typically re-
quires specific representations and algorithms, de-
pending on the type of alternatives that should be
represented and the specified operations for creat-
ing them. In our work, alternatives are created by
rule application, where a newly derived subtree is
set as an alternative to existing subtrees. Alterna-
tives are specified locally using d-edges.
Packed chart representations for parse forests
were introduced in classical parsing algorithms
such as CYK and Earley (Jurafsky and Martin,
2008), and have been extended in later work
for various purposes (Maxwell III and Kaplan,
1991; Kay, 1996). Alternatives in the parse chart
stem from syntactic ambiguities, and are speci-
fied locally as the possible decompositions of each
phrase into its sub-phrases.
Packed representations have been utilized also
in transfer-based machine translation. Emele and
Dorna (1998) translated packed source language
representation to packed target language represen-
tation while avoiding unnecessary unpacking dur-
ing transfer. Unlike our rule application, in their
work transfer rules preserve ambiguity stemming
from source language, rather than generating new
alternatives. Mi et al(2008) applied statistical ma-
chine translation to a source language parse forest,
rather than to the 1-best parse. Their transfer rules
are tree-to-string, contrary to our tree-to-tree rules,
and chaining is not attempted (rules are applied in
a single top-down pass over the source forest), and
thus their representation and algorithms are quite
different from ours.
6 Conclusion
This work addresses the efficiency of entailment
and paraphrase rule application. We presented a
novel compact data structure and a rule application
algorithm for it, which are provably a valid imple-
mentation of a generic inference formalism. We il-
lustrated inference efficiency both analytically and
empirically. Beyond entailment inference, we sug-
gest that the compact forest would also be use-
ful in generation tasks (e.g. paraphrasing). Our
efficient representation of the consequent search
space opens the way to future investigation of the
benefit of larger-scale rule chaining, and to the de-
velopment of efficient search strategies required to
support such inferences.
Acknowledgments
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886, the
FBK-irst/Bar-Ilan University collaboration and
the Israel Science Foundation grant 1112/08. The
second author is grateful to the Azrieli Foundation
for the award of an Azrieli Fellowship. The au-
thors would like to thank Yonatan Aumann, Marco
Pennacchiotti and Marc Dymetman for their valu-
able feedback on this work.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
1064
Szpektor. 2009. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of the First Text Analysis Conference
(TAC 2008).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
Martin C. Emele and Michael Dorna. 1998. Ambi-
guity preserving machine translation using packed
representations. In Proceedings of Coling-ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2009. The
Fourth PASCAL Recognizing Textual Entailment
Challenge. In Proceedings of the First Text Analy-
sis Conference (TAC 2008).
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of EMNLP.
Andrew Hickl. 2008. Using discourse commitments
to recognize textual entailment. In Proceedings of
COLING.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition.
Martin Kay. 1996. Chart generation. In Proceedings
of ACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
John T. Maxwell III and Ronald M. Kaplan. 1991.
A method for disjunctive constraint satisfaction. In
Masaru Tomita, editor, Current Issues in Parsing
Technology. Kluwer Academic Publishers.
A. Meyers, R. Reeves, Catherine Macleod, Rachel
Szekeley, Veronkia Zielinska, and Brian Young.
2004. The cross-breeding of dictionaries. In Pro-
ceedings of LREC.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL-HLT.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of Hu-
man Language Technology Conference (HLT 2002),
San Diego, USA.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP.
Idan Szpektor and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
WordNet-based inference with argument mapping.
In Proceedings of ACL-IJCNLP Workshop on Ap-
plied Textual Inference (TextInfer).
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP.
1065
Proceedings of ACL-08: HLT, pages 683?691,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Contextual Preferences
Idan Szpektor, Ido Dagan, Roy Bar-Haim
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
{szpekti,dagan,barhair}@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat Gan, Israel
goldbej@eng.biu.ac.il
Abstract
The validity of semantic inferences depends
on the contexts in which they are applied.
We propose a generic framework for handling
contextual considerations within applied in-
ference, termed Contextual Preferences. This
framework defines the various context-aware
components needed for inference and their
relationships. Contextual preferences extend
and generalize previous notions, such as se-
lectional preferences, while experiments show
that the extended framework allows improving
inference quality on real application data.
1 Introduction
Applied semantic inference is typically concerned
with inferring a target meaning from a given text.
For example, to answer ?Who wrote Idomeneo??,
Question Answering (QA) systems need to infer the
target meaning ?Mozart wrote Idomeneo? from a
given text ?Mozart composed Idomeneo?. Following
common Textual Entailment terminology (Giampic-
colo et al, 2007), we denote the target meaning by h
(for hypothesis) and the given text by t.
A typical applied inference operation is matching.
Sometimes, h can be directly matched in t (in the
example above, if the given sentence would be liter-
ally ?Mozart wrote Idomeneo?). Generally, the tar-
get meaning can be expressed in t in many differ-
ent ways. Indirect matching is then needed, using
inference knowledge that may be captured through
rules, termed here entailment rules. In our exam-
ple, ?Mozart wrote Idomeneo? can be inferred using
the rule ?X compose Y ? X write Y ?. Recently,
several algorithms were proposed for automatically
learning entailment rules and paraphrases (viewed
as bi-directional entailment rules) (Lin and Pantel,
2001; Ravichandran and Hovy, 2002; Shinyama et
al., 2002; Szpektor et al, 2004; Sekine, 2005).
A common practice is to try matching the struc-
ture of h, or of the left-hand-side of a rule r, within
t. However, context should be considered to allow
valid matching. For example, suppose that to find
acquisitions of companies we specify the target tem-
plate hypothesis (a hypothesis with variables) ?X ac-
quire Y ?. This h should not be matched in ?children
acquire language quickly?, because in this context
Y is not a company. Similarly, the rule ?X charge
Y ? X accuse Y ? should not be applied to ?This
store charged my account?, since the assumed sense
of ?charge? in the rule is different than its sense in
the text. Thus, the intended contexts for h and r
and the context within the given t should be prop-
erly matched to verify valid inference.
Context matching at inference time was of-
ten approached in an application-specific manner
(Harabagiu et al, 2003; Patwardhan and Riloff,
2007). Recently, some generic methods were pro-
posed to handle context-sensitive inference (Dagan
et al, 2006; Pantel et al, 2007; Downey et al, 2007;
Connor and Roth, 2007), but these usually treat
only a single aspect of context matching (see Sec-
tion 6). We propose a comprehensive framework for
handling various contextual considerations, termed
Contextual Preferences. It extends and generalizes
previous work, defining the needed contextual com-
ponents and their relationships. We also present and
implement concrete representation models and un-
683
supervised matching methods for these components.
While our presentation focuses on semantic infer-
ence using lexical-syntactic structures, the proposed
framework and models seem suitable for other com-
mon types of representations as well.
We applied our models to a test set derived from
the ACE 2005 event detection task, a standard In-
formation Extraction (IE) benchmark. We show the
benefits of our extended framework for textual in-
ference and present component-wise analysis of the
results. To the best of our knowledge, these are also
the first unsupervised results for event argument ex-
traction in the ACE 2005 dataset.
2 Contextual Preferences
2.1 Notation
As mentioned above, we follow the generic Tex-
tual Entailment (TE) setting, testing whether a target
meaning hypothesis h can be inferred from a given
text t. We allow h to be either a text or a template,
a text fragment with variables. For example, ?The
stock rose 8%? entails an instantiation of the tem-
plate hypothesis ?X gain Y ?. Typically, h represents
an information need requested in some application,
such as a target predicate in IE.
In this paper, we focus on parse-based lexical-
syntactic representation of texts and hypotheses, and
on the basic inference operation of matching. Fol-
lowing common practice (de Salvo Braz et al, 2005;
Romano et al, 2006; Bar-Haim et al, 2007), h is
syntactically matched in t if it can be embedded in
t?s parse tree. For template hypotheses, the matching
induces a mapping between h?s variables and their
instantiation in t.
Matching h in t can be performed either directly
or indirectly using entailment rules. An entailment
rule r: ?LHS ? RHS? is a directional entailment
relation between two templates. h is matched in t us-
ing r if LHS is matched in t and h matches RHS.
In the example above, r: ?X rise Y ?X gain Y ? al-
lows us to entail ?X gain Y ?, with ?stock? and ?8%?
instantiating h?s variables. We denote vars(z) the
set of variables of z, where z is a template or a rule.
2.2 Motivation
When matching considers only the structure of hy-
potheses, texts and rules it may result in incorrect
inference due to contextual mismatches. For exam-
ple, an IE system may identify mentions of public
demonstrations using the hypothesis h: ?X demon-
strate?. However, h should not be matched in ?Engi-
neers demonstrated the new system?, due to a mis-
match between the intended sense of ?demonstrate?
in h and its sense in t. Similarly, when looking for
physical attack mentions using the hypothesis ?X at-
tack Y ?, we should not utilize the rule r: ?X accuse
Y ?X attack Y ?, due to a mismatch between a ver-
bal attack in r and an intended physical attack in h.
Finally, r: ?X produce Y ? X lay Y ? (applicable
when X refers to poultry and Y to eggs) should not
be matched in t: ?Bugatti produce the fastest cars?,
due to a mismatch between the meanings of ?pro-
duce? in r and t. Overall, such incorrect inferences
may be avoided by considering contextual informa-
tion for t, h and r during their matching process.
2.3 The Contextual Preferences Framework
We propose the Contextual Preferences (CP) frame-
work for addressing context at inference time. In this
framework, the representation of an object z, where
z may be a text, a template or an entailment rule, is
enriched with contextual information denoted cp(z).
This information helps constraining or disambiguat-
ing the meaning of z, and is used to validate proper
matching between pairs of objects.
We consider two components within cp(z): (a)
a representation for the global (?topical?) context
in which z typically occurs, denoted cpg(z); (b)
a representation for the preferences and constraints
(?hard? preferences) on the possible terms that can
instantiate variables within z, denoted cpv(z). For
example, cpv(?X produce Y ? X lay Y ?) may
specify that X?s instantiations should be similar to
?chicken? or ?duck?.
Contextual Preferences are used when entailment
is assessed between a text t and a hypothesis h, ei-
ther directly or by utilizing an entailment-rule r. On
top of structural matching, we now require that the
Contextual Preferences of the participants in the in-
ference will also match. When h is directly matched
in t, we require that each component in cp(h) will
be matched with its counterpart in cp(t). When r is
utilized, we additionally require that cp(r) will be
matched with both cp(t) and cp(h). Figure 1 sum-
marizes the matching relationships between the CP
684
Figure 1: The directional matching relationships between
a hypothesis (h), an entailment rule (r) and a text (t) in the
Contextual Preferences framework.
components of h, t and r.
Like Textual Entailment inference, Contextual
Preferences matching is directional. When matching
h with t we require that the global context prefer-
ences specified by cpg(h) would subsume those in-
duced by cpg(t), and that the instantiations of h?s
variables in t would adhere to the preferences in
cpv(h) (since t should entail h, but not necessarily
vice versa). For example, if the preferred global con-
text of a hypothesis is sports, it would match a text
that discusses the more specific topic of basketball.
To implement the CP framework, concrete models
are needed for each component, specifying its repre-
sentation, how it is constructed, and an appropriate
matching procedure. Section 3 describes the specific
CP models that were implemented in this paper.
The CP framework provides a generic view of
contextual modeling in applied semantic inference.
Mapping from a specific application to the generic
framework follows the mappings assumed in the
Textual Entailment paradigm. For example, in QA
the hypothesis to be proved corresponds to the affir-
mative template derived from the question (e.g. h:
?X invented the PC? for ?Who invented the PC??).
Thus, cpg(h) can be constructed with respect to
the question?s focus while cpv(h) may be gener-
ated from the expected answer type (Moldovan et
al., 2000; Harabagiu et al, 2003). Construction of
hypotheses? CP for IE is demonstrated in Section 4.
3 Contextual Preferences Models
This section presents the current models that we im-
plemented for the various components of the CP
framework. For each component type we describe
its representation, how it is constructed, and a cor-
responding unsupervised match score. Finally, the
different component scores are combined to yield
an overall match score, which is used in our exper-
iments to rank inference instances by the likelihood
of their validity. Our goal in this paper is to cover the
entire scope of the CP framework by including spe-
cific models that were proposed in previous work,
where available, and elsewhere propose initial mod-
els to complete the CP scope.
3.1 Contextual Preferences for Global Context
To represent the global context of an object z we
utilize Latent Semantic Analysis (LSA) (Deerwester
et al, 1990), a well-known method for representing
the contextual-usage of words based on corpus sta-
tistics. We use LSA analysis of the BNC corpus1,
in which every term is represented by a normalized
vector of the top 100 SVD dimensions, as described
in (Gliozzo, 2005).
To construct cpg(z) we first collect a set of terms
that are representative for the preferred general con-
text of z. Then, the (single) vector which is the sum
of the LSA vectors of the representative terms be-
comes the representation of cpg(z). This LSA vec-
tor captures the ?average? typical contexts in which
the representative terms occur.
The set of representative terms for a text t con-
sists of all the nouns and verbs in it, represented
by their lemma and part of speech. For a rule r:
?LHS ? RHS?, the representative terms are the
words appearing in LHS and in RHS. For exam-
ple, the representative terms for ?X divorce Y ? X
marry Y ? are {divorce:v, marry:v}. As mentioned
earlier, construction of hypotheses and their contex-
tual preferences depends on the application at hand.
In our experiments these are defined manually, as
described in Section 4, derived from the manual de-
finitions of target meanings in the IE data.
The score of matching the cpg components of two
objects, denoted by mg(?, ?), is the Cosine similarity
of their LSA vectors. Negative values are set to 0.
3.2 Contextual Preferences for Variables
3.2.1 Representation
For comparison with prior work, we follow (Pan-
tel et al, 2007) and represent preferences for vari-
1http://www.natcorp.ox.ac.uk/
685
able instantiations using a distributional approach,
and in addition incorporate a standard specification
of named-entity types. Thus, cpv is represented by
two lists. The first list, denoted cpv:e, contains ex-
amples for valid instantiations of that variable. For
example, cpv:e(X kill Y ? Y die of X) may be
[X: {snakebite, disease}, Y : {man, patient}]. The
second list, denoted cpv:n, contains the variable?s
preferred named-entity types (if any). For exam-
ple, cpv:n(X born in Y ) may be [X: {Person}, Y :
{Location}]. We denote cpv:e(z)[j] and cpv:n(z)[j]
as the lists for a specific variable j of the object z.
For a text t, in which a template p is matched, the
preference cpv:e(t) for each template variable is sim-
ply its instantiation in t. For example, when ?X eat
Y ? is matched in t: ?Many Americans eat fish reg-
ularly?, we construct cpv:e(t) = [X: {Many Ameri-
cans}, Y : {fish}]. Similarly, cpv:n(t) for each vari-
able is the named-entity type of its instantiation in
t (if it is a named entity). We identify entity types
using the default Lingpipe2 Named-Entity Recog-
nizer (NER), which recognizes the types Location,
Person and Organization. In the above example,
cpv:n(t)[X] would be {Person}.
For a rule r: LHS ? RHS, we automatically
add to cpv:e(r) all the variable instantiations that
were found common for both LHS and RHS in a
corpus (see Section 4), as in (Pantel et al, 2007; Pen-
nacchiotti et al, 2007). To construct cpv:n(r), we
currently use a simple approach where each individ-
ual term in cpv:e(r) is analyzed by the NER system,
and its type (if any) is added to cpv:n(r).
For a template hypothesis, we currently repre-
sent cpv(h) only by its list of preferred named-entity
types, cpv:n. Similarly to cpg(h), the preferred types
for each template variable were adapted from those
defined in our IE data (see Section 4).
To allow compatible comparisons with previous
work (see Sections 5 and 6), we utilize in this
paper only cpv:e when matching between cpv(r)
and cpv(t), as only this representation was exam-
ined in prior work on context-sensitive rule applica-
tions. cpv:n is utilized for context matches involving
cpv(h). We denote the score of matching two cpv
components by mv(?, ?).
2http://www.alias-i.com/lingpipe/
3.2.2 Matching cpv:e
Our primary matching method is based on repli-
cating the best-performing method reported in (Pan-
tel et al, 2007), which utilizes the CBC distribu-
tional word clustering algorithm (Pantel, 2003). In
short, this method extends each cpv:e list with CBC
clusters that contain at least one term in the list, scor-
ing them according to their ?relevancy?. The score
of matching two cpv:e lists, denoted here SCBC(?, ?),
is the score of the highest scoring member that ap-
pears in both lists.
We applied the final binary match score presented
in (Pantel et al, 2007), denoted here binaryCBC:
mv:e(r, t) is 1 if SCBC(r, t) is above a threshold and
0 otherwise. As a more natural ranking method, we
also utilize SCBC directly, denoted rankedCBC,
having mv:e(r, t) = SCBC(r, t).
In addition, we tried a simpler method that di-
rectly compares the terms in two cpv:e lists, uti-
lizing the commonly-used term similarity metric of
(Lin, 1998a). This method, denoted LIN , uses the
same raw distributional data as CBC but computes
only pair-wise similarities, without any clustering
phase. We calculated the scores of the 1000 most
similar terms for every term in the Reuters RVC1
corpus3. Then, a directional similarity of term a
to term b, s(a, b), is set to be their similarity score
if a is in b?s 1000 most similar terms and 0 other-
wise. The final score of matching r with t is deter-
mined by a nearest-neighbor approach, as the score
of the most similar pair of terms in the correspond-
ing two lists of the same variable: mv:e(r, t) =
maxj?vars(r)[maxa?cpv:e(t)[j],b?cpv:e(r)[j][s(a, b)]].
3.2.3 Matching cpv:n
We use a simple scoring mechanism for compar-
ing between two named-entity types a and b, s(a, b):
1 for identical types and 0.8 otherwise.
A variable j has a single preferred entity type
in cpv:n(t)[j], the type of its instantiation in t.
However, it can have several preferred types for h.
When matching h with t, j?s match score is that
of its highest scoring type, and the final score is
the product of all variable scores: mv:n(h, t) =?
j?vars(h)(maxa?cpv:n(h)[j][s(a, cpv:n(t)[j])]).
Variable j may also have several types in r, the
3http://about.reuters.com/researchandstandards/corpus/
686
types of the common arguments in cpv:e(r). When
matching h with r, s(a, cpv:n(t)[j]) is replaced with
the average score for a and each type in cpv:n(r)[j].
3.3 Overall Score for a Match
A final score for a given match, denoted allCP, is
obtained by the product of all six matching scores
of the various CP components (multiplying by 1
if a component score is missing). The six scores
are the results of matching any of the two compo-
nents of h, t and r: mg(h, t), mv(h, t), mg(h, r),
mv(h, r), mg(r, t) and mv(r, t) (as specified above,
mv(r, t) is based on matching cpv:e while mv(h, r)
and mv(h, t) are based on matching cpv:n). We use
rankedCBC for calculating mv(r, t).
Unlike previous work (e.g. (Pantel et al, 2007)),
we also utilize the prior score of a rule r, which
is provided by the rule-learning algorithm (see next
section). We denote by allCP+pr the final match
score obtained by the product of the allCP score
with the prior score of the matched rule.
4 Experimental Settings
Evaluating the contribution of Contextual Prefer-
ences models requires: (a) a sample of test hypothe-
ses, and (b) a corresponding corpus that contains
sentences which entail these hypotheses, where all
hypothesis matches (either direct or via rules) are an-
notated. We found that the available event mention
annotations in the ACE 2005 training set4 provide a
useful test set that meets these generic criteria, with
the added value of a standard real-world dataset.
The ACE annotation includes 33 types of events,
for which all event mentions are annotated in the
corpus. The annotation of each mention includes the
instantiated arguments for the predicates, which rep-
resent the participants in the event, as well as general
attributes such as time and place. ACE guidelines
specify for each event type its possible arguments,
where all arguments are optional. Each argument is
associated with a semantic role and a list of possible
named-entity types. For instance, an Injure event
may have the arguments {Agent, Victim, Instrument,
Time, Place}, where Victim should be a person.
For each event type we manually created a small
set of template hypotheses that correspond to the
4http://projects.ldc.upenn.edu/ace/
given event predicate, and specified the appropri-
ate semantic roles for each variable. We consid-
ered only binary hypotheses, due to the type of
available entailment rules (see below). For In-
jure, the set of hypotheses included ?A injure V?
and ?injure V in T? where role(A)={Agent, In-
strument}, role(V)={Victim}, and role(T)={Time,
Place}. Thus, correct match of an argument corre-
sponds to correct role identification. The templates
were represented as Minipar (Lin, 1998b) depen-
dency parse-trees.
The Contextual Preferences for h were con-
structed manually: the named-entity types for
cpv:n(h) were set by adapting the entity types given
in the guidelines to the types supported by the Ling-
pipe NER (described in Section 3.2). cpg(h) was
generated from a short list of nouns and verbs that
were extracted from the verbal event definition in
the ACE guidelines. For Injure, this list included
{injure:v, injury:n, wound:v}. This assumes that
when writing down an event definition the user
would also specify such representative keywords.
Entailment-rules for a given h (rules in which
RHS is equal to h) were learned automatically by
the DIRT algorithm (Lin and Pantel, 2001), which
also produces a quality score for each rule. We im-
plemented a canonized version of DIRT (Szpektor
and Dagan, 2007) on the Reuters corpus parsed by
Minipar. Each rule?s arguments for cpv(r) were also
collected from this corpus.
We assessed the CP framework by its ability to
correctly rank, for each predicate (event), all the
candidate entailing mentions that are found for it
in the test corpus. Such ranking evaluation is suit-
able for unsupervised settings, with a perfect rank-
ing placing all correct mentions before any incor-
rect ones. The candidate mentions are found in the
parsed test corpus by matching the specified event
hypotheses, either directly or via the given set of en-
tailment rules, using a syntactic matcher similar to
the one in (Szpektor and Dagan, 2007). Finally, the
mentions are ranked by their match scores, as de-
scribed in Section 3.3. As detailed in the next sec-
tion, those candidate mentions which are also an-
notated as mentions of the same event in ACE are
considered correct.
The evaluation aims to assess the correctness of
inferring a target semantic meaning, which is de-
687
noted by a specific predicate. Therefore, we elim-
inated four ACE event types that correspond to mul-
tiple distinct predicates. For instance, the Transfer-
Money event refers to both donating and lending
money, which are not distinguished by the ACE an-
notation. We also omitted three events with less than
10 mentions and two events for which the given set
of learned rules could not match any mention. We
were left with 24 event types for evaluation, which
amount to 4085 event mentions in the dataset. Out of
these, our binary templates can correctly match only
mentions with at least two arguments, which appear
2076 times in the dataset.
Comparing with previous evaluation methodolo-
gies, in (Szpektor et al, 2007; Pantel et al, 2007)
proper context matching was evaluated by post-hoc
judgment of a sample of rule applications for a sam-
ple of rules. Such annotation needs to be repeated
each time the set of rules is changed. In addition,
since the corpus annotation is not exhaustive, re-
call could not be computed. By contrast, we use a
standard real-world dataset, in which all mentions
are annotated. This allows immediate comparison
of different rule sets and matching methods, without
requiring any additional (post-hoc) annotation.
5 Results and Analysis
We experimented with three rule setups over the
ACE dataset, in order to measure the contribution
of the CP framework. In the first setup no rules are
used, applying only direct matches of template hy-
potheses to identify event mentions. In the other two
setups we also utilized DIRT?s top 50 or 100 rules
for each hypothesis.
A match is considered correct when all matched
arguments are extracted correctly according to their
annotated event roles. This main measurement is de-
noted All. As an additional measurement, denoted
Any, we consider a match as correct if at least one
argument is extracted correctly.
Once event matches are extracted, we first mea-
sure for each event its Recall, the number of correct
mentions identified out of all annotated event men-
tions5 and Precision, the number of correct matches
out of all extracted candidate matches. These figures
5For Recall, we ignored mentions with less than two argu-
ments, as they cannot be correctly matched by binary templates.
quantify the baseline performance of the DIRT rule
set used. To assess our ranking quality, we measure
for each event the commonly used Average Preci-
sion (AP) measure (Voorhees and Harmann, 1998),
which is the area under the non-interpolated recall-
precision curve, while considering for each setup all
correct extracted matches as 100% Recall. Overall,
we report Mean Average Precision (MAP), macro
average Precision and macro average Recall over the
ACE events. Tables 1 and 2 summarize the main re-
sults of our experiments. As far as we know, these
are the first published unsupervised results for iden-
tifying event arguments in the ACE 2005 dataset.
Examining Recall, we see that it increases sub-
stantially when rules are applied: by more than
100% for the top 50 rules, and by about 150% for
the top 100, showing the benefit of entailment-rules
to covering language variability. The difference be-
tween All and Any results shows that about 65%
of the rules that correctly match one argument also
match correctly both arguments.
We use two baselines for measuring the CP rank-
ing contribution: Precision, which corresponds to
the expected MAP of random ranking, and MAP
of ranking using the prior rule score provided by
DIRT. Without rules, the baseline All Precision is
34.1%, showing that even the manually constructed
hypotheses, which correspond directly to the event
predicate, extract event mentions with limited accu-
racy when context is ignored. When rules are ap-
plied, Precision is very low. But ranking is consider-
ably improved using only the prior score (from 1.4%
to 22.7% for 50 rules), showing that the prior is an
informative indicator for valid matches.
Our main result is that the allCP and allCP+pr
methods rank matches statistically significantly bet-
ter than the baselines in all setups (according to the
Wilcoxon double-sided signed-ranks test at the level
of 0.01 (Wilcoxon, 1945)). In the All setup, ranking
is improved by 70% for direct matching (Table 1).
When entailment-rules are also utilized, prior-only
ranking is improved by about 35% and 50% when
using allCP and allCP+pr, respectively (Table 2).
Figure 2 presents the average Recall-Precision curve
of the ?50 Rules, All? setup for applying allCP or
allCP+pr, compared to prior-only ranking baseline
(other setups behave similarly). The improvement
in ranking is evident: the drop in precision is signif-
688
R P MAP (%)
(%) (%) cpv cpg allCP
All 14.0 34.1 46.5 52.2 60.2
Any 21.8 66.0 72.2 80.5 84.1
Table 1: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when only matching template hypotheses
directly.
# R P MAP (%)
Rules (%) (%) prior allCP allCP+pr
All 50 29.6 1.4 22.7 30.6 34.1100 34.9 0.7 20.5 26.3 30.2
Any 50 46.5 3.5 41.2 43.7 48.6100 52.9 1.8 35.5 35.1 40.8
Table 2: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when also using rules for matching.
icantly slower when CP is used. The behavior of CP
with and without the prior is largely the same up to
50% Recall, but later on our implemented CP mod-
els are noisier and should be combined with the prior
rule score.
Templates are incorrectly matched for several rea-
sons. First, there are context mismatches which are
not scored sufficiently low by our models. Another
main cause is incorrect learned rules in which LHS
and RHS are topically related, e.g. ?X convict Y ?
X arrest Y ?, or rules that are used in the wrong en-
tailment direction, e.g. ?X marry Y ?X divorce Y ?
(DIRT does not learn rule direction). As such rules
do correspond to plausible contexts of the hypothe-
sis, their matches obtain relatively high CP scores.
In addition, some incorrect matches are caused by
our syntactic matcher, which currently does not han-
dle certain phenomena such as co-reference, modal-
ity or negation, and due to Minipar parse errors.
5.1 Component Analysis
Table 3 displays the contribution of different CP
components to ranking, when adding only that com-
ponent?s match score to the baselines, and under ab-
lation tests, when using all CP component scores ex-
cept the tested component, with or without the prior.
As it turns out, matching h with t (i.e. cp(h, t),
which combines cpg(h, t) and cpv(h, t)) is most use-
ful. With our current models, using only cp(h, t)
along with the prior, while ignoring cp(r), achieves
50 Rules  -  All
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100Relative Recall
Prec
ision
baseline CP CP + prior
Figure 2: Recall-Precision curves for ranking using: (a)
only the prior (baseline); (b) allCP; (c) allCP+pr.
the highest score in the table. The strong impact of
matching h and t?s preferences is also evident in Ta-
ble 1, where ranking based on either cpg or cpv sub-
stantially improves precision, while their combina-
tion provides the best ranking. These results indicate
that the two CP components capture complementary
information and both are needed to assess the cor-
rectness of a match.
When ignoring the prior rule score, cp(r, t) is the
major contributor over the baseline Precision. For
cpv(r, t), this is in synch with the result in (Pantel
et al, 2007), which is based on this single model
without utilizing prior rule scores. On the other
hand, cpv(r, t) does not improve the ranking when
the prior is used, suggesting that this contextual
model for the rule?s variables is not stronger than the
context-insensitive prior rule score. Furthermore,
relative to this cpv(r, t) model from (Pantel et al,
2007), our combined allCP model, with or without
the prior (first row of Table 2), obtains statistically
significantly better ranking (at the level of 0.01).
Comparing between the algorithms for match-
ing cpv:e (Section 3.2.2) we found that while
rankedCBC is statistically significantly better than
binaryCBC, rankedCBC and LIN generally
achieve the same results. When considering the
tradeoffs between the two, LIN is based on a much
simpler learning algorithm while CBC?s output is
more compact and allows faster CP matches.
689
Addition To Ablation From
P prior allCP allCP+pr
Baseline 1.4 22.7 30.6 34.1
cpg(h, t) ?10.4 ?35.4 32.4 33.7
cpv(h, t) ?11.0 29.9 27.6 32.9
cp(h, t) ?8.9 ?37.5 28.6 30.0
cpg(r, t) ?4.2 ?30.6 32.5 35.4
cpv(r, t) ?21.7 21.9 ?12.9 33.6
cp(r, t) ?26.0 ?29.6 ?17.9 36.8
cpg(h, r) ?8.1 22.4 31.9 34.3
cpv(h, r) ?10.7 22.7 ?27.9 34.4
cp(h, r) ?16.5 22.4 ?29.2 34.4
cpg(h, r, t) ?7.7 ?30.2 ?27.5 ?29.2
cpv(h, r, t) ?27.5 29.2 ?7.7 30.2
? Indicates statistically significant changes compared to the baseline,
according to the Wilcoxon test at the level of 0.01.
Table 3: MAP(%), under the ?50 rules, All? setup, when
adding component match scores to Precision (P) or prior-
only MAP baselines, and when ranking with allCP or
allCP+pr methods but ignoring that component scores.
Currently, some models do not improve the re-
sults when the prior is used. Yet, we would like to
further weaken the dependency on the prior score,
since it is biased towards frequent contexts. We
aim to properly identify also infrequent contexts (or
meanings) at inference time, which may be achieved
by better CP models. More generally, when used
on top of all other components, some of the mod-
els slightly degrade performance, as can be seen by
those figures in the ablation tests which are higher
than the corresponding baseline. However, due to
their different roles, each of the matching compo-
nents might capture some unique preferences. For
example, cp(h, r) should be useful to filter out rules
that don?t match the intended meaning of the given
h. Overall, this suggests that future research for bet-
ter models should aim to obtain a marginal improve-
ment by each component.
6 Related Work
Context sensitive inference was mainly investigated
in an application-dependent manner. For exam-
ple, (Harabagiu et al, 2003) describe techniques for
identifying the question focus and the answer type in
QA. (Patwardhan and Riloff, 2007) propose a super-
vised approach for IE, in which relevant text regions
for a target relation are identified prior to applying
extraction rules.
Recently, the need for context-aware inference
was raised (Szpektor et al, 2007). (Pantel et al,
2007) propose to learn the preferred instantiations of
rule variables, termed Inferential Selectional Prefer-
ences (ISP). Their clustering-based model is the one
we implemented for mv(r, t). A similar approach
is taken in (Pennacchiotti et al, 2007), where LSA
similarity is used to compare between the preferred
variable instantiations for a rule and their instanti-
ations in the matched text. (Downey et al, 2007)
use HMM-based similarity for the same purpose.
All these methods are analogous to matching cpv(r)
with cpv(t) in the CP framework.
(Dagan et al, 2006; Connor and Roth, 2007) pro-
posed generic approaches for identifying valid appli-
cations of lexical rules by classifying the surround-
ing global context of a word as valid or not for that
rule. These approaches are analogous to matching
cpg(r) with cpg(t) in our framework.
7 Conclusions
We presented the Contextual Preferences (CP)
framework for assessing the validity of inferences
in context. CP enriches the representation of tex-
tual objects with typical contextual information that
constrains or disambiguates their meaning, and pro-
vides matching functions that compare the prefer-
ences of objects involved in the inference. Experi-
ments with our implemented CP models, over real-
world IE data, show significant improvements rela-
tive to baselines and some previous work.
In future research we plan to investigate improved
models for representing and matching CP, and to ex-
tend the experiments to additional applied datasets.
We also plan to apply the framework to lexical infer-
ence rules, for which it seems directly applicable.
Acknowledgements
The authors would like to thank Alfio Massimiliano
Gliozzo for valuable discussions. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
the NEGEV project (www.negev-initiative.org) and
the FBK-irst/Bar-Ilan University collaboration.
690
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of the European Conference on Machine
Learning (ECML).
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of ACL.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of ACL.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis. Advisor-
Carlo Strapparava.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain textual question answer-
ing techniques. Nat. Lang. Eng., 9(3):231?267.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and
Vasile Rus. 2000. The structure and performance of
an open-domain question answering system. In Pro-
ceedings of the 38th Annual Meeting of ACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference of
NAACL; Proceedings of the Main Conference.
Patrick Andre Pantel. 2003. Clustering by committee.
Ph.D. thesis. Advisor-Dekang Lin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning selectional prefer-
ences for entailment or paraphrasing rules. In Pro-
ceedings of RANLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of the 11th Conference of the
EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference.
Idan Szpektor and Ido Dagan. 2007. Learning canonical
forms of entailment rules. In Proceedings of RANLP.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP 2004,
pages 41?48, Barcelona, Spain.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
ACL.
Ellen M. Voorhees and Donna Harmann. 1998.
Overview of the seventh text retrieval conference
(trec?7). In The Seventh Text Retrieval Conference.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
691
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 39?46,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Choosing an Optimal Architecture for Segmentation and POS-Tagging of
Modern Hebrew
Roy Bar-Haim
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
barhair@cs.biu.ac.il
Khalil Sima?an
ILLC
Universiteit van Amsterdam
Amsterdam, The Netherlands
simaan@science.uva.nl
Yoad Winter
Dept. of Computer Science
Technion
Haifa 32000, Israel
winter@cs.technion.ac.il
Abstract
A major architectural decision in de-
signing a disambiguation model for seg-
mentation and Part-of-Speech (POS) tag-
ging in Semitic languages concerns the
choice of the input-output terminal sym-
bols over which the probability distribu-
tions are defined. In this paper we de-
velop a segmenter and a tagger for He-
brew based on Hidden Markov Models
(HMMs). We start out from a morpholog-
ical analyzer and a very small morpholog-
ically annotated corpus. We show that a
model whose terminal symbols are word
segments (=morphemes), is advantageous
over a word-level model for the task of
POS tagging. However, for segmentation
alone, the morpheme-level model has no
significant advantage over the word-level
model. Error analysis shows that both
models are not adequate for resolving a
common type of segmentation ambiguity
in Hebrew ? whether or not a word in a
written text is prefixed by a definiteness
marker. Hence, we propose a morpheme-
level model where the definiteness mor-
pheme is treated as a possible feature of
morpheme terminals. This model exhibits
the best overall performance, both in POS
tagging and in segmentation. Despite the
small size of the annotated corpus avail-
able for Hebrew, the results achieved us-
ing our best model are on par with recent
results on Modern Standard Arabic.
1 Introduction
Texts in Semitic languages like Modern Hebrew
(henceforth Hebrew) and Modern Standard Ara-
bic (henceforth Arabic), are based on writing sys-
tems that allow the concatenation of different lexi-
cal units, called morphemes. Morphemes may be-
long to various Part-of-Speech (POS) classes, and
their concatenation forms textual units delimited by
white space, which are commonly referred to as
words. Hence, the task of POS tagging for Semitic
languages consists of a segmentation subtask and
a classification subtask. Crucially, words can be
segmented into different alternative morpheme se-
quences, where in each segmentation morphemes
may be ambiguous in terms of their POS tag. This
results in a high level of overall ambiguity, aggra-
vated by the lack of vocalization in modern Semitic
texts.
One crucial problem concerning POS tagging of
Semitic languages is how to adapt existing methods
in the best way, and which architectural choices have
to be made in light of the limited availability of an-
notated corpora (especially for Hebrew). This paper
outlines some alternative architectures for POS tag-
ging of Hebrew text, and studies them empirically.
This leads to some general conclusions about the op-
timal architecture for disambiguating Hebrew, and
(reasonably) other Semitic languages as well. The
choice of tokenization level has major consequences
for the implementation using HMMs, the sparseness
of the statistics, the balance of the Markov condi-
39
tioning, and the possible loss of information. The
paper reports on extensive experiments for compar-
ing different architectures and studying the effects
of this choice on the overall result. Our best result
is on par with the best reported POS tagging results
for Arabic, despite the much smaller size of our an-
notated corpus.
The paper is structured as follows. Section 2 de-
fines the task of POS tagging in Hebrew, describes
the existing corpora and discusses existing related
work. Section 3 concentrates on defining the dif-
ferent levels of tokenization, specifies the details of
the probabilistic framework that the tagger employs,
and describes the techniques used for smoothing the
probability estimates. Section 4 compares the differ-
ent levels of tokenization empirically, discusses their
limitations, and proposes an improved model, which
outperforms both of the initial models. Finally, sec-
tion 5 discusses the conclusions of our study for seg-
mentation and POS tagging of Hebrew in particular,
and Semitic languages in general.
2 Task definition, corpora and related
work
Words in Hebrew texts, similar to words in Ara-
bic and other Semitic languages, consist of a stem
and optional prefixes and suffixes. Prefixes include
conjunctions, prepositions, complementizers and the
definiteness marker (in a strict well-defined order).
Suffixes include inflectional suffixes (denoting gen-
der, number, person and tense), pronominal comple-
ments with verbs and prepositions, and possessive
pronouns with nouns.
By the term word segmentation we henceforth re-
fer to identifying the prefixes, the stem and suffixes
of the word. By POS tag disambiguation we mean
the assignment of a proper POS tag to each of these
morphemes.
In defining the task of segmentation and POS tag-
ging, we ignore part of the information that is usu-
ally found in Hebrew morphological analyses. The
internal morphological structure of stems is not an-
alyzed, and the POS tag assigned to stems includes
no information about their root, template/pattern, in-
flectional features and suffixes. Only pronominal
complement suffixes on verbs and prepositions are
identified as separate morphemes. The construct
state/absolute,1 and the existence of a possessive
suffix are identified using the POS tag assigned to
the stem, and not as a separate segment or feature.
Some of these conventions are illustrated by the seg-
mentation and POS tagging of the word wfnpgfnw
(?and that we met?, pronounced ve-she-nifgashnu):2
w/CC: conjunction
f /COM: complementizer
npgfnw/VB: verb
Our segmentation and POS tagging conform with
the annotation scheme used in the Hebrew Treebank
(Sima?an et al, 2001), described next.
2.1 Available corpora
The Hebrew Treebank (Sima?an et al, 2001) con-
sists of syntactically annotated sentences taken from
articles from the Ha?aretz daily newspaper. We ex-
tracted from the treebank a mapping from each word
to its analysis as a sequence of POS tagged mor-
phemes. The treebank version used in the current
work contains 57 articles, which amount to 1,892
sentences, 35,848 words, and 48,332 morphemes.
In addition to the manually tagged corpus, we have
access to an untagged corpus containing 337,651
words, also originating from Ha?aretz newspaper.
The tag set, containing 28 categories, was ob-
tained from the full morphological tagging by re-
moving the gender, number, person and tense fea-
tures. This tag set was used for training the POS
tagger. In the evaluation of the results, however, we
perform a further grouping of some POS tags, lead-
ing to a reduced POS tag set of 21 categories. The
tag set and the grouping scheme are shown below:
{NN}, {NN-H}, {NNT}, {NNP}, {PRP,AGR}, {JJ}, {JJT},
{RB,MOD}, {RBR}, {VB,AUX}, {VB-M}, {IN,COM,REL},
{CC}, {QW}, {HAM}, {WDT,DT}, {CD,CDT}, {AT}, {H},
{POS}, {ZVL}.
2.2 Related work on Hebrew and Arabic
Due to the lack of substantial tagged corpora, most
previous corpus-based work on Hebrew focus on the
1The Semitic construct state is a special form of a word
that participates in compounds. For instance, in the Hebrew
compound bdiqt hjenh (?check of the claim?), the word bdiqt
(?check of?/?test of?) is the construct form of the absolute form
bdiqh (?check?/?test?).
2In this paper we use Latin transliteration for Hebrew letters
following (Sima?an et al, 2001).
40
development of techniques for learning probabilities
from large unannotated corpora. The candidate anal-
yses for each word were usually obtained from a
morphological analyzer.
Levinger et al (1995) propose a method for
choosing a most probable analysis for Hebrew
words using an unannotated corpus, where each
analysis consists of the lemma and a set of morpho-
logical features. They estimate the relative frequen-
cies of the possible analyses for a given word w by
defining a set of ?similar words? SW (A) for each
possible analysis A of w. Each word w? in SW (A)
corresponds to an analysis A? which differs from A
in exactly one feature. Since each set is expected to
contain different words, it is possible to approximate
the frequency of the different analyses using the av-
erage frequency of the words in each set, estimated
from the untagged corpus.
Carmel and Maarek (1999) follow Levinger et
al. in estimating context independent probabilities
from an untagged corpus. Their algorithm learns fre-
quencies of morphological patterns (combinations
of morphological features) from the unambiguous
words in the corpus.
Several works aimed at improving the ?similar
words? method by considering the context of the
word. Levinger (1992) adds a short context filter that
enforces grammatical constraints and rules out im-
possible analyses. Segal?s (2000) system includes,
in addition to a somewhat different implementation
of ?similar words?, two additional components: cor-
rection rules a` la Brill (1995), and a rudimentary de-
terministic syntactic parser.
Using HMMs for POS tagging and segmenting
Hebrew was previously discussed in (Adler, 2001).
The HMM in Adler?s work is trained on an untagged
corpus, using the Baum-Welch algorithm (Baum,
1972). Adler suggests various methods for perform-
ing both tagging and segmentation, most notable are
(a) The usage of word-level tags, which uniquely de-
termine the segmentation and the tag of each mor-
pheme, and (b) The usage of a two-dimensional
Markov model with morpheme-level tags. Only the
first method (word-level tags) was tested, resulting
in an accuracy of 82%. In the present paper, both
word-level tagging and morpheme-level tagging are
evaluated.
Moving on to Arabic, Lee et al (2003) describe a
word segmentation system for Arabic that uses an n-
gram language model over morphemes. They start
with a seed segmenter, based on a language model
and a stem vocabulary derived from a manually seg-
mented corpus. The seed segmenter is improved it-
eratively by applying a bootstrapping scheme to a
large unsegmented corpus. Their system achieves
accuracy of 97.1% (per word).
Diab et al (2004) use Support Vector Machines
(SVMs) for the tasks of word segmentation and POS
tagging (and also Base Phrase Chunking). For seg-
mentation, they report precision of 99.09% and re-
call of 99.15%, when measuring morphemes that
were correctly identified. For tagging, Diab et al
report accuracy of 95.49%, with a tag set of 24 POS
tags. Tagging was applied to segmented words, us-
ing the ?gold? segmentation from the annotated cor-
pus (Mona Diab, p.c.).
3 Architectures for POS tagging Semitic
languages
Our segmentation and POS tagging system consists
of a morphological analyzer that assigns a set of
possible candidate analyses to each word, and a dis-
ambiguator that selects from this set a single pre-
ferred analysis per word. Each candidate analysis
consists of a segmentation of the word into mor-
phemes, and a POS tag assignment to these mor-
phemes. In this section we concentrate on the ar-
chitectural decisions in devising an optimal disam-
biguator, given a morphological analyzer for He-
brew (or another Semitic language).
3.1 Defining the input/output
An initial crucial decision in building a disambigua-
tor for a Semitic text concerns the ?tokenization? of
the input sentence: what constitutes a terminal (i.e.,
input) symbol. Unlike English POS tagging, where
the terminals are usually assumed to be words (de-
limited by white spaces), in Semitic texts there are
two reasonable options for fixing the kind of termi-
nal symbols, which directly define the correspond-
ing kind of nonterminal (i.e., output) symbols:
Words (W): The terminals are words as they ap-
pear in the text. In this case a nonterminal a
that is assigned to a word w consists of a se-
quence of POS tags, each assigned to a mor-
41
pheme of w, delimited with a special segmenta-
tion symbol. We henceforth refer to such com-
plex nonterminals as analyses. For instance,
the analysis IN-H-NN for the Hebrew word
bbit uniquely encodes the segmentation b-h-bit.
In Hebrew, this unique encoding of the segmen-
tation by the sequence of POS tags in the anal-
ysis is a general property: given a word w and
a complex nonterminal a = [t1 . . . tp] for w, it
is possible to extend a back to a full analysis
a? = [(m1, t1) . . . (mp, tp)], which includes the
morphemes m1 . . .mp that make out w. This is
done by finding a match for a in Analyses(w),
the set of possible analyses of w. Except for
very rare cases, this match is unique.
Morphemes (M): In this case the nonterminals are
the usual POS tags, and the segmentation is
given by the input morpheme sequence. Note
that information about how morphemes are
joined into words is lost in this case.
Having described the main input-output options for
the disambiguator, we move on to describing the
probabilistic framework that underlies their work-
ings.
3.2 The probabilistic framework
Let wk1 be the input sentence, a sequence of words
w1 . . . wk. If tokenization is per word, then the
disambiguator aims at finding the nonterminal se-
quence ak1 that has the highest joint probability with
the given sentence wk1 :
argmax
ak1
P (wk1 ,a
k
1) (1)
This setting is the standard formulation of proba-
bilistic tagging for languages like English.
If tokenization is per morpheme, the disambigua-
tor aims at finding a combination of a segmentation
mn1 and a tagging tn1 for mn1 , such that their joint
probability with the given sentence, wk1 , is maxi-
mized:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (wk1 ,m
n
1 , t
n
1 ), (2)
where ANALY SES(wk1) is the set of possible
analyses for the input sentence wk1 (output by the
morphological analyzer). Note that n can be dif-
ferent from k, and may vary for different segmen-
tations. The original sentence can be uniquely re-
covered from the segmentation and the tagging.
Since all the ?mn1 , tn1 ? pairs that are the input for
the disambiguator were derived from wk1 , we have
P (wk1 |m
n
1 , t
n
1 ) = 1, and thus P (wk1 ,mn1 , tn1 ) =
P (tn1 ,m
n
1 ). Therefore, Formula (2) can be simpli-
fied as:
argmax
(mn1 ,t
n
1 )?ANALY SES(w
k
1 )
P (mn1 , t
n
1 ) (3)
Formulas (1) and (3) can be represented in a unified
formula that applies to both word tokenization and
morpheme tokenization:
argmax
(en1 ,A
n
1 )?ANALY SES(w
k
1 )
P (en1 , A
n
1 ) (4)
In Formula (4) en1 represents either a sequence of
words or a sequence of morphemes, depending on
the level of tokenization, and An1 are the respective
nonterminals ? either POS tags or word-level anal-
yses. Thus, the disambiguator aims at finding the
most probable ?terminal sequence, nonterminal
sequence? for the given sentence, where in the
case of word-tokenization there is only one possible
terminal sequence for the sentence.
3.3 HMM probabilistic model
The actual probabilistic model used in this work for
estimating P (en1 , An1 ) is based on Hidden Markov
Models (HMMs). HMMs underly many successful
POS taggers , e.g. (Church, 1988; Charniak et al,
1993).
For a k-th order Markov model (k = 1 or k = 2),
we rewrite (4) as:
argmax
en1 ,A
n
1
P (en1 , A
n
1 ) ?
argmax
en1 ,A
n
1
n?
i=1
P (Ai | Ai?k, . . . , Ai?1)P (ei | Ai)
(5)
For reasons of data sparseness, actual models we use
work with k = 2 for the morpheme level tokeniza-
tion, and with k = 1 for the word level tokenization.
42
For these models, two kinds of probabilities need
to be estimated: P (ei | Ai) (lexical model) and
P (Ai |Ai?k, . . . , Ai?1) (language model). Because
the only manually POS tagged corpus that was avail-
able to us for training the HMM was relatively small
(less than 4% of the Wall Street Journal (WSJ) por-
tion of the Penn treebank), it is inevitable that major
effort must be dedicated to alleviating the sparseness
problems that arise. For smoothing the nonterminal
language model probabilities we employ the stan-
dard backoff smoothing method of Katz (1987).
Naturally, the relative frequency estimates of
the lexical model suffer from more severe data-
sparseness than the estimates for the language
model. On average, 31.3% of the test words do
not appear in the training corpus. Our smooth-
ing method for the lexical probabilities is described
next.
3.4 Bootstrapping a better lexical model
For the sake of exposition, we assume word-level
tokenization for the rest of this subsection. The
method used for the morpheme-level tagger is very
similar.
The smoothing of the lexical probability of a word
w given an analysis a, i.e., P (w | a) = P (w,a)P (a) ,
is accomplished by smoothing the joint probability
P (w,a) only, i.e., we do not smooth P (a).3 To
smooth P (w,a), we use a linear interpolation of
the relative frequency estimates from the annotated
training corpus (denoted rf tr(w,a)) together with
estimates obtained by unsupervised estimation from
a large unannotated corpus (denoted emauto(w,a)):
P (w,a) = ? rf tr(w,a)+(1??) emauto(w,a)
(6)
where ? is an interpolation factor, experimentally set
to 0.85.
Our unsupervised estimation method can be
viewed as a single iteration of the Baum-Welch
(Forward-Backward) estimation algorithm (Baum,
1972) with minor differences. We apply this method
to the untagged corpus of 340K words. Our method
starts out from a naively smoothed relative fre-
3the smoothed probabilities are normalized so that?
w P (w,a) = P (a)
quency lexical model in our POS tagger:
PLM0(w|a) =
{
(1 ? p0) rf tr(w,a) ftr(w) > 0
p0 otherwise
(7)
Where ftr(w) is the occurrence frequency of w in
the training corpus, and p0 is a constant set experi-
mentally to 10?10. We denote the tagger that em-
ploys a smoothed language model and the lexical
model PLM0 by the probability distribution Pbasic
(over analyses, i.e., morpheme-tag sequences).
In the unsupervised algorithm, the model Pbasic
is used to induce a distribution of alternative analy-
ses (morpheme-tag sequences) for each of the sen-
tences in the untagged corpus; we limit the num-
ber of alternative analyses per sentence to 300. This
way we transform the untagged corpus into a ?cor-
pus? containing weighted analyses (i.e., morpheme-
tag sequences). This corpus is then used to calcu-
late the updated lexical model probabilities using
maximum-likelihood estimation. Adding the test
sentences to the untagged corpus ensures non-zero
probabilities for the test words.
3.5 Implementation4
The set of candidate analyses was obtained from Se-
gal?s morphological analyzer (Segal, 2000). The
analyzer?s dictionary contains 17,544 base forms
that can be inflected. After this dictionary was ex-
tended with the tagged training corpus, it recog-
nizes 96.14% of the words in the test set.5 For each
train/test split of the corpus, we only use the training
data for enhancing the dictionary. We used SRILM
(Stolcke, 2002) for constructing language models,
and for disambiguation.
4 Evaluation
In this section we report on an empirical comparison
between the two levels of tokenization presented in
the previous section. Analysis of the results leads to
an improved morpheme-level model, which outper-
forms both of the initial models.
Each architectural configuration was evaluated in
5-fold cross-validated experiments. In a train/test
4http://www.cs.technion.ac.il/?barhaim/MorphTagger/
5Unrecognized words are assumed to be proper nouns, and
the morphological analyzer proposes possible segmentations for
the word, based on the recognition of possible prefixes.
43
split of the corpus, the training set includes 1,598
sentences on average, which on average amount to
28,738 words and 39,282 morphemes. The test set
includes 250 sentences. We estimate segmentation
accuracy ? the percentage of words correctly seg-
mented into morphemes, as well as tagging accu-
racy ? the percentage of words that were correctly
segmented for which each morpheme was assigned
the correct POS tag.
For each parameter, the average over the five folds
is reported, with the standard deviation in parenthe-
ses. We used two-tailed paired t-test for testing the
significance of the difference between the average
results of different systems. The significance level
(p-value) is reported.
The first two lines in Table 1 detail the results ob-
tained for both word (W) and morpheme (M) lev-
els of tokenization. The tagging accuracy of the
Accuracy per word (%)
Tokenization Tagging Segmentation
W 89.42 (0.9) 96.43 (0.3)
M 90.21 (1.2) 96.25 (0.5)
M+h 90.51 (1.0) 96.74 (0.5)
Table 1: Level of tokenization - experimental results
morpheme tagger is considerably better than what
is achieved by the word tagger (difference of 0.79%
with significance level p = 0.01). This is in spite of
the fact that the segmentation achieved by the word
tagger is a little better (and a segmentation error im-
plies incorrect tagging). Our hypothesis is that:
Morpheme-level taggers outperform
word-level taggers in their tagging ac-
curacy, since they suffer less from data
sparseness. However, they lack some
word-level knowledge that is required for
segmentation.
This hypothesis is supported by the number of
once-occurring terminals in each level: 8,582 in the
word level, versus 5,129 in the morpheme level.
Motivated by this hypothesis, we next consider
what kind of word-level information is required for
the morpheme-level tagger in order to do better in
segmentation. One natural enhancement for the
morpheme-level model involves adding information
about word boundaries to the tag set. In the en-
hanced tag set, nonterminal symbols include addi-
tional features that indicate whether the tagged mor-
pheme starts/ends a word. Unfortunately, we found
that adding word boundary information in this way
did not improve segmentation accuracy.
However, error analysis revealed a very common
type of segmentation errors, which was found to be
considerably more frequent in morpheme tagging
than in word tagging. This kind of errors involves
a missing or an extra covert definiteness marker ?h?.
For example, the word bbit can be segmented either
as b-bit (?in a house?) or as b-h-bit (?in the house?),
pronounced bebayit and babayit, respectively. Un-
like other cases of segmentation ambiguity, which
often just manifest lexical facts about spelling of He-
brew stems, this kind of ambiguity is productive: it
occurs whenever the stem?s POS allows definiteness,
and is preceded by one of the prepositions b/k/l. In
morpheme tagging, this type of error was found on
average in 1.71% of the words (46% of the segmen-
tation errors). In word tagging, it was found only
in 1.36% of the words (38% of the segmentation er-
rors).
Since in Hebrew there should be agreement be-
tween the definiteness status of a noun and its related
adjective, this kind of ambiguity can sometimes be
resolved syntactically. For instance:
?bbit hgdwl? implies b-h-bit (?in the big house?)
?bbit gdwl? implies b-bit (?in a big house?)
By contrast, in many other cases both analyses
are syntactically valid, and the choice between them
requires consideration of a wider context, or some
world knowledge. For example, in the sentence
hlknw lmsibh (?we went to a/the party?), lmsibh
can be analyzed either as l-msibh (indefinite,?to a
party?) or as l-h-mbsibh (definite,?to the party?).
Whether we prefer ?the party? or ?a party? depends
on contextual information that is not available for
the POS tagger.
Lexical statistics can provide valuable informa-
tion in such situations, since some nouns are more
common in their definite form, while other nouns are
more common as indefinite. For example, consider
the word lmmflh (?to a/the government?), which can
be segmented either as l-mmflh or l-h-mmflh. The
44
Tokenization Analysis
W (lmmflh IN-H-NN)
M (IN l) (H h) (NN mmflh)
M+h (IN l) (H-NN hmmflh)
Table 2: Representation of l-h-mmflh in each level
of tokenization
stem mmflh (?government?) was found 25 times in
the corpus, out of which only two occurrences were
indefinite. This strong lexical evidence in favor of
l-h-mmflh is completely missed by the morpheme-
level tagger, in which morphemes are assumed to
be independent. The lexical model of the word-
level tagger better models this difference, since it
does take into account the frequencies of l-mmflh
and l-h-mmlh, in measuring P(lmmflh|IN-NN) and
P(lmmflh|IN-H-NN). However, since the word tag-
ger considers lmmflh, hmmflh (?the government?),
and mmflh (?a government?) as independent words,
it still exploits only part of the potential lexical evi-
dence about definiteness.
In order to better model such situations, we
changed the morpheme-level model as follows. In
definite words the definiteness article h is treated
as a manifestation of a morphological feature of the
stem. Hence the definiteness marker?s POS tag (H)
is prefixed to the POS tag of the stem. We refer by
M+h to the resulting model that uses this assump-
tion, which is rather standard in theoretical linguistic
studies of Hebrew. The M+h model can be viewed as
an intermediate level of tokenization, between mor-
pheme and word tokenization. The different analy-
ses obtained by the three models of tokenization are
demonstrated in Table 2.
As shown in Table 1, the M+h model shows
remarkable improvement in segmentation (0.49%,
p < 0.001) compared with the initial morpheme-
level model (M). As expected, the frequency of seg-
mentation errors that involve covert definiteness (h)
dropped from 1.71% to 1.25%. The adjusted mor-
pheme tagger also outperforms the word level tagger
in segmentation (0.31%, p = 0.069). Tagging was
improved as well (0.3%, p = 0.068). According to
these results, tokenization as in the M+h model is
preferable to both plain-morpheme and plain-word
tokenization.
5 Conclusion
Developing a word segmenter and POS tagger for
Hebrew with less than 30K annotated words for
training is a challenging task, especially given the
morphological complexity and high degree of am-
biguity in Hebrew. For comparison, in English a
baseline model that selects the most frequent POS
tag achieves accuracy of around the 90% (Charniak
et al, 1993). However, in Hebrew we found that a
parallel baseline model achieves only 84% using the
available corpus.
The architecture proposed in this paper addresses
the severe sparseness problems that arise in a num-
ber of ways. First, the M+h model, which was
found to perform best, is based on morpheme-
level tokenization, which suffers of data sparse-
ness less than word tokenization, and makes use of
multi-morpheme nonterminals only in specific cases
where it was found to be valuable. The number of
nonterminal types found in the corpus for this model
is 49 (including 11 types of punctuation marks),
which is much closer to the morpheme-level model
(39 types) than to the word-level model (205 types).
Second, the bootstrapping method we present ex-
ploits additional resources such as a morphological
analyzer and an untagged corpus, to improve lexi-
cal probabilities, which suffer from data sparseness
the most. The improved lexical model contributes
1.5% to the tagging accuracy, and 0.6% to the seg-
mentation accuracy (compared with using the basic
lexical model), making it a crucial component of our
system.
Among the few other tools available for POS tag-
ging and morphological disambiguation in Hebrew,
the only one that is freely available for extensive
training and evaluation as performed in this paper
is Segal?s ((Segal, 2000), see section 2.2). Com-
paring our best architecture to the Segal tagger?s re-
sults under the same experimental setting shows an
improvement of 1.5% in segmentation accuracy and
4.5% in tagging accuracy over Segal?s results.
Moving on to Arabic, in a setting comparable to
(Diab et al, 2004), in which the correct segmenta-
tion is given, our tagger achieves accuracy per mor-
pheme of 94.9%. This result is close to the re-
45
sult reported by Diab et al, although our result was
achieved using a much smaller annotated corpus.
We therefore believe that future work may benefit
from applying our model, or variations thereof, to
Arabic and other Semitic languages.
One of the main sources for tagging errors in our
model is the coverage of the morphological analyzer.
The analyzer misses the correct analysis of 3.78% of
the test words. Hence, the upper bound for the accu-
racy of the disambiguator is 96.22%. Increasing the
coverage while maintaining the quality of the pro-
posed analyses (avoiding over-generation as much
as possible), is crucial for improving the tagging re-
sults.
It should also be mentioned that a new version of
the Hebrew treebank, now containing approximately
5,000 sentences, was released after the current work
was completed. We believe that the additional an-
notated data will allow to refine our model, both in
terms of accuracy and in terms of coverage, by ex-
panding the tag set with additional morpho-syntactic
features like gender and number, which are prevalent
in Hebrew and other Semitic languages.
Acknowledgments
We thank Gilad Ben-Avi, Ido Dagan and Alon Itai
for their insightful remarks on major aspects of this
work. The financial and computational support of
the Knowledge Center for Processing Hebrew is
gratefully acknowledged. The first author would like
to thank the Technion for partially funding his part
of the research. The first and third authors are grate-
ful to the ILLC of the University of Amsterdam for
its hospitality while working on this research. We
also thank Andreas Stolcke for his devoted technical
assistance with SRILM.
References
Meni Adler. 2001. Hidden Markov Model for Hebrew
part-of-speech tagging. Master?s thesis, Ben Gurion
University, Israel. In Hebrew.
Leonard Baum. 1972. An inequality and associated max-
imization technique in statistical estimation for proba-
bilistic functions of a Markov process. In Inequalities
III:Proceedings of the Third Symposium on Inequali-
ties, University of California, Los Angeles, pp.1-8.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistic, 21:784?789.
David Carmel and Yoelle Maarek. 1999. Morphological
disambiguation for Hebrew search systems. In Pro-
ceedings of the 4th international workshop,NGITS-99.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on Artificial
Intelligence, pages 784?789.
K. W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Proc. of
the Second Conference on Applied Natural Language
Processing, pages 136?143, Austin, TX.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In HLT-NAACL 2004: Short Pa-
pers, pages 149?152.
S.M. Katz. 1987. Estimation of probabilities from sparse
data from the language model component of a speech
recognizer. IEEE Transactions of Acoustics, Speech
and Signal Processing, 35(3):400?401.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In ACL,
pages 399?406.
M. Levinger, U. Ornan, and A. Itai. 1995. Morphological
disambiguation in Hebrew using a priori probabilities.
Computational Linguistics, 21:383?404.
Moshe Levinger. 1992. Morphological disambiguation
in Hebrew. Master?s thesis, Computer Science Depart-
ment, Technion, Haifa, Israel. In Hebrew.
Erel Segal. 2000. Hebrew morphological ana-
lyzer for Hebrew undotted texts. Master?s the-
sis, Computer Science Department, Technion,
Haifa, Israel. http://www.cs.technion.ac.il/-
?erelsgl/bxi/hmntx/teud.html.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of Modern Hebrew text.
Traitment Automatique des Langues, 42:347?380.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages 901?904, Denver,
Colorado, September.
46
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 55?60,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Definition and Analysis of Intermediate Entailment Levels
Roy Bar-Haim, Idan Szpektor, Oren Glickman
Computer Science Department
Bar Ilan University
Ramat-Gan 52900, Israel
{barhair,szpekti,glikmao}@cs.biu.ac.il
Abstract
In this paper we define two intermediate
models of textual entailment, which corre-
spond to lexical and lexical-syntactic lev-
els of representation. We manually anno-
tated a sample from the RTE dataset ac-
cording to each model, compared the out-
come for the two models, and explored
how well they approximate the notion of
entailment. We show that the lexical-
syntactic model outperforms the lexical
model, mainly due to a much lower rate
of false-positives, but both models fail to
achieve high recall. Our analysis also
shows that paraphrases stand out as a
dominant contributor to the entailment
task. We suggest that our models and an-
notation methods can serve as an evalua-
tion scheme for entailment at these levels.
1 Introduction
Textual entailment has been proposed recently as
a generic framework for modeling semantic vari-
ability in many Natural Language Processing ap-
plications, such as Question Answering, Informa-
tion Extraction, Information Retrieval and Docu-
ment Summarization. The textual entailment rela-
tionship holds between two text fragments, termed
text and hypothesis, if the truth of the hypothesis can
be inferred from the text.
Identifying entailment is a complex task that in-
corporates many levels of linguistic knowledge and
inference. The complexity of modeling entail-
ment was demonstrated in the first PASCAL Chal-
lenge Workshop on Recognizing Textual Entailment
(RTE) (Dagan et al, 2005). Systems that partici-
pated in the challenge used various combinations of
NLP components in order to perform entailment in-
ferences. These components can largely be classi-
fied as operating at the lexical, syntactic and seman-
tic levels (see Table 1 in (Dagan et al, 2005)). How-
ever, only little research was done to analyze the
contribution of each inference level, and on the con-
tribution of individual inference mechanisms within
each level.
This paper suggests that decomposing the com-
plex task of entailment into subtasks, and analyz-
ing the contribution of individual NLP components
for these subtasks would make a step towards bet-
ter understanding of the problem, and for pursuing
better entailment engines. We set three goals in this
paper. First, we consider two modeling levels that
employ only part of the inference mechanisms, but
perform perfectly at each level. We explore how
well these models approximate the notion of entail-
ment, and analyze the differences between the out-
come of the different levels. Second, for each of the
presented levels, we evaluate the distribution (and
contribution) of each of the inference mechanisms
typically associated with that level. Finally, we sug-
gest that the definitions of entailment at different
levels of inference, as proposed in this paper, can
serve as guidelines for manual annotation of a ?gold
standard? for evaluating systems that operate at a
particular level. Altogether, we set forth a possible
methodology for annotation and analysis of entail-
55
ment datasets.
We introduce two levels of entailment: Lexical
and Lexical-Syntactic. We propose these levels as
intermediate stages towards a complete entailment
model. We define an entailment model for each
level and manually evaluate its performance over a
sample from the RTE test-set. We focus on these
two levels as they correspond to well-studied NLP
tasks, for which robust tools and resources exist,
e.g. parsers, part of speech taggers and lexicons. At
each level we included inference types that represent
common practice in the field. More advanced pro-
cessing levels which involve logical/semantic infer-
ence are less mature and were left beyond the scope
of this paper.
We found that the main difference between the
lexical and lexical-syntactic levels is that the lexical-
syntactic level corrects many false-positive infer-
ences done at the lexical level, while introducing
only a few false-positives of its own. As for iden-
tifying positive cases (recall), both systems exhibit
similar performance, and were found to be comple-
mentary. Neither of the levels was able to iden-
tify more than half of the positive cases, which
emphasizes the need for deeper levels of analysis.
Among the different inference components, para-
phrases stand out as a dominant contributor to the
entailment task, while synonyms and derivational
transformations were found to be the most frequent
at the lexical level.
Using our definitions of entailment models as
guidelines for manual annotation resulted in a high
level of agreement between two annotators, suggest-
ing that the proposed models are well-defined.
Our study follows on previous work (Vander-
wende et al, 2005), which analyzed the RTE Chal-
lenge test-set to find the percentage of cases in
which syntactic analysis alone (with optional use
of thesaurus for the lexical level) suffices to decide
whether or not entailment holds. Our study extends
this work by considering a broader range of infer-
ence levels and inference mechanisms and providing
a more detailed view. A fundamental difference be-
tween the two works is that while Vanderwende et al
did not make judgements on cases where additional
knowledge was required beyond syntax, our entail-
ment models were evaluated over all of the cases,
including those that require higher levels of infer-
ence. This allows us to view the entailment model at
each level as an idealized system approximating full
entailment, and to evaluate its overall success.
The rest of the paper is organized as follows: sec-
tion 2 provides definitions for the two entailment
levels; section 3 describes the annotation experiment
we performed, its results and analysis; section 4 con-
cludes and presents planned future work.
2 Definition of Entailment Levels
In this section we present definitions for two en-
tailment models that correspond to the Lexical and
Lexical-Syntactic levels. For each level we de-
scribe the available inference mechanisms. Table 1
presents several examples from the RTE test-set to-
gether with annotation of entailment at the different
levels.
2.1 The Lexical entailment level
At the lexical level we assume that the text T and
hypothesis H are represented by a bag of (possibly
multi-word) terms, ignoring function words. At this
level we define that entailment holds between T and
H if every term h in H can be matched by a corre-
sponding entailing term t in T . t is considered as en-
tailing h if either h and t share the same lemma and
part of speech, or t can be matched with h through a
sequence of lexical transformations of the types de-
scribed below.
Morphological derivations This inference mech-
anism considers two terms as equivalent if one can
be obtained from the other by some morphologi-
cal derivation. Examples include nominalizations
(e.g. ?acquisition ? acquire?), pertainyms (e.g.
?Afghanistan ? Afghan?), or nominal derivations
like ?terrorist ? terror?.
Ontological relations This inference mechanism
refers to ontological relations between terms. A
term is inferred from another term if a chain of valid
ontological relations between the two terms exists
(Andreevskaia et al, 2005). In our experiment we
regarded the following three ontological relations
as providing entailment inferences: (1) ?synonyms?
(e.g. ?free ? release? in example 1361, Table 1);
(2) ?hypernym? (e.g. ?produce ? make?) and (3)
?meronym-holonym? (e.g. ?executive ? company?).
56
No. Text Hypothesis Task Ent. Lex.
Ent.
Syn.
Ent.
322 Turnout for the historic vote for the first
time since the EU took in 10 new mem-
bers in May has hit a record low of
45.3%.
New members joined the
EU.
IR true false true
1361 A Filipino hostage in Iraq was released. A Filipino hostage was
freed in Iraq.
CD true true true
1584 Although a Roscommon man by birth,
born in Rooskey in 1932, Albert ?The
Slasher? Reynolds will forever be a
Longford man by association.
Albert Reynolds was born
in Co. Roscommon.
QA true true true
1911 The SPD got just 21.5% of the vote
in the European Parliament elections,
while the conservative opposition par-
ties polled 44.5%.
The SPD is defeated by
the opposition parties.
IE true false false
2127 Coyote shot after biting girl in Vanier
Park.
Girl shot in park. IR false true false
Table 1: Examples of text-hypothesis pairs, taken from the PASCAL RTE test-set. Each line includes the
example number at the RTE test-set, the text and hypothesis, the task within the test-set, whether entailment
holds between the text and hypothesis (Ent.), whether Lexical entailment holds (Lex. Ent.) and whether
Lexical-Syntactic entailment holds (Syn. Ent.).
Lexical World knowledge This inference mech-
anism refers to world knowledge reflected at the
lexical level, by which the meaning of one term
can be inferred from the other. It includes both
knowledge about named entities, such as ?Tal-
iban ? organization? and ?Roscommon ? Co.
Roscommon? (example 1584 in Table 1), and other
lexical relations between words, such as WordNet?s
relations ?cause? (e.g. ?kill ? die?) and ?entail? (e.g.
?snore ? sleep?).
2.2 The Lexical-syntactic entailment level
At the lexical-syntactic level we assume that the
text and the hypothesis are represented by the set of
syntactic dependency relations of their dependency
parse. At this level we ignore determiners and aux-
iliary verbs, but do include relations involving other
function words. We define that entailment holds be-
tween T and H if the relations within H can be
?covered? by the relations in T . In the trivial case,
lexical-syntactic entailment holds if all the relations
composing H appear verbatim in T (while addi-
tional relations within T are allowed). Otherwise,
such coverage can be obtained by a sequence of
transformations applied to the relations in T , which
should yield all the relations in H .
One type of such transformations are the lexical
transformations, which replace corresponding lexi-
cal items, as described in sub-section 2.1. When ap-
plying morphological derivations it is assumed that
the syntactic structure is appropriately adjusted. For
example, ?Mexico produces oil? can be mapped to
?oil production by Mexico? (the NOMLEX resource
(Macleod et al, 1998) provides a good example for
systematic specification of such transformations).
Additional types of transformations at this level
are specified below.
Syntactic transformations This inference mech-
anism refers to transformations between syntactic
structures that involve the same lexical elements and
preserve the meaning of the relationships between
them (as analyzed in (Vanderwende et al, 2005)).
Typical transformations include passive-active and
apposition (e.g. ?An Wang, a native of Shanghai ?
An Wang is a native of Shanghai?).
57
Entailment paraphrases This inference mecha-
nism refers to transformations that modify the syn-
tactic structure of a text fragment as well as some
of its lexical elements, while holding an entailment
relationship between the original text and the trans-
formed one. Such transformations are typically de-
noted as ?paraphrases? in the literature, where a
wealth of methods for their automatic acquisition
were proposed (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Szpektor et al,
2004). Following the same spirit, we focus here on
transformations that are local in nature, which, ac-
cording to the literature, may be amenable for large
scale acquisition. Examples include: ?X is Y man
by birth ? X was born in Y? (example 1584 in Ta-
ble 1), ?X take in Y ? Y join X?1 and ?X is holy
book of Y ? Y follow X?2.
Co-reference Co-references provide equivalence
relations between different terms in the text and
thus induce transformations that replace one term
in a text with any of its co-referenced terms. For
example, the sentence ?Italy and Germany have
each played twice, and they haven?t beaten anybody
yet.?3 entails ?Neither Italy nor Germany have
won yet?, involving the co-reference transformation
?they ? Italy and Germany?.
Example 1584 in Table 1 demonstrates the
need to combine different inference mechanisms
to achieve lexical-syntactic entailment, requiring
world-knowledge, paraphrases and syntactic trans-
formations.
3 Empirical Analysis
In this section we present the experiment that we
conducted in order to analyze the two entailment
levels, which are presented in section 2, in terms of
relative performance and correlation with the notion
of textual entailment.
3.1 Data and annotation procedure
The RTE test-set4 contains 800 Text-Hypothesis
pairs (usually single sentences), which are typical
1Example no 322 in the PASCAL RTE test-set.
2Example no 1575 in the PASCAL RTE test-set.
3Example no 298 in the PASCAL RTE test-set.
4The complete RTE dataset can be obtained at
http://www.pascal-network.org/Challenges/RTE/Datasets/
to various NLP applications. Each pair is annotated
with a boolean value, indicating whether the hypoth-
esis is entailed by the text or not, and the test-set
is balanced in terms of positive and negative cases.
We shall henceforth refer to this annotation as the
gold standard. We constructed a sample of 240 pairs
from four different tasks in the test-set, which corre-
spond to the main applications that may benefit from
entailment: information extraction (IE), information
retrieval (IR), question answering (QA), and compa-
rable documents (CD). We randomly picked 60 pairs
from each task, and in total 118 of the cases were
positive and 122 were negative.
In our experiment, two of the authors annotated,
for each of the two levels, whether or not entailment
can be established in each of the 240 pairs. The an-
notators agreed on 89.6% of the cases at the lexical
level, and 88.8% of the cases at the lexical-syntactic
level, with Kappa statistics of 0.78 and 0.73, re-
spectively, corresponding to ?substantial agreement?
(Landis and Koch, 1977). This relatively high level
of agreement suggests that the notion of lexical and
lexical-syntactic entailment we propose are indeed
well-defined.
Finally, in order to establish statistics from the an-
notations, the annotators discussed all the examples
they disagreed on and produced a final joint deci-
sion.
3.2 Evaluating the different levels of entailment
L LS
True positive (118) 52 59
False positive (122) 36 10
Recall 44% 50%
Precision 59% 86%
F1 0.5 0.63
Accuracy 58% 71%
Table 2: Results per level of entailment.
Table 2 summarizes the results obtained from our
annotated dataset for both lexical (L) and lexical-
syntactic (LS) levels. Taking a ?system?-oriented
perspective, the annotations at each level can be
viewed as the classifications made by an idealized
system that includes a perfect implementation of the
inference mechanisms in that level. The first two
58
rows show for each level how the cases, which were
recognized as positive by this level (i.e. the entail-
ment holds), are distributed between ?true positive?
(i.e. positive according to the gold standard) and
?false positive? (negative according to the gold stan-
dard). The total number of positive and negative
pairs in the dataset is reported in parentheses. The
rest of the table details recall, precision, F1 and ac-
curacy.
The distribution of the examples in the RTE test-
set cannot be considered representative of a real-
world distribution (especially because of the con-
trolled balance between positive and negative exam-
ples). Thus, our statistics are not appropriate for
accurate prediction of application performance. In-
stead, we analyze how well these simplified models
of entailment succeed in approximating ?real? en-
tailment, and how they compare with each other.
The proportion between true and false positive
cases at the lexical level indicates that the correla-
tion between lexical match and entailment is quite
low, reflected in the low precision achieved by this
level (only 59%). This result can be partly attributed
to the idiosyncracies of the RTE test-set: as reported
in (Dagan et al, 2005), samples with high lexical
match were found to be biased towards the negative
side. Interestingly, our measured accuracy correlates
well with the performance of systems at the PAS-
CAL RTE Workshop, where the highest reported ac-
curacy of a lexical system is 0.586 (Dagan et al,
2005).
As one can expect, adding syntax considerably re-
duces the number of false positives - from 36 to only
10. Surprisingly, at the same time the number of true
positive cases grows from 52 to 59, and correspond-
ingly, precision rise to 86%. Interestingly, neither
the lexical nor the lexical-syntactic level are able to
cover more than half of the positive cases (e.g. ex-
ample 1911 in Table 1).
In order to better understand the differences be-
tween the two levels, we next analyze the overlap
between them, presented in Table 3. Looking at
Table 3(a), which contains only the positive cases,
we see that many examples were recognized only by
one of the levels. This interesting phenomenon can
be explained on the one hand by lexical matches that
could not be validated in the syntactic level, and on
the other hand by the use of paraphrases, which are
Lexical-Syntactic
H ? T H; T
Lexical H ? T 38 14H; T 21 45
(a) positive examples
Lexical-Syntactic
H ? T H; T
Lexical H ? T 7 29H; T 3 83
(b) negative examples
Table 3: Correlation between the entailment lev-
els. (a) includes only the positive examples from
the RTE dataset sample, and (b) includes only the
negative examples.
introduced only in the lexical-syntactic level. (e.g.
example 322 in Table 1).
This relatively symmetric situation changes as we
move to the negative cases, as shown in Table 3(b).
By adding syntactic constraints, the lexical-syntactic
level was able to fix 29 false positive errors, misclas-
sified at the lexical level (as demonstrated in exam-
ple 2127, Table 1), while introducing only 3 new
false-positive errors. This exemplifies the impor-
tance of syntactic matching for precision.
3.3 The contribution of various inference
mechanisms
Inference Mechanism f 4R %
Synonym 19 14.4% 16.1%
Morphological 16 10.1% 13.5%
Lexical World knowledge 12 8.4% 10.1%
Hypernym 7 4.2% 5.9%
Mernoym 1 0.8% 0.8%
Entailment Paraphrases 37 26.2% 31.3%
Syntactic transformations 22 16.9% 18.6%
Coreference 10 5.0% 8.4%
Table 4: The frequency (f ), contribution to recall
(4R) and percentage (%), within the gold standard
positive examples, of the various inference mecha-
nisms at each level, ordered by their significance.
59
In order to get a sense of the contribution of the
various components at each level, statistics on the in-
ference mechanisms that contributed to the coverage
of the hypothesis by the text (either full or partial)
were recorded by one annotator. Only the positive
cases in the gold standard were considered.
For each inference mechanism we measured its
frequency, its contribution to the recall of the related
level and the percentage of cases in which it is re-
quired for establishing entailment. The latter also
takes into account cases where only partial cover-
age could be achieved, and thus indicates the signif-
icance of each inference mechanism for any entail-
ment system, regardless of the models presented in
this paper. The results are summarized in Table 4.
From Table 4 it stands that paraphrases are the
most notable contributors to recall. This result in-
dicates the importance of paraphrases to the en-
tailment task and the need for large-scale para-
phrase collections. Syntactic transformations are
also shown to contribute considerably, indicating the
need for collections of syntactic transformations as
well. In that perspective, we propose our annota-
tion framework as means for evaluating collections
of paraphrases or syntactic transformations in terms
of recall.
Finally, we note that the co-reference moderate
contribution can be partly attributed to the idiosyn-
cracies of the RTE test-set: the annotators were
guided to replace anaphors with the appropriate ref-
erence, as reported in (Dagan et al, 2005).
4 Conclusions
In this paper we presented the definition of two en-
tailment models, Lexical and Lexical-Syntactic, and
analyzed their performance manually. Our experi-
ment shows that the lexical-syntactic level outper-
forms the lexical level in all measured aspects. Fur-
thermore, paraphrases and syntactic transformations
emerged as the main contributors to recall. These
results suggest that a lexical-syntactic framework
is a promising step towards a complete entailment
model.
Beyond these empirical findings we suggest that
the presented methodology can be used generically
to annotate and analyze entailment datasets.
In future work, it would be interesting to analyze
higher levels of entailment, such as logical inference
and deep semantic understanding of the text.
Acknowledgements
We would like to thank Ido Dagan for helpful discus-
sions and for his scientific supervision. This work
was supported in part by the IST Programme of the
European Community, under the PASCAL Network
of Excellence, IST-2002-506778. This publication
only reflects the authors? views.
References
Alina Andreevskaia, Zhuoyan Li and Sabine Bergler.
2005. Can Shallow Predicate Argument Structures
Determine Entailment?. In Proceedings of Pascal
Challenge Workshop on Recognizing Textual Entail-
ment, 2005.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003. pages 16-23, Edmonton, Canada.
Ido Dagan, Bernardo Magnini and Oren Glickman. 2005.
The PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of Pascal Challenge Workshop
on Recognizing Textual Entailment, 2005.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159-174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for Question Answering. Natural Language
Engineering, 7(4):343-360.
C. Macleod, R. Grishman, A. Meyers, L. Barrett and R.
Reeves. 1998. Nomlex: A lexicon of nominalizations.
In Proceedings of the 8th International Congress of the
European Association for Lexicography, 1998. Lie`ge,
Belgium: EURALEX.
Yusuke Shinyama and Satoshi Sekine, Kiyoshi Sudo and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002). San
Diego, USA.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonnaven-
tura Coppola. 2004. Scaling Web-based Acquistion
of Entailment Relations. In Proceedings of EMNLP
2004.
Lucy Vanderwende, Deborah Coughlin and Bill Dolan.
2005. What Syntax Contribute in Entailment Task. In
Proceedings of Pascal Challenge Workshop on Recog-
nizing Textual Entailment, 2005.
60
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 131?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic Inference at the Lexical-Syntactic Level for Textual Entailment
Recognition
Roy Bar-Haim?,Ido Dagan?, Iddo Greental?, Idan Szpektor? and Moshe Friedman?
?Computer Science Department, Bar-Ilan University, Ramat-Gan 52900, Israel
?Linguistics Department, Tel Aviv University, Ramat Aviv 69978, Israel
{barhair,dagan}@cs.biu.ac.il,greenta@post.tau.ac.il,
{szpekti,friedmm}@cs.biu.ac.il
Abstract
We present a new framework for textual en-
tailment, which provides a modular integra-
tion between knowledge-based exact infer-
ence and cost-based approximate matching.
Diverse types of knowledge are uniformly
represented as entailment rules, which were
acquired both manually and automatically.
Our proof system operates directly on parse
trees, and infers new trees by applying en-
tailment rules, aiming to strictly generate the
target hypothesis from the source text. In or-
der to cope with inevitable knowledge gaps,
a cost function is used to measure the re-
maining ?distance? from the hypothesis.
1 Introduction
According to the traditional formal semantics ap-
proach, inference is conducted at the logical level.
However, practical text understanding systems usu-
ally employ shallower lexical and lexical-syntactic
representations, augmented with partial semantic
annotations. Such practices are typically partial and
quite ad-hoc, and lack a clear formalism that speci-
fies how inference knowledge should be represented
and applied. The current paper proposes a step to-
wards filling this gap, by defining a principled se-
mantic inference mechanism over parse-based rep-
resentations.
Within the textual entailment setting a system is
required to recognize whether a hypothesized state-
ment h can be inferred from an asserted text t.
Some inferences can be based on available knowl-
edge, such as information about synonyms and para-
phrases. However, some gaps usually arise and it
is often not possible to derive a complete ?proof?
based on available inference knowledge. Such sit-
uations are typically handled through approximate
matching methods.
This paper focuses on knowledge-based infer-
ence, while employing rather basic methods for ap-
proximate matching. We define a proof system
that operates over syntactic parse trees. New trees
are derived using entailment rules, which provide a
principled and uniform mechanism for incorporat-
ing a wide variety of manually and automatically-
acquired inference knowledge. Interpretation into
stipulated semantic representations, which is often
difficult to obtain, is circumvented altogether. Our
research goal is to explore how far we can get with
such an inference approach, and identify the scope
in which semantic interpretation may not be needed.
For a detailed discussion of our approach and related
work, see (Bar-Haim et al, 2007).
2 Inference Framework
The main contribution of the current work is a prin-
cipled semantic inference mechanism, that aims to
generate a target text from a source text using en-
tailment rules, analogously to logic-based proof sys-
tems. Given two parsed text fragments, termed
text (t) and hypothesis (h), the inference system (or
prover) determines whether t entails h. The prover
applies entailment rules that aim to transform t into
h through a sequence of intermediate parse trees.
For each generated tree p, a heuristic cost function is
employed to measure the likelihood of p entailing h.
131
ROOT
i 
rain VERB
expletive
ssfff
ff
ff
ff
f wha
++XX
XX
XX
XX
XX
it OTHER when PREP
i 
see VERB
obj
qqcccccc
cccc
cccc
cccc
cc
bessff
ff
ff
ff
ff
by

mod
++XX
XX
XX
XX
XX
Mary NOUN
mod 
be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i 
rain VERB
expletive
rreeee
eee
eee
e wha
,,YYY
YYY
YYY
YY
it OTHER when PREP
i 
see VERB
subj
rreeee
eee
eee
e
obj

mod
,,YYY
YYY
YYY
YY
John NOUN Mary NOUN
mod 
yesterday NOUN
beautiful ADJ
Source: it rained when beautiful Mary was seen by John yester-
day
Derived: it rained when John saw beautiful Mary yesterday
(a) Application of passive to active transformation
L
V VERB
obj
ssfff
ff
ff
ff
f
be 
by
++XX
XX
XX
XX
XX
R
V VERB
subj
ssfff
ff
ff
ff
f obj
++XX
XX
XX
XX
XX
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(b) Passive to active transformation (substitution rule). The dotted arc represents alignment.
Figure 1: Application of inference rules. POS and relation labels are based on Minipar (Lin, 1998b)
If a complete proof is found (h was generated), the
prover concludes that entailment holds. Otherwise,
entailment is determined by comparing the minimal
cost found during the proof search to some threshold
?.
3 Proof System
Like logic-based systems, our proof system consists
of propositions (t, h, and intermediate premises),
and inference (entailment) rules, which derive new
propositions from previously established ones.
3.1 Propositions
Propositions are represented as dependency trees,
where nodes represent words, and hold a set of fea-
tures and their values. In our representation these
features include the word lemma and part-of-speech,
and additional features that may be added during the
proof process. Edges are annotated with dependency
relations.
3.2 Inference Rules
At each step of the proof an inference rule gener-
ates a derived tree d from a source tree s. A rule
is primarily composed of two templates, termed left-
hand-side (L), and right-hand-side (R). Templates
are dependency subtrees which may contain vari-
ables. Figure 1(b) shows an inference rule, where
V , N1 and N2 are common variables. L specifies
the subtree of s to be modified, and R specifies the
new generated subtree. Rule application consists of
the following steps:
L matching The prover first tries to match L in s.
L is matched in s if there exists a one-to-one node
mapping function f from L to s, such that: (i) For
each node u, f(u) has the same features and feature
values as u. Variables match any lemma value in
f(u). (ii) For each edge u ? v in L, there is an
edge f(u) ? f(v) in s, with the same dependency
relation. If matching fails, the rule is not applicable
to s. Otherwise, successful matching induces vari-
132
LROOT
i 
R
ROOT
i 
V1 VERB
wha 
V2 VERB
when ADJ
i 
V2 VERB
Figure 2: Temporal clausal modifier extraction (in-
troduction rule)
able binding b(X), for each variableX in L, defined
as the full subtree rooted in f(X) if X is a leaf, or
f(X) alone otherwise. We denote by l the subtree
in s to which L was mapped (as illustrated in bold
in Figure 1(a), left tree).
R instantiation An instantiation of R, which we
denote r, is generated in two steps: (i) creating a
copy of R; (ii) replacing each variable X with a
copy of its binding b(X) (as set during L matching).
In our example this results in the subtree John saw
beautiful Mary.
Alignment copying the alignment relation be-
tween pairs of nodes in L and R specifies which
modifiers in l that are not part of the rule structure
need to be copied to the generated tree r. Formally,
for any two nodes u in l and v in r whose matching
nodes in L and R are aligned, we copy the daugh-
ter subtrees of u in s, which are not already part of
l, to become daughter subtrees of v in r. The bold
nodes in the right part of Figure 1(b) correspond to
r after alignment. yesterday was copied to r due to
the alignment of its parent verb node.
Derived tree generation by rule type Our for-
malism has two methods for generating the derived
tree: substitution and introduction, as specified by
the rule type. With substitution rules, the derived
tree d is obtained by making a local modification to
the source tree s. Except for this modification s and
d are identical (a typical example is a lexical rule,
such as buy ? purchase). For this type, d is formed
by copying s while replacing l (and the descendants
of l?s nodes) with r. This is the case for the passive
rule. The right part of Figure 1(a) shows the derived
tree for the passive rule application. By contrast, in-
troduction rules are used to make inferences from a
subtree of s, while the other parts of s are ignored
and do not affect d. A typical example is inference
of a proposition embedded as a relative clause in s.
In this case the derived tree d is simply taken to be
r. Figure 2 presents such a rule that derives propo-
sitions embedded within temporal modifiers. Note
that the derived tree does not depend on the main
clause. Applying this rule to the right part of Figure
1(b) yields the proposition John saw beautiful Mary
yesterday.
3.3 Annotation Rules
Annotation rules add features to parse tree nodes,
and are used in our system to annotate negation and
modality. Annotation rules do not have an R. In-
stead, nodes of L may contain annotation features.
If L is matched in a tree then the annotations are
copied to the matched nodes. Annotation rules are
applied to t and to each inferred premise prior to
any entailment rule application and these features
may block inappropriate subsequent rule applica-
tions, such as for negated predicates.
4 Rules for Generic Linguistic Structures
Based on the above framework we have manually
created a rule base for generic linguistic phenomena.
4.1 Syntactic-Based Rules
These rules capture entailment inferences associ-
ated with common syntactic structures. They have
three major functions: (i) simplification and canon-
ization of the source tree (categories 6 and 7 in Ta-
ble 1); (ii) extracting embedded propositions (cate-
gories 1, 2, 3); (iii) inferring propositions from non-
propositional subtrees (category 4).
4.2 Polarity-Based Rules
Consider the following two examples:
John knows that Mary is here?Mary is here.
John believes that Mary is here;Mary is here.
Valid inference of propositions embedded as verb
complements depends on the verb properties, and
the polarity of the context in which the verb appears
(positive, negative, or unknown) (Nairn et al, 2006).
We extracted from the polarity lexicon of Nairn et
al. a list of verbs for which inference is allowed in
positive polarity context, and generated entailment
133
# Category Example: source Example: derived
1 Conjunctions Helena?s very experienced and has played a long
time on the tour.
? Helena has played a long time on the tour.
2 Clausal modi-
fiers
But celebrations were muted as many Iranians ob-
served a Shi?ite mourning month.
? Many Iranians observed a Shi?ite mourning
month.
3 Relative
clauses
The assailants fired six bullets at the car, which car-
ried Vladimir Skobtsov.
? The car carried Vladimir Skobtsov.
4 Appositives Frank Robinson, a one-time manager of the Indians,
has the distinction for the NL.
? Frank Robinson is a one-time manager of the
Indians.
5 Determiners The plaintiffs filed their lawsuit last year in U.S.
District Court in Miami.
? The plaintiffs filed a lawsuit last year in U.S.
District Court in Miami.
6 Passive We have been approached by the investment banker. ? The investment banker approached us.
7 Genitive
modifier
Malaysia?s crude palm oil output is estimated to
have risen by up to six percent.
? The crude palm oil output of Malasia is esti-
mated to have risen by up to six percent.
8 Polarity Yadav was forced to resign. ? Yadav resigned.
9 Negation,
modality
What we?ve never seen is actual costs come
down.
What we?ve never seen is actual costs come down.
(;What we?ve seen is actual costs come down.)
Table 1: Summary of rule base for generic linguistic structures.
rules for these verbs (category 8). The list was com-
plemented with a few reporting verbs, such as say
and announce, assuming that in the news domain the
speaker is usually considered reliable.
4.3 Negation and Modality Annotation Rules
We use annotation rules to mark negation and
modality of predicates (mainly verbs), based on their
descendent modifiers. Category 9 in Table 1 illus-
trates a negation rule, annotating the verb seen for
negation due to the presence of never.
4.4 Generic Default Rules
Generic default rules are used to define default be-
havior in situations where no case-by-case rules are
available. We used one default rule that allows re-
moval of any modifiers from nodes.
5 Lexical-based Rules
These rules have open class lexical components, and
consequently are numerous compared to the generic
rules described in section 4. Such rules are acquired
either lexicographically or automatically.
The rules described in the section 4 are applied
whenever their L template is matched in the source
premise. For high fan-out rules such as lexical-based
rules (e.g. words with many possible synonyms),
this may drastically increase the size of the search
space. Therefore, the rules described below are ap-
plied only if L is matched in the source premise p
and R is matched in h.
5.1 Lexical Rules
Lexical entailment rules, such as ?steal ? take? and
?Britain ? UK? were created based on WordNet
(Fellbaum, 1998). Given p and h, a lexical rule
lemmap ? lemmah may be applied if lemmap
and lemmah are lemmas of open-class words ap-
pearing in p and h respectively, and there is a path
from lemmah to lemmap in the WordNet ontology,
through synonym and hyponym relations.
5.2 Lexical-Syntactic Rules
In order to find lexical-syntactic paraphrases and en-
tailment rules, such as ?X strike Y ? X hit Y ? and
?X buy Y ?X own Y ? that would bridge between p
and h, we applied the DIRT algorithm (Lin and Pan-
tel, 2001) to the first CD of the Reuters RCV1 cor-
pus1. DIRT does not identify the entailment direc-
tion, hence we assumed bi-directional entailment.
We calculate off-line only the feature vector of ev-
ery template found in the corpus, where each path
between head nouns is considered a template in-
stance. Then, given a premise p, we first mark all
lexical noun alignments between p and h. Next, for
every pair of alignments we extract the path between
the two nouns in p, labeled pathp, and the corre-
sponding path between the aligned nouns in h, la-
beled pathh. We then on-the-fly test whether there
is a rule ?pathp ? pathh? by extracting the stored
feature vectors of pathp and pathh and measuring
1http://about.reuters.com/researchandstandards/corpus/
134
their similarity. If the score exceeds a given thresh-
old2, we apply the rule to p.
Another enhancement that we added to DIRT is
template canonization. At learning time, we trans-
form every template identified in the corpus into
its canonized form3 using a set of morpho-syntactic
rules, similar to the ones described in Section 4. In
addition, we apply nominalization rules such as ?ac-
quisition of Y by X ? X acquire Y ?, which trans-
form a nominal template into its related verbal form.
We automatically generate these rules (Ron, 2006),
based on Nomlex (Macleod et al, 1998).
At inference time, before retrieving feature vec-
tors, we canonize pathp into pathcp and pathh into
pathch. We then assess the rule ?path
c
p ? path
c
h?,
and if valid, we apply the rule ?pathp ? pathh? to
p. In order to ensure the validity of the implicature
?pathp ? pathcp ? path
c
h ? pathh?, we canonize
pathp using the same rule set used at learning time,
but we apply only bi-directional rules to pathh (e.g.
conjunct heads are not removed from pathh).
6 Approximate Matching
As mentioned in section 2, approximate matching
is incorporated into our system via a cost function,
which estimates the likelihood of h being entailed
from a given premise p. Our cost function C(p, h) is
a linear combination of two measures: lexical cost,
Clex(p, h) and lexical-syntactic cost ClexSyn(p, h):
C(p, h) = ?ClexSyn(p, h)+ (1??)Clex(p, h) (1)
Let m?() be a (possibly partial) 1-1 mapping of the
nodes of h to the nodes of p, where each node
is mapped to a node with the same lemma, such
that the number of matched edges is maximized.
An edge u ? v in h is matched in p if m?(u)
and m?(v) are both defined, and there is an edge
m?(u) ? m?(v) in p, with the same dependency rela-
tion. ClexSyn(p, h) is then defined as the percentage
of unmatched edges in h.
Similarly, Clex(p, h) is the percentage of un-
matched lemmas in h, considering only open-class
words, defined as:
Clex(p, h) = 1?
?
l?h Score(l)
#OpenClassWords(h)
(2)
2We set the threshold to 0.01
3The active verbal form with direct modifiers
where Score(l) is 1 if it appears in p, or if it is
a derivation of a word in p (according to Word-
Net). Otherwise, Score(l) is the maximal Lin
dependency-based similarity score between l and the
lemmas of p (Lin, 1998a) (synonyms and hyper-
nyms/hyponyms are handled by the lexical rules).
7 System Implementation
Deriving the initial propositions t and h from the in-
put text fragments consists of the following steps:
(i) Anaphora resolution, using the MARS system
(Mitkov et al, 2002). Each anaphor was replaced by
its antecedent. (ii) Sentence splitting, using mxter-
minator (Reynar and Ratnaparkhi, 1997). (iii) De-
pendency parsing, using Minipar (Lin, 1998b).
The proof search is implemented as a depth-first
search, with maximal depth (i.e. proof length) of
4. If the text contains more than one sentence, the
prover aims to prove h from each of the parsed sen-
tences, and entailment is determined based on the
minimal cost. Thus, the only cross-sentence infor-
mation that is considered is via anaphora resolution.
8 Evaluation
Full (run1) Lexical (run2)
Dataset Task Acc. Avg.P Acc. Avg.P
Test IE 0.4950 0.5021 0.5000 0.5379
Official IR 0.6600 0.6174 0.6450 0.6539
Results QA 0.7050 0.8085 0.6600 0.8075
SUM 0.5850 0.6200 0.5300 0.5927
All 0.6112 0.6118 0.5837 0.6093
Dev. All 0.6443 0.6699 0.6143 0.6559
Table 2: Empirical evaluation - results.
The results for our submitted runs are listed in Ta-
ble 2, including per-task scores. run1 is our full sys-
tem, denoted F . It was tuned on a random sample
of 100 sentences from the development set, result-
ing in ? = 0.6 and ? = 0.6242 (entailment thresh-
old). run2 is a lexical configuration, denoted L, in
which ? = 0 (lexical cost only), ? = 0.2375 and
the only inference rules used were WordNet Lexical
rules. We found that the higher accuracy achieved
by F as compared to L might have been merely due
to a lucky choice of threshold. Setting the threshold
to its optimal value with respect to the test set re-
sulted in an accuracy of 62.4% for F , and 62.9% for
135
L. This is also hinted by the very close average pre-
cision scores for both systems, which do not depend
on the threshold. The last row in the table shows
the results obtained for 7/8 of the development set
that was not used for tuning, denoted Dev, using the
same parameter settings. Again, F performs bet-
ter than L. F is still better when using an optimal
threshold (which increases accuracy up to 65.3% for
F and 63.9% for L. Overall, F does not show yet a
consistent significant improvement over L.
Initial analysis of the results (based on Dev) sug-
gests that the coverage of the current rules is still
rather low. Without approximate matching (h must
be fully proved using the entailment rules) the re-
call is only 4.3%, although the precision (92%) is
encouraging. Lexical-syntactic rules were applied
in about 3% of the attempted proofs, and in most
cases involved only morpho-syntactic canonization,
with no lexical variation. As a result, entailment was
determined mainly by the cost function. Entailment
rules managed to reduce the cost in about 30% of the
attempted proofs.
We have qualitatively analyzed a subset of false
negative cases, to determine whether failure to com-
plete the proof is due to deficient components of
the system or due to higher linguistic and knowl-
edge levels. For each pair, we assessed the reasoning
steps a successful derivation of h from t would take.
We classified each pair according to the most de-
manding type of reasoning step it would require. We
allowed rules that are presently unavailable in our
system, as long as they are similar in power to those
that are currently available. We found that while
the single dominant cause for proof failure is lack
of world knowledge, e.g. the king?s son is a mem-
ber of the royal family, the combination of miss-
ing lexical-syntactic rules and parser failures equally
contributed to proof failure.
9 Conclusion
We defined a novel framework for semantic infer-
ence at the lexical-syntactic level, which allows a
unified representation of a wide variety of inference
knowledge. In order to reach reasonable recall on
RTE data, we found that we must scale our rule ac-
quisition, mainly by improving methods for auto-
matic rule learning.
Acknowledgments
We are grateful to Cleo Condoravdi for making the
polarity lexicon developed at PARC available for
this research. We also wish to thank Ruslan Mitkov,
Richard Evans, and Viktor Pekar from University of
Wolverhampton for running the MARS system for
us. This work was partially supported by ISF grant
1095/05, the IST Programme of the European Com-
munity under the PASCAL Network of Excellence
IST-2002-506778, the Israel Internet Association
(ISOC-IL) grant 9022 and the ITC-irst/University of
Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In AAAI (to appear).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Dekang Lin and Patrik Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 4(7):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. Nomlex: A lexicon of nominal-
izations. In EURALEX.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of CICLing.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of ANLP.
Tal Ron. 2006. Generating entailment rules based on
online lexical resources. Master?s thesis, Computer
Science Department, Bar-Ilan University, Ramat-Gan,
Israel.
136
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 6?9, Dublin, Ireland, August 23-29 2014.
Claims on demand ? an initial demonstration of a system for automatic 
detection and polarity identification of context dependent claims in 
massive corpora  
 
Ehud Aharoni Carlos Alzate Roy Bar-Haim Yonatan Bilu 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Dublin Research 
Lab, Ireland 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Lena Dankin Iris Eiron Daniel Hershcovich Shay Hummel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Mitesh Khapra Tamar Lavee1 Ran Levy Paul Matchen 
IBM Bangalore Re-
search Lab, India 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM YKT Research 
Lab, US 
Anatoly Polnarov Vikas Raykar Ruty Rinott Amrita Saha 
Hebrew University, 
Jerusalem, Israel 
IBM Bangalore Re-
search Lab, India 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Bangalore Re-
search Lab, India 
Naama Zwerdling David Konopnicki Dan Gutfreund Noam Slonim2 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
IBM Haifa Research 
Lab, Haifa, Israel 
Abstract 
While discussing a concrete controversial topic, most humans will find it challenging to swiftly raise a 
diverse set of convincing and relevant claims that should set the basis of their arguments. Here, we dem-
onstrate the initial capabilities of a system that, given a controversial topic, can automatically pinpoint 
relevant claims in Wikipedia, determine their polarity with respect to the given topic, and articulate them 
per the user's request. 
1 Introduction 
The ability to argue in a persuasive manner is an important aspect of human interaction that naturally 
arises in various domains such as politics, marketing, law, and health-care. Furthermore, good decision 
making relies on the quality of the arguments being presented and the process by which they are re-
solved. Thus, it is not surprising that argumentation has long been a topic of interest in academic re-
search, and different models have been proposed to capture the notion of an argument (Freeley and 
Steinberg, 2008). 
A fundamental component which is common to all these models is the concept of claim (or conclu-
sion). Specifically, at the heart of every argument lies a single claim, which is the assertion the argu-
ment aims to prove. Given a concrete topic, or context, most humans will find it challenging to swiftly 
raise a diverse set of convincing and relevant claims that should set the basis of their arguments.  
                                                    
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
 
1 Present affiliation: Yahoo! 
2 Corresponding author, at noams@il.ibm.com 
 
6
In this work we demonstrate the initial capabilities of a system that, given a controversial topic, can 
automatically pinpoint relevant claims in Wikipedia, determine their polarity with respect to the given 
topic, and articulate them per the user's request. 
 
2 Basic concepts and associated challenges 
We define and rely on the following two concepts:  
Topic: Short, usually controversial statement that defines the subject of interest.  
Context Dependent Claim (CDC): General, and concise statement, that directly supports or contests 
the given Topic. 
Given these definitions, as well as a few more detailed criteria to reduce the variability in the manu-
ally labeled data, human labelers were asked to detect CDCs for a diverse set of Topics, in relevant 
Wikipedia articles.  The collected data were used to train and assess the performance of the statistical 
models that underlie our system. These data are freely available for academic research (Aharoni et al 
2014). 
The distinction between a CDC and other related texts can be quite subtle, as illustrated in Table 1.  
For example, automatically distinguishing a CDC like S1 from a statement that simply defines a rele-
vant concept like S4, from a claim which is not relevant enough to the given Topic like S5, from a 
statement like S6 that merely repeats the given Topic in different words, or from a statement that repre-
sents a relevant claim which is not general enough like S7, is clearly challenging. Further, CDCs can 
be of different flavors, ranging from factual assertions like S1 to statements that are more of a matter of 
opinion (Pang and Lee 2008) like S2, adding to the complexity of the task. Moreover, our data suggest 
that even if one focuses on Wikipedia articles that are highly relevant to the given Topic, only ?2% of 
their sentences include CDCs.  
Furthermore, since CDCs are by definition concise statements, they typically do not span entire 
Wikipedia sentences but rather sub-sentences. This is illustrated in Table 2. There are many optional 
boundaries to consider when trying to identify the exact boundaries of a CDC within a typical Wikipe-
dia sentence. This task further complicates the CDC detection problem. Thus, we are faced with a large 
number of candidate CDCs, of which only a tiny fraction represents positive examples that might be 
quite reminiscent of some of the negative examples. Finally, automatically determining the correct 
Pro/Con polarity of a candidate CDC with respect to the Topic poses additional unique challenges. 
Nonetheless, by breaking the problem into a set of modular tangible problems and by employing vari-
ous techniques - specifically designed to the problems at hand - we obtain promising results, demon-
strated by the capabilities of our system. 
 
Topic The sale of violent video games to minors should be banned 
(Pro) CDC S1: Violent video games can increase children?s aggression 
(Pro) CDC S2: Video game publishers unethically train children in the use of weapons 
Note, that a valid CDC is not necessarily factual.  
(Con) CDC S3: Violent games affect children positively 
Invalid 
CDC 1 
S4: Video game addiction is excessive or compulsive use of computer and video games 
that interferes with daily life. 
This statement defines a concept relevant to the Topic, not a relevant claim.  
Invalid 
CDC 2 
S5: Violent TV shows just mirror the violence that goes on in the real world.  
This statement is not relevant enough to the Topic. 
Invalid 
CDC 3 
S6: Violent video games should not be sold to children. 
This statement simply repeats the Topic, and thus is not considered a valid CDC.  
Invalid 
CDC 4 
S7: ?Doom? has been blamed for nationally covered school shooting. 
This statement is not general enough to represent a CDC, as it focuses on a specific 
single video game. 
Table 1. Examples of CDCs and invalid CDCs.  
 
 
7
  
 
Because violence in video games is interactive and not passive, critics such as Dave Grossman and 
Jack Thompson argue that violence in games  hardens children to unethical acts, calling first-person 
shooter games ``murder simulators'', although no conclusive evidence has supported this belief. 
Table 2. A CDC is often only a small part of a single Wikipedia sentence - e.g., the part marked in bold 
in this example. 
 
 
Figure 1. High level architecture of the demonstrated system. 
3 High Level Architecture 
The demonstrated system relies on a cascade of engines, depicted in Figure 1. In general, these engines 
rely on various IR, NLP and ML technologies, as well as different resources and lexicons like 
WordNet (Miller, 1995). Some engines are more mature than others, and, correspondingly, already 
employ a complex inner architecture, that will be discussed in more detail elsewhere. Given a Topic, 
the Topic Analysis engine starts with initial semantic analysis of the Topic, aiming to identify the 
main concepts mentioned in this Topic and the sentiment towards each concept. Next, the CDC 
Oriented Article Retrieval engine employs IR and opinion mining techniques in order to retrieve 
Wikipedia articles that with high probability contain CDCs. Next, the CDC Detection engine relies on 
a combination of NLP and ML techniques to zoom-in within the retrieved articles and detect candidate 
CDCs. A detailed description of this engine can be found in (Levy et al 2014). Next, the CDC 
Pro/Con engine aims to automatically determine the polarity of the candidate CDC with respect to the 
given Topic by analyzing and contrasting the sentiment towards key concepts mentioned in the Topic 
and within the candidate CDC. Next, the CDC Equivalence engine uses techniques reminiscent of 
automatic paraphrase detection to identify whether two candidate CDCs are semantically equivalent, so 
to avoid redundancy in the generated output. Finally, the CDC Refinement engine aims to improve 
the precision of the generated output, based on the results collected thus far; e.g., using a simple rule-
based approach, we remove candidate CDCs for which the predicted Pro/Con polarity has low 
confidence. The remaining predictions are sent to the Text To Speech engine that articulates the top 
CDC predictions at the user's request.    
4 Summary 
Given a Topic, the demonstrated system is currently focused on detecting and articulating relevant 
CDCs. Combining this system with technologies that could automatically detect evidence to support 
these CDCs, may give rise to a new generation of automatic argumentation systems. In principle, such 
systems may swiftly detect relevant CDCs in massive corpora, and support these CDCs with evidence 
detected within other articles, or even within entirely different corpora, ending up with automatically 
8
generated arguments that were never explicitly proposed before in this form by humans. The system 
described herein represents an important step in pursuing this vision. 
Acknowledgments 
We would like to thank our colleagues in the IBM debating technologies team who participate in 
generating more advanced versions of the system presented in this work, and further continue to 
contribute to essential aspects of this project. These include Priyanka Agrawal, Indrajit Bhattacharya, 
Feng Cao, Lea Deleris, Francesco Dinuzzo, Liat Ein-Dor, Ron Hoory, Hui Jia Zhu, Qiong Kai Xu, 
Abhishek Kumar, Ofer Lavi, Naftali Liberman, Yosi Mass, Yuan Ni, Asaf Rendel, Haggai Roitman, 
Bogdan Sacaleanu, Dafna Sheinwald, Eyal Shnarch, Mathieu Sinn, Orith Toledo-Ronen, Doron 
Veltzer and Yoav Katz. 
References 
Austin J. Freeley and David L. Steinberg. 2008. Argumentation and Debate. Wadsworth, Belmont, 
California. 
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel Hershcovich, Ran Levy, Ruty Rinott, Dan Gut-
freund, and Noam Slonim. 2014. "A Benchmark Dataset for Automatic Detection of Claims and 
Evidence in the Context of Controversial Topics", in Proceedings of the First Workshop on Argu-
mentation and Computation, ACL 2014, to appear. 
Bo Pang and Lillian Lee. 2008. "Opinion mining and sentiment analysis" in Foundations and Trends in 
Information Retrieval, Vol. 2, pp. 1-135. 
George A Miller. 1995. " WordNet: A Lexical Database for English" in Communications of the ACM, 
Vol. 38, pp. 29-41. 
Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni and Noam Slonim. 2014. ?Context De-
pendent Claim Detection.?  In Proceedings of the 25th International Conference on Computational 
Linguistics, COLING 2014, to appear.   
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1310?1319,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying and Following Expert Investors in Stock Microblogs
1Roy Bar-Haim, 1Elad Dinur, 1,2Ronen Feldman, 1Moshe Fresko and 1Guy Goldstein
1Digital Trowel, Airport City, Israel
2School of Business Administration, The Hebrew University of Jerusalem, Jerusalem, Israel
{roy, moshe}@digitaltrowel.com, ronen.feldman@huji.ac.il
Abstract
Information published in online stock invest-
ment message boards, and more recently in
stock microblogs, is considered highly valu-
able by many investors. Previous work fo-
cused on aggregation of sentiment from all
users. However, in this work we show that it
is beneficial to distinguish expert users from
non-experts. We propose a general framework
for identifying expert investors, and use it as a
basis for several models that predict stock rise
from stock microblogging messages (stock
tweets). In particular, we present two methods
that combine expert identification and per-user
unsupervised learning. These methods were
shown to achieve relatively high precision in
predicting stock rise, and significantly outper-
form our baseline. In addition, our work pro-
vides an in-depth analysis of the content and
potential usefulness of stock tweets.
1 Introduction
Online investment message boards such as Yahoo!
Finance and Raging Bull allow investors to share
trading ideas, advice and opinions on public com-
panies. Recently, stock microblogging services such
as StockTwits (which started as a filtering service
over the Twitter platform) have become very popu-
lar. These forums are considered by many investors
as highly valuable sources for making their trading
decisions.
This work aims to mine useful investment in-
formation from messages published in stock mi-
croblogs. We shall henceforth refer to these mes-
sages as stock tweets. Ultimately, we would like to
transform those tweets into buy and sell decisions.
Given a set of stock-related messages, this process
typically comprises two steps:
1. Classify each message as ?bullish? (having a
positive outlook on the stock), ?bearish? (hav-
ing a negative outlook on the stock), or neutral.
2. Make trading decisions based on these message
classifications.
Previous work on stock investment forums and
microblogs usually regarded the first step (message
classification) as a sentiment analysis problem, and
aligned bullish with positive sentiment and bearish
with negative sentiment. Messages were classified
by matching positive and negative terms from sen-
timent lexicons, learning from a hand-labeled set of
messages, or some combination of the two (Das and
Chen, 2007; Antweiler and Frank, 2004; Chua et al,
2009; Zhang and Skiena, 2010; Sprenger andWelpe,
2010). Trading decisions were made by aggregating
the sentiment for a given stock over all the tweets,
and picking stocks with strongest sentiment signal
(buying the most bullish stocks and short-selling the
most bearish ones).
Sentiment aggregation reflects the opinion of the
investors community as a whole, but overlooks the
variability in user expertise. Clearly, not all investors
are born equal, and if we could tell experts from non-
experts, we would reduce the noise in these forums
and obtain high-quality signals to follow. This pa-
per presents a framework for identifying experts in
stock microblogs by monitoring their performance
in a training period. We show that following the ex-
perts results in more precise predictions.
1310
Based on the expert identification framework, we
experiment with different methods for deriving pre-
dictions from stock tweets. While previous work
largely aligned bullishness with message sentiment,
our in-depth content analysis of stock tweets (to be
presented in section 2.2) suggests that this view is
too simplistic. To start with, one important dif-
ference between bullishness/bearishness and posi-
tive/negative sentiment is that while the former rep-
resents belief about the future, the latter may also
refer to the past or present. For example, a user re-
porting on making profit from a buying stock yester-
day and selling it today is clearly positive about the
stock, but does not express any prediction about its
future performance. Furthermore, messages that do
refer to the future differ considerably in their signif-
icance. A tweet reporting on buying a stock by the
user conveys a much stronger bullishness signal than
a tweet that merely expresses an opinion. Overall, it
would seem that judging bullishness is far more elu-
sive than judging sentiment.
We therefore propose and compare two alterna-
tive approaches that sidestep the complexities of as-
sessing tweets bullishness. These two approaches
can be viewed as representing two extremes. The
first approach restricts our attention to the most ex-
plicit signals of bullishness and bearishness, namely,
tweets that report actual buy and sell transactions
performed by the user. In the second approach we
learn directly the relation between tweets content
and stock prices, following previous work on pre-
dicting stock price movement from factual sources
such as news articles (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
This approach poses no restrictions on the tweets
content and avoids any stipulated tweet classifica-
tion. However, user-generated messages are largely
subjective, and their correlation with the stock prices
depends on user?s expertise. This introduces much
noise into the learning process. We show that by
making the learning user-sensitive we can improve
the results substantially. Overall, our work illus-
trates the feasibility of finding expert investors, and
the utility of following them.
2 Stock Tweets
2.1 Stock Tweets Language
Stock tweets, as Twitter messages in general, are
short textual messages of up to 140 characters. They
are distinguished by having one or more references
to stock symbols (tickers), prefixed by a dollar sign.
For instance, the stock of Apple, Inc. is referenced
as $AAPL. Two other noteworthy Twitter conven-
tions that are also found in stock tweets are hashtags,
user-defined labels starting with ?#?, and references
to other users, starting with ?@?. Table 1 lists some
examples of stock tweets.
As common with Twitter messages, stock tweets
are typically abbreviated and ungrammatical utter-
ances. The language is informal and includes many
slang expressions, many of which are unique to the
stock tweets community. Thus, many positive and
negative expressions common to stock tweets are not
found in standard sentiment lexicons. Their unique
language and terminology often make stock tweets
hard to understand for an outsider. Many words
are abbreviated and appear in several non-standard
forms. For example, the word bought may also ap-
pear as bot or bght, and today may appear as 2day.
Stock tweets also contain many sentiment expres-
sions which may appear in many variations, e.g.
wow, woooow, woooooooow and so on. These char-
acteristics make the analysis of stock tweets a par-
ticularly challenging task.
2.2 Content Analysis
A preliminary step of this research was an exten-
sive data analysis, aimed to gain better understand-
ing of the major types of content conveyed in stock
tweets. First, we developed a taxonomy of tweet
categories while reading a few thousands of tweets.
Based on this taxonomy we then tagged a sample
of 350 tweets to obtain statistics on the frequency
of each category. The sample contained only tweets
that mention exactly one ticker. The following types
of tweets were considered irrelevant:
? Tweets that express question. These tweets
were labeled as Question.
? Obscure tweets, e.g. ?$AAPL fat?, tweets
that contain insufficient information (e.g.
?http://url.com $AAPL?) and tweets that seem
1311
Example %
Fact
News $KFRC: Deutsche Bank starts at Buy 14.3%
Chart Pattern $C (Citigroup Inc) $3.81 crossed its 2nd Pivot Point Support
http://empirasign.com/s/x4c
10.9%
Trade bot back some $AXP this morning 12.9%
Trade Outcome Sold $CELG at 55.80 for day-trade, +0.90 (+1.6%)X 2.9%
Opinion
Speculation thinking of hedging my shorts by buying some oil. thinking of
buying as much $goog as i can in my IRA. but i need more doing,
less thinking.
4.0%
Chart Prediction http://chart.ly/wsy5ny $GS - not looking good for this one -
breaks this support line on volume will nibble a few short
12.9%
Recommendation $WFC if you have to own financials, WFC would be my choice.
http://fsc.bz/448 #WORDEN
1.7%
Sentiment $ivn is rocking 8.6%
Question $aapl breaking out but in this mkt should wait till close? 7.1%
Irrelevant $CLNE follow Mr. Clean $$ 24.9%
Table 1: Tweets categories and their relative frequencies
to contain no useful information (e.g ?Even
Steve Jobs is wrong sometimes... $AAPL
http://ow.ly/1Tw0Z?). These tweets were la-
beled Irrelevant.
The rest of the tweets were classified into two major
categories: Facts and Opinions.
Facts can be divided into four main subcategories:
1. News: such tweets are generally in the form of
a tweeted headline describing news or a current
event generally drawn from mass media. As
such they are reliable but, since the information
is available in far greater detail elsewhere, their
added value is limited.
2. Chart Pattern: technical analysis aims to pro-
vide insight into trends and emerging patterns
in a stock?s price. These tweets describe pat-
terns in the stock?s chart without the inclusion
of any predicted or projected movement, an im-
portant contrast to Chart Prediction, which is
an opinion tweet described below. Chart pat-
tern tweets, like news, are a condensed form of
information already available through more in-
depth sources and as such their added value is
limited.
3. Trade: reports an actual purchase or sale of a
stock by the user. We consider this as the most
valuable form of tweet.
4. Trade Outcome: provides details of an ?inverse
trade?, the secondary trade to exit the initial
position along with the outcome of the over-
all trade (profit/loss). The value of these tweets
is debatable since although they provide details
of a trade, they generally describe the ?exit?
transaction. This creates a dilemma for ana-
lysts since traders will often exit not because
of a perceived change in the stock?s potential
but as a result of many short-term trading ac-
tivities. For this reason trade outcome provides
a moderate insight into a user?s position which
should be viewed with some degree of caution.
Opinions can also be divided into four main subcat-
egories:
1. Speculation: provides individual predictions of
future events relating to a company or actions
of the company. These are amongst the least
reliable categories, as the individual user is typ-
ically unable to justify his or her insight into the
predicted action.
2. Chart Prediction: describes a user?s prediction
of a future chart movement based on technical
analysis of the stock?s chart.
3. Recommendation: As with analyst recommen-
dations, this category represents users who
summarize their understanding and insight into
1312
a stock with a simple and effective recommen-
dation to take a certain course of action with
regard to a particular share. Recommendation
is the less determinate counterpart to Trade.
4. Sentiment: These tweets express pure senti-
ment toward the stock, rather than any factual
content.
Table 1 shows examples for each of the tweet cate-
gories, as well as their relative frequency in the ana-
lyzed sample.
3 An Expert Finding Framework
In this section we present a general procedure for
finding experts in stock microblogs. Based on this
procedure, we will develop in the next sections sev-
eral models for extracting reliable trading signals
from tweets.
We assume that a stock tweet refers to exactly one
stock, and therefore there is a one-to-one mapping
between tweets and stocks. Other tweets are dis-
carded. We define expertise as the ability to pre-
dict stock rise with high precision. Thus, a user is
an expert if a high percentage of his or her bullish
tweets is followed by a stock rise. In principle, we
could analogously follow bearish tweets, and see if
they are followed by a stock fall. However, bearish
tweets are somewhat more difficult to interpret: for
example, selling a share may indicate a negative out-
look on the stock, but it may also result from other
considerations, e.g. following a trading strategy that
holds the stock for a fixed period (cf. the discussion
on Trade Outcome tweets in the previous section).
We now describe a procedure that determines
whether a user u is an expert. The procedure re-
ceives a training set T of tweets posted by u, where
each tweet is annotated with its posting time. It is
also given a classifier C, which classifies each tweet
as bullish or not bullish (either bearish or neutral).
The procedure first applies the classifier C to iden-
tify the bullish tweets in T . It then determines the
correctness of each bullish tweet. Given a tweet t,
we observe the price change of the stock referenced
by t over a one day period starting at the next trading
day. The exact definition of mapping tweets to stock
prices is given in section 5.1. A one-day holding
period was chosen as it was found to perform well
in previous works on tweet-based trading (Zhang
and Skiena, 2010; Sprenger and Welpe, 2010), in
particular for long positions (buy transactions). A
bullish tweet is considered correct if it is followed
by a stock rise, and as incorrect otherwise1. Given a
set of tweets, we define its precision as the percent-
age of correct tweets in the set. Let Cu, Iu denote
the number of correct and incorrect bullish tweets
of user u, respectively. The precision of u?s bullish
tweets is therefore:
Pu =
Cu
Cu + Iu
Let Pbl be the baseline precision. In this work we
chose the baseline precision to be the proportion of
tweets that are followed by a stock rise in the whole
training set (including all the users). This represents
the expected precision when picking tweets at ran-
dom. Clearly, if Pu ? Pbl then u is not an expert.
If Pu > Pbl, we apply the following statistical test
to assess whether the difference is statistically sig-
nificant. First, we compute the expected number of
correct and incorrect transactions Cbl, Ibl according
to the baseline:
Cbl = Pbl ? (Cu + Iu)
Ibl = (1? Pbl)? (Cu + Iu)
We then compare the observed counts (Cu, Iu) to
the expected counts (Cbl, Ibl), using Pearson?s Chi-
square test. Since it is required for this test that
Cbl and Ibl are at least 5, cases that do not meet
this requirement are discarded. If the resulting p-
value satisfies the required significance level ?, then
u is considered an expert. In this work we take
? = 0.05. Note that since the statistical test takes
into account the number of observations, it will re-
ject cases where the number of the observations is
very small, even if the precision is very high. The
output of the procedure is a classification of u as
expert/non-expert, as well as the p-value (for ex-
perts). The expert finding procedure is summarized
in Algorithm 1.
In the next two sections we propose several alter-
natives for the classifier C.
1For about 1% of the tweets the stock price did not change
in the next trading day. These tweets are also considered correct
throughout this work.
1313
Algorithm 1 Determine if a user u is an expert
Input: set of tweets T posted by u, bullishness
classifier C, baseline probability Pbl, significance
level ?
Output: NON-EXPERT/(EXPERT, p-value)
Tbullish ? tweets in T classified by C as bullish
Cu ? 0 ; Iu ? 0
for each t ? Tbullish do
if t is followed by a stock rise then
Cu++
else
Iu++
end if
end for
Pu = CuCu+Iuif Pu ? Pbl then
return NON-EXPERT
else
Cbl ? Pbl ? (Cu + Iu)
Ibl ? (1? Pbl)? (Cu + Iu)
p? ChiSquareTest(Cu, Iu, Cbl, Ibl)
if p > ? then
return NON-EXPERT
else
return (EXPERT, p)
end if
end if
4 Following Explicit Transactions
The first approach we attempt for classifying bullish
(and bearish) tweets aims to identify only tweets that
report buy and sell transactions (that is, tweets in
the Trade category). According to our data analysis
(reported in section 2.2), about 13% of the tweets
belong to this category. There are two reasons to
focus on these tweets. First, as we already noted,
actual transactions are clearly the strongest signal
of bulishness/bearishness. Second, the buy and sell
actions are usually reported using a closed set of
expressions, making these tweets relatively easy to
identify. A few examples for buy and sell tweets are
shown in Table 2.
While buy and sell transactions can be captured
reasonably well by a relatively small set of patterns,
the examples in Table 2 show that stock tweets have
sell sold sum $OMNI 2.14 +12%
buy bot $MSPD for earnings testing
new indicator as well.
sell Out 1/2 $RIMM calls @ 1.84
(+0.81)
buy added to $joez 2.56
buy I picked up some $X JUL 50 Puts@
3.20 for gap fill play about an hour
ago.
buy long $BIDU 74.01
buy $$ Anxiously sitting at the bid on
$CWCO @ 11.85 It seems the ask
and I are at an impasse. 20 min of
this so far. Who will budge? (not
me)
buy In 300 $GOOG @ 471.15.
sell sold $THOR 41.84 for $400 the
FreeFactory is rocking
sell That was quick stopped out $ICE
sell Initiated a short position in $NEM.
Table 2: Buy and sell tweets
their unique language for reporting these transac-
tions, which must be investigated in order to come
by these patterns. Thus, in order to develop a clas-
sifier for these tweets, we created a training and test
corpora as follows. Based on our preliminary anal-
ysis of several thousand tweets, we composed a vo-
cabulary of keywords which trade tweets must in-
clude2. This vocabulary contained words such as in,
out, bot, bght, sld and so on. Filtering out tweets that
match none of the keywords removed two thirds of
the tweets. Out of the remaining tweets, about 5700
tweets were tagged. The training set contains about
3700 tweets, 700 of which are transactions. The test
set contains about 2000 tweets, 350 of which are
transactions.
Since the transaction tweets can be characterized
by a closed set of recurring patterns, we developed
a classifier that is based on a few dozens of man-
ually composed pattern matching rules, formulated
as regular expressions. The classifier works in three
stages:
1. Normalization: The tweet is transformed into
a canonical form. For example, user name
2That is, we did not come across any trade tweet that does
not include at least one of the keywords in the large sample we
analyzed, so we assume that such tweets are negligible.
1314
Dataset Transaction P R F1
Train Buy 94.0% 84.0% 0.89Sell 96.0% 83.0% 0.89
Test Buy 85.0% 70.0% 0.77Sell 88.5% 79.0% 0.84
Table 3: Results for buy/sell transactition classifier. Pre-
cision (P), Recall (R), and F-measure (F1) are reported.
is transformed into USERNAME; ticker name
is transformed into TICKER; buy, buying,
bought, bot, bght are transformed into BUY,
and so on.
2. Matching: Trying to match one of the buy/sell
patterns in the normalized tweet.
3. Filtering: Filtering out tweets that match ?dis-
qualifying? patterns. The simplest examples
are a tweet starting with an ?if? or a tweet con-
taining a question mark.
The results of the classifier on the train and test set
are summarized in Table 3. The results show that
our classifier identifies buy/sell transactions with a
good precision and a reasonable recall.
5 Unsupervised Learning from Stock
Prices
The drawback of the method presented in the pre-
vious section is that it only considers a small part
of the available tweets. In this section we propose
an alternative method, which considers all the avail-
able tweets, and does not require any tagged corpus
of tweets. Instead, we use actual stock price move-
ments as our labels.
5.1 Associating Tweets with Stock Prices
We used stock prices to label tweets as follows. Each
tweet message has a time stamp (eastern time), indi-
cating when it was published. Our policy is to buy
in the opening price of the next trading day (PB),
and sell on the opening price of the following trad-
ing day (PS). Tweets that are posted until 9:25 in the
morning (market hours begin at 9:30) are associated
with the same day, while those are posted after that
time are associated with the next trading date.
5.2 Training
Given the buy and sell prices associated with each
tweet, we construct positive and negative training
examples as follows: positive examples are tweets
where PS?PBPB ? 3%, and negative examples are
tweets where PS?PBPB ? ?3%.We used the SVM-light package (Joachims,
1999), with the following features:
? The existence of the following elements in the
message text:
? Reference to a ticker
? Reference to a user
? URL
? Number
? Hashtag
? Question mark
? The case-insensitive words in the message after
dropping the above elements.
? The 3, 4, 5 letter prefixes of each word.
? The name of the user who authored the tweet,
if it is a frequent user (at least 50 messages in
the training data). Otherwise, the user name is
taken to be ?anonymous?.
? Whether the stock price was up or down 1% or
more in the previous trading day.
? 2, 3, 4-word expressions which are typical to
tweets (that is, their relative frequency in tweets
is much higher than in general news text).
6 Empirical Evaluation
In this section we focus on the empirical task of
tweet ranking: ordering the tweets in the test set ac-
cording to their likelihood to be followed by a stock
rise. This is similar to the common IR task of rank-
ing documents according to their relevance. A per-
fect ranking would place all the correct tweets before
all the incorrect ones.
We present several ranking models that use the
expert finding framework and the bullishness classi-
fication methods discussed in the previous sections
as building blocks. The performance of these mod-
els is evaluated on the test set. By considering the
1315
precision at various points along the list of ranked
tweets, we can compare the precision-recall trade-
offs achieved by each model.
Before we discuss the ranking models and the em-
pirical results, we describe the datasets used to train
and test these models.
6.1 Datasets
Stock tweets were downloaded from the StockTwits
website3, during two periods: from April 25, 2010
to November 1, 2011, and from December 14, 2010
to February 3, 2011. A total of 700K tweets mes-
sages were downloaded. Tweets that do not contain
exactly one stock ticker (traded in NYSE or NAS-
DAQ) were filtered out. The remaining 340K tweets
were divided as follows:
? Development set: April 25, 2010 to August 31,
2010: 124K messages
? Held out set: September 1, 2010 to November
1, 2010: 110K messages
? Test set: December 14, 2010 to February 3,
2011: 106K messages
We consider the union of the development and held
out sets as our training set.
6.2 Ranking Models
6.2.1 Joint-All Model
This is our baseline model, as it does not attempt
to identify experts. It learns a single SVM model
as described in Section 5 from all the tweets in the
training set. It then applies the SVM model to each
tweet in the test set, and ranks them according to the
SVM classification score.
6.2.2 Transaction Model
This model finds expert users in the training set
(Algorithm 1), using the buy/sell classifier described
in Section 4. Tweets classified as buy are considered
bullish, and the rest are considered non-bullish. Ex-
pert users are ranked according to their p value (in
ascending order). The same classifier is then applied
to the tweets of the expert users in the test set. The
tweets classified as bullish are ordered according to
the ranking of their author (first all the bullish tweets
3stocktwits.com
of the highest-ranked expert user, then all the bullish
tweets of the expert ranked second, and so on).
6.2.3 Per-User Model
The joint all model suffers from the tweets of
non-experts twice: at training time, these tweets in-
troduce much noise into the training of the SVM
model. At test time, we follow these unreliable
tweets along with the more reliable tweets of the ex-
perts. The per-user model addresses both problems.
This model learns from the development set a sep-
arate SVM model Cu for each user u, based solely
on the user?s tweets. We then optimize the clas-
sification threshold of the learnt SVM model Cu
as follows. Setting the threshold to ? results in a
new classifier Cu,?. Algorithm 1 is applied to u?s
tweets in the held-out set (denoted Hu), using the
classifier Cu,?. For the ease of presentation, we de-
fine ExpertPValue(Hu, Cu,?,Pbl,?) as a function that
calls Algorithm 1 with the given parameters, and re-
turns the obtained p-value if u is an expert and 1
otherwise. We search exhaustively for the thresh-
old ?? for which this function is minimized (in other
words, the threshold that results in the best p-value).
The threshold of Cu is then set to ??, and the user?s
p-value is set to the best p-value found. If u is a
non-expert for all of the attempted ? values then u is
discarded. Otherwise, u is identified as an expert.
The rest of the process is similar to the transac-
tion model: the tweets of each expert u in the test
set are classified using the optimized per-user clas-
sifier Cu. The final ranking is obtained by sorting
the tweets that were classified as bullish according
to the p-value of their author. The per-user ranking
procedure is summarized in Algorithm 2.
6.2.4 Joint-Experts Model
The joint experts model makes use of the experts
identified by the per-user model, and builds a sin-
gle joint SVM model from the tweets of these users.
This results in a model that is trained on more exam-
ples than in the previous per-user method, but unlike
the joint all method, it learns only from high-quality
users. As with the joint all model, test tweets are
ranked according to the SVM?s score. However, the
model considers only the tweets of expert users in
the test set.
1316








 	

	

	

       




	

	
Figure 1: Empirical model comparison
Algorithm 2 Per-user ranking model
Input: dev. set D, held-out set H, test set S , base-
line probability Pbl, significance level ?
Output: A ranked listR of tweets in S
// Learning from the training set
E ? ? // set of expert users
for each user u do
Du ? u?s tweets in D
Cu ? SVM classifier learnt from Du
Hu ? u?s tweets inH
?? = argmin? ExpertPValue(Hu, Cu,?,Pbl,?)
Cu ? Cu,??
pu ?ExpertPValue(Hu, Cu,??,Pbl,?)if pu ? ? then
add u to E
end if
end for
// Classifying and ranking the test set
for each user u ? E do
Sbullish,u ? u?s tweets in S that were classified
as bullish by Cu
end for
R ? tweets in ?u Sbullish,u sorted by pureturn R
6.3 Results
Figure 1 summarizes the results obtained for the
various models. Each model was used to rank the
tweets according to the confidence that they predict
a positive stock price movement. Each data point
corresponds to the precision obtained for the first k
tweets ranked by the model, and the results for vary-
ing k values illustrate the precision/recall tradeoff of
the model. These data points were obtained as fol-
lows:
? For methods that learn a single SVM model
(joint all and joint experts), the graph was ob-
tained by decreasing the threshold of the SVM
classifier, at fixed intervals of 0.05. For each
threshold value, k is the number of tweets clas-
sified as bullish by the model.
? For methods that rank the users by their p value
and order the tweets accordingly (transaction
and per user), the i-th data point corresponds
to the cumulative precision for the tweets clas-
sified as bullish by the first i users. For the per
user method we show the cumulative results for
the first 20 users. For the transaction method
we show all the users that were identified as ex-
perts.
The random line is our baseline. It shows the ex-
pected results for randomly ordering the tweets in
the test set. The expected precision at any point is
equal to the percentage of tweets in the test set that
were followed by a stock rise, which was found to
be 51.4%.
We first consider the joint all method, which
learns a single model from all the tweets. The only
1317
Correct Incorrect P p
87 46 65.4 0.001
142 86 62.3 0.001
162 103 61.1 0.002
220 158 58.2 0.008
232 168 58.0 0.008
244 176 58.1 0.006
299 229 56.6 0.016
335 255 56.8 0.009
338 268 55.8 0.031
344 269 56.1 0.019
419 346 54.8 0.062
452 387 53.9 0.152
455 389 53.9 0.145
479 428 52.8 0.395
481 430 52.8 0.398
487 435 52.8 0.388
675 564 54.5 0.030
683 569 54.6 0.026
690 573 54.6 0.022
720 591 54.9 0.011
Table 4: Per user model: cumulative results for first 20
users. The table lists the number of correct and incorrect
tweets, the precision P and the significance level p.
per-user information available to this model is a fea-
ture fed to the SVM classifier, which, as we found,
does not contribute to the results. Except for the
first 58 tweets, which achieved precision of 55%,
the precision quickly dropped to a level of around
52%, which is just a little better than the random
baseline. Next, we consider the transaction configu-
ration, which is based on detecting buy transactions.
Only 10 users were found to be experts according to
this method, and in the test period these users had a
total of 173 tweets. These 173 tweets achieve good
precision (57.1% for the first 161 tweets, and 54.9%
for the first 173 tweets). However this method re-
sulted in a low number of transactions. This happens
because it is able to utilize only a small fraction of
the tweets (explicit buy transactions).
Remarkably, per user and joint experts, the two
methods which rely on identifying the experts via
unsupervised learning are by far the best methods.
Both models seem to have comparable performance,
where the results of the join experts model are some-
what smoother, as expected. Table 4 shows cumu-
lative results for the first 20 users in the per-user
model. The results show that this model achieves
good precision for a relatively large number of
tweets, and for most of the data points reported in the
table the results significantly outperform the base-
line (as indicated by the p value). Overall, these re-
sults show the effectiveness of our methods for find-
ing experts through unsupervised learning.
7 Related Work
A growing body of work aims at extracting senti-
ment and opinions from tweets, and exploit this in-
formation in a variety of application domains. Davi-
dov et al (2010) propose utilizing twitter hash-
tag and smileys to learn enhanced sentiment types.
O?Connor et al (2010) propose a sentiment detec-
tor based on Twitter data that may be used as a re-
placement for public opinion polls. Bollen et al
(2011) measure six different dimensions of public
mood from a very large tweet collection, and show
that some of these dimensions improve the predica-
tion of changes in the Dow Jones Industrial Average
(DJIA).
Sentiment analysis of news articles and financial
blogs and their application for stock prediction were
the subject of several studies in recent years. Some
of these works focus on document-level sentiment
classification (Devitt and Ahmad, 2007; O?Hare et
al., 2009). Other works also aimed at predicting
stock movement (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
All these methods rely on predefined sentiment lex-
icons, manually classified training texts, or their
combination. Lavrenko et al (2000), Koppel and
Shtrimberg (2004), and Schumaker and Chen (2010)
exploit stock prices for training, and thus save the
need in supervised learning.
Previous work on stock message boards include
(Das and Chen, 2007; Antweiler and Frank, 2004;
Chua et al, 2009). (Sprenger andWelpe, 2010) is, to
the best of our knowledge, the first work to address
specifically stock microblogs. All these works take
a similar approach for classifying message bullish-
ness: they train a classifier (Na??ve Bayes, which Das
and Chen combined with additional classifiers and
a sentiment lexicon, and Chua et al presented im-
provement for) on a collection of manually labeled
messages (classified into Buy, Sell, Hold). Interest-
ingly, Chua et al made use of an Australian mes-
1318
sage board (HotCopper), where, unlike most of the
stock message boards, these labels are added by the
message author. Another related work is (Zhang and
Skiena, 2010), who apply lexicon-based sentiment
analysis to several sources of news and blogs, in-
cluding tweets. However, their data set does not in-
clude stock microblogs, but tweets mentioning the
official company name.
Our work differs from previous work on stock
messages in two vital aspects. Firstly, these works
did not attempt to distinguish between experts and
non-expert users, but aggregated the sentiment over
all the users when studying the relation between sen-
timent and the stock market. Secondly, unlike these
works, our best-performing methods are completely
unsupervised, and require no manually tagged train-
ing data or sentiment lexicons.
8 Conclusion
This paper investigated the novel task of finding ex-
pert investors in online stock forums. In particular,
we focused on stock microblogs. We proposed a
framework for finding expert investors, and exper-
imented with several methods for tweet classifica-
tion using this framework. We found that combin-
ing our framework with user-specific unsupervised
learning allows us to predict stock price movement
with high precision, and the results were shown to be
statistically significant. Our results illustrate the im-
portance of distinguishing experts from non-experts.
An additional contribution of this work is an in-
depth analysis of stock tweets, which sheds light on
their content and its potential utility.
In future work we plan to improve the features of
the SVM classifier, and further investigate the use-
fulness of our approach for trading.
References
Werner Antweiler and Murray Z. Frank. 2004. Is all
that talk just noise? the information content of in-
ternet stock message boards. Journal of Finance,
59(3):1259?1294.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for in-
ternet stock message boards. In Proceedings of the
Australasian Language Technology Association Work-
shop 2009.
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375?1388.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10. Association for Computational Linguis-
tics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 984?991.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? Let the market decide. In Proceedings
of the AAAI Spring Symposium on Exploring Attitude
and Affect in Text: Theories and Applications.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Min-
ing of concurrent text and time series. In Proceedings
of the 6th ACM SIGKDD Int?l Conference on Knowl-
edge Discovery and Data Mining Workshop on Text
Mining.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
Neil O?Hare, Michael Davy, Adam Bermingham, Paul
Ferguson, Pvraic Sheridan, Cathal Gurrin, and Alan F
Smeaton. 2009. Topic-dependent sentiment analysis
of financial blogs. In TSA?09 - 1st International CIKM
Workshop on Topic-Sentiment Analysis for Mass Opin-
ion Measurement.
Robert P. Schumaker and Hsinchun Chen. 2010. A dis-
crete stock price prediction engine based on financial
news. Computer, 43:51?56.
Timm O. Sprenger and Isabell M. Welpe. 2010. Tweets
and trades: The information content of stock mi-
croblogs. Technical report, TUM School of Manage-
ment, December. working paper.
Wenbin Zhang and Steven Skiena. 2010. Trading
strategies to exploit blog and news sentiment. In
ICWSM?10.
1319
