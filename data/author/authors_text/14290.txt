Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 22?31,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment
Thomas Schoenemann
Heinrich-Heine-Universita?t Du?sseldorf, Germany
Universita?tsstr. 1
40225 Du?sseldorf, Germany
Abstract
We derive variants of the fertility based
models IBM-3 and IBM-4 that, while
maintaining their zero and first order pa-
rameters, are nondeficient. Subsequently,
we proceed to derive a method to com-
pute a likely alignment and its neighbors
as well as give a solution of EM training.
The arising M-step energies are non-trivial
and handled via projected gradient ascent.
Our evaluation on gold alignments shows
substantial improvements (in weighted F-
measure) for the IBM-3. For the IBM-
4 there are no consistent improvements.
Training the nondeficient IBM-5 in the
regular way gives surprisingly good re-
sults.
Using the resulting alignments for phrase-
based translation systems offers no clear
insights w.r.t. BLEU scores.
1 Introduction
While most people think of the translation and
word alignment models IBM-3 and IBM-4 as in-
herently deficient models (i.e. models that assign
non-zero probability mass to impossible events),
in this paper we derive nondeficient variants main-
taining their zero order (IBM-3) and first order
(IBM-4) parameters. This is possible as IBM-3
and IBM-4 are very special cases of general log-
linear models: they are properly derived by the
chain rule of probabilities. Deficiency is only in-
troduced by ignoring a part of the history to be
conditioned on in the individual factors of the
chain rule factorization. While at first glance this
seems necessary to obtain zero and first order de-
Figure 1: Plot of the negative log. likelihoods
(the quantity to be minimized) arising in training
deficient and nondeficient models (for Europarl
German | English, training scheme 15H53545).
1/3/4=IBM-1/3/4, H=HMM, T=Transfer iteration.
The curves are identical up to iteration 11.
Iteration 11 shows that merely 5.14% of the
(HMM) probability mass are covered by the
Viterbi alignment and its neighbors. With deficient
models (and deficient empty words) the final neg-
ative log likelihood is higher than the initial HMM
one, with nondeficient models it is lower than for
the HMM, as it should be for a better model.
pendencies, we show that with proper renormal-
ization all factors can be made nondeficient.
Having introduced the model variants, we pro-
ceed to derive a hillclimbing method to compute
a likely alignment (ideally the Viterbi alignment)
and its neighbors. As for the deficient models, this
plays an important role in the E-step of the sub-
sequently derived expectation maximization (EM)
training scheme. As usual, expectations in EM are
approximated, but we now also get non-trivial M-
step energies. We deal with these via projected
gradient ascent.
22
The downside of our method is its resource con-
sumption, but still we present results on corpora
with 100.000 sentence pairs. The source code of
this project is available in our word alignment soft-
ware RegAligner1, version 1.2 and later.
Figure 1 gives a first demonstration of how
much the proposed variants differ from the stan-
dard models by visualizing the resulting negative
log likelihoods2, the quantity to be minimized in
EM-training. The nondeficient IBM-4 derives a
lower negative log likelihood than the HMM, the
regular deficient variant only a lower one than
the IBM-1. As an aside, the transfer iteration
from HMM to IBM3 (iteration 11) reveals that
only 5.14% of the probability mass3 are preserved
when using the Viterbi alignment and its neighbors
instead of all alignments.
Indeed, it is widely recognized that ? with
proper initialization ? fertility based models out-
perform sequence based ones. In particular, se-
quence based models can simply ignore a part of
the sentence to be conditioned on, while fertility
based models explicitly factor in a probability of
words in this sentence to have no aligned words
(or any other number of aligned words, called the
fertility). Hence, it is encouraging to see that the
nondeficient IBM-4 indeed derives a higher likeli-
hood than the sequence based HMM.
Related Work Today?s most widely used mod-
els for word alignment are still the models IBM
1-5 of Brown et al (1993) and the HMM of Vo-
gel et al (1996), thoroughly evaluated in (Och
and Ney, 2003). While it is known that fertility-
based models outperform sequence-based ones,
the large bulk of word alignment literature follow-
ing these publications has mostly ignored fertility-
based models. This is different in the present paper
which deals exclusively with such models.
One reason for the lack of interest is surely that
computing expectations and Viterbi alignments for
these models is a hard problem (Udupa and Maji,
2006). Nevertheless, computing Viterbi align-
1https://github.com/Thomas1205/RegAligner,
for the reported results we used a slightly earlier version.
2Note that the figure slightly favors IBM-1 and HMM as
for them the length J of the foreign sequence is assumed to
be known whereas IBM-3 and IBM-4 explicitly predict it.
3This number regards the corpus probability as in (9) to
the power of 1/S, i.e. the objective function in maximum
likelihood training. The number is not entirely fair as align-
ments where more than half the words align to the empty
word are assigned a probability of 0. Still, this is an issue
only for short sentences.
ments for the IBM-3 has been shown to often
be practicable (Ravi and Knight, 2010; Schoen-
emann, 2010).
Much work has been spent on HMM-based
formulations, focusing on the computationally
tractable side (Toutanova et al, 2002; Sumita et
al., 2004; Deng and Byrne, 2005). In addition,
some rather complex models have been proposed
that usually aim to replace the fertility based mod-
els (Wang and Waibel, 1998; Fraser and Marcu,
2007a).
Another line of models (Melamed, 2000; Marcu
and Wong, 2002; Cromie`res and Kurohashi, 2009)
focuses on joint probabilities to get around the
garbage collection effect (i.e. that for conditional
models, rare words in the given language align to
too many words in the predicted language). The
downside is that these models are computationally
harder to handle.
A more recent line of work introduces various
forms of regularity terms, often in the form of
symmetrization (Liang et al, 2006; Grac?a et al,
2010; Bansal et al, 2011) and recently by using
L0 norms (Vaswani et al, 2012).
2 The models IBM-3, IBM-4 and IBM-5
We begin with a short review of fertility-based
models in general and IBM-3, IBM-4 and IBM-
5 specifically. All are due to (Brown et al, 1993)
who proposed to use the deficient models IBM-3
and IBM-4 to initialize the nondeficient IBM-5.
For a foreign sentence f = fJ1 = (f1, . . . , fJ)
with J words and an English one e = eI1 =
(e1, . . . , eI) with I words, the (conditional) proba-
bility p(fJ1 |eI1) of getting the foreign sentence as a
translation of the English one is modeled by intro-
ducing the word alignment a as a hidden variable:
p(fJ1 |eI1) =
?
a
p(fJ1 ,a|eI1)
All IBM models restrict the space of alignments
to those where a foreign word can align to at most
one target word. The resulting alignment is then
written as a vector aJ1 , where each aj takes integral
values between 0 and I , with 0 indicating that fj
has no English correspondence.
The fertility-based models IBM-3, IBM-4
and IBM-5 factor the (conditional) probability
p(fJ1 , aJ1 |eI1) of obtaining an alignment and a
translation given an English sentence according to
the following generative story:
23
1. For i = 1, 2, . . . , I , decide on the number ?i
of foreign words aligned to ei. This number
is called the fertility of ei. Choose with prob-
ability p(?i|eI1,?i?11 ) = p(?i|ei).
2. Choose the number ?0 of unaligned words
in the (still unknown) foreign sequence.
Choose with probability p(?0|eI1,?I1) =
p(?0|
?I
i=1 ?i). Since each foreign word be-
longs to exactly one English position (includ-
ing 0), the foreign sequence is now known to
be of length J = ?Ii=0 ?i.
3. For each i = 1, 2, . . . , I , and k = 1, . . . ,?i
decide on
(a) the identity fi,k of the next foreign
word aligned to ei. Choose with probability
p(fi,k|eI1,?I0,di?11 , di,1, . . . , di,k?1, fi,k) =
p(fi,k|ei), where di comprises all di,k for
word i (see point b) below) and fi,k com-
prises all foreign words known at that point.
(b) the position di,k of the just gener-
ated foreign word fi,k, with probability
p(di,k|eI1,?I0,di?11 , di,1, . . . , di,k?1, fi,k, fi,k)
= p(di,k|ei,di?11 , di,1, . . . , di,k?1, fi,k, J).
4. The remaining ?0 open positions in the for-
eign sequence align to position 0. Decide
on the corresponding foreign words with
p(fd0,k |e0), where e0 is an artificial ?empty
word?.
To model the probability for the number of un-
aligned words in step 2, each of the?Ii=1 ?i prop-
erly aligned foreign words generates an unaligned
foreign word with probability p0, resulting in
p
(
?0
???
I?
i=1
?i
)
=
?
??
I?
i=1
?i
?0
?
??p?i0 (1?p0)(
?
i ?i)??0 ,
with a base probability p0 and the combinato-
rial coefficients
( n
k
)
= n!k!(n?k)! , where n! =?n
k=1 k denotes the factorial of n. The main dif-
ference between IBM-3, IBM-4 and IBM-5 is the
choice of probability model in step 3 b), called a
distortion model. The choices are now detailed.
2.1 IBM-3
The IBM-3 implements a zero order distortion
model, resulting in
p(di,k|i, J) .
Since most of the context to be conditioned on is
ignored, this allows invalid configurations to occur
with non-zero probability: some foreign positions
can be chosen several times, while others remain
empty. One says that the model is deficient. On
the other hand, the model for p(?0|?Ii=1 ?i) is
nondeficient, and in training this often results in
very high probabilities p0. To prevent this it is
common to make this model deficient as well (Och
and Ney, 2003), which improves performance im-
mensely and gives much better results than simply
fixing p0 in the original model.
As for each i the di,k can appear in any order
(i.e. need not be in ascending order), there are?I
i=1 ?i! ways to generate the same alignment aJ1
(where the ?i are the fertilities induced by aJ1 ).
In total, the IBM-3 has the following probability
model:
p(fJ1 , aJ1 |eI1) =
J?
j=1
[
p(fj |eaj ) ? p(j|aj , J)
]
(1)
? p
(
?0|
I?
i=1
?i
)
?
I?
i=1
?i! p(?i|ei) .
Reducing the Number of Parameters While
using non-parametric models p(j|i, J) is conve-
nient for closed-form M-steps in EM training,
these parameters are not very intuitive. Instead,
in this paper we use the parametric model
p(j|i, J) = p(j|i)?J
j=1 p(j|i)
(2)
with the more intuitive parameters p(j|i). The
arising M-step energy is addressed by projected
gradient ascent (see below).
These parameters are also used for the nondefi-
cient variants. Using the original non-parametric
ones can be handled in a very similar manner to
the methods set forth below.
2.2 IBM-4
The distortion model of the IBM-4 is a first order
one that generates the di,k of each English position
i in ascending order (i.e. for 1 < k ? ?i we have
di,k > di,k?1). There is then a one-to-one cor-
respondence between alignments aJ1 and (valid)
distortion parameters (di,k)i=1,...,I, k=1,...,?i and
therefore no longer a factor of?Ii=1 ?i! .
The IBM-4 has two sub-distortion models, one
for the first aligned word (k = 1) of an English po-
sition and one for all following words (k > 1, only
24
if ?i > 1). For position i, let [i]=arg max{i?|1?
i? < i,?i? > 0} denote4 the closest preceding En-
glish word that has aligned foreign words. The
aligned foreign positions of [i] are combined into
a center position [i], the rounded average of the
positions. Now, the distortion probability for the
first word (k = 1) is
p=1(di,1|[i],A(fi,1),B(e[i]), J) ,
where A gives the word class of a foreign word
and B the word class of an English word (there are
typically 50 classes per language, derived by ma-
chine learning techniques). The probability is fur-
ther reduced to a dependency on the difference of
the positions, i.e. p=1(di,1?[i] | A(fi,1),B(e[i])).
For k > 1 the model is
p>1(di,k|di,k?1,A(fi,k), J) ,
which is likewise reduced to p>1(di,k ?
di,k?1 | A(fi,k)). Note that in both difference-
based formulations the dependence on J has to
be dropped to get closed-form solutions of the
M-step in EM training, and Brown et al note
themselves that the IBM-4 can place words before
the start and after the end of the sentence.
Reducing Deficiency In this paper, we also in-
vestigate the effect of reducing the amount of
wasted probability mass by enforcing the depen-
dence on J by proper renormalization, i.e. using
p=1(j|j?,A(fi,1),B(e[i]), J) = (3)
p=1(j ? j?|A(fi,1),B(e[i]))?J
j??=1 p=1(j?? ? j?|A(fi,1),B(e[i]))
,
for the first aligned word and
p>1(j|j?,A(fi,k), J) = (4)
p>1(j ? j? | A(fi,k))?J
j??=1 p>1(j?? ? j? | A(fi,k))
for all following words, again handling the M-step
in EM training via projected gradient ascent. With
this strategy words can no longer be placed out-
side the sentence, but a lot of probability mass is
still wasted on configurations where at least one
foreign (or predicted) position j aligns to two or
more positions i, i? in the English (or given) lan-
guage (and consequently there are more unaligned
4If the set is empty, instead a sentence start probability
is used. Note that we differ slightly in notation compared to
(Brown et al, 1993).
source words than the generated ?0). Therefore,
here, too, the probability for ?0 has to be made
deficient to get good performance.
In summary, the base model for the IBM-4 is:
p(fJ1 , aJ1 |eI1) = p
(
?0|
I?
i=1
?i
)
(5)
?
J?
j=1
p(fj |eaj ) ?
I?
i=1
p(?i|ei)
?
?
i:?i>0
[
p=1(di,1 ?[i]|A(fi,1),B(e[i]))
?
?i?
k=2
p>1(di,k ? di,k?1|A(fi,k))
]
,
where empty products are understood to be 1.
2.3 IBM-5
We note in passing that the distortion model of the
IBM-5 is nondeficient and has parameters for fill-
ing the nth open gap in the foreign sequence given
that there are N positions to choose from ? see
the next section for exactly what positions one can
choose from. There is also a dependence on word
classes for the foreign language.
This is neither a zero order nor a first order de-
pendence, and in (Och and Ney, 2003) the first or-
der model of the IBM-4, though deficient, outper-
formed the IBM-5. The IBM-5 is therefore rarely
used in practice. This motivated us to instead re-
formulate IBM-3 and IBM-4 as nondeficient mod-
els. In our results, however, the IBM-5 gave sur-
prisingly good results and was often superior to all
variants of the IBM-4.
3 Nondeficient Variants of IBM-3 and
IBM-4
From now on we always enforce that for each po-
sition i the indices di,k are generated in ascending
order (di,k > di,k?1 for k > 1). A central con-
cept for the generation of di,k in step 3(b) is the
set of positions in the foreign sequence that are
still without alignment. We denote the set of these
positions by
Ji,k,J = {1, . . . , J} ? {di,k? | 1 ? k? < k}
?{di?,k? | 1 ? i? < i, 1 ? k? ? ?i?}
where the dependence on the various di?,k? is not
made explicit in the following.
It is tempting to think that in a nondeficient
model all members of Ji,k,J can be chosen for
25
di,k, but this holds only ?i = 1. Otherwise, the
requirement of generating the di,k in ascending or-
der prevents us from choosing the (?i?k) largest
entries inJi,k,J . For k > 1 we also have to remove
all positions smaller than di,k?1.
Let J ?ii,k,J denote the set where these positionshave been removed. With that, we can state the
nondeficient variants of IBM-3 and IBM-4.
3.1 Nondeficient IBM-3
For the IBM-3, we define the auxiliary quantity
q(di,k = j | i,J ?ii,k,J) =
{
p(j|i) if j ? J ?ii,k,J
0 else ,
where we use the zero order parameters p(j|i) we
also use for the standard (deficient) IBM-3, com-
pare (2). To get a nondeficient variant, it remains
to renormalize, resulting in
p(di,k = j|i,J ?ii,k,J) =
q(j|i,J ?ii,k,J)?J
j=1 q(j|i,J ?ii,k,J)
. (6)
Further, note that the factors ?i! now have to
be removed from (1) as the di,k are generated in
ascending order. Lastly, here we use the original
nondeficient empty word model p(?0|?Ii=1 ?i),
resulting in a totally nondeficient model.
3.2 Nondeficient IBM-4
With the notation set up, it is rather straightfor-
ward to derive a nondeficient variant of the IBM-
4. Here, there are the two cases k = 1 and k > 1.
We begin with the case k = 1. Abbreviating
? = A(fi,1) and ? = B(e[i]), we define the auxil-
iary quantity
q=1(di,1 = j|[i], ?, ?,J ?ii,k,J) = (7){
p=1(j ?[i]|?, ?) if j ? J ?ii,k,J
0 else ,
again using the - now first order - parameters
of the base model. The nondeficient distribution
p=1(di,1 = j|[i], ?, ?,J ?ii,k,J) is again obtainedby renormalization.
For the case k > 1, we abbreviate ? = A(fi,k)
and introduce the auxiliary quantity
q>1(di,k = j|di,k?1, ?,J ?ii,k,J) = (8){
p>1(j ? di,k?1|?) if j ? J ?ii,k,J
0 else ,
from which the nondeficient distribution
p>1(di,k=j|di,k?1, ?,J ?ii,k,J) is again obtained byrenormalization.
4 Training the New Variants
For the task of word alignment, we infer the pa-
rameters of the models using the maximum likeli-
hood criterion
max
?
S?
s=1
p?(fs|es) (9)
on a set of training data (i.e. sentence pairs s =
1, . . . , S). Here, ? comprises all base parameters
of the respective model (e.g. for the IBM-3 all
p(f |e), all p(?, e) and all p(j|i) ) and p? signifies
the dependence of the model on the parameters.
Note that (9) is truly a constrained optimization
problem as the parameters ? have to satisfy a num-
ber of probability normalization constraints.
When p?(?) denotes a fertility based model the
resulting problem is a non-concave maximization
problem with many local minima and no (known)
closed-form solutions. Hence, it is handled by
computational methods, which typically apply the
logarithm to the above function.
Our method of choice to attack the maximum
likelihood problem is expectation maximization
(EM), the standard in the field, which we explain
below. Due to non-concaveness the starting point
for EM is of extreme importance. As is common,
we first train an IBM-1 and then an HMM before
proceeding to the IBM-3 and finally the IBM-4.
As in the training of the deficient IBM-3 and
IBM-4 models, we approximate the expectations
in the E-step by a set of likely alignments, ideally
centered around the Viterbi alignment, but already
for the regular deficient variants computing it is
NP-hard (Udupa and Maji, 2006). A first task is
therefore to compute such a set. This task is also
needed for the actual task of word alignment (an-
notating a given sentence pair with an alignment).
4.1 Alignment Computation
For computing alignments, we use the common
procedure of hillclimbing where we start with an
alignment, then iteratively compute the probabili-
ties of all alignments differing by a move or a swap
(Brown et al, 1993) and move to the best of these
if it beats the current alignment.
Since we cannot ignore parts of the history and
still get a nondeficient model, computing the prob-
abilities of the neighbors cannot be handled in-
crementally (or rather only partially, for the dic-
tionary and fertility models). While this does in-
crease running times, in practice the M-steps take
longer than the E-steps.
26
For self-containment, we recall here that for an
alignment aJ1 applying the move aJ1 [j? i] results
in the alignment a?J1 defined by a?j = i and a?j?=aj?
for j? 6= j. Applying the swap aJ1 [j1 ? j2] results
in the alignment a?J1 defined by a?j1 =aj2 , a?j2 =aj1
and a?j? = aj? elsewhere. If aJ1 is the alignment
produced by hillclimbing, the move matrix m ?
IRJ?I+1 is defined bymj,i being the probability of
aJ1 [j ? i] as long as aj 6= i, otherwise 0. Likewise
the swap matrix s ? IRJ?J is defined as sj1,j2
being the probability of aJ1 [j1 ? j2] for aj1 6=aj2 ,
0 otherwise. The move and swap matrices are used
to approximate expectations in EM training (see
below).
4.2 Parameter Update
Naive Scheme It is tempting to account for the
changes in the model in hillclimbing, but to oth-
erwise use the regular M-step procedures (closed
form solution when not conditioning on J for the
IBM-4 and for the non-parametric IBM-3, other-
wise projected gradient ascent) for the deficient
models. However, we verified that this is not a
good idea: not only can the likelihood go down
in the process (even if we could compute expecta-
tions exactly), but these schemes also heavily in-
crease p0 in each iteration, i.e. the same problem
Och and Ney (2003) found for the deficient mod-
els. There is therefore the need to execute the M-
step properly, and when done the problem is in-
deed resolved.
Proper EM The expectation maximization
(EM) framework (Dempster et al, 1977; Neal and
Hinton, 1998) is a class of template procedures
(rather than a proper algorithm) that iteratively
requires solving the task
max
?k
S?
s=1
?
as
p?k?1(as|fs, es) log
(
p?k(fs,as|es)
)
(10)
by appropriate means. Here, ?k?1 are the parame-
ters from the previous iteration, while ?k are those
derived in the current iteration. Of course, here
and in the following the normalization constraints
on ? apply, as already in (9). On explicit request
of a reviewer we give a detailed account for our
setting here. Readers not interested in the details
can safely move on to the next section.
Details on EM For the corpora occurring in
practice, the function (10) has many more terms
than there are atoms in the universe. The trick is
that p?k(fs,as|es) is a product of factors, where
each factor depends on very few components of
?k only. Taking the logarithm gives a sum of
logarithms, and in the end we are left with the
problem of computing the weights of each factor,
which turn out to be expectations. To apply this
to the (deficient) IBM-3 model with parametric
distortion we simplify p?k?1(as|fs, es) = p(as)
and define the counts nf,e(as) = ?Jsj=1 ?(fsj , f) ?
?(esasj , e), n?,e(as) =
?Is
i=1 ?(esi , e) ??(?i(as),?)
and nj,i(as) = ?(asj , i). We also use short hand
notations for sets, e.g. {p(f |e)} is meant as the
set of all translation probabilities induced by the
given corpus. With this notation, after reordering
the terms problem (10) can be written as
max
{p(f |e)},{p(?|e)},{p(j|i)}
(11)
?
e,f
[ S?
s=1
?
as
p(as)nf,e(as)
]
log
(
p(f |e)
)
+
?
e,?
[ S?
s=1
?
as
p(as)n?,e(as)
]
log
(
p(?, e)
)
+
?
i,j
[ S?
s=1
?
as
p(as)nj,i(as)
]
log
(
p(j|i, J)
)
.
Indeed, the weights in each line turn out to be
nothing else than expectations of the respective
factor under the distribution p?k?1(as|fs, es) and
will henceforth be written as wf,e, w?,e and wj,i,J .
Therefore, executing an iteration of EM requires
first calculating all expectations (E-step) and then
solving the maximization problems (M-step). For
models such as IBM-1 and HMM the expectations
can be calculated efficiently, so the enormous sum
of terms in (10) is equivalently written as a man-
ageable one. In this case it can be shown5 that
the new ?k must have a higher likelihood (9) than
?k?1 (unless a stationary point is reached). In fact,
any ? that has a higher value in the auxiliary func-
tion (11) than ?k?1 must also have a higher like-
lihood. This is an important background for para-
metric models such as (2) where the M-step cannot
be solved exactly.
For IBM-3/4/5 computing exact expectations is
intractable (Udupa and Maji, 2006) and approx-
imations have to be used (in fact, even comput-
ing the likelihood for a given ? is intractable). We
5See e.g. the author?s course notes (in German), currently
http://user.phil-fak.uni-duesseldorf.de/
?tosch/downloads/statmt/wordalign.pdf.
27
use the common procedure based on hillclimbing
and the move/swap matrices. The likelihood is not
guaranteed to increase but it (or rather its approx-
imation) always did in each of the five run itera-
tions. Nevertheless, the main advantage of EM is
preserved: problem (11) decomposes into several
smaller problems, one for each probability distri-
bution since the parameters are tied by the nor-
malization constraints. The result is one problem
for each e involving all p(f |e), one for each e in-
volving all p(?|e) and one for each i involving all
p(j|i).
The problems for the translation probabilities
and the fertility probabilities yield the known stan-
dard update rules. The most interesting case is the
problem for the (parametric) distortion models. In
the deficient setting, the problem for each i is
max
{p(j|i)}
?
J
wi,j,J log
(
p(j|i)
?J
j?=1 p(j?|i)
)
In the nondeficient setting, we now drop the sub-
scripts i, k, J and the superscript ? from the sets
defined in the previous sections, i.e. we write J
instead of J ?i,k,J . The M-step problem is then
max
{p(j|i)}
Ei =
?
j
?
J :j?J
wj,i,J log
(
p(j|i,J )
)
,
where wj,i,J (with j ? J ) is the expectation for
aligning j to iwhen one can choose among the po-
sitions inJ , and with p(j|i,J ) as in (6). In princi-
ple there is an exponential number of expectations
wj,i,J . However, since we approximate expecta-
tions from the move and swap matrices, and hence
by O((I + J) ? J) alignments per sentence pair,
in the end we get a polynomial number of terms.
Currently we only consider alignments with (ap-
proximated) p?k?1(as|fs, es) > 10?6.
Importantly, the fact that we get separate M-step
problems for different i allows us to reduce mem-
ory consumption by using refined data structures
when storing the expectations.
For both the deficient and the nondeficient vari-
ants, the M-step problems for the distortion pa-
rameters p(j|i) are non-trivial, non-concave and
have no (known) closed form solutions. We ap-
proach them via the method of projected gradient
ascent (PGA), where the gradient for the nondefi-
cient problem is
?Ei
?p(j|i) =
?
J :j?J
[
wj,J
p(j|i) ?
?
j??J wj?,J?
j??J p(j?|i)
]
.
When running PGA we guarantee that the result-
ing {p(j|i)} has a higher function value Ei than
the input ones (unless a stationary point is input).
We stop when a cutoff criterion indicates a local
maximum or 250 iterations are used up.
Projected Gradient Ascent This method is
used in a couple of recent papers, notably (Schoen-
emann, 2011; Vaswani et al, 2012) and is briefly
sketched here for self-containment (see those pa-
pers for more details). To solve a maximization
problem
max
p(j|i)?0,?j p(j|i)=1
Ei({p(j|i)})
for some (differentiable) function Ei(?), one iter-
atively starts at the current point {pk(j|i)}, com-
putes the gradient ?Ei({pk(j|i)}) and goes to the
point
q(j|i) = pk(j|i) + ??Ei(pk(j|i)) , j = 1, . . . , J
for some step-length ?. This point is generally
not a probability distribution, so one computes the
nearest probability distribution
min
q?(j|i)?0,?j q?(j|i)=1
J?
j=1
(
q?(j|i)? q(j|i)
)2 ,
a step known as projection which we solve with
the method of (Michelot, 1986). The new dis-
tribution {q?(j|i)} is not guaranteed to have a
higher Ei(?), but (since the constraint set is a con-
vex one) a suitable interpolation of {pk(j|i)} and
{q?(j|i)} is guaranteed to have a higher value (un-
less {pk(j|i)} is a local maximum or minimum
of Ei(?)). Such a point is computed by back-
tracking line search and defines the next iterate
{pk+1(j|i)}.
IBM-4 When moving from the IBM-3 to the
IBM-4, only the last line in (11) changes. In
the end one gets two new kinds of problems, for
p=1(?) and p>1(?). For p=1(?) we have one prob-
lem for each foreign class ? and each English class
?, of the form
max
{p=1(j|j?,?,?)}
?
j,j?,J
wj,j?,J,?,? log
(
p=1(j|j?, ?, ?, J)
)
for reduced deficiency (with p=1(j|j?, ?, ?, J) as
in (3) ) and of the form
max
{p=1(j|j?,?,?)}
?
j,j?,J
wj,j?,J ,?,? log
(
p=1(j|j?, ?, ?,J )
)
28
Model Degree of Deficiency De|En En|De Es|En En|Es
HMM nondeficient (our) 73.8 77.6 77.4 76.1
IBM-3 full (GIZA++) 74.2 76.5 74.3 74.5
IBM-3 full (our) 75.6 79.2 75.2 73.7
IBM-3 nondeficient (our) 76.1 79.8 76.8 75.5
IBM-4, 1 x 1 word class full (GIZA++) 77.9 79.4 78.6 78.4
IBM-4, 1 x 1 word class full (our) 76.1 81.5 77.8 78.0
IBM-4, 1 x 1 word class reduced (our) 77.2 80.6 77.9 78.3
IBM-4, 1 x 1 word class nondeficient (our) 77.6 81.5 80.0 78.4
IBM-4, 50 x 50 word classes full (GIZA++) 78.6 80.4 79.3 79.3
IBM-4, 50 x 50 word classes full (our) 78.0 82.4 79.2 79.4
IBM-4, 50 x 50 word classes reduced (our) 78.5 82.1 79.2 79.0
IBM-4, 50 x 50 word classes nondeficient (our) 77.9 82.5 79.7 78.2
IBM-5, 50 word classes nondeficient (GIZA++) 79.4 81.1 80.0 79.5
IBM-5, 50 word classes nondeficient (our) 79.2 82.7 79.7 79.5
Table 1: Alignment accuracy (weighted F-measure times 100, ? = 0.1) on Europarl with 100.000
sentence pairs. Reduced deficiency means renormalization as in (3) and (4), so that words cannot be
placed before or after the sentence. For the IBM-3, the nondeficient variant is clearly best. For the
IBM-4 it is better in roughly half the cases, both with and without word classes.
for the nondeficient variant, with
p=1(j|j?, ?, ?,J ) based on (7).
For p>1(?) we have one problem per foreign
class ?, of the form
max
{p>1(j|j?,?)}
?
j,j?,J
wj,j?,J,? log
(
p>1(j|j?, ?, J)
)
for reduced deficiency, with p>1(j|j?, ?, J) based
on (4), and for the nondeficient variant it has the
form
max
{p>1(j|j?,?)}
?
j,j?,J
wj,j?,J ,? log
(
p>1(j|j?, ?,J )
)
,
with p>1(j|j?, ?,J ) based on (8). Calculating the
gradients is analogous to the IBM-3.
5 Experiments
We test the proposed methods on subsets of the
Europarl corpus for German and English as well
as Spanish and English, using lower-cased cor-
pora. We evaluate alignment accuracies on gold
alignments6 in the form of weighted F-measures
with ?=0.1, which performed well in (Fraser and
Marcu, 2007b). In addition we evaluate the effect
on phrase-based translation on one of the tasks.
We implement the proposed methods in our
own framework RegAligner rather than GIZA++,
6from (Lambert et al, 2005) and from
http://user.phil-fak.uni-duesseldorf.de/
?tosch/downloads.html.
which is only rudimentally maintained. Therefore,
we compare to the deficient models in our own
software as well as to those in GIZA++.
We run 5 iterations of IBM-1, followed by 5
iterations of HMM, 5 of IBM-3 and finally 5 of
IBM-4. The first iteration of the IBM-3 collects
counts from the HMM, and likewise the first iter-
ation of the IBM-4 collects counts from the IBM-
3 (in both cases the move and swap matrices are
filled with probabilities of the former model, then
theses matrices are used as in a regular model iter-
ation). A nondeficient IBM-4 is always initialized
by a nondeficient IBM-3. We did not set a fertility
limit (except for GIZA++).
Experiments were run on a Core i5 with 2.5
GHz and 8 GB of memory. The latter was the
main reason why we did not use still larger cor-
pora7. The running times for the entire training
were half a day without word classes and a day
with word classes. With 50 instead of 250 PGA it-
erations in all M-steps we get only half these run-
ning times, but the resulting F-measures deterio-
rate, especially for the IBM-4 with classes.
The running times of our implementation of the
IBM-5 are much more favorable: the entire train-
ing then runs in little more than an hour.
7The main memory bottleneck is the IBM-4 (6 GB with-
out classes, 8 GB with). Using refined data structures should
reduce this bottleneck.
29
5.1 Alignment Accuracy
The alignment accuracies ? weighted F-measures
with ? = 0.1 ? for the tested corpora and model
variants are given in Table 1. Clearly, nondefi-
ciency greatly improves the accuracy of the IBM-
3, both compared to our deficient implementation
and that of GIZA++.
For the IBM-4 we get improvements for the
nondeficient variant in roughly half the cases, both
with and without word classes. We think this is
an issue of local minima, inexactly solved M-steps
and sensitiveness to initialization.
Interestingly, also the reduced deficient IBM-4
is not always better than the fully deficient variant.
Again, we think this is due to problems with the
non-concave nature of the models.
There is also quite some surprise regarding the
IBM-5: contrary to the findings of (Och and Ney,
2003) the IBM-5 in GIZA++ performs best in
three out of four cases - when competing with both
deficient and nondeficient variants of IBM-3 and
IBM-4. Our own implementation gives slightly
different results (as we do not use smoothing), but
it, too, performs very well.
5.2 Effect on Translation Performance
We also check the effect of the various align-
ments (all produced by RegAligner) on trans-
lation performance for phrase-based translation,
randomly choosing translation from German to
English. We use MOSES with a 5-gram lan-
guage model (trained on 500.000 sentence pairs)
and the standard setup in the MOSES Experi-
ment Management System: training is run in both
directions, the alignments are combined using
diag-grow-final-and (Och and Ney, 2003)
and the parameters of MOSES are optimized on
750 development sentences.
The resulting BLEU-scores are shown in Table
2. However, the table shows no clear trends and
even the IBM-3 is not clearly inferior to the IBM-
4. We think that one would need to handle larger
corpora (or run multiple instances of Minimum Er-
ror Rate Training with different random seeds) to
get more meaningful insights. Hence, at present
our paper is primarily of theoretical value.
6 Conclusion
We have shown that the word alignment models
IBM-3 and IBM-4 can be turned into nondeficient
Model #Classes Deficiency BLEU
HMM - nondeficient 29.72
IBM-3 - deficient 29.63
IBM-3 - nondeficient 29.73
IBM-4 1 x 1 fully deficient 29.91
IBM-4 1 x 1 reduced deficient 29.88
IBM-4 1 x 1 nondeficient 30.18
IBM-4 50 x 50 fully deficient 29.86
IBM-4 50 x 50 reduced deficient 30.14
IBM-4 50 x 50 nondeficient 29.90
IBM-5 50 nondeficient 29.84
Table 2: Evaluation of phrase-based translation
from German to English with the obtained align-
ments (for 100.000 sentence pairs). Training is run
in both directions and the resulting alignments are
combined via diag-grow-final-and. The
table shows no clear superiority of any method.
In fact, the IBM-4 is not superior to the IBM-3
and the HMM is about equal to the IBM-3. We
think that one needs to handle larger corpora to
get clearer insights.
variants, an important aim of probabilistic model-
ing for word alignment.
Here we have exploited that the models are
proper applications of the chain rule of probabili-
ties, where deficiency is only introduced by ignor-
ing parts of the history for the distortion factors in
the factorization. By proper renormalization the
desired nondeficient variants are obtained.
The arising models are trained via expectation
maximization. In the E-step we use hillclimb-
ing to get a likely alignment (ideally the Viterbi
alignment). While this cannot be handled fully
incrementally, it is still fast enough in practice.
The M-step energies are non-concave and have no
(known) closed-form solutions. They are handled
via projected gradient ascent.
For the IBM-3 nondeficiency clearly improves
alignment accuracy. For the IBM-4 we get im-
proved accuracies in roughly half the cases, both
with and without word classes. The IBM-5 per-
forms surprisingly well, it is often best and hence
much better than its reputation. An evaluation of
phrase based translation showed no clear insights.
Nevertheless, we think that nondeficiency in
fertility based models is an important issue, and
that at the very least our paper is of theoretical
value. The implementations are publicly available
in RegAligner 1.2.
30
References
M. Bansal, C. Quirk, and R. Moore. 2011. Gappy
phrasal alignment by agreement. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Portland, Oregon, June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
F. Cromie`res and S. Kurohashi. 2009. An alignment
algorithm using Belief Propagation and a structure-
based distortion model. In Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), Athens, Greece, April.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?38.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In
HLT-EMNLP, Vancouver, Canada, October.
A. Fraser and D. Marcu. 2007a. Getting the structure
right for word alignment: LEAF. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Prague, Czech Republic, June.
A. Fraser and D. Marcu. 2007b. Measuring word
alignment quality for statistical machine translation.
Computational Linguistics, 33(3):293?303, Septem-
ber.
J. Grac?a, K. Ganchev, and B. Taskar. 2010. Learning
tractable word alignment models with complex con-
straints. Computational Linguistics, 36, September.
P. Lambert, A.D. Gispert, R. Banchs, and J.B. Marino.
2005. Guidelines for word alignment evaluation and
manual alignment. Language Resources and Evalu-
ation, 39(4):267?285.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, New York,
New York, June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Philadelphia,
Pennsylvania, July.
D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
C. Michelot. 1986. A finite algorithm for finding the
projection of a point onto the canonical simplex of
IRn. Journal on Optimization Theory and Applica-
tions, 50(1), July.
R.M. Neal and G.E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In M.I. Jordan, editor, Learning in
Graphical Models. MIT press.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
S. Ravi and K. Knight. 2010. Does GIZA++ make
search errors? Computational Linguistics, 36(3).
T. Schoenemann. 2010. Computing optimal align-
ments for the IBM-3 translation model. In Confer-
ence on Computational Natural Language Learning
(CoNLL), Uppsala, Sweden, July.
T. Schoenemann. 2011. Regularizing mono- and bi-
word models for word alignment. In International
Joint Conference on Natural Language Processing
(IJCNLP), Chiang Mai, Thailand, November.
E. Sumita, Y. Akiba, T. Doi, A. Finch, K. Imamura,
H. Okuma, M. Paul, M. Shimohata, and T. Watan-
abe. 2004. EBMT, SMT, Hybrid and more: ATR
spoken language translation system. In Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Kyoto, Japan, September.
K. Toutanova, H.T. Ilhan, and C.D. Manning. 2002.
Extensions to HMM-based statistical word align-
ment models. In Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Philadelphia, Pennsylvania, July.
R. Udupa and H.K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), Trento, Italy,
April.
A. Vaswani, L. Huang, and D. Chiang. 2012. Smaller
alignment models for better translations: Unsuper-
vised word alignment with the l0-norm. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Jeju, Korea, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Inter-
national Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
Y.-Y. Wang and A. Waibel. 1998. Modeling with
structures in statistical machine translation. In In-
ternational Conference on Computational Linguis-
tics (COLING), Montreal, Canada, August.
31
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 98?106,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Computing Optimal Alignments for the IBM-3 Translation Model
Thomas Schoenemann
Centre for Mathematical Sciences
Lund University, Sweden
Abstract
Prior work on training the IBM-3 transla-
tion model is based on suboptimal meth-
ods for computing Viterbi alignments. In
this paper, we present the first method
guaranteed to produce globally optimal
alignments. This not only results in im-
proved alignments, it also gives us the op-
portunity to evaluate the quality of stan-
dard hillclimbing methods. Indeed, hill-
climbing works reasonably well in prac-
tice but still fails to find the global opti-
mum for between 2% and 12% of all sen-
tence pairs and the probabilities can be
several tens of orders of magnitude away
from the Viterbi alignment.
By reformulating the alignment problem
as an Integer Linear Program, we can
use standard machinery from global opti-
mization theory to compute the solutions.
We use the well-known branch-and-cut
method, but also show how it can be cus-
tomized to the specific problem discussed
in this paper. In fact, a large number of
alignments can be excluded from the start
without losing global optimality.
1 Introduction
Brown et al (1993) proposed to approach the
problem of automatic natural language translation
from a statistical viewpoint and introduced five
probability models, known as IBM 1-5. Their
models were single word based, where each
source word could produce at most one target
word.
State-of-the-art statistical translation systems
follow the phrase based approach, e.g. (Och and
Ney, 2000; Marcu and Wong, 2002; Koehn, 2004;
Chiang, 2007; Hoang et al, 2007), and hence al-
low more general alignments. Yet, single word
based models (Brown et al, 1993; Brown et al,
1995; Vogel et al, 1996) are still highly relevant:
many phrase based systems extract phrases from
the alignments found by training the single word
based models, and those that train phrases directly
usually underperform these systems (DeNero et
al., 2006).
Single word based models can be divided into
two classes. On the one hand, models like IBM-1,
IBM-2 and the HMM are computationally easy to
handle: both marginals and Viterbi alignments can
be computed by dynamic programming or even
simpler techniques.
On the other hand there are fertility based mod-
els, including IBM 3-5 and Model 6. These mod-
els have been shown to be of higher practical rel-
evance than the members of the first class (Och
and Ney, 2003) since they usually produce better
alignments. At the same time, computing Viterbi
alignments for these methods has been shown to
be NP-hard (Udupa and Maji, 2006), and comput-
ing marginals is no easier.
The standard way to handle these models ? as
implemented in GIZA++ (Al-Onaizan et al, 1999;
Och and Ney, 2003) ? is to use a hillclimbing al-
gorithm. Recently Udupa and Maji (2005) pro-
posed an interesting approximation based on solv-
ing sequences of exponentially large subproblems
by means of dynamic programming and also ad-
dressed the decoding problem. In both cases there
is no way to tell how far away the result is from
the Viterbi alignment.
In this paper we solve the problem of find-
ing IBM-3 Viterbi alignments by means of Inte-
ger Linear Programming (Schrijver, 1986). While
there is no polynomial run-time guarantee, in prac-
tice the applied branch-and-cut framework is fast
enough to find optimal solutions even for the large
Canadian Hansards task (restricted to sentences
with at most 75 words), with a training time of 6
hours on a 2.4 GHz Core 2 Duo (single threaded).
98
Integer Linear Programming in the context of
machine translation first appeared in the work of
Germann et al (2004), who addressed the trans-
lation problem (often called decoding) in terms of
a travelings-salesman like formulation. Recently,
DeNero and Klein (2008) addressed the training
problem for phrase-based models by means of
integer linear programming, and proved that the
problem is NP-hard. The main difference to our
work is that they allow only consecutive words in
the phrases. In their formulation, allowing arbi-
trary phrases would require an exponential number
of variables. In contrast, our approach handles the
classical single word based model where any kind
of ?phrases? in the source sentence are aligned to
one-word phrases in the target sentence.
Lacoste-Julien et al (2006) propose an inte-
ger linear program for a symmetrized word-level
alignment model. Their approach also allows to
take the alignments of neighboring words into ac-
count. In contrast to our work, they only have a
very crude fertility model and they are consider-
ing a substantially different model. It should be
noted, however, that a subclass of their problems
can be solved in polynomial time - the problem is
closely related to bipartite graph matching. Less
general approaches based on matching have been
proposed in (Matusov et al, 2004) and (Taskar et
al., 2005).
Recently Bodrumlu et al (2009) proposed a
very innovative cost function for jointly optimiz-
ing dictionary entries and alignments, which they
minimize using integer linear programming. They
also include a mechanism to derive N-best lists.
However, they mention rather long computation
times for rather small corpora. It is not clear if the
large Hansards tasks could be addressed by their
method.
An overview of integer linear programming ap-
proaches for natural language processing can be
found on http://ilpnlp.wikidot.com/.
To facilitate further research in this area, the
source code will be made publicly available.
Contribution The key contribution of our work
is a method to handle exact fertility models as aris-
ing in the IBM-3 model in a global optimization
framework. This is done by a linear number of
linear consistency constraints. Unlike all previ-
ous works on integer linear programming for ma-
chine translation, we do not solely use binary co-
efficients in the constraint matrix, hence showing
that the full potential of the method has so far not
been explored.
At the same time, our method allows us to give a
detailed analysis of the quality of hillclimbing ap-
proaches. Moreover, we give a more detailed de-
scription of how to obtain a fast problem-tailored
integer solver than in previous publications, and
include a mechanism to a priori exclude some vari-
ables without losing optimality.
2 The IBM-3 Translation Model
Given a source sentence fJ1 , the statistical ap-
proach to machine translation is to assign each
possible target sentence eI1 a probability to be an
accurate translation. For convenience in the trans-
lation process, this probability is usually rewritten
as
P (eI1|fJ1 ) =
1
p(fJ1 )
? p(eI1) ? p(fJ1 |eI1) ,
and the training problem is to derive suitable pa-
rameters for the latter term from a bilingual cor-
pus. Here, the probability is expressed by sum-
ming over hidden variables called alignments. The
common assumption in single word based models
is that each source position j produces a single tar-
get position aj ? {0, . . . , I}, where an artificial 0-
position has been introduced to mark words with-
out a correspondence in the target sentence. The
alignment of a source sentence is then a vector aJ1 ,
and the probability can now be written as
p(fJ1 |eI1) =
?
aJ1
p(fJ1 , a
J
1 |eI1) .
We will focus on training the IBM-3 model which
is based on the concept of fertilities: given an
alignment aJ1 , the fertility ?i(aJ1 ) =
?
j:aj=i 1
of target word i expresses the number of source
words aligned to it. Omitting the dependence on
aJ1 (and defining p(j|0) = 1), the probability is
expressed as
p(fJ1 , a
J
1 |eI1) = p(?0|J) ?
I
?
i=1
[
?i! p(?i|ei)
]
?
?
j
[
p(fj|eaj ) ? p(j|aj)
]
. (1)
For the probability p(?0|J) of the fertility of the
empty word, we use the modification introduced in
(Och and Ney, 2003), see there for details. In sum-
mary, the model comprises a single word based
99
translation model, an inverted zero-order align-
ment model and a fertility model. We now discuss
how to find the optimal alignment for given prob-
abilities, i.e. to solve the problem
argmax
aJ1
p(fJ1 , a
J
1 |eI1) (2)
for each bilingual sentence pair in the training
set. This is a desirable step in the approximate
EM-algorithm that is commonly used to train the
model.
3 Finding IBM-3 Viterbi Alignments via
Integer Linear Programming
Instead of solving (2) directly we consider the
equivalent task of minimizing the negative loga-
rithm of the probability function. A significant
part of the arising cost function is already linear
in terms of the alignment variables, a first step for
the integer linear program (ILP) we will derive.
To model the problem as an ILP, we introduce
two sets of variables. Firstly, for any source po-
sition j ? {1, . . . , J} and any target position
i ? {0, . . . , I} we introduce an integer variable
xij ? {0, 1} which we want to be 1 exactly if
aj = i and 0 otherwise. Since each source posi-
tion must be aligned to exactly one target position,
we arrive at the set of linear constraints
?
i
xij = 1 , j = 1, . . . , J . (3)
The negative logarithm of the bottom row of (1) is
now easily written as a linear function in terms of
the variables xij :
?
i,j
cxij ? xij ,
cxij = ? log
[
p(fj|ei) ? p(j|i)
]
.
For the part of the cost depending on the fertilities,
we introduce another set of integer variables yif ?
{0, 1}. Here i ? {0, . . . , I} and f ranges from 0 to
some pre-specified limit on the maximal fertility,
which we set to max(15, J/2) in our experiments
(fertilities > J need not be considered). We want
yif to be 1 if the fertility of i is f , 0 otherwise.
Hence, again these variables must sum to 1:
?
f
yif = 1 , i = 0, . . . , I . (4)
The associated part of the cost function is written
as
?
i,f
cyif ? yif ,
cyif = ? log
[
f ! p(f |ei)
]
, i = 1, . . . , I
cy0f = ? log
[
p(?0 = f |J)
]
.
It remains to ensure that the variables yif express-
ing the fertilities are consistent with the fertilities
induced by the alignment variables xij . This is
done via the following set of linear constraints:
?
j
xij =
?
f
f ? yif , i = 0, . . . , I . (5)
Problem (2) is now reduced to solving the integer
linear program
arg min
{xij},{yif}
?
i,j
cxij xij +
?
i,f
cyif yif
subject to (3), (4), (5)
xij ? {0, 1}, yif ? {0, 1} , (6)
with roughly 2 I J variables and roughly J + 2I
constraints.
4 Solving the Integer Linear Program
To solve the arising integer linear programming
problem, we first relax the integrality constraints
on the variables to continuous ones:
xij ? [0, 1], yif ? [0, 1] ,
and obtain a lower bound on the problems by solv-
ing the arising linear programming relaxation via
the dual simplex method.
While in practice this can be done in a matter of
milli-seconds even for sentences with I, J > 50,
the result is frequently a fractional solution. Here
the alignment variables are usually integral but the
fertility variables are not.
In case the LP-relaxation does not produce an
integer solution, the found solution is used as the
initialization of a branch-and-cut framework. Here
one first tries to strengthen the LP-relaxation by
deriving additional inequalities that must be valid
for all integral solutions see e.g. (Schrijver, 1986;
Wolter, 2006) and www.coin-or.org. These
inequalities are commonly called cuts. Then one
applies a branch-and-bound scheme on the inte-
ger variables. In each step of this scheme, addi-
tional inequalities are derived. The process is fur-
ther sped-up by introducing a heuristic to derive an
100
upper bound on the cost function. Such bounds are
generally given by feasible integral solutions. We
use our own heuristic as a plug-in to the solver.
It generates solutions by thresholding the align-
ment variables (winner-take-all) and deriving the
induced fertility variables. An initial upper bound
is furthermore given by the alignment found by
hillclimbing.
We suspect that further speed-ups are possible
by using so-called follow-up nodes: e.g. if in the
branch-and-bound an alignment variable xij is set
to 1, one can conclude that the fertility variable
yi0 must be 0. Also, sets of binary variables that
must sum to 1 as in (3) and (4) are known as spe-
cial ordered sets of type I and there are variants
of branch-and-cut that can exploit these proper-
ties. However, in our context they did not result
in speed-ups.
Our code is currently based on the open source
COIN-OR project1 and involves the linear pro-
gramming solver CLP, the integer programming
solver CBC, and the cut generator library CGL.
We have also tested two commercial solvers. For
the problem described in this paper, CBC per-
formed best. Tests on other integer programming
tasks showed however that the Gurobi solver out-
performs CBC on quite a number of problems.
5 Speed-ups by Deriving Bounds
It turns out that, depending on the cost function,
some variables may a priori be excluded from the
optimization problem without losing global opti-
mality. That is, they can be excluded even before
the first LP-relaxation is solved.
The affected variables have relatively high cost
coefficients and they are identified by considering
lower bounds and an upper bound on the cost func-
tion. Starting from the lower bounds, one can then
identify variables that when included in a solution
would raise the cost beyond the upper bound.
An upper bound u on the problem is given by
any alignment. We use the one found by hillclimb-
ing. If during the branch-and-cut process tighter
upper bounds become available, the process could
be reapplied (as a so-called column cut generator).
For the lower bounds we use different ones to
exclude alignment variables and to exclude fertil-
ity variables.
1www.coin-or.org
5.1 Excluding Alignment Variables
To derive a lower bound for the alignment vari-
ables, we first observe that the cost cxij for the
alignment variables are all positive, whereas the
cost cyif for the fertilities are frequently negative,
due to the factorial of f . A rather tight lower
bound on the fertility cost can be derived by solv-
ing the problem
lF,1 = min
{?i}
I
?
i=0
cyi?i
s.t.
?
i ?i = J , (7)
which is easily solved by dynamic programming
proceeding along i. A lower bound on the align-
ment cost is given by
lA =
?
j lA,j ,
where lA,j = min
i=0,...,I
cxij .
The lower bound is then given by l1 = lF,1 + lA,
and we can be certain that source word j will not
be aligned to target word i if
cxij > lA,j + (u ? l1) .
5.2 Excluding Fertility Variables
Excluding fertility variables is more difficult as
cost can be negative and we have used a constraint
to derive lF,1 above.
At present we are using a two ways to gener-
ate a lower bound and apply the exclusion process
with each of them sequentially. Both bounds are
looser than l1, but they immensely help to get the
computation times to an acceptable level.
The first bound builds upon l1 as derived above,
but using a looser bound lF,2 for the fertility cost:
lF,2 =
?
i
min
?i
cyi?i .
This results in a bound l2 = lF,2 + lA, and fertility
variables can now be excluded in a similar manner
as above.
Our second bound is usually much tighter and
purely based on the fertility variables:
l3 =
?
i
min
?i
[
cyi?i + minJ?{1,...,J} : |J=?i|
cxi (J )
]
,
with cxi (J ) =
?
j?J
cxij ,
101
and where the cost of the empty set is defined as 0.
Although this expression looks rather involved, it
is actually quite easy to compute by simply sorting
the respective cost entries. A fertility variable yif
can now be excluded if the difference between cyif
and the contribution of i to l3 exceeds u ? l3.
We consider it likely that more variables can be
excluded by deriving bounds in the spirit of (7),
but with the additional constraint that ?i = f for
some i and f . We leave this for future work.
6 Experiments
We have tested our method on three different tasks
involving a total of three different languages and
each in both directions. The first task is the well-
known Canadian Hansards2 task (senate debates)
for French and English. Because of the large
dataset we are currently only considering sentence
pairs where both sentences have at most 75 words.
Longer sentences are usually not useful to derive
model parameters.
The other two datasets are released by the Eu-
ropean Corpus Initiative3. We choose the Union
Bank of Switzerland (UBS) corpus for English and
German and the Avalanche Bulletins, originally
released by SFISAR, for French and German. For
the latter task we have annotated alignments for
150 of the training sentences, where one annota-
tor specified sure and possible alignments. For de-
tails, also on the alignment error rate, see (Och and
Ney, 2003).
All corpora have been preprocessed with
language-specific rules; their statistics are given in
Table 1. We have integrated our method into the
standard toolkit GIZA++4 and are using the train-
ing scheme 15H53545 for all tasks. While we fo-
cus on the IBM-3 stage, we also discuss the quality
of the resulting IBM-4 parameters and alignments.
Experiments were run on a 2.4 GHz Core 2
Duo with 4 GB memory. For most sentence pairs,
the memory consumption of our method is only
marginally more than in standard GIZA++ (600
MB). In the first iteration on the large Hansards
task, however, there are a few very difficult sen-
tence pairs where the solver needs up to 90 min-
utes and 1.5 GB . We observed this in both trans-
lation directions.
2www.isi.edu/natural-language/
download/hansard/
3The entire CD with many more corpora is available for
currently 50 Euros.
4available at code.google.com/p/giza-pp/ .
Avalanche Bulletin
French German
# sentences 2989
max. sentence length 88 57
total words 64825 45629
vocabulary size 1707 2113
UBS
English German
# sentences 2689
max. sentence length 92 91
total words 62617 53417
vocabulary size 5785 9127
Canadian Hansards (max. 75)
French English
# sentences 180706
max. sentence length 75 75
total words 3730570 3329022
vocabulary size 48065 37633
Table 1: Corpus statistics for all employed (train-
ing) corpora, after preprocessing.
6.1 Evaluating Hillclimbing
In our first set of experiments, we compute Viterbi
alignments merely to evaluate the quality of the
standard training process. That is, the model
parameters are updated based on the alignments
found by hillclimbing. Table 2 reveals that, as
expected, hillclimbing does not always find the
global optimum: depending on the task and it-
eration number, between 2 and 12 percent of all
hillclimbing alignments are suboptimal. For short
sentences (i.e. I, J ? 20) hillclimbing usually
finds the global optimum.
Somewhat more surprisingly, even when a good
and hence quite focused initialization of the IBM-
3 model parameters is given (by training HMMs
first), the probability of the Viterbi alignment can
be up to a factor of 1037 away from the optimum.
This factor occurred on the Hansards task for a
sentence pair with 46 source and 46 target words
and the fertility of the empty word changed from
9 (for hillclimbing) to 5.
6.2 Hillclimbing vs. Viterbi Alignments
We now turn to a training scheme where the
Viterbi alignments are used to actually update the
model parameters, and compare it to the standard
training scheme (based on hillclimbing).
102
Candian Hansards (max 75)
French ? English
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 10.7% 10.7% 10.8% 11.1% 11.4%
Maximal factor to Viterbi alignment 1.9 ? 1037 9.1 ? 1017 7.3 ? 1014 3.3 ? 1012 8.1 ? 1014
English ? French
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 7.3% 7.5% 7.4% 7.4% 7.5%
Maximal factor to Viterbi alignment 5.6 ? 1038 6.6 ? 1020 7.6 ? 1011 4.3 ? 1010 8.3 ? 1011
Avalanche Bulletins
French ? German
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 7.5% 5.6% 4.9% 4.9% 4.4%
Maximal factor to Viterbi alignment 6.1 ? 105 877 368 2.5 ? 104 429
German ? French
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 4.2% 2.7% 2.5% 2.3% 2.1%
Maximal factor to Viterbi alignment 40 302 44 3.3 ? 104 9.2 ? 104
Union Bank of Switzerland (UBS)
English ? German
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 5.0% 4.0% 3.5% 3.3% 3.2%
Maximal factor to Viterbi alignment 677 22 53 40 32
German ? English
Iteration # 1 2 3 4 5
# suboptimal alignments in hillclimbing 5.5% 3.3% 2.5% 2.2% 2.3%
Maximal factor to Viterbi alignment 1.4 ? 107 808 33 33 1.8 ? 104
Table 2: Analysis of Hillclimbing on all considered tasks. All numbers are for the IBM-3 translation
model. Iteration 1 is the first iteration after the transfer from HMM, the final iteration is the transfer to
IBM4. The factors are w.r.t. the original formulation, not the negative logarithm of it and are defined as
the maximal ratio between the Viterbi probability and the hillclimbing probability.
103
une baisse de la tempe?rature a en ge?ne?ral stabilise? la couverture neigeuse .
ein Temperaturru?ckgang hat die Schneedecke im allgemeinen stabilisiert .
Standard training (hillclimbing).
une baisse de la tempe?rature a en ge?ne?ral stabilise? la couverture neigeuse .
ein Temperaturru?ckgang hat die Schneedecke im allgemeinen stabilisiert .
Proposed training (Viterbi alignments).
Figure 1: Comparison of training schemes. Shown are the alignments of the final IBM-3 iteration.
Indeed Table 3 demonstrates that with the new
training scheme, the perplexities of the final IBM-
3 iteration are consistently lower. Yet, this effect
does not carry over to IBM-4 training, where the
perplexities are consistently higher. Either this is
due to overfitting or it is better to use the same
method for alignment computation for both IBM-
3 and IBM-4. After all, both start from the HMM
Viterbi alignments.
Interestingly, the maximal factor between the
hillclimbing alignment and the Viterbi alignment
is now consistently higher on all tasks and in all
iterations. The extreme cases are a factor of 1076
for the Canadian Hansards English ? French task
and 1030 for the Bulletin French ? German task.
Table 4 demonstrates that the alignment error
rates of both schemes are comparable. Indeed, a
manual evaluation of the alignments showed that
most of the changes affect words like articles or
prepositions that are generally hard to translate.
In many cases neither the heuristic nor the Viterbi
alignment could be considered correct. An inter-
esting case where the proposed scheme produced
the better alignment is shown in Figure 1.
In summary, our results give a thorough justi-
fication for the commonly used heuristics. A test
with the original non-deficient empty word model
of the IBM-3 furthermore confirmed the impres-
sion of (Och and Ney, 2003) that overly many
words are aligned to the empty word: the tendency
is even stronger in the Viterbi alignments.
6.3 Optimizing Running Time
The possibilities to influence the run-times of the
branch-and-cut framework are vast: there are nu-
Union Bank (UBS) E ? G
Final IBM-3 Final IBM-4
Standard train. 49.21 35.73
Proposed train. 49.00 35.76
Union Bank (UBS) G ? E
Final IBM-3 Final IBM-4
Standard train. 62.38 47.39
Proposed train. 62.08 47.43
Avalanche F ? G
Final IBM-3 Final IBM-4
Standard train. 35.44 21.99
Proposed train. 35.23 22.04
Avalanche G ? F
Final IBM-3 Final IBM-4
Standard train. 34.60 22.78
Proposed train. 34.48 22.76
Canadian Hansards F ? E
Final IBM-3 Final IBM-4
Standard train. 105.28 55.22
Proposed train. 92.09 55.35
Canadian Hansards E ? F
Final IBM-3 Final IBM-4
Standard train. 70.58 37.64
Proposed train. 70.03 37.73
Table 3: Analysis of the perplexities in training.
104
French ? German
Final IBM-3 Final IBM-4
Standard train. 24.31% 23.01%
Proposed train. 24.31% 23.24%
German ? French
Final IBM-3 Final IBM-4
Standard train. 33.03% 33.44%
Proposed train. 33.00% 33.27%
Table 4: Alignment error rates on the Avalanche
bulletin task.
merous ways to generate cuts and several of them
can be used simultaneously. The CBC-package
also allows to specify how many rounds of cuts
to derive at each node. Then there is the question
of whether to use the bounds derived in Section
5 to a priori exclude variables. Finally, branch-
and-cut need not be done on all variables: since
solving LP-relaxations typically results in integral
alignments, it suffices to do branch-and-cut on the
fertility variables and only add the alignment vari-
ables in case non-integral values arise (this never
happened in our experiments5).
We could not possibly test all combinations
of the listed possibilities, and our primary focus
was to achieve acceptable run-times for the large
Hansards task. Still, in the end we have a quite
uniform picture: the lowest run-times are achieved
by using Gomory Cuts only. Moreover, including
all variables for branching was between 1.5 and 2
times faster than only including fertility variables.
Only by exploiting the bounds derived in Section
5 the run-times for the Hansards task in direction
from English to French became acceptable. We
believe that further speed-ups are possible by de-
riving tighter bounds, and are planning to investi-
gate this in the future.
We end up with roughly 6 hours for the
Hansards task, roughly 3 minutes for the UBS
task, and about 2.5 minutes for the Avalanche task.
In all cases the run-times are much higher than
in the standard GIZA++ training. However, we
are now getting optimality guarantees where pre-
viously one could not even tell how far away one is
from the optimum. And the Viterbi alignments of
several sentence pairs can of course be computed
in parallel.
Lastly, we mention the possibility of setting a
5In fact, when fixing the fertility variables, the problem
reduces to the polynomial time solvable assignment problem.
limit on the branch-and-cut process, either on the
running time or on the number of nodes. There is
then no longer a guarantee for global optimality,
but at least one is getting a bound on the gap to the
optimum and one can be certain that the training
time will be sufficiently low.
7 Conclusion
We present the first method to compute IBM-3
Viterbi alignments with a guarantee of optimal-
ity. In contrast to other works on integer linear
programming for machine translation, our formu-
lation is able to include a precise and very gen-
eral fertility model. The resulting integer linear
program can be solved sufficiently fast in prac-
tice, and we have given many comments on how
problem-specific knowledge can be incorporated
into standard solvers.
The proposed method allows for the first time
to analyze the quality of hillclimbing approaches
for IBM-3 training. It was shown that they can be
very far from the optimum. At the same time, this
seems to happen mostly for difficult sentences that
are not suitable to derive good model parameters.
In future work we want to derive tighter bounds
to a priori exclude variables, combine the method
with the N-best list generation of (Bodrumlu et
al., 2009) and evaluate on a larger set of corpora.
Finally we are planning to test other integer pro-
gramming solvers.
Acknowledgments We thank Fredrik Kahl for
helpful comments and an anonymous reviewer for
pointing out freely available software packages.
This research was funded by the European Re-
search Council (GlobalVision grant no. 209480).
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, Final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.
2009. A new objective function for word align-
ment. In Proceedings of the Workshop on Inte-
ger Linear Programming for Natural Langauge Pro-
cessing (ILP), Boulder, Colorado, June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
105
P.F. Brown, J. Cocke, S.A. Della Pietra, V.J. Della
Pietra, F. Jelinek, J. Lai, and R.L. Mercer. 1995.
Method and system for natural language translation.
U.S. patent #5.477.451.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
J. DeNero and D. Klein. 2008. The complexity
of phrase alignment problems. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Columbus, Ohio, June.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In StatMT ?06: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, pages 31?38, Morristown, NJ, USA, June.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast decoding and optimal decod-
ing for machine translation. Artificial Intelligence,
154(1?2), April.
H. Hoang, A. Birch, C. Callison-Burch, R. Zens,
A. Constantin, M. Federico, N. Bertoldi, C. Dyer,
B. Cowan, W. Shen, C. Moran, and O. Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 177?
180, Prague, Czech Republic, June.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation mod-
els. In Conference of the Association for Machine
Translation in the Americas (AMTA), pages 115?
124, Washington, D.C., October.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, New York, New York, June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Philadelphia,
Pennsylvania, July.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In International Conference on Computational Lin-
guistics (COLING), Geneva, Switzerland, August.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 440?447, Hongkong, China, October.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
A. Schrijver. 1986. Theory of Linear and Integer Pro-
gramming. Wiley-Interscience Series in Discrete
Mathematics and Optimization. John Wiley & Sons.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005.
A discriminative matching approach to word align-
ment. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Vancouver,
Canada, October.
R. Udupa and H.K. Maji. 2005. Theory of align-
ment generators and applications to statistical ma-
chine translation. In The International Joint Con-
ferences on Artificial Intelligence, Edinburgh, Scot-
land, August.
R. Udupa and H.K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), Trento, Italy,
April.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Inter-
national Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
K. Wolter. 2006. Implementation of Cutting Plane
Separators for Mixed Integer Programs. Master?s
thesis, Technische Universita?t Berlin, Germany.
106
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 172?180,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Probabilistic Word Alignment under the L0-norm
Thomas Schoenemann
Center for Mathematical Sciences
Lund University, Sweden
Abstract
This paper makes two contributions to the
area of single-word based word alignment for
bilingual sentence pairs. Firstly, it integrates
the ? seemingly rather different ? works of
(Bodrumlu et al, 2009) and the standard prob-
abilistic ones into a single framework.
Secondly, we present two algorithms to opti-
mize the arising task. The first is an iterative
scheme similar to Viterbi training, able to han-
dle large tasks. The second is based on the in-
exact solution of an integer program. While it
can handle only small corpora, it allows more
insight into the quality of the model and the
performance of the iterative scheme.
Finally, we present an alternative way to
handle prior dictionary knowledge and dis-
cuss connections to computing IBM-3 Viterbi
alignments.
1 Introduction
The training of single word based translation mod-
els (Brown et al, 1993b; Vogel et al, 1996) is an es-
sential building block for most state-of-the-art trans-
lation systems. Indeed, even more refined transla-
tion models (Wang and Waibel, 1998; Sumita et al,
2004; Deng and Byrne, 2005; Fraser and Marcu,
2007a) are initialized by the parameters of single
word based ones. The exception is here the joint
approach of Marcu and Wong (2002), but its refine-
ment by Birch et al (2006) again relies on the well-
known IBM models.
Traditionally (Brown et al, 1993b; Al-Onaizan
et al, 1999) single word based models are trained
by the EM-algorithm, which has the advantageous
property that the collection of counts can be de-
composed over the sentences. Refinements that also
allow symmetrized models are based on bipartite
graph matching (Matusov et al, 2004; Taskar et al,
2005) or quadratic assignment problems (Lacoste-
Julien et al, 2006). Recently, Bodrumlu et al
(2009) proposed the first method that treats a non-
decomposable problem by handling all sentence
pairs at once and via integer linear programming.
Their (non-probabilistic) approach finds dictionaries
with a minimal number of entries. However, the ap-
proach does not include a position model.
In this work we combine the two strategies into
a single framework. That is, the dictionary sparsity
objective of Bodrumlu et al will become a regu-
larity term in our framework. It is combined with
the maximal alignment probability of every sentence
pair, where we consider the models IBM-1, IBM-2
and HMM. This allows us to write dictionary spar-
sity as the (non-convex) L0 norm of the dictionary
parameters of the respective models.
For supervised training, regularity terms are quite
common, e.g. (Taskar et al, 2005; Lacoste-Julien et
al., 2006). For the unsupervised problem addressed
in this paper they have recently been introduced in
the form of posterior constraints (Ganchev et al,
2010). In related fields of NLP lately Dirichlet pri-
ors have been investigated, e.g. (Johnson, 2007).
We present two strategies to handle the objec-
tive function addressed in this paper. One of these
schemes relies, like (Germann et al, 2004; Lacoste-
Julien et al, 2006; DeNero and Klein, 2008; Bo-
drumlu et al, 2009), on integer linear programming
172
(see e.g. (Schrijver, 1986; Achterberg, 2007)), but
due to the large-scale nature of our problem we
solve only the LP-relaxation, followed by successive
strengthening. For the latter, we develop our own,
exponentially large set of cuts and show that it can
be handled as a polynomially sized system, though
in practice this is too inefficient.
2 The Models
Before we introduce our objective function we give
a brief description of the (standard) models we con-
sider. In all cases, one is given a set of bilin-
gual sentence pairs containing a foreign language
and English. The models formalize the probabil-
ity of obtaining the foreign sentence from a given
English sentence, by considering hidden variables
called alignments:
pd,l(fs|es) =
?
as
pd,l(fs,as|es) .
Here, the subscripts d and l denote two sets of pa-
rameters: whereas the set l defines the probability of
an alignment without knowing any of the sentences,
d describes the translational probability given an
alignment and a source sentence.
For a source (English) sentence of length I and a
target (foreign) sentence of length J , the set of ad-
missible alignments is generally the set of subsets
of {1, . . . , I} ? {1, . . . , J}. However, for compu-
tational reasons the considered models only allow
restricted alignments, where each target word may
align to at most one source word. Any such align-
ment is expressed as a vector aJ1 ? {0, . . . , I}
J .
2.1 Considered models
For a source sentence es = eI1 and a target sentence
f s = fJ1 , the considered models all factor as follows:
pd,l(f
s,as|es) = (1)
J?
j=1
pd(fj |eaj ) ? pl(aj |aj?1, j, I)
In all cases, the translational probability is non-
parametric, i.e. d contains one parameter for every
co-occurring pair of source and target words. Since
the model is probabilistic, the parameters of all f for
a given e have to sum up to one.
With respect to the alignment probability, the
models differ. For the IBM-1 the set l is actually
empty, so pl(aj |aj?1, j, I) = 1/(I+1). The IBM-2
models1 p(aj |j, I), with a respective set of parame-
ters. Finally, the HMM models p(aj |aj?1, I).
It is common to further reduce the alignment pa-
rameters. In this paper we consider a nonparametric
distribution for the IBM-2, but both a nonparamet-
ric and a parametric one for the HMM. In contrast
to GIZA++, we have a parameter for every possible
difference, i.e. we do not group differences with ab-
solutes greater than 5. Also, commonly one uses a
distribution p(i|i?, I) = r(i? i?)/
?I
i??=1 r(i
?? ? i?),
but for technical reasons, we drop the denominator
and instead constrain the r(?)-parameters to sum to
1. In future work we hope to implement both the
normalization and the grouping of bins.
2.2 Word Alignment
Originally the considered models were used for the
actual translation problem. Hence, the parameters
d and l had to be inferred from a training corpus,
which was based on maximizing the probability
max
d,l
?
s
?
a
pd,l(f
s,as|es) . (2)
Today the major application of the models lies in
word alignment. Instead of estimating continuous
parameters, one is now faced with the discrete opti-
mization problem of assigning a single alignment to
every sentence pair in the corpus. This lead to the
recent innovative work of (Bodrumlu et al, 2009)
where the alignments are the only unknown quanti-
ties.
Nevertheless, the use of probabilistic models re-
mains attractive, in particular since they contribute
statistics of likely alignments. In this work, we com-
bine the two concepts into the criterion
min
d,l
? log
[
?
s
max
as
pd,l(f
s,as|es)
]
+ ? ?d?0 ,
where ? ? 0 is a weighting parameter and we now
estimate a single alignment for every sentence.
The second term denotes the L0-norm of the
translational parameters, i.e. the number of non-zero
1The original work considered a dependence on I and J , but
it is common to drop J .
173
parameters. Since we only consider a single align-
ment per sentence, this term is equivalent to Bo-
drumlu et al?s objective function. Minimizing the
first term is closely related to the common criterion
(2). For parameter estimation it is known as the max-
imum approximation, but for word alignment it is a
perfectly valid model.
For the IBM-1 model the first term alone results in
a convex, but not strictly convex minimization prob-
lem2. However, EM-like iterative methods generally
do not reach the minimum: they are doing block co-
ordinate descent (Bertsekas, 1999, chap. 2.7) which
generally gives the optimum only for strictly convex
functions. Indeed, our experiments showed a strong
dependence on initialization and lead to heavily lo-
cal solutions.
In the following we present two strategies to min-
imize the new objective. We start with an iterative
method that also handles the regularity term.
3 An Iterative Scheme
To derive our algorithms, we first switch the mini-
mum and maximum in the objective and obtain
min
{as}
min
d,l
?
?
s
log
[
pd,l(f
s,as|es)
]
+ ? ?d?0 ,
where the notation{as} denotes the alignments of
all sentence pairs. Ignoring the L0-term for the mo-
ment, we now make use of a result of (Vicente et al,
2009) in their recent work on histogram-based im-
age segmentation: for any given set of alignments,
the inner minimization over the parameters is solved
by relative frequencies. When plugging this solution
into the functional, one gets a model that does not
decompose over the sentences, but one that is still
reasonably easy to handle.
Before we get into details, we observe that this
minimizer is valid even when including the L0 term:
if two words are never linked, both terms will set the
respective parameter to 0. If they are linked, how-
ever, then setting this parameter to 0 would make
the first term infinite. All non-zero parameters are
treated equally by the L0 term, so the restriction
to relative frequencies does not change the optimal
value. In fact, this can be extended to weighted L0
2This is due to taking the maximizing alignment. Summing
over all alignments is strictly convex.
terms, and later on we exploit this to derive an al-
ternative way to handle a dictionary prior. Note that
the same principle could also be applied to the work
of (Bodrumlu et al, 2009).
3.1 Mathematical Formulation
We detail our scheme for the IBM-1 model, the ex-
tension to other models is easy. For given align-
ments we introduce the counts
Nf,e({a
s}) =
?
s
?
j
?(f, fj) ? ?(e, eaj )
Ne({as}) =
?
s
?
j
?(e, eaj ) ,
where ?(?, ?) is the Kronecker-delta. The op-
timal translation parameters are then given by
Nf,e({as})/Ne({as}), and plugging this into the
first term in the objective gives (up to a constant)
min
{as}
?
f,e
?Nf,e({a
s}) log
(
Nf,e({as})
Ne({as})
)
.
The second term is simply ?
?
f,e ?Nf,e({a
s})?0,
and since N(e) =
?
f N(f, e), in total we get
min
{as}
?
f,e
?Nf,e({a
s}) log (Nf,e({a
s}))
+
?
e
Ne({as}) log (Ne({as})) .
+ ?
?
f,e
?Nf,e({a
s})?0 (3)
In essence we are now dealing with the function
x log(x), where its value for 0 is defined as 0.
3.2 Algorithm
For the new objective, we were able to entirely get
rid of the model parameters, leaving only alignment
variables. Nevertheless, the algorithm we present
maintains these parameters, and it requires an initial
choice. While we initialize the alignment parame-
ters uniformly, for the translation parameters we use
co-occurrence counts. This performed much better
than a uniform initialization. The algorithm, called
AM (for alternating minimization), now iterates two
steps:
174
1. Starting from the current setting of d and
l, derive Viterbi alignments for all sentence
pairs. E.g. for the IBM-1 we set asj =
argmax
i
d(fj |ei). For the IBM-2 the term is
similar, while for the HMM one can use dy-
namic programming.
Note that this step does not consider the L0-
term. This term can however not increase.
2. Run the Iterated Conditional Modes (Besag,
1986), i.e. go sequentially over all alignment
variables and set them to their optimal value
when keeping the others fixed.
Here, we need to keep track of the current
alignment counts. In every step we need to
compute the objective cost associated to a count
that increases by 1, and (another) one that
decreases by 1. For the IBM-2 we need to
consider the alignment counts, too, and for
the HMM usually two alignment terms are af-
fected. In case of 0-alignments there can be
more than two. We presently do not consider
these cases and hence do not find the exact op-
timum there.
Afterwards, reestimate the parameters d and l
from the final counts.
4 Integer Linear Programming
The above algorithm is fast and can handle large cor-
pora. However, it still gets stuck in local minima,
and there is no way of telling how close to the opti-
mum one got.
This motivates the second algorithm where we
cast the objective function as an integer linear pro-
gram (ILP). In practice it is too large to be solved
exactly, so we solve its linear programming relax-
ation, followed by successive strengthening. Here
we derive our own set of cuts. Now we also get a
lower bound on the problem and obtain lower en-
ergy solutions in practice. But we can handle only
small corpora.
We limit this method to the models IBM-1 and
IBM-2. Handling an HMM would be possible,
but its first order alignment model would introduce
many more variables. Handling the IBM-3, based on
(Ravi and Knight, 2010; Schoenemann, 2010) seems
a more promising direction.
4.1 An ILP for the Regularized IBM-1
The basis of our ILP is the fact that the counts Nf,e
and Ne can only assume a finite, a-priori known set
of integral values, including 0. We introduce a bi-
nary variable ncf,e ? {0, 1} for each possible value
c, where we want ncf,e = 1 if Nf,e(a
s) = c, oth-
erwise ncf,e = 0. This is analogous for the vari-
ables nce and Ne(a
s). Finally, since the counts de-
pend on the alignments, we also need binary vari-
ables xsi,j ? {0, 1} that we want to be 1 if and only
if asj = i.
The cost function of (3) can now be written as a
linear function in terms of the integer variables ncf,e
and nce, with coefficients
wce,f = ?c log(c) + ??c?0 , w
c
e = c log(c) .
However, we need additional constraints. In particu-
lar we need to ensure that for a given f and e exactly
one variable ncf,e is 1. Equivalently we can postulate
that the sum of these variables is one. We proceed
analogous for each e and the variables nce.
Then, we need to ensure that for each source word
in each sentence f s an alignment is specified, i.e.
that for each given s and j the variables xsi,j sum
to 1. Finally, the count variables have to reflect the
counts induced by the alignment variables. For the
counts Nf,e this is expressed by
?
s,i,j:fsj =f,e
s
i=e
xsi,j =
?
c
c ? ncf,e ?f, e ,
and likewise for the counts Ne.
Altogether, we arrive at the following system:
min
{xsi,j},{n
c
f,e},{n
c
e}
?
e,c
wce n
c
e +
?
f,e,c
wcf,e n
c
f,e
s.t.
?
i
xsi,j = 1 ?s, j
?
c
ncf,e = 1 ?f, e
?
c
nce = 1 ?e
?
s,i,j:fj=f,ei=e
xsi,j =
?
c
c ? ncf,e ?f, e
?
s,i,j:ei=e
xsi,j =
?
c
c ? nce ?e
xsi,j ? {0, 1}, n
c
e ? {0, 1}, n
c
e,f ? {0, 1} .
175
4.2 Handling the IBM-2
The above mentioned system can be easily adapted
to the IBM-2 model. To this end, we introduce vari-
ables nci,j,I ? {0, 1} to express how often source
word j is aligned to target word i given that there
are I words in the target sentence. Note that the
number of times source word j is aligned given that
the target sentence has I words is known a-priori
and does not depend on the alignment to be opti-
mized. We denote it Cj,I . The cost function of
the ILP is augmented by
?
i,j,I,cw
c
i,j,I n
c
i,j,I , with
wci,j,I = c log(c/Cj,I). In addition we add the fol-
lowing constraints to the system:
?
s:Is=I
xsi,j =
?
c
c ? nci,j,I ?i, j, I .
5 Cutting Planes
Integer linear programming is an NP-hard problem
(see e.g. (Schrijver, 1986)). While for problems
with a few thousand variables it can often be solved
exactly via the branch and cut method, in our setting
none of the solvers we tried terminated. Already
solving the linear programming relaxation requires
up to 4 GB of memory for corpora with roughly
3000 sentence pairs.
So instead of looking for an exact solution, we
make use of a few iterations of the cutting planes
method (Gomory, 1958), where repeatedly an LP-
relaxation is solved, then additionally valid inequal-
ities, called cuts, are added to the system. Every
round gives a tighter relaxation, i.e. a better lower
bound on the optimal integral value.
After solving each LP-relaxation we derive an in-
tegral solution by starting the iterative method from
section 3 from the fractional LP-solution. In the end
we output the best found integral solution.
For deriving cuts we tried all the methods imple-
mented in the COIN Cut Generation Library CGL3,
based on the solver Clp from the same project line.
However, either the methods were very slow in pro-
ducing cuts or they produced very few cuts only. So
eventually we derived our own set of cuts that will
now be presented. Note that they alone do not give
an integral solution.
3http://www.coin-or.org/projects/Cgl.xml
5.1 A Set of Count-based Cuts
The derived ILP contains several constraints of the
form ?
i
yi =
?
c
c ? zc , (4)
where all variables are binary. Expressions of this
kind arise wherever we need to ensure consistency
between alignment variables and count variables.
Our cuts aim at strengthening each of these equa-
tions individually.
Assume that equation (4) involves the variables
y1, . . . , yK and hence also the variables z0, . . . , zK .
The trouble with the equation is that even if the
left hand side is integral, the right-hand side is usu-
ally not. As an example, consider the case where
?K
i=1 yi = 3. Then the fractional assignment z0 =
1?3/K, zK = 3/K and zc = 0 for all other c satis-
fies (4). Indeed, if the cost function for zc is concave
in c, as is the function?c log(c) we use, this will be
the optimal solution for the given left hand side.
Hence we want to enforce that for an integral
value k of the left hand side, all variables zc for
0 ? c < k are zero. This is ensured by the fol-
lowing system of inequalities that is exponentially
large in k:
?
i?K
yi +
k?1?
c=0
zc ? k (5)
?K ? {1, . . . ,K} : |K| = k .
It turns out that this system can be formulated quite
compactly.
5.2 Polynomial Formulation
We now formalize the result for the compact formu-
lation of (5).
Proposition 1 The union of the systems (5) for all k
can be represented by polynomially many variables
and linear constraints.
Proof: we first observe that it suffices to enforce
[
max
K:|K|=k
?
i?K
yi
]
+
k?1?
c=0
zc ? k
for all k. These are polynomially many equations
(one for each k), but each involves a maximization
176
over exponentially many sets. However, this maxi-
mization can be expressed by the auxiliary variables
?kl := max
K?{1,...,l}:|K|?k
?
i?K
yi
= max{?k?1l?1 + yl , ?
k
l?1}
Now we only have to maximize over two linear ex-
pressions for each of the new, polynomially many,
variables. We can enforce ?kl to be an upper bound
on the maximum by postulating ?kl ? ?
k?1
l?1 + yl and
?kl ? ?
k
l?1. Since the original maximum occurred on
the left hand side of a less-than constraint, this upper
bound will be tight wherever this is necessary. 2
In practice the arising system is numerically hard
to solve. Since usually only polynomially many cuts
of the form (5) are actually violated during the cut-
ting planes process, we add them in passes and get
significantly lower running times. Moreover, each
round of cuts gives a new opportunity to derive a
heuristic integral solution.
5.3 Backward Cuts
We call the cuts we have derived above forward cuts
as they focus on variables that are smaller than k. If
we could be sure that the left-hand side of (4) was
always integral, they should indeed suffice. In prac-
tice this is not the case, so we now also derive back-
ward cuts where we focus on all variables that are
larger than k, with the following reasoning: once we
know that at least K ? k variables yi are inactive
(i.e. yi = 0), we can conclude that all zc with c > k
must be zero, too. This can be expressed by the set
of inequalities
?
i?K
(1? yi) +
K?
c=k+1
zc ? K ? k
?K ? {1, . . . ,K} : |K| = K ? k ,
or equivalently
?
i?K
?yi +
K?
c=k+1
zc ? 0 ?K : |K| = K ? k .
5.4 Other Applications
A related constraint system arises in recent work
(Ravi and Knight, 2010; Schoenemann, 2010) on
computing IBM-3 Viterbi alignments. We imple-
mented4 the polynomial formulation of the above
forward cuts for this system, and got mild speed-ups
(224 instead of 237 minutes for the Hansards task
reported in the second paper). With an additionally
improved fertility exclusion stage5 this is reduced to
176 minutes.
6 Experiments
We evaluate the proposed strategies on both small
scale and (where applicable) large scale tasks. We
compare to standard EM with sums over alignments,
where for the IBM-1 and the HMM we use GIZA++.
In addition, we evaluate several variants (our imple-
mentations) of the HMM, with non-parametric and
parametric alignment models. Note that for the non-
parametric variant we estimate distributions for the
first aligned position, for the parametric all initial
positions are equally likely. For the IBM-2 we con-
sider the non-parametric variant and hence our im-
plementation. We also evaluate our schemes on the
task without regularization.
All experiments in this work were executed on a
3 GHz Core 2 Duo machine with 8 GB of memory,
where up to 4 GB were actually used. The itera-
tive scheme was run for 15 iterations, where it was
nearly converged. This setting was also used for our
own EM-implementations. Solely for GIZA++ we
used the standard setting of 5 iterations, and the im-
plemented smoothing process. For the IBM-2 and
HMM we follow the standard strategy to first train
an IBM-1 with the same objective function.
6.1 Large Scale Tasks
We consider two well-known corpora with publicly
available gold alignments, and run both translation
directions for each of them. The first task is the
Canadian Hansards task (French and English) with
roughly 1 million sentences. The second task is
Europarl Spanish-English, where we take the first
500000 sentences. Our iterative scheme runs in
4based on code available at www.maths.lth.se/
matematiklth/personal/tosch/download.html.
5In (Schoenemann, 2010) we stated that the difference be-
tween cyif and the contribution of i to the bound has to exceed
u ? l3. This can be tightened if one additionally adds the cost
of the best f alignments to i to the cost cyif .
177
Canadian Hansards
Fr? En En? Fr
HMM (Giza++) 0.918 0.918
par. HMM (our EM) 0.887 0.896
par. HMM (Viterbi) 0.873 0.897
par. HMM + L0 0.891 0.907
nonpar. HMM (our EM) 0.873 0.911
nonpar. HMM (Viterbi) 0.881 0.909
nonpar. HMM + L0 0.902 0.917
Europarl
Es? En En? Es
HMM (Giza++) 0.764 0.754
nonpar. HMM (our EM) 0.738 0.733
nonpar. HMM (Viterbi) 0.726 0.716
nonpar. HMM + L0 0.729 0.73
Table 1: For large corpora, the proposed scheme outper-
forms Viterbi training and sometimes even our EM.
roughly 5 hours (with room for improvements), us-
ing 2.5 GB memory. We found that an L0-weight of
? = 5.0 performs very well. Hence, we will use this
for all our experiments.
We compare to the standard GIZA++ implemen-
tation and our own HMM implementations with EM.
Here we ran 15 iterations for IBM-1 and HMM each.
As shown in Table 1 adding the L0 term improves
the standard Viterbi training. Our method also some-
times beats the simple EM implementation but not
GIZA++. This may be due to the special paramet-
ric model of GIZA++, its smoothing process or the
lower number of iterations. Our deficient paramet-
ric model is inferior for the Hansards task, so we did
not run it for Europarl.
6.2 Small Scale Tasks
To evaluate the ILP strategy we consider four small
scale tasks released by the European Corpus Ini-
tiative6. See (Schoenemann, 2010) for the corpus
statistics. We report weighted f-measures (Fraser
and Marcu, 2007b) on gold alignments (sure and
possible) specified by one annotator, for 144 and 110
sentences respectively. The number of cut rounds
was selected so that the execution times remained
below 2 days for all tasks. This was 50 rounds for
the IBM-1 and 2 rounds for the IBM-2. In fact, with
6http://www.elsnet.org/eci.html
these numbers the Avalanche task is processed in lit-
tle less than a day.
We tested a number of LP solvers and found that
most of them need several hours to solve the root re-
laxation. This is different for the commercial solver
FICO Xpress, which only needs around 15 minutes.
However, it is slower in processing the subsequent
cut iterations. Hence, for the IBM-1 we use the open
source Clp7.
The resulting f-measures for the tested strategies
are given in Table 2. In all cases, adding the L0
term greatly improves the standard Viterbi training.
Moreover, for the small scale tasks, the parametric
HMM is clearly the best choice when using the L0
penalty. In the majority of cases the ILP strategy
performs better than the iterative scheme. In fact, it
always found the lower energy solution. The most
extreme difference we observed for the IBM-2 on
the UBS English to German task: here AM finds an
energy of 318147, where the ILP gives 303674.
Finally, Table 3 evaluates the effectiveness of
the cut strategy exemplarily on one of the tasks.
Clearly, the gaps are reduced significantly compared
to the LP-relaxation. However, except for the IBM-
1 (which is convex for ? = 0) the lower bounds are
still quite loose.
6.3 Handling Dictionary Knowledge
The presented L0 regularity is easily modified to in-
clude dictionary knowledge8. To this end, we intro-
duce a weighted L0-norm: whenever a pair of source
and target words is listed in the dictionary, the entry
is not penalized. All remaining entries are penalized
by ?.
Note that this is different from the standard way
(Brown et al, 1993a) of handling dictionary knowl-
edge, which appends the dictionary to the corpus
(with a proper weighting factor). We tried both
schemes with several weighting factors, then chose
the best-performing for the UBS task. For the UBS
German to English task we get an accuracy of 0.445,
which beats GIZA++ both with (0.438) and without
(0.398) dictionary knowledge. In the reverse direc-
tion both schemes profit from the extra knowledge,
7http://www.coin-or.org/projects/Clp.xml
8Our data are based on www.dict.info/uddl.php
and www.ilovelanguages.com/idp and the stemming
algorithms at snowball.tartarus.org.
178
Avalanche French? German
Model EM AM ILP
IBM-1 0.603 0.619 0.591
IBM-1 + L0 ? 0.64 0.625
IBM-2 0.568 0.632 0.60
IBM-2 + L0 ? 0.680 0.636
par. HMM 0.752 0.621 ?
par. HMM + L0 ? 0.779 ?
nonpar. HMM 0.752 0.655 ?
nonpar. HMM + L0 ? 0.714 ?
Avalanche German? French
Model EM AM ILP
IBM-1 0.494 0.485 0.507
IBM-1 + L0 ? 0.497 0.488
IBM-2 0.428 0.459 0.526
IBM-2 + L0 ? 0.483 0.55
par. HMM 0.606 0.49 ?
par. HMM + L0 ? 0.592 ?
nonpar. HMM 0.582 0.501 ?
nonpar. HMM + L0 ? 0.537 ?
UBS German? English
Model EM AM ILP
IBM-1 0.381 0.359 0.335
IBM-1 + L0 ? 0.350 0.442
IBM-2 0.315 0.324 0.340
IBM-2 + L0 ? 0.383 0.462
par. HMM 0.398 0.229 ?
par. HMM + L0 ? 0.383 ?
nonpar. HMM 0.421 0.29 ?
nonpar. HMM + L0 ? 0.371 ?
UBS English? German
Model EM AM ILP
IBM-1 0.515 0.435 0.489
IBM-1 + L0 ? 0.444 0.504
IBM-2 0.417 0.40 0.435
IBM-2 + L0 ? 0.52 0.571
par. HMM 0.625 0.404 ?
par. HMM + L0 ? 0.537 ?
nonpar. HMM 0.623 0.436 ?
nonpar. HMM + L0 ? 0.524 ?
Table 2: Alignment accuracy (weighted f-measure) for
different algorithms. We use a dictionary penalty of
? = 5 and the standard EM (GIZA++ for IBM-1 and
parametric HMM, our implementation otherwise) train-
ing scheme with 5 iterations for each model.
UBS English? German
L0-weight IBM-1 IBM-2
root relaxation 0.0 1.098 7.697
after cut rounds 0.0 1.081 5.67
root relaxation 5.0 1.16 2.76
after cut rounds 5.0 1.107 2.36
Table 3: Ratios of the best known integer solution and the
best known lower bounds for all considered tasks.
but GIZA++ remains the clear winner. Applying
the same weights to the above mentioned Hansards
task slightly improved GIZA++, whereas it slightly
worsened the performance of our scheme in the one
direction and slightly improved it in the other. We
intend to investigate this more thoroughly in the fu-
ture.
7 Discussion
In this paper we have shown that an L0 prior on
the dictionary parameters greatly improves Viterbi
training. A simple iterative scheme often nearly
matches our EM-implementation of the HMM.
We have also derived two algorithms to deal with
the new objective. A simple iterative scheme gives
quite accurate results on large scale tasks. On small
scale tasks our inexact ILP strategy shows that the
iterative scheme does not find the optimum in prac-
tice, a point that may well carry over to other mod-
els trained with the maximum approximation. This
strategy also provides lower bounds, but at present
they are quite loose.
Moreover, we have presented an alternative way
of handling dictionary knowledge. Finally, we have
discussed connections to computing IBM-3 Viterbi
alignments, where we got mild speed-ups.
In future work we intend to investigate the effect
of the generated alignments on the translation qual-
ity of phrase-based approaches. We also want to ex-
plore strategies to determine the regularity weight.
Finally, we want to handle a non-deficient paramet-
ric HMM.
Acknowledgements. We thank Ben Taskar and
Joa?o Grac?a for helpful discussions. This work was
funded by the European Research Council (Glob-
alVision grant no. 209480).
179
References
T. Achterberg. 2007. Constraint Integer Programming.
Ph.D. thesis, Zuse Institut, TU Berlin, Germany, July.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical
machine translation, Final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/.
D.P. Bertsekas. 1999. Nonlinear Programming, 2nd edi-
tion. Athena Scientific.
J. Besag. 1986. On the statistical analysis of dirty pic-
tures. Journal of the Royal Statistical Society, Series
B, 48(3):259?302.
A. Birch, C. Callison-Burch, and M. Osborne. 2006.
Constraining the phrase-based, joint probability statis-
tical translation model. In Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Cambridge, Massachusetts, August.
T. Bodrumlu, K. Knight, and S. Ravi. 2009. A new ob-
jective function for word alignment. In Proceedings of
the Workshop on Integer Linear Programming for Nat-
ural Language Processing (ILP), Boulder, Colorado,
June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, M.J.
Goldsmith, J. Hajic, R.L. Mercer, and S. Mohanty.
1993a. But dictionaries are data too. In HLT work-
shop on Human Language Technology.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993b. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Columbus,
Ohio, June.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In HLT-
EMNLP, Vancouver, Canada, October.
A. Fraser and D. Marcu. 2007a. Getting the structure
right for word alignment: LEAF. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Prague, Czech Republic, June.
A. Fraser and D. Marcu. 2007b. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3):293?303, September.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001?2049, July.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast decoding and optimal decoding for
machine translation. Artificial Intelligence, 154(1?2),
April.
R.E. Gomory. 1958. Outline of an algorithm for integer
solutions to linear programs. Bulletin of the American
Mathematical Society, 64:275?278.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Conference on Empirical Methods
in Natural Language Processing (EMNLP), Prague,
Czech Republic, June.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Human Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, New York, New York, June.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Philadelphia, Pennsylva-
nia, July.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In In-
ternational Conference on Computational Linguistics
(COLING), Geneva, Switzerland, August.
S. Ravi and K. Knight. 2010. Does GIZA++ make search
errors? Computational Linguistics, 36(3).
T. Schoenemann. 2010. Computing optimal alignments
for the IBM-3 translation model. In Conference on
Computational Natural Language Learning (CoNLL),
Uppsala, Sweden, July.
A. Schrijver. 1986. Theory of Linear and Integer
Programming. Wiley-Interscience Series in Discrete
Mathematics and Optimization. John Wiley & Sons.
E. Sumita, Y. Akiba, T. Doi, A. Finch, K. Imamura,
H. Okuma, M. Paul, M. Shimohata, and T. Watanabe.
2004. EBMT, SMT, Hybrid and more: ATR spoken
language translation system. In International Work-
shop on Spoken Language Translation (IWSLT), Ky-
oto, Japan, September.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A
discriminative matching approach to word alignment.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Vancouver, Canada, Oc-
tober.
S. Vicente, V.N. Kolmogorov, and C. Rother. 2009. Joint
optimization of segmentation and appearance models.
In IEEE International Conference on Computer Vision
(ICCV), Kyoto, Japan, September.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In In-
ternational Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
Y.-Y. Wang and A. Waibel. 1998. Modeling with
structures in statistical machine translation. In In-
ternational Conference on Computational Linguistics
(COLING), Montreal, Canada, August.
180
