Proceedings of the 12th Conference of the European Chapter of the ACL, pages 728?736,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Frequency Matters: Pitch Accents and Information Status
Katrin Schweitzer, Michael Walsh, Bernd Mo?bius,
Arndt Riester, Antje Schweitzer, Hinrich Schu?tze
University of Stuttgart
Stuttgart, Germany
<firstname>.<surname>@ims.uni-stuttgart.de
Abstract
This paper presents the results of a series
of experiments which examine the impact
of two information status categories (given
and new) and frequency of occurrence on
pitch accent realisations. More specifi-
cally the experiments explore within-type
similarity of pitch accent productions and
the effect information status and frequency
of occurrence have on these productions.
The results indicate a significant influence
of both pitch accent type and information
status category on the degree of within-
type variability, in line with exemplar-
theoretic expectations.
1 Introduction
It seems both intuitive and likely that prosody
should have a significant role to play in marking
information status in speech. While there are well
established expectations concerning typical asso-
ciations between categories of information status
and categories of pitch accent, e.g. rising L?H
accents are often a marker for givenness, there
is nevertheless some variability here (Baumann,
2006). Furthermore, little research has focused on
how pitch accent tokens of the same type are re-
alised nor have the effects of information status
and frequency of occurrence been considered.
From the perspective of speech technology, the
tasks of automatically inferring and assigning in-
formation status clearly have significant impor-
tance for speech synthesis and speech understand-
ing systems.
The research presented in this paper examines a
number of questions concerning the relationship
between two information status categories (new
and given), and how tokens of associated pitch ac-
cent types are realised. Furthermore the effect of
frequency of occurrence is also examined from an
exemplar-theoretic perspective.
The questions directly addressed in this paper
are as follows:
1. How are different tokens of a pitch accent
type realised?
Does frequency of occurrence of the pitch ac-
cent type play a role?
2. What effect does information status have on
realisations of a pitch accent type?
Does frequency of occurrence of the informa-
tion status category play a role?
3. Does frequency of occurrence in pitch ac-
cents and in information status play a role,
i.e. is there a combined effect?
In examining the realisation of pitch accent to-
kens, their degree of similarity is the characteristic
under investigation. Similarity is calculated by de-
termining the cosine of the angle between pairs of
pitch accent vector representations (see section 6).
The results in this study are examined from
an exemplar-theoretic perspective (see section 3).
The expectations within that framework are based
upon two different aspects. Firstly, it is expected
that, since all exemplars are stored, exemplars of
a type that occur often, offer the speaker a wider
selection of exemplars to choose from during pro-
duction (Schweitzer and Mo?bius, 2004), i.e. the
realisations are expected to be more variable than
those of a rare type. However, another aspect of
Exemplar Theory has to be considered, namely en-
trenchment (Pierrehumbert, 2001; Bybee, 2006).
The central idea here is that frequently occurring
behaviours undergo processes of entrenchment,
they become in some sense routine. Therefore re-
alisations of a very frequent type are expected to
be realised similar to each other. Thus, similarity
and variability are expressions of the same charac-
teristic: the higher the degree of similarity of pitch
accent tokens, the lower their realisation variabil-
ity.
728
The structure of this paper is as follows: Sec-
tion 2 briefly examines previous work on the in-
teraction of information status categories and pitch
accents. Section 3 provides a short introduction to
Exemplar Theory. In this study similarity of pitch
accent realisations on syllables, annotated with the
information status categories of the words they be-
long to, is examined using the parametric intona-
tion model (Mo?hler, 1998) which is outlined in
Section 4. Section 5 discusses the corpus em-
ployed. Section 6 introduces a general methodol-
ogy which is used in the experiments in Sections 7,
8 and 9. Section 10 then presents some discussion,
conclusions and opportunities for future research.
2 Information Status and Intonation
It is commonly assumed that pitch accents are the
main correlate of information status1 in speech
(Halliday, 1967). Generally, accenting is said
to signal novelty while deaccenting signals given
information (Brown, 1983), although there is
counter evidence: various studies note given in-
formation being accented (Yule, 1980; Bard and
Aylett, 1999). Terken and Hirschberg (1994) point
out that new information can also be deaccented.
As for the question of which pitch accent type
(in terms of ToBI categories (Silverman et al,
1992)) is typically assigned to different degrees of
givenness, Pierrehumbert and Hirschberg (1990)
find H? to be the standard novelty accent for En-
glish, a finding which has also been confirmed by
Baumann (2006) and Schweitzer et al (2008) for
German. Given information on the other hand, if
accented at all, is found to carry L? accent in En-
glish (Pierrehumbert and Hirschberg, 1990). Bau-
mann (2006) finds deaccentuation to be the most
preferred realisation for givenness in his experi-
mental phonetics studies on German. However,
Baumann (2006) points out that H+L? has also
been found as a marker of givenness in a German
corpus study. Previous findings on the corpus used
in the present study found L?H being the typical
marker for givenness (Schweitzer et al, 2008).
Leaving the phonological level and examining
correlates of information status in acoustic detail,
Kohler (1991) reports that in a falling accent, an
early peak indicates established facts, while a me-
dial peak is used to mark novelty. In a recent
1The term information status is used in (Prince, 1992) for
the first time. Before that the terms givenness, novelty or in-
formation structure were used for these concepts.
study Ku?gler and Fe?ry (2008) found givenness to
lower the high tones of prenuclear pitch accents
and to cancel them out postnuclearly. These find-
ings among others (Ku?gler and Fe?ry, 2008) moti-
vate an examination of the acoustic detail of pitch
accent shape across different information status
categories.
The experiments presented here go one step fur-
ther, however, in that they also investigate poten-
tial exemplar-theoretic effects.
3 Exemplar Theory
Exemplar Theory is concerned with the idea that
the acquisition of language is significantly facil-
itated by repeated exposure to concrete language
input, and it has successfully accounted for a num-
ber of language phenomena, including diachronic
language change and frequency of occurrence ef-
fects (Bybee, 2006), the emergence of gram-
matical knowledge (Abbot-Smith and Tomasello,
2006), syllable duration variability (Schweitzer
and Mo?bius, 2004; Walsh et al, 2007), entrench-
ment and lenition (Pierrehumbert, 2001), among
others. Central to Exemplar Theory are the notions
of exemplar storage, frequency of occurrence, re-
cency of occurrence, and similarity. There is an
increasing body of evidence which indicates that
significant storage of language input exemplars,
rich in detail, takes place in memory (Johnson,
1997; Croot and Rastle, 2004; Whiteside and Var-
ley, 1998). These stored exemplars are then em-
ployed in the categorisation of new input percepts.
Similarly, production is facilitated by accessing
these stored exemplars. Computational models of
the exemplar memory also argue that it is in a con-
stant state of flux with new inputs updating it and
old unused exemplars gradually fading away (Pier-
rehumbert, 2001).
Up to now, virtually no exemplar-theoretic re-
search has examined pitch accent prosody (but
see Marsi et al (2003) for memory-based predic-
tion of pitch accents and prosodic boundaries, and
Walsh et al (2008)(discussed below)) and to the
authors? knowledge this paper represents the first
attempt to examine the relationship between pitch
accent prosody and information status from an
exemplar-theoretic perspective. Given the consid-
erable weight of evidence for the influence of fre-
quency of occurrence effects in a variety of other
linguistic domains it seems reasonable to explore
such effects on pitch accent and information sta-
729
tus realisations. For example, what effect might
givenness have on a frequently/infrequently occur-
ring pitch accent? Does novelty produce a similar
result?
The search for possible frequency of occur-
rence effects takes place with respect to pitch ac-
cent shapes captured by the parametric intonation
model discussed next.
4 The Parametric Representation of
Intonation Events - PaIntE
The model approximates stretches of F0 by em-
ploying a phonetically motivated model function
(Mo?hler, 1998). This function consists of the sum
of two sigmoids (rising and falling) with a fixed
time delay which is selected so that the peak does
not fall below 96% of the function?s range. The re-
sulting function has six parameters which describe
the contour and were employed in the analysis: pa-
rameters a1 and a2 express the gradient of the ac-
cent?s rise and fall, parameter b describes the ac-
cent?s temporal alignment (which has been shown
to be crucial in the description of an accent?s shape
(van Santen and Mo?bius, 2000)), c1 and c2 model
the ranges of the rising and falling amplitude of
the accent?s contour, respectively, and parameter d
expresses the peak height of the accent.2 These six
parameters are thus appropriate to describe differ-
ent pitch accent shapes.
For the annotation of intonation the GToBI(S)
annotation scheme (Mayer, 1995) was used. In
earlier versions of PaIntE, the approximation of
the F0-contour for H?L and H? was carried out on
the accented and post?accented syllables. How-
ever, for these accents the beginning of the rise is
likely to start at the preaccented syllable. In the
current version of PaIntE the window used for the
approximation of the F0-contour for H?L and H?
accents has been extended to the preaccented syl-
lable, so that the parameters are calculated over
the span of the accented syllables and its immedi-
ate neighbours (unless it is followed by a boundary
tone which causes the window to end at the end of
the accented syllable).
5 Corpus
The experiments that follow (sections 7, 9 and 8),
were carried out on German pitch accents from the
2Further information and illustrations concerning the me-
chanics of the PaIntE model can be found in Mo?hler and
Conkie (1998).
IMS Radio News Corpus (Rapp, 1998). This cor-
pus was automatically segmented and manually la-
belled according to GToBI(S) (Mayer, 1995). In
the corpus, 1233 syllables are associated with an
L?H accent, 704 with an H?L accent and 162 with
an H? accent.
The corpus contains data from three speakers,
two female and a male one, but the majority of the
data is produced by the male speaker (888 L?H
accents, 527 H?L accents and 152 H? accents). In
order to maximise the number of tokens, all three
speakers were combined. Of the analysed data,
77.92% come from the male speaker. However,
it is not necessarily the case that the same percent-
age of the variability also comes from this speaker:
Both, PaIntE and z-scoring (cf. section 6) nor-
malise across speakers, so the contribution from
each individual speaker is unclear.
The textual transcription of the corpus was an-
notated with respect to information status using
the annotation scheme proposed by Riester (2008).
In this taxonomy information status categories re-
flect the default contexts in which presuppositions
are resolved, which include e. g. discourse context,
environment context or encyclopaedic context.
The annotations are based solely on the written
text and follow strict semantic criteria. Given that
textual information alone (i.e. without prosodic
or speech related information) is not necessarily
sufficient to unambiguously determine the infor-
mation status associated with a particular word,
there are therefore cases where words have mul-
tiple annotations, reflecting underspecification of
information status. However, it is important to
note that in all the experiments reported here, only
unambiguous cases are considered.
The rich annotation scheme employed in the
corpus makes establishing inter-annotator agree-
ment a time-consuming task which is currently un-
derway. Nevertheless, the annotation process was
set up in a way to ensure a maximal smoothing of
uncertainties. Texts were independently labelled
by two annotators. Subsequently, a third, more ex-
perienced annotator compared the two results and,
in the case of discrepancies, took a final decision.
In the present study the categories given and
new are examined. These categories do not rep-
resent a binary distinction but are two extremes
from a set of clearly distinguished categories. For
the most part they correspond to the categories tex-
tually given and brand-new that are used in Bau-
730
mann (2006), but their scope is more tightly con-
strained. The information status annotations are
mapped to the phonetically transcribed speech sig-
nals, from which individual syllable tokens bear-
ing information status are derived.
Syllables for which one of the PaIntE-
parameters was identified as an outlier, were re-
moved. Outliers were defined such that the upper
2.5 percentile as well as the lower 2.5 percentile
of the data were excluded. This led to a reduced
number of pitch accent tokens: 1021 L?H accents,
571H?L accents and 134H? accents. Thus, there
is a continuum of frequency of occurrence, high to
low, from L?H to H?.
With respect to information status, 102 L?H ac-
cents, 87H?L accents and 21H? accents were un-
ambiguously labelled as new. For givenness the
number of tokens is: 114 L?H accents, 44H?L ac-
cents and 10H? accents.
6 General Methodology
In the experiments the general methodology for
calculation of similarity detailed in this section
was employed.
For tokens of the pitch accent types L?H, H?L
and H?, each token was modelled using the full
set of PaIntE parameters. Thus, each token was
represented in terms of a 6-dimensional vector.
For each of the pitch accent types the following
steps were carried out:
? For each 6-dimensional pitch accent category
token calculate the z-score value for each di-
mension. The z-score value represents the
number of standard deviations the value is
away from the mean value for that dimension
and allows comparison of values from differ-
ent normal distributions. The z-score is given
by:
z ? scoredim =
valuedim ?meandim
sdevdim
(1)
Hence, at this point each pitch accent is repre-
sented by a 6-dimensional vector where each
dimension value is a z-score.
? For each token z-scored vector calculate how
similar it is to every other z-scored vector
within the same pitch accent category, and,
in Experiment 2 and 3, with the same infor-
mation status value (e.g. new), using the co-
sine of the angle between the vectors. This is
given by:
cos(~i,~j) =
~i ?~j
?~i ?? ~j ?
(2)
where i and j are vectors of the same pitch ac-
cent category and ? represents the dot prod-
uct.
Each comparison between vectors yields a
similarity score in the range [-1,1], where -1
represents high dissimilarity and 1 represents
high similarity.
The experiments that follow examine distribu-
tions of token similarity. In order to establish
whether distributions differ significantly two dif-
ferent levels of significance were employed, de-
pending on the number of pairwise comparisons
performed.
When comparing two distributions (i.e. per-
forming one test), the significance level was set to
? = 0.05. In those cases where multiple tests were
carried out (Experiment 1 and Experiment 3), the
level of significance was adjusted (Bonferroni cor-
rection) according to the following formula:
? = 1? (1? ?1)
1
n (3)
where ?1 represents the target significance level
(set to 0.05) and n represents the number of tests
being performed. The Bonferroni correction is of-
ten discussed controversially. The main criticism
concerns the increased likelihood of type II errors
that lead to non-significance of actually significant
findings (Pernegger, 1998). Although this conser-
vative adjustment was applied, the statistical tests
in this study resulted in significant p-values indi-
cating the robustness of the findings.
7 Experiment 1: Examining frequency of
occurrence effects in pitch accents
In accordance with the general methodology set
out in section 6, the PaIntE vectors of pitch ac-
cent tokens of types L?H, H?L, and H? were all
z-scored and, within each type, every token was
compared for similarity against every other token
of the same type, using the cosine of the angle be-
tween their vectors. In essence, this experiment
illustrates how similarly pitch accents of the same
type are realised.
Figure 1 depicts the results of the analysis. It
shows the density plot for each distribution of
cosine-similarity comparison values, whereby the
731
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
Frequency of Occurrence Effects in Pitch Accents
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 1: Density plots for similarity within pitch ac-
cent types. All distributions differ significantly from each
other. There is a trend towards greater similarity from high-
frequency L?H to low-frequency H?.
distributions can be compared directly ? irrespec-
tive of the different number of data points.
An initial observation is that L?H tokens tend
to be realised fairly variably, the main portion
of the distribution is centred around zero. To-
kens of H?L tend to be produced more simi-
larly (i.e. the distribution is centred around a
higher similarity value), and tokens of H? more
similarly again. These three distributions were
tested against each other for significance using the
Kolmogorov-Smirnov test (? = 0.017), yielding
p-values of p  0.001. Thus there are significant
differences between these distributions.
What is particularly noteworthy is that a de-
crease in frequency of occurrence across pitch ac-
cent types co-occurs significantly with an increase
in within-type token similarity.
While the differences between the graphed dis-
tributions do not appear to be highly marked
the frequency of occurrence effect is nevertheless
in keeping with exemplar-theoretic expectations
as posited by Bybee (2006) and Schweitzer and
Mo?bius (2004), that is, the high frequency of oc-
currence entails a large number of stored exem-
plars, giving the speaker the choice from among
a large number of production targets. This wider
choice leads to a broader range of chosen targets
for different productions and thus to more variable
realisations of tokens of the same type.
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
H*L: Frequency of Occurrence Effects 
  in Information Status Categories
Cosine?Similarity Comparison Values
Den
sity
givennew
Figure 2: Density plots for similarity of H?L tokens. To-
kens of the low-frequency information status category given
display greater similarity to each other than those of the high-
frequency information status category new.
Walsh et al (2008) also reported significant
differences between these distributions, however,
there did not appear to be a clear frequency of oc-
currence effect. The results in the present study
differ from their results because the distributions
centre around different ranges of the similarity
scale clearly indicating that each accent type be-
haves differently in terms of similarity/variability
between the tokens of the respective type. The dif-
ferences between the two findings can be ascribed
to the augmented PaIntE model (section 4).
Given the results from this experiment, the next
experiment seeks to establish what relationship, if
any, exists between information status and pitch
accent production variability.
8 Experiment 2: Examining frequency of
occurrence effects in information
status categories
This experiment was carried out in the same man-
ner as Experiment 1 above with the exception that
in this experiment a subset of the corpus was em-
ployed: only syllables that were unambiguously
labelled with either the information status cate-
gory new or the category given were included in
the analyses. The experiment aims to investigate
the effect of information status on the similar-
ity/variability of tokens of different pitch accent
types. For each pitch accent type, tokens that were
labelled with the information status category new
732
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
L*H: Frequency of Occurrence Effects 
  in Information Status Categories
Cosine?Similarity Comparison Values
Den
sity
givennew
Figure 3: Density plots for similarity of L?H tokens. The
curves differ significantly, a trend towards greater similarity
is not observable. The number of tokens for both information
status categories is comparable.
were compared to tokens labelled as given. Again,
a pairwise Kolmogorov-Smirnov test was applied
for each comparison (? = 0.05). Figure 2 depicts
the results for H?L accents. The K-S test yielded a
highly significant difference between the two dis-
tributions (p  0.001), reflecting the clearly visi-
ble difference between the two curves. It is note-
worthy here that for H?L the information status
category new is more frequent than the category
given. Indeed, approximately twice as many are
labelled as new than those labelled given. Figure 2
illustrates that new H?L accents are realised more
variably than given ones. That is, again, an in-
crease in frequency of occurrence co-occurs with
an increase in similarity, this time at the level of
information status.
Figure 3 depicts the difference in similar-
ity/variability for L?H between new tokens and
given tokens. It is clearly visible that the two
curves do not differ as much as those under the
H?L condition. Both curves centre around zero re-
flecting the fact that for both types the tokens are
variable. Although the Kolmogorov-Smirnov test
indicates significance (? = 0.05, p = 0.044), the
nature of the impact that information status has in
this case is unclear.
Here again an effect of frequency of occurrence
might be the reason for this result. The high fre-
quency of L?H accents in general results in a rel-
ative high frequency of given L?H tokens. So the
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
Effect of Information Status Category "new" 
 across Pitch Accent Types
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 4: Density plots for similarity of new tokens across
three pitch accent types. In comparison to fig. 1 the trend
towards greater similarity from high-frequency L?H to low-
frequency H? is even more pronounced.
token number for both types is similar (102 new
L?H tokens vs. 114 given L?H tokens), there is
high frequency in both cases, hence variability.
These results, particularly in the case of H?L
(fig. 2) indicate that information status affects
pitch accent realisation. The next experiment
compares the effect across different pitch accent
types.
9 Experiment 3: Examining the effect of
information status across pitch accent
types
This experiment was carried out in the same man-
ner as Experiments 1 and 2 above. For each pitch
accent type, figure 4 depicts within-type pitch ac-
cent similarity for tokens unambiguously labelled
as new.
As with Experiments 1 and 2, frequency of
occurrence once more appears to play a signifi-
cant role. Again, all Kolmogorov-Smirnov tests
yielded significant results (p < 0.017 in all cases).
Indeed, the difference between the distributions
of L?H, H?L, and H? similarity plots appears to
be considerably more prominent than in Experi-
ment 1 (see fig. 1). This indicates that under the
condition of novelty the frequency of occurrence
effect is more pronounced. In other words, there is
a considerably more noticeable difference across
the distributions of L?H, H?L and H?, when nov-
733
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Effect of Information Status Category "given" 
 across Pitch Accent Types
Cosine?Similarity Comparison Values
Den
sity
H*LL*HH*
Figure 5: Density plots for similarity of given tokens across
three pitch accent types. Mid-frequency H?L displays greater
similarity than high-frequency L?H. For lowest frequency H?
(only 10 tokens) the trend cannot be observed.
elty is considered: novelty compounds the fre-
quency of occurrence effect.
Figure 5 illustrates results of the same analysis
methodology but applied to tokens of pitch accents
unambiguously labelled as given. Once again
there is a considerable difference between the dis-
tributions of L?H and H?L tokens (p < 0.017).
And again, this difference reflects a more pro-
nounced frequency of occurrence effect for given
tokens than for all accents pooled (as described
in Experiment 1): the information status category
given compounds the frequency of occurrence ef-
fect for L?H and H?L.
For H? the result is not as clear as for the two
more frequent accents. The comparison between
H? and L?H results in a significant difference
(p < 0.017) whereas the comparison between H?
and H?L is slightly above the conservative signif-
icance level (p = 0.0186). Moreover, the dis-
tribution is centred between the distributions for
L?H and H?L and it is thus not clear how to inter-
pret this result with respect to a possible frequency
of occurrence effect. However, having only ten
instances of given H?, the explanatory power of
these comparisons is questionable.
10 Discussion
The experiments discussed above yield a num-
ber of interesting results with implications for re-
search in prosody, information status, the interac-
tion between the two domains, and for exemplar
theory.
Returning to the first question posed at the out-
set in section 1, it is quite clear from Experiment 1
that a certain amount of variability exists when
different tokens of the same pitch accent type are
produced. It is also clear, from the same experi-
ment, that the frequency of occurrence of the pitch
accent type does indeed play a role: with an in-
crease in frequency comes an increase in vari-
ability. This result is in line with the exemplar-
theoretic view that since all exemplars are stored,
exemplars of a type that occur often are more vari-
able because they offer the speaker a wider se-
lection of exemplars to choose from during pro-
duction (Schweitzer and Mo?bius, 2004). How-
ever, with respect to entrenchment (Pierrehum-
bert, 2001; Bybee, 2006), i.e. the idea that fre-
quently occurring behaviours undergo processes
of entrenchment, in Experiment 1 one might ex-
pect to see greater similarity in the realisations of
L?H. However, it is important to note that while
tokens of L?H are not particularly similar to each
other (the bulk of the distribution is around zero
(see figure 1)), they are not too dissimilar either.
That is, they rest at the midpoint of the similar-
ity continuum produced by cosine calculation, in
quite a normal looking distribution. This is not
at odds with the idea of entrenchment. As pro-
ductions of a pitch accent type become more fre-
quent, the distribution of similarity spreads from
the right side of the graph (where infrequent and
highly similar H? tokens lie) leftwards (through
H?L) to the point where the L?H distribution is
found. Beyond this point tokens are excessively
different.
The second question posed in section 1, and ad-
dressed in Experiment 2, sought to ascertain the
impact, if any, information status has on pitch ac-
cent realisation. Distributions of given and new
H?L similarity scores differed significantly, as
did distributions of given and new L?H similar-
ity scores, indicating that information status af-
fects realisation. In other words, for both pitch
accent types, given and new tokens behave dif-
ferently. Concerning the frequency of occurrence
of the information status categories, certainly in
the case of H?L the higher frequency new tokens
exhibited more variability. In the case of L?H
similar numbers of new and given tokens, possi-
bly due to the high frequency of L?H in general,
734
?1.0 ?0.5 0.0 0.5 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Combined Frequency of Occurrence Effect 
 on L*H and H*L
Cosine?Similarity Comparison Values
Den
sity
given L*H new L*Hnew H*Lgiven H*L
Figure 6: Density plots for similarity of combinations of
information status categories given and new with pitch ac-
cent types L?H and H?L. The distributions show a clear
trend towards greater similarity form high-frequency ?given
L?H? and ?new L?H? to mid-frequency ?new H?L? and
low-frequency ?given H?L?.
led to visually similar yet significantly different
distributions. Once again sensitivity to frequency
of occurrence seems to be present, in line with
exemplar-theoretic predictions.
The final question concerns the possibility of a
combined effect of pitch accent frequency of oc-
currence and information status frequency of oc-
currence. Figures 4 and 5 depict a clear com-
pounding effect of both information status cate-
gories across the different pitch accent types (and
their inherent frequencies) when compared to fig-
ure 1. Interestingly, the less frequently occurring
given appears to have a greater impact, particularly
on high frequency L?H.
Figure 6 displays all possible combinations of
L?H, H?L, given and new. H? is omitted in this
graph because of the small number of tokens (10
given, 21 new) and the resulting lack of explana-
tory power. It is evident that an overall frequency
of occurrence effect can be observed: ?given L?H?
and ?new L?H?, which have a similar number of
instances (114 vs. 102 tokens) both centre around
zero and are thus the most leftward skewed curves
in the graph. The distribution of ?new H?L? (87
tokens) shows a trend towards the right hand side
of the graph and thus represents greater similarity
of the tokens. The distribution of similarity values
for the least frequent combination of pitch accent
and information status, ?given H?L? (44 tokens),
centres between 0.5 and 1.0 and is thus the most
rightward curve in the graph, reflecting the high-
est similarity between the tokens.
These results highlight an intricate relationship
between pitch accent production and information
status. The information status of the word influ-
ences not only the type and shape of the pitch ac-
cent (Pierrehumbert and Hirschberg, 1990; Bau-
mann, 2006; Ku?gler and Fe?ry, 2008; Schweitzer et
al., 2008) but also the similarity of tokens within a
pitch accent type. Moreover, this effect is well ex-
plainable within the framework of Exemplar The-
ory as it is subject to frequency of occurrence:
tokens of rare types are produced more similar to
each other than tokens of frequent types.
In the context of speech technology, unfortu-
nately the high variability in highly frequent pitch
accents has a negative consequence, as the correla-
tion between a certain pitch accent or a certain in-
formation status category and the F0 contour is not
a one-to-one relationship. However, forewarned
is forearmed and perhaps a finer grained contex-
tual analysis might yield more context specific so-
lutions.
11 Future Work
The methodology outlined in section 6 gives a lu-
cid insight into the levels of similarity found in
pitch accent realisations. Further insights, how-
ever, could be gleaned from a fine-grained exam-
ination of the PaIntE parameters. For example,
which parameters differ and under what conditions
when examining highly variable tokens? Informa-
tion status evidently plays a role in pitch accent
production but the contexts in which this takes
place have yet to be examined. In addition, the
role of information structure (focus-background,
contrast) also needs to be investigated. A further
line of research worth pursuing concerns the im-
pact of information status on the temporal struc-
ture of spoken utterances and possible compound-
ing with frequency of occurrence effects.
References
Kirsten Abbot-Smith and Michael Tomasello. 2006.
Exemplar-learning and schematization in a usage-
based account of syntactic acquisition. The Linguis-
tic Review, 23(3):275?290.
Ellen G. Bard and M. P. Aylett. 1999. The dissocia-
tion of deaccenting, givenness, and syntactic role in
735
spontaneous speech. In Proceedings of ICPhS (San
Francisco), volume 3, pages 1753?1756.
Stefan Baumann. 2006. The Intonation of Givenness
? Evidence from German., volume 508 of Linguis-
tische Arbeiten. Niemeyer, Tu?bingen. Ph.D. thesis,
Saarland University.
Gillian Brown. 1983. Prosodic structure and the
given/new distinction. In Anne Cutler and D. Robert
Ladd, editors, Prosody: Models and Measurements,
pages 67?77. Springer, New York.
Joan Bybee. 2006. From usage to grammar: The
mind?s response to repetition. Language, 84:529?
551.
Karen Croot and Kathleen Rastle. 2004. Is there
a syllabary containing stored articulatory plans for
speech production in English? In Proceedings of the
10th Australian International Conference on Speech
Science and Technology (Sydney), pages 376?381.
Michael A. K. Halliday. 1967. Intonation and Gram-
mar in British English. Mouton, The Hague.
Keith Johnson. 1997. Speech perception without
speaker normalization: An exemplar model. In
K. Johnson and J. W. Mullennix, editors, Talker
Variability in Speech Processing, pages 145?165.
Academic Press, San Diego.
Klaus J. Kohler. 1991. Studies in german intonation.
AIPUK (Univ. Kiel), 25.
Frank Ku?gler and Caroline Fe?ry. 2008. Pitch accent
scaling on given, new and focused constituents in
german. Journal of Phonetics.
Erwin Marsi, Martin Reynaert, Antal van den Bosch,
Walter Daelemans, and Ve?ronique Hoste. 2003.
Learning to predict pitch accents and prosodic
boundaries in dutch. In Proceedings of the ACL-
2003 Conference (Sapporo, Japan), pages 489?496.
Jo?rg Mayer. 1995. Transcribing German In-
tonation ? The Stuttgart System. Technical
report, Universita?t Stuttgart. http://www.ims.uni-
stuttgart.de/phonetik/joerg/labman/STGTsystem.html.
Gregor Mo?hler and Alistair Conkie. 1998. Paramet-
ric modeling of intonation using vector quantization.
In Third Intern. Workshop on Speech Synth (Jenolan
Caves), pages 311?316.
Gregor Mo?hler. 1998. Describing intonation with a
parametric model. In Proceedings ICSLP, volume 7,
pages 2851?2854.
T. V. Pernegger. 1998. What?s wrong with Bonferroni
adjustment. British Medical Journal, 316:1236?
1238.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpreta-
tion of discourse. In P. R. Cohen, J. Morgan, and
M. E. Pollack, editors, Intentions in Communication,
pages 271?311. MIT Press, Cambridge.
Janet Pierrehumbert. 2001. Exemplar dynamics: Word
frequency, lenition and contrast. In Joan Bybee and
Paul Hopper, editors, Frequency and the Emergence
of Linguistic Structure, pages 137?157. Amsterdam.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. C. Mann
and S. A. Thompson, editors, Discourse Descrip-
tion: Diverse Linguistic Analyses of a Fund-Raising
Text, pages 295?325. Amsterdam.
Stefan Rapp. 1998. Automatisierte Erstellung von Ko-
rpora fu?r die Prosodieforschung. Ph.D. thesis, IMS,
Universita?t Stuttgart. AIMS 4 (1).
Arndt Riester. 2008. A Semantic Explication of In-
formation Status and the Underspecification of the
Recipients? Knowledge. In Atle Gr?nn, editor, Pro-
ceedings of Sinn und Bedeutung 12, Oslo.
Antje Schweitzer and Bernd Mo?bius. 2004. Exemplar-
based production of prosody: Evidence from seg-
ment and syllable durations. In Speech Prosody
2004 (Nara, Japan), pages 459?462.
Katrin Schweitzer, Arndt Riester, Hans Kamp, and
Grzegorz Dogil. 2008. Phonological and acoustic
specification of information status - a semantic and
phonetic analysis. Poster at ?Experimental and The-
oretical Advances in Prosody?, Cornell University.
Kim Silverman, Mary Backman, John Pitrelli, Mari
Ostendorf, Colin Wightman, Patti Price, Janet Pier-
rehumbert, and Julia Hirschberg. 1992. Tobi: A
standard for Labeling English Prosody. In Proceed-
ings of ICSLP (Banff, Kanada), volume 2, pages
867?870, Banff, Canada.
Jacques Terken and Julia Hirschberg. 1994. Deaccen-
tuation of words representing ?given? information:
effects of persistence of grammatical function and
surface position. Language and Speech, 37:125?
145.
Jan P. H. van Santen and BerndMo?bius. 2000. A quan-
titative model of F0 generation and alignment. In
A. Botinis, editor, Intonation?Analysis, Modelling
and Technology, pages 269?288. Kluwer.
Michael Walsh, Hinrich Schu?tze, Bernd Mo?bius, and
Antje Schweitzer. 2007. An exemplar-theoretic ac-
count of syllable frequency effects. In Proceedings
of ICPhS (Saarbru?cken), pages 481?484.
Michael Walsh, Katrin Schweitzer, Bernd Mo?bius, and
Hinrich Schu?tze. 2008. Examining pitch-accent
variability from an exemplar-theoretic perspective.
In Proceedings of Interspeech 2008 (Brisbane).
Sandra P. Whiteside and Rosemary A. Varley. 1998.
Dual-route phonetic encoding: Some acoustic evi-
dence. In Proceedings of ICSLP (Sydney), volume 7,
pages 3155?3158.
George Yule. 1980. Intonation and Givenness in Spo-
ken Discourse. Studies in Language, pages 271?
286.
736
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817?825,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Incorporating Information Status into Generation Ranking
Aoife Cahill and Arndt Riester
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
{aoife.cahill,arndt.riester}@ims.uni-stuttgart.de
Abstract
We investigate the influence of informa-
tion status (IS) on constituent order in Ger-
man, and integrate our findings into a log-
linear surface realisation ranking model.
We show that the distribution of pairs of IS
categories is strongly asymmetric. More-
over, each category is correlated with mor-
phosyntactic features, which can be au-
tomatically detected. We build a log-
linear model that incorporates these asym-
metries for ranking German string reali-
sations from input LFG F-structures. We
show that it achieves a statistically signif-
icantly higher BLEU score than the base-
line system without these features.
1 Introduction
There are many factors that influence word order,
e.g. humanness, definiteness, linear order of gram-
matical functions, givenness, focus, constituent
weight. In some cases, it can be relatively straight-
forward to automatically detect these features (i.e.
in the case of definiteness, this is a syntactic prop-
erty). The more complex the feature, the more dif-
ficult it is to automatically detect. It is common
knowledge that information status1 (henceforth,
IS) has a strong influence on syntax and word or-
der; for instance, in inversions, where the subject
follows some preposed element, Birner (1994) re-
ports that the preposed element must not be newer
in the discourse than the subject. We would like
to be able to use information related to IS in the
automatic generation of German text. Ideally, we
would automatically annotate text with IS labels
and learn from this data. Unfortunately, however,
to date, there has been little success in automati-
cally annotating text with IS.
1We take information status to be a subarea of information
structure; the one dealing with varieties of givenness but not
with contrast and focus in the strictest sense.
We believe, however, that despite this shortcom-
ing, we can still take advantage of some of the in-
sights gained from looking at the influence of IS
on word order. Specifically, we look at the prob-
lem from a more general perspective by comput-
ing an asymmetry ratio for each pair of IS cate-
gories. Results show that there are a large num-
ber of pairs exhibiting clear ordering preferences
when co-occurring in the same clause. The ques-
tion then becomes, without being able to auto-
matically detect these IS category pairs, can we,
nevertheless, take advantage of these strong asym-
metric patterns in generation. We investigate the
(automatically detectable) morphosyntactic char-
acteristics of each asymmetric IS pair and inte-
grate these syntactic asymmetric properties into
the generation process.
The paper is structured as follows: Section 2
outlines the underlying realisation ranking system
for our experiments. Section 3 introduces infor-
mation status and Section 4 describes how we ex-
tract and measure asymmetries in information sta-
tus. In Section 5, we examine the syntactic charac-
teristics of the IS asymmetries. Section 6 outlines
realisation ranking experiments to test the integra-
tion of IS into the system. We discuss our findings
in Section 7 and finally we conclude in Section 8.
2 Generation Ranking
The task we are considering is generation rank-
ing. In generation (or more specifically, surface
realisation) ranking, we take an abstract represen-
tation of a sentence (for example, as produced by
a machine translation or automatic summarisation
system), produce a number of alternative string
realisations corresponding to that input and use
some model to choose the most likely string. We
take the model outlined in Cahill et al (2007), a
log-linear model based on the Lexical Functional
Grammar (LFG) Framework (Kaplan and Bres-
nan, 1982). LFG has two main levels of represen-
817
CS 1: ROOT:1458
CProot[std]:1451
DP[std]:906
DPx[std]:903
D[std]:593
die:34
NP:738
N[comm]:693
Beh?rden:85
Cbar:1448
Cbar-flat:1436
V[v,fin]:976
Vx[v,fin]:973
warnten:117
PP[std]:2081
PPx[std]:2072
P[pre]:1013
vor:154
DP[std]:1894
DPx[std]:1956
NP:1952
AP[std,+infl]:1946
APx[std,+infl]:1928
A[+infl]:1039
m?glichen:185
N[comm]:1252
Nachbeben:263
PERIOD:397
.:389
"Die Beh?rden warnten vor m?glichen Nachbeben."
'warnen<[34:Beh?rde], [263:Nachbeben]>'PRED
'Beh?rde'PRED
'die'PREDDETSPEC
CASE nom, NUM pl, PERS 334
SUBJ
'vor<[263:Nachbeben]>'PRED
'Nachbeben'PRED
'm?glich<[263:Nachbeben]>'PRED [263:Nachbeben]SUBJ
attributiveATYPE185
ADJUNCT
CASE dat, NUM pl, PERS 3263
OBJ
154
OBL
MOOD indicative, TENSE pastTNS-ASP
[34:Beh?rde]TOPIC117
Figure 1: An example C(onstituent) and F(unctional) Structure pair for (1)
tation, C(onstituent)-Structure and F(unctional)-
Structure. C-Structure is a context-free tree rep-
resentation that captures characteristics of the sur-
face string while F-Structure is an abstract repre-
sentation of the basic predicate-argument structure
of the string. An example C- and F-Structure pair
for the sentence in (1) is given in Figure 1.
(1) Die
the
Beho?rden
authorities
warnten
warned
vor
of
mo?glichen
possible
Nachbeben.
aftershocks
?The authorities warned of possible aftershocks.?
The input to the generation system is an F-
Structure. A hand-crafted, bi-directional LFG of
German (Rohrer and Forst, 2006) is used to gener-
ate all possible strings (licensed by the grammar)
for this input. As the grammar is hand-crafted,
it is designed only to parse (and therefore) gen-
erate grammatical strings.2 The task of the reali-
sation ranking system is then to choose the most
likely string. Cahill et al (2007) describe a log-
linear model that uses linguistically motivated fea-
tures and improves over a simple tri-gram lan-
guage model baseline. We take this log-linear
model as our starting point.3
2There are some rare instances of the grammar parsing
and therefore also generating ungrammatical output.
3Forst (2007) presents a model for parse disambiguation
that incorporates features such as humanness, definiteness,
linear order of grammatical functions, constituent weight.
Many of these features are already present in the Cahill et
al. (2007) model.
An error analysis of the output of that system
revealed that sometimes ?unnatural? outputs were
being selected as most probable, and that often
information structural effects were the cause of
subtle differences in possible alternatives. For
instance, Example (3) appeared in the original
TIGER corpus with the 2 preceding sentences (2).
(2) Denn ausdru?cklich ist darin der rechtliche Ma?stab
der Vorinstanz, des Sa?chsischen Oberverwaltungs-
gerichtes, besta?tigt worden. Und der besagt: Die
Beteiligung am politischen Strafrecht der DDR, der
Mangel an kritischer Auseinandersetzung mit to-
talita?ren U?berzeugungen rechtfertigen den Ausschluss
von der Dritten Gewalt.
?Because, the legal benchmark has explicitly been con-
firmed by the lower instance, the Saxonian Higher Ad-
ministrative Court. And it indicates: the participation
in the political criminal law of the GDR as well as
deficits regarding the critical debate on totalitarian con-
victions justify an expulsion from the judiciary.?
(3) Man
one
hat
has
aus
out of
der
the
Vergangenheitsaufarbeitung
coming to terms with the past
gelernt.
learnt
?People have learnt from dealing with the past mis-
takes.?
The five alternatives output by the grammar are:
a. Man hat aus der Vergangenheitsaufarbeitung gelernt.
b. Aus der Vergangenheitsaufarbeitung hat man gelernt.
c. Aus der Vergangenheitsaufarbeitung gelernt hat man.
d. Gelernt hat man aus der Vergangenheitsaufarbeitung.
e. Gelernt hat aus der Vergangenheitsaufarbeitung man.
818
The string chosen as most likely by the system of
Cahill et al (2007) is Alternative (b). No mat-
ter whether the context in (2) is available or the
sentence is presented without any context, there
seems to be a preference by native speakers for
the original string (a). Alternative (e) is extremely
marked4 to the point of being ungrammatical. Al-
ternative (c) is also very marked and so is Alterna-
tive (d), although less so than (c) and (e). Alter-
native (b) is a little more marked than the original
string, but it is easier to imagine a preceding con-
text where this sentence would be perfectly appro-
priate. Such a context would be, e.g. (4).
(4) Vergangenheitsaufarbeitung und Abwiegeln sind zwei
sehr unterschiedliche Arten, mit dem Geschehenen
umzugehen.
?Dealing with the mistakes or playing them down are
two very different ways to handle the past.?
If we limit ourselves to single sentences, the
task for the model is then to choose the string that
is closest to the ?default? expected word order (i.e.
appropriate in the most number of contexts). In
this work, we concentrate on integrating insights
from work on information status into the realisa-
tion ranking process.
3 Information Status
The concept of information status (Prince, 1981;
Prince, 1992) involves classifying NP/PP/DP ex-
pressions in texts according to various ways of
their being given or new. It replaces and specifies
more clearly the often vaguely used term given-
ness. The process of labelling a corpus for IS can
be seen as a means of discourse analysis. Different
classification systems have been proposed in the
literature; see Riester (2008a) for a comparison of
several IS labelling schemes and Riester (2008b)
for a new proposal based on criteria from presup-
position theory. In the work described here, we
use the scheme of Riester (2008b). His main theo-
retic assumption is that IS categories (for definites)
should group expressions according to the contex-
tual resources in which their presuppositions find
an antecedent. For definites, the set of main cate-
gory labels found in Table 1 is assumed.
The idea of resolution contexts derives from
the concept of a presupposition trigger (e.g. a
definite description) as potentially establishing an
4By marked, we mean that there are relatively few or spe-
cialised contexts in which this sentence is acceptable.
Context resource IS label
discourse D-GIVEN
context
encyclopedic/ ACCESSIBLE-GENERAL
knowledge
context
environment/ SITUATIVE
situative
context
bridging BRIDGING
context (scenario)
accommodation ACCESSIBLE-
(no context) DESCRIPTION
Table 1: IS classification for definites
anaphoric relation (van der Sandt, 1992) to an en-
tity being available by some means or other. But
there are some expressions whose referent cannot
be identified and needs to be accommodated, com-
pare (5).
(5) [die monatelange Fu?hrungskrise der Hamburger
Sozialdemokraten]ACC-DESC
?the leadership crisis lasting for months among the
Hamburg Social Democrats?
Examples like this one have been mentioned
early on in the literature (e.g. Hawkins (1978),
Clark and Marshall (1981)). Nevertheless, label-
ing schemes so far have neglected this issue, which
is explicitly incorporated in the system of Riester
(2008b).
The status of an expression is ACCESSIBLE-
GENERAL (or unused, following Prince (1981))
if it is not present in the previous discourse but
refers to an entity that is known to the intended
recipent. There is a further differentiation of the
ACCESSIBLE-GENERAL class into generic (TYPE)
and non-generic (TOKEN) items.
An expression is D-GIVEN (or textually evoked)
if and only if an antecedent is available in the
discourse context. D-GIVEN entities are subdi-
vided according to whether they are repetitions of
their antecedent, short forms thereof, pronouns or
whether they use new linguistic material to add in-
formation about an already existing discourse ref-
erent (label: EPITHET). Examples representing a
co-reference chain are shown in (6).
(6) [Angela Merkel]ACC-GEN (first mention) . . . [An-
gela Merkel]D-GIV-REPEATED (second mention) . . .
[Merkel]D-GIV-SHORT . . . [she]D-GIV-PRONOUN . . .
[herself]D-GIV-REFLEXIVE . . . [the Hamburg-born
politician]D-GIV-EPITHET
Indexicals (referring to entities in the environ-
ment context) are labeled as SITUATIVE. Definite
819
items that can be identified within a scenario con-
text evoked by a non-coreferential item receive the
label BRIDGING; compare Example (7).
(7) In
in
Sri Lanka
Sri Lanka
haben
have
tamilische
Tamil
Rebellen
rebels
erstmals
for the first time
einen
an
Luftangriff
airstrike
[gegen
against
die
the
Streitkra?fte]BRIDG
armed forces
geflogen.
flown.
?In Sri Lanka, Tamil rebels have, for the first time, car-
ried out an airstrike against the armed forces.?
In the indefinite domain, a simple classification
along the lines of Table 2 is proposed.
Type IS label
unrelated to context NEW
part-whole relation PARTITIVE
to previous entity
other (unspecified) INDEF-REL
relation to context
Table 2: IS classification for indefinites
There are a few more subdivisions. Table 3,
for instance, contains the labels BRIDGING-CON-
TAINED and PARTITIVE-CONTAINED, going back
to Prince?s (1981:236) ?containing inferrables?.
The entire IS label inventory used in this study
comprises 19 (sub)classes in total.
4 Asymmetries in IS
In order to find out whether IS categories are un-
evenly distributed within German sentences we
examine a corpus of German radio news bulletins
that has been manually annotated for IS (496 an-
notated sentences in total) using the scheme of
Riester (2008b).5
For each pair of IS labels X and Y we count
how often they co-occur in the corpus within a sin-
gle clause. In doing so, we distinguish the num-
bers for ?X preceding Y ? (=A) and ?Y preceding
X? (= B). The larger group is referred to as the
dominant order. Subsequently, we compute a ratio
indicating the degree of asymmetry between the
two orders. If, for instance, the dominant pattern
occurs 20 times (A) and the reverse pattern only 5
times (B), the asymmetry ratio B/A is 0.25.6
5The corpus was labeled by two independent annotators
and the results were compared by a third person who took
the final decision in case of disagreement. An evaluation as
regards inter-coder agreement is currently underway.
6Even if some of the sentences we are learning from are
marked in terms of word order, the ratios allow us to still learn
the predominant order, since the marked order should occur
much less frequently and the ratio will remain low.
Dominant order (: ?before?) B/A Total
D-GIV-PROINDEF-REL 0 19
D-GIV-PROD-GIV-CAT 0.1 11
D-GIV-RELNEW 0.11 31
D-GIV-PROSIT 0.13 17
ACC-DESCINDEF-REL 0.14 24
ACC-DESCACC-GEN-TY 0.19 19
D-GIV-EPIINDEF-REL 0.2 12
D-GIV-REPNEW 0.21 23
D-GIV-PROACC-GEN-TY 0.22 11
ACC-GEN-TOACC-GEN-TY 0.24 42
D-GIV-PROACC-DESC 0.24 46
EXPLNEW 0.25 30
D-GIV-RELD-GIV-EPI 0.25 15
BRIDG-CONTPART-CONT 0.25 15
ACC-DESCEXPL 0.29 27
D-GIV-PROD-GIV-REP 0.29 18
D-GIV-PRONEW 0.29 88
D-GIV-RELACC-DESC 0.3 26
SITEXPL 0.31 17
D-GIV-PROBRIDG-CONT 0.31 21
D-GIV-PROD-GIV-SHORT 0.32 29
. . . . . .
ACC-DESCACC-GEN-TO 0.91 201
SITBRIDG 0.92 23
EXPLACC-DESC 1 12
Table 3: Asymmetric pairs of IS labels
Table 3 gives the top asymmetry pairs down to
a ratio of about 1:3 as well as, down at the bottom,
the pairs that are most evenly distributed. This
means that the top pairs exhibit strong ordering
preferences and are, hence, unevenly distributed
in German sentences. For instance, the ordering
D-GIVEN-PRONOUN before INDEF-REL (top line),
shown in Example (8), occurs 19 times in the ex-
amined corpus while there is no example in the
corpus for the reverse order.7
(8) [Sie]D-GIV-PRO
she
wu?rde
would
auch
also
[bei
at
verringerter
reduced
Anzahl]INDEF-REL
number
jede
every
vernu?nftige
sensible
Verteidigungsplanung
defence planning
sprengen.
blast
?Even if the numbers were reduced it would blow every
sensible defence planning out of proportion.?
5 Syntactic IS Asymmetries
It seems that IS could, in principle, be quite bene-
ficial in the generation ranking task. The problem,
of course, is that we do not possess any reliable
system of automatically assigning IS labels to un-
known text and manual annotations are costly and
time-consuming. As a substitute, we identify a list
7Note that we are not claiming that the reverse pattern is
ungrammatical or impossible, we just observe that it is ex-
tremely infrequent.
820
of morphosyntactic characteristics that the expres-
sions can adopt and investigate how these are cor-
related to our inventory of IS categories.
For some IS labels there is a direct link between
the typical phrases that fall into that IS category,
and the syntactic features that describe it. One
such example is D-GIVEN-PRONOUN, which al-
ways corresponds to a pronoun, or EXPL which
always corresponds to expletive items. Such syn-
tactic markers can easily be identified in the LFG
F-structures. On the other hand, there are many
IS labels for which there is no clear cut syntac-
tic class that describes its typical phrases. Ex-
amples include NEW, ACCESSIBLE-GENERAL or
ACCESSIBLE-DESCRIPTION.
In order to determine whether we can ascertain
a set of syntactic features that are representative
of a particular IS label, we design an inventory of
syntactic features that are found in all types of IS
phrases. The complete inventory is given in Table
5. It is a much easier task to identify these syntac-
tic characteristics than to try and automatically de-
tect IS labels directly, which would require a deep
semantic understanding of the text. We automati-
cally mark up the news corpus with these syntactic
characteristics, giving us a corpus both annotated
for IS and syntactic features.
We can now identify, for each IS label, what the
most frequent syntactic characteristics of that la-
bel are. Some examples and their frequencies are
given in Table 4.
Syntactic feature Count
D-GIVEN-PRONOUN
PERS PRON 39
DA PRON 25
DEMON PRON 19
GENERIC PRON 11
NEW
SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
. . .
Table 4: Syntactic characteristics of IS labels
Combining the most frequent syntactic charac-
teristics with the asymmetries presented in Table 3
gives us Table 6.8
8For reasons of space, we are only showing the very top
of the table.
6 Generation Ranking Experiments
Using the augmented set of IS asymmetries,
we design new features to be included into the
original model of Cahill et al (2007). For each
IS asymmetry, we extract all precedence patterns
of the corresponding syntactic features. For
example, from the first asymmetry in Table 6, we
extract the following features:
PERS PRON precedes INDEF ATTR
PERS PRON precedes SIMPLE INDEF
DA PRON precedes INDEF ATTR
DA PRON precedes SIMPLE INDEF
DEMON PRON precedes INDEF ATTR
DEMON PRON precedes SIMPLE INDEF
GENERIC PRON precedes INDEF ATTR
GENERIC PRON precedes SIMPLE INDEF
We extract these patterns for all of the asym-
metric pairs in Table 3 (augmented with syntac-
tic characteristics) that have a ratio >0.4. The
patterns we extract need to be checked for incon-
sistencies because not all of them are valid. By
inconsistencies, we mean patterns of the type X
precedes X, Y precedes Y, and any pat-
tern where the variant X precedes Y as well
as Y precedes X is present. These are all auto-
matically removed from the list of features to give
a total of 130 new features for the log-linear rank-
ing model.
We train the log-linear ranking model on 7759
F-structures from the TIGER treebank. We gen-
erate strings from each F-structure and take the
original treebank string to be the labelled exam-
ple. All other examples are viewed as unlabelled.
We tune the parameters of the log-linear model on
a small development set of 63 sentences, and carry
out the final evaluation on 261 unseen sentences.
The ranking results of the model with the addi-
tional IS-inspired features are given in Table 7.
Exact
Model BLEU Match
(%)
Cahill et al (2007) 0.7366 52.49
New Model (Model 1) 0.7534 54.40
Table 7: Ranking Results for new model with IS-
inspired syntactic asymmetry features.
We evaluate the string chosen by the log-linear
model against the original treebank string in terms
of exact match and BLEU score (Papineni et al,
821
Syntactic feature Type
Definites
Definite descriptions
SIMPLE DEF simple definite descriptions
POSS DEF simple definite descriptions with a possessive determiner
(pronoun or possibly genitive name)
DEF ATTR ADJ definite descriptions with adjectival modifier
DEF GENARG definite descriptions with a genitive argument
DEF PPADJ definite descriptions with a PP adjunct
DEF RELARG definite descriptions including a relative clause
DEF APP definite descriptions including a title or job description
as well as a proper name (e.g. an apposition)
Names
PROPER combinations of position/title and proper name (without article)
BARE PROPER bare proper names
Demonstrative descriptions
SIMPLE DEMON simple demonstrative descriptions
MOD DEMON adjectivally modified demonstrative descriptions
Pronouns
PERS PRON personal pronouns
EXPL PRON expletive pronoun
REFL PRON reflexive pronoun
DEMON PRON demonstrative pronouns (not: determiners)
GENERIC PRON generic pronoun (man ? one)
DA PRON ?da?-pronouns (darauf, daru?ber, dazu, . . . )
LOC ADV location-referring pronouns
TEMP ADV,YEAR Dates and times
Indefinites
SIMPLE INDEF simple indefinites
NEG INDEF negative indefinites
INDEF ATTR indefinites with adjectival modifiers
INDEF CONTRAST indefinites with contrastive modifiers
(einige ? some, andere ? other, weitere ? further, . . . )
INDEF PPADJ indefinites with PP adjuncts
INDEF REL indefinites with relative clause adjunct
INDEF GEN indefinites with genitive adjuncts
INDEF NUM measure/number phrases
INDEF QUANT quantified indefinites
Table 5: An inventory of interesting syntactic characteristics in IS phrases
Label 1 (+ features) Label 2 (+ features) B/A Total
D-GIVEN-PRONOUN INDEF-REL 0 19
PERS PRON 39 INDEF ATTR 23
DA PRON 25 SIMPLE INDEF 17
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-PRONOUN D-GIVEN-CATAPHOR 0.1 11
PERS PRON 39 SIMPLE DEF 13
DA PRON 25 DA PRON 10
DEMON PRON 19
GENERIC PRON 11
D-GIVEN-REFLEXIVE NEW 0.11 31
REFL PRON 54 SIMPLE INDEF 113
INDEF ATTR 53
INDEF NUM 32
INDEF PPADJ 26
INDEF GEN 25
...
Table 6: IS asymmetric pairs augmented with syntactic characteristics
822
2002). We achieve an improvement of 0.0168
BLEU points and 1.91 percentage points in exact
match. The improvement in BLEU is statistically
significant (p < 0.01) using the paired bootstrap
resampling significance test (Koehn, 2004).
Going back to Example (3), the new model
chooses a ?better? string than the Cahill et al
(2007) model. The new model chooses the orig-
inal string. While the string chosen by the Cahill
et al (2007) system is also a perfectly valid sen-
tence, our empirical findings from the news corpus
were that the default order of generic pronoun be-
fore definite NP were more frequent. The system
with the new features helped to choose the original
string, as it had learnt this asymmetry.
Was it just the syntax?
The results in Table 7 clearly show that the new
model is beneficial. However, we want to know
how much of the improvement gained is due to
the IS asymmetries, and how much the syntactic
asymmetries on their own can contribute. To this
end, we carry out a further experiment where we
calculate syntactic asymmetries based on the au-
tomatic markup of the corpus, and ignore the IS
labels completely. Again we remove any incon-
sistent asymmetries and only choose asymmetries
with a ratio of higher than 0.4. The top asymme-
tries are given in Table 8.
Dominant order (: ?before?) B/A Total
BAREPROPERINDEF NUM 0 33
DA PRONINDEF NUM 0 16
DEF PPADJTEMP ADV 0 15
SIMPLE INDEFINDEF QUANT 0 14
PERS PRONINDEF ATTR 0 12
DEF PPADJEXPL PRON 0 12
GENERIC PRONINDEF ATTR 0 12
REFL PRONYEAR 0 11
INDEF PPADJINDEF NUM 0.02 57
DEF APPBAREPROPER 0.03 34
BAREPROPERTEMP ADV 0.04 26
TEMP ADVINDEF NUM 0.04 25
PROPERINDEF GEN 0.05 20
DEF GENARGINDEF ATTR 0.06 18
. . . . . .
Table 8: Purely syntactic asymmetries
For each asymmetry, we create a new feature X
precedes Y. This results in a total of 66 fea-
tures. Of these 30 overlap with the features used
in the above experiment. We do not include the
features extracted in the first attempt in this exper-
iment. The same training procedure is carried out
and we test on the same heldout test set of 261 sen-
tences. The results are given in Table 9. Finally,
we combine the two lists of features and evaluate,
these results are also presented in Table 9.
Exact
Model BLEU Match
(%)
Cahill et al (2007) 0.7366 52.49
Model 1 0.7534 54.40
Synt.-asym.-based Model 0.7419 54.02
Combination 0.7437 53.64
Table 9: Results for ranking model with purely
syntactic asymmetry features
They show that although the syntactic asymme-
tries alone contribute to an improvement over the
baseline, the gain is not as large as when the syn-
tactic asymmetries are constrained to correspond
to IS label asymmetries (Model 1).9 Interest-
ingly, the combination of the lists of features does
not result in an improvement over Model 1. The
difference in BLEU score between the model of
Cahill et al (2007) and the model that only takes
syntactic-based asymmetries into account is not
statistically significant, while the difference be-
tween Model 1 and this model is statistically sig-
nificant (p < 0.05).
7 Discussion
In the work described here, we concentrate only on
taking advantage of the information that is read-
ily available to us. Ideally, we would like to be
able to use the IS asymmetries directly as features,
however, without any means of automatically an-
notating new text with these categories, this is im-
possible. Our experiments were designed to test,
whether we can achieve an improvement in the
generation of German text, without a fully labelled
corpus, using the insight that at least some IS cate-
gories correspond to morphosyntactic characteris-
tics that can be easily identified. We do not claim
to go beyond this level to the point where true IS
labels would be used, rather we attempt to pro-
vide a crude approximation of IS using only mor-
phosyntactic information. To be able to fully auto-
matically annotate text with IS labels, one would
need to supplement the morphosyntactic features
9The difference may also be due to the fewer features used
in the second experiment. However, this emphasises, that
the asymmetries gleaned from syntactic information alone are
not strong enough to be able to determine the prevailing order
of constituents. When we take the IS labels into account, we
are honing in on a particular subset of interesting syntactic
asymmetries.
823
with information about anaphora resolution, world
knowledge, ontologies, and possibly even build
dynamic discourse representations.
We would also like to emphasise that we are
only looking at one sentence at a time. Of course,
there are other inter-sentential factors (not relying
on external resources) that play a role in choosing
the optimal string realisation, for example paral-
lelism or the position of the sentence in the para-
graph or text. Given that we only looked at IS fac-
tors within a sentence, we think that such a sig-
nificant improvement in BLEU and exact match
scores is very encouraging. In future work, we will
look at what information can be automatically ac-
quired to help generation ranking based on more
than one sentence.
While the experiments presented this paper are
limited to a German realisation ranking system,
there is nothing in the methodology that precludes
it from being applied to another language. The IS
annotation scheme is language-independent, and
so all one needs to be able to apply this to another
language is a corpus annotated with IS categories.
We extracted our IS asymmetry patterns from a
small corpus of spoken news items. This corpus
contains text of a similar domain to the TIGER
treebank. Further experiments are required to de-
termine how domain specific the asymmetries are.
Much related work on incorporating informa-
tion status (or information structure) into language
generation has been on spoken text, since infor-
mation structure is often encoded by means of
prosody. In a limited domain setting, Prevost
(1996) describes a two-tiered information struc-
ture representation. During the high level plan-
ning stage of generation, using a small knowl-
edge base, elements in the discourse are automat-
ically marked as new or given. Contrast and fo-
cus are also assigned automatically. These mark-
ings influence the final string generated. We are
focusing on a broad-coverage system, and do not
use any external world-knowledge resources. Van
Deemter and Odijk (1997) annotate the syntac-
tic component from which they are generating
with information about givenness. This informa-
tion is determined by detecting contradictions and
parallel sentences. Pulman (1997) also uses in-
formation about parallelism to predict word or-
der. In contrast, we only look at one sentence
when we approximate information status, future
work will look at cross sentential factors. Endriss
and Klabunde (2000) describe a sentence planner
for German that annotates the propositional in-
put with discourse-related features in order to de-
termine the focus, and thus influence word order
and accentuation. Their system, again, is domain-
specific (generating monologue describing a film
plot) and requires the existence of a knowledge
base. The same holds for Yampolska (2007), who
presents suggestions for generating information
structure in Russian and Ukrainian football re-
ports, using rules to determine parallel structures
for the placement of contrastive accent, following
similar work by Theune (1997). While our paper
does not address the generation of speech / accen-
tuation, it is of course conceivable to employ the
IS annotated radio news corpus from which we de-
rived the label asymmetries (and which also exists
in a spoken and prosodically annotated version) in
a similar task of learning the correlations between
IS labels and pitch accents. Finally, Bresnan et
al. (2007) present work on predicting the dative
alternation in English using 14 features relating to
information status which were manually annotated
in their corpus. In our work, we manually annotate
a small corpus in order to learn generalisations.
From these we learn features that approximate the
generalisations, enabling us to apply them to large
amounts of unseen data without further manual an-
notation.
8 Conclusions
In this paper we presented a novel method of in-
cluding IS into the task of generation ranking.
Since automatic annotation of IS labels them-
selves is not currently possible, we approximate
the IS categories by their syntactic characteristics.
By calculating strong asymmetries between pairs
of IS labels, and establishing the most frequent
syntactic characteristics of these asymmetries, we
designed a new set of features for a log-linear
ranking model. In comparison to a baseline model,
we achieve statistically significant improvement in
BLEU score. We showed that these improvements
were not only due to the effect of purely syntac-
tic asymmetries, but that the IS asymmetries were
what drove the improved model.
Acknowledgments
This work was funded by the Collaborative Re-
search Centre (SFB 732) at the University of
Stuttgart.
824
References
Betty J. Birner. 1994. Information Status and Word
Order: an Analysis of English Inversion. Language,
70(2):233?259.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and
R. Harald Baayen. 2007. Predicting the Dative Al-
ternation. Cognitive Foundations of Interpretation,
pages 69?94.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17?24, Saarbru?cken, Germany. DFKI GmbH.
Herbert H. Clark and Catherine R. Marshall. 1981.
Definite Reference and Mutual Knowledge. In Ar-
avind Joshi, Bonnie Webber, and Ivan Sag, editors,
Elements of Discourse Understanding, pages 10?63.
Cambridge University Press.
Kees van Deemter and Jan Odijk. 1997. Context
Modeling and the Generation of Spoken Discourse.
Speech Communication, 21(1-2):101?121.
Cornelia Endriss and Ralf Klabunde. 2000. Planning
Word-Order Dependent Focus Assignments. In Pro-
ceedings of the First International Conference on
Natural Language Generation (INLG), pages 156?
162, Morristown, NJ. Association for Computa-
tional Linguistics.
Martin Forst. 2007. Disambiguation for a Linguis-
tically Precise German Parser. Ph.D. thesis, Uni-
versity of Stuttgart. Arbeitspapiere des Instituts
fu?r Maschinelle Sprachverarbeitung (AIMS), Vol.
13(3).
John A. Hawkins. 1978. Definiteness and Indefinite-
ness: A Study in Reference and Grammaticality Pre-
diction. Croom Helm, London.
Ron Kaplan and Joan Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammatical
Representation. In Joan Bresnan, editor, The Men-
tal Representation of Grammatical Relations, pages
173?281. MIT Press, Cambridge, MA.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), pages 388?395, Barcelona.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311?
318, Philadelphia, PA.
Scott Prevost. 1996. An Information Structural Ap-
proach to Spoken Language Generation. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 1996),
pages 294?301, Morristown, NJ.
Ellen F. Prince. 1981. Toward a Taxonomy of Given-
New Information. In P. Cole, editor, Radical Prag-
matics, pages 233?255. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. C. Mann
and S. A. Thompson, editors, Discourse Descrip-
tion: Diverse Linguistic Analyses of a Fund-Raising
Text, pages 295?325. Benjamins, Amsterdam.
Stephen G. Pulman. 1997. Higher Order Unification
and the Interpretation of Focus. Linguistics and Phi-
losophy, 20:73?115.
Arndt Riester. 2008a. A Semantic Explication of ?In-
formation Status? and the Underspecification of the
Recipients? Knowledge. In Atle Gr?nn, editor, Pro-
ceedings of Sinn und Bedeutung 12, University of
Oslo.
Arndt Riester. 2008b. The Components of Focus
and their Use in Annotating Information Struc-
ture. Ph.D. thesis, University of Stuttgart. Ar-
beitspapiere des Instituts fu?r Maschinelle Sprachver-
arbeitung (AIMS), Vol. 14(2).
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC 2006),
Genoa, Italy.
Rob van der Sandt. 1992. Presupposition Projection as
Anaphora Resolution. Journal of Semantics, 9:333?
377.
Marie?t Theune. 1997. Goalgetter: Predicting Con-
trastive Accent in Data-to-Speech Generation. In
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL/EACL
1997), pages 519?521, Madrid. Student paper.
Nadiya Yampolska. 2007. Information Structure in
Natural Language Generation: an Account for East-
Slavic Languages. Term paper. Universita?t des Saar-
landes.
825
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232?236,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Automatically Acquiring Fine-Grained
Information Status Distinctions in German
Aoife Cahill
Educational Testing Service,
660 Rosedale Road,
Princeton, NJ 08541, USA
acahill@ets.org
Arndt Riester
Institute for Natural Language Processing (IMS)
Pfaffenwaldring 5b
70569 Stuttgart, Germany
arndt.riester@ims.uni-stuttgart.de
Abstract
We present a model for automatically predict-
ing information status labels for German refer-
ring expressions. We train a CRF on manually
annotated phrases, and predict a fine-grained
set of labels. We achieve an accuracy score of
69.56% on our most detailed label set, 76.62%
when gold standard coreference is available.
1 Introduction
The automatic identification of information status
(Prince, 1981; 1992), i.e. categorizing discourse en-
tities into different classes on the given-new scale,
has recently been identified as an important issue in
natural language processing (Nissim, 2006; Rahman
and Ng, 2011; 2012). It is widely acknowledged that
information status and, more generally, information
structure,1 is reflected in word order, in the form of
referring expressions as well as in prosody. In com-
putational linguistics, the ability to automatically la-
bel text with information status, therefore, could be
of great benefit to many applications, including sur-
face realization, text-to-speech synthesis, anaphora
resolution, summarization, etc.
The task of automatically labeling text with infor-
mation status, however, is a difficult one. Part of
1Information structure is usually taken to describe clause-
internal divisions into focus-background, topic-comment, or
theme-rheme, which are in turn defined in terms of contex-
tual factors such as given-new information, salience, contrast
and alternatives, cf. Steedman and Kruijff-Korbayova? (2003),
Krifka (2007). Information status is the subfield of information
structure which exclusively deals with the given-new distinction
and which is normally confined to referring expressions.
the difficulty arises from the fact that, to a certain
degree, such labeling requires world knowledge and
semantic comprehension of the text, but another ob-
stacle is simply that theoretical notions of informa-
tion status are not used consistently in the literature.
In this paper we outline a system, trained on a
small amount of data, that achieves encouraging
results on the task of automatically labeling tran-
scribed German radio news data with fine-grained
information status labels.
2 Learning information status
A simpler variant of the task is anaphoricity de-
tection (discourse-new detection) (Bean and Riloff,
1999; Ng and Cardie, 2002; Uryupina, 2003; Denis
and Baldridge, 2007; Zhou and Kong, 2011), which
divides discourse entities into anaphoric (given) and
new. Identifying discourse-new expressions in texts
is helpful as a precursor to coreference resolution,
since, by definition, there is no need to identify an-
tecedents for new entities.
In the linguistic literature, referring expressions
have been distinguished in much more detail, and
there is reason to believe that this could also provide
useful information for NLP applications. Nissim
(2006) and Rahman and Ng (2011) developed meth-
ods to automatically identify three different classes:
OLD, MEDIATED and NEW expressions. This classi-
fication, which is described in Nissim et al (2004),
has been used for annotating the Switchboard dialog
corpus (Calhoun et al, 2010), on which both studies
are based. Most recently, Rahman and Ng (2012)
extend their automatic prediction system to a more
fine-grained set of 16 subtypes.
232
Old. The class of OLD entities in Nissim et al
(2004) is not limited to full-fledged anaphors like in
Example (1a) but also includes cases of generic and
first/second person pronouns like in (1b), which may
or may not possess a previous mention.
(1) a. Shares in General Electric rose as investors
bet that the US company would take more
lucrative engine orders for the A380.
b. I wonder where this comes from.
Mediated. The group of MEDIATED entities mainly
has two subtypes: (2a) shows an expression which
has not been mentioned before but which is depen-
dent on previous context. Such items have also been
called bridging anaphors (Poesio and Vieira, 1998).
(2b) contains a phrase which is generally known but
does not depend on the discourse context.
(2) a. Tomorrow, the Shenzhou 8 spacecraft will
be in a position to attempt the docking.
b. They hope that he will be given the right to
remain in the Netherlands.
New. The label NEW, following Nissim et al (2004:
1024), applies ?to entities that have not yet been in-
troduced in the dialog and that the hearer cannot in-
fer from previously mentioned entities.?2 Two kinds
of expressions which fall into this category are unfa-
miliar definites (3a) and (specific) indefinites (3b).
(3) a. The man who shot a policeman yesterday
has not been caught yet.
b. Klose scored a penalty in the 80th minute.
Based on work described in Nissim (2006), Rahman
and Ng (2011) develop a machine learning approach
to information-status determination. They develop a
support vector machine (SVM) model from the an-
notated Switchboard dialogs in order to predict the
three possible classes. In an extension of this work,
Rahman and Ng (2012) compare a rule-based sys-
tem to a classifier with features based on the rules to
predict 16 subtypes of the three basic types. On this
extended label set on the dialog data, they achieve
accuracy of 86.4% with gold standard coreference
and 78.7% with automatically detected coreference.
3 Extending Information Status prediction
The work we present here is most similar to that
of Rahman and Ng (2012), however, our work dif-
2Note that this definition fails to exclude cases like (2b).
fers from theirs in a number of important respects.
We (i) experiment with a different information status
classification, derived from Riester et al (2010), (ii)
use (morpho-)syntactic and functional features auto-
matically extracted from a deep linguistic parser in
our CRF sequence model, (iii) test our approach on
a different language (German), (iv) show that high
accuracy can be achieved with a limited number of
training examples, and (v) that the approach works
on a different genre (transcribed radio news bulletins
which contain complex embedded phrases like an
offer to the minority Tamil population of Sri Lanka,
not typically found in spoken dialog).
The annotation scheme by Riester et al (2010)
divides referring items differently to Nissim et al
(2004). Arguments are provided in the former pa-
per and in Baumann and Riester (to appear). As it
stands, the scheme provides too many labels for our
purpose. As a compromise, we group them in seven
classes: GIVEN, SITUATIVE, BRIDGING, UNUSED,
NEW, GENERIC and EXPLETIVE.
Given. Givenness is a central notion in informa-
tion structure theory. Schwarzschild (1999) de-
fines givenness of individual-type entities in terms
of coreference. If desired, GIVEN items can be sub-
classified, e.g. whether they are pronouns or full
noun phrases, and whether the latter are repetitions
or short forms of earlier material, or whether they
consist of lexically new material (epithets).
Situative. 1st and 2nd person pronouns, locative and
temporal adverbials, usually count as deictic expres-
sions since they refer to elements in the utterance sit-
uation. We therefore count them as a separate class.
SITUATIVE entities may, but need not, corefer.
Bridging. Bridging anaphors, as in (2a) above, have
received much attention, see e.g. Asher and Las-
carides (1998) or Poesio and Vieira (1998). Al-
though they are discourse-new, they share properties
with coreference anaphors since they depend on the
discourse context. They represent a class which can
be easily identified by human annotators but are dif-
ficult to capture by automatic techniques.
Unused. In manual annotation practice, it is very of-
ten impossible to decide whether an entity is hearer-
known, since this depends on who we assume the
hearer to be; and even if we agree on a recipient, we
may still be mistaken about their knowledge. For ex-
ample, Wolfgang Bosbach, deputy chairman of the
233
Countable Boolean Descriptive
# Words in phrase* Phrase contains a compound noun Adverbial type, e.g. locative
# Predicative phrases Phrase contains coordination Determiner type, e.g. definite *
# DPs and NPs in phrase Phrase contains time expression Left/Right-most POS tag of phrase
# top category children Phrase contains < 2, 5 or 10 words Highest syntactic node label
# Labels/titles Phrase does not have a complete parse that dominates the phrase
# Depth of syntactic phrase Phrase is a pronoun Grammatical function, e.g. SUBJ *
# Cardinal numbers Phrase contains more than 1 DP Type of pronoun, e.g. demonstrative
# Depth of syntactic phrase and 1 NP (i.e. phrase contains Syntactic shape, e.g. apposition with
ignoring unary branching an embedded argument) a determiner and attributive modifier
# Apposition phrases Head noun appears (partly or completely) Head noun type, e.g. common *
# Year phrases in previous 10 sentences * Head noun number, e.g. singular
Table 1: Features of the CRF prediction model (* indicates feature used in baseline model)
CDU parliamentary group may be known to parts
of a German audience but not to other people.
We address this by collecting both hearer-known
and hearer-unknown definite expressions into one
class UNUSED. This does not rule out further sub-
classification (known/unknown) or the possibility of
using machine learning techniques to identify this
distinction, see Nenkova et al (2005). The fact that
Rahman and Ng (2011) report the highest confusion
rate between NEW and MEDIATED entities may have
its roots in this issue.
New. Only (specific) indefinites are labeled NEW.
Generic. An issue which is not dealt with in Nissim
et al (2004) are GENERIC expressions as in Lions
have manes. Reiter and Frank (2010) discuss the
task of identifying generic items in a manner sim-
ilar to the learning tasks presented above, using a
Bayesian network. We believe it makes sense to in-
tegrate genericity detection into information-status
prediction.3
4 German data
Our work is based on the DIRNDL radio news cor-
pus of Eckart et al (2012) which has been hand-
annotated with information status labels. We choose
a selection of 6668 annotated phrases (1420 sen-
tences). This is an order of magnitude smaller than
the annotated Switchboard corpus of Calhoun et al
(2010). We parse each sentence with the German
Lexical Functional Grammar of Rohrer and Forst
(2006) using the XLE parser in order to automati-
3Note that in coreference annotation it is an open question
whether two identical generic terms should count as coreferent.
cally extract (morpho-)syntactic and functional fea-
tures for our model.
5 Prediction Model for Information Status
Cahill and Riester (2009) show that there are asym-
metries between pairs of information status labels
contained in sentences, i.e. certain classes of expres-
sions tend to precede certain other classes. We there-
fore treat the prediction of IS labels as a sequence
labeling task.4 We train a CRF using wapiti
(Lavergne et al, 2010), with the features outlined in
Table 1. We also include a basic ?coreference? fea-
ture, similar to the lexical features of Rahman and
Ng (2011), that fires if there is some lexical overlap
of nouns (or compound nouns) in the preceding 10
sentences. The original label set described in Riester
et al (2010) contains 21 labels. Here we work with
a subset of maximally 12 labels, but also consider
smaller subsets of labels and carry out a mapping to
the Nissim (2006) label set (Table 2).5 We run a 10-
fold cross-validation experiment and report average
prediction accuracy. The results are given in Table
3a. As an informed baseline, we run the same cross-
validation experiment with a subset of features that
roughly correspond to the features of Nissim (2006).
Our models perform statistically significantly better
than the baseline (p < 0.001, using the approximate
randomization test) for all label sets.
4Preliminary experimental evidence showed that the CRF
performed slightly better than a simple multiclass logistic re-
gression model (e.g. compare 72.19 to 72.43 in Table 3a).
5Unfortunately, due to underlying theoretical differences, it
is impossible to map between the Riester label set and the ex-
tended label set used in Rahman and Ng (2012).
234
Total Riester 1 Riester 2 Riester 3 Nissim ?06
462 GIVEN- GIVEN-
GIVEN OLD
PRONOUN PRONOUN
143 GIVEN- GIVEN-REFLEXIVE REFLEXIVE
427 GIVEN-EPITHET
169 GIVEN- GIVEN-REPEATED NOUN
204 GIVEN-SHORT
265 SITUATIVE SITUATIVE SITUATIVE
449 BRIDGING BRIDGING BRIDGING
MEDIATED1271 UNUSED- UNUSED- UNUSEDKNOWN KNOWN
1227 UNUSED- UNUSED-
NEWUNKNOWN UNKNOWN1282 NEW NEW NEW
632 GENERIC GENERIC GENERIC
96 EXPLETIVE EXPLETIVE EXPLETIVE OTHER
Table 2: Varying the granularity of the label sets
As expected, the less fine-grained a label set, the
easier it is to predict the labels. It remains for fu-
ture work to show the effect of different label set
granularities in practical applications. We approx-
imate gold standard coreference information from
the manually annotated labels (e.g. all GIVEN la-
bel types are by their nature coreferent), and carry
out an experiment with gold-standard approximation
of coreference marking. These results are also re-
ported in Table 3a. Here we see a clear performance
difference in the effect of gold-standard corefer-
ence on the Riester label set (increasing around 6-
10%), compared to the Nissim label set (decreasing
slightly). This is an artifact of the way the mapping
was carried out, deriving the gold standard corefer-
ence information from the Riester label set. There is
not a one-to-one mapping between OLD and GIVEN,
and, in the Riester label set, coreferential entities
that are labeled as SITUATIVE (deictic terms) are not
recognized as such.
The feature set in Table 1 reflects the morpho-
syntactic properties of the phrases to be labeled.
Sometimes world knowledge is required in order
to be able to accurately predict a label; for exam-
ple, to know that the pope can be categorized as
UNUSED-KNOWN, because it can occur discourse-
initially, whereas the priest must usually be cate-
gorized as GIVEN. The BRIDGING relationship is
also difficult to capture without some world knowl-
edge. For example, to infer that the waitress can
be categorized as BRIDGING in the context of the
restaurant requires information that links the two
concepts. Rahman and Ng (2012) also note this and
include features based on FrameNet, WordNet and
the ReVerb corpus for English.
For German, we address this issue by introducing
two further types of features into our model based on
the GermaNet resource (Hamp and Feldweg, 1997).
The first type is based on the GermaNet synset of
the head noun in the phrase and its distance from the
root node (the assumption is that entities closer to
root are more generic than those further away). The
second include the sum and maximum of the Lin
semantic relatedness measures (Lin, 1998) of how
similar the head noun of the phrase is to the other
nouns in current and immediately preceding sen-
tence surrounding the phrase (calculated with Ger-
maNet Pathfinder; Finthammer and Cramer, 2008).
The results are given in Table 3b. Here we see a
consistent increase in performance of around 4% for
each label set over the model that does not include
the GermaNet features. Again, we see the same de-
crease in performance on the Nissim label set when
using gold standard coreference information.
Label Set Accuracy Gold Baseline
coref. feats.
Riester 1 65.49 72.49 57.25
Riester 2 67.21 76.88 58.82
Riester 3 72.43 82.22 64.20
Nissim ?06 76.24 74.06 71.70
(a) Only morpho-syntactic features
Label Set Accuracy Gold coreference
Riester 1 69.56 76.62
Riester 2 71.99 79.86
Riester 3 75.82 84.76
Nissim ?06 79.61 78.46
(b) Morpho-syntactic + GermaNet features
Table 3: Cross validation accuracy results
6 Conclusion
In this paper we presented a model for automatically
labeling German text with fine-grained information
status labels. The results reported here show that we
can achieve high accuracy prediction on a complex
text type (transcribed radio news), even with a lim-
ited amount of data.
235
References
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83?113.
Stefan Baumann and Arndt Riester. to appear. Ref-
erential and Lexical Givenness: Semantic, Prosodic
and Cognitive Aspects. In G. Elordieta and P. Prieto,
editors, Prosody and Meaning. Mouton de Gruyter,
Berlin.
David L. Bean and Ellen Riloff. 1999. Corpus-Based
Identification of Non-Anaphoric Noun Phrases. In
Proceedings of ACL, pages 373?380, College Park,
MD.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of ACL-IJCNLP, pages 817?825, Singapore.
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,
Dan Jurafsky, Mark Steedman, and David Beaver.
2010. The NXT-Format Switchboard Corpus: A
Rich Resource for Investigating the Syntax, Seman-
tics, Pragmatics and Prosody of Dialogue. Language
Resources and Evaluation, 44(4):387?419.
Pascal Denis and Jason Baldridge. 2007. Global Joint
Determination of Anaphoricity and Coreference Res-
olution Usinger Integer Programming. In Proceedings
of ACL-HLT, Rochester, NY.
Kerstin Eckart, Arndt Riester, and Katrin Schweitzer.
2012. A Discourse Information Radio News Database
for Linguistic Analysis. In C. Chiarcos et al, edi-
tors, Linked Data in Linguistics, pages 65?76, Berlin.
Springer.
Marc Finthammer and Irene Cramer. 2008. Exploring
and Navigating: Tools for GermaNet. In Proceedings
of LREC, Marrakech, Morocco.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet ? a
Lexical-Semantic Net for German. In Proceedings of
the ACL Workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Manfred Krifka. 2007. Basic Notions of Information
Structure. In C. Fe?ry and M. Krifka, editors, The No-
tions of Information Structure, pages 57?68. Univer-
sita?tsverlag Potsdam.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical Very Large Scale CRFs. In Proceed-
ings of ACL, pages 504?513.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In International Conference on Machine
Learning, pages 296?304.
Ani Nenkova, Advaith Siddharthan, and Kathleen McK-
eown. 2005. Automatically Learning Cognitive Sta-
tus for Multi-Document Summarization of Newswire.
In Proceedings of HLT/EMNLP, pages 241?248, Van-
couver.
Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-Anaphoric Noun Phrases to Im-
prove Coreference Resolution. In Proceedings of
COLING, pages 730?736, Taipei, Taiwan.
Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark
Steedman. 2004. An Annotation Scheme for Infor-
mation Status in Dialogue. In Proceedings of LREC,
Lisbon.
Malvina Nissim. 2006. Learning Information Status of
Discourse Entities. In Proceedings of EMNLP, pages
94?102, Sydney.
Massimo Poesio and Renata Vieira. 1998. A Corpus-
Based Investigation of Definite Description Use.
Computational Linguistics, 24(2).
Ellen F. Prince. 1981. Toward a Taxonomy of Given-
New Information. In P. Cole, editor, Radical Prag-
matics, pages 233?255. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG Letter: Subjects, Def-
initeness and Information Status. In W. Mann and
S. Thompson, editors, Discourse Description, pages
295?325. Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the Infor-
mation Status of Noun Phrases in Spoken Dialogues.
In Proceedings of EMNLP, pages 1069?1080, Edin-
burgh.
Altaf Rahman and Vincent Ng. 2012. Learning the Fine-
Grained Information Status of Discourse Entities. In
Proceedings of EACL 2012, Avignon, France.
Nils Reiter and Anette Frank. 2010. Identifying Generic
Noun Phrases. In Proceedings of ACL, pages 40?49,
Uppsala, Sweden.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A Recursive Annotation Scheme for Referential In-
formation Status. In Proceedings of LREC, Valletta,
Malta.
Christian Rohrer and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-Scale LFG
for German. In Proceedings of LREC, Genoa, Italy.
Roger Schwarzschild. 1999. GIVENness, AvoidF, and
other Constraints on the Placement of Accent. Natural
Language Semantics, 7(2):141?177.
Mark Steedman and Ivana Kruijff-Korbayova?. 2003.
Discourse Structure and Information Structure. Jour-
nal of Logic, Language and Information, 12:249?259.
Olga Uryupina. 2003. High-precision Identification of
Discourse New and Unique Noun Phrases. In Pro-
ceedings of the ACL Student Workshop, pages 80?86,
Sapporo.
Guodong Zhou and Fang Kong. 2011. Learning Noun
Phrase Anaphoricity in Coreference Resolution via
Label Propagation. Journal of Computer Science and
Technology, 26(1).
236
