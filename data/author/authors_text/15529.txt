Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 700?711,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
The Imagination of Crowds: Conversational AAC Language Modeling using
Crowdsourcing and Large Data Sources
Keith Vertanen
Department of Computer Science
Princeton University
vertanen@princeton.edu
Per Ola Kristensson
School of Computer Science
University of St Andrews
pok@st-andrews.ac.uk
Abstract
Augmented and alternative communication
(AAC) devices enable users with certain com-
munication disabilities to participate in every-
day conversations. Such devices often rely
on statistical language models to improve text
entry by offering word predictions. These
predictions can be improved if the language
model is trained on data that closely reflects
the style of the users? intended communica-
tions. Unfortunately, there is no large dataset
consisting of genuine AAC messages. In this
paper we demonstrate how we can crowd-
source the creation of a large set of fictional
AAC messages. We show that these messages
model conversational AAC better than the cur-
rently used datasets based on telephone con-
versations or newswire text. We leverage our
crowdsourced messages to intelligently select
sentences from much larger sets of Twitter,
blog and Usenet data. Compared to a model
trained only on telephone transcripts, our best
performing model reduced perplexity on three
test sets of AAC-like communications by 60?
82% relative. This translated to a potential
keystroke savings in a predictive keyboard in-
terface of 5?11%.
1 Introduction
Users with certain communication disabilities
rely on augmented and alternative communication
(AAC) devices to take part in everyday conversa-
tions. Often these devices consist of a predictive
text input method coupled with text-to-speech out-
put. Unfortunately, the text entry rates provided by
AAC devices are typically low, between 0.5 and 16
words-per-minute (Trnka et al, 2009).
As a consequence, researchers have made nu-
merous efforts to increase AAC text entry rates by
employing a variety of improved language model-
ing techniques. Examples of approaches include
adapting the language model to recently used words
(Wandmacher et al, 2008; Trnka, 2008), using syn-
tactic information (Hunnicutt, 1989; Garay-Vitoria
and Gonza?lez-Abascal, 1997), using semantic in-
formation (Wandmacher and Antoine, 2007; Li
and Hirst, 2005), and modeling topics (Lesher and
Rinkus, 2002; Trnka et al, 2006). For a recent sur-
vey, see Garay-Vitoria and Abascal (2006).
While such language model improvement tech-
niques are undoubtedly helpful, certainly they can
all benefit from starting with a long-span language
model trained on large amounts of closely matched
data. For AAC devices this means closely modeling
everyday face-to-face communications. However,
a long-standing problem in the field is the lack of
good data sources that adequately model such AAC
communications. Due to privacy-reasons and other
ethical concerns, there is no large dataset consist-
ing of genuine AAC messages. Therefore, previous
research has used transcripts of telephone conversa-
tions or newswire text. However, these data sources
are unlikely to be an ideal basis for AAC language
models.
In this paper we show that it is possible to signif-
icantly improve conversational AAC language mod-
eling by first crowdsourcing the creation of a fic-
tional collection of AAC messages on the Amazon
Mechanical Turk microtask market. Using a care-
700
fully designed microtask we collected 5890 mes-
sages from 298 unique workers. As we will see,
word-for-word these fictional AAC messages are
better at predicting AAC test sets than a wide-range
of other text sources. Further, we demonstrate that
Twitter, blog and Usenet data outperform telephone
transcripts or newswire text.
While our crowdsourced AAC data is better than
other text sources, it is too small to train high-quality
long-span language models. We therefore investi-
gate how to use our crowdsourced collection to in-
telligently select AAC-like sentences from Twitter,
blog and Usenet data. We compare a variety of
different techniques for doing this intelligent selec-
tion. We find that the best selection technique is the
recently proposed cross-entropy difference method
(Moore and Lewis, 2010). Using this method, we
build a compact and well-performing mixture model
from the Twitter, blog and Usenet sentences most
similar to our crowdsourced data.
We evaluate our mixture model on four different
test sets. On the three most AAC-like test sets, we
found substantial reductions in not only perplexity
but also in potential keystroke savings when used
in a predictive keyboard interface. Finally, to aid
other AAC researchers, we have publicly released
our crowdsourced AAC collection, word lists and
best-performing language models1.
2 Crowdsourcing AAC-like Messages
As we mentioned in the introduction, there are un-
fortunately no publicly available sources of gen-
uine conversational AAC messages. We conjectured
we could create surrogate data by asking workers
on Amazon Mechanical Turk to imagine they were
a user of an AAC device and having them invent
things they might want to say. While crowdsourcing
is commonly used for simple human computation
tasks, such as labeling images and transcribing au-
dio, it is an open research question whether we can
leverage workers? creativity to invent plausible and
useful AAC-like messages. In this section, we de-
scribe our carefully constructed microtask and com-
pare how well our collected messages correspond to
communications from actual AAC users.
1http://www.aactext.org/imagine/
Figure 1: The interface for HITs of type 1 in our
crowdsourced data collection.
Figure 2: The interface for HITs of type 2 in our
crowdsourced data collection.
2.1 Collection Tasks
To collect our data, we used two different types
of human intelligence tasks (HITs). In type 1, the
workers were told to imagine that due to an accident
or medical condition they had to use a communica-
tion device to speak for them. Workers were asked
to invent a plausible communication. Workers were
prevented from pasting text. After several pilot ex-
periments, we arrived at the instructions shown in
figure 1.
In type 2, a worker first judged the plausibility
of a communication written by a previous worker
(figure 2). After judging, the worker was asked
to ?invent a completely new communication? as if
the worker was the AAC user. Workers were pre-
vented from pasting text or typing the identical text
as the one just judged. The same communication
701
was judged by three separate workers. In this work
we did not make use of these judgments.
2.2 Data Cleaning
While most workers produced plausible and often
creative communications, some workers entered ob-
vious garbage. These workers were identified by a
quick visual scan of the submitted communications.
We rejected the work of 9% of the workers in type
1 and 4% of the workers in type 2. After removing
these workers, we had 2481 communications from
type 1 and 4440 communications from type 2.
After combining the data from all accepted HITs,
we conducted further semi-automatic data clean-
ing. We first manually reviewed communications
sorted by worker. We removed workers whose text
was non-fluent English or not plausible (e.g. some
workers entered news headlines or proverbs). Iden-
tical communications from the same worker were
removed. We removed communications with an
out-of-vocabulary (OOV) rate of over 20% with re-
spect to a large word list of 330K words obtained
from human-edited dictionaries2. We also removed
communications that were all in upper case, con-
tained common texting abbreviations (e.g. ?plz?,
?ru?, ?2day?), communications over 80 characters,
and communications with excessive letter repeti-
tions (e.g. ?yippeeee?). After cleaning, we had 5890
messages from 298 unique workers.
2.3 Results
Tables 1 and 2 show some example communications
obtained in each HIT type. Sometimes, but not al-
ways, type 2 resulted in the worker writing a similar
communication as the one judged. This is a mixed
blessing. While it may reduce the diversity of com-
munications, we found that workers were more ea-
ger to accept HITs of type 2. The average HIT com-
pletion time was also shorter, 24 seconds in type 2
versus 36 seconds in type 1. While we initially paid
$0.04/HIT for both types, we found in subsequent
rounds that we could pay $0.02/HIT for type 2. We
also had to reject less work in type 2 and qualita-
tively found the communications to be more AAC-
like. Since workers had to imagine themselves in a
2We combined Wiktionary, Webster?s dictionary provided
by Project Gutenberg, the CMU pronouncing dictionary and
GNU aspell.
Is the dog friendly?
Can I have some water please?
I need to start making a shopping list soon.
What I would really like right now is a plate of fruit.
Who will drive me to the doctor?s office tomorrow?
Table 1: Example communications from type 1.
Can you bring my slippers?
I am cold, is there another blanket.
How did Pam take the news?
Bring the fuzzy slippers here.
Did you have breakfast?
why are you so late?
I am pretty hungry, can we go eat?
I had bacon eggs and hashbrowns for breakfast.
Table 2: Example communications from type 2. The
text in bold is the message workers judged. It is fol-
lowed in plain text by the workers? new messages.
very unfamiliar situation, it appears that providing a
concrete example was helpful to workers.
3 Comparison of Training Sources
In this section, we compare the predictive perfor-
mance of language models trained on our Turk AAC
data with models trained on other text sources. We
use the following training sets:
? NEWS ? Newspaper articles from the CSR-III
(Graff et al, 1995) and Gigaword corpora (Graff,
2003). 60M sentences, 1323M words.
? WIKIPEDIA ? Current articles and discussion
threads from a snapshot of Wikipedia (January 3,
2008). 24M sentences, 452M words.
? USENET ? Messages from a Usenet corpus
(Shaoul and Westbury, 2009). 123M sentences,
1847M words.
? SWITCHBOARD ? Transcripts of 2217 telephone
conversations from the Switchboard corpus (God-
frey et al, 1992). Due to its conversational style,
this corpus has been popular for AAC language
modeling (Lesher and Rinkus, 2002; Trnka et al,
2009). 0.2M sentences, 2.6M words.
? BLOG ? Blog posts from the ICWSM corpus
(Burton et al, 2009). 25M sentences, 387M
words.
702
? TWITTER ? We collected Twitter messages via
the streaming API between December 2010 and
March 2011. We used the free Twitter stream
which provides access to 5% of all tweets. Twit-
ter may be particularly well suited for modeling
AAC communications as tweets are short typed
messages that are often informal person-to-person
communications. Twitter has previously been
proposed as a candidate for modeling conversa-
tions, see for example Ritter et al (2010). 7M
sentences, 55M words.
? TURKTRAIN ? Communications from 80% of the
workers in our crowdsourced collection. 4981
sentences, 24860 words.
WIKIPEDIA, USENET, BLOG and TWITTER all
consisted of raw text that required significant filter-
ing to eliminate garbage, spam, repeated messages,
XML tags, non-English text, etc. Given the large
amount of data available, our approach was to throw
away any text that did not appear to be a sensible
English sentence. For example, we eliminated any
sentence having a large number of words not in our
330K word list.
3.1 Test Sets
We evaluated our models on the following test sets:
? COMM ? Sentences written in response to hy-
pothetical communication situations collected by
Venkatagiri (1999). We removed nine sentences
containing numbers. This set is used throughout
the paper. 251 sentences, 1789 words.
? SPECIALISTS ? Context specific phrases sug-
gested by AAC specialists3. This set is used
throughout the paper. 952 sentences, 3842 words.
? TURKDEV ? Communications from 10% of the
workers in our crowdsourced collection (disjoint
from TURKTRAIN and TURKTEST). This set will
be used for initial evaluations and also to tune our
models. 551 sentences, 2916 words.
? TURKTEST ? Communications from 10% of the
workers in our crowdsourced collection (disjoint
from TURKTRAIN and TURKDEV). This set is
used only in the final evaluation section. 563 sen-
tences, 2721 words.
3http://aac.unl.edu/vocabulary.html
Test set Sentence
COMM I love your new haircut.
COMM How many children do you have?
SPECIALISTS Are you sure you don?t mind?
SPECIALISTS I?ll keep an eye on that for you
SWITCHTEST yeah he?s a good actor though
SWITCHTEST what did she have like
Table 3: Examples from three of our test sets.
? SWITCHTEST ? Transcripts of three Switchboard
conversations (disjoint from the SWITCHBOARD
training set). This is the same set used in Trnka et
al. (2009). We dropped one sentence containing a
dash. This set is only used in the final evaluation
section. 59 sentences, 508 words.
TURKDEV and TURKTEST contain text similar
to table 1 and 2. Table 3 shows some examples from
the other three test sets. Sentences in COMM tended
to be richer in vocabulary and subject matter than
those in SPECIALISTS. The SPECIALISTS sentences
tended to be general phrases that avoided mention-
ing specific situations, proper names, etc. Sentences
in SWITCHTEST exhibited phenomena typical of
human-to-human voice conversations (filler words,
backchannels, interruptions, etc).
3.2 Language Model Training
All language models were trained using the SRILM
toolkit (Stolcke, 2002). All models used interpo-
lated modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998). In this sec-
tion, we trained 3-gram language models with no
count-cutoffs. All text was converted to lowercase
and we removed punctuation except for apostrophes.
We believe punctuation would likely slow down a
user?s conversation for only a small potential advan-
tage (e.g. improving text-to-speech prosody).
All models used a vocabulary of 63K words in-
cluding an unknown word. We obtained our vocab-
ulary by taking all words occurring in TURKTRAIN
and all words occurring four or more times in the
TWITTER training set. We restricted our vocabu-
lary to words from our large list of 330K words.
This restriction prevented the inclusion of com-
mon misspellings prevalent in many of our train-
ing sets. Our 63K vocabulary resulted in low OOV
703
l l l l l l l l l l l
4 6 8 10 12 14 16 18 20 22 240
500
1000
1500
Words of training data (K)
Avera
ge per
plexity
l
NewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain
(a) TURKDEV test set
l
l l l l l l l l l l
4 6 8 10 12 14 16 18 20 22 240
500
1000
1500
Words of training data (K)
Avera
ge per
plexity
l
NewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain
(b) COMM test set
l l l l l l l l l l l
4 6 8 10 12 14 16 18 20 22 240
500
1000
1500
Words of training data (K)
Avera
ge per
plexity
l
NewsWikipediaUsenetSwitchboardBlogTwitterTurkTrain
(c) SPECIALISTS test set
Figure 3: Perplexity of language models trained on the same amount of data from different sources. The
perplexity is the average of 20 models trained on random subsets of the training data (one standard deviation
error bars).
rates for all test sets: COMM 0%, SPECIALISTS
0.05%, TURKDEV 0.1%, TURKTEST 0.07%, and
SWITCHTEST 0.8%.
3.3 Small Training Size Experiment
We trained language models on each dataset, vary-
ing the number of training words from 4K to 24K
(the limit of the TURKTRAIN set). For each dataset
and training amount, we built 20 different models by
choosing sentences from the full training set at ran-
dom. We computed the mean and standard deviation
of the per-word perplexity of the set of 20 models.
As shown in figure 3, word-for-word the TURK-
TRAIN data was superior for our three most AAC-
like test sets. Thus it appears our crowdsourcing pro-
cedure was successful at generating AAC-like data.
TWITTER was consistently the second best. BLOG,
USENET and SWITCHBOARD also performed well.
3.4 Large Training Size Experiment
The previous experiment used a small amount of
training data. We selected the best three datasets
having tens of millions of words of training data:
USENET, BLOG, and TWITTER. As in the previ-
ous experiment, we computed the mean and stan-
dard deviation of the per-word perplexity of a set
of 20 models. Increasing the amount of training
data substantially reduced perplexity compared to
our small TURKTRAIN collection (figure 4). Tweets
were clearly well suited for modeling AAC-like text
as 3M words of TWITTER data was better than 40M
words of BLOG data.
3.5 Comparison with Real AAC Data
Beukelman et al (1984) analyzed the communica-
tions made by five nonspeaking adults over 14 days.
All users were experienced using a tape-typewriter
AAC device. Beukelman gives a ranked list of the
top 500 words, the frequency of the top 20 words,
and statistics calculated on the communications.
For the top 10 words in Beukelman?s AAC user
data, we computed the probability of each word in
our various datasets (figure 5). As shown, some
words such as ?to? and ?a? occur with similar fre-
quency across all datasets. Some words such as
?the? are overrepresented in data such as news text.
Other words such as ?I? and ?you? are much more
variable. Our Turk data has the closest matching
frequency for the most popular word ?I?. Interest-
ingly, our Turk data shows a much higher probabil-
ity for ?you? than the AAC data. We believe this re-
sulted from the situation we asked workers to imag-
ine (i.e. communicating via a letter-at-a-time scan-
ning interface). Workers presumed in such a situa-
tion they would need to ask others to do many tasks.
We observed many requests in the data such as ?Can
704
l l l l l l l l l l l l l l l l l l
0 10 20 30 400
50
100
150
200
Words of training data (M)
Avera
ge pe
rplexit
y l
UsenetBlogTwitter
(a) TURKDEV test set
l l l l l l l l l l l l l l l l l l
0 10 20 30 400
50
100
150
200
Words of training data (M)
Avera
ge pe
rplexit
y l
UsenetBlogTwitter
(b) COMM test set
l l l l l l l l l l l l l l l l l l
0 10 20 30 400
50
100
150
200
Words of training data (M)
Avera
ge pe
rplexit
y l
UsenetBlogTwitter
(c) SPECIALISTS test set
Figure 4: Perplexity of language models trained on increasing amounts of data from three different training
sources. Results on the TURKDEV, COMM and SPECIALISTS test sets.
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
i to you the a it my and in is
Un
igr
am
 p
ro
ba
bi
lit
y 
AAC users, Beukelman
Turk workers
Switchboard
Twitter
Blog
Usenet
Wikipedia
News
Figure 5: The unigram probabilities of the top 10 words reported by Beukelman et al (1984).
you change my sheets?? and ?Can you walk the dog
for me??
Beukelman reports 33% of all communications
could be made using only the top 500 words. The
same 500 words allowed writing of 34% of our Turk
communications. Other datasets exhibited much
lower percentages. Note that this is at least partially
due to the longer sentences present in some datasets.
Unfortunately, Beukelman does not report the aver-
age communication length. Our Turk communica-
tions were 5.0 words on average. The next shortest
dataset was TWITTER with 7.5 words per communi-
cation. Despite their short average length, only 10%
of Tweets could be written using the top 500 words.
Beukelman reports that 80% of words in the AAC
users? communications were in the top 500 words.
81% of the words in our crowdsourced data were in
this word list. For comparison, only 65% of words
in our TWITTER data were in the 500 word vocabu-
lary. While our TURKTRAIN set contains only 2141
unique words, this may in fact be good since it has
been argued that rare words have received too much
attention in AAC (Baker et al, 2000).
4 Using Large Datasets Effectively
In the previous section, we found our crowdsourced
data was good at predicting AAC-like test sets.
However, in order to build a good long-span lan-
guage model, we would require millions of such
communications. Crowdsourcing such a large col-
lection would be prohibitively expensive. There-
fore, we instead investigated how to use our crowd-
sourced data to intelligently select AAC-like data
from other large datasets. For large datasets, we
used TWITTER, BLOG and USENET as they were
both large and well-matched to AAC data.
4.1 Selecting AAC-like Data
For each training sentence, we calculated three val-
ues:
? WER ? The minimum word error rate between
the training sentence and one of the crowdsourced
705
lllllllllllllllllll l lll l l l l l l
l
0 5 10 15 20 25
40
60
80
100
120
140
Language model parameters (M)
Perp
lexity
l
Entropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference
(a) TWITTER
l l l l l l l l l l l l l l l l
0 5 10 15 20 25
40
60
80
100
120
140
Language model parameters (M)
Perp
lexity
l
Entropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference
(b) BLOG
l l l l l l l l l
l
ll
l l l l l l l
0 5 10 15 20 25
40
60
80
100
120
140
Language model parameters (M)
Perp
lexity
l
Entropy pruningCount cutoff pruningCross?entropy selectionWER selectionCross?entropy difference
(c) USENET
Figure 6: Perplexity on TURKDEV using different data selection and pruning techniques.
communications. This is the minimum number of
words that must be inserted, substituted or deleted
to transform the training sentence into a TURK-
TRAIN sentence divided by the number of words
in the TURKTRAIN sentence. For example, the
training sentence ?I didn?t sleep well Monday
night either? was given a WER of 0.33 because
two word-changes transformed it into a message
written by a worker: ?I didn?t sleep well last
night?.
? Cross-entropy, in-domain ? The average per-word
cross-entropy of the training sentence under a 3-
gram model trained on TURKTRAIN.
? Cross-entropy, background ? The average per-
word cross-entropy of the training sentence un-
der a 3-gram model trained on a random portion
of the training set. The random portion was the
same size as TURKTRAIN.
We used these values to limit training to only
AAC-like sentences. We tried three different selec-
tion methods. In WER selection, only sentences be-
low a threshold on the word error rate were kept in
the training data. This tends to find variants of exist-
ing communications in our Turk collection.
In cross-entropy selection, we used only sen-
tences below a threshold on the per-word cross-
entropy with respect to a TURKTRAIN language
model. This is equivalent to placing a threshold on
the perplexity. Previously this technique has been
used to improve language models based on web data
(Bulyko et al, 2007; Gao et al, 2002) and to con-
struct domain-specific models (Lin et al, 1997).
In cross-entropy difference selection, a sentence?s
score is the in-domain cross-entropy minus the back-
ground cross-entropy (Moore and Lewis, 2010).
This technique has been used to supplement Euro-
pean parliamentary text (48M words) with newswire
data (3.4B words) (Moore and Lewis, 2010). We
were curious how this technique would work given
our much smaller in-domain set of 24K words.
4.2 Data Selection and Pruning
We built models selecting sentences below different
thresholds on the WER, in-domain cross-entropy, or
cross-entropy difference. For comparison, we also
pruned our models using conventional count-cutoff
and entropy pruning (Stolcke, 1998). During en-
tropy pruning, we used a Good-Turing estimated
model for computing the history marginals as the
lower-order Kneser-Ney distributions are unsuitable
for this purpose (Chelba et al, 2010).
We calculated the perplexity of each model on
three test sets. We also tallied the number of model
parameters (all n-gram probabilities plus all backoff
weights). On TURKDEV, cross-entropy difference
selection performed the best for all models sizes and
for all training sets (figure 6). We also found cross-
706
l l l l l l l l l l l l l l l
l l l l
l l
40
50
60
70
80
90
100
Threshold
Perple
xity
?1 0 1
l
2?gram3?gram4?gram
(a) TWITTER
l l l l l l l lllllll
llll
l l l
l l
l l
l
Threshold?1 0 1
l
2?gram3?gram4?gram
(b) BLOG
l l l l l l l l l lllllll
lllll
llll
lll
l
Threshold?1.4 ?0.4 0.4
l
2?gram3?gram4?gram
(c) USENET
Figure 7: Perplexity on TURKDEV varying the
cross-entropy difference threshold.
entropy difference was the best on COMM, reducing
perplexity by 10?20% relative compared to cross-
entropy selection. Results on SPECIALISTS showed
that WER and both forms of cross-entropy selection
performed similarly. All three data selection meth-
ods were superior to count-cutoff or entropy prun-
ing. We use cross-entropy difference selection for
the remainder of this paper.
4.3 Model Order and Optimal Thresholds
We created 2-gram, 3-gram and 4-gram models on
TWITTER, BLOG, and USENET using a range of
cross-entropy difference thresholds. 4-gram models
slightly outperformed 3-gram models (figure 7). The
optimal threshold for 4-gram models were as fol-
lows: TWITTER 0.0, BLOG -0.4, and USENET -0.7.
These thresholds resulted in using 20% of TWIT-
TER, 5% of BLOG, and 1% of USENET.
4.4 Mixture Model
We created a mixture model using linear interpo-
lation from the TWITTER, USENET and BLOG 4-
gram models created with each set?s optimal thresh-
old. The mixture weights were optimized with re-
spect to TURKDEV using SRILM. The final mix-
ture weights were: TWITTER 0.42, BLOG 0.29, and
USENET 0.29. Our final 4-gram mixture model had
43M total parameters and a compressed disk size of
316 MB.
5 Evaluation
In this section, we compare our mixture model
against baseline models. We show performance with
respect to usage in a typical AAC text entry interface
based on word prediction.
5.1 Predictive Text Entry
Many AAC communication devices use word pre-
dictions. In a word prediction interface users type
letters and the interface offers word completions
based on the prefix of the current word and often the
prior text. By selecting one of the predictions, the
user can potentially save keystrokes as compared to
typing out every letter of each word.
We assume a hypothetical predictive keyboard in-
terface that displays five word predictions. Our key-
board makes predictions based on up to three words
of prior context. Our keyboard predicts words even
before the first letter of a new word is typed. As
a user types letters, predictions are limited to words
consistent with the typed letters. If the system makes
a correct prediction, we assume it takes only one
keystroke to enter the word and any following space.
We only predict words in our 63K word vocab-
ulary (empty prediction slots are possible). We dis-
play a word even if it was already a proposed predic-
tion for a shorter prefix of the current word. The first
word in a sentence is conditioned on the sentence-
start pseudo-word. If an out-of-vocabulary word is
typed, the word is replaced in the language model?s
context with the unknown pseudo-word.
We evaluate our predictive keyboard using the
common metric of keystroke savings (KS):
KS =
(
1?
(kp
ka
))
? 100%,
where kp is the number of keystrokes required with
word predictions and ka is the number of keystrokes
required without word prediction.
5.2 Predictive Performance Experiment
We compared our mixture model using cross-
entropy difference selection with three baseline
models trained on all of TWITTER, SWITCHBOARD
and TURKTRAIN. The baseline models were un-
pruned 4-gram models trained using interpolated
modified Kneser-Ney smoothing. They had 72M,
5M, and 129K parameters respectively.
As shown in table 4, our mixture model per-
formed the best on the three most AAC-like test
sets (COMM, SPECIALISTS, and TURKTEST). The
707
LM Test set PPL KS
Mixture COMM 47.9 62.5%
Twitter COMM 55.9 60.9%
Switchboard COMM 151.1 54.4%
Turk COMM 165.9 52.7%
Mixture SPECIALISTS 25.7 63.1%
Twitter SPECIALISTS 27.3 61.9%
Switchboard SPECIALISTS 64.5 57.7%
Turk SPECIALISTS 85.9 52.8%
Mixture TURKTEST 31.2 62.0%
Twitter TURKTEST 42.3 59.3%
Switchboard TURKTEST 172.5 50.6%
Turk TURKTEST 51.0 57.6%
Mixture SWITCHTEST 174.3 52.8%
Twitter SWITCHTEST 142.6 54.9%
Switchboard SWITCHTEST 79.2 58.8%
Turk SWITCHTEST 642.5 42.9%
Table 4: Perplexity (PPL) and keystroke savings
(KS) of different language models on four test sets.
The bold line shows the best performing language
model on each test set.
mixture model provided substantial increases in
keystroke savings compared to a model trained
solely on Switchboard. The mixture model also per-
formed better than simply training a model on a
large amount of Twitter data. The model trained on
only 24K words of Turk data did surprisingly well
given its extremely limited training data.
Our Switchboard model performed the best on
SWITCHTEST with a keystroke savings of 58.8%.
For comparison, past work reported a keystroke sav-
ings of 55.7% on SWITCHTEST using a 3-gram
model trained on Switchboard (Trnka et al, 2009).
While our mixture model performed less well on
SWITCHTEST (52.8%), it is likely the other three
test sets better represent AAC communications.
5.3 Larger Mixture Model Experiment
Our mixture language model used the best thresh-
olds with respect to TURKDEV. This resulted in
throwing away most of the training data. This might
be suboptimal in practice if an AAC user?s com-
munications are somewhat different or more diverse
than the language generated by the Turk workers.
We trained a series of mixture models in which
we varied the cross-entropy difference thresholds
l l l l l l l l l l l l l l l l
50
55
60
65
Change from optimal thresholds
Keys
troke
 sav
ings 
(%)
?0.5 0.0 0.5 1.0
l
SpecialistCommTurkTestSwitchTest
Figure 8: Keystroke savings on mixture models
varying a constant added to the optimal thresholds
with respect to TURKDEV.
by adding a constant to all three thresholds. The
mixture weights for each new model were opti-
mized with respect to TURKDEV. Using somewhat
larger models did improve keystroke savings for all
test sets except for TURKTEST (figure 8). How-
ever, using too large thresholds eventually hurt per-
formance except on SWITCHTEST. Performance
on SWITCHTEST steadily increased from 52.8% to
56.6%. These gains however came at the cost of big-
ger models. The model using +1.0 of the optimal
thresholds had 384M parameters and a compressed
size of 3.0 GB.
6 Discussion
Given the ethical implications of collecting mes-
sages from actual AAC users, it is unlikely that a
large corpus of genuine AAC messages will ever be
available to researchers. An important finding in
this paper is that crowdsourcing can be an effective
way to obtain surrogate data for improving AAC lan-
guage models. Another finding is that Twitter pro-
vides a continuous stream of large amounts of very
AAC-like data. Twitter also has the advantage of al-
lowing models to be continually updated to reflect
current events, new vocabulary, etc.
6.1 Limitations and Implications
We collected data from a large number of workers,
some of whom may have written only a single com-
708
munication. This may have resulted in more mes-
sages about simple situations and perceived needs
which could differ from true AAC usage.
Our data does not contain long-term two-sided
conversations. Thus it may not be as useful for eval-
uating techniques that adapt to past messages or that
use the conversation partner?s communications.
We asked workers to imagine they were using
a scanning-style AAC device. We believe this led
workers to presume they would require assistance
in many routine physical tasks. Our workers were
(presumably) without cognitive or language impair-
ments. Thus our collection is more representative
of one subgroup of AAC communicators (scanning
users with normal cognitive function and language
skills). By modifying the situation given to workers,
it is likely we can expand our collection to better rep-
resent other groups of AAC users, such as those us-
ing predictive keyboards or eye-trackers. However,
obtaining data representative of users with cognitive
or language impairments via crowdsourcing would
probably be difficult.
While we were unable to obtain real AAC mes-
sages for testing, we believe the COMM and SPE-
CIALIST test sets provide a good indication of the
real-world potential for our methods. Our collected
Turk data was compared with reported data from ac-
tual AAC users (though this comparison was neces-
sarily coarse-grained). We hope that by releasing
our data and models it may be possible for those
privy to real AAC communications to validate and
report about the techniques described in this paper.
We evaluated our models in terms of perplexity
and keystrokes savings within the auspices of a pre-
dictive keyboard. Further work is needed to inves-
tigate how our numeric gains translate to real-world
benefits to users. However, past work indicates more
accurate predictions do in fact yield improvements
in human performance (Trnka et al, 2009).
Finally, while the predictive keyboard is a com-
monly studied interface, it is not appropriate for all
AAC users. Eye-tracker users may prefer an in-
terface such as Dasher (Ward and MacKay, 2002).
Single-switch users may prefer an interface such as
Nomon (Broderick and MacKay, 2009). Any AAC
interface based on word- or letter-based predictions
stands to benefit from the methods described in this
paper.
7 Conclusions
In this paper we have shown how workers? creativity
on a microtask crowdsourcing market can be used
to create fictional but plausible AAC communica-
tions. We have demonstrated that these messages
model conversational AAC better than the currently
used datasets based on telephone conversations or
newswire text. We used our new crowdsourced
dataset to intelligently select sentences from Twit-
ter, blog and Usenet data.
We compared a variety of different techniques for
intelligent training data selection. We found that
even for our small amount of in-domain data, the
recently proposed cross-entropy difference method
was consistently the best (Moore and Lewis, 2010).
Finally, compared to a model trained only on
Switchboard, our best performing model reduced
perplexity by 60-82% relative on three AAC-like test
sets. This translated to a potential keystroke savings
in a predictive keyboard interface of 5?11%.
In conclusion, we have shown how to create long-
span AAC language models using openly avail-
able resources. Our models significantly outperform
models trained on the commonly used data sources
of telephone transcripts and newswire text. To aid
other researchers, we have publicly released our
crowdsourced AAC collection, word lists and best-
performing models. We hope complementary tech-
niques such as topic modeling and language model
adaptation will provide additive gains to those ob-
tained by training models on large amounts of AAC-
like data. We plan to use our models to design and
test new interfaces that enable faster communication
for AAC users.
Acknowledgments
We thank Keith Trnka and Horabail Venkatagiri for
their assistance. This work was supported by the En-
gineering and Physical Sciences Research Council
(grant number EP/H027408/1).
References
Bruce Baker, Katya Hill, and Richard Devylder. 2000.
Core vocabulary is the same across environments. In
California State University at Northridge Conference.
David R. Beukelman, Kathryn M. Yorkston, Miguel
Poblete, and Carlos Naranjo. 1984. Frequency of
709
word occurrence in communication samples produced
by adult communication aid users. Journal of Speech
and Hearing Disorders, 49:360?367.
Tamara Broderick and David J. C. MacKay. 2009. Fast
and flexible selection with a single switch. PLoS ONE,
4(10):e7481.
Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,
Andreas Stolcke, and O?zgu?r C?etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Transactions on Speech and
Language Processing, 5(1):1?25.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r dataset. In Proceedings of the
3rd Annual Conference on Weblogs and Social Media.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and Kneser-Ney smoothing. In Proceedings of the
International Conference on Spoken Language Pro-
cessing, pages 2422?2425.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard University.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. ACM Transactions on
Asian Language Information Processing, 1:3?33.
Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: A survey. Universal Access in the
Information Society, 4:188?203.
Nestor Garay-Vitoria and Julio Gonza?lez-Abascal. 1997.
Intelligent word-prediction to enhance text input rate.
In Proceedings of the 2nd ACM International Confer-
ence on Intelligent User Interfaces, pages 241?244.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. Proceedings of the IEEE
Conference on Acoustics, Speech, and Signal Process-
ing, pages 517?520.
David Graff, Roni Rosenfeld, and Doug Pau. 1995.
CSR-III text. Linguistic Data Consortium, Philadel-
phia, PA, USA.
David Graff. 2003. English gigaword corpus. Linguistic
Data Consortium, Philadelphia, PA, USA.
Sheri Hunnicutt. 1989. Using syntactic and semantic in-
formation in a word prediction aid. In Proceedings of
the 1st European Conference on Speech Communica-
tion and Technology, pages 1191?1193.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE Conference on Acoustics,
Speech, and Signal Processing, pages 181?184.
Gregory W. Lesher and Gerard J. Rinkus. 2002.
Domain-specific word prediction for augmentative
communication. In Proceedings of the RESNA 2002
Annual Conference.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In Proceedings of the 7th
International ACM SIGACCESS Conference on Com-
puters and Accessibility, pages 121?128.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language models.
In Proceedings of the 5th European Conference on
Speech Communication and Technology, pages 1463?
1466.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the 48th Annual Meeting of the Association of
Computational Linguistics, pages 220?224.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL 2010, pages 172?180.
Cyrus Shaoul and Chris Westbury. 2009. A USENET
corpus (2005-2009). University of Alberta, Canada.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the 7th Annual In-
ternational Conference on Spoken Language Process-
ing, pages 901?904.
Keith Trnka, Debra Yarrington, and Christopher Penning-
ton. 2006. Topic modeling in fringe word prediction
for AAC. In Proceedings of the 11th ACM Interna-
tional Conference on Intelligent User Interfaces, pages
276?278.
Keith Trnka, John McCaw, Debra Yarrington, Kathleen F.
McCoy, and Christopher Pennington. 2009. User in-
teraction with word prediction: The effects of predic-
tion quality. ACM Transactions on Accessible Com-
puting, 1:17:1?17:34.
Keith Trnka. 2008. Adaptive language modeling for
word prediction. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Student Re-
search Workshop, pages 61?66.
Horabail Venkatagiri. 1999. Efficient keyboard layouts
for sequential access in augmentative and alternative
communication. Augmentative and Alternative Com-
munication, 15(2):126?134.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with semantic
710
information for a word prediction component. Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 506?513.
Tonio Wandmacher, Jean-Yves Antoine, Franck Poirier,
and Jean-Paul De?parte. 2008. SIBYLLE, an assis-
tive communication system adapting to the context and
its user. ACM Transactions on Accessible Computing,
1:6:1?6:30.
D. J. Ward and D. J. C. MacKay. 2002. Fast hands-free
writing by gaze direction. Nature, 418(6900):838.
711
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 19?27,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Applying Prediction Techniques to Phoneme-based AAC Systems   Keith Vertanen Department of Computer  Science Montana Tech of the University of Montana keithv@keithv.com   
Ha Trinh, Annalu Waller,  Vicki L. Hanson School of Computing University of Dundee {hatrinh, awaller, vlh} @computing.dundee.ac.uk 
Per Ola Kristensson       School of Computer  Science University of St Andrews pok@st-andrews.ac.uk     Abstract 
It is well documented that people with severe speech and physical impairments (SSPI) often experience literacy difficulties, which hinder them from effectively using orthographic-based AAC systems for communication. To address this problem, phoneme-based AAC systems have been proposed, which enable users to access a set of spoken phonemes and combine phonemes into speech output. In this paper we investigate how prediction tech-niques can be applied to improve user perfor-mance of such systems. We have developed a phoneme-based prediction system, which sup-ports single phoneme prediction and pho-neme-based word prediction using statistical language models generated using a crowdsourced AAC-like corpus. We incorpo-rated our prediction system into a hypothetical 12-key reduced phoneme keyboard. A compu-tational experiment showed that our prediction system led to 56.3% average keystroke sav-ings. 1 Introduction Over the last forty years there has been an increas-ing number of high-tech AAC systems developed to provide communication support for individuals with severe speech and physical impairments (SSPI). Most of existing AAC systems can be clas-sified into two categories, namely graphic-based and orthographic-based systems. Graphic-based systems utilize symbols to encode a limited set of frequently used words and utterances, thereby sup-porting fast access to pre-stored items. However, there is a high cognitive overhead associated with 
learning the encoding methods of these systems, which can be problematic for many AAC users, especially those with intellectual disabilities. In addition, users of these systems are limited to pre-programmed items rather than being able to create novel words and messages spontaneously. In con-trast, orthographic-based AAC systems allow users to spell out their own messages. Prediction tech-niques, such as character or word prediction, are often applied to improve the usability and accessi-bility of these systems. However, these systems require users to master literacy skills, a well-documented problem for many children and adults with SSPI (Koppenhaver and Yoder, 1992). The question arises as to how AAC systems can be designed to enable pre-literate users with SSPI to generate novel words and messages in sponta-neous conversations. A potential solution for this question is to adopt a phoneme-to-speech genera-tion approach. This approach allows users to ac-cess a limited set of spoken phonemes and blend phonemes into speech output, thereby enabling them to create spontaneous messages without knowledge of orthographic spelling. This approach has been applied in several phoneme-based AAC systems to support communication (Glennen and DeCoste, 1997) and literacy learning (Black et al, 2008). It has also been utilized as an alternative typing method for people with spelling difficulties (Schroeder, 2005).  Despite such potential, phoneme-based AAC systems have been an under-researched topic. In particular, little work has been done on the applica-tion of Natural Language Processing (NLP) tech-niques to these systems. Thus, in this paper we investigate how prediction methods can be incor-porated into phoneme-based AAC systems to facil-
19
itate phoneme entry. We develop a basic phoneme-based prediction system, which provides predic-tions at both phoneme and word levels based on statistical language modeling techniques. We use a 6-gram phoneme mixture model and a 3-gram word mixture model trained on a large set of AAC-like data assembled from multiple sources, such as Twitter, Blog, and Usenet data. We take into con-sideration issues such as pronunciation variants and user accents in the design of our system. We performed a theoretical evaluation of our system on three different test sets using a simulated inter-face and report results of hit rate and potential key-stroke savings. Finally, we propose a number of further studies to extend the current work. 2 Background  2.1 Phoneme-based AAC Systems The idea of using phonemes in AAC systems was first commercially introduced by Phonic Ear in 1978 in the HandiVoice 110 (Creech, 2004; Glennen and DeCoste, 1997; Williams, 1995). The device provided users with direct access to a mixed vocabulary consisting of pre-programmed words, short phrases, letters, morphemes, and 45 pho-nemes. Users could generate synthetic speech from phoneme sequences using the Votrax speech syn-thesizer. Similar to the HandiVoice is the Finger Foniks, a handheld communicator developed by Words+ (Glennen and DeCoste, 1997). The device enables users to access prerecorded messages and a set of 36 phonemes from which they could gener-ate unlimited speech output. Neither of these de-vices offered any prediction features.  The PhonicStick?, a talking joystick (Black et al, 2008), is a phoneme-based AAC device devel-oped by researchers at the University of Dundee.  Unlike the HandiVoice and the Finger Foniks, the primary use of the PhonicStick? is to facilitate language play and phonics teaching for children with SSPI. The device allows users to access the 42 phonemes used in the Jolly Phonics literacy program (Lloyd, 1998) by moving the joystick along pre-defined paths. A prototype of the Phon-icStick?, using a subset of 6 Jolly Phonics? pho-nemes, has been evaluated with seven children without and with SSPI. Results of the evaluations demonstrated that the participants could create short words using the phonemes. However, some 
participants with poor hand function experienced significant difficulties in using the joystick to se-lect target phonemes (Black et al, 2008). This suggests that the PhonicStick? could benefit from prediction mechanisms to reduce the number of difficult joystick movements required for each phoneme entry. The phoneme-to-speech approach is not only ap-plied in dedicated AAC systems but also in alterna-tive typing interfaces for individuals with spelling difficulties. An example of such applications is the REACH Sound-It-Out Phonetic Keyboard? (Schroeder, 2005). This on-screen keyboard com-prises 40 phonemes and 4 phoneme combinations. It offers two types of prediction features, including phoneme prediction and word prediction. The pho-neme prediction feature uses a pronunciation dic-tionary to determine which phonemes cannot follow the currently selected phonemes. These phonemes are then removed from the keyboard, thereby facilitating users in visually scanning and identifying the next phoneme in the intended word. The word prediction feature also uses a dictionary to search for the most frequently used words that phonetically match the currently selected phoneme sequence. To our knowledge, this is the only cur-rently available system that provides phoneme-based predictions. However, these predictions use a simple dictionary-based prediction algorithm, which does not take into account contextual infor-mation (e.g. prior text). There has been little or no published research into how more advanced NLP techniques can be employed to improve the per-formance of phoneme-based predictions.  2.2 Prediction in AAC Systems Prediction techniques have been extensively uti-lized in many AAC systems to achieve keystroke savings and potential communication rate en-hancement (Garay-Victoria and Abascal, 2005). There are various prediction strategies that have been developed in these systems, of which the most commonly used are character prediction and word prediction. Character prediction anticipates next probable characters given the preceding char-acters. It is typically applied in reduced keyboards and scanning-based AAC systems to augment the scanning process (Lesher et al, 1998). Word pre-diction anticipates the word being entered on the basis of the previously selected characters and 
20
words, thereby saving the user the effort of enter-ing every character of a word.  Most existing prediction systems employ statisti-cal language modelling techniques to perform pre-diction tasks. Prediction accuracy generally increases with higher-order n-gram language mod-els. However, most systems are limited to 6-gram models for character prediction and 3-gram models for word prediction, as the gain from higher-order models is often small at the cost of considerably increased computational and storage resources. To further improve the prediction performance, a number of advanced language modelling tech-niques have been investigated, which take into ac-count additional information such as word recency (Swiffin et al, 1987), syntactic information (Hunnicutt and Caarlberger, 2001; Swiffin et al, 1987), semantic information (Li and Hirst, 2005), and topic modelling (Trnka et al, 2006). These techniques have the potential of improved key-stroke savings at the cost of increased computa-tional complexity. A fundamental issue of the statistical-based pre-diction approach is that its performance is heavily dependent on the size of the training corpus and the degree to which the corpus represents the do-main of use. Therefore, in the development of sta-tistical-based prediction for conversational AAC systems, it may be ideal to construct language models from a large corpus of transcribed conver-sations of real AAC users. However, such a corpus has been unavailable to date. To address this prob-lem, previous research has utilized corpora of tele-phone transcripts, such as the Switchboard corpus, and performed cleanup processing to make them a more appropriate approximate of AAC communi-cation (Lesher and Rinkus, 2002; Trnka et al, 2006). Vertanen and Kristensson (2011) have re-cently proposed a novel solution to this problem by creating a large corpus of fictional AAC messages. Using Amazon Mechanical Market, the researchers crowdsourced a small dataset of AAC-like mes-sages, which was then used to select a much larger set of AAC-like data from Twitter, Blog, and Use-net datasets. The language models trained on this AAC-like corpus were proved to outperform other models trained on telephone transcripts (Vertanen and Kristensson, 2011). 3 Phoneme-based Prediction System  
Although statistical-based predictions have been a well-studied topic, little or no research has been published on how well these predictions can be adapted to phoneme-based AAC systems. In this section, we describe our phoneme-based prediction system, which employs statistical language model-ing techniques to perform phoneme prediction and phoneme-based word prediction.  Phoneme predic-tion predicts probable next phonemes based on the previously entered phonemes. Word prediction predicts the word currently being entered based on the current phoneme prefix and prior words. 3.1 Phoneme Set  Unlike traditional orthographic-based AAC sys-tems that operate on a standard character set, dif-ferent phoneme-based systems tend to use slightly different phoneme sets. For our prediction system, we use the phoneme set from the Jolly Phonics, a systematic synthetic phonics program widely used in the UK for literacy teaching (Lloyd, 1998). The phoneme set, to be called the PHONICS set, con-sists of 42 phonemes, with 17 vowels and 25 con-sonants. By using a literacy-linked phoneme set, our prediction system can readily be integrated into both literacy learning tools (such as the Phon-icStick? joystick (Black et al, 2008)) and com-munication aids. Other systems that use different phoneme sets can also be easily adapted to utilize our prediction system by providing a phoneme mapping scheme between their phoneme sets and the PHONICS set. 3.2 Pronunciation Dictionary 3.2.1 The PHONICS Dictionary The development of phoneme-based predictions requires a pronunciation dictionary, which should be accent-specific as pronunciations may vary across different accents. There has been no dic-tionary to date that contains word pronunciations using the PHONICS set. To address this problem, we built our PHONICS pronunciation dictionary based on the Unisyn1 lexicon, as it provides facili-ties for generating dictionaries in different accents. The Unisyn uses the concept of key-symbols (i.e. meta-phonemes) to encode the characteristics of                                                             1 http://www.cstr.ed.ac.uk/projects/unisyn/ 2 http://aac.unl.edu/vocabulary.html, accessed 4 September 
21
multiple accents into a single base lexicon. Accent-specific rules can then be applied to the base lexi-con to produce pronunciations in a given accent.  To create the PHONICS dictionary, we first de-rived a lexicon in the Edinburgh accent from the base lexicon using a set of Perl scripts supplied with Unisyn. We also performed additional clean-up processing to remove unwanted information, such as stress and boundary markers. We then cre-ated a mapping function from the set of 61 pho-nemes and allophones used in the Edinburgh lexicon to the PHONICS set. As the PHONICS set only contains 42 phonemes, several allophones in the Edinburgh set were mapped to the same pho-nemes in the PHONICS set. This mapping function was then used to convert the Edinburgh lexicon to the PHONICS pronunciation dictionary. The re-sulting dictionary consists of 121,004 pronuncia-tion entries for 117,625 unique words. 3.2.2 The Schwa Phoneme An issue of the phoneme mapping is that the Edin-burgh set contains the schwa phoneme (denoted by the symbol ?@?), which cannot be mapped to any phonemes in the PHONICS set. The schwa, a re-duced form of full vowels in unstressed syllables, occurs in 41,539 entries in the PHONICS diction-ary. An example of a word containing the schwa phoneme is ?today? (/t @ d ai/). While the schwa is the most commonly used vowel sound in spoken English (Gimson and Cruttenden, 2001), it is not included in the Jolly Phonics teaching as it is a dif-ficult concept to understand for literacy learners at early stages.  The simplest solution for this issue would be to explicitly add the schwa phoneme into the PHONICS set in our prediction system. However, learning to use the schwa correctly can be chal-lenging for users with SSPI and literacy difficul-ties. Thus, we decided to support two modes in our system, namely the SCHWA_ON and the SCHWA_OFF modes. In the SCHWA_ON mode, the schwa phoneme is explicitly added to the PHONICS set, increasing the set to 43 phonemes. In the SCHWA_OFF mode, the schwa is not added into the PHONICS set and therefore is not offered to the users for selection. To deal with the absence of the schwa, we employed a basic auto-correction method. To search for a word given a phoneme sequence, we apply a limited set of schwa insertion 
and replacement rules (e.g. replacing vowels with schwas) to generate a set of alternative sequences. These sequences and the original sequence are then used to look up a list of matching words in the PHONICS dictionary. Once the user has selected a word from this list, the correct pronunciation of the selected word (which might include the schwas) would be used to replace the original phoneme se-quence in the currently selected phoneme string. This corrected phoneme string would then be input to the phoneme language model (described in Sec-tion 3.3.1) to predict probable next phonemes.  3.3 Phoneme Prediction We trained a 6-gram phoneme language model starting with training data from: ? Twitter messages collected via the free streaming API between December 2010 and July 2011. 36M sentences, 251M words. ? Blog posts from the ICWSM corpus (Burton et al, 2009). 25M sentences, 387M words. ? Usenet messages (Shaoul and Westbury, 2009). 123M sentences, 1847M words. We used the crowdsourced data from Vertanen and Kristensson (2011) to select AAC-like sentences using cross-entropy difference selection (Moore and Lewis, 2010). The selection process retained 6.9M, 1.6M, and 2.3M words of data from the Twitter, Blog and Usenet data sets respectively. We converted the words in the selected sentences to pronunciation strings using the PHONICS dic-tionary. Whenever we encountered a word with multiple pronunciations, we chose a pronunciation at random. If a sentence had a word not in the PHONICS dictionary, we dropped the entire train-ing sentence. We trained a 6-gram phoneme language model for each of the Twitter, Blog, and Usenet data sets. Estimation of unigrams used Witten-Bell discount-ing while all higher order n-grams used modified Kneser-Ney discounting with interpolation. We then created a mixture model via linear interpola-tion with mixture weights optimized on the crowdsourced development set from Vertanen and Kristensson (2011). The optimized mixture weights were: Twitter 0.54, Blog 0.25, and Usenet 0.21. Our final mixture model has 2.0M parameters and a compressed disk size of 14 MB. 
22
 Figure 1. Hit rates of the phoneme prediction for prediction list lengths 1-15 in the SWCHA_ON and SCHWA_OFF modes. Results on the SPECIALISTS, COMM, and SWITCHTEST test sets.  3.3.1 Hit Rate We evaluated the accuracy of our phoneme predic-tion using hit rate. Hit rate (HR) is defined as the percentage of times that the intended phonemes appear in the prediction list: 
HR = 
? 
Number of times the
phoneme is predicted
Number of phonemes
?100% We computed the hit rates for prediction lists of lengths 1-15 in both SCHWA_ON and SCHWA_OFF modes. The results of this evalua-tion would help inform the decision of the number of predicted items to be presented to the users, which is a key usability factor of prediction sys-tems.  We evaluated the hit rates on the following test sets: ? SPECIALISTS: A collection of context specific conversational phrases recom-mended by AAC professionals2. 966 sen-tences, 3814 words. Out-of-vocabulary (OOV) rate: 0.05%. ? COMM: A collection of sentences written by college students in response to 10 hypo-thetical communication situations (Venkatagiri, 1999). 251 sentences, 1789 words. OOV rate: 0.3%. ? SWITCHTEST: Three telephone tran-scripts taken from the Switchboard corpus, used in Trnka et al (2009). 59 sentences, 508 words. OOV rate: 0.4%. These three test sets are used throughout this pa-per. For each sentence in the test sets, we generat-                                                            2 http://aac.unl.edu/vocabulary.html, accessed 4 September 2011 
ed its pronunciation string using the PHONICS dictionary. During this generation, any time we encountered a word with multiple pronunciations, we chose a pronunciation at random. We manually added pronunciations for OOV words. The gener-ated pronunciations were used to calculate the hit rates in the SCHWA_ON mode. We then created a ?non-schwa? version of each pronunciation string, in which we removed all schwa occurrences by either deleting them or replacing them with appro-priate vowels in the PHONICS set. The ?non-schwa? pronunciations were used to calculate the hit rates in the SCHWA_OFF mode. As shown in Figure 1, the hit rate improved as the prediction list length (L) increased in both the SCHWA_OFF and SCHWA_ON modes for all the three test sets. For most L values, the system per-formed the best on the SPECIALISTS test set and the worst on the SWITCHTEST set. At L=1, the average hit rates for the three test sets were 47.1% in the SCHWA_OFF mode and 50.1% in the SWITCH_ON mode. At L=5 (which is the length usually offered in prediction systems), the average hit rate increased to 76.2% in the SCHWA_OFF mode and 78.4% in the SCHWA_ON mode. At L=15, the system reached high average hit rates of 93.6% in the SCHWA_OFF mode and 94.3% in the SCHWA_ON mode. The SCHWA_ON mode achieved higher hit rates than the SCHWA_OFF mode for all L values. However, the hit rate differences between these two modes tended to diminish as L increased. At L=1, the average difference for the three test sets was 3.0%. At L=5, the average difference reduced to 2.2%. At L=15, the average difference was very small, at 0.7%. 
23
 Figure 2. Hit rates of the word prediction for prediction list lengths 1-15 in the SWCHA_ON and SCHWA_OFF modes for 1-phoneme and 2-phoneme prefixes. Results on the SPECIALISTS, COMM, and SWITCHTEST test sets. 3.4 Phoneme-based Word Prediction We used a publicly available 3-gram word mixture model3, which was created from three 3-gram models trained on AAC-like data from Twitter, Blog, and Usenet (Vertanen and Kristensson, 2011). Although a 4-gram model trained on the same datasets is also available, it was not used in our system as it has been shown to only slightly outperform the 3-gram model at the cost of a much bigger model size (Vertanen and Kristensson, 2011). Our aim is to keep our prediction system?s size reasonably small, thereby allowing it to be easily integrated into devices with limited re-sources, such as mobile devices. To perform word prediction given a phoneme prefix, we first search for a set of matching words in the PHONICS dictionary. In the SCHWA_OFF mode, the phoneme prefix is input to the auto-correction function to generate alternative prefixes, which are then used to look up matching words in the dictionary. If there is no matching word, an unknown word (denoted as <unk>) is returned. The matching words are then input to the word model to calculate their probabilities based on up to two prior words. 3.4.1 Hit Rate We computed the hit rate (HR) of word prediction for prediction list lengths 1-15 in two conditions: (1) after the first phoneme is entered, (2) after the first two phonemes are entered: 
                                                            3 http://www.aactext.org/imagine/lm_mix_top3_3gram_abs0.0.arpa.gz 
HR =
  
? 
Number of times the word is predicted
Number of words
?100% Figure 2 shows the hit rates of word prediction in the SCHWA_OFF and SCHWA_ON modes on the three test sets. As expected, the hit rates improved as the prediction list length (L) increased. Table 1 summarizes the average hit rates for several list lengths for 1-phoneme and 2-phoneme prefixes. At L=5, the average hit rates were 92.5% in the SCHWA_OFF mode and 93.2% in the SCHWA_ON mode after the first two phonemes are entered. This means that in most cases, the in-tended word is predicted after two keystrokes. The SCHWA_ON mode achieved higher hit rates than the SCHWA_OFF mode in all cases. However, the hit rate differences between these two modes were very small (<1%), which implies that our auto-correction mechanism was effective.  
L SCHWA_OFF SCHWA_ON 1-phoneme 2-phoneme 1-phoneme 2-phoneme 1 55.6% 80.4% 55.9% 80.8% 5 79.0% 92.5% 79.7% 93.2% 10 86.0% 94.5% 86.2% 95.0% 15 88.0% 95.1% 88.3% 95.8%  Table 1. Average hit rates of word prediction. 4 Theoretical Evaluation AAC users with physical impairments often expe-rience difficulties in accessing a large number of keys on conventional full-sized keyboards.  To address this problem, previous research has pro-posed the use of reduced keyboards (i.e. keyboards on which each key is assigned a group of charac-
24
ters, such as the 12-key mobile phone keyboard) (Arnott and Javed, 1992; Kushler, 1998). Character prediction and word prediction can be applied to these keyboards to disambiguate characters on each key. We adopted this idea by creating a hypo-thetical 12-key phoneme keyboard and evaluated the benefits of incorporating phoneme prediction and word prediction into the keyboard. 4.1 Phoneme-based Predictive Interface Our 12-key phoneme keyboard contains 8 pho-neme keys, which represent 3 vowel groups and 5 consonant groups. These groups, introduced in the PhonicStick? talking joystick (Black et al, 2008; Lindstr?m and Peronius, 2010), are formed accord-ing to the manner of articulation of the phonemes (see Figure 3a). Each key represents three to seven phonemes; the schwa phoneme is excluded. The phonemes on each key are initially arranged ac-cording to the unigram probabilities estimated by our phoneme language model.   
 Figure 3. Phoneme-based reduced keyboard.  The keyboard provides two phoneme entry modes, namely the MULTITAP and the PREDICTIVE modes. In the MULTITAP mode, the user enters a phoneme by pressing a corre-sponding key repeatedly until the intended pho-neme appears (e.g. pressing the ?Unvoiced Plosives? key 3 times to enter /p/). In the PREDICTIVE mode, the keyboard utilizes our prediction system in its SCHWA_OFF mode to predict probable next phonemes and words. Each time the user presses a key the phoneme prediction is applied to guess which of the possible phonemes on the pressed key is actually the user?s intended phoneme. If the prediction is incorrect, the user can repeatedly press the NEXT key until the correct 
phoneme is selected. After each phoneme selec-tion, we present a list of up to 5 predicted words. We only offer word predictions after the first pho-neme of a new word is entered. If the intended word appears in the prediction list, we assume it takes one keystroke for the user to add the word and a following space to the current sentence (this can be implemented using automatic scanning (Glennen and DeCoste, 1997)). 4.2 Results We evaluated our prediction system using two commonly used metrics: keystroke savings and keystrokes per character. 4.2.1 Keystroke Savings Keystroke Savings (KS) is defined as the percent-age of keystrokes that the user saves by using pre-diction methods compared to using the MULTITAP method: 
  
? 
KS = 1?
PREDICTION
Keystrokes
MULTITAP
Keystrokes
# 
$ 
% 
% 
& 
' 
( 
( 
?100% We computed KS on the three test sets for three methods: (1) only phoneme prediction (PP), (2) only word prediction (WP), (3) combined phoneme prediction and word prediction (PP+WP) (i.e. the PREDICTIVE mode).  As shown in Figure 4, a combined phoneme and word prediction method performed the best with an average keystroke savings of 56.3%. Using only word prediction led to a 46.4% average KS while using only phoneme prediction resulted in 29.9% average KS.  
 Figure 4. Keystroke Savings (KS) for prediction meth-ods on three test sets. 
25
4.2.2 Keystrokes Per Character Keystrokes per character (KSPC) is defined as the average number of keystrokes required to produce a character in the test set: 
  
? 
KSPC=
Keystrokes
Number of characters (including spaces) The evaluation of KSPC allows us to compare our keyboard with existing character-based reduced keyboards. We computed the KSPC for four meth-ods: (1) MULTITAP, (2) PP, (3) WP, (4) PP+WP. For comparison, we also calculated the KSPC for a standard 12-key mobile phone alphabetic keyboard (Figure 3b), which uses the character-based multi-tap method for text entry.  As shown in Figure 5, our frequency-based pho-neme keyboard outperformed the standard mobile phone keyboard even when no prediction methods are applied (i.e. in the MULTITAP mode) (see Figure 5). At an average KSPC of 1.568, our key-board required 19.1% fewer keystrokes per charac-ter than the mobile phone multitap keyboard (KSPC=1.937). There are two reasons that might explain this result. First, on average one phoneme represents more than one character (in our diction-ary, the character/phoneme ratio is 1.208). Second, our keyboard?s phonemes were initially ordered by the unigram frequencies.  When applying only phoneme prediction, the av-erage KSPC decreased to 1.100, which closely ap-proaches the KSPC of a QWERTY keyboard (KSPC=1). The KSPC further reduced to 0.841 with solely word prediction and 0.685 with com-bined phoneme and word prediction.  
 Figure 5. Keystrokes Per Character (KSPC) for different text entry methods on three test sets. 5 Conclusions and Future Work In this paper we have described how statistical lan-guage modeling techniques can be used to provide 
phoneme prediction and word prediction for pho-neme-based AAC systems. Using hit rate meas-urement we demonstrated how the prediction accuracy improved as the prediction list length in-creased. However, a large prediction list might re-sult in an increased time and cognitive workload required from the user to scan the list and select the desired item. Therefore, hit rate data need to be combined with empirical experiments with real users in order to determine an appropriate predic-tion list length.  We evaluated our prediction system on a 12-key phoneme keyboard, in which phonemes are grouped based on the manner of articulation and ordered using our phoneme unigram frequencies. We showed that we could achieve a potential key-stroke savings of 56.3% by applying a combined phoneme and word prediction to our keyboard. Using word prediction alone proved to be more effective than using phoneme prediction alone, in terms of keystroke savings.  We plan to take this work forward by exploring two complementary research directions.  First, we plan to conduct empirical experiments with a group of AAC users to evaluate the usability of our phoneme predictive keyboard. We are inter-ested in finding out if the potential keystroke sav-ings can be translated into an actual keystroke savings and communication rate enhancement. In addition, we will analyze user?s errors in phoneme selection, which can be used to produce a more advanced auto-correction method.  Second, we will explore how our prediction sys-tem can be integrated into existing phoneme-based AAC systems rather than our reduced keyboard. In particular, we will focus on the REACH Sound-It-Out Phonetic Keyboard? (Schroeder, 2005), which uses a different phoneme set than our PHONICS set, and the PhonicStick? (Black et al, 2008), which has the same phoneme groupings as our keyboard. Finally, we will investigate how NLP techniques, such as the joint-multigram model (Bisani and Ney, 2008), can be applied to automatically gener-ate orthographic spellings for OOV words. Our current system simply uses a <unk> placeholder for OOV words. While these words can still be spoken out by synthesizing their phoneme strings, it is potentially more beneficial to suggest actual spellings than to use such a placeholder. 
26
References  John L. Arnott and Muhammad Y. Javed (1992). Probabilistic character disambiguation for reduced keyboard using small text samples. Augmentative and Alternative Communication, 8(3), 215-223. Maximilian Bisani and Hermann Ney (2008). Joint-sequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5), 434-451. Rolf Black, Annalu Waller, Graham Pullin, and Eric Abel (2008). Introducing the PhonicStick: Preliminary evaluation with seven children. Paper presented at the 13th Biennial Conference of the International Society for Augmentative and Alternative Communication Montreal, Canada. Kevin Burton, Akashay Java, and Ian Soboroff (2009). The ICWSM 2009 Spinn3r dataset. Paper presented at the 3rd Annual Conference on Weblog and Social Media. Rick Creech (2004). Rick Creech, 2004 Edwin and Esther Prentke AAC Distinguished Lecturer, from http://www.aacinstitute.org/Resources/PrentkeLecture/2004/RickCreech.html Nestor Garay-Victoria and Julio Abascal (2005). Text prediction systems: a survey. Universal Access in the Information Society, 4, 188-203. Alfred. C. Gimson and Alan Cruttenden (2001). Gimson's Pronunciation of English: Hodder Arnold. Sharon L. Glennen and Denise C. DeCoste (1997). The Handbook of Augmentative and Alternative Communication: Thomson Delmar Learning. Sheri Hunnicutt and Johan Caarlberger (2001). Improving Word prediction using markov models and heuristic methods. Augmentative and Alternative Communication, 17(4), 255-264. David A. Koppenhaver and David E. Yoder, D (1992). Literacy issues in persons with severe speech and physical impairments. In R. Gaylord-Ross, Ed. (Ed.), Issues and research in special education (Vol. 2, pp. 156-201). NY: Teachers College Press, Columbia University, New York. Cliff Kushler (1998). AAC: Using a reduced keyboard. Paper presented at the Technology and Persons with Disabilities Conference, Los Angeles, USA. Gregory W. Lesher and Gerald J. Rinkus (2002). Domain-specific word prediction for augmentatve communication. Paper presented at the The RESNA '02 Annual Conference. Gregory W. Lesher, Bryan J. Moulton, and Jeffrey D. Higginbotham (1998). Techniques for augmenting scanning communication. Augmentative and Alternative Communication, 14, 81-101. Jianhua Li and Graeme Hirst (2005). Semantic knowledge in word completion. Paper presented at the 7th International ACM SIGACCESS Conference on Computers and Accessibility.  
Nina Lindstr?m and Irmeli Peronius (2010). The PhonicStick nursery study: Can phonological awareness be initiated by using a speaking joystick. Uppsala University. Susan M. Lloyd (1998). The Phonics Handbook. Chigwell: Jolly Learning Ltd. Robert C. Moore and William Lewis (2010). Intelligent selection of language model training data. Paper presented at the 48th Annual Meeting of the Association of Computational Linguistics. James E. Schroeder (2005). Improved spelling for persons with learning disabilities. Paper presented at the The 20th Annual International Conference on Technology and Persons with Disabilities, California, USA. Cyrus Shaoul and Chris Westbury (2009). A USENET corpus (2005-2009). University of Alberta, Canada. Andrew L. Swiffin, John L. Arnott,  and Alan Newell (1987). The use of syntax in a predictive communication aid for the physically handicapped. Paper presented at the RESNA 10th Annual Conference, San Jose, California. Andrew L. Swiffin, John L. Arnott, Andrian J. Pickering, and Alan Newell (1987). Adaptive and predictive techniques in a communication prosthesis. Augmentative and Alternative Communication, 3(4), 181-191. Keith Trnka, Debra Yarrington, Kathleen F. McCoy, and Christopher Pennington (2006). Topic modeling in fringe word prediction for AAC. Paper presented at the 11th International Conference on Intelligent User Interfaces.  Keith Trnka, John McCaw, Debra Yarrington, Kathleen F. McCoy, Christopher Pennington (2009). User interaction with word prediction: The effects of prediction quality. ACM Transactions on Accessible Computing, 1(3), 1-34. Horabail S. Venkatagiri (1999). Efficient keyboard layouts for sequential access in augmentative and alternative communication. Augmentative and Alternative Communication, 15(2), 126-134. Keith Vertanen and Per Ola Kristensson (2011). The imagination of Crowds: Conversational AAC language modelling using crowdsourcing and large data sources. Paper presented at the International Conference on Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, United Kingdom. Michael B. Williams (1995). Transitions and transformations. Paper presented at the 9th Annual Minspeak Conference, Wooster, OH.  
27
