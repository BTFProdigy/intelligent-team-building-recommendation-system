Coling 2008: Companion volume ? Posters and Demonstrations, pages 115?118
Manchester, August 2008
On ?redundancy? in selecting attributes for generating referring
expressions
Philipp Spanger Kurosawa Takehiro
Department of Computer Science
Tokyo Institute of Technology
Tokyo Meguro
?
Ookayama 2-12-1, 152-8550 Japan
{philipp, kurosawa, take}@cl.cs.titech.ac.jp
Tokunaga Takenobu
Abstract
We seek to develop an efficient algorithm
selecting attributes that approximates hu-
man selection. In contrast to previous work
we sought to combine the strengths of cog-
nitive theories and simple learning algo-
rithms. We then developed a new algo-
rithm for attribute selection based on ob-
servations from a corpus, which outper-
formed a simple base algorithm by a sig-
nificant margin. We then carried out a de-
tailed comparison between our algorithm
and Reiter & Dale?s ?Incremental Algo-
rithm?. In terms of achieving a human-like
attribute selection, the overall performance
of both algorithms is fundamentally equiv-
alent, while differing in the handling of re-
dundancy in selected attributes. We further
investigated this phenomenon and draw
some conclusions for further improvement
of attribute-selection algorithms.
1 Introduction
Referring expressions are a key research area in
human-agent communication. In the generation of
referring expressions humans do not necessarily
produce the most effective (i.e. minimal) expres-
sions in a computational sense. Given evolution-
ary development of human linguistic capabilities,
we can assume that human-produced expressions
are generally optimal to identify a target for other
human subjects. Thus the generation of human-
like referring expressions is an important task as
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the generation of those expressions that are most
easily understandable for humans.
The seminal work in this field is the ?Incremen-
tal algorithm? (IA) (Dale and Reiter, 1995). Their
work is based on an analysis of the overall cog-
nitive tendencies of humans in the selection of at-
tributes. In recent years, there have been a number
of important extensions to this algorithm, dealing
with very specific problems. This need for a sys-
tematic approach and unified evaluation of those
vastly differing algorithms provided the motiva-
tion for the creation of the TUNA-corpus
1
that was
developed at Aberdeen University as part of the
TUNA project (van Deemter, 2007). Work has be-
gun to use this corpus for evaluating different al-
gorithms for attribute selection.
Our research is carried out within this general
trend, seeking to take advantage of common re-
sources (e.g. TUNA-corpus). A critical question is
how to combine the generic human cognitive ten-
dencies and the dependency of attribute selection
on a specific distribution of attributes in a specific
case. In this research we tackle this question in
a corpus-based approach. Specifically, in a given
environment, we seek to develop an efficient algo-
rithm for selection of attributes that approximates
human selection.
2 The corpus
We utilized a simplified version of the TUNA-
corpus, which was also the basis for the GRE-
challenge held as part of the UCNLG+MT work-
shop in 2007 (Belz and Gatt, 2007). The corpus
consists of a collection of paired pictures of objects
and human-produced referring expressions anno-
tated with attribute sets. Figure 1 shows an image
1
TUNA-corpus: www.csd.abdn.ac.uk/research/tuna
115
green
green
greengreen
red
red blue
target
Figure 1: Image of a TUNA-corpus picture
of such a case
2
. This corpus provides information
on the attribute-value pairs of the target and the
distractors as well as of the referring expressions
humans produced. Every item in our corpus con-
sists of an input part (?case?) and an output part
(?description?). Each individual case consists of
seven case entities: one target referent and six dis-
tractors. Every entity consists of a set of attribute-
value pairs and all descriptions consist of a subset
of the attribute-value pairs of the target referent in
the same format as any entity. This corpus com-
prises two domains: a ?Furniture? and a ?Person?
- domain. We note that within the corpus there
were some cases that we judged as inappropriate
for this study and thus excluded from the overall
evaluation. This included cases where attribute-
values were unspecified and/or inconsistent.
3 The base algorithm
We developed a base algorithm as a baseline for
evaluation. We define ?discriminative power? of a
specific attribute as the number of entities in the
case that have a different value from the target for
this attribute.
We add attributes in descending order of dis-
criminative power until the target can be identified
uniquely. The generated attribute set is the output.
Every time an attribute is selected, we recalcu-
late the discriminative power of the attributes of
exclusively those distractors that could not be ex-
cluded by this stage.
4 Analysis of human-produced referring
expressions
Our hypothesis is that in human generation of re-
ferring expressions, a combination of generic cog-
2
Actual pictures in the TUNA-corpus do neither show
colour labels nor a target-marker.
nitive factors as well as case-dependent factors
have to be dealt with. In order to account for the
cognitive factor, we define a ?selection probabil-
ity? over a whole domain (i.e. independent from a
specific case) and calculate the differences of this
selection probability over the different attributes.
We define the selection probability of a specific at-
tribute a in a specific domain as equation (1).
SP (a) =
C(a)
?
x?X
C(x)
(1)
where C(x) denotes the number of occurrences of
attribute x in the corpus.
We observe that in the Furniture-domain the at-
tributes colour and type have extraordinarily high
selection probabilities and in particular the at-
tribute type is selected virtually unconditionally.
We observe the same tendency of a very high selec-
tion probability for the attribute type in the Person-
domain, even though all distractors as well as the
target are of same type ?person?. Since the at-
tribute type becomes the head of the noun phrase
in the linguistic realisation of a referring expres-
sion, it is natural to mention the type. Overall, we
can conclude that the different values for the selec-
tion probabilities reflect the cognitive load humans
assign different attributes in a given domain.
4.1 Co-occurrence of attributes
We hypothesize that the selection of attributes is
limited by co-occurence - dependencies between
attributes.
In order to measure this degree of co-
occurrence, we defined a ?degree of dependency?
between attributes as in equation (2). If the degree
of dependency approaches 1, there is practically no
dependency in the occurrence of attributes a and b.
If this factor grows above 1, the two attributes eas-
ily occur jointly in the referring expression, on the
other hand, the further it decreases below 1, the
less likely are the two attributes to occur jointly.
In the equation P(a, b) is the probability that the
two attributes will be selected together, P(x) is
the probability that the attribute x will be selected.
D(a, b) is the degree of dependency between at-
tributes
D(a, b) =
P(a, b)
P(a)? P(b)
(2)
We observed that in the Furniture-domain, size
or orientation and dimension are less likely to oc-
116
cur together in a referring expression. Further-
more, in the Person - domain, hairColour and
hasHair or hasBeard have a high degree of depen-
dency, i.e. they likely occur together.
4.2 Redundancy of attributes
Even though in many referring expressions unique
identification with few attributes is possible, hu-
mans show a tendency to add ?redundant? at-
tributes, i.e. that are in a strict sense not necessary
for identification. By adding redundancy, humans
add robustness to the expression as well as pos-
sibly reducing the cognitive load for humans in a
specific context. Within the corpus, we counted the
number of expressions containing redundancy. In
the Furniture-domain there were 220 out of all 278
expressions and in the Person-domain there were
213 out of 230.
Table 1: Number of selected redundant attributes
Furniture (278 cases) Person (230 cases)
attribute occurrences attribute occurrences
colour 110 type 201
orientation 15 x-dimension 4
size 10 hasBeard 42
type 210 hasGlasses 41
x-dimension 18 hasHair 32
This level of redundancy indicates that in or-
der to produce human-like sets of attributes for the
generation of referring expressions, it is not neces-
sary to aim for a minimal set.
5 Our proposed algorithm for effective
attribute selection
Based on our analysis of co-occurrence and redun-
dancy of attributes, we centrally implemented the
following improvements of the base algorithm.
Co-occurrence Based on the results from sec-
tion 4.1, when a certain attribute is selected, we
raise the selection probabilities of those attributes
that have a tendency to co-occur with it, on the
other hand we lower the selection probabilities of
those attributes that have a tendency not to co-
occur with this attribute.
Redundancy Based on the results in section 4.2,
having selected the attributes to uniquely deter-
mine the target, we add the next candidate in the
list of attributes as a redundant attribute .
Combination We combine both individual im-
provements. First of all, we add the type-attribute
and then score the result based on the selec-
tion probability. With each selection of a spe-
cific attribute, we change the scores based on co-
occurrence, and at the end we add a redundant at-
tribute.
6 Evaluation of proposed algorithm
We measured the proximity of the sets of attributes
by our system to the human-produced set of at-
tributes. We utilize the Dice-coefficient (DC) ?
a measure of proximity for sets. For purposes of
Table 2: Average DC for key improvements
Furniture Person
Base algorithm 0.305 0.314
Base+selection probability 0.784 0.669
Base+co-occurrence 0.254 0.314
Base+redundancy 0.401 0.341
Combination 0.811 0.703
Incremental algorithm 0.811 0.705
comparison, we implemented a version of the In-
cremental algorithm, where we calculated the or-
der of selection of attributes according to the se-
lection probabilities of attributes in the overall do-
main (Furniture or Person). It is of note that our
algorithm (combination of all individual improve-
ments) performs almost equivalent to the IA.
6.1 Comparison with Incremental Algorithm
We carried out a detailed analysis of the results of
our algorithm and those of the IA. We found that
the results of both algorithms in the Furniture - do-
main are exactly the same; however the results of
the Person - domain show significant differences.
Thus we concentrate on further analysis of the re-
sults in the Person - domain.
We divided all cases from the Person - domain
into three sets; a set of cases where our algorithm
performs better than the IA (sys-superior cases: 27
cases), a set of cases where the opposite is true (IA-
superior cases: 24 cases) and a set of tie cases. We
then compared the first two sets.
Investigating these sets, we observed that the
key difference between these two algorithms lay
in the treatment of redundancy. The IA often fails
in the case where humans use fewer attributes and
add only type as redundant attribute. On the other
hand, our algorithm fails in the case where humans
use more complex expressions, that is, more at-
tributes including several redundant ones.
We investigated the redundant attributes which
are selected by humans but not by the algorithms.
117
In the IA-superior cases, our system fails to se-
lect the hasBeard attribute compared with the IA
in 20 out of 24 cases, while in the sys-superior
cases both algorithms fail to select almost the same
redundant attributes. We investigated for both al-
gorithms, which attributes the algorithms wrongly
select; i.e. which are not selected by humans. In
the sys-superior cases, the IA wrongly selects at-
tributes in all 27 cases, with 23 out of those in-
cluding the wrongly-selected hasBeard attribute.
In the IA-superior cases, the number of cases
with wrongly selected attributes is much smaller
(9 cases for each) and they are largely equiavalent.
Thus, our detailed analysis showed an over-
all opposite tendency in one attribute; hasBeard.
While in sys-superior cases about 85% of the cases
in which the IA output wrong attributes included
hasBeard, in IA-superior cases our system failed
to select exactly hasBeard at a largely equivalent
rate (about 83%). At this moment, we do not have
any reasonable explanation for this peculiarity of
hasBeard, but suspect it might possibly be related
to the characteristics of the corpus.
However, from the overall observation that our
algorithm achieved an equivalent level of human-
likeness to the IA while being weaker in cases of
more complex redundancy, we conclude that fur-
ther improvement in selecting redundant attributes
is crucial to outperform the IA.
7 Concluding Remarks
Based on observations from the TUNA-corpus,
we developed an algorithm for attribute-selection
modeling human referring expressions. Our
corpus-based algorithm sought to combine human
generic tendencies of attribute selection in a cer-
tain domain with case-dependent variation of the
salience of specific attributes. Our improved algo-
rithm outperformed the base algorithm by a signif-
icant margin. However, we got qualitatively equiv-
alent results to our implementation of the IA.
A detailed analysis of the characteristics of our
algorithm in comparison to the IA pointed to the
importance of the phenomenon of redundancy as
possibly a central aspect that needs to be further
investigated to achieve a qualitative improvement
over the IA.
Our investigations into redundancy show that in
those cases where our algorithm outperformed the
IA, our algorithm almost exclusively added solely
the type-attribute. In contrast in more complex
cases of redundancy in referring expressions, the
IA has shown to be superior. Since we achieved
overall parity to the IA even though generally per-
forming worse than the IA in cases of more com-
plex redundancy, we can conclude that outside of
this phenomenon our algorithm performs better
than the IA in terms of human-likeness.
In previous research there has been some discus-
sion on ?redundancy? vs. ?minimality? in refer-
ring expressions (e.g. (Viethen and Dale, 2006)).
Through our research we have identified the phe-
nomenon of redundancy as a critical topic for fur-
ther research and for achieving further progress
in the generation of human-like referring expres-
sions.
Our algorithm includes some strong simplifica-
tions, e.g. our treatment of attributes did not take
account of the fact that attribute-values are also of
different type and did not explore what implica-
tions this has for the process of producing refer-
ring expressions; binary (hasHair), discrete (hair-
Colour) or graded (x-dim). In future these factors
should be integrated into attribute selection algo-
rithms.
In future work, we will seek to provide a more
detailed investigation of the phenomenon of re-
dundancy, including its variation over different do-
mains. Such an analysis should also contribute to
further our understanding of the human cognitive
process in the selection of attributes for the gener-
ation of referring expressions.
References
Belz, Anja and Albert Gatt. 2007. The attribute se-
lection for GRE challenge: Overview and evaluation
results. In Proceedings of the MT Summit XI Work-
shop Using Corpora for Natural Language Gener-
ation: Language Generation and Machine Transla-
tion (UCNLG+MT), pages 75?83.
Dale, Robert. and Ehud Reiter. 1995. Computational
interpretation of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
van Deemter, Kees. 2007. TUNA: To-
wards a unified algorithm for the genera-
tion of referring expressions - Final Report -.
http://www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Viethen, Jette and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the Fourth Inter-
national Natural Language Generation Conference,
pages 63?70.
118
Proceedings of the 12th European Workshop on Natural Language Generation, pages 110?113,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Japanese corpus of referring expressions used in a situated
collaboration task
Philipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga Takenobu
Department of Computer Science
Tokyo Institute of Technology
{philipp, yasuhara, ryu-i, take}@cl.cs.titech.ac.jp
Abstract
In order to pursue research on generating
referring expressions in a situated collab-
oration task, we set up a data-collection
experiment based on the Tangram puzzle.
For a pair of participants we recorded ev-
ery utterance in synchronisation with the
current state of the puzzle as well as all
operations by the participants. Referring
expressions were annotated with their ref-
erents in order to build a referring expres-
sion corpus in Japanese. We provide pre-
liminary results on the analysis of the cor-
pus from various standpoints, focussing on
action-mentioning expressions.
1 Introduction
Referring expressions are a linguistic device to re-
fer to a certain object, enabling smooth collabo-
ration between humans and agents where physical
operations are involved. Previous research often
either selectively focussed only on a limited num-
ber of expression-types or set up overly controlled
experiments. In contrast, we intend to work to-
wards analysing the whole breadth of referring ex-
pressions in a situated domain. For this purpose
we created a corpus (in Japanese) and analysed it
from various standpoints.
From very early on in referring expression re-
search, there has been some interest in the col-
laborative aspect of the reference process (Clark
and Wilkes-Gibbs, 1986). This has more recently
developed into creating situated corpora in order
to analyse the referring expressions occurring in
situated collaborative tasks. The COCONUT cor-
pus (Di Eugenio et al, 2000) is collected from
keyboard-input dialogues between two partici-
pants who are collaboratively working on a sim-
ple 2-D design task (buying and arranging furni-
ture for two rooms). In contrast, the QUAKE cor-
pus (Byron et al, 2005) ? as well as the more re-
cent SCARE corpus (Stoia et al, 2008), which is
an extension of QUAKE ? is based on an interac-
tion captured in a 3-D virtual reality (VR) world
where two participants collaboratively carry out
a treasure hunting task. There has been ongoing
work to exploit these two resources for research on
different aspects of referring expressions (Pamela
W. Jordan, 2005; Byron, 2005).
However, while these resources have inspired
new research into different aspects of referring ex-
pressions, at the same time they have clear limi-
tations. The COCONUT corpus is collected from
dialogues in which participants refer to symbol-
like objects in a 2-D world. It thus resem-
bles the more recent (non-collaborative) TUNA-
corpus (van Deemter, 2007) in tending to en-
courage very simple types of expressions. Fur-
thermore, limiting participants? interaction to key-
board input makes the dialogue less natural. While
the QUAKE corpus deals with a more complex do-
main (3-D virtual world), the participating sub-
jects were only able to carry out limited kinds of
actions (pushing buttons, picking up or dropping
objects) as compared with the complexity of the
three-dimensional target domain.
In contrast to these two corpora, we set up a
comparatively simple collaborative task (Tangram
Puzzle) allowing participants to freely communi-
cate via speech and to perform actions various
enough to accomplish the given task, e.g. pick-
ing, moving, turning and rotating pieces. All ut-
terances by participants were recorded in synchro-
nisation with operations on objects and the object
arrangement. The utterances were transcribed and
all referring expressions found were annotated to-
gether with their referents. Thus, this corpus al-
lows us to study in detail human-human interac-
tion, particularly referring expressions in a situ-
ated setting. In what follows, we first describe de-
tails of the building of the corpus and then provide
110
results of our preliminary analysis. This analysis
reveals a novel type of referring expression men-
tioning an action on objects, which we call action-
mentioning expressions.
2 Building the corpus
Figure 1: Screenshot of the Tangram simulator
2.1 Experimental setting
We recruited 12 Japanese graduate students (4 fe-
males, 8 males) and split them into 6 pairs. Each
pair was instructed to solve the Tangram puzzle
(an ancient Chinese geometrical puzzle) coopera-
tively. The goal of Tangram is to construct a given
shape by arranging seven pieces of simple figures
as shown in Figure 1.
In order to record detailed information of the
interaction (position of pieces, participants? ac-
tions), we implemented a Tangram simulator in
which the pieces on the computer display can be
moved, rotated and flipped with simple mouse op-
erations. Figure 1 shows the simulator interface in
which the left shows the goal shape area and the
right the working area. We assigned two differ-
ent roles to participants, a solver and an operator;
the solver thinks of the arrangement of the pieces
to make the goal shape and gives instructions to
the operator, while the operator manipulates the
pieces with the mouse according to the solver?s in-
structions.
A solver and an operator sit side by side in front
of their own computer display. Both participants
share the same working area of the simulator. The
operator can manipulate the pieces, but cannot see
the goal shape. In contrast, the solver sees the goal
shape but cannot move pieces. A shield screen was
set between the participants in order to prevent
them from peeking at their partner?s display. In
this asymmetrical interaction, we can expect many
referring expressions during the interaction.
Each pair is assigned four exercises and the par-
ticipants exchanged roles after two exercises. We
set a time limit of 15 minutes for an exercise.
Utterances by the participants are recorded sep-
arately in stereo through headset microphones in
synchronisation with the position of the pieces and
the mouse actions. In total, we collected 24 dia-
logues of about four hours. The average length of
a dialogue was 10 minutes 43 seconds.
2.2 Annotation
Recorded dialogues were transcribed with a time
code attached to each utterance. Since our main
concern is collecting referring expressions, we de-
fined an utterance to be a complete sentence to
prevent a referring expression being split into sev-
eral utterances. Referring expressions were an-
notated together with their referents by using the
multi-purpose annotation tool SLAT (Noguchi et
al., 2008). Two annotators (two of the authors) an-
notated four dialogue texts separately. We anno-
tated all 24 dialogue texts and corrected discrep-
ancies by discussion between the annotators.
3 Analysis of the corpus
We collected a total of 1,509 tokens and 449 types
of referring expressions in 24 dialogues. Our
asymmetric experimental setting tended to encour-
age referring expressions from the solver, while
the operator was constrained to confirming his un-
derstanding of the solver?s instructions. This is re-
flected in the number of referring expressions by
the solver (1,287) largely outnumbering those of
the operator (222). There are a number of expres-
sions (215 expressions; 15% of the total) referring
to multiple objects (referring to 2 or more pieces)
and we excluded them from our current analysis.
We exclusively deal here with expressions refer-
ring to a specific single piece or indefinite expres-
sions, i.e. those that have no definite referent (in
total 1,294 tokens).
We found the following syntactic/semantic fea-
tures used in the expressions: i) demonstratives
(adjectives and pronouns), ii) object attribute-
values, iii) spatial relations, iv) actions on an ob-
ject and v) others. The number of these features is
summarised in Table 1. (Note that multiple fea-
tures can be used in a single expression.) The
right-most column shows an example with its En-
111
Table 1: Features of referring expressions
Feature types tokens Example
i) demonstrative 118 745
adjective 100 196 ?ano migigawa no sankakkei (that triangle at the right side)?
pronoun 19 551 ?kore (this)?
ii) attribute 303 641
size 165 267 ?tittyai sankakkei (the small triangle)?
shape 271 605 ?o?kii sankakkei (the large triangle)?
direction 6 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?
iii) spatial relations 129 148
projective 125 144 ?hidari no okkii sankakkei (the small triangle on the left)?
topological 2 2 ?o?kii hanareteiru yatu (the big distant one)?
overlapping 2 2 ?sono sita ni aru sankakkei (the triangle underneath it)?
iv) action-mentioning 78 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?
v) others 29 30
remaining 15 15 ?nokotteiru o?kii sankakkei (the remaining large triangle)?
similarity 14 15 ?sore to onazi katati no (the one of the same shape as that one)?
glish translation. The identified feature in the re-
ferring expression is underlined.
We note here a tendency to employ object at-
tributes, particularly the attribute ?shape? as well
as use of demonstratives, particularly demonstra-
tive pronouns. These kinds of referring expres-
sions are quite general and appear in a variety of
other non-situated settings as well. In addition,
we found another kind of expression not usually
employed by humans outside of situated collabo-
ration tasks; referring expressions mentioning an
action on an object. We have 85 expressions (over
6% of the total) of this type in our corpus.
4 Action-mentioning expressions
We further analysed those expressions that men-
tion an action on an object, which we call action-
mentioning expressions hereafter. Although there
was significant variation in usage of action-
mentioning expressions among the pairs, all 6
pairs of participants used at least one action-
mentioning expression, indicating that it is a fun-
damental type of expression for this task set-
ting. Action-mentioning expressions are different
from haptic-ostensive referring expressions (Fos-
ter et al, 2008) since action-mentioning expres-
sions are not necessarily accompanied by simulta-
neous physical operation on an object.
Action-mentioning expressions can be again di-
vided into three categories: i) combination of a
temporal adverbial with a verb indicating an ac-
tion (?turned?, ?put?, ?moved?, etc) (55 tokens or
about 65% of action-mentioning expression), ii)
use of temporal adverbials without a verb, i.e. verb
ellipsis (22 tokens or about 26%) and iii) expres-
sions with a verb without temporal adverbials (8
tokens or about 9%). The second category includ-
ing verb ellipsis would be rare in English, but it is
quite natural in Japanese.
Only less than 10% of this kind of expression
did not include any temporal adverbial, indicating
that humans tend to describe the temporal aspect
of an action. This needs to be integrated into any
generation algorithm for this task domain. The
temporal adverbials used by the participants were
the Japanese ?sakki no NP (the NP [verb-ed] just
before)? or ?ima no NP (the current NP/the NP
[you are verb-ing] now/the NP [verb-ed] just be-
fore)?. ?Ima? generally refers to the current time
point (?now?). It can, however, refer to a past time
point as well, thus it is ambiguous.
Participants tended to use ?ima? largely in its
perfect meaning (completed action). The fre-
quency of use of ?ima? in its perfect meaning in
comparison to its progressive meaning was about
2:1. The distribution of the two types of tempo-
ral adverbials ?sakki? and ?ima? was about 2:3.
The slight preference here for ?ima? might be ex-
plained by its dual meanings (progressive and per-
fect) in contrast to the exclusive use of ?sakki? for
past actions.
Figure 2 shows the distribution of ?sakki (just
before)? and past-cases of ?ima (now)? dependent
on the time-distance to the action they refer to. For
actions occurring within a timeframe of about 10
seconds previous to uttering an expression, partic-
ipants had an overwhelming preference for ?ima?.
The frequency of ?ima? decreases quickly for ac-
tions that occurred 10-20 seconds prior to the ut-
terance. In contrast, after 20 seconds from the ac-
112
04
8
12
16
0
?
1
0
1
0
?
2
0
2
0
?
3
0
3
0
?
4
0
4
0
?
5
0
5
0
?
6
0
6
0
?
7
0
7
0
?
8
0
8
0
?
9
0
9
0
?
1
0
0
1
0
0
?
f
r
e
q
u
e
n
c
y
time (sec)
"sakki (just before)"
"ima (now)"
Figure 2: Frequency of ?sakki? and ?ima? over the
time-distance to referenced action
tion, participants prefered ?sakki?.
In addition, we investigated what actions oc-
curred in between the utterance and the action
mentioned. The actions we take into account here
are basic manipulations of an object like ?move?,
?flip?, ?click? and so on. Referring to an immedi-
ately preceding action, participants had a strong
preference for using ?ima?. Interestingly, with
only one other action in between, the participants?
preference becomes opposite (i.e. ?sakki? is pre-
ferred.). For referring to actions further in the past
(i.e. more actions in between), there was a con-
tinous preference for ?sakki? over ?ima?. Further
analysis should also investigate the phenomenon
of the difference in use of temporal adverbials for
other languages and whether this is related to char-
acteristics of the Japanese language or rather an in-
herent property of the use of temporal adverbials
in natural language.
5 Conclusion and future work
We collected a corpus of Japanese referring ex-
pressions as a first step towards developing algo-
rithms for generating referring expressions in a sit-
uated collaboration. We carried out an initial anal-
ysis of the collected expressions, focussing on ex-
pressions that include a mention of an action on
an object. We noted that they are often combined
with temporal adverbials with participants seek-
ing to make a temporal ordering of actions. In
addition, we intend to further analyse other types
of expressisons (demonstratives, etc) and work on
developing generation algorithms for this domain.
In future work, we intend to generalise this exper-
iment in the Tangram-domain to other domains.
Furthermore, information such as gestures and eye
movements should be incorporated in data collec-
tion. This will lay the basis for the development of
more general models for the generation of refer-
ring expressions in a situated collaborative task.
References
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna K. Byron. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
B. Di Eugenio, P. W. Jordan, R. H. Thomason, and J. D
Moore. 2000. The agreement process: An empirical
investigation of human-human computer-mediated
collaborative dialogues. International Journal of
Human-Computer Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT ?
Segment and link-based annotation tool. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61?64.
Marilyn A. Walker Pamela W. Jordan. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence
Research, 24:157?194.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. SCARE: A sit-
uated corpus with annotated referring expressions.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation (LREC
2008).
Kees van Deemter. 2007. TUNA: Towards a UNified
Algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
113
Proceedings of the 12th European Workshop on Natural Language Generation, pages 191?194,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Probabilistic Model of Referring Expressions for Complex Objects
Kotaro Funakoshi? Philipp Spanger?
?Honda Research Institute Japan Co., Ltd.
Saitama, Japan
funakoshi@jp.honda-ri.com
nakano@jp.honda-ri.com
Mikio Nakano? Takenobu Tokunaga?
?Tokyo Institute of Technology
Tokyo, Japan
philipp@cl.cs.titech.ac.jp
take@cl.cs.titech.ac.jp
Abstract
This paper presents a probabilistic model
both for generation and understanding of
referring expressions. This model intro-
duces the concept of parts of objects, mod-
elling the necessity to deal with the char-
acteristics of separate parts of an object in
the referring process. This was ignored or
implicit in previous literature. Integrating
this concept into a probabilistic formula-
tion, the model captures human character-
istics of visual perception and some type
of pragmatic implicature in referring ex-
pressions. Developing this kind of model
is critical to deal with more complex do-
mains in the future. As a first step in our
research, we validate the model with the
TUNA corpus to show that it includes con-
ventional domain modeling as a subset.
1 Introduction
Generation of referring expressions has been stud-
ied for the last two decades. The basic orientation
of this research was pursuing an algorithm that
generates a minimal description which uniquely
identifies a target object from distractors. Thus
the research was oriented and limited by two con-
straints: minimality and uniqueness.
The constraint on minimality has, however,
been relaxed due to the computational complexity
of generation, the perceived naturalness of redun-
dant expressions, and the easiness of understand-
ing them (e.g., (Dale and Reiter, 1995; Spanger et
al., 2008)). On the other hand, the other constraint
of uniqueness has not been paid much attention
to. One major aim of our research is to relax this
constraint on uniqueness because of the reason ex-
plained below.
The fundamental goal of our research is to deal
with multipartite objects, which have constituents
with different attribute values. Typical domain set-
tings in previous literature use uniform objects like
the table A shown in Figure 1. However, real life
is not so simple. Multipartite objects such as ta-
bles B and C can be found easily. Therefore this
paper introduces the concept of parts of objects to
deal with more complex domains containing such
objects. Hereby the constraint on uniqueness be-
comes problematic because people easily generate
and understand logically ambiguous expressions
in such domains.
For example, people often use an expression
such as ?the table with red corners? to identify
table B. Logically speaking, this expression is
equally applicable both to A and to B, that is, vio-
lating the constraint on uniqueness. And yet peo-
ple seem to have no problem identifying the in-
tended target correctly and have little reluctance to
use such an expression (Evidence is presented in
Section 3). We think that this reflects some type of
pragmatic implicature arising from human charac-
teristics of visual perception and that is important
both for understanding human-produced expres-
sions and for generating human-friendly expres-
sions in a real environment. This paper proposes a
model of referring expressions both for generation
and understanding. Our model uses probabilities
to solve ambiguity under the relaxed constraint on
uniqueness while considering human perception.
No adequate data is currently available in or-
der to provide a comprehensive evaluation of our
model. As a first step in our research, we validate
the model with the TUNA corpus to show that it
includes conventional domain modeling.
Figure 1: An example scene
191
2 Related work
Horacek (2005) proposes to introduce probabili-
ties to overcome uncertainties due to discrepan-
cies in knowledge and cognition between subjects.
While our model shares the same awareness of is-
sues with Horacek?s work, our focus is on rather
different issues (i.e., handling multipartite objects
and relaxing the constraint on uniqueness). In
addition, Horacek?s work is concerned only with
generation while our model is available both for
generation and understanding. Roy (2002) also
proposes a probabilistic model for generation but
presupposes uniform objects.
Horacek (2006) deals with references for struc-
tured objects such as documents. Although it con-
siders parts of objects, the motivation and focus of
the work are on quite different aspects from ours.
3 Evidence against logical uniqueness
We conducted two psycholinguistic experiments
using the visual stimulus shown in Figure 1.
In the first experiment, thirteen Japanese sub-
jects were presented with an expression ?kado no
akai tukue (the table with red corners)? and asked
to choose a table from the three in the figure.
Twelve out of the thirteen chose table B. Seven
out of the twelve subjects answered that the given
expression was not ambiguous.
In the second experiment, thirteen different
Japanese subjects were asked to make a descrip-
tion for table B without using positional relations.
Ten out of the thirteen made expressions seman-
tically equivalent to the expression used in the
first experiment. Only three subjects made log-
ically discriminative expressions such as ?asi to
yotu kado dake akai tukue (the table whose four
corners and leg only are red).?
These results show that people easily gener-
ate/understand logically ambiguous expressions.
4 Proposed model
We define pi = {p1, p2, . . . , pk} as the set of k
parts of objects (classes of sub-parts) that appears
in a domain. Here p1 is special and always means
the whole of an object. In a furniture domain, p1
means a piece of furniture regardless of the kind
of the object (chair, table, whatever). pi(i 6= 1)
means a sub-part class such as leg. Note that pi is
defined not for each object but for a domain. Thus,
objects may have no part corresponding to pi (e.g.,
some chairs have no leg.).
A referring expression e is represented as a set
of n pairs of an attribute value expression eaj and a
part expression epj modified by eaj as
e = {(ep1, e
a
1), (e
p
2, e
a
2), . . . , (epn, ean)}. (1)
For example, an expression ?the white table with
a red leg? is represented as
{(?table?, ?white?), (?leg?, ?red?)}.
Given a set of objects ? and a referring ex-
pression e, the probability with which the expres-
sion e refers to an object o ? ? is denoted as
Pr(O = o|E = e,? = ?). If we seek to provide
a more realistic model, we can model a probabilis-
tic distribution even for ?. In this paper, however,
we assume that ? is fixed to ? and it is shared by
interlocutors exactly. Thus, hereafter, Pr(o|e) is
equal to Pr(o|e, ?).
Following the definition (1), we estimate
Pr(o|e) as follows:
Pr(o|e) ? N
?
i
Pr(o|epi , e
a
i ). (2)
Here, N is a normalization coefficient. According
to Bayes? rule,
Pr(o|epi , e
a
i ) =
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (3)
Therefore,
Pr(o|e) ? N
?
i
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (4)
We decompose Pr(epi , eai |o) as
?
u
?
v
Pr(epi |pu, o)Pr(e
a
i |av, o)Pr(pu, av|o)
(5)
where pu is one of parts of objects that could be
expressed with epi , and av is one of attribute val-
ues1 that could be expressed with eai . Under the
simplifying assumption that epi and eai are not am-
biguous and are single possible expressions for
a part of objects and an attribute value indepen-
dently of objects 2,
Pr(o|e) ? N
?
i
Pr(o)Pr(pi, ai|o)
Pr(pi, ai)
(6)
? N
?
i
Pr(o|pi, ai) (7)
1Each attribute value belongs to an attribute ?, a set of
attribute values. E.g., ?color = {red, white, . . .}.
2That is, we ignore lexical selection matters in this paper,
although our model is potentially able to handle those matters
including training from corpora.
192
Pr(o|p, a) concerns attribute selection in gen-
eration of referring expressions. Most attribute
selection algorithms presented in past work are
based on set operations over multiple attributes
with discrete (i.e., symbolized) values such as col-
ors (red, brown, white, etc) to find a uniquely dis-
tinguishing description. The simplest estimation
of Pr(o|p, a) following this conventional Boolean
domain modeling is
Pr(o|p, a) ?
{
|??|?1 (p in o has a)
0 (p in o does not have a) (8)
where ?? is the subset of ?, each member of which
has attribute value a in its part of p.
As Horacek (2005) pointed out, however, this
standard approach is problematic in a real envi-
ronment because many physical attributes are non-
discrete and the symbolization of these continuous
attributes have uncertainties. For example, even
if two objects are blue, one can be more blueish
than the other. Some subjects may say it?s blue
but others may say it?s purple. Moreover, there
is the problem of logical ambiguity pointed out
in Section 1. That is, even if an attribute itself
is equally applicable to several objects in a logi-
cal sense, other available information (such as vi-
sual context) might influence the interpretation of
a given referring expression.
Such phenomena could be captured by estimat-
ing Pr(o|p, a) as
Pr(o|p, a) ? Pr(a|p, o)Pr(p|o)Pr(o)
Pr(p, a)
. (9)
Pr(a|p, o) represents the relevance of attribute
value a to part p in object o. Pr(p|o) represents
the salience of part p in object o. The underlying
idea to deal with the problem of logical ambiguity
is ?If some part of an object is mentioned, it should
be more salient than other parts.? This is related
to Grice?s maxims in a different way from mat-
ters discussed in (Dale and Reiter, 1995). Pr(p|o)
could be computed in some manner by using the
saliency map (Itti et al, 1998). Pr(o) is the prior
probability that object o is chosen. If potential
functions (such as used in (Tokunaga et al, 2005))
are used for computing Pr(o), we can naturally
rank objects, which are equally relevant to a given
referring expression, according to distances from
interlocutors.
5 Algorithms
5.1 Understanding
Understanding a referring expression e is identify-
ing the target object o? from a set of objects ?. This
is formulated in a straightforward way as
o? = argmax
o??
Pr(o|e). (10)
5.2 Generation
Generation of a referring expression is choosing
the best appropriate expression e? to discriminate a
given object o? from a set of distractors. A simple
formulation is
e? = argmax
e??
Pr(e)Pr(o?|e). (11)
? is a pre-generated set of candidate expressions
for o?. This paper does not explain how to generate
a set of candidates.
Pr(e) is the generation probability of an ex-
pression e independent of objects. This probabil-
ity can be learned from a corpus. In the evaluation
described in Section 6, we estimate Pr(e) as
Pr(e) ? Pr(|e|)
?
i
Pr(?i). (12)
Here, Pr(|e|) is the distribution of expression
length in terms of numbers of attributes used.
Pr(?) is the selection probability of a specific at-
tribute ? (SP (a) in (Spanger et al, 2008)).
6 Preliminary evaluation
As mentioned above, no adequate corpus is cur-
rently available in order to provide an initial vali-
dation of our model which we present in this pa-
per. In this section, we validate our model us-
ing the TUNA corpus (the ?Participant?s Pack?
available for download as part of the Generation
Challenge 2009) to show that it includes tradi-
tional domain modeling. We use the training-
part of the corpus for training our model and the
development-part for evaluation.
We note that we here assume a homogeneous
distribution of the probability Pr(o|p, a), i.e., we
are applying formula (8) here in order to calculate
this probability. We first implemented our proba-
bilistic model for the area of understanding. This
means our algorithm took as input the user?s selec-
tion of attribute?value pairs in the description and
calculated the most likely target object. This was
193
Table 1: Initial evaluation of proposed model for
generation in TUNA-domain
Furniture People
Total cases 80 68
Mean Dice-score 0.78 0.66
carried out for both the furniture and people do-
mains. Overall, outside of exceptional cases (e.g.,
human error), our algorithm was able to distin-
guish the target object for all human descriptions
(precision of 100%). This means it covers all the
cases the original approach dealt with.
We then implemented our model for the case of
generation. We measured the similarity of the out-
put of our algorithm with the human-produced sets
by using the Dice-coefficient (see (Belz and Gatt,
2007)). We evaluated this both for the Furniture
and People domain. The results are summarized
in Table 1.
Our focus was here to fundamentally show how
our model includes traditional modelling as a sub-
set, without much focus or effort on tuning in order
to achieve a maximum Dice-score. However, we
note that the Dice-score of our algorithm was com-
parable to the top 5-7 systems in the 2007 GRE-
Challenge (see (Belz and Gatt, 2007)) and thus
produced a relatively good result. This shows how
our algorithm ? providing a model of the referring
process in a more complex domain ? is applica-
ble as well to the very simple TUNA-domain as a
special case.
7 Discussion
In past work, parts of objects were ignored or im-
plicit. In case of the TUNA corpus, while the Fur-
niture domain ignores parts of objects, the People
domain contained parts of objects such as hair,
glasses, beard, etc. However, they were implic-
itly modeled by combining a pair of a part and its
attribute as an attribute such as hairColor. One
major advantage of our model is that, by explicitly
modelling parts of objects, it can handle the prob-
lem of logical ambiguity that is newly reported in
this paper. Although it might be possible to han-
dle the problem by extending previously proposed
algorithms in some ways, our formulation would
be clearer. Moreover, our model is directly avail-
able both for generation and understanding. Re-
ferring expressions using attributes (such as dis-
cussed in this paper) and those using discourse
contexts (such as ?it?) are separately approached
in past work. Our model possibly handles both of
them in a unified manner with a small extension.
This paper ignored relations between objects.
We, however, think that it is not difficult to prepare
algorithms handling relations using our model.
Generation using our model is performed in a
generate-and-test manner. Therefore computa-
tional complexity is a matter of concern. However,
that could be controlled by limiting the numbers
of attributes and parts under consideration accord-
ing to relevance and salience, because our model is
under the relaxed constraint of uniqueness unlike
previous work.
As future work, we have to gather data to eval-
uate our model and to statistically train lexical se-
lection in a new domain containing multipartite
objects.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proc. the MT Summit XI Workshop Using
Corpora for Natural Language Generation: Lan-
guage Generation and Machine Translation (UC-
NLG+MT), pages 75?83.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In Proc.
ENLG 05.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proc. ACL 06.
L Itti, C. Koch, and E. Niebur. 1998. A model of
saliency-based visual attention for rapid scene anal-
ysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(11):1254?1259.
Deb Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3).
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting
attributes for generating referring expressions. In
Proc. COLING 08.
Takenobu Tokunaga, Tomonori Koyama, and Suguru
Saito. 2005. Meaning of Japanese spatial nouns.
In Proc. the Second ACL-SIGSEM Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistics Formalisms and Appli-
cations, pages 93 ? 100.
194
Towards an Extrinsic Evaluation of Referring Expressions
in Situated Dialogs
Philipp SPANGER IIDA Ryu TOKUNAGA Takenobu
{philipp,ryu-i,take}@cl.cs.titech.ac.jp
TERAI Asuka KURIYAMA Naoko
asuka@nm.hum.titech.ac.jp kuriyama@hum.titech.ac.jp
Tokyo Institute of Technology
Abstract
In the field of referring expression gener-
ation, while in the static domain both in-
trinsic and extrinsic evaluations have been
considered, extrinsic evaluation in the dy-
namic domain, such as in a situated col-
laborative dialog, has not been discussed
in depth. In a dynamic domain, a cru-
cial problem is that referring expressions
do not make sense without an appropriate
preceding dialog context. It is unrealistic
for an evaluation to simply show a human
evaluator the whole period from the be-
ginning of a dialog up to the time point
at which a referring expression is used.
Hence, to make evaluation feasible it is
indispensable to determine an appropriate
shorter context. In order to investigate the
context necessary to understand a referring
expression in a situated collaborative dia-
log, we carried out an experiment with 33
evaluators and a Japanese referring expres-
sion corpus. The results contribute to find-
ing the proper contexts for extrinsic evalu-
tion in dynamic domains.
1 Introduction
In recent years, the NLG community has paid sig-
nificant attention to the task of generating referring
expressions, reflected in the seting-up of several
competitive events such as the TUNA and GIVE-
Challenges at ENLG 2009 (Gatt et al, 2009; By-
ron et al, 2009).
With the development of increasingly complex
generation systems, there has been heightened in-
terest in and an ongoing significant discussion on
different evaluation measures for referring expres-
sions. This discussion is carried out broadly in the
field of generation, including in the multi-modal
domain, e.g. (Stent et al, 2005; Foster, 2008).
!"#$%&
!"#$%&'()$)&'
'($)*$+%"&,#-+."/
*+),&#(&'
&#),&#(&'
-$,$./#&0!"#$%1
234456
78$#0!"#$%10
234496
:$#0!*,0;<=&(0!"#$%1
2344>6
;)/&$0!"#$%1
234456
?/,!$#0@A$<B*,
2344C6
D*<E0@0F$))0
2344G6
H8&(0I$I*,
234J46
K/()*,#!"#$%&#
234496
Figure 1: Overview of recent work on evaluation
of referring expressions
Figure 1 shows a schematic overview of recent
work on evaluation of referring expressions along
the two axes of evaluation method and domain in
which referring expressions are used.
There are two different evaluation methods cor-
responding to the bottom and the top of the verti-
cal axis in Figure 1: intrinsic and extrinsic eval-
uations (Sparck Jones and Galliers, 1996). In-
trinsic methods often measure similarity between
the system output and the gold standard corpora
using metrics such as tree similarity, string-edit-
distance and BLEU (Papineni et al, 2002). Intrin-
sic methods have recently become popular in the
NLG community. In contrast, extrinsic methods
evaluate generated expressions based on an exter-
nal metric, such as its impact on human task per-
formance.
While intrinsic evaluations have been widely
used in NLG, e.g. (Reiter et al, 2005), (Cahill
and van Genabith, 2006) and the competitive 2009
TUNA-Challenge, there have been a number of
criticisms against this type of evaluation. (Reiter
and Sripada, 2002) argue, for example, that gener-
ated text might be very different from a corpus but
still achieve the specific communicative goal.
An additional problem is that corpus-similarity
metrics measure how well a system reproduces
what speakers (or writers) do, while for most NLG
systems ultimately the most important considera-
tion is its effect on the human user (i.e. listener
or reader). Thus (Khan et al, 2009) argues that
?measuring human-likeness disregards effective-
ness of these expressions?.
Furthermore, as (Belz and Gatt, 2008) state
?there are no significant correlations between in-
trinsic and extrinsic evaluation measures?, con-
cluding that ?similarity to human-produced refer-
ence texts is not necessarily indicative of quality
as measured by human task performance?.
From early on in the NLG community, task-
based extrinsic evaluations have been considered
as the most meaningful evaluation, especially
when having to convince people in other commu-
nities of the usefulness of a system (Reiter and
Belz, 2009). Task performance evaluation is rec-
ognized as the ?only known way to measure the ef-
fectiveness of NLG systems with real users? (Re-
iter et al, 2003). Following this direction, the
GIVE-Challenges (Koller et al, 2009) at INLG
2010 (instruction generation) also include a task-
performance evaluation.
In contrast to the vertical axis of Figure 1, there
is the horizontal axis of the domain in which refer-
ring expressions are used. Referring expressions
can thus be distinguished according to whether
they are used in a static or a dynamic domain, cor-
responding to the left and right of the horizontal
axis of Figure 1. A static domain is one such as the
TUNA corpus (van Deemter, 2007), which col-
lects referring expressions based on a motionless
image. In contrast, a dynamic domain comprises a
constantly changing situation where humans need
context information to identify the referent of a re-
ferring expression.
Referring expressions in the static domain have
been evaluated relatively extensively. A recent ex-
ample of an intrinsic evaluation is (van der Sluis
et al, 2007), who employed the Dice-coefficient
measuring corpus-similarity. There have been a
number of extrinsic evaluations as well, such as
(Paraboni et al, 2006) and (Khan et al, 2009), re-
spectively measuring the effect of overspecifica-
tion on task performance and the impact of gener-
ated text on accuracy as well as processing speed.
They belong thus in the top-left quadrant of Fig-
ure 1.
Over a recent period, research in the generation
of referring expressions has moved to dynamic do-
mains such as situated dialog, e.g. (Jordan and
Walker, 2005) and (Stoia et al, 2006). However,
both of them carried out an intrinsic evaluation
measuring corpus-similarity or asking evaluators
to compare system output to expressions used by
human (the right bottom quadrant in Figure 1).
The construction of effective generation sys-
tems in the dynamic domain requires the imple-
mentation of an extrinsic task performance evalu-
ation. There has been work on extrinsic evalua-
tion of instructions in the dynamic domain on the
GIVE-2 challenge (Byron et al, 2009), which is a
task to generate instructions in a virtual world. It is
based on the GIVE-corpus (Gargett et al, 2010),
which is collected through keyboard interaction.
The evaluation measures used are e.g. the number
of successfully completed trials, completion time
as well as the numbers of instructions the system
sent to the user. As part of the JAST project, a
Joint Construction Task (JCT) puzzle construction
corpus (Foster et al, 2008) was created which is
similar in some ways in its set-up to the REX-
J corpus which we use in the current research.
There has been some work on evaluating gener-
ation strategies of instructions for a collaborative
construction task on this corpus, both considering
intrinsic as well as extrinsic measures (Foster et
al., 2009). Their main concern is, however, the in-
teraction between the text structure and usage of
referring expressions. Therefore, their ?context?
was given a priori.
However, as can be seen from Figure 1, in the
field of referring expression generation, while in
the static domain both intrinsic and extrinsic eval-
uations have been considered, the question of re-
alizing an extrinsic evaluation in the dynamic do-
main has not been dealt with in depth by previous
work. This paper addresses this shortcoming of
previous work and contributes to ?filling in? the
missing quadrant of Figure 1 (the top-right).
The realization of such an extrinsic evaluation
faces one key difficulty. In a static domain, an ex-
trinsic evaluation can be realized relatively easily
by showing evaluators the static context (e.g. any
image) and a referring expression, even though
this is still costly in comparison to intrinsic meth-
ods (Belz and Gatt, 2008).
In contrast, an extrinsic evaluation in the dy-
namic domain needs to present an evaluator with
the dynamic context (e.g. a certain length of the
recorded dialog) preceding a referring expression.
It is clearly not feasible to simply show the whole
preceding dialog; this would make even a very
small-scale evaluation much too costly. Thus, in
order to realize a cost-effective extrinsic evalua-
tion in a dynamic domain we have to deal with the
additional parameter of time length and content of
the context shown to evaluators.
This paper investigates the context necessary for
humans to understand different types of referring
expressions in a situated domain. This work thus
charts new territory and contributes to developing
a extrinsic evaluation in a dynamic domain. Sig-
nificantly, we consider not only linguistic but also
extra-linguistic information as part of the context,
such as the actions that have been carried out in the
preceding interaction. Our results indicate that, at
least in this domain, extrinsic evaluation results
in dynamic domains can depend on the specific
amount of context shown to the evaluator. Based
on the results from our evaluation experiments, we
discuss the broader conclusions to be drawn and
directions for future work.
2 Referring Expressions in the REX-J
Corpus
We utilize the REX-J corpus, a Japanese corpus
of referring expressions in a situated collaborative
task (Spanger et al, 2009a). It was collected by
recording the interaction of a pair of dialog partic-
ipants solving the Tangram puzzle cooperatively.
The goal of the Tangram puzzle is to construct a
given shape by arranging seven pieces of simple
figures as shown in Figure 2
!"#$%&'#()
*"+,-.!%#+)#
Figure 2: Screenshot of the Tangram simulator
In order to record the precise position of every
piece and every action by the participants, we im-
plemented a simulator. The simulator displays two
areas: a goal shape area, and a working area where
pieces are shown and can be manipulated.
We assigned different roles to the two partici-
pants of a pair: solver and operator. The solver
can see the goal shape but cannot manipulate the
pieces and hence gives instructions to the opera-
tor; by contrast, the operator can manipulate the
pieces but can not see the goal shape. The two
participants collaboratively solve the puzzle shar-
ing the working area in Figure 2.
In contrast to other recent corpora of refer-
ring expressions in situated collaborative tasks
(e.g. COCONUT corpus (Di Eugenio et al, 2000)
and SCARE corpora (Byron et al, 2005)), in
the REX-J corpus we allowed comparatively large
real-world flexibility in the actions necessary to
achieve the task (such as flipping, turning and
moving of puzzle pieces at different degrees), rel-
ative to the task complexity. The REX-J corpus
thus allows us to investigate the interaction of lin-
guistic and extra-linguistic information. Interest-
ingly, the GIVE-2 challenge at INLG 2010 notes
its ?main novelty? is allowing ?continuous moves
rather than discrete steps as in GIVE-1?. Our work
is in line with the broader research trend in the
NLG community of trying to get away from sim-
ple ?discrete? worlds to more realistic settings.
The REX-J corpus contains a total of 1,444 to-
kens of referring expressions in 24 dialogs with a
total time of about 4 hours and 17 minutes. The
average length of each dialog is 10 minutes 43
seconds. The asymmetric data-collection setting
encouraged referring expressions from the solver
(solver: 1,244 tokens, operator: 200 tokens). We
exclude from consideration 203 expressions refer-
ring to either groups of pieces or whose referent
cannot be determined due to ambiguity, thus leav-
ing us 1,241 expressions.
We identified syntactic/semantic features in the
collected referring expressions as listed in Table 1:
(a) demonstratives (adjectives and pronouns), (b)
object attribute-values, (c) spatial relations and (d)
actions on an object. The underlined part of the
examples denotes the feature in question.
3 Design of Evaluation Experiment
The aim of our experiment is to investigate the
?context? (content of the time span of the recorded
Table 1: Syntactic and semantic features of refer-
ring expressions in the REX-J corpus
Feature Tokens Example
(a) demonstrative 742 ano migigawa no sankakkei
(that triangle at the right side)
(b) attribute 795 tittyai sankakkei
(the small triangle)
(c) spatial relations 147 hidari no okkii sankakkei
(the small triangle on the left)
(d) action-mentioning 85 migi ue ni doketa sankakkei
(the triangle you put away to
the top right)
interaction prior to the uttering of the referring ex-
pression) necessary to enable successful identifi-
cation of the referent of a referring expression.
Our method is to vary the context presented to
evaluators and then to study the impact on human
referent identification. In order to realize this, for
each instance of a referring expression, we vary
the length of the video shown to the evaluator.
!"#$ %&'()*+,!-./0#
!1#$ 234*&,&5,678+*4,9&+:3(;,8+*8,3(,)7*,63<'=8)&+
!%#$ >))*+8(?*,3(?='43(;,)7*,+*5*++3(;,*@A+*663&(,)&,
*B8='8)*,!67&9(,3(,+*4#
!C#$ D)8+)E+*A*8),F'))&(
!G#$ D*=*?)3&(,F'))&(6,!-.H#,8(4,IJ,4&(K),:(&9I.F'))&(,,,,,,,,,,
!"#
!1#
!%#
!C#
!G#
Figure 3: The interface presented to evaluators
The basic procedure of our evaluation experi-
ment is as follows:
(1) present human evaluators with speech and
video from a dialog that captures shared
working area of a certain length previous to
the uttering of a referring expression,
(2) stop the video and display as text the next
solver?s utterance including the referring ex-
pression (shown in red),
(3) ask the evaluator to identify the referent
of the presented referring expression (if the
evaluator wishes, he/she can replay the video
as many times as he likes),
(4) proceed to the next referring expression (go
to (1)).
Figure 3 shows a screenshot of the interface pre-
pared for this experiment.
The test data consists of three types of referring
expressions: DPs (demonstrative pronouns),
AMEs (action-mentioning expressions), and
OTHERs (any other expression that is neither a
DP nor AME, e.g intrinsic attributes and spatial
relations). DPs are the most frequent type of
referring expression in the corpus. AMEs are
expressions that utilize an action on the referent
such as ?the triangle you put away to the top
right? (see Table 1)1. As we pointed out in our
previous paper (Spanger et al, 2009a), they are
also a fundamental type of referring expression in
this domain.
The basic question in investigating a suitable
context is what information to consider about the
preceding interaction; i.e. over what parameters to
vary the context. In previous work on the gener-
ation of demonstrative pronouns in a situated do-
main (Spanger et al, 2009b), we investigated the
role of linguistic and extra-linguistic information,
and found that time distance from the last action
(LA) on the referent as well as the last mention
(LM) to the referent had a significant influence on
the usage of referring expressions. Based on those
results, we focus on the information on the refer-
ent, namely LA and LM.
For both AMEs and OTHERs, we only consider
two possibilities of the order in which LM and LA
appear before a referring expression (REX), de-
pending on which comes first. These are shown in
Figure 4, context patterns (a) LA-LM and (b) LM-
LA. Towards the very beginning of a dialog, some
referring expressions have no LM and LA; those
expressions are not considered in this research.
All instances of AMEs and OTHERs in our test
data belong to either the LA-LM or the LM-LA
1An action on the referent is usually described by a verb
as in this example. However, there are cases with a verb el-
lipsis. While this would be difficult in English, it is natural
and grammatical in Japanese.
!"#$%&'()&*+'()&
,($%
-./%+ !"#$
%&
'"()
!"#$%&'%($)"**+,-
!"#*+'()&$%&'()&
,($%
-./%+ !"#$
%*
'"()
!.#$%('%&$)"**+,-
!"#
$%&'()&
,($%
-./%+
%*
'"()
!/#$%('%&0$)"**+,-
*+'()&
-./%+
-./%+
Figure 4: Schematic overview of the three context
Patterns
pattern. For each of these two context patterns,
there are three possible contexts2: Both (including
both LA and LM), LA/LM (including either LA or
LM) and None (including neither). Depending on
the order of LA and LM prior to an expression,
only one of the variations of LA/LM is possible
(see Figure 4 (a) and (b)).
In contrast, DPs tend to be utilized in a deic-
tic way in such situated dialogs (Piwek, 2007).
We further noted in (Spanger et al, 2009b), that
DPs in a collaborative task are also frequently used
when the referent is under operation. While they
belong neither to the LA-LM nor the LM-LA pat-
tern, it would be inappropriate to exclude those
cases. Hence, for DPs we consider another situa-
tion where the last action on the referent overlaps
with the utterance of the DP (Figure 4 (c) LM-LA?
pattern). In this case, we consider an ongoing op-
eration on the referent as a ?last action?. Another
peculiarity of the LM-LA? pattern is that we have
no None context in this case, since there is no way
to show a video without showing LA (the current
operation).
Given the three basic variations of context, we
recruited 33 university students as evaluators and
2To be more precise, we set a margin at the beginning of
contexts as shown in Figure 4.
divided them equally into three groups, i.e. 11
evaluators per group. As for the referring ex-
pressions to evaluate, we selected 60 referring ex-
pressions used by the solver from the REX-J cor-
pus (20 from each category), ensuring all were
correctly understood by the operator during the
recorded dialog. We selected those 60 instances
from expressions where both LM and LA ap-
peared within the last 30 secs previous to the re-
ferring expression. This selection excludes initial
mentions, as well as expressions where only LA
or only LM exists or they do not appear within 30
secs. Hence the data utilized for this experiment
is limited in this sense. We need further experi-
ments to investigate the relation between the time
length of contexts and the accuarcy of evaluators.
We will return to this issue in the conclusion.
We combined 60 referring expressions and the
three contexts to make the test instances. Follow-
ing the Latin square design, we divided these test
instances into three groups, distributing each of
the three contexts for every referring expression
to each group. The number of contexts was uni-
formly distributed over the groups. Each instance
group was assigned to each evaluator group.
For each referring expression instance, we
record whether the evaluator was able to correctly
identify the referent, how long it took them to
identify it and whether they repeated the video
(and if so how many times).
Reflecting the distribution of the data available
in our corpus, the number of instances per context
pattern differs for each type of referring expres-
sion. For AMEs, overwhelmingly the last action
on the referent was more recent than the last men-
tion. Hence we have only two LA-LM patterns
among the 20 AMEs in our data. For OTHERs, the
balance is 8 to 12, with a slight majority of LM-
LA patterns. For DPs, there is a strong tendency to
use a DP when a piece is under operation (Spanger
et al, 2009b). Of the 20 DPs in the data, 2 were
LA-LM, 5 were LM-LA pattern while 13 were of
the LM-LA? pattern (i.e. their referents were under
operation at the time of the utterance). For these
13 instances of LM-LA? we do not have a None
context.
The average stimulus times, i.e. time period of
presented context, were 7.48 secs for None, 11.04
secs for LM/LA and 18.10 secs for Both.
Table 2: Accuracy of referring expression identification per type and context
Type context pattern\Context None LM/LA Both Increase [None ? Both]
(LA-LM) 0.909 0.955 0.955 0.046
DP (20/22) (21/22) (21/22)
(LM-LA) 0.455 0.783 0.843 0.388
(25/55) (155/198) (167/198)
Total 0.584 0.800 0.855 0.271
(LA-LM) 0.227 0.455 0.682 0.455
AME (5/22) (10/22) (15/22)
(LM-LA) 0.530 0.859 0.879 0.349
(105/198) (170/198) (174/198)
Total 0.500 0.818 0.859 0.359
(LA-LM) 0.784 0.852 0.943 0.159
OTHER (69/88) (75/88) (83/88)
(LM-LA) 0.765 0.788 0.879 0.114
(101/132) (104/132) (116/132)
Total 0.773 0.814 0.905 0.132
Overall 0.629 0.811 0.903 0.274
(325/517) (535/660) (576/638)
4 Results and Analysis
In this section we discuss the results of our evalua-
tion experiment. In total 33 evaluators participated
in our experiment, each solving 60 problems of
referent identification. Taking into account the ab-
sence of the None context for the DPs of the LM-
LA? pattern (see (c) in Figure 4), we have 1,815
responses to analyze. We focus on the impact of
the three contexts on the three types of referring
expressions, considering the two context patterns
LA-MA and LM-LA.
4.1 Overview of Results
Table 2 shows the micro averages of the accura-
cies of referent identification of all evaluators over
different types of referring expressions with differ-
ent contexts. Accuracies increase with an increase
in the amount of information in the context; from
None to Both by between 13.2% (OTHERs) and
35.9% (AMEs). The average increase of accuracy
is 27.4%.
Overall, for AMEs the impact of the context is
the greatest, while for OTHERs it is the smallest.
This is not surprising given that OTHERs tend to
include intrinsic attributes of the piece and its spa-
tial relations, which are independent of the pre-
ceding context. We conducted ANOVA with the
context as the independent variable, testing its ef-
fect on identification accuracy. The main effect
of the context was significant on accuracy with
F (2, 1320) = 9.17, p < 0.01. Given that for
DPs we did not have an even distribution between
contexts, we only utilized the results of AMEs and
OTHERs.
There are differences between expression types
in terms of the impact of addition of LM/LA into
the context, which underlines that when studying
context, the relative role and contribution of LA
and LM (and their interaction) must be looked at in
detail for different types of referring expressions.
Over all referring expressions, the addition into
a None context of LM yields an average increase
in accuracy of 9.1% for all referring expression
types, while for the same conditions the addition
of LA yields an average increase of 21.3%. Hence,
interestingly for our test data, the addition of LA
to the context has a positive impact on accuracy by
more than two times over the addition of LM.
It is also notable that even with neither LA nor
LM present (i.e. the None context), the evaluators
were still able to correctly identify referents in be-
tween 50?68.6% (average: 62.9%) of the cases.
While this accuracy would be insufficient for the
evaluation of machine generated referring expres-
sions, it is still higher than one might expect and
further investigation of this case is necessary.
4.2 Demonstrative Pronouns
For DPs, there is a very clear difference between
the two patterns (LM-LA and LA-LM) in terms of
the increase of accuracy with a change of context.
While accuracy for the LA-LM pattern remains at
a high level (over 90%) for all three contexts (and
there is only a very small increase from None to
Both), for the LM-LA pattern there is a strong in-
crease from None to Both of 38.8%.
The difference in accuracy between the two
context patterns of DPs in the None context might
come from the mouse cursor effect. The two ex-
pressions of LA-LM pattern happened to have a
mouse cursor on the referent, when they were
used, resulting in high accuracy. On the other
hand, 4 out of 5 expressions of LM-LA pattern did
not have a mouse cursor on the referent. We have
currently no explanation for the relation between
context patterns and the mouse position. While
we have only 7 expressions in the None context
for DPs and hence cannot draw any decisive con-
clusions, we note that the impact of the mouse po-
sition is a likely factor.
For the LM-LA pattern, there is an increase
in accuracy of 32.8% from None to the LA-
context. Overwhelmingly, this represents in-
stances in which the referents are being operated
at the point in time when the solver utters a DP
(this is in fact the LM-LA? pattern, which has no
None context). For those instances, the current
operation information is sufficient to identify the
referents. In contrast, addition of LM leads only
to a small increase in accuracy of 5.6%. This re-
sult is in accordance with our previous work on the
generation of DPs, which stressed the importance
of extra-linguistic information in the framework of
considering the interaction between linguistic and
extra-linguistic information.
4.3 Action-mentioning Expressions
While for AMEs the number of instances is very
uneven between patterns (similar to the distribu-
tion for DPs), there is a strong increase in accuracy
from the None context to the Both context for both
patterns (between 30% to almost 50%). However,
there is a difference between the two patterns in
terms of the relative contribution of LM and LA to
this increase.
While for the LA-LM pattern the impact of
adding LM and LA is very similar, for the LM-LA
pattern the major increase in accuracy is due to
adding LA into the None context. This indicates
that for AMEs, LA has a stronger impact on ac-
curacy than LM, as is to be expected. The strong
increase for AMEs of the LM-LA pattern when
adding LA into the context is not surprising, given
that the evaluators were able to see the action men-
tioned in the AME.
For the opposite reason, it is not surprising that
AMEs show the lowest accuracy in the None con-
text, given that the last action on the referent is
not seen by the evaluators. However, accuracy
was still slightly over 50% in the LM-LA pattern.
Overall, of the 18 instances of AMEs of the LM-
LA pattern, in the None context a majority of eval-
uators correctly identified 9 and erred on the other
9. Further analysis of the difference between cor-
rectly and incorrectly identified AMEs led us to
note again the important role of the mouse cursor
also for AMEs.
Comparing to the LM-LA pattern, we had very
low accuracy even with the Both context. As we
mentioned in the previous section, we had very
skewed test instances for AME, i.e. 18 LM-LA
patterns vs. 2 LA-LM patterns. We need further
investigation on the LA-LM pattern of AME with
more large number of instances.
Of the 18 LM-LA instances of AMEs, there are
14 instances that mention a verb describing an ac-
tion on the referent. The referents of 6 of those
14 AMEs were correctly determined by the evalu-
ators and in all cases the mouse cursor played an
important role in enabling the evaluator to deter-
mine the referent. The evaluators seem to utilize
the mouse position at the time of the uttering of the
referring expression as well as mouse movements
in the video shown. In contrast, for 8 out of the
9 incorrectly determined AMEs no such informa-
tion from the mouse was available. There was a
very similar pattern for AMEs that did not include
a verb. These points indicate that movements and
the position of the mouse both during the video as
well as the time point of the uttering of the refer-
ring expression give important clues to evaluators.
4.4 Other Expressions
There is a relatively even gain in identification ac-
curacy from None to Both of between about 10?
15% for both patterns. However, there is a simi-
lar tendency as for AMEs, since there is a differ-
ence between the two patterns in terms of the rel-
ative contribution of LM and LA to this increase.
While for the LA-LM pattern the impact of adding
LM and LA is roughly equivalent, for the LM-LA
pattern the major increase in accuracy is due to
adding LM into the LA-context.
For this pattern of OTHERs, LM has a stronger
impact on accuracy than LA, which is exactly the
opposite tendency to AMEs. For OTHERs (e.g.
use of attributes for object identification), seeing
the last action on the target has a less positive im-
pact than listening to the last linguistic mention.
Furthermore, we note the relatively high accuracy
in the None context for OTHERs, underlining the
context-independence of expressions utilizing at-
tributes and spatial relations of the pieces.
4.5 Error Analysis
We analyzed those instances whose referents were
not correctly identified by a majority of evalua-
tors in the Both context. Among the three expres-
sion types, there were about 13?16% of wrong an-
swers. In total for 7 of the 60 expressions a ma-
jority of evaluators gave wrong answers (4 DPs, 2
AMEs and 1 OTHER). Analysis of these instances
indicates that some improvements of our concep-
tion of ?context? is needed.
For 3 out of the 4 DPs, the mouse was not over
the referent or was closer to another piece. In addi-
tion, these DPs included expressions that pointed
to the role of a piece in the overall construction of
the goal shape, e.g. ?soitu ga atama (that is the
head)?, or where a DP is used as part of a more
complex referring expression, e.g. ?sore to onazi
katati . . . (the same shape as this)?, intended to
identify a different piece. For a non-participant
of the task, such expressions might be difficult to
understand in any context. This phenomenon is
related to the ?overhearer-effect? (Schober et al,
1989).
The two AMEs that the majority of evaluators
failed to identify in the Both context were also
misidentified in the LA context. Both AMEs were
missing a verb describing an action on the referent.
While for AMEs including a verb the accuracy in-
creased from None to Both by 50%, for AMEs
without a verb there was an increase by slightly
over 30%, indicating that in the case where an
AME lacks a verb, the context has a smaller pos-
itive impact on accuracy than for AMEs that in-
clude a verb. In order to account for those cases,
further work is necessary, such as investigating
how to account for the information on the distrac-
tors.
5 Conclusions and Future Work
In order to address the task of designing a flexi-
ble experiment set-up with relatively low cost for
extrinsic evaluations of referring expressions, we
investigated the context that needs to be shown to
evaluators in order to correctly determine the ref-
erent of an expression.
The analysis of our results showed that the con-
text had a significant impact on referent identifi-
cation. The impact was strongest for AMEs and
DPs and less so for OTHERs. Interestingly, we
found for both DPs and AMEs that including LA
in the context had a stronger positive impact than
including LM. This emphasizes the importance of
taking into account extra-linguistic information in
a situated domain, as considered in this study.
Our analysis of those expressions whose refer-
ent was incorrectly identified in the Both context
indicated some directions for improving the ?con-
text? used in our experiments, for example look-
ing further into AMEs without a verb describing
an action on the referent. Generally, there is a
necessity to account for mouse movements during
the video shown to evaluators as well as the prob-
lem for extrinsic evaluations of how to address the
?overhearer?s effect?.
While likely differing in the specifics of the set-
up, the methodology in the experiment design dis-
cussed in this paper is applicable to other domains,
in that it allows a low-cost flexible design of eval-
uating referring expressions in a dynamic domain.
In order to avoid the additional effort of analyzing
cases in relation to LM and LA, in the future it will
be desirable to simply set a certain time period and
base an evaluation on such a set-up.
However, we cannot simply assume that a
longer context would yield a higher identification
accuracy, given that evaluators in our set-up are
not actively participating in the interaction. Thus
there is a possibility that identification accuracy
actually decreases with longer video segments,
due to a loss of the evaluator?s concentration. Fur-
ther investigation of this question is indicated.
Based on the work reported in this paper, we
plan to implement an extrinsic task-performance
evaluation in the dynamic domain. Even with
the large potential cost-savings based on the re-
sults reported in this paper, extrinsic evaluations
will remain costly. Thus one important future task
for extrinsic evaluations will be to investigate the
correlation between extrinsic and intrinsic evalua-
tion metrics. This in turn will enable the use of
cost-effective intrinsic evaluations whose results
are strongly correlated to task-performance eval-
uations. This paper made an important contribu-
tion by pointing the direction for further research
in extrinsic evaluations in the dynamic domain.
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironments (GIVE). In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 165?173.
Aoife Cahill and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired lfg approximations. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033?
1040.
Barbara Di Eugenio, Pamela. W. Jordan, Richmond H.
Thomason, and Johanna. D Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Mary Ellen Foster, Manuel Giuliani, Amy Isard, Colin
Matheson, Jon Oberlander, and Alois Knoll. 2009.
Evaluating description and reference strategies in a
cooperative human-robot dialogue system. In Pro-
ceedings of the 21st international jont conference
on Artifical intelligence (IJCAI 2009), pages 1818?
1823.
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In Proceed-
ings of the 5th International Natural Language Gen-
eration Conference (INLG 2008), pages 95?103.
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 174?182.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. Journal of Artificial
Intelligence Research, 24:157?194.
Imtiaz Hussain Khan, Kees van Deemter, Graeme
Ritchie, Albert Gatt, and Alexandra A. Cleland.
2009. A hearer-oriented evaluation of referring ex-
pression generation. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 98?101.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jon
Oberlander, and Johanna Moore. 2009. Validating
the web-based evaluation of nlg systems. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 301?304.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics (ACL 2002), pages 311?318.
Ivandre? Paraboni, Judith Masthoff, and Kees van
Deemter. 2006. Overspecified reference in hierar-
chical domains: Measuring the benefits for readers.
In Proceedings of the 4th International Natural Lan-
guage Generation Conference (INLG 2006), pages
55?62.
Paul L.A. Piwek. 2007. Modality choise for generation
of referring acts. In Proceedings of the Workshop on
Multimodal Output Generation (MOG 2007), pages
129?139.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceesings of 2nd International Natural Language
Generation Conference (INLG 2002), pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl M. Osman.
2003. Lessons from a failure: generating tailored
smoking cessation letters. Artificial Intelligence,
144(1-2):41?58.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1-2):137?169.
Michael F. Schober, Herbert, and H. Clark. 1989. Un-
derstanding by addressees and overhearers. Cogni-
tive Psychology, 21:211?232.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collabo-
ration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Philipp Spanger, Masaaki Yasuhara, Iida Ryu, and
Tokunaga Takenobu. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Karen Sparck Jones and Julia R. Galliers. 1996. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Springer-Verlag.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation
in the presence of variation. In Linguistics and In-
telligent Text Processing, pages 341?351. Springer-
Verlag.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Natural Language Generation
Conference (INLG 2006), pages 81?88.
Kees van Deemter. 2007. TUNA: Towards a unified
algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Ielka van der Sluis, Albert Gatt, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions: Going beyond toy domains.
In Proceedings of Recent Advances in Natural Lan-
guae Processing (RANLP 2007).
